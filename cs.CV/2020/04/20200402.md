# Arxiv Papers in cs.CV on 2020-04-02
### Image Denoising Using Sparsifying Transform Learning and Weighted Singular Values Minimization
- **Arxiv ID**: http://arxiv.org/abs/2004.00753v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.00753v1)
- **Published**: 2020-04-02 00:30:29+00:00
- **Updated**: 2020-04-02 00:30:29+00:00
- **Authors**: Yanwei Zhao, Ping Yang, Qiu Guan, Jianwei Zheng, Wanliang Wang
- **Comment**: 17 pages, 10 figures, 5 tables
- **Journal**: None
- **Summary**: In image denoising (IDN) processing, the low-rank property is usually considered as an important image prior. As a convex relaxation approximation of low rank, nuclear norm based algorithms and their variants have attracted significant attention. These algorithms can be collectively called image domain based methods, whose common drawback is the requirement of great number of iterations for some acceptable solution. Meanwhile, the sparsity of images in a certain transform domain has also been exploited in image denoising problems. Sparsity transform learning algorithms can achieve extremely fast computations as well as desirable performance. By taking both advantages of image domain and transform domain in a general framework, we propose a sparsity transform learning and weighted singular values minimization method (STLWSM) for IDN problems. The proposed method can make full use of the preponderance of both domains. For solving the non-convex cost function, we also present an efficient alternative solution for acceleration. Experimental results show that the proposed STLWSM achieves improvement both visually and quantitatively with a large margin over state-of-the-art approaches based on an alternatively single domain. It also needs much less iteration than all the image domain algorithms.



### Consistent Multiple Sequence Decoding
- **Arxiv ID**: http://arxiv.org/abs/2004.00760v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.00760v2)
- **Published**: 2020-04-02 00:43:54+00:00
- **Updated**: 2020-04-15 21:19:16+00:00
- **Authors**: Bicheng Xu, Leonid Sigal
- **Comment**: None
- **Journal**: None
- **Summary**: Sequence decoding is one of the core components of most visual-lingual models. However, typical neural decoders when faced with decoding multiple, possibly correlated, sequences of tokens resort to simple independent decoding schemes. In this paper, we introduce a consistent multiple sequence decoding architecture, which is while relatively simple, is general and allows for consistent and simultaneous decoding of an arbitrary number of sequences. Our formulation utilizes a consistency fusion mechanism, implemented using message passing in a Graph Neural Network (GNN), to aggregate context from related decoders. This context is then utilized as a secondary input, in addition to previously generated output, to make a prediction at a given step of decoding. Self-attention, in the GNN, is used to modulate the fusion mechanism locally at each node and each step in the decoding process. We show the efficacy of our consistent multiple sequence decoder on the task of dense relational image captioning and illustrate state-of-the-art performance (+ 5.2% in mAP) on the task. More importantly, we illustrate that the decoded sentences, for the same regions, are more consistent (improvement of 9.5%), while across images and regions maintain diversity.



### Scene-Adaptive Video Frame Interpolation via Meta-Learning
- **Arxiv ID**: http://arxiv.org/abs/2004.00779v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.00779v1)
- **Published**: 2020-04-02 02:46:44+00:00
- **Updated**: 2020-04-02 02:46:44+00:00
- **Authors**: Myungsub Choi, Janghoon Choi, Sungyong Baik, Tae Hyun Kim, Kyoung Mu Lee
- **Comment**: CVPR 2020
- **Journal**: None
- **Summary**: Video frame interpolation is a challenging problem because there are different scenarios for each video depending on the variety of foreground and background motion, frame rate, and occlusion. It is therefore difficult for a single network with fixed parameters to generalize across different videos. Ideally, one could have a different network for each scenario, but this is computationally infeasible for practical applications. In this work, we propose to adapt the model to each video by making use of additional information that is readily available at test time and yet has not been exploited in previous works. We first show the benefits of `test-time adaptation' through simple fine-tuning of a network, then we greatly improve its efficiency by incorporating meta-learning. We obtain significant performance gains with only a single gradient update without any additional parameters. Finally, we show that our meta-learning framework can be easily employed to any video frame interpolation network and can consistently improve its performance on multiple benchmark datasets.



### Graph-based fusion for change detection in multi-spectral images
- **Arxiv ID**: http://arxiv.org/abs/2004.00786v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.00786v1)
- **Published**: 2020-04-02 02:59:00+00:00
- **Updated**: 2020-04-02 02:59:00+00:00
- **Authors**: David Alejandro Jimenez Sierra, Hernán Darío Benítez Restrepo, Hernán Darío Vargas Cardonay, Jocelyn Chanussot
- **Comment**: Four pages conference paper, four figures
- **Journal**: None
- **Summary**: In this paper we address the problem of change detection in multi-spectral images by proposing a data-driven framework of graph-based data fusion. The main steps of the proposed approach are: (i) The generation of a multi-temporal pixel based graph, by the fusion of intra-graphs of each temporal data; (ii) the use of Nystr\"om extension to obtain the eigenvalues and eigenvectors of the fused graph, and the selection of the final change map. We validated our approach in two real cases of remote sensing according to both qualitative and quantitative analyses. The results confirm the potential of the proposed graph-based change detection algorithm outperforming state-of-the-art methods.



### Alleviating Semantic-level Shift: A Semi-supervised Domain Adaptation Method for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2004.00794v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.00794v2)
- **Published**: 2020-04-02 03:25:05+00:00
- **Updated**: 2020-06-09 22:38:27+00:00
- **Authors**: Zhonghao Wang, Yunchao Wei, Rogerior Feris, Jinjun Xiong, Wen-Mei Hwu, Thomas S. Huang, Humphrey Shi
- **Comment**: CVPRW 2020
- **Journal**: None
- **Summary**: Learning segmentation from synthetic data and adapting to real data can significantly relieve human efforts in labelling pixel-level masks. A key challenge of this task is how to alleviate the data distribution discrepancy between the source and target domains, i.e. reducing domain shift. The common approach to this problem is to minimize the discrepancy between feature distributions from different domains through adversarial training. However, directly aligning the feature distribution globally cannot guarantee consistency from a local view (i.e. semantic-level), which prevents certain semantic knowledge learned on the source domain from being applied to the target domain. To tackle this issue, we propose a semi-supervised approach named Alleviating Semantic-level Shift (ASS), which can successfully promote the distribution consistency from both global and local views. Specifically, leveraging a small number of labeled data from the target domain, we directly extract semantic-level feature representations from both the source and the target domains by averaging the features corresponding to same categories advised by pixel-level masks. We then feed the produced features to the discriminator to conduct semantic-level adversarial learning, which collaborates with the adversarial learning from the global view to better alleviate the domain shift. We apply our ASS to two domain adaptation tasks, from GTA5 to Cityscapes and from Synthia to Cityscapes. Extensive experiments demonstrate that: (1) ASS can significantly outperform the current unsupervised state-of-the-arts by employing a small number of annotated samples from the target domain; (2) ASS can beat the oracle model trained on the whole target dataset by over 3 points by augmenting the synthetic source data with annotated samples from the target domain without suffering from the prevalent problem of overfitting to the source domain.



### SSHFD: Single Shot Human Fall Detection with Occluded Joints Resilience
- **Arxiv ID**: http://arxiv.org/abs/2004.00797v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.00797v2)
- **Published**: 2020-04-02 03:41:39+00:00
- **Updated**: 2020-04-03 02:45:33+00:00
- **Authors**: Umar Asif, Stefan Von Cavallar, Jianbin Tang, Stefan Harrer
- **Comment**: None
- **Journal**: None
- **Summary**: Falling can have fatal consequences for elderly people especially if the fallen person is unable to call for help due to loss of consciousness or any injury. Automatic fall detection systems can assist through prompt fall alarms and by minimizing the fear of falling when living independently at home. Existing vision-based fall detection systems lack generalization to unseen environments due to challenges such as variations in physical appearances, different camera viewpoints, occlusions, and background clutter. In this paper, we explore ways to overcome the above challenges and present Single Shot Human Fall Detector (SSHFD), a deep learning based framework for automatic fall detection from a single image. This is achieved through two key innovations. First, we present a human pose based fall representation which is invariant to appearance characteristics. Second, we present neural network models for 3d pose estimation and fall recognition which are resilient to missing joints due to occluded body parts. Experiments on public fall datasets show that our framework successfully transfers knowledge of 3d pose estimation and fall recognition learnt purely from synthetic data to unseen real-world data, showcasing its generalization capability for accurate fall detection in real-world scenarios.



### Tracking by Instance Detection: A Meta-Learning Approach
- **Arxiv ID**: http://arxiv.org/abs/2004.00830v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.00830v1)
- **Published**: 2020-04-02 05:55:06+00:00
- **Updated**: 2020-04-02 05:55:06+00:00
- **Authors**: Guangting Wang, Chong Luo, Xiaoyan Sun, Zhiwei Xiong, Wenjun Zeng
- **Comment**: This paper has been accepted by CVPR'20 as an oral
- **Journal**: None
- **Summary**: We consider the tracking problem as a special type of object detection problem, which we call instance detection. With proper initialization, a detector can be quickly converted into a tracker by learning the new instance from a single image. We find that model-agnostic meta-learning (MAML) offers a strategy to initialize the detector that satisfies our needs. We propose a principled three-step approach to build a high-performance tracker. First, pick any modern object detector trained with gradient descent. Second, conduct offline training (or initialization) with MAML. Third, perform domain adaptation using the initial frame. We follow this procedure to build two trackers, named Retina-MAML and FCOS-MAML, based on two modern detectors RetinaNet and FCOS. Evaluations on four benchmarks show that both trackers are competitive against state-of-the-art trackers. On OTB-100, Retina-MAML achieves the highest ever AUC of 0.712. On TrackingNet, FCOS-MAML ranks the first on the leader board with an AUC of 0.757 and the normalized precision of 0.822. Both trackers run in real-time at 40 FPS.



### Improving 3D Object Detection through Progressive Population Based Augmentation
- **Arxiv ID**: http://arxiv.org/abs/2004.00831v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.00831v2)
- **Published**: 2020-04-02 05:57:02+00:00
- **Updated**: 2020-07-16 19:07:15+00:00
- **Authors**: Shuyang Cheng, Zhaoqi Leng, Ekin Dogus Cubuk, Barret Zoph, Chunyan Bai, Jiquan Ngiam, Yang Song, Benjamin Caine, Vijay Vasudevan, Congcong Li, Quoc V. Le, Jonathon Shlens, Dragomir Anguelov
- **Comment**: Accepted at ECCV 2020
- **Journal**: None
- **Summary**: Data augmentation has been widely adopted for object detection in 3D point clouds. However, all previous related efforts have focused on manually designing specific data augmentation methods for individual architectures. In this work, we present the first attempt to automate the design of data augmentation policies for 3D object detection. We introduce the Progressive Population Based Augmentation (PPBA) algorithm, which learns to optimize augmentation strategies by narrowing down the search space and adopting the best parameters discovered in previous iterations. On the KITTI 3D detection test set, PPBA improves the StarNet detector by substantial margins on the moderate difficulty category of cars, pedestrians, and cyclists, outperforming all current state-of-the-art single-stage detection models. Additional experiments on the Waymo Open Dataset indicate that PPBA continues to effectively improve the StarNet and PointPillars detectors on a 20x larger dataset compared to KITTI. The magnitude of the improvements may be comparable to advances in 3D perception architectures and the gains come without an incurred cost at inference time. In subsequent experiments, we find that PPBA may be up to 10x more data efficient than baseline 3D detection models without augmentation, highlighting that 3D detection models may achieve competitive accuracy with far fewer labeled examples.



### Learning Formation of Physically-Based Face Attributes
- **Arxiv ID**: http://arxiv.org/abs/2004.03458v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.03458v2)
- **Published**: 2020-04-02 07:01:30+00:00
- **Updated**: 2020-04-24 03:29:48+00:00
- **Authors**: Ruilong Li, Karl Bladin, Yajie Zhao, Chinmay Chinara, Owen Ingraham, Pengda Xiang, Xinglei Ren, Pratusha Prasad, Bipin Kishore, Jun Xing, Hao Li
- **Comment**: 12 pages, 16 figures
- **Journal**: None
- **Summary**: Based on a combined data set of 4000 high resolution facial scans, we introduce a non-linear morphable face model, capable of producing multifarious face geometry of pore-level resolution, coupled with material attributes for use in physically-based rendering. We aim to maximize the variety of face identities, while increasing the robustness of correspondence between unique components, including middle-frequency geometry, albedo maps, specular intensity maps and high-frequency displacement details. Our deep learning based generative model learns to correlate albedo and geometry, which ensures the anatomical correctness of the generated assets. We demonstrate potential use of our generative model for novel identity generation, model fitting, interpolation, animation, high fidelity data visualization, and low-to-high resolution data domain transferring. We hope the release of this generative model will encourage further cooperation between all graphics, vision, and data focused professionals while demonstrating the cumulative value of every individual's complete biometric profile.



### Robust Single-Image Super-Resolution via CNNs and TV-TV Minimization
- **Arxiv ID**: http://arxiv.org/abs/2004.00843v1
- **DOI**: 10.1109/TIP.2021.3108907
- **Categories**: **cs.CV**, cs.LG, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/2004.00843v1)
- **Published**: 2020-04-02 07:06:55+00:00
- **Updated**: 2020-04-02 07:06:55+00:00
- **Authors**: Marija Vella, João F. C. Mota
- **Comment**: Under peer review
- **Journal**: None
- **Summary**: Single-image super-resolution is the process of increasing the resolution of an image, obtaining a high-resolution (HR) image from a low-resolution (LR) one. By leveraging large training datasets, convolutional neural networks (CNNs) currently achieve the state-of-the-art performance in this task. Yet, during testing/deployment, they fail to enforce consistency between the HR and LR images: if we downsample the output HR image, it never matches its LR input. Based on this observation, we propose to post-process the CNN outputs with an optimization problem that we call TV-TV minimization, which enforces consistency. As our extensive experiments show, such post-processing not only improves the quality of the images, in terms of PSNR and SSIM, but also makes the super-resolution task robust to operator mismatch, i.e., when the true downsampling operator is different from the one used to create the training dataset.



### Occlusion-Aware Depth Estimation with Adaptive Normal Constraints
- **Arxiv ID**: http://arxiv.org/abs/2004.00845v4
- **DOI**: 10.1007/978-3-030-58545-7_37
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.00845v4)
- **Published**: 2020-04-02 07:10:45+00:00
- **Updated**: 2021-07-12 16:23:55+00:00
- **Authors**: Xiaoxiao Long, Lingjie Liu, Christian Theobalt, Wenping Wang
- **Comment**: ECCV 2020
- **Journal**: ECCV 2020. Lecture Notes in Computer Science, vol 12354. Springer,
  Cham
- **Summary**: We present a new learning-based method for multi-frame depth estimation from a color video, which is a fundamental problem in scene understanding, robot navigation or handheld 3D reconstruction. While recent learning-based methods estimate depth at high accuracy, 3D point clouds exported from their depth maps often fail to preserve important geometric feature (e.g., corners, edges, planes) of man-made scenes. Widely-used pixel-wise depth errors do not specifically penalize inconsistency on these features. These inaccuracies are particularly severe when subsequent depth reconstructions are accumulated in an attempt to scan a full environment with man-made objects with this kind of features. Our depth estimation algorithm therefore introduces a Combined Normal Map (CNM) constraint, which is designed to better preserve high-curvature features and global planar regions. In order to further improve the depth estimation accuracy, we introduce a new occlusion-aware strategy that aggregates initial depth predictions from multiple adjacent views into one final depth map and one occlusion probability map for the current reference view. Our method outperforms the state-of-the-art in terms of depth estimation accuracy, and preserves essential geometric features of man-made indoor scenes much better than other algorithms.



### Pixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal Transformers
- **Arxiv ID**: http://arxiv.org/abs/2004.00849v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2004.00849v2)
- **Published**: 2020-04-02 07:39:28+00:00
- **Updated**: 2020-06-22 09:09:22+00:00
- **Authors**: Zhicheng Huang, Zhaoyang Zeng, Bei Liu, Dongmei Fu, Jianlong Fu
- **Comment**: None
- **Journal**: None
- **Summary**: We propose Pixel-BERT to align image pixels with text by deep multi-modal transformers that jointly learn visual and language embedding in a unified end-to-end framework. We aim to build a more accurate and thorough connection between image pixels and language semantics directly from image and sentence pairs instead of using region-based image features as the most recent vision and language tasks. Our Pixel-BERT which aligns semantic connection in pixel and text level solves the limitation of task-specific visual representation for vision and language tasks. It also relieves the cost of bounding box annotations and overcomes the unbalance between semantic labels in visual task and language semantic. To provide a better representation for down-stream tasks, we pre-train a universal end-to-end model with image and sentence pairs from Visual Genome dataset and MS-COCO dataset. We propose to use a random pixel sampling mechanism to enhance the robustness of visual representation and to apply the Masked Language Model and Image-Text Matching as pre-training tasks. Extensive experiments on downstream tasks with our pre-trained model show that our approach makes the most state-of-the-arts in downstream tasks, including Visual Question Answering (VQA), image-text retrieval, Natural Language for Visual Reasoning for Real (NLVR). Particularly, we boost the performance of a single model in VQA task by 2.17 points compared with SOTA under fair comparison.



### Detection of Coronavirus (COVID-19) Associated Pneumonia based on Generative Adversarial Networks and a Fine-Tuned Deep Transfer Learning Model using Chest X-ray Dataset
- **Arxiv ID**: http://arxiv.org/abs/2004.01184v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.01184v1)
- **Published**: 2020-04-02 08:14:37+00:00
- **Updated**: 2020-04-02 08:14:37+00:00
- **Authors**: Nour Eldeen M. Khalifa, Mohamed Hamed N. Taha, Aboul Ella Hassanien, Sally Elghamrawy
- **Comment**: 15 pages, 3 Tables and 10 Figures
- **Journal**: None
- **Summary**: The COVID-19 coronavirus is one of the devastating viruses according to the world health organization. This novel virus leads to pneumonia, which is an infection that inflames the lungs' air sacs of a human. One of the methods to detect those inflames is by using x-rays for the chest. In this paper, a pneumonia chest x-ray detection based on generative adversarial networks (GAN) with a fine-tuned deep transfer learning for a limited dataset will be presented. The use of GAN positively affects the proposed model robustness and made it immune to the overfitting problem and helps in generating more images from the dataset. The dataset used in this research consists of 5863 X-ray images with two categories: Normal and Pneumonia. This research uses only 10% of the dataset for training data and generates 90% of images using GAN to prove the efficiency of the proposed model. Through the paper, AlexNet, GoogLeNet, Squeeznet, and Resnet18 are selected as deep transfer learning models to detect the pneumonia from chest x-rays. Those models are selected based on their small number of layers on their architectures, which will reflect in reducing the complexity of the models and the consumed memory and time. Using a combination of GAN and deep transfer models proved it is efficiency according to testing accuracy measurement. The research concludes that the Resnet18 is the most appropriate deep transfer model according to testing accuracy measurement and achieved 99% with the other performance metrics such as precision, recall, and F1 score while using GAN as an image augmenter. Finally, a comparison result was carried out at the end of the research with related work which used the same dataset except that this research used only 10% of original dataset. The presented work achieved a superior result than the related work in terms of testing accuracy.



### End-To-End Convolutional Neural Network for 3D Reconstruction of Knee Bones From Bi-Planar X-Ray Images
- **Arxiv ID**: http://arxiv.org/abs/2004.00871v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.00871v2)
- **Published**: 2020-04-02 08:37:11+00:00
- **Updated**: 2020-08-12 17:20:56+00:00
- **Authors**: Yoni Kasten, Daniel Doktofsky, Ilya Kovler
- **Comment**: None
- **Journal**: None
- **Summary**: We present an end-to-end Convolutional Neural Network (CNN) approach for 3D reconstruction of knee bones directly from two bi-planar X-ray images. Clinically, capturing the 3D models of the bones is crucial for surgical planning, implant fitting, and postoperative evaluation. X-ray imaging significantly reduces the exposure of patients to ionizing radiation compared to Computer Tomography (CT) imaging, and is much more common and inexpensive compared to Magnetic Resonance Imaging (MRI) scanners. However, retrieving 3D models from such 2D scans is extremely challenging. In contrast to the common approach of statistically modeling the shape of each bone, our deep network learns the distribution of the bones' shapes directly from the training images. We train our model with both supervised and unsupervised losses using Digitally Reconstructed Radiograph (DRR) images generated from CT scans. To apply our model to X-Ray data, we use style transfer to transform between X-Ray and DRR modalities. As a result, at test time, without further optimization, our solution directly outputs a 3D reconstruction from a pair of bi-planar X-ray images, while preserving geometric constraints. Our results indicate that our deep learning model is very efficient, generalizes well and produces high quality reconstructions.



### Go Fetch: Mobile Manipulation in Unstructured Environments
- **Arxiv ID**: http://arxiv.org/abs/2004.00899v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.00899v1)
- **Published**: 2020-04-02 09:33:59+00:00
- **Updated**: 2020-04-02 09:33:59+00:00
- **Authors**: Kenneth Blomqvist, Michel Breyer, Andrei Cramariuc, Julian Förster, Margarita Grinvald, Florian Tschopp, Jen Jen Chung, Lionel Ott, Juan Nieto, Roland Siegwart
- **Comment**: Kenneth Blomqvist, Michel Breyer, Andrei Cramariuc, Julian F\"orster,
  Margarita Grinvald, and Florian Tschopp contributed equally to this work
- **Journal**: ICRA 2020 Workshop on Perception, Action, Learning: From
  Metric-Semantic Scene Understanding to High-level Task Execution, 2020
- **Summary**: With humankind facing new and increasingly large-scale challenges in the medical and domestic spheres, automation of the service sector carries a tremendous potential for improved efficiency, quality, and safety of operations. Mobile robotics can offer solutions with a high degree of mobility and dexterity, however these complex systems require a multitude of heterogeneous components to be carefully integrated into one consistent framework. This work presents a mobile manipulation system that combines perception, localization, navigation, motion planning and grasping skills into one common workflow for fetch and carry applications in unstructured indoor environments. The tight integration across the various modules is experimentally demonstrated on the task of finding a commonly available object in an office environment, grasping it, and delivering it to a desired drop-off location. The accompanying video is available at https://youtu.be/e89_Xg1sLnY.



### Learning to Segment the Tail
- **Arxiv ID**: http://arxiv.org/abs/2004.00900v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.00900v2)
- **Published**: 2020-04-02 09:39:08+00:00
- **Updated**: 2020-04-03 03:27:08+00:00
- **Authors**: Xinting Hu, Yi Jiang, Kaihua Tang, Jingyuan Chen, Chunyan Miao, Hanwang Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Real-world visual recognition requires handling the extreme sample imbalance in large-scale long-tailed data. We propose a "divide&conquer" strategy for the challenging LVIS task: divide the whole data into balanced parts and then apply incremental learning to conquer each one. This derives a novel learning paradigm: class-incremental few-shot learning, which is especially effective for the challenge evolving over time: 1) the class imbalance among the old-class knowledge review and 2) the few-shot data in new-class learning. We call our approach Learning to Segment the Tail (LST). In particular, we design an instance-level balanced replay scheme, which is a memory-efficient approximation to balance the instance-level samples from the old-class images. We also propose to use a meta-module for new-class learning, where the module parameters are shared across incremental phases, gaining the learning-to-learn knowledge incrementally, from the data-rich head to the data-poor tail. We empirically show that: at the expense of a little sacrifice of head-class forgetting, we can gain a significant 8.3% AP improvement for the tail classes with less than 10 instances, achieving an overall 2.0% AP boost for the whole 1,230 classes.



### Learning Representations For Images With Hierarchical Labels
- **Arxiv ID**: http://arxiv.org/abs/2004.00909v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.00909v2)
- **Published**: 2020-04-02 09:56:03+00:00
- **Updated**: 2020-04-11 17:51:39+00:00
- **Authors**: Ankit Dhall
- **Comment**: Master thesis
- **Journal**: None
- **Summary**: Image classification has been studied extensively but there has been limited work in the direction of using non-conventional, external guidance other than traditional image-label pairs to train such models. In this thesis we present a set of methods to leverage information about the semantic hierarchy induced by class labels. In the first part of the thesis, we inject label-hierarchy knowledge to an arbitrary classifier and empirically show that availability of such external semantic information in conjunction with the visual semantics from images boosts overall performance. Taking a step further in this direction, we model more explicitly the label-label and label-image interactions by using order-preserving embedding-based models, prevalent in natural language, and tailor them to the domain of computer vision to perform image classification. Although, contrasting in nature, both the CNN-classifiers injected with hierarchical information, and the embedding-based models outperform a hierarchy-agnostic model on the newly presented, real-world ETH Entomological Collection image dataset https://www.research-collection.ethz.ch/handle/20.500.11850/365379.



### Controllable Orthogonalization in Training DNNs
- **Arxiv ID**: http://arxiv.org/abs/2004.00917v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.00917v1)
- **Published**: 2020-04-02 10:14:27+00:00
- **Updated**: 2020-04-02 10:14:27+00:00
- **Authors**: Lei Huang, Li Liu, Fan Zhu, Diwen Wan, Zehuan Yuan, Bo Li, Ling Shao
- **Comment**: Accepted to CVPR 2020. The Code is available at
  https://github.com/huangleiBuaa/ONI
- **Journal**: None
- **Summary**: Orthogonality is widely used for training deep neural networks (DNNs) due to its ability to maintain all singular values of the Jacobian close to 1 and reduce redundancy in representation. This paper proposes a computationally efficient and numerically stable orthogonalization method using Newton's iteration (ONI), to learn a layer-wise orthogonal weight matrix in DNNs. ONI works by iteratively stretching the singular values of a weight matrix towards 1. This property enables it to control the orthogonality of a weight matrix by its number of iterations. We show that our method improves the performance of image classification networks by effectively controlling the orthogonality to provide an optimal tradeoff between optimization benefits and representational capacity reduction. We also show that ONI stabilizes the training of generative adversarial networks (GANs) by maintaining the Lipschitz continuity of a network, similar to spectral normalization (SN), and further outperforms SN by providing controllable orthogonality.



### Hierarchical Image Classification using Entailment Cone Embeddings
- **Arxiv ID**: http://arxiv.org/abs/2004.03459v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.03459v2)
- **Published**: 2020-04-02 10:22:02+00:00
- **Updated**: 2020-04-25 12:56:07+00:00
- **Authors**: Ankit Dhall, Anastasia Makarova, Octavian Ganea, Dario Pavllo, Michael Greeff, Andreas Krause
- **Comment**: Accepted in the CVPR 2020 Workshop on Differential Geometry in
  Computer Vision and Machine Learning
- **Journal**: None
- **Summary**: Image classification has been studied extensively, but there has been limited work in using unconventional, external guidance other than traditional image-label pairs for training. We present a set of methods for leveraging information about the semantic hierarchy embedded in class labels. We first inject label-hierarchy knowledge into an arbitrary CNN-based classifier and empirically show that availability of such external semantic information in conjunction with the visual semantics from images boosts overall performance. Taking a step further in this direction, we model more explicitly the label-label and label-image interactions using order-preserving embeddings governed by both Euclidean and hyperbolic geometries, prevalent in natural language, and tailor them to hierarchical image classification and representation learning. We empirically validate all the models on the hierarchical ETHEC dataset.



### PaStaNet: Toward Human Activity Knowledge Engine
- **Arxiv ID**: http://arxiv.org/abs/2004.00945v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.00945v2)
- **Published**: 2020-04-02 11:35:59+00:00
- **Updated**: 2020-04-21 11:55:17+00:00
- **Authors**: Yong-Lu Li, Liang Xu, Xinpeng Liu, Xijie Huang, Yue Xu, Shiyi Wang, Hao-Shu Fang, Ze Ma, Mingyang Chen, Cewu Lu
- **Comment**: Accepted to CVPR 2020, supplementary materials included, code
  available: http://hake-mvig.cn/
- **Journal**: None
- **Summary**: Existing image-based activity understanding methods mainly adopt direct mapping, i.e. from image to activity concepts, which may encounter performance bottleneck since the huge gap. In light of this, we propose a new path: infer human part states first and then reason out the activities based on part-level semantics. Human Body Part States (PaSta) are fine-grained action semantic tokens, e.g. <hand, hold, something>, which can compose the activities and help us step toward human activity knowledge engine. To fully utilize the power of PaSta, we build a large-scale knowledge base PaStaNet, which contains 7M+ PaSta annotations. And two corresponding models are proposed: first, we design a model named Activity2Vec to extract PaSta features, which aim to be general representations for various activities. Second, we use a PaSta-based Reasoning method to infer activities. Promoted by PaStaNet, our method achieves significant improvements, e.g. 6.4 and 13.9 mAP on full and one-shot sets of HICO in supervised learning, and 3.2 and 4.2 mAP on V-COCO and images-based AVA in transfer learning. Code and data are available at http://hake-mvig.cn/.



### RSS-Net: Weakly-Supervised Multi-Class Semantic Segmentation with FMCW Radar
- **Arxiv ID**: http://arxiv.org/abs/2004.03451v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2004.03451v1)
- **Published**: 2020-04-02 11:40:26+00:00
- **Updated**: 2020-04-02 11:40:26+00:00
- **Authors**: Prannay Kaul, Daniele De Martini, Matthew Gadd, Paul Newman
- **Comment**: submitted to IEEE Intelligent Vehicles Symposium (IV) 2020
- **Journal**: None
- **Summary**: This paper presents an efficient annotation procedure and an application thereof to end-to-end, rich semantic segmentation of the sensed environment using FMCW scanning radar. We advocate radar over the traditional sensors used for this task as it operates at longer ranges and is substantially more robust to adverse weather and illumination conditions. We avoid laborious manual labelling by exploiting the largest radar-focused urban autonomy dataset collected to date, correlating radar scans with RGB cameras and LiDAR sensors, for which semantic segmentation is an already consolidated procedure. The training procedure leverages a state-of-the-art natural image segmentation system which is publicly available and as such, in contrast to previous approaches, allows for the production of copious labels for the radar stream by incorporating four camera and two LiDAR streams. Additionally, the losses are computed taking into account labels to the radar sensor horizon by accumulating LiDAR returns along a pose-chain ahead and behind of the current vehicle position. Finally, we present the network with multi-channel radar scan inputs in order to deal with ephemeral and dynamic scene objects.



### DualConvMesh-Net: Joint Geodesic and Euclidean Convolutions on 3D Meshes
- **Arxiv ID**: http://arxiv.org/abs/2004.01002v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4.6; I.4.10; I.2.6; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2004.01002v1)
- **Published**: 2020-04-02 13:52:00+00:00
- **Updated**: 2020-04-02 13:52:00+00:00
- **Authors**: Jonas Schult, Francis Engelmann, Theodora Kontogianni, Bastian Leibe
- **Comment**: CVPR 2020 camera-ready version
- **Journal**: None
- **Summary**: We propose DualConvMesh-Nets (DCM-Net) a family of deep hierarchical convolutional networks over 3D geometric data that combines two types of convolutions. The first type, geodesic convolutions, defines the kernel weights over mesh surfaces or graphs. That is, the convolutional kernel weights are mapped to the local surface of a given mesh. The second type, Euclidean convolutions, is independent of any underlying mesh structure. The convolutional kernel is applied on a neighborhood obtained from a local affinity representation based on the Euclidean distance between 3D points. Intuitively, geodesic convolutions can easily separate objects that are spatially close but have disconnected surfaces, while Euclidean convolutions can represent interactions between nearby objects better, as they are oblivious to object surfaces. To realize a multi-resolution architecture, we borrow well-established mesh simplification methods from the geometry processing domain and adapt them to define mesh-preserving pooling and unpooling operations. We experimentally show that combining both types of convolutions in our architecture leads to significant performance gains for 3D semantic segmentation, and we report competitive results on three scene segmentation benchmarks. Our models and code are publicly available.



### Introducing Anisotropic Minkowski Functionals and Quantitative Anisotropy Measures for Local Structure Analysis in Biomedical Imaging
- **Arxiv ID**: http://arxiv.org/abs/2004.01185v1
- **DOI**: 10.1117/86720I-86720I-8
- **Categories**: **eess.IV**, cs.CV, cs.LG, math.AT
- **Links**: [PDF](http://arxiv.org/pdf/2004.01185v1)
- **Published**: 2020-04-02 14:13:50+00:00
- **Updated**: 2020-04-02 14:13:50+00:00
- **Authors**: Axel Wismueller, Titas De, Eva Lochmueller, Felix Eckstein, Mahesh B. Nagarajan
- **Comment**: SPIE Medical Imaging 2013. arXiv admin note: text overlap with
  arXiv:2002.07156
- **Journal**: None
- **Summary**: The ability of Minkowski Functionals to characterize local structure in different biological tissue types has been demonstrated in a variety of medical image processing tasks. We introduce anisotropic Minkowski Functionals (AMFs) as a novel variant that captures the inherent anisotropy of the underlying gray-level structures. To quantify the anisotropy characterized by our approach, we further introduce a method to compute a quantitative measure motivated by a technique utilized in MR diffusion tensor imaging, namely fractional anisotropy. We showcase the applicability of our method in the research context of characterizing the local structure properties of trabecular bone micro-architecture in the proximal femur as visualized on multi-detector CT. To this end, AMFs were computed locally for each pixel of ROIs extracted from the head, neck and trochanter regions. Fractional anisotropy was then used to quantify the local anisotropy of the trabecular structures found in these ROIs and to compare its distribution in different anatomical regions. Our results suggest a significantly greater concentration of anisotropic trabecular structures in the head and neck regions when compared to the trochanter region (p < 10-4). We also evaluated the ability of such AMFs to predict bone strength in the femoral head of proximal femur specimens obtained from 50 donors. Our results suggest that such AMFs, when used in conjunction with multi-regression models, can outperform more conventional features such as BMD in predicting failure load. We conclude that such anisotropic Minkowski Functionals can capture valuable information regarding directional attributes of local structure, which may be useful in a wide scope of biomedical imaging applications.



### Face Quality Estimation and Its Correlation to Demographic and Non-Demographic Bias in Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2004.01019v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.01019v3)
- **Published**: 2020-04-02 14:19:12+00:00
- **Updated**: 2020-07-10 11:24:37+00:00
- **Authors**: Philipp Terhörst, Jan Niklas Kolf, Naser Damer, Florian Kirchbuchner, Arjan Kuijper
- **Comment**: Accepted at IJCB2020
- **Journal**: None
- **Summary**: Face quality assessment aims at estimating the utility of a face image for the purpose of recognition. It is a key factor to achieve high face recognition performances. Currently, the high performance of these face recognition systems come with the cost of a strong bias against demographic and non-demographic sub-groups. Recent work has shown that face quality assessment algorithms should adapt to the deployed face recognition system, in order to achieve highly accurate and robust quality estimations. However, this could lead to a bias transfer towards the face quality assessment leading to discriminatory effects e.g. during enrolment. In this work, we present an in-depth analysis of the correlation between bias in face recognition and face quality assessment. Experiments were conducted on two publicly available datasets captured under controlled and uncontrolled circumstances with two popular face embeddings. We evaluated four state-of-the-art solutions for face quality assessment towards biases to pose, ethnicity, and age. The experiments showed that the face quality assessment solutions assign significantly lower quality values towards subgroups affected by the recognition bias demonstrating that these approaches are biased as well. This raises ethical questions towards fairness and discrimination which future works have to address.



### Multi-Modal Video Forensic Platform for Investigating Post-Terrorist Attack Scenarios
- **Arxiv ID**: http://arxiv.org/abs/2004.01023v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV, cs.CY, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2004.01023v1)
- **Published**: 2020-04-02 14:29:27+00:00
- **Updated**: 2020-04-02 14:29:27+00:00
- **Authors**: Alexander Schindler, Andrew Lindley, Anahid Jalali, Martin Boyer, Sergiu Gordea, Ross King
- **Comment**: None
- **Journal**: In Proceedings of the 11th ACM Multimedia Systems Conference
  (MMSys2020), June 06-11, 2020, Istanbul, Turkey
- **Summary**: The forensic investigation of a terrorist attack poses a significant challenge to the investigative authorities, as often several thousand hours of video footage must be viewed. Large scale Video Analytic Platforms (VAP) assist law enforcement agencies (LEA) in identifying suspects and securing evidence. Current platforms focus primarily on the integration of different computer vision methods and thus are restricted to a single modality. We present a video analytic platform that integrates visual and audio analytic modules and fuses information from surveillance cameras and video uploads from eyewitnesses. Videos are analyzed according their acoustic and visual content. Specifically, Audio Event Detection is applied to index the content according to attack-specific acoustic concepts. Audio similarity search is utilized to identify similar video sequences recorded from different perspectives. Visual object detection and tracking are used to index the content according to relevant concepts. Innovative user-interface concepts are introduced to harness the full potential of the heterogeneous results of the analytical modules, allowing investigators to more quickly follow-up on leads and eyewitness reports.



### Introducing Anisotropic Minkowski Functionals for Local Structure Analysis and Prediction of Biomechanical Strength of Proximal Femur Specimens
- **Arxiv ID**: http://arxiv.org/abs/2004.01029v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.01029v1)
- **Published**: 2020-04-02 14:33:03+00:00
- **Updated**: 2020-04-02 14:33:03+00:00
- **Authors**: Titas De
- **Comment**: None
- **Journal**: Master's Thesis - 2013
- **Summary**: Bone fragility and fracture caused by osteoporosis or injury are prevalent in adults over the age of 50 and can reduce their quality of life. Hence, predicting the biomechanical bone strength, specifically of the proximal femur, through non-invasive imaging-based methods is an important goal for the diagnosis of Osteoporosis as well as estimating fracture risk. Dual X-ray absorptiometry (DXA) has been used as a standard clinical procedure for assessment and diagnosis of bone strength and osteoporosis through bone mineral density (BMD) measurements. However, previous studies have shown that quantitative computer tomography (QCT) can be more sensitive and specific to trabecular bone characterization because it reduces the overlap effects and interferences from the surrounding soft tissue and cortical shell.   This study proposes a new method to predict the bone strength of proximal femur specimens from quantitative multi-detector computer tomography (MDCT) images. Texture analysis methods such as conventional statistical moments (BMD mean), Isotropic Minkowski Functionals (IMF) and Anisotropic Minkowski Functionals (AMF) are used to quantify BMD properties of the trabecular bone micro-architecture. Combinations of these extracted features are then used to predict the biomechanical strength of the femur specimens using sophisticated machine learning techniques such as multiregression (MultiReg) and support vector regression with linear kernel (SVRlin). The prediction performance achieved with these feature sets is compared to the standard approach that uses the mean BMD of the specimens and multiregression models using root mean square error (RMSE).



### Effect of Annotation Errors on Drone Detection with YOLOv3
- **Arxiv ID**: http://arxiv.org/abs/2004.01059v4
- **DOI**: 10.1109/CVPRW50498.2020.00523
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.01059v4)
- **Published**: 2020-04-02 15:06:14+00:00
- **Updated**: 2021-01-12 14:54:00+00:00
- **Authors**: Aybora Koksal, Kutalmis Gokalp Ince, A. Aydin Alatan
- **Comment**: Best Paper Award at The 1st Anti-UAV Workshop & Challenge - CVPR
  Workshops, 2020
- **Journal**: None
- **Summary**: Following the recent advances in deep networks, object detection and tracking algorithms with deep learning backbones have been improved significantly; however, this rapid development resulted in the necessity of large amounts of annotated labels. Even if the details of such semi-automatic annotation processes for most of these datasets are not known precisely, especially for the video annotations, some automated labeling processes are usually employed. Unfortunately, such approaches might result with erroneous annotations. In this work, different types of annotation errors for object detection problem are simulated and the performance of a popular state-of-the-art object detector, YOLOv3, with erroneous annotations during training and testing stages is examined. Moreover, some inevitable annotation errors in CVPR-2020 Anti-UAV Challenge dataset is also examined in this manner, while proposing a solution to correct such annotation errors of this valuable data set.



### Model-based occlusion disentanglement for image-to-image translation
- **Arxiv ID**: http://arxiv.org/abs/2004.01071v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.01071v2)
- **Published**: 2020-04-02 15:24:41+00:00
- **Updated**: 2020-07-20 09:27:54+00:00
- **Authors**: Fabio Pizzati, Pietro Cerri, Raoul de Charette
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: Image-to-image translation is affected by entanglement phenomena, which may occur in case of target data encompassing occlusions such as raindrops, dirt, etc. Our unsupervised model-based learning disentangles scene and occlusions, while benefiting from an adversarial pipeline to regress physical parameters of the occlusion model. The experiments demonstrate our method is able to handle varying types of occlusions and generate highly realistic translations, qualitatively and quantitatively outperforming the state-of-the-art on multiple datasets.



### Learning Longterm Representations for Person Re-Identification Using Radio Signals
- **Arxiv ID**: http://arxiv.org/abs/2004.01091v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.01091v1)
- **Published**: 2020-04-02 15:50:42+00:00
- **Updated**: 2020-04-02 15:50:42+00:00
- **Authors**: Lijie Fan, Tianhong Li, Rongyao Fang, Rumen Hristov, Yuan Yuan, Dina Katabi
- **Comment**: CVPR 2020. The first three authors contributed equally to this paper
- **Journal**: None
- **Summary**: Person Re-Identification (ReID) aims to recognize a person-of-interest across different places and times. Existing ReID methods rely on images or videos collected using RGB cameras. They extract appearance features like clothes, shoes, hair, etc. Such features, however, can change drastically from one day to the next, leading to inability to identify people over extended time periods. In this paper, we introduce RF-ReID, a novel approach that harnesses radio frequency (RF) signals for longterm person ReID. RF signals traverse clothes and reflect off the human body; thus they can be used to extract more persistent human-identifying features like body size and shape. We evaluate the performance of RF-ReID on longitudinal datasets that span days and weeks, where the person may wear different clothes across days. Our experiments demonstrate that RF-ReID outperforms state-of-the-art RGB-based ReID approaches for long term person ReID. Our results also reveal two interesting features: First since RF signals work in the presence of occlusions and poor lighting, RF-ReID allows for person ReID in such scenarios. Second, unlike photos and videos which reveal personal and private information, RF signals are more privacy-preserving, and hence can help extend person ReID to privacy-concerned domains, like healthcare.



### MCEN: Bridging Cross-Modal Gap between Cooking Recipes and Dish Images with Latent Variable Model
- **Arxiv ID**: http://arxiv.org/abs/2004.01095v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2004.01095v1)
- **Published**: 2020-04-02 16:00:10+00:00
- **Updated**: 2020-04-02 16:00:10+00:00
- **Authors**: Han Fu, Rui Wu, Chenghao Liu, Jianling Sun
- **Comment**: Accepted to CVPR 2020
- **Journal**: None
- **Summary**: Nowadays, driven by the increasing concern on diet and health, food computing has attracted enormous attention from both industry and research community. One of the most popular research topics in this domain is Food Retrieval, due to its profound influence on health-oriented applications. In this paper, we focus on the task of cross-modal retrieval between food images and cooking recipes. We present Modality-Consistent Embedding Network (MCEN) that learns modality-invariant representations by projecting images and texts to the same embedding space. To capture the latent alignments between modalities, we incorporate stochastic latent variables to explicitly exploit the interactions between textual and visual features. Importantly, our method learns the cross-modal alignments during training but computes embeddings of different modalities independently at inference time for the sake of efficiency. Extensive experimental results clearly demonstrate that the proposed MCEN outperforms all existing approaches on the benchmark Recipe1M dataset and requires less computational cost.



### Map-Enhanced Ego-Lane Detection in the Missing Feature Scenarios
- **Arxiv ID**: http://arxiv.org/abs/2004.01101v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.01101v2)
- **Published**: 2020-04-02 16:06:48+00:00
- **Updated**: 2020-04-04 10:50:24+00:00
- **Authors**: Xiaoliang Wang, Yeqiang Qian, Chunxiang Wang, Ming Yang
- **Comment**: There has some mistake in experiments
- **Journal**: None
- **Summary**: As one of the most important tasks in autonomous driving systems, ego-lane detection has been extensively studied and has achieved impressive results in many scenarios. However, ego-lane detection in the missing feature scenarios is still an unsolved problem. To address this problem, previous methods have been devoted to proposing more complicated feature extraction algorithms, but they are very time-consuming and cannot deal with extreme scenarios. Different from others, this paper exploits prior knowledge contained in digital maps, which has a strong capability to enhance the performance of detection algorithms. Specifically, we employ the road shape extracted from OpenStreetMap as lane model, which is highly consistent with the real lane shape and irrelevant to lane features. In this way, only a few lane features are needed to eliminate the position error between the road shape and the real lane, and a search-based optimization algorithm is proposed. Experiments show that the proposed method can be applied to various scenarios and can run in real-time at a frequency of 20 Hz. At the same time, we evaluated the proposed method on the public KITTI Lane dataset where it achieves state-of-the-art performance. Moreover, our code will be open source after publication.



### An Attention-Based Deep Learning Model for Multiple Pedestrian Attributes Recognition
- **Arxiv ID**: http://arxiv.org/abs/2004.01110v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.01110v1)
- **Published**: 2020-04-02 16:21:14+00:00
- **Updated**: 2020-04-02 16:21:14+00:00
- **Authors**: Ehsan Yaghoubi, Diana Borza, João Neves, Aruna Kumar, Hugo Proença
- **Comment**: Submitted to Image and Vision Computing journal
- **Journal**: None
- **Summary**: The automatic characterization of pedestrians in surveillance footage is a tough challenge, particularly when the data is extremely diverse with cluttered backgrounds, and subjects are captured from varying distances, under multiple poses, with partial occlusion. Having observed that the state-of-the-art performance is still unsatisfactory, this paper provides a novel solution to the problem, with two-fold contributions: 1) considering the strong semantic correlation between the different full-body attributes, we propose a multi-task deep model that uses an element-wise multiplication layer to extract more comprehensive feature representations. In practice, this layer serves as a filter to remove irrelevant background features, and is particularly important to handle complex, cluttered data; and 2) we introduce a weighted-sum term to the loss function that not only relativizes the contribution of each task (kind of attributed) but also is crucial for performance improvement in multiple-attribute inference settings. Our experiments were performed on two well-known datasets (RAP and PETA) and point for the superiority of the proposed method with respect to the state-of-the-art. The code is available at https://github.com/Ehsan-Yaghoubi/MAN-PAR-.



### ProxyNCA++: Revisiting and Revitalizing Proxy Neighborhood Component Analysis
- **Arxiv ID**: http://arxiv.org/abs/2004.01113v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.01113v2)
- **Published**: 2020-04-02 16:24:30+00:00
- **Updated**: 2020-07-23 14:21:13+00:00
- **Authors**: Eu Wern Teh, Terrance DeVries, Graham W. Taylor
- **Comment**: To appear in the European Conference on Computer Vision (ECCV) 2020
- **Journal**: None
- **Summary**: We consider the problem of distance metric learning (DML), where the task is to learn an effective similarity measure between images. We revisit ProxyNCA and incorporate several enhancements. We find that low temperature scaling is a performance-critical component and explain why it works. Besides, we also discover that Global Max Pooling works better in general when compared to Global Average Pooling. Additionally, our proposed fast moving proxies also addresses small gradient issue of proxies, and this component synergizes well with low temperature scaling and Global Max Pooling. Our enhanced model, called ProxyNCA++, achieves a 22.9 percentage point average improvement of Recall@1 across four different zero-shot retrieval datasets compared to the original ProxyNCA algorithm. Furthermore, we achieve state-of-the-art results on the CUB200, Cars196, Sop, and InShop datasets, achieving Recall@1 scores of 72.2, 90.1, 81.4, and 90.9, respectively.



### Handling new target classes in semantic segmentation with domain adaptation
- **Arxiv ID**: http://arxiv.org/abs/2004.01130v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.01130v2)
- **Published**: 2020-04-02 16:59:57+00:00
- **Updated**: 2021-02-16 13:08:20+00:00
- **Authors**: Maxime Bucher, Tuan-Hung Vu, Matthieu Cord, Patrick Pérez
- **Comment**: Under review at CVIU
- **Journal**: None
- **Summary**: In this work, we define and address a novel domain adaptation (DA) problem in semantic scene segmentation, where the target domain not only exhibits a data distribution shift w.r.t. the source domain, but also includes novel classes that do not exist in the latter. Different to "open-set" and "universal domain adaptation", which both regard all objects from new classes as "unknown", we aim at explicit test-time prediction for these new classes. To reach this goal, we propose a framework that leverages domain adaptation and zero-shot learning techniques to enable "boundless" adaptation in the target domain. It relies on a novel architecture, along with a dedicated learning scheme, to bridge the source-target domain gap while learning how to map new classes' labels to relevant visual representations. The performance is further improved using self-training on target-domain pseudo-labels. For validation, we consider different domain adaptation set-ups, namely synthetic-2-real, country-2-country and dataset-2-dataset. Our framework outperforms the baselines by significant margins, setting competitive standards on all benchmarks for the new task. Code and models are available at https://github.com/valeoai/buda.



### Bodies at Rest: 3D Human Pose and Shape Estimation from a Pressure Image using Synthetic Data
- **Arxiv ID**: http://arxiv.org/abs/2004.01166v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.01166v1)
- **Published**: 2020-04-02 17:44:58+00:00
- **Updated**: 2020-04-02 17:44:58+00:00
- **Authors**: Henry M. Clever, Zackory Erickson, Ariel Kapusta, Greg Turk, C. Karen Liu, Charles C. Kemp
- **Comment**: 18 pages, 18 figures, 5 tables. Accepted for oral presentation at
  CVPR 2020
- **Journal**: None
- **Summary**: People spend a substantial part of their lives at rest in bed. 3D human pose and shape estimation for this activity would have numerous beneficial applications, yet line-of-sight perception is complicated by occlusion from bedding. Pressure sensing mats are a promising alternative, but training data is challenging to collect at scale. We describe a physics-based method that simulates human bodies at rest in a bed with a pressure sensing mat, and present PressurePose, a synthetic dataset with 206K pressure images with 3D human poses and shapes. We also present PressureNet, a deep learning model that estimates human pose and shape given a pressure image and gender. PressureNet incorporates a pressure map reconstruction (PMR) network that models pressure image generation to promote consistency between estimated 3D body models and pressure image input. In our evaluations, PressureNet performed well with real data from participants in diverse poses, even though it had only been trained with synthetic data. When we ablated the PMR network, performance dropped substantially.



### DOPS: Learning to Detect 3D Objects and Predict their 3D Shapes
- **Arxiv ID**: http://arxiv.org/abs/2004.01170v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.01170v2)
- **Published**: 2020-04-02 17:48:50+00:00
- **Updated**: 2020-04-07 00:40:57+00:00
- **Authors**: Mahyar Najibi, Guangda Lai, Abhijit Kundu, Zhichao Lu, Vivek Rathod, Thomas Funkhouser, Caroline Pantofaru, David Ross, Larry S. Davis, Alireza Fathi
- **Comment**: To appear in CVPR 2020
- **Journal**: None
- **Summary**: We propose DOPS, a fast single-stage 3D object detection method for LIDAR data. Previous methods often make domain-specific design decisions, for example projecting points into a bird-eye view image in autonomous driving scenarios. In contrast, we propose a general-purpose method that works on both indoor and outdoor scenes. The core novelty of our method is a fast, single-pass architecture that both detects objects in 3D and estimates their shapes. 3D bounding box parameters are estimated in one pass for every point, aggregated through graph convolutions, and fed into a branch of the network that predicts latent codes representing the shape of each detected object. The latent shape space and shape decoder are learned on a synthetic dataset and then used as supervision for the end-to-end training of the 3D object detection pipeline. Thus our model is able to extract shapes without access to ground-truth shape information in the target dataset. During experiments, we find that our proposed method achieves state-of-the-art results by ~5% on object detection in ScanNet scenes, and it gets top results by 3.4% in the Waymo Open Dataset, while reproducing the shapes of detected cars.



### Learning Unsupervised Hierarchical Part Decomposition of 3D Objects from a Single RGB Image
- **Arxiv ID**: http://arxiv.org/abs/2004.01176v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.01176v1)
- **Published**: 2020-04-02 17:58:05+00:00
- **Updated**: 2020-04-02 17:58:05+00:00
- **Authors**: Despoina Paschalidou, Luc van Gool, Andreas Geiger
- **Comment**: To appear at CVPR 2020, project page
  https://github.com/paschalidoud/hierarchical_primitives
- **Journal**: None
- **Summary**: Humans perceive the 3D world as a set of distinct objects that are characterized by various low-level (geometry, reflectance) and high-level (connectivity, adjacency, symmetry) properties. Recent methods based on convolutional neural networks (CNNs) demonstrated impressive progress in 3D reconstruction, even when using a single 2D image as input. However, the majority of these methods focuses on recovering the local 3D geometry of an object without considering its part-based decomposition or relations between parts. We address this challenging problem by proposing a novel formulation that allows to jointly recover the geometry of a 3D object as a set of primitives as well as their latent hierarchical structure without part-level supervision. Our model recovers the higher level structural decomposition of various objects in the form of a binary tree of primitives, where simple parts are represented with fewer primitives and more complex parts are modeled with more components. Our experiments on the ShapeNet and D-FAUST datasets demonstrate that considering the organization of parts indeed facilitates reasoning about 3D geometry.



### Tracking Objects as Points
- **Arxiv ID**: http://arxiv.org/abs/2004.01177v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.01177v2)
- **Published**: 2020-04-02 17:58:40+00:00
- **Updated**: 2020-08-21 16:28:05+00:00
- **Authors**: Xingyi Zhou, Vladlen Koltun, Philipp Krähenbühl
- **Comment**: ECCV 2020 Camera-ready version. Updated track rebirth results. Code
  available at https://github.com/xingyizhou/CenterTrack
- **Journal**: None
- **Summary**: Tracking has traditionally been the art of following interest points through space and time. This changed with the rise of powerful deep networks. Nowadays, tracking is dominated by pipelines that perform object detection followed by temporal association, also known as tracking-by-detection. In this paper, we present a simultaneous detection and tracking algorithm that is simpler, faster, and more accurate than the state of the art. Our tracker, CenterTrack, applies a detection model to a pair of images and detections from the prior frame. Given this minimal input, CenterTrack localizes objects and predicts their associations with the previous frame. That's it. CenterTrack is simple, online (no peeking into the future), and real-time. It achieves 67.3% MOTA on the MOT17 challenge at 22 FPS and 89.4% MOTA on the KITTI tracking benchmark at 15 FPS, setting a new state of the art on both datasets. CenterTrack is easily extended to monocular 3D tracking by regressing additional 3D attributes. Using monocular video input, it achieves 28.3% AMOTA@0.2 on the newly released nuScenes 3D tracking benchmark, substantially outperforming the monocular baseline on this benchmark while running at 28 FPS.



### Unsupervised Real-world Image Super Resolution via Domain-distance Aware Training
- **Arxiv ID**: http://arxiv.org/abs/2004.01178v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.01178v1)
- **Published**: 2020-04-02 17:59:03+00:00
- **Updated**: 2020-04-02 17:59:03+00:00
- **Authors**: Yunxuan Wei, Shuhang Gu, Yawei Li, Longcun Jin
- **Comment**: Code will be available at https://github.com/ShuhangGu/DASR
- **Journal**: None
- **Summary**: These days, unsupervised super-resolution (SR) has been soaring due to its practical and promising potential in real scenarios. The philosophy of off-the-shelf approaches lies in the augmentation of unpaired data, i.e. first generating synthetic low-resolution (LR) images $\mathcal{Y}^g$ corresponding to real-world high-resolution (HR) images $\mathcal{X}^r$ in the real-world LR domain $\mathcal{Y}^r$, and then utilizing the pseudo pairs $\{\mathcal{Y}^g, \mathcal{X}^r\}$ for training in a supervised manner. Unfortunately, since image translation itself is an extremely challenging task, the SR performance of these approaches are severely limited by the domain gap between generated synthetic LR images and real LR images. In this paper, we propose a novel domain-distance aware super-resolution (DASR) approach for unsupervised real-world image SR. The domain gap between training data (e.g. $\mathcal{Y}^g$) and testing data (e.g. $\mathcal{Y}^r$) is addressed with our \textbf{domain-gap aware training} and \textbf{domain-distance weighted supervision} strategies. Domain-gap aware training takes additional benefit from real data in the target domain while domain-distance weighted supervision brings forward the more rational use of labeled source domain data. The proposed method is validated on synthetic and real datasets and the experimental results show that DASR consistently outperforms state-of-the-art unsupervised SR approaches in generating SR outputs with more realistic and natural textures.



### Single-Image HDR Reconstruction by Learning to Reverse the Camera Pipeline
- **Arxiv ID**: http://arxiv.org/abs/2004.01179v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.01179v1)
- **Published**: 2020-04-02 17:59:04+00:00
- **Updated**: 2020-04-02 17:59:04+00:00
- **Authors**: Yu-Lun Liu, Wei-Sheng Lai, Yu-Sheng Chen, Yi-Lung Kao, Ming-Hsuan Yang, Yung-Yu Chuang, Jia-Bin Huang
- **Comment**: CVPR 2020. Project page:
  https://www.cmlab.csie.ntu.edu.tw/~yulunliu/SingleHDR Code:
  https://github.com/alex04072000/SingleHDR
- **Journal**: None
- **Summary**: Recovering a high dynamic range (HDR) image from a single low dynamic range (LDR) input image is challenging due to missing details in under-/over-exposed regions caused by quantization and saturation of camera sensors. In contrast to existing learning-based methods, our core idea is to incorporate the domain knowledge of the LDR image formation pipeline into our model. We model the HDRto-LDR image formation pipeline as the (1) dynamic range clipping, (2) non-linear mapping from a camera response function, and (3) quantization. We then propose to learn three specialized CNNs to reverse these steps. By decomposing the problem into specific sub-tasks, we impose effective physical constraints to facilitate the training of individual sub-networks. Finally, we jointly fine-tune the entire model end-to-end to reduce error accumulation. With extensive quantitative and qualitative experiments on diverse image datasets, we demonstrate that the proposed method performs favorably against state-of-the-art single-image HDR reconstruction algorithms.



### Learning to See Through Obstructions
- **Arxiv ID**: http://arxiv.org/abs/2004.01180v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.01180v1)
- **Published**: 2020-04-02 17:59:12+00:00
- **Updated**: 2020-04-02 17:59:12+00:00
- **Authors**: Yu-Lun Liu, Wei-Sheng Lai, Ming-Hsuan Yang, Yung-Yu Chuang, Jia-Bin Huang
- **Comment**: CVPR 2020. Project page:
  https://www.cmlab.csie.ntu.edu.tw/~yulunliu/ObstructionRemoval Code:
  https://github.com/alex04072000/ObstructionRemoval
- **Journal**: None
- **Summary**: We present a learning-based approach for removing unwanted obstructions, such as window reflections, fence occlusions or raindrops, from a short sequence of images captured by a moving camera. Our method leverages the motion differences between the background and the obstructing elements to recover both layers. Specifically, we alternate between estimating dense optical flow fields of the two layers and reconstructing each layer from the flow-warped images via a deep convolutional neural network. The learning-based layer reconstruction allows us to accommodate potential errors in the flow estimation and brittle assumptions such as brightness consistency. We show that training on synthetically generated data transfers well to real images. Our results on numerous challenging scenarios of reflection and fence removal demonstrate the effectiveness of the proposed method.



### Temporal Accumulative Features for Sign Language Recognition
- **Arxiv ID**: http://arxiv.org/abs/2004.01225v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.01225v1)
- **Published**: 2020-04-02 19:03:40+00:00
- **Updated**: 2020-04-02 19:03:40+00:00
- **Authors**: Ahmet Alp Kındıroğlu, Oğulcan Özdemir, Lale Akarun
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: In this paper, we propose a set of features called temporal accumulative features (TAF) for representing and recognizing isolated sign language gestures. By incorporating sign language specific constructs to better represent the unique linguistic characteristic of sign language videos, we have devised an efficient and fast SLR method for recognizing isolated sign language gestures. The proposed method is an HSV based accumulative video representation where keyframes based on the linguistic movement-hold model are represented by different colors. We also incorporate hand shape information and using a small scale convolutional neural network, demonstrate that sequential modeling of accumulative features for linguistic subunits improves upon baseline classification results.



### Deformation-Aware 3D Model Embedding and Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2004.01228v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.01228v3)
- **Published**: 2020-04-02 19:10:57+00:00
- **Updated**: 2020-07-31 05:10:25+00:00
- **Authors**: Mikaela Angelina Uy, Jingwei Huang, Minhyuk Sung, Tolga Birdal, Leonidas Guibas
- **Comment**: Accepted for publication at ECCV 2020. Project page under
  https://deformscan2cad.github.io
- **Journal**: None
- **Summary**: We introduce a new problem of retrieving 3D models that are deformable to a given query shape and present a novel deep deformation-aware embedding to solve this retrieval task. 3D model retrieval is a fundamental operation for recovering a clean and complete 3D model from a noisy and partial 3D scan. However, given a finite collection of 3D shapes, even the closest model to a query may not be satisfactory. This motivates us to apply 3D model deformation techniques to adapt the retrieved model so as to better fit the query. Yet, certain restrictions are enforced in most 3D deformation techniques to preserve important features of the original model that prevent a perfect fitting of the deformed model to the query. This gap between the deformed model and the query induces asymmetric relationships among the models, which cannot be handled by typical metric learning techniques. Thus, to retrieve the best models for fitting, we propose a novel deep embedding approach that learns the asymmetric relationships by leveraging location-dependent egocentric distance fields. We also propose two strategies for training the embedding network. We demonstrate that both of these approaches outperform other baselines in our experiments with both synthetic and real data. Our project page can be found at https://deformscan2cad.github.io/.



### Semantic Segmentation of Underwater Imagery: Dataset and Benchmark
- **Arxiv ID**: http://arxiv.org/abs/2004.01241v3
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.01241v3)
- **Published**: 2020-04-02 19:53:14+00:00
- **Updated**: 2020-09-13 23:47:39+00:00
- **Authors**: Md Jahidul Islam, Chelsey Edge, Yuyang Xiao, Peigen Luo, Muntaqim Mehtaz, Christopher Morse, Sadman Sakib Enan, Junaed Sattar
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present the first large-scale dataset for semantic Segmentation of Underwater IMagery (SUIM). It contains over 1500 images with pixel annotations for eight object categories: fish (vertebrates), reefs (invertebrates), aquatic plants, wrecks/ruins, human divers, robots, and sea-floor. The images have been rigorously collected during oceanic explorations and human-robot collaborative experiments, and annotated by human participants. We also present a benchmark evaluation of state-of-the-art semantic segmentation approaches based on standard performance metrics. In addition, we present SUIM-Net, a fully-convolutional encoder-decoder model that balances the trade-off between performance and computational efficiency. It offers competitive performance while ensuring fast end-to-end inference, which is essential for its use in the autonomy pipeline of visually-guided underwater robots. In particular, we demonstrate its usability benefits for visual servoing, saliency prediction, and detailed scene understanding. With a variety of use cases, the proposed model and benchmark dataset open up promising opportunities for future research in underwater robot vision.



### Guided Variational Autoencoder for Disentanglement Learning
- **Arxiv ID**: http://arxiv.org/abs/2004.01255v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.01255v1)
- **Published**: 2020-04-02 20:49:15+00:00
- **Updated**: 2020-04-02 20:49:15+00:00
- **Authors**: Zheng Ding, Yifan Xu, Weijian Xu, Gaurav Parmar, Yang Yang, Max Welling, Zhuowen Tu
- **Comment**: Accepted to CVPR 2020
- **Journal**: None
- **Summary**: We propose an algorithm, guided variational autoencoder (Guided-VAE), that is able to learn a controllable generative model by performing latent representation disentanglement learning. The learning objective is achieved by providing signals to the latent encoding/embedding in VAE without changing its main backbone architecture, hence retaining the desirable properties of the VAE. We design an unsupervised strategy and a supervised strategy in Guided-VAE and observe enhanced modeling and controlling capability over the vanilla VAE. In the unsupervised strategy, we guide the VAE learning by introducing a lightweight decoder that learns latent geometric transformation and principal components; in the supervised strategy, we use an adversarial excitation and inhibition mechanism to encourage the disentanglement of the latent variables. Guided-VAE enjoys its transparency and simplicity for the general representation learning task, as well as disentanglement learning. On a number of experiments for representation learning, improved synthesis/sampling, better disentanglement for classification, and reduced classification errors in meta-learning have been observed.



### Knowing What, Where and When to Look: Efficient Video Action Modeling with Attention
- **Arxiv ID**: http://arxiv.org/abs/2004.01278v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.01278v1)
- **Published**: 2020-04-02 21:48:11+00:00
- **Updated**: 2020-04-02 21:48:11+00:00
- **Authors**: Juan-Manuel Perez-Rua, Brais Martinez, Xiatian Zhu, Antoine Toisoul, Victor Escorcia, Tao Xiang
- **Comment**: None
- **Journal**: None
- **Summary**: Attentive video modeling is essential for action recognition in unconstrained videos due to their rich yet redundant information over space and time. However, introducing attention in a deep neural network for action recognition is challenging for two reasons. First, an effective attention module needs to learn what (objects and their local motion patterns), where (spatially), and when (temporally) to focus on. Second, a video attention module must be efficient because existing action recognition models already suffer from high computational cost. To address both challenges, a novel What-Where-When (W3) video attention module is proposed. Departing from existing alternatives, our W3 module models all three facets of video attention jointly. Crucially, it is extremely efficient by factorizing the high-dimensional video feature data into low-dimensional meaningful spaces (1D channel vector for `what' and 2D spatial tensors for `where'), followed by lightweight temporal attention reasoning. Extensive experiments show that our attention model brings significant improvements to existing action recognition models, achieving new state-of-the-art performance on a number of benchmarks.



### Automated Quantification of CT Patterns Associated with COVID-19 from Chest CT
- **Arxiv ID**: http://arxiv.org/abs/2004.01279v7
- **DOI**: 10.1148/ryai.2020200048#
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.01279v7)
- **Published**: 2020-04-02 21:49:14+00:00
- **Updated**: 2020-11-18 21:17:32+00:00
- **Authors**: Shikha Chaganti, Abishek Balachandran, Guillaume Chabin, Stuart Cohen, Thomas Flohr, Bogdan Georgescu, Philippe Grenier, Sasa Grbic, Siqi Liu, François Mellot, Nicolas Murray, Savvas Nicolaou, William Parker, Thomas Re, Pina Sanelli, Alexander W. Sauter, Zhoubing Xu, Youngjin Yoo, Valentin Ziebandt, Dorin Comaniciu
- **Comment**: None
- **Journal**: Radiology: Artificial Intelligence, Vol. 2, No. 4, 2020
- **Summary**: Purpose: To present a method that automatically segments and quantifies abnormal CT patterns commonly present in coronavirus disease 2019 (COVID-19), namely ground glass opacities and consolidations. Materials and Methods: In this retrospective study, the proposed method takes as input a non-contrasted chest CT and segments the lesions, lungs, and lobes in three dimensions, based on a dataset of 9749 chest CT volumes. The method outputs two combined measures of the severity of lung and lobe involvement, quantifying both the extent of COVID-19 abnormalities and presence of high opacities, based on deep learning and deep reinforcement learning. The first measure of (PO, PHO) is global, while the second of (LSS, LHOS) is lobewise. Evaluation of the algorithm is reported on CTs of 200 participants (100 COVID-19 confirmed patients and 100 healthy controls) from institutions from Canada, Europe and the United States collected between 2002-Present (April, 2020). Ground truth is established by manual annotations of lesions, lungs, and lobes. Correlation and regression analyses were performed to compare the prediction to the ground truth. Results: Pearson correlation coefficient between method prediction and ground truth for COVID-19 cases was calculated as 0.92 for PO (P < .001), 0.97 for PHO(P < .001), 0.91 for LSS (P < .001), 0.90 for LHOS (P < .001). 98 of 100 healthy controls had a predicted PO of less than 1%, 2 had between 1-2%. Automated processing time to compute the severity scores was 10 seconds per case compared to 30 minutes required for manual annotations. Conclusion: A new method segments regions of CT abnormalities associated with COVID-19 and computes (PO, PHO), as well as (LSS, LHOS) severity scores.



### BosphorusSign22k Sign Language Recognition Dataset
- **Arxiv ID**: http://arxiv.org/abs/2004.01283v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.01283v2)
- **Published**: 2020-04-02 22:15:38+00:00
- **Updated**: 2020-04-09 14:07:10+00:00
- **Authors**: Oğulcan Özdemir, Ahmet Alp Kındıroğlu, Necati Cihan Camgöz, Lale Akarun
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: Sign Language Recognition is a challenging research domain. It has recently seen several advancements with the increased availability of data. In this paper, we introduce the BosphorusSign22k, a publicly available large scale sign language dataset aimed at computer vision, video recognition and deep learning research communities. The primary objective of this dataset is to serve as a new benchmark in Turkish Sign Language Recognition for its vast lexicon, the high number of repetitions by native signers, high recording quality, and the unique syntactic properties of the signs it encompasses. We also provide state-of-the-art human pose estimates to encourage other tasks such as Sign Language Production. We survey other publicly available datasets and expand on how BosphorusSign22k can contribute to future research that is being made possible through the widespread availability of similar Sign Language resources. We have conducted extensive experiments and present baseline results to underpin future research on our dataset.



### Extraction and Assessment of Naturalistic Human Driving Trajectories from Infrastructure Camera and Radar Sensors
- **Arxiv ID**: http://arxiv.org/abs/2004.01288v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.01288v1)
- **Published**: 2020-04-02 22:28:29+00:00
- **Updated**: 2020-04-02 22:28:29+00:00
- **Authors**: Dominik Notz, Felix Becker, Thomas Kühbeck, Daniel Watzenig
- **Comment**: 8 pages, 9 figures
- **Journal**: None
- **Summary**: Collecting realistic driving trajectories is crucial for training machine learning models that imitate human driving behavior. Most of today's autonomous driving datasets contain only a few trajectories per location and are recorded with test vehicles that are cautiously driven by trained drivers. In particular in interactive scenarios such as highway merges, the test driver's behavior significantly influences other vehicles. This influence prevents recording the whole traffic space of human driving behavior. In this work, we present a novel methodology to extract trajectories of traffic objects using infrastructure sensors. Infrastructure sensors allow us to record a lot of data for one location and take the test drivers out of the loop. We develop both a hardware setup consisting of a camera and a traffic surveillance radar and a trajectory extraction algorithm. Our vision pipeline accurately detects objects, fuses camera and radar detections and tracks them over time. We improve a state-of-the-art object tracker by combining the tracking in image coordinates with a Kalman filter in road coordinates. We show that our sensor fusion approach successfully combines the advantages of camera and radar detections and outperforms either single sensor. Finally, we also evaluate the accuracy of our trajectory extraction pipeline. For that, we equip our test vehicle with a differential GPS sensor and use it to collect ground truth trajectories. With this data we compute the measurement errors. While we use the mean error to de-bias the trajectories, the error standard deviation is in the magnitude of the ground truth data inaccuracy. Hence, the extracted trajectories are not only naturalistic but also highly accurate and prove the potential of using infrastructure sensors to extract real-world trajectories.



### Novel View Synthesis of Dynamic Scenes with Globally Coherent Depths from a Monocular Camera
- **Arxiv ID**: http://arxiv.org/abs/2004.01294v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.01294v1)
- **Published**: 2020-04-02 22:45:53+00:00
- **Updated**: 2020-04-02 22:45:53+00:00
- **Authors**: Jae Shin Yoon, Kihwan Kim, Orazio Gallo, Hyun Soo Park, Jan Kautz
- **Comment**: This paper is accepted to CVPR 2020
- **Journal**: None
- **Summary**: This paper presents a new method to synthesize an image from arbitrary views and times given a collection of images of a dynamic scene. A key challenge for the novel view synthesis arises from dynamic scene reconstruction where epipolar geometry does not apply to the local motion of dynamic contents. To address this challenge, we propose to combine the depth from single view (DSV) and the depth from multi-view stereo (DMV), where DSV is complete, i.e., a depth is assigned to every pixel, yet view-variant in its scale, while DMV is view-invariant yet incomplete. Our insight is that although its scale and quality are inconsistent with other views, the depth estimation from a single view can be used to reason about the globally coherent geometry of dynamic contents. We cast this problem as learning to correct the scale of DSV, and to refine each depth with locally consistent motions between views to form a coherent depth estimation. We integrate these tasks into a depth fusion network in a self-supervised fashion. Given the fused depth maps, we synthesize a photorealistic virtual view in a specific location and time with our deep blending network that completes the scene and renders the virtual view. We evaluate our method of depth estimation and view synthesis on diverse real-world dynamic scenes and show the outstanding performance over existing methods.



### Generative PointNet: Deep Energy-Based Learning on Unordered Point Sets for 3D Generation, Reconstruction and Classification
- **Arxiv ID**: http://arxiv.org/abs/2004.01301v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.01301v2)
- **Published**: 2020-04-02 23:08:10+00:00
- **Updated**: 2021-04-07 14:37:08+00:00
- **Authors**: Jianwen Xie, Yifei Xu, Zilong Zheng, Song-Chun Zhu, Ying Nian Wu
- **Comment**: None
- **Journal**: IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
  2021
- **Summary**: We propose a generative model of unordered point sets, such as point clouds, in the form of an energy-based model, where the energy function is parameterized by an input-permutation-invariant bottom-up neural network. The energy function learns a coordinate encoding of each point and then aggregates all individual point features into an energy for the whole point cloud. We call our model the Generative PointNet because it can be derived from the discriminative PointNet. Our model can be trained by MCMC-based maximum likelihood learning (as well as its variants), without the help of any assisting networks like those in GANs and VAEs. Unlike most point cloud generators that rely on hand-crafted distance metrics, our model does not require any hand-crafted distance metric for the point cloud generation, because it synthesizes point clouds by matching observed examples in terms of statistical properties defined by the energy function. Furthermore, we can learn a short-run MCMC toward the energy-based model as a flow-like generator for point cloud reconstruction and interpolation. The learned point cloud representation can be useful for point cloud classification. Experiments demonstrate the advantages of the proposed generative model of point clouds.



### STAN-CT: Standardizing CT Image using Generative Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/2004.01307v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.01307v1)
- **Published**: 2020-04-02 23:43:06+00:00
- **Updated**: 2020-04-02 23:43:06+00:00
- **Authors**: Md Selim, Jie Zhang, Baowei Fei, Guo-Qiang Zhang, Jin Chen
- **Comment**: 10 pages, 5 figures
- **Journal**: None
- **Summary**: Computed tomography (CT) plays an important role in lung malignancy diagnostics and therapy assessment and facilitating precision medicine delivery. However, the use of personalized imaging protocols poses a challenge in large-scale cross-center CT image radiomic studies. We present an end-to-end solution called STAN-CT for CT image standardization and normalization, which effectively reduces discrepancies in image features caused by using different imaging protocols or using different CT scanners with the same imaging protocol. STAN-CT consists of two components: 1) a novel Generative Adversarial Networks (GAN) model that is capable of effectively learning the data distribution of a standard imaging protocol with only a few rounds of generator training, and 2) an automatic DICOM reconstruction pipeline with systematic image quality control that ensure the generation of high-quality standard DICOM images. Experimental results indicate that the training efficiency and model performance of STAN-CT have been significantly improved compared to the state-of-the-art CT image standardization and normalization algorithms.



