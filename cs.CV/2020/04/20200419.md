# Arxiv Papers in cs.CV on 2020-04-19
### A Committee of Convolutional Neural Networks for Image Classication in the Concurrent Presence of Feature and Label Noise
- **Arxiv ID**: http://arxiv.org/abs/2004.10705v2
- **DOI**: 10.1007/978-3-030-58112-1_34
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.10705v2)
- **Published**: 2020-04-19 00:22:11+00:00
- **Updated**: 2020-06-22 20:54:13+00:00
- **Authors**: Stanisław Kaźmierczak, Jacek Mańdziuk
- **Comment**: None
- **Journal**: 16th International Conference on Parallel Problem Solving from
  Nature, PPSN 2020, 498-511, LNCS vol. 12269
- **Summary**: Image classification has become a ubiquitous task. Models trained on good quality data achieve accuracy which in some application domains is already above human-level performance. Unfortunately, real-world data are quite often degenerated by the noise existing in features and/or labels. There are quite many papers that handle the problem of either feature or label noise, separately. However, to the best of our knowledge, this piece of research is the first attempt to address the problem of concurrent occurrence of both types of noise. Basing on the MNIST, CIFAR-10 and CIFAR-100 datasets, we experimentally proved that the difference by which committees beat single models increases along with noise level, no matter it is an attribute or label disruption. Thus, it makes ensembles legitimate to be applied to noisy images with noisy labels. The aforementioned committees' advantage over single models is positively correlated with dataset difficulty level as well. We propose three committee selection algorithms that outperform a strong baseline algorithm which relies on an ensemble of individual (nonassociated) best models.



### An end-to-end CNN framework for polarimetric vision tasks based on polarization-parameter-constructing network
- **Arxiv ID**: http://arxiv.org/abs/2004.08740v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.08740v1)
- **Published**: 2020-04-19 01:33:10+00:00
- **Updated**: 2020-04-19 01:33:10+00:00
- **Authors**: Yong Wang, Qi Liu, Hongyu Zu, Xiao Liu, Ruichao Xie, Feng Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Pixel-wise operations between polarimetric images are important for processing polarization information. For the lack of such operations, the polarization information cannot be fully utilized in convolutional neural network(CNN). In this paper, a novel end-to-end CNN framework for polarization vision tasks is proposed, which enables the networks to take full advantage of polarimetric images. The framework consists of two sub-networks: a polarization-parameter-constructing network (PPCN) and a task network. PPCN implements pixel-wise operations between images in the CNN form with 1x1 convolution kernels. It takes raw polarimetric images as input, and outputs polarization-parametric images to task network so as to complete a vison task. By training together, the PPCN can learn to provide the most suitable polarization-parametric images for the task network and the dataset. Taking faster R-CNN as task network, the experimental results show that compared with existing methods, the proposed framework achieves much higher mean-average-precision (mAP) in object detection task



### Are we pretraining it right? Digging deeper into visio-linguistic pretraining
- **Arxiv ID**: http://arxiv.org/abs/2004.08744v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2004.08744v1)
- **Published**: 2020-04-19 01:55:19+00:00
- **Updated**: 2020-04-19 01:55:19+00:00
- **Authors**: Amanpreet Singh, Vedanuj Goswami, Devi Parikh
- **Comment**: 23 pages, 6 figures. First two authors contributed equally. More info
  at https://github.com/facebookresearch/pythia
- **Journal**: None
- **Summary**: Numerous recent works have proposed pretraining generic visio-linguistic representations and then finetuning them for downstream vision and language tasks. While architecture and objective function design choices have received attention, the choice of pretraining datasets has received little attention. In this work, we question some of the default choices made in literature. For instance, we systematically study how varying similarity between the pretraining dataset domain (textual and visual) and the downstream domain affects performance. Surprisingly, we show that automatically generated data in a domain closer to the downstream task (e.g., VQA v2) is a better choice for pretraining than "natural" data but of a slightly different domain (e.g., Conceptual Captions). On the other hand, some seemingly reasonable choices of pretraining datasets were found to be entirely ineffective for some downstream tasks. This suggests that despite the numerous recent efforts, vision & language pretraining does not quite work "out of the box" yet. Overall, as a by-product of our study, we find that simple design choices in pretraining can help us achieve close to state-of-art results on downstream tasks without any architectural changes.



### PointTriNet: Learned Triangulation of 3D Point Sets
- **Arxiv ID**: http://arxiv.org/abs/2005.02138v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2005.02138v2)
- **Published**: 2020-04-19 01:58:35+00:00
- **Updated**: 2020-07-23 12:37:01+00:00
- **Authors**: Nicholas Sharp, Maks Ovsjanikov
- **Comment**: 21 pages, 9 figures
- **Journal**: None
- **Summary**: This work considers a new task in geometric deep learning: generating a triangulation among a set of points in 3D space. We present PointTriNet, a differentiable and scalable approach enabling point set triangulation as a layer in 3D learning pipelines. The method iteratively applies two neural networks: a classification network predicts whether a candidate triangle should appear in the triangulation, while a proposal network suggests additional candidates. Both networks are structured as PointNets over nearby points and triangles, using a novel triangle-relative input encoding. Since these learning problems operate on local geometric data, our method is efficient and scalable, and generalizes to unseen shape categories. Our networks are trained in an unsupervised manner from a collection of shapes represented as point clouds. We demonstrate the effectiveness of this approach for classical meshing tasks, robustness to outliers, and as a component in end-to-end learning systems.



### Learning to Evaluate Perception Models Using Planner-Centric Metrics
- **Arxiv ID**: http://arxiv.org/abs/2004.08745v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.08745v1)
- **Published**: 2020-04-19 02:14:00+00:00
- **Updated**: 2020-04-19 02:14:00+00:00
- **Authors**: Jonah Philion, Amlan Kar, Sanja Fidler
- **Comment**: CVPR 2020 poster
- **Journal**: None
- **Summary**: Variants of accuracy and precision are the gold-standard by which the computer vision community measures progress of perception algorithms. One reason for the ubiquity of these metrics is that they are largely task-agnostic; we in general seek to detect zero false negatives or positives. The downside of these metrics is that, at worst, they penalize all incorrect detections equally without conditioning on the task or scene, and at best, heuristics need to be chosen to ensure that different mistakes count differently. In this paper, we propose a principled metric for 3D object detection specifically for the task of self-driving. The core idea behind our metric is to isolate the task of object detection and measure the impact the produced detections would induce on the downstream task of driving. Without hand-designing it to, we find that our metric penalizes many of the mistakes that other metrics penalize by design. In addition, our metric downweighs detections based on additional factors such as distance from a detection to the ego car and the speed of the detection in intuitive ways that other detection metrics do not. For human evaluation, we generate scenes in which standard metrics and our metric disagree and find that humans side with our metric 79% of the time. Our project page including an evaluation server can be found at https://nv-tlabs.github.io/detection-relevance.



### Tensor completion using enhanced multiple modes low-rank prior and total variation
- **Arxiv ID**: http://arxiv.org/abs/2004.08747v3
- **DOI**: None
- **Categories**: **cs.CV**, 94A12
- **Links**: [PDF](http://arxiv.org/pdf/2004.08747v3)
- **Published**: 2020-04-19 02:23:06+00:00
- **Updated**: 2020-05-06 00:57:57+00:00
- **Authors**: Haijin Zeng, Xiaozhen Xie, Jifeng Ning
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a novel model to recover a low-rank tensor by simultaneously performing double nuclear norm regularized low-rank matrix factorizations to the all-mode matricizations of the underlying tensor. An block successive upper-bound minimization algorithm is applied to solve the model. Subsequence convergence of our algorithm can be established, and our algorithm converges to the coordinate-wise minimizers in some mild conditions. Several experiments on three types of public data sets show that our algorithm can recover a variety of low-rank tensors from significantly fewer samples than the other testing tensor completion methods.



### Lightweight Mask R-CNN for Long-Range Wireless Power Transfer Systems
- **Arxiv ID**: http://arxiv.org/abs/2004.08761v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.08761v1)
- **Published**: 2020-04-19 03:50:15+00:00
- **Updated**: 2020-04-19 03:50:15+00:00
- **Authors**: Hao Li, Aozhou Wu, Wen Fang, Qingqing Zhang, Mingqing Liu, Qingwen Liu, Wei Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Resonant Beam Charging (RBC) is a wireless charging technology which supports multi-watt power transfer over meter-level distance. The features of safety, mobility and simultaneous charging capability enable RBC to charge multiple mobile devices safely at the same time. To detect the devices that need to be charged, a Mask R-CNN based dection model is proposed in previous work. However, considering the constraints of the RBC system, it's not easy to apply Mask R-CNN in lightweight hardware-embedded devices because of its heavy model and huge computation. Thus, we propose a machine learning detection approach which provides a lighter and faster model based on traditional Mask R-CNN. The proposed approach makes the object detection much easier to be transplanted on mobile devices and reduce the burden of hardware computation. By adjusting the structure of the backbone and the head part of Mask R-CNN, we reduce the average detection time from $1.02\mbox{s}$ per image to $0.6132\mbox{s}$, and reduce the model size from $245\mbox{MB}$ to $47.1\mbox{MB}$. The improved model is much more suitable for the application in the RBC system.



### TriGAN: Image-to-Image Translation for Multi-Source Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2004.08769v1
- **DOI**: 10.1007/s00138-020-01164-4
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.08769v1)
- **Published**: 2020-04-19 05:07:22+00:00
- **Updated**: 2020-04-19 05:07:22+00:00
- **Authors**: Subhankar Roy, Aliaksandr Siarohin, Enver Sangineto, Nicu Sebe, Elisa Ricci
- **Comment**: None
- **Journal**: Machine Vision and Applications 2021
- **Summary**: Most domain adaptation methods consider the problem of transferring knowledge to the target domain from a single source dataset. However, in practical applications, we typically have access to multiple sources. In this paper we propose the first approach for Multi-Source Domain Adaptation (MSDA) based on Generative Adversarial Networks. Our method is inspired by the observation that the appearance of a given image depends on three factors: the domain, the style (characterized in terms of low-level features variations) and the content. For this reason we propose to project the image features onto a space where only the dependence from the content is kept, and then re-project this invariant representation onto the pixel space using the target domain and style. In this way, new labeled images can be generated which are used to train a final target classifier. We test our approach using common MSDA benchmarks, showing that it outperforms state-of-the-art methods.



### AD-Cluster: Augmented Discriminative Clustering for Domain Adaptive Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/2004.08787v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.08787v2)
- **Published**: 2020-04-19 07:34:37+00:00
- **Updated**: 2020-04-23 09:50:43+00:00
- **Authors**: Yunpeng Zhai, Shijian Lu, Qixiang Ye, Xuebo Shan, Jie Chen, Rongrong Ji, Yonghong Tian
- **Comment**: Accepted by CVPR'20
- **Journal**: None
- **Summary**: Domain adaptive person re-identification (re-ID) is a challenging task, especially when person identities in target domains are unknown. Existing methods attempt to address this challenge by transferring image styles or aligning feature distributions across domains, whereas the rich unlabeled samples in target domains are not sufficiently exploited. This paper presents a novel augmented discriminative clustering (AD-Cluster) technique that estimates and augments person clusters in target domains and enforces the discrimination ability of re-ID models with the augmented clusters. AD-Cluster is trained by iterative density-based clustering, adaptive sample augmentation, and discriminative feature learning. It learns an image generator and a feature encoder which aim to maximize the intra-cluster diversity in the sample space and minimize the intra-cluster distance in the feature space in an adversarial min-max manner. Finally, AD-Cluster increases the diversity of sample clusters and improves the discrimination capability of re-ID models greatly. Extensive experiments over Market-1501 and DukeMTMC-reID show that AD-Cluster outperforms the state-of-the-art with large margins.



### UNet 3+: A Full-Scale Connected UNet for Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2004.08790v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.08790v1)
- **Published**: 2020-04-19 08:05:59+00:00
- **Updated**: 2020-04-19 08:05:59+00:00
- **Authors**: Huimin Huang, Lanfen Lin, Ruofeng Tong, Hongjie Hu, Qiaowei Zhang, Yutaro Iwamoto, Xianhua Han, Yen-Wei Chen, Jian Wu
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, a growing interest has been seen in deep learning-based semantic segmentation. UNet, which is one of deep learning networks with an encoder-decoder architecture, is widely used in medical image segmentation. Combining multi-scale features is one of important factors for accurate segmentation. UNet++ was developed as a modified Unet by designing an architecture with nested and dense skip connections. However, it does not explore sufficient information from full scales and there is still a large room for improvement. In this paper, we propose a novel UNet 3+, which takes advantage of full-scale skip connections and deep supervisions. The full-scale skip connections incorporate low-level details with high-level semantics from feature maps in different scales; while the deep supervision learns hierarchical representations from the full-scale aggregated feature maps. The proposed method is especially benefiting for organs that appear at varying scales. In addition to accuracy improvements, the proposed UNet 3+ can reduce the network parameters to improve the computation efficiency. We further propose a hybrid loss function and devise a classification-guided module to enhance the organ boundary and reduce the over-segmentation in a non-organ image, yielding more accurate segmentation results. The effectiveness of the proposed method is demonstrated on two datasets. The code is available at: github.com/ZJUGiveLab/UNet-Version



### When Residual Learning Meets Dense Aggregation: Rethinking the Aggregation of Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2004.08796v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.08796v2)
- **Published**: 2020-04-19 08:34:52+00:00
- **Updated**: 2020-04-24 05:21:07+00:00
- **Authors**: Zhiyu Zhu, Zhen-Peng Bian, Junhui Hou, Yi Wang, Lap-Pui Chau
- **Comment**: None
- **Journal**: None
- **Summary**: Various architectures (such as GoogLeNets, ResNets, and DenseNets) have been proposed. However, the existing networks usually suffer from either redundancy of convolutional layers or insufficient utilization of parameters. To handle these challenging issues, we propose Micro-Dense Nets, a novel architecture with global residual learning and local micro-dense aggregations. Specifically, residual learning aims to efficiently retrieve features from different convolutional blocks, while the micro-dense aggregation is able to enhance each block and avoid redundancy of convolutional layers by lessening residual aggregations. Moreover, the proposed micro-dense architecture has two characteristics: pyramidal multi-level feature learning which can widen the deeper layer in a block progressively, and dimension cardinality adaptive convolution which can balance each layer using linearly increasing dimension cardinality. The experimental results over three datasets (i.e., CIFAR-10, CIFAR-100, and ImageNet-1K) demonstrate that the proposed Micro-Dense Net with only 4M parameters can achieve higher classification accuracy than state-of-the-art networks, while being 12.1$\times$ smaller depends on the number of parameters. In addition, our micro-dense block can be integrated with neural architecture search based models to boost their performance, validating the advantage of our architecture. We believe our design and findings will be beneficial to the DNN community.



### Graph-Structured Referring Expression Reasoning in The Wild
- **Arxiv ID**: http://arxiv.org/abs/2004.08814v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2004.08814v1)
- **Published**: 2020-04-19 11:00:30+00:00
- **Updated**: 2020-04-19 11:00:30+00:00
- **Authors**: Sibei Yang, Guanbin Li, Yizhou Yu
- **Comment**: CVPR 2020 Accepted Oral Paper. Data and code are available at
  https://github.com/sibeiyang/sgmn
- **Journal**: None
- **Summary**: Grounding referring expressions aims to locate in an image an object referred to by a natural language expression. The linguistic structure of a referring expression provides a layout of reasoning over the visual contents, and it is often crucial to align and jointly understand the image and the referring expression. In this paper, we propose a scene graph guided modular network (SGMN), which performs reasoning over a semantic graph and a scene graph with neural modules under the guidance of the linguistic structure of the expression. In particular, we model the image as a structured semantic graph, and parse the expression into a language scene graph. The language scene graph not only decodes the linguistic structure of the expression, but also has a consistent representation with the image semantic graph. In addition to exploring structured solutions to grounding referring expressions, we also propose Ref-Reasoning, a large-scale real-world dataset for structured referring expression reasoning. We automatically generate referring expressions over the scene graphs of images using diverse expression templates and functional programs. This dataset is equipped with real-world visual contents as well as semantically rich expressions with different reasoning layouts. Experimental results show that our SGMN not only significantly outperforms existing state-of-the-art algorithms on the new Ref-Reasoning dataset, but also surpasses state-of-the-art structured methods on commonly used benchmark datasets. It can also provide interpretable visual evidences of reasoning. Data and code are available at https://github.com/sibeiyang/sgmn



### A Wearable Social Interaction Aid for Children with Autism
- **Arxiv ID**: http://arxiv.org/abs/2004.14281v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.14281v1)
- **Published**: 2020-04-19 13:14:32+00:00
- **Updated**: 2020-04-19 13:14:32+00:00
- **Authors**: Nick Haber, Catalin Voss, Jena Daniels, Peter Washington, Azar Fazel, Aaron Kline, Titas De, Terry Winograd, Carl Feinstein, Dennis P. Wall
- **Comment**: None
- **Journal**: None
- **Summary**: With most recent estimates giving an incidence rate of 1 in 68 children in the United States, the autism spectrum disorder (ASD) is a growing public health crisis. Many of these children struggle to make eye contact, recognize facial expressions, and engage in social interactions. Today the standard for treatment of the core autism-related deficits focuses on a form of behavior training known as Applied Behavioral Analysis. To address perceived deficits in expression recognition, ABA approaches routinely involve the use of prompts such as flash cards for repetitive emotion recognition training via memorization. These techniques must be administered by trained practitioners and often at clinical centers that are far outnumbered by and out of reach from the many children and families in need of attention. Waitlists for access are up to 18 months long, and this wait may lead to children regressing down a path of isolation that worsens their long-term prognosis. There is an urgent need to innovate new methods of care delivery that can appropriately empower caregivers of children at risk or with a diagnosis of autism, and that capitalize on mobile tools and wearable devices for use outside of clinical settings.



### ktrain: A Low-Code Library for Augmented Machine Learning
- **Arxiv ID**: http://arxiv.org/abs/2004.10703v5
- **DOI**: None
- **Categories**: **cs.LG**, cs.CL, cs.CV, cs.SI
- **Links**: [PDF](http://arxiv.org/pdf/2004.10703v5)
- **Published**: 2020-04-19 14:18:20+00:00
- **Updated**: 2022-04-05 18:49:01+00:00
- **Authors**: Arun S. Maiya
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: We present ktrain, a low-code Python library that makes machine learning more accessible and easier to apply. As a wrapper to TensorFlow and many other libraries (e.g., transformers, scikit-learn, stellargraph), it is designed to make sophisticated, state-of-the-art machine learning models simple to build, train, inspect, and apply by both beginners and experienced practitioners. Featuring modules that support text data (e.g., text classification, sequence tagging, open-domain question-answering), vision data (e.g., image classification), graph data (e.g., node classification, link prediction), and tabular data, ktrain presents a simple unified interface enabling one to quickly solve a wide range of tasks in as little as three or four "commands" or lines of code.



### Uncertainty-Aware Consistency Regularization for Cross-Domain Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2004.08878v4
- **DOI**: 10.1016/j.cviu.2022.103448
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.08878v4)
- **Published**: 2020-04-19 15:30:26+00:00
- **Updated**: 2021-08-19 06:57:22+00:00
- **Authors**: Qianyu Zhou, Zhengyang Feng, Qiqi Gu, Guangliang Cheng, Xuequan Lu, Jianping Shi, Lizhuang Ma
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised domain adaptation (UDA) aims to adapt existing models of the source domain to a new target domain with only unlabeled data. Most existing methods suffer from noticeable negative transfer resulting from either the error-prone discriminator network or the unreasonable teacher model. Besides, the local regional consistency in UDA has been largely neglected, and only extracting the global-level pattern information is not powerful enough for feature alignment due to the abuse use of contexts. To this end, we propose an uncertainty-aware consistency regularization method for cross-domain semantic segmentation. Firstly, we introduce an uncertainty-guided consistency loss with a dynamic weighting scheme by exploiting the latent uncertainty information of the target samples. As such, more meaningful and reliable knowledge from the teacher model can be transferred to the student model. We further reveal the reason why the current consistency regularization is often unstable in minimizing the domain discrepancy. Besides, we design a ClassDrop mask generation algorithm to produce strong class-wise perturbations. Guided by this mask, we propose a ClassOut strategy to realize effective regional consistency in a fine-grained manner. Experiments demonstrate that our method outperforms the state-of-the-art methods on four domain adaptation benchmarks, i.e., GTAV $\rightarrow $ Cityscapes and SYNTHIA $\rightarrow $ Cityscapes, Virtual KITTI $\rightarrow$ KITTI and Cityscapes $\rightarrow$ KITTI.



### A Biologically Interpretable Two-stage Deep Neural Network (BIT-DNN) For Vegetation Recognition From Hyperspectral Imagery
- **Arxiv ID**: http://arxiv.org/abs/2004.08886v2
- **DOI**: 10.1109/TGRS.2021.3058782
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.08886v2)
- **Published**: 2020-04-19 15:58:19+00:00
- **Updated**: 2021-04-14 14:26:14+00:00
- **Authors**: Yue Shi, Liangxiu Han, Wenjiang Huang, Sheng Chang, Yingying Dong, Darren Dancey, Lianghao Han
- **Comment**: 13 pages, 11 figures
- **Journal**: None
- **Summary**: Spectral-spatial based deep learning models have recently proven to be effective in hyperspectral image (HSI) classification for various earth monitoring applications such as land cover classification and agricultural monitoring. However, due to the nature of "black-box" model representation, how to explain and interpret the learning process and the model decision, especially for vegetation classification, remains an open challenge. This study proposes a novel interpretable deep learning model -- a biologically interpretable two-stage deep neural network (BIT-DNN), by incorporating the prior-knowledge (i.e. biophysical and biochemical attributes and their hierarchical structures of target entities) based spectral-spatial feature transformation into the proposed framework, capable of achieving both high accuracy and interpretability on HSI based classification tasks. The proposed model introduces a two-stage feature learning process: in the first stage, an enhanced interpretable feature block extracts the low-level spectral features associated with the biophysical and biochemical attributes of target entities; and in the second stage, an interpretable capsule block extracts and encapsulates the high-level joint spectral-spatial features representing the hierarchical structure of biophysical and biochemical attributes of these target entities, which provides the model an improved performance on classification and intrinsic interpretability with reduced computational complexity. We have tested and evaluated the model using four real HSI datasets for four separate tasks (i.e. plant species classification, land cover classification, urban scene recognition, and crop disease recognition tasks). The proposed model has been compared with five state-of-the-art deep learning models.



### Autonomous task planning and situation awareness in robotic surgery
- **Arxiv ID**: http://arxiv.org/abs/2004.08911v1
- **DOI**: 10.1109/IROS45743.2020.9341382
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.08911v1)
- **Published**: 2020-04-19 17:08:22+00:00
- **Updated**: 2020-04-19 17:08:22+00:00
- **Authors**: Michele Ginesi, Daniele Meli, Andrea Roberti, Nicola Sansonetto, Paolo Fiorini
- **Comment**: Submitted to IROS 2020 conference
- **Journal**: 2020 IEEE International Conference on Intelligent Robots and
  Systems
- **Summary**: The use of robots in minimally invasive surgery has improved the quality of standard surgical procedures. So far, only the automation of simple surgical actions has been investigated by researchers, while the execution of structured tasks requiring reasoning on the environment and the choice among multiple actions is still managed by human surgeons. In this paper, we propose a framework to implement surgical task automation. The framework consists of a task-level reasoning module based on answer set programming, a low-level motion planning module based on dynamic movement primitives, and a situation awareness module. The logic-based reasoning module generates explainable plans and is able to recover from failure conditions, which are identified and explained by the situation awareness module interfacing to a human supervisor, for enhanced safety. Dynamic Movement Primitives allow to replicate the dexterity of surgeons and to adapt to obstacles and changes in the environment. The framework is validated on different versions of the standard surgical training peg-and-ring task.



### MER-GCN: Micro Expression Recognition Based on Relation Modeling with Graph Convolutional Network
- **Arxiv ID**: http://arxiv.org/abs/2004.08915v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.08915v1)
- **Published**: 2020-04-19 17:25:30+00:00
- **Updated**: 2020-04-19 17:25:30+00:00
- **Authors**: Ling Lo, Hong-Xia Xie, Hong-Han Shuai, Wen-Huang Cheng
- **Comment**: Accepted by IEEE MIPR 2020
- **Journal**: None
- **Summary**: Micro-Expression (ME) is the spontaneous, involuntary movement of a face that can reveal the true feeling. Recently, increasing researches have paid attention to this field combing deep learning techniques. Action units (AUs) are the fundamental actions reflecting the facial muscle movements and AU detection has been adopted by many researches to classify facial expressions. However, the time-consuming annotation process makes it difficult to correlate the combinations of AUs to specific emotion classes. Inspired by the nodes relationship building Graph Convolutional Networks (GCN), we propose an end-to-end AU-oriented graph classification network, namely MER-GCN, which uses 3D ConvNets to extract AU features and applies GCN layers to discover the dependency laying between AU nodes for ME categorization. To our best knowledge, this work is the first end-to-end architecture for Micro-Expression Recognition (MER) using AUs based GCN. The experimental results show that our approach outperforms CNN-based MER networks.



### Calculating Pose with Vanishing Points of Visual-Sphere Perspective Model
- **Arxiv ID**: http://arxiv.org/abs/2004.08933v4
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, 51A15, 51A25, 51A50, 65D19, 14N05, I.4.7; I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/2004.08933v4)
- **Published**: 2020-04-19 18:39:08+00:00
- **Updated**: 2023-05-14 03:23:47+00:00
- **Authors**: Jakub Maksymilian Fober
- **Comment**: 10 pages, 8 figures, 7 listings
- **Journal**: None
- **Summary**: The goal of the proposed method is to directly obtain a pose matrix of a known rectangular target, without estimation, using geometric techniques. This method is specifically tailored for real-time, extreme imaging setups exceeding 180{\deg} field of view, such as a fish-eye camera view. The introduced algorithm employs geometric algebra to determine the pose for a pair of coplanar parallel lines (ideally a tangent pair as in a rectangle). This is achieved by computing vanishing points on a visual unit sphere, which correspond to pose matrix vectors. The algorithm can determine pose for an extremely distorted view source without prior rectification, owing to a visual-sphere perspective model mapping of view coordinates. Mapping can be performed using either a perspective map lookup or a parametric universal perspective distortion model, which is also presented in this paper. The outcome is a robust pose matrix computation that can be executed on an embedded system using a microcontroller, offering high accuracy and low latency. This method can be further extended to a cubic target setup for comprehensive camera calibration. It may also prove valuable in other applications requiring low latency and extreme viewing angles.



### Exploring Racial Bias within Face Recognition via per-subject Adversarially-Enabled Data Augmentation
- **Arxiv ID**: http://arxiv.org/abs/2004.08945v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.08945v1)
- **Published**: 2020-04-19 19:46:32+00:00
- **Updated**: 2020-04-19 19:46:32+00:00
- **Authors**: Seyma Yucer, Samet Akçay, Noura Al-Moubayed, Toby P. Breckon
- **Comment**: CVPR 2020 - Fair, Data Efficient and Trusted Computer Vision Workshop
- **Journal**: None
- **Summary**: Whilst face recognition applications are becoming increasingly prevalent within our daily lives, leading approaches in the field still suffer from performance bias to the detriment of some racial profiles within society. In this study, we propose a novel adversarial derived data augmentation methodology that aims to enable dataset balance at a per-subject level via the use of image-to-image transformation for the transfer of sensitive racial characteristic facial features. Our aim is to automatically construct a synthesised dataset by transforming facial images across varying racial domains, while still preserving identity-related features, such that racially dependant features subsequently become irrelevant within the determination of subject identity. We construct our experiments on three significant face recognition variants: Softmax, CosFace and ArcFace loss over a common convolutional neural network backbone. In a side-by-side comparison, we show the positive impact our proposed technique can have on the recognition performance for (racial) minority groups within an originally imbalanced training dataset by reducing the pre-race variance in performance.



### Desmoking laparoscopy surgery images using an image-to-image translation guided by an embedded dark channel
- **Arxiv ID**: http://arxiv.org/abs/2004.08947v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.08947v1)
- **Published**: 2020-04-19 19:51:24+00:00
- **Updated**: 2020-04-19 19:51:24+00:00
- **Authors**: Sebastián Salazar-Colores, Hugo Alberto-Moreno, César Javier Ortiz-Echeverri, Gerardo Flores
- **Comment**: None
- **Journal**: None
- **Summary**: In laparoscopic surgery, the visibility in the image can be severely degraded by the smoke caused by the $CO_2$ injection, and dissection tools, thus reducing the visibility of organs and tissues. This lack of visibility increases the surgery time and even the probability of mistakes conducted by the surgeon, then producing negative consequences on the patient's health. In this paper, a novel computational approach to remove the smoke effects is introduced. The proposed method is based on an image-to-image conditional generative adversarial network in which a dark channel is used as an embedded guide mask. Obtained experimental results are evaluated and compared quantitatively with other desmoking and dehazing state-of-art methods using the metrics of the Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity (SSIM) index. Based on these metrics, it is found that the proposed method has improved performance compared to the state-of-the-art. Moreover, the processing time required by our method is 92 frames per second, and thus, it can be applied in a real-time medical system trough an embedded device.



### ResNeSt: Split-Attention Networks
- **Arxiv ID**: http://arxiv.org/abs/2004.08955v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.08955v2)
- **Published**: 2020-04-19 20:40:31+00:00
- **Updated**: 2020-12-30 05:05:15+00:00
- **Authors**: Hang Zhang, Chongruo Wu, Zhongyue Zhang, Yi Zhu, Haibin Lin, Zhi Zhang, Yue Sun, Tong He, Jonas Mueller, R. Manmatha, Mu Li, Alexander Smola
- **Comment**: None
- **Journal**: None
- **Summary**: It is well known that featuremap attention and multi-path representation are important for visual recognition. In this paper, we present a modularized architecture, which applies the channel-wise attention on different network branches to leverage their success in capturing cross-feature interactions and learning diverse representations. Our design results in a simple and unified computation block, which can be parameterized using only a few variables. Our model, named ResNeSt, outperforms EfficientNet in accuracy and latency trade-off on image classification. In addition, ResNeSt has achieved superior transfer learning results on several public benchmarks serving as the backbone, and has been adopted by the winning entries of COCO-LVIS challenge. The source code for complete system and pretrained models are publicly available.



### Data Augmentation Imbalance For Imbalanced Attribute Classification
- **Arxiv ID**: http://arxiv.org/abs/2004.13628v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.13628v3)
- **Published**: 2020-04-19 20:43:29+00:00
- **Updated**: 2020-05-21 07:36:49+00:00
- **Authors**: Yang Hu, Xiaying Bai, Pan Zhou, Fanhua Shang, Shengmei Shen
- **Comment**: This paper needs further revision
- **Journal**: None
- **Summary**: Pedestrian attribute recognition is an important multi-label classification problem. Although the convolutional neural networks are prominent in learning discriminative features from images, the data imbalance in multi-label setting for fine-grained tasks remains an open problem. In this paper, we propose a new re-sampling algorithm called: data augmentation imbalance (DAI) to explicitly enhance the ability to discriminate the fewer attributes via increasing the proportion of labels accounting for a small part. Fundamentally, by applying over-sampling and under-sampling on the multi-label dataset at the same time, the thought of robbing the rich attributes and helping the poor makes a significant contribution to DAI. Extensive empirical evidence shows that our DAI algorithm achieves state-of-the-art results, based on pedestrian attribute datasets, i.e. standard PA-100K and PETA datasets.



### Spectral GUI for Automated Tissue and Lesion Segmentation of T1 Weighted Breast MR Images
- **Arxiv ID**: http://arxiv.org/abs/2004.08960v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.08960v1)
- **Published**: 2020-04-19 20:52:03+00:00
- **Updated**: 2020-04-19 20:52:03+00:00
- **Authors**: Prajval Koul
- **Comment**: None
- **Journal**: None
- **Summary**: We present Spectral GUI, a multiplatform breast MR image analysis tool designed to facilitate the segmentation of fibro glandular tissues and lesions in T1 weighted breast MR images via a graphical user interface (GUI). Spectral GUIR uses spectrum loft method [1] for breast MR image segmentation. Not only is it interactive, but robust and expeditious at the same time. Being devoid of any machine learning algorithm, it shows exceptionally high execution speed with minimal overheads. The accuracy of the results has been simultaneously measured using performance metrics and expert entailment. The validity and applicability of the tool are discussed in the paper along with a crisp contrast with traditional machine learning principles, establishing the unequivocal foundation of it as a competent tool in the field of image analysis.



### Machine Learning based Pallets Detection and Tracking in AGVs
- **Arxiv ID**: http://arxiv.org/abs/2004.08965v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.SY, eess.IV, eess.SY
- **Links**: [PDF](http://arxiv.org/pdf/2004.08965v1)
- **Published**: 2020-04-19 21:17:13+00:00
- **Updated**: 2020-04-19 21:17:13+00:00
- **Authors**: Shengchang Zhang, Jie Xiang, Weijian Han
- **Comment**: 6 pages, 8 figures, 1 table
- **Journal**: None
- **Summary**: The use of automated guided vehicles (AGVs) has played a pivotal role in manufacturing and distribution operations, providing reliable and efficient product handling. In this project, we constructed a deep learning-based pallets detection and tracking architecture for pallets detection and position tracking. By using data preprocessing and augmentation techniques and experiment with hyperparameter tuning, we achieved the result with 25% reduction of error rate, 28.5% reduction of false negative rate, and 20% reduction of training time.



### Traffic Lane Detection using FCN
- **Arxiv ID**: http://arxiv.org/abs/2004.08977v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.08977v1)
- **Published**: 2020-04-19 22:25:12+00:00
- **Updated**: 2020-04-19 22:25:12+00:00
- **Authors**: Shengchang Zhang, Ahmed EI Koubia, Khaled Abdul Karim Mohammed
- **Comment**: 6 pages, 6 figures
- **Journal**: None
- **Summary**: Automatic lane detection is a crucial technology that enables self-driving cars to properly position themselves in a multi-lane urban driving environments. However, detecting diverse road markings in various weather conditions is a challenging task for conventional image processing or computer vision techniques. In recent years, the application of Deep Learning and Neural Networks in this area has proven to be very effective. In this project, we designed an Encoder- Decoder, Fully Convolutional Network for lane detection. This model was applied to a real-world large scale dataset and achieved a level of accuracy that outperformed our baseline model.



