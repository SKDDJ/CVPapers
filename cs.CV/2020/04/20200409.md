# Arxiv Papers in cs.CV on 2020-04-09
### Physics-enhanced machine learning for virtual fluorescence microscopy
- **Arxiv ID**: http://arxiv.org/abs/2004.04306v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.04306v2)
- **Published**: 2020-04-09 00:17:00+00:00
- **Updated**: 2020-04-21 22:19:15+00:00
- **Authors**: Colin L. Cooke, Fanjie Kong, Amey Chaware, Kevin C. Zhou, Kanghyun Kim, Rong Xu, D. Michael Ando, Samuel J. Yang, Pavan Chandra Konda, Roarke Horstmeyer
- **Comment**: 12 pages, 13 figures
- **Journal**: None
- **Summary**: This paper introduces a new method of data-driven microscope design for virtual fluorescence microscopy. Our results show that by including a model of illumination within the first layers of a deep convolutional neural network, it is possible to learn task-specific LED patterns that substantially improve the ability to infer fluorescence image information from unstained transmission microscopy images. We validated our method on two different experimental setups, with different magnifications and different sample types, to show a consistent improvement in performance as compared to conventional illumination methods. Additionally, to understand the importance of learned illumination on inference task, we varied the dynamic range of the fluorescent image targets (from one to seven bits), and showed that the margin of improvement for learned patterns increased with the information content of the target. This work demonstrates the power of programmable optical elements at enabling better machine learning algorithm performance and at providing physical insight into next generation of machine-controlled imaging systems.



### Learning to Scale Multilingual Representations for Vision-Language Tasks
- **Arxiv ID**: http://arxiv.org/abs/2004.04312v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2004.04312v2)
- **Published**: 2020-04-09 01:03:44+00:00
- **Updated**: 2020-08-27 19:01:28+00:00
- **Authors**: Andrea Burns, Donghyun Kim, Derry Wijaya, Kate Saenko, Bryan A. Plummer
- **Comment**: ECCV 2020 accepted spotlight paper
- **Journal**: None
- **Summary**: Current multilingual vision-language models either require a large number of additional parameters for each supported language, or suffer performance degradation as languages are added. In this paper, we propose a Scalable Multilingual Aligned Language Representation (SMALR) that supports many languages with few model parameters without sacrificing downstream task performance. SMALR learns a fixed size language-agnostic representation for most words in a multilingual vocabulary, keeping language-specific features for just a few. We use a masked cross-language modeling loss to align features with context from other languages. Additionally, we propose a cross-lingual consistency module that ensures predictions made for a query and its machine translation are comparable. The effectiveness of SMALR is demonstrated with ten diverse languages, over twice the number supported in vision-language tasks to date. We evaluate on multilingual image-sentence retrieval and outperform prior work by 3-4% with less than 1/5th the training parameters compared to other word embedding methods.



### Identification of splicing edges in tampered image based on Dichromatic Reflection Model
- **Arxiv ID**: http://arxiv.org/abs/2004.04317v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.04317v1)
- **Published**: 2020-04-09 01:25:28+00:00
- **Updated**: 2020-04-09 01:25:28+00:00
- **Authors**: Zhe Shen, Peng Sun, Yubo Lang, Lei Liu, Silong Peng
- **Comment**: None
- **Journal**: None
- **Summary**: Imaging is a sophisticated process combining a plenty of photovoltaic conversions, which lead to some spectral signatures beyond visual perception in the final images. Any manipulation against an original image will destroy these signatures and inevitably leave some traces in the final forgery. Therefore we present a novel optic-physical method to discriminate splicing edges from natural edges in a tampered image. First, we transform the forensic image from RGB into color space of S and o1o2. Then on the assumption of Dichromatic Reflection Model, edges in the image are discovered by composite gradient and classified into different types based on their different photometric properties. Finally, splicing edges are reserved against natural ones by a simple logical algorithm. Experiment results show the efficacy of the proposed method.



### LIAAD: Lightweight Attentive Angular Distillation for Large-scale Age-Invariant Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2004.05085v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.05085v2)
- **Published**: 2020-04-09 01:28:47+00:00
- **Updated**: 2022-09-11 19:18:08+00:00
- **Authors**: Thanh-Dat Truong, Chi Nhan Duong, Kha Gia Quach, Ngan Le, Tien D. Bui, Khoa Luu
- **Comment**: arXiv admin note: text overlap with arXiv:1905.10620
- **Journal**: None
- **Summary**: Disentangled representations have been commonly adopted to Age-invariant Face Recognition (AiFR) tasks. However, these methods have reached some limitations with (1) the requirement of large-scale face recognition (FR) training data with age labels, which is limited in practice; (2) heavy deep network architectures for high performance; and (3) their evaluations are usually taken place on age-related face databases while neglecting the standard large-scale FR databases to guarantee robustness. This work presents a novel Lightweight Attentive Angular Distillation (LIAAD) approach to Large-scale Lightweight AiFR that overcomes these limitations. Given two high-performance heavy networks as teachers with different specialized knowledge, LIAAD introduces a learning paradigm to efficiently distill the age-invariant attentive and angular knowledge from those teachers to a lightweight student network making it more powerful with higher FR accuracy and robust against age factor. Consequently, LIAAD approach is able to take the advantages of both FR datasets with and without age labels to train an AiFR model. Far apart from prior distillation methods mainly focusing on accuracy and compression ratios in closed-set problems, our LIAAD aims to solve the open-set problem, i.e. large-scale face recognition. Evaluations on LFW, IJB-B and IJB-C Janus, AgeDB and MegaFace-FGNet with one million distractors have demonstrated the efficiency of the proposed approach on light-weight structure. This work also presents a new longitudinal face aging (LogiFace) database \footnote{This database will be made available} for further studies in age-related facial problems in future.



### TOG: Targeted Adversarial Objectness Gradient Attacks on Real-time Object Detection Systems
- **Arxiv ID**: http://arxiv.org/abs/2004.04320v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.04320v1)
- **Published**: 2020-04-09 01:36:23+00:00
- **Updated**: 2020-04-09 01:36:23+00:00
- **Authors**: Ka-Ho Chow, Ling Liu, Mehmet Emre Gursoy, Stacey Truex, Wenqi Wei, Yanzhao Wu
- **Comment**: None
- **Journal**: None
- **Summary**: The rapid growth of real-time huge data capturing has pushed the deep learning and data analytic computing to the edge systems. Real-time object recognition on the edge is one of the representative deep neural network (DNN) powered edge systems for real-world mission-critical applications, such as autonomous driving and augmented reality. While DNN powered object detection edge systems celebrate many life-enriching opportunities, they also open doors for misuse and abuse. This paper presents three Targeted adversarial Objectness Gradient attacks, coined as TOG, which can cause the state-of-the-art deep object detection networks to suffer from object-vanishing, object-fabrication, and object-mislabeling attacks. We also present a universal objectness gradient attack to use adversarial transferability for black-box attacks, which is effective on any inputs with negligible attack time cost, low human perceptibility, and particularly detrimental to object detection edge systems. We report our experimental measurements using two benchmark datasets (PASCAL VOC and MS COCO) on two state-of-the-art detection algorithms (YOLO and SSD). The results demonstrate serious adversarial vulnerabilities and the compelling need for developing robust object detection systems.



### Quasi-Newton Solver for Robust Non-Rigid Registration
- **Arxiv ID**: http://arxiv.org/abs/2004.04322v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2004.04322v1)
- **Published**: 2020-04-09 01:45:05+00:00
- **Updated**: 2020-04-09 01:45:05+00:00
- **Authors**: Yuxin Yao, Bailin Deng, Weiwei Xu, Juyong Zhang
- **Comment**: Accepted to CVPR2020. The source code is available at
  https://github.com/Juyong/Fast_RNRR
- **Journal**: None
- **Summary**: Imperfect data (noise, outliers and partial overlap) and high degrees of freedom make non-rigid registration a classical challenging problem in computer vision. Existing methods typically adopt the $\ell_{p}$ type robust estimator to regularize the fitting and smoothness, and the proximal operator is used to solve the resulting non-smooth problem. However, the slow convergence of these algorithms limits its wide applications. In this paper, we propose a formulation for robust non-rigid registration based on a globally smooth robust estimator for data fitting and regularization, which can handle outliers and partial overlaps. We apply the majorization-minimization algorithm to the problem, which reduces each iteration to solving a simple least-squares problem with L-BFGS. Extensive experiments demonstrate the effectiveness of our method for non-rigid alignment between two shapes with outliers and partial overlap, with quantitative evaluation showing that it outperforms state-of-the-art methods in terms of registration accuracy and computational speed. The source code is available at https://github.com/Juyong/Fast_RNRR.



### MoreFusion: Multi-object Reasoning for 6D Pose Estimation from Volumetric Fusion
- **Arxiv ID**: http://arxiv.org/abs/2004.04336v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2004.04336v1)
- **Published**: 2020-04-09 02:29:30+00:00
- **Updated**: 2020-04-09 02:29:30+00:00
- **Authors**: Kentaro Wada, Edgar Sucar, Stephen James, Daniel Lenton, Andrew J. Davison
- **Comment**: 10 pages, 10 figures, IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR) 2020
- **Journal**: None
- **Summary**: Robots and other smart devices need efficient object-based scene representations from their on-board vision systems to reason about contact, physics and occlusion. Recognized precise object models will play an important role alongside non-parametric reconstructions of unrecognized structures. We present a system which can estimate the accurate poses of multiple known objects in contact and occlusion from real-time, embodied multi-view vision. Our approach makes 3D object pose proposals from single RGB-D views, accumulates pose estimates and non-parametric occupancy information from multiple views as the camera moves, and performs joint optimization to estimate consistent, non-intersecting poses for multiple objects in contact.   We verify the accuracy and robustness of our approach experimentally on 2 object datasets: YCB-Video, and our own challenging Cluttered YCB-Video. We demonstrate a real-time robotics application where a robot arm precisely and orderly disassembles complicated piles of objects, using only on-board RGB-D vision.



### Reciprocal Learning Networks for Human Trajectory Prediction
- **Arxiv ID**: http://arxiv.org/abs/2004.04340v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.04340v1)
- **Published**: 2020-04-09 02:50:29+00:00
- **Updated**: 2020-04-09 02:50:29+00:00
- **Authors**: Hao Sun, Zhiqun Zhao, Zhihai He
- **Comment**: None
- **Journal**: None
- **Summary**: We observe that the human trajectory is not only forward predictable, but also backward predictable. Both forward and backward trajectories follow the same social norms and obey the same physical constraints with the only difference in their time directions. Based on this unique property, we develop a new approach, called reciprocal learning, for human trajectory prediction. Two networks, forward and backward prediction networks, are tightly coupled, satisfying the reciprocal constraint, which allows them to be jointly learned. Based on this constraint, we borrow the concept of adversarial attacks of deep neural networks, which iteratively modifies the input of the network to match the given or forced network output, and develop a new method for network prediction, called reciprocal attack for matched prediction. It further improves the prediction accuracy. Our experimental results on benchmark datasets demonstrate that our new method outperforms the state-of-the-art methods for human trajectory prediction.



### Feedback Recurrent Autoencoder for Video Compression
- **Arxiv ID**: http://arxiv.org/abs/2004.04342v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.04342v1)
- **Published**: 2020-04-09 02:58:07+00:00
- **Updated**: 2020-04-09 02:58:07+00:00
- **Authors**: Adam Golinski, Reza Pourreza, Yang Yang, Guillaume Sautiere, Taco S Cohen
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advances in deep generative modeling have enabled efficient modeling of high dimensional data distributions and opened up a new horizon for solving data compression problems. Specifically, autoencoder based learned image or video compression solutions are emerging as strong competitors to traditional approaches. In this work, We propose a new network architecture, based on common and well studied components, for learned video compression operating in low latency mode. Our method yields state of the art MS-SSIM/rate performance on the high-resolution UVG dataset, among both learned video compression approaches and classical video compression methods (H.265 and H.264) in the rate range of interest for streaming applications. Additionally, we provide an analysis of existing approaches through the lens of their underlying probabilistic graphical models. Finally, we point out issues with temporal consistency and color shift observed in empirical evaluation, and suggest directions forward to alleviate those.



### Capsules for Biomedical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2004.04736v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.04736v2)
- **Published**: 2020-04-09 03:01:31+00:00
- **Updated**: 2020-12-10 21:53:16+00:00
- **Authors**: Rodney LaLonde, Ziyue Xu, Ismail Irmakci, Sanjay Jain, Ulas Bagci
- **Comment**: Extension of the non-archival Capsules of Object Segmentation with
  experiments on both clinical and pre-clinical pathological lung segmentation
  from CT scans and muscular and adipose tissue segmentation from MR images.
  Accepted for publication in Medical Image Analysis. DOI:
  https://doi.org/10.1016/j.media.2020.101889. arXiv admin note: text overlap
  with arXiv:1804.04241
- **Journal**: None
- **Summary**: Our work expands the use of capsule networks to the task of object segmentation for the first time in the literature. This is made possible via the introduction of locally-constrained routing and transformation matrix sharing, which reduces the parameter/memory burden and allows for the segmentation of objects at large resolutions. To compensate for the loss of global information in constraining the routing, we propose the concept of "deconvolutional" capsules to create a deep encoder-decoder style network, called SegCaps. We extend the masked reconstruction regularization to the task of segmentation and perform thorough ablation experiments on each component of our method. The proposed convolutional-deconvolutional capsule network, SegCaps, shows state-of-the-art results while using a fraction of the parameters of popular segmentation networks. To validate our proposed method, we perform experiments segmenting pathological lungs from clinical and pre-clinical thoracic computed tomography (CT) scans and segmenting muscle and adipose (fat) tissue from magnetic resonance imaging (MRI) scans of human subjects' thighs. Notably, our experiments in lung segmentation represent the largest-scale study in pathological lung segmentation in the literature, where we conduct experiments across five extremely challenging datasets, containing both clinical and pre-clinical subjects, and nearly 2000 computed-tomography scans. Our newly developed segmentation platform outperforms other methods across all datasets while utilizing less than 5% of the parameters in the popular U-Net for biomedical image segmentation. Further, we demonstrate capsules' ability to generalize to unseen rotations/reflections on natural images.



### Masked GANs for Unsupervised Depth and Pose Prediction with Scale Consistency
- **Arxiv ID**: http://arxiv.org/abs/2004.04345v3
- **DOI**: 10.1109/TNNLS.2020.3044181
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.04345v3)
- **Published**: 2020-04-09 03:12:52+00:00
- **Updated**: 2021-04-13 14:05:30+00:00
- **Authors**: Chaoqiang Zhao, Gary G. Yen, Qiyu Sun, Chongzhen Zhang, Yang Tang
- **Comment**: 11 pages, 7 figures
- **Journal**: None
- **Summary**: Previous work has shown that adversarial learning can be used for unsupervised monocular depth and visual odometry (VO) estimation, in which the adversarial loss and the geometric image reconstruction loss are utilized as the mainly supervisory signals to train the whole unsupervised framework. However, the performance of the adversarial framework and image reconstruction is usually limited by occlusions and the visual field changes between frames. This paper proposes a masked generative adversarial network (GAN) for unsupervised monocular depth and ego-motion estimation.The MaskNet and Boolean mask scheme are designed in this framework to eliminate the effects of occlusions and impacts of visual field changes on the reconstruction loss and adversarial loss, respectively. Furthermore, we also consider the scale consistency of our pose network by utilizing a new scale-consistency loss, and therefore, our pose network is capable of providing the full camera trajectory over a long monocular sequence. Extensive experiments on the KITTI dataset show that each component proposed in this paper contributes to the performance, and both our depth and trajectory predictions achieve competitive performance on the KITTI and Make3D datasets.



### Towards Inheritable Models for Open-Set Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2004.04388v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.04388v1)
- **Published**: 2020-04-09 07:16:30+00:00
- **Updated**: 2020-04-09 07:16:30+00:00
- **Authors**: Jogendra Nath Kundu, Naveen Venkat, Ambareesh Revanur, Rahul M V, R. Venkatesh Babu
- **Comment**: CVPR 2020 (Oral). Code available at
  https://github.com/val-iisc/inheritune
- **Journal**: None
- **Summary**: There has been a tremendous progress in Domain Adaptation (DA) for visual recognition tasks. Particularly, open-set DA has gained considerable attention wherein the target domain contains additional unseen categories. Existing open-set DA approaches demand access to a labeled source dataset along with unlabeled target instances. However, this reliance on co-existing source and target data is highly impractical in scenarios where data-sharing is restricted due to its proprietary nature or privacy concerns. Addressing this, we introduce a practical DA paradigm where a source-trained model is used to facilitate adaptation in the absence of the source dataset in future. To this end, we formalize knowledge inheritability as a novel concept and propose a simple yet effective solution to realize inheritable models suitable for the above practical paradigm. Further, we present an objective way to quantify inheritability to enable the selection of the most suitable source model for a given target domain, even in the absence of the source data. We provide theoretical insights followed by a thorough empirical evaluation demonstrating state-of-the-art open-set domain adaptation performance.



### Universal Source-Free Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2004.04393v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.04393v1)
- **Published**: 2020-04-09 07:26:20+00:00
- **Updated**: 2020-04-09 07:26:20+00:00
- **Authors**: Jogendra Nath Kundu, Naveen Venkat, Rahul M V, R. Venkatesh Babu
- **Comment**: CVPR 2020. Code available at https://github.com/val-iisc/usfda
- **Journal**: None
- **Summary**: There is a strong incentive to develop versatile learning techniques that can transfer the knowledge of class-separability from a labeled source domain to an unlabeled target domain in the presence of a domain-shift. Existing domain adaptation (DA) approaches are not equipped for practical DA scenarios as a result of their reliance on the knowledge of source-target label-set relationship (e.g. Closed-set, Open-set or Partial DA). Furthermore, almost all prior unsupervised DA works require coexistence of source and target samples even during deployment, making them unsuitable for real-time adaptation. Devoid of such impractical assumptions, we propose a novel two-stage learning process. 1) In the Procurement stage, we aim to equip the model for future source-free deployment, assuming no prior knowledge of the upcoming category-gap and domain-shift. To achieve this, we enhance the model's ability to reject out-of-source distribution samples by leveraging the available source data, in a novel generative classifier framework. 2) In the Deployment stage, the goal is to design a unified adaptation algorithm capable of operating across a wide range of category-gaps, with no access to the previously seen source samples. To this end, in contrast to the usage of complex adversarial training regimes, we define a simple yet effective source-free adaptation objective by utilizing a novel instance-level weighting mechanism, named as Source Similarity Metric (SSM). A thorough evaluation shows the practical usability of the proposed learning framework with superior DA performance even over state-of-the-art source-dependent approaches.



### Hierarchical Group Sparse Regularization for Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2004.04394v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.04394v1)
- **Published**: 2020-04-09 07:27:06+00:00
- **Updated**: 2020-04-09 07:27:06+00:00
- **Authors**: Kakeru Mitsuno, Junichi Miyao, Takio Kurita
- **Comment**: Accepted to IJCNN 2020
- **Journal**: None
- **Summary**: In a deep neural network (DNN), the number of the parameters is usually huge to get high learning performances. For that reason, it costs a lot of memory and substantial computational resources, and also causes overfitting. It is known that some parameters are redundant and can be removed from the network without decreasing performance. Many sparse regularization criteria have been proposed to solve this problem. In a convolutional neural network (CNN), group sparse regularizations are often used to remove unnecessary subsets of the weights, such as filters or channels. When we apply a group sparse regularization for the weights connected to a neuron as a group, each convolution filter is not treated as a target group in the regularization. In this paper, we introduce the concept of hierarchical grouping to solve this problem, and we propose several hierarchical group sparse regularization criteria for CNNs. Our proposed the hierarchical group sparse regularization can treat the weight for the input-neuron or the output-neuron as a group and convolutional filter as a group in the same group to prune the unnecessary subsets of weights. As a result, we can prune the weights more adequately depending on the structure of the network and the number of channels keeping high performance. In the experiment, we investigate the effectiveness of the proposed sparse regularizations through intensive comparison experiments on public datasets with several network architectures. Code is available on GitHub: "https://github.com/K-Mitsuno/hierarchical-group-sparse-regularization"



### Score-Guided Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2004.04396v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.04396v2)
- **Published**: 2020-04-09 07:32:43+00:00
- **Updated**: 2020-05-27 04:27:26+00:00
- **Authors**: Minhyeok Lee, Junhee Seok
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a Generative Adversarial Network (GAN) that introduces an evaluator module using pre-trained networks. The proposed model, called score-guided GAN (ScoreGAN), is trained with an evaluation metric for GANs, i.e., the Inception score, as a rough guide for the training of the generator. By using another pre-trained network instead of the Inception network, ScoreGAN circumvents the overfitting of the Inception network in order that generated samples do not correspond to adversarial examples of the Inception network. Also, to prevent the overfitting, the evaluation metrics are employed only as an auxiliary role, while the conventional target of GANs is mainly used. Evaluated with the CIFAR-10 dataset, ScoreGAN demonstrated an Inception score of 10.36$\pm$0.15, which corresponds to state-of-the-art performance. Furthermore, to generalize the effectiveness of ScoreGAN, the model was further evaluated with another dataset, i.e., the CIFAR-100; as a result, ScoreGAN outperformed the other existing methods, where the Fr\'echet Inception Distance (FID) was 13.98.



### Online Meta-Learning for Multi-Source and Semi-Supervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2004.04398v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.04398v2)
- **Published**: 2020-04-09 07:48:22+00:00
- **Updated**: 2020-07-27 12:55:37+00:00
- **Authors**: Da Li, Timothy Hospedales
- **Comment**: ECCV 2020 CR version
- **Journal**: None
- **Summary**: Domain adaptation (DA) is the topical problem of adapting models from labelled source datasets so that they perform well on target datasets where only unlabelled or partially labelled data is available. Many methods have been proposed to address this problem through different ways to minimise the domain shift between source and target datasets. In this paper we take an orthogonal perspective and propose a framework to further enhance performance by meta-learning the initial conditions of existing DA algorithms. This is challenging compared to the more widely considered setting of few-shot meta-learning, due to the length of the computation graph involved. Therefore we propose an online shortest-path meta-learning framework that is both computationally tractable and practically effective for improving DA performance. We present variants for both multi-source unsupervised domain adaptation (MSDA), and semi-supervised domain adaptation (SSDA). Importantly, our approach is agnostic to the base adaptation algorithm, and can be applied to improve many techniques. Experimentally, we demonstrate improvements on classic (DANN) and recent (MCD and MME) techniques for MSDA and SSDA, and ultimately achieve state of the art results on several DA benchmarks including the largest scale DomainNet.



### Self-Supervised 3D Human Pose Estimation via Part Guided Novel Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2004.04400v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.04400v1)
- **Published**: 2020-04-09 07:55:01+00:00
- **Updated**: 2020-04-09 07:55:01+00:00
- **Authors**: Jogendra Nath Kundu, Siddharth Seth, Varun Jampani, Mugalodi Rakesh, R. Venkatesh Babu, Anirban Chakraborty
- **Comment**: CVPR 2020 (Oral)
- **Journal**: None
- **Summary**: Camera captured human pose is an outcome of several sources of variation. Performance of supervised 3D pose estimation approaches comes at the cost of dispensing with variations, such as shape and appearance, that may be useful for solving other related tasks. As a result, the learned model not only inculcates task-bias but also dataset-bias because of its strong reliance on the annotated samples, which also holds true for weakly-supervised models. Acknowledging this, we propose a self-supervised learning framework to disentangle such variations from unlabeled video frames. We leverage the prior knowledge on human skeleton and poses in the form of a single part-based 2D puppet model, human pose articulation constraints, and a set of unpaired 3D poses. Our differentiable formalization, bridging the representation gap between the 3D pose and spatial part maps, not only facilitates discovery of interpretable pose disentanglement but also allows us to operate on videos with diverse camera movements. Qualitative results on unseen in-the-wild datasets establish our superior generalization across multiple tasks beyond the primary tasks of 3D pose estimation and part segmentation. Furthermore, we demonstrate state-of-the-art weakly-supervised 3D pose estimation performance on both Human3.6M and MPI-INF-3DHP datasets.



### Automatic detection of acute ischemic stroke using non-contrast computed tomography and two-stage deep learning model
- **Arxiv ID**: http://arxiv.org/abs/2004.04432v2
- **DOI**: 10.1016/j.cmpb.2020.105711
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.04432v2)
- **Published**: 2020-04-09 09:14:24+00:00
- **Updated**: 2020-09-08 08:56:01+00:00
- **Authors**: Mizuho Nishio, Sho Koyasu, Shunjiro Noguchi, Takao Kiguchi, Kanako Nakatsu, Thai Akasaka, Hiroki Yamada, Kyo Itoh
- **Comment**: None
- **Journal**: Computer Methods and Programs in Biomedicine 196 (2020) 105711
- **Summary**: Background and Purpose: We aimed to develop and evaluate an automatic acute ischemic stroke-related (AIS) detection system involving a two-stage deep learning model.   Methods: We included 238 cases from two different institutions. AIS-related findings were annotated on each of the 238 sets of head CT images by referring to head magnetic resonance imaging (MRI) images in which an MRI examination was performed within 24 h following the CT scan. These 238 annotated cases were divided into a training set including 189 cases and test set including 49 cases. Subsequently, a two-stage deep learning detection model was constructed from the training set using the You Only Look Once v3 model and Visual Geometry Group 16 classification model. Then, the two-stage model performed the AIS detection process in the test set. To assess the detection model's results, a board-certified radiologist also evaluated the test set head CT images with and without the aid of the detection model. The sensitivity of AIS detection and number of false positives were calculated for the evaluation of the test set detection results. The sensitivity of the radiologist with and without the software detection results was compared using the McNemar test. A p-value of less than 0.05 was considered statistically significant.   Results: For the two-stage model and radiologist without and with the use of the software results, the sensitivity was 37.3%, 33.3%, and 41.3%, respectively, and the number of false positives per one case was 1.265, 0.327, and 0.388, respectively. On using the two-stage detection model's results, the board-certified radiologist's detection sensitivity significantly improved (p-value = 0.0313).   Conclusions: Our detection system involving the two-stage deep learning model significantly improved the radiologist's sensitivity in AIS detection.



### DeepSEE: Deep Disentangled Semantic Explorative Extreme Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2004.04433v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.04433v3)
- **Published**: 2020-04-09 09:14:42+00:00
- **Updated**: 2020-10-02 08:48:07+00:00
- **Authors**: Marcel C. Bühler, Andrés Romero, Radu Timofte
- **Comment**: 19 pages. Supplementary material is available on the project page.
  Accepted for oral presentation at the 15th Asian Conference on Computer
  Vision (ACCV) 2020
- **Journal**: None
- **Summary**: Super-resolution (SR) is by definition ill-posed. There are infinitely many plausible high-resolution variants for a given low-resolution natural image. Most of the current literature aims at a single deterministic solution of either high reconstruction fidelity or photo-realistic perceptual quality. In this work, we propose an explorative facial super-resolution framework, DeepSEE, for Deep disentangled Semantic Explorative Extreme super-resolution. To the best of our knowledge, DeepSEE is the first method to leverage semantic maps for explorative super-resolution. In particular, it provides control of the semantic regions, their disentangled appearance and it allows a broad range of image manipulations. We validate DeepSEE on faces, for up to 32x magnification and exploration of the space of super-resolution. Our code and models are available at: https://mcbuehler.github.io/DeepSEE/



### CenterMask: single shot instance segmentation with point representation
- **Arxiv ID**: http://arxiv.org/abs/2004.04446v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.04446v2)
- **Published**: 2020-04-09 09:35:15+00:00
- **Updated**: 2020-04-11 05:12:10+00:00
- **Authors**: Yuqing Wang, Zhaoliang Xu, Hao Shen, Baoshan Cheng, Lirong Yang
- **Comment**: To appear at CVPR 2020
- **Journal**: None
- **Summary**: In this paper, we propose a single-shot instance segmentation method, which is simple, fast and accurate. There are two main challenges for one-stage instance segmentation: object instances differentiation and pixel-wise feature alignment. Accordingly, we decompose the instance segmentation into two parallel subtasks: Local Shape prediction that separates instances even in overlapping conditions, and Global Saliency generation that segments the whole image in a pixel-to-pixel manner. The outputs of the two branches are assembled to form the final instance masks. To realize that, the local shape information is adopted from the representation of object center points. Totally trained from scratch and without any bells and whistles, the proposed CenterMask achieves 34.5 mask AP with a speed of 12.3 fps, using a single-model with single-scale training/testing on the challenging COCO dataset. The accuracy is higher than all other one-stage instance segmentation methods except the 5 times slower TensorMask, which shows the effectiveness of CenterMask. Besides, our method can be easily embedded to other one-stage object detectors such as FCOS and performs well, showing the generalization of CenterMask.



### TensorProjection Layer: A Tensor-Based Dimensionality Reduction Method in CNN
- **Arxiv ID**: http://arxiv.org/abs/2004.04454v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.04454v1)
- **Published**: 2020-04-09 09:52:49+00:00
- **Updated**: 2020-04-09 09:52:49+00:00
- **Authors**: Toshinari Morimoto, Su-Yun Huang
- **Comment**: 18 pages
- **Journal**: None
- **Summary**: In this paper, we propose a dimensionality reduction method applied to tensor-structured data as a hidden layer (we call it TensorProjection Layer) in a convolutional neural network. Our proposed method transforms input tensors into ones with a smaller dimension by projection. The directions of projection are viewed as training parameters associated with our proposed layer and trained via a supervised learning criterion such as minimization of the cross-entropy loss function. We discuss the gradients of the loss function with respect to the parameters associated with our proposed layer. We also implement simple numerical experiments to evaluate the performance of the TensorProjection Layer.



### Decoupled Gradient Harmonized Detector for Partial Annotation: Application to Signet Ring Cell Detection
- **Arxiv ID**: http://arxiv.org/abs/2004.04455v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.04455v1)
- **Published**: 2020-04-09 09:53:11+00:00
- **Updated**: 2020-04-09 09:53:11+00:00
- **Authors**: Tiancheng Lin, Yuanfan Guo, Canqian Yang, Jiancheng Yang, Yi Xu
- **Comment**: accepted to Neurocomputing; 1st runner up of MICCAI DigestPath2019
  challenge
- **Journal**: None
- **Summary**: Early diagnosis of signet ring cell carcinoma dramatically improves the survival rate of patients. Due to lack of public dataset and expert-level annotations, automatic detection on signet ring cell (SRC) has not been thoroughly investigated. In MICCAI DigestPath2019 challenge, apart from foreground (SRC region)-background (normal tissue area) class imbalance, SRCs are partially annotated due to costly medical image annotation, which introduces extra label noise. To address the issues simultaneously, we propose Decoupled Gradient Harmonizing Mechanism (DGHM) and embed it into classification loss, denoted as DGHM-C loss. Specifically, besides positive (SRCs) and negative (normal tissues) examples, we further decouple noisy examples from clean examples and harmonize the corresponding gradient distributions in classification respectively. Without whistles and bells, we achieved the 2nd place in the challenge. Ablation studies and controlled label missing rate experiments demonstrate that DGHM-C loss can bring substantial improvement in partially annotated object detection.



### FKAConv: Feature-Kernel Alignment for Point Cloud Convolution
- **Arxiv ID**: http://arxiv.org/abs/2004.04462v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, 68T10, I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/2004.04462v3)
- **Published**: 2020-04-09 10:12:45+00:00
- **Updated**: 2020-11-24 10:32:36+00:00
- **Authors**: Alexandre Boulch, Gilles Puy, Renaud Marlet
- **Comment**: None
- **Journal**: None
- **Summary**: Recent state-of-the-art methods for point cloud processing are based on the notion of point convolution, for which several approaches have been proposed. In this paper, inspired by discrete convolution in image processing, we provide a formulation to relate and analyze a number of point convolution methods. We also propose our own convolution variant, that separates the estimation of geometry-less kernel weights and their alignment to the spatial support of features. Additionally, we define a point sampling strategy for convolution that is both effective and fast. Finally, using our convolution and sampling strategy, we show competitive results on classification and semantic segmentation benchmarks while being time and memory efficient.



### Adversarial Latent Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/2004.04467v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.04467v1)
- **Published**: 2020-04-09 10:33:44+00:00
- **Updated**: 2020-04-09 10:33:44+00:00
- **Authors**: Stanislav Pidhorskyi, Donald Adjeroh, Gianfranco Doretto
- **Comment**: None
- **Journal**: None
- **Summary**: Autoencoder networks are unsupervised approaches aiming at combining generative and representational properties by learning simultaneously an encoder-generator map. Although studied extensively, the issues of whether they have the same generative power of GANs, or learn disentangled representations, have not been fully addressed. We introduce an autoencoder that tackles these issues jointly, which we call Adversarial Latent Autoencoder (ALAE). It is a general architecture that can leverage recent improvements on GAN training procedures. We designed two autoencoders: one based on a MLP encoder, and another based on a StyleGAN generator, which we call StyleALAE. We verify the disentanglement properties of both architectures. We show that StyleALAE can not only generate 1024x1024 face images with comparable quality of StyleGAN, but at the same resolution can also produce face reconstructions and manipulations based on real images. This makes ALAE the first autoencoder able to compare with, and go beyond the capabilities of a generator-only type of architecture.



### NodeSLAM: Neural Object Descriptors for Multi-View Shape Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2004.04485v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.04485v2)
- **Published**: 2020-04-09 11:09:56+00:00
- **Updated**: 2020-10-10 16:41:59+00:00
- **Authors**: Edgar Sucar, Kentaro Wada, Andrew Davison
- **Comment**: to be published in 3DV
- **Journal**: None
- **Summary**: The choice of scene representation is crucial in both the shape inference algorithms it requires and the smart applications it enables. We present efficient and optimisable multi-class learned object descriptors together with a novel probabilistic and differential rendering engine, for principled full object shape inference from one or more RGB-D images. Our framework allows for accurate and robust 3D object reconstruction which enables multiple applications including robot grasping and placing, augmented reality, and the first object-level SLAM system capable of optimising object poses and shapes jointly with camera trajectory.



### Multi-Granularity Canonical Appearance Pooling for Remote Sensing Scene Classification
- **Arxiv ID**: http://arxiv.org/abs/2004.04491v1
- **DOI**: 10.1109/TIP.2020.2983560
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.04491v1)
- **Published**: 2020-04-09 11:24:00+00:00
- **Updated**: 2020-04-09 11:24:00+00:00
- **Authors**: S. Wang, Y. Guan, L. Shao
- **Comment**: This paper is going to be published by IEEE Transactions on Image
  Processing
- **Journal**: IEEE Transactions on Image Processing 29, 5396--5407 (2020)
- **Summary**: Recognising remote sensing scene images remains challenging due to large visual-semantic discrepancies. These mainly arise due to the lack of detailed annotations that can be employed to align pixel-level representations with high-level semantic labels. As the tagging process is labour-intensive and subjective, we hereby propose a novel Multi-Granularity Canonical Appearance Pooling (MG-CAP) to automatically capture the latent ontological structure of remote sensing datasets. We design a granular framework that allows progressively cropping the input image to learn multi-grained features. For each specific granularity, we discover the canonical appearance from a set of pre-defined transformations and learn the corresponding CNN features through a maxout-based Siamese style architecture. Then, we replace the standard CNN features with Gaussian covariance matrices and adopt the proper matrix normalisations for improving the discriminative power of features. Besides, we provide a stable solution for training the eigenvalue-decomposition function (EIG) in a GPU and demonstrate the corresponding back-propagation using matrix calculus. Extensive experiments have shown that our framework can achieve promising results in public remote sensing scene datasets.



### A Proposed IoT Smart Trap using Computer Vision for Sustainable Pest Control in Coffee Culture
- **Arxiv ID**: http://arxiv.org/abs/2004.04504v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NI, 65D19, I.5.4; C.2.0
- **Links**: [PDF](http://arxiv.org/pdf/2004.04504v1)
- **Published**: 2020-04-09 12:04:15+00:00
- **Updated**: 2020-04-09 12:04:15+00:00
- **Authors**: Vitor Alexandre Campos Figueiredo, Samuel Mafra, Joel Rodrigues
- **Comment**: 10 pages, 7 figures
- **Journal**: None
- **Summary**: The Internet of Things (IoT) is emerging as a multi-purpose technology with enormous potential for improving the quality of life in several areas. In particular, IoT has been applied in agriculture to make it more sustainable ecologically. For instance, electronic traps have the potential to perform pest control without any pesticide. In this paper, a smart trap with IoT capabilities that uses computer vision to identify the insect of interest is proposed. The solution includes 1) an embedded system with camera, GPS sensor and motor actuators; 2) an IoT middleware as database service provider, and 3) a Web application to present data by a configurable heat map. The demonstration of proposed solution is exposed and the main conclusions are the perception about pest concentration at the plantation and the viability as alternative pest control over traditional control based on pesticides.



### Learnable Subspace Clustering
- **Arxiv ID**: http://arxiv.org/abs/2004.04520v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.04520v1)
- **Published**: 2020-04-09 12:53:28+00:00
- **Updated**: 2020-04-09 12:53:28+00:00
- **Authors**: Jun Li, Hongfu Liu, Zhiqiang Tao, Handong Zhao, Yun Fu
- **Comment**: IEEE Transactions on Neural Networks and Learning Systems (accepted
  with minor revision)
- **Journal**: None
- **Summary**: This paper studies the large-scale subspace clustering (LSSC) problem with million data points. Many popular subspace clustering methods cannot directly handle the LSSC problem although they have been considered as state-of-the-art methods for small-scale data points. A basic reason is that these methods often choose all data points as a big dictionary to build huge coding models, which results in a high time and space complexity. In this paper, we develop a learnable subspace clustering paradigm to efficiently solve the LSSC problem. The key idea is to learn a parametric function to partition the high-dimensional subspaces into their underlying low-dimensional subspaces instead of the expensive costs of the classical coding models. Moreover, we propose a unified robust predictive coding machine (RPCM) to learn the parametric function, which can be solved by an alternating minimization algorithm. In addition, we provide a bounded contraction analysis of the parametric function. To the best of our knowledge, this paper is the first work to efficiently cluster millions of data points among the subspace clustering methods. Experiments on million-scale datasets verify that our paradigm outperforms the related state-of-the-art methods in both efficiency and effectiveness.



### Spatial Information Guided Convolution for Real-Time RGBD Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2004.04534v2
- **DOI**: 10.1109/TIP.2021.3049332
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.04534v2)
- **Published**: 2020-04-09 13:38:05+00:00
- **Updated**: 2021-01-08 04:24:35+00:00
- **Authors**: Lin-Zhuo Chen, Zheng Lin, Ziqin Wang, Yong-Liang Yang, Ming-Ming Cheng
- **Comment**: None
- **Journal**: None
- **Summary**: 3D spatial information is known to be beneficial to the semantic segmentation task. Most existing methods take 3D spatial data as an additional input, leading to a two-stream segmentation network that processes RGB and 3D spatial information separately. This solution greatly increases the inference time and severely limits its scope for real-time applications. To solve this problem, we propose Spatial information guided Convolution (S-Conv), which allows efficient RGB feature and 3D spatial information integration. S-Conv is competent to infer the sampling offset of the convolution kernel guided by the 3D spatial information, helping the convolutional layer adjust the receptive field and adapt to geometric transformations. S-Conv also incorporates geometric information into the feature learning process by generating spatially adaptive convolutional weights. The capability of perceiving geometry is largely enhanced without much affecting the amount of parameters and computational cost. We further embed S-Conv into a semantic segmentation network, called Spatial information Guided convolutional Network (SGNet), resulting in real-time inference and state-of-the-art performance on NYUDv2 and SUNRGBD datasets.



### SpatialSim: Recognizing Spatial Configurations of Objects with Graph Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2004.04546v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.04546v2)
- **Published**: 2020-04-09 14:13:20+00:00
- **Updated**: 2020-07-16 18:16:31+00:00
- **Authors**: Laetitia Teodorescu, Katja Hofmann, Pierre-Yves Oudeyer
- **Comment**: None
- **Journal**: None
- **Summary**: Recognizing precise geometrical configurations of groups of objects is a key capability of human spatial cognition, yet little studied in the deep learning literature so far. In particular, a fundamental problem is how a machine can learn and compare classes of geometric spatial configurations that are invariant to the point of view of an external observer. In this paper we make two key contributions. First, we propose SpatialSim (Spatial Similarity), a novel geometrical reasoning benchmark, and argue that progress on this benchmark would pave the way towards a general solution to address this challenge in the real world. This benchmark is composed of two tasks: Identification and Comparison, each one instantiated in increasing levels of difficulty. Secondly, we study how relational inductive biases exhibited by fully-connected message-passing Graph Neural Networks (MPGNNs) are useful to solve those tasks, and show their advantages over less relational baselines such as Deep Sets and unstructured models such as Multi-Layer Perceptrons. Finally, we highlight the current limits of GNNs in these tasks.



### Sequential View Synthesis with Transformer
- **Arxiv ID**: http://arxiv.org/abs/2004.04548v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.04548v2)
- **Published**: 2020-04-09 14:15:27+00:00
- **Updated**: 2020-09-22 08:53:28+00:00
- **Authors**: Phong Nguyen-Ha, Lam Huynh, Esa Rahtu, Janne Heikkila
- **Comment**: Code is available at: https://github.com/phongnhhn92/TransformerGQN;
  Supplementary material: https://bit.ly/3kEgnzU
- **Journal**: None
- **Summary**: This paper addresses the problem of novel view synthesis by means of neural rendering, where we are interested in predicting the novel view at an arbitrary camera pose based on a given set of input images from other viewpoints. Using the known query pose and input poses, we create an ordered set of observations that leads to the target view. Thus, the problem of single novel view synthesis is reformulated as a sequential view prediction task. In this paper, the proposed Transformer-based Generative Query Network (T-GQN) extends the neural-rendering methods by adding two new concepts. First, we use multi-view attention learning between context images to obtain multiple implicit scene representations. Second, we introduce a sequential rendering decoder to predict an image sequence, including the target view, based on the learned representations. Finally, we evaluate our model on various challenging datasets and demonstrate that our model not only gives consistent predictions but also doesn't require any retraining for finetuning.



### Self-supervised Equivariant Attention Mechanism for Weakly Supervised Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2004.04581v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.04581v1)
- **Published**: 2020-04-09 14:57:57+00:00
- **Updated**: 2020-04-09 14:57:57+00:00
- **Authors**: Yude Wang, Jie Zhang, Meina Kan, Shiguang Shan, Xilin Chen
- **Comment**: To appear at CVPR 2020 (Oral)
- **Journal**: None
- **Summary**: Image-level weakly supervised semantic segmentation is a challenging problem that has been deeply studied in recent years. Most of advanced solutions exploit class activation map (CAM). However, CAMs can hardly serve as the object mask due to the gap between full and weak supervisions. In this paper, we propose a self-supervised equivariant attention mechanism (SEAM) to discover additional supervision and narrow the gap. Our method is based on the observation that equivariance is an implicit constraint in fully supervised semantic segmentation, whose pixel-level labels take the same spatial transformation as the input images during data augmentation. However, this constraint is lost on the CAMs trained by image-level supervision. Therefore, we propose consistency regularization on predicted CAMs from various transformed images to provide self-supervision for network learning. Moreover, we propose a pixel correlation module (PCM), which exploits context appearance information and refines the prediction of current pixel by its similar neighbors, leading to further improvement on CAMs consistency. Extensive experiments on PASCAL VOC 2012 dataset demonstrate our method outperforms state-of-the-art methods using the same level of supervision. The code is released online.



### DeepCOVIDExplainer: Explainable COVID-19 Diagnosis Based on Chest X-ray Images
- **Arxiv ID**: http://arxiv.org/abs/2004.04582v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.04582v3)
- **Published**: 2020-04-09 15:03:58+00:00
- **Updated**: 2020-06-06 20:31:13+00:00
- **Authors**: Md. Rezaul Karim, Till Döhmen, Dietrich Rebholz-Schuhmann, Stefan Decker, Michael Cochez, Oya Beyan
- **Comment**: None
- **Journal**: None
- **Summary**: Amid the coronavirus disease(COVID-19) pandemic, humanity experiences a rapid increase in infection numbers across the world. Challenge hospitals are faced with, in the fight against the virus, is the effective screening of incoming patients. One methodology is the assessment of chest radiography(CXR) images, which usually requires expert radiologist's knowledge. In this paper, we propose an explainable deep neural networks(DNN)-based method for automatic detection of COVID-19 symptoms from CXR images, which we call DeepCOVIDExplainer. We used 15,959 CXR images of 15,854 patients, covering normal, pneumonia, and COVID-19 cases. CXR images are first comprehensively preprocessed, before being augmented and classified with a neural ensemble method, followed by highlighting class-discriminating regions using gradient-guided class activation maps(Grad-CAM++) and layer-wise relevance propagation(LRP). Further, we provide human-interpretable explanations of the predictions. Evaluation results based on hold-out data show that our approach can identify COVID-19 confidently with a positive predictive value(PPV) of 91.6%, 92.45%, and 96.12%; precision, recall, and F1 score of 94.6%, 94.3%, and 94.6%, respectively for normal, pneumonia, and COVID-19 cases, respectively, making it comparable or improved results over recent approaches. We hope that our findings will be a useful contribution to the fight against COVID-19 and, in more general, towards an increasing acceptance and adoption of AI-assisted applications in the clinical practice.



### Cortical surface registration using unsupervised learning
- **Arxiv ID**: http://arxiv.org/abs/2004.04617v2
- **DOI**: 10.1016/j.neuroimage.2020.117161
- **Categories**: **eess.IV**, cs.CV, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/2004.04617v2)
- **Published**: 2020-04-09 15:59:13+00:00
- **Updated**: 2020-07-09 09:06:55+00:00
- **Authors**: Jieyu Cheng, Adrian V. Dalca, Bruce Fischl, Lilla Zollei
- **Comment**: cortical surface registration, deep network, unsupervised learning,
  registration, deep learning, cortical, spherical, invertible
- **Journal**: NeuroImage, Volume 221, 1 November 2020, 117161
- **Summary**: Non-rigid cortical registration is an important and challenging task due to the geometric complexity of the human cortex and the high degree of inter-subject variability. A conventional solution is to use a spherical representation of surface properties and perform registration by aligning cortical folding patterns in that space. This strategy produces accurate spatial alignment but often requires a high computational cost. Recently, convolutional neural networks (CNNs) have demonstrated the potential to dramatically speed up volumetric registration. However, due to distortions introduced by projecting a sphere to a 2D plane, a direct application of recent learning-based methods to surfaces yields poor results. In this study, we present SphereMorph, a diffeomorphic registration framework for cortical surfaces using deep networks that addresses these issues. SphereMorph uses a UNet-style network associated with a spherical kernel to learn the displacement field and warps the sphere using a modified spatial transformer layer. We propose a resampling weight in computing the data fitting loss to account for distortions introduced by polar projection, and demonstrate the performance of our proposed method on two tasks, including cortical parcellation and group-wise functional area alignment. The experiments show that the proposed SphereMorph is capable of modeling the geometric registration problem in a CNN framework and demonstrate superior registration accuracy and computational efficiency. The source code of SphereMorph will be released to the public upon acceptance of this manuscript at https://github.com/voxelmorph/spheremorph.



### AdaStereo: A Simple and Efficient Approach for Adaptive Stereo Matching
- **Arxiv ID**: http://arxiv.org/abs/2004.04627v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.04627v3)
- **Published**: 2020-04-09 16:15:13+00:00
- **Updated**: 2021-03-27 03:40:13+00:00
- **Authors**: Xiao Song, Guorun Yang, Xinge Zhu, Hui Zhou, Zhe Wang, Jianping Shi
- **Comment**: Accepted by CVPR 2021
- **Journal**: None
- **Summary**: Recently, records on stereo matching benchmarks are constantly broken by end-to-end disparity networks. However, the domain adaptation ability of these deep models is quite poor. Addressing such problem, we present a novel domain-adaptive pipeline called AdaStereo that aims to align multi-level representations for deep stereo matching networks. Compared to previous methods for adaptive stereo matching, our AdaStereo realizes a more standard, complete and effective domain adaptation pipeline. Firstly, we propose a non-adversarial progressive color transfer algorithm for input image-level alignment. Secondly, we design an efficient parameter-free cost normalization layer for internal feature-level alignment. Lastly, a highly related auxiliary task, self-supervised occlusion-aware reconstruction is presented to narrow down the gaps in output space. Our AdaStereo models achieve state-of-the-art cross-domain performance on multiple stereo benchmarks, including KITTI, Middlebury, ETH3D, and DrivingStereo, even outperforming disparity networks finetuned with target-domain ground-truths.



### Where Does It End? -- Reasoning About Hidden Surfaces by Object Intersection Constraints
- **Arxiv ID**: http://arxiv.org/abs/2004.04630v3
- **DOI**: 10.1109/CVPR42600.2020.00961
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.04630v3)
- **Published**: 2020-04-09 16:18:02+00:00
- **Updated**: 2020-11-24 15:26:40+00:00
- **Authors**: Michael Strecke, Joerg Stueckler
- **Comment**: IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
  2020, Project page: https://cosection.is.tue.mpg.de/, Source code:
  https://github.com/EmbodiedVision/cosection
- **Journal**: IEEE/CVF Conference on Computer Vision and Pattern Recognition
  (CVPR) 2020
- **Summary**: Dynamic scene understanding is an essential capability in robotics and VR/AR. In this paper we propose Co-Section, an optimization-based approach to 3D dynamic scene reconstruction, which infers hidden shape information from intersection constraints. An object-level dynamic SLAM frontend detects, segments, tracks and maps dynamic objects in the scene. Our optimization backend completes the shapes using hull and intersection constraints between the objects. In experiments, we demonstrate our approach on real and synthetic dynamic scene datasets. We also assess the shape completion performance of our method quantitatively. To the best of our knowledge, our approach is the first method to incorporate such physical plausibility constraints on object intersections for shape completion of dynamic objects in an energy minimization framework.



### TuiGAN: Learning Versatile Image-to-Image Translation with Two Unpaired Images
- **Arxiv ID**: http://arxiv.org/abs/2004.04634v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.04634v2)
- **Published**: 2020-04-09 16:23:59+00:00
- **Updated**: 2020-08-09 02:23:03+00:00
- **Authors**: Jianxin Lin, Yingxue Pang, Yingce Xia, Zhibo Chen, Jiebo Luo
- **Comment**: ECCV 2020 Spotlight
- **Journal**: None
- **Summary**: An unsupervised image-to-image translation (UI2I) task deals with learning a mapping between two domains without paired images. While existing UI2I methods usually require numerous unpaired images from different domains for training, there are many scenarios where training data is quite limited. In this paper, we argue that even if each domain contains a single image, UI2I can still be achieved. To this end, we propose TuiGAN, a generative model that is trained on only two unpaired images and amounts to one-shot unsupervised learning. With TuiGAN, an image is translated in a coarse-to-fine manner where the generated image is gradually refined from global structures to local details. We conduct extensive experiments to verify that our versatile method can outperform strong baselines on a wide variety of UI2I tasks. Moreover, TuiGAN is capable of achieving comparable performance with the state-of-the-art UI2I models trained with sufficient data.



### Test-Time Adaptable Neural Networks for Robust Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2004.04668v4
- **DOI**: 10.1016/j.media.2020.101907
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.04668v4)
- **Published**: 2020-04-09 16:57:27+00:00
- **Updated**: 2021-01-23 16:14:08+00:00
- **Authors**: Neerav Karani, Ertunc Erdil, Krishna Chaitanya, Ender Konukoglu
- **Comment**: Published in Medical Image Analysis journal:
  https://doi.org/10.1016/j.media.2020.101907
- **Journal**: Medical Image Analysis, Volume 68, 2021, 101907, ISSN 1361-8415.
  http://www.sciencedirect.com/science/article/pii/S1361841520302711
- **Summary**: Convolutional Neural Networks (CNNs) work very well for supervised learning problems when the training dataset is representative of the variations expected to be encountered at test time. In medical image segmentation, this premise is violated when there is a mismatch between training and test images in terms of their acquisition details, such as the scanner model or the protocol. Remarkable performance degradation of CNNs in this scenario is well documented in the literature. To address this problem, we design the segmentation CNN as a concatenation of two sub-networks: a relatively shallow image normalization CNN, followed by a deep CNN that segments the normalized image. We train both these sub-networks using a training dataset, consisting of annotated images from a particular scanner and protocol setting. Now, at test time, we adapt the image normalization sub-network for \emph{each test image}, guided by an implicit prior on the predicted segmentation labels. We employ an independently trained denoising autoencoder (DAE) in order to model such an implicit prior on plausible anatomical segmentation labels. We validate the proposed idea on multi-center Magnetic Resonance imaging datasets of three anatomies: brain, heart and prostate. The proposed test-time adaptation consistently provides performance improvement, demonstrating the promise and generality of the approach. Being agnostic to the architecture of the deep CNN, the second sub-network, the proposed design can be utilized with any segmentation network to increase robustness to variations in imaging scanners and protocols. Our code is available at: \url{https://github.com/neerakara/test-time-adaptable-neural-networks-for-domain-generalization}.



### Orthogonal Over-Parameterized Training
- **Arxiv ID**: http://arxiv.org/abs/2004.04690v6
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.04690v6)
- **Published**: 2020-04-09 17:16:38+00:00
- **Updated**: 2021-06-05 00:31:21+00:00
- **Authors**: Weiyang Liu, Rongmei Lin, Zhen Liu, James M. Rehg, Liam Paull, Li Xiong, Le Song, Adrian Weller
- **Comment**: CVPR 2021 Oral (43 Pages, Substantial Update from v3, Typos Fixed
  from v5)
- **Journal**: None
- **Summary**: The inductive bias of a neural network is largely determined by the architecture and the training algorithm. To achieve good generalization, how to effectively train a neural network is of great importance. We propose a novel orthogonal over-parameterized training (OPT) framework that can provably minimize the hyperspherical energy which characterizes the diversity of neurons on a hypersphere. By maintaining the minimum hyperspherical energy during training, OPT can greatly improve the empirical generalization. Specifically, OPT fixes the randomly initialized weights of the neurons and learns an orthogonal transformation that applies to these neurons. We consider multiple ways to learn such an orthogonal transformation, including unrolling orthogonalization algorithms, applying orthogonal parameterization, and designing orthogonality-preserving gradient descent. For better scalability, we propose the stochastic OPT which performs orthogonal transformation stochastically for partial dimensions of neurons. Interestingly, OPT reveals that learning a proper coordinate system for neurons is crucial to generalization. We provide some insights on why OPT yields better generalization. Extensive experiments validate the superiority of OPT over the standard training.



### Rethinking the Trigger of Backdoor Attack
- **Arxiv ID**: http://arxiv.org/abs/2004.04692v3
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.04692v3)
- **Published**: 2020-04-09 17:19:37+00:00
- **Updated**: 2021-01-31 17:25:49+00:00
- **Authors**: Yiming Li, Tongqing Zhai, Baoyuan Wu, Yong Jiang, Zhifeng Li, Shutao Xia
- **Comment**: 18 pages
- **Journal**: None
- **Summary**: Backdoor attack intends to inject hidden backdoor into the deep neural networks (DNNs), such that the prediction of the infected model will be maliciously changed if the hidden backdoor is activated by the attacker-defined trigger, while it performs well on benign samples. Currently, most of existing backdoor attacks adopted the setting of \emph{static} trigger, $i.e.,$ triggers across the training and testing images follow the same appearance and are located in the same area. In this paper, we revisit this attack paradigm by analyzing the characteristics of the static trigger. We demonstrate that such an attack paradigm is vulnerable when the trigger in testing images is not consistent with the one used for training. We further explore how to utilize this property for backdoor defense, and discuss how to alleviate such vulnerability of existing attacks.



### Learning to Drive Off Road on Smooth Terrain in Unstructured Environments Using an On-Board Camera and Sparse Aerial Images
- **Arxiv ID**: http://arxiv.org/abs/2004.04697v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.04697v1)
- **Published**: 2020-04-09 17:27:09+00:00
- **Updated**: 2020-04-09 17:27:09+00:00
- **Authors**: Travis Manderson, Stefan Wapnick, David Meger, Gregory Dudek
- **Comment**: ICRA 2020. Video and project details can be found at
  http://www.cim.mcgill.ca/mrl/offroad_driving/
- **Journal**: None
- **Summary**: We present a method for learning to drive on smooth terrain while simultaneously avoiding collisions in challenging off-road and unstructured outdoor environments using only visual inputs. Our approach applies a hybrid model-based and model-free reinforcement learning method that is entirely self-supervised in labeling terrain roughness and collisions using on-board sensors. Notably, we provide both first-person and overhead aerial image inputs to our model. We find that the fusion of these complementary inputs improves planning foresight and makes the model robust to visual obstructions. Our results show the ability to generalize to environments with plentiful vegetation, various types of rock, and sandy trails. During evaluation, our policy attained 90% smooth terrain traversal and reduced the proportion of rough terrain driven over by 6.1 times compared to a model using only first-person imagery.



### Scalable Active Learning for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2004.04699v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.04699v1)
- **Published**: 2020-04-09 17:28:56+00:00
- **Updated**: 2020-04-09 17:28:56+00:00
- **Authors**: Elmar Haussmann, Michele Fenzi, Kashyap Chitta, Jan Ivanecky, Hanson Xu, Donna Roy, Akshita Mittel, Nicolas Koumchatzky, Clement Farabet, Jose M. Alvarez
- **Comment**: accepted at IEEE-IV2020
- **Journal**: None
- **Summary**: Deep Neural Networks trained in a fully supervised fashion are the dominant technology in perception-based autonomous driving systems. While collecting large amounts of unlabeled data is already a major undertaking, only a subset of it can be labeled by humans due to the effort needed for high-quality annotation. Therefore, finding the right data to label has become a key challenge. Active learning is a powerful technique to improve data efficiency for supervised learning methods, as it aims at selecting the smallest possible training set to reach a required performance. We have built a scalable production system for active learning in the domain of autonomous driving. In this paper, we describe the resulting high-level design, sketch some of the challenges and their solutions, present our current results at scale, and briefly describe the open problems and future directions.



### Instance-aware, Context-focused, and Memory-efficient Weakly Supervised Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2004.04725v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.04725v3)
- **Published**: 2020-04-09 17:57:09+00:00
- **Updated**: 2020-10-21 07:32:04+00:00
- **Authors**: Zhongzheng Ren, Zhiding Yu, Xiaodong Yang, Ming-Yu Liu, Yong Jae Lee, Alexander G. Schwing, Jan Kautz
- **Comment**: CVPR 2020
- **Journal**: None
- **Summary**: Weakly supervised learning has emerged as a compelling tool for object detection by reducing the need for strong supervision during training. However, major challenges remain: (1) differentiation of object instances can be ambiguous; (2) detectors tend to focus on discriminative parts rather than entire objects; (3) without ground truth, object proposals have to be redundant for high recalls, causing significant memory consumption. Addressing these challenges is difficult, as it often requires to eliminate uncertainties and trivial solutions. To target these issues we develop an instance-aware and context-focused unified framework. It employs an instance-aware self-training algorithm and a learnable Concrete DropBlock while devising a memory-efficient sequential batch back-propagation. Our proposed method achieves state-of-the-art results on COCO ($12.1\% ~AP$, $24.8\% ~AP_{50}$), VOC 2007 ($54.9\% ~AP$), and VOC 2012 ($52.1\% ~AP$), improving baselines by great margins. In addition, the proposed method is the first to benchmark ResNet based models and weakly supervised video object detection. Code, models, and more details will be made available at: https://github.com/NVlabs/wetectron.



### 3D Photography using Context-aware Layered Depth Inpainting
- **Arxiv ID**: http://arxiv.org/abs/2004.04727v3
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.04727v3)
- **Published**: 2020-04-09 17:59:06+00:00
- **Updated**: 2020-06-10 14:21:03+00:00
- **Authors**: Meng-Li Shih, Shih-Yang Su, Johannes Kopf, Jia-Bin Huang
- **Comment**: CVPR 2020. Project page:
  https://shihmengli.github.io/3D-Photo-Inpainting/ Code:
  https://github.com/vt-vl-lab/3d-photo-inpainting Demo:
  https://colab.research.google.com/drive/1706ToQrkIZshRSJSHvZ1RuCiM__YX3Bz
- **Journal**: None
- **Summary**: We propose a method for converting a single RGB-D input image into a 3D photo - a multi-layer representation for novel view synthesis that contains hallucinated color and depth structures in regions occluded in the original view. We use a Layered Depth Image with explicit pixel connectivity as underlying representation, and present a learning-based inpainting model that synthesizes new local color-and-depth content into the occluded region in a spatial context-aware manner. The resulting 3D photos can be efficiently rendered with motion parallax using standard graphics engines. We validate the effectiveness of our method on a wide range of challenging everyday scenes and show fewer artifacts compared with the state of the arts.



### X3D: Expanding Architectures for Efficient Video Recognition
- **Arxiv ID**: http://arxiv.org/abs/2004.04730v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.04730v1)
- **Published**: 2020-04-09 17:59:47+00:00
- **Updated**: 2020-04-09 17:59:47+00:00
- **Authors**: Christoph Feichtenhofer
- **Comment**: CVPR 2020 (Oral)
- **Journal**: None
- **Summary**: This paper presents X3D, a family of efficient video networks that progressively expand a tiny 2D image classification architecture along multiple network axes, in space, time, width and depth. Inspired by feature selection methods in machine learning, a simple stepwise network expansion approach is employed that expands a single axis in each step, such that good accuracy to complexity trade-off is achieved. To expand X3D to a specific target complexity, we perform progressive forward expansion followed by backward contraction. X3D achieves state-of-the-art performance while requiring 4.8x and 5.5x fewer multiply-adds and parameters for similar accuracy as previous work. Our most surprising finding is that networks with high spatiotemporal resolution can perform well, while being extremely light in terms of network width and parameters. We report competitive accuracy at unprecedented efficiency on video classification and detection benchmarks. Code will be available at: https://github.com/facebookresearch/SlowFast



### Early Disease Diagnosis for Rice Crop
- **Arxiv ID**: http://arxiv.org/abs/2004.04775v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.04775v1)
- **Published**: 2020-04-09 19:05:43+00:00
- **Updated**: 2020-04-09 19:05:43+00:00
- **Authors**: M. Hammad Masood, Habiba Saim, Murtaza Taj, Mian M. Awais
- **Comment**: Paper presented at the ICLR 2020 Workshop on Computer Vision for
  Agriculture (CV4A)
- **Journal**: None
- **Summary**: Many existing techniques provide automatic estimation of crop damage due to various diseases. However, early detection can prevent or reduce the extend of damage itself. The limited performance of existing techniques in early detection is lack of localized information. We instead propose a dataset with annotations for each diseased segment in each image. Unlike existing approaches, instead of classifying images into either healthy or diseased, we propose to provide localized classification for each segment of an images. Our method is based on Mask RCNN and provides location as well as extend of infected regions on the plant. Thus the extend of damage on the crop can be estimated. Our method has obtained overall 87.6% accuracy on the proposed dataset as compared to 58.4% obtained without incorporating localized information.



### D-SRGAN: DEM Super-Resolution with Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2004.04788v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.04788v2)
- **Published**: 2020-04-09 19:57:49+00:00
- **Updated**: 2020-04-16 17:42:46+00:00
- **Authors**: Bekir Z Demiray, Muhammed Sit, Ibrahim Demir
- **Comment**: 8 pages, 8 figures, 2 tables
- **Journal**: None
- **Summary**: LIDAR (light detection and ranging) is an optical remote-sensing technique that measures the distance between sensor and object, and the reflected energy from the object. Over the years, LIDAR data has been used as the primary source of Digital Elevation Models (DEMs). DEMs have been used in a variety of applications like road extraction, hydrological modeling, flood mapping, and surface analysis. A number of studies in flooding suggest the usage of high-resolution DEMs as inputs in the applications improve the overall reliability and accuracy. Despite the importance of high-resolution DEM, many areas in the United States and the world do not have access to high-resolution DEM due to technological limitations or the cost of the data collection. With recent development in Graphical Processing Units (GPU) and novel algorithms, deep learning techniques have become attractive to researchers for their performance in learning features from high-resolution datasets. Numerous new methods have been proposed such as Generative Adversarial Networks (GANs) to create intelligent models that correct and augment large-scale datasets. In this paper, a GAN based model is developed and evaluated, inspired by single image super-resolution methods, to increase the spatial resolution of a given DEM dataset up to 4 times without additional information related to data.



### Exemplar VAE: Linking Generative Models, Nearest Neighbor Retrieval, and Data Augmentation
- **Arxiv ID**: http://arxiv.org/abs/2004.04795v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.04795v3)
- **Published**: 2020-04-09 20:21:45+00:00
- **Updated**: 2020-11-24 18:51:11+00:00
- **Authors**: Sajad Norouzi, David J. Fleet, Mohammad Norouzi
- **Comment**: NeurIPS 2020
- **Journal**: None
- **Summary**: We introduce Exemplar VAEs, a family of generative models that bridge the gap between parametric and non-parametric, exemplar based generative models. Exemplar VAE is a variant of VAE with a non-parametric prior in the latent space based on a Parzen window estimator. To sample from it, one first draws a random exemplar from a training set, then stochastically transforms that exemplar into a latent code and a new observation. We propose retrieval augmented training (RAT) as a way to speed up Exemplar VAE training by using approximate nearest neighbor search in the latent space to define a lower bound on log marginal likelihood. To enhance generalization, model parameters are learned using exemplar leave-one-out and subsampling. Experiments demonstrate the effectiveness of Exemplar VAEs on density estimation and representation learning. Importantly, generative data augmentation using Exemplar VAEs on permutation invariant MNIST and Fashion MNIST reduces classification error from 1.17% to 0.69% and from 8.56% to 8.16%.



### Models Genesis
- **Arxiv ID**: http://arxiv.org/abs/2004.07882v4
- **DOI**: 10.1016/j.media.2020.101840
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.07882v4)
- **Published**: 2020-04-09 20:37:33+00:00
- **Updated**: 2020-12-16 19:58:08+00:00
- **Authors**: Zongwei Zhou, Vatsal Sodha, Jiaxuan Pang, Michael B. Gotway, Jianming Liang
- **Comment**: Journal version of arXiv:1908.06912, accepted by Medical Image
  Analysis
- **Journal**: None
- **Summary**: Transfer learning from natural images to medical images has been established as one of the most practical paradigms in deep learning for medical image analysis. To fit this paradigm, however, 3D imaging tasks in the most prominent imaging modalities (e.g., CT and MRI) have to be reformulated and solved in 2D, losing rich 3D anatomical information, thereby inevitably compromising its performance. To overcome this limitation, we have built a set of models, called Generic Autodidactic Models, nicknamed Models Genesis, because they are created ex nihilo (with no manual labeling), self-taught (learnt by self-supervision), and generic (served as source models for generating application-specific target models). Our extensive experiments demonstrate that our Models Genesis significantly outperform learning from scratch and existing pre-trained 3D models in all five target 3D applications covering both segmentation and classification. More importantly, learning a model from scratch simply in 3D may not necessarily yield performance better than transfer learning from ImageNet in 2D, but our Models Genesis consistently top any 2D/2.5D approaches including fine-tuning the models pre-trained from ImageNet as well as fine-tuning the 2D versions of our Models Genesis, confirming the importance of 3D anatomical information and significance of Models Genesis for 3D medical imaging. This performance is attributed to our unified self-supervised learning framework, built on a simple yet powerful observation: the sophisticated and recurrent anatomy in medical images can serve as strong yet free supervision signals for deep models to learn common anatomical representation automatically via self-supervision. As open science, all codes and pre-trained Models Genesis are available at https://github.com/MrGiovanni/ModelsGenesis.



### 6D Camera Relocalization in Ambiguous Scenes via Continuous Multimodal Inference
- **Arxiv ID**: http://arxiv.org/abs/2004.04807v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.04807v2)
- **Published**: 2020-04-09 20:55:06+00:00
- **Updated**: 2020-07-16 07:06:27+00:00
- **Authors**: Mai Bui, Tolga Birdal, Haowen Deng, Shadi Albarqouni, Leonidas Guibas, Slobodan Ilic, Nassir Navab
- **Comment**: Accepted for publication at ECCV 2020. Project page under
  https://multimodal3dvision.github.io
- **Journal**: None
- **Summary**: We present a multimodal camera relocalization framework that captures ambiguities and uncertainties with continuous mixture models defined on the manifold of camera poses. In highly ambiguous environments, which can easily arise due to symmetries and repetitive structures in the scene, computing one plausible solution (what most state-of-the-art methods currently regress) may not be sufficient. Instead we predict multiple camera pose hypotheses as well as the respective uncertainty for each prediction. Towards this aim, we use Bingham distributions, to model the orientation of the camera pose, and a multivariate Gaussian to model the position, with an end-to-end deep neural network. By incorporating a Winner-Takes-All training scheme, we finally obtain a mixture model that is well suited for explaining ambiguities in the scene, yet does not suffer from mode collapse, a common problem with mixture density networks. We introduce a new dataset specifically designed to foster camera localization research in ambiguous environments and exhaustively evaluate our method on synthetic as well as real data on both ambiguous scenes and on non-ambiguous benchmark datasets. We plan to release our code and dataset under $\href{https://multimodal3dvision.github.io}{multimodal3dvision.github.io}$.



### Analysis on DeepLabV3+ Performance for Automatic Steel Defects Detection
- **Arxiv ID**: http://arxiv.org/abs/2004.04822v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.04822v2)
- **Published**: 2020-04-09 21:17:01+00:00
- **Updated**: 2020-04-15 03:50:16+00:00
- **Authors**: Zheng Nie, Jiachen Xu, Shengchang Zhang
- **Comment**: 10 pages, 30 figures
- **Journal**: None
- **Summary**: Our works experimented DeepLabV3+ with different backbones on a large volume of steel images aiming to automatically detect different types of steel defects. Our methods applied random weighted augmentation to balance different defects types in the training set. And then applied DeeplabV3+ model three different backbones, ResNet, DenseNet and EfficientNet, on segmenting defection regions on the steel images. Based on experiments, we found that applying ResNet101 or EfficientNet as backbones could reach the best IoU scores on the test set, which is around 0.57, comparing with 0.325 for using DenseNet. Also, DeepLabV3+ model with ResNet101 as backbone has the fewest training time.



### Spatial Priming for Detecting Human-Object Interactions
- **Arxiv ID**: http://arxiv.org/abs/2004.04851v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2004.04851v1)
- **Published**: 2020-04-09 23:20:30+00:00
- **Updated**: 2020-04-09 23:20:30+00:00
- **Authors**: Ankan Bansal, Sai Saketh Rambhatla, Abhinav Shrivastava, Rama Chellappa
- **Comment**: None
- **Journal**: None
- **Summary**: The relative spatial layout of a human and an object is an important cue for determining how they interact. However, until now, spatial layout has been used just as side-information for detecting human-object interactions (HOIs). In this paper, we present a method for exploiting this spatial layout information for detecting HOIs in images. The proposed method consists of a layout module which primes a visual module to predict the type of interaction between a human and an object. The visual and layout modules share information through lateral connections at several stages. The model uses predictions from the layout module as a prior to the visual module and the prediction from the visual module is given as the final output. It also incorporates semantic information about the object using word2vec vectors. The proposed model reaches an mAP of 24.79% for HICO-Det dataset which is about 2.8% absolute points higher than the current state-of-the-art.



