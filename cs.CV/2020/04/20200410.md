# Arxiv Papers in cs.CV on 2020-04-10
### Decentralized Differentially Private Segmentation with PATE
- **Arxiv ID**: http://arxiv.org/abs/2004.06567v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.06567v1)
- **Published**: 2020-04-10 00:05:48+00:00
- **Updated**: 2020-04-10 00:05:48+00:00
- **Authors**: Dominik Fay, Jens Sjölund, Tobias J. Oechtering
- **Comment**: Under review for MICCAI 2020
- **Journal**: None
- **Summary**: When it comes to preserving privacy in medical machine learning, two important considerations are (1) keeping data local to the institution and (2) avoiding inference of sensitive information from the trained model. These are often addressed using federated learning and differential privacy, respectively. However, the commonly used Federated Averaging algorithm requires a high degree of synchronization between participating institutions. For this reason, we turn our attention to Private Aggregation of Teacher Ensembles (PATE), where all local models can be trained independently without inter-institutional communication. The purpose of this paper is thus to explore how PATE -- originally designed for classification -- can best be adapted for semantic segmentation. To this end, we build low-dimensional representations of segmentation masks which the student can obtain through low-sensitivity queries to the private aggregator. On the Brain Tumor Segmentation (BraTS 2019) dataset, an Autoencoder-based PATE variant achieves a higher Dice coefficient for the same privacy guarantee than prior work based on noisy Federated Averaging.



### Latent regularization for feature selection using kernel methods in tumor classification
- **Arxiv ID**: http://arxiv.org/abs/2004.04866v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV, q-bio.GN, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.04866v1)
- **Published**: 2020-04-10 00:46:02+00:00
- **Updated**: 2020-04-10 00:46:02+00:00
- **Authors**: Martin Palazzo, Patricio Yankilevich, Pierre Beauseroy
- **Comment**: None
- **Journal**: None
- **Summary**: The transcriptomics of cancer tumors are characterized with tens of thousands of gene expression features. Patient prognosis or tumor stage can be assessed by machine learning techniques like supervised classification tasks given a gene expression profile. Feature selection is a useful approach to select the key genes which helps to classify tumors. In this work we propose a feature selection method based on Multiple Kernel Learning that results in a reduced subset of genes and a custom kernel that improves the classification performance when used in support vector classification. During the feature selection process this method performs a novel latent regularisation by relaxing the supervised target problem by introducing unsupervised structure obtained from the latent space learned by a non linear dimensionality reduction model. An improvement of the generalization capacity is obtained and assessed by the tumor classification performance on new unseen test samples when the classifier is trained with the features selected by the proposed method in comparison with other supervised feature selection approaches.



### MRQy: An Open-Source Tool for Quality Control of MR Imaging Data
- **Arxiv ID**: http://arxiv.org/abs/2004.04871v3
- **DOI**: 10.1002/mp.14593
- **Categories**: **eess.IV**, cs.CV, cs.LG, q-bio.QM, stat.AP
- **Links**: [PDF](http://arxiv.org/pdf/2004.04871v3)
- **Published**: 2020-04-10 01:30:51+00:00
- **Updated**: 2020-08-17 14:04:25+00:00
- **Authors**: Amir Reza Sadri, Andrew Janowczyk, Ren Zou, Ruchika Verma, Niha Beig, Jacob Antunes, Anant Madabhushi, Pallavi Tiwari, Satish E. Viswanath
- **Comment**: 28 pages, 7 figures. Submitted to Medical Physics
- **Journal**: None
- **Summary**: We sought to develop a quantitative tool to quickly determine relative differences in MRI volumes both within and between large MR imaging cohorts (such as available in The Cancer Imaging Archive (TCIA)), in order to help determine the generalizability of radiomics and machine learning schemes to unseen datasets. The tool is intended to help quantify presence of (a) site- or scanner-specific variations in image resolution, field-of-view, or image contrast, or (b) imaging artifacts such as noise, motion, inhomogeneity, ringing, or aliasing; which can adversely affect relative image quality between data cohorts. We present MRQy, a new open-source quality control tool to (a) interrogate MRI cohorts for site- or equipment-based differences, and (b) quantify the impact of MRI artifacts on relative image quality; to help determine how to correct for these variations prior to model development. MRQy extracts a series of quality measures (e.g. noise ratios, variation metrics, entropy and energy criteria) and MR image metadata (e.g. voxel resolution, image dimensions) for subsequent interrogation via a specialized HTML5 based front-end designed for real-time filtering and trend visualization. MRQy was used to evaluate (a) n=133 brain MRIs from TCIA (7 sites), and (b) n=104 rectal MRIs (3 local sites). MRQy measures revealed significant site-specific variations in both cohorts, indicating potential batch effects. Marked differences in specific MRQy measures were also able to identify outlier MRI datasets that needed to be corrected for common MR imaging artifacts. MRQy is designed to be a standalone, unsupervised tool that can be efficiently run on a standard desktop computer. It has been made freely accessible at \url{http://github.com/ccipd/MRQy} for wider community use and feedback.



### Socioeconomic correlations of urban patterns inferred from aerial images: interpreting activation maps of Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2004.04907v1
- **DOI**: None
- **Categories**: **physics.soc-ph**, cs.CV, cs.CY, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.04907v1)
- **Published**: 2020-04-10 04:57:20+00:00
- **Updated**: 2020-04-10 04:57:20+00:00
- **Authors**: Jacob Levy Abitbol, Márton Karsai
- **Comment**: 19 pages, 17 figures
- **Journal**: None
- **Summary**: Urbanisation is a great challenge for modern societies, promising better access to economic opportunities while widening socioeconomic inequalities. Accurately tracking how this process unfolds has been challenging for traditional data collection methods, while remote sensing information offers an alternative to gather a more complete view on these societal changes. By feeding a neural network with satellite images one may recover the socioeconomic information associated to that area, however these models lack to explain how visual features contained in a sample, trigger a given prediction. Here we close this gap by predicting socioeconomic status across France from aerial images and interpreting class activation mappings in terms of urban topology. We show that the model disregards the spatial correlations existing between urban class and socioeconomic status to derive its predictions. These results pave the way to build interpretable models, which may help to better track and understand urbanisation and its consequences.



### Analyze and Development System with Multiple Biometric Identification
- **Arxiv ID**: http://arxiv.org/abs/2004.04911v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.04911v1)
- **Published**: 2020-04-10 05:08:54+00:00
- **Updated**: 2020-04-10 05:08:54+00:00
- **Authors**: Sher Dadakhanov
- **Comment**: Multiple Biometric Identification
- **Journal**: None
- **Summary**: Cause of a rapid increase in technological development, increasing identity theft, consumer fraud, the threat to personal data is also increasing every day. Methods developed earlier to ensure personal the information from the thefts was not effective and safe. Biometrics were introduced when it was needed technology for more efficient security of personal information. Old-fashioned traditional approaches like Personal identification number( PIN), passwords, keys, login ID can be forgotten, stolen or lost. In biometric authentication system, user may not remember any passwords or carry any keys. As people they recognize each other by the physical appearance and behavioral characteristics that biometric systems use physical characteristics, such as fingerprints, facial recognition, voice recognition, in order to distinguish between the actual user and scammer. In order to increase safety in 2005, biometric identification methods were developed government and business sectors, but today it has reached almost all private sectors as Banking, Finance, home security and protection, healthcare, business security and security etc. Since biometric samples and templates of a biometric system having one biometric character to detect and the user can be replaced and duplicated, the new idea of merging multiple biometric identification technologies has so-called multimodal biometric recognition systems have been introduced that use two or more biometric data characteristics of the individual that can be identified as a real user or not.



### Person Re-Identification via Active Hard Sample Mining
- **Arxiv ID**: http://arxiv.org/abs/2004.04912v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.04912v2)
- **Published**: 2020-04-10 05:11:11+00:00
- **Updated**: 2020-04-14 06:49:29+00:00
- **Authors**: Xin Xu, Lei Liu, Weifeng Liu, Meng Wang, Ruimin Hu
- **Comment**: None
- **Journal**: None
- **Summary**: Annotating a large-scale image dataset is very tedious, yet necessary for training person re-identification models. To alleviate such a problem, we present an active hard sample mining framework via training an effective re-ID model with the least labeling efforts. Considering that hard samples can provide informative patterns, we first formulate an uncertainty estimation to actively select hard samples to iteratively train a re-ID model from scratch. Then, intra-diversity estimation is designed to reduce the redundant hard samples by maximizing their diversity. Moreover, we propose a computer-assisted identity recommendation module embedded in the active hard sample mining framework to help human annotators to rapidly and accurately label the selected samples. Extensive experiments were carried out to demonstrate the effectiveness of our method on several public datasets. Experimental results indicate that our method can reduce 57%, 63%, and 49% annotation efforts on the Market1501, MSMT17, and CUHK03, respectively, while maximizing the performance of the re-ID model.



### Deep Residual Correction Network for Partial Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2004.04914v1
- **DOI**: 10.1109/TPAMI.2020.2964173
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.04914v1)
- **Published**: 2020-04-10 06:07:16+00:00
- **Updated**: 2020-04-10 06:07:16+00:00
- **Authors**: Shuang Li, Chi Harold Liu, Qiuxia Lin, Qi Wen, Limin Su, Gao Huang, Zhengming Ding
- **Comment**: Accepted by T-PAMI, 2020
- **Journal**: None
- **Summary**: Deep domain adaptation methods have achieved appealing performance by learning transferable representations from a well-labeled source domain to a different but related unlabeled target domain. Most existing works assume source and target data share the identical label space, which is often difficult to be satisfied in many real-world applications. With the emergence of big data, there is a more practical scenario called partial domain adaptation, where we are always accessible to a more large-scale source domain while working on a relative small-scale target domain. In this case, the conventional domain adaptation assumption should be relaxed, and the target label space tends to be a subset of the source label space. Intuitively, reinforcing the positive effects of the most relevant source subclasses and reducing the negative impacts of irrelevant source subclasses are of vital importance to address partial domain adaptation challenge. This paper proposes an efficiently-implemented Deep Residual Correction Network (DRCN) by plugging one residual block into the source network along with the task-specific feature layer, which effectively enhances the adaptation from source to target and explicitly weakens the influence from the irrelevant source classes. Specifically, the plugged residual block, which consists of several fully-connected layers, could deepen basic network and boost its feature representation capability correspondingly. Moreover, we design a weighted class-wise domain alignment loss to couple two domains by matching the feature distributions of shared classes between source and target. Comprehensive experiments on partial, traditional and fine-grained cross-domain visual recognition demonstrate that DRCN is superior to the competitive deep domain adaptation approaches.



### Multimodal Categorization of Crisis Events in Social Media
- **Arxiv ID**: http://arxiv.org/abs/2004.04917v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CL, cs.CV, I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/2004.04917v1)
- **Published**: 2020-04-10 06:31:30+00:00
- **Updated**: 2020-04-10 06:31:30+00:00
- **Authors**: Mahdi Abavisani, Liwei Wu, Shengli Hu, Joel Tetreault, Alejandro Jaimes
- **Comment**: Conference on Computer Vision and Pattern Recognition (CVPR 2020)
- **Journal**: Conference on Computer Vision and Pattern Recognition (CVPR 2020)
- **Summary**: Recent developments in image classification and natural language processing, coupled with the rapid growth in social media usage, have enabled fundamental advances in detecting breaking events around the world in real-time. Emergency response is one such area that stands to gain from these advances. By processing billions of texts and images a minute, events can be automatically detected to enable emergency response workers to better assess rapidly evolving situations and deploy resources accordingly. To date, most event detection techniques in this area have focused on image-only or text-only approaches, limiting detection performance and impacting the quality of information delivered to crisis response teams. In this paper, we present a new multimodal fusion method that leverages both images and texts as input. In particular, we introduce a cross-attention module that can filter uninformative and misleading components from weak modalities on a sample by sample basis. In addition, we employ a multimodal graph-based approach to stochastically transition between embeddings of different multimodal pairs during training to better regularize the learning process as well as dealing with limited training data by constructing new matched pairs from different samples. We show that our method outperforms the unimodal approaches and strong multimodal baselines by a large margin on three crisis-related tasks.



### Phase Consistent Ecological Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2004.04923v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.04923v1)
- **Published**: 2020-04-10 06:58:03+00:00
- **Updated**: 2020-04-10 06:58:03+00:00
- **Authors**: Yanchao Yang, Dong Lao, Ganesh Sundaramoorthi, Stefano Soatto
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce two criteria to regularize the optimization involved in learning a classifier in a domain where no annotated data are available, leveraging annotated data in a different domain, a problem known as unsupervised domain adaptation. We focus on the task of semantic segmentation, where annotated synthetic data are aplenty, but annotating real data is laborious. The first criterion, inspired by visual psychophysics, is that the map between the two image domains be phase-preserving. This restricts the set of possible learned maps, while enabling enough flexibility to transfer semantic information. The second criterion aims to leverage ecological statistics, or regularities in the scene which are manifest in any image of it, regardless of the characteristics of the illuminant or the imaging sensor. It is implemented using a deep neural network that scores the likelihood of each possible segmentation given a single un-annotated image. Incorporating these two priors in a standard domain adaptation framework improves performance across the board in the most common unsupervised domain adaptation benchmarks for semantic segmentation.



### Real-world Person Re-Identification via Degradation Invariance Learning
- **Arxiv ID**: http://arxiv.org/abs/2004.04933v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.04933v1)
- **Published**: 2020-04-10 07:58:50+00:00
- **Updated**: 2020-04-10 07:58:50+00:00
- **Authors**: Yukun Huang, Zheng-Jun Zha, Xueyang Fu, Richang Hong, Liang Li
- **Comment**: To appear in CVPR2020
- **Journal**: None
- **Summary**: Person re-identification (Re-ID) in real-world scenarios usually suffers from various degradation factors, e.g., low-resolution, weak illumination, blurring and adverse weather. On the one hand, these degradations lead to severe discriminative information loss, which significantly obstructs identity representation learning; on the other hand, the feature mismatch problem caused by low-level visual variations greatly reduces retrieval performance. An intuitive solution to this problem is to utilize low-level image restoration methods to improve the image quality. However, existing restoration methods cannot directly serve to real-world Re-ID due to various limitations, e.g., the requirements of reference samples, domain gap between synthesis and reality, and incompatibility between low-level and high-level methods. In this paper, to solve the above problem, we propose a degradation invariance learning framework for real-world person Re-ID. By introducing a self-supervised disentangled representation learning strategy, our method is able to simultaneously extract identity-related robust features and remove real-world degradations without extra supervision. We use low-resolution images as the main demonstration, and experiments show that our approach is able to achieve state-of-the-art performance on several Re-ID benchmarks. In addition, our framework can be easily extended to other real-world degradation factors, such as weak illumination, with only a few modifications.



### ContourNet: Taking a Further Step toward Accurate Arbitrary-shaped Scene Text Detection
- **Arxiv ID**: http://arxiv.org/abs/2004.04940v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.04940v1)
- **Published**: 2020-04-10 08:15:23+00:00
- **Updated**: 2020-04-10 08:15:23+00:00
- **Authors**: Yuxin Wang, Hongtao Xie, Zhengjun Zha, Mengting Xing, Zilong Fu, Yongdong Zhang
- **Comment**: Accepted by CVPR2020
- **Journal**: None
- **Summary**: Scene text detection has witnessed rapid development in recent years. However, there still exists two main challenges: 1) many methods suffer from false positives in their text representations; 2) the large scale variance of scene texts makes it hard for network to learn samples. In this paper, we propose the ContourNet, which effectively handles these two problems taking a further step toward accurate arbitrary-shaped text detection. At first, a scale-insensitive Adaptive Region Proposal Network (Adaptive-RPN) is proposed to generate text proposals by only focusing on the Intersection over Union (IoU) values between predicted and ground-truth bounding boxes. Then a novel Local Orthogonal Texture-aware Module (LOTM) models the local texture information of proposal features in two orthogonal directions and represents text region with a set of contour points. Considering that the strong unidirectional or weakly orthogonal activation is usually caused by the monotonous texture characteristic of false-positive patterns (e.g. streaks.), our method effectively suppresses these false positives by only outputting predictions with high response value in both orthogonal directions. This gives more accurate description of text regions. Extensive experiments on three challenging datasets (Total-Text, CTW1500 and ICDAR2015) verify that our method achieves the state-of-the-art performance. Code is available at https://github.com/wangyuxin87/ContourNet.



### State-Relabeling Adversarial Active Learning
- **Arxiv ID**: http://arxiv.org/abs/2004.04943v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, I.4.8; I.2.6
- **Links**: [PDF](http://arxiv.org/pdf/2004.04943v1)
- **Published**: 2020-04-10 08:23:59+00:00
- **Updated**: 2020-04-10 08:23:59+00:00
- **Authors**: Beichen Zhang, Liang Li, Shijie Yang, Shuhui Wang, Zheng-Jun Zha, Qingming Huang
- **Comment**: Accepted as Oral at CVPR 2020
- **Journal**: None
- **Summary**: Active learning is to design label-efficient algorithms by sampling the most representative samples to be labeled by an oracle. In this paper, we propose a state relabeling adversarial active learning model (SRAAL), that leverages both the annotation and the labeled/unlabeled state information for deriving the most informative unlabeled samples. The SRAAL consists of a representation generator and a state discriminator. The generator uses the complementary annotation information with traditional reconstruction information to generate the unified representation of samples, which embeds the semantic into the whole data representation. Then, we design an online uncertainty indicator in the discriminator, which endues unlabeled samples with different importance. As a result, we can select the most informative samples based on the discriminator's predicted state. We also design an algorithm to initialize the labeled pool, which makes subsequent sampling more efficient. The experiments conducted on various datasets show that our model outperforms the previous state-of-art active learning methods and our initially sampling algorithm achieves better performance.



### Learning to Visually Navigate in Photorealistic Environments Without any Supervision
- **Arxiv ID**: http://arxiv.org/abs/2004.04954v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2004.04954v1)
- **Published**: 2020-04-10 08:59:32+00:00
- **Updated**: 2020-04-10 08:59:32+00:00
- **Authors**: Lina Mezghani, Sainbayar Sukhbaatar, Arthur Szlam, Armand Joulin, Piotr Bojanowski
- **Comment**: None
- **Journal**: None
- **Summary**: Learning to navigate in a realistic setting where an agent must rely solely on visual inputs is a challenging task, in part because the lack of position information makes it difficult to provide supervision during training. In this paper, we introduce a novel approach for learning to navigate from image inputs without external supervision or reward. Our approach consists of three stages: learning a good representation of first-person views, then learning to explore using memory, and finally learning to navigate by setting its own goals. The model is trained with intrinsic rewards only so that it can be applied to any environment with image observations. We show the benefits of our approach by training an agent to navigate challenging photo-realistic environments from the Gibson dataset with RGB inputs only.



### Boosting Semantic Human Matting with Coarse Annotations
- **Arxiv ID**: http://arxiv.org/abs/2004.04955v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.04955v1)
- **Published**: 2020-04-10 09:11:02+00:00
- **Updated**: 2020-04-10 09:11:02+00:00
- **Authors**: Jinlin Liu, Yuan Yao, Wendi Hou, Miaomiao Cui, Xuansong Xie, Changshui Zhang, Xian-sheng Hua
- **Comment**: None
- **Journal**: None
- **Summary**: Semantic human matting aims to estimate the per-pixel opacity of the foreground human regions. It is quite challenging and usually requires user interactive trimaps and plenty of high quality annotated data. Annotating such kind of data is labor intensive and requires great skills beyond normal users, especially considering the very detailed hair part of humans. In contrast, coarse annotated human dataset is much easier to acquire and collect from the public dataset. In this paper, we propose to use coarse annotated data coupled with fine annotated data to boost end-to-end semantic human matting without trimaps as extra input. Specifically, we train a mask prediction network to estimate the coarse semantic mask using the hybrid data, and then propose a quality unification network to unify the quality of the previous coarse mask outputs. A matting refinement network takes in the unified mask and the input image to predict the final alpha matte. The collected coarse annotated dataset enriches our dataset significantly, allows generating high quality alpha matte for real images. Experimental results show that the proposed method performs comparably against state-of-the-art methods. Moreover, the proposed method can be used for refining coarse annotated public dataset, as well as semantic segmentation methods, which reduces the cost of annotating high quality human data to a great extent.



### 3D IoU-Net: IoU Guided 3D Object Detector for Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2004.04962v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.04962v1)
- **Published**: 2020-04-10 09:24:29+00:00
- **Updated**: 2020-04-10 09:24:29+00:00
- **Authors**: Jiale Li, Shujie Luo, Ziqi Zhu, Hang Dai, Andrey S. Krylov, Yong Ding, Ling Shao
- **Comment**: 11 pages, 9 figures
- **Journal**: None
- **Summary**: Most existing point cloud based 3D object detectors focus on the tasks of classification and box regression. However, another bottleneck in this area is achieving an accurate detection confidence for the Non-Maximum Suppression (NMS) post-processing. In this paper, we add a 3D IoU prediction branch to the regular classification and regression branches. The predicted IoU is used as the detection confidence for NMS. In order to obtain a more accurate IoU prediction, we propose a 3D IoU-Net with IoU sensitive feature learning and an IoU alignment operation. To obtain a perspective-invariant prediction head, we propose an Attentive Corner Aggregation (ACA) module by aggregating a local point cloud feature from each perspective of eight corners and adaptively weighting the contribution of each perspective with different attentions. We propose a Corner Geometry Encoding (CGE) module for geometry information embedding. To the best of our knowledge, this is the first time geometric embedding information has been introduced in proposal feature learning. These two feature parts are then adaptively fused by a multi-layer perceptron (MLP) network as our IoU sensitive feature. The IoU alignment operation is introduced to resolve the mismatching between the bounding box regression head and IoU prediction, thereby further enhancing the accuracy of IoU prediction. The experimental results on the KITTI car detection benchmark show that 3D IoU-Net with IoU perception achieves state-of-the-art performance.



### Rephrasing visual questions by specifying the entropy of the answer distribution
- **Arxiv ID**: http://arxiv.org/abs/2004.04963v1
- **DOI**: 10.1587/transinf.2020EDP7089
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.04963v1)
- **Published**: 2020-04-10 09:32:37+00:00
- **Updated**: 2020-04-10 09:32:37+00:00
- **Authors**: Kento Terao, Toru Tamaki, Bisser Raytchev, Kazufumi Kaneda, Shun'ichi Satoh
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Visual question answering (VQA) is a task of answering a visual question that is a pair of question and image. Some visual questions are ambiguous and some are clear, and it may be appropriate to change the ambiguity of questions from situation to situation. However, this issue has not been addressed by any prior work. We propose a novel task, rephrasing the questions by controlling the ambiguity of the questions. The ambiguity of a visual question is defined by the use of the entropy of the answer distribution predicted by a VQA model. The proposed model rephrases a source question given with an image so that the rephrased question has the ambiguity (or entropy) specified by users. We propose two learning strategies to train the proposed model with the VQA v2 dataset, which has no ambiguity information. We demonstrate the advantage of our approach that can control the ambiguity of the rephrased questions, and an interesting observation that it is harder to increase than to reduce ambiguity.



### Would Mega-scale Datasets Further Enhance Spatiotemporal 3D CNNs?
- **Arxiv ID**: http://arxiv.org/abs/2004.04968v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.04968v1)
- **Published**: 2020-04-10 09:44:19+00:00
- **Updated**: 2020-04-10 09:44:19+00:00
- **Authors**: Hirokatsu Kataoka, Tenga Wakamiya, Kensho Hara, Yutaka Satoh
- **Comment**: Codes and pre-trained models are publicly available:
  https://github.com/kenshohara/3D-ResNets-PyTorch
- **Journal**: None
- **Summary**: How can we collect and use a video dataset to further improve spatiotemporal 3D Convolutional Neural Networks (3D CNNs)? In order to positively answer this open question in video recognition, we have conducted an exploration study using a couple of large-scale video datasets and 3D CNNs. In the early era of deep neural networks, 2D CNNs have been better than 3D CNNs in the context of video recognition. Recent studies revealed that 3D CNNs can outperform 2D CNNs trained on a large-scale video dataset. However, we heavily rely on architecture exploration instead of dataset consideration. Therefore, in the present paper, we conduct exploration study in order to improve spatiotemporal 3D CNNs as follows: (i) Recently proposed large-scale video datasets help improve spatiotemporal 3D CNNs in terms of video classification accuracy. We reveal that a carefully annotated dataset (e.g., Kinetics-700) effectively pre-trains a video representation for a video classification task. (ii) We confirm the relationships between #category/#instance and video classification accuracy. The results show that #category should initially be fixed, and then #instance is increased on a video dataset in case of dataset construction. (iii) In order to practically extend a video dataset, we simply concatenate publicly available datasets, such as Kinetics-700 and Moments in Time (MiT) datasets. Compared with Kinetics-700 pre-training, we further enhance spatiotemporal 3D CNNs with the merged dataset, e.g., +0.9, +3.4, and +1.1 on UCF-101, HMDB-51, and ActivityNet datasets, respectively, in terms of fine-tuning. (iv) In terms of recognition architecture, the Kinetics-700 and merged dataset pre-trained models increase the recognition performance to 200 layers with the Residual Network (ResNet), while the Kinetics-400 pre-trained model cannot successfully optimize the 200-layer architecture.



### SESAME: Semantic Editing of Scenes by Adding, Manipulating or Erasing Objects
- **Arxiv ID**: http://arxiv.org/abs/2004.04977v2
- **DOI**: 10.1007/978-3-030-58542-6_24
- **Categories**: **cs.CV**, cs.GR, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.04977v2)
- **Published**: 2020-04-10 10:19:19+00:00
- **Updated**: 2020-10-08 14:52:01+00:00
- **Authors**: Evangelos Ntavelis, Andrés Romero, Iason Kastanis, Luc Van Gool, Radu Timofte
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advances in image generation gave rise to powerful tools for semantic image editing. However, existing approaches can either operate on a single image or require an abundance of additional information. They are not capable of handling the complete set of editing operations, that is addition, manipulation or removal of semantic concepts. To address these limitations, we propose SESAME, a novel generator-discriminator pair for Semantic Editing of Scenes by Adding, Manipulating or Erasing objects. In our setup, the user provides the semantic labels of the areas to be edited and the generator synthesizes the corresponding pixels. In contrast to previous methods that employ a discriminator that trivially concatenates semantics and image as an input, the SESAME discriminator is composed of two input streams that independently process the image and its semantics, using the latter to manipulate the results of the former. We evaluate our model on a diverse set of datasets and report state-of-the-art performance on two tasks: (a) image manipulation and (b) image generation conditioned on semantic labels.



### Co-Saliency Spatio-Temporal Interaction Network for Person Re-Identification in Videos
- **Arxiv ID**: http://arxiv.org/abs/2004.04979v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.04979v2)
- **Published**: 2020-04-10 10:23:58+00:00
- **Updated**: 2020-05-11 10:04:19+00:00
- **Authors**: Jiawei Liu, Zheng-Jun Zha, Xierong Zhu, Na Jiang
- **Comment**: None
- **Journal**: None
- **Summary**: Person re-identification aims at identifying a certain pedestrian across non-overlapping camera networks. Video-based re-identification approaches have gained significant attention recently, expanding image-based approaches by learning features from multiple frames. In this work, we propose a novel Co-Saliency Spatio-Temporal Interaction Network (CSTNet) for person re-identification in videos. It captures the common salient foreground regions among video frames and explores the spatial-temporal long-range context interdependency from such regions, towards learning discriminative pedestrian representation. Specifically, multiple co-saliency learning modules within CSTNet are designed to utilize the correlated information across video frames to extract the salient features from the task-relevant regions and suppress background interference. Moreover, multiple spatialtemporal interaction modules within CSTNet are proposed, which exploit the spatial and temporal long-range context interdependencies on such features and spatial-temporal information correlation, to enhance feature representation. Extensive experiments on two benchmarks have demonstrated the effectiveness of the proposed method.



### Spatiotemporal Fusion in 3D CNNs: A Probabilistic View
- **Arxiv ID**: http://arxiv.org/abs/2004.04981v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.04981v1)
- **Published**: 2020-04-10 10:40:35+00:00
- **Updated**: 2020-04-10 10:40:35+00:00
- **Authors**: Yizhou Zhou, Xiaoyan Sun, Chong Luo, Zheng-Jun Zha, Wenjun Zeng
- **Comment**: To be published in IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR), 2020
- **Journal**: None
- **Summary**: Despite the success in still image recognition, deep neural networks for spatiotemporal signal tasks (such as human action recognition in videos) still suffers from low efficacy and inefficiency over the past years. Recently, human experts have put more efforts into analyzing the importance of different components in 3D convolutional neural networks (3D CNNs) to design more powerful spatiotemporal learning backbones. Among many others, spatiotemporal fusion is one of the essentials. It controls how spatial and temporal signals are extracted at each layer during inference. Previous attempts usually start by ad-hoc designs that empirically combine certain convolutions and then draw conclusions based on the performance obtained by training the corresponding networks. These methods only support network-level analysis on limited number of fusion strategies. In this paper, we propose to convert the spatiotemporal fusion strategies into a probability space, which allows us to perform network-level evaluations of various fusion strategies without having to train them separately. Besides, we can also obtain fine-grained numerical information such as layer-level preference on spatiotemporal fusion within the probability space. Our approach greatly boosts the efficiency of analyzing spatiotemporal fusion. Based on the probability space, we further generate new fusion strategies which achieve the state-of-the-art performance on four well-known action recognition datasets.



### Improved Residual Networks for Image and Video Recognition
- **Arxiv ID**: http://arxiv.org/abs/2004.04989v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.04989v1)
- **Published**: 2020-04-10 11:09:50+00:00
- **Updated**: 2020-04-10 11:09:50+00:00
- **Authors**: Ionut Cosmin Duta, Li Liu, Fan Zhu, Ling Shao
- **Comment**: None
- **Journal**: None
- **Summary**: Residual networks (ResNets) represent a powerful type of convolutional neural network (CNN) architecture, widely adopted and used in various tasks. In this work we propose an improved version of ResNets. Our proposed improvements address all three main components of a ResNet: the flow of information through the network layers, the residual building block, and the projection shortcut. We are able to show consistent improvements in accuracy and learning convergence over the baseline. For instance, on ImageNet dataset, using the ResNet with 50 layers, for top-1 accuracy we can report a 1.19% improvement over the baseline in one setting and around 2% boost in another. Importantly, these improvements are obtained without increasing the model complexity. Our proposed approach allows us to train extremely deep networks, while the baseline shows severe optimization issues. We report results on three tasks over six datasets: image classification (ImageNet, CIFAR-10 and CIFAR-100), object detection (COCO) and video action recognition (Kinetics-400 and Something-Something-v2). In the deep learning era, we establish a new milestone for the depth of a CNN. We successfully train a 404-layer deep CNN on the ImageNet dataset and a 3002-layer network on CIFAR-10 and CIFAR-100, while the baseline is not able to converge at such extreme depths. Code is available at: https://github.com/iduta/iresnet



### Robust Line Segments Matching via Graph Convolution Networks
- **Arxiv ID**: http://arxiv.org/abs/2004.04993v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.04993v2)
- **Published**: 2020-04-10 11:33:18+00:00
- **Updated**: 2020-04-13 04:58:30+00:00
- **Authors**: QuanMeng Ma, Guang Jiang, DianZhi Lai
- **Comment**: None
- **Journal**: None
- **Summary**: Line matching plays an essential role in structure from motion (SFM) and simultaneous localization and mapping (SLAM), especially in low-textured and repetitive scenes. In this paper, we present a new method of using a graph convolution network to match line segments in a pair of images, and we design a graph-based strategy of matching line segments with relaxing to an optimal transport problem. In contrast to hand-crafted line matching algorithms, our approach learns local line segment descriptor and the matching simultaneously through end-to-end training. The results show our method outperforms the state-of-the-art techniques, and especially, the recall is improved from 45.28% to 70.47% under a similar presicion. The code of our work is available at https://github.com/mameng1/GraphLineMatching.



### ModuleNet: Knowledge-inherited Neural Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/2004.05020v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.05020v2)
- **Published**: 2020-04-10 13:03:52+00:00
- **Updated**: 2020-04-14 03:39:26+00:00
- **Authors**: Yaran Chen, Ruiyuan Gao, Fenggang Liu, Dongbin Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: Although Neural Architecture Search (NAS) can bring improvement to deep models, they always neglect precious knowledge of existing models.   The computation and time costing property in NAS also means that we should not start from scratch to search, but make every attempt to reuse the existing knowledge.   In this paper, we discuss what kind of knowledge in a model can and should be used for new architecture design.   Then, we propose a new NAS algorithm, namely ModuleNet, which can fully inherit knowledge from existing convolutional neural networks.   To make full use of existing models, we decompose existing models into different \textit{module}s which also keep their weights, consisting of a knowledge base.   Then we sample and search for new architecture according to the knowledge base.   Unlike previous search algorithms, and benefiting from inherited knowledge, our method is able to directly search for architectures in the macro space by NSGA-II algorithm without tuning parameters in these \textit{module}s.   Experiments show that our strategy can efficiently evaluate the performance of new architecture even without tuning weights in convolutional layers.   With the help of knowledge we inherited, our search results can always achieve better performance on various datasets (CIFAR10, CIFAR100) over original architectures.



### Parsing-based View-aware Embedding Network for Vehicle Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2004.05021v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.05021v1)
- **Published**: 2020-04-10 13:06:09+00:00
- **Updated**: 2020-04-10 13:06:09+00:00
- **Authors**: Dechao Meng, Liang Li, Xuejing Liu, Yadong Li, Shijie Yang, Zhengjun Zha, Xingyu Gao, Shuhui Wang, Qingming Huang
- **Comment**: 10 pages, 6 figures
- **Journal**: None
- **Summary**: Vehicle Re-Identification is to find images of the same vehicle from various views in the cross-camera scenario. The main challenges of this task are the large intra-instance distance caused by different views and the subtle inter-instance discrepancy caused by similar vehicles. In this paper, we propose a parsing-based view-aware embedding network (PVEN) to achieve the view-aware feature alignment and enhancement for vehicle ReID. First, we introduce a parsing network to parse a vehicle into four different views, and then align the features by mask average pooling. Such alignment provides a fine-grained representation of the vehicle. Second, in order to enhance the view-aware features, we design a common-visible attention to focus on the common visible views, which not only shortens the distance among intra-instances, but also enlarges the discrepancy of inter-instances. The PVEN helps capture the stable discriminative information of vehicle under different views. The experiments conducted on three datasets show that our model outperforms state-of-the-art methods by a large margin.



### Weakly supervised multiple instance learning histopathological tumor segmentation
- **Arxiv ID**: http://arxiv.org/abs/2004.05024v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.05024v4)
- **Published**: 2020-04-10 13:12:47+00:00
- **Updated**: 2021-05-11 12:26:39+00:00
- **Authors**: Marvin Lerousseau, Maria Vakalopoulou, Marion Classe, Julien Adam, Enzo Battistella, Alexandre Carré, Théo Estienne, Théophraste Henry, Eric Deutsch, Nikos Paragios
- **Comment**: Accepted MICCAI 2020; added code + results url; 10 pages, 3 figures
- **Journal**: None
- **Summary**: Histopathological image segmentation is a challenging and important topic in medical imaging with tremendous potential impact in clinical practice. State of the art methods rely on hand-crafted annotations which hinder clinical translation since histology suffers from significant variations between cancer phenotypes. In this paper, we propose a weakly supervised framework for whole slide imaging segmentation that relies on standard clinical annotations, available in most medical systems. In particular, we exploit a multiple instance learning scheme for training models. The proposed framework has been evaluated on multi-locations and multi-centric public data from The Cancer Genome Atlas and the PatchCamelyon dataset. Promising results when compared with experts' annotations demonstrate the potentials of the presented approach. The complete framework, including $6481$ generated tumor maps and data processing, is available at https://github.com/marvinler/tcga_segmentation.



### Hyperspectral Image Clustering with Spatially-Regularized Ultrametrics
- **Arxiv ID**: http://arxiv.org/abs/2004.05048v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.AP
- **Links**: [PDF](http://arxiv.org/pdf/2004.05048v1)
- **Published**: 2020-04-10 14:27:41+00:00
- **Updated**: 2020-04-10 14:27:41+00:00
- **Authors**: Shukun Zhang, James M. Murphy
- **Comment**: 5 pages, 2 columns, 9 figures
- **Journal**: None
- **Summary**: We propose a method for the unsupervised clustering of hyperspectral images based on spatially regularized spectral clustering with ultrametric path distances. The proposed method efficiently combines data density and geometry to distinguish between material classes in the data, without the need for training labels. The proposed method is efficient, with quasilinear scaling in the number of data points, and enjoys robust theoretical performance guarantees. Extensive experiments on synthetic and real HSI data demonstrate its strong performance compared to benchmark and state-of-the-art methods. In particular, the proposed method achieves not only excellent labeling accuracy, but also efficiently estimates the number of clusters.



### ASL Recognition with Metric-Learning based Lightweight Network
- **Arxiv ID**: http://arxiv.org/abs/2004.05054v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.05054v1)
- **Published**: 2020-04-10 14:41:30+00:00
- **Updated**: 2020-04-10 14:41:30+00:00
- **Authors**: Evgeny Izutov
- **Comment**: None
- **Journal**: None
- **Summary**: In the past decades the set of human tasks that are solved by machines was extended dramatically. From simple image classification problems researchers now move towards solving more sophisticated and vital problems, like, autonomous driving and language translation. The case of language translation includes a challenging area of sign language translation that incorporates both image and language processing. We make a step in that direction by proposing a lightweight network for ASL gesture recognition with a performance sufficient for practical applications. The proposed solution demonstrates impressive robustness on MS-ASL dataset and in live mode for continuous sign gesture recognition scenario. Additionally, we describe how to combine action recognition model training with metric-learning to train the network on the database of limited size. The training code is available as part of Intel OpenVINO Training Extensions.



### CNN Encoder to Reduce the Dimensionality of Data Image for Motion Planning
- **Arxiv ID**: http://arxiv.org/abs/2004.05077v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.05077v1)
- **Published**: 2020-04-10 15:44:52+00:00
- **Updated**: 2020-04-10 15:44:52+00:00
- **Authors**: Janderson Ferreira, Agostinho A. F. Júnior, Yves M. Galvão, Bruno J. T. Fernandes, Pablo Barros
- **Comment**: None
- **Journal**: None
- **Summary**: Many real-world applications need path planning algorithms to solve tasks in different areas, such as social applications, autonomous cars, and tracking activities. And most importantly motion planning. Although the use of path planning is sufficient in most motion planning scenarios, they represent potential bottlenecks in large environments with dynamic changes. To tackle this problem, the number of possible routes could be reduced to make it easier for path planning algorithms to find the shortest path with less efforts. An traditional algorithm for path planning is the A*, it uses an heuristic to work faster than other solutions. In this work, we propose a CNN encoder capable of eliminating useless routes for motion planning problems, then we combine the proposed neural network output with A*. To measure the efficiency of our solution, we propose a database with different scenarios of motion planning problems. The evaluated metric is the number of the iterations to find the shortest path. The A* was compared with the CNN Encoder (proposal) with A*. In all evaluated scenarios, our solution reduced the number of iterations by more than 60\%.



### MA 3 : Model Agnostic Adversarial Augmentation for Few Shot learning
- **Arxiv ID**: http://arxiv.org/abs/2004.05100v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.05100v1)
- **Published**: 2020-04-10 16:35:49+00:00
- **Updated**: 2020-04-10 16:35:49+00:00
- **Authors**: Rohit Jena, Shirsendu Sukanta Halder, Katia Sycara
- **Comment**: Accepted at CVPR Workshop on Visual Learning with Limited Labels 2020
- **Journal**: None
- **Summary**: Despite the recent developments in vision-related problems using deep neural networks, there still remains a wide scope in the improvement of generalizing these models to unseen examples. In this paper, we explore the domain of few-shot learning with a novel augmentation technique. In contrast to other generative augmentation techniques, where the distribution over input images are learnt, we propose to learn the probability distribution over the image transformation parameters which are easier and quicker to learn. Our technique is fully differentiable which enables its extension to versatile data-sets and base models. We evaluate our proposed method on multiple base-networks and 2 data-sets to establish the robustness and efficiency of this method. We obtain an improvement of nearly 4% by adding our augmentation module without making any change in network architectures. We also make the code readily available for usage by the community.



### Deep transfer learning for improving single-EEG arousal detection
- **Arxiv ID**: http://arxiv.org/abs/2004.05111v2
- **DOI**: 10.1109/EMBC44109.2020.9176723
- **Categories**: **cs.CV**, eess.SP, stat.AP, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.05111v2)
- **Published**: 2020-04-10 16:51:06+00:00
- **Updated**: 2020-05-07 11:18:28+00:00
- **Authors**: Alexander Neergaard Olesen, Poul Jennum, Emmanuel Mignot, Helge B. D. Sorensen
- **Comment**: Accepted for presentation at EMBC2020
- **Journal**: None
- **Summary**: Datasets in sleep science present challenges for machine learning algorithms due to differences in recording setups across clinics. We investigate two deep transfer learning strategies for overcoming the channel mismatch problem for cases where two datasets do not contain exactly the same setup leading to degraded performance in single-EEG models. Specifically, we train a baseline model on multivariate polysomnography data and subsequently replace the first two layers to prepare the architecture for single-channel electroencephalography data. Using a fine-tuning strategy, our model yields similar performance to the baseline model (F1=0.682 and F1=0.694, respectively), and was significantly better than a comparable single-channel model. Our results are promising for researchers working with small databases who wish to use deep learning models pre-trained on larger databases.



### Model Uncertainty Quantification for Reliable Deep Vision Structural Health Monitoring
- **Arxiv ID**: http://arxiv.org/abs/2004.05151v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.05151v1)
- **Published**: 2020-04-10 17:54:10+00:00
- **Updated**: 2020-04-10 17:54:10+00:00
- **Authors**: Seyed Omid Sajedi, Xiao Liang
- **Comment**: None
- **Journal**: None
- **Summary**: Computer vision leveraging deep learning has achieved significant success in the last decade. Despite the promising performance of the existing deep models in the recent literature, the extent of models' reliability remains unknown. Structural health monitoring (SHM) is a crucial task for the safety and sustainability of structures, and thus prediction mistakes can have fatal outcomes. This paper proposes Bayesian inference for deep vision SHM models where uncertainty can be quantified using the Monte Carlo dropout sampling. Three independent case studies for cracks, local damage identification, and bridge component detection are investigated using Bayesian inference. Aside from better prediction results, mean class softmax variance and entropy, the two uncertainty metrics, are shown to have good correlations with misclassifications. While the uncertainty metrics can be used to trigger human intervention and potentially improve prediction results, interpretation of uncertainty masks can be challenging. Therefore, surrogate models are introduced to take the uncertainty as input such that the performance can be further boosted. The proposed methodology in this paper can be applied to future deep vision SHM frameworks to incorporate model uncertainty in the inspection processes.



### Learning to Explore using Active Neural SLAM
- **Arxiv ID**: http://arxiv.org/abs/2004.05155v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2004.05155v1)
- **Published**: 2020-04-10 17:57:29+00:00
- **Updated**: 2020-04-10 17:57:29+00:00
- **Authors**: Devendra Singh Chaplot, Dhiraj Gandhi, Saurabh Gupta, Abhinav Gupta, Ruslan Salakhutdinov
- **Comment**: Published in ICLR-2020. See the project webpage at
  https://devendrachaplot.github.io/projects/Neural-SLAM for supplementary
  videos. The code is available at
  https://github.com/devendrachaplot/Neural-SLAM
- **Journal**: None
- **Summary**: This work presents a modular and hierarchical approach to learn policies for exploring 3D environments, called `Active Neural SLAM'. Our approach leverages the strengths of both classical and learning-based methods, by using analytical path planners with learned SLAM module, and global and local policies. The use of learning provides flexibility with respect to input modalities (in the SLAM module), leverages structural regularities of the world (in global policies), and provides robustness to errors in state estimation (in local policies). Such use of learning within each module retains its benefits, while at the same time, hierarchical decomposition and modular training allow us to sidestep the high sample complexities associated with training end-to-end policies. Our experiments in visually and physically realistic simulated 3D environments demonstrate the effectiveness of our approach over past learning and geometry-based approaches. The proposed model can also be easily transferred to the PointGoal task and was the winning entry of the CVPR 2019 Habitat PointGoal Navigation Challenge.



### Hamiltonian Dynamics for Real-World Shape Interpolation
- **Arxiv ID**: http://arxiv.org/abs/2004.05199v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.05199v1)
- **Published**: 2020-04-10 18:38:52+00:00
- **Updated**: 2020-04-10 18:38:52+00:00
- **Authors**: Marvin Eisenberger, Daniel Cremers
- **Comment**: None
- **Journal**: None
- **Summary**: We revisit the classical problem of 3D shape interpolation and propose a novel, physically plausible approach based on Hamiltonian dynamics. While most prior work focuses on synthetic input shapes, our formulation is designed to be applicable to real-world scans with imperfect input correspondences and various types of noise. To that end, we use recent progress on dynamic thin shell simulation and divergence-free shape deformation and combine them to address the inverse problem of finding a plausible intermediate sequence for two input shapes. In comparison to prior work that mainly focuses on small distortion of consecutive frames, we explicitly model volume preservation and momentum conservation, as well as an anisotropic local distortion model. We argue that, in order to get a robust interpolation for imperfect inputs, we need to model the input noise explicitly which results in an alignment based formulation. Finally, we show a qualitative and quantitative improvement over prior work on a broad range of synthetic and scanned data. Besides being more robust to noisy inputs, our method yields exactly volume preserving intermediate shapes, avoids self-intersections and is scalable to high resolution scans.



### A Review on Deep Learning Techniques for Video Prediction
- **Arxiv ID**: http://arxiv.org/abs/2004.05214v2
- **DOI**: 10.1109/TPAMI.2020.3045007
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.05214v2)
- **Published**: 2020-04-10 19:58:44+00:00
- **Updated**: 2020-04-15 00:24:31+00:00
- **Authors**: Sergiu Oprea, Pablo Martinez-Gonzalez, Alberto Garcia-Garcia, John Alejandro Castro-Vargas, Sergio Orts-Escolano, Jose Garcia-Rodriguez, Antonis Argyros
- **Comment**: Submitted to TPAMI
- **Journal**: None
- **Summary**: The ability to predict, anticipate and reason about future outcomes is a key component of intelligent decision-making systems. In light of the success of deep learning in computer vision, deep-learning-based video prediction emerged as a promising research direction. Defined as a self-supervised learning task, video prediction represents a suitable framework for representation learning, as it demonstrated potential capabilities for extracting meaningful representations of the underlying patterns in natural videos. Motivated by the increasing interest in this task, we provide a review on the deep learning methods for prediction in video sequences. We firstly define the video prediction fundamentals, as well as mandatory background concepts and the most used datasets. Next, we carefully analyze existing video prediction models organized according to a proposed taxonomy, highlighting their contributions and their significance in the field. The summary of the datasets and methods is accompanied with experimental results that facilitate the assessment of the state of the art on a quantitative basis. The paper is summarized by drawing some general conclusions, identifying open research challenges and by pointing out future research directions.



### Deep Learning for Image and Point Cloud Fusion in Autonomous Driving: A Review
- **Arxiv ID**: http://arxiv.org/abs/2004.05224v2
- **DOI**: 10.1109/TITS.2020.3023541
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2004.05224v2)
- **Published**: 2020-04-10 20:43:14+00:00
- **Updated**: 2020-09-09 14:12:13+00:00
- **Authors**: Yaodong Cui, Ren Chen, Wenbo Chu, Long Chen, Daxin Tian, Ying Li, Dongpu Cao
- **Comment**: None
- **Journal**: IEEE Transactions on Intelligent Transportation Systems.(2021)
- **Summary**: Autonomous vehicles were experiencing rapid development in the past few years. However, achieving full autonomy is not a trivial task, due to the nature of the complex and dynamic driving environment. Therefore, autonomous vehicles are equipped with a suite of different sensors to ensure robust, accurate environmental perception. In particular, the camera-LiDAR fusion is becoming an emerging research theme. However, so far there has been no critical review that focuses on deep-learning-based camera-LiDAR fusion methods. To bridge this gap and motivate future research, this paper devotes to review recent deep-learning-based data fusion approaches that leverage both image and point cloud. This review gives a brief overview of deep learning on image and point cloud data processing. Followed by in-depth reviews of camera-LiDAR fusion methods in depth completion, object detection, semantic segmentation, tracking and online cross-sensor calibration, which are organized based on their respective fusion levels. Furthermore, we compare these methods on publicly available datasets. Finally, we identified gaps and over-looked challenges between current academic researches and real-world applications. Based on these observations, we provide our insights and point out promising research directions.



### End-to-end Learning Improves Static Object Geo-localization in Monocular Video
- **Arxiv ID**: http://arxiv.org/abs/2004.05232v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.05232v4)
- **Published**: 2020-04-10 21:10:34+00:00
- **Updated**: 2021-01-03 17:36:27+00:00
- **Authors**: Mohamed Chaabane, Lionel Gueguen, Ameni Trabelsi, Ross Beveridge, Stephen O'Hara
- **Comment**: WACV 2021
- **Journal**: None
- **Summary**: Accurately estimating the position of static objects, such as traffic lights, from the moving camera of a self-driving car is a challenging problem. In this work, we present a system that improves the localization of static objects by jointly-optimizing the components of the system via learning. Our system is comprised of networks that perform: 1) 5DoF object pose estimation from a single image, 2) association of objects between pairs of frames, and 3) multi-object tracking to produce the final geo-localization of the static objects within the scene. We evaluate our approach using a publicly-available data set, focusing on traffic lights due to data availability. For each component, we compare against contemporary alternatives and show significantly-improved performance. We also show that the end-to-end system performance is further improved via joint-training of the constituent models.



### Shape Estimation for Elongated Deformable Object using B-spline Chained Multiple Random Matrices Model
- **Arxiv ID**: http://arxiv.org/abs/2004.05233v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.SY, eess.SY
- **Links**: [PDF](http://arxiv.org/pdf/2004.05233v1)
- **Published**: 2020-04-10 21:15:54+00:00
- **Updated**: 2020-04-10 21:15:54+00:00
- **Authors**: Gang Yao, Ryan Saltus, Ashwin Dani
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, a B-spline chained multiple random matrices representation is proposed to model geometric characteristics of an elongated deformable object. The hyper degrees of freedom structure of the elongated deformable object make its shape estimation challenging. Based on the likelihood function of the proposed model, an expectation-maximization (EM) method is derived to estimate the shape of the elongated deformable object. A split and merge method based on the Euclidean minimum spanning tree (EMST) is proposed to provide initialization for the EM algorithm. The proposed algorithm is evaluated for the shape estimation of the elongated deformable objects in scenarios, such as the static rope with various configurations (including configurations with intersection), the continuous manipulation of a rope and a plastic tube, and the assembly of two plastic tubes. The execution time is computed and the accuracy of the shape estimation results is evaluated based on the comparisons between the estimated width values and its ground-truth, and the intersection over union (IoU) metric.



### Attend and Decode: 4D fMRI Task State Decoding Using Attention Models
- **Arxiv ID**: http://arxiv.org/abs/2004.05234v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.05234v2)
- **Published**: 2020-04-10 21:29:34+00:00
- **Updated**: 2021-01-19 06:35:49+00:00
- **Authors**: Sam Nguyen, Brenda Ng, Alan D. Kaplan, Priyadip Ray
- **Comment**: None
- **Journal**: Proceedings of the Machine Learning for Health NeurIPS Workshop,
  PMLR 136:267-279, 2020
- **Summary**: Functional magnetic resonance imaging (fMRI) is a neuroimaging modality that captures the blood oxygen level in a subject's brain while the subject either rests or performs a variety of functional tasks under different conditions. Given fMRI data, the problem of inferring the task, known as task state decoding, is challenging due to the high dimensionality (hundreds of million sampling points per datum) and complex spatio-temporal blood flow patterns inherent in the data. In this work, we propose to tackle the fMRI task state decoding problem by casting it as a 4D spatio-temporal classification problem. We present a novel architecture called Brain Attend and Decode (BAnD), that uses residual convolutional neural networks for spatial feature extraction and self-attention mechanisms for temporal modeling. We achieve significant performance gain compared to previous works on a 7-task benchmark from the large-scale Human Connectome Project-Young Adult (HCP-YA) dataset. We also investigate the transferability of BAnD's extracted features on unseen HCP tasks, either by freezing the spatial feature extraction layers and retraining the temporal model, or finetuning the entire model. The pre-trained features from BAnD are useful on similar tasks while finetuning them yields competitive results on unseen tasks/conditions.



### FLIVVER: Fly Lobula Inspired Visual Velocity Estimation & Ranging
- **Arxiv ID**: http://arxiv.org/abs/2004.05247v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.05247v1)
- **Published**: 2020-04-10 22:35:13+00:00
- **Updated**: 2020-04-10 22:35:13+00:00
- **Authors**: Bryson Lingenfelter, Arunava Nag, Floris van Breugel
- **Comment**: 8 pages, 6 figures
- **Journal**: None
- **Summary**: The mechanism by which a tiny insect or insect-sized robot could estimate its absolute velocity and distance to nearby objects remains unknown. However, this ability is critical for behaviors that require estimating wind direction during flight, such as odor-plume tracking. Neuroscience and behavior studies with insects have shown that they rely on the perception of image motion, or optic flow, to estimate relative motion, equivalent to a ratio of their velocity and distance to objects in the world. The key open challenge is therefore to decouple these two states from a single measurement of their ratio. Although modern SLAM (Simultaneous Localization and Mapping) methods provide a solution to this problem for robotic systems, these methods typically rely on computations that insects likely cannot perform, such as simultaneously tracking multiple individual visual features, remembering a 3D map of the world, and solving nonlinear optimization problems using iterative algorithms. Here we present a novel algorithm, FLIVVER, which combines the geometry of dynamic forward motion with inspiration from insect visual processing to \textit{directly} estimate absolute ground velocity from a combination of optic flow and acceleration information. Our algorithm provides a clear hypothesis for how insects might estimate absolute velocity, and also provides a theoretical framework for designing fast analog circuitry for efficient state estimation, which could be applied to insect-sized robots.



