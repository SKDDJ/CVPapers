# Arxiv Papers in cs.CV on 2020-04-15
### Intuitive, Interactive Beard and Hair Synthesis with Generative Models
- **Arxiv ID**: http://arxiv.org/abs/2004.06848v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.HC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.06848v1)
- **Published**: 2020-04-15 01:20:10+00:00
- **Updated**: 2020-04-15 01:20:10+00:00
- **Authors**: Kyle Olszewski, Duygu Ceylan, Jun Xing, Jose Echevarria, Zhili Chen, Weikai Chen, Hao Li
- **Comment**: To be presented in the 2020 Conference on Computer Vision and Pattern
  Recognition (CVPR 2020, Oral Presentation). Supplementary video can be seen
  at: https://www.youtube.com/watch?v=v4qOtBATrvM
- **Journal**: None
- **Summary**: We present an interactive approach to synthesizing realistic variations in facial hair in images, ranging from subtle edits to existing hair to the addition of complex and challenging hair in images of clean-shaven subjects. To circumvent the tedious and computationally expensive tasks of modeling, rendering and compositing the 3D geometry of the target hairstyle using the traditional graphics pipeline, we employ a neural network pipeline that synthesizes realistic and detailed images of facial hair directly in the target image in under one second. The synthesis is controlled by simple and sparse guide strokes from the user defining the general structural and color properties of the target hairstyle. We qualitatively and quantitatively evaluate our chosen method compared to several alternative approaches. We show compelling interactive editing results with a prototype user interface that allows novice users to progressively refine the generated image to match their desired hairstyle, and demonstrate that our approach also allows for flexible and high-fidelity scalp hair synthesis.



### Mosaic Super-resolution via Sequential Feature Pyramid Networks
- **Arxiv ID**: http://arxiv.org/abs/2004.06853v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.06853v1)
- **Published**: 2020-04-15 01:46:24+00:00
- **Updated**: 2020-04-15 01:46:24+00:00
- **Authors**: Mehrdad Shoeiby, Mohammad Ali Armin, Sadegh Aliakbarian, Saeed Anwar, Lars Petersson
- **Comment**: Accepted by IEEE CVPR Workshop
- **Journal**: None
- **Summary**: Advances in the design of multi-spectral cameras have led to great interests in a wide range of applications, from astronomy to autonomous driving. However, such cameras inherently suffer from a trade-off between the spatial and spectral resolution. In this paper, we propose to address this limitation by introducing a novel method to carry out super-resolution on raw mosaic images, multi-spectral or RGB Bayer, captured by modern real-time single-shot mosaic sensors. To this end, we design a deep super-resolution architecture that benefits from a sequential feature pyramid along the depth of the network. This, in fact, is achieved by utilizing a convolutional LSTM (ConvLSTM) to learn the inter-dependencies between features at different receptive fields. Additionally, by investigating the effect of different attention mechanisms in our framework, we show that a ConvLSTM inspired module is able to provide superior attention in our context. Our extensive experiments and analyses evidence that our approach yields significant super-resolution quality, outperforming current state-of-the-art mosaic super-resolution methods on both Bayer and multi-spectral images. Additionally, to the best of our knowledge, our method is the first specialized method to super-resolve mosaic images, whether it be multi-spectral or Bayer.



### A Practical Blockchain Framework using Image Hashing for Image Authentication
- **Arxiv ID**: http://arxiv.org/abs/2004.06860v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.06860v1)
- **Published**: 2020-04-15 02:57:32+00:00
- **Updated**: 2020-04-15 02:57:32+00:00
- **Authors**: Cameron White, Manoranjan Paul, Subrata Chakraborty
- **Comment**: This is un-published paper
- **Journal**: None
- **Summary**: Blockchain is a relatively new technology that can be seen as a decentralised database. Blockchain systems heavily rely on cryptographic hash functions to store their data, which makes it difficult to tamper with any data stored in the system. A topic that was researched along with blockchain is image authentication. Image authentication focuses on investigating and maintaining the integrity of images. As a blockchain system can be useful for maintaining data integrity, image authentication has the potential to be enhanced by blockchain. There are many techniques that can be used to authenticate images; the technique investigated by this work is image hashing. Image hashing is a technique used to calculate how similar two different images are. This is done by converting the images into hashes and then comparing them using a distance formula. To investigate the topic, an experiment involving a simulated blockchain was created. The blockchain acted as a database for images. This blockchain was made up of devices which contained their own unique image hashing algorithms. The blockchain was tested by creating modified copies of the images contained in the database, and then submitting them to the blockchain to see if it will return the original image. Through this experiment it was discovered that it is plausible to create an image authentication system using blockchain and image hashing. However, the design proposed by this work requires refinement, as it appears to struggle in some situations. This work shows that blockchain can be a suitable approach for authenticating images, particularly via image hashing. Other observations include that using multiple image hash algorithms at the same time can increase performance in some cases, as well as that each type of test done to the blockchain has its own unique pattern to its data.



### Eigendecomposition-Free Training of Deep Networks for Linear Least-Square Problems
- **Arxiv ID**: http://arxiv.org/abs/2004.07931v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.07931v1)
- **Published**: 2020-04-15 04:29:34+00:00
- **Updated**: 2020-04-15 04:29:34+00:00
- **Authors**: Zheng Dang, Kwang Moo Yi, Yinlin Hu, Fei Wang, Pascal Fua, Mathieu Salzmann
- **Comment**: 16 pages, Accepted by TPAMI. arXiv admin note: substantial text
  overlap with arXiv:1803.08071
- **Journal**: None
- **Summary**: Many classical Computer Vision problems, such as essential matrix computation and pose estimation from 3D to 2D correspondences, can be tackled by solving a linear least-square problem, which can be done by finding the eigenvector corresponding to the smallest, or zero, eigenvalue of a matrix representing a linear system. Incorporating this in deep learning frameworks would allow us to explicitly encode known notions of geometry, instead of having the network implicitly learn them from data. However, performing eigendecomposition within a network requires the ability to differentiate this operation. While theoretically doable, this introduces numerical instability in the optimization process in practice. In this paper, we introduce an eigendecomposition-free approach to training a deep network whose loss depends on the eigenvector corresponding to a zero eigenvalue of a matrix predicted by the network. We demonstrate that our approach is much more robust than explicit differentiation of the eigendecomposition using two general tasks, outlier rejection and denoising, with several practical examples including wide-baseline stereo, the perspective-n-point problem, and ellipse fitting. Empirically, our method has better convergence properties and yields state-of-the-art results.



### Effect of Input Noise Dimension in GANs
- **Arxiv ID**: http://arxiv.org/abs/2004.06882v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.06882v1)
- **Published**: 2020-04-15 04:56:52+00:00
- **Updated**: 2020-04-15 04:56:52+00:00
- **Authors**: Manisha Padala, Debojit Das, Sujit Gujar
- **Comment**: None
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) are by far the most successful generative models. Learning the transformation which maps a low dimensional input noise to the data distribution forms the foundation for GANs. Although they have been applied in various domains, they are prone to certain challenges like mode collapse and unstable training. To overcome the challenges, researchers have proposed novel loss functions, architectures, and optimization methods. In our work here, unlike the previous approaches, we focus on the input noise and its role in the generation.   We aim to quantitatively and qualitatively study the effect of the dimension of the input noise on the performance of GANs. For quantitative measures, typically \emph{Fr\'{e}chet Inception Distance (FID)} and \emph{Inception Score (IS)} are used as performance measure on image data-sets. We compare the FID and IS values for DCGAN and WGAN-GP. We use three different image data-sets -- each consisting of different levels of complexity. Through our experiments, we show that the right dimension of input noise for optimal results depends on the data-set and architecture used. We also observe that the state of the art performance measures does not provide enough useful insights. Hence we conclude that we need further theoretical analysis for understanding the relationship between the low dimensional distribution and the generated images. We also require better performance measures.



### Analysis of Scoliosis From Spinal X-Ray Images
- **Arxiv ID**: http://arxiv.org/abs/2004.06887v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.06887v1)
- **Published**: 2020-04-15 05:36:28+00:00
- **Updated**: 2020-04-15 05:36:28+00:00
- **Authors**: Abdullah-Al-Zubaer Imran, Chao Huang, Hui Tang, Wei Fan, Kenneth M. C. Cheung, Michael To, Zhen Qian, Demetri Terzopoulos
- **Comment**: 6 pages, 6 figures, 3 tables
- **Journal**: None
- **Summary**: Scoliosis is a congenital disease in which the spine is deformed from its normal shape. Measurement of scoliosis requires labeling and identification of vertebrae in the spine. Spine radiographs are the most cost-effective and accessible modality for imaging the spine. Reliable and accurate vertebrae segmentation in spine radiographs is crucial in image-guided spinal assessment, disease diagnosis, and treatment planning. Conventional assessments rely on tedious and time-consuming manual measurement, which is subject to inter-observer variability. A fully automatic method that can accurately identify and segment the associated vertebrae is unavailable in the literature. Leveraging a carefully-adjusted U-Net model with progressive side outputs, we propose an end-to-end segmentation model that provides a fully automatic and reliable segmentation of the vertebrae associated with scoliosis measurement. Our experimental results from a set of anterior-posterior spine X-Ray images indicate that our model, which achieves an average Dice score of 0.993, promises to be an effective tool in the identification and labeling of spinal vertebrae, eventually helping doctors in the reliable estimation of scoliosis. Moreover, estimation of Cobb angles from the segmented vertebrae further demonstrates the effectiveness of our model.



### Computer Vision For COVID-19 Control: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2004.09420v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/2004.09420v2)
- **Published**: 2020-04-15 05:43:52+00:00
- **Updated**: 2020-05-05 06:01:17+00:00
- **Authors**: Anwaar Ulhaq, Asim Khan, Douglas Gomes, Manoranjan Paul
- **Comment**: 24 Pages, 9 Figures
- **Journal**: None
- **Summary**: The COVID-19 pandemic has triggered an urgent need to contribute to the fight against an immense threat to the human population. Computer Vision, as a subfield of Artificial Intelligence, has enjoyed recent success in solving various complex problems in health care and has the potential to contribute to the fight of controlling COVID-19. In response to this call, computer vision researchers are putting their knowledge base at work to devise effective ways to counter COVID-19 challenge and serve the global community. New contributions are being shared with every passing day. It motivated us to review the recent work, collect information about available research resources and an indication of future research directions. We want to make it available to computer vision researchers to save precious time. This survey paper is intended to provide a preliminary review of the available literature on the computer vision efforts against COVID-19 pandemic.



### Continuous learning of face attribute synthesis
- **Arxiv ID**: http://arxiv.org/abs/2004.06904v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.06904v1)
- **Published**: 2020-04-15 06:44:13+00:00
- **Updated**: 2020-04-15 06:44:13+00:00
- **Authors**: Xin Ning, Shaohui Xu, Xiaoli Dong, Weijun Li, Fangzhe Nan, Yuanzhou Yao
- **Comment**: None
- **Journal**: None
- **Summary**: The generative adversarial network (GAN) exhibits great superiority in the face attribute synthesis task. However, existing methods have very limited effects on the expansion of new attributes. To overcome the limitations of a single network in new attribute synthesis, a continuous learning method for face attribute synthesis is proposed in this work. First, the feature vector of the input image is extracted and attribute direction regression is performed in the feature space to obtain the axes of different attributes. The feature vector is then linearly guided along the axis so that images with target attributes can be synthesized by the decoder. Finally, to make the network capable of continuous learning, the orthogonal direction modification module is used to extend the newly-added attributes. Experimental results show that the proposed method can endow a single network with the ability to learn attributes continuously, and, as compared to those produced by the current state-of-the-art methods, the synthetic attributes have higher accuracy.



### Combining Visible Light and Infrared Imaging for Efficient Detection of Respiratory Infections such as COVID-19 on Portable Device
- **Arxiv ID**: http://arxiv.org/abs/2004.06912v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.06912v1)
- **Published**: 2020-04-15 07:22:02+00:00
- **Updated**: 2020-04-15 07:22:02+00:00
- **Authors**: Zheng Jiang, Menghan Hu, Lei Fan, Yaling Pan, Wei Tang, Guangtao Zhai, Yong Lu
- **Comment**: None
- **Journal**: None
- **Summary**: Coronavirus Disease 2019 (COVID-19) has become a serious global epidemic in the past few months and caused huge loss to human society worldwide. For such a large-scale epidemic, early detection and isolation of potential virus carriers is essential to curb the spread of the epidemic. Recent studies have shown that one important feature of COVID-19 is the abnormal respiratory status caused by viral infections. During the epidemic, many people tend to wear masks to reduce the risk of getting sick. Therefore, in this paper, we propose a portable non-contact method to screen the health condition of people wearing masks through analysis of the respiratory characteristics. The device mainly consists of a FLIR one thermal camera and an Android phone. This may help identify those potential patients of COVID-19 under practical scenarios such as pre-inspection in schools and hospitals. In this work, we perform the health screening through the combination of the RGB and thermal videos obtained from the dual-mode camera and deep learning architecture.We first accomplish a respiratory data capture technique for people wearing masks by using face recognition. Then, a bidirectional GRU neural network with attention mechanism is applied to the respiratory data to obtain the health screening result. The results of validation experiments show that our model can identify the health status on respiratory with the accuracy of 83.7\% on the real-world dataset. The abnormal respiratory data and part of normal respiratory data are collected from Ruijin Hospital Affiliated to The Shanghai Jiao Tong University Medical School. Other normal respiratory data are obtained from healthy people around our researchers. This work demonstrates that the proposed portable and intelligent health screening device can be used as a pre-scan method for respiratory infections, which may help fight the current COVID-19 epidemic.



### Explaining Regression Based Neural Network Model
- **Arxiv ID**: http://arxiv.org/abs/2004.06918v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.06918v1)
- **Published**: 2020-04-15 07:38:40+00:00
- **Updated**: 2020-04-15 07:38:40+00:00
- **Authors**: Mégane Millan, Catherine Achard
- **Comment**: 7 pages, 5 figures, 4 tables
- **Journal**: None
- **Summary**: Several methods have been proposed to explain Deep Neural Network (DNN). However, to our knowledge, only classification networks have been studied to try to determine which input dimensions motivated the decision. Furthermore, as there is no ground truth to this problem, results are only assessed qualitatively in regards to what would be meaningful for a human. In this work, we design an experimental settings where the ground truth can been established: we generate ideal signals and disrupted signals with errors and learn a neural network that determines the quality of the signals. This quality is simply a score based on the distance between the disrupted signals and the corresponding ideal signal. We then try to find out how the network estimated this score and hope to find the time-step and dimensions of the signal where errors are present. This experimental setting enables us to compare several methods for network explanation and to propose a new method, named AGRA for Accurate Gradient, based on several trainings that decrease the noise present in most state-of-the-art results. Comparative results show that the proposed method outperforms state-of-the-art methods for locating time-steps where errors occur in the signal.



### Light Weight Residual Dense Attention Net for Spectral Reconstruction from RGB Images
- **Arxiv ID**: http://arxiv.org/abs/2004.06930v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2004.06930v2)
- **Published**: 2020-04-15 07:58:15+00:00
- **Updated**: 2020-04-19 03:04:57+00:00
- **Authors**: D. Sabari Nathan, K. Uma, D Synthiya Vinothini, B. Sathya Bama, S. M. Md Mansoor Roomi
- **Comment**: 6pages,4 figures
- **Journal**: None
- **Summary**: Hyperspectral Imaging is the acquisition of spectral and spatial information of a particular scene. Capturing such information from a specialized hyperspectral camera remains costly. Reconstructing such information from the RGB image achieves a better solution in both classification and object recognition tasks. This work proposes a novel light weight network with very less number of parameters about 233,059 parameters based on Residual dense model with attention mechanism to obtain this solution. This network uses Coordination Convolutional Block to get the spatial information. The weights from this block are shared by two independent feature extraction mechanisms, one by dense feature extraction and the other by the multiscale hierarchical feature extraction. Finally, the features from both the feature extraction mechanisms are globally fused to produce the 31 spectral bands. The network is trained with NTIRE 2020 challenge dataset and thus achieved 0.0457 MRAE metric value with less computational complexity.



### Targeted Attack for Deep Hashing based Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2004.07955v3
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.07955v3)
- **Published**: 2020-04-15 08:36:58+00:00
- **Updated**: 2020-07-23 08:24:04+00:00
- **Authors**: Jiawang Bai, Bin Chen, Yiming Li, Dongxian Wu, Weiwei Guo, Shu-tao Xia, En-hui Yang
- **Comment**: Accepted by ECCV 2020 as Oral
- **Journal**: None
- **Summary**: The deep hashing based retrieval method is widely adopted in large-scale image and video retrieval. However, there is little investigation on its security. In this paper, we propose a novel method, dubbed deep hashing targeted attack (DHTA), to study the targeted attack on such retrieval. Specifically, we first formulate the targeted attack as a point-to-set optimization, which minimizes the average distance between the hash code of an adversarial example and those of a set of objects with the target label. Then we design a novel component-voting scheme to obtain an anchor code as the representative of the set of hash codes of objects with the target label, whose optimality guarantee is also theoretically derived. To balance the performance and perceptibility, we propose to minimize the Hamming distance between the hash code of the adversarial example and the anchor code under the $\ell^\infty$ restriction on the perturbation. Extensive experiments verify that DHTA is effective in attacking both deep hashing based image retrieval and video retrieval.



### Self-Supervised training for blind multi-frame video denoising
- **Arxiv ID**: http://arxiv.org/abs/2004.06957v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.06957v4)
- **Published**: 2020-04-15 09:08:09+00:00
- **Updated**: 2021-04-20 17:18:55+00:00
- **Authors**: Valéry Dewil, Jérémy Anger, Axel Davy, Thibaud Ehret, Pablo Arias, Gabriele Facciolo
- **Comment**: 14 pages
- **Journal**: None
- **Summary**: We propose a self-supervised approach for training multi-frame video denoising networks. These networks predict frame t from a window of frames around t. Our self-supervised approach benefits from the video temporal consistency by penalizing a loss between the predicted frame t and a neighboring target frame, which are aligned using an optical flow. We use the proposed strategy for online internal learning, where a pre-trained network is fine-tuned to denoise a new unknown noise type from a single video. After a few frames, the proposed fine-tuning reaches and sometimes surpasses the performance of a state-of-the-art network trained with supervision. In addition, for a wide range of noise types, it can be applied blindly without knowing the noise distribution. We demonstrate this by showing results on blind denoising of different synthetic and realistic noises.



### Unified Dynamic Convolutional Network for Super-Resolution with Variational Degradations
- **Arxiv ID**: http://arxiv.org/abs/2004.06965v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.06965v1)
- **Published**: 2020-04-15 09:21:01+00:00
- **Updated**: 2020-04-15 09:21:01+00:00
- **Authors**: Yu-Syuan Xu, Shou-Yao Roy Tseng, Yu Tseng, Hsien-Kai Kuo, Yi-Min Tsai
- **Comment**: CVPR 2020
- **Journal**: None
- **Summary**: Deep Convolutional Neural Networks (CNNs) have achieved remarkable results on Single Image Super-Resolution (SISR). Despite considering only a single degradation, recent studies also include multiple degrading effects to better reflect real-world cases. However, most of the works assume a fixed combination of degrading effects, or even train an individual network for different combinations. Instead, a more practical approach is to train a single network for wide-ranging and variational degradations. To fulfill this requirement, this paper proposes a unified network to accommodate the variations from inter-image (cross-image variations) and intra-image (spatial variations). Different from the existing works, we incorporate dynamic convolution which is a far more flexible alternative to handle different variations. In SISR with non-blind setting, our Unified Dynamic Convolutional Network for Variational Degradations (UDVD) is evaluated on both synthetic and real images with an extensive set of variations. The qualitative results demonstrate the effectiveness of UDVD over various existing works. Extensive experiments show that our UDVD achieves favorable or comparable performance on both synthetic and real images.



### Exploration of Indoor Environments Predicting the Layout of Partially Observed Rooms
- **Arxiv ID**: http://arxiv.org/abs/2004.06967v1
- **DOI**: 10.5555/3461017.3461113
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.06967v1)
- **Published**: 2020-04-15 09:28:40+00:00
- **Updated**: 2020-04-15 09:28:40+00:00
- **Authors**: Matteo Luperto, Luca Fochetta, Francesco Amigoni
- **Comment**: None
- **Journal**: None
- **Summary**: We consider exploration tasks in which an autonomous mobile robot incrementally builds maps of initially unknown indoor environments. In such tasks, the robot makes a sequence of decisions on where to move next that, usually, are based on knowledge about the observed parts of the environment. In this paper, we present an approach that exploits a prediction of the geometric structure of the unknown parts of an environment to improve exploration performance. In particular, we leverage an existing method that reconstructs the layout of an environment starting from a partial grid map and that predicts the shape of partially observed rooms on the basis of geometric features representing the regularities of the indoor environment. Then, we originally employ the predicted layout to estimate the amount of new area the robot would observe from candidate locations in order to inform the selection of the next best location and to early stop the exploration when no further relevant area is expected to be discovered. Experimental activities show that our approach is able to effectively predict the layout of partially observed rooms and to use such knowledge to speed up the exploration.



### Roommate Compatibility Detection Through Machine Learning Techniques
- **Arxiv ID**: http://arxiv.org/abs/2004.06970v2
- **DOI**: None
- **Categories**: **cs.SI**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.06970v2)
- **Published**: 2020-04-15 09:34:55+00:00
- **Updated**: 2020-11-28 13:27:05+00:00
- **Authors**: Mansha Lamba, Raunak Goswami, Mr. Vinay, Mohit Lamba
- **Comment**: Detected severe inconsitencies in this pre-eliminary work and
  requires a thorough re-examination
- **Journal**: None
- **Summary**: Our objective is to develop an artificially intelligent system which aims at checking the compatibility between the roommates of same or different sex sharing a common area of residence. There are a few key factors determining one's compatibility with the other person. Interpersonal behaviour , situational awareness, communication skills. Here we are trying to build a system that evaluates user on these key factors not via pen paper test but through a highly engaging set of questions and answers. Hence using these scores as an input to our machine learning algorithm which is based on previous trends to come up with percentage probability of user being compatible with another user. With the growing population there is always a challenge for organisation and educational institutions to make the students and their employees more and more productive and in such cases a person's social environment comes into play. A person may be a genius but as long as he is not able to work well with his peers there will always be a chance of more productive performance. It is a well-established fact that human are and have always been a social animal and this has helped in creating communities of like-minded people. Many times, even when there are a large no of people employed to do a particular task the result may not be as expected as people may not compatible in working with one another. This at the end creates performance gaps, hinders organisation success and in many cases loss of precious resources. Our intent is not to remove the non-compatible people from the picture but to find out the perfect compatible match for the person elsewhere that will not only save the resources will also enable effective use of resources. Through the use of various machine learning classification techniques, we intent to do this.



### ActionSpotter: Deep Reinforcement Learning Framework for Temporal Action Spotting in Videos
- **Arxiv ID**: http://arxiv.org/abs/2004.06971v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.06971v2)
- **Published**: 2020-04-15 09:36:37+00:00
- **Updated**: 2020-11-10 16:43:56+00:00
- **Authors**: Guillaume Vaudaux-Ruth, Adrien Chan-Hon-Tong, Catherine Achard
- **Comment**: None
- **Journal**: None
- **Summary**: Summarizing video content is an important task in many applications. This task can be defined as the computation of the ordered list of actions present in a video. Such a list could be extracted using action detection algorithms. However, it is not necessary to determine the temporal boundaries of actions to know their existence. Moreover, localizing precise boundaries usually requires dense video analysis to be effective. In this work, we propose to directly compute this ordered list by sparsely browsing the video and selecting one frame per action instance, task known as action spotting in literature. To do this, we propose ActionSpotter, a spotting algorithm that takes advantage of Deep Reinforcement Learning to efficiently spot actions while adapting its video browsing speed, without additional supervision. Experiments performed on datasets THUMOS14 and ActivityNet show that our framework outperforms state of the art detection methods. In particular, the spotting mean Average Precision on THUMOS14 is significantly improved from 59.7% to 65.6% while skipping 23% of video.



### MXR-U-Nets for Real Time Hyperspectral Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2004.07003v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, I.4.5; I.4.10
- **Links**: [PDF](http://arxiv.org/pdf/2004.07003v1)
- **Published**: 2020-04-15 11:06:48+00:00
- **Updated**: 2020-04-15 11:06:48+00:00
- **Authors**: Atmadeep Banerjee, Akash Palrecha
- **Comment**: None
- **Journal**: None
- **Summary**: In recent times, CNNs have made significant contributions to applications in image generation, super-resolution and style transfer. In this paper, we build upon the work of Howard and Gugger, He et al. and Misra, D. and propose a CNN architecture that accurately reconstructs hyperspectral images from their RGB counterparts. We also propose a much shallower version of our best model with a 10% relative memory footprint and 3x faster inference, thus enabling real-time video applications while still experiencing only about a 0.5% decrease in performance.



### Visual Descriptor Learning from Monocular Video
- **Arxiv ID**: http://arxiv.org/abs/2004.07007v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.07007v1)
- **Published**: 2020-04-15 11:19:57+00:00
- **Updated**: 2020-04-15 11:19:57+00:00
- **Authors**: Umashankar Deekshith, Nishit Gajjar, Max Schwarz, Sven Behnke
- **Comment**: International Conference on Computer Vision Theory and Applications
  (VISAPP) 2020
- **Journal**: None
- **Summary**: Correspondence estimation is one of the most widely researched and yet only partially solved area of computer vision with many applications in tracking, mapping, recognition of objects and environment. In this paper, we propose a novel way to estimate dense correspondence on an RGB image where visual descriptors are learned from video examples by training a fully convolutional network. Most deep learning methods solve this by training the network with a large set of expensive labeled data or perform labeling through strong 3D generative models using RGB-D videos. Our method learns from RGB videos using contrastive loss, where relative labeling is estimated from optical flow. We demonstrate the functionality in a quantitative analysis on rendered videos, where ground truth information is available. Not only does the method perform well on test data with the same background, it also generalizes to situations with a new background. The descriptors learned are unique and the representations determined by the network are global. We further show the applicability of the method to real-world videos.



### Code-Aligned Autoencoders for Unsupervised Change Detection in Multimodal Remote Sensing Images
- **Arxiv ID**: http://arxiv.org/abs/2004.07011v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.07011v1)
- **Published**: 2020-04-15 11:24:51+00:00
- **Updated**: 2020-04-15 11:24:51+00:00
- **Authors**: Luigi T. Luppino, Mads A. Hansen, Michael Kampffmeyer, Filippo M. Bianchi, Gabriele Moser, Robert Jenssen, Stian N. Anfinsen
- **Comment**: None
- **Journal**: None
- **Summary**: Image translation with convolutional autoencoders has recently been used as an approach to multimodal change detection in bitemporal satellite images. A main challenge is the alignment of the code spaces by reducing the contribution of change pixels to the learning of the translation function. Many existing approaches train the networks by exploiting supervised information of the change areas, which, however, is not always available. We propose to extract relational pixel information captured by domain-specific affinity matrices at the input and use this to enforce alignment of the code spaces and reduce the impact of change pixels on the learning objective. A change prior is derived in an unsupervised fashion from pixel pair affinities that are comparable across domains. To achieve code space alignment we enforce that pixel with similar affinity relations in the input domains should be correlated also in code space. We demonstrate the utility of this procedure in combination with cycle consistency. The proposed approach are compared with state-of-the-art deep learning algorithms. Experiments conducted on four real datasets show the effectiveness of our methodology.



### Contextual Pyramid Attention Network for Building Segmentation in Aerial Imagery
- **Arxiv ID**: http://arxiv.org/abs/2004.07018v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.07018v1)
- **Published**: 2020-04-15 11:36:26+00:00
- **Updated**: 2020-04-15 11:36:26+00:00
- **Authors**: Clint Sebastian, Raffaele Imbriaco, Egor Bondarev, Peter H. N. de With
- **Comment**: None
- **Journal**: None
- **Summary**: Building extraction from aerial images has several applications in problems such as urban planning, change detection, and disaster management. With the increasing availability of data, Convolutional Neural Networks (CNNs) for semantic segmentation of remote sensing imagery has improved significantly in recent years. However, convolutions operate in local neighborhoods and fail to capture non-local features that are essential in semantic understanding of aerial images. In this work, we propose to improve building segmentation of different sizes by capturing long-range dependencies using contextual pyramid attention (CPA). The pathways process the input at multiple scales efficiently and combine them in a weighted manner, similar to an ensemble model. The proposed method obtains state-of-the-art performance on the Inria Aerial Image Labelling Dataset with minimal computation costs. Our method improves 1.8 points over current state-of-the-art methods and 12.6 points higher than existing baselines on the Intersection over Union (IoU) metric without any post-processing. Code and models will be made publicly available.



### 4DFlowNet: Super-Resolution 4D Flow MRI using Deep Learning and Computational Fluid Dynamics
- **Arxiv ID**: http://arxiv.org/abs/2004.07035v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2004.07035v1)
- **Published**: 2020-04-15 12:16:52+00:00
- **Updated**: 2020-04-15 12:16:52+00:00
- **Authors**: Edward Ferdian, Avan Suinesiaputra, David Dubowitz, Debbie Zhao, Alan Wang, Brett Cowan, Alistair Young
- **Comment**: accepted to Frontiers in Cardiovascular Medicine
- **Journal**: None
- **Summary**: 4D-flow magnetic resonance imaging (MRI) is an emerging imaging technique where spatiotemporal 3D blood velocity can be captured with full volumetric coverage in a single non-invasive examination. This enables qualitative and quantitative analysis of hemodynamic flow parameters of the heart and great vessels. An increase in the image resolution would provide more accuracy and allow better assessment of the blood flow, especially for patients with abnormal flows. However, this must be balanced with increasing imaging time. The recent success of deep learning in generating super resolution images shows promise for implementation in medical images. We utilized computational fluid dynamics simulations to generate fluid flow simulations and represent them as synthetic 4D flow MRI data. We built our training dataset to mimic actual 4D flow MRI data with its corresponding noise distribution. Our novel 4DFlowNet network was trained on this synthetic 4D flow data and was capable in producing noise-free super resolution 4D flow phase images with upsample factor of 2. We also tested the 4DFlowNet in actual 4D flow MR images of a phantom and normal volunteer data, and demonstrated comparable results with the actual flow rate measurements giving an absolute relative error of 0.6 to 5.8% and 1.1 to 3.8% in the phantom data and normal volunteer data, respectively.



### Extending Unsupervised Neural Image Compression With Supervised Multitask Learning
- **Arxiv ID**: http://arxiv.org/abs/2004.07041v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.07041v1)
- **Published**: 2020-04-15 12:20:22+00:00
- **Updated**: 2020-04-15 12:20:22+00:00
- **Authors**: David Tellez, Diederik Hoppener, Cornelis Verhoef, Dirk Grunhagen, Pieter Nierop, Michal Drozdzal, Jeroen van der Laak, Francesco Ciompi
- **Comment**: Medical Imaging with Deep Learning 2020 (MIDL20)
- **Journal**: None
- **Summary**: We focus on the problem of training convolutional neural networks on gigapixel histopathology images to predict image-level targets. For this purpose, we extend Neural Image Compression (NIC), an image compression framework that reduces the dimensionality of these images using an encoder network trained unsupervisedly. We propose to train this encoder using supervised multitask learning (MTL) instead. We applied the proposed MTL NIC to two histopathology datasets and three tasks. First, we obtained state-of-the-art results in the Tumor Proliferation Assessment Challenge of 2016 (TUPAC16). Second, we successfully classified histopathological growth patterns in images with colorectal liver metastasis (CLM). Third, we predicted patient risk of death by learning directly from overall survival in the same CLM data. Our experimental results suggest that the representations learned by the MTL objective are: (1) highly specific, due to the supervised training signal, and (2) transferable, since the same features perform well across different tasks. Additionally, we trained multiple encoders with different training objectives, e.g. unsupervised and variants of MTL, and observed a positive correlation between the number of tasks in MTL and the system performance on the TUPAC16 dataset.



### JCS: An Explainable COVID-19 Diagnosis System by Joint Classification and Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2004.07054v3
- **DOI**: 10.1109/TIP.2021.3058783
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.07054v3)
- **Published**: 2020-04-15 12:30:40+00:00
- **Updated**: 2021-08-03 13:44:25+00:00
- **Authors**: Yu-Huan Wu, Shang-Hua Gao, Jie Mei, Jun Xu, Deng-Ping Fan, Rong-Guo Zhang, Ming-Ming Cheng
- **Comment**: To appear in IEEE Transactions on Image Processing. Dataset and code
  are available at https://github.com/yuhuan-wu/JCS
- **Journal**: None
- **Summary**: Recently, the coronavirus disease 2019 (COVID-19) has caused a pandemic disease in over 200 countries, influencing billions of humans. To control the infection, identifying and separating the infected people is the most crucial step. The main diagnostic tool is the Reverse Transcription Polymerase Chain Reaction (RT-PCR) test. Still, the sensitivity of the RT-PCR test is not high enough to effectively prevent the pandemic. The chest CT scan test provides a valuable complementary tool to the RT-PCR test, and it can identify the patients in the early-stage with high sensitivity. However, the chest CT scan test is usually time-consuming, requiring about 21.5 minutes per case. This paper develops a novel Joint Classification and Segmentation (JCS) system to perform real-time and explainable COVID-19 chest CT diagnosis. To train our JCS system, we construct a large scale COVID-19 Classification and Segmentation (COVID-CS) dataset, with 144,167 chest CT images of 400 COVID-19 patients and 350 uninfected cases. 3,855 chest CT images of 200 patients are annotated with fine-grained pixel-level labels of opacifications, which are increased attenuation of the lung parenchyma. We also have annotated lesion counts, opacification areas, and locations and thus benefit various diagnosis aspects. Extensive experiments demonstrate that the proposed JCS diagnosis system is very efficient for COVID-19 classification and segmentation. It obtains an average sensitivity of 95.0% and a specificity of 93.0% on the classification test set, and 78.5% Dice score on the segmentation test set of our COVID-CS dataset. The COVID-CS dataset and code are available at https://github.com/yuhuan-wu/JCS.



### Fully Automated Myocardial Strain Estimation from CMR Tagged Images using a Deep Learning Framework in the UK Biobank
- **Arxiv ID**: http://arxiv.org/abs/2004.07064v1
- **DOI**: 10.1148/ryct.2020190032
- **Categories**: **eess.IV**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2004.07064v1)
- **Published**: 2020-04-15 12:49:15+00:00
- **Updated**: 2020-04-15 12:49:15+00:00
- **Authors**: Edward Ferdian, Avan Suinesiaputra, Kenneth Fung, Nay Aung, Elena Lukaschuk, Ahmet Barutcu, Edd Maclean, Jose Paiva, Stefan K. Piechnik, Stefan Neubauer, Steffen E Petersen, Alistair A. Young
- **Comment**: accepted in Radiology Cardiothoracic Imaging
- **Journal**: Radiology: Cardiothoracic Imaging 2020; 2(1):e190032
- **Summary**: Purpose: To demonstrate the feasibility and performance of a fully automated deep learning framework to estimate myocardial strain from short-axis cardiac magnetic resonance tagged images. Methods and Materials: In this retrospective cross-sectional study, 4508 cases from the UK Biobank were split randomly into 3244 training and 812 validation cases, and 452 test cases. Ground truth myocardial landmarks were defined and tracked by manual initialization and correction of deformable image registration using previously validated software with five readers. The fully automatic framework consisted of 1) a convolutional neural network (CNN) for localization, and 2) a combination of a recurrent neural network (RNN) and a CNN to detect and track the myocardial landmarks through the image sequence for each slice. Radial and circumferential strain were then calculated from the motion of the landmarks and averaged on a slice basis. Results: Within the test set, myocardial end-systolic circumferential Green strain errors were -0.001 +/- 0.025, -0.001 +/- 0.021, and 0.004 +/- 0.035 in basal, mid, and apical slices respectively (mean +/- std. dev. of differences between predicted and manual strain). The framework reproduced significant reductions in circumferential strain in diabetics, hypertensives, and participants with previous heart attack. Typical processing time was ~260 frames (~13 slices) per second on an NVIDIA Tesla K40 with 12GB RAM, compared with 6-8 minutes per slice for the manual analysis. Conclusions: The fully automated RNNCNN framework for analysis of myocardial strain enabled unbiased strain evaluation in a high-throughput workflow, with similar ability to distinguish impairment due to diabetes, hypertension, and previous heart attack.



### Image Segmentation Using Hybrid Representations
- **Arxiv ID**: http://arxiv.org/abs/2004.07071v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.07071v1)
- **Published**: 2020-04-15 13:07:35+00:00
- **Updated**: 2020-04-15 13:07:35+00:00
- **Authors**: Alakh Desai, Ruchi Chauhan, Jayanthi Sivaswamy
- **Comment**: 4 pages, 6 figures, to be published in ISBI 2020
- **Journal**: None
- **Summary**: This work explores a hybrid approach to segmentation as an alternative to a purely data-driven approach. We introduce an end-to-end U-Net based network called DU-Net, which uses additional frequency preserving features, namely the Scattering Coefficients (SC), for medical image segmentation. SC are translation invariant and Lipschitz continuous to deformations which help DU-Net outperform other conventional CNN counterparts on four datasets and two segmentation tasks: Optic Disc and Optic Cup in color fundus images and fetal Head in ultrasound images. The proposed method shows remarkable improvement over the basic U-Net with performance competitive to state-of-the-art methods. The results indicate that it is possible to use a lighter network trained with fewer images (without any augmentation) to attain good segmentation results.



### ALCN: Adaptive Local Contrast Normalization
- **Arxiv ID**: http://arxiv.org/abs/2004.07945v1
- **DOI**: 10.1016/j.cviu.2020.102947
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.07945v1)
- **Published**: 2020-04-15 13:40:03+00:00
- **Updated**: 2020-04-15 13:40:03+00:00
- **Authors**: Mahdi Rad, Peter M. Roth, Vincent Lepetit
- **Comment**: This version corresponds to the pre-print of the paper accepted for
  Computer Vision and Image Understanding (CVIU). arXiv admin note: substantial
  text overlap with arXiv:1708.09633
- **Journal**: None
- **Summary**: To make Robotics and Augmented Reality applications robust to illumination changes, the current trend is to train a Deep Network with training images captured under many different lighting conditions. Unfortunately, creating such a training set is a very unwieldy and complex task. We therefore propose a novel illumination normalization method that can easily be used for different problems with challenging illumination conditions. Our preliminary experiments show that among current normalization methods, the Difference-of Gaussians method remains a very good baseline, and we introduce a novel illumination normalization model that generalizes it. Our key insight is then that the normalization parameters should depend on the input image, and we aim to train a Convolutional Neural Network to predict these parameters from the input image. This, however, cannot be done in a supervised manner, as the optimal parameters are not known a priori. We thus designed a method to train this network jointly with another network that aims to recognize objects under different illuminations: The latter network performs well when the former network predicts good values for the normalization parameters. We show that our method significantly outperforms standard normalization methods and would also be appear to be universal since it does not have to be re-trained for each new application. Our method improves the robustness to light changes of state-of-the-art 3D object detection and face recognition methods.



### Seeing Red: PPG Biometrics Using Smartphone Cameras
- **Arxiv ID**: http://arxiv.org/abs/2004.07088v1
- **DOI**: 10.1109/CVPRW50498.2020.00417
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.07088v1)
- **Published**: 2020-04-15 13:50:36+00:00
- **Updated**: 2020-04-15 13:50:36+00:00
- **Authors**: Giulio Lovisotto, Henry Turner, Simon Eberz, Ivan Martinovic
- **Comment**: 8 pages, 15th IEEE Computer Society Workshop on Biometrics 2020
- **Journal**: None
- **Summary**: In this paper, we propose a system that enables photoplethysmogram (PPG)-based authentication by using a smartphone camera. PPG signals are obtained by recording a video from the camera as users are resting their finger on top of the camera lens. The signals can be extracted based on subtle changes in the video that are due to changes in the light reflection properties of the skin as the blood flows through the finger. We collect a dataset of PPG measurements from a set of 15 users over the course of 6-11 sessions per user using an iPhone X for the measurements. We design an authentication pipeline that leverages the uniqueness of each individual's cardiovascular system, identifying a set of distinctive features from each heartbeat. We conduct a set of experiments to evaluate the recognition performance of the PPG biometric trait, including cross-session scenarios which have been disregarded in previous work. We found that when aggregating sufficient samples for the decision we achieve an EER as low as 8%, but that the performance greatly decreases in the cross-session scenario, with an average EER of 20%.



### DeeSCo: Deep heterogeneous ensemble with Stochastic Combinatory loss for gaze estimation
- **Arxiv ID**: http://arxiv.org/abs/2004.07098v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.07098v1)
- **Published**: 2020-04-15 14:06:31+00:00
- **Updated**: 2020-04-15 14:06:31+00:00
- **Authors**: Edouard Yvinec, Arnaud Dapogny, Kévin Bailly
- **Comment**: 7 pages, 6 figures, FG 2020
- **Journal**: None
- **Summary**: From medical research to gaming applications, gaze estimation is becoming a valuable tool. While there exists a number of hardware-based solutions, recent deep learning-based approaches, coupled with the availability of large-scale databases, have allowed to provide a precise gaze estimate using only consumer sensors. However, there remains a number of questions, regarding the problem formulation, architectural choices and learning paradigms for designing gaze estimation systems in order to bridge the gap between geometry-based systems involving specific hardware and approaches using consumer sensors only. In this paper, we introduce a deep, end-to-end trainable ensemble of heatmap-based weak predictors for 2D/3D gaze estimation. We show that, through heterogeneous architectural design of these weak predictors, we can improve the decorrelation between the latter predictors to design more robust deep ensemble models. Furthermore, we propose a stochastic combinatory loss that consists in randomly sampling combinations of weak predictors at train time. This allows to train better individual weak predictors, with lower correlation between them. This, in turns, allows to significantly enhance the performance of the deep ensemble. We show that our Deep heterogeneous ensemble with Stochastic Combinatory loss (DeeSCo) outperforms state-of-the-art approaches for 2D/3D gaze estimation on multiple datasets.



### Fully Convolutional Online Tracking
- **Arxiv ID**: http://arxiv.org/abs/2004.07109v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.07109v5)
- **Published**: 2020-04-15 14:21:57+00:00
- **Updated**: 2021-09-26 09:13:16+00:00
- **Authors**: Yutao Cui, Cheng Jiang, Limin Wang, Gangshan Wu
- **Comment**: We have re-formulated our online regression model generator. And the
  new results on the datasets are easily reproduced
- **Journal**: None
- **Summary**: Online learning has turned out to be effective for improving tracking performance. However, it could be simply applied for classification branch, but still remains challenging to adapt to regression branch due to its complex design and intrinsic requirement for high-quality online samples. To tackle this issue, we present the fully convolutional online tracking framework, coined as FCOT, and focus on enabling online learning for both classification and regression branches by using a target filter based tracking paradigm. Our key contribution is to introduce an online regression model generator (RMG) for initializing weights of the target filter with online samples and then optimizing this target filter weights based on the groundtruth samples at the first frame. Based on the online RGM, we devise a simple anchor-free tracker (FCOT), composed of a feature backbone, an up-sampling decoder, a multi-scale classification branch, and a multi-scale regression branch. Thanks to the unique design of RMG, our FCOT can not only be more effective in handling target variation along temporal dimension thus generating more precise results, but also overcome the issue of error accumulation during the tracking procedure. In addition, due to its simplicity in design, our FCOT could be trained and deployed in a fully convolutional manner with a real-time running speed. The proposed FCOT achieves the state-of-the-art performance on seven benchmarks, including VOT2018, LaSOT, TrackingNet, GOT-10k, OTB100, UAV123, and NFS. Code and models of our FCOT have been released at: \url{https://github.com/MCG-NJU/FCOT}.



### A Novel CNN-based Method for Accurate Ship Detection in HR Optical Remote Sensing Images via Rotated Bounding Box
- **Arxiv ID**: http://arxiv.org/abs/2004.07124v2
- **DOI**: 10.1109/TGRS.2020.2995477
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.07124v2)
- **Published**: 2020-04-15 14:48:46+00:00
- **Updated**: 2020-05-08 03:22:37+00:00
- **Authors**: Linhao Li, Zhiqiang Zhou, Bo Wang, Lingjuan Miao, Hua Zong
- **Comment**: None
- **Journal**: [J]. IEEE Transactions on Geoscience and Remote Sensing, 2020,
  59(1): 686-699
- **Summary**: Currently, reliable and accurate ship detection in optical remote sensing images is still challenging. Even the state-of-the-art convolutional neural network (CNN) based methods cannot obtain very satisfactory results. To more accurately locate the ships in diverse orientations, some recent methods conduct the detection via the rotated bounding box. However, it further increases the difficulty of detection, because an additional variable of ship orientation must be accurately predicted in the algorithm. In this paper, a novel CNN-based ship detection method is proposed, by overcoming some common deficiencies of current CNN-based methods in ship detection. Specifically, to generate rotated region proposals, current methods have to predefine multi-oriented anchors, and predict all unknown variables together in one regression process, limiting the quality of overall prediction. By contrast, we are able to predict the orientation and other variables independently, and yet more effectively, with a novel dual-branch regression network, based on the observation that the ship targets are nearly rotation-invariant in remote sensing images. Next, a shape-adaptive pooling method is proposed, to overcome the limitation of typical regular ROI-pooling in extracting the features of the ships with various aspect ratios. Furthermore, we propose to incorporate multilevel features via the spatially-variant adaptive pooling. This novel approach, called multilevel adaptive pooling, leads to a compact feature representation more qualified for the simultaneous ship classification and localization. Finally, detailed ablation study performed on the proposed approaches is provided, along with some useful insights. Experimental results demonstrate the great superiority of the proposed method in ship detection.



### Extreme Consistency: Overcoming Annotation Scarcity and Domain Shifts
- **Arxiv ID**: http://arxiv.org/abs/2004.11966v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.11966v1)
- **Published**: 2020-04-15 15:32:01+00:00
- **Updated**: 2020-04-15 15:32:01+00:00
- **Authors**: Gaurav Fotedar, Nima Tajbakhsh, Shilpa Ananth, Xiaowei Ding
- **Comment**: submitted for peer-review on March 17
- **Journal**: None
- **Summary**: Supervised learning has proved effective for medical image analysis. However, it can utilize only the small labeled portion of data; it fails to leverage the large amounts of unlabeled data that is often available in medical image datasets. Supervised models are further handicapped by domain shifts, when the labeled dataset, despite being large enough, fails to cover different protocols or ethnicities. In this paper, we introduce \emph{extreme consistency}, which overcomes the above limitations, by maximally leveraging unlabeled data from the same or a different domain in a teacher-student semi-supervised paradigm. Extreme consistency is the process of sending an extreme transformation of a given image to the student network and then constraining its prediction to be consistent with the teacher network's prediction for the untransformed image. The extreme nature of our consistency loss distinguishes our method from related works that yield suboptimal performance by exercising only mild prediction consistency. Our method is 1) auto-didactic, as it requires no extra expert annotations; 2) versatile, as it handles both domain shift and limited annotation problems; 3) generic, as it is readily applicable to classification, segmentation, and detection tasks; and 4) simple to implement, as it requires no adversarial training. We evaluate our method for the tasks of lesion and retinal vessel segmentation in skin and fundus images. Our experiments demonstrate a significant performance gain over both modern supervised networks and recent semi-supervised models. This performance is attributed to the strong regularization enforced by extreme consistency, which enables the student network to learn how to handle extreme variants of both labeled and unlabeled images. This enhances the network's ability to tackle the inevitable same- and cross-domain data variability during inference.



### Residual-driven Fuzzy C-Means Clustering for Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2004.07160v2
- **DOI**: 10.1109/JAS.2020.1003420
- **Categories**: **eess.IV**, cs.CV, 62H30, I.4.6
- **Links**: [PDF](http://arxiv.org/pdf/2004.07160v2)
- **Published**: 2020-04-15 15:46:09+00:00
- **Updated**: 2020-04-20 14:15:04+00:00
- **Authors**: Cong Wang, Witold Pedrycz, ZhiWu Li, MengChu Zhou
- **Comment**: 14 pages, 13 figures, 6 tables
- **Journal**: IEEE/CAA Journal of Automatica Sinica, 2020
- **Summary**: Due to its inferior characteristics, an observed (noisy) image's direct use gives rise to poor segmentation results. Intuitively, using its noise-free image can favorably impact image segmentation. Hence, the accurate estimation of the residual between observed and noise-free images is an important task. To do so, we elaborate on residual-driven Fuzzy C-Means (FCM) for image segmentation, which is the first approach that realizes accurate residual estimation and leads noise-free image to participate in clustering. We propose a residual-driven FCM framework by integrating into FCM a residual-related fidelity term derived from the distribution of different types of noise. Built on this framework, we present a weighted $\ell_{2}$-norm fidelity term by weighting mixed noise distribution, thus resulting in a universal residual-driven FCM algorithm in presence of mixed or unknown noise. Besides, with the constraint of spatial information, the residual estimation becomes more reliable than that only considering an observed image itself. Supporting experiments on synthetic, medical, and real-world images are conducted. The results demonstrate the superior effectiveness and efficiency of the proposed algorithm over existing FCM-related algorithms.



### Bias in Multimodal AI: Testbed for Fair Automatic Recruitment
- **Arxiv ID**: http://arxiv.org/abs/2004.07173v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.07173v1)
- **Published**: 2020-04-15 15:58:05+00:00
- **Updated**: 2020-04-15 15:58:05+00:00
- **Authors**: Alejandro Peña, Ignacio Serna, Aythami Morales, Julian Fierrez
- **Comment**: None
- **Journal**: IEEE CVPR Workshop on Fair, Data Efficient and Trusted Computer
  Vision, Washington, Seattle, USA, 2020
- **Summary**: The presence of decision-making algorithms in society is rapidly increasing nowadays, while concerns about their transparency and the possibility of these algorithms becoming new sources of discrimination are arising. In fact, many relevant automated systems have been shown to make decisions based on sensitive information or discriminate certain social groups (e.g. certain biometric systems for person recognition). With the aim of studying how current multimodal algorithms based on heterogeneous sources of information are affected by sensitive elements and inner biases in the data, we propose a fictitious automated recruitment testbed: FairCVtest. We train automatic recruitment algorithms using a set of multimodal synthetic profiles consciously scored with gender and racial biases. FairCVtest shows the capacity of the Artificial Intelligence (AI) behind such recruitment tool to extract sensitive information from unstructured data, and exploit it in combination to data biases in undesirable (unfair) ways. Finally, we present a list of recent works developing techniques capable of removing sensitive information from the decision-making process of deep learning architectures. We have used one of these algorithms (SensitiveNets) to experiment discrimination-aware learning for the elimination of sensitive information in our multimodal AI framework. Our methodology and results show how to generate fairer AI-based tools in general, and in particular fairer automated recruitment systems.



### A Transductive Approach for Video Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2004.07193v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.07193v2)
- **Published**: 2020-04-15 16:39:36+00:00
- **Updated**: 2020-04-16 16:15:04+00:00
- **Authors**: Yizhuo Zhang, Zhirong Wu, Houwen Peng, Stephen Lin
- **Comment**: To Appear in CVPR 2020
- **Journal**: None
- **Summary**: Semi-supervised video object segmentation aims to separate a target object from a video sequence, given the mask in the first frame. Most of current prevailing methods utilize information from additional modules trained in other domains like optical flow and instance segmentation, and as a result they do not compete with other methods on common ground. To address this issue, we propose a simple yet strong transductive method, in which additional modules, datasets, and dedicated architectural designs are not needed. Our method takes a label propagation approach where pixel labels are passed forward based on feature similarity in an embedding space. Different from other propagation methods, ours diffuses temporal information in a holistic manner which take accounts of long-term object appearance. In addition, our method requires few additional computational overhead, and runs at a fast $\sim$37 fps speed. Our single model with a vanilla ResNet50 backbone achieves an overall score of 72.3 on the DAVIS 2017 validation set and 63.1 on the test set. This simple yet high performing and efficient method can serve as a solid baseline that facilitates future research. Code and models are available at \url{https://github.com/microsoft/transductive-vos.pytorch}.



### Defining Benchmarks for Continual Few-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2004.11967v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.11967v1)
- **Published**: 2020-04-15 16:41:01+00:00
- **Updated**: 2020-04-15 16:41:01+00:00
- **Authors**: Antreas Antoniou, Massimiliano Patacchiola, Mateusz Ochal, Amos Storkey
- **Comment**: None
- **Journal**: None
- **Summary**: Both few-shot and continual learning have seen substantial progress in the last years due to the introduction of proper benchmarks. That being said, the field has still to frame a suite of benchmarks for the highly desirable setting of continual few-shot learning, where the learner is presented a number of few-shot tasks, one after the other, and then asked to perform well on a validation set stemming from all previously seen tasks. Continual few-shot learning has a small computational footprint and is thus an excellent setting for efficient investigation and experimentation. In this paper we first define a theoretical framework for continual few-shot learning, taking into account recent literature, then we propose a range of flexible benchmarks that unify the evaluation criteria and allows exploring the problem from multiple perspectives. As part of the benchmark, we introduce a compact variant of ImageNet, called SlimageNet64, which retains all original 1000 classes but only contains 200 instances of each one (a total of 200K data-points) downscaled to 64 x 64 pixels. We provide baselines for the proposed benchmarks using a number of popular few-shot learning algorithms, as a result, exposing previously unknown strengths and weaknesses of those algorithms in continual and data-limited settings.



### Continual Learning for Anomaly Detection in Surveillance Videos
- **Arxiv ID**: http://arxiv.org/abs/2004.07941v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.07941v1)
- **Published**: 2020-04-15 16:41:20+00:00
- **Updated**: 2020-04-15 16:41:20+00:00
- **Authors**: Keval Doshi, Yasin Yilmaz
- **Comment**: accepted to CVPR 2020: Workshop on Continual Learning in Computer
  Vision. arXiv admin note: text overlap with arXiv:2004.02072
- **Journal**: None
- **Summary**: Anomaly detection in surveillance videos has been recently gaining attention. A challenging aspect of high-dimensional applications such as video surveillance is continual learning. While current state-of-the-art deep learning approaches perform well on existing public datasets, they fail to work in a continual learning framework due to computational and storage issues. Furthermore, online decision making is an important but mostly neglected factor in this domain. Motivated by these research gaps, we propose an online anomaly detection method for surveillance videos using transfer learning and continual learning, which in turn significantly reduces the training complexity and provides a mechanism for continually learning from recent data without suffering from catastrophic forgetting. Our proposed algorithm leverages the feature extraction power of neural network-based models for transfer learning, and the continual learning capability of statistical detection methods.



### Zero-Shot Compositional Policy Learning via Language Grounding
- **Arxiv ID**: http://arxiv.org/abs/2004.07200v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.07200v2)
- **Published**: 2020-04-15 16:58:19+00:00
- **Updated**: 2023-04-17 17:36:49+00:00
- **Authors**: Tianshi Cao, Jingkang Wang, Yining Zhang, Sivabalan Manivasagam
- **Comment**: Extended version for ICLRW-2020 (BeTR-RL) paper
- **Journal**: None
- **Summary**: Despite recent breakthroughs in reinforcement learning (RL) and imitation learning (IL), existing algorithms fail to generalize beyond the training environments. In reality, humans can adapt to new tasks quickly by leveraging prior knowledge about the world such as language descriptions. To facilitate the research on language-guided agents with domain adaption, we propose a novel zero-shot compositional policy learning task, where the environments are characterized as a composition of different attributes. Since there are no public environments supporting this study, we introduce a new research platform BabyAI++ in which the dynamics of environments are disentangled from visual appearance. At each episode, BabyAI++ provides varied vision-dynamics combinations along with corresponding descriptive texts. To evaluate the adaption capability of learned agents, a set of vision-dynamics pairings are held-out for testing on BabyAI++. Unsurprisingly, we find that current language-guided RL/IL techniques overfit to the training environments and suffer from a huge performance drop when facing unseen combinations. In response, we propose a multi-modal fusion method with an attention mechanism to perform visual language-grounding. Extensive experiments show strong evidence that language grounding is able to improve the generalization of agents across environments with varied dynamics.



### Using Player's Body-Orientation to Model Pass Feasibility in Soccer
- **Arxiv ID**: http://arxiv.org/abs/2004.07209v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.07209v1)
- **Published**: 2020-04-15 17:09:51+00:00
- **Updated**: 2020-04-15 17:09:51+00:00
- **Authors**: Adrià Arbués-Sangüesa, Adrián Martín, Javier Fernández, Coloma Ballester, Gloria Haro
- **Comment**: Accepted at the Computer Vision in Sports Workshop at CVPR 2020
- **Journal**: None
- **Summary**: Given a monocular video of a soccer match, this paper presents a computational model to estimate the most feasible pass at any given time. The method leverages offensive player's orientation (plus their location) and opponents' spatial configuration to compute the feasibility of pass events within players of the same team. Orientation data is gathered from body pose estimations that are properly projected onto the 2D game field; moreover, a geometrical solution is provided, through the definition of a feasibility measure, to determine which players are better oriented towards each other. Once analyzed more than 6000 pass events, results show that, by including orientation as a feasibility measure, a robust computational model can be built, reaching more than 0.7 Top-3 accuracy. Finally, the combination of the orientation feasibility measure with the recently introduced Expected Possession Value metric is studied; promising results are obtained, thus showing that existing models can be refined by using orientation as a key feature. These models could help both coaches and analysts to have a better understanding of the game and to improve the players' decision-making process.



### On Box-Cox Transformation for Image Normality and Pattern Classification
- **Arxiv ID**: http://arxiv.org/abs/2004.07210v3
- **DOI**: 10.1109/ACCESS.2020.3018874
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.07210v3)
- **Published**: 2020-04-15 17:10:18+00:00
- **Updated**: 2020-10-01 13:13:17+00:00
- **Authors**: Abbas Cheddad
- **Comment**: The paper has 4 Tables and 6 Figures
- **Journal**: IEEE Access, vol. 8, pp. 154975-154983, 2020
- **Summary**: A unique member of the power transformation family is known as the Box-Cox transformation. The latter can be seen as a mathematical operation that leads to finding the optimum lambda ({\lambda}) value that maximizes the log-likelihood function to transform a data to a normal distribution and to reduce heteroscedasticity. In data analytics, a normality assumption underlies a variety of statistical test models. This technique, however, is best known in statistical analysis to handle one-dimensional data. Herein, this paper revolves around the utility of such a tool as a pre-processing step to transform two-dimensional data, namely, digital images and to study its effect. Moreover, to reduce time complexity, it suffices to estimate the parameter lambda in real-time for large two-dimensional matrices by merely considering their probability density function as a statistical inference of the underlying data distribution. We compare the effect of this light-weight Box-Cox transformation with well-established state-of-the-art low light image enhancement techniques. We also demonstrate the effectiveness of our approach through several test-bed data sets for generic improvement of visual appearance of images and for ameliorating the performance of a colour pattern classification algorithm as an example application. Results with and without the proposed approach, are compared using the AlexNet (transfer deep learning) pretrained model. To the best of our knowledge, this is the first time that the Box-Cox transformation is extended to digital images by exploiting histogram transformation.



### Learning visual policies for building 3D shape categories
- **Arxiv ID**: http://arxiv.org/abs/2004.07950v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.07950v2)
- **Published**: 2020-04-15 17:29:10+00:00
- **Updated**: 2020-09-30 22:24:32+00:00
- **Authors**: Alexander Pashevich, Igor Kalevatykh, Ivan Laptev, Cordelia Schmid
- **Comment**: IROS 2020
- **Journal**: None
- **Summary**: Manipulation and assembly tasks require non-trivial planning of actions depending on the environment and the final goal. Previous work in this domain often assembles particular instances of objects from known sets of primitives. In contrast, we aim to handle varying sets of primitives and to construct different objects of a shape category. Given a single object instance of a category, e.g. an arch, and a binary shape classifier, we learn a visual policy to assemble other instances of the same category. In particular, we propose a disassembly procedure and learn a state policy that discovers new object instances and their assembly plans in state space. We then render simulated states in the observation space and learn a heatmap representation to predict alternative actions from a given input image. To validate our approach, we first demonstrate its efficiency for building object categories in state space. We then show the success of our visual policies for building arches from different primitives. Moreover, we demonstrate (i) the reactive ability of our method to re-assemble objects using additional primitives and (ii) the robust performance of our policy for unseen primitives resembling building blocks used during training. Our visual assembly policies are trained with no real images and reach up to 95% success rate when evaluated on a real robot.



### A Hybrid Method for Training Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2005.04153v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2005.04153v1)
- **Published**: 2020-04-15 17:52:48+00:00
- **Updated**: 2020-04-15 17:52:48+00:00
- **Authors**: Vasco Lopes, Paulo Fazendeiro
- **Comment**: 1 figure, 6 pages
- **Journal**: None
- **Summary**: Artificial Intelligence algorithms have been steadily increasing in popularity and usage. Deep Learning, allows neural networks to be trained using huge datasets and also removes the need for human extracted features, as it automates the feature learning process. In the hearth of training deep neural networks, such as Convolutional Neural Networks, we find backpropagation, that by computing the gradient of the loss function with respect to the weights of the network for a given input, it allows the weights of the network to be adjusted to better perform in the given task. In this paper, we propose a hybrid method that uses both backpropagation and evolutionary strategies to train Convolutional Neural Networks, where the evolutionary strategies are used to help to avoid local minimas and fine-tune the weights, so that the network achieves higher accuracy results. We show that the proposed hybrid method is capable of improving upon regular training in the task of image classification in CIFAR-10, where a VGG16 model was used and the final test results increased 0.61%, in average, when compared to using only backpropagation.



### Learning Furniture Compatibility with Graph Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2004.07268v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.07268v1)
- **Published**: 2020-04-15 18:04:06+00:00
- **Updated**: 2020-04-15 18:04:06+00:00
- **Authors**: Luisa F. Polania, Mauricio Flores, Yiran Li, Matthew Nokleby
- **Comment**: Accepted for publication at CVPR Workshops
- **Journal**: None
- **Summary**: We propose a graph neural network (GNN) approach to the problem of predicting the stylistic compatibility of a set of furniture items from images. While most existing results are based on siamese networks which evaluate pairwise compatibility between items, the proposed GNN architecture exploits relational information among groups of items. We present two GNN models, both of which comprise a deep CNN that extracts a feature representation for each image, a gated recurrent unit (GRU) network that models interactions between the furniture items in a set, and an aggregation function that calculates the compatibility score. In the first model, a generalized contrastive loss function that promotes the generation of clustered embeddings for items belonging to the same furniture set is introduced. Also, in the first model, the edge function between nodes in the GRU and the aggregation function are fixed in order to limit model complexity and allow training on smaller datasets; in the second model, the edge function and aggregation function are learned directly from the data. We demonstrate state-of-the art accuracy for compatibility prediction and "fill in the blank" tasks on the Bonn and Singapore furniture datasets. We further introduce a new dataset, called the Target Furniture Collections dataset, which contains over 6000 furniture items that have been hand-curated by stylists to make up 1632 compatible sets. We also demonstrate superior prediction accuracy on this dataset.



### ESResNet: Environmental Sound Classification Based on Visual Domain Models
- **Arxiv ID**: http://arxiv.org/abs/2004.07301v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2004.07301v1)
- **Published**: 2020-04-15 19:07:55+00:00
- **Updated**: 2020-04-15 19:07:55+00:00
- **Authors**: Andrey Guzhov, Federico Raue, Jörn Hees, Andreas Dengel
- **Comment**: 8 pages, 4 figures; submitted to ICPR 2020
- **Journal**: None
- **Summary**: Environmental Sound Classification (ESC) is an active research area in the audio domain and has seen a lot of progress in the past years. However, many of the existing approaches achieve high accuracy by relying on domain-specific features and architectures, making it harder to benefit from advances in other fields (e.g., the image domain). Additionally, some of the past successes have been attributed to a discrepancy of how results are evaluated (i.e., on unofficial splits of the UrbanSound8K (US8K) dataset), distorting the overall progression of the field.   The contribution of this paper is twofold. First, we present a model that is inherently compatible with mono and stereo sound inputs. Our model is based on simple log-power Short-Time Fourier Transform (STFT) spectrograms and combines them with several well-known approaches from the image domain (i.e., ResNet, Siamese-like networks and attention). We investigate the influence of cross-domain pre-training, architectural changes, and evaluate our model on standard datasets. We find that our model out-performs all previously known approaches in a fair comparison by achieving accuracies of 97.0 % (ESC-10), 91.5 % (ESC-50) and 84.2 % / 85.4 % (US8K mono / stereo).   Second, we provide a comprehensive overview of the actual state of the field, by differentiating several previously reported results on the US8K dataset between official or unofficial splits. For better reproducibility, our code (including any re-implementations) is made available.



### RescueNet: Joint Building Segmentation and Damage Assessment from Satellite Imagery
- **Arxiv ID**: http://arxiv.org/abs/2004.07312v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, I.4.6
- **Links**: [PDF](http://arxiv.org/pdf/2004.07312v1)
- **Published**: 2020-04-15 19:52:09+00:00
- **Updated**: 2020-04-15 19:52:09+00:00
- **Authors**: Rohit Gupta, Mubarak Shah
- **Comment**: 7 pages, 3 figures, submitted to ICPR
- **Journal**: None
- **Summary**: Accurate and fine-grained information about the extent of damage to buildings is essential for directing Humanitarian Aid and Disaster Response (HADR) operations in the immediate aftermath of any natural calamity. In recent years, satellite and UAV (drone) imagery has been used for this purpose, sometimes aided by computer vision algorithms. Existing Computer Vision approaches for building damage assessment typically rely on a two stage approach, consisting of building detection using an object detection model, followed by damage assessment through classification of the detected building tiles. These multi-stage methods are not end-to-end trainable, and suffer from poor overall results. We propose RescueNet, a unified model that can simultaneously segment buildings and assess the damage levels to individual buildings and can be trained end-toend. In order to to model the composite nature of this problem, we propose a novel localization aware loss function, which consists of a Binary Cross Entropy loss for building segmentation, and a foreground only selective Categorical Cross-Entropy loss for damage classification, and show significant improvement over the widely used Cross-Entropy loss. RescueNet is tested on the large scale and diverse xBD dataset and achieves significantly better building segmentation and damage classification performance than previous methods and achieves generalization across varied geographical regions and disaster types.



### An Evaluation of DNN Architectures for Page Segmentation of Historical Newspapers
- **Arxiv ID**: http://arxiv.org/abs/2004.07317v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4.6
- **Links**: [PDF](http://arxiv.org/pdf/2004.07317v1)
- **Published**: 2020-04-15 20:05:54+00:00
- **Updated**: 2020-04-15 20:05:54+00:00
- **Authors**: Bernhard Liebl, Manuel Burghardt
- **Comment**: Evaluation of deep neural networks for the segmentation of pages of
  historical newspapers; 21 pages total (incl. references and appendix), 7
  figures, 5 tables
- **Journal**: None
- **Summary**: One important and particularly challenging step in the optical character recognition (OCR) of historical documents with complex layouts, such as newspapers, is the separation of text from non-text content (e.g. page borders or illustrations). This step is commonly referred to as page segmentation. While various rule-based algorithms have been proposed, the applicability of Deep Neural Networks (DNNs) for this task recently has gained a lot of attention. In this paper, we perform a systematic evaluation of 11 different published DNN backbone architectures and 9 different tiling and scaling configurations for separating text, tables or table column lines. We also show the influence of the number of labels and the number of training pages on the segmentation quality, which we measure using the Matthews Correlation Coefficient. Our results show that (depending on the task) Inception-ResNet-v2 and EfficientNet backbones work best, vertical tiling is generally preferable to other tiling approaches, and training data that comprises 30 to 40 pages will be sufficient most of the time.



### An Adaptive Intelligence Algorithm for Undersampled Knee MRI Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2004.07339v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.07339v2)
- **Published**: 2020-04-15 20:59:56+00:00
- **Updated**: 2020-10-27 15:19:33+00:00
- **Authors**: Nicola Pezzotti, Sahar Yousefi, Mohamed S. Elmahdy, Jeroen van Gemert, Christophe Schülke, Mariya Doneva, Tim Nielsen, Sergey Kastryulin, Boudewijn P. F. Lelieveldt, Matthias J. P. van Osch, Elwin de Weerdt, Marius Staring
- **Comment**: None
- **Journal**: None
- **Summary**: Adaptive intelligence aims at empowering machine learning techniques with the additional use of domain knowledge. In this work, we present the application of adaptive intelligence to accelerate MR acquisition. Starting from undersampled k-space data, an iterative learning-based reconstruction scheme inspired by compressed sensing theory is used to reconstruct the images. We adopt deep neural networks to refine and correct prior reconstruction assumptions given the training data. The network was trained and tested on a knee MRI dataset from the 2019 fastMRI challenge organized by Facebook AI Research and NYU Langone Health. All submissions to the challenge were initially ranked based on similarity with a known groundtruth, after which the top 4 submissions were evaluated radiologically. Our method was evaluated by the fastMRI organizers on an independent challenge dataset. It ranked #1, shared #1, and #3 on respectively the 8x accelerated multi-coil, the 4x multi-coil, and the 4x single-coil track. This demonstrates the superior performance and wide applicability of the method.



### Neuromorphic Event-Based Slip Detection and suppression in Robotic Grasping and Manipulation
- **Arxiv ID**: http://arxiv.org/abs/2004.07386v1
- **DOI**: 10.1109/ACCESS.2020.3017738
- **Categories**: **cs.RO**, cs.CV, I.2, I.2.9, I.2.10, I.2.1
- **Links**: [PDF](http://arxiv.org/pdf/2004.07386v1)
- **Published**: 2020-04-15 23:12:30+00:00
- **Updated**: 2020-04-15 23:12:30+00:00
- **Authors**: Rajkumar Muthusamy, Xiaoqian Huang, Yahya Zweiri, Lakmal Seneviratne, Dongming Gan
- **Comment**: 18 pages, 14 figures
- **Journal**: IEEE Access,Volume: 8, 19 August 2020
- **Summary**: Slip detection is essential for robots to make robust grasping and fine manipulation. In this paper, a novel dynamic vision-based finger system for slip detection and suppression is proposed. We also present a baseline and feature based approach to detect object slips under illumination and vibration uncertainty. A threshold method is devised to autonomously sample noise in real-time to improve slip detection. Moreover, a fuzzy based suppression strategy using incipient slip feedback is proposed for regulating the grip force. A comprehensive experimental study of our proposed approaches under uncertainty and system for high-performance precision manipulation are presented. We also propose a slip metric to evaluate such performance quantitatively. Results indicate that the system can effectively detect incipient slip events at a sampling rate of 2kHz ($\Delta t = 500\mu s$) and suppress them before a gross slip occurs. The event-based approach holds promises to high precision manipulation task requirement in industrial manufacturing and household services.



### Joint Supervised and Self-Supervised Learning for 3D Real-World Challenges
- **Arxiv ID**: http://arxiv.org/abs/2004.07392v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.07392v1)
- **Published**: 2020-04-15 23:34:03+00:00
- **Updated**: 2020-04-15 23:34:03+00:00
- **Authors**: Antonio Alliegro, Davide Boscaini, Tatiana Tommasi
- **Comment**: None
- **Journal**: None
- **Summary**: Point cloud processing and 3D shape understanding are very challenging tasks for which deep learning techniques have demonstrated great potentials. Still further progresses are essential to allow artificial intelligent agents to interact with the real world, where the amount of annotated data may be limited and integrating new sources of knowledge becomes crucial to support autonomous learning. Here we consider several possible scenarios involving synthetic and real-world point clouds where supervised learning fails due to data scarcity and large domain gaps. We propose to enrich standard feature representations by leveraging self-supervision through a multi-task model that can solve a 3D puzzle while learning the main task of shape classification or part segmentation. An extensive analysis investigating few-shot, transfer learning and cross-domain settings shows the effectiveness of our approach with state-of-the-art results for 3D shape classification and part segmentation.



### Generalized Shortest Path-based Superpixels for Accurate Segmentation of Spherical Images
- **Arxiv ID**: http://arxiv.org/abs/2004.07394v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.07394v3)
- **Published**: 2020-04-15 23:41:32+00:00
- **Updated**: 2020-07-14 13:00:43+00:00
- **Authors**: Rémi Giraud, Rodrigo Borba Pinheiro, Yannick Berthoumieu
- **Comment**: None
- **Journal**: International Conference on Pattern Recognition (ICPR 2020)
- **Summary**: Most of existing superpixel methods are designed to segment standard planar images as pre-processing for computer vision pipelines. Nevertheless, the increasing number of applications based on wide angle capture devices, mainly generating 360{\deg} spherical images, have enforced the need for dedicated superpixel approaches. In this paper, we introduce a new superpixel method for spherical images called SphSPS (for Spherical Shortest Path-based Superpixels). Our approach respects the spherical geometry and generalizes the notion of shortest path between a pixel and a superpixel center on the 3D spherical acquisition space. We show that the feature information on such path can be efficiently integrated into our clustering framework and jointly improves the respect of object contours and the shape regularity. To relevantly evaluate this last aspect in the spherical space, we also generalize a planar global regularity metric. Finally, the proposed SphSPS method obtains significantly better performance than both planar and recent spherical superpixel approaches on the reference 360{\deg} spherical panorama segmentation dataset.



### Neuromorphic Eye-in-Hand Visual Servoing
- **Arxiv ID**: http://arxiv.org/abs/2004.07398v1
- **DOI**: 10.1109/ACCESS.2021.3071261
- **Categories**: **cs.RO**, cs.CV, cs.SY, eess.SY, I.2; I.2.9; I.2.8
- **Links**: [PDF](http://arxiv.org/pdf/2004.07398v1)
- **Published**: 2020-04-15 23:57:54+00:00
- **Updated**: 2020-04-15 23:57:54+00:00
- **Authors**: Rajkumar Muthusamy, Abdulla Ayyad, Mohamad Halwani, Yahya Zweiri, Dongming Gan, Lakmal Seneviratne
- **Comment**: 8 pages, 10 figures
- **Journal**: IEEE Access, Volume: 9, 05 April 2021
- **Summary**: Robotic vision plays a major role in factory automation to service robot applications. However, the traditional use of frame-based camera sets a limitation on continuous visual feedback due to their low sampling rate and redundant data in real-time image processing, especially in the case of high-speed tasks. Event cameras give human-like vision capabilities such as observing the dynamic changes asynchronously at a high temporal resolution ($1\mu s$) with low latency and wide dynamic range.   In this paper, we present a visual servoing method using an event camera and a switching control strategy to explore, reach and grasp to achieve a manipulation task. We devise three surface layers of active events to directly process stream of events from relative motion. A purely event based approach is adopted to extract corner features, localize them robustly using heat maps and generate virtual features for tracking and alignment. Based on the visual feedback, the motion of the robot is controlled to make the temporal upcoming event features converge to the desired event in spatio-temporal space. The controller switches its strategy based on the sequence of operation to establish a stable grasp. The event based visual servoing (EVBS) method is validated experimentally using a commercial robot manipulator in an eye-in-hand configuration. Experiments prove the effectiveness of the EBVS method to track and grasp objects of different shapes without the need for re-tuning.



