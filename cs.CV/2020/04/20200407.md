# Arxiv Papers in cs.CV on 2020-04-07
### Manifold-driven Attention Maps for Weakly Supervised Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2004.03046v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.03046v1)
- **Published**: 2020-04-07 00:03:28+00:00
- **Updated**: 2020-04-07 00:03:28+00:00
- **Authors**: Sukesh Adiga V, Jose Dolz, Herve Lombaert
- **Comment**: Paper is submitted to MICCAI2020
- **Journal**: None
- **Summary**: Segmentation using deep learning has shown promising directions in medical imaging as it aids in the analysis and diagnosis of diseases. Nevertheless, a main drawback of deep models is that they require a large amount of pixel-level labels, which are laborious and expensive to obtain. To mitigate this problem, weakly supervised learning has emerged as an efficient alternative, which employs image-level labels, scribbles, points, or bounding boxes as supervision. Among these, image-level labels are easier to obtain. However, since this type of annotation only contains object category information, the segmentation task under this learning paradigm is a challenging problem. To address this issue, visual salient regions derived from trained classification networks are typically used. Despite their success to identify important regions on classification tasks, these saliency regions only focus on the most discriminant areas of an image, limiting their use in semantic segmentation. In this work, we propose a manifold driven attention-based network to enhance visual salient regions, thereby improving segmentation accuracy in a weakly supervised setting. Our method generates superior attention maps directly during inference without the need of extra computations. We evaluate the benefits of our approach in the task of segmentation using a public benchmark on skin lesion images. Results demonstrate that our method outperforms the state-of-the-art GradCAM by a margin of ~22% in terms of Dice score.



### Depth Sensing Beyond LiDAR Range
- **Arxiv ID**: http://arxiv.org/abs/2004.03048v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.03048v1)
- **Published**: 2020-04-07 00:09:51+00:00
- **Updated**: 2020-04-07 00:09:51+00:00
- **Authors**: Kai Zhang, Jiaxin Xie, Noah Snavely, Qifeng Chen
- **Comment**: Accepted to CVPR 2020
- **Journal**: None
- **Summary**: Depth sensing is a critical component of autonomous driving technologies, but today's LiDAR- or stereo camera-based solutions have limited range. We seek to increase the maximum range of self-driving vehicles' depth perception modules for the sake of better safety. To that end, we propose a novel three-camera system that utilizes small field of view cameras. Our system, along with our novel algorithm for computing metric depth, does not require full pre-calibration and can output dense depth maps with practically acceptable accuracy for scenes and objects at long distances not well covered by most commercial LiDARs.



### Coarse-to-Fine Gaze Redirection with Numerical and Pictorial Guidance
- **Arxiv ID**: http://arxiv.org/abs/2004.03064v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.03064v4)
- **Published**: 2020-04-07 01:17:27+00:00
- **Updated**: 2020-11-26 06:17:15+00:00
- **Authors**: Jingjing Chen, Jichao Zhang, Enver Sangineto, Jiayuan Fan, Tao Chen, Nicu Sebe
- **Comment**: 12 pages, accepted by WACV 2021
- **Journal**: None
- **Summary**: Gaze redirection aims at manipulating the gaze of a given face image with respect to a desired direction (i.e., a reference angle) and it can be applied to many real life scenarios, such as video-conferencing or taking group photos. However, previous work on this topic mainly suffers of two limitations: (1) Low-quality image generation and (2) Low redirection precision. In this paper, we propose to alleviate these problems by means of a novel gaze redirection framework which exploits both a numerical and a pictorial direction guidance, jointly with a coarse-to-fine learning strategy. Specifically, the coarse branch learns the spatial transformation which warps input image according to desired gaze. On the other hand, the fine-grained branch consists of a generator network with conditional residual image learning and a multi-task discriminator. This second branch reduces the gap between the previously warped image and the ground-truth image and recovers finer texture details. Moreover, we propose a numerical and pictorial guidance module~(NPG) which uses a pictorial gazemap description and numerical angles as an extra guide to further improve the precision of gaze redirection. Extensive experiments on a benchmark dataset show that the proposed method outperforms the state-of-the-art approaches in terms of both image quality and redirection precision. The code is available at https://github.com/jingjingchen777/CFGR



### A Method for Curation of Web-Scraped Face Image Datasets
- **Arxiv ID**: http://arxiv.org/abs/2004.03074v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.03074v1)
- **Published**: 2020-04-07 01:57:32+00:00
- **Updated**: 2020-04-07 01:57:32+00:00
- **Authors**: Kai Zhang, VÃ­tor Albiero, Kevin W. Bowyer
- **Comment**: This paper will appear at IWBF 2020
- **Journal**: None
- **Summary**: Web-scraped, in-the-wild datasets have become the norm in face recognition research. The numbers of subjects and images acquired in web-scraped datasets are usually very large, with number of images on the millions scale. A variety of issues occur when collecting a dataset in-the-wild, including images with the wrong identity label, duplicate images, duplicate subjects and variation in quality. With the number of images being in the millions, a manual cleaning procedure is not feasible. But fully automated methods used to date result in a less-than-ideal level of clean dataset. We propose a semi-automated method, where the goal is to have a clean dataset for testing face recognition methods, with similar quality across men and women, to support comparison of accuracy across gender. Our approach removes near-duplicate images, merges duplicate subjects, corrects mislabeled images, and removes images outside a defined range of pose and quality. We conduct the curation on the Asian Face Dataset (AFD) and VGGFace2 test dataset. The experiments show that a state-of-the-art method achieves a much higher accuracy on the datasets after they are curated. Finally, we release our cleaned versions of both datasets to the research community.



### End-to-End Pseudo-LiDAR for Image-Based 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2004.03080v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.03080v2)
- **Published**: 2020-04-07 02:18:38+00:00
- **Updated**: 2020-05-14 14:39:42+00:00
- **Authors**: Rui Qian, Divyansh Garg, Yan Wang, Yurong You, Serge Belongie, Bharath Hariharan, Mark Campbell, Kilian Q. Weinberger, Wei-Lun Chao
- **Comment**: Accepted to 2020 Conference on Computer Vision and Pattern
  Recognition (CVPR 2020)
- **Journal**: None
- **Summary**: Reliable and accurate 3D object detection is a necessity for safe autonomous driving. Although LiDAR sensors can provide accurate 3D point cloud estimates of the environment, they are also prohibitively expensive for many settings. Recently, the introduction of pseudo-LiDAR (PL) has led to a drastic reduction in the accuracy gap between methods based on LiDAR sensors and those based on cheap stereo cameras. PL combines state-of-the-art deep neural networks for 3D depth estimation with those for 3D object detection by converting 2D depth map outputs to 3D point cloud inputs. However, so far these two networks have to be trained separately. In this paper, we introduce a new framework based on differentiable Change of Representation (CoR) modules that allow the entire PL pipeline to be trained end-to-end. The resulting framework is compatible with most state-of-the-art networks for both tasks and in combination with PointRCNN improves over PL consistently across all benchmarks -- yielding the highest entry on the KITTI image-based 3D object detection leaderboard at the time of submission. Our code will be made available at https://github.com/mileyan/pseudo-LiDAR_e2e.



### Multimodal Image Synthesis with Conditional Implicit Maximum Likelihood Estimation
- **Arxiv ID**: http://arxiv.org/abs/2004.03590v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG, cs.NE, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.03590v1)
- **Published**: 2020-04-07 03:06:55+00:00
- **Updated**: 2020-04-07 03:06:55+00:00
- **Authors**: Ke Li, Shichong Peng, Tianhao Zhang, Jitendra Malik
- **Comment**: To appear in International Journal of Computer Vision (IJCV). arXiv
  admin note: text overlap with arXiv:1811.12373
- **Journal**: None
- **Summary**: Many tasks in computer vision and graphics fall within the framework of conditional image synthesis. In recent years, generative adversarial nets (GANs) have delivered impressive advances in quality of synthesized images. However, it remains a challenge to generate both diverse and plausible images for the same input, due to the problem of mode collapse. In this paper, we develop a new generic multimodal conditional image synthesis method based on Implicit Maximum Likelihood Estimation (IMLE) and demonstrate improved multimodal image synthesis performance on two tasks, single image super-resolution and image synthesis from scene layouts. We make our implementation publicly available.



### Generalized Label Enhancement with Sample Correlations
- **Arxiv ID**: http://arxiv.org/abs/2004.03104v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.03104v3)
- **Published**: 2020-04-07 03:32:36+00:00
- **Updated**: 2021-04-12 02:47:35+00:00
- **Authors**: Qinghai Zheng, Jihua Zhu, Haoyu Tang, Xinyuan Liu, Zhongyu Li, Huimin Lu
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, label distribution learning (LDL) has drawn much attention in machine learning, where LDL model is learned from labelel instances. Different from single-label and multi-label annotations, label distributions describe the instance by multiple labels with different intensities and accommodate to more general scenes. Since most existing machine learning datasets merely provide logical labels, label distributions are unavailable in many real-world applications. To handle this problem, we propose two novel label enhancement methods, i.e., Label Enhancement with Sample Correlations (LESC) and generalized Label Enhancement with Sample Correlations (gLESC). More specifically, LESC employs a low-rank representation of samples in the feature space, and gLESC leverages a tensor multi-rank minimization to further investigate the sample correlations in both the feature space and label space. Benefitting from the sample correlations, the proposed methods can boost the performance of label enhancement. Extensive experiments on 14 benchmark datasets demonstrate the effectiveness and superiority of our methods.



### Generative Adversarial Zero-shot Learning via Knowledge Graphs
- **Arxiv ID**: http://arxiv.org/abs/2004.03109v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.03109v1)
- **Published**: 2020-04-07 03:55:26+00:00
- **Updated**: 2020-04-07 03:55:26+00:00
- **Authors**: Yuxia Geng, Jiaoyan Chen, Zhuo Chen, Zhiquan Ye, Zonggang Yuan, Yantao Jia, Huajun Chen
- **Comment**: under review
- **Journal**: None
- **Summary**: Zero-shot learning (ZSL) is to handle the prediction of those unseen classes that have no labeled training data. Recently, generative methods like Generative Adversarial Networks (GANs) are being widely investigated for ZSL due to their high accuracy, generalization capability and so on. However, the side information of classes used now is limited to text descriptions and attribute annotations, which are in short of semantics of the classes. In this paper, we introduce a new generative ZSL method named KG-GAN by incorporating rich semantics in a knowledge graph (KG) into GANs. Specifically, we build upon Graph Neural Networks and encode KG from two views: class view and attribute view considering the different semantics of KG. With well-learned semantic embeddings for each node (representing a visual category), we leverage GANs to synthesize compelling visual features for unseen classes. According to our evaluation with multiple image classification datasets, KG-GAN can achieve better performance than the state-of-the-art baselines.



### Toward Fine-grained Facial Expression Manipulation
- **Arxiv ID**: http://arxiv.org/abs/2004.03132v2
- **DOI**: 10.1007/978-3-030-58604-1_3
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.03132v2)
- **Published**: 2020-04-07 05:14:15+00:00
- **Updated**: 2020-12-04 08:19:55+00:00
- **Authors**: Jun Ling, Han Xue, Li Song, Shuhui Yang, Rong Xie, Xiao Gu
- **Comment**: None
- **Journal**: None
- **Summary**: Facial expression manipulation aims at editing facial expression with a given condition. Previous methods edit an input image under the guidance of a discrete emotion label or absolute condition (e.g., facial action units) to possess the desired expression. However, these methods either suffer from changing condition-irrelevant regions or are inefficient for fine-grained editing. In this study, we take these two objectives into consideration and propose a novel method. First, we replace continuous absolute condition with relative condition, specifically, relative action units. With relative action units, the generator learns to only transform regions of interest which are specified by non-zero-valued relative AUs. Second, our generator is built on U-Net but strengthened by Multi-Scale Feature Fusion (MSF) mechanism for high-quality expression editing purposes. Extensive experiments on both quantitative and qualitative evaluation demonstrate the improvements of our proposed approach compared to the state-of-the-art expression editing methods. Code is available at \url{https://github.com/junleen/Expression-manipulator}.



### Human Motion Transfer from Poses in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2004.03142v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.03142v1)
- **Published**: 2020-04-07 05:59:53+00:00
- **Updated**: 2020-04-07 05:59:53+00:00
- **Authors**: Jian Ren, Menglei Chai, Sergey Tulyakov, Chen Fang, Xiaohui Shen, Jianchao Yang
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we tackle the problem of human motion transfer, where we synthesize novel motion video for a target person that imitates the movement from a reference video. It is a video-to-video translation task in which the estimated poses are used to bridge two domains. Despite substantial progress on the topic, there exist several problems with the previous methods. First, there is a domain gap between training and testing pose sequences--the model is tested on poses it has not seen during training, such as difficult dancing moves. Furthermore, pose detection errors are inevitable, making the job of the generator harder. Finally, generating realistic pixels from sparse poses is challenging in a single step. To address these challenges, we introduce a novel pose-to-video translation framework for generating high-quality videos that are temporally coherent even for in-the-wild pose sequences unseen during training. We propose a pose augmentation method to minimize the training-test gap, a unified paired and unpaired learning strategy to improve the robustness to detection errors, and two-stage network architecture to achieve superior texture quality. To further boost research on the topic, we build two human motion datasets. Finally, we show the superiority of our approach over the state-of-the-art studies through extensive experiments and evaluations on different datasets.



### Predicting Camera Viewpoint Improves Cross-dataset Generalization for 3D Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2004.03143v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.03143v1)
- **Published**: 2020-04-07 06:06:20+00:00
- **Updated**: 2020-04-07 06:06:20+00:00
- **Authors**: Zhe Wang, Daeyun Shin, Charless C. Fowlkes
- **Comment**: http://wangzheallen.github.io/cross-dataset-generalization
- **Journal**: None
- **Summary**: Monocular estimation of 3d human pose has attracted increased attention with the availability of large ground-truth motion capture datasets. However, the diversity of training data available is limited and it is not clear to what extent methods generalize outside the specific datasets they are trained on. In this work we carry out a systematic study of the diversity and biases present in specific datasets and its effect on cross-dataset generalization across a compendium of 5 pose datasets. We specifically focus on systematic differences in the distribution of camera viewpoints relative to a body-centered coordinate frame. Based on this observation, we propose an auxiliary task of predicting the camera viewpoint in addition to pose. We find that models trained to jointly predict viewpoint and pose systematically show significantly improved cross-dataset generalization.



### Plug-and-play ISTA converges with kernel denoisers
- **Arxiv ID**: http://arxiv.org/abs/2004.03145v2
- **DOI**: 10.1109/LSP.2020.2986643
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.03145v2)
- **Published**: 2020-04-07 06:25:34+00:00
- **Updated**: 2020-04-14 14:24:53+00:00
- **Authors**: Ruturaj G. Gavaskar, Kunal N. Chaudhury
- **Comment**: 5 pages, Accepted to IEEE Signal Processing Letters
- **Journal**: None
- **Summary**: Plug-and-play (PnP) method is a recent paradigm for image regularization, where the proximal operator (associated with some given regularizer) in an iterative algorithm is replaced with a powerful denoiser. Algorithmically, this involves repeated inversion (of the forward model) and denoising until convergence. Remarkably, PnP regularization produces promising results for several restoration applications. However, a fundamental question in this regard is the theoretical convergence of the PnP iterations, since the algorithm is not strictly derived from an optimization framework. This question has been investigated in recent works, but there are still many unresolved problems. For example, it is not known if convergence can be guaranteed if we use generic kernel denoisers (e.g. nonlocal means) within the ISTA framework (PnP-ISTA). We prove that, under reasonable assumptions, fixed-point convergence of PnP-ISTA is indeed guaranteed for linear inverse problems such as deblurring, inpainting and superresolution (the assumptions are verifiable for inpainting). We compare our theoretical findings with existing results, validate them numerically, and explain their practical relevance.



### Deep Attentive Generative Adversarial Network for Photo-Realistic Image De-Quantization
- **Arxiv ID**: http://arxiv.org/abs/2004.03150v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.03150v1)
- **Published**: 2020-04-07 06:45:01+00:00
- **Updated**: 2020-04-07 06:45:01+00:00
- **Authors**: Yang Zhang, Changhui Hu, Xiaobo Lu
- **Comment**: None
- **Journal**: None
- **Summary**: Most of current display devices are with eight or higher bit-depth. However, the quality of most multimedia tools cannot achieve this bit-depth standard for the generating images. De-quantization can improve the visual quality of low bit-depth image to display on high bit-depth screen. This paper proposes DAGAN algorithm to perform super-resolution on image intensity resolution, which is orthogonal to the spatial resolution, realizing photo-realistic de-quantization via an end-to-end learning pattern. Until now, this is the first attempt to apply Generative Adversarial Network (GAN) framework for image de-quantization. Specifically, we propose the Dense Residual Self-attention (DenseResAtt) module, which is consisted of dense residual blocks armed with self-attention mechanism, to pay more attention on high-frequency information. Moreover, the series connection of sequential DenseResAtt modules forms deep attentive network with superior discriminative learning ability in image de-quantization, modeling representative feature maps to recover as much useful information as possible. In addition, due to the adversarial learning framework can reliably produce high quality natural images, the specified content loss as well as the adversarial loss are back-propagated to optimize the training of model. Above all, DAGAN is able to generate the photo-realistic high bit-depth image without banding artifacts. Experiment results on several public benchmarks prove that the DAGAN algorithm possesses ability to achieve excellent visual effect and satisfied quantitative performance.



### Adaptive Multiscale Illumination-Invariant Feature Representation for Undersampled Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2004.03153v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.03153v1)
- **Published**: 2020-04-07 06:48:44+00:00
- **Updated**: 2020-04-07 06:48:44+00:00
- **Authors**: Yang Zhang, Changhui Hu, Xiaobo Lu
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents an novel illumination-invariant feature representation approach used to eliminate the varying illumination affection in undersampled face recognition. Firstly, a new illumination level classification technique based on Singular Value Decomposition (SVD) is proposed to judge the illumination level of input image. Secondly, we construct the logarithm edgemaps feature (LEF) based on lambertian model and local near neighbor feature of the face image, applying to local region within multiple scales. Then, the illumination level is referenced to construct the high performance LEF as well realize adaptive fusion for multiple scales LEFs for the face image, performing JLEF-feature. In addition, the constrain operation is used to remove the useless high-frequency interference, disentangling useful facial feature edges and constructing AJLEF-face. Finally, the effects of the our methods and other state-of-the-art algorithms including deep learning methods are tested on Extended Yale B, CMU PIE, AR as well as our Self-build Driver database (SDB). The experimental results demonstrate that the JLEF-feature and AJLEF-face outperform other related approaches for undersampled face recognition under varying illumination.



### Real-time Classification from Short Event-Camera Streams using Input-filtering Neural ODEs
- **Arxiv ID**: http://arxiv.org/abs/2004.03156v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2004.03156v1)
- **Published**: 2020-04-07 06:58:38+00:00
- **Updated**: 2020-04-07 06:58:38+00:00
- **Authors**: Giorgio Giannone, Asha Anoosheh, Alessio Quaglino, Pierluca D'Oro, Marco Gallieri, Jonathan Masci
- **Comment**: Submitted to ICML 2020
- **Journal**: None
- **Summary**: Event-based cameras are novel, efficient sensors inspired by the human vision system, generating an asynchronous, pixel-wise stream of data. Learning from such data is generally performed through heavy preprocessing and event integration into images. This requires buffering of possibly long sequences and can limit the response time of the inference system. In this work, we instead propose to directly use events from a DVS camera, a stream of intensity changes and their spatial coordinates. This sequence is used as the input for a novel \emph{asynchronous} RNN-like architecture, the Input-filtering Neural ODEs (INODE). This is inspired by the dynamical systems and filtering literature. INODE is an extension of Neural ODEs (NODE) that allows for input signals to be continuously fed to the network, like in filtering. The approach naturally handles batches of time series with irregular time-stamps by implementing a batch forward Euler solver. INODE is trained like a standard RNN, it learns to discriminate short event sequences and to perform event-by-event online inference. We demonstrate our approach on a series of classification tasks, comparing against a set of LSTM baselines. We show that, independently of the camera resolution, INODE can outperform the baselines by a large margin on the ASL task and it's on par with a much larger LSTM for the NCALTECH task. Finally, we show that INODE is accurate even when provided with very few events.



### Multi-Task Learning via Co-Attentive Sharing for Pedestrian Attribute Recognition
- **Arxiv ID**: http://arxiv.org/abs/2004.03164v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.03164v1)
- **Published**: 2020-04-07 07:24:22+00:00
- **Updated**: 2020-04-07 07:24:22+00:00
- **Authors**: Haitian Zeng, Haizhou Ai, Zijie Zhuang, Long Chen
- **Comment**: 2020 IEEE International Conference on Multimedia & Expo
- **Journal**: None
- **Summary**: Learning to predict multiple attributes of a pedestrian is a multi-task learning problem. To share feature representation between two individual task networks, conventional methods like Cross-Stitch and Sluice network learn a linear combination of features or feature subspaces. However, linear combination rules out the complex interdependency between channels. Moreover, spatial information exchanging is less-considered. In this paper, we propose a novel Co-Attentive Sharing (CAS) module which extracts discriminative channels and spatial regions for more effective feature sharing in multi-task learning. The module consists of three branches, which leverage different channels for between-task feature fusing, attention generation and task-specific feature enhancing, respectively. Experiments on two pedestrian attribute recognition datasets show that our module outperforms the conventional sharing units and achieves superior results compared to the state-of-the-art approaches using many metrics.



### Iconify: Converting Photographs into Icons
- **Arxiv ID**: http://arxiv.org/abs/2004.03179v1
- **DOI**: 10.1145/3379173.3393708
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.03179v1)
- **Published**: 2020-04-07 08:01:47+00:00
- **Updated**: 2020-04-07 08:01:47+00:00
- **Authors**: Takuro Karamatsu, Gibran Benitez-Garcia, Keiji Yanai, Seiichi Uchida
- **Comment**: to appear at 2020 Joint Workshop on Multimedia Artworks Analysis and
  Attractiveness Computing in Multimedia (MMArt-ACM'20)
- **Journal**: None
- **Summary**: In this paper, we tackle a challenging domain conversion task between photo and icon images. Although icons often originate from real object images (i.e., photographs), severe abstractions and simplifications are applied to generate icon images by professional graphic designers. Moreover, there is no one-to-one correspondence between the two domains, for this reason we cannot use it as the ground-truth for learning a direct conversion function. Since generative adversarial networks (GAN) can undertake the problem of domain conversion without any correspondence, we test CycleGAN and UNIT to generate icons from objects segmented from photo images. Our experiments with several image datasets prove that CycleGAN learns sufficient abstraction and simplification ability to generate icon-like images.



### Text-Guided Neural Image Inpainting
- **Arxiv ID**: http://arxiv.org/abs/2004.03212v4
- **DOI**: 10.1145/3394171.3414017
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2004.03212v4)
- **Published**: 2020-04-07 09:04:43+00:00
- **Updated**: 2021-03-22 08:12:02+00:00
- **Authors**: Lisai Zhang, Qingcai Chen, Baotian Hu, Shuoran Jiang
- **Comment**: ACM MM'2020 (Oral). 9 pages, 4 tables, 7 figures
- **Journal**: None
- **Summary**: Image inpainting task requires filling the corrupted image with contents coherent with the context. This research field has achieved promising progress by using neural image inpainting methods. Nevertheless, there is still a critical challenge in guessing the missed content with only the context pixels. The goal of this paper is to fill the semantic information in corrupted images according to the provided descriptive text. Unique from existing text-guided image generation works, the inpainting models are required to compare the semantic content of the given text and the remaining part of the image, then find out the semantic content that should be filled for missing part. To fulfill such a task, we propose a novel inpainting model named Text-Guided Dual Attention Inpainting Network (TDANet). Firstly, a dual multimodal attention mechanism is designed to extract the explicit semantic information about the corrupted regions, which is done by comparing the descriptive text and complementary image areas through reciprocal attention. Secondly, an image-text matching loss is applied to maximize the semantic similarity of the generated image and the text. Experiments are conducted on two open datasets. Results show that the proposed TDANet model reaches new state-of-the-art on both quantitative and qualitative measures. Result analysis suggests that the generated images are consistent with the guidance text, enabling the generation of various results by providing different descriptions. Codes are available at https://github.com/idealwhite/TDANet



### 3D Human Pose Estimation using Spatio-Temporal Networks with Explicit Occlusion Training
- **Arxiv ID**: http://arxiv.org/abs/2004.11822v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.11822v1)
- **Published**: 2020-04-07 09:12:12+00:00
- **Updated**: 2020-04-07 09:12:12+00:00
- **Authors**: Yu Cheng, Bo Yang, Bo Wang, Robby T. Tan
- **Comment**: 8 pages, AAAI 2020
- **Journal**: None
- **Summary**: Estimating 3D poses from a monocular video is still a challenging task, despite the significant progress that has been made in recent years. Generally, the performance of existing methods drops when the target person is too small/large, or the motion is too fast/slow relative to the scale and speed of the training data. Moreover, to our knowledge, many of these methods are not designed or trained under severe occlusion explicitly, making their performance on handling occlusion compromised. Addressing these problems, we introduce a spatio-temporal network for robust 3D human pose estimation. As humans in videos may appear in different scales and have various motion speeds, we apply multi-scale spatial features for 2D joints or keypoints prediction in each individual frame, and multi-stride temporal convolutional net-works (TCNs) to estimate 3D joints or keypoints. Furthermore, we design a spatio-temporal discriminator based on body structures as well as limb motions to assess whether the predicted pose forms a valid pose and a valid movement. During training, we explicitly mask out some keypoints to simulate various occlusion cases, from minor to severe occlusion, so that our network can learn better and becomes robust to various degrees of occlusion. As there are limited 3D ground-truth data, we further utilize 2D video data to inject a semi-supervised learning capability to our network. Experiments on public datasets validate the effectiveness of our method, and our ablation studies show the strengths of our network\'s individual submodules.



### Optimistic Agent: Accurate Graph-Based Value Estimation for More Successful Visual Navigation
- **Arxiv ID**: http://arxiv.org/abs/2004.03222v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.03222v2)
- **Published**: 2020-04-07 09:31:07+00:00
- **Updated**: 2020-12-06 11:30:14+00:00
- **Authors**: Mahdi Kazemi Moghaddam, Qi Wu, Ehsan Abbasnejad, Javen Qinfeng Shi
- **Comment**: Accepted for publication at WACV 2021
- **Journal**: None
- **Summary**: We humans can impeccably search for a target object, given its name only, even in an unseen environment. We argue that this ability is largely due to three main reasons: the incorporation of prior knowledge (or experience), the adaptation of it to the new environment using the observed visual cues and most importantly optimistically searching without giving up early. This is currently missing in the state-of-the-art visual navigation methods based on Reinforcement Learning (RL). In this paper, we propose to use externally learned prior knowledge of the relative object locations and integrate it into our model by constructing a neural graph. In order to efficiently incorporate the graph without increasing the state-space complexity, we propose our Graph-based Value Estimation (GVE) module. GVE provides a more accurate baseline for estimating the Advantage function in actor-critic RL algorithm. This results in reduced value estimation error and, consequently, convergence to a more optimal policy. Through empirical studies, we show that our agent, dubbed as the optimistic agent, has a more realistic estimate of the state value during a navigation episode which leads to a higher success rate. Our extensive ablation studies show the efficacy of our simple method which achieves the state-of-the-art results measured by the conventional visual navigation metrics, e.g. Success Rate (SR) and Success weighted by Path Length (SPL), in AI2THOR environment.



### Motion-supervised Co-Part Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2004.03234v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.03234v2)
- **Published**: 2020-04-07 09:56:45+00:00
- **Updated**: 2020-04-15 20:13:39+00:00
- **Authors**: Aliaksandr Siarohin, Subhankar Roy, StÃ©phane LathuiliÃ¨re, Sergey Tulyakov, Elisa Ricci, Nicu Sebe
- **Comment**: None
- **Journal**: ICPR 2021
- **Summary**: Recent co-part segmentation methods mostly operate in a supervised learning setting, which requires a large amount of annotated data for training. To overcome this limitation, we propose a self-supervised deep learning method for co-part segmentation. Differently from previous works, our approach develops the idea that motion information inferred from videos can be leveraged to discover meaningful object parts. To this end, our method relies on pairs of frames sampled from the same video. The network learns to predict part segments together with a representation of the motion between two frames, which permits reconstruction of the target image. Through extensive experimental evaluation on publicly available video sequences we demonstrate that our approach can produce improved segmentation maps with respect to previous self-supervised co-part segmentation approaches.



### Hierarchical Opacity Propagation for Image Matting
- **Arxiv ID**: http://arxiv.org/abs/2004.03249v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.03249v1)
- **Published**: 2020-04-07 10:39:55+00:00
- **Updated**: 2020-04-07 10:39:55+00:00
- **Authors**: Yaoyi Li, Qingyao Xu, Hongtao Lu
- **Comment**: None
- **Journal**: None
- **Summary**: Natural image matting is a fundamental problem in computational photography and computer vision. Deep neural networks have seen the surge of successful methods in natural image matting in recent years. In contrast to traditional propagation-based matting methods, some top-tier deep image matting approaches tend to perform propagation in the neural network implicitly. A novel structure for more direct alpha matte propagation between pixels is in demand. To this end, this paper presents a hierarchical opacity propagation (HOP) matting method, where the opacity information is propagated in the neighborhood of each point at different semantic levels. The hierarchical structure is based on one global and multiple local propagation blocks. With the HOP structure, every feature point pair in high-resolution feature maps will be connected based on the appearance of input image. We further propose a scale-insensitive positional encoding tailored for image matting to deal with the unfixed size of input image and introduce the random interpolation augmentation into image matting. Extensive experiments and ablation study show that HOP matting is capable of outperforming state-of-the-art matting methods.



### What and Where: Modeling Skeletons from Semantic and Spatial Perspectives for Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2004.03259v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.03259v2)
- **Published**: 2020-04-07 10:53:45+00:00
- **Updated**: 2021-03-22 12:31:40+00:00
- **Authors**: Lei Shi, Yifan Zhang, Jian Cheng, Hanqing Lu
- **Comment**: None
- **Journal**: None
- **Summary**: Skeleton data, which consists of only the 2D/3D coordinates of the human joints, has been widely studied for human action recognition. Existing methods take the semantics as prior knowledge to group human joints and draw correlations according to their spatial locations, which we call the semantic perspective for skeleton modeling. In this paper, in contrast to previous approaches, we propose to model skeletons from a novel spatial perspective, from which the model takes the spatial location as prior knowledge to group human joints and mines the discriminative patterns of local areas in a hierarchical manner. The two perspectives are orthogonal and complementary to each other; and by fusing them in a unified framework, our method achieves a more comprehensive understanding of the skeleton data. Besides, we customized two networks for the two perspectives. From the semantic perspective, we propose a Transformer-like network that is expert in modeling joint correlations, and present three effective techniques to adapt it for skeleton data. From the spatial perspective, we transform the skeleton data into the sparse format for efficient feature extraction and present two types of sparse convolutional networks for sparse skeleton modeling. Extensive experiments are conducted on three challenging datasets for skeleton-based human action/gesture recognition, namely, NTU-60, NTU-120 and SHREC, where our method achieves state-of-the-art performance.



### Inspector Gadget: A Data Programming-based Labeling System for Industrial Images
- **Arxiv ID**: http://arxiv.org/abs/2004.03264v3
- **DOI**: 10.14778/3421424.3421429
- **Categories**: **cs.LG**, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.03264v3)
- **Published**: 2020-04-07 11:00:29+00:00
- **Updated**: 2020-08-21 04:12:15+00:00
- **Authors**: Geon Heo, Yuji Roh, Seonghyeon Hwang, Dayun Lee, Steven Euijong Whang
- **Comment**: 10 pages, 11 figures
- **Journal**: Proceedings of the VLDB Endowment, Volume 14, Issue 1, September
  2020
- **Summary**: As machine learning for images becomes democratized in the Software 2.0 era, one of the serious bottlenecks is securing enough labeled data for training. This problem is especially critical in a manufacturing setting where smart factories rely on machine learning for product quality control by analyzing industrial images. Such images are typically large and may only need to be partially analyzed where only a small portion is problematic (e.g., identifying defects on a surface). Since manual labeling these images is expensive, weak supervision is an attractive alternative where the idea is to generate weak labels that are not perfect, but can be produced at scale. Data programming is a recent paradigm in this category where it uses human knowledge in the form of labeling functions and combines them into a generative model. Data programming has been successful in applications based on text or structured data and can also be applied to images usually if one can find a way to convert them into structured data. In this work, we expand the horizon of data programming by directly applying it to images without this conversion, which is a common scenario for industrial applications. We propose Inspector Gadget, an image labeling system that combines crowdsourcing, data augmentation, and data programming to produce weak labels at scale for image classification. We perform experiments on real industrial image datasets and show that Inspector Gadget obtains better performance than other weak-labeling techniques: Snuba, GOGGLES, and self-learning baselines using convolutional neural networks (CNNs) without pre-training.



### Autoencoders for Unsupervised Anomaly Segmentation in Brain MR Images: A Comparative Study
- **Arxiv ID**: http://arxiv.org/abs/2004.03271v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.03271v2)
- **Published**: 2020-04-07 11:12:07+00:00
- **Updated**: 2020-04-08 08:04:04+00:00
- **Authors**: Christoph Baur, Stefan Denner, Benedikt Wiestler, Shadi Albarqouni, Nassir Navab
- **Comment**: None
- **Journal**: None
- **Summary**: Deep unsupervised representation learning has recently led to new approaches in the field of Unsupervised Anomaly Detection (UAD) in brain MRI. The main principle behind these works is to learn a model of normal anatomy by learning to compress and recover healthy data. This allows to spot abnormal structures from erroneous recoveries of compressed, potentially anomalous samples. The concept is of great interest to the medical image analysis community as it i) relieves from the need of vast amounts of manually segmented training data---a necessity for and pitfall of current supervised Deep Learning---and ii) theoretically allows to detect arbitrary, even rare pathologies which supervised approaches might fail to find. To date, the experimental design of most works hinders a valid comparison, because i) they are evaluated against different datasets and different pathologies, ii) use different image resolutions and iii) different model architectures with varying complexity. The intent of this work is to establish comparability among recent methods by utilizing a single architecture, a single resolution and the same dataset(s). Besides providing a ranking of the methods, we also try to answer questions like i) how many healthy training subjects are needed to model normality and ii) if the reviewed approaches are also sensitive to domain shift. Further, we identify open challenges and provide suggestions for future community efforts and research directions.



### Super-resolution of clinical CT volumes with modified CycleGAN using micro CT volumes
- **Arxiv ID**: http://arxiv.org/abs/2004.03272v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.03272v1)
- **Published**: 2020-04-07 11:12:24+00:00
- **Updated**: 2020-04-07 11:12:24+00:00
- **Authors**: Tong ZHENG, Hirohisa ODA, Takayasu MORIYA, Takaaki SUGINO, Shota NAKAMURA, Masahiro ODA, Masaki MORI, Hirotsugu TAKABATAKE, Hiroshi NATORI, Kensaku MORI
- **Comment**: 6 pages, 2 figures
- **Journal**: None
- **Summary**: This paper presents a super-resolution (SR) method with unpaired training dataset of clinical CT and micro CT volumes. For obtaining very detailed information such as cancer invasion from pre-operative clinical CT volumes of lung cancer patients, SR of clinical CT volumes to $\m$}CT level is desired. While most SR methods require paired low- and high- resolution images for training, it is infeasible to obtain paired clinical CT and {\mu}CT volumes. We propose a SR approach based on CycleGAN, which could perform SR on clinical CT into $\mu$CT level. We proposed new loss functions to keep cycle consistency, while training without paired volumes. Experimental results demonstrated that our proposed method successfully performed SR of clinical CT volume of lung cancer patients into $\mu$CT level.



### Teacher-Class Network: A Neural Network Compression Mechanism
- **Arxiv ID**: http://arxiv.org/abs/2004.03281v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.03281v3)
- **Published**: 2020-04-07 11:31:20+00:00
- **Updated**: 2021-10-29 18:58:25+00:00
- **Authors**: Shaiq Munir Malik, Muhammad Umair Haider, Mohbat Tharani, Musab Rasheed, Murtaza Taj
- **Comment**: Published in BMVC 2021
- **Journal**: None
- **Summary**: To reduce the overwhelming size of Deep Neural Networks (DNN) teacher-student methodology tries to transfer knowledge from a complex teacher network to a simple student network. We instead propose a novel method called the teacher-class network consisting of a single teacher and multiple student networks (i.e. class of students). Instead of transferring knowledge to one student only, the proposed method transfers a chunk of knowledge to each student. Our students are not trained for problem-specific logits, they are trained to mimic knowledge (dense representation) learned by the teacher network thus the combined knowledge learned by the class of students can be used to solve other problems as well. The proposed teacher-class architecture is evaluated on several benchmark datasets such as MNIST, Fashion MNIST, IMDB Movie Reviews, CAMVid, CIFAR-10 and ImageNet on multiple tasks including image classification, sentiment classification and segmentation. Our approach outperforms the state of-the-art single student approach in terms of accuracy as well as computational cost while achieving 10-30 times reduction in parameters.



### Pyramid Focusing Network for mutation prediction and classification in CT images
- **Arxiv ID**: http://arxiv.org/abs/2004.03302v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.03302v2)
- **Published**: 2020-04-07 12:14:05+00:00
- **Updated**: 2020-04-13 06:39:59+00:00
- **Authors**: Xukun Zhang, Wenxin Hu, Wen Wu
- **Comment**: The carelessness of writing the paper led to the incorrect
  experimental results shown in the paper.In addition, the experimental design
  shown in the paper is incomplete, and I need to revise it substantially
  before submitting it
- **Journal**: None
- **Summary**: Predicting the mutation status of genes in tumors is of great clinical significance. Recent studies have suggested that certain mutations may be noninvasively predicted by studying image features of the tumors from Computed Tomography (CT) data. Currently, this kind of image feature identification method mainly relies on manual processing to extract generalized image features alone or machine processing without considering the morphological differences of the tumor itself, which makes it difficult to achieve further breakthroughs. In this paper, we propose a pyramid focusing network (PFNet) for mutation prediction and classification based on CT images. Firstly, we use Space Pyramid Pooling to collect semantic cues in feature maps from multiple scales according to the observation that the shape and size of the tumors are varied.Secondly, we improve the loss function based on the consideration that the features required for proper mutation detection are often not obvious in cross-sections of tumor edges, which raises more attention to these hard examples in the network. Finally, we devise a training scheme based on data augmentation to enhance the generalization ability of networks. Extensively verified on clinical gastric CT datasets of 20 testing volumes with 63648 CT images, our method achieves the accuracy of 94.90% in predicting the HER-2 genes mutation status of at the CT image.



### Towards Efficient Unconstrained Palmprint Recognition via Deep Distillation Hashing
- **Arxiv ID**: http://arxiv.org/abs/2004.03303v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.03303v1)
- **Published**: 2020-04-07 12:15:04+00:00
- **Updated**: 2020-04-07 12:15:04+00:00
- **Authors**: Huikai Shao, Dexing Zhong, Xuefeng Du
- **Comment**: 13 pages, 8 figures, to access database, see
  http://gr.xjtu.edu.cn/web/bell/resource
- **Journal**: None
- **Summary**: Deep palmprint recognition has become an emerging issue with great potential for personal authentication on handheld and wearable consumer devices. Previous studies of palmprint recognition are mainly based on constrained datasets collected by dedicated devices in controlled environments, which has to reduce the flexibility and convenience. In addition, general deep palmprint recognition algorithms are often too heavy to meet the real-time requirements of embedded system. In this paper, a new palmprint benchmark is established, which consists of more than 20,000 images collected by 5 brands of smart phones in an unconstrained manner. Each image has been manually labeled with 14 key points for region of interest (ROI) extraction. Further, the approach called Deep Distillation Hashing (DDH) is proposed as benchmark for efficient deep palmprint recognition. Palmprint images are converted to binary codes to improve the efficiency of feature matching. Derived from knowledge distillation, novel distillation loss functions are constructed to compress deep model to further improve the efficiency of feature extraction on light network. Comprehensive experiments are conducted on both constrained and unconstrained palmprint databases. Using DDH, the accuracy of palmprint identification can be increased by up to 11.37%, and the Equal Error Rate (EER) of palmprint verification can be reduced by up to 3.11%. The results indicate the feasibility of our database, and DDH can outperform other baselines to achieve the state-of-the-art performance. The collected dataset and related source codes are publicly available at http://gr.xjtu.edu.cn/web/bell/resource.



### Cascaded Refinement Network for Point Cloud Completion
- **Arxiv ID**: http://arxiv.org/abs/2004.03327v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.03327v3)
- **Published**: 2020-04-07 13:03:29+00:00
- **Updated**: 2020-06-05 14:45:49+00:00
- **Authors**: Xiaogang Wang, Marcelo H Ang Jr, Gim Hee Lee
- **Comment**: CVPR2020
- **Journal**: None
- **Summary**: Point clouds are often sparse and incomplete. Existing shape completion methods are incapable of generating details of objects or learning the complex point distributions. To this end, we propose a cascaded refinement network together with a coarse-to-fine strategy to synthesize the detailed object shapes. Considering the local details of partial input with the global shape information together, we can preserve the existing details in the incomplete point set and generate the missing parts with high fidelity. We also design a patch discriminator that guarantees every local area has the same pattern with the ground truth to learn the complicated point distribution. Quantitative and qualitative experiments on different datasets show that our method achieves superior results compared to existing state-of-the-art approaches on the 3D point cloud completion task. Our source code is available at https://github.com/xiaogangw/cascaded-point-completion.git.



### Two-Stage Resampling for Convolutional Neural Network Training in the Imbalanced Colorectal Cancer Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2004.03332v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.03332v2)
- **Published**: 2020-04-07 13:11:17+00:00
- **Updated**: 2021-04-17 13:44:04+00:00
- **Authors**: MichaÅ Koziarski
- **Comment**: None
- **Journal**: None
- **Summary**: Data imbalance remains one of the open challenges in the contemporary machine learning. It is especially prevalent in case of medical data, such as histopathological images. Traditional data-level approaches for dealing with data imbalance are ill-suited for image data: oversampling methods such as SMOTE and its derivatives lead to creation of unrealistic synthetic observations, whereas undersampling reduces the amount of available data, critical for successful training of convolutional neural networks. To alleviate the problems associated with over- and undersampling we propose a novel two-stage resampling methodology, in which we initially use the oversampling techniques in the image space to leverage a large amount of data for training of a convolutional neural network, and afterwards apply undersampling in the feature space to fine-tune the last layers of the network. Experiments conducted on a colorectal cancer image dataset indicate the usefulness of the proposed approach.



### Inclusive GAN: Improving Data and Minority Coverage in Generative Models
- **Arxiv ID**: http://arxiv.org/abs/2004.03355v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.03355v3)
- **Published**: 2020-04-07 13:31:33+00:00
- **Updated**: 2020-08-23 01:10:45+00:00
- **Authors**: Ning Yu, Ke Li, Peng Zhou, Jitendra Malik, Larry Davis, Mario Fritz
- **Comment**: Accepted to ECCV'20
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) have brought about rapid progress towards generating photorealistic images. Yet the equitable allocation of their modeling capacity among subgroups has received less attention, which could lead to potential biases against underrepresented minorities if left uncontrolled. In this work, we first formalize the problem of minority inclusion as one of data coverage, and then propose to improve data coverage by harmonizing adversarial training with reconstructive generation. The experiments show that our method outperforms the existing state-of-the-art methods in terms of data coverage on both seen and unseen data. We develop an extension that allows explicit control over the minority subgroups that the model should ensure to include, and validate its effectiveness at little compromise from the overall performance on the entire dataset. Code, models, and supplemental videos are available at GitHub.



### Improving BPSO-based feature selection applied to offline WI handwritten signature verification through overfitting control
- **Arxiv ID**: http://arxiv.org/abs/2004.03373v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.03373v2)
- **Published**: 2020-04-07 13:42:29+00:00
- **Updated**: 2020-05-11 11:18:02+00:00
- **Authors**: Victor L. F. Souza, Adriano L. I. Oliveira, Rafael M. O. Cruz, Robert Sabourin
- **Comment**: None
- **Journal**: None
- **Summary**: This paper investigates the presence of overfitting when using Binary Particle Swarm Optimization (BPSO) to perform the feature selection in a context of Handwritten Signature Verification (HSV). SigNet is a state of the art Deep CNN model for feature representation in the HSV context and contains 2048 dimensions. Some of these dimensions may include redundant information in the dissimilarity representation space generated by the dichotomy transformation (DT) used by the writer-independent (WI) approach. The analysis is carried out on the GPDS-960 dataset. Experiments demonstrate that the proposed method is able to control overfitting during the search for the most discriminant representation.



### Automated Smartphone based System for Diagnosis of Diabetic Retinopathy
- **Arxiv ID**: http://arxiv.org/abs/2004.03408v1
- **DOI**: 10.1109/ICCCIS48478.2019.8974492
- **Categories**: **eess.IV**, cs.CV, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2004.03408v1)
- **Published**: 2020-04-07 14:01:36+00:00
- **Updated**: 2020-04-07 14:01:36+00:00
- **Authors**: Misgina Tsighe Hagos, Shri Kant, Surayya Ado Bala
- **Comment**: 12 pages, 4 figures, 4 tables, 1 appendix. Copyright \copyright 2019,
  IEEE. Published in: 2019 International Conference on Computing,
  Communication, and Intelligent Systems (ICCCIS)
- **Journal**: International Conference on Computing, Communication, and
  Intelligent Systems (ICCCIS), Greater Noida, India, 2019, pp. 256-261
- **Summary**: Early diagnosis of diabetic retinopathy for treatment of the disease has been failing to reach diabetic people living in rural areas. Shortage of trained ophthalmologists, limited availability of healthcare centers, and expensiveness of diagnostic equipment are among the reasons. Although many deep learning-based automatic diagnosis of diabetic retinopathy techniques have been implemented in the literature, these methods still fail to provide a point-of-care diagnosis. This raises the need for an independent diagnostic of diabetic retinopathy that can be used by a non-expert. Recently the usage of smartphones has been increasing across the world. Automated diagnoses of diabetic retinopathy can be deployed on smartphones in order to provide an instant diagnosis to diabetic people residing in remote areas. In this paper, inception based convolutional neural network and binary decision tree-based ensemble of classifiers have been proposed and implemented to detect and classify diabetic retinopathy. The proposed method was further imported into a smartphone application for mobile-based classification, which provides an offline and automatic system for diagnosis of diabetic retinopathy.



### JHU-CROWD++: Large-Scale Crowd Counting Dataset and A Benchmark Method
- **Arxiv ID**: http://arxiv.org/abs/2004.03597v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.03597v2)
- **Published**: 2020-04-07 14:59:35+00:00
- **Updated**: 2020-11-02 17:52:27+00:00
- **Authors**: Vishwanath A. Sindagi, Rajeev Yasarla, Vishal M. Patel
- **Comment**: Accepted at T-PAMI 2020. The dataset can be downloaded from
  http://www.crowd-counting.com. arXiv admin note: substantial text overlap
  with arXiv:1910.12384
- **Journal**: None
- **Summary**: Due to its variety of applications in the real-world, the task of single image-based crowd counting has received a lot of interest in the recent years. Recently, several approaches have been proposed to address various problems encountered in crowd counting. These approaches are essentially based on convolutional neural networks that require large amounts of data to train the network parameters. Considering this, we introduce a new large scale unconstrained crowd counting dataset (JHU-CROWD++) that contains "4,372" images with "1.51 million" annotations. In comparison to existing datasets, the proposed dataset is collected under a variety of diverse scenarios and environmental conditions. Specifically, the dataset includes several images with weather-based degradations and illumination variations, making it a very challenging dataset. Additionally, the dataset consists of a rich set of annotations at both image-level and head-level. Several recent methods are evaluated and compared on this dataset. The dataset can be downloaded from http://www.crowd-counting.com .   Furthermore, we propose a novel crowd counting network that progressively generates crowd density maps via residual error estimation. The proposed method uses VGG16 as the backbone network and employs density map generated by the final layer as a coarse prediction to refine and generate finer density maps in a progressive fashion using residual learning. Additionally, the residual learning is guided by an uncertainty-based confidence weighting mechanism that permits the flow of only high-confidence residuals in the refinement path. The proposed Confidence Guided Deep Residual Counting Network (CG-DRCN) is evaluated on recent complex datasets, and it achieves significant improvements in errors.



### U-Net Using Stacked Dilated Convolutions for Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2004.03466v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.03466v2)
- **Published**: 2020-04-07 15:06:20+00:00
- **Updated**: 2020-04-10 13:27:16+00:00
- **Authors**: Shuhang Wang, Szu-Yeu Hu, Eugene Cheah, Xiaohong Wang, Jingchao Wang, Lei Chen, Masoud Baikpour, Arinc Ozturk, Qian Li, Shinn-Huey Chou, Constance D. Lehman, Viksit Kumar, Anthony Samir
- **Comment**: 8 pages MICCAI
- **Journal**: None
- **Summary**: This paper proposes a novel U-Net variant using stacked dilated convolutions for medical image segmentation (SDU-Net). SDU-Net adopts the architecture of vanilla U-Net with modifications in the encoder and decoder operations (an operation indicates all the processing for feature maps of the same resolution). Unlike vanilla U-Net which incorporates two standard convolutions in each encoder/decoder operation, SDU-Net uses one standard convolution followed by multiple dilated convolutions and concatenates all dilated convolution outputs as input to the next operation. Experiments showed that SDU-Net outperformed vanilla U-Net, attention U-Net (AttU-Net), and recurrent residual U-Net (R2U-Net) in all four tested segmentation tasks while using parameters around 40% of vanilla U-Net's, 17% of AttU-Net's, and 15% of R2U-Net's.



### Bayesian aggregation improves traditional single image crop classification approaches
- **Arxiv ID**: http://arxiv.org/abs/2004.03468v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.03468v1)
- **Published**: 2020-04-07 15:14:03+00:00
- **Updated**: 2020-04-07 15:14:03+00:00
- **Authors**: Ivan Matvienko, Mikhail Gasanov, Anna Petrovskaia, Raghavendra Belur Jana, Maria Pukalchik, Ivan Oseledets
- **Comment**: Paper presented at the ICLR 2020 Workshop on Computer Vision for
  Agriculture (CV4A)
- **Journal**: None
- **Summary**: Machine learning (ML) methods and neural networks (NN) are widely implemented for crop types recognition and classification based on satellite images. However, most of these studies use several multi-temporal images which could be inapplicable for cloudy regions. We present a comparison between the classical ML approaches and U-Net NN for classifying crops with a single satellite image. The results show the advantages of using field-wise classification over pixel-wise approach. We first used a Bayesian aggregation for field-wise classification and improved on 1.5% results between majority voting aggregation. The best result for single satellite image crop classification is achieved for gradient boosting with an overall accuracy of 77.4% and macro F1-score 0.66.



### Deep Multi-Shot Network for modelling Appearance Similarity in Multi-Person Tracking applications
- **Arxiv ID**: http://arxiv.org/abs/2004.03531v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.03531v1)
- **Published**: 2020-04-07 16:43:35+00:00
- **Updated**: 2020-04-07 16:43:35+00:00
- **Authors**: MarÃ­a J. GÃ³mez-Silva
- **Comment**: None
- **Journal**: None
- **Summary**: The automatization of Multi-Object Tracking becomes a demanding task in real unconstrained scenarios, where the algorithms have to deal with crowds, crossing people, occlusions, disappearances and the presence of visually similar individuals. In those circumstances, the data association between the incoming detections and their corresponding identities could miss some tracks or produce identity switches. In order to reduce these tracking errors, and even their propagation in further frames, this article presents a Deep Multi-Shot neural model for measuring the Degree of Appearance Similarity (MS-DoAS) between person observations. This model provides temporal consistency to the individuals' appearance representation, and provides an affinity metric to perform frame-by-frame data association, allowing online tracking. The model has been deliberately trained to be able to manage the presence of previous identity switches and missed observations in the handled tracks. With that purpose, a novel data generation tool has been designed to create training tracklets that simulate such situations. The model has demonstrated a high capacity to discern when a new observation corresponds to a certain track, achieving a classification accuracy of 97\% in a hard test that simulates tracks with previous mistakes. Moreover, the tracking efficiency of the model in a Surveillance application has been demonstrated by integrating that into the frame-by-frame association of a Tracking-by-Detection algorithm.



### Dense Regression Network for Video Grounding
- **Arxiv ID**: http://arxiv.org/abs/2004.03545v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.03545v1)
- **Published**: 2020-04-07 17:15:37+00:00
- **Updated**: 2020-04-07 17:15:37+00:00
- **Authors**: Runhao Zeng, Haoming Xu, Wenbing Huang, Peihao Chen, Mingkui Tan, Chuang Gan
- **Comment**: CVPR 2020
- **Journal**: None
- **Summary**: We address the problem of video grounding from natural language queries. The key challenge in this task is that one training video might only contain a few annotated starting/ending frames that can be used as positive examples for model training. Most conventional approaches directly train a binary classifier using such imbalance data, thus achieving inferior results. The key idea of this paper is to use the distances between the frame within the ground truth and the starting (ending) frame as dense supervisions to improve the video grounding accuracy. Specifically, we design a novel dense regression network (DRN) to regress the distances from each frame to the starting (ending) frame of the video segment described by the query. We also propose a simple but effective IoU regression head module to explicitly consider the localization quality of the grounding results (i.e., the IoU between the predicted location and the ground truth). Experimental results show that our approach significantly outperforms state-of-the-arts on three datasets (i.e., Charades-STA, ActivityNet-Captions, and TACoS).



### Unsupervised Person Re-identification via Softened Similarity Learning
- **Arxiv ID**: http://arxiv.org/abs/2004.03547v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.03547v1)
- **Published**: 2020-04-07 17:16:41+00:00
- **Updated**: 2020-04-07 17:16:41+00:00
- **Authors**: Yutian Lin, Lingxi Xie, Yu Wu, Chenggang Yan, Qi Tian
- **Comment**: Accepted to CVPR 2020
- **Journal**: None
- **Summary**: Person re-identification (re-ID) is an important topic in computer vision. This paper studies the unsupervised setting of re-ID, which does not require any labeled information and thus is freely deployed to new scenarios. There are very few studies under this setting, and one of the best approach till now used iterative clustering and classification, so that unlabeled images are clustered into pseudo classes for a classifier to get trained, and the updated features are used for clustering and so on. This approach suffers two problems, namely, the difficulty of determining the number of clusters, and the hard quantization loss in clustering. In this paper, we follow the iterative training mechanism but discard clustering, since it incurs loss from hard quantization, yet its only product, image-level similarity, can be easily replaced by pairwise computation and a softened classification task. With these improvements, our approach becomes more elegant and is more robust to hyper-parameter changes. Experiments on two image-based and video-based datasets demonstrate state-of-the-art performance under the unsupervised re-ID setting.



### Temporal Pyramid Network for Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2004.03548v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.03548v2)
- **Published**: 2020-04-07 17:17:23+00:00
- **Updated**: 2020-06-15 02:05:13+00:00
- **Authors**: Ceyuan Yang, Yinghao Xu, Jianping Shi, Bo Dai, Bolei Zhou
- **Comment**: To appear in CVPR 2020. Code is available at
  https://github.com/decisionforce/TPN
- **Journal**: None
- **Summary**: Visual tempo characterizes the dynamics and the temporal scale of an action. Modeling such visual tempos of different actions facilitates their recognition. Previous works often capture the visual tempo through sampling raw videos at multiple rates and constructing an input-level frame pyramid, which usually requires a costly multi-branch network to handle. In this work we propose a generic Temporal Pyramid Network (TPN) at the feature-level, which can be flexibly integrated into 2D or 3D backbone networks in a plug-and-play manner. Two essential components of TPN, the source of features and the fusion of features, form a feature hierarchy for the backbone so that it can capture action instances at various tempos. TPN also shows consistent improvements over other challenging baselines on several action recognition datasets. Specifically, when equipped with TPN, the 3D ResNet-50 with dense sampling obtains a 2% gain on the validation set of Kinetics-400. A further analysis also reveals that TPN gains most of its improvements on action classes that have large variances in their visual tempos, validating the effectiveness of TPN.



### Disp R-CNN: Stereo 3D Object Detection via Shape Prior Guided Instance Disparity Estimation
- **Arxiv ID**: http://arxiv.org/abs/2004.03572v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2004.03572v1)
- **Published**: 2020-04-07 17:48:45+00:00
- **Updated**: 2020-04-07 17:48:45+00:00
- **Authors**: Jiaming Sun, Linghao Chen, Yiming Xie, Siyu Zhang, Qinhong Jiang, Xiaowei Zhou, Hujun Bao
- **Comment**: Accepted to CVPR 2020. Code is available at
  https://github.com/zju3dv/disprcnn
- **Journal**: None
- **Summary**: In this paper, we propose a novel system named Disp R-CNN for 3D object detection from stereo images. Many recent works solve this problem by first recovering a point cloud with disparity estimation and then apply a 3D detector. The disparity map is computed for the entire image, which is costly and fails to leverage category-specific prior. In contrast, we design an instance disparity estimation network (iDispNet) that predicts disparity only for pixels on objects of interest and learns a category-specific shape prior for more accurate disparity estimation. To address the challenge from scarcity of disparity annotation in training, we propose to use a statistical shape model to generate dense disparity pseudo-ground-truth without the need of LiDAR point clouds, which makes our system more widely applicable. Experiments on the KITTI dataset show that, even when LiDAR ground-truth is not available at training time, Disp R-CNN achieves competitive performance and outperforms previous state-of-the-art methods by 20% in terms of average precision.



### Event Based, Near Eye Gaze Tracking Beyond 10,000Hz
- **Arxiv ID**: http://arxiv.org/abs/2004.03577v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2004.03577v3)
- **Published**: 2020-04-07 17:57:18+00:00
- **Updated**: 2022-08-08 16:52:15+00:00
- **Authors**: Anastasios N. Angelopoulos, Julien N. P. Martel, Amit P. S. Kohli, Jorg Conradt, Gordon Wetzstein
- **Comment**: IEEEVR oral/TVCG paper Dataset at
  https://github.com/aangelopoulos/event_based_gaze_tracking Some typo fixes in
  the new version
- **Journal**: None
- **Summary**: The cameras in modern gaze-tracking systems suffer from fundamental bandwidth and power limitations, constraining data acquisition speed to 300 Hz realistically. This obstructs the use of mobile eye trackers to perform, e.g., low latency predictive rendering, or to study quick and subtle eye motions like microsaccades using head-mounted devices in the wild. Here, we propose a hybrid frame-event-based near-eye gaze tracking system offering update rates beyond 10,000 Hz with an accuracy that matches that of high-end desktop-mounted commercial trackers when evaluated in the same conditions. Our system builds on emerging event cameras that simultaneously acquire regularly sampled frames and adaptively sampled events. We develop an online 2D pupil fitting method that updates a parametric model every one or few events. Moreover, we propose a polynomial regressor for estimating the point of gaze from the parametric pupil model in real time. Using the first event-based gaze dataset, available at https://github.com/aangelopoulos/event_based_gaze_tracking , we demonstrate that our system achieves accuracies of 0.45 degrees--1.75 degrees for fields of view from 45 degrees to 98 degrees. With this technology, we hope to enable a new generation of ultra-low-latency gaze-contingent rendering and display techniques for virtual and augmented reality.



### Training End-to-end Single Image Generators without GANs
- **Arxiv ID**: http://arxiv.org/abs/2004.06014v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.06014v1)
- **Published**: 2020-04-07 17:58:03+00:00
- **Updated**: 2020-04-07 17:58:03+00:00
- **Authors**: Yael Vinker, Nir Zabari, Yedid Hoshen
- **Comment**: Project page: http://www.vision.huji.ac.il/augurone
- **Journal**: None
- **Summary**: We present AugurOne, a novel approach for training single image generative models. Our approach trains an upscaling neural network using non-affine augmentations of the (single) input image, particularly including non-rigid thin plate spline image warps. The extensive augmentations significantly increase the in-sample distribution for the upsampling network enabling the upscaling of highly variable inputs. A compact latent space is jointly learned allowing for controlled image synthesis. Differently from Single Image GAN, our approach does not require GAN training and takes place in an end-to-end fashion allowing fast and stable training. We experimentally evaluate our method and show that it obtains compelling novel animations of single-image, as well as, state-of-the-art performance on conditional generation tasks e.g. paint-to-image and edges-to-image.



### Feature Pyramid Grids
- **Arxiv ID**: http://arxiv.org/abs/2004.03580v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.03580v1)
- **Published**: 2020-04-07 17:59:52+00:00
- **Updated**: 2020-04-07 17:59:52+00:00
- **Authors**: Kai Chen, Yuhang Cao, Chen Change Loy, Dahua Lin, Christoph Feichtenhofer
- **Comment**: Technical report
- **Journal**: None
- **Summary**: Feature pyramid networks have been widely adopted in the object detection literature to improve feature representations for better handling of variations in scale. In this paper, we present Feature Pyramid Grids (FPG), a deep multi-pathway feature pyramid, that represents the feature scale-space as a regular grid of parallel bottom-up pathways which are fused by multi-directional lateral connections. FPG can improve single-pathway feature pyramid networks by significantly increasing its performance at similar computation cost, highlighting importance of deep pyramid representations. In addition to its general and uniform structure, over complicated structures that have been found with neural architecture search, it also compares favorably against such approaches without relying on search. We hope that FPG with its uniform and effective nature can serve as a strong component for future work in object recognition.



### PatchVAE: Learning Local Latent Codes for Recognition
- **Arxiv ID**: http://arxiv.org/abs/2004.03623v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.03623v1)
- **Published**: 2020-04-07 18:01:26+00:00
- **Updated**: 2020-04-07 18:01:26+00:00
- **Authors**: Kamal Gupta, Saurabh Singh, Abhinav Shrivastava
- **Comment**: To appear at CVPR 2020
- **Journal**: None
- **Summary**: Unsupervised representation learning holds the promise of exploiting large amounts of unlabeled data to learn general representations. A promising technique for unsupervised learning is the framework of Variational Auto-encoders (VAEs). However, unsupervised representations learned by VAEs are significantly outperformed by those learned by supervised learning for recognition. Our hypothesis is that to learn useful representations for recognition the model needs to be encouraged to learn about repeating and consistent patterns in data. Drawing inspiration from the mid-level representation discovery work, we propose PatchVAE, that reasons about images at patch level. Our key contribution is a bottleneck formulation that encourages mid-level style representations in the VAE framework. Our experiments demonstrate that representations learned by our method perform much better on the recognition tasks compared to those learned by vanilla VAEs.



### The relationship between Fully Connected Layers and number of classes for the analysis of retinal images
- **Arxiv ID**: http://arxiv.org/abs/2004.03624v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.03624v2)
- **Published**: 2020-04-07 18:03:02+00:00
- **Updated**: 2020-04-14 15:50:05+00:00
- **Authors**: Ajna Ram, Constantino Carlos Reyes-Aldasoro
- **Comment**: None
- **Journal**: None
- **Summary**: This paper experiments with the number of fully-connected layers in a deep convolutional neural network as applied to the classification of fundus retinal images. The images analysed corresponded to the ODIR 2019 (Peking University International Competition on Ocular Disease Intelligent Recognition) [9], which included images of various eye diseases (cataract, glaucoma, myopia, diabetic retinopathy, age-related macular degeneration (AMD), hypertension) as well as normal cases. This work focused on the classification of Normal, Cataract, AMD and Myopia. The feature extraction (convolutional) part of the neural network is kept the same while the feature mapping (linear) part of the network is changed. Different data sets are also explored on these neural nets. Each data set differs from another by the number of classes it has. This paper hence aims to find the relationship between number of classes and number of fully-connected layers. It was found out that the effect of increasing the number of fully-connected layers of a neural networks depends on the type of data set being used. For simple, linearly separable data sets, addition of fully-connected layer is something that should be explored and that could result in better training accuracy, but a direct correlation was not found. However as complexity of the data set goes up(more overlapping classes), increasing the number of fully-connected layers causes the neural network to stop learning. This phenomenon happens quicker the more complex the data set is.



### TypeNet: Scaling up Keystroke Biometrics
- **Arxiv ID**: http://arxiv.org/abs/2004.03627v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/2004.03627v2)
- **Published**: 2020-04-07 18:05:33+00:00
- **Updated**: 2020-04-19 09:01:58+00:00
- **Authors**: Alejandro Acien, John V. Monaco, Aythami Morales, Ruben Vera-Rodriguez, Julian Fierrez
- **Comment**: None
- **Journal**: IAPR/IEEE International Joint Conference on Biometrics (IJCB),
  Houston, USA, 2020
- **Summary**: We study the suitability of keystroke dynamics to authenticate 100K users typing free-text. For this, we first analyze to what extent our method based on a Siamese Recurrent Neural Network (RNN) is able to authenticate users when the amount of data per user is scarce, a common scenario in free-text keystroke authentication. With 1K users for testing the network, a population size comparable to previous works, TypeNet obtains an equal error rate of 4.8% using only 5 enrollment sequences and 1 test sequence per user with 50 keystrokes per sequence. Using the same amount of data per user, as the number of test users is scaled up to 100K, the performance in comparison to 1K decays relatively by less than 5%, demonstrating the potential of TypeNet to scale well at large scale number of users. Our experiments are conducted with the Aalto University keystroke database. To the best of our knowledge, this is the largest free-text keystroke database captured with more than 136M keystrokes from 168K users.



### Query-controllable Video Summarization
- **Arxiv ID**: http://arxiv.org/abs/2004.03661v1
- **DOI**: None
- **Categories**: **cs.IR**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.03661v1)
- **Published**: 2020-04-07 19:35:04+00:00
- **Updated**: 2020-04-07 19:35:04+00:00
- **Authors**: Jia-Hong Huang, Marcel Worring
- **Comment**: This paper is accepted by ACM International Conference on Multimedia
  Retrieval (ICMR), 2020
- **Journal**: None
- **Summary**: When video collections become huge, how to explore both within and across videos efficiently is challenging. Video summarization is one of the ways to tackle this issue. Traditional summarization approaches limit the effectiveness of video exploration because they only generate one fixed video summary for a given input video independent of the information need of the user. In this work, we introduce a method which takes a text-based query as input and generates a video summary corresponding to it. We do so by modeling video summarization as a supervised learning problem and propose an end-to-end deep learning based method for query-controllable video summarization to generate a query-dependent video summary. Our proposed method consists of a video summary controller, video summary generator, and video summary output module. To foster the research of query-controllable video summarization and conduct our experiments, we introduce a dataset that contains frame-based relevance score labels. Based on our experimental result, it shows that the text-based query helps control the video summary. It also shows the text-based query improves our model performance. Our code and dataset: https://github.com/Jhhuangkay/Query-controllable-Video-Summarization.



### Radon cumulative distribution transform subspace modeling for image classification
- **Arxiv ID**: http://arxiv.org/abs/2004.03669v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.03669v3)
- **Published**: 2020-04-07 19:47:26+00:00
- **Updated**: 2022-03-02 20:43:03+00:00
- **Authors**: Mohammad Shifat-E-Rabbi, Xuwang Yin, Abu Hasnat Mohammad Rubaiyat, Shiying Li, Soheil Kolouri, Akram Aldroubi, Jonathan M. Nichols, Gustavo K. Rohde
- **Comment**: 14 pages, 11 figures
- **Journal**: None
- **Summary**: We present a new supervised image classification method applicable to a broad class of image deformation models. The method makes use of the previously described Radon Cumulative Distribution Transform (R-CDT) for image data, whose mathematical properties are exploited to express the image data in a form that is more suitable for machine learning. While certain operations such as translation, scaling, and higher-order transformations are challenging to model in native image space, we show the R-CDT can capture some of these variations and thus render the associated image classification problems easier to solve. The method -- utilizing a nearest-subspace algorithm in R-CDT space -- is simple to implement, non-iterative, has no hyper-parameters to tune, is computationally efficient, label efficient, and provides competitive accuracies to state-of-the-art neural networks for many types of classification problems. In addition to the test accuracy performances, we show improvements (with respect to neural network-based methods) in terms of computational efficiency (it can be implemented without the use of GPUs), number of training samples needed for training, as well as out-of-distribution generalization. The Python code for reproducing our results is available at https://github.com/rohdelab/rcdt_ns_classifier.



### Spatio-temporal Learning from Longitudinal Data for Multiple Sclerosis Lesion Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2004.03675v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.03675v2)
- **Published**: 2020-04-07 19:57:24+00:00
- **Updated**: 2020-09-26 19:42:07+00:00
- **Authors**: Stefan Denner, Ashkan Khakzar, Moiz Sajid, Mahdi Saleh, Ziga Spiclin, Seong Tae Kim, Nassir Navab
- **Comment**: Accepted at BrainLes Workshop in MICCAI2020
- **Journal**: None
- **Summary**: Segmentation of Multiple Sclerosis (MS) lesions in longitudinal brain MR scans is performed for monitoring the progression of MS lesions. We hypothesize that the spatio-temporal cues in longitudinal data can aid the segmentation algorithm. Therefore, we propose a multi-task learning approach by defining an auxiliary self-supervised task of deformable registration between two time-points to guide the neural network toward learning from spatio-temporal changes. We show the efficacy of our method on a clinical dataset comprised of 70 patients with one follow-up study for each patient. Our results show that spatio-temporal information in longitudinal data is a beneficial cue for improving segmentation. We improve the result of current state-of-the-art by 2.6% in terms of overall score (p<0.05). Code is publicly available.



### Semantic Image Manipulation Using Scene Graphs
- **Arxiv ID**: http://arxiv.org/abs/2004.03677v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.03677v1)
- **Published**: 2020-04-07 20:02:49+00:00
- **Updated**: 2020-04-07 20:02:49+00:00
- **Authors**: Helisa Dhamo, Azade Farshad, Iro Laina, Nassir Navab, Gregory D. Hager, Federico Tombari, Christian Rupprecht
- **Comment**: CVPR 2020
- **Journal**: None
- **Summary**: Image manipulation can be considered a special case of image generation where the image to be produced is a modification of an existing image. Image generation and manipulation have been, for the most part, tasks that operate on raw pixels. However, the remarkable progress in learning rich image and object representations has opened the way for tasks such as text-to-image or layout-to-image generation that are mainly driven by semantics. In our work, we address the novel problem of image manipulation from scene graphs, in which a user can edit images by merely applying changes in the nodes or edges of a semantic graph that is generated from the image. Our goal is to encode image information in a given constellation and from there on generate new constellations, such as replacing objects or even changing relationships between objects, while respecting the semantics and style from the original image. We introduce a spatio-semantic scene graph network that does not require direct supervision for constellation changes or image edits. This makes it possible to train the system from existing real-world datasets with no additional annotation effort.



### Exemplar Fine-Tuning for 3D Human Model Fitting Towards In-the-Wild 3D Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2004.03686v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.03686v3)
- **Published**: 2020-04-07 20:21:18+00:00
- **Updated**: 2021-10-22 02:55:04+00:00
- **Authors**: Hanbyul Joo, Natalia Neverova, Andrea Vedaldi
- **Comment**: None
- **Journal**: None
- **Summary**: Differently from 2D image datasets such as COCO, large-scale human datasets with 3D ground-truth annotations are very difficult to obtain in the wild. In this paper, we address this problem by augmenting existing 2D datasets with high-quality 3D pose fits. Remarkably, the resulting annotations are sufficient to train from scratch 3D pose regressor networks that outperform the current state-of-the-art on in-the-wild benchmarks such as 3DPW. Additionally, training on our augmented data is straightforward as it does not require to mix multiple and incompatible 2D and 3D datasets or to use complicated network architectures and training procedures. This simplified pipeline affords additional improvements, including injecting extreme crop augmentations to better reconstruct highly truncated people, and incorporating auxiliary inputs to improve 3D pose estimation accuracy. It also reduces the dependency on 3D datasets such as H36M that have restrictive licenses. We also use our method to introduce new benchmarks for the study of real-world challenges such as occlusions, truncations, and rare body poses. In order to obtain such high quality 3D pseudo-annotations, inspired by progress in internal learning, we introduce Exemplar Fine-Tuning (EFT). EFT combines the re-projection accuracy of fitting methods like SMPLify with a 3D pose prior implicitly captured by a pre-trained 3D pose regressor network. We show that EFT produces 3D annotations that result in better downstream performance and are qualitatively preferable in an extensive human-based assessment.



### SA-UNet: Spatial Attention U-Net for Retinal Vessel Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2004.03696v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.03696v3)
- **Published**: 2020-04-07 20:41:12+00:00
- **Updated**: 2020-10-20 19:46:11+00:00
- **Authors**: Changlu Guo, MÃ¡rton Szemenyei, Yugen Yi, Wenle Wang, Buer Chen, Changqi Fan
- **Comment**: ICPR 2020
- **Journal**: None
- **Summary**: The precise segmentation of retinal blood vessels is of great significance for early diagnosis of eye-related diseases such as diabetes and hypertension. In this work, we propose a lightweight network named Spatial Attention U-Net (SA-UNet) that does not require thousands of annotated training samples and can be utilized in a data augmentation manner to use the available annotated samples more efficiently. SA-UNet introduces a spatial attention module which infers the attention map along the spatial dimension, and multiplies the attention map by the input feature map for adaptive feature refinement. In addition, the proposed network employs structured dropout convolutional blocks instead of the original convolutional blocks of U-Net to prevent the network from overfitting. We evaluate SA-UNet based on two benchmark retinal datasets: the Vascular Extraction (DRIVE) dataset and the Child Heart and Health Study (CHASE_DB1) dataset. The results show that the proposed SA-UNet achieves state-of-the-art performance on both datasets.The implementation and the trained networks are available on Github1.



### Dense Residual Network for Retinal Vessel Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2004.03697v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.03697v1)
- **Published**: 2020-04-07 20:42:13+00:00
- **Updated**: 2020-04-07 20:42:13+00:00
- **Authors**: Changlu Guo, MÃ¡rton Szemenyei, Yugen Yi, Ying Xue, Wei Zhou, Yangyuan Li
- **Comment**: Accepted by IEEE ICASSP 2020
- **Journal**: None
- **Summary**: Retinal vessel segmentation plays an imaportant role in the field of retinal image analysis because changes in retinal vascular structure can aid in the diagnosis of diseases such as hypertension and diabetes. In recent research, numerous successful segmentation methods for fundus images have been proposed. But for other retinal imaging modalities, more research is needed to explore vascular extraction. In this work, we propose an efficient method to segment blood vessels in Scanning Laser Ophthalmoscopy (SLO) retinal images. Inspired by U-Net, "feature map reuse" and residual learning, we propose a deep dense residual network structure called DRNet. In DRNet, feature maps of previous blocks are adaptively aggregated into subsequent layers as input, which not only facilitates spatial reconstruction, but also learns more efficiently due to more stable gradients. Furthermore, we introduce DropBlock to alleviate the overfitting problem of the network. We train and test this model on the recent SLO public dataset. The results show that our method achieves the state-of-the-art performance even without data augmentation.



### Coronavirus (COVID-19) Classification using Deep Features Fusion and Ranking Technique
- **Arxiv ID**: http://arxiv.org/abs/2004.03698v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, cs.NE, I.2.0
- **Links**: [PDF](http://arxiv.org/pdf/2004.03698v1)
- **Published**: 2020-04-07 20:43:44+00:00
- **Updated**: 2020-04-07 20:43:44+00:00
- **Authors**: Umut Ozkaya, Saban Ozturk, Mucahid Barstugan
- **Comment**: 13 pages, 6 figures
- **Journal**: None
- **Summary**: Coronavirus (COVID-19) emerged towards the end of 2019. World Health Organization (WHO) was identified it as a global epidemic. Consensus occurred in the opinion that using Computerized Tomography (CT) techniques for early diagnosis of pandemic disease gives both fast and accurate results. It was stated by expert radiologists that COVID-19 displays different behaviours in CT images. In this study, a novel method was proposed as fusing and ranking deep features to detect COVID-19 in early phase. 16x16 (Subset-1) and 32x32 (Subset-2) patches were obtained from 150 CT images to generate sub-datasets. Within the scope of the proposed method, 3000 patch images have been labelled as CoVID-19 and No finding for using in training and testing phase. Feature fusion and ranking method have been applied in order to increase the performance of the proposed method. Then, the processed data was classified with a Support Vector Machine (SVM). According to other pre-trained Convolutional Neural Network (CNN) models used in transfer learning, the proposed method shows high performance on Subset-2 with 98.27% accuracy, 98.93% sensitivity, 97.60% specificity, 97.63% precision, 98.28% F1-score and 96.54% Matthews Correlation Coefficient (MCC) metrics.



### Channel Attention Residual U-Net for Retinal Vessel Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2004.03702v5
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.03702v5)
- **Published**: 2020-04-07 20:47:40+00:00
- **Updated**: 2020-10-20 19:48:55+00:00
- **Authors**: Changlu Guo, MÃ¡rton Szemenyei, Yangtao Hu, Wenle Wang, Wei Zhou, Yugen Yi
- **Comment**: None
- **Journal**: None
- **Summary**: Retinal vessel segmentation is a vital step for the diagnosis of many early eye-related diseases. In this work, we propose a new deep learning model, namely Channel Attention Residual U-Net (CAR-UNet), to accurately segment retinal vascular and non-vascular pixels. In this model, we introduced a novel Modified Efficient Channel Attention (MECA) to enhance the discriminative ability of the network by considering the interdependence between feature maps. On the one hand, we apply MECA to the "skip connections" in the traditional U-shaped networks, instead of simply copying the feature maps of the contracting path to the corresponding expansive path. On the other hand, we propose a Channel Attention Double Residual Block (CADRB), which integrates MECA into a residual structure as a core structure to construct the proposed CAR-UNet. The results show that our proposed CAR-UNet has reached the state-of-the-art performance on three publicly available retinal vessel datasets: DRIVE, CHASE DB1 and STARE.



### Long-Tailed Recognition Using Class-Balanced Experts
- **Arxiv ID**: http://arxiv.org/abs/2004.03706v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.03706v2)
- **Published**: 2020-04-07 20:57:44+00:00
- **Updated**: 2020-10-19 11:22:02+00:00
- **Authors**: Saurabh Sharma, Ning Yu, Mario Fritz, Bernt Schiele
- **Comment**: Accepted and presented at 42nd German Conference on Pattern
  Recognition (DAGM-GCPR 2020), T\"ubingen, Germany
- **Journal**: None
- **Summary**: Deep learning enables impressive performance in image recognition using large-scale artificially-balanced datasets. However, real-world datasets exhibit highly class-imbalanced distributions, yielding two main challenges: relative imbalance amongst the classes and data scarcity for mediumshot or fewshot classes. In this work, we address the problem of long-tailed recognition wherein the training set is highly imbalanced and the test set is kept balanced. Differently from existing paradigms relying on data-resampling, cost-sensitive learning, online hard example mining, loss objective reshaping, and/or memory-based modeling, we propose an ensemble of class-balanced experts that combines the strength of diverse classifiers. Our ensemble of class-balanced experts reaches results close to state-of-the-art and an extended ensemble establishes a new state-of-the-art on two benchmarks for long-tailed recognition. We conduct extensive experiments to analyse the performance of the ensembles, and discover that in modern large-scale datasets, relative imbalance is a harder problem than data scarcity. The training and evaluation code is available at https://github.com/ssfootball04/class-balanced-experts.



### Context-Aware Group Captioning via Self-Attention and Contrastive Features
- **Arxiv ID**: http://arxiv.org/abs/2004.03708v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.03708v1)
- **Published**: 2020-04-07 20:59:53+00:00
- **Updated**: 2020-04-07 20:59:53+00:00
- **Authors**: Zhuowan Li, Quan Tran, Long Mai, Zhe Lin, Alan Yuille
- **Comment**: To appear in CVPR 2020; Project page:
  https://lizw14.github.io/project/groupcap
- **Journal**: None
- **Summary**: While image captioning has progressed rapidly, existing works focus mainly on describing single images. In this paper, we introduce a new task, context-aware group captioning, which aims to describe a group of target images in the context of another group of related reference images. Context-aware group captioning requires not only summarizing information from both the target and reference image group but also contrasting between them. To solve this problem, we propose a framework combining self-attention mechanism with contrastive feature construction to effectively summarize common information from each image group while capturing discriminative information between them. To build the dataset for this task, we propose to group the images and generate the group captions based on single image captions using scene graphs matching. Our datasets are constructed on top of the public Conceptual Captions dataset and our new Stock Captions dataset. Experiments on the two datasets show the effectiveness of our method on this new task. Related Datasets and code are released at https://lizw14.github.io/project/groupcap .



### Mobile-Based Deep Learning Models for Banana Diseases Detection
- **Arxiv ID**: http://arxiv.org/abs/2004.03718v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.03718v1)
- **Published**: 2020-04-07 21:17:45+00:00
- **Updated**: 2020-04-07 21:17:45+00:00
- **Authors**: Sophia Sanga, Victor Mero, Dina Machuve, Davis Mwanganda
- **Comment**: Paper presented at the ICLR 2020 Workshop on Computer Vision for
  Agriculture (CV4A)
- **Journal**: None
- **Summary**: Smallholder farmers in Tanzania are challenged on the lack of tools for early detection of banana diseases. This study aimed at developing a mobile application for early detection of Fusarium wilt race 1 and black Sigatoka banana diseases using deep learning. We used a dataset of 3000 banana leaves images. We pre-trained our model on Resnet152 and Inceptionv3 Convolution Neural Network architectures. The Resnet152 achieved an accuracy of 99.2% and Inceptionv3 an accuracy of 95.41%. On deployment using Android mobile phones, we chose Inceptionv3 since it has lower memory requirements compared to Resnet152. The mobile application on real environment detected the two diseases with a confidence level of 99% of the captured leaf area. This result indicates the potential in improving the yield of bananas by smallholder farmers using a tool for early detection of diseases.



### Learning to Detect Head Movement in Unconstrained Remote Gaze Estimation in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2004.03737v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.03737v1)
- **Published**: 2020-04-07 22:38:49+00:00
- **Updated**: 2020-04-07 22:38:49+00:00
- **Authors**: Zhecan Wang, Jian Zhao, Cheng Lu, Han Huang, Fan Yang, Lianji Li, Yandong Guo
- **Comment**: 2020 Winter Conference on Applications of Computer Vision
- **Journal**: None
- **Summary**: Unconstrained remote gaze estimation remains challenging mostly due to its vulnerability to the large variability in head-pose. Prior solutions struggle to maintain reliable accuracy in unconstrained remote gaze tracking. Among them, appearance-based solutions demonstrate tremendous potential in improving gaze accuracy. However, existing works still suffer from head movement and are not robust enough to handle real-world scenarios. Especially most of them study gaze estimation under controlled scenarios where the collected datasets often cover limited ranges of both head-pose and gaze which introduces further bias. In this paper, we propose novel end-to-end appearance-based gaze estimation methods that could more robustly incorporate different levels of head-pose representations into gaze estimation. Our method could generalize to real-world scenarios with low image quality, different lightings and scenarios where direct head-pose information is not available. To better demonstrate the advantage of our methods, we further propose a new benchmark dataset with the most rich distribution of head-gaze combination reflecting real-world scenarios. Extensive evaluations on several public datasets and our own dataset demonstrate that our method consistently outperforms the state-of-the-art by a significant margin.



### e-SNLI-VE: Corrected Visual-Textual Entailment with Natural Language Explanations
- **Arxiv ID**: http://arxiv.org/abs/2004.03744v3
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.03744v3)
- **Published**: 2020-04-07 23:12:51+00:00
- **Updated**: 2021-08-19 09:26:21+00:00
- **Authors**: Virginie Do, Oana-Maria Camburu, Zeynep Akata, Thomas Lukasiewicz
- **Comment**: None
- **Journal**: IEEE CVPR Workshop on Fair, Data Efficient and Trusted Computer
  Vision, 2020
- **Summary**: The recently proposed SNLI-VE corpus for recognising visual-textual entailment is a large, real-world dataset for fine-grained multimodal reasoning. However, the automatic way in which SNLI-VE has been assembled (via combining parts of two related datasets) gives rise to a large number of errors in the labels of this corpus. In this paper, we first present a data collection effort to correct the class with the highest error rate in SNLI-VE. Secondly, we re-evaluate an existing model on the corrected corpus, which we call SNLI-VE-2.0, and provide a quantitative comparison with its performance on the non-corrected corpus. Thirdly, we introduce e-SNLI-VE, which appends human-written natural language explanations to SNLI-VE-2.0. Finally, we train models that learn from these explanations at training time, and output such explanations at testing time.



### COVID_MTNet: COVID-19 Detection with Multi-Task Deep Learning Approaches
- **Arxiv ID**: http://arxiv.org/abs/2004.03747v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.03747v3)
- **Published**: 2020-04-07 23:19:59+00:00
- **Updated**: 2020-04-18 19:01:10+00:00
- **Authors**: Md Zahangir Alom, M M Shaifur Rahman, Mst Shamima Nasrin, Tarek M. Taha, Vijayan K. Asari
- **Comment**: 11 pages, 15 figures
- **Journal**: None
- **Summary**: COVID-19 is currently one the most life-threatening problems around the world. The fast and accurate detection of the COVID-19 infection is essential to identify, take better decisions and ensure treatment for the patients which will help save their lives. In this paper, we propose a fast and efficient way to identify COVID-19 patients with multi-task deep learning (DL) methods. Both X-ray and CT scan images are considered to evaluate the proposed technique. We employ our Inception Residual Recurrent Convolutional Neural Network with Transfer Learning (TL) approach for COVID-19 detection and our NABLA-N network model for segmenting the regions infected by COVID-19. The detection model shows around 84.67% testing accuracy from X-ray images and 98.78% accuracy in CT-images. A novel quantitative analysis strategy is also proposed in this paper to determine the percentage of infected regions in X-ray and CT images. The qualitative and quantitative results demonstrate promising results for COVID-19 detection and infected region localization.



