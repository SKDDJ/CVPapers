# Arxiv Papers in cs.CV on 2020-04-12
### Verification of Deep Convolutional Neural Networks Using ImageStars
- **Arxiv ID**: http://arxiv.org/abs/2004.05511v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.05511v2)
- **Published**: 2020-04-12 00:37:21+00:00
- **Updated**: 2020-05-14 20:02:06+00:00
- **Authors**: Hoang-Dung Tran, Stanley Bak, Weiming Xiang, Taylor T. Johnson
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNN) have redefined the state-of-the-art in many real-world applications, such as facial recognition, image classification, human pose estimation, and semantic segmentation. Despite their success, CNNs are vulnerable to adversarial attacks, where slight changes to their inputs may lead to sharp changes in their output in even well-trained networks. Set-based analysis methods can detect or prove the absence of bounded adversarial attacks, which can then be used to evaluate the effectiveness of neural network training methodology. Unfortunately, existing verification approaches have limited scalability in terms of the size of networks that can be analyzed.   In this paper, we describe a set-based framework that successfully deals with real-world CNNs, such as VGG16 and VGG19, that have high accuracy on ImageNet. Our approach is based on a new set representation called the ImageStar, which enables efficient exact and over-approximative analysis of CNNs. ImageStars perform efficient set-based analysis by combining operations on concrete images with linear programming (LP). Our approach is implemented in a tool called NNV, and can verify the robustness of VGG networks with respect to a small set of input states, derived from adversarial attacks, such as the DeepFool attack. The experimental results show that our approach is less conservative and faster than existing zonotope methods, such as those used in DeepZ, and the polytope method used in DeepPoly.



### Density Map Guided Object Detection in Aerial Images
- **Arxiv ID**: http://arxiv.org/abs/2004.05520v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.05520v1)
- **Published**: 2020-04-12 01:32:00+00:00
- **Updated**: 2020-04-12 01:32:00+00:00
- **Authors**: Changlin Li, Taojiannan Yang, Sijie Zhu, Chen Chen, Shanyue Guan
- **Comment**: CVPR 2020 EarthVision Workshop
- **Journal**: None
- **Summary**: Object detection in high-resolution aerial images is a challenging task because of 1) the large variation in object size, and 2) non-uniform distribution of objects. A common solution is to divide the large aerial image into small (uniform) crops and then apply object detection on each small crop. In this paper, we investigate the image cropping strategy to address these challenges. Specifically, we propose a Density-Map guided object detection Network (DMNet), which is inspired from the observation that the object density map of an image presents how objects distribute in terms of the pixel intensity of the map. As pixel intensity varies, it is able to tell whether a region has objects or not, which in turn provides guidance for cropping images statistically. DMNet has three key components: a density map generation module, an image cropping module and an object detector. DMNet generates a density map and learns scale information based on density intensities to form cropping regions. Extensive experiments show that DMNet achieves state-of-the-art performance on two popular aerial image datasets, i.e. VisionDrone and UAVDT.



### DeepEDN: A Deep Learning-based Image Encryption and Decryption Network for Internet of Medical Things
- **Arxiv ID**: http://arxiv.org/abs/2004.05523v2
- **DOI**: 10.1109/JIOT.2020.3012452
- **Categories**: **cs.CR**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.05523v2)
- **Published**: 2020-04-12 01:42:47+00:00
- **Updated**: 2020-05-05 14:19:41+00:00
- **Authors**: Yi Ding, Guozheng Wu, Dajiang Chen, Ning Zhang, Linpeng Gong, Mingsheng Cao, Zhiguang Qin
- **Comment**: None
- **Journal**: IEEE Internet of Things Journal,2020
- **Summary**: Internet of Medical Things (IoMT) can connect many medical imaging equipments to the medical information network to facilitate the process of diagnosing and treating for doctors. As medical image contains sensitive information, it is of importance yet very challenging to safeguard the privacy or security of the patient. In this work, a deep learning based encryption and decryption network (DeepEDN) is proposed to fulfill the process of encrypting and decrypting the medical image. Specifically, in DeepEDN, the Cycle-Generative Adversarial Network (Cycle-GAN) is employed as the main learning network to transfer the medical image from its original domain into the target domain. Target domain is regarded as a "Hidden Factors" to guide the learning model for realizing the encryption. The encrypted image is restored to the original (plaintext) image through a reconstruction network to achieve an image decryption. In order to facilitate the data mining directly from the privacy-protected environment, a region of interest(ROI)-mining-network is proposed to extract the interested object from the encrypted image. The proposed DeepEDN is evaluated on the chest X-ray dataset. Extensive experimental results and security analysis show that the proposed method can achieve a high level of security with a good performance in efficiency.



### Building Disaster Damage Assessment in Satellite Imagery with Multi-Temporal Fusion
- **Arxiv ID**: http://arxiv.org/abs/2004.05525v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.05525v1)
- **Published**: 2020-04-12 02:06:12+00:00
- **Updated**: 2020-04-12 02:06:12+00:00
- **Authors**: Ethan Weber, Hassan Kan√©
- **Comment**: Accepted for presentation at the ICLR 2020 AI For Earth Sciences
  Workshop
- **Journal**: None
- **Summary**: Automatic change detection and disaster damage assessment are currently procedures requiring a huge amount of labor and manual work by satellite imagery analysts. In the occurrences of natural disasters, timely change detection can save lives. In this work, we report findings on problem framing, data processing and training procedures which are specifically helpful for the task of building damage assessment using the newly released xBD dataset. Our insights lead to substantial improvement over the xBD baseline models, and we score among top results on the xView2 challenge leaderboard. We release our code used for the competition.



### Gradients as Features for Deep Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2004.05529v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.05529v1)
- **Published**: 2020-04-12 02:57:28+00:00
- **Updated**: 2020-04-12 02:57:28+00:00
- **Authors**: Fangzhou Mu, Yingyu Liang, Yin Li
- **Comment**: ICLR 2020 conference paper
- **Journal**: None
- **Summary**: We address the challenging problem of deep representation learning--the efficient adaption of a pre-trained deep network to different tasks. Specifically, we propose to explore gradient-based features. These features are gradients of the model parameters with respect to a task-specific loss given an input sample. Our key innovation is the design of a linear model that incorporates both gradient and activation of the pre-trained network. We show that our model provides a local linear approximation to an underlying deep model, and discuss important theoretical insights. Moreover, we present an efficient algorithm for the training and inference of our model without computing the actual gradient. Our method is evaluated across a number of representation-learning tasks on several datasets and using different network architectures. Strong results are obtained in all settings, and are well-aligned with our theoretical insights.



### A Unified DNN Weight Compression Framework Using Reweighted Optimization Methods
- **Arxiv ID**: http://arxiv.org/abs/2004.05531v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2004.05531v1)
- **Published**: 2020-04-12 02:59:06+00:00
- **Updated**: 2020-04-12 02:59:06+00:00
- **Authors**: Tianyun Zhang, Xiaolong Ma, Zheng Zhan, Shanglin Zhou, Minghai Qin, Fei Sun, Yen-Kuang Chen, Caiwen Ding, Makan Fardad, Yanzhi Wang
- **Comment**: None
- **Journal**: None
- **Summary**: To address the large model size and intensive computation requirement of deep neural networks (DNNs), weight pruning techniques have been proposed and generally fall into two categories, i.e., static regularization-based pruning and dynamic regularization-based pruning. However, the former method currently suffers either complex workloads or accuracy degradation, while the latter one takes a long time to tune the parameters to achieve the desired pruning rate without accuracy loss. In this paper, we propose a unified DNN weight pruning framework with dynamically updated regularization terms bounded by the designated constraint, which can generate both non-structured sparsity and different kinds of structured sparsity. We also extend our method to an integrated framework for the combination of different DNN compression tasks.



### Online Initialization and Extrinsic Spatial-Temporal Calibration for Monocular Visual-Inertial Odometry
- **Arxiv ID**: http://arxiv.org/abs/2004.05534v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2004.05534v1)
- **Published**: 2020-04-12 03:13:08+00:00
- **Updated**: 2020-04-12 03:13:08+00:00
- **Authors**: Weibo Huang, Hong Liu, Weiwei Wan
- **Comment**: 15 pages, 7 figures
- **Journal**: None
- **Summary**: This paper presents an online initialization method for bootstrapping the optimization-based monocular visual-inertial odometry (VIO). The method can online calibrate the relative transformation (spatial) and time offsets (temporal) among camera and IMU, as well as estimate the initial values of metric scale, velocity, gravity, gyroscope bias, and accelerometer bias during the initialization stage. To compensate for the impact of time offset, our method includes two short-term motion interpolation algorithms for the camera and IMU pose estimation. Besides, it includes a three-step process to incrementally estimate the parameters from coarse to fine. First, the extrinsic rotation, gyroscope bias, and time offset are estimated by minimizing the rotation difference between the camera and IMU. Second, the metric scale, gravity, and extrinsic translation are approximately estimated by using the compensated camera poses and ignoring the accelerometer bias. Third, these values are refined by taking into account the accelerometer bias and the gravitational magnitude. For further optimizing the system states, a nonlinear optimization algorithm, which considers the time offset, is introduced for global and local optimization. Experimental results on public datasets show that the initial values and the extrinsic parameters, as well as the sensor poses, can be accurately estimated by the proposed method.



### Decoupling Global and Local Representations via Invertible Generative Flows
- **Arxiv ID**: http://arxiv.org/abs/2004.11820v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.11820v2)
- **Published**: 2020-04-12 03:18:13+00:00
- **Updated**: 2021-03-15 20:17:34+00:00
- **Authors**: Xuezhe Ma, Xiang Kong, Shanghang Zhang, Eduard Hovy
- **Comment**: Camera-ready at ICLR 2021. 23 pages (plus appendix), 16 figures, 5
  tables. Due to arxiv size constraints, this version is using downscaled
  images. Please download the full-resolution version from
  https://vixra.org/abs/2004.0222
- **Journal**: None
- **Summary**: In this work, we propose a new generative model that is capable of automatically decoupling global and local representations of images in an entirely unsupervised setting, by embedding a generative flow in the VAE framework to model the decoder. Specifically, the proposed model utilizes the variational auto-encoding framework to learn a (low-dimensional) vector of latent variables to capture the global information of an image, which is fed as a conditional input to a flow-based invertible decoder with architecture borrowed from style transfer literature. Experimental results on standard image benchmarks demonstrate the effectiveness of our model in terms of density estimation, image generation and unsupervised representation learning. Importantly, this work demonstrates that with only architectural inductive biases, a generative model with a likelihood-based objective is capable of learning decoupled representations, requiring no explicit supervision. The code for our model is available at https://github.com/XuezheMax/wolf.



### Self-Supervised Tuning for Few-Shot Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2004.05538v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.05538v2)
- **Published**: 2020-04-12 03:53:53+00:00
- **Updated**: 2020-12-14 02:52:44+00:00
- **Authors**: Kai Zhu, Wei Zhai, Zheng-Jun Zha, Yang Cao
- **Comment**: Accepted to IJCAI 2020
- **Journal**: None
- **Summary**: Few-shot segmentation aims at assigning a category label to each image pixel with few annotated samples. It is a challenging task since the dense prediction can only be achieved under the guidance of latent features defined by sparse annotations. Existing meta-learning method tends to fail in generating category-specifically discriminative descriptor when the visual features extracted from support images are marginalized in embedding space. To address this issue, this paper presents an adaptive tuning framework, in which the distribution of latent features across different episodes is dynamically adjusted based on a self-segmentation scheme, augmenting category-specific descriptors for label prediction. Specifically, a novel self-supervised inner-loop is firstly devised as the base learner to extract the underlying semantic features from the support image. Then, gradient maps are calculated by back-propagating self-supervised loss through the obtained features, and leveraged as guidance for augmenting the corresponding elements in embedding space. Finally, with the ability to continuously learn from different episodes, an optimization-based meta-learner is adopted as outer loop of our proposed framework to gradually refine the segmentation results. Extensive experiments on benchmark PASCAL-$5^{i}$ and COCO-$20^{i}$ datasets demonstrate the superiority of our proposed method over state-of-the-art.



### Individual Tooth Detection and Identification from Dental Panoramic X-Ray Images via Point-wise Localization and Distance Regularization
- **Arxiv ID**: http://arxiv.org/abs/2004.05543v1
- **DOI**: 10.1016/j.artmed.2020.101996
- **Categories**: **cs.CV**, 68U10
- **Links**: [PDF](http://arxiv.org/pdf/2004.05543v1)
- **Published**: 2020-04-12 04:14:14+00:00
- **Updated**: 2020-04-12 04:14:14+00:00
- **Authors**: Minyoung Chung, Jusang Lee, Sanguk Park, Minkyung Lee, Chae Eun Lee, Jeongjin Lee, Yeong-Gil Shin
- **Comment**: 10 pages, 7 figures
- **Journal**: None
- **Summary**: Dental panoramic X-ray imaging is a popular diagnostic method owing to its very small dose of radiation. For an automated computer-aided diagnosis system in dental clinics, automatic detection and identification of individual teeth from panoramic X-ray images are critical prerequisites. In this study, we propose a point-wise tooth localization neural network by introducing a spatial distance regularization loss. The proposed network initially performs center point regression for all the anatomical teeth (i.e., 32 points), which automatically identifies each tooth. A novel distance regularization penalty is employed on the 32 points by considering $L_2$ regularization loss of Laplacian on spatial distances. Subsequently, teeth boxes are individually localized using a cascaded neural network on a patch basis. A multitask offset training is employed on the final output to improve the localization accuracy. Our method successfully localizes not only the existing teeth but also missing teeth; consequently, highly accurate detection and identification are achieved. The experimental results demonstrate that the proposed algorithm outperforms state-of-the-art approaches by increasing the average precision of teeth detection by 15.71% compared to the best performing method. The accuracy of identification achieved a precision of 0.997 and recall value of 0.972. Moreover, the proposed network does not require any additional identification algorithm owing to the preceding regression of the fixed 32 points regardless of the existence of the teeth.



### OpenMix: Reviving Known Knowledge for Discovering Novel Visual Categories in An Open World
- **Arxiv ID**: http://arxiv.org/abs/2004.05551v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.05551v1)
- **Published**: 2020-04-12 05:52:39+00:00
- **Updated**: 2020-04-12 05:52:39+00:00
- **Authors**: Zhun Zhong, Linchao Zhu, Zhiming Luo, Shaozi Li, Yi Yang, Nicu Sebe
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we tackle the problem of discovering new classes in unlabeled visual data given labeled data from disjoint classes. Existing methods typically first pre-train a model with labeled data, and then identify new classes in unlabeled data via unsupervised clustering. However, the labeled data that provide essential knowledge are often underexplored in the second step. The challenge is that the labeled and unlabeled examples are from non-overlapping classes, which makes it difficult to build the learning relationship between them. In this work, we introduce OpenMix to mix the unlabeled examples from an open set and the labeled examples from known classes, where their non-overlapping labels and pseudo-labels are simultaneously mixed into a joint label distribution. OpenMix dynamically compounds examples in two ways. First, we produce mixed training images by incorporating labeled examples with unlabeled examples. With the benefits of unique prior knowledge in novel class discovery, the generated pseudo-labels will be more credible than the original unlabeled predictions. As a result, OpenMix helps to prevent the model from overfitting on unlabeled samples that may be assigned with wrong pseudo-labels. Second, the first way encourages the unlabeled examples with high class-probabilities to have considerable accuracy. We introduce these examples as reliable anchors and further integrate them with unlabeled samples. This enables us to generate more combinations in unlabeled examples and exploit finer object relations among the new classes. Experiments on three classification datasets demonstrate the effectiveness of the proposed OpenMix, which is superior to state-of-the-art methods in novel class discovery.



### Feature Lenses: Plug-and-play Neural Modules for Transformation-Invariant Visual Representations
- **Arxiv ID**: http://arxiv.org/abs/2004.05554v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.05554v1)
- **Published**: 2020-04-12 06:36:15+00:00
- **Updated**: 2020-04-12 06:36:15+00:00
- **Authors**: Shaohua Li, Xiuchao Sui, Jie Fu, Yong Liu, Rick Siow Mong Goh
- **Comment**: 20 pages
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) are known to be brittle under various image transformations, including rotations, scalings, and changes of lighting conditions. We observe that the features of a transformed image are drastically different from the ones of the original image. To make CNNs more invariant to transformations, we propose "Feature Lenses", a set of ad-hoc modules that can be easily plugged into a trained model (referred to as the "host model"). Each individual lens reconstructs the original features given the features of a transformed image under a particular transformation. These lenses jointly counteract feature distortions caused by various transformations, thus making the host model more robust without retraining. By only updating lenses, the host model is freed from iterative updating when facing new transformations absent in the training data; as feature semantics are preserved, downstream applications, such as classifiers and detectors, automatically gain robustness without retraining. Lenses are trained in a self-supervised fashion with no annotations, by minimizing a novel "Top-K Activation Contrast Loss" between lens-transformed features and original features. Evaluated on ImageNet, MNIST-rot, and CIFAR-10, Feature Lenses show clear advantages over baseline methods.



### Toward Hierarchical Self-Supervised Monocular Absolute Depth Estimation for Autonomous Driving Applications
- **Arxiv ID**: http://arxiv.org/abs/2004.05560v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2004.05560v2)
- **Published**: 2020-04-12 07:57:03+00:00
- **Updated**: 2020-09-09 10:48:55+00:00
- **Authors**: Feng Xue, Guirong Zhuo, Ziyuan Huang, Wufei Fu, Zhuoyue Wu, Marcelo H. Ang Jr
- **Comment**: 8 pages, 10 figures, accepted by 2020 IEEE/RJS International
  Conference on Intelligent Robots and Systems(IROS)
- **Journal**: None
- **Summary**: In recent years, self-supervised methods for monocular depth estimation has rapidly become an significant branch of depth estimation task, especially for autonomous driving applications. Despite the high overall precision achieved, current methods still suffer from a) imprecise object-level depth inference and b) uncertain scale factor. The former problem would cause texture copy or provide inaccurate object boundary, and the latter would require current methods to have an additional sensor like LiDAR to provide depth ground-truth or stereo camera as additional training inputs, which makes them difficult to implement. In this work, we propose to address these two problems together by introducing DNet. Our contributions are twofold: a) a novel dense connected prediction (DCP) layer is proposed to provide better object-level depth estimation and b) specifically for autonomous driving scenarios, dense geometrical constrains (DGC) is introduced so that precise scale factor can be recovered without additional cost for autonomous vehicles. Extensive experiments have been conducted and, both DCP layer and DGC module are proved to be effectively solving the aforementioned problems respectively. Thanks to DCP layer, object boundary can now be better distinguished in the depth map and the depth is more continues on object level. It is also demonstrated that the performance of using DGC to perform scale recovery is comparable to that using ground-truth information, when the camera height is given and the ground point takes up more than 1.03\% of the pixels. Code is available at https://github.com/TJ-IPLab/DNet.



### FBNetV2: Differentiable Neural Architecture Search for Spatial and Channel Dimensions
- **Arxiv ID**: http://arxiv.org/abs/2004.05565v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2004.05565v1)
- **Published**: 2020-04-12 08:52:15+00:00
- **Updated**: 2020-04-12 08:52:15+00:00
- **Authors**: Alvin Wan, Xiaoliang Dai, Peizhao Zhang, Zijian He, Yuandong Tian, Saining Xie, Bichen Wu, Matthew Yu, Tao Xu, Kan Chen, Peter Vajda, Joseph E. Gonzalez
- **Comment**: 8 pages, 10 figures, accepted to CVPR 2020
- **Journal**: None
- **Summary**: Differentiable Neural Architecture Search (DNAS) has demonstrated great success in designing state-of-the-art, efficient neural networks. However, DARTS-based DNAS's search space is small when compared to other search methods', since all candidate network layers must be explicitly instantiated in memory. To address this bottleneck, we propose a memory and computationally efficient DNAS variant: DMaskingNAS. This algorithm expands the search space by up to $10^{14}\times$ over conventional DNAS, supporting searches over spatial and channel dimensions that are otherwise prohibitively expensive: input resolution and number of filters. We propose a masking mechanism for feature map reuse, so that memory and computational costs stay nearly constant as the search space expands. Furthermore, we employ effective shape propagation to maximize per-FLOP or per-parameter accuracy. The searched FBNetV2s yield state-of-the-art performance when compared with all previous architectures. With up to 421$\times$ less search cost, DMaskingNAS finds models with 0.9% higher accuracy, 15% fewer FLOPs than MobileNetV3-Small; and with similar accuracy but 20% fewer FLOPs than Efficient-B0. Furthermore, our FBNetV2 outperforms MobileNetV3 by 2.6% in accuracy, with equivalent model size. FBNetV2 models are open-sourced at https://github.com/facebookresearch/mobile-vision.



### Cross-domain Correspondence Learning for Exemplar-based Image Translation
- **Arxiv ID**: http://arxiv.org/abs/2004.05571v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.05571v1)
- **Published**: 2020-04-12 09:10:57+00:00
- **Updated**: 2020-04-12 09:10:57+00:00
- **Authors**: Pan Zhang, Bo Zhang, Dong Chen, Lu Yuan, Fang Wen
- **Comment**: Accepted as a CVPR 2020 oral paper
- **Journal**: CVPR 2020
- **Summary**: We present a general framework for exemplar-based image translation, which synthesizes a photo-realistic image from the input in a distinct domain (e.g., semantic segmentation mask, or edge map, or pose keypoints), given an exemplar image. The output has the style (e.g., color, texture) in consistency with the semantically corresponding objects in the exemplar. We propose to jointly learn the crossdomain correspondence and the image translation, where both tasks facilitate each other and thus can be learned with weak supervision. The images from distinct domains are first aligned to an intermediate domain where dense correspondence is established. Then, the network synthesizes images based on the appearance of semantically corresponding patches in the exemplar. We demonstrate the effectiveness of our approach in several image translation tasks. Our method is superior to state-of-the-art methods in terms of image quality significantly, with the image style faithful to the exemplar with semantic consistency. Moreover, we show the utility of our method for several applications



### YouMakeup VQA Challenge: Towards Fine-grained Action Understanding in Domain-Specific Videos
- **Arxiv ID**: http://arxiv.org/abs/2004.05573v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.05573v1)
- **Published**: 2020-04-12 09:25:36+00:00
- **Updated**: 2020-04-12 09:25:36+00:00
- **Authors**: Shizhe Chen, Weiying Wang, Ludan Ruan, Linli Yao, Qin Jin
- **Comment**: CVPR LVVU Workshop 2020 YouMakeup VQA Challenge
- **Journal**: None
- **Summary**: The goal of the YouMakeup VQA Challenge 2020 is to provide a common benchmark for fine-grained action understanding in domain-specific videos e.g. makeup instructional videos. We propose two novel question-answering tasks to evaluate models' fine-grained action understanding abilities. The first task is \textbf{Facial Image Ordering}, which aims to understand visual effects of different actions expressed in natural language to the facial object. The second task is \textbf{Step Ordering}, which aims to measure cross-modal semantic alignments between untrimmed videos and multi-sentence texts. In this paper, we present the challenge guidelines, the dataset used, and performances of baseline models on the two proposed tasks. The baseline codes and models are released at \url{https://github.com/AIM3-RUC/YouMakeup_Baseline}.



### Image Co-skeletonization via Co-segmentation
- **Arxiv ID**: http://arxiv.org/abs/2004.05575v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.05575v1)
- **Published**: 2020-04-12 09:35:54+00:00
- **Updated**: 2020-04-12 09:35:54+00:00
- **Authors**: Koteswar Rao Jerripothula, Jianfei Cai, Jiangbo Lu, Junsong Yuan
- **Comment**: 13 pages, 12 figures, Submitted to IEEE Transactions on Image
  Processing (TIP)
- **Journal**: None
- **Summary**: Recent advances in the joint processing of images have certainly shown its advantages over individual processing. Different from the existing works geared towards co-segmentation or co-localization, in this paper, we explore a new joint processing topic: image co-skeletonization, which is defined as joint skeleton extraction of objects in an image collection. Object skeletonization in a single natural image is a challenging problem because there is hardly any prior knowledge about the object. Therefore, we resort to the idea of object co-skeletonization, hoping that the commonness prior that exists across the images may help, just as it does for other joint processing problems such as co-segmentation. We observe that the skeleton can provide good scribbles for segmentation, and skeletonization, in turn, needs good segmentation. Therefore, we propose a coupled framework for co-skeletonization and co-segmentation tasks so that they are well informed by each other, and benefit each other synergistically. Since it is a new problem, we also construct a benchmark dataset by annotating nearly 1.8k images spread across 38 categories. Extensive experiments demonstrate that the proposed method achieves promising results in all the three possible scenarios of joint-processing: weakly-supervised, supervised, and unsupervised.



### When Weak Becomes Strong: Robust Quantification of White Matter Hyperintensities in Brain MRI scans
- **Arxiv ID**: http://arxiv.org/abs/2004.05578v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.05578v1)
- **Published**: 2020-04-12 10:14:14+00:00
- **Updated**: 2020-04-12 10:14:14+00:00
- **Authors**: Oliver Werner, Kimberlin M. H. van Wijnen, Wiro J. Niessen, Marius de Groot, Meike W. Vernooij, Florian Dubost, Marleen de Bruijne
- **Comment**: 11 pages, 3 figures
- **Journal**: None
- **Summary**: To measure the volume of specific image structures, a typical approach is to first segment those structures using a neural network trained on voxel-wise (strong) labels and subsequently compute the volume from the segmentation. A more straightforward approach would be to predict the volume directly using a neural network based regression approach, trained on image-level (weak) labels indicating volume.   In this article, we compared networks optimized with weak and strong labels, and study their ability to generalize to other datasets. We experimented with white matter hyperintensity (WMH) volume prediction in brain MRI scans. Neural networks were trained on a large local dataset and their performance was evaluated on four independent public datasets. We showed that networks optimized using only weak labels reflecting WMH volume generalized better for WMH volume prediction than networks optimized with voxel-wise segmentations of WMH. The attention maps of networks trained with weak labels did not seem to delineate WMHs, but highlighted instead areas with smooth contours around or near WMHs. By correcting for possible confounders we showed that networks trained on weak labels may have learnt other meaningful features that are more suited to generalization to unseen data. Our results suggest that for imaging biomarkers that can be derived from segmentations, training networks to predict the biomarker directly may provide more robust results than solving an intermediate segmentation step.



### Sharing Matters for Generalization in Deep Metric Learning
- **Arxiv ID**: http://arxiv.org/abs/2004.05582v3
- **DOI**: 10.1109/TPAMI.2020.3009620
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.05582v3)
- **Published**: 2020-04-12 10:21:15+00:00
- **Updated**: 2021-09-09 08:02:54+00:00
- **Authors**: Timo Milbich, Karsten Roth, Biagio Brattoli, Bj√∂rn Ommer
- **Comment**: IEEE Transactions on Pattern Analysis and Machine Intelligence
- **Journal**: None
- **Summary**: Learning the similarity between images constitutes the foundation for numerous vision tasks. The common paradigm is discriminative metric learning, which seeks an embedding that separates different training classes. However, the main challenge is to learn a metric that not only generalizes from training to novel, but related, test samples. It should also transfer to different object classes. So what complementary information is missed by the discriminative paradigm? Besides finding characteristics that separate between classes, we also need them to likely occur in novel categories, which is indicated if they are shared across training classes. This work investigates how to learn such characteristics without the need for extra annotations or training data. By formulating our approach as a novel triplet sampling strategy, it can be easily applied on top of recent ranking loss frameworks. Experiments show that, independent of the underlying network architecture and the specific ranking loss, our approach significantly improves performance in deep metric learning, leading to new the state-of-the-art results on various standard benchmark datasets. Preliminary early access page can be found here: https://ieeexplore.ieee.org/document/9141449



### An Entropy Clustering Approach for Assessing Visual Question Difficulty
- **Arxiv ID**: http://arxiv.org/abs/2004.05595v3
- **DOI**: 10.1109/ACCESS.2020.3022063
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.05595v3)
- **Published**: 2020-04-12 12:06:03+00:00
- **Updated**: 2022-09-02 07:01:22+00:00
- **Authors**: Kento Terao, Toru Tamaki, Bisser Raytchev, Kazufumi Kaneda, Shun'ichi Satoh
- **Comment**: None
- **Journal**: IEEE Access, Vol.8, pp. 180633-180645, Sep 2020
- **Summary**: We propose a novel approach to identify the difficulty of visual questions for Visual Question Answering (VQA) without direct supervision or annotations to the difficulty. Prior works have considered the diversity of ground-truth answers of human annotators. In contrast, we analyze the difficulty of visual questions based on the behavior of multiple different VQA models. We propose to cluster the entropy values of the predicted answer distributions obtained by three different models: a baseline method that takes as input images and questions, and two variants that take as input images only and questions only. We use a simple k-means to cluster the visual questions of the VQA v2 validation set. Then we use state-of-the-art methods to determine the accuracy and the entropy of the answer distributions for each cluster. A benefit of the proposed method is that no annotation of the difficulty is required, because the accuracy of each cluster reflects the difficulty of visual questions that belong to it. Our approach can identify clusters of difficult visual questions that are not answered correctly by state-of-the-art methods. Detailed analysis on the VQA v2 dataset reveals that 1) all methods show poor performances on the most difficult cluster (about 10\% accuracy), 2) as the cluster difficulty increases, the answers predicted by the different methods begin to differ, and 3) the values of cluster entropy are highly correlated with the cluster accuracy. We show that our approach has the advantage of being able to assess the difficulty of visual questions without ground-truth (\ie, the test set of VQA v2) by assigning them to one of the clusters. We expect that this can stimulate the development of novel directions of research and new algorithms.



### Relational Learning between Multiple Pulmonary Nodules via Deep Set Attention Transformers
- **Arxiv ID**: http://arxiv.org/abs/2004.05640v1
- **DOI**: 10.1109/ISBI45749.2020.9098722
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.05640v1)
- **Published**: 2020-04-12 16:05:08+00:00
- **Updated**: 2020-04-12 16:05:08+00:00
- **Authors**: Jiancheng Yang, Haoran Deng, Xiaoyang Huang, Bingbing Ni, Yi Xu
- **Comment**: 2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI
  2020)
- **Journal**: None
- **Summary**: Diagnosis and treatment of multiple pulmonary nodules are clinically important but challenging. Prior studies on nodule characterization use solitary-nodule approaches on multiple nodular patients, which ignores the relations between nodules. In this study, we propose a multiple instance learning (MIL) approach and empirically prove the benefit to learn the relations between multiple nodules. By treating the multiple nodules from a same patient as a whole, critical relational information between solitary-nodule voxels is extracted. To our knowledge, it is the first study to learn the relations between multiple pulmonary nodules. Inspired by recent advances in natural language processing (NLP) domain, we introduce a self-attention transformer equipped with 3D CNN, named {NoduleSAT}, to replace typical pooling-based aggregation in multiple instance learning. Extensive experiments on lung nodule false positive reduction on LUNA16 database, and malignancy classification on LIDC-IDRI database, validate the effectiveness of the proposed method.



### Residual Attention U-Net for Automated Multi-Class Segmentation of COVID-19 Chest CT Images
- **Arxiv ID**: http://arxiv.org/abs/2004.05645v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2004.05645v1)
- **Published**: 2020-04-12 16:24:59+00:00
- **Updated**: 2020-04-12 16:24:59+00:00
- **Authors**: Xiaocong Chen, Lina Yao, Yu Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: The novel coronavirus disease 2019 (COVID-19) has been spreading rapidly around the world and caused significant impact on the public health and economy. However, there is still lack of studies on effectively quantifying the lung infection caused by COVID-19. As a basic but challenging task of the diagnostic framework, segmentation plays a crucial role in accurate quantification of COVID-19 infection measured by computed tomography (CT) images. To this end, we proposed a novel deep learning algorithm for automated segmentation of multiple COVID-19 infection regions. Specifically, we use the Aggregated Residual Transformations to learn a robust and expressive feature representation and apply the soft attention mechanism to improve the capability of the model to distinguish a variety of symptoms of the COVID-19. With a public CT image dataset, we validate the efficacy of the proposed algorithm in comparison with other competing methods. Experimental results demonstrate the outstanding performance of our algorithm for automated segmentation of COVID-19 Chest CT images. Our study provides a promising deep leaning-based segmentation tool to lay a foundation to quantitative diagnosis of COVID-19 lung infection in CT images.



### MLCVNet: Multi-Level Context VoteNet for 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2004.05679v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2004.05679v1)
- **Published**: 2020-04-12 19:10:24+00:00
- **Updated**: 2020-04-12 19:10:24+00:00
- **Authors**: Qian Xie, Yu-Kun Lai, Jing Wu, Zhoutao Wang, Yiming Zhang, Kai Xu, Jun Wang
- **Comment**: To be presented at CVPR 2020
- **Journal**: None
- **Summary**: In this paper, we address the 3D object detection task by capturing multi-level contextual information with the self-attention mechanism and multi-scale feature fusion. Most existing 3D object detection methods recognize objects individually, without giving any consideration on contextual information between these objects. Comparatively, we propose Multi-Level Context VoteNet (MLCVNet) to recognize 3D objects correlatively, building on the state-of-the-art VoteNet. We introduce three context modules into the voting and classifying stages of VoteNet to encode contextual information at different levels. Specifically, a Patch-to-Patch Context (PPC) module is employed to capture contextual information between the point patches, before voting for their corresponding object centroid points. Subsequently, an Object-to-Object Context (OOC) module is incorporated before the proposal and classification stage, to capture the contextual information between object candidates. Finally, a Global Scene Context (GSC) module is designed to learn the global scene context. We demonstrate these by capturing contextual information at patch, object and scene levels. Our method is an effective way to promote detection accuracy, achieving new state-of-the-art detection performance on challenging 3D object detection datasets, i.e., SUN RGBD and ScanNet. We also release our code at https://github.com/NUAAXQ/MLCVNet.



### PatchAttack: A Black-box Texture-based Attack with Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/2004.05682v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.05682v2)
- **Published**: 2020-04-12 19:31:09+00:00
- **Updated**: 2020-07-19 22:36:25+00:00
- **Authors**: Chenglin Yang, Adam Kortylewski, Cihang Xie, Yinzhi Cao, Alan Yuille
- **Comment**: To appear in ECCV 2020
- **Journal**: None
- **Summary**: Patch-based attacks introduce a perceptible but localized change to the input that induces misclassification. A limitation of current patch-based black-box attacks is that they perform poorly for targeted attacks, and even for the less challenging non-targeted scenarios, they require a large number of queries. Our proposed PatchAttack is query efficient and can break models for both targeted and non-targeted attacks. PatchAttack induces misclassifications by superimposing small textured patches on the input image. We parametrize the appearance of these patches by a dictionary of class-specific textures. This texture dictionary is learned by clustering Gram matrices of feature activations from a VGG backbone. PatchAttack optimizes the position and texture parameters of each patch using reinforcement learning. Our experiments show that PatchAttack achieves > 99% success rate on ImageNet for a wide range of architectures, while only manipulating 3% of the image for non-targeted attacks and 10% on average for targeted attacks. Furthermore, we show that PatchAttack circumvents state-of-the-art adversarial defense methods successfully.



### Low-Resolution Overhead Thermal Tripwire for Occupancy Estimation
- **Arxiv ID**: http://arxiv.org/abs/2004.05685v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.05685v4)
- **Published**: 2020-04-12 19:48:19+00:00
- **Updated**: 2020-05-05 16:30:08+00:00
- **Authors**: Mertcan Cokbas, Prakash Ishwar, Janusz Konrad
- **Comment**: None
- **Journal**: None
- **Summary**: Smart buildings use occupancy sensing for various tasks ranging from energy-efficient HVAC and lighting to space-utilization analysis and emergency response. We propose a people counting system which uses a low-resolution thermal sensor. Unlike previous people-counting systems based on thermal sensors, we use an overhead tripwire configuration at entryways to detect and track transient entries or exits. We develop two distinct people counting algorithms for this configuration. To evaluate our algorithms, we have collected and labeled a low-resolution thermal video dataset using the proposed system. The dataset, the first of its kind, is public and available for download. We also propose new evaluation metrics that are more suitable for systems that are subject to drift and jitter.



### Y-net: Biomedical Image Segmentation and Clustering
- **Arxiv ID**: http://arxiv.org/abs/2004.05698v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.05698v2)
- **Published**: 2020-04-12 21:08:31+00:00
- **Updated**: 2020-05-27 02:08:16+00:00
- **Authors**: Sharmin Pathan, Anant Tripathi
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a deep clustering architecture alongside image segmentation for medical image analysis. The main idea is based on unsupervised learning to cluster images on severity of the disease in the subject's sample, and this image is then segmented to highlight and outline regions of interest. We start with training an autoencoder on the images for segmentation. The encoder part from the autoencoder branches out to a clustering node and segmentation node. Deep clustering using Kmeans clustering is performed at the clustering branch and a lightweight model is used for segmentation. Each of the branches use extracted features from the autoencoder. We demonstrate our results on ISIC 2018 Skin Lesion Analysis Towards Melanoma Detection and Cityscapes datasets for segmentation and clustering. The proposed architecture beats UNet and DeepLab results on the two datasets, and has less than half the number of parameters. We use the deep clustering branch for clustering images into four clusters. Our approach can be applied to work with high complexity datasets of medical imaging for analyzing survival prediction for severe diseases or customizing treatment based on how far the disease has propagated. Clustering patients can help understand how binning should be done on real valued features to reduce feature sparsity and improve accuracy on classification tasks. The proposed architecture can provide an early diagnosis and reduce human intervention on labeling as it can become quite costly as the datasets grow larger. The main idea is to propose a one shot approach to segmentation with deep clustering.



### A negative case analysis of visual grounding methods for VQA
- **Arxiv ID**: http://arxiv.org/abs/2004.05704v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2004.05704v2)
- **Published**: 2020-04-12 21:45:23+00:00
- **Updated**: 2020-04-15 17:38:04+00:00
- **Authors**: Robik Shrestha, Kushal Kafle, Christopher Kanan
- **Comment**: None
- **Journal**: None
- **Summary**: Existing Visual Question Answering (VQA) methods tend to exploit dataset biases and spurious statistical correlations, instead of producing right answers for the right reasons. To address this issue, recent bias mitigation methods for VQA propose to incorporate visual cues (e.g., human attention maps) to better ground the VQA models, showcasing impressive gains. However, we show that the performance improvements are not a result of improved visual grounding, but a regularization effect which prevents over-fitting to linguistic priors. For instance, we find that it is not actually necessary to provide proper, human-based cues; random, insensible cues also result in similar improvements. Based on this observation, we propose a simpler regularization scheme that does not require any external annotations and yet achieves near state-of-the-art performance on VQA-CPv2.



### Learning Spatial Relationships between Samples of Patent Image Shapes
- **Arxiv ID**: http://arxiv.org/abs/2004.05713v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.05713v3)
- **Published**: 2020-04-12 23:05:19+00:00
- **Updated**: 2020-04-27 14:26:30+00:00
- **Authors**: Juan Castorena, Manish Bhattarai, Diane Oyen
- **Comment**: None
- **Journal**: None
- **Summary**: Binary image based classification and retrieval of documents of an intellectual nature is a very challenging problem. Variations in the binary image generation mechanisms which are subject to the document artisan designer including drawing style, view-point, inclusion of multiple image components are plausible causes for increasing the complexity of the problem. In this work, we propose a method suitable to binary images which bridges some of the successes of deep learning (DL) to alleviate the problems introduced by the aforementioned variations. The method consists on extracting the shape of interest from the binary image and applying a non-Euclidean geometric neural-net architecture to learn the local and global spatial relationships of the shape. Empirical results show that our method is in some sense invariant to the image generation mechanism variations and achieves results outperforming existing methods in a patent image dataset benchmark.



### Towards an Effective and Efficient Deep Learning Model for COVID-19 Patterns Detection in X-ray Images
- **Arxiv ID**: http://arxiv.org/abs/2004.05717v5
- **DOI**: 10.1007/s42600-021-00151-6
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.05717v5)
- **Published**: 2020-04-12 23:26:56+00:00
- **Updated**: 2021-04-24 12:36:57+00:00
- **Authors**: Eduardo Luz, Pedro Lopes Silva, Rodrigo Silva, Ludmila Silva, Gladston Moreira, David Menotti
- **Comment**: This is a preprint of an article published in Research on Biomedical
  Engineering. The final authenticated version is available online at
  https://doi.org/10.1007/s42600-021-00151-6
- **Journal**: None
- **Summary**: Confronting the pandemic of COVID-19, is nowadays one of the most prominent challenges of the human species. A key factor in slowing down the virus propagation is the rapid diagnosis and isolation of infected patients. The standard method for COVID-19 identification, the Reverse transcription polymerase chain reaction method, is time-consuming and in short supply due to the pandemic. Thus, researchers have been looking for alternative screening methods and deep learning applied to chest X-rays of patients has been showing promising results. Despite their success, the computational cost of these methods remains high, which imposes difficulties to their accessibility and availability. Thus, the main goal of this work is to propose an accurate yet efficient method in terms of memory and processing time for the problem of COVID-19 screening in chest X-rays. Methods: To achieve the defined objective we exploit and extend the EfficientNet family of deep artificial neural networks which are known for their high accuracy and low footprints in other applications. We also exploit the underlying taxonomy of the problem with a hierarchical classifier. A dataset of 13,569 X-ray images divided into healthy, non-COVID-19 pneumonia, and COVID-19 patients is used to train the proposed approaches and other 5 competing architectures. Finally, 231 images of the three classes were used to assess the quality of the methods. Results: The results show that the proposed approach was able to produce a high-quality model, with an overall accuracy of 93.9%, COVID-19, sensitivity of 96.8% and positive prediction of 100%, while having from 5 to 30 times fewer parameters than other than the other tested architectures. Larger and more heterogeneous databases are still needed for validation before claiming that deep learning can assist physicians in the task of detecting COVID-19 in X-ray images.



### Principal Neighbourhood Aggregation for Graph Nets
- **Arxiv ID**: http://arxiv.org/abs/2004.05718v5
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.05718v5)
- **Published**: 2020-04-12 23:30:00+00:00
- **Updated**: 2020-12-31 08:23:06+00:00
- **Authors**: Gabriele Corso, Luca Cavalleri, Dominique Beaini, Pietro Li√≤, Petar Veliƒçkoviƒá
- **Comment**: 34th Conference on Neural Information Processing Systems (NeurIPS
  2020)
- **Journal**: None
- **Summary**: Graph Neural Networks (GNNs) have been shown to be effective models for different predictive tasks on graph-structured data. Recent work on their expressive power has focused on isomorphism tasks and countable feature spaces. We extend this theoretical framework to include continuous features - which occur regularly in real-world input domains and within the hidden layers of GNNs - and we demonstrate the requirement for multiple aggregation functions in this context. Accordingly, we propose Principal Neighbourhood Aggregation (PNA), a novel architecture combining multiple aggregators with degree-scalers (which generalize the sum aggregator). Finally, we compare the capacity of different models to capture and exploit the graph structure via a novel benchmark containing multiple tasks taken from classical graph theory, alongside existing benchmarks from real-world domains, all of which demonstrate the strength of our model. With this work, we hope to steer some of the GNN research towards new aggregation methods which we believe are essential in the search for powerful and robust models.



### Improving Calibration and Out-of-Distribution Detection in Medical Image Segmentation with Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2004.06569v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.06569v3)
- **Published**: 2020-04-12 23:42:51+00:00
- **Updated**: 2022-12-04 18:25:32+00:00
- **Authors**: Davood Karimi, Ali Gholipour
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) have shown to be powerful medical image segmentation models. In this study, we address some of the main unresolved issues regarding these models. Specifically, training of these models on small medical image datasets is still challenging, with many studies promoting techniques such as transfer learning. Moreover, these models are infamous for producing over-confident predictions and for failing silently when presented with out-of-distribution (OOD) data at test time. In this paper, we advocate for multi-task learning, i.e., training a single model on several different datasets, spanning several different organs of interest and different imaging modalities. We show that not only a single CNN learns to automatically recognize the context and accurately segment the organ of interest in each context, but also that such a joint model often has more accurate and better-calibrated predictions than dedicated models trained separately on each dataset. Our experiments show that multi-task learning can outperform transfer learning in medical image segmentation tasks. For detecting OOD data, we propose a method based on spectral analysis of CNN feature maps. We show that different datasets, representing different imaging modalities and/or different organs of interest, have distinct spectral signatures, which can be used to identify whether or not a test image is similar to the images used to train a model. We show that this approach is far more accurate than OOD detection based on prediction uncertainty. The methods proposed in this paper contribute significantly to improving the accuracy and reliability of CNN-based medical image segmentation models.



