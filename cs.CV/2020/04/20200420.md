# Arxiv Papers in cs.CV on 2020-04-20
### Multi-Scale Thermal to Visible Face Verification via Attribute Guided Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2004.09502v2
- **DOI**: 10.1109/TBIOM.2021.3060641
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.09502v2)
- **Published**: 2020-04-20 01:45:05+00:00
- **Updated**: 2021-02-14 01:50:08+00:00
- **Authors**: Xing Di, Benjamin S. Riggan, Shuowen Hu, Nathaniel J. Short, Vishal M. Patel
- **Comment**: accepted by IEEE Transactions on Biometrics, Behavior, and Identity
  Science (T-BIOM). arXiv admin note: substantial text overlap with
  arXiv:1901.00889
- **Journal**: None
- **Summary**: Thermal-to-visible face verification is a challenging problem due to the large domain discrepancy between the modalities. Existing approaches either attempt to synthesize visible faces from thermal faces or learn domain-invariant robust features from these modalities for cross-modal matching. In this paper, we use attributes extracted from visible images to synthesize attribute-preserved visible images from thermal imagery for cross-modal matching. A pre-trained attribute predictor network is used to extract the attributes from the visible image. Then, a novel multi-scale generator is proposed to synthesize the visible image from the thermal image guided by the extracted attributes. Finally, a pre-trained VGG-Face network is leveraged to extract features from the synthesized image and the input visible image for verification. Extensive experiments evaluated on three datasets (ARL Face Database, Visible and Thermal Paired Face Database, and Tufts Face Database) demonstrate that the proposed method achieves state-of-the-art performance. In particular, it achieves around 2.41\%, 2.85\% and 1.77\% improvements in Equal Error Rate (EER) over the state-of-the-art methods on the ARL Face Database, Visible and Thermal Paired Face Database, and Tufts Face Database, respectively. An extended dataset (ARL Face Dataset volume III) consisting of polarimetric thermal faces of 121 subjects is also introduced in this paper. Furthermore, an ablation study is conducted to demonstrate the effectiveness of different modules in the proposed method.



### OSLNet: Deep Small-Sample Classification with an Orthogonal Softmax Layer
- **Arxiv ID**: http://arxiv.org/abs/2004.09033v1
- **DOI**: 10.1109/TIP.2020.2990277
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.09033v1)
- **Published**: 2020-04-20 02:41:01+00:00
- **Updated**: 2020-04-20 02:41:01+00:00
- **Authors**: Xiaoxu Li, Dongliang Chang, Zhanyu Ma, Zheng-Hua Tan, Jing-Hao Xue, Jie Cao, Jingyi Yu, Jun Guo
- **Comment**: TIP 2020. Code available at https://github.com/dongliangchang/OSLNet
- **Journal**: None
- **Summary**: A deep neural network of multiple nonlinear layers forms a large function space, which can easily lead to overfitting when it encounters small-sample data. To mitigate overfitting in small-sample classification, learning more discriminative features from small-sample data is becoming a new trend. To this end, this paper aims to find a subspace of neural networks that can facilitate a large decision margin. Specifically, we propose the Orthogonal Softmax Layer (OSL), which makes the weight vectors in the classification layer remain orthogonal during both the training and test processes. The Rademacher complexity of a network using the OSL is only $\frac{1}{K}$, where $K$ is the number of classes, of that of a network using the fully connected classification layer, leading to a tighter generalization error bound. Experimental results demonstrate that the proposed OSL has better performance than the methods used for comparison on four small-sample benchmark datasets, as well as its applicability to large-sample datasets. Codes are available at: https://github.com/dongliangchang/OSLNet.



### Learning What Makes a Difference from Counterfactual Examples and Gradient Supervision
- **Arxiv ID**: http://arxiv.org/abs/2004.09034v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.09034v1)
- **Published**: 2020-04-20 02:47:49+00:00
- **Updated**: 2020-04-20 02:47:49+00:00
- **Authors**: Damien Teney, Ehsan Abbasnedjad, Anton van den Hengel
- **Comment**: None
- **Journal**: None
- **Summary**: One of the primary challenges limiting the applicability of deep learning is its susceptibility to learning spurious correlations rather than the underlying mechanisms of the task of interest. The resulting failure to generalise cannot be addressed by simply using more data from the same distribution. We propose an auxiliary training objective that improves the generalization capabilities of neural networks by leveraging an overlooked supervisory signal found in existing datasets. We use pairs of minimally-different examples with different labels, a.k.a counterfactual or contrasting examples, which provide a signal indicative of the underlying causal structure of the task. We show that such pairs can be identified in a number of existing datasets in computer vision (visual question answering, multi-label image classification) and natural language processing (sentiment analysis, natural language inference). The new training objective orients the gradient of a model's decision function with pairs of counterfactual examples. Models trained with this technique demonstrate improved performance on out-of-distribution test sets.



### X-Ray: Mechanical Search for an Occluded Object by Minimizing Support of Learned Occupancy Distributions
- **Arxiv ID**: http://arxiv.org/abs/2004.09039v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.09039v2)
- **Published**: 2020-04-20 03:25:10+00:00
- **Updated**: 2020-10-10 19:44:44+00:00
- **Authors**: Michael Danielczuk, Anelia Angelova, Vincent Vanhoucke, Ken Goldberg
- **Comment**: IROS 2020. 8 pages, 6 figures
- **Journal**: None
- **Summary**: For applications in e-commerce, warehouses, healthcare, and home service, robots are often required to search through heaps of objects to grasp a specific target object. For mechanical search, we introduce X-Ray, an algorithm based on learned occupancy distributions. We train a neural network using a synthetic dataset of RGBD heap images labeled for a set of standard bounding box targets with varying aspect ratios. X-Ray minimizes support of the learned distribution as part of a mechanical search policy in both simulated and real environments. We benchmark these policies against two baseline policies on 1,000 heaps of 15 objects in simulation where the target object is partially or fully occluded. Results suggest that X-Ray is significantly more efficient, as it succeeds in extracting the target object 82% of the time, 15% more often than the best-performing baseline. Experiments on an ABB YuMi robot with 20 heaps of 25 household objects suggest that the learned policy transfers easily to a physical system, where it outperforms baseline policies by 15% in success rate with 17% fewer actions. Datasets, videos, and experiments are available at https://sites.google.com/berkeley.edu/x-ray.



### Dark, Beyond Deep: A Paradigm Shift to Cognitive AI with Humanlike Common Sense
- **Arxiv ID**: http://arxiv.org/abs/2004.09044v1
- **DOI**: 10.1016/j.eng.2020.01.011
- **Categories**: **cs.AI**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.09044v1)
- **Published**: 2020-04-20 04:07:28+00:00
- **Updated**: 2020-04-20 04:07:28+00:00
- **Authors**: Yixin Zhu, Tao Gao, Lifeng Fan, Siyuan Huang, Mark Edmonds, Hangxin Liu, Feng Gao, Chi Zhang, Siyuan Qi, Ying Nian Wu, Joshua B. Tenenbaum, Song-Chun Zhu
- **Comment**: For high quality figures, please refer to
  http://wellyzhang.github.io/attach/dark.pdf
- **Journal**: Engineering, Feb, 2020
- **Summary**: Recent progress in deep learning is essentially based on a "big data for small tasks" paradigm, under which massive amounts of data are used to train a classifier for a single narrow task. In this paper, we call for a shift that flips this paradigm upside down. Specifically, we propose a "small data for big tasks" paradigm, wherein a single artificial intelligence (AI) system is challenged to develop "common sense", enabling it to solve a wide range of tasks with little training data. We illustrate the potential power of this new paradigm by reviewing models of common sense that synthesize recent breakthroughs in both machine and human vision. We identify functionality, physics, intent, causality, and utility (FPICU) as the five core domains of cognitive AI with humanlike common sense. When taken as a unified concept, FPICU is concerned with the questions of "why" and "how", beyond the dominant "what" and "where" framework for understanding vision. They are invisible in terms of pixels but nevertheless drive the creation, maintenance, and development of visual scenes. We therefore coin them the "dark matter" of vision. Just as our universe cannot be understood by merely studying observable matter, we argue that vision cannot be understood without studying FPICU. We demonstrate the power of this perspective to develop cognitive AI systems with humanlike common sense by showing how to observe and apply FPICU with little training data to solve a wide range of challenging tasks, including tool use, planning, utility inference, and social learning. In summary, we argue that the next generation of AI must embrace "dark" humanlike common sense for solving novel tasks.



### Extending DeepSDF for automatic 3D shape retrieval and similarity transform estimation
- **Arxiv ID**: http://arxiv.org/abs/2004.09048v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.09048v3)
- **Published**: 2020-04-20 04:28:45+00:00
- **Updated**: 2020-10-26 05:01:44+00:00
- **Authors**: Oladapo Afolabi, Allen Y. Yang, S. Shankar Sastry
- **Comment**: 21 pages
- **Journal**: None
- **Summary**: Recent advances in computer graphics and computer vision have found successful application of deep neural network models for 3D shapes based on signed distance functions (SDFs) that are useful for shape representation, retrieval, and completion. However, this approach has been limited by the need to have query shapes in the same canonical scale and pose as those observed during training, restricting its effectiveness on real world scenes. We present a formulation to overcome this issue by jointly estimating shape and similarity transform parameters. We conduct experiments to demonstrate the effectiveness of this formulation on synthetic and real datasets and report favorable comparisons to the state of the art. Finally, we also emphasize the viability of this approach as a form of 3D model compression.



### Colon Shape Estimation Method for Colonoscope Tracking using Recurrent Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2004.13629v1
- **DOI**: 10.1007/978-3-030-00937-3_21
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.13629v1)
- **Published**: 2020-04-20 04:43:58+00:00
- **Updated**: 2020-04-20 04:43:58+00:00
- **Authors**: Masahiro Oda, Holger R. Roth, Takayuki Kitasaka, Kazuhiro Furukawa, Ryoji Miyahara, Yoshiki Hirooka, Hidemi Goto, Nassir Navab, Kensaku Mori
- **Comment**: Accepted paper as a poster presentation at MICCAI 2018 (International
  Conference on Medical Image Computing and Computer-Assisted Intervention),
  Granada, Spain
- **Journal**: Published in Proceedings of MICCAI 2018, LNCS 11073, pp 176-184
- **Summary**: We propose an estimation method using a recurrent neural network (RNN) of the colon's shape where deformation was occurred by a colonoscope insertion. Colonoscope tracking or a navigation system that navigates physician to polyp positions is needed to reduce such complications as colon perforation. Previous tracking methods caused large tracking errors at the transverse and sigmoid colons because these areas largely deform during colonoscope insertion. Colon deformation should be taken into account in tracking processes. We propose a colon deformation estimation method using RNN and obtain the colonoscope shape from electromagnetic sensors during its insertion into the colon. This method obtains positional, directional, and an insertion length from the colonoscope shape. From its shape, we also calculate the relative features that represent the positional and directional relationships between two points on a colonoscope. Long short-term memory is used to estimate the current colon shape from the past transition of the features of the colonoscope shape. We performed colon shape estimation in a phantom study and correctly estimated the colon shapes during colonoscope insertion with 12.39 (mm) estimation error.



### Colonoscope tracking method based on shape estimation network
- **Arxiv ID**: http://arxiv.org/abs/2004.09056v1
- **DOI**: 10.1117/12.2512729
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.09056v1)
- **Published**: 2020-04-20 05:10:38+00:00
- **Updated**: 2020-04-20 05:10:38+00:00
- **Authors**: Masahiro Oda, Holger R. Roth, Takayuki Kitasaka, Kazuhiro Furukawa, Ryoji Miyahara, Yoshiki Hirooka, Nassir Navab, Kensaku Mori
- **Comment**: Accepted paper as an oral presentation at SPIE Medical Imaging 2019,
  San Diego, CA, USA
- **Journal**: Proceedings of SPIE Medical Imaging 2019: Image-Guided Procedures,
  Robotic Interventions, and Modeling, Vol.10951, 109510Q
- **Summary**: This paper presents a colonoscope tracking method utilizing a colon shape estimation method. CT colonography is used as a less-invasive colon diagnosis method. If colonic polyps or early-stage cancers are found, they are removed in a colonoscopic examination. In the colonoscopic examination, understanding where the colonoscope running in the colon is difficult. A colonoscope navigation system is necessary to reduce overlooking of polyps. We propose a colonoscope tracking method for navigation systems. Previous colonoscope tracking methods caused large tracking errors because they do not consider deformations of the colon during colonoscope insertions. We utilize the shape estimation network (SEN), which estimates deformed colon shape during colonoscope insertions. The SEN is a neural network containing long short-term memory (LSTM) layer. To perform colon shape estimation suitable to the real clinical situation, we trained the SEN using data obtained during colonoscope operations of physicians. The proposed tracking method performs mapping of the colonoscope tip position to a position in the colon using estimation results of the SEN. We evaluated the proposed method in a phantom study. We confirmed that tracking errors of the proposed method was enough small to perform navigation in the ascending, transverse, and descending colons.



### Airborne LiDAR Point Cloud Classification with Graph Attention Convolution Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2004.09057v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.09057v1)
- **Published**: 2020-04-20 05:12:31+00:00
- **Updated**: 2020-04-20 05:12:31+00:00
- **Authors**: Congcong Wen, Xiang Li, Xiaojing Yao, Ling Peng, Tianhe Chi
- **Comment**: None
- **Journal**: None
- **Summary**: Airborne light detection and ranging (LiDAR) plays an increasingly significant role in urban planning, topographic mapping, environmental monitoring, power line detection and other fields thanks to its capability to quickly acquire large-scale and high-precision ground information. To achieve point cloud classification, previous studies proposed point cloud deep learning models that can directly process raw point clouds based on PointNet-like architectures. And some recent works proposed graph convolution neural network based on the inherent topology of point clouds. However, the above point cloud deep learning models only pay attention to exploring local geometric structures, yet ignore global contextual relationships among all points. In this paper, we present a graph attention convolution neural network (GACNN) that can be directly applied to the classification of unstructured 3D point clouds obtained by airborne LiDAR. Specifically, we first introduce a graph attention convolution module that incorporates global contextual information and local structural features. Based on the proposed graph attention convolution module, we further design an end-to-end encoder-decoder network, named GACNN, to capture multiscale features of the point clouds and therefore enable more accurate airborne point cloud classification. Experiments on the ISPRS 3D labeling dataset show that the proposed model achieves a new state-of-the-art performance in terms of average F1 score (71.5\%) and a satisfying overall accuracy (83.2\%). Additionally, experiments further conducted on the 2019 Data Fusion Contest Dataset by comparing with other prevalent point cloud deep learning models demonstrate the favorable generalization capability of the proposed model.



### Semantic Correspondence via 2D-3D-2D Cycle
- **Arxiv ID**: http://arxiv.org/abs/2004.09061v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.09061v2)
- **Published**: 2020-04-20 05:27:45+00:00
- **Updated**: 2021-04-13 06:20:57+00:00
- **Authors**: Yang You, Chengkun Li, Yujing Lou, Zhoujun Cheng, Lizhuang Ma, Cewu Lu, Weiming Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Visual semantic correspondence is an important topic in computer vision and could help machine understand objects in our daily life. However, most previous methods directly train on correspondences in 2D images, which is end-to-end but loses plenty of information in 3D spaces. In this paper, we propose a new method on predicting semantic correspondences by leveraging it to 3D domain and then project corresponding 3D models back to 2D domain, with their semantic labels. Our method leverages the advantages in 3D vision and can explicitly reason about objects self-occlusion and visibility. We show that our method gives comparative and even superior results on standard semantic benchmarks. We also conduct thorough and detailed experiments to analyze our network components. The code and experiments are publicly available at https://github.com/qq456cvb/SemanticTransfer.



### CatSIM: A Categorical Image Similarity Metric
- **Arxiv ID**: http://arxiv.org/abs/2004.09073v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.09073v1)
- **Published**: 2020-04-20 06:03:58+00:00
- **Updated**: 2020-04-20 06:03:58+00:00
- **Authors**: Geoffrey Z. Thompson, Ranjan Maitra
- **Comment**: 17 pages, 16 figures, 10 tables
- **Journal**: None
- **Summary**: We introduce CatSIM, a new similarity metric for binary and multinary two- and three-dimensional images and volumes. CatSIM uses a structural similarity image quality paradigm and is robust to small perturbations in location so that structures in similar, but not entirely overlapping, regions of two images are rated higher than using simple matching. The metric can also compare arbitrary regions inside images. CatSIM is evaluated on artificial data sets, image quality assessment surveys and two imaging applications



### Deep Exposure Fusion with Deghosting via Homography Estimation and Attention Learning
- **Arxiv ID**: http://arxiv.org/abs/2004.09089v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.09089v1)
- **Published**: 2020-04-20 07:00:14+00:00
- **Updated**: 2020-04-20 07:00:14+00:00
- **Authors**: Sheng-Yeh Chen, Yung-Yu Chuang
- **Comment**: ICASSP 2020
- **Journal**: None
- **Summary**: Modern cameras have limited dynamic ranges and often produce images with saturated or dark regions using a single exposure. Although the problem could be addressed by taking multiple images with different exposures, exposure fusion methods need to deal with ghosting artifacts and detail loss caused by camera motion or moving objects. This paper proposes a deep network for exposure fusion. For reducing the potential ghosting problem, our network only takes two images, an underexposed image and an overexposed one. Our network integrates together homography estimation for compensating camera motion, attention mechanism for correcting remaining misalignment and moving pixels, and adversarial learning for alleviating other remaining artifacts. Experiments on real-world photos taken using handheld mobile phones show that the proposed method can generate high-quality images with faithful detail and vivid color rendition in both dark and bright areas.



### IPN Hand: A Video Dataset and Benchmark for Real-Time Continuous Hand Gesture Recognition
- **Arxiv ID**: http://arxiv.org/abs/2005.02134v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.02134v2)
- **Published**: 2020-04-20 08:52:32+00:00
- **Updated**: 2020-10-20 14:50:42+00:00
- **Authors**: Gibran Benitez-Garcia, Jesus Olivares-Mercado, Gabriel Sanchez-Perez, Keiji Yanai
- **Comment**: Accepted at ICPR 2020
- **Journal**: None
- **Summary**: In this paper, we introduce a new benchmark dataset named IPN Hand with sufficient size, variety, and real-world elements able to train and evaluate deep neural networks. This dataset contains more than 4,000 gesture samples and 800,000 RGB frames from 50 distinct subjects. We design 13 different static and dynamic gestures focused on interaction with touchless screens. We especially consider the scenario when continuous gestures are performed without transition states, and when subjects perform natural movements with their hands as non-gesture actions. Gestures were collected from about 30 diverse scenes, with real-world variation in background and illumination. With our dataset, the performance of three 3D-CNN models is evaluated on the tasks of isolated and continuous real-time HGR. Furthermore, we analyze the possibility of increasing the recognition accuracy by adding multiple modalities derived from RGB frames, i.e., optical flow and semantic segmentation, while keeping the real-time performance of the 3D-CNN model. Our empirical study also provides a comparison with the publicly available nvGesture (NVIDIA) dataset. The experimental results show that the state-of-the-art ResNext-101 model decreases about 30% accuracy when using our real-world dataset, demonstrating that the IPN Hand dataset can be used as a benchmark, and may help the community to step forward in the continuous HGR. Our dataset and pre-trained models used in the evaluation are publicly available at https://github.com/GibranBenitez/IPN-hand.



### Spatial Action Maps for Mobile Manipulation
- **Arxiv ID**: http://arxiv.org/abs/2004.09141v2
- **DOI**: 10.15607/RSS.2020.XVI.035
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.09141v2)
- **Published**: 2020-04-20 09:06:10+00:00
- **Updated**: 2020-06-04 10:56:49+00:00
- **Authors**: Jimmy Wu, Xingyuan Sun, Andy Zeng, Shuran Song, Johnny Lee, Szymon Rusinkiewicz, Thomas Funkhouser
- **Comment**: To appear at Robotics: Science and Systems (RSS), 2020. Project page:
  https://spatial-action-maps.cs.princeton.edu
- **Journal**: None
- **Summary**: Typical end-to-end formulations for learning robotic navigation involve predicting a small set of steering command actions (e.g., step forward, turn left, turn right, etc.) from images of the current state (e.g., a bird's-eye view of a SLAM reconstruction). Instead, we show that it can be advantageous to learn with dense action representations defined in the same domain as the state. In this work, we present "spatial action maps," in which the set of possible actions is represented by a pixel map (aligned with the input image of the current state), where each pixel represents a local navigational endpoint at the corresponding scene location. Using ConvNets to infer spatial action maps from state images, action predictions are thereby spatially anchored on local visual features in the scene, enabling significantly faster learning of complex behaviors for mobile manipulation tasks with reinforcement learning. In our experiments, we task a robot with pushing objects to a goal location, and find that policies learned with spatial action maps achieve much better performance than traditional alternatives.



### Transformer Reasoning Network for Image-Text Matching and Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2004.09144v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.09144v3)
- **Published**: 2020-04-20 09:09:01+00:00
- **Updated**: 2021-01-25 21:19:28+00:00
- **Authors**: Nicola Messina, Fabrizio Falchi, Andrea Esuli, Giuseppe Amato
- **Comment**: Presented at ICPR 2020
- **Journal**: None
- **Summary**: Image-text matching is an interesting and fascinating task in modern AI research. Despite the evolution of deep-learning-based image and text processing systems, multi-modal matching remains a challenging problem. In this work, we consider the problem of accurate image-text matching for the task of multi-modal large-scale information retrieval. State-of-the-art results in image-text matching are achieved by inter-playing image and text features from the two different processing pipelines, usually using mutual attention mechanisms. However, this invalidates any chance to extract separate visual and textual features needed for later indexing steps in large-scale retrieval systems. In this regard, we introduce the Transformer Encoder Reasoning Network (TERN), an architecture built upon one of the modern relationship-aware self-attentive architectures, the Transformer Encoder (TE). This architecture is able to separately reason on the two different modalities and to enforce a final common abstract concept space by sharing the weights of the deeper transformer layers. Thanks to this design, the implemented network is able to produce compact and very rich visual and textual features available for the successive indexing step. Experiments are conducted on the MS-COCO dataset, and we evaluate the results using a discounted cumulative gain metric with relevance computed exploiting caption similarities, in order to assess possibly non-exact but relevant search results. We demonstrate that on this metric we are able to achieve state-of-the-art results in the image retrieval task. Our code is freely available at https://github.com/mesnico/TERN.



### Cosmetic-Aware Makeup Cleanser
- **Arxiv ID**: http://arxiv.org/abs/2004.09147v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.09147v1)
- **Published**: 2020-04-20 09:18:23+00:00
- **Updated**: 2020-04-20 09:18:23+00:00
- **Authors**: Yi Li, Huaibo Huang, Junchi Yu, Ran He, Tieniu Tan
- **Comment**: Accepted by BTAS 2019 (the 10th IEEE International Conference on
  Biometrics: Theory, Applications and Systems)
- **Journal**: None
- **Summary**: Face verification aims at determining whether a pair of face images belongs to the same identity. Recent studies have revealed the negative impact of facial makeup on the verification performance. With the rapid development of deep generative models, this paper proposes a semanticaware makeup cleanser (SAMC) to remove facial makeup under different poses and expressions and achieve verification via generation. The intuition lies in the fact that makeup is a combined effect of multiple cosmetics and tailored treatments should be imposed on different cosmetic regions. To this end, we present both unsupervised and supervised semantic-aware learning strategies in SAMC. At image level, an unsupervised attention module is jointly learned with the generator to locate cosmetic regions and estimate the degree. At feature level, we resort to the effort of face parsing merely in training phase and design a localized texture loss to serve complements and pursue superior synthetic quality. The experimental results on four makeuprelated datasets verify that SAMC not only produces appealing de-makeup outputs at a resolution of 256*256, but also facilitates makeup-invariant face verification through image generation.



### VOC-ReID: Vehicle Re-identification based on Vehicle-Orientation-Camera
- **Arxiv ID**: http://arxiv.org/abs/2004.09164v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.09164v2)
- **Published**: 2020-04-20 09:44:07+00:00
- **Updated**: 2020-05-15 07:59:43+00:00
- **Authors**: Xiangyu Zhu, Zhenbo Luo, Pei Fu, Xiang Ji
- **Comment**: AICity2020 Challenge, CVPR 2020 workshop, code avaible at github(link
  in abstract)
- **Journal**: None
- **Summary**: Vehicle re-identification is a challenging task due to high intra-class variances and small inter-class variances. In this work, we focus on the failure cases caused by similar background and shape. They pose serve bias on similarity, making it easier to neglect fine-grained information. To reduce the bias, we propose an approach named VOC-ReID, taking the triplet vehicle-orientation-camera as a whole and reforming background/shape similarity as camera/orientation re-identification. At first, we train models for vehicle, orientation and camera re-identification respectively. Then we use orientation and camera similarity as penalty to get final similarity. Besides, we propose a high performance baseline boosted by bag of tricks and weakly supervised data augmentation. Our algorithm achieves the second place in vehicle re-identification at the NVIDIA AI City Challenge 2020.



### Invariant Integration in Deep Convolutional Feature Space
- **Arxiv ID**: http://arxiv.org/abs/2004.09166v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.09166v1)
- **Published**: 2020-04-20 09:45:43+00:00
- **Updated**: 2020-04-20 09:45:43+00:00
- **Authors**: Matthias Rath, Alexandru Paul Condurache
- **Comment**: Accepted at ESANN 2020 (European Symposium on Artificial Neural
  Networks, Computational Intelligence and Machine Learning)
- **Journal**: None
- **Summary**: In this contribution, we show how to incorporate prior knowledge to a deep neural network architecture in a principled manner. We enforce feature space invariances using a novel layer based on invariant integration. This allows us to construct a complete feature space invariant to finite transformation groups.   We apply our proposed layer to explicitly insert invariance properties for vision-related classification tasks, demonstrate our approach for the case of rotation invariance and report state-of-the-art performance on the Rotated-MNIST dataset. Our method is especially beneficial when training with limited data.



### Pose Manipulation with Identity Preservation
- **Arxiv ID**: http://arxiv.org/abs/2004.09169v1
- **DOI**: 10.15837/ijccc.2020.2.3862
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.09169v1)
- **Published**: 2020-04-20 09:51:31+00:00
- **Updated**: 2020-04-20 09:51:31+00:00
- **Authors**: Andrei-Timotei Ardelean, Lucian Mircea Sasu
- **Comment**: 9 pages, journal article
- **Journal**: International Journal of Computers Communications & Control, Vol
  15, Nr 2, 3862, 2020
- **Summary**: This paper describes a new model which generates images in novel poses e.g. by altering face expression and orientation, from just a few instances of a human subject. Unlike previous approaches which require large datasets of a specific person for training, our approach may start from a scarce set of images, even from a single image. To this end, we introduce Character Adaptive Identity Normalization GAN (CainGAN) which uses spatial characteristic features extracted by an embedder and combined across source images. The identity information is propagated throughout the network by applying conditional normalization. After extensive adversarial training, CainGAN receives figures of faces from a certain individual and produces new ones while preserving the person's identity. Experimental results show that the quality of generated images scales with the size of the input set used during inference. Furthermore, quantitative measurements indicate that CainGAN performs better compared to other methods when training data is limited.



### GraN: An Efficient Gradient-Norm Based Detector for Adversarial and Misclassified Examples
- **Arxiv ID**: http://arxiv.org/abs/2004.09179v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.09179v1)
- **Published**: 2020-04-20 10:09:27+00:00
- **Updated**: 2020-04-20 10:09:27+00:00
- **Authors**: Julia Lust, Alexandru Paul Condurache
- **Comment**: Accepted at ESANN 2020 (European Symposium on Artificial Neural
  Networks, Computational Intelligence and Machine Learning)
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) are vulnerable to adversarial examples and other data perturbations. Especially in safety critical applications of DNNs, it is therefore crucial to detect misclassified samples. The current state-of-the-art detection methods require either significantly more runtime or more parameters than the original network itself. This paper therefore proposes GraN, a time- and parameter-efficient method that is easily adaptable to any DNN.   GraN is based on the layer-wise norm of the DNN's gradient regarding the loss of the current input-output combination, which can be computed via backpropagation. GraN achieves state-of-the-art performance on numerous problem set-ups.



### CMRNet++: Map and Camera Agnostic Monocular Visual Localization in LiDAR Maps
- **Arxiv ID**: http://arxiv.org/abs/2004.13795v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2004.13795v2)
- **Published**: 2020-04-20 10:10:14+00:00
- **Updated**: 2020-05-22 09:00:25+00:00
- **Authors**: Daniele Cattaneo, Domenico Giorgio Sorrenti, Abhinav Valada
- **Comment**: Spotlight talk at IEEE ICRA 2020 Workshop on Emerging Learning and
  Algorithmic Methods for Data Association in Robotics
- **Journal**: None
- **Summary**: Localization is a critically essential and crucial enabler of autonomous robots. While deep learning has made significant strides in many computer vision tasks, it is still yet to make a sizeable impact on improving capabilities of metric visual localization. One of the major hindrances has been the inability of existing Convolutional Neural Network (CNN)-based pose regression methods to generalize to previously unseen places. Our recently introduced CMRNet effectively addresses this limitation by enabling map independent monocular localization in LiDAR-maps. In this paper, we now take it a step further by introducing CMRNet++, which is a significantly more robust model that not only generalizes to new places effectively, but is also independent of the camera parameters. We enable this capability by combining deep learning with geometric techniques, and by moving the metric reasoning outside the learning process. In this way, the weights of the network are not tied to a specific camera. Extensive evaluations of CMRNet++ on three challenging autonomous driving datasets, i.e., KITTI, Argoverse, and Lyft5, show that CMRNet++ outperforms CMRNet as well as other baselines by a large margin. More importantly, for the first-time, we demonstrate the ability of a deep learning approach to accurately localize without any retraining or fine-tuning in a completely new environment and independent of the camera parameters.



### Landmark Detection and 3D Face Reconstruction for Caricature using a Nonlinear Parametric Model
- **Arxiv ID**: http://arxiv.org/abs/2004.09190v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2004.09190v2)
- **Published**: 2020-04-20 10:34:52+00:00
- **Updated**: 2021-03-08 12:52:28+00:00
- **Authors**: Hongrui Cai, Yudong Guo, Zhuang Peng, Juyong Zhang
- **Comment**: The code is available at https://github.com/Juyong/CaricatureFace
- **Journal**: None
- **Summary**: Caricature is an artistic abstraction of the human face by distorting or exaggerating certain facial features, while still retains a likeness with the given face. Due to the large diversity of geometric and texture variations, automatic landmark detection and 3D face reconstruction for caricature is a challenging problem and has rarely been studied before. In this paper, we propose the first automatic method for this task by a novel 3D approach. To this end, we first build a dataset with various styles of 2D caricatures and their corresponding 3D shapes, and then build a parametric model on vertex based deformation space for 3D caricature face. Based on the constructed dataset and the nonlinear parametric model, we propose a neural network based method to regress the 3D face shape and orientation from the input 2D caricature image. Ablation studies and comparison with state-of-the-art methods demonstrate the effectiveness of our algorithm design. Extensive experimental results demonstrate that our method works well for various caricatures. Our constructed dataset, source code and trained model are available at https://github.com/Juyong/CaricatureFace.



### LSM: Learning Subspace Minimization for Low-level Vision
- **Arxiv ID**: http://arxiv.org/abs/2004.09197v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.09197v1)
- **Published**: 2020-04-20 10:49:38+00:00
- **Updated**: 2020-04-20 10:49:38+00:00
- **Authors**: Chengzhou Tang, Lu Yuan, Ping Tan
- **Comment**: To be presented at CVPR2020
- **Journal**: None
- **Summary**: We study the energy minimization problem in low-level vision tasks from a novel perspective. We replace the heuristic regularization term with a learnable subspace constraint, and preserve the data term to exploit domain knowledge derived from the first principle of a task. This learning subspace minimization (LSM) framework unifies the network structures and the parameters for many low-level vision tasks, which allows us to train a single network for multiple tasks simultaneously with completely shared parameters, and even generalizes the trained network to an unseen task as long as its data term can be formulated. We demonstrate our LSM framework on four low-level tasks including interactive image segmentation, video segmentation, stereo matching, and optical flow, and validate the network on various datasets. The experiments show that the proposed LSM generates state-of-the-art results with smaller model size, faster training convergence, and real-time inference.



### Generative Feature Replay For Class-Incremental Learning
- **Arxiv ID**: http://arxiv.org/abs/2004.09199v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.09199v1)
- **Published**: 2020-04-20 10:58:20+00:00
- **Updated**: 2020-04-20 10:58:20+00:00
- **Authors**: Xialei Liu, Chenshen Wu, Mikel Menta, Luis Herranz, Bogdan Raducanu, Andrew D. Bagdanov, Shangling Jui, Joost van de Weijer
- **Comment**: Accepted at CVPR2020: Workshop on Continual Learning in Computer
  Vision
- **Journal**: None
- **Summary**: Humans are capable of learning new tasks without forgetting previous ones, while neural networks fail due to catastrophic forgetting between new and previously-learned tasks. We consider a class-incremental setting which means that the task-ID is unknown at inference time. The imbalance between old and new classes typically results in a bias of the network towards the newest ones. This imbalance problem can either be addressed by storing exemplars from previous tasks, or by using image replay methods. However, the latter can only be applied to toy datasets since image generation for complex datasets is a hard problem.   We propose a solution to the imbalance problem based on generative feature replay which does not require any exemplars. To do this, we split the network into two parts: a feature extractor and a classifier. To prevent forgetting, we combine generative feature replay in the classifier with feature distillation in the feature extractor. Through feature generation, our method reduces the complexity of generative replay and prevents the imbalance problem. Our approach is computationally efficient and scalable to large datasets. Experiments confirm that our approach achieves state-of-the-art results on CIFAR-100 and ImageNet, while requiring only a fraction of the storage needed for exemplar-based continual learning. Code available at \url{https://github.com/xialeiliu/GFR-IL}.



### CatNet: Class Incremental 3D ConvNets for Lifelong Egocentric Gesture Recognition
- **Arxiv ID**: http://arxiv.org/abs/2004.09215v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.09215v1)
- **Published**: 2020-04-20 11:36:02+00:00
- **Updated**: 2020-04-20 11:36:02+00:00
- **Authors**: Zhengwei Wang, Qi She, Tejo Chalasani, Aljosa Smolic
- **Comment**: CVPR 2020 Workshop at Continual Learning (CLVISION)
- **Journal**: None
- **Summary**: Egocentric gestures are the most natural form of communication for humans to interact with wearable devices such as VR/AR helmets and glasses. A major issue in such scenarios for real-world applications is that may easily become necessary to add new gestures to the system e.g., a proper VR system should allow users to customize gestures incrementally. Traditional deep learning methods require storing all previous class samples in the system and training the model again from scratch by incorporating previous samples and new samples, which costs humongous memory and significantly increases computation over time. In this work, we demonstrate a lifelong 3D convolutional framework -- c(C)la(a)ss increment(t)al net(Net)work (CatNet), which considers temporal information in videos and enables lifelong learning for egocentric gesture video recognition by learning the feature representation of an exemplar set selected from previous class samples. Importantly, we propose a two-stream CatNet, which deploys RGB and depth modalities to train two separate networks. We evaluate CatNets on a publicly available dataset -- EgoGesture dataset, and show that CatNets can learn many classes incrementally over a long period of time. Results also demonstrate that the two-stream architecture achieves the best performance on both joint training and class incremental training compared to 3 other one-stream architectures. The codes and pre-trained models used in this work are provided at https://github.com/villawang/CatNet.



### 4D Deep Learning for Multiple Sclerosis Lesion Activity Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2004.09216v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2004.09216v2)
- **Published**: 2020-04-20 11:41:01+00:00
- **Updated**: 2020-05-29 19:28:54+00:00
- **Authors**: Nils Gessert, Marcel Bengs, Julia Krüger, Roland Opfer, Ann-Christin Ostwaldt, Praveena Manogaran, Sven Schippling, Alexander Schlaefer
- **Comment**: Accepted at MIDL 2020
- **Journal**: None
- **Summary**: Multiple sclerosis lesion activity segmentation is the task of detecting new and enlarging lesions that appeared between a baseline and a follow-up brain MRI scan. While deep learning methods for single-scan lesion segmentation are common, deep learning approaches for lesion activity have only been proposed recently. Here, a two-path architecture processes two 3D MRI volumes from two time points. In this work, we investigate whether extending this problem to full 4D deep learning using a history of MRI volumes and thus an extended baseline can improve performance. For this purpose, we design a recurrent multi-encoder-decoder architecture for processing 4D data. We find that adding more temporal information is beneficial and our proposed architecture outperforms previous approaches with a lesion-wise true positive rate of 0.84 at a lesion-wise false positive rate of 0.19.



### Boosting Deep Open World Recognition by Clustering
- **Arxiv ID**: http://arxiv.org/abs/2004.13849v2
- **DOI**: 10.1109/LRA.2020.3010753
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2004.13849v2)
- **Published**: 2020-04-20 12:07:39+00:00
- **Updated**: 2020-11-30 09:35:37+00:00
- **Authors**: Dario Fontanel, Fabio Cermelli, Massimiliano Mancini, Samuel Rota Bulò, Elisa Ricci, Barbara Caputo
- **Comment**: IROS/RAL 2020
- **Journal**: IEEE Robotics and Automation Letters 2020
- **Summary**: While convolutional neural networks have brought significant advances in robot vision, their ability is often limited to closed world scenarios, where the number of semantic concepts to be recognized is determined by the available training set. Since it is practically impossible to capture all possible semantic concepts present in the real world in a single training set, we need to break the closed world assumption, equipping our robot with the capability to act in an open world. To provide such ability, a robot vision system should be able to (i) identify whether an instance does not belong to the set of known categories (i.e. open set recognition), and (ii) extend its knowledge to learn new classes over time (i.e. incremental learning). In this work, we show how we can boost the performance of deep open world recognition algorithms by means of a new loss formulation enforcing a global to local clustering of class-specific features. In particular, a first loss term, i.e. global clustering, forces the network to map samples closer to the class centroid they belong to while the second one, local clustering, shapes the representation space in such a way that samples of the same class get closer in the representation space while pushing away neighbours belonging to other classes. Moreover, we propose a strategy to learn class-specific rejection thresholds, instead of heuristically estimating a single global threshold, as in previous works. Experiments on RGB-D Object and Core50 datasets show the effectiveness of our approach.



### End-to-End Learning for Video Frame Compression with Self-Attention
- **Arxiv ID**: http://arxiv.org/abs/2004.09226v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.09226v1)
- **Published**: 2020-04-20 12:11:08+00:00
- **Updated**: 2020-04-20 12:11:08+00:00
- **Authors**: Nannan Zou, Honglei Zhang, Francesco Cricri, Hamed R. Tavakoli, Jani Lainema, Emre Aksu, Miska Hannuksela, Esa Rahtu
- **Comment**: None
- **Journal**: None
- **Summary**: One of the core components of conventional (i.e., non-learned) video codecs consists of predicting a frame from a previously-decoded frame, by leveraging temporal correlations. In this paper, we propose an end-to-end learned system for compressing video frames. Instead of relying on pixel-space motion (as with optical flow), our system learns deep embeddings of frames and encodes their difference in latent space. At decoder-side, an attention mechanism is designed to attend to the latent space of frames to decide how different parts of the previous and current frame are combined to form the final predicted current frame. Spatially-varying channel allocation is achieved by using importance masks acting on the feature-channels. The model is trained to reduce the bitrate by minimizing a loss on importance maps and a loss on the probability output by a context model for arithmetic coding. In our experiments, we show that the proposed system achieves high compression rates and high objective visual quality as measured by MS-SSIM and PSNR. Furthermore, we provide ablation studies where we highlight the contribution of different components.



### Unsupervised Person Re-identification via Multi-label Classification
- **Arxiv ID**: http://arxiv.org/abs/2004.09228v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.09228v1)
- **Published**: 2020-04-20 12:13:43+00:00
- **Updated**: 2020-04-20 12:13:43+00:00
- **Authors**: Dongkai Wang, Shiliang Zhang
- **Comment**: CVPR2020
- **Journal**: None
- **Summary**: The challenge of unsupervised person re-identification (ReID) lies in learning discriminative features without true labels. This paper formulates unsupervised person ReID as a multi-label classification task to progressively seek true labels. Our method starts by assigning each person image with a single-class label, then evolves to multi-label classification by leveraging the updated ReID model for label prediction. The label prediction comprises similarity computation and cycle consistency to ensure the quality of predicted labels. To boost the ReID model training efficiency in multi-label classification, we further propose the memory-based multi-label classification loss (MMCL). MMCL works with memory-based non-parametric classifier and integrates multi-label classification and single-label classification in a unified framework. Our label prediction and MMCL work iteratively and substantially boost the ReID performance. Experiments on several large-scale person ReID datasets demonstrate the superiority of our method in unsupervised person ReID. Our method also allows to use labeled person images in other domains. Under this transfer learning setting, our method also achieves state-of-the-art performance.



### AI-Driven CT-based quantification, staging and short-term outcome prediction of COVID-19 pneumonia
- **Arxiv ID**: http://arxiv.org/abs/2004.12852v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, physics.med-ph, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2004.12852v1)
- **Published**: 2020-04-20 12:24:08+00:00
- **Updated**: 2020-04-20 12:24:08+00:00
- **Authors**: Guillaume Chassagnon, Maria Vakalopoulou, Enzo Battistella, Stergios Christodoulidis, Trieu-Nghi Hoang-Thi, Severine Dangeard, Eric Deutsch, Fabrice Andre, Enora Guillo, Nara Halm, Stefany El Hajj, Florian Bompard, Sophie Neveu, Chahinez Hani, Ines Saab, Alienor Campredon, Hasmik Koulakian, Souhail Bennani, Gael Freche, Aurelien Lombard, Laure Fournier, Hippolyte Monnier, Teodor Grand, Jules Gregory, Antoine Khalil, Elyas Mahdjoub, Pierre-Yves Brillet, Stephane Tran Ba, Valerie Bousson, Marie-Pierre Revel, Nikos Paragios
- **Comment**: None
- **Journal**: None
- **Summary**: Chest computed tomography (CT) is widely used for the management of Coronavirus disease 2019 (COVID-19) pneumonia because of its availability and rapidity. The standard of reference for confirming COVID-19 relies on microbiological tests but these tests might not be available in an emergency setting and their results are not immediately available, contrary to CT. In addition to its role for early diagnosis, CT has a prognostic role by allowing visually evaluating the extent of COVID-19 lung abnormalities. The objective of this study is to address prediction of short-term outcomes, especially need for mechanical ventilation. In this multi-centric study, we propose an end-to-end artificial intelligence solution for automatic quantification and prognosis assessment by combining automatic CT delineation of lung disease meeting performance of experts and data-driven identification of biomarkers for its prognosis. AI-driven combination of variables with CT-based biomarkers offers perspectives for optimal patient management given the shortage of intensive care beds and ventilators.



### Unsupervised Vehicle Counting via Multiple Camera Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2004.09251v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.09251v2)
- **Published**: 2020-04-20 13:00:46+00:00
- **Updated**: 2020-09-13 17:34:22+00:00
- **Authors**: Luca Ciampi, Carlos Santiago, Joao Paulo Costeira, Claudio Gennaro, Giuseppe Amato
- **Comment**: 1st International Workshop on New Foundations for Human-Centered AI
  (NeHuAI) at ECAI-2020
- **Journal**: None
- **Summary**: Monitoring vehicle flows in cities is crucial to improve the urban environment and quality of life of citizens. Images are the best sensing modality to perceive and assess the flow of vehicles in large areas. Current technologies for vehicle counting in images hinge on large quantities of annotated data, preventing their scalability to city-scale as new cameras are added to the system. This is a recurrent problem when dealing with physical systems and a key research area in Machine Learning and AI. We propose and discuss a new methodology to design image-based vehicle density estimators with few labeled data via multiple camera domain adaptations.



### A Revised Generative Evaluation of Visual Dialogue
- **Arxiv ID**: http://arxiv.org/abs/2004.09272v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2004.09272v2)
- **Published**: 2020-04-20 13:26:45+00:00
- **Updated**: 2020-04-24 08:48:25+00:00
- **Authors**: Daniela Massiceti, Viveka Kulharia, Puneet K. Dokania, N. Siddharth, Philip H. S. Torr
- **Comment**: 16 pages, 5 figures
- **Journal**: None
- **Summary**: Evaluating Visual Dialogue, the task of answering a sequence of questions relating to a visual input, remains an open research challenge. The current evaluation scheme of the VisDial dataset computes the ranks of ground-truth answers in predefined candidate sets, which Massiceti et al. (2018) show can be susceptible to the exploitation of dataset biases. This scheme also does little to account for the different ways of expressing the same answer--an aspect of language that has been well studied in NLP. We propose a revised evaluation scheme for the VisDial dataset leveraging metrics from the NLP literature to measure consensus between answers generated by the model and a set of relevant answers. We construct these relevant answer sets using a simple and effective semi-supervised method based on correlation, which allows us to automatically extend and scale sparse relevance annotations from humans to the entire dataset. We release these sets and code for the revised evaluation scheme as DenseVisDial, and intend them to be an improvement to the dataset in the face of its existing constraints and design choices.



### Joint Spatial-Temporal Optimization for Stereo 3D Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/2004.09305v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.09305v1)
- **Published**: 2020-04-20 13:59:46+00:00
- **Updated**: 2020-04-20 13:59:46+00:00
- **Authors**: Peiliang Li, Jieqi Shi, Shaojie Shen
- **Comment**: cvpr2020
- **Journal**: None
- **Summary**: Directly learning multiple 3D objects motion from sequential images is difficult, while the geometric bundle adjustment lacks the ability to localize the invisible object centroid. To benefit from both the powerful object understanding skill from deep neural network meanwhile tackle precise geometry modeling for consistent trajectory estimation, we propose a joint spatial-temporal optimization-based stereo 3D object tracking method. From the network, we detect corresponding 2D bounding boxes on adjacent images and regress an initial 3D bounding box. Dense object cues (local depth and local coordinates) that associating to the object centroid are then predicted using a region-based network. Considering both the instant localization accuracy and motion consistency, our optimization models the relations between the object centroid and observed cues into a joint spatial-temporal error function. All historic cues will be summarized to contribute to the current estimation by a per-frame marginalization strategy without repeated computation. Quantitative evaluation on the KITTI tracking dataset shows our approach outperforms previous image-based 3D tracking methods by significant margins. We also report extensive results on multiple categories and larger datasets (KITTI raw and Argoverse Tracking) for future benchmarking.



### How Do Neural Networks Estimate Optical Flow? A Neuropsychology-Inspired Study
- **Arxiv ID**: http://arxiv.org/abs/2004.09317v2
- **DOI**: 10.1109/TPAMI.2021.3083538
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.09317v2)
- **Published**: 2020-04-20 14:08:28+00:00
- **Updated**: 2021-06-02 08:16:45+00:00
- **Authors**: D. B. de Jong, F. Paredes-Vallés, G. C. H. E. de Croon
- **Comment**: 16 pages, 15 figures
- **Journal**: IEEE Transactions on Pattern Analysis and Machine Intelligence,
  2021
- **Summary**: End-to-end trained convolutional neural networks have led to a breakthrough in optical flow estimation. The most recent advances focus on improving the optical flow estimation by improving the architecture and setting a new benchmark on the publicly available MPI-Sintel dataset. Instead, in this article, we investigate how deep neural networks estimate optical flow. A better understanding of how these networks function is important for (i) assessing their generalization capabilities to unseen inputs, and (ii) suggesting changes to improve their performance. For our investigation, we focus on FlowNetS, as it is the prototype of an encoder-decoder neural network for optical flow estimation. Furthermore, we use a filter identification method that has played a major role in uncovering the motion filters present in animal brains in neuropsychological research. The method shows that the filters in the deepest layer of FlowNetS are sensitive to a variety of motion patterns. Not only do we find translation filters, as demonstrated in animal brains, but thanks to the easier measurements in artificial neural networks, we even unveil dilation, rotation, and occlusion filters. Furthermore, we find similarities in the refinement part of the network and the perceptual filling-in process which occurs in the mammal primary visual cortex.



### Combining multimodal information for Metal Artefact Reduction: An unsupervised deep learning framework
- **Arxiv ID**: http://arxiv.org/abs/2004.09321v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.09321v1)
- **Published**: 2020-04-20 14:12:00+00:00
- **Updated**: 2020-04-20 14:12:00+00:00
- **Authors**: Marta B. M. Ranzini, Irme Groothuis, Kerstin Kläser, M. Jorge Cardoso, Johann Henckel, Sébastien Ourselin, Alister Hart, Marc Modat
- **Comment**: Accepted at IEEE International Symposium on Biomedical Imaging (ISBI)
  2020
- **Journal**: None
- **Summary**: Metal artefact reduction (MAR) techniques aim at removing metal-induced noise from clinical images. In Computed Tomography (CT), supervised deep learning approaches have been shown effective but limited in generalisability, as they mostly rely on synthetic data. In Magnetic Resonance Imaging (MRI) instead, no method has yet been introduced to correct the susceptibility artefact, still present even in MAR-specific acquisitions. In this work, we hypothesise that a multimodal approach to MAR would improve both CT and MRI. Given their different artefact appearance, their complementary information can compensate for the corrupted signal in either modality. We thus propose an unsupervised deep learning method for multimodal MAR. We introduce the use of Locally Normalised Cross Correlation as a loss term to encourage the fusion of multimodal information. Experiments show that our approach favours a smoother correction in the CT, while promoting signal recovery in the MRI.



### Robust Partial Matching for Person Search in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2004.09329v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.09329v1)
- **Published**: 2020-04-20 14:21:03+00:00
- **Updated**: 2020-04-20 14:21:03+00:00
- **Authors**: Yingji Zhong, Xiaoyu Wang, Shiliang Zhang
- **Comment**: 9 pages, 7 figures, accepted to CVPR 2020. The dataset will be
  released soon
- **Journal**: None
- **Summary**: Various factors like occlusions, backgrounds, etc., would lead to misaligned detected bounding boxes , e.g., ones covering only portions of human body. This issue is common but overlooked by previous person search works. To alleviate this issue, this paper proposes an Align-to-Part Network (APNet) for person detection and re-Identification (reID). APNet refines detected bounding boxes to cover the estimated holistic body regions, from which discriminative part features can be extracted and aligned. Aligned part features naturally formulate reID as a partial feature matching procedure, where valid part features are selected for similarity computation, while part features on occluded or noisy regions are discarded. This design enhances the robustness of person search to real-world challenges with marginal computation overhead. This paper also contributes a Large-Scale dataset for Person Search in the wild (LSPS), which is by far the largest and the most challenging dataset for person search. Experiments show that APNet brings considerable performance improvement on LSPS. Meanwhile, it achieves competitive performance on existing person search benchmarks like CUHK-SYSU and PRW.



### Deep-COVID: Predicting COVID-19 From Chest X-Ray Images Using Deep Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/2004.09363v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.09363v3)
- **Published**: 2020-04-20 15:09:14+00:00
- **Updated**: 2020-07-21 14:10:22+00:00
- **Authors**: Shervin Minaee, Rahele Kafieh, Milan Sonka, Shakib Yazdani, Ghazaleh Jamalipour Soufi
- **Comment**: Accepted by Medical Image Analysis
- **Journal**: None
- **Summary**: The COVID-19 pandemic is causing a major outbreak in more than 150 countries around the world, having a severe impact on the health and life of many people globally. One of the crucial step in fighting COVID-19 is the ability to detect the infected patients early enough, and put them under special care. Detecting this disease from radiography and radiology images is perhaps one of the fastest ways to diagnose the patients. Some of the early studies showed specific abnormalities in the chest radiograms of patients infected with COVID-19. Inspired by earlier works, we study the application of deep learning models to detect COVID-19 patients from their chest radiography images. We first prepare a dataset of 5,000 Chest X-rays from the publicly available datasets. Images exhibiting COVID-19 disease presence were identified by board-certified radiologist. Transfer learning on a subset of 2,000 radiograms was used to train four popular convolutional neural networks, including ResNet18, ResNet50, SqueezeNet, and DenseNet-121, to identify COVID-19 disease in the analyzed chest X-ray images. We evaluated these models on the remaining 3,000 images, and most of these networks achieved a sensitivity rate of 98% ($\pm$ 3%), while having a specificity rate of around 90%. Besides sensitivity and specificity rates, we also present the receiver operating characteristic (ROC) curve, precision-recall curve, average prediction, and confusion matrix of each model. We also used a technique to generate heatmaps of lung regions potentially infected by COVID-19 and show that the generated heatmaps contain most of the infected areas annotated by our board certified radiologist. While the achieved performance is very encouraging, further analysis is required on a larger set of COVID-19 images, to have a more reliable estimation of accuracy rates. The dataset, model implementations (in PyTorch), and evaluations, are all made publicly available for research community at https://github.com/shervinmin/DeepCovid.git



### Complex-Object Visual Inspection via Multiple Lighting Configurations
- **Arxiv ID**: http://arxiv.org/abs/2004.09374v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.09374v1)
- **Published**: 2020-04-20 15:25:03+00:00
- **Updated**: 2020-04-20 15:25:03+00:00
- **Authors**: Maya Aghaei, Matteo Bustreo, Pietro Morerio, Nicolo Carissimi, Alessio Del Bue, Vittorio Murino
- **Comment**: 8 pages, 7 figures, submitted to ICPR2020
- **Journal**: None
- **Summary**: The design of an automatic visual inspection system is usually performed in two stages. While the first stage consists in selecting the most suitable hardware setup for highlighting most effectively the defects on the surface to be inspected, the second stage concerns the development of algorithmic solutions to exploit the potentials offered by the collected data.   In this paper, first, we present a novel illumination setup embedding four illumination configurations to resemble diffused, dark-field, and front lighting techniques. Second, we analyze the contributions brought by deploying the proposed setup in training phase only - mimicking the scenario in which an already developed visual inspection system cannot be modified on the customer site - and in evaluation phase. Along with an exhaustive set of experiments, in this paper, we demonstrate the suitability of the proposed setup for effective illumination of complex-objects, defined as manufactured items with variable surface characteristics that cannot be determined a priori. Moreover, we discuss the importance of multiple light configurations availability during training and their natural boosting effect which, without the need to modify the system design in evaluation phase, lead to improvements in the overall system performance.



### Pseudo-healthy synthesis with pathology disentanglement and adversarial learning
- **Arxiv ID**: http://arxiv.org/abs/2005.01607v3
- **DOI**: 10.1016/j.media.2020.101719
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.01607v3)
- **Published**: 2020-04-20 15:38:05+00:00
- **Updated**: 2021-06-18 11:39:10+00:00
- **Authors**: Tian Xia, Agisilaos Chartsias, Sotirios A. Tsaftaris
- **Comment**: This paper has been accepted by Medical Image Analysis
- **Journal**: None
- **Summary**: Pseudo-healthy synthesis is the task of creating a subject-specific `healthy' image from a pathological one. Such images can be helpful in tasks such as anomaly detection and understanding changes induced by pathology and disease. In this paper, we present a model that is encouraged to disentangle the information of pathology from what seems to be healthy. We disentangle what appears to be healthy and where disease is as a segmentation map, which are then recombined by a network to reconstruct the input disease image. We train our models adversarially using either paired or unpaired settings, where we pair disease images and maps when available. We quantitatively and subjectively, with a human study, evaluate the quality of pseudo-healthy images using several criteria. We show in a series of experiments, performed on ISLES, BraTS and Cam-CAN datasets, that our method is better than several baselines and methods from the literature. We also show that due to better training processes we could recover deformations, on surrounding tissue, caused by disease. Our implementation is publicly available at https://github.com/xiat0616/pseudo-healthy-synthesis. This paper has been accepted by Medical Image Analysis: https://doi.org/10.1016/j.media.2020.101719.



### Class Distribution Alignment for Adversarial Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2004.09403v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.09403v1)
- **Published**: 2020-04-20 15:58:11+00:00
- **Updated**: 2020-04-20 15:58:11+00:00
- **Authors**: Wanqi Yang, Tong Ling, Chengmei Yang, Lei Wang, Yinghuan Shi, Luping Zhou, Ming Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Most existing unsupervised domain adaptation methods mainly focused on aligning the marginal distributions of samples between the source and target domains. This setting does not sufficiently consider the class distribution information between the two domains, which could adversely affect the reduction of domain gap. To address this issue, we propose a novel approach called Conditional ADversarial Image Translation (CADIT) to explicitly align the class distributions given samples between the two domains. It integrates a discriminative structure-preserving loss and a joint adversarial generation loss. The former effectively prevents undesired label-flipping during the whole process of image translation, while the latter maintains the joint distribution alignment of images and labels. Furthermore, our approach enforces the classification consistence of target domain images before and after adaptation to aid the classifier training in both domains. Extensive experiments were conducted on multiple benchmark datasets including Digits, Faces, Scenes and Office31, showing that our approach achieved superior classification in the target domain when compared to the state-of-the-art methods. Also, both qualitative and quantitative results well supported our motivation that aligning the class distributions can indeed improve domain adaptation.



### Quality Guided Sketch-to-Photo Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2005.02133v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2005.02133v1)
- **Published**: 2020-04-20 16:00:01+00:00
- **Updated**: 2020-04-20 16:00:01+00:00
- **Authors**: Uche Osahor, Hadi Kazemi, Ali Dabouei, Nasser Nasrabadi
- **Comment**: 10 pages, 9 figures
- **Journal**: None
- **Summary**: Facial sketches drawn by artists are widely used for visual identification applications and mostly by law enforcement agencies, but the quality of these sketches depend on the ability of the artist to clearly replicate all the key facial features that could aid in capturing the true identity of a subject. Recent works have attempted to synthesize these sketches into plausible visual images to improve visual recognition and identification. However, synthesizing photo-realistic images from sketches proves to be an even more challenging task, especially for sensitive applications such as suspect identification. In this work, we propose a novel approach that adopts a generative adversarial network that synthesizes a single sketch into multiple synthetic images with unique attributes like hair color, sex, etc. We incorporate a hybrid discriminator which performs attribute classification of multiple target attributes, a quality guided encoder that minimizes the perceptual dissimilarity of the latent space embedding of the synthesized and real image at different layers in the network and an identity preserving network that maintains the identity of the synthesised image throughout the training process. Our approach is aimed at improving the visual appeal of the synthesised images while incorporating multiple attribute assignment to the generator without compromising the identity of the synthesised image. We synthesised sketches using XDOG filter for the CelebA, WVU Multi-modal and CelebA-HQ datasets and from an auxiliary generator trained on sketches from CUHK, IIT-D and FERET datasets. Our results are impressive compared to current state of the art.



### Five Points to Check when Comparing Visual Perception in Humans and Machines
- **Arxiv ID**: http://arxiv.org/abs/2004.09406v3
- **DOI**: 10.1167/jov.21.3.16
- **Categories**: **cs.CV**, cs.AI, cs.LG, q-bio.NC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.09406v3)
- **Published**: 2020-04-20 16:05:36+00:00
- **Updated**: 2021-04-13 16:03:20+00:00
- **Authors**: Christina M. Funke, Judy Borowski, Karolina Stosio, Wieland Brendel, Thomas S. A. Wallis, Matthias Bethge
- **Comment**: V3: minor changes like in published JOV version
  (https://doi.org/10.1167/jov.21.3.16) V2: New title; added general section
  (checklist); manuscript restructured such that each case study is one
  chapter; adversarial examples in first study replaced by different analysis
- **Journal**: Journal of Vision 21, no. 3 (2021): 16-16
- **Summary**: With the rise of machines to human-level performance in complex recognition tasks, a growing amount of work is directed towards comparing information processing in humans and machines. These studies are an exciting chance to learn about one system by studying the other. Here, we propose ideas on how to design, conduct and interpret experiments such that they adequately support the investigation of mechanisms when comparing human and machine perception. We demonstrate and apply these ideas through three case studies. The first case study shows how human bias can affect how we interpret results, and that several analytic tools can help to overcome this human reference point. In the second case study, we highlight the difference between necessary and sufficient mechanisms in visual reasoning tasks. Thereby, we show that contrary to previous suggestions, feedback mechanisms might not be necessary for the tasks in question. The third case study highlights the importance of aligning experimental conditions. We find that a previously-observed difference in object recognition does not hold when adapting the experiment to make conditions more equitable between humans and machines. In presenting a checklist for comparative studies of visual reasoning in humans and machines, we hope to highlight how to overcome potential pitfalls in design or inference.



### Shape-Oriented Convolution Neural Network for Point Cloud Analysis
- **Arxiv ID**: http://arxiv.org/abs/2004.09411v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.09411v1)
- **Published**: 2020-04-20 16:11:51+00:00
- **Updated**: 2020-04-20 16:11:51+00:00
- **Authors**: Chaoyi Zhang, Yang Song, Lina Yao, Weidong Cai
- **Comment**: 8 pages, 6 figures, AAAI2020
- **Journal**: https://www.aaai.org/Papers/AAAI/2020GB/AAAI-ZhangC.4075.pdf
- **Summary**: Point cloud is a principal data structure adopted for 3D geometric information encoding. Unlike other conventional visual data, such as images and videos, these irregular points describe the complex shape features of 3D objects, which makes shape feature learning an essential component of point cloud analysis. To this end, a shape-oriented message passing scheme dubbed ShapeConv is proposed to focus on the representation learning of the underlying shape formed by each local neighboring point. Despite this intra-shape relationship learning, ShapeConv is also designed to incorporate the contextual effects from the inter-shape relationship through capturing the long-ranged dependencies between local underlying shapes. This shape-oriented operator is stacked into our hierarchical learning architecture, namely Shape-Oriented Convolutional Neural Network (SOCNN), developed for point cloud analysis. Extensive experiments have been performed to evaluate its significance in the tasks of point cloud classification and part segmentation.



### Characters as Graphs: Recognizing Online Handwritten Chinese Characters via Spatial Graph Convolutional Network
- **Arxiv ID**: http://arxiv.org/abs/2004.09412v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.09412v1)
- **Published**: 2020-04-20 16:12:03+00:00
- **Updated**: 2020-04-20 16:12:03+00:00
- **Authors**: Ji Gan, Weiqiang Wang, Ke Lu
- **Comment**: 8 pages, 4 figures. A full version of this paper has been submitted
  to an international journal
- **Journal**: None
- **Summary**: Chinese is one of the most widely used languages in the world, yet online handwritten Chinese character recognition (OLHCCR) remains challenging. To recognize Chinese characters, one popular choice is to adopt the 2D convolutional neural network (2D-CNN) on the extracted feature images, and another one is to employ the recurrent neural network (RNN) or 1D-CNN on the time-series features. Instead of viewing characters as either static images or temporal trajectories, here we propose to represent characters as geometric graphs, retaining both spatial structures and temporal orders. Accordingly, we propose a novel spatial graph convolution network (SGCN) to effectively classify those character graphs for the first time. Specifically, our SGCN incorporates the local neighbourhood information via spatial graph convolutions and further learns the global shape properties with a hierarchical residual structure. Experiments on IAHCC-UCAS2016, ICDAR-2013, and UNIPEN datasets demonstrate that the SGCN can achieve comparable recognition performance with the state-of-the-art methods for character recognition.



### Improving correlation method with convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/2004.09430v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.09430v1)
- **Published**: 2020-04-20 16:36:01+00:00
- **Updated**: 2020-04-20 16:36:01+00:00
- **Authors**: Dmitriy Goncharov, Rostislav Starikov
- **Comment**: 8 pages, 3 figures, 2 tables, 1 formula
- **Journal**: None
- **Summary**: We present a convolutional neural network for the classification of correlation responses obtained by correlation filters. The proposed approach can improve the accuracy of classification, as well as achieve invariance to the image classes and parameters.



### A Spatially Constrained Deep Convolutional Neural Network for Nerve Fiber Segmentation in Corneal Confocal Microscopic Images using Inaccurate Annotations
- **Arxiv ID**: http://arxiv.org/abs/2004.09443v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.09443v1)
- **Published**: 2020-04-20 16:56:13+00:00
- **Updated**: 2020-04-20 16:56:13+00:00
- **Authors**: Ning Zhang, Susan Francis, Rayaz Malik, Xin Chen
- **Comment**: 4 pages, accepted for publication at IEEE International Symposium on
  Biomedical Imaging (ISBI) 2020
- **Journal**: 2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI
  2020)
- **Summary**: Semantic image segmentation is one of the most important tasks in medical image analysis. Most state-of-the-art deep learning methods require a large number of accurately annotated examples for model training. However, accurate annotation is difficult to obtain especially in medical applications. In this paper, we propose a spatially constrained deep convolutional neural network (DCNN) to achieve smooth and robust image segmentation using inaccurately annotated labels for training. In our proposed method, image segmentation is formulated as a graph optimization problem that is solved by a DCNN model learning process. The cost function to be optimized consists of a unary term that is calculated by cross entropy measurement and a pairwise term that is based on enforcing a local label consistency. The proposed method has been evaluated based on corneal confocal microscopic (CCM) images for nerve fiber segmentation, where accurate annotations are extremely difficult to be obtained. Based on both the quantitative result of a synthetic dataset and qualitative assessment of a real dataset, the proposed method has achieved superior performance in producing high quality segmentation results even with inaccurate labels for training.



### Adversarial Distortion for Learned Video Compression
- **Arxiv ID**: http://arxiv.org/abs/2004.09508v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.09508v3)
- **Published**: 2020-04-20 17:06:31+00:00
- **Updated**: 2021-06-18 18:42:25+00:00
- **Authors**: Vijay Veerabadran, Reza Pourreza, Amirhossein Habibian, Taco Cohen
- **Comment**: CVPR Workshops, 2020
- **Journal**: None
- **Summary**: In this paper, we present a novel adversarial lossy video compression model. At extremely low bit-rates, standard video coding schemes suffer from unpleasant reconstruction artifacts such as blocking, ringing etc. Existing learned neural approaches to video compression have achieved reasonable success on reducing the bit-rate for efficient transmission and reduce the impact of artifacts to an extent. However, they still tend to produce blurred results under extreme compression. In this paper, we present a deep adversarial learned video compression model that minimizes an auxiliary adversarial distortion objective. We find this adversarial objective to correlate better with human perceptual quality judgement relative to traditional quality metrics such as MS-SSIM and PSNR. Our experiments using a state-of-the-art learned video compression system demonstrate a reduction of perceptual artifacts and reconstruction of detail lost especially under extremely high compression.



### Music Gesture for Visual Sound Separation
- **Arxiv ID**: http://arxiv.org/abs/2004.09476v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2004.09476v1)
- **Published**: 2020-04-20 17:53:46+00:00
- **Updated**: 2020-04-20 17:53:46+00:00
- **Authors**: Chuang Gan, Deng Huang, Hang Zhao, Joshua B. Tenenbaum, Antonio Torralba
- **Comment**: CVPR 2020. Project page: http://music-gesture.csail.mit.edu
- **Journal**: None
- **Summary**: Recent deep learning approaches have achieved impressive performance on visual sound separation tasks. However, these approaches are mostly built on appearance and optical flow like motion feature representations, which exhibit limited abilities to find the correlations between audio signals and visual points, especially when separating multiple instruments of the same types, such as multiple violins in a scene. To address this, we propose "Music Gesture," a keypoint-based structured representation to explicitly model the body and finger movements of musicians when they perform music. We first adopt a context-aware graph network to integrate visual semantic context with body dynamics, and then apply an audio-visual fusion model to associate body movements with the corresponding audio signals. Experimental results on three music performance datasets show: 1) strong improvements upon benchmark metrics for hetero-musical separation tasks (i.e. different instruments); 2) new ability for effective homo-musical separation for piano, flute, and trumpet duets, which to our best knowledge has never been achieved with alternative methods. Project page: http://music-gesture.csail.mit.edu.



### Bringing Old Photos Back to Life
- **Arxiv ID**: http://arxiv.org/abs/2004.09484v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.09484v1)
- **Published**: 2020-04-20 17:59:23+00:00
- **Updated**: 2020-04-20 17:59:23+00:00
- **Authors**: Ziyu Wan, Bo Zhang, Dongdong Chen, Pan Zhang, Dong Chen, Jing Liao, Fang Wen
- **Comment**: CVPR 2020 Oral, project website: http://raywzy.com/Old_Photo/
- **Journal**: None
- **Summary**: We propose to restore old photos that suffer from severe degradation through a deep learning approach. Unlike conventional restoration tasks that can be solved through supervised learning, the degradation in real photos is complex and the domain gap between synthetic images and real old photos makes the network fail to generalize. Therefore, we propose a novel triplet domain translation network by leveraging real photos along with massive synthetic image pairs. Specifically, we train two variational autoencoders (VAEs) to respectively transform old photos and clean photos into two latent spaces. And the translation between these two latent spaces is learned with synthetic paired data. This translation generalizes well to real photos because the domain gap is closed in the compact latent space. Besides, to address multiple degradations mixed in one old photo, we design a global branch with a partial nonlocal block targeting to the structured defects, such as scratches and dust spots, and a local branch targeting to the unstructured defects, such as noises and blurriness. Two branches are fused in the latent space, leading to improved capability to restore old photos from multiple defects. The proposed method outperforms state-of-the-art methods in terms of visual quality for old photos restoration.



### AANet: Adaptive Aggregation Network for Efficient Stereo Matching
- **Arxiv ID**: http://arxiv.org/abs/2004.09548v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.09548v1)
- **Published**: 2020-04-20 18:07:55+00:00
- **Updated**: 2020-04-20 18:07:55+00:00
- **Authors**: Haofei Xu, Juyong Zhang
- **Comment**: CVPR 2020. The improved version AANet+ is also included. Code:
  https://github.com/haofeixu/aanet
- **Journal**: None
- **Summary**: Despite the remarkable progress made by learning based stereo matching algorithms, one key challenge remains unsolved. Current state-of-the-art stereo models are mostly based on costly 3D convolutions, the cubic computational complexity and high memory consumption make it quite expensive to deploy in real-world applications. In this paper, we aim at completely replacing the commonly used 3D convolutions to achieve fast inference speed while maintaining comparable accuracy. To this end, we first propose a sparse points based intra-scale cost aggregation method to alleviate the well-known edge-fattening issue at disparity discontinuities. Further, we approximate traditional cross-scale cost aggregation algorithm with neural network layers to handle large textureless regions. Both modules are simple, lightweight, and complementary, leading to an effective and efficient architecture for cost aggregation. With these two modules, we can not only significantly speed up existing top-performing models (e.g., $41\times$ than GC-Net, $4\times$ than PSMNet and $38\times$ than GA-Net), but also improve the performance of fast stereo models (e.g., StereoNet). We also achieve competitive results on Scene Flow and KITTI datasets while running at 62ms, demonstrating the versatility and high efficiency of the proposed method. Our full framework is available at https://github.com/haofeixu/aanet .



### Utilizing Mask R-CNN for Waterline Detection in Canoe Sprint Video Analysis
- **Arxiv ID**: http://arxiv.org/abs/2004.09573v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.09573v1)
- **Published**: 2020-04-20 19:00:45+00:00
- **Updated**: 2020-04-20 19:00:45+00:00
- **Authors**: Marie-Sophie von Braun, Patrick Frenzel, Christian Käding, Mirco Fuchs
- **Comment**: (Accepted / In press) 2020 IEEE/CVF Conference on Computer Vision and
  Pattern Recognition Workshop (CVPRW)
- **Journal**: None
- **Summary**: Determining a waterline in images recorded in canoe sprint training is an important component for the kinematic parameter analysis to assess an athlete's performance. Here, we propose an approach for the automated waterline detection. First, we utilized a pre-trained Mask R-CNN by means of transfer learning for canoe segmentation. Second, we developed a multi-stage approach to estimate a waterline from the outline of the segments. It consists of two linear regression stages and the systematic selection of canoe parts. We then introduced a parameterization of the waterline as a basis for further evaluations. Next, we conducted a study among several experts to estimate the ground truth waterlines. This not only included an average waterline drawn from the individual experts annotations but, more importantly, a measure for the uncertainty between individual results. Finally, we assessed our method with respect to the question whether the predicted waterlines are in accordance with the experts annotations. Our method demonstrated a high performance and provides opportunities for new applications in the field of automated video analysis in canoe sprint.



### LSQ+: Improving low-bit quantization through learnable offsets and better initialization
- **Arxiv ID**: http://arxiv.org/abs/2004.09576v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.09576v1)
- **Published**: 2020-04-20 19:04:51+00:00
- **Updated**: 2020-04-20 19:04:51+00:00
- **Authors**: Yash Bhalgat, Jinwon Lee, Markus Nagel, Tijmen Blankevoort, Nojun Kwak
- **Comment**: Camera-ready for Joint Workshop on Efficient Deep Learning in
  Computer Vision, CVPR 2020
- **Journal**: None
- **Summary**: Unlike ReLU, newer activation functions (like Swish, H-swish, Mish) that are frequently employed in popular efficient architectures can also result in negative activation values, with skewed positive and negative ranges. Typical learnable quantization schemes [PACT, LSQ] assume unsigned quantization for activations and quantize all negative activations to zero which leads to significant loss in performance. Naively using signed quantization to accommodate these negative values requires an extra sign bit which is expensive for low-bit (2-, 3-, 4-bit) quantization. To solve this problem, we propose LSQ+, a natural extension of LSQ, wherein we introduce a general asymmetric quantization scheme with trainable scale and offset parameters that can learn to accommodate the negative activations. Gradient-based learnable quantization schemes also commonly suffer from high instability or variance in the final training performance, hence requiring a great deal of hyper-parameter tuning to reach a satisfactory performance. LSQ+ alleviates this problem by using an MSE-based initialization scheme for the quantization parameters. We show that this initialization leads to significantly lower variance in final performance across multiple training runs. Overall, LSQ+ shows state-of-the-art results for EfficientNet and MixNet and also significantly outperforms LSQ for low-bit quantization of neural nets with Swish activations (e.g.: 1.8% gain with W4A4 quantization and upto 5.6% gain with W2A2 quantization of EfficientNet-B0 on ImageNet dataset). To the best of our knowledge, ours is the first work to quantize such architectures to extremely low bit-widths.



### Deep variational network for rapid 4D flow MRI reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2004.09610v1
- **DOI**: 10.1038/s42256-020-0165-6
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.09610v1)
- **Published**: 2020-04-20 20:17:49+00:00
- **Updated**: 2020-04-20 20:17:49+00:00
- **Authors**: Valery Vishnevskiy, Jonas Walheim, Sebastian Kozerke
- **Comment**: 15 pages, 6 figures
- **Journal**: Nature Machine Intelligence 2, 228-235 (2020)
- **Summary**: Phase-contrast magnetic resonance imaging (MRI) provides time-resolved quantification of blood flow dynamics that can aid clinical diagnosis. Long in vivo scan times due to repeated three-dimensional (3D) volume sampling over cardiac phases and breathing cycles necessitate accelerated imaging techniques that leverage data correlations. Standard compressed sensing reconstruction methods require tuning of hyperparameters and are computationally expensive, which diminishes the potential reduction of examination times. We propose an efficient model-based deep neural reconstruction network and evaluate its performance on clinical aortic flow data. The network is shown to reconstruct undersampled 4D flow MRI data in under a minute on standard consumer hardware. Remarkably, the relatively low amounts of tunable parameters allowed the network to be trained on images from 11 reference scans while generalizing well to retrospective and prospective undersampled data for various acceleration factors and anatomies.



### Self-Supervised Feature Extraction for 3D Axon Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2004.09629v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.09629v1)
- **Published**: 2020-04-20 20:46:04+00:00
- **Updated**: 2020-04-20 20:46:04+00:00
- **Authors**: Tzofi Klinghoffer, Peter Morales, Young-Gyun Park, Nicholas Evans, Kwanghun Chung, Laura J. Brattain
- **Comment**: Accepted to CVPR Computer Vision for Microscopy Image Analysis
  Workshop 2020. 7 pages. 3 Figures
- **Journal**: None
- **Summary**: Existing learning-based methods to automatically trace axons in 3D brain imagery often rely on manually annotated segmentation labels. Labeling is a labor-intensive process and is not scalable to whole-brain analysis, which is needed for improved understanding of brain function. We propose a self-supervised auxiliary task that utilizes the tube-like structure of axons to build a feature extractor from unlabeled data. The proposed auxiliary task constrains a 3D convolutional neural network (CNN) to predict the order of permuted slices in an input 3D volume. By solving this task, the 3D CNN is able to learn features without ground-truth labels that are useful for downstream segmentation with the 3D U-Net model. To the best of our knowledge, our model is the first to perform automated segmentation of axons imaged at subcellular resolution with the SHIELD technique. We demonstrate improved segmentation performance over the 3D U-Net model on both the SHIELD PVGPe dataset and the BigNeuron Project, single neuron Janelia dataset.



### Intelligent Querying for Target Tracking in Camera Networks using Deep Q-Learning with n-Step Bootstrapping
- **Arxiv ID**: http://arxiv.org/abs/2004.09632v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.09632v1)
- **Published**: 2020-04-20 20:49:52+00:00
- **Updated**: 2020-04-20 20:49:52+00:00
- **Authors**: Anil Sharma, Saket Anand, Sanjit K. Kaul
- **Comment**: Camera Selections for Target Tracking
- **Journal**: None
- **Summary**: Surveillance camera networks are a useful infrastructure for various visual analytics applications, where high-level inferences and predictions could be made based on target tracking across the network. Most multi-camera tracking works focus on target re-identification and trajectory association problems to track the target. However, since camera networks can generate enormous amount of video data, inefficient schemes for making re-identification or trajectory association queries can incur prohibitively large computational requirements. In this paper, we address the problem of intelligent scheduling of re-identification queries in a multi-camera tracking setting. To this end, we formulate the target tracking problem in a camera network as an MDP and learn a reinforcement learning based policy that selects a camera for making a re-identification query. The proposed approach to camera selection does not assume the knowledge of the camera network topology but the resulting policy implicitly learns it. We have also shown that such a policy can be learnt directly from data. Using the NLPR MCT and the Duke MTMC multi-camera multi-target tracking benchmarks, we empirically show that the proposed approach substantially reduces the number of frames queried.



### Local Clustering with Mean Teacher for Semi-supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2004.09665v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.09665v2)
- **Published**: 2020-04-20 22:31:31+00:00
- **Updated**: 2020-07-24 00:47:01+00:00
- **Authors**: Zexi Chen, Benjamin Dutton, Bharathkumar Ramachandra, Tianfu Wu, Ranga Raju Vatsavai
- **Comment**: 8 pages, 7 figures
- **Journal**: None
- **Summary**: The Mean Teacher (MT) model of Tarvainen and Valpola has shown favorable performance on several semi-supervised benchmark datasets. MT maintains a teacher model's weights as the exponential moving average of a student model's weights and minimizes the divergence between their probability predictions under diverse perturbations of the inputs. However, MT is known to suffer from confirmation bias, that is, reinforcing incorrect teacher model predictions. In this work, we propose a simple yet effective method called Local Clustering (LC) to mitigate the effect of confirmation bias. In MT, each data point is considered independent of other points during training; however, data points are likely to be close to each other in feature space if they share similar features. Motivated by this, we cluster data points locally by minimizing the pairwise distance between neighboring data points in feature space. Combined with a standard classification cross-entropy objective on labeled data points, the misclassified unlabeled data points are pulled towards high-density regions of their correct class with the help of their neighbors, thus improving model performance. We demonstrate on semi-supervised benchmark datasets SVHN and CIFAR-10 that adding our LC loss to MT yields significant improvements compared to MT and performance comparable to the state of the art in semi-supervised learning.



### Data Efficient and Weakly Supervised Computational Pathology on Whole Slide Images
- **Arxiv ID**: http://arxiv.org/abs/2004.09666v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, q-bio.TO
- **Links**: [PDF](http://arxiv.org/pdf/2004.09666v2)
- **Published**: 2020-04-20 23:00:13+00:00
- **Updated**: 2020-05-22 02:03:49+00:00
- **Authors**: Ming Y. Lu, Drew F. K. Williamson, Tiffany Y. Chen, Richard J. Chen, Matteo Barbieri, Faisal Mahmood
- **Comment**: None
- **Journal**: None
- **Summary**: The rapidly emerging field of computational pathology has the potential to enable objective diagnosis, therapeutic response prediction and identification of new morphological features of clinical relevance. However, deep learning-based computational pathology approaches either require manual annotation of gigapixel whole slide images (WSIs) in fully-supervised settings or thousands of WSIs with slide-level labels in a weakly-supervised setting. Moreover, whole slide level computational pathology methods also suffer from domain adaptation and interpretability issues. These challenges have prevented the broad adaptation of computational pathology for clinical and research purposes. Here we present CLAM - Clustering-constrained attention multiple instance learning, an easy-to-use, high-throughput, and interpretable WSI-level processing and learning method that only requires slide-level labels while being data efficient, adaptable and capable of handling multi-class subtyping problems. CLAM is a deep-learning-based weakly-supervised method that uses attention-based learning to automatically identify sub-regions of high diagnostic value in order to accurately classify the whole slide, while also utilizing instance-level clustering over the representative regions identified to constrain and refine the feature space. In three separate analyses, we demonstrate the data efficiency and adaptability of CLAM and its superior performance over standard weakly-supervised classification. We demonstrate that CLAM models are interpretable and can be used to identify well-known and new morphological features. We further show that models trained using CLAM are adaptable to independent test cohorts, cell phone microscopy images, and biopsies. CLAM is a general-purpose and adaptable method that can be used for a variety of different computational pathology tasks in both clinical and research settings.



### LRCN-RetailNet: A recurrent neural network architecture for accurate people counting
- **Arxiv ID**: http://arxiv.org/abs/2004.09672v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.09672v2)
- **Published**: 2020-04-20 23:25:37+00:00
- **Updated**: 2020-05-12 16:21:12+00:00
- **Authors**: Lucas Massa, Adriano Barbosa, Krerley Oliveira, Thales Vieira
- **Comment**: None
- **Journal**: None
- **Summary**: Measuring and analyzing the flow of customers in retail stores is essential for a retailer to better comprehend customers' behavior and support decision-making. Nevertheless, not much attention has been given to the development of novel technologies for automatic people counting. We introduce LRCN-RetailNet: a recurrent neural network architecture capable of learning a non-linear regression model and accurately predicting the people count from videos captured by low-cost surveillance cameras. The input video format follows the recently proposed RGBP image format, which is comprised of color and people (foreground) information. Our architecture is capable of considering two relevant aspects: spatial features extracted through convolutional layers from the RGBP images; and the temporal coherence of the problem, which is exploited by recurrent layers. We show that, through a supervised learning approach, the trained models are capable of predicting the people count with high accuracy. Additionally, we present and demonstrate that a straightforward modification of the methodology is effective to exclude salespeople from the people count. Comprehensive experiments were conducted to validate, evaluate and compare the proposed architecture. Results corroborated that LRCN-RetailNet remarkably outperforms both the previous RetailNet architecture, which was limited to evaluating a single image per iteration; and a state-of-the-art neural network for object detection. Finally, computational performance experiments confirmed that the entire methodology is effective to estimate people count in real-time.



### Facial Action Unit Intensity Estimation via Semantic Correspondence Learning with Dynamic Graph Convolution
- **Arxiv ID**: http://arxiv.org/abs/2004.09681v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.09681v1)
- **Published**: 2020-04-20 23:55:30+00:00
- **Updated**: 2020-04-20 23:55:30+00:00
- **Authors**: Yingruo Fan, Jacqueline C. K. Lam, Victor O. K. Li
- **Comment**: Accepted at AAAI2020
- **Journal**: None
- **Summary**: The intensity estimation of facial action units (AUs) is challenging due to subtle changes in the person's facial appearance. Previous approaches mainly rely on probabilistic models or predefined rules for modeling co-occurrence relationships among AUs, leading to limited generalization. In contrast, we present a new learning framework that automatically learns the latent relationships of AUs via establishing semantic correspondences between feature maps. In the heatmap regression-based network, feature maps preserve rich semantic information associated with AU intensities and locations. Moreover, the AU co-occurring pattern can be reflected by activating a set of feature channels, where each channel encodes a specific visual pattern of AU. This motivates us to model the correlation among feature channels, which implicitly represents the co-occurrence relationship of AU intensity levels. Specifically, we introduce a semantic correspondence convolution (SCC) module to dynamically compute the correspondences from deep and low resolution feature maps, and thus enhancing the discriminability of features. The experimental results demonstrate the effectiveness and the superior performance of our method on two benchmark datasets.



