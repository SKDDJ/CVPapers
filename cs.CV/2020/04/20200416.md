# Arxiv Papers in cs.CV on 2020-04-16
### Representation Learning of Histopathology Images using Graph Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2004.07399v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.07399v2)
- **Published**: 2020-04-16 00:09:20+00:00
- **Updated**: 2020-04-17 16:39:26+00:00
- **Authors**: Mohammed Adnan, Shivam Kalra, Hamid R. Tizhoosh
- **Comment**: Published in CVMI at CVPR Workshops, 2020
- **Journal**: None
- **Summary**: Representation learning for Whole Slide Images (WSIs) is pivotal in developing image-based systems to achieve higher precision in diagnostic pathology. We propose a two-stage framework for WSI representation learning. We sample relevant patches using a color-based method and use graph neural networks to learn relations among sampled patches to aggregate the image information into a single vector representation. We introduce attention via graph pooling to automatically infer patches with higher relevance. We demonstrate the performance of our approach for discriminating two sub-types of lung cancers, Lung Adenocarcinoma (LUAD) & Lung Squamous Cell Carcinoma (LUSC). We collected 1,026 lung cancer WSIs with the 40$\times$ magnification from The Cancer Genome Atlas (TCGA) dataset, the largest public repository of histopathology images and achieved state-of-the-art accuracy of 88.8% and AUC of 0.89 on lung cancer sub-type classification by extracting features from a pre-trained DenseNet



### Iteratively Pruned Deep Learning Ensembles for COVID-19 Detection in Chest X-rays
- **Arxiv ID**: http://arxiv.org/abs/2004.08379v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML, I.1.2; I.1.4; I.1.5
- **Links**: [PDF](http://arxiv.org/pdf/2004.08379v3)
- **Published**: 2020-04-16 00:09:29+00:00
- **Updated**: 2021-03-05 15:05:31+00:00
- **Authors**: Sivaramakrishnan Rajaraman, Jen Siegelman, Philip O. Alderson, Lucas S. Folio, Les R. Folio, Sameer K. Antani
- **Comment**: 11 pages, 8 figures, IEEE Access journal published version
- **Journal**: None
- **Summary**: We demonstrate use of iteratively pruned deep learning model ensembles for detecting pulmonary manifestation of COVID-19 with chest X-rays. This disease is caused by the novel Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-CoV-2) virus, also known as the novel Coronavirus (2019-nCoV). A custom convolutional neural network and a selection of ImageNet pretrained models are trained and evaluated at patient-level on publicly available CXR collections to learn modality-specific feature representations. The learned knowledge is transferred and fine-tuned to improve performance and generalization in the related task of classifying CXRs as normal, showing bacterial pneumonia, or COVID-19-viral abnormalities. The best performing models are iteratively pruned to reduce complexity and improve memory efficiency. The predictions of the best-performing pruned models are combined through different ensemble strategies to improve classification performance. Empirical evaluations demonstrate that the weighted average of the best-performing pruned models significantly improves performance resulting in an accuracy of 99.01% and area under the curve of 0.9972 in detecting COVID-19 findings on CXRs. The combined use of modality-specific knowledge transfer, iterative model pruning, and ensemble learning resulted in improved predictions. We expect that this model can be quickly adopted for COVID-19 screening using chest radiographs.



### Radiologist-Level COVID-19 Detection Using CT Scans with Detail-Oriented Capsule Networks
- **Arxiv ID**: http://arxiv.org/abs/2004.07407v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.07407v1)
- **Published**: 2020-04-16 00:48:32+00:00
- **Updated**: 2020-04-16 00:48:32+00:00
- **Authors**: Aryan Mobiny, Pietro Antonio Cicalese, Samira Zare, Pengyu Yuan, Mohammadsajad Abavisani, Carol C. Wu, Jitesh Ahuja, Patricia M. de Groot, Hien Van Nguyen
- **Comment**: None
- **Journal**: None
- **Summary**: Radiographic images offer an alternative method for the rapid screening and monitoring of Coronavirus Disease 2019 (COVID-19) patients. This approach is limited by the shortage of radiology experts who can provide a timely interpretation of these images. Motivated by this challenge, our paper proposes a novel learning architecture, called Detail-Oriented Capsule Networks (DECAPS), for the automatic diagnosis of COVID-19 from Computed Tomography (CT) scans. Our network combines the strength of Capsule Networks with several architecture improvements meant to boost classification accuracies. First, DECAPS uses an Inverted Dynamic Routing mechanism which increases model stability by preventing the passage of information from non-descriptive regions. Second, DECAPS employs a Peekaboo training procedure which uses a two-stage patch crop and drop strategy to encourage the network to generate activation maps for every target concept. The network then uses the activation maps to focus on regions of interest and combines both coarse and fine-grained representations of the data. Finally, we use a data augmentation method based on conditional generative adversarial networks to deal with the issue of data scarcity. Our model achieves 84.3% precision, 91.5% recall, and 96.1% area under the ROC curve, significantly outperforming state-of-the-art methods. We compare the performance of the DECAPS model with three experienced, well-trained thoracic radiologists and show that the architecture significantly outperforms them. While further studies on larger datasets are required to confirm this finding, our results imply that architectures like DECAPS can be used to assist radiologists in the CT scan mediated diagnosis of COVID-19.



### Combinatorial 3D Shape Generation via Sequential Assembly
- **Arxiv ID**: http://arxiv.org/abs/2004.07414v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.07414v2)
- **Published**: 2020-04-16 01:23:14+00:00
- **Updated**: 2020-11-25 03:51:49+00:00
- **Authors**: Jungtaek Kim, Hyunsoo Chung, Jinhwi Lee, Minsu Cho, Jaesik Park
- **Comment**: 14 pages, 20 figures, 1 table, presented at NeurIPS 2020 Workshop on
  Machine Learning for Engineering Modeling, Simulation, and Design
- **Journal**: None
- **Summary**: Sequential assembly with geometric primitives has drawn attention in robotics and 3D vision since it yields a practical blueprint to construct a target shape. However, due to its combinatorial property, a greedy method falls short of generating a sequence of volumetric primitives. To alleviate this consequence induced by a huge number of feasible combinations, we propose a combinatorial 3D shape generation framework. The proposed framework reflects an important aspect of human generation processes in real life -- we often create a 3D shape by sequentially assembling unit primitives with geometric constraints. To find the desired combination regarding combination evaluations, we adopt Bayesian optimization, which is able to exploit and explore efficiently the feasible regions constrained by the current primitive placements. An evaluation function conveys global structure guidance for an assembly process and stability in terms of gravity and external forces simultaneously. Experimental results demonstrate that our method successfully generates combinatorial 3D shapes and simulates more realistic generation processes. We also introduce a new dataset for combinatorial 3D shape generation. All the codes are available at \url{https://github.com/POSTECH-CVLab/Combinatorial-3D-Shape-Generation}.



### Measuring Human and Economic Activity from Satellite Imagery to Support City-Scale Decision-Making during COVID-19 Pandemic
- **Arxiv ID**: http://arxiv.org/abs/2004.07438v4
- **DOI**: 10.1109/TBDATA.2020.3032839
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.07438v4)
- **Published**: 2020-04-16 03:47:11+00:00
- **Updated**: 2020-11-12 14:45:11+00:00
- **Authors**: Rodrigo Minetto, Mauricio Pamplona Segundo, Gilbert Rotich, Sudeep Sarkar
- **Comment**: 13 pages, 10 figures, 2 tables
- **Journal**: None
- **Summary**: The COVID-19 outbreak forced governments worldwide to impose lockdowns and quarantines to prevent virus transmission. As a consequence, there are disruptions in human and economic activities all over the globe. The recovery process is also expected to be rough. Economic activities impact social behaviors, which leave signatures in satellite images that can be automatically detected and classified. Satellite imagery can support the decision-making of analysts and policymakers by providing a different kind of visibility into the unfolding economic changes. In this work, we use a deep learning approach that combines strategic location sampling and an ensemble of lightweight convolutional neural networks (CNNs) to recognize specific elements in satellite images that could be used to compute economic indicators based on it, automatically. This CNN ensemble framework ranked third place in the US Department of Defense xView challenge, the most advanced benchmark for object detection in satellite images. We show the potential of our framework for temporal analysis using the US IARPA Function Map of the World (fMoW) dataset. We also show results on real examples of different sites before and after the COVID-19 outbreak to illustrate different measurable indicators. Our code and annotated high-resolution aerial scenes before and after the outbreak are available on GitHub (https://github.com/maups/covid19-satellite-analysis).



### Relational Modeling for Robust and Efficient Pulmonary Lobe Segmentation in CT Scans
- **Arxiv ID**: http://arxiv.org/abs/2004.07443v4
- **DOI**: 10.1109/TMI.2020.2995108
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.07443v4)
- **Published**: 2020-04-16 03:54:12+00:00
- **Updated**: 2020-05-12 16:20:51+00:00
- **Authors**: Weiyi Xie, Colin Jacobs, Jean-Paul Charbonnier, Bram van Ginneken
- **Comment**: None
- **Journal**: None
- **Summary**: Pulmonary lobe segmentation in computed tomography scans is essential for regional assessment of pulmonary diseases. Recent works based on convolution neural networks have achieved good performance for this task. However, they are still limited in capturing structured relationships due to the nature of convolution. The shape of the pulmonary lobes affect each other and their borders relate to the appearance of other structures, such as vessels, airways, and the pleural wall. We argue that such structural relationships play a critical role in the accurate delineation of pulmonary lobes when the lungs are affected by diseases such as COVID-19 or COPD.   In this paper, we propose a relational approach (RTSU-Net) that leverages structured relationships by introducing a novel non-local neural network module. The proposed module learns both visual and geometric relationships among all convolution features to produce self-attention weights.   With a limited amount of training data available from COVID-19 subjects, we initially train and validate RTSU-Net on a cohort of 5000 subjects from the COPDGene study (4000 for training and 1000 for evaluation). Using models pre-trained on COPDGene, we apply transfer learning to retrain and evaluate RTSU-Net on 470 COVID-19 suspects (370 for retraining and 100 for evaluation). Experimental results show that RTSU-Net outperforms three baselines and performs robustly on cases with severe lung infection due to COVID-19.



### Single upper limb pose estimation method based on improved stacked hourglass network
- **Arxiv ID**: http://arxiv.org/abs/2004.07456v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.07456v1)
- **Published**: 2020-04-16 04:48:40+00:00
- **Updated**: 2020-04-16 04:48:40+00:00
- **Authors**: Gang Peng, Yuezhi Zheng, Jianfeng Li, Jin Yang, Zhonghua Deng
- **Comment**: None
- **Journal**: None
- **Summary**: At present, most high-accuracy single-person pose estimation methods have high computational complexity and insufficient real-time performance due to the complex structure of the network model. However, a single-person pose estimation method with high real-time performance also needs to improve its accuracy due to the simple structure of the network model. It is currently difficult to achieve both high accuracy and real-time performance in single-person pose estimation. For use in human-machine cooperative operations, this paper proposes a single-person upper limb pose estimation method based on an end-to-end approach for accurate and real-time limb pose estimation. Using the stacked hourglass network model, a single-person upper limb skeleton key point detection model was designed.Deconvolution was employed to replace the up-sampling operation of the hourglass module in the original model, solving the problem of rough feature maps. Integral regression was used to calculate the position coordinates of key points of the skeleton, reducing quantization errors and calculations. Experiments showed that the developed single-person upper limb skeleton key point detection model achieves high accuracy and that the pose estimation method based on the end-to-end approach provides high accuracy and real-time performance.



### PICK: Processing Key Information Extraction from Documents using Improved Graph Learning-Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/2004.07464v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.07464v3)
- **Published**: 2020-04-16 05:20:16+00:00
- **Updated**: 2020-07-18 08:13:53+00:00
- **Authors**: Wenwen Yu, Ning Lu, Xianbiao Qi, Ping Gong, Rong Xiao
- **Comment**: Accepted by ICPR2020. Code at
  https://github.com/wenwenyu/PICK-pytorch
- **Journal**: None
- **Summary**: Computer vision with state-of-the-art deep learning models has achieved huge success in the field of Optical Character Recognition (OCR) including text detection and recognition tasks recently. However, Key Information Extraction (KIE) from documents as the downstream task of OCR, having a large number of use scenarios in real-world, remains a challenge because documents not only have textual features extracting from OCR systems but also have semantic visual features that are not fully exploited and play a critical role in KIE. Too little work has been devoted to efficiently make full use of both textual and visual features of the documents. In this paper, we introduce PICK, a framework that is effective and robust in handling complex documents layout for KIE by combining graph learning with graph convolution operation, yielding a richer semantic representation containing the textual and visual features and global layout without ambiguity. Extensive experiments on real-world datasets have been conducted to show that our method outperforms baselines methods by significant margins. Our code is available at https://github.com/wenwenyu/PICK-pytorch.



### SQE: a Self Quality Evaluation Metric for Parameters Optimization in Multi-Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/2004.07472v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.07472v1)
- **Published**: 2020-04-16 06:07:29+00:00
- **Updated**: 2020-04-16 06:07:29+00:00
- **Authors**: Yanru Huang, Feiyu Zhu, Zheni Zeng, Xi Qiu, Yuan Shen, Jianan Wu
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel self quality evaluation metric SQE for parameters optimization in the challenging yet critical multi-object tracking task. Current evaluation metrics all require annotated ground truth, thus will fail in the test environment and realistic circumstances prohibiting further optimization after training. By contrast, our metric reflects the internal characteristics of trajectory hypotheses and measures tracking performance without ground truth. We demonstrate that trajectories with different qualities exhibit different single or multiple peaks over feature distance distribution, inspiring us to design a simple yet effective method to assess the quality of trajectories using a two-class Gaussian mixture model. Experiments mainly on MOT16 Challenge data sets verify the effectiveness of our method in both correlating with existing metrics and enabling parameters self-optimization to achieve better performance. We believe that our conclusions and method are inspiring for future multi-object tracking in practice.



### The Role of the Hercules Autonomous Vehicle During the COVID-19 Pandemic: An Autonomous Logistic Vehicle for Contactless Goods Transportation
- **Arxiv ID**: http://arxiv.org/abs/2004.07480v2
- **DOI**: 10.1109/MRA.2020.3045040
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.07480v2)
- **Published**: 2020-04-16 06:37:06+00:00
- **Updated**: 2021-02-17 04:43:52+00:00
- **Authors**: Tianyu Liu, Qinghai Liao, Lu Gan, Fulong Ma, Jie Cheng, Xupeng Xie, Zhe Wang, Yingbing Chen, Yilong Zhu, Shuyang Zhang, Zhengyong Chen, Yang Liu, Meng Xie, Yang Yu, Zitong Guo, Guang Li, Peidong Yuan, Dong Han, Yuying Chen, Haoyang Ye, Jianhao Jiao, Peng Yun, Zhenhua Xu, Hengli Wang, Huaiyang Huang, Sukai Wang, Peide Cai, Yuxiang Sun, Yandong Liu, Lujia Wang, Ming Liu
- **Comment**: None
- **Journal**: IEEE Robotics and Automation Magazine, 2021
- **Summary**: Since early 2020, the coronavirus disease 2019 (COVID-19) has spread rapidly across the world. As at the date of writing this article, the disease has been globally reported in 223 countries and regions, infected over 108 million people and caused over 2.4 million deaths (https://covid19.who.int/, accessed on Feb. 17, 2021). Avoiding person-to-person transmission is an effective approach to control and prevent the pandemic. However, many daily activities, such as transporting goods in our daily life, inevitably involve person-to-person contact. Using an autonomous logistic vehicle to achieve contact-less goods transportation could alleviate this issue. For example, it can reduce the risk of virus transmission between the driver and customers. Moreover, many countries have imposed tough lockdown measures to reduce the virus transmission (e.g., retail, catering) during the pandemic, which causes inconveniences for human daily life. Autonomous vehicle can deliver the goods bought by humans, so that humans can get the goods without going out. These demands motivate us to develop an autonomous vehicle, named as Hercules, for contact-less goods transportation during the COVID-19 pandemic. The vehicle is evaluated through real-world delivering tasks under various traffic conditions.



### ArTIST: Autoregressive Trajectory Inpainting and Scoring for Tracking
- **Arxiv ID**: http://arxiv.org/abs/2004.07482v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.07482v1)
- **Published**: 2020-04-16 06:43:11+00:00
- **Updated**: 2020-04-16 06:43:11+00:00
- **Authors**: Fatemeh Saleh, Sadegh Aliakbarian, Mathieu Salzmann, Stephen Gould
- **Comment**: None
- **Journal**: None
- **Summary**: One of the core components in online multiple object tracking (MOT) frameworks is associating new detections with existing tracklets, typically done via a scoring function. Despite the great advances in MOT, designing a reliable scoring function remains a challenge. In this paper, we introduce a probabilistic autoregressive generative model to score tracklet proposals by directly measuring the likelihood that a tracklet represents natural motion. One key property of our model is its ability to generate multiple likely futures of a tracklet given partial observations. This allows us to not only score tracklets but also effectively maintain existing tracklets when the detector fails to detect some objects even for a long time, e.g., due to occlusion, by sampling trajectories so as to inpaint the gaps caused by misdetection. Our experiments demonstrate the effectiveness of our approach to scoring and inpainting tracklets on several MOT benchmark datasets. We additionally show the generality of our generative model by using it to produce future representations in the challenging task of human motion prediction.



### Asynchronous Interaction Aggregation for Action Detection
- **Arxiv ID**: http://arxiv.org/abs/2004.07485v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.07485v1)
- **Published**: 2020-04-16 07:03:20+00:00
- **Updated**: 2020-04-16 07:03:20+00:00
- **Authors**: Jiajun Tang, Jin Xia, Xinzhi Mu, Bo Pang, Cewu Lu
- **Comment**: None
- **Journal**: None
- **Summary**: Understanding interaction is an essential part of video action detection. We propose the Asynchronous Interaction Aggregation network (AIA) that leverages different interactions to boost action detection. There are two key designs in it: one is the Interaction Aggregation structure (IA) adopting a uniform paradigm to model and integrate multiple types of interaction; the other is the Asynchronous Memory Update algorithm (AMU) that enables us to achieve better performance by modeling very long-term interaction dynamically without huge computation cost. We provide empirical evidence to show that our network can gain notable accuracy from the integrative interactions and is easy to train end-to-end. Our method reports the new state-of-the-art performance on AVA dataset, with 3.7 mAP gain (12.6% relative improvement) on validation split comparing to our strong baseline. The results on dataset UCF101-24 and EPIC-Kitchens further illustrate the effectiveness of our approach. Source code will be made public at: https://github.com/MVIG-SJTU/AlphAction .



### A Local Descriptor with Physiological Characteristic for Finger Vein Recognition
- **Arxiv ID**: http://arxiv.org/abs/2004.07489v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.07489v1)
- **Published**: 2020-04-16 07:22:28+00:00
- **Updated**: 2020-04-16 07:22:28+00:00
- **Authors**: Liping Zhang, Weijun Li, Xin Ning
- **Comment**: None
- **Journal**: None
- **Summary**: Local feature descriptors exhibit great superiority in finger vein recognition due to their stability and robustness against local changes in images. However, most of these are methods use general-purpose descriptors that do not consider finger vein-specific features. In this work, we propose a finger vein-specific local feature descriptors based physiological characteristic of finger vein patterns, i.e., histogram of oriented physiological Gabor responses (HOPGR), for finger vein recognition. First, prior of directional characteristic of finger vein patterns is obtained in an unsupervised manner. Then the physiological Gabor filter banks are set up based on the prior information to extract the physiological responses and orientation. Finally, to make feature has robustness against local changes in images, histogram is generated as output by dividing the image into non-overlapping cells and overlapping blocks. Extensive experimental results on several databases clearly demonstrate that the proposed method outperforms most current state-of-the-art finger vein recognition methods.



### Continual Learning with Extended Kronecker-factored Approximate Curvature
- **Arxiv ID**: http://arxiv.org/abs/2004.07507v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.07507v1)
- **Published**: 2020-04-16 07:58:47+00:00
- **Updated**: 2020-04-16 07:58:47+00:00
- **Authors**: Janghyeon Lee, Hyeong Gwon Hong, Donggyu Joo, Junmo Kim
- **Comment**: CVPR 2020
- **Journal**: None
- **Summary**: We propose a quadratic penalty method for continual learning of neural networks that contain batch normalization (BN) layers. The Hessian of a loss function represents the curvature of the quadratic penalty function, and a Kronecker-factored approximate curvature (K-FAC) is used widely to practically compute the Hessian of a neural network. However, the approximation is not valid if there is dependence between examples, typically caused by BN layers in deep network architectures. We extend the K-FAC method so that the inter-example relations are taken into account and the Hessian of deep neural networks can be properly approximated under practical assumptions. We also propose a method of weight merging and reparameterization to properly handle statistical parameters of BN, which plays a critical role for continual learning with BN, and a method that selects hyperparameters without source task data. Our method shows better performance than baselines in the permuted MNIST task with BN layers and in sequential learning from the ImageNet classification task to fine-grained classification tasks with ResNet-50, without any explicit or implicit use of source task data for hyperparameter selection.



### Explainable Image Classification with Evidence Counterfactual
- **Arxiv ID**: http://arxiv.org/abs/2004.07511v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.07511v1)
- **Published**: 2020-04-16 08:02:48+00:00
- **Updated**: 2020-04-16 08:02:48+00:00
- **Authors**: Tom Vermeire, David Martens
- **Comment**: 23 pages, 13 figures
- **Journal**: None
- **Summary**: The complexity of state-of-the-art modeling techniques for image classification impedes the ability to explain model predictions in an interpretable way. Existing explanation methods generally create importance rankings in terms of pixels or pixel groups. However, the resulting explanations lack an optimal size, do not consider feature dependence and are only related to one class. Counterfactual explanation methods are considered promising to explain complex model decisions, since they are associated with a high degree of human interpretability. In this paper, SEDC is introduced as a model-agnostic instance-level explanation method for image classification to obtain visual counterfactual explanations. For a given image, SEDC searches a small set of segments that, in case of removal, alters the classification. As image classification tasks are typically multiclass problems, SEDC-T is proposed as an alternative method that allows specifying a target counterfactual class. We compare SEDC(-T) with popular feature importance methods such as LRP, LIME and SHAP, and we describe how the mentioned importance ranking issues are addressed. Moreover, concrete examples and experiments illustrate the potential of our approach (1) to obtain trust and insight, and (2) to obtain input for model improvement by explaining misclassifications.



### Local-Global Video-Text Interactions for Temporal Grounding
- **Arxiv ID**: http://arxiv.org/abs/2004.07514v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.07514v1)
- **Published**: 2020-04-16 08:10:41+00:00
- **Updated**: 2020-04-16 08:10:41+00:00
- **Authors**: Jonghwan Mun, Minsu Cho, Bohyung Han
- **Comment**: CVPR 2020; code available in
  https://github.com/JonghwanMun/LGI4temporalgrounding
- **Journal**: None
- **Summary**: This paper addresses the problem of text-to-video temporal grounding, which aims to identify the time interval in a video semantically relevant to a text query. We tackle this problem using a novel regression-based model that learns to extract a collection of mid-level features for semantic phrases in a text query, which corresponds to important semantic entities described in the query (e.g., actors, objects, and actions), and reflect bi-modal interactions between the linguistic features of the query and the visual features of the video in multiple levels. The proposed method effectively predicts the target time interval by exploiting contextual information from local to global during bi-modal interactions. Through in-depth ablation studies, we find out that incorporating both local and global context in video and text interactions is crucial to the accurate grounding. Our experiment shows that the proposed method outperforms the state of the arts on Charades-STA and ActivityNet Captions datasets by large margins, 7.44\% and 4.61\% points at Recall@tIoU=0.5 metric, respectively. Code is available in https://github.com/JonghwanMun/LGI4temporalgrounding.



### DeepFakes Evolution: Analysis of Facial Regions and Fake Detection Performance
- **Arxiv ID**: http://arxiv.org/abs/2004.07532v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2004.07532v2)
- **Published**: 2020-04-16 08:49:32+00:00
- **Updated**: 2020-07-02 16:24:22+00:00
- **Authors**: Ruben Tolosana, Sergio Romero-Tapiador, Julian Fierrez, Ruben Vera-Rodriguez
- **Comment**: None
- **Journal**: Proc. International Conference on Pattern Recognition Workshops
  2021
- **Summary**: Media forensics has attracted a lot of attention in the last years in part due to the increasing concerns around DeepFakes. Since the initial DeepFake databases from the 1st generation such as UADFV and FaceForensics++ up to the latest databases of the 2nd generation such as Celeb-DF and DFDC, many visual improvements have been carried out, making fake videos almost indistinguishable to the human eye. This study provides an exhaustive analysis of both 1st and 2nd DeepFake generations in terms of facial regions and fake detection performance. Two different methods are considered in our experimental framework: i) the traditional one followed in the literature and based on selecting the entire face as input to the fake detection system, and ii) a novel approach based on the selection of specific facial regions as input to the fake detection system.   Among all the findings resulting from our experiments, we highlight the poor fake detection results achieved even by the strongest state-of-the-art fake detectors in the latest DeepFake databases of the 2nd generation, with Equal Error Rate results ranging from 15% to 30%. These results remark the necessity of further research to develop more sophisticated fake detectors.



### Fast Template Matching and Update for Video Object Tracking and Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2004.07538v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.07538v1)
- **Published**: 2020-04-16 08:58:45+00:00
- **Updated**: 2020-04-16 08:58:45+00:00
- **Authors**: Mingjie Sun, Jimin Xiao, Eng Gee Lim, Bingfeng Zhang, Yao Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, the main task we aim to tackle is the multi-instance semi-supervised video object segmentation across a sequence of frames where only the first-frame box-level ground-truth is provided. Detection-based algorithms are widely adopted to handle this task, and the challenges lie in the selection of the matching method to predict the result as well as to decide whether to update the target template using the newly predicted result. The existing methods, however, make these selections in a rough and inflexible way, compromising their performance. To overcome this limitation, we propose a novel approach which utilizes reinforcement learning to make these two decisions at the same time. Specifically, the reinforcement learning agent learns to decide whether to update the target template according to the quality of the predicted result. The choice of the matching method will be determined at the same time, based on the action history of the reinforcement learning agent. Experiments show that our method is almost 10 times faster than the previous state-of-the-art method with even higher accuracy (region similarity of 69.1% on DAVIS 2017 dataset).



### Label Propagation Adaptive Resonance Theory for Semi-supervised Continuous Learning
- **Arxiv ID**: http://arxiv.org/abs/2005.02137v1
- **DOI**: 10.1109/ICASSP40776.2020.9054655
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.02137v1)
- **Published**: 2020-04-16 09:12:56+00:00
- **Updated**: 2020-04-16 09:12:56+00:00
- **Authors**: Taehyeong Kim, Injune Hwang, Gi-Cheon Kang, Won-Seok Choi, Hyunseo Kim, Byoung-Tak Zhang
- **Comment**: 5 pages, 2 figures, 1 table, accepted in ICASSP 2020
- **Journal**: None
- **Summary**: Semi-supervised learning and continuous learning are fundamental paradigms for human-level intelligence. To deal with real-world problems where labels are rarely given and the opportunity to access the same data is limited, it is necessary to apply these two paradigms in a joined fashion. In this paper, we propose Label Propagation Adaptive Resonance Theory (LPART) for semi-supervised continuous learning. LPART uses an online label propagation mechanism to perform classification and gradually improves its accuracy as the observed data accumulates. We evaluated the proposed model on visual (MNIST, SVHN, CIFAR-10) and audio (NSynth) datasets by adjusting the ratio of the labeled and unlabeled data. The accuracies are much higher when both labeled and unlabeled data are used, demonstrating the significant advantage of LPART in environments where the data labels are scarce.



### Multimodal and multiview distillation for real-time player detection on a football field
- **Arxiv ID**: http://arxiv.org/abs/2004.07544v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.07544v1)
- **Published**: 2020-04-16 09:16:20+00:00
- **Updated**: 2020-04-16 09:16:20+00:00
- **Authors**: Anthony Cioppa, Adrien Deliège, Noor Ul Huda, Rikke Gade, Marc Van Droogenbroeck, Thomas B. Moeslund
- **Comment**: Accepted for the CVSports workshop of CVPR 2020 ; 8 pages +
  references
- **Journal**: None
- **Summary**: Monitoring the occupancy of public sports facilities is essential to assess their use and to motivate their construction in new places. In the case of a football field, the area to cover is large, thus several regular cameras should be used, which makes the setup expensive and complex. As an alternative, we developed a system that detects players from a unique cheap and wide-angle fisheye camera assisted by a single narrow-angle thermal camera. In this work, we train a network in a knowledge distillation approach in which the student and the teacher have different modalities and a different view of the same scene. In particular, we design a custom data augmentation combined with a motion detection algorithm to handle the training in the region of the fisheye camera not covered by the thermal one. We show that our solution is effective in detecting players on the whole field filmed by the fisheye camera. We evaluate it quantitatively and qualitatively in the case of an online distillation, where the student detects players in real time while being continuously adapted to the latest video conditions.



### Visible fingerprint of X-ray images of epoxy resins using singular value decomposition of deep learning features
- **Arxiv ID**: http://arxiv.org/abs/2004.11968v2
- **DOI**: 10.1016/j.commatsci.2020.109996
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.11968v2)
- **Published**: 2020-04-16 09:44:08+00:00
- **Updated**: 2020-09-23 03:46:45+00:00
- **Authors**: Edgar Avalos, Kazuto Akagi, Yasumasa Nishiura
- **Comment**: 43 pages, 16 figures
- **Journal**: COMMAT Volume 186, January 2021, 109996
- **Summary**: Although the process variables of epoxy resins alter their mechanical properties, the visual identification of the characteristic features of X-ray images of samples of these materials is challenging. To facilitate the identification, we approximate the magnitude of the gradient of the intensity field of the X-ray images of different kinds of epoxy resins and then we use deep learning to discover the most representative features of the transformed images. In this solution of the inverse problem to finding characteristic features to discriminate samples of heterogeneous materials, we use the eigenvectors obtained from the singular value decomposition of all the channels of the feature maps of the early layers in a convolutional neural network. While the strongest activated channel gives a visual representation of the characteristic features, often these are not robust enough in some practical settings. On the other hand, the left singular vectors of the matrix decomposition of the feature maps, barely change when variables such as the capacity of the network or network architecture change. High classification accuracy and robustness of characteristic features are presented in this work.



### Learning to Detect Important People in Unlabelled Images for Semi-supervised Important People Detection
- **Arxiv ID**: http://arxiv.org/abs/2004.07568v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.07568v1)
- **Published**: 2020-04-16 10:09:37+00:00
- **Updated**: 2020-04-16 10:09:37+00:00
- **Authors**: Fa-Ting Hong, Wei-Hong Li, Wei-Shi Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: Important people detection is to automatically detect the individuals who play the most important roles in a social event image, which requires the designed model to understand a high-level pattern. However, existing methods rely heavily on supervised learning using large quantities of annotated image samples, which are more costly to collect for important people detection than for individual entity recognition (eg, object recognition). To overcome this problem, we propose learning important people detection on partially annotated images. Our approach iteratively learns to assign pseudo-labels to individuals in un-annotated images and learns to update the important people detection model based on data with both labels and pseudo-labels. To alleviate the pseudo-labelling imbalance problem, we introduce a ranking strategy for pseudo-label estimation, and also introduce two weighting strategies: one for weighting the confidence that individuals are important people to strengthen the learning on important people and the other for neglecting noisy unlabelled images (ie, images without any important people). We have collected two large-scale datasets for evaluation. The extensive experimental results clearly confirm the efficacy of our method attained by leveraging unlabelled images for improving the performance of important people detection.



### Spatially Attentive Output Layer for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2004.07570v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.07570v1)
- **Published**: 2020-04-16 10:11:38+00:00
- **Updated**: 2020-04-16 10:11:38+00:00
- **Authors**: Ildoo Kim, Woonhyuk Baek, Sungwoong Kim
- **Comment**: First two authors contributed equally. Accepted at CVPR 2020
- **Journal**: None
- **Summary**: Most convolutional neural networks (CNNs) for image classification use a global average pooling (GAP) followed by a fully-connected (FC) layer for output logits. However, this spatial aggregation procedure inherently restricts the utilization of location-specific information at the output layer, although this spatial information can be beneficial for classification. In this paper, we propose a novel spatial output layer on top of the existing convolutional feature maps to explicitly exploit the location-specific output information. In specific, given the spatial feature maps, we replace the previous GAP-FC layer with a spatially attentive output layer (SAOL) by employing a attention mask on spatial logits. The proposed location-specific attention selectively aggregates spatial logits within a target region, which leads to not only the performance improvement but also spatially interpretable outputs. Moreover, the proposed SAOL also permits to fully exploit location-specific self-supervision as well as self-distillation to enhance the generalization ability during training. The proposed SAOL with self-supervision and self-distillation can be easily plugged into existing CNNs. Experimental results on various classification tasks with representative architectures show consistent performance improvements by SAOL at almost the same computational cost.



### Unsupervised Deformable Medical Image Registration via Pyramidal Residual Deformation Fields Estimation
- **Arxiv ID**: http://arxiv.org/abs/2004.07624v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.07624v1)
- **Published**: 2020-04-16 12:24:27+00:00
- **Updated**: 2020-04-16 12:24:27+00:00
- **Authors**: Yujia Zhou, Shumao Pang, Jun Cheng, Yuhang Sun, Yi Wu, Lei Zhao, Yaqin Liu, Zhentai Lu, Wei Yang, Qianjin Feng
- **Comment**: None
- **Journal**: None
- **Summary**: Deformation field estimation is an important and challenging issue in many medical image registration applications. In recent years, deep learning technique has become a promising approach for simplifying registration problems, and has been gradually applied to medical image registration. However, most existing deep learning registrations do not consider the problem that when the receptive field cannot cover the corresponding features in the moving image and the fixed image, it cannot output accurate displacement values. In fact, due to the limitation of the receptive field, the 3 x 3 kernel has difficulty in covering the corresponding features at high/original resolution. Multi-resolution and multi-convolution techniques can improve but fail to avoid this problem. In this study, we constructed pyramidal feature sets on moving and fixed images and used the warped moving and fixed features to estimate their "residual" deformation field at each scale, called the Pyramidal Residual Deformation Field Estimation module (PRDFE-Module). The "total" deformation field at each scale was computed by upsampling and weighted summing all the "residual" deformation fields at all its previous scales, which can effectively and accurately transfer the deformation fields from low resolution to high resolution and is used for warping the moving features at each scale. Simulation and real brain data results show that our method improves the accuracy of the registration and the rationality of the deformation field.



### Top-Down Networks: A coarse-to-fine reimagination of CNNs
- **Arxiv ID**: http://arxiv.org/abs/2004.07629v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.07629v1)
- **Published**: 2020-04-16 12:29:48+00:00
- **Updated**: 2020-04-16 12:29:48+00:00
- **Authors**: Ioannis Lelekas, Nergis Tomen, Silvia L. Pintea, Jan C. van Gemert
- **Comment**: CVPR Workshop Deep Vision 2020
- **Journal**: None
- **Summary**: Biological vision adopts a coarse-to-fine information processing pathway, from initial visual detection and binding of salient features of a visual scene, to the enhanced and preferential processing given relevant stimuli. On the contrary, CNNs employ a fine-to-coarse processing, moving from local, edge-detecting filters to more global ones extracting abstract representations of the input. In this paper we reverse the feature extraction part of standard bottom-up architectures and turn them upside-down: We propose top-down networks. Our proposed coarse-to-fine pathway, by blurring higher frequency information and restoring it only at later stages, offers a line of defence against adversarial attacks that introduce high frequency noise. Moreover, since we increase image resolution with depth, the high resolution of the feature map in the final convolutional layer contributes to the explainability of the network's decision making process. This favors object-driven decisions over context driven ones, and thus provides better localized class activation maps. This paper offers empirical evidence for the applicability of the top-down resolution processing to various existing architectures on multiple visual tasks.



### Where can I drive? A System Approach: Deep Ego Corridor Estimation for Robust Automated Driving
- **Arxiv ID**: http://arxiv.org/abs/2004.07639v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.07639v2)
- **Published**: 2020-04-16 13:04:18+00:00
- **Updated**: 2021-06-24 11:59:56+00:00
- **Authors**: Thomas Michalke, Di Feng, Claudius Gläser, Fabian Timm
- **Comment**: 8 pages, preprint, accepted at IEEE ITSC 2021
- **Journal**: None
- **Summary**: Lane detection is an essential part of the perception sub-architecture of any automated driving (AD) or advanced driver assistance system (ADAS). When focusing on low-cost, large scale products for automated driving, model-driven approaches for the detection of lane markings have proven good performance. More recently, data-driven approaches have been proposed that target the drivable area / freespace mainly in inner-city applications. Focus of these approaches is less on lane-based driving due to the fact that the lane concept does not fully apply in unstructured, residential inner-city environments. So-far the concept of drivable area is seldom used for highway and inter-urban applications due to the specific requirements of these scenarios that require clear lane associations of all traffic participants. We believe that lane-based, mapless driving in inter-urban and highway scenarios is still not fully handled with sufficient robustness and availability. Especially for challenging weather situations such as heavy rain, fog, low-standing sun, darkness or reflections in puddles, the mapless detection of lane markings decreases significantly or completely fails. We see potential in applying specifically designed data-driven freespace approaches in more lane-based driving applications for highways and inter-urban use. Therefore, we propose to classify specifically a drivable corridor of the ego lane on pixel level with a deep learning approach. Our approach is kept computationally efficient with only 0.66 million parameters allowing its application in large scale products. Thus, we were able to easily integrate into an online AD system of a test vehicle. We demonstrate the performance of our approach under challenging conditions qualitatively and quantitatively in comparison to a state-of-the-art model-driven approach.



### Old is Gold: Redefining the Adversarially Learned One-Class Classifier Training Paradigm
- **Arxiv ID**: http://arxiv.org/abs/2004.07657v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.07657v4)
- **Published**: 2020-04-16 13:48:58+00:00
- **Updated**: 2020-06-19 08:06:34+00:00
- **Authors**: Muhammad Zaigham Zaheer, Jin-ha Lee, Marcella Astrid, Seung-Ik Lee
- **Comment**: Accepted at the Conference on Computer Vision and Pattern Recognition
  CVPR 2020.
  http://openaccess.thecvf.com/content_CVPR_2020/html/Zaheer_Old_Is_Gold_Redefining_the_Adversarially_Learned_One-Class_Classifier_Training_CVPR_2020_paper.html
- **Journal**: None
- **Summary**: A popular method for anomaly detection is to use the generator of an adversarial network to formulate anomaly scores over reconstruction loss of input. Due to the rare occurrence of anomalies, optimizing such networks can be a cumbersome task. Another possible approach is to use both generator and discriminator for anomaly detection. However, attributed to the involvement of adversarial training, this model is often unstable in a way that the performance fluctuates drastically with each training step. In this study, we propose a framework that effectively generates stable results across a wide range of training steps and allows us to use both the generator and the discriminator of an adversarial model for efficient and robust anomaly detection. Our approach transforms the fundamental role of a discriminator from identifying real and fake data to distinguishing between good and bad quality reconstructions. To this end, we prepare training examples for the good quality reconstruction by employing the current generator, whereas poor quality examples are obtained by utilizing an old state of the same generator. This way, the discriminator learns to detect subtle distortions that often appear in reconstructions of the anomaly inputs. Extensive experiments performed on Caltech-256 and MNIST image datasets for novelty detection show superior results. Furthermore, on UCSD Ped2 video dataset for anomaly detection, our model achieves a frame-level AUC of 98.1%, surpassing recent state-of-the-art methods.



### Video Face Manipulation Detection Through Ensemble of CNNs
- **Arxiv ID**: http://arxiv.org/abs/2004.07676v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.07676v1)
- **Published**: 2020-04-16 14:19:40+00:00
- **Updated**: 2020-04-16 14:19:40+00:00
- **Authors**: Nicolò Bonettini, Edoardo Daniele Cannas, Sara Mandelli, Luca Bondi, Paolo Bestagini, Stefano Tubaro
- **Comment**: None
- **Journal**: None
- **Summary**: In the last few years, several techniques for facial manipulation in videos have been successfully developed and made available to the masses (i.e., FaceSwap, deepfake, etc.). These methods enable anyone to easily edit faces in video sequences with incredibly realistic results and a very little effort. Despite the usefulness of these tools in many fields, if used maliciously, they can have a significantly bad impact on society (e.g., fake news spreading, cyber bullying through fake revenge porn). The ability of objectively detecting whether a face has been manipulated in a video sequence is then a task of utmost importance. In this paper, we tackle the problem of face manipulation detection in video sequences targeting modern facial manipulation techniques. In particular, we study the ensembling of different trained Convolutional Neural Network (CNN) models. In the proposed solution, different models are obtained starting from a base network (i.e., EfficientNetB4) making use of two different concepts: (i) attention layers; (ii) siamese training. We show that combining these networks leads to promising face manipulation detection results on two publicly available datasets with more than 119000 videos.



### On the use of Benford's law to detect GAN-generated images
- **Arxiv ID**: http://arxiv.org/abs/2004.07682v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.07682v1)
- **Published**: 2020-04-16 14:42:14+00:00
- **Updated**: 2020-04-16 14:42:14+00:00
- **Authors**: Nicolò Bonettini, Paolo Bestagini, Simone Milani, Stefano Tubaro
- **Comment**: None
- **Journal**: None
- **Summary**: The advent of Generative Adversarial Network (GAN) architectures has given anyone the ability of generating incredibly realistic synthetic imagery. The malicious diffusion of GAN-generated images may lead to serious social and political consequences (e.g., fake news spreading, opinion formation, etc.). It is therefore important to regulate the widespread distribution of synthetic imagery by developing solutions able to detect them. In this paper, we study the possibility of using Benford's law to discriminate GAN-generated images from natural photographs. Benford's law describes the distribution of the most significant digit for quantized Discrete Cosine Transform (DCT) coefficients. Extending and generalizing this property, we show that it is possible to extract a compact feature vector from an image. This feature vector can be fed to an extremely simple classifier for GAN-generated image detection purpose.



### Joint Semantic Segmentation and Boundary Detection using Iterative Pyramid Contexts
- **Arxiv ID**: http://arxiv.org/abs/2004.07684v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.07684v1)
- **Published**: 2020-04-16 14:46:58+00:00
- **Updated**: 2020-04-16 14:46:58+00:00
- **Authors**: Mingmin Zhen, Jinglu Wang, Lei Zhou, Shiwei Li, Tianwei Shen, Jiaxiang Shang, Tian Fang, Quan Long
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present a joint multi-task learning framework for semantic segmentation and boundary detection. The critical component in the framework is the iterative pyramid context module (PCM), which couples two tasks and stores the shared latent semantics to interact between the two tasks. For semantic boundary detection, we propose the novel spatial gradient fusion to suppress nonsemantic edges. As semantic boundary detection is the dual task of semantic segmentation, we introduce a loss function with boundary consistency constraint to improve the boundary pixel accuracy for semantic segmentation. Our extensive experiments demonstrate superior performance over state-of-the-art works, not only in semantic segmentation but also in semantic boundary detection. In particular, a mean IoU score of 81:8% on Cityscapes test set is achieved without using coarse data or any external data for semantic segmentation. For semantic boundary detection, we improve over previous state-of-the-art works by 9.9% in terms of AP and 6:8% in terms of MF(ODS).



### In Search of Life: Learning from Synthetic Data to Detect Vital Signs in Videos
- **Arxiv ID**: http://arxiv.org/abs/2004.07691v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.07691v2)
- **Published**: 2020-04-16 15:02:46+00:00
- **Updated**: 2020-04-23 18:18:39+00:00
- **Authors**: Florin Condrea, Victor-Andrei Ivan, Marius Leordeanu
- **Comment**: Computer Vision and Pattern Recognition (CVPR) Workshop on Computer
  Vision for Physiological Measurement (CVPM) 2020
- **Journal**: None
- **Summary**: Automatically detecting vital signs in videos, such as the estimation of heart and respiration rates, is a challenging research problem in computer vision with important applications in the medical field. One of the key difficulties in tackling this task is the lack of sufficient supervised training data, which severely limits the use of powerful deep neural networks. In this paper we address this limitation through a novel deep learning approach, in which a recurrent deep neural network is trained to detect vital signs in the infrared thermal domain from purely synthetic data. What is most surprising is that our novel method for synthetic training data generation is general, relatively simple and uses almost no prior medical domain knowledge. Moreover, our system, which is trained in a purely automatic manner and needs no human annotation, also learns to predict the respiration or heart intensity signal for each moment in time and to detect the region of interest that is most relevant for the given task, e.g. the nose area in the case of respiration. We test the effectiveness of our proposed system on the recent LCAS dataset and obtain state-of-the-art results.



### Unsupervised Intra-domain Adaptation for Semantic Segmentation through Self-Supervision
- **Arxiv ID**: http://arxiv.org/abs/2004.07703v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2004.07703v4)
- **Published**: 2020-04-16 15:24:11+00:00
- **Updated**: 2020-07-15 11:29:25+00:00
- **Authors**: Fei Pan, Inkyu Shin, Francois Rameau, Seokju Lee, In So Kweon
- **Comment**: Accepted to CVPR 2020 as an Oral Presentation. Code is available at
  https://github.com/feipan664/IntraDA
- **Journal**: None
- **Summary**: Convolutional neural network-based approaches have achieved remarkable progress in semantic segmentation. However, these approaches heavily rely on annotated data which are labor intensive. To cope with this limitation, automatically annotated data generated from graphic engines are used to train segmentation models. However, the models trained from synthetic data are difficult to transfer to real images. To tackle this issue, previous works have considered directly adapting models from the source data to the unlabeled target data (to reduce the inter-domain gap). Nonetheless, these techniques do not consider the large distribution gap among the target data itself (intra-domain gap). In this work, we propose a two-step self-supervised domain adaptation approach to minimize the inter-domain and intra-domain gap together. First, we conduct the inter-domain adaptation of the model; from this adaptation, we separate the target domain into an easy and hard split using an entropy-based ranking function. Finally, to decrease the intra-domain gap, we propose to employ a self-supervised adaptation technique from the easy to the hard split. Experimental results on numerous benchmark datasets highlight the effectiveness of our method against existing state-of-the-art approaches. The source code is available at https://github.com/feipan664/IntraDA.git.



### Knowledge Distillation for Action Anticipation via Label Smoothing
- **Arxiv ID**: http://arxiv.org/abs/2004.07711v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.07711v2)
- **Published**: 2020-04-16 15:38:53+00:00
- **Updated**: 2020-12-18 13:28:40+00:00
- **Authors**: Guglielmo Camporese, Pasquale Coscia, Antonino Furnari, Giovanni Maria Farinella, Lamberto Ballan
- **Comment**: Accepted to ICPR 2020
- **Journal**: None
- **Summary**: Human capability to anticipate near future from visual observations and non-verbal cues is essential for developing intelligent systems that need to interact with people. Several research areas, such as human-robot interaction (HRI), assisted living or autonomous driving need to foresee future events to avoid crashes or help people. Egocentric scenarios are classic examples where action anticipation is applied due to their numerous applications. Such challenging task demands to capture and model domain's hidden structure to reduce prediction uncertainty. Since multiple actions may equally occur in the future, we treat action anticipation as a multi-label problem with missing labels extending the concept of label smoothing. This idea resembles the knowledge distillation process since useful information is injected into the model during training. We implement a multi-modal framework based on long short-term memory (LSTM) networks to summarize past observations and make predictions at different time steps. We perform extensive experiments on EPIC-Kitchens and EGTEA Gaze+ datasets including more than 2500 and 100 action classes, respectively. The experiments show that label smoothing systematically improves performance of state-of-the-art models for action anticipation.



### Image Quality Assessment: Unifying Structure and Texture Similarity
- **Arxiv ID**: http://arxiv.org/abs/2004.07728v3
- **DOI**: 10.1109/TPAMI.2020.3045810
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.07728v3)
- **Published**: 2020-04-16 16:11:46+00:00
- **Updated**: 2020-12-16 12:56:44+00:00
- **Authors**: Keyan Ding, Kede Ma, Shiqi Wang, Eero P. Simoncelli
- **Comment**: None
- **Journal**: IEEE Transactions on Pattern Analysis and Machine Intelligence,
  2021
- **Summary**: Objective measures of image quality generally operate by comparing pixels of a "degraded" image to those of the original. Relative to human observers, these measures are overly sensitive to resampling of texture regions (e.g., replacing one patch of grass with another). Here, we develop the first full-reference image quality model with explicit tolerance to texture resampling. Using a convolutional neural network, we construct an injective and differentiable function that transforms images to multi-scale overcomplete representations. We demonstrate empirically that the spatial averages of the feature maps in this representation capture texture appearance, in that they provide a set of sufficient statistical constraints to synthesize a wide variety of texture patterns. We then describe an image quality method that combines correlations of these spatial averages ("texture similarity") with correlations of the feature maps ("structure similarity"). The parameters of the proposed measure are jointly optimized to match human ratings of image quality, while minimizing the reported distances between subimages cropped from the same texture images. Experiments show that the optimized method explains human perceptual scores, both on conventional image quality databases, as well as on texture databases. The measure also offers competitive performance on related tasks such as texture classification and retrieval. Finally, we show that our method is relatively insensitive to geometric transformations (e.g., translation and dilation), without use of any specialized training or data augmentation. Code is available at https://github.com/dingkeyan93/DISTS.



### MPLP++: Fast, Parallel Dual Block-Coordinate Ascent for Dense Graphical Models
- **Arxiv ID**: http://arxiv.org/abs/2004.08227v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.08227v1)
- **Published**: 2020-04-16 16:20:53+00:00
- **Updated**: 2020-04-16 16:20:53+00:00
- **Authors**: Siddharth Tourani, Alexander Shekhovtsov, Carsten Rother, Bogdan Savchynskyy
- **Comment**: Accepted in ECCV-2018
- **Journal**: None
- **Summary**: Dense, discrete Graphical Models with pairwise potentials are a powerful class of models which are employed in state-of-the-art computer vision and bio-imaging applications. This work introduces a new MAP-solver, based on the popular Dual Block-Coordinate Ascent principle. Surprisingly, by making a small change to the low-performing solver, the Max Product Linear Programming (MPLP) algorithm, we derive the new solver MPLP++ that significantly outperforms all existing solvers by a large margin, including the state-of-the-art solver Tree-Reweighted Sequential (TRWS) message-passing algorithm. Additionally, our solver is highly parallel, in contrast to TRWS, which gives a further boost in performance with the proposed GPU and multi-thread CPU implementations. We verify the superiority of our algorithm on dense problems from publicly available benchmarks, as well, as a new benchmark for 6D Object Pose estimation. We also provide an ablation study with respect to graph density.



### Blur Aware Calibration of Multi-Focus Plenoptic Camera
- **Arxiv ID**: http://arxiv.org/abs/2004.07745v1
- **DOI**: 10.1109/CVPR42600.2020.00262
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.07745v1)
- **Published**: 2020-04-16 16:29:34+00:00
- **Updated**: 2020-04-16 16:29:34+00:00
- **Authors**: Mathieu Labussière, Céline Teulière, Frédéric Bernardin, Omar Ait-Aider
- **Comment**: 2020 IEEE. Personal use of this material is permitted. Permission
  from IEEE must be obtained for all other uses, in any current or future
  media, including reprinting/republishing this material for advertising or
  promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works
- **Journal**: None
- **Summary**: This paper presents a novel calibration algorithm for Multi-Focus Plenoptic Cameras (MFPCs) using raw images only. The design of such cameras is usually complex and relies on precise placement of optic elements. Several calibration procedures have been proposed to retrieve the camera parameters but relying on simplified models, reconstructed images to extract features, or multiple calibrations when several types of micro-lens are used. Considering blur information, we propose a new Blur Aware Plenoptic (BAP) feature. It is first exploited in a pre-calibration step that retrieves initial camera parameters, and secondly to express a new cost function for our single optimization process. The effectiveness of our calibration method is validated by quantitative and qualitative experiments.



### SCOUT: Self-aware Discriminant Counterfactual Explanations
- **Arxiv ID**: http://arxiv.org/abs/2004.07769v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.07769v1)
- **Published**: 2020-04-16 17:05:49+00:00
- **Updated**: 2020-04-16 17:05:49+00:00
- **Authors**: Pei Wang, Nuno Vasconcelos
- **Comment**: Accepted to CVPR2020
- **Journal**: None
- **Summary**: The problem of counterfactual visual explanations is considered. A new family of discriminant explanations is introduced. These produce heatmaps that attribute high scores to image regions informative of a classifier prediction but not of a counter class. They connect attributive explanations, which are based on a single heat map, to counterfactual explanations, which account for both predicted class and counter class. The latter are shown to be computable by combination of two discriminant explanations, with reversed class pairs. It is argued that self-awareness, namely the ability to produce classification confidence scores, is important for the computation of discriminant explanations, which seek to identify regions where it is easy to discriminate between prediction and counter class. This suggests the computation of discriminant explanations by the combination of three attribution maps. The resulting counterfactual explanations are optimization free and thus much faster than previous methods. To address the difficulty of their evaluation, a proxy task and set of quantitative metrics are also proposed. Experiments under this protocol show that the proposed counterfactual explanations outperform the state of the art while achieving much higher speeds, for popular networks. In a human-learning machine teaching experiment, they are also shown to improve mean student accuracy from chance level to 95\%.



### Gaze-Net: Appearance-Based Gaze Estimation using Capsule Networks
- **Arxiv ID**: http://arxiv.org/abs/2004.07777v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2004.07777v1)
- **Published**: 2020-04-16 17:12:06+00:00
- **Updated**: 2020-04-16 17:12:06+00:00
- **Authors**: Bhanuka Mahanama, Yasith Jayawardana, Sampath Jayarathna
- **Comment**: None
- **Journal**: None
- **Summary**: Recent studies on appearance based gaze estimation indicate the ability of Neural Networks to decode gaze information from facial images encompassing pose information. In this paper, we propose Gaze-Net: A capsule network capable of decoding, representing, and estimating gaze information from ocular region images. We evaluate our proposed system using two publicly available datasets, MPIIGaze (200,000+ images in the wild) and Columbia Gaze (5000+ images of users with 21 gaze directions observed at 5 camera angles/positions). Our model achieves a Mean Absolute Error (MAE) of 2.84$^\circ$ for Combined angle error estimate within dataset for MPI-IGaze dataset. Further, model achieves a MAE of 10.04$^\circ$ for across dataset gaze estimation error for Columbia gaze dataset. Through transfer learning, the error is reduced to 5.9$^\circ$. The results show this approach is promising with implications towards using commodity webcams to develop low-cost multi-user gaze tracking systems.



### Shortcut Learning in Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2004.07780v4
- **DOI**: 10.1038/s42256-020-00257-z
- **Categories**: **cs.CV**, cs.AI, cs.LG, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/2004.07780v4)
- **Published**: 2020-04-16 17:18:49+00:00
- **Updated**: 2021-03-26 13:53:12+00:00
- **Authors**: Robert Geirhos, Jörn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge, Felix A. Wichmann
- **Comment**: perspective article published at Nature Machine Intelligence
  (https://doi.org/10.1038/s42256-020-00257-z)
- **Journal**: None
- **Summary**: Deep learning has triggered the current rise of artificial intelligence and is the workhorse of today's machine intelligence. Numerous success stories have rapidly spread all over science, industry and society, but its limitations have only recently come into focus. In this perspective we seek to distil how many of deep learning's problem can be seen as different symptoms of the same underlying problem: shortcut learning. Shortcuts are decision rules that perform well on standard benchmarks but fail to transfer to more challenging testing conditions, such as real-world scenarios. Related issues are known in Comparative Psychology, Education and Linguistics, suggesting that shortcut learning may be a common characteristic of learning systems, biological and artificial alike. Based on these observations, we develop a set of recommendations for model interpretation and benchmarking, highlighting recent advances in machine learning to improve robustness and transferability from the lab to real-world applications.



### Multi-Object Tracking with Siamese Track-RCNN
- **Arxiv ID**: http://arxiv.org/abs/2004.07786v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.07786v1)
- **Published**: 2020-04-16 17:28:52+00:00
- **Updated**: 2020-04-16 17:28:52+00:00
- **Authors**: Bing Shuai, Andrew G. Berneshawi, Davide Modolo, Joseph Tighe
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-object tracking systems often consist of a combination of a detector, a short term linker, a re-identification feature extractor and a solver that takes the output from these separate components and makes a final prediction. Differently, this work aims to unify all these in a single tracking system. Towards this, we propose Siamese Track-RCNN, a two stage detect-and-track framework which consists of three functional branches: (1) the detection branch localizes object instances; (2) the Siamese-based track branch estimates the object motion and (3) the object re-identification branch re-activates the previously terminated tracks when they re-emerge. We test our tracking system on two popular datasets of the MOTChallenge. Siamese Track-RCNN achieves significantly higher results than the state-of-the-art, while also being much more efficient, thanks to its unified design.



### RGBD-Dog: Predicting Canine Pose from RGBD Sensors
- **Arxiv ID**: http://arxiv.org/abs/2004.07788v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.07788v1)
- **Published**: 2020-04-16 17:34:45+00:00
- **Updated**: 2020-04-16 17:34:45+00:00
- **Authors**: Sinead Kearney, Wenbin Li, Martin Parsons, Kwang In Kim, Darren Cosker
- **Comment**: 18 pages, 16 figures, to be published in CVPR 2020
- **Journal**: None
- **Summary**: The automatic extraction of animal \reb{3D} pose from images without markers is of interest in a range of scientific fields. Most work to date predicts animal pose from RGB images, based on 2D labelling of joint positions. However, due to the difficult nature of obtaining training data, no ground truth dataset of 3D animal motion is available to quantitatively evaluate these approaches. In addition, a lack of 3D animal pose data also makes it difficult to train 3D pose-prediction methods in a similar manner to the popular field of body-pose prediction. In our work, we focus on the problem of 3D canine pose estimation from RGBD images, recording a diverse range of dog breeds with several Microsoft Kinect v2s, simultaneously obtaining the 3D ground truth skeleton via a motion capture system. We generate a dataset of synthetic RGBD images from this data. A stacked hourglass network is trained to predict 3D joint locations, which is then constrained using prior models of shape and pose. We evaluate our model on both synthetic and real RGBD images and compare our results to previously published work fitting canine models to images. Finally, despite our training set consisting only of dog data, visual inspection implies that our network can produce good predictions for images of other quadrupeds -- e.g. horses or cats -- when their pose is similar to that contained in our training set.



### Geometry-Aware Gradient Algorithms for Neural Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/2004.07802v5
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, math.OC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.07802v5)
- **Published**: 2020-04-16 17:46:39+00:00
- **Updated**: 2021-03-18 17:47:28+00:00
- **Authors**: Liam Li, Mikhail Khodak, Maria-Florina Balcan, Ameet Talwalkar
- **Comment**: ICLR 2021 Camera-Ready
- **Journal**: None
- **Summary**: Recent state-of-the-art methods for neural architecture search (NAS) exploit gradient-based optimization by relaxing the problem into continuous optimization over architectures and shared-weights, a noisy process that remains poorly understood. We argue for the study of single-level empirical risk minimization to understand NAS with weight-sharing, reducing the design of NAS methods to devising optimizers and regularizers that can quickly obtain high-quality solutions to this problem. Invoking the theory of mirror descent, we present a geometry-aware framework that exploits the underlying structure of this optimization to return sparse architectural parameters, leading to simple yet novel algorithms that enjoy fast convergence guarantees and achieve state-of-the-art accuracy on the latest NAS benchmarks in computer vision. Notably, we exceed the best published results for both CIFAR and ImageNet on both the DARTS search space and NAS-Bench201; on the latter we achieve near-oracle-optimal performance on CIFAR-10 and CIFAR-100. Together, our theory and experiments demonstrate a principled way to co-design optimizers and continuous relaxations of discrete NAS search spaces.



### Leveraging Planar Regularities for Point Line Visual-Inertial Odometry
- **Arxiv ID**: http://arxiv.org/abs/2004.11969v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2004.11969v2)
- **Published**: 2020-04-16 18:20:00+00:00
- **Updated**: 2021-01-14 07:06:17+00:00
- **Authors**: Xin Li, Yijia He, Jinlong Lin, Xiao Liu
- **Comment**: Accepted to IROS 2020
- **Journal**: None
- **Summary**: With monocular Visual-Inertial Odometry (VIO) system, 3D point cloud and camera motion can be estimated simultaneously. Because pure sparse 3D points provide a structureless representation of the environment, generating 3D mesh from sparse points can further model the environment topology and produce dense mapping. To improve the accuracy of 3D mesh generation and localization, we propose a tightly-coupled monocular VIO system, PLP-VIO, which exploits point features and line features as well as plane regularities. The co-planarity constraints are used to leverage additional structure information for the more accurate estimation of 3D points and spatial lines in state estimator. To detect plane and 3D mesh robustly, we combine both the line features with point features in the detection method. The effectiveness of the proposed method is verified on both synthetic data and public datasets and is compared with other state-of-the-art algorithms.



### Symmetry as an Organizing Principle for Geometric Intelligence
- **Arxiv ID**: http://arxiv.org/abs/2004.07879v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.07879v1)
- **Published**: 2020-04-16 18:58:15+00:00
- **Updated**: 2020-04-16 18:58:15+00:00
- **Authors**: Snejana Sheghava, Ashok Goel
- **Comment**: 11 pages, 12 figures
- **Journal**: None
- **Summary**: The exploration of geometrical patterns stimulates imagination and encourages abstract reasoning which is a distinctive feature of human intelligence. In cognitive science, Gestalt principles such as symmetry have often explained significant aspects of human perception. We present a computational technique for building artificial intelligence (AI) agents that use symmetry as the organizing principle for addressing Dehaene's test of geometric intelligence \cite{dehaene2006core}. The performance of our model is on par with extant AI models of problem solving on the Dehaene's test and seems correlated with some elements of human behavior on the same test.



### Divergent Search for Few-Shot Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2004.07903v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.07903v1)
- **Published**: 2020-04-16 19:47:50+00:00
- **Updated**: 2020-04-16 19:47:50+00:00
- **Authors**: Jeremy Tan, Bernhard Kainz
- **Comment**: Submitted to GECCO2020 for review
- **Journal**: None
- **Summary**: When data is unlabelled and the target task is not known a priori, divergent search offers a strategy for learning a wide range of skills. Having such a repertoire allows a system to adapt to new, unforeseen tasks. Unlabelled image data is plentiful, but it is not always known which features will be required for downstream tasks. We propose a method for divergent search in the few-shot image classification setting and evaluate with Omniglot and Mini-ImageNet. This high-dimensional behavior space includes all possible ways of partitioning the data. To manage divergent search in this space, we rely on a meta-learning framework to integrate useful features from diverse tasks into a single model. The final layer of this model is used as an index into the `archive' of all past behaviors. We search for regions in the behavior space that the current archive cannot reach. As expected, divergent search is outperformed by models with a strong bias toward the evaluation tasks. But it is able to match and sometimes exceed the performance of models that have a weak bias toward the target task or none at all. This demonstrates that divergent search is a viable approach, even in high-dimensional behavior spaces.



### Deep Neural Network (DNN) for Water/Fat Separation: Supervised Training, Unsupervised Training, and No Training
- **Arxiv ID**: http://arxiv.org/abs/2004.07923v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.07923v1)
- **Published**: 2020-04-16 20:24:29+00:00
- **Updated**: 2020-04-16 20:24:29+00:00
- **Authors**: R. Jafari, P. Spincemaille, J. Zhang, T. D. Nguyen, M. R. Prince, X. Luo, J. Cho, D. Margolis, Y. Wang
- **Comment**: 19 pages, 5 figures
- **Journal**: None
- **Summary**: Purpose: To use a deep neural network (DNN) for solving the optimization problem of water/fat separation and to compare supervised and unsupervised training.   Methods: The current T2*-IDEAL algorithm for solving fat/water separation is dependent on initialization. Recently, deep neural networks (DNN) have been proposed to solve fat/water separation without the need for suitable initialization. However, this approach requires supervised training of DNN (STD) using the reference fat/water separation images. Here we propose two novel DNN water/fat separation methods 1) unsupervised training of DNN (UTD) using the physical forward problem as the cost function during training, and 2) no-training of DNN (NTD) using physical cost and backpropagation to directly reconstruct a single dataset. The STD, UTD and NTD methods were compared with the reference T2*-IDEAL.   Results: All DNN methods generated consistent water/fat separation results that agreed well with T2*-IDEAL under proper initialization.   Conclusion: The water/fat separation problem can be solved using unsupervised deep neural networks.



### Unsupervised Learning of Landmarks based on Inter-Intra Subject Consistencies
- **Arxiv ID**: http://arxiv.org/abs/2004.07936v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.07936v2)
- **Published**: 2020-04-16 20:38:16+00:00
- **Updated**: 2020-07-07 23:04:42+00:00
- **Authors**: Weijian Li, Haofu Liao, Shun Miao, Le Lu, Jiebo Luo
- **Comment**: Accepted to ICPR-20
- **Journal**: None
- **Summary**: We present a novel unsupervised learning approach to image landmark discovery by incorporating the inter-subject landmark consistencies on facial images. This is achieved via an inter-subject mapping module that transforms original subject landmarks based on an auxiliary subject-related structure. To recover from the transformed images back to the original subject, the landmark detector is forced to learn spatial locations that contain the consistent semantic meanings both for the paired intra-subject images and between the paired inter-subject images. Our proposed method is extensively evaluated on two public facial image datasets (MAFL, AFLW) with various settings. Experimental results indicate that our method can extract the consistent landmarks for both datasets and achieve better performances compared to the previous state-of-the-art methods quantitatively and qualitatively.



### Cityscapes-Panoptic-Parts and PASCAL-Panoptic-Parts datasets for Scene Understanding
- **Arxiv ID**: http://arxiv.org/abs/2004.07944v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.07944v1)
- **Published**: 2020-04-16 20:42:51+00:00
- **Updated**: 2020-04-16 20:42:51+00:00
- **Authors**: Panagiotis Meletis, Xiaoxiao Wen, Chenyang Lu, Daan de Geus, Gijs Dubbelman
- **Comment**: None
- **Journal**: None
- **Summary**: In this technical report, we present two novel datasets for image scene understanding. Both datasets have annotations compatible with panoptic segmentation and additionally they have part-level labels for selected semantic classes. This report describes the format of the two datasets, the annotation protocols, the merging strategies, and presents the datasets statistics. The datasets labels together with code for processing and visualization will be published at https://github.com/tue-mps/panoptic_parts.



### A DICOM Framework for Machine Learning Pipelines against Real-Time Radiology Images
- **Arxiv ID**: http://arxiv.org/abs/2004.07965v4
- **DOI**: 10.1007/s10278-021-00491-w
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.07965v4)
- **Published**: 2020-04-16 21:06:49+00:00
- **Updated**: 2020-08-05 04:55:24+00:00
- **Authors**: Pradeeban Kathiravelu, Puneet Sharma, Ashish Sharma, Imon Banerjee, Hari Trivedi, Saptarshi Purkayastha, Priyanshu Sinha, Alexandre Cadrin-Chenevert, Nabile Safdar, Judy Wawira Gichoya
- **Comment**: Preprint
- **Journal**: Journal of Digital Imaging (JDI), 2021
- **Summary**: Executing machine learning (ML) pipelines in real-time on radiology images is hard due to the limited computing resources in clinical environments and the lack of efficient data transfer capabilities to run them on research clusters. We propose Niffler, an integrated framework that enables the execution of ML pipelines at research clusters by efficiently querying and retrieving radiology images from the Picture Archiving and Communication Systems (PACS) of the hospitals. Niffler uses the Digital Imaging and Communications in Medicine (DICOM) protocol to fetch and store imaging data and provides metadata extraction capabilities and Application programming interfaces (APIs) to apply filters on the images. Niffler further enables the sharing of the outcomes from the ML pipelines in a de-identified manner. Niffler has been running stable for more than 19 months and has supported several research projects at the department. In this paper, we present its architecture and three of its use cases: an inferior vena cava (IVC) filter detection from the images in real-time, identification of scanner utilization, and scanner clock calibration. Evaluations on the Niffler prototype highlight its feasibility and efficiency in facilitating the ML pipelines on the images and metadata in real-time and retrospectively.



### Multiple Visual-Semantic Embedding for Video Retrieval from Query Sentence
- **Arxiv ID**: http://arxiv.org/abs/2004.07967v1
- **DOI**: 10.3390/app11073214
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.07967v1)
- **Published**: 2020-04-16 21:12:32+00:00
- **Updated**: 2020-04-16 21:12:32+00:00
- **Authors**: Huy Manh Nguyen, Tomo Miyazaki, Yoshihiro Sugaya, Shinichiro Omachi
- **Comment**: 8 pages, 5 figures
- **Journal**: Applied Sciences, 2021
- **Summary**: Visual-semantic embedding aims to learn a joint embedding space where related video and sentence instances are located close to each other. Most existing methods put instances in a single embedding space. However, they struggle to embed instances due to the difficulty of matching visual dynamics in videos to textual features in sentences. A single space is not enough to accommodate various videos and sentences. In this paper, we propose a novel framework that maps instances into multiple individual embedding spaces so that we can capture multiple relationships between instances, leading to compelling video retrieval. We propose to produce a final similarity between instances by fusing similarities measured in each embedding space using a weighted sum strategy. We determine the weights according to a sentence. Therefore, we can flexibly emphasize an embedding space. We conducted sentence-to-video retrieval experiments on a benchmark dataset. The proposed method achieved superior performance, and the results are competitive to state-of-the-art methods. These experimental results demonstrated the effectiveness of the proposed multiple embedding approach compared to existing methods.



### A generic ensemble based deep convolutional neural network for semi-supervised medical image segmentation
- **Arxiv ID**: http://arxiv.org/abs/2004.07995v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.07995v1)
- **Published**: 2020-04-16 23:41:50+00:00
- **Updated**: 2020-04-16 23:41:50+00:00
- **Authors**: Ruizhe Li, Dorothee Auer, Christian Wagner, Xin Chen
- **Comment**: Accepted for publication at IEEE International Symposium on
  Biomedical Imaging (ISBI) 2020
- **Journal**: 2020 IEEE 16th International Symposium on Biomedical Imaging (ISBI
  2020)
- **Summary**: Deep learning based image segmentation has achieved the state-of-the-art performance in many medical applications such as lesion quantification, organ detection, etc. However, most of the methods rely on supervised learning, which require a large set of high-quality labeled data. Data annotation is generally an extremely time-consuming process. To address this problem, we propose a generic semi-supervised learning framework for image segmentation based on a deep convolutional neural network (DCNN). An encoder-decoder based DCNN is initially trained using a few annotated training samples. This initially trained model is then copied into sub-models and improved iteratively using random subsets of unlabeled data with pseudo labels generated from models trained in the previous iteration. The number of sub-models is gradually decreased to one in the final iteration. We evaluate the proposed method on a public grand-challenge dataset for skin lesion segmentation. Our method is able to significantly improve beyond fully supervised model learning by incorporating unlabeled data.



### REVISE: A Tool for Measuring and Mitigating Bias in Visual Datasets
- **Arxiv ID**: http://arxiv.org/abs/2004.07999v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.07999v4)
- **Published**: 2020-04-16 23:54:37+00:00
- **Updated**: 2021-07-23 18:41:42+00:00
- **Authors**: Angelina Wang, Alexander Liu, Ryan Zhang, Anat Kleiman, Leslie Kim, Dora Zhao, Iroha Shirai, Arvind Narayanan, Olga Russakovsky
- **Comment**: Extended version of ECCV 2020 Spotlight paper
- **Journal**: None
- **Summary**: Machine learning models are known to perpetuate and even amplify the biases present in the data. However, these data biases frequently do not become apparent until after the models are deployed. Our work tackles this issue and enables the preemptive analysis of large-scale datasets. REVISE (REvealing VIsual biaSEs) is a tool that assists in the investigation of a visual dataset, surfacing potential biases along three dimensions: (1) object-based, (2) person-based, and (3) geography-based. Object-based biases relate to the size, context, or diversity of the depicted objects. Person-based metrics focus on analyzing the portrayal of people within the dataset. Geography-based analyses consider the representation of different geographic locations. These three dimensions are deeply intertwined in how they interact to bias a dataset, and REVISE sheds light on this; the responsibility then lies with the user to consider the cultural and historical context, and to determine which of the revealed biases may be problematic. The tool further assists the user by suggesting actionable steps that may be taken to mitigate the revealed biases. Overall, the key aim of our work is to tackle the machine learning bias problem early in the pipeline. REVISE is available at https://github.com/princetonvisualai/revise-tool



