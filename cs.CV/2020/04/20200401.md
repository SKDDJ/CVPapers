# Arxiv Papers in cs.CV on 2020-04-01
### The Edge of Depth: Explicit Constraints between Segmentation and Depth
- **Arxiv ID**: http://arxiv.org/abs/2004.00171v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.00171v1)
- **Published**: 2020-04-01 00:03:20+00:00
- **Updated**: 2020-04-01 00:03:20+00:00
- **Authors**: Shengjie Zhu, Garrick Brazil, Xiaoming Liu
- **Comment**: None
- **Journal**: None
- **Summary**: In this work we study the mutual benefits of two common computer vision tasks, self-supervised depth estimation and semantic segmentation from images. For example, to help unsupervised monocular depth estimation, constraints from semantic segmentation has been explored implicitly such as sharing and transforming features. In contrast, we propose to explicitly measure the border consistency between segmentation and depth and minimize it in a greedy manner by iteratively supervising the network towards a locally optimal solution. Partially this is motivated by our observation that semantic segmentation even trained with limited ground truth (200 images of KITTI) can offer more accurate border than that of any (monocular or stereo) image-based depth estimation. Through extensive experiments, our proposed approach advances the state of the art on unsupervised monocular depth estimation in the KITTI.



### Manifold-Aware CycleGAN for High-Resolution Structural-to-DTI Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2004.00173v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.00173v3)
- **Published**: 2020-04-01 00:08:14+00:00
- **Updated**: 2020-09-18 00:19:36+00:00
- **Authors**: Benoit Anctil-Robitaille, Christian Desrosiers, Herve Lombaert
- **Comment**: Accepted at MICCAI 2020 International Workshop on Computational
  Diffusion MRI
- **Journal**: None
- **Summary**: Unpaired image-to-image translation has been applied successfully to natural images but has received very little attention for manifold-valued data such as in diffusion tensor imaging (DTI). The non-Euclidean nature of DTI prevents current generative adversarial networks (GANs) from generating plausible images and has mainly limited their application to diffusion MRI scalar maps, such as fractional anisotropy (FA) or mean diffusivity (MD). Even if these scalar maps are clinically useful, they mostly ignore fiber orientations and therefore have limited applications for analyzing brain fibers. Here, we propose a manifold-aware CycleGAN that learns the generation of high-resolution DTI from unpaired T1w images. We formulate the objective as a Wasserstein distance minimization problem of data distributions on a Riemannian manifold of symmetric positive definite 3x3 matrices SPD(3), using adversarial and cycle-consistency losses. To ensure that the generated diffusion tensors lie on the SPD(3) manifold, we exploit the theoretical properties of the exponential and logarithm maps of the Log-Euclidean metric. We demonstrate that, unlike standard GANs, our method is able to generate realistic high-resolution DTI that can be used to compute diffusion-based metrics and potentially run fiber tractography algorithms. To evaluate our model's performance, we compute the cosine similarity between the generated tensors principal orientation and their ground-truth orientation, the mean squared error (MSE) of their derived FA values and the Log-Euclidean distance between the tensors. We demonstrate that our method produces 2.5 times better FA MSE than a standard CycleGAN and up to 30% better cosine similarity than a manifold-aware Wasserstein GAN while synthesizing sharp high-resolution DTI.



### Knowledge as Priors: Cross-Modal Knowledge Generalization for Datasets without Superior Knowledge
- **Arxiv ID**: http://arxiv.org/abs/2004.00176v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.00176v1)
- **Published**: 2020-04-01 00:28:15+00:00
- **Updated**: 2020-04-01 00:28:15+00:00
- **Authors**: Long Zhao, Xi Peng, Yuxiao Chen, Mubbasir Kapadia, Dimitris N. Metaxas
- **Comment**: In CVPR 2020. (15 pages including supplementary material)
- **Journal**: None
- **Summary**: Cross-modal knowledge distillation deals with transferring knowledge from a model trained with superior modalities (Teacher) to another model trained with weak modalities (Student). Existing approaches require paired training examples exist in both modalities. However, accessing the data from superior modalities may not always be feasible. For example, in the case of 3D hand pose estimation, depth maps, point clouds, or stereo images usually capture better hand structures than RGB images, but most of them are expensive to be collected. In this paper, we propose a novel scheme to train the Student in a Target dataset where the Teacher is unavailable. Our key idea is to generalize the distilled cross-modal knowledge learned from a Source dataset, which contains paired examples from both modalities, to the Target dataset by modeling knowledge as priors on parameters of the Student. We name our method "Cross-Modal Knowledge Generalization" and demonstrate that our scheme results in competitive performance for 3D hand pose estimation on standard benchmark datasets.



### Spatio-Temporal Action Detection with Multi-Object Interaction
- **Arxiv ID**: http://arxiv.org/abs/2004.00180v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.00180v1)
- **Published**: 2020-04-01 00:54:56+00:00
- **Updated**: 2020-04-01 00:54:56+00:00
- **Authors**: Huijuan Xu, Lizhi Yang, Stan Sclaroff, Kate Saenko, Trevor Darrell
- **Comment**: None
- **Journal**: None
- **Summary**: Spatio-temporal action detection in videos requires localizing the action both spatially and temporally in the form of an "action tube". Nowadays, most spatio-temporal action detection datasets (e.g. UCF101-24, AVA, DALY) are annotated with action tubes that contain a single person performing the action, thus the predominant action detection models simply employ a person detection and tracking pipeline for localization. However, when the action is defined as an interaction between multiple objects, such methods may fail since each bounding box in the action tube contains multiple objects instead of one person. In this paper, we study the spatio-temporal action detection problem with multi-object interaction. We introduce a new dataset that is annotated with action tubes containing multi-object interactions. Moreover, we propose an end-to-end spatio-temporal action detection model that performs both spatial and temporal regression simultaneously. Our spatial regression may enclose multiple objects participating in the action. During test time, we simply connect the regressed bounding boxes within the predicted temporal duration using a simple heuristic. We report the baseline results of our proposed model on this new dataset, and also show competitive results on the standard benchmark UCF101-24 using only RGB input.



### A theory of independent mechanisms for extrapolation in generative models
- **Arxiv ID**: http://arxiv.org/abs/2004.00184v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.00184v2)
- **Published**: 2020-04-01 01:01:43+00:00
- **Updated**: 2021-12-31 18:33:04+00:00
- **Authors**: Michel Besserve, Rémy Sun, Dominik Janzing, Bernhard Schölkopf
- **Comment**: 21 pages
- **Journal**: None
- **Summary**: Generative models can be trained to emulate complex empirical data, but are they useful to make predictions in the context of previously unobserved environments? An intuitive idea to promote such extrapolation capabilities is to have the architecture of such model reflect a causal graph of the true data generating process, such that one can intervene on each node independently of the others. However, the nodes of this graph are usually unobserved, leading to overparameterization and lack of identifiability of the causal structure. We develop a theoretical framework to address this challenging situation by defining a weaker form of identifiability, based on the principle of independence of mechanisms. We demonstrate on toy examples that classical stochastic gradient descent can hinder the model's extrapolation capabilities, suggesting independence of mechanisms should be enforced explicitly during training. Experiments on deep generative models trained on real world data support these insights and illustrate how the extrapolation capabilities of such models can be leveraged.



### Boundary-Aware Dense Feature Indicator for Single-Stage 3D Object Detection from Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2004.00186v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.00186v1)
- **Published**: 2020-04-01 01:21:23+00:00
- **Updated**: 2020-04-01 01:21:23+00:00
- **Authors**: Guodong Xu, Wenxiao Wang, Zili Liu, Liang Xie, Zheng Yang, Haifeng Liu, Deng Cai
- **Comment**: None
- **Journal**: None
- **Summary**: 3D object detection based on point clouds has become more and more popular. Some methods propose localizing 3D objects directly from raw point clouds to avoid information loss. However, these methods come with complex structures and significant computational overhead, limiting its broader application in real-time scenarios. Some methods choose to transform the point cloud data into compact tensors first and leverage off-the-shelf 2D detectors to propose 3D objects, which is much faster and achieves state-of-the-art results. However, because of the inconsistency between 2D and 3D data, we argue that the performance of compact tensor-based 3D detectors is restricted if we use 2D detectors without corresponding modification. Specifically, the distribution of point clouds is uneven, with most points gather on the boundary of objects, while detectors for 2D data always extract features evenly. Motivated by this observation, we propose DENse Feature Indicator (DENFI), a universal module that helps 3D detectors focus on the densest region of the point clouds in a boundary-aware manner. Moreover, DENFI is lightweight and guarantees real-time speed when applied to 3D object detectors. Experiments on KITTI dataset show that DENFI improves the performance of the baseline single-stage detector remarkably, which achieves new state-of-the-art performance among previous 3D detectors, including both two-stage and multi-sensor fusion methods, in terms of mAP with a 34FPS detection speed.



### Semi-Supervised Cervical Dysplasia Classification With Learnable Graph Convolutional Network
- **Arxiv ID**: http://arxiv.org/abs/2004.00191v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.00191v1)
- **Published**: 2020-04-01 01:53:26+00:00
- **Updated**: 2020-04-01 01:53:26+00:00
- **Authors**: Yanglan Ou, Yuan Xue, Ye Yuan, Tao Xu, Vincent Pisztora, Jia Li, Xiaolei Huang
- **Comment**: ISBI 2020
- **Journal**: None
- **Summary**: Cervical cancer is the second most prevalent cancer affecting women today. As the early detection of cervical carcinoma relies heavily upon screening and pre-clinical testing, digital cervicography has great potential as a primary or auxiliary screening tool, especially in low-resource regions due to its low cost and easy access. Although an automated cervical dysplasia detection system has been desirable, traditional fully-supervised training of such systems requires large amounts of annotated data which are often labor-intensive to collect. To alleviate the need for much manual annotation, we propose a novel graph convolutional network (GCN) based semi-supervised classification model that can be trained with fewer annotations. In existing GCNs, graphs are constructed with fixed features and can not be updated during the learning process. This limits their ability to exploit new features learned during graph convolution. In this paper, we propose a novel and more flexible GCN model with a feature encoder that adaptively updates the adjacency matrix during learning and demonstrate that this model design leads to improved performance. Our experimental results on a cervical dysplasia classification dataset show that the proposed framework outperforms previous methods under a semi-supervised setting, especially when the labeled samples are scarce.



### Shared Cross-Modal Trajectory Prediction for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2004.00202v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2004.00202v3)
- **Published**: 2020-04-01 02:44:30+00:00
- **Updated**: 2021-06-11 21:24:38+00:00
- **Authors**: Chiho Choi, Joon Hee Choi, Srikanth Malla, Jiachen Li
- **Comment**: CVPR 2021 [Oral]. arXiv admin note: substantial text overlap with
  arXiv:2011.08436
- **Journal**: None
- **Summary**: Predicting future trajectories of traffic agents in highly interactive environments is an essential and challenging problem for the safe operation of autonomous driving systems. On the basis of the fact that self-driving vehicles are equipped with various types of sensors (e.g., LiDAR scanner, RGB camera, radar, etc.), we propose a Cross-Modal Embedding framework that aims to benefit from the use of multiple input modalities. At training time, our model learns to embed a set of complementary features in a shared latent space by jointly optimizing the objective functions across different types of input data. At test time, a single input modality (e.g., LiDAR data) is required to generate predictions from the input perspective (i.e., in the LiDAR space), while taking advantages from the model trained with multiple sensor modalities. An extensive evaluation is conducted to show the efficacy of the proposed framework using two benchmark driving datasets.



### Region Proposal Network with Graph Prior and IoU-Balance Loss for Landmark Detection in 3D Ultrasound
- **Arxiv ID**: http://arxiv.org/abs/2004.00207v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.00207v1)
- **Published**: 2020-04-01 03:00:03+00:00
- **Updated**: 2020-04-01 03:00:03+00:00
- **Authors**: Chaoyu Chen, Xin Yang, Ruobing Huang, Wenlong Shi, Shengfeng Liu, Mingrong Lin, Yuhao Huang, Yong Yang, Yuanji Zhang, Huanjia Luo, Yankai Huang, Yi Xiong, Dong Ni
- **Comment**: IEEE International Symposium on Biomedical Imaging (IEEE ISBI 2020)
- **Journal**: None
- **Summary**: 3D ultrasound (US) can facilitate detailed prenatal examinations for fetal growth monitoring. To analyze a 3D US volume, it is fundamental to identify anatomical landmarks of the evaluated organs accurately. Typical deep learning methods usually regress the coordinates directly or involve heatmap-matching. However, these methods struggle to deal with volumes with large sizes and the highly-varying positions and orientations of fetuses. In this work, we exploit an object detection framework to detect landmarks in 3D fetal facial US volumes. By regressing multiple parameters of the landmark-centered bounding box (B-box) with a strict criteria, the proposed model is able to pinpoint the exact location of the targeted landmarks. Specifically, the model uses a 3D region proposal network (RPN) to generate 3D candidate regions, followed by several 3D classification branches to select the best candidate. It also adopts an IoU-balance loss to improve communications between branches that benefits the learning process. Furthermore, it leverages a distance-based graph prior to regularize the training and helps to reduce false positive predictions. The performance of the proposed framework is evaluated on a 3D US dataset to detect five key fetal facial landmarks. Results showed the proposed method outperforms some of the state-of-the-art methods in efficacy and efficiency.



### BCNet: Learning Body and Cloth Shape from A Single Image
- **Arxiv ID**: http://arxiv.org/abs/2004.00214v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2004.00214v2)
- **Published**: 2020-04-01 03:41:36+00:00
- **Updated**: 2020-08-03 10:03:24+00:00
- **Authors**: Boyi Jiang, Juyong Zhang, Yang Hong, Jinhao Luo, Ligang Liu, Hujun Bao
- **Comment**: Accepted to ECCV2020
- **Journal**: None
- **Summary**: In this paper, we consider the problem to automatically reconstruct garment and body shapes from a single near-front view RGB image. To this end, we propose a layered garment representation on top of SMPL and novelly make the skinning weight of garment independent of the body mesh, which significantly improves the expression ability of our garment model. Compared with existing methods, our method can support more garment categories and recover more accurate geometry. To train our model, we construct two large scale datasets with ground truth body and garment geometries as well as paired color images. Compared with single mesh or non-parametric representation, our method can achieve more flexible control with separate meshes, makes applications like re-pose, garment transfer, and garment texture mapping possible. Code and some data is available at https://github.com/jby1993/BCNet.



### 3D Deep Learning on Medical Images: A Review
- **Arxiv ID**: http://arxiv.org/abs/2004.00218v4
- **DOI**: None
- **Categories**: **q-bio.QM**, cs.CV, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.00218v4)
- **Published**: 2020-04-01 03:56:48+00:00
- **Updated**: 2020-10-13 08:38:19+00:00
- **Authors**: Satya P. Singh, Lipo Wang, Sukrit Gupta, Haveesh Goli, Parasuraman Padmanabhan, Balázs Gulyás
- **Comment**: Published in Sensors Journal
  (https://www.mdpi.com/1424-8220/20/18/5097)
- **Journal**: Sensors 2020, 20, 5097
- **Summary**: The rapid advancements in machine learning, graphics processing technologies and the availability of medical imaging data have led to a rapid increase in the use of deep learning models in the medical domain. This was exacerbated by the rapid advancements in convolutional neural network (CNN) based architectures, which were adopted by the medical imaging community to assist clinicians in disease diagnosis. Since the grand success of AlexNet in 2012, CNNs have been increasingly used in medical image analysis to improve the efficiency of human clinicians. In recent years, three-dimensional (3D) CNNs have been employed for the analysis of medical images. In this paper, we trace the history of how the 3D CNN was developed from its machine learning roots, we provide a brief mathematical description of 3D CNN and provide the preprocessing steps required for medical images before feeding them to 3D CNNs. We review the significant research in the field of 3D medical imaging analysis using 3D CNNs (and its variants) in different medical areas such as classification, segmentation, detection and localization. We conclude by discussing the challenges associated with the use of 3D CNNs in the medical imaging domain (and the use of deep learning models in general) and possible future trends in the field.



### NBDT: Neural-Backed Decision Trees
- **Arxiv ID**: http://arxiv.org/abs/2004.00221v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2004.00221v3)
- **Published**: 2020-04-01 04:04:03+00:00
- **Updated**: 2021-01-28 03:06:26+00:00
- **Authors**: Alvin Wan, Lisa Dunlap, Daniel Ho, Jihan Yin, Scott Lee, Henry Jin, Suzanne Petryk, Sarah Adel Bargal, Joseph E. Gonzalez
- **Comment**: 8 pages, 7 figures, accepted to ICLR 2021
- **Journal**: None
- **Summary**: Machine learning applications such as finance and medicine demand accurate and justifiable predictions, barring most deep learning methods from use. In response, previous work combines decision trees with deep learning, yielding models that (1) sacrifice interpretability for accuracy or (2) sacrifice accuracy for interpretability. We forgo this dilemma by jointly improving accuracy and interpretability using Neural-Backed Decision Trees (NBDTs). NBDTs replace a neural network's final linear layer with a differentiable sequence of decisions and a surrogate loss. This forces the model to learn high-level concepts and lessens reliance on highly-uncertain decisions, yielding (1) accuracy: NBDTs match or outperform modern neural networks on CIFAR, ImageNet and better generalize to unseen classes by up to 16%. Furthermore, our surrogate loss improves the original model's accuracy by up to 2%. NBDTs also afford (2) interpretability: improving human trustby clearly identifying model mistakes and assisting in dataset debugging. Code and pretrained NBDTs are at https://github.com/alvinwan/neural-backed-decision-trees.



### Video Anomaly Detection for Smart Surveillance
- **Arxiv ID**: http://arxiv.org/abs/2004.00222v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.00222v3)
- **Published**: 2020-04-01 04:13:55+00:00
- **Updated**: 2020-04-11 19:07:05+00:00
- **Authors**: Sijie Zhu, Chen Chen, Waqas Sultani
- **Comment**: None
- **Journal**: None
- **Summary**: In modern intelligent video surveillance systems, automatic anomaly detection through computer vision analytics plays a pivotal role which not only significantly increases monitoring efficiency but also reduces the burden on live monitoring. Anomalies in videos are broadly defined as events or activities that are unusual and signify irregular behavior. The goal of anomaly detection is to temporally or spatially localize the anomaly events in video sequences. Temporal localization (i.e. indicating the start and end frames of the anomaly event in a video) is referred to as frame-level detection. Spatial localization, which is more challenging, means to identify the pixels within each anomaly frame that correspond to the anomaly event. This setting is usually referred to as pixel-level detection. In this paper, we provide a brief overview of the recent research progress on video anomaly detection and highlight a few future research directions.



### MetaPoison: Practical General-purpose Clean-label Data Poisoning
- **Arxiv ID**: http://arxiv.org/abs/2004.00225v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.00225v2)
- **Published**: 2020-04-01 04:23:20+00:00
- **Updated**: 2021-02-21 02:40:40+00:00
- **Authors**: W. Ronny Huang, Jonas Geiping, Liam Fowl, Gavin Taylor, Tom Goldstein
- **Comment**: Conference paper at NeurIPS 2020. First two authors contributed
  equally
- **Journal**: None
- **Summary**: Data poisoning -- the process by which an attacker takes control of a model by making imperceptible changes to a subset of the training data -- is an emerging threat in the context of neural networks. Existing attacks for data poisoning neural networks have relied on hand-crafted heuristics, because solving the poisoning problem directly via bilevel optimization is generally thought of as intractable for deep models. We propose MetaPoison, a first-order method that approximates the bilevel problem via meta-learning and crafts poisons that fool neural networks. MetaPoison is effective: it outperforms previous clean-label poisoning methods by a large margin. MetaPoison is robust: poisoned data made for one model transfer to a variety of victim models with unknown training settings and architectures. MetaPoison is general-purpose, it works not only in fine-tuning scenarios, but also for end-to-end training from scratch, which till now hasn't been feasible for clean-label attacks with deep nets. MetaPoison can achieve arbitrary adversary goals -- like using poisons of one class to make a target image don the label of another arbitrarily chosen class. Finally, MetaPoison works in the real-world. We demonstrate for the first time successful data poisoning of models trained on the black-box Google Cloud AutoML API. Code and premade poisons are provided at https://github.com/wronnyhuang/metapoison



### Synthesis and Edition of Ultrasound Images via Sketch Guided Progressive Growing GANs
- **Arxiv ID**: http://arxiv.org/abs/2004.00226v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.00226v1)
- **Published**: 2020-04-01 04:24:01+00:00
- **Updated**: 2020-04-01 04:24:01+00:00
- **Authors**: Jiamin Liang, Xin Yang, Haoming Li, Yi Wang, Manh The Van, Haoran Dou, Chaoyu Chen, Jinghui Fang, Xiaowen Liang, Zixin Mai, Guowen Zhu, Zhiyi Chen, Dong Ni
- **Comment**: IEEE International Symposium on Biomedical Imaging (IEEE ISBI 2020)
- **Journal**: None
- **Summary**: Ultrasound (US) is widely accepted in clinic for anatomical structure inspection. However, lacking in resources to practice US scan, novices often struggle to learn the operation skills. Also, in the deep learning era, automated US image analysis is limited by the lack of annotated samples. Efficiently synthesizing realistic, editable and high resolution US images can solve the problems. The task is challenging and previous methods can only partially complete it. In this paper, we devise a new framework for US image synthesis. Particularly, we firstly adopt a sketch generative adversarial networks (Sgan) to introduce background sketch upon object mask in a conditioned generative adversarial network. With enriched sketch cues, Sgan can generate realistic US images with editable and fine-grained structure details. Although effective, Sgan is hard to generate high resolution US images. To achieve this, we further implant the Sgan into a progressive growing scheme (PGSgan). By smoothly growing both generator and discriminator, PGSgan can gradually synthesize US images from low to high resolution. By synthesizing ovary and follicle US images, our extensive perceptual evaluation, user study and segmentation results prove the promising efficacy and efficiency of the proposed PGSgan.



### Pose-guided Visible Part Matching for Occluded Person ReID
- **Arxiv ID**: http://arxiv.org/abs/2004.00230v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.00230v1)
- **Published**: 2020-04-01 04:36:51+00:00
- **Updated**: 2020-04-01 04:36:51+00:00
- **Authors**: Shang Gao, Jingya Wang, Huchuan Lu, Zimo Liu
- **Comment**: Accepted to CVPR2020
- **Journal**: None
- **Summary**: Occluded person re-identification is a challenging task as the appearance varies substantially with various obstacles, especially in the crowd scenario. To address this issue, we propose a Pose-guided Visible Part Matching (PVPM) method that jointly learns the discriminative features with pose-guided attention and self-mines the part visibility in an end-to-end framework. Specifically, the proposed PVPM includes two key components: 1) pose-guided attention (PGA) method for part feature pooling that exploits more discriminative local features; 2) pose-guided visibility predictor (PVP) that estimates whether a part suffers the occlusion or not. As there are no ground truth training annotations for the occluded part, we turn to utilize the characteristic of part correspondence in positive pairs and self-mining the correspondence scores via graph matching. The generated correspondence scores are then utilized as pseudo-labels for visibility predictor (PVP). Experimental results on three reported occluded benchmarks show that the proposed method achieves competitive performance to state-of-the-art methods. The source codes are available at https://github.com/hh23333/PVPM



### Self-Augmentation: Generalizing Deep Networks to Unseen Classes for Few-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2004.00251v3
- **DOI**: 10.1016/j.neunet.2021.02.007
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.00251v3)
- **Published**: 2020-04-01 06:39:08+00:00
- **Updated**: 2020-08-04 08:37:35+00:00
- **Authors**: Jin-Woo Seo, Hong-Gyu Jung, Seong-Whan Lee
- **Comment**: The first two authors contributed equally to this work
- **Journal**: None
- **Summary**: Few-shot learning aims to classify unseen classes with a few training examples. While recent works have shown that standard mini-batch training with a carefully designed training strategy can improve generalization ability for unseen classes, well-known problems in deep networks such as memorizing training statistics have been less explored for few-shot learning. To tackle this issue, we propose self-augmentation that consolidates self-mix and self-distillation. Specifically, we exploit a regional dropout technique called self-mix, in which a patch of an image is substituted into other values in the same image. Then, we employ a backbone network that has auxiliary branches with its own classifier to enforce knowledge sharing. Lastly, we present a local representation learner to further exploit a few training examples for unseen classes. Experimental results show that the proposed method outperforms the state-of-the-art methods for prevalent few-shot benchmarks and improves the generalization ability.



### Progressive Multi-Stage Learning for Discriminative Tracking
- **Arxiv ID**: http://arxiv.org/abs/2004.00255v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.00255v1)
- **Published**: 2020-04-01 07:01:30+00:00
- **Updated**: 2020-04-01 07:01:30+00:00
- **Authors**: Weichao Li, Xi Li, Omar Elfarouk Bourahla, Fuxian Huang, Fei Wu, Wei Liu, Zhiheng Wang, Hongmin Liu
- **Comment**: accepted to IEEE Transactions on Cybernetics
- **Journal**: None
- **Summary**: Visual tracking is typically solved as a discriminative learning problem that usually requires high-quality samples for online model adaptation. It is a critical and challenging problem to evaluate the training samples collected from previous predictions and employ sample selection by their quality to train the model.   To tackle the above problem, we propose a joint discriminative learning scheme with the progressive multi-stage optimization policy of sample selection for robust visual tracking. The proposed scheme presents a novel time-weighted and detection-guided self-paced learning strategy for easy-to-hard sample selection, which is capable of tolerating relatively large intra-class variations while maintaining inter-class separability. Such a self-paced learning strategy is jointly optimized in conjunction with the discriminative tracking process, resulting in robust tracking results. Experiments on the benchmark datasets demonstrate the effectiveness of the proposed learning framework.



### An Efficient Agreement Mechanism in CapsNets By Pairwise Product
- **Arxiv ID**: http://arxiv.org/abs/2004.00272v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.00272v1)
- **Published**: 2020-04-01 08:09:23+00:00
- **Updated**: 2020-04-01 08:09:23+00:00
- **Authors**: Lei Zhao, Xiaohui Wang, Lei Huang
- **Comment**: Accepted to ECAI 2020
- **Journal**: None
- **Summary**: Capsule networks (CapsNets) are capable of modeling visual hierarchical relationships, which is achieved by the "routing-by-agreement" mechanism. This paper proposes a pairwise agreement mechanism to build capsules, inspired by the feature interactions of factorization machines (FMs). The proposed method has a much lower computation complexity. We further proposed a new CapsNet architecture that combines the strengths of residual networks in representing low-level visual features and CapsNets in modeling the relationships of parts to wholes. We conduct comprehensive experiments to compare the routing algorithms, including dynamic routing, EM routing, and our proposed FM agreement, based on both architectures of original CapsNet and our proposed one, and the results show that our method achieves both excellent performance and efficiency under a variety of situations.



### Graph Structured Network for Image-Text Matching
- **Arxiv ID**: http://arxiv.org/abs/2004.00277v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.00277v1)
- **Published**: 2020-04-01 08:20:42+00:00
- **Updated**: 2020-04-01 08:20:42+00:00
- **Authors**: Chunxiao Liu, Zhendong Mao, Tianzhu Zhang, Hongtao Xie, Bin Wang, Yongdong Zhang
- **Comment**: Accepted by CVPR2020
- **Journal**: None
- **Summary**: Image-text matching has received growing interest since it bridges vision and language. The key challenge lies in how to learn correspondence between image and text. Existing works learn coarse correspondence based on object co-occurrence statistics, while failing to learn fine-grained phrase correspondence. In this paper, we present a novel Graph Structured Matching Network (GSMN) to learn fine-grained correspondence. The GSMN explicitly models object, relation and attribute as a structured phrase, which not only allows to learn correspondence of object, relation and attribute separately, but also benefits to learn fine-grained correspondence of structured phrase. This is achieved by node-level matching and structure-level matching. The node-level matching associates each node with its relevant nodes from another modality, where the node can be object, relation or attribute. The associated nodes then jointly infer fine-grained correspondence by fusing neighborhood associations at structure-level matching. Comprehensive experiments show that GSMN outperforms state-of-the-art methods on benchmarks, with relative Recall@1 improvements of nearly 7% and 2% on Flickr30K and MSCOCO, respectively. Code will be released at: https://github.com/CrossmodalGroup/GSMN.



### Creating Something from Nothing: Unsupervised Knowledge Distillation for Cross-Modal Hashing
- **Arxiv ID**: http://arxiv.org/abs/2004.00280v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.00280v1)
- **Published**: 2020-04-01 08:32:15+00:00
- **Updated**: 2020-04-01 08:32:15+00:00
- **Authors**: Hengtong Hu, Lingxi Xie, Richang Hong, Qi Tian
- **Comment**: This paper has been accepted for CVPR2020
- **Journal**: None
- **Summary**: In recent years, cross-modal hashing (CMH) has attracted increasing attentions, mainly because its potential ability of mapping contents from different modalities, especially in vision and language, into the same space, so that it becomes efficient in cross-modal data retrieval. There are two main frameworks for CMH, differing from each other in whether semantic supervision is required. Compared to the unsupervised methods, the supervised methods often enjoy more accurate results, but require much heavier labors in data annotation. In this paper, we propose a novel approach that enables guiding a supervised method using outputs produced by an unsupervised method. Specifically, we make use of teacher-student optimization for propagating knowledge. Experiments are performed on two popular CMH benchmarks, i.e., the MIRFlickr and NUS-WIDE datasets. Our approach outperforms all existing unsupervised methods by a large margin.



### CurricularFace: Adaptive Curriculum Learning Loss for Deep Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2004.00288v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.00288v1)
- **Published**: 2020-04-01 08:43:10+00:00
- **Updated**: 2020-04-01 08:43:10+00:00
- **Authors**: Yuge Huang, Yuhan Wang, Ying Tai, Xiaoming Liu, Pengcheng Shen, Shaoxin Li, Jilin Li, Feiyue Huang
- **Comment**: CVPR 2020
- **Journal**: None
- **Summary**: As an emerging topic in face recognition, designing margin-based loss functions can increase the feature margin between different classes for enhanced discriminability. More recently, the idea of mining-based strategies is adopted to emphasize the misclassified samples, achieving promising results. However, during the entire training process, the prior methods either do not explicitly emphasize the sample based on its importance that renders the hard samples not fully exploited; or explicitly emphasize the effects of semi-hard/hard samples even at the early training stage that may lead to convergence issue. In this work, we propose a novel Adaptive Curriculum Learning loss (CurricularFace) that embeds the idea of curriculum learning into the loss function to achieve a novel training strategy for deep face recognition, which mainly addresses easy samples in the early training stage and hard ones in the later stage. Specifically, our CurricularFace adaptively adjusts the relative importance of easy and hard samples during different training stages. In each stage, different samples are assigned with different importance according to their corresponding difficultness. Extensive experimental results on popular benchmarks demonstrate the superiority of our CurricularFace over the state-of-the-art competitors.



### Evaluation of Model Selection for Kernel Fragment Recognition in Corn Silage
- **Arxiv ID**: http://arxiv.org/abs/2004.00292v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.00292v1)
- **Published**: 2020-04-01 08:56:01+00:00
- **Updated**: 2020-04-01 08:56:01+00:00
- **Authors**: Christoffer Bøgelund Rasmussen, Thomas B. Moeslund
- **Comment**: Paper presented at the ICLR 2020 Workshop on Computer Vision for
  Agriculture (CV4A)
- **Journal**: None
- **Summary**: Model selection when designing deep learning systems for specific use-cases can be a challenging task as many options exist and it can be difficult to know the trade-off between them. Therefore, we investigate a number of state of the art CNN models for the task of measuring kernel fragmentation in harvested corn silage. The models are evaluated across a number of feature extractors and image sizes in order to determine optimal model design choices based upon the trade-off between model complexity, accuracy and speed. We show that accuracy improvements can be made with more complex meta-architectures and speed can be optimised by decreasing the image size with only slight losses in accuracy. Additionally, we show improvements in Average Precision at an Intersection over Union of 0.5 of up to 20 percentage points while also decreasing inference time in comparison to previously published work. This result for better model selection enables opportunities for creating systems that can aid farmers in improving their silage quality while harvesting.



### Mapping individual differences in cortical architecture using multi-view representation learning
- **Arxiv ID**: http://arxiv.org/abs/2004.02804v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.02804v1)
- **Published**: 2020-04-01 09:01:25+00:00
- **Updated**: 2020-04-01 09:01:25+00:00
- **Authors**: Akrem Sellami, François-Xavier Dupé, Bastien Cagna, Hachem Kadri, Stéphane Ayache, Thierry Artières, Sylvain Takerkart
- **Comment**: None
- **Journal**: IJCNN 2020 - International Joint Conference on Neural Networks,
  Jul 2020, Glasgow, United Kingdom
- **Summary**: In neuroscience, understanding inter-individual differences has recently emerged as a major challenge, for which functional magnetic resonance imaging (fMRI) has proven invaluable. For this, neuroscientists rely on basic methods such as univariate linear correlations between single brain features and a score that quantifies either the severity of a disease or the subject's performance in a cognitive task. However, to this date, task-fMRI and resting-state fMRI have been exploited separately for this question, because of the lack of methods to effectively combine them. In this paper, we introduce a novel machine learning method which allows combining the activation-and connectivity-based information respectively measured through these two fMRI protocols to identify markers of individual differences in the functional organization of the brain. It combines a multi-view deep autoencoder which is designed to fuse the two fMRI modalities into a joint representation space within which a predictive model is trained to guess a scalar score that characterizes the patient. Our experimental results demonstrate the ability of the proposed method to outperform competitive approaches and to produce interpretable and biologically plausible results.



### Application of Structural Similarity Analysis of Visually Salient Areas and Hierarchical Clustering in the Screening of Similar Wireless Capsule Endoscopic Images
- **Arxiv ID**: http://arxiv.org/abs/2004.02805v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.02805v1)
- **Published**: 2020-04-01 09:03:33+00:00
- **Updated**: 2020-04-01 09:03:33+00:00
- **Authors**: Rui Nie, Huan Yang, Hejuan Peng, Wenbin Luo, Weiya Fan, Jie Zhang, Jing Liao, Fang Huang, Yufeng Xiao
- **Comment**: None
- **Journal**: None
- **Summary**: Small intestinal capsule endoscopy is the mainstream method for inspecting small intestinal lesions,but a single small intestinal capsule endoscopy will produce 60,000 - 120,000 images, the majority of which are similar and have no diagnostic value. It takes 2 - 3 hours for doctors to identify lesions from these images. This is time-consuming and increase the probability of misdiagnosis and missed diagnosis since doctors are likely to experience visual fatigue while focusing on a large number of similar images for an extended period of time.In order to solve these problems, we proposed a similar wireless capsule endoscope (WCE) image screening method based on structural similarity analysis and the hierarchical clustering of visually salient sub-image blocks. The similarity clustering of images was automatically identified by hierarchical clustering based on the hue,saturation,value (HSV) spatial color characteristics of the images,and the keyframe images were extracted based on the structural similarity of the visually salient sub-image blocks, in order to accurately identify and screen out similar small intestinal capsule endoscopic images. Subsequently, the proposed method was applied to the capsule endoscope imaging workstation. After screening out similar images in the complete data gathered by the Type I OMOM Small Intestinal Capsule Endoscope from 52 cases covering 17 common types of small intestinal lesions, we obtained a lesion recall of 100% and an average similar image reduction ratio of 76%. With similar images screened out, the average play time of the OMOM image workstation was 18 minutes, which greatly reduced the time spent by doctors viewing the images.



### Transfer Learning of Photometric Phenotypes in Agriculture Using Metadata
- **Arxiv ID**: http://arxiv.org/abs/2004.00303v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.00303v1)
- **Published**: 2020-04-01 09:24:34+00:00
- **Updated**: 2020-04-01 09:24:34+00:00
- **Authors**: Dan Halbersberg, Aharon Bar Hillel, Shon Mendelson, Daniel Koster, Lena Karol, Boaz Lerner
- **Comment**: Paper presented at the ICLR 2020 Workshop on Computer Vision for
  Agriculture (CV4A)
- **Journal**: None
- **Summary**: Estimation of photometric plant phenotypes (e.g., hue, shine, chroma) in field conditions is important for decisions on the expected yield quality, fruit ripeness, and need for further breeding. Estimating these from images is difficult due to large variances in lighting conditions, shadows, and sensor properties. We combine the image and metadata regarding capturing conditions embedded into a network, enabling more accurate estimation and transfer between different conditions. Compared to a state-of-the-art deep CNN and a human expert, metadata embedding improves the estimation of the tomato's hue and chroma.



### High-Performance Long-Term Tracking with Meta-Updater
- **Arxiv ID**: http://arxiv.org/abs/2004.00305v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.00305v1)
- **Published**: 2020-04-01 09:29:23+00:00
- **Updated**: 2020-04-01 09:29:23+00:00
- **Authors**: Kenan Dai, Yunhua Zhang, Dong Wang, Jianhua Li, Huchuan Lu, Xiaoyun Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Long-term visual tracking has drawn increasing attention because it is much closer to practical applications than short-term tracking. Most top-ranked long-term trackers adopt the offline-trained Siamese architectures, thus, they cannot benefit from great progress of short-term trackers with online update. However, it is quite risky to straightforwardly introduce online-update-based trackers to solve the long-term problem, due to long-term uncertain and noisy observations. In this work, we propose a novel offline-trained Meta-Updater to address an important but unsolved problem: Is the tracker ready for updating in the current frame? The proposed meta-updater can effectively integrate geometric, discriminative, and appearance cues in a sequential manner, and then mine the sequential information with a designed cascaded LSTM module. Our meta-updater learns a binary output to guide the tracker's update and can be easily embedded into different trackers. This work also introduces a long-term tracking framework consisting of an online local tracker, an online verifier, a SiamRPN-based re-detector, and our meta-updater. Numerous experimental results on the VOT2018LT, VOT2019LT, OxUvALT, TLP, and LaSOT benchmarks show that our tracker performs remarkably better than other competing algorithms. Our project is available on the website: https://github.com/Daikenan/LTMU.



### Towards Achieving Adversarial Robustness by Enforcing Feature Consistency Across Bit Planes
- **Arxiv ID**: http://arxiv.org/abs/2004.00306v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.00306v1)
- **Published**: 2020-04-01 09:31:10+00:00
- **Updated**: 2020-04-01 09:31:10+00:00
- **Authors**: Sravanti Addepalli, Vivek B. S., Arya Baburaj, Gaurang Sriramanan, R. Venkatesh Babu
- **Comment**: CVPR 2020
- **Journal**: None
- **Summary**: As humans, we inherently perceive images based on their predominant features, and ignore noise embedded within lower bit planes. On the contrary, Deep Neural Networks are known to confidently misclassify images corrupted with meticulously crafted perturbations that are nearly imperceptible to the human eye. In this work, we attempt to address this problem by training networks to form coarse impressions based on the information in higher bit planes, and use the lower bit planes only to refine their prediction. We demonstrate that, by imposing consistency on the representations learned across differently quantized images, the adversarial robustness of networks improves significantly when compared to a normally trained model. Present state-of-the-art defenses against adversarial attacks require the networks to be explicitly trained using adversarial samples that are computationally expensive to generate. While such methods that use adversarial training continue to achieve the best results, this work paves the way towards achieving robustness without having to explicitly train on adversarial samples. The proposed approach is therefore faster, and also closer to the natural learning process in humans.



### Learning to Select Base Classes for Few-shot Classification
- **Arxiv ID**: http://arxiv.org/abs/2004.00315v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.00315v1)
- **Published**: 2020-04-01 09:55:18+00:00
- **Updated**: 2020-04-01 09:55:18+00:00
- **Authors**: Linjun Zhou, Peng Cui, Xu Jia, Shiqiang Yang, Qi Tian
- **Comment**: None
- **Journal**: None
- **Summary**: Few-shot learning has attracted intensive research attention in recent years. Many methods have been proposed to generalize a model learned from provided base classes to novel classes, but no previous work studies how to select base classes, or even whether different base classes will result in different generalization performance of the learned model. In this paper, we utilize a simple yet effective measure, the Similarity Ratio, as an indicator for the generalization performance of a few-shot model. We then formulate the base class selection problem as a submodular optimization problem over Similarity Ratio. We further provide theoretical analysis on the optimization lower bound of different optimization methods, which could be used to identify the most appropriate algorithm for different experimental settings. The extensive experiments on ImageNet, Caltech256 and CUB-200-2011 demonstrate that our proposed method is effective in selecting a better base dataset.



### SoftSMPL: Data-driven Modeling of Nonlinear Soft-tissue Dynamics for Parametric Humans
- **Arxiv ID**: http://arxiv.org/abs/2004.00326v1
- **DOI**: 10.1111/cgf.13912
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2004.00326v1)
- **Published**: 2020-04-01 10:35:06+00:00
- **Updated**: 2020-04-01 10:35:06+00:00
- **Authors**: Igor Santesteban, Elena Garces, Miguel A. Otaduy, Dan Casas
- **Comment**: Accepted at Eurographics 2020. Project website:
  http://dancasas.github.io/projects/SoftSMPL
- **Journal**: None
- **Summary**: We present SoftSMPL, a learning-based method to model realistic soft-tissue dynamics as a function of body shape and motion. Datasets to learn such task are scarce and expensive to generate, which makes training models prone to overfitting. At the core of our method there are three key contributions that enable us to model highly realistic dynamics and better generalization capabilities than state-of-the-art methods, while training on the same data. First, a novel motion descriptor that disentangles the standard pose representation by removing subject-specific features; second, a neural-network-based recurrent regressor that generalizes to unseen shapes and motions; and third, a highly efficient nonlinear deformation subspace capable of representing soft-tissue deformations of arbitrary shapes. We demonstrate qualitative and quantitative improvements over existing methods and, additionally, we show the robustness of our method on a variety of motion capture databases.



### Compressed Volumetric Heatmaps for Multi-Person 3D Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2004.00329v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.00329v1)
- **Published**: 2020-04-01 10:37:39+00:00
- **Updated**: 2020-04-01 10:37:39+00:00
- **Authors**: Matteo Fabbri, Fabio Lanzi, Simone Calderara, Stefano Alletto, Rita Cucchiara
- **Comment**: CVPR 2020
- **Journal**: None
- **Summary**: In this paper we present a novel approach for bottom-up multi-person 3D human pose estimation from monocular RGB images. We propose to use high resolution volumetric heatmaps to model joint locations, devising a simple and effective compression method to drastically reduce the size of this representation. At the core of the proposed method lies our Volumetric Heatmap Autoencoder, a fully-convolutional network tasked with the compression of ground-truth heatmaps into a dense intermediate representation. A second model, the Code Predictor, is then trained to predict these codes, which can be decompressed at test time to re-obtain the original representation. Our experimental evaluation shows that our method performs favorably when compared to state of the art on both multi-person and single-person 3D human pose estimation datasets and, thanks to our novel compression strategy, can process full-HD images at the constant runtime of 8 fps regardless of the number of subjects in the scene. Code and models available at https://github.com/fabbrimatteo/LoCO .



### Digit Recognition Using Convolution Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2004.00331v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.00331v1)
- **Published**: 2020-04-01 10:41:57+00:00
- **Updated**: 2020-04-01 10:41:57+00:00
- **Authors**: Kajol Gupta
- **Comment**: 7 pages, 4 figures and 2 tables
- **Journal**: None
- **Summary**: In pattern recognition, digit recognition has always been a very challenging task. This paper aims to extracting a correct feature so that it can achieve better accuracy for recognition of digits. The applications of digit recognition such as in password, bank check process, etc. to recognize the valid user identification. Earlier, several researchers have used various different machine learning algorithms in pattern recognition i.e. KNN, SVM, RFC. The main objective of this work is to obtain highest accuracy 99.15% by using convolution neural network (CNN) to recognize the digit without doing too much pre-processing of dataset.



### Single Image Optical Flow Estimation with an Event Camera
- **Arxiv ID**: http://arxiv.org/abs/2004.00347v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.00347v1)
- **Published**: 2020-04-01 11:28:30+00:00
- **Updated**: 2020-04-01 11:28:30+00:00
- **Authors**: Liyuan Pan, Miaomiao Liu, Richard Hartley
- **Comment**: CVPR2020
- **Journal**: None
- **Summary**: Event cameras are bio-inspired sensors that asynchronously report intensity changes in microsecond resolution. DAVIS can capture high dynamics of a scene and simultaneously output high temporal resolution events and low frame-rate intensity images. In this paper, we propose a single image (potentially blurred) and events based optical flow estimation approach. First, we demonstrate how events can be used to improve flow estimates. To this end, we encode the relation between flow and events effectively by presenting an event-based photometric consistency formulation. Then, we consider the special case of image blur caused by high dynamics in the visual environments and show that including the blur formation in our model further constrains flow estimation. This is in sharp contrast to existing works that ignore the blurred images while our formulation can naturally handle either blurred or sharp images to achieve accurate flow estimation. Finally, we reduce flow estimation, as well as image deblurring, to an alternative optimization problem of an objective function using the primal-dual algorithm. Experimental results on both synthetic and real data (with blurred and non-blurred images) show the superiority of our model in comparison to state-of-the-art approaches.



### More Grounded Image Captioning by Distilling Image-Text Matching Model
- **Arxiv ID**: http://arxiv.org/abs/2004.00390v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2004.00390v1)
- **Published**: 2020-04-01 12:42:06+00:00
- **Updated**: 2020-04-01 12:42:06+00:00
- **Authors**: Yuanen Zhou, Meng Wang, Daqing Liu, Zhenzhen Hu, Hanwang Zhang
- **Comment**: Accepted by CVPR 2020
- **Journal**: None
- **Summary**: Visual attention not only improves the performance of image captioners, but also serves as a visual interpretation to qualitatively measure the caption rationality and model transparency. Specifically, we expect that a captioner can fix its attentive gaze on the correct objects while generating the corresponding words. This ability is also known as grounded image captioning. However, the grounding accuracy of existing captioners is far from satisfactory. To improve the grounding accuracy while retaining the captioning quality, it is expensive to collect the word-region alignment as strong supervision. To this end, we propose a Part-of-Speech (POS) enhanced image-text matching model (SCAN \cite{lee2018stacked}): POS-SCAN, as the effective knowledge distillation for more grounded image captioning. The benefits are two-fold: 1) given a sentence and an image, POS-SCAN can ground the objects more accurately than SCAN; 2) POS-SCAN serves as a word-region alignment regularization for the captioner's visual attention module. By showing benchmark experimental results, we demonstrate that conventional image captioners equipped with POS-SCAN can significantly improve the grounding accuracy without strong supervision. Last but not the least, we explore the indispensable Self-Critical Sequence Training (SCST) \cite{Rennie_2017_CVPR} in the context of grounded image captioning and show that the image-text matching score can serve as a reward for more grounded captioning \footnote{https://github.com/YuanEZhou/Grounded-Image-Captioning}.



### Two-shot Spatially-varying BRDF and Shape Estimation
- **Arxiv ID**: http://arxiv.org/abs/2004.00403v1
- **DOI**: 10.1109/CVPR42600.2020.00404
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.00403v1)
- **Published**: 2020-04-01 12:56:13+00:00
- **Updated**: 2020-04-01 12:56:13+00:00
- **Authors**: Mark Boss, Varun Jampani, Kihwan Kim, Hendrik P. A. Lensch, Jan Kautz
- **Comment**: None
- **Journal**: None
- **Summary**: Capturing the shape and spatially-varying appearance (SVBRDF) of an object from images is a challenging task that has applications in both computer vision and graphics. Traditional optimization-based approaches often need a large number of images taken from multiple views in a controlled environment. Newer deep learning-based approaches require only a few input images, but the reconstruction quality is not on par with optimization techniques. We propose a novel deep learning architecture with a stage-wise estimation of shape and SVBRDF. The previous predictions guide each estimation, and a joint refinement network later refines both SVBRDF and shape. We follow a practical mobile image capture setting and use unaligned two-shot flash and no-flash images as input. Both our two-shot image capture and network inference can run on mobile hardware. We also create a large-scale synthetic training dataset with domain-randomized geometry and realistic materials. Extensive experiments on both synthetic and real-world datasets show that our network trained on a synthetic dataset can generalize well to real-world images. Comparisons with recent approaches demonstrate the superior performance of the proposed approach.



### Image Demoireing with Learnable Bandpass Filters
- **Arxiv ID**: http://arxiv.org/abs/2004.00406v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.00406v1)
- **Published**: 2020-04-01 12:57:26+00:00
- **Updated**: 2020-04-01 12:57:26+00:00
- **Authors**: Bolun Zheng, Shanxin Yuan, Gregory Slabaugh, Ales Leonardis
- **Comment**: Accepted by CVPR2020. Code is available at
  https://github.com/zhenngbolun/Learnbale_Bandpass_Filter
- **Journal**: None
- **Summary**: Image demoireing is a multi-faceted image restoration task involving both texture and color restoration. In this paper, we propose a novel multiscale bandpass convolutional neural network (MBCNN) to address this problem. As an end-to-end solution, MBCNN respectively solves the two sub-problems. For texture restoration, we propose a learnable bandpass filter (LBF) to learn the frequency prior for moire texture removal. For color restoration, we propose a two-step tone mapping strategy, which first applies a global tone mapping to correct for a global color shift, and then performs local fine tuning of the color per pixel. Through an ablation study, we demonstrate the effectiveness of the different components of MBCNN. Experimental results on two public datasets show that our method outperforms state-of-the-art methods by a large margin (more than 2dB in terms of PSNR).



### M2m: Imbalanced Classification via Major-to-minor Translation
- **Arxiv ID**: http://arxiv.org/abs/2004.00431v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.00431v2)
- **Published**: 2020-04-01 13:21:17+00:00
- **Updated**: 2020-12-20 10:48:34+00:00
- **Authors**: Jaehyung Kim, Jongheon Jeong, Jinwoo Shin
- **Comment**: 12 pages; CVPR 2020
- **Journal**: None
- **Summary**: In most real-world scenarios, labeled training datasets are highly class-imbalanced, where deep neural networks suffer from generalizing to a balanced testing criterion. In this paper, we explore a novel yet simple way to alleviate this issue by augmenting less-frequent classes via translating samples (e.g., images) from more-frequent classes. This simple approach enables a classifier to learn more generalizable features of minority classes, by transferring and leveraging the diversity of the majority information. Our experimental results on a variety of class-imbalanced datasets show that the proposed method improves the generalization on minority classes significantly compared to other existing re-sampling or re-weighting methods. The performance of our method even surpasses those of previous state-of-the-art methods for the imbalanced classification.



### Semantic Drift Compensation for Class-Incremental Learning
- **Arxiv ID**: http://arxiv.org/abs/2004.00440v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.00440v1)
- **Published**: 2020-04-01 13:31:19+00:00
- **Updated**: 2020-04-01 13:31:19+00:00
- **Authors**: Lu Yu, Bartłomiej Twardowski, Xialei Liu, Luis Herranz, Kai Wang, Yongmei Cheng, Shangling Jui, Joost van de Weijer
- **Comment**: Accepted at CVPR2020, Code available at
  \url{https://github.com/yulu0724/SDC-IL}
- **Journal**: None
- **Summary**: Class-incremental learning of deep networks sequentially increases the number of classes to be classified. During training, the network has only access to data of one task at a time, where each task contains several classes. In this setting, networks suffer from catastrophic forgetting which refers to the drastic drop in performance on previous tasks. The vast majority of methods have studied this scenario for classification networks, where for each new task the classification layer of the network must be augmented with additional weights to make room for the newly added classes. Embedding networks have the advantage that new classes can be naturally included into the network without adding new weights. Therefore, we study incremental learning for embedding networks. In addition, we propose a new method to estimate the drift, called semantic drift, of features and compensate for it without the need of any exemplars. We approximate the drift of previous tasks based on the drift that is experienced by current task data. We perform experiments on fine-grained datasets, CIFAR100 and ImageNet-Subset. We demonstrate that embedding networks suffer significantly less from catastrophic forgetting. We outperform existing methods which do not require exemplars and obtain competitive results compared to methods which store exemplars. Furthermore, we show that our proposed SDC when combined with existing methods to prevent forgetting consistently improves results.



### Learning to Cluster Faces via Confidence and Connectivity Estimation
- **Arxiv ID**: http://arxiv.org/abs/2004.00445v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.00445v2)
- **Published**: 2020-04-01 13:39:37+00:00
- **Updated**: 2020-04-03 05:38:44+00:00
- **Authors**: Lei Yang, Dapeng Chen, Xiaohang Zhan, Rui Zhao, Chen Change Loy, Dahua Lin
- **Comment**: 8 pages, 6 figures, CVPR 2020
- **Journal**: None
- **Summary**: Face clustering is an essential tool for exploiting the unlabeled face data, and has a wide range of applications including face annotation and retrieval. Recent works show that supervised clustering can result in noticeable performance gain. However, they usually involve heuristic steps and require numerous overlapped subgraphs, severely restricting their accuracy and efficiency. In this paper, we propose a fully learnable clustering framework without requiring a large number of overlapped subgraphs. Instead, we transform the clustering problem into two sub-problems. Specifically, two graph convolutional networks, named GCN-V and GCN-E, are designed to estimate the confidence of vertices and the connectivity of edges, respectively. With the vertex confidence and edge connectivity, we can naturally organize more relevant vertices on the affinity graph and group them into clusters. Experiments on two large-scale benchmarks show that our method significantly improves clustering accuracy and thus performance of the recognition models trained on top, yet it is an order of magnitude more efficient than existing supervised methods.



### Rethinking Data Augmentation for Image Super-resolution: A Comprehensive Analysis and a New Strategy
- **Arxiv ID**: http://arxiv.org/abs/2004.00448v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.00448v2)
- **Published**: 2020-04-01 13:49:38+00:00
- **Updated**: 2020-04-23 08:28:10+00:00
- **Authors**: Jaejun Yoo, Namhyuk Ahn, Kyung-Ah Sohn
- **Comment**: None
- **Journal**: None
- **Summary**: Data augmentation is an effective way to improve the performance of deep networks. Unfortunately, current methods are mostly developed for high-level vision tasks (e.g., classification) and few are studied for low-level vision tasks (e.g., image restoration). In this paper, we provide a comprehensive analysis of the existing augmentation methods applied to the super-resolution task. We find that the methods discarding or manipulating the pixels or features too much hamper the image restoration, where the spatial relationship is very important. Based on our analyses, we propose CutBlur that cuts a low-resolution patch and pastes it to the corresponding high-resolution image region and vice versa. The key intuition of CutBlur is to enable a model to learn not only "how" but also "where" to super-resolve an image. By doing so, the model can understand "how much", instead of blindly learning to apply super-resolution to every given pixel. Our method consistently and significantly improves the performance across various scenarios, especially when the model size is big and the data is collected under real-world environments. We also show that our method improves other low-level vision tasks, such as denoising and compression artifact removal.



### Spatio-temporal Tubelet Feature Aggregation and Object Linking in Videos
- **Arxiv ID**: http://arxiv.org/abs/2004.00451v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.00451v2)
- **Published**: 2020-04-01 13:52:03+00:00
- **Updated**: 2020-11-06 12:17:33+00:00
- **Authors**: Daniel Cores, Víctor M. Brea, Manuel Mucientes
- **Comment**: None
- **Journal**: None
- **Summary**: This paper addresses the problem of how to exploit spatio-temporal information available in videos to improve the object detection precision. We propose a two stage object detector called FANet based on short-term spatio-temporal feature aggregation to give a first detection set, and long-term object linking to refine these detections. Firstly, we generate a set of short tubelet proposals containing the object in $N$ consecutive frames. Then, we aggregate RoI pooled deep features through the tubelet using a temporal pooling operator that summarizes the information with a fixed size output independent of the number of input frames. On top of that, we define a double head implementation that we feed with spatio-temporal aggregated information for spatio-temporal object classification, and with spatial information extracted from the current frame for object localization and spatial classification. Furthermore, we also specialize each head branch architecture to better perform in each task taking into account the input data. Finally, a long-term linking method builds long tubes using the previously calculated short tubelets to overcome detection errors. We have evaluated our model in the widely used ImageNet VID dataset achieving a 80.9% mAP, which is the new state-of-the-art result for single models. Also, in the challenging small object detection dataset USC-GRAD-STDdb, our proposal outperforms the single frame baseline by 5.4% mAP.



### PIFuHD: Multi-Level Pixel-Aligned Implicit Function for High-Resolution 3D Human Digitization
- **Arxiv ID**: http://arxiv.org/abs/2004.00452v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2004.00452v1)
- **Published**: 2020-04-01 13:52:53+00:00
- **Updated**: 2020-04-01 13:52:53+00:00
- **Authors**: Shunsuke Saito, Tomas Simon, Jason Saragih, Hanbyul Joo
- **Comment**: project page: https://shunsukesaito.github.io/PIFuHD
- **Journal**: The IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR), 2020
- **Summary**: Recent advances in image-based 3D human shape estimation have been driven by the significant improvement in representation power afforded by deep neural networks. Although current approaches have demonstrated the potential in real world settings, they still fail to produce reconstructions with the level of detail often present in the input images. We argue that this limitation stems primarily form two conflicting requirements; accurate predictions require large context, but precise predictions require high resolution. Due to memory limitations in current hardware, previous approaches tend to take low resolution images as input to cover large spatial context, and produce less precise (or low resolution) 3D estimates as a result. We address this limitation by formulating a multi-level architecture that is end-to-end trainable. A coarse level observes the whole image at lower resolution and focuses on holistic reasoning. This provides context to an fine level which estimates highly detailed geometry by observing higher-resolution images. We demonstrate that our approach significantly outperforms existing state-of-the-art techniques on single image human shape reconstruction by fully leveraging 1k-resolution input images.



### A Survey of Convolutional Neural Networks: Analysis, Applications, and Prospects
- **Arxiv ID**: http://arxiv.org/abs/2004.02806v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, 68T07 (Primary) 68T10, 68T45 (Secondary), I.2.6; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2004.02806v1)
- **Published**: 2020-04-01 14:04:10+00:00
- **Updated**: 2020-04-01 14:04:10+00:00
- **Authors**: Zewen Li, Wenjie Yang, Shouheng Peng, Fan Liu
- **Comment**: 21 pages, 33 figures, journal
- **Journal**: None
- **Summary**: Convolutional Neural Network (CNN) is one of the most significant networks in the deep learning field. Since CNN made impressive achievements in many areas, including but not limited to computer vision and natural language processing, it attracted much attention both of industry and academia in the past few years. The existing reviews mainly focus on the applications of CNN in different scenarios without considering CNN from a general perspective, and some novel ideas proposed recently are not covered. In this review, we aim to provide novel ideas and prospects in this fast-growing field as much as possible. Besides, not only two-dimensional convolution but also one-dimensional and multi-dimensional ones are involved. First, this review starts with a brief introduction to the history of CNN. Second, we provide an overview of CNN. Third, classic and advanced CNN models are introduced, especially those key points making them reach state-of-the-art results. Fourth, through experimental analysis, we draw some conclusions and provide several rules of thumb for function selection. Fifth, the applications of one-dimensional, two-dimensional, and multi-dimensional convolution are covered. Finally, some open issues and promising directions for CNN are discussed to serve as guidelines for future work.



### Objects of violence: synthetic data for practical ML in human rights investigations
- **Arxiv ID**: http://arxiv.org/abs/2004.01030v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.01030v1)
- **Published**: 2020-04-01 14:50:43+00:00
- **Updated**: 2020-04-01 14:50:43+00:00
- **Authors**: Lachlan Kermode, Jan Freyberg, Alican Akturk, Robert Trafford, Denis Kochetkov, Rafael Pardinas, Eyal Weizman, Julien Cornebise
- **Comment**: Presented at NeurIPS 2019 in the AI for Social Good track
- **Journal**: None
- **Summary**: We introduce a machine learning workflow to search for, identify, and meaningfully triage videos and images of munitions, weapons, and military equipment, even when limited training data exists for the object of interest. This workflow is designed to expedite the work of OSINT ("open source intelligence") researchers in human rights investigations. It consists of three components: automatic rendering and annotating of synthetic datasets that make up for a lack of training data; training image classifiers from combined sets of photographic and synthetic data; and mtriage, an open source software that orchestrates these classifiers' deployment to triage public domain media, and visualise predictions in a web interface. We show that synthetic data helps to train classifiers more effectively, and that certain approaches yield better results for different architectures. We then demonstrate our workflow in two real-world human rights investigations: the use of the Triple-Chaser tear gas grenade against civilians, and the verification of allegations of military presence in Ukraine in 2014.



### Medical-based Deep Curriculum Learning for Improved Fracture Classification
- **Arxiv ID**: http://arxiv.org/abs/2004.00482v1
- **DOI**: 10.1007/978-3-030-32226-7_77
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.00482v1)
- **Published**: 2020-04-01 14:56:43+00:00
- **Updated**: 2020-04-01 14:56:43+00:00
- **Authors**: Amelia Jiménez-Sánchez, Diana Mateus, Sonja Kirchhoff, Chlodwig Kirchhoff, Peter Biberthaler, Nassir Navab, Miguel A. González Ballester, Gemma Piella
- **Comment**: MICCAI 2019
- **Journal**: None
- **Summary**: Current deep-learning based methods do not easily integrate to clinical protocols, neither take full advantage of medical knowledge. In this work, we propose and compare several strategies relying on curriculum learning, to support the classification of proximal femur fracture from X-ray images, a challenging problem as reflected by existing intra- and inter-expert disagreement. Our strategies are derived from knowledge such as medical decision trees and inconsistencies in the annotations of multiple experts, which allows us to assign a degree of difficulty to each training sample. We demonstrate that if we start learning "easy" examples and move towards "hard", the model can reach a better performance, even with fewer data. The evaluation is performed on the classification of a clinical dataset of about 1000 X-ray images. Our results show that, compared to class-uniform and random strategies, the proposed medical knowledge-based curriculum, performs up to 15% better in terms of accuracy, achieving the performance of experienced trauma surgeons.



### Future Video Synthesis with Object Motion Prediction
- **Arxiv ID**: http://arxiv.org/abs/2004.00542v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.00542v2)
- **Published**: 2020-04-01 16:09:54+00:00
- **Updated**: 2020-04-15 10:55:42+00:00
- **Authors**: Yue Wu, Rongrong Gao, Jaesik Park, Qifeng Chen
- **Comment**: CVPR 2020
- **Journal**: None
- **Summary**: We present an approach to predict future video frames given a sequence of continuous video frames in the past. Instead of synthesizing images directly, our approach is designed to understand the complex scene dynamics by decoupling the background scene and moving objects. The appearance of the scene components in the future is predicted by non-rigid deformation of the background and affine transformation of moving objects. The anticipated appearances are combined to create a reasonable video in the future. With this procedure, our method exhibits much less tearing or distortion artifact compared to other approaches. Experimental results on the Cityscapes and KITTI datasets show that our model outperforms the state-of-the-art in terms of visual quality and accuracy.



### Physically Realizable Adversarial Examples for LiDAR Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2004.00543v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2004.00543v2)
- **Published**: 2020-04-01 16:11:04+00:00
- **Updated**: 2020-04-02 16:02:41+00:00
- **Authors**: James Tu, Mengye Ren, Siva Manivasagam, Ming Liang, Bin Yang, Richard Du, Frank Cheng, Raquel Urtasun
- **Comment**: Accepted to CVPR 2020
- **Journal**: None
- **Summary**: Modern autonomous driving systems rely heavily on deep learning models to process point cloud sensory data; meanwhile, deep models have been shown to be susceptible to adversarial attacks with visually imperceptible perturbations. Despite the fact that this poses a security concern for the self-driving industry, there has been very little exploration in terms of 3D perception, as most adversarial attacks have only been applied to 2D flat images. In this paper, we address this issue and present a method to generate universal 3D adversarial objects to fool LiDAR detectors. In particular, we demonstrate that placing an adversarial object on the rooftop of any target vehicle to hide the vehicle entirely from LiDAR detectors with a success rate of 80%. We report attack results on a suite of detectors using various input representation of point clouds. We also conduct a pilot study on adversarial defense using data augmentation. This is one step closer towards safer self-driving under unseen conditions from limited training data.



### Feature-Driven Super-Resolution for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2004.00554v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.00554v1)
- **Published**: 2020-04-01 16:33:07+00:00
- **Updated**: 2020-04-01 16:33:07+00:00
- **Authors**: Bin Wang, Tao Lu, Yanduo Zhang
- **Comment**: 4 pages, 3 figures
- **Journal**: None
- **Summary**: Although some convolutional neural networks (CNNs) based super-resolution (SR) algorithms yield good visual performances on single images recently. Most of them focus on perfect perceptual quality but ignore specific needs of subsequent detection task. This paper proposes a simple but powerful feature-driven super-resolution (FDSR) to improve the detection performance of low-resolution (LR) images. First, the proposed method uses feature-domain prior which extracts from an existing detector backbone to guide the HR image reconstruction. Then, with the aligned features, FDSR update SR parameters for better detection performance. Comparing with some state-of-the-art SR algorithms with 4$\times$ scale factor, FDSR outperforms the detection performance mAP on MS COCO validation, VOC2007 databases with good generalization to other detection networks.



### LGVTON: A Landmark Guided Approach to Virtual Try-On
- **Arxiv ID**: http://arxiv.org/abs/2004.00562v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.00562v2)
- **Published**: 2020-04-01 16:49:57+00:00
- **Updated**: 2021-09-29 05:46:45+00:00
- **Authors**: Debapriya Roy, Sanchayan Santra, Bhabatosh Chanda
- **Comment**: Accepted for publication in Multimedia tools and applications journal
- **Journal**: None
- **Summary**: In this paper, we propose a Landmark Guided Virtual Try-On (LGVTON) method for clothes, which aims to solve the problem of clothing trials on e-commerce websites. Given the images of two people: a person and a model, it generates a rendition of the person wearing the clothes of the model. This is useful considering the fact that on most e-commerce websites images of only clothes are not usually available. We follow a three-stage approach to achieve our objective. In the first stage, LGVTON warps the clothes of the model using a Thin-Plate Spline (TPS) based transformation to fit the person. Unlike previous TPS-based methods, we use the landmarks (of human and clothes) to compute the TPS transformation. This enables the warping to work independently of the complex patterns, such as stripes, florals, and textures, present on the clothes. However, this computed warp may not always be very precise. We, therefore, further refine it in the subsequent stages with the help of a mask generator (Stage 2) and an image synthesizer (Stage 3) modules. The mask generator improves the fit of the warped clothes, and the image synthesizer ensures a realistic output. To tackle the problem of lack of paired training data, we resort to a self-supervised training strategy. Here paired data refers to the image pair of model and person wearing the same cloth. We compare LGVTON with four existing methods on two popular fashion datasets namely MPV and DeepFashion using two performance measures, FID (Fr\'echet Inception Distance) and SSIM (Structural Similarity Index). The proposed method in most cases outperforms the state-of-the-art methods.



### Improving Deep Hyperspectral Image Classification Performance with Spectral Unmixing
- **Arxiv ID**: http://arxiv.org/abs/2004.00583v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.00583v4)
- **Published**: 2020-04-01 17:14:05+00:00
- **Updated**: 2020-12-21 05:10:08+00:00
- **Authors**: Alan J. X. Guo, Fei Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advances in neural networks have made great progress in the hyperspectral image (HSI) classification. However, the overfitting effect, which is mainly caused by complicated model structure and small training set, remains a major concern. Reducing the complexity of the neural networks could prevent overfitting to some extent, but also declines the networks' ability to express more abstract features. Enlarging the training set is also difficult, for the high expense of acquisition and manual labeling. In this paper, we propose an abundance-based multi-HSI classification method. Firstly, we convert every HSI from the spectral domain to the abundance domain by a dataset-specific autoencoder. Secondly, the abundance representations from multiple HSIs are collected to form an enlarged dataset. Lastly, we train an abundance-based classifier and employ the classifier to predict over all the involved HSI datasets. Different from the spectra that are usually highly mixed, the abundance features are more representative in reduced dimension with less noise. This benefits the proposed method to employ simple classifiers and enlarged training data, and to expect less overfitting issues. The effectiveness of the proposed method is verified by the ablation study and the comparative experiments.



### Symmetry and Group in Attribute-Object Compositions
- **Arxiv ID**: http://arxiv.org/abs/2004.00587v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.00587v1)
- **Published**: 2020-04-01 17:16:57+00:00
- **Updated**: 2020-04-01 17:16:57+00:00
- **Authors**: Yong-Lu Li, Yue Xu, Xiaohan Mao, Cewu Lu
- **Comment**: Accepted to CVPR 2020, supplementary materials included, code
  available:https://github.com/DirtyHarryLYL/SymNet
- **Journal**: None
- **Summary**: Attributes and objects can compose diverse compositions. To model the compositional nature of these general concepts, it is a good choice to learn them through transformations, such as coupling and decoupling. However, complex transformations need to satisfy specific principles to guarantee the rationality. In this paper, we first propose a previously ignored principle of attribute-object transformation: Symmetry. For example, coupling peeled-apple with attribute peeled should result in peeled-apple, and decoupling peeled from apple should still output apple. Incorporating the symmetry principle, a transformation framework inspired by group theory is built, i.e. SymNet. SymNet consists of two modules, Coupling Network and Decoupling Network. With the group axioms and symmetry property as objectives, we adopt Deep Neural Networks to implement SymNet and train it in an end-to-end paradigm. Moreover, we propose a Relative Moving Distance (RMD) based recognition method to utilize the attribute change instead of the attribute pattern itself to classify attributes. Our symmetry learning can be utilized for the Compositional Zero-Shot Learning task and outperforms the state-of-the-art on widely-used benchmarks. Code is available at https://github.com/DirtyHarryLYL/SymNet.



### Better Sign Language Translation with STMC-Transformer
- **Arxiv ID**: http://arxiv.org/abs/2004.00588v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.HC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.00588v2)
- **Published**: 2020-04-01 17:20:04+00:00
- **Updated**: 2020-11-03 00:59:54+00:00
- **Authors**: Kayo Yin, Jesse Read
- **Comment**: Proceedings of the 28th International Conference on Computational
  Linguistics (COLING'2020)
- **Journal**: 28th International Conference on Computational Linguistics 2020
- **Summary**: Sign Language Translation (SLT) first uses a Sign Language Recognition (SLR) system to extract sign language glosses from videos. Then, a translation system generates spoken language translations from the sign language glosses. This paper focuses on the translation system and introduces the STMC-Transformer which improves on the current state-of-the-art by over 5 and 7 BLEU respectively on gloss-to-text and video-to-text translation of the PHOENIX-Weather 2014T dataset. On the ASLG-PC12 corpus, we report an increase of over 16 BLEU.   We also demonstrate the problem in current methods that rely on gloss supervision. The video-to-text translation of our STMC-Transformer outperforms translation of GT glosses. This contradicts previous claims that GT gloss translation acts as an upper bound for SLT performance and reveals that glosses are an inefficient representation of sign language. For future SLT research, we therefore suggest an end-to-end training of the recognition and translation models, or using a different sign language annotation scheme.



### Robust Image Reconstruction with Misaligned Structural Information
- **Arxiv ID**: http://arxiv.org/abs/2004.00589v3
- **DOI**: 10.1109/ACCESS.2020.3043638
- **Categories**: **eess.IV**, cs.CV, cs.NA, math.NA, 65K10, 68U10, 94A08, I.4.5; I.4.9; J.2
- **Links**: [PDF](http://arxiv.org/pdf/2004.00589v3)
- **Published**: 2020-04-01 17:21:25+00:00
- **Updated**: 2020-12-24 12:11:40+00:00
- **Authors**: Leon Bungert, Matthias J. Ehrhardt
- **Comment**: None
- **Journal**: IEEE Access, vol. 8, pp. 222944-222955, 2020,
- **Summary**: Multi-modality (or multi-channel) imaging is becoming increasingly important and more widely available, e.g. hyperspectral imaging in remote sensing, spectral CT in material sciences as well as multi-contrast MRI and PET-MR in medicine. Research in the last decades resulted in a plethora of mathematical methods to combine data from several modalities. State-of-the-art methods, often formulated as variational regularization, have shown to significantly improve image reconstruction both quantitatively and qualitatively. Almost all of these models rely on the assumption that the modalities are perfectly registered, which is not the case in most real world applications. We propose a variational framework which jointly performs reconstruction and registration, thereby overcoming this hurdle. Our approach is the first to achieve this for different modalities and outranks established approaches in terms of accuracy of both reconstruction and registration. Numerical results on simulated and real data show the potential of the proposed strategy for various applications in multi-contrast MRI, PET-MR, and hyperspectral imaging: typical misalignments between modalities such as rotations, translations, zooms can be effectively corrected during the reconstruction process. Therefore the proposed framework allows the robust exploitation of shared information across multiple modalities under real conditions.



### Background Matting: The World is Your Green Screen
- **Arxiv ID**: http://arxiv.org/abs/2004.00626v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.00626v2)
- **Published**: 2020-04-01 17:38:55+00:00
- **Updated**: 2020-04-10 03:31:38+00:00
- **Authors**: Soumyadip Sengupta, Vivek Jayaram, Brian Curless, Steve Seitz, Ira Kemelmacher-Shlizerman
- **Comment**: Accepted to CVPR 2020
- **Journal**: None
- **Summary**: We propose a method for creating a matte -- the per-pixel foreground color and alpha -- of a person by taking photos or videos in an everyday setting with a handheld camera. Most existing matting methods require a green screen background or a manually created trimap to produce a good matte. Automatic, trimap-free methods are appearing, but are not of comparable quality. In our trimap free approach, we ask the user to take an additional photo of the background without the subject at the time of capture. This step requires a small amount of foresight but is far less time-consuming than creating a trimap. We train a deep network with an adversarial loss to predict the matte. We first train a matting network with supervised loss on ground truth data with synthetic composites. To bridge the domain gap to real imagery with no labeling, we train another matting network guided by the first network and by a discriminator that judges the quality of composites. We demonstrate results on a wide variety of photos and videos and show significant improvement over the state of the art.



### EPOS: Estimating 6D Pose of Objects with Symmetries
- **Arxiv ID**: http://arxiv.org/abs/2004.00605v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.00605v1)
- **Published**: 2020-04-01 17:41:08+00:00
- **Updated**: 2020-04-01 17:41:08+00:00
- **Authors**: Tomas Hodan, Daniel Barath, Jiri Matas
- **Comment**: Accepted to CVPR 2020
- **Journal**: None
- **Summary**: We present a new method for estimating the 6D pose of rigid objects with available 3D models from a single RGB input image. The method is applicable to a broad range of objects, including challenging ones with global or partial symmetries. An object is represented by compact surface fragments which allow handling symmetries in a systematic manner. Correspondences between densely sampled pixels and the fragments are predicted using an encoder-decoder network. At each pixel, the network predicts: (i) the probability of each object's presence, (ii) the probability of the fragments given the object's presence, and (iii) the precise 3D location on each fragment. A data-dependent number of corresponding 3D locations is selected per pixel, and poses of possibly multiple object instances are estimated using a robust and efficient variant of the PnP-RANSAC algorithm. In the BOP Challenge 2019, the method outperforms all RGB and most RGB-D and D methods on the T-LESS and LM-O datasets. On the YCB-V dataset, it is superior to all competitors, with a large margin over the second-best RGB method. Source code is at: cmp.felk.cvut.cz/epos.



### Articulation-aware Canonical Surface Mapping
- **Arxiv ID**: http://arxiv.org/abs/2004.00614v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.00614v3)
- **Published**: 2020-04-01 17:56:45+00:00
- **Updated**: 2020-05-26 22:22:44+00:00
- **Authors**: Nilesh Kulkarni, Abhinav Gupta, David F. Fouhey, Shubham Tulsiani
- **Comment**: To appear at CVPR 2020, project page
  https://nileshkulkarni.github.io/acsm/
- **Journal**: None
- **Summary**: We tackle the tasks of: 1) predicting a Canonical Surface Mapping (CSM) that indicates the mapping from 2D pixels to corresponding points on a canonical template shape, and 2) inferring the articulation and pose of the template corresponding to the input image. While previous approaches rely on keypoint supervision for learning, we present an approach that can learn without such annotations. Our key insight is that these tasks are geometrically related, and we can obtain supervisory signal via enforcing consistency among the predictions. We present results across a diverse set of animal object categories, showing that our method can learn articulation and CSM prediction from image collections using only foreground mask labels for training. We empirically show that allowing articulation helps learn more accurate CSM prediction, and that enforcing the consistency with predicted CSM is similarly critical for learning meaningful articulation.



### Evading Deepfake-Image Detectors with White- and Black-Box Attacks
- **Arxiv ID**: http://arxiv.org/abs/2004.00622v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/2004.00622v1)
- **Published**: 2020-04-01 17:59:59+00:00
- **Updated**: 2020-04-01 17:59:59+00:00
- **Authors**: Nicholas Carlini, Hany Farid
- **Comment**: None
- **Journal**: None
- **Summary**: It is now possible to synthesize highly realistic images of people who don't exist. Such content has, for example, been implicated in the creation of fraudulent social-media profiles responsible for dis-information campaigns. Significant efforts are, therefore, being deployed to detect synthetically-generated content. One popular forensic approach trains a neural network to distinguish real from synthetic content.   We show that such forensic classifiers are vulnerable to a range of attacks that reduce the classifier to near-0% accuracy. We develop five attack case studies on a state-of-the-art classifier that achieves an area under the ROC curve (AUC) of 0.95 on almost all existing image generators, when only trained on one generator. With full access to the classifier, we can flip the lowest bit of each pixel in an image to reduce the classifier's AUC to 0.0005; perturb 1% of the image area to reduce the classifier's AUC to 0.08; or add a single noise pattern in the synthesizer's latent space to reduce the classifier's AUC to 0.17. We also develop a black-box attack that, with no access to the target classifier, reduces the AUC to 0.22. These attacks reveal significant vulnerabilities of certain image-forensic classifiers.



### Object-Centric Image Generation with Factored Depths, Locations, and Appearances
- **Arxiv ID**: http://arxiv.org/abs/2004.00642v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.00642v1)
- **Published**: 2020-04-01 18:00:11+00:00
- **Updated**: 2020-04-01 18:00:11+00:00
- **Authors**: Titas Anciukevicius, Christoph H. Lampert, Paul Henderson
- **Comment**: None
- **Journal**: None
- **Summary**: We present a generative model of images that explicitly reasons over the set of objects they show. Our model learns a structured latent representation that separates objects from each other and from the background; unlike prior works, it explicitly represents the 2D position and depth of each object, as well as an embedding of its segmentation mask and appearance. The model can be trained from images alone in a purely unsupervised fashion without the need for object masks or depth information. Moreover, it always generates complete objects, even though a significant fraction of training images contain occlusions. Finally, we show that our model can infer decompositions of novel images into their constituent objects, including accurate prediction of depth ordering and segmentation of occluded parts.



### Synchronizing Probability Measures on Rotations via Optimal Transport
- **Arxiv ID**: http://arxiv.org/abs/2004.00663v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.00663v1)
- **Published**: 2020-04-01 18:44:18+00:00
- **Updated**: 2020-04-01 18:44:18+00:00
- **Authors**: Tolga Birdal, Michael Arbel, Umut Şimşekli, Leonidas Guibas
- **Comment**: Accepted for publication at CVPR 2020, includes supplementary
  material. Project website: https://github.com/SynchInVision/probsync
- **Journal**: None
- **Summary**: We introduce a new paradigm, $\textit{measure synchronization}$, for synchronizing graphs with measure-valued edges. We formulate this problem as maximization of the cycle-consistency in the space of probability measures over relative rotations. In particular, we aim at estimating marginal distributions of absolute orientations by synchronizing the $\textit{conditional}$ ones, which are defined on the Riemannian manifold of quaternions. Such graph optimization on distributions-on-manifolds enables a natural treatment of multimodal hypotheses, ambiguities and uncertainties arising in many computer vision applications such as SLAM, SfM, and object pose estimation. We first formally define the problem as a generalization of the classical rotation graph synchronization, where in our case the vertices denote probability measures over rotations. We then measure the quality of the synchronization by using Sinkhorn divergences, which reduces to other popular metrics such as Wasserstein distance or the maximum mean discrepancy as limit cases. We propose a nonparametric Riemannian particle optimization approach to solve the problem. Even though the problem is non-convex, by drawing a connection to the recently proposed sparse optimization methods, we show that the proposed algorithm converges to the global optimum in a special case of the problem under certain conditions. Our qualitative and quantitative experiments show the validity of our approach and we bring in new perspectives to the study of synchronization.



### Generalized Zero-Shot Learning Via Over-Complete Distribution
- **Arxiv ID**: http://arxiv.org/abs/2004.00666v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.00666v1)
- **Published**: 2020-04-01 19:05:28+00:00
- **Updated**: 2020-04-01 19:05:28+00:00
- **Authors**: Rohit Keshari, Richa Singh, Mayank Vatsa
- **Comment**: 9 pages, 5 figures, Accepted in CVPR 2020
- **Journal**: None
- **Summary**: A well trained and generalized deep neural network (DNN) should be robust to both seen and unseen classes. However, the performance of most of the existing supervised DNN algorithms degrade for classes which are unseen in the training set. To learn a discriminative classifier which yields good performance in Zero-Shot Learning (ZSL) settings, we propose to generate an Over-Complete Distribution (OCD) using Conditional Variational Autoencoder (CVAE) of both seen and unseen classes. In order to enforce the separability between classes and reduce the class scatter, we propose the use of Online Batch Triplet Loss (OBTL) and Center Loss (CL) on the generated OCD. The effectiveness of the framework is evaluated using both Zero-Shot Learning and Generalized Zero-Shot Learning protocols on three publicly available benchmark databases, SUN, CUB and AWA2. The results show that generating over-complete distributions and enforcing the classifier to learn a transform function from overlapping to non-overlapping distributions can improve the performance on both seen and unseen classes.



### Adversarial Learning for Personalized Tag Recommendation
- **Arxiv ID**: http://arxiv.org/abs/2004.00698v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.00698v1)
- **Published**: 2020-04-01 20:41:41+00:00
- **Updated**: 2020-04-01 20:41:41+00:00
- **Authors**: Erik Quintanilla, Yogesh Rawat, Andrey Sakryukin, Mubarak Shah, Mohan Kankanhalli
- **Comment**: None
- **Journal**: None
- **Summary**: We have recently seen great progress in image classification due to the success of deep convolutional neural networks and the availability of large-scale datasets. Most of the existing work focuses on single-label image classification. However, there are usually multiple tags associated with an image. The existing works on multi-label classification are mainly based on lab curated labels. Humans assign tags to their images differently, which is mainly based on their interests and personal tagging behavior. In this paper, we address the problem of personalized tag recommendation and propose an end-to-end deep network which can be trained on large-scale datasets. The user-preference is learned within the network in an unsupervised way where the network performs joint optimization for user-preference and visual encoding. A joint training of user-preference and visual encoding allows the network to efficiently integrate the visual preference with tagging behavior for a better user recommendation. In addition, we propose the use of adversarial learning, which enforces the network to predict tags resembling user-generated tags. We demonstrate the effectiveness of the proposed model on two different large-scale and publicly available datasets, YFCC100M and NUS-WIDE. The proposed method achieves significantly better performance on both the datasets when compared to the baselines and other state-of-the-art methods. The code is publicly available at https://github.com/vyzuer/ALTReco.



### Revisiting Pose-Normalization for Fine-Grained Few-Shot Recognition
- **Arxiv ID**: http://arxiv.org/abs/2004.00705v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.00705v1)
- **Published**: 2020-04-01 21:00:06+00:00
- **Updated**: 2020-04-01 21:00:06+00:00
- **Authors**: Luming Tang, Davis Wertheimer, Bharath Hariharan
- **Comment**: To appear in CVPR 2020
- **Journal**: None
- **Summary**: Few-shot, fine-grained classification requires a model to learn subtle, fine-grained distinctions between different classes (e.g., birds) based on a few images alone. This requires a remarkable degree of invariance to pose, articulation and background. A solution is to use pose-normalized representations: first localize semantic parts in each image, and then describe images by characterizing the appearance of each part. While such representations are out of favor for fully supervised classification, we show that they are extremely effective for few-shot fine-grained classification. With a minimal increase in model capacity, pose normalization improves accuracy between 10 and 20 percentage points for shallow and deep architectures, generalizes better to new domains, and is effective for multiple few-shot algorithms and network backbones. Code is available at https://github.com/Tsingularity/PoseNorm_Fewshot



### Memory-Efficient Incremental Learning Through Feature Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2004.00713v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.00713v2)
- **Published**: 2020-04-01 21:16:05+00:00
- **Updated**: 2020-08-24 21:44:38+00:00
- **Authors**: Ahmet Iscen, Jeffrey Zhang, Svetlana Lazebnik, Cordelia Schmid
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce an approach for incremental learning that preserves feature descriptors of training images from previously learned classes, instead of the images themselves, unlike most existing work. Keeping the much lower-dimensional feature embeddings of images reduces the memory footprint significantly. We assume that the model is updated incrementally for new classes as new data becomes available sequentially.This requires adapting the previously stored feature vectors to the updated feature space without having access to the corresponding original training images. Feature adaptation is learned with a multi-layer perceptron, which is trained on feature pairs corresponding to the outputs of the original and updated network on a training image. We validate experimentally that such a transformation generalizes well to the features of the previous set of classes, and maps features to a discriminative subspace in the feature space. As a result, the classifier is optimized jointly over new and old classes without requiring old class images. Experimental results show that our method achieves state-of-the-art classification accuracy in incremental learning benchmarks, while having at least an order of magnitude lower memory footprint compared to image-preserving strategies.



### Robust Single Rotation Averaging
- **Arxiv ID**: http://arxiv.org/abs/2004.00732v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.00732v4)
- **Published**: 2020-04-01 23:06:57+00:00
- **Updated**: 2020-11-05 00:17:44+00:00
- **Authors**: Seong Hun Lee, Javier Civera
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel method for single rotation averaging using the Weiszfeld algorithm. Our contribution is threefold: First, we propose a robust initialization based on the elementwise median of the input rotation matrices. Our initial solution is more accurate and robust than the commonly used chordal $L_2$-mean. Second, we propose an outlier rejection scheme that can be incorporated in the Weiszfeld algorithm to improve the robustness of $L_1$ rotation averaging. Third, we propose a method for approximating the chordal $L_1$-mean using the Weiszfeld algorithm. An extensive evaluation shows that both our method and the state of the art perform equally well with the proposed outlier rejection scheme, but ours is $2-4$ times faster.



### Monocular Camera Localization in Prior LiDAR Maps with 2D-3D Line Correspondences
- **Arxiv ID**: http://arxiv.org/abs/2004.00740v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.00740v2)
- **Published**: 2020-04-01 23:36:30+00:00
- **Updated**: 2020-07-31 17:22:35+00:00
- **Authors**: Huai Yu, Weikun Zhen, Wen Yang, Ji Zhang, Sebastian Scherer
- **Comment**: accepted by IROS 2020
- **Journal**: None
- **Summary**: Light-weight camera localization in existing maps is essential for vision-based navigation. Currently, visual and visual-inertial odometry (VO\&VIO) techniques are well-developed for state estimation but with inevitable accumulated drifts and pose jumps upon loop closure. To overcome these problems, we propose an efficient monocular camera localization method in prior LiDAR maps using direct 2D-3D line correspondences. To handle the appearance differences and modality gaps between LiDAR point clouds and images, geometric 3D lines are extracted offline from LiDAR maps while robust 2D lines are extracted online from video sequences. With the pose prediction from VIO, we can efficiently obtain coarse 2D-3D line correspondences. Then the camera poses and 2D-3D correspondences are iteratively optimized by minimizing the projection error of correspondences and rejecting outliers. Experimental results on the EurocMav dataset and our collected dataset demonstrate that the proposed method can efficiently estimate camera poses without accumulated drifts or pose jumps in structured environments.



