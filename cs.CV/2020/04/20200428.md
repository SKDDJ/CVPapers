# Arxiv Papers in cs.CV on 2020-04-28
### Inferring Temporal Compositions of Actions Using Probabilistic Automata
- **Arxiv ID**: http://arxiv.org/abs/2004.13217v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.13217v1)
- **Published**: 2020-04-28 00:15:26+00:00
- **Updated**: 2020-04-28 00:15:26+00:00
- **Authors**: Rodrigo Santa Cruz, Anoop Cherian, Basura Fernando, Dylan Campbell, Stephen Gould
- **Comment**: Accepted in Workshop on Compositionality in Computer Vision at CVPR,
  2020
- **Journal**: None
- **Summary**: This paper presents a framework to recognize temporal compositions of atomic actions in videos. Specifically, we propose to express temporal compositions of actions as semantic regular expressions and derive an inference framework using probabilistic automata to recognize complex actions as satisfying these expressions on the input video features. Our approach is different from existing works that either predict long-range complex activities as unordered sets of atomic actions, or retrieve videos using natural language sentences. Instead, the proposed approach allows recognizing complex fine-grained activities using only pretrained action classifiers, without requiring any additional data, annotations or neural network training. To evaluate the potential of our approach, we provide experiments on synthetic datasets and challenging real action recognition datasets, such as MultiTHUMOS and Charades. We conclude that the proposed approach can extend state-of-the-art primitive action classifiers to vastly more complex activities without large performance degradation.



### Deep Auto-Encoders with Sequential Learning for Multimodal Dimensional Emotion Recognition
- **Arxiv ID**: http://arxiv.org/abs/2004.13236v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.13236v1)
- **Published**: 2020-04-28 01:25:00+00:00
- **Updated**: 2020-04-28 01:25:00+00:00
- **Authors**: Dung Nguyen, Duc Thanh Nguyen, Rui Zeng, Thanh Thi Nguyen, Son N. Tran, Thin Nguyen, Sridha Sridharan, Clinton Fookes
- **Comment**: Under Review on Transaction on Multimedia
- **Journal**: None
- **Summary**: Multimodal dimensional emotion recognition has drawn a great attention from the affective computing community and numerous schemes have been extensively investigated, making a significant progress in this area. However, several questions still remain unanswered for most of existing approaches including: (i) how to simultaneously learn compact yet representative features from multimodal data, (ii) how to effectively capture complementary features from multimodal streams, and (iii) how to perform all the tasks in an end-to-end manner. To address these challenges, in this paper, we propose a novel deep neural network architecture consisting of a two-stream auto-encoder and a long short term memory for effectively integrating visual and audio signal streams for emotion recognition. To validate the robustness of our proposed architecture, we carry out extensive experiments on the multimodal emotion in the wild dataset: RECOLA. Experimental results show that the proposed method achieves state-of-the-art recognition performance and surpasses existing schemes by a significant margin.



### Attacks on Image Encryption Schemes for Privacy-Preserving Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2004.13263v2
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.13263v2)
- **Published**: 2020-04-28 03:34:01+00:00
- **Updated**: 2020-04-29 06:22:13+00:00
- **Authors**: Alex Habeen Chang, Benjamin M. Case
- **Comment**: For associated code, see
  https://github.com/ahchang98/image-encryption-scheme-attacks
- **Journal**: None
- **Summary**: Privacy preserving machine learning is an active area of research usually relying on techniques such as homomorphic encryption or secure multiparty computation. Recent novel encryption techniques for performing machine learning using deep neural nets on images have recently been proposed by Tanaka and Sirichotedumrong, Kinoshita, and Kiya. We present new chosen-plaintext and ciphertext-only attacks against both of these proposed image encryption schemes and demonstrate the attacks' effectiveness on several examples.



### Trainable Activation Function in Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2004.13271v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2004.13271v2)
- **Published**: 2020-04-28 03:50:53+00:00
- **Updated**: 2020-06-05 09:05:35+00:00
- **Authors**: Zhaohe Liao
- **Comment**: None
- **Journal**: None
- **Summary**: In the current research of neural networks, the activation function is manually specified by human and not able to change themselves during training. This paper focus on how to make the activation function trainable for deep neural networks. We use series and linear combination of different activation functions make activation functions continuously variable. Also, we test the performance of CNNs with Fourier series simulated activation(Fourier-CNN) and CNNs with linear combined activation function (LC-CNN) on Cifar-10 dataset. The result shows our trainable activation function reveals better performance than the most used ReLU activation function. Finally, we improves the performance of Fourier-CNN with Autoencoder, and test the performance of PSO algorithm in optimizing the parameters of networks



### VD-BERT: A Unified Vision and Dialog Transformer with BERT
- **Arxiv ID**: http://arxiv.org/abs/2004.13278v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2004.13278v3)
- **Published**: 2020-04-28 04:08:46+00:00
- **Updated**: 2020-11-02 09:07:41+00:00
- **Authors**: Yue Wang, Shafiq Joty, Michael R. Lyu, Irwin King, Caiming Xiong, Steven C. H. Hoi
- **Comment**: EMNLP 2020 (14 pages)
- **Journal**: None
- **Summary**: Visual dialog is a challenging vision-language task, where a dialog agent needs to answer a series of questions through reasoning on the image content and dialog history. Prior work has mostly focused on various attention mechanisms to model such intricate interactions. By contrast, in this work, we propose VD-BERT, a simple yet effective framework of unified vision-dialog Transformer that leverages the pretrained BERT language models for Visual Dialog tasks. The model is unified in that (1) it captures all the interactions between the image and the multi-turn dialog using a single-stream Transformer encoder, and (2) it supports both answer ranking and answer generation seamlessly through the same architecture. More crucially, we adapt BERT for the effective fusion of vision and dialog contents via visually grounded training. Without the need of pretraining on external vision-language data, our model yields new state of the art, achieving the top position in both single-model and ensemble settings (74.54 and 75.35 NDCG scores) on the visual dialog leaderboard. Our code and pretrained models are released at https://github.com/salesforce/VD-BERT.



### Neural Hair Rendering
- **Arxiv ID**: http://arxiv.org/abs/2004.13297v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.13297v2)
- **Published**: 2020-04-28 04:36:49+00:00
- **Updated**: 2020-07-21 19:29:30+00:00
- **Authors**: Menglei Chai, Jian Ren, Sergey Tulyakov
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: In this paper, we propose a generic neural-based hair rendering pipeline that can synthesize photo-realistic images from virtual 3D hair models. Unlike existing supervised translation methods that require model-level similarity to preserve consistent structure representation for both real images and fake renderings, our method adopts an unsupervised solution to work on arbitrary hair models. The key component of our method is a shared latent space to encode appearance-invariant structure information of both domains, which generates realistic renderings conditioned by extra appearance inputs. This is achieved by domain-specific pre-disentangled structure representation, partially shared domain encoder layers and a structure discriminator. We also propose a simple yet effective temporal conditioning method to enforce consistency for video sequence generation. We demonstrate the superiority of our method by testing it on a large number of portraits and comparing it with alternative baselines and state-of-the-art unsupervised image translation methods.



### SCRDet++: Detecting Small, Cluttered and Rotated Objects via Instance-Level Feature Denoising and Rotation Loss Smoothing
- **Arxiv ID**: http://arxiv.org/abs/2004.13316v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.13316v2)
- **Published**: 2020-04-28 06:03:54+00:00
- **Updated**: 2022-04-28 07:24:19+00:00
- **Authors**: Xue Yang, Junchi Yan, Wenlong Liao, Xiaokang Yang, Jin Tang, Tao He
- **Comment**: 15 pages, 12 figures, 11 tables, accepted by TPAMI
- **Journal**: None
- **Summary**: Small and cluttered objects are common in real-world which are challenging for detection. The difficulty is further pronounced when the objects are rotated, as traditional detectors often routinely locate the objects in horizontal bounding box such that the region of interest is contaminated with background or nearby interleaved objects. In this paper, we first innovatively introduce the idea of denoising to object detection. Instance-level denoising on the feature map is performed to enhance the detection to small and cluttered objects. To handle the rotation variation, we also add a novel IoU constant factor to the smooth L1 loss to address the long standing boundary problem, which to our analysis, is mainly caused by the periodicity of angular (PoA) and exchangeability of edges (EoE). By combing these two features, our proposed detector is termed as SCRDet++. Extensive experiments are performed on large aerial images public datasets DOTA, DIOR, UCAS-AOD as well as natural image dataset COCO, scene text dataset ICDAR2015, small traffic light dataset BSTLD and our released S$^2$TLD by this paper. The results show the effectiveness of our approach. The released dataset S2TLD is made public available, which contains 5,786 images with 14,130 traffic light instances across five categories.



### Learning Feature Descriptors using Camera Pose Supervision
- **Arxiv ID**: http://arxiv.org/abs/2004.13324v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.13324v2)
- **Published**: 2020-04-28 06:35:27+00:00
- **Updated**: 2020-08-12 07:23:23+00:00
- **Authors**: Qianqian Wang, Xiaowei Zhou, Bharath Hariharan, Noah Snavely
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: Recent research on learned visual descriptors has shown promising improvements in correspondence estimation, a key component of many 3D vision tasks. However, existing descriptor learning frameworks typically require ground-truth correspondences between feature points for training, which are challenging to acquire at scale. In this paper we propose a novel weakly-supervised framework that can learn feature descriptors solely from relative camera poses between images. To do so, we devise both a new loss function that exploits the epipolar constraint given by camera poses, and a new model architecture that makes the whole pipeline differentiable and efficient. Because we no longer need pixel-level ground-truth correspondences, our framework opens up the possibility of training on much larger and more diverse datasets for better and unbiased descriptors. We call the resulting descriptors CAmera Pose Supervised, or CAPS, descriptors. Though trained with weak supervision, CAPS descriptors outperform even prior fully-supervised descriptors and achieve state-of-the-art performance on a variety of geometric tasks.



### Transferable Active Grasping and Real Embodied Dataset
- **Arxiv ID**: http://arxiv.org/abs/2004.13358v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.13358v1)
- **Published**: 2020-04-28 08:15:35+00:00
- **Updated**: 2020-04-28 08:15:35+00:00
- **Authors**: Xiangyu Chen, Zelin Ye, Jiankai Sun, Yuda Fan, Fang Hu, Chenxi Wang, Cewu Lu
- **Comment**: None
- **Journal**: None
- **Summary**: Grasping in cluttered scenes is challenging for robot vision systems, as detection accuracy can be hindered by partial occlusion of objects. We adopt a reinforcement learning (RL) framework and 3D vision architectures to search for feasible viewpoints for grasping by the use of hand-mounted RGB-D cameras. To overcome the disadvantages of photo-realistic environment simulation, we propose a large-scale dataset called Real Embodied Dataset (RED), which includes full-viewpoint real samples on the upper hemisphere with amodal annotation and enables a simulator that has real visual feedback. Based on this dataset, a practical 3-stage transferable active grasping pipeline is developed, that is adaptive to unseen clutter scenes. In our pipeline, we propose a novel mask-guided reward to overcome the sparse reward issue in grasping and ensure category-irrelevant behavior. The grasping pipeline and its possible variants are evaluated with extensive experiments both in simulation and on a real-world UR-5 robotic arm.



### Gradient-Induced Co-Saliency Detection
- **Arxiv ID**: http://arxiv.org/abs/2004.13364v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.13364v3)
- **Published**: 2020-04-28 08:40:55+00:00
- **Updated**: 2020-12-12 08:03:45+00:00
- **Authors**: Zhao Zhang, Wenda Jin, Jun Xu, Ming-Ming Cheng
- **Comment**: Accepted by ECCV 2020
- **Journal**: None
- **Summary**: Co-saliency detection (Co-SOD) aims to segment the common salient foreground in a group of relevant images. In this paper, inspired by human behavior, we propose a gradient-induced co-saliency detection (GICD) method. We first abstract a consensus representation for the grouped images in the embedding space; then, by comparing the single image with consensus representation, we utilize the feedback gradient information to induce more attention to the discriminative co-salient features. In addition, due to the lack of Co-SOD training data, we design a jigsaw training strategy, with which Co-SOD networks can be trained on general saliency datasets without extra pixel-level annotations. To evaluate the performance of Co-SOD methods on discovering the co-salient object among multiple foregrounds, we construct a challenging CoCA dataset, where each image contains at least one extraneous foreground along with the co-salient object. Experiments demonstrate that our GICD achieves state-of-the-art performance. Our codes and dataset are available at https://mmcheng.net/gicd/.



### SSIM-Based CTU-Level Joint Optimal Bit Allocation and Rate Distortion Optimization
- **Arxiv ID**: http://arxiv.org/abs/2004.13369v2
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.13369v2)
- **Published**: 2020-04-28 08:55:21+00:00
- **Updated**: 2021-04-03 05:46:29+00:00
- **Authors**: Yang Li, Xuanqin Mou
- **Comment**: An improved version of this manuscript has been accepted by IEEE
  Transactions on Broadcasting DOI 10.1109/TBC.2021.3068871. The project page
  is located at http://gr.xjtu.edu.cn/web/xqmou/sosr
- **Journal**: None
- **Summary**: Structural similarity (SSIM)-based distortion $D_\text{SSIM}$ is more consistent with human perception than the traditional mean squared error $D_\text{MSE}$. To achieve better video quality, many studies on optimal bit allocation (OBA) and rate-distortion optimization (RDO) used $D_\text{SSIM}$ as the distortion metric. However, many of them failed to optimize OBA and RDO jointly based on SSIM, thus causing a non-optimal R-$D_\text{SSIM}$ performance. This problem is due to the lack of an accurate R-$D_\text{SSIM}$ model that can be used uniformly in both OBA and RDO. To solve this problem, we propose a $D_\text{SSIM}$-$D_\text{MSE}$ model first. Based on this model, the complex R-$D_\text{SSIM}$ cost in RDO can be calculated as simpler R-$D_\text{MSE}$ cost with a new SSIM-related Lagrange multiplier. This not only reduces the computation burden of SSIM-based RDO, but also enables the R-$D_\text{SSIM}$ model to be uniformly used in OBA and RDO. Moreover, with the new SSIM-related Lagrange multiplier in hand, the joint relationship of R-$D_\text{SSIM}$-$\lambda_\text{SSIM}$ (the negative derivative of R-$D_\text{SSIM}$) can be built, based on which the R-$D_\text{SSIM}$ model parameters can be calculated accurately. With accurate and unified R-$D_\text{SSIM}$ model, SSIM-based OBA and SSIM-based RDO are unified together in our scheme, called SOSR. Compared with the HEVC reference encoder HM16.20, SOSR saves 4%, 10%, and 14% bitrate under the same SSIM in all-intra, hierarchical and non-hierarchical low-delay-B configurations, which is superior to other state-of-the-art schemes.



### 3D Solid Spherical Bispectrum CNNs for Biomedical Texture Analysis
- **Arxiv ID**: http://arxiv.org/abs/2004.13371v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.13371v2)
- **Published**: 2020-04-28 09:01:13+00:00
- **Updated**: 2020-06-02 11:21:48+00:00
- **Authors**: Valentin Oreiller, Vincent Andrearczyk, Julien Fageot, John O. Prior, Adrien Depeursinge
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Locally Rotation Invariant (LRI) operators have shown great potential in biomedical texture analysis where patterns appear at random positions and orientations. LRI operators can be obtained by computing the responses to the discrete rotation of local descriptors, such as Local Binary Patterns (LBP) or the Scale Invariant Feature Transform (SIFT). Other strategies achieve this invariance using Laplacian of Gaussian or steerable wavelets for instance, preventing the introduction of sampling errors during the discretization of the rotations. In this work, we obtain LRI operators via the local projection of the image on the spherical harmonics basis, followed by the computation of the bispectrum, which shares and extends the invariance properties of the spectrum. We investigate the benefits of using the bispectrum over the spectrum in the design of a LRI layer embedded in a shallow Convolutional Neural Network (CNN) for 3D image analysis. The performance of each design is evaluated on two datasets and compared against a standard 3D CNN. The first dataset is made of 3D volumes composed of synthetically generated rotated patterns, while the second contains malignant and benign pulmonary nodules in Computed Tomography (CT) images. The results indicate that bispectrum CNNs allows for a significantly better characterization of 3D textures than both the spectral and standard CNN. In addition, it can efficiently learn with fewer training examples and trainable parameters when compared to a standard convolutional layer.



### Multi-Task Learning for Dense Prediction Tasks: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2004.13379v3
- **DOI**: 10.1109/TPAMI.2021.3054719
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.13379v3)
- **Published**: 2020-04-28 09:15:50+00:00
- **Updated**: 2021-01-24 18:56:09+00:00
- **Authors**: Simon Vandenhende, Stamatios Georgoulis, Wouter Van Gansbeke, Marc Proesmans, Dengxin Dai, Luc Van Gool
- **Comment**: Accepted to T-PAMI. Code + Suppl. Mat. can be found here:
  https://github.com/SimonVandenhende/Multi-Task-Learning-PyTorch IEEE
  Copyright Notice
- **Journal**: None
- **Summary**: With the advent of deep learning, many dense prediction tasks, i.e. tasks that produce pixel-level predictions, have seen significant performance improvements. The typical approach is to learn these tasks in isolation, that is, a separate neural network is trained for each individual task. Yet, recent multi-task learning (MTL) techniques have shown promising results w.r.t. performance, computations and/or memory footprint, by jointly tackling multiple tasks through a learned shared representation. In this survey, we provide a well-rounded view on state-of-the-art deep learning approaches for MTL in computer vision, explicitly emphasizing on dense prediction tasks. Our contributions concern the following. First, we consider MTL from a network architecture point-of-view. We include an extensive overview and discuss the advantages/disadvantages of recent popular MTL models. Second, we examine various optimization methods to tackle the joint learning of multiple tasks. We summarize the qualitative elements of these works and explore their commonalities and differences. Finally, we provide an extensive experimental evaluation across a variety of dense prediction benchmarks to examine the pros and cons of the different methods, including both architectural and optimization based strategies.



### The Immersion of Directed Multi-graphs in Embedding Fields. Generalisations
- **Arxiv ID**: http://arxiv.org/abs/2004.13384v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.13384v1)
- **Published**: 2020-04-28 09:28:08+00:00
- **Updated**: 2020-04-28 09:28:08+00:00
- **Authors**: Bogdan Bocse, Ioan Radu Jinga
- **Comment**: None
- **Journal**: None
- **Summary**: The purpose of this paper is to outline a generalised model for representing hybrids of relational-categorical, symbolic, perceptual-sensory and perceptual-latent data, so as to embody, in the same architectural data layer, representations for the input, output and latent tensors. This variety of representation is currently used by various machine-learning models in computer vision, NLP/NLU, reinforcement learning which allows for direct application of cross-domain queries and functions. This is achieved by endowing a directed Tensor-Typed Multi-Graph with at least some edge attributes which represent the embeddings from various latent spaces, so as to define, construct and compute new similarity and distance relationships between and across tensorial forms, including visual, linguistic, auditory latent representations, thus stitching the logical-categorical view of the observed universe to the Bayesian/statistical view.



### Multi-Scale Boosted Dehazing Network with Dense Feature Fusion
- **Arxiv ID**: http://arxiv.org/abs/2004.13388v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.13388v1)
- **Published**: 2020-04-28 09:34:47+00:00
- **Updated**: 2020-04-28 09:34:47+00:00
- **Authors**: Hang Dong, Jinshan Pan, Lei Xiang, Zhe Hu, Xinyi Zhang, Fei Wang, Ming-Hsuan Yang
- **Comment**: Accepted by CVPR 2020. The code are available at
  https://github.com/BookerDeWitt/MSBDN-DFF
- **Journal**: None
- **Summary**: In this paper, we propose a Multi-Scale Boosted Dehazing Network with Dense Feature Fusion based on the U-Net architecture. The proposed method is designed based on two principles, boosting and error feedback, and we show that they are suitable for the dehazing problem. By incorporating the Strengthen-Operate-Subtract boosting strategy in the decoder of the proposed model, we develop a simple yet effective boosted decoder to progressively restore the haze-free image. To address the issue of preserving spatial information in the U-Net architecture, we design a dense feature fusion module using the back-projection feedback scheme. We show that the dense feature fusion module can simultaneously remedy the missing spatial information from high-resolution features and exploit the non-adjacent features. Extensive evaluations demonstrate that the proposed model performs favorably against the state-of-the-art approaches on the benchmark datasets as well as real-world hazy images.



### Identification of Cervical Pathology using Adversarial Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2004.13406v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.13406v1)
- **Published**: 2020-04-28 10:22:16+00:00
- **Updated**: 2020-04-28 10:22:16+00:00
- **Authors**: Abhilash Nandy, Rachana Sathish, Debdoot Sheet
- **Comment**: 9 pages, 10 images, 5th MedImage Workshop of 11th Indian Conference
  on Computer Vision, Graphics and Image Processing, Hyderabad, India, 2018
- **Journal**: None
- **Summary**: Various screening and diagnostic methods have led to a large reduction of cervical cancer death rates in developed countries. However, cervical cancer is the leading cause of cancer related deaths in women in India and other low and middle income countries (LMICs) especially among the urban poor and slum dwellers. Several sophisticated techniques such as cytology tests, HPV tests etc. have been widely used for screening of cervical cancer. These tests are inherently time consuming. In this paper, we propose a convolutional autoencoder based framework, having an architecture similar to SegNet which is trained in an adversarial fashion for classifying images of the cervix acquired using a colposcope. We validate performance on the Intel-Mobile ODT cervical image classification dataset. The proposed method outperforms the standard technique of fine-tuning convolutional neural networks pre-trained on ImageNet database with an average accuracy of 73.75%.



### Real-Time Apple Detection System Using Embedded Systems With Hardware Accelerators: An Edge AI Application
- **Arxiv ID**: http://arxiv.org/abs/2004.13410v1
- **DOI**: 10.1109/ACCESS.2020.2964608
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.13410v1)
- **Published**: 2020-04-28 10:40:01+00:00
- **Updated**: 2020-04-28 10:40:01+00:00
- **Authors**: Vittorio Mazzia, Francesco Salvetti, Aleem Khaliq, Marcello Chiaberge
- **Comment**: None
- **Journal**: IEEE Access, vol. 8, pp. 9102-9114, 2020
- **Summary**: Real-time apple detection in orchards is one of the most effective ways of estimating apple yields, which helps in managing apple supplies more effectively. Traditional detection methods used highly computational machine learning algorithms with intensive hardware set up, which are not suitable for infield real-time apple detection due to their weight and power constraints. In this study, a real-time embedded solution inspired from "Edge AI" is proposed for apple detection with the implementation of YOLOv3-tiny algorithm on various embedded platforms such as Raspberry Pi 3 B+ in combination with Intel Movidius Neural Computing Stick (NCS), Nvidia's Jetson Nano and Jetson AGX Xavier. Data set for training were compiled using acquired images during field survey of apple orchard situated in the north region of Italy, and images used for testing were taken from widely used google data set by filtering out the images containing apples in different scenes to ensure the robustness of the algorithm. The proposed study adapts YOLOv3-tiny architecture to detect small objects. It shows the feasibility of deployment of the customized model on cheap and power-efficient embedded hardware without compromising mean average detection accuracy (83.64%) and achieved frame rate up to 30 fps even for the difficult scenarios such as overlapping apples, complex background, less exposure of apple due to leaves and branches. Furthermore, the proposed embedded solution can be deployed on the unmanned ground vehicles to detect, count, and measure the size of the apples in real-time to help the farmers and agronomists in their decision making and management skills.



### Angle-based Search Space Shrinking for Neural Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/2004.13431v3
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.13431v3)
- **Published**: 2020-04-28 11:26:46+00:00
- **Updated**: 2020-07-16 14:45:22+00:00
- **Authors**: Yiming Hu, Yuding Liang, Zichao Guo, Ruosi Wan, Xiangyu Zhang, Yichen Wei, Qingyi Gu, Jian Sun
- **Comment**: Accepted in ECCV 2020
- **Journal**: None
- **Summary**: In this work, we present a simple and general search space shrinking method, called Angle-Based search space Shrinking (ABS), for Neural Architecture Search (NAS). Our approach progressively simplifies the original search space by dropping unpromising candidates, thus can reduce difficulties for existing NAS methods to find superior architectures. In particular, we propose an angle-based metric to guide the shrinking process. We provide comprehensive evidences showing that, in weight-sharing supernet, the proposed metric is more stable and accurate than accuracy-based and magnitude-based metrics to predict the capability of child models. We also show that the angle-based metric can converge fast while training supernet, enabling us to get promising shrunk search spaces efficiently. ABS can easily apply to most of NAS approaches (e.g. SPOS, FairNAS, ProxylessNAS, DARTS and PDARTS). Comprehensive experiments show that ABS can dramatically enhance existing NAS approaches by providing a promising shrunk search space.



### Leveraging Photometric Consistency over Time for Sparsely Supervised Hand-Object Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2004.13449v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.13449v1)
- **Published**: 2020-04-28 12:03:14+00:00
- **Updated**: 2020-04-28 12:03:14+00:00
- **Authors**: Yana Hasson, Bugra Tekin, Federica Bogo, Ivan Laptev, Marc Pollefeys, Cordelia Schmid
- **Comment**: CVPR 2020. See the project webpage at
  https://hassony2.github.io/handobjectconsist.html
- **Journal**: None
- **Summary**: Modeling hand-object manipulations is essential for understanding how humans interact with their environment. While of practical importance, estimating the pose of hands and objects during interactions is challenging due to the large mutual occlusions that occur during manipulation. Recent efforts have been directed towards fully-supervised methods that require large amounts of labeled training samples. Collecting 3D ground-truth data for hand-object interactions, however, is costly, tedious, and error-prone. To overcome this challenge we present a method to leverage photometric consistency across time when annotations are only available for a sparse subset of frames in a video. Our model is trained end-to-end on color images to jointly reconstruct hands and objects in 3D by inferring their poses. Given our estimated reconstructions, we differentiably render the optical flow between pairs of adjacent images and use it within the network to warp one frame to another. We then apply a self-supervised photometric loss that relies on the visual consistency between nearby images. We achieve state-of-the-art results on 3D hand-object reconstruction benchmarks and demonstrate that our approach allows us to improve the pose estimation accuracy by leveraging information from neighboring frames in low-data regimes.



### DRU-net: An Efficient Deep Convolutional Neural Network for Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2004.13453v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.13453v1)
- **Published**: 2020-04-28 12:16:24+00:00
- **Updated**: 2020-04-28 12:16:24+00:00
- **Authors**: Mina Jafari, Dorothee Auer, Susan Francis, Jonathan Garibaldi, Xin Chen
- **Comment**: Accepted for publication at IEEE International Symposium on
  Biomedical Imaging (ISBI) 2020, 5 pages, 3 figures
- **Journal**: 2020 IEEE 16th International Symposium on Biomedical Imaging (ISBI
  2020)
- **Summary**: Residual network (ResNet) and densely connected network (DenseNet) have significantly improved the training efficiency and performance of deep convolutional neural networks (DCNNs) mainly for object classification tasks. In this paper, we propose an efficient network architecture by considering advantages of both networks. The proposed method is integrated into an encoder-decoder DCNN model for medical image segmentation. Our method adds additional skip connections compared to ResNet but uses significantly fewer model parameters than DenseNet. We evaluate the proposed method on a public dataset (ISIC 2018 grand-challenge) for skin lesion segmentation and a local brain MRI dataset. In comparison with ResNet-based, DenseNet-based and attention network (AttnNet) based methods within the same encoder-decoder network structure, our method achieves significantly higher segmentation accuracy with fewer number of model parameters than DenseNet and AttnNet. The code is available on GitHub (GitHub link: https://github.com/MinaJf/DRU-net).



### DiVA: Diverse Visual Feature Aggregation for Deep Metric Learning
- **Arxiv ID**: http://arxiv.org/abs/2004.13458v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.13458v4)
- **Published**: 2020-04-28 12:26:50+00:00
- **Updated**: 2020-09-10 16:19:05+00:00
- **Authors**: Timo Milbich, Karsten Roth, Homanga Bharadhwaj, Samarth Sinha, Yoshua Bengio, Björn Ommer, Joseph Paul Cohen
- **Comment**: published at ECCV 2020
- **Journal**: None
- **Summary**: Visual Similarity plays an important role in many computer vision applications. Deep metric learning (DML) is a powerful framework for learning such similarities which not only generalize from training data to identically distributed test distributions, but in particular also translate to unknown test classes. However, its prevailing learning paradigm is class-discriminative supervised training, which typically results in representations specialized in separating training classes. For effective generalization, however, such an image representation needs to capture a diverse range of data characteristics. To this end, we propose and study multiple complementary learning tasks, targeting conceptually different data relationships by only resorting to the available training samples and labels of a standard DML setting. Through simultaneous optimization of our tasks we learn a single model to aggregate their training signals, resulting in strong generalization and state-of-the-art performance on multiple established DML benchmark datasets.



### FU-net: Multi-class Image Segmentation Using Feedback Weighted U-net
- **Arxiv ID**: http://arxiv.org/abs/2004.13470v1
- **DOI**: 10.1007/978-3-030-34110-7_44
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.13470v1)
- **Published**: 2020-04-28 13:08:14+00:00
- **Updated**: 2020-04-28 13:08:14+00:00
- **Authors**: Mina Jafari, Ruizhe Li, Yue Xing, Dorothee Auer, Susan Francis, Jonathan Garibaldi, Xin Chen
- **Comment**: Accepted for publication at International Conference on Image and
  Graphics (ICIG 2019)
- **Journal**: The 10th International Conference on Image and Graphics (ICIG
  2019)
- **Summary**: In this paper, we present a generic deep convolutional neural network (DCNN) for multi-class image segmentation. It is based on a well-established supervised end-to-end DCNN model, known as U-net. U-net is firstly modified by adding widely used batch normalization and residual block (named as BRU-net) to improve the efficiency of model training. Based on BRU-net, we further introduce a dynamically weighted cross-entropy loss function. The weighting scheme is calculated based on the pixel-wise prediction accuracy during the training process. Assigning higher weights to pixels with lower segmentation accuracies enables the network to learn more from poorly predicted image regions. Our method is named as feedback weighted U-net (FU-net). We have evaluated our method based on T1- weighted brain MRI for the segmentation of midbrain and substantia nigra, where the number of pixels in each class is extremely unbalanced to each other. Based on the dice coefficient measurement, our proposed FU-net has outperformed BRU-net and U-net with statistical significance, especially when only a small number of training examples are available. The code is publicly available in GitHub (GitHub link: https://github.com/MinaJf/FU-net).



### PODNet: Pooled Outputs Distillation for Small-Tasks Incremental Learning
- **Arxiv ID**: http://arxiv.org/abs/2004.13513v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.13513v3)
- **Published**: 2020-04-28 13:45:23+00:00
- **Updated**: 2020-10-06 16:10:33+00:00
- **Authors**: Arthur Douillard, Matthieu Cord, Charles Ollion, Thomas Robert, Eduardo Valle
- **Comment**: Accepted at ECCV 2020
- **Journal**: None
- **Summary**: Lifelong learning has attracted much attention, but existing works still struggle to fight catastrophic forgetting and accumulate knowledge over long stretches of incremental learning. In this work, we propose PODNet, a model inspired by representation learning. By carefully balancing the compromise between remembering the old classes and learning new ones, PODNet fights catastrophic forgetting, even over very long runs of small incremental tasks --a setting so far unexplored by current works. PODNet innovates on existing art with an efficient spatial-based distillation-loss applied throughout the model and a representation comprising multiple proxy vectors for each class. We validate those innovations thoroughly, comparing PODNet with three state-of-the-art models on three datasets: CIFAR100, ImageNet100, and ImageNet1000. Our results showcase a significant advantage of PODNet over existing art, with accuracy gains of 12.10, 6.51, and 2.85 percentage points, respectively. Code is available at https://github.com/arthurdouillard/incremental_learning.pytorch



### Addressing Artificial Intelligence Bias in Retinal Disease Diagnostics
- **Arxiv ID**: http://arxiv.org/abs/2004.13515v4
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.13515v4)
- **Published**: 2020-04-28 13:46:54+00:00
- **Updated**: 2020-12-02 15:11:18+00:00
- **Authors**: Philippe Burlina, Neil Joshi, William Paul, Katia D. Pacheco, Neil M. Bressler
- **Comment**: Accepted for publication at journal of Translational Vision Science
  and Technology
- **Journal**: None
- **Summary**: This study evaluated generative methods to potentially mitigate AI bias when diagnosing diabetic retinopathy (DR) resulting from training data imbalance, or domain generalization which occurs when deep learning systems (DLS) face concepts at test/inference time they were not initially trained on. The public domain Kaggle-EyePACS dataset (88,692 fundi and 44,346 individuals, originally diverse for ethnicity) was modified by adding clinician-annotated labels and constructing an artificial scenario of data imbalance and domain generalization by disallowing training (but not testing) exemplars for images of retinas with DR warranting referral (DR-referable) and from darker-skin individuals, who presumably have greater concentration of melanin within uveal melanocytes, on average, contributing to retinal image pigmentation. A traditional/baseline diagnostic DLS was compared against new DLSs that would use training data augmented via generative models for debiasing. Accuracy (95% confidence intervals [CI]) of the baseline diagnostics DLS for fundus images of lighter-skin individuals was 73.0% (66.9%, 79.2%) vs. darker-skin of 60.5% (53.5%, 67.3%), demonstrating bias/disparity (delta=12.5%) (Welch t-test t=2.670, P=.008) in AI performance across protected subpopulations. Using novel generative methods for addressing missing subpopulation training data (DR-referable darker-skin) achieved instead accuracy, for lighter-skin, of 72.0% (65.8%, 78.2%), and for darker-skin, of 71.5% (65.2%,77.8%), demonstrating closer parity (delta=0.5%) in accuracy across subpopulations (Welch t-test t=0.111, P=.912). Findings illustrate how data imbalance and domain generalization can lead to disparity of accuracy across subpopulations, and show that novel generative methods of synthetic fundus images may play a role for debiasing AI.



### Multi-task Ensembles with Crowdsourced Features Improve Skin Lesion Diagnosis
- **Arxiv ID**: http://arxiv.org/abs/2004.14745v2
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.14745v2)
- **Published**: 2020-04-28 13:48:40+00:00
- **Updated**: 2020-07-06 18:12:41+00:00
- **Authors**: Ralf Raumanns, Elif K Contar, Gerard Schouten, Veronika Cheplygina
- **Comment**: None
- **Journal**: None
- **Summary**: Machine learning has a recognised need for large amounts of annotated data. Due to the high cost of expert annotations, crowdsourcing, where non-experts are asked to label or outline images, has been proposed as an alternative. Although many promising results are reported, the quality of diagnostic crowdsourced labels is still unclear. We propose to address this by instead asking the crowd about visual features of the images, which can be provided more intuitively, and by using these features in a multi-task learning framework through ensemble strategies. We compare our proposed approach to a baseline model with a set of 2000 skin lesions from the ISIC 2017 challenge dataset. The baseline model only predicts a binary label from the skin lesion image, while our multi-task model also predicts one of the following features: asymmetry of the lesion, border irregularity and color. We show that multi-task models with individual crowdsourced features have limited effect on the model, but when combined in an ensembles, leads to improved generalisation. The area under the receiver operating characteristic curve is 0.794 for the baseline model and 0.811 and 0.808 for multi-task ensembles respectively. Finally, we discuss the findings, identify some limitations and recommend directions for further research. The code of the models is available at https://github.com/raumannsr/hints_crowd.



### Multivariate Confidence Calibration for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2004.13546v1
- **DOI**: 10.1109/CVPRW50498.2020.00171
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.13546v1)
- **Published**: 2020-04-28 14:17:41+00:00
- **Updated**: 2020-04-28 14:17:41+00:00
- **Authors**: Fabian Küppers, Jan Kronenberger, Amirhossein Shantia, Anselm Haselhoff
- **Comment**: Accepted on CVPR 2020 Workshop: "2nd Workshop on Safe Artificial
  Intelligence for Automated Driving (SAIAD)"
- **Journal**: None
- **Summary**: Unbiased confidence estimates of neural networks are crucial especially for safety-critical applications. Many methods have been developed to calibrate biased confidence estimates. Though there is a variety of methods for classification, the field of object detection has not been addressed yet. Therefore, we present a novel framework to measure and calibrate biased (or miscalibrated) confidence estimates of object detection methods. The main difference to related work in the field of classifier calibration is that we also use additional information of the regression output of an object detector for calibration. Our approach allows, for the first time, to obtain calibrated confidence estimates with respect to image location and box scale. In addition, we propose a new measure to evaluate miscalibration of object detectors. Finally, we show that our developed methods outperform state-of-the-art calibration models for the task of object detection and provides reliable confidence estimates across different locations and scales.



### A fast and memory-efficient algorithm for smooth interpolation of polyrigid transformations: application to human joint tracking
- **Arxiv ID**: http://arxiv.org/abs/2005.02159v3
- **DOI**: None
- **Categories**: **cs.CV**, math.GR
- **Links**: [PDF](http://arxiv.org/pdf/2005.02159v3)
- **Published**: 2020-04-28 14:30:40+00:00
- **Updated**: 2020-06-08 18:51:57+00:00
- **Authors**: K. Makki, B. Borotikar, M. Garetier, S. Brochard, D. Ben Salem, F. Rousseau
- **Comment**: None
- **Journal**: None
- **Summary**: The log Euclidean polyrigid registration framework provides a way to smoothly estimate and interpolate poly-rigid/affine transformations for which the invertibility is guaranteed. This powerful and flexible mathematical framework is currently being used to track the human joint dynamics by first imposing bone rigidity constraints in order to synthetize the spatio-temporal joint deformations later. However, since no closed-form exists, then a computationally expensive integration of ordinary differential equations (ODEs) is required to perform image registration using this framework. To tackle this problem, the exponential map for solving these ODEs is computed using the scaling and squaring method in the literature. In this paper, we propose an algorithm using a matrix diagonalization based method for smooth interpolation of homogeneous polyrigid transformations of human joints during motion. The use of this alternative computational approach to integrate ODEs is well motivated by the fact that bone rigid transformations satisfy the mechanical constraints of human joint motion, which provide conditions that guarantee the diagonalizability of local bone transformations and consequently of the resulting joint transformations. In a comparison with the scaling and squaring method, we discuss the usefulness of the matrix eigendecomposition technique which reduces significantly the computational burden associated with the computation of matrix exponential over a dense regular grid. Finally, we have applied the method to enhance the temporal resolution of dynamic MRI sequences of the ankle joint. To conclude, numerical experiments show that the eigendecomposition method is more capable of balancing the trade-off between accuracy, computation time, and memory requirements.



### Hybrid Attention for Automatic Segmentation of Whole Fetal Head in Prenatal Ultrasound Volumes
- **Arxiv ID**: http://arxiv.org/abs/2004.13567v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.13567v1)
- **Published**: 2020-04-28 14:43:05+00:00
- **Updated**: 2020-04-28 14:43:05+00:00
- **Authors**: Xin Yang, Xu Wang, Yi Wang, Haoran Dou, Shengli Li, Huaxuan Wen, Yi Lin, Pheng-Ann Heng, Dong Ni
- **Comment**: Accepted by Computer Methods and Programs in Biomedicine
- **Journal**: None
- **Summary**: Background and Objective: Biometric measurements of fetal head are important indicators for maternal and fetal health monitoring during pregnancy. 3D ultrasound (US) has unique advantages over 2D scan in covering the whole fetal head and may promote the diagnoses. However, automatically segmenting the whole fetal head in US volumes still pends as an emerging and unsolved problem. The challenges that automated solutions need to tackle include the poor image quality, boundary ambiguity, long-span occlusion, and the appearance variability across different fetal poses and gestational ages. In this paper, we propose the first fully-automated solution to segment the whole fetal head in US volumes.   Methods: The segmentation task is firstly formulated as an end-to-end volumetric mapping under an encoder-decoder deep architecture. We then combine the segmentor with a proposed hybrid attention scheme (HAS) to select discriminative features and suppress the non-informative volumetric features in a composite and hierarchical way. With little computation overhead, HAS proves to be effective in addressing boundary ambiguity and deficiency. To enhance the spatial consistency in segmentation, we further organize multiple segmentors in a cascaded fashion to refine the results by revisiting context in the prediction of predecessors.   Results: Validated on a large dataset collected from 100 healthy volunteers, our method presents superior segmentation performance (DSC (Dice Similarity Coefficient), 96.05%), remarkable agreements with experts. With another 156 volumes collected from 52 volunteers, we ahieve high reproducibilities (mean standard deviation 11.524 mL) against scan variations.   Conclusion: This is the first investigation about whole fetal head segmentation in 3D US. Our method is promising to be a feasible solution in assisting the volumetric US-based prenatal studies.



### Unifying Neural Learning and Symbolic Reasoning for Spinal Medical Report Generation
- **Arxiv ID**: http://arxiv.org/abs/2004.13577v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.13577v1)
- **Published**: 2020-04-28 15:06:24+00:00
- **Updated**: 2020-04-28 15:06:24+00:00
- **Authors**: Zhongyi Han, Benzheng Wei, Yilong Yin, Shuo Li
- **Comment**: Under review
- **Journal**: None
- **Summary**: Automated medical report generation in spine radiology, i.e., given spinal medical images and directly create radiologist-level diagnosis reports to support clinical decision making, is a novel yet fundamental study in the domain of artificial intelligence in healthcare. However, it is incredibly challenging because it is an extremely complicated task that involves visual perception and high-level reasoning processes. In this paper, we propose the neural-symbolic learning (NSL) framework that performs human-like learning by unifying deep neural learning and symbolic logical reasoning for the spinal medical report generation. Generally speaking, the NSL framework firstly employs deep neural learning to imitate human visual perception for detecting abnormalities of target spinal structures. Concretely, we design an adversarial graph network that interpolates a symbolic graph reasoning module into a generative adversarial network through embedding prior domain knowledge, achieving semantic segmentation of spinal structures with high complexity and variability. NSL secondly conducts human-like symbolic logical reasoning that realizes unsupervised causal effect analysis of detected entities of abnormalities through meta-interpretive learning. NSL finally fills these discoveries of target diseases into a unified template, successfully achieving a comprehensive medical report generation. When it employed in a real-world clinical dataset, a series of empirical studies demonstrate its capacity on spinal medical report generation as well as show that our algorithm remarkably exceeds existing methods in the detection of spinal structures. These indicate its potential as a clinical tool that contributes to computer-aided diagnosis.



### Do We Need Fully Connected Output Layers in Convolutional Networks?
- **Arxiv ID**: http://arxiv.org/abs/2004.13587v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.13587v2)
- **Published**: 2020-04-28 15:21:44+00:00
- **Updated**: 2020-04-29 03:20:47+00:00
- **Authors**: Zhongchao Qian, Tyler L. Hayes, Kushal Kafle, Christopher Kanan
- **Comment**: None
- **Journal**: None
- **Summary**: Traditionally, deep convolutional neural networks consist of a series of convolutional and pooling layers followed by one or more fully connected (FC) layers to perform the final classification. While this design has been successful, for datasets with a large number of categories, the fully connected layers often account for a large percentage of the network's parameters. For applications with memory constraints, such as mobile devices and embedded platforms, this is not ideal. Recently, a family of architectures that involve replacing the learned fully connected output layer with a fixed layer has been proposed as a way to achieve better efficiency. In this paper we examine this idea further and demonstrate that fixed classifiers offer no additional benefit compared to simply removing the output layer along with its parameters. We further demonstrate that the typical approach of having a fully connected final output layer is inefficient in terms of parameter count. We are able to achieve comparable performance to a traditionally learned fully connected classification output layer on the ImageNet-1K, CIFAR-100, Stanford Cars-196, and Oxford Flowers-102 datasets, while not having a fully connected output layer at all.



### Exploring Self-attention for Image Recognition
- **Arxiv ID**: http://arxiv.org/abs/2004.13621v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.13621v1)
- **Published**: 2020-04-28 16:01:48+00:00
- **Updated**: 2020-04-28 16:01:48+00:00
- **Authors**: Hengshuang Zhao, Jiaya Jia, Vladlen Koltun
- **Comment**: CVPR 2020, Code available at https://github.com/hszhao/SAN
- **Journal**: None
- **Summary**: Recent work has shown that self-attention can serve as a basic building block for image recognition models. We explore variations of self-attention and assess their effectiveness for image recognition. We consider two forms of self-attention. One is pairwise self-attention, which generalizes standard dot-product attention and is fundamentally a set operator. The other is patchwise self-attention, which is strictly more powerful than convolution. Our pairwise self-attention networks match or outperform their convolutional counterparts, and the patchwise models substantially outperform the convolutional baselines. We also conduct experiments that probe the robustness of learned representations and conclude that self-attention networks may have significant benefits in terms of robustness and generalization.



### Image Augmentation Is All You Need: Regularizing Deep Reinforcement Learning from Pixels
- **Arxiv ID**: http://arxiv.org/abs/2004.13649v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.13649v4)
- **Published**: 2020-04-28 16:48:16+00:00
- **Updated**: 2021-03-07 16:37:37+00:00
- **Authors**: Ilya Kostrikov, Denis Yarats, Rob Fergus
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a simple data augmentation technique that can be applied to standard model-free reinforcement learning algorithms, enabling robust learning directly from pixels without the need for auxiliary losses or pre-training. The approach leverages input perturbations commonly used in computer vision tasks to regularize the value function. Existing model-free approaches, such as Soft Actor-Critic (SAC), are not able to train deep networks effectively from image pixels. However, the addition of our augmentation method dramatically improves SAC's performance, enabling it to reach state-of-the-art performance on the DeepMind control suite, surpassing model-based (Dreamer, PlaNet, and SLAC) methods and recently proposed contrastive learning (CURL). Our approach can be combined with any model-free reinforcement learning algorithm, requiring only minor modifications. An implementation can be found at https://sites.google.com/view/data-regularized-q.



### Event-based Robotic Grasping Detection with Neuromorphic Vision Sensor and Event-Stream Dataset
- **Arxiv ID**: http://arxiv.org/abs/2004.13652v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.13652v2)
- **Published**: 2020-04-28 16:55:19+00:00
- **Updated**: 2020-05-01 16:59:14+00:00
- **Authors**: Bin Li, Hu Cao, Zhongnan Qu, Yingbai Hu, Zhenke Wang, Zichen Liang
- **Comment**: submit to the Frontiers Neurorobotics
- **Journal**: None
- **Summary**: Robotic grasping plays an important role in the field of robotics. The current state-of-the-art robotic grasping detection systems are usually built on the conventional vision, such as RGB-D camera. Compared to traditional frame-based computer vision, neuromorphic vision is a small and young community of research. Currently, there are limited event-based datasets due to the troublesome annotation of the asynchronous event stream. Annotating large scale vision dataset often takes lots of computation resources, especially the troublesome data for video-level annotation. In this work, we consider the problem of detecting robotic grasps in a moving camera view of a scene containing objects. To obtain more agile robotic perception, a neuromorphic vision sensor (DAVIS) attaching to the robot gripper is introduced to explore the potential usage in grasping detection. We construct a robotic grasping dataset named Event-Stream Dataset with 91 objects. A spatio-temporal mixed particle filter (SMP Filter) is proposed to track the led-based grasp rectangles which enables video-level annotation of a single grasp rectangle per object. As leds blink at high frequency, the Event-Stream dataset is annotated in a high frequency of 1 kHz. Based on the Event-Stream dataset, we develop a deep neural network for grasping detection which consider the angle learning problem as classification instead of regression. The method performs high detection accuracy on our Event-Stream dataset with 93% precision at object-wise level. This work provides a large-scale and well-annotated dataset, and promotes the neuromorphic vision applications in agile robot.



### Visual Grounding of Learned Physical Models
- **Arxiv ID**: http://arxiv.org/abs/2004.13664v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.13664v2)
- **Published**: 2020-04-28 17:06:38+00:00
- **Updated**: 2020-06-29 15:13:21+00:00
- **Authors**: Yunzhu Li, Toru Lin, Kexin Yi, Daniel M. Bear, Daniel L. K. Yamins, Jiajun Wu, Joshua B. Tenenbaum, Antonio Torralba
- **Comment**: The second and the third authors contributed equally to this paper,
  and are listed in alphabetical order. Project Page:
  http://visual-physics-grounding.csail.mit.edu/
- **Journal**: None
- **Summary**: Humans intuitively recognize objects' physical properties and predict their motion, even when the objects are engaged in complicated interactions. The abilities to perform physical reasoning and to adapt to new environments, while intrinsic to humans, remain challenging to state-of-the-art computational models. In this work, we present a neural model that simultaneously reasons about physics and makes future predictions based on visual and dynamics priors. The visual prior predicts a particle-based representation of the system from visual observations. An inference module operates on those particles, predicting and refining estimates of particle locations, object states, and physical parameters, subject to the constraints imposed by the dynamics prior, which we refer to as visual grounding. We demonstrate the effectiveness of our method in environments involving rigid objects, deformable materials, and fluids. Experiments show that our model can infer the physical properties within a few observations, which allows the model to quickly adapt to unseen scenarios and make accurate predictions into the future.



### A novel Region of Interest Extraction Layer for Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2004.13665v2
- **DOI**: 10.1109/ICPR48806.2021.9412258
- **Categories**: **cs.CV**, I.4.6; I.5.1
- **Links**: [PDF](http://arxiv.org/pdf/2004.13665v2)
- **Published**: 2020-04-28 17:07:32+00:00
- **Updated**: 2020-10-01 14:12:03+00:00
- **Authors**: Leonardo Rossi, Akbar Karimi, Andrea Prati
- **Comment**: None
- **Journal**: International Conference on Pattern Recognition (ICPR). IEEE, 2021
- **Summary**: Given the wide diffusion of deep neural network architectures for computer vision tasks, several new applications are nowadays more and more feasible. Among them, a particular attention has been recently given to instance segmentation, by exploiting the results achievable by two-stage networks (such as Mask R-CNN or Faster R-CNN), derived from R-CNN. In these complex architectures, a crucial role is played by the Region of Interest (RoI) extraction layer, devoted to extracting a coherent subset of features from a single Feature Pyramid Network (FPN) layer attached on top of a backbone.   This paper is motivated by the need to overcome the limitations of existing RoI extractors which select only one (the best) layer from FPN. Our intuition is that all the layers of FPN retain useful information. Therefore, the proposed layer (called Generic RoI Extractor - GRoIE) introduces non-local building blocks and attention mechanisms to boost the performance.   A comprehensive ablation study at component level is conducted to find the best set of algorithms and parameters for the GRoIE layer. Moreover, GRoIE can be integrated seamlessly with every two-stage architecture for both object detection and instance segmentation tasks. Therefore, the improvements brought about by the use of GRoIE in different state-of-the-art architectures are also evaluated. The proposed layer leads up to gain a 1.1% AP improvement on bounding box detection and 1.7% AP improvement on instance segmentation.   The code is publicly available on GitHub repository at https://github.com/IMPLabUniPr/mmdetection/tree/groie_dev



### Psychophysical Evaluation of Deep Re-Identification Models
- **Arxiv ID**: http://arxiv.org/abs/2005.02136v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.02136v1)
- **Published**: 2020-04-28 17:11:50+00:00
- **Updated**: 2020-04-28 17:11:50+00:00
- **Authors**: Hamish Nicholson
- **Comment**: Submitted as a senior thesis at Harvard College
- **Journal**: None
- **Summary**: Pedestrian re-identification (ReID) is the task of continuously recognising the sameindividual across time and camera views. Researchers of pedestrian ReID and theirGPUs spend enormous energy producing novel algorithms, challenging datasets,and readily accessible tools to successfully improve results on standard metrics.Yet practitioners in biometrics, surveillance, and autonomous driving have not re-alized benefits that reflect these metrics. Different detections, slight occlusions,changes in perspective, and other banal perturbations render the best neural net-works virtually useless. This work makes two contributions. First, we introducethe ReID community to a budding area of computer vision research in model eval-uation. By adapting established principles of psychophysical evaluation from psy-chology, we can quantify the performance degradation and begin research thatwill improve the utility of pedestrian ReID models; not just their performance ontest sets. Second, we introduce NuscenesReID, a challenging new ReID datasetdesigned to reflect the real world autonomous vehicle conditions in which ReIDalgorithms are used. We show that, despite performing well on existing ReIDdatasets, most models are not robust to synthetic augmentations or to the morerealistic NuscenesReID data.



### Residual Channel Attention Generative Adversarial Network for Image Super-Resolution and Noise Reduction
- **Arxiv ID**: http://arxiv.org/abs/2004.13674v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.13674v1)
- **Published**: 2020-04-28 17:23:46+00:00
- **Updated**: 2020-04-28 17:23:46+00:00
- **Authors**: Jie Cai, Zibo Meng, Chiu Man Ho
- **Comment**: None
- **Journal**: None
- **Summary**: Image super-resolution is one of the important computer vision techniques aiming to reconstruct high-resolution images from corresponding low-resolution ones. Most recently, deep learning-based approaches have been demonstrated for image super-resolution. However, as the deep networks go deeper, they become more difficult to train and more difficult to restore the finer texture details, especially under real-world settings. In this paper, we propose a Residual Channel Attention-Generative Adversarial Network(RCA-GAN) to solve these problems. Specifically, a novel residual channel attention block is proposed to form RCA-GAN, which consists of a set of residual blocks with shortcut connections, and a channel attention mechanism to model the interdependence and interaction of the feature representations among different channels. Besides, a generative adversarial network (GAN) is employed to further produce realistic and highly detailed results. Benefiting from these improvements, the proposed RCA-GAN yields consistently better visual quality with more detailed and natural textures than baseline models; and achieves comparable or better performance compared with the state-of-the-art methods for real-world image super-resolution.



### Style-transfer GANs for bridging the domain gap in synthetic pose estimator training
- **Arxiv ID**: http://arxiv.org/abs/2004.13681v2
- **DOI**: 10.1109/AIVR50618.2020.00039
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.13681v2)
- **Published**: 2020-04-28 17:35:03+00:00
- **Updated**: 2020-12-16 19:06:13+00:00
- **Authors**: Pavel Rojtberg, Thomas Pöllabauer, Arjan Kuijper
- **Comment**: None
- **Journal**: None
- **Summary**: Given the dependency of current CNN architectures on a large training set, the possibility of using synthetic data is alluring as it allows generating a virtually infinite amount of labeled training data. However, producing such data is a non-trivial task as current CNN architectures are sensitive to the domain gap between real and synthetic data. We propose to adopt general-purpose GAN models for pixel-level image translation, allowing to formulate the domain gap itself as a learning problem. The obtained models are then used either during training or inference to bridge the domain gap. Here, we focus on training the single-stage YOLO6D object pose estimator on synthetic CAD geometry only, where not even approximate surface information is available. When employing paired GAN models, we use an edge-based intermediate domain and introduce different mappings to represent the unknown surface properties. Our evaluation shows a considerable improvement in model performance when compared to a model trained with the same degree of domain randomization, while requiring only very little additional effort.



### Cross-modal Speaker Verification and Recognition: A Multilingual Perspective
- **Arxiv ID**: http://arxiv.org/abs/2004.13780v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2004.13780v2)
- **Published**: 2020-04-28 19:15:23+00:00
- **Updated**: 2021-04-22 15:10:21+00:00
- **Authors**: Muhammad Saad Saeed, Shah Nawaz, Pietro Morerio, Arif Mahmood, Ignazio Gallo, Muhammad Haroon Yousaf, Alessio Del Bue
- **Comment**: Accepted: CVPRW
- **Journal**: None
- **Summary**: Recent years have seen a surge in finding association between faces and voices within a cross-modal biometric application along with speaker recognition. Inspired from this, we introduce a challenging task in establishing association between faces and voices across multiple languages spoken by the same set of persons. The aim of this paper is to answer two closely related questions: "Is face-voice association language independent?" and "Can a speaker be recognised irrespective of the spoken language?". These two questions are very important to understand effectiveness and to boost development of multilingual biometric systems. To answer them, we collected a Multilingual Audio-Visual dataset, containing human speech clips of $154$ identities with $3$ language annotations extracted from various videos uploaded online. Extensive experiments on the three splits of the proposed dataset have been performed to investigate and answer these novel research questions that clearly point out the relevance of the multilingual problem.



### Minority Reports Defense: Defending Against Adversarial Patches
- **Arxiv ID**: http://arxiv.org/abs/2004.13799v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.13799v1)
- **Published**: 2020-04-28 20:11:18+00:00
- **Updated**: 2020-04-28 20:11:18+00:00
- **Authors**: Michael McCoyd, Won Park, Steven Chen, Neil Shah, Ryan Roggenkemper, Minjune Hwang, Jason Xinyu Liu, David Wagner
- **Comment**: 9 pages, 5 figures
- **Journal**: None
- **Summary**: Deep learning image classification is vulnerable to adversarial attack, even if the attacker changes just a small patch of the image. We propose a defense against patch attacks based on partially occluding the image around each candidate patch location, so that a few occlusions each completely hide the patch. We demonstrate on CIFAR-10, Fashion MNIST, and MNIST that our defense provides certified security against patch attacks of a certain size.



### Pyramid Attention Networks for Image Restoration
- **Arxiv ID**: http://arxiv.org/abs/2004.13824v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.13824v4)
- **Published**: 2020-04-28 21:12:36+00:00
- **Updated**: 2020-06-03 18:47:11+00:00
- **Authors**: Yiqun Mei, Yuchen Fan, Yulun Zhang, Jiahui Yu, Yuqian Zhou, Ding Liu, Yun Fu, Thomas S. Huang, Humphrey Shi
- **Comment**: None
- **Journal**: None
- **Summary**: Self-similarity refers to the image prior widely used in image restoration algorithms that small but similar patterns tend to occur at different locations and scales. However, recent advanced deep convolutional neural network based methods for image restoration do not take full advantage of self-similarities by relying on self-attention neural modules that only process information at the same scale. To solve this problem, we present a novel Pyramid Attention module for image restoration, which captures long-range feature correspondences from a multi-scale feature pyramid. Inspired by the fact that corruptions, such as noise or compression artifacts, drop drastically at coarser image scales, our attention module is designed to be able to borrow clean signals from their "clean" correspondences at the coarser levels. The proposed pyramid attention module is a generic building block that can be flexibly integrated into various neural architectures. Its effectiveness is validated through extensive experiments on multiple image restoration tasks: image denoising, demosaicing, compression artifact reduction, and super resolution. Without any bells and whistles, our PANet (pyramid attention module with simple network backbones) can produce state-of-the-art results with superior accuracy and visual quality. Our code will be available at https://github.com/SHI-Labs/Pyramid-Attention-Networks



### Less is More: Sample Selection and Label Conditioning Improve Skin Lesion Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2004.13856v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.13856v1)
- **Published**: 2020-04-28 21:24:51+00:00
- **Updated**: 2020-04-28 21:24:51+00:00
- **Authors**: Vinicius Ribeiro, Sandra Avila, Eduardo Valle
- **Comment**: Accepted to the ISIC Skin Image Analysis Workshop @ CVPR 2020
- **Journal**: None
- **Summary**: Segmenting skin lesions images is relevant both for itself and for assisting in lesion classification, but suffers from the challenge in obtaining annotated data. In this work, we show that segmentation may improve with less data, by selecting the training samples with best inter-annotator agreement, and conditioning the ground-truth masks to remove excessive detail. We perform an exhaustive experimental design considering several sources of variation, including three different test sets, two different deep-learning architectures, and several replications, for a total of 540 experimental runs. We found that sample selection and detail removal may have impacts corresponding, respectively, to 12% and 16% of the one obtained by picking a better deep-learning model.



### Deflating Dataset Bias Using Synthetic Data Augmentation
- **Arxiv ID**: http://arxiv.org/abs/2004.13866v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.13866v1)
- **Published**: 2020-04-28 21:56:10+00:00
- **Updated**: 2020-04-28 21:56:10+00:00
- **Authors**: Nikita Jaipuria, Xianling Zhang, Rohan Bhasin, Mayar Arafa, Punarjay Chakravarty, Shubham Shrivastava, Sagar Manglani, Vidya N. Murali
- **Comment**: None
- **Journal**: None
- **Summary**: Deep Learning has seen an unprecedented increase in vision applications since the publication of large-scale object recognition datasets and introduction of scalable compute hardware. State-of-the-art methods for most vision tasks for Autonomous Vehicles (AVs) rely on supervised learning and often fail to generalize to domain shifts and/or outliers. Dataset diversity is thus key to successful real-world deployment. No matter how big the size of the dataset, capturing long tails of the distribution pertaining to task-specific environmental factors is impractical. The goal of this paper is to investigate the use of targeted synthetic data augmentation - combining the benefits of gaming engine simulations and sim2real style transfer techniques - for filling gaps in real datasets for vision tasks. Empirical studies on three different computer vision tasks of practical use to AVs - parking slot detection, lane detection and monocular depth estimation - consistently show that having synthetic data in the training mix provides a significant boost in cross-dataset generalization performance as compared to training on real data only, for the same size of the training set.



### Histogram-based Auto Segmentation: A Novel Approach to Segmenting Integrated Circuit Structures from SEM Images
- **Arxiv ID**: http://arxiv.org/abs/2004.13874v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.13874v1)
- **Published**: 2020-04-28 22:24:08+00:00
- **Updated**: 2020-04-28 22:24:08+00:00
- **Authors**: Ronald Wilson, Navid Asadizanjani, Domenic Forte, Damon L. Woodard
- **Comment**: None
- **Journal**: None
- **Summary**: In the Reverse Engineering and Hardware Assurance domain, a majority of the data acquisition is done through electron microscopy techniques such as Scanning Electron Microscopy (SEM). However, unlike its counterparts in optical imaging, only a limited number of techniques are available to enhance and extract information from the raw SEM images. In this paper, we introduce an algorithm to segment out Integrated Circuit (IC) structures from the SEM image. Unlike existing algorithms discussed in this paper, this algorithm is unsupervised, parameter-free and does not require prior information on the noise model or features in the target image making it effective in low quality image acquisition scenarios as well. Furthermore, the results from the application of the algorithm on various structures and layers in the IC are reported and discussed.



### Classifying Image Sequences of Astronomical Transients with Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2004.13877v2
- **DOI**: 10.1093/mnras/staa2973
- **Categories**: **astro-ph.IM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.13877v2)
- **Published**: 2020-04-28 22:29:01+00:00
- **Updated**: 2020-10-02 19:18:00+00:00
- **Authors**: Catalina Gómez, Mauricio Neira, Marcela Hernández Hoyos, Pablo Arbeláez, Jaime E. Forero-Romero
- **Comment**: 11 pages, 7 figures. MNRAS accepted
- **Journal**: None
- **Summary**: Supervised classification of temporal sequences of astronomical images into meaningful transient astrophysical phenomena has been considered a hard problem because it requires the intervention of human experts. The classifier uses the expert's knowledge to find heuristic features to process the images, for instance, by performing image subtraction or by extracting sparse information such as flux time series, also known as light curves. We present a successful deep learning approach that learns directly from imaging data. Our method models explicitly the spatio-temporal patterns with Deep Convolutional Neural Networks and Gated Recurrent Units. We train these deep neural networks using 1.3 million real astronomical images from the Catalina Real-Time Transient Survey to classify the sequences into five different types of astronomical transient classes. The TAO-Net (for Transient Astronomical Objects Network) architecture outperforms the results from random forest classification on light curves by 10 percentage points as measured by the F1 score for each class; the average F1 over classes goes from $45\%$ with random forest classification to $55\%$ with TAO-Net. This achievement with TAO-Net opens the possibility to develop new deep learning architectures for early transient detection. We make available the training dataset and trained models of TAO-Net to allow for future extensions of this work.



### Unmanned Aerial Systems for Wildland and Forest Fires
- **Arxiv ID**: http://arxiv.org/abs/2004.13883v2
- **DOI**: 10.3390/drones5010015
- **Categories**: **cs.RO**, cs.CV, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.13883v2)
- **Published**: 2020-04-28 23:01:12+00:00
- **Updated**: 2021-03-05 16:33:41+00:00
- **Authors**: Moulay A. Akhloufi, Nicolas A. Castro, Andy Couturier
- **Comment**: A recent published version of this paper is available at:
  https://doi.org/10.3390/drones5010015
- **Journal**: Drones. 2021; 5(1):15; pp 1-25
- **Summary**: Wildfires represent an important natural risk causing economic losses, human death and important environmental damage. In recent years, we witness an increase in fire intensity and frequency. Research has been conducted towards the development of dedicated solutions for wildland and forest fire assistance and fighting. Systems were proposed for the remote detection and tracking of fires. These systems have shown improvements in the area of efficient data collection and fire characterization within small scale environments. However, wildfires cover large areas making some of the proposed ground-based systems unsuitable for optimal coverage. To tackle this limitation, Unmanned Aerial Systems (UAS) were proposed. UAS have proven to be useful due to their maneuverability, allowing for the implementation of remote sensing, allocation strategies and task planning. They can provide a low-cost alternative for the prevention, detection and real-time support of firefighting. In this paper we review previous work related to the use of UAS in wildfires. Onboard sensor instruments, fire perception algorithms and coordination strategies are considered. In addition, we present some of the recent frameworks proposing the use of both aerial vehicles and Unmanned Ground Vehicles (UV) for a more efficient wildland firefighting strategy at a larger scale.



