# Arxiv Papers in cs.CV on 2020-04-24
### Mining self-similarity: Label super-resolution with epitomic representations
- **Arxiv ID**: http://arxiv.org/abs/2004.11498v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.11498v2)
- **Published**: 2020-04-24 00:42:59+00:00
- **Updated**: 2021-12-13 21:12:26+00:00
- **Authors**: Nikolay Malkin, Anthony Ortiz, Caleb Robinson, Nebojsa Jojic
- **Comment**: ECCV 2020 final version
- **Journal**: None
- **Summary**: We show that simple patch-based models, such as epitomes, can have superior performance to the current state of the art in semantic segmentation and label super-resolution, which uses deep convolutional neural networks. We derive a new training algorithm for epitomes which allows, for the first time, learning from very large data sets and derive a label super-resolution algorithm as a statistical inference algorithm over epitomic representations. We illustrate our methods on land cover mapping and medical image analysis tasks.



### What Can Be Transferred: Unsupervised Domain Adaptation for Endoscopic Lesions Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2004.11500v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.11500v1)
- **Published**: 2020-04-24 00:57:05+00:00
- **Updated**: 2020-04-24 00:57:05+00:00
- **Authors**: Jiahua Dong, Yang Cong, Gan Sun, Bineng Zhong, Xiaowei Xu
- **Comment**: This paper is accepted by IEEE Conference on Computer Vision and
  Pattern Recognition 2020 (CVPR 2020)
- **Journal**: None
- **Summary**: Unsupervised domain adaptation has attracted growing research attention on semantic segmentation. However, 1) most existing models cannot be directly applied into lesions transfer of medical images, due to the diverse appearances of same lesion among different datasets; 2) equal attention has been paid into all semantic representations instead of neglecting irrelevant knowledge, which leads to negative transfer of untransferable knowledge. To address these challenges, we develop a new unsupervised semantic transfer model including two complementary modules (i.e., T_D and T_F ) for endoscopic lesions segmentation, which can alternatively determine where and how to explore transferable domain-invariant knowledge between labeled source lesions dataset (e.g., gastroscope) and unlabeled target diseases dataset (e.g., enteroscopy). Specifically, T_D focuses on where to translate transferable visual information of medical lesions via residual transferability-aware bottleneck, while neglecting untransferable visual characterizations. Furthermore, T_F highlights how to augment transferable semantic features of various lesions and automatically ignore untransferable representations, which explores domain-invariant knowledge and in return improves the performance of T_D. To the end, theoretical analysis and extensive experiments on medical endoscopic dataset and several non-medical public datasets well demonstrate the superiority of our proposed model.



### Automatic low-bit hybrid quantization of neural networks through meta learning
- **Arxiv ID**: http://arxiv.org/abs/2004.11506v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.11506v1)
- **Published**: 2020-04-24 02:01:26+00:00
- **Updated**: 2020-04-24 02:01:26+00:00
- **Authors**: Tao Wang, Junsong Wang, Chang Xu, Chao Xue
- **Comment**: 7 pages, 3 figures
- **Journal**: None
- **Summary**: Model quantization is a widely used technique to compress and accelerate deep neural network (DNN) inference, especially when deploying to edge or IoT devices with limited computation capacity and power consumption budget. The uniform bit width quantization across all the layers is usually sub-optimal and the exploration of hybrid quantization for different layers is vital for efficient deep compression. In this paper, we employ the meta learning method to automatically realize low-bit hybrid quantization of neural networks. A MetaQuantNet, together with a Quantization function, are trained to generate the quantized weights for the target DNN. Then, we apply a genetic algorithm to search the best hybrid quantization policy that meets compression constraints. With the best searched quantization policy, we subsequently retrain or finetune to further improve the performance of the quantized target network. Extensive experiments demonstrate the performance of searched hybrid quantization scheme surpass that of uniform bitwidth counterpart. Compared to the existing reinforcement learning (RL) based hybrid quantization search approach that relies on tedious explorations, our meta learning approach is more efficient and effective for any compression requirements since the MetaQuantNet only needs be trained once.



### RAIN: A Simple Approach for Robust and Accurate Image Classification Networks
- **Arxiv ID**: http://arxiv.org/abs/2004.14798v4
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.14798v4)
- **Published**: 2020-04-24 02:03:56+00:00
- **Updated**: 2020-11-04 13:24:52+00:00
- **Authors**: Jiawei Du, Hanshu Yan, Vincent Y. F. Tan, Joey Tianyi Zhou, Rick Siow Mong Goh, Jiashi Feng
- **Comment**: None
- **Journal**: None
- **Summary**: It has been shown that the majority of existing adversarial defense methods achieve robustness at the cost of sacrificing prediction accuracy. The undesirable severe drop in accuracy adversely affects the reliability of machine learning algorithms and prohibits their deployment in realistic applications. This paper aims to address this dilemma by proposing a novel preprocessing framework, which we term Robust and Accurate Image classificatioN(RAIN), to improve the robustness of given CNN classifiers and, at the same time, preserve their high prediction accuracies. RAIN introduces a new randomization-enhancement scheme. It applies randomization over inputs to break the ties between the model forward prediction path and the backward gradient path, thus improving the model robustness. However, similar to existing preprocessing-based methods, the randomized process will degrade the prediction accuracy. To understand why this is the case, we compare the difference between original and processed images, and find it is the loss of high-frequency components in the input image that leads to accuracy drop of the classifier. Based on this finding, RAIN enhances the input's high-frequency details to retain the CNN's high prediction accuracy. Concretely, RAIN consists of two novel randomization modules: randomized small circular shift (RdmSCS) and randomized down-upsampling (RdmDU). The RdmDU module randomly downsamples the input image, and then the RdmSCS module circularly shifts the input image along a randomly chosen direction by a small but random number of pixels. Finally, the RdmDU module performs upsampling with a detail-enhancement model, such as deep super-resolution networks. We conduct extensive experiments on the STL10 and ImageNet datasets to verify the effectiveness of RAIN against various types of adversarial attacks.



### Detecting and Tracking Communal Bird Roosts in Weather Radar Data
- **Arxiv ID**: http://arxiv.org/abs/2004.12819v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.12819v1)
- **Published**: 2020-04-24 02:40:50+00:00
- **Updated**: 2020-04-24 02:40:50+00:00
- **Authors**: Zezhou Cheng, Saadia Gabriel, Pankaj Bhambhani, Daniel Sheldon, Subhransu Maji, Andrew Laughlin, David Winkler
- **Comment**: 9 pages, 6 figures, AAAI 2020 (AI for Social Impact Track)
- **Journal**: None
- **Summary**: The US weather radar archive holds detailed information about biological phenomena in the atmosphere over the last 20 years. Communally roosting birds congregate in large numbers at nighttime roosting locations, and their morning exodus from the roost is often visible as a distinctive pattern in radar images. This paper describes a machine learning system to detect and track roost signatures in weather radar data. A significant challenge is that labels were collected opportunistically from previous research studies and there are systematic differences in labeling style. We contribute a latent variable model and EM algorithm to learn a detection model together with models of labeling styles for individual annotators. By properly accounting for these variations we learn a significantly more accurate detector. The resulting system detects previously unknown roosting locations and provides comprehensive spatio-temporal data about roosts across the US. This data will provide biologists important information about the poorly understood phenomena of broad-scale habitat use and movements of communally roosting birds during the non-breeding season.



### Systematic Evaluation of Backdoor Data Poisoning Attacks on Image Classifiers
- **Arxiv ID**: http://arxiv.org/abs/2004.11514v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2004.11514v1)
- **Published**: 2020-04-24 02:58:22+00:00
- **Updated**: 2020-04-24 02:58:22+00:00
- **Authors**: Loc Truong, Chace Jones, Brian Hutchinson, Andrew August, Brenda Praggastis, Robert Jasper, Nicole Nichols, Aaron Tuor
- **Comment**: None
- **Journal**: None
- **Summary**: Backdoor data poisoning attacks have recently been demonstrated in computer vision research as a potential safety risk for machine learning (ML) systems. Traditional data poisoning attacks manipulate training data to induce unreliability of an ML model, whereas backdoor data poisoning attacks maintain system performance unless the ML model is presented with an input containing an embedded "trigger" that provides a predetermined response advantageous to the adversary. Our work builds upon prior backdoor data-poisoning research for ML image classifiers and systematically assesses different experimental conditions including types of trigger patterns, persistence of trigger patterns during retraining, poisoning strategies, architectures (ResNet-50, NasNet, NasNet-Mobile), datasets (Flowers, CIFAR-10), and potential defensive regularization techniques (Contrastive Loss, Logit Squeezing, Manifold Mixup, Soft-Nearest-Neighbors Loss). Experiments yield four key findings. First, the success rate of backdoor poisoning attacks varies widely, depending on several factors, including model architecture, trigger pattern and regularization technique. Second, we find that poisoned models are hard to detect through performance inspection alone. Third, regularization typically reduces backdoor success rate, although it can have no effect or even slightly increase it, depending on the form of regularization. Finally, backdoors inserted through data poisoning can be rendered ineffective after just a few epochs of additional training on a small set of clean data without affecting the model's performance.



### Deep Global Registration
- **Arxiv ID**: http://arxiv.org/abs/2004.11540v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CG, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.11540v2)
- **Published**: 2020-04-24 05:47:32+00:00
- **Updated**: 2020-05-08 08:59:52+00:00
- **Authors**: Christopher Choy, Wei Dong, Vladlen Koltun
- **Comment**: Accepted for CVPR'20 oral presentation
- **Journal**: None
- **Summary**: We present Deep Global Registration, a differentiable framework for pairwise registration of real-world 3D scans. Deep global registration is based on three modules: a 6-dimensional convolutional network for correspondence confidence prediction, a differentiable Weighted Procrustes algorithm for closed-form pose estimation, and a robust gradient-based SE(3) optimizer for pose refinement. Experiments demonstrate that our approach outperforms state-of-the-art methods, both learning-based and classical, on real-world data.



### Dropout as an Implicit Gating Mechanism For Continual Learning
- **Arxiv ID**: http://arxiv.org/abs/2004.11545v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.11545v1)
- **Published**: 2020-04-24 06:04:15+00:00
- **Updated**: 2020-04-24 06:04:15+00:00
- **Authors**: Seyed-Iman Mirzadeh, Mehrdad Farajtabar, Hassan Ghasemzadeh
- **Comment**: CVPR 2020 Workshops
- **Journal**: None
- **Summary**: In recent years, neural networks have demonstrated an outstanding ability to achieve complex learning tasks across various domains. However, they suffer from the "catastrophic forgetting" problem when they face a sequence of learning tasks, where they forget the old ones as they learn new tasks. This problem is also highly related to the "stability-plasticity dilemma". The more plastic the network, the easier it can learn new tasks, but the faster it also forgets previous ones. Conversely, a stable network cannot learn new tasks as fast as a very plastic network. However, it is more reliable to preserve the knowledge it has learned from the previous tasks. Several solutions have been proposed to overcome the forgetting problem by making the neural network parameters more stable, and some of them have mentioned the significance of dropout in continual learning. However, their relationship has not been sufficiently studied yet. In this paper, we investigate this relationship and show that a stable network with dropout learns a gating mechanism such that for different tasks, different paths of the network are active. Our experiments show that the stability achieved by this implicit gating plays a very critical role in leading to performance comparable to or better than other involved continual learning algorithms to overcome catastrophic forgetting.



### Expanding Sparse Guidance for Stereo Matching
- **Arxiv ID**: http://arxiv.org/abs/2005.02123v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2005.02123v1)
- **Published**: 2020-04-24 06:41:11+00:00
- **Updated**: 2020-04-24 06:41:11+00:00
- **Authors**: Yu-Kai Huang, Yueh-Cheng Liu, Tsung-Han Wu, Hung-Ting Su, Winston H. Hsu
- **Comment**: None
- **Journal**: None
- **Summary**: The performance of image based stereo estimation suffers from lighting variations, repetitive patterns and homogeneous appearance. Moreover, to achieve good performance, stereo supervision requires sufficient densely-labeled data, which are hard to obtain. In this work, we leverage small amount of data with very sparse but accurate disparity cues from LiDAR to bridge the gap. We propose a novel sparsity expansion technique to expand the sparse cues concerning RGB images for local feature enhancement. The feature enhancement method can be easily applied to any stereo estimation algorithms with cost volume at the test stage. Extensive experiments on stereo datasets demonstrate the effectiveness and robustness across different backbones on domain adaption and self-supervision scenario. Our sparsity expansion method outperforms previous methods in terms of disparity by more than 2 pixel error on KITTI Stereo 2012 and 3 pixel error on KITTI Stereo 2015. Our approach significantly boosts the existing state-of-the-art stereo algorithms with extremely sparse cues.



### Deep Feature-preserving Normal Estimation for Point Cloud Filtering
- **Arxiv ID**: http://arxiv.org/abs/2004.11563v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.11563v1)
- **Published**: 2020-04-24 07:05:48+00:00
- **Updated**: 2020-04-24 07:05:48+00:00
- **Authors**: Dening Lu, Xuequan Lu, Yangxing Sun, Jun Wang
- **Comment**: accepted to Computer Aided Design (Symposium on Solid and Physical
  Modeling 2020)
- **Journal**: None
- **Summary**: Point cloud filtering, the main bottleneck of which is removing noise (outliers) while preserving geometric features, is a fundamental problem in 3D field. The two-step schemes involving normal estimation and position update have been shown to produce promising results. Nevertheless, the current normal estimation methods including optimization ones and deep learning ones, often either have limited automation or cannot preserve sharp features. In this paper, we propose a novel feature-preserving normal estimation method for point cloud filtering with preserving geometric features. It is a learning method and thus achieves automatic prediction for normals. For training phase, we first generate patch based samples which are then fed to a classification network to classify feature and non-feature points. We finally train the samples of feature and non-feature points separately, to achieve decent results. Regarding testing, given a noisy point cloud, its normals can be automatically estimated. For further point cloud filtering, we iterate the above normal estimation and a current position update algorithm for a few times. Various experiments demonstrate that our method outperforms state-of-the-art normal estimation methods and point cloud filtering techniques, in terms of both quality and quantity.



### A Review of an Old Dilemma: Demosaicking First, or Denoising First?
- **Arxiv ID**: http://arxiv.org/abs/2004.11577v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.11577v1)
- **Published**: 2020-04-24 07:32:17+00:00
- **Updated**: 2020-04-24 07:32:17+00:00
- **Authors**: Qiyu Jin, Gabriele Facciolo, Jean-Michel Morel
- **Comment**: None
- **Journal**: None
- **Summary**: Image denoising and demosaicking are the most important early stages in digital camera pipelines. They constitute a severely ill-posed problem that aims at reconstructing a full color image from a noisy color filter array (CFA) image. In most of the literature, denoising and demosaicking are treated as two independent problems, without considering their interaction, or asking which should be applied first. Several recent works have started addressing them jointly in works that involve heavy weight CNNs, thus incompatible with low power portable imaging devices. Hence, the question of how to combine denoising and demosaicking to reconstruct full color images remains very relevant: Is denoising to be applied first, or should that be demosaicking first? In this paper, we review the main variants of these strategies and carry-out an extensive evaluation to find the best way to reconstruct full color images from a noisy mosaic. We conclude that demosaicking should applied first, followed by denoising. Yet we prove that this requires an adaptation of classic denoising algorithms to demosaicked noise, which we justify and specify.



### A Light CNN for detecting COVID-19 from CT scans of the chest
- **Arxiv ID**: http://arxiv.org/abs/2004.12837v1
- **DOI**: 10.1016/j.patrec.2020.10.001
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.12837v1)
- **Published**: 2020-04-24 07:58:49+00:00
- **Updated**: 2020-04-24 07:58:49+00:00
- **Authors**: Matteo Polsinelli, Luigi Cinque, Giuseppe Placidi
- **Comment**: None
- **Journal**: Pattern Recognition Letters. 140 (2020) 95-100
- **Summary**: OVID-19 is a world-wide disease that has been declared as a pandemic by the World Health Organization. Computer Tomography (CT) imaging of the chest seems to be a valid diagnosis tool to detect COVID-19 promptly and to control the spread of the disease. Deep Learning has been extensively used in medical imaging and convolutional neural networks (CNNs) have been also used for classification of CT images. We propose a light CNN design based on the model of the SqueezeNet, for the efficient discrimination of COVID-19 CT images with other CT images (community-acquired pneumonia and/or healthy images). On the tested datasets, the proposed modified SqueezeNet CNN achieved 83.00\% of accuracy, 85.00\% of sensitivity, 81.00\% of specificity, 81.73\% of precision and 0.8333 of F1Score in a very efficient way (7.81 seconds medium-end laptot without GPU acceleration). Besides performance, the average classification time is very competitive with respect to more complex CNN designs, thus allowing its usability also on medium power computers. In the next future we aim at improving the performances of the method along two directions: 1) by increasing the training dataset (as soon as other CT images will be available); 2) by introducing an efficient pre-processing strategy.



### Quantifying Graft Detachment after Descemet's Membrane Endothelial Keratoplasty with Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2004.12807v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.12807v1)
- **Published**: 2020-04-24 08:01:01+00:00
- **Updated**: 2020-04-24 08:01:01+00:00
- **Authors**: Friso G. Heslinga, Mark Alberti, Josien P. W. Pluim, Javier Cabrerizo, Mitko Veta
- **Comment**: To be published in Translational Vision Science & Technology
- **Journal**: None
- **Summary**: Purpose: We developed a method to automatically locate and quantify graft detachment after Descemet's Membrane Endothelial Keratoplasty (DMEK) in Anterior Segment Optical Coherence Tomography (AS-OCT) scans. Methods: 1280 AS-OCT B-scans were annotated by a DMEK expert. Using the annotations, a deep learning pipeline was developed to localize scleral spur, center the AS-OCT B-scans and segment the detached graft sections. Detachment segmentation model performance was evaluated per B-scan by comparing (1) length of detachment and (2) horizontal projection of the detached sections with the expert annotations. Horizontal projections were used to construct graft detachment maps. All final evaluations were done on a test set that was set apart during training of the models. A second DMEK expert annotated the test set to determine inter-rater performance. Results: Mean scleral spur localization error was 0.155 mm, whereas the inter-rater difference was 0.090 mm. The estimated graft detachment lengths were in 69% of the cases within a 10-pixel (~150{\mu}m) difference from the ground truth (77% for the second DMEK expert). Dice scores for the horizontal projections of all B-scans with detachments were 0.896 and 0.880 for our model and the second DMEK expert respectively. Conclusion: Our deep learning model can be used to automatically and instantly localize graft detachment in AS-OCT B-scans. Horizontal detachment projections can be determined with the same accuracy as a human DMEK expert, allowing for the construction of accurate graft detachment maps. Translational Relevance: Automated localization and quantification of graft detachment can support DMEK research and standardize clinical decision making.



### Deep 3D Portrait from a Single Image
- **Arxiv ID**: http://arxiv.org/abs/2004.11598v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.11598v1)
- **Published**: 2020-04-24 08:55:37+00:00
- **Updated**: 2020-04-24 08:55:37+00:00
- **Authors**: Sicheng Xu, Jiaolong Yang, Dong Chen, Fang Wen, Yu Deng, Yunde Jia, Xin Tong
- **Comment**: Accepted by CVPR2020; Code: https://github.com/sicxu/Deep3dPortrait
- **Journal**: None
- **Summary**: In this paper, we present a learning-based approach for recovering the 3D geometry of human head from a single portrait image. Our method is learned in an unsupervised manner without any ground-truth 3D data.   We represent the head geometry with a parametric 3D face model together with a depth map for other head regions including hair and ear. A two-step geometry learning scheme is proposed to learn 3D head reconstruction from in-the-wild face images, where we first learn face shape on single images using self-reconstruction and then learn hair and ear geometry using pairs of images in a stereo-matching fashion. The second step is based on the output of the first to not only improve the accuracy but also ensure the consistency of overall head geometry.   We evaluate the accuracy of our method both in 3D and with pose manipulation tasks on 2D images. We alter pose based on the recovered geometry and apply a refinement network trained with adversarial learning to ameliorate the reprojected images and translate them to the real image domain. Extensive evaluations and comparison with previous methods show that our new method can produce high-fidelity 3D head geometry and head pose manipulation results.



### Boosting Connectivity in Retinal Vessel Segmentation via a Recursive Semantics-Guided Network
- **Arxiv ID**: http://arxiv.org/abs/2004.12776v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.12776v1)
- **Published**: 2020-04-24 09:18:04+00:00
- **Updated**: 2020-04-24 09:18:04+00:00
- **Authors**: Rui Xu, Tiantian Liu, Xinchen Ye, Yen-Wei Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Many deep learning based methods have been proposed for retinal vessel segmentation, however few of them focus on the connectivity of segmented vessels, which is quite important for a practical computer-aided diagnosis system on retinal images. In this paper, we propose an efficient network to address this problem. A U-shape network is enhanced by introducing a semantics-guided module, which integrates the enriched semantics information to shallow layers for guiding the network to explore more powerful features. Besides, a recursive refinement iteratively applies the same network over the previous segmentation results for progressively boosting the performance while increasing no extra network parameters. The carefully designed recursive semantics-guided network has been extensively evaluated on several public datasets. Experimental results have shown the efficiency of the proposed method.



### Vision based hardware-software real-time control system for autonomous landing of an UAV
- **Arxiv ID**: http://arxiv.org/abs/2004.11612v1
- **DOI**: 10.1007/978-3-030-59006-2_2
- **Categories**: **cs.CV**, cs.SY, eess.IV, eess.SY
- **Links**: [PDF](http://arxiv.org/pdf/2004.11612v1)
- **Published**: 2020-04-24 09:24:14+00:00
- **Updated**: 2020-04-24 09:24:14+00:00
- **Authors**: Krzysztof Blachut, Hubert Szolc, Mateusz Wasala, Tomasz Kryjak, Marek Gorgon
- **Comment**: 7 pages, 9 figures, submitted to MMAR 2020 conference
- **Journal**: None
- **Summary**: In this paper we present a vision based hardware-software control system enabling autonomous landing of a multirotor unmanned aerial vehicle (UAV). It allows the detection of a marked landing pad in real-time for a 1280 x 720 @ 60 fps video stream. In addition, a LiDAR sensor is used to measure the altitude above ground. A heterogeneous Zynq SoC device is used as the computing platform. The solution was tested on a number of sequences and the landing pad was detected with 96% accuracy. This research shows that a reprogrammable heterogeneous computing system is a good solution for UAVs because it enables real-time data stream processing with relatively low energy consumption.



### Low-latency hand gesture recognition with a low resolution thermal imager
- **Arxiv ID**: http://arxiv.org/abs/2004.11623v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.11623v1)
- **Published**: 2020-04-24 09:43:48+00:00
- **Updated**: 2020-04-24 09:43:48+00:00
- **Authors**: Maarten Vandersteegen, Wouter Reusen, Kristof Van Beeck Toon Goedeme
- **Comment**: None
- **Journal**: None
- **Summary**: Using hand gestures to answer a call or to control the radio while driving a car, is nowadays an established feature in more expensive cars. High resolution time-of-flight cameras and powerful embedded processors usually form the heart of these gesture recognition systems. This however comes with a price tag. We therefore investigate the possibility to design an algorithm that predicts hand gestures using a cheap low-resolution thermal camera with only 32x24 pixels, which is light-weight enough to run on a low-cost processor. We recorded a new dataset of over 1300 video clips for training and evaluation and propose a light-weight low-latency prediction algorithm. Our best model achieves 95.9% classification accuracy and 83% mAP detection accuracy while its processing pipeline has a latency of only one frame.



### Dynamic Sampling for Deep Metric Learning
- **Arxiv ID**: http://arxiv.org/abs/2004.11624v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.11624v2)
- **Published**: 2020-04-24 09:47:23+00:00
- **Updated**: 2020-09-11 01:29:01+00:00
- **Authors**: Chang-Hui Liang, Wan-Lei Zhao, Run-Qing Chen
- **Comment**: 11 pages, 4 figures
- **Journal**: None
- **Summary**: Deep metric learning maps visually similar images onto nearby locations and visually dissimilar images apart from each other in an embedding manifold. The learning process is mainly based on the supplied image negative and positive training pairs. In this paper, a dynamic sampling strategy is proposed to organize the training pairs in an easy-to-hard order to feed into the network. It allows the network to learn general boundaries between categories from the easy training pairs at its early stages and finalize the details of the model mainly relying on the hard training samples in the later. Compared to the existing training sample mining approaches, the hard samples are mined with little harm to the learned general model. This dynamic sampling strategy is formularized as two simple terms that are compatible with various loss functions. Consistent performance boost is observed when it is integrated with several popular loss functions on fashion search, fine-grained classification, and person re-identification tasks.



### Convolution-Weight-Distribution Assumption: Rethinking the Criteria of Channel Pruning
- **Arxiv ID**: http://arxiv.org/abs/2004.11627v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.11627v3)
- **Published**: 2020-04-24 09:54:21+00:00
- **Updated**: 2021-10-25 13:06:28+00:00
- **Authors**: Zhongzhan Huang, Wenqi Shao, Xinjiang Wang, Liang Lin, Ping Luo
- **Comment**: Accepted by NeurIPS 2021
- **Journal**: None
- **Summary**: Channel pruning is a popular technique for compressing convolutional neural networks (CNNs), where various pruning criteria have been proposed to remove the redundant filters. From our comprehensive experiments, we found two blind spots in the study of pruning criteria: (1) Similarity: There are some strong similarities among several primary pruning criteria that are widely cited and compared. According to these criteria, the ranks of filters'Importance Score are almost identical, resulting in similar pruned structures. (2) Applicability: The filters'Importance Score measured by some pruning criteria are too close to distinguish the network redundancy well. In this paper, we analyze these two blind spots on different types of pruning criteria with layer-wise pruning or global pruning. The analyses are based on the empirical experiments and our assumption (Convolutional Weight Distribution Assumption) that the well-trained convolutional filters each layer approximately follow a Gaussian-alike distribution. This assumption has been verified through systematic and extensive statistical tests.



### Survey on Visual Sentiment Analysis
- **Arxiv ID**: http://arxiv.org/abs/2004.11639v2
- **DOI**: 10.1049/iet-ipr.2019.1270
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2004.11639v2)
- **Published**: 2020-04-24 10:15:22+00:00
- **Updated**: 2020-05-18 11:09:48+00:00
- **Authors**: Alessandro Ortis, Giovanni Maria Farinella, Sebastiano Battiato
- **Comment**: This paper is a postprint of a paper accepted by IET Image Processing
  and is subject to Institution of Engineering and Technology Copyright. When
  the final version is published, the copy of record will be available at the
  IET Digital Library
- **Journal**: None
- **Summary**: Visual Sentiment Analysis aims to understand how images affect people, in terms of evoked emotions. Although this field is rather new, a broad range of techniques have been developed for various data sources and problems, resulting in a large body of research. This paper reviews pertinent publications and tries to present an exhaustive overview of the field. After a description of the task and the related applications, the subject is tackled under different main headings. The paper also describes principles of design of general Visual Sentiment Analysis systems from three main points of view: emotional models, dataset definition, feature design. A formalization of the problem is discussed, considering different levels of granularity, as well as the components that can affect the sentiment toward an image in different ways. To this aim, this paper considers a structured formalization of the problem which is usually used for the analysis of text, and discusses it's suitability in the context of Visual Sentiment Analysis. The paper also includes a description of new challenges, the evaluation from the viewpoint of progress toward more sophisticated systems and related practical applications, as well as a summary of the insights resulting from this study.



### Any Motion Detector: Learning Class-agnostic Scene Dynamics from a Sequence of LiDAR Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2004.11647v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2004.11647v1)
- **Published**: 2020-04-24 10:40:07+00:00
- **Updated**: 2020-04-24 10:40:07+00:00
- **Authors**: Artem Filatov, Andrey Rykov, Viacheslav Murashkin
- **Comment**: Accepted to ICRA 2020
- **Journal**: None
- **Summary**: Object detection and motion parameters estimation are crucial tasks for self-driving vehicle safe navigation in a complex urban environment. In this work we propose a novel real-time approach of temporal context aggregation for motion detection and motion parameters estimation based on 3D point cloud sequence. We introduce an ego-motion compensation layer to achieve real-time inference with performance comparable to a naive odometric transform of the original point cloud sequence. Not only is the proposed architecture capable of estimating the motion of common road participants like vehicles or pedestrians but also generalizes to other object categories which are not present in training data. We also conduct an in-deep analysis of different temporal context aggregation strategies such as recurrent cells and 3D convolutions. Finally, we provide comparison results of our state-of-the-art model with existing solutions on KITTI Scene Flow dataset.



### YCB-M: A Multi-Camera RGB-D Dataset for Object Recognition and 6DoF Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2004.11657v2
- **DOI**: 10.1109/ICRA40945.2020.9197426
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.11657v2)
- **Published**: 2020-04-24 11:14:04+00:00
- **Updated**: 2020-09-29 07:58:18+00:00
- **Authors**: Till Grenzdörffer, Martin Günther, Joachim Hertzberg
- **Comment**: Published at ICRA-2020
- **Journal**: None
- **Summary**: While a great variety of 3D cameras have been introduced in recent years, most publicly available datasets for object recognition and pose estimation focus on one single camera. In this work, we present a dataset of 32 scenes that have been captured by 7 different 3D cameras, totaling 49,294 frames. This allows evaluating the sensitivity of pose estimation algorithms to the specifics of the used camera and the development of more robust algorithms that are more independent of the camera model. Vice versa, our dataset enables researchers to perform a quantitative comparison of the data from several different cameras and depth sensing technologies and evaluate their algorithms before selecting a camera for their specific task. The scenes in our dataset contain 20 different objects from the common benchmark YCB object and model set [1], [2]. We provide full ground truth 6DoF poses for each object, per-pixel segmentation, 2D and 3D bounding boxes and a measure of the amount of occlusion of each object. We have also performed an initial evaluation of the cameras using our dataset on a state-of-the-art object recognition and pose estimation system [3].



### Disentangled and Controllable Face Image Generation via 3D Imitative-Contrastive Learning
- **Arxiv ID**: http://arxiv.org/abs/2004.11660v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.11660v2)
- **Published**: 2020-04-24 11:21:13+00:00
- **Updated**: 2020-09-04 04:32:25+00:00
- **Authors**: Yu Deng, Jiaolong Yang, Dong Chen, Fang Wen, Xin Tong
- **Comment**: Accepted by CVPR2020 (Oral)
- **Journal**: None
- **Summary**: We propose DiscoFaceGAN, an approach for face image generation of virtual people with disentangled, precisely-controllable latent representations for identity of non-existing people, expression, pose, and illumination. We embed 3D priors into adversarial learning and train the network to imitate the image formation of an analytic 3D face deformation and rendering process. To deal with the generation freedom induced by the domain gap between real and rendered faces, we further introduce contrastive learning to promote disentanglement by comparing pairs of generated images. Experiments show that through our imitative-contrastive learning, the factor variations are very well disentangled and the properties of a generated face can be precisely controlled. We also analyze the learned latent space and present several meaningful properties supporting factor disentanglement. Our method can also be used to embed real images into the disentangled latent space. We hope our method could provide new understandings of the relationship between physical properties and deep image synthesis.



### Understanding when spatial transformer networks do not support invariance, and what to do about it
- **Arxiv ID**: http://arxiv.org/abs/2004.11678v5
- **DOI**: 10.1109/ICPR48806.2021.9412997
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.11678v5)
- **Published**: 2020-04-24 12:20:35+00:00
- **Updated**: 2021-05-18 09:14:59+00:00
- **Authors**: Lukas Finnveden, Ylva Jansson, Tony Lindeberg
- **Comment**: 13 pages, 7 figures, 6 tables
- **Journal**: Shortened version in International Conference on Pattern
  Recognition (ICPR 2020), pages 3427--3434, Jan 2021
- **Summary**: Spatial transformer networks (STNs) were designed to enable convolutional neural networks (CNNs) to learn invariance to image transformations. STNs were originally proposed to transform CNN feature maps as well as input images. This enables the use of more complex features when predicting transformation parameters. However, since STNs perform a purely spatial transformation, they do not, in the general case, have the ability to align the feature maps of a transformed image with those of its original. STNs are therefore unable to support invariance when transforming CNN feature maps. We present a simple proof for this and study the practical implications, showing that this inability is coupled with decreased classification accuracy. We therefore investigate alternative STN architectures that make use of complex features. We find that while deeper localization networks are difficult to train, localization networks that share parameters with the classification network remain stable as they grow deeper, which allows for higher classification accuracy on difficult datasets. Finally, we explore the interaction between localization network complexity and iterative image alignment.



### Optic disc and fovea localisation in ultra-widefield scanning laser ophthalmoscope images captured in multiple modalities
- **Arxiv ID**: http://arxiv.org/abs/2004.11691v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.11691v1)
- **Published**: 2020-04-24 12:29:10+00:00
- **Updated**: 2020-04-24 12:29:10+00:00
- **Authors**: Peter Robert Wakeford, Enrico Pellegrini, Gavin Robertson, Michael Verhoek, Alan Duncan Fleming, Jano van Hemert, Ik Siong Heng
- **Comment**: Submitted to the 23rd Conference on Medical Image Understanding and
  Analysis (MIUA)
- **Journal**: None
- **Summary**: We propose a convolutional neural network for localising the centres of the optic disc (OD) and fovea in ultra-wide field of view scanning laser ophthalmoscope (UWFoV-SLO) images of the retina. Images captured in both reflectance and autofluorescence (AF) modes, and central pole and eyesteered gazes, were used. The method achieved an OD localisation accuracy of 99.4% within one OD radius, and fovea localisation accuracy of 99.1% within one OD radius on a test set comprising of 1790 images. The performance of fovea localisation in AF images was comparable to the variation between human annotators at this task. The laterality of the image (whether the image is of the left or right eye) was inferred from the OD and fovea coordinates with an accuracy of 99.9%



### A Systematic Search over Deep Convolutional Neural Network Architectures for Screening Chest Radiographs
- **Arxiv ID**: http://arxiv.org/abs/2004.11693v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.11693v1)
- **Published**: 2020-04-24 12:30:40+00:00
- **Updated**: 2020-04-24 12:30:40+00:00
- **Authors**: Arka Mitra, Arunava Chakravarty, Nirmalya Ghosh, Tandra Sarkar, Ramanathan Sethuraman, Debdoot Sheet
- **Comment**: accepted in EMBC 2020, 4 pages+2 page Appendix
- **Journal**: None
- **Summary**: Chest radiographs are primarily employed for the screening of pulmonary and cardio-/thoracic conditions. Being undertaken at primary healthcare centers, they require the presence of an on-premise reporting Radiologist, which is a challenge in low and middle income countries. This has inspired the development of machine learning based automation of the screening process. While recent efforts demonstrate a performance benchmark using an ensemble of deep convolutional neural networks (CNN), our systematic search over multiple standard CNN architectures identified single candidate CNN models whose classification performances were found to be at par with ensembles. Over 63 experiments spanning 400 hours, executed on a 11:3 FP32 TensorTFLOPS compute system, we found the Xception and ResNet-18 architectures to be consistent performers in identifying co-existing disease conditions with an average AUC of 0.87 across nine pathologies. We conclude on the reliability of the models by assessing their saliency maps generated using the randomized input sampling for explanation (RISE) method and qualitatively validating them against manual annotations locally sourced from an experienced Radiologist. We also draw a critical note on the limitations of the publicly available CheXpert dataset primarily on account of disparity in class distribution in training vs. testing sets, and unavailability of sufficient samples for few classes, which hampers quantitative reporting due to sample insufficiency.



### Learning Decision Ensemble using a Graph Neural Network for Comorbidity Aware Chest Radiograph Screening
- **Arxiv ID**: http://arxiv.org/abs/2004.11721v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.11721v1)
- **Published**: 2020-04-24 12:57:50+00:00
- **Updated**: 2020-04-24 12:57:50+00:00
- **Authors**: Arunava Chakravarty, Tandra Sarkar, Nirmalya Ghosh, Ramanathan Sethuraman, Debdoot Sheet
- **Comment**: accepted in EMBC 2020, 4pg+2pg Supplementary Material
- **Journal**: None
- **Summary**: Chest radiographs are primarily employed for the screening of cardio, thoracic and pulmonary conditions. Machine learning based automated solutions are being developed to reduce the burden of routine screening on Radiologists, allowing them to focus on critical cases. While recent efforts demonstrate the use of ensemble of deep convolutional neural networks(CNN), they do not take disease comorbidity into consideration, thus lowering their screening performance. To address this issue, we propose a Graph Neural Network (GNN) based solution to obtain ensemble predictions which models the dependencies between different diseases. A comprehensive evaluation of the proposed method demonstrated its potential by improving the performance over standard ensembling technique across a wide range of ensemble constructions. The best performance was achieved using the GNN ensemble of DenseNet121 with an average AUC of 0.821 across thirteen disease comorbidities.



### A Two-Stage Multiple Instance Learning Framework for the Detection of Breast Cancer in Mammograms
- **Arxiv ID**: http://arxiv.org/abs/2004.11726v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.11726v1)
- **Published**: 2020-04-24 13:06:47+00:00
- **Updated**: 2020-04-24 13:06:47+00:00
- **Authors**: Sarath Chandra K, Arunava Chakravarty, Nirmalya Ghosh, Tandra Sarkar, Ramanathan Sethuraman, Debdoot Sheet
- **Comment**: accepted in EMBC 2020, 4 pg+1 pg Supplementary
- **Journal**: None
- **Summary**: Mammograms are commonly employed in the large scale screening of breast cancer which is primarily characterized by the presence of malignant masses. However, automated image-level detection of malignancy is a challenging task given the small size of the mass regions and difficulty in discriminating between malignant, benign mass and healthy dense fibro-glandular tissue. To address these issues, we explore a two-stage Multiple Instance Learning (MIL) framework. A Convolutional Neural Network (CNN) is trained in the first stage to extract local candidate patches in the mammograms that may contain either a benign or malignant mass. The second stage employs a MIL strategy for an image level benign vs. malignant classification. A global image-level feature is computed as a weighted average of patch-level features learned using a CNN. Our method performed well on the task of localization of masses with an average Precision/Recall of 0.76/0.80 and acheived an average AUC of 0.91 on the imagelevel classification task using a five-fold cross-validation on the INbreast dataset. Restricting the MIL only to the candidate patches extracted in Stage 1 led to a significant improvement in classification performance in comparison to a dense extraction of patches from the entire mammogram.



### Prominent Attribute Modification using Attribute Dependent Generative Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/2005.02122v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.02122v1)
- **Published**: 2020-04-24 13:38:05+00:00
- **Updated**: 2020-04-24 13:38:05+00:00
- **Authors**: Naeem Ul Islam, Sungmin Lee, Jaebyung Park
- **Comment**: None
- **Journal**: Ubiquitous Robots 2020
- **Summary**: Modifying the facial images with desired attributes is important, though challenging tasks in computer vision, where it aims to modify single or multiple attributes of the face image. Some of the existing methods are either based on attribute independent approaches where the modification is done in the latent representation or attribute dependent approaches. The attribute independent methods are limited in performance as they require the desired paired data for changing the desired attributes. Secondly, the attribute independent constraint may result in the loss of information and, hence, fail in generating the required attributes in the face image. In contrast, the attribute dependent approaches are effective as these approaches are capable of modifying the required features along with preserving the information in the given image. However, attribute dependent approaches are sensitive and require a careful model design in generating high-quality results. To address this problem, we propose an attribute dependent face modification approach. The proposed approach is based on two generators and two discriminators that utilize the binary as well as the real representation of the attributes and, in return, generate high-quality attribute modification results. Experiments on the CelebA dataset show that our method effectively performs the multiple attribute editing with preserving other facial details intactly.



### PipeNet: Selective Modal Pipeline of Fusion Network for Multi-Modal Face Anti-Spoofing
- **Arxiv ID**: http://arxiv.org/abs/2004.11744v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.11744v1)
- **Published**: 2020-04-24 13:46:00+00:00
- **Updated**: 2020-04-24 13:46:00+00:00
- **Authors**: Qing Yang, Xia Zhu, Jong-Kae Fwu, Yun Ye, Ganmei You, Yuan Zhu
- **Comment**: Accepted to appear in CVPR2020 WMF
- **Journal**: None
- **Summary**: Face anti-spoofing has become an increasingly important and critical security feature for authentication systems, due to rampant and easily launchable presentation attacks. Addressing the shortage of multi-modal face dataset, CASIA recently released the largest up-to-date CASIA-SURF Cross-ethnicity Face Anti-spoofing(CeFA) dataset, covering 3 ethnicities, 3 modalities, 1607 subjects, and 2D plus 3D attack types in four protocols, and focusing on the challenge of improving the generalization capability of face anti-spoofing in cross-ethnicity and multi-modal continuous data. In this paper, we propose a novel pipeline-based multi-stream CNN architecture called PipeNet for multi-modal face anti-spoofing. Unlike previous works, Selective Modal Pipeline (SMP) is designed to enable a customized pipeline for each data modality to take full advantage of multi-modal data. Limited Frame Vote (LFV) is designed to ensure stable and accurate prediction for video classification. The proposed method wins the third place in the final ranking of Chalearn Multi-modal Cross-ethnicity Face Anti-spoofing Recognition Challenge@CVPR2020. Our final submission achieves the Average Classification Error Rate (ACER) of 2.21 with Standard Deviation of 1.26 on the test set.



### Ultra Fast Structure-aware Deep Lane Detection
- **Arxiv ID**: http://arxiv.org/abs/2004.11757v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.11757v4)
- **Published**: 2020-04-24 13:58:49+00:00
- **Updated**: 2020-08-05 02:59:50+00:00
- **Authors**: Zequn Qin, Huanyu Wang, Xi Li
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: Modern methods mainly regard lane detection as a problem of pixel-wise segmentation, which is struggling to address the problem of challenging scenarios and speed. Inspired by human perception, the recognition of lanes under severe occlusion and extreme lighting conditions is mainly based on contextual and global information. Motivated by this observation, we propose a novel, simple, yet effective formulation aiming at extremely fast speed and challenging scenarios. Specifically, we treat the process of lane detection as a row-based selecting problem using global features. With the help of row-based selecting, our formulation could significantly reduce the computational cost. Using a large receptive field on global features, we could also handle the challenging scenarios. Moreover, based on the formulation, we also propose a structural loss to explicitly model the structure of lanes. Extensive experiments on two lane detection benchmark datasets show that our method could achieve the state-of-the-art performance in terms of both speed and accuracy. A light-weight version could even achieve 300+ frames per second with the same resolution, which is at least 4x faster than previous state-of-the-art methods. Our code will be made publicly available.



### GAPS: Generator for Automatic Polynomial Solvers
- **Arxiv ID**: http://arxiv.org/abs/2004.11765v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MS, cs.RO, cs.SC
- **Links**: [PDF](http://arxiv.org/pdf/2004.11765v1)
- **Published**: 2020-04-24 14:11:28+00:00
- **Updated**: 2020-04-24 14:11:28+00:00
- **Authors**: Bo Li, Viktor Larsson
- **Comment**: None
- **Journal**: None
- **Summary**: Minimal problems in computer vision raise the demand of generating efficient automatic solvers for polynomial equation systems. Given a polynomial system repeated with different coefficient instances, the traditional Gr\"obner basis or normal form based solution is very inefficient. Fortunately the Gr\"obner basis of a same polynomial system with different coefficients is found to share consistent inner structure. By precomputing such structures offline, Gr\"obner basis as well as the polynomial system solutions can be solved automatically and efficiently online. In the past decade, several tools have been released to generate automatic solvers for a general minimal problems. The most recent tool autogen from Larsson et al. is a representative of these tools with state-of-the-art performance in solver efficiency. GAPS wraps and improves autogen with more user-friendly interface, more functionality and better stability. We demonstrate in this report the main approach and enhancement features of GAPS. A short tutorial of the software is also included.



### Quantization of Deep Neural Networks for Accumulator-constrained Processors
- **Arxiv ID**: http://arxiv.org/abs/2004.11783v1
- **DOI**: 10.1016/j.micpro.2019.102872
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.11783v1)
- **Published**: 2020-04-24 14:47:14+00:00
- **Updated**: 2020-04-24 14:47:14+00:00
- **Authors**: Barry de Bruin, Zoran Zivkovic, Henk Corporaal
- **Comment**: 20 pages, 13 figures
- **Journal**: Microprocessors and Microsystems Volume 72, February 2020, 102872
- **Summary**: We introduce an Artificial Neural Network (ANN) quantization methodology for platforms without wide accumulation registers. This enables fixed-point model deployment on embedded compute platforms that are not specifically designed for large kernel computations (i.e. accumulator-constrained processors). We formulate the quantization problem as a function of accumulator size, and aim to maximize the model accuracy by maximizing bit width of input data and weights. To reduce the number of configurations to consider, only solutions that fully utilize the available accumulator bits are being tested. We demonstrate that 16-bit accumulators are able to obtain a classification accuracy within 1\% of the floating-point baselines on the CIFAR-10 and ILSVRC2012 image classification benchmarks. Additionally, a near-optimal $2\times$ speedup is obtained on an ARM processor, by exploiting 16-bit accumulators for image classification on the All-CNN-C and AlexNet networks.



### DPDist : Comparing Point Clouds Using Deep Point Cloud Distance
- **Arxiv ID**: http://arxiv.org/abs/2004.11784v2
- **DOI**: 10.1007/978-3-030-58621-8_32
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.11784v2)
- **Published**: 2020-04-24 14:47:44+00:00
- **Updated**: 2020-07-23 07:46:09+00:00
- **Authors**: Dahlia Urbach, Yizhak Ben-Shabat, Michael Lindenbaum
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a new deep learning method for point cloud comparison. Our approach, named Deep Point Cloud Distance (DPDist), measures the distance between the points in one cloud and the estimated surface from which the other point cloud is sampled. The surface is estimated locally and efficiently using the 3D modified Fisher vector representation. The local representation reduces the complexity of the surface, enabling efficient and effective learning, which generalizes well between object categories. We test the proposed distance in challenging tasks, such as similar object comparison and registration, and show that it provides significant improvements over commonly used distances such as Chamfer distance, Earth mover's distance, and others.



### A Cascaded Learning Strategy for Robust COVID-19 Pneumonia Chest X-Ray Screening
- **Arxiv ID**: http://arxiv.org/abs/2004.12786v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.12786v2)
- **Published**: 2020-04-24 15:44:51+00:00
- **Updated**: 2020-04-30 09:46:13+00:00
- **Authors**: Chun-Fu Yeh, Hsien-Tzu Cheng, Andy Wei, Hsin-Ming Chen, Po-Chen Kuo, Keng-Chi Liu, Mong-Chi Ko, Ray-Jade Chen, Po-Chang Lee, Jen-Hsiang Chuang, Chi-Mai Chen, Yi-Chang Chen, Wen-Jeng Lee, Ning Chien, Jo-Yu Chen, Yu-Sen Huang, Yu-Chien Chang, Yu-Cheng Huang, Nai-Kuan Chou, Kuan-Hua Chao, Yi-Chin Tu, Yeun-Chung Chang, Tyng-Luh Liu
- **Comment**: 14 pages, 6 figures
- **Journal**: None
- **Summary**: We introduce a comprehensive screening platform for the COVID-19 (a.k.a., SARS-CoV-2) pneumonia. The proposed AI-based system works on chest x-ray (CXR) images to predict whether a patient is infected with the COVID-19 disease. Although the recent international joint effort on making the availability of all sorts of open data, the public collection of CXR images is still relatively small for reliably training a deep neural network (DNN) to carry out COVID-19 prediction. To better address such inefficiency, we design a cascaded learning strategy to improve both the sensitivity and the specificity of the resulting DNN classification model. Our approach leverages a large CXR image dataset of non-COVID-19 pneumonia to generalize the original well-trained classification model via a cascaded learning scheme. The resulting screening system is shown to achieve good classification performance on the expanded dataset, including those newly added COVID-19 CXR images.



### Deep Interleaved Network for Image Super-Resolution With Asymmetric Co-Attention
- **Arxiv ID**: http://arxiv.org/abs/2004.11814v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.11814v1)
- **Published**: 2020-04-24 15:49:18+00:00
- **Updated**: 2020-04-24 15:49:18+00:00
- **Authors**: Feng Li, Runming Cong, Huihui Bai, Yifan He
- **Comment**: Accepted by the IJCAI-PRICAI 2020
- **Journal**: None
- **Summary**: Recently, Convolutional Neural Networks (CNN) based image super-resolution (SR) have shown significant success in the literature. However, these methods are implemented as single-path stream to enrich feature maps from the input for the final prediction, which fail to fully incorporate former low-level features into later high-level features. In this paper, to tackle this problem, we propose a deep interleaved network (DIN) to learn how information at different states should be combined for image SR where shallow information guides deep representative features prediction. Our DIN follows a multi-branch pattern allowing multiple interconnected branches to interleave and fuse at different states. Besides, the asymmetric co-attention (AsyCA) is proposed and attacked to the interleaved nodes to adaptively emphasize informative features from different states and improve the discriminative ability of networks. Extensive experiments demonstrate the superiority of our proposed DIN in comparison with the state-of-the-art SR methods.



### Detecting Unsigned Physical Road Incidents from Driver-View Images
- **Arxiv ID**: http://arxiv.org/abs/2004.11824v1
- **DOI**: 10.1109/TIV.2020.2991963
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.11824v1)
- **Published**: 2020-04-24 16:02:17+00:00
- **Updated**: 2020-04-24 16:02:17+00:00
- **Authors**: Alex Levering, Martin Tomko, Devis Tuia, Kourosh Khoshelham
- **Comment**: Preprint to T-IV paper
- **Journal**: None
- **Summary**: Safety on roads is of uttermost importance, especially in the context of autonomous vehicles. A critical need is to detect and communicate disruptive incidents early and effectively. In this paper we propose a system based on an off-the-shelf deep neural network architecture that is able to detect and recognize types of unsigned (non-placarded, such as traffic signs), physical (visible in images) road incidents. We develop a taxonomy for unsigned physical incidents to provide a means of organizing and grouping related incidents. After selecting eight target types of incidents, we collect a dataset of twelve thousand images gathered from publicly-available web sources. We subsequently fine-tune a convolutional neural network to recognize the eight types of road incidents. The proposed model is able to recognize incidents with a high level of accuracy (higher than 90%). We further show that while our system generalizes well across spatial context by training a classifier on geostratified data in the United Kingdom (with an accuracy of over 90%), the translation to visually less similar environments requires spatially distributed data collection.   Note: this is a pre-print version of work accepted in IEEE Transactions on Intelligent Vehicles (T-IV;in press). The paper is currently in production, and the DOI link will be added soon.



### DFUC2020: Analysis Towards Diabetic Foot Ulcer Detection
- **Arxiv ID**: http://arxiv.org/abs/2004.11853v3
- **DOI**: 10.17925/EE.2021.1.1.5
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.11853v3)
- **Published**: 2020-04-24 16:56:48+00:00
- **Updated**: 2021-05-24 12:20:32+00:00
- **Authors**: Bill Cassidy, Neil D. Reeves, Pappachan Joseph, David Gillespie, Claire O'Shea, Satyan Rajbhandari, Arun G. Maiya, Eibe Frank, Andrew Boulton, David Armstrong, Bijan Najafi, Justina Wu, Moi Hoon Yap
- **Comment**: 16 pages, 8 figures
- **Journal**: touchREVIEWS in Endocrinology, 17(1):5-11 (2021)
- **Summary**: Every 20 seconds, a limb is amputated somewhere in the world due to diabetes. This is a global health problem that requires a global solution. The MICCAI challenge discussed in this paper, which concerns the automated detection of diabetic foot ulcers using machine learning techniques, will accelerate the development of innovative healthcare technology to address this unmet medical need. In an effort to improve patient care and reduce the strain on healthcare systems, recent research has focused on the creation of cloud-based detection algorithms. These can be consumed as a service by a mobile app that patients (or a carer, partner or family member) could use themselves at home to monitor their condition and to detect the appearance of a diabetic foot ulcer (DFU). Collaborative work between Manchester Metropolitan University, Lancashire Teaching Hospital and the Manchester University NHS Foundation Trust has created a repository of 4,000 DFU images for the purpose of supporting research toward more advanced methods of DFU detection. Based on a joint effort involving the lead scientists of the UK, US, India and New Zealand, this challenge will solicit original work, and promote interactions between researchers and interdisciplinary collaborations. This paper presents a dataset description and analysis, assessment methods, benchmark algorithms and initial evaluation results. It facilitates the challenge by providing useful insights into state-of-the-art and ongoing research. This grand challenge takes on even greater urgency in a peri and post-pandemic period, where stresses on resource utilization will increase the need for technology that allows people to remain active, healthy and intact in their home.



### Learning Gaussian Maps for Dense Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2004.11855v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.11855v2)
- **Published**: 2020-04-24 17:01:25+00:00
- **Updated**: 2020-04-30 09:51:10+00:00
- **Authors**: Sonaal Kant
- **Comment**: None
- **Journal**: None
- **Summary**: Object detection is a famous branch of research in computer vision, many state of the art object detection algorithms have been introduced in the recent past, but how good are those object detectors when it comes to dense object detection? In this paper we review common and highly accurate object detection methods on the scenes where numerous similar looking objects are placed in close proximity with each other. We also show that, multi-task learning of gaussian maps along with classification and bounding box regression gives us a significant boost in accuracy over the baseline. We introduce Gaussian Layer and Gaussian Decoder in the existing RetinaNet network for better accuracy in dense scenes, with the same computational cost as the RetinaNet. We show the gain of 6\% and 5\% in mAP with respect to baseline RetinaNet. Our method also achieves the state of the art accuracy on the SKU110K \cite{sku110k} dataset.



### Facial Action Unit Detection on ICU Data for Pain Assessment
- **Arxiv ID**: http://arxiv.org/abs/2005.02121v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.02121v1)
- **Published**: 2020-04-24 17:12:56+00:00
- **Updated**: 2020-04-24 17:12:56+00:00
- **Authors**: Subhash Nerella, Azra Bihorac, Patrick Tighe, Parisa Rashidi
- **Comment**: 4 tables
- **Journal**: None
- **Summary**: Current day pain assessment methods rely on patient self-report or by an observer like the Intensive Care Unit (ICU) nurses. Patient self-report is subjective to the individual and suffers due to poor recall. Pain assessment by manual observation is limited by the number of administrations per day and staff workload. Previous studies showed the feasibility of automatic pain assessment by detecting Facial Action Units (AUs). Pain is observed to be associated with certain facial action units (AUs). This method of pain assessment can overcome the pitfalls of present-day pain assessment techniques. All the previous studies are limited to controlled environment data. In this study, we evaluated the performance of OpenFace an open-source facial behavior analysis tool and AU R-CNN on the real-world ICU data. Presence of assisted breathing devices, variable lighting of ICUs, patient orientation with respect to camera significantly affected the performance of the models, although these showed the state-of-the-art results in facial behavior analysis tasks. In this study, we show the need for automated pain assessment system which is trained on real-world ICU data for clinically acceptable pain assessment system.



### MoVie: Revisiting Modulated Convolutions for Visual Counting and Beyond
- **Arxiv ID**: http://arxiv.org/abs/2004.11883v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.11883v3)
- **Published**: 2020-04-24 17:49:56+00:00
- **Updated**: 2020-10-07 06:50:48+00:00
- **Authors**: Duy-Kien Nguyen, Vedanuj Goswami, Xinlei Chen
- **Comment**: None
- **Journal**: None
- **Summary**: This paper focuses on visual counting, which aims to predict the number of occurrences given a natural image and a query (e.g. a question or a category). Unlike most prior works that use explicit, symbolic models which can be computationally expensive and limited in generalization, we propose a simple and effective alternative by revisiting modulated convolutions that fuse the query and the image locally. Following the design of residual bottleneck, we call our method MoVie, short for Modulated conVolutional bottlenecks. Notably, MoVie reasons implicitly and holistically and only needs a single forward-pass during inference. Nevertheless, MoVie showcases strong performance for counting: 1) advancing the state-of-the-art on counting-specific VQA tasks while being more efficient; 2) outperforming prior-art on difficult benchmarks like COCO for common object counting; 3) helped us secure the first place of 2020 VQA challenge when integrated as a module for 'number' related questions in generic VQA models. Finally, we show evidence that modulated convolutions such as MoVie can serve as a general mechanism for reasoning tasks beyond counting.



### The Plant Pathology 2020 challenge dataset to classify foliar disease of apples
- **Arxiv ID**: http://arxiv.org/abs/2004.11958v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, eess.IV, I.2.1; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2004.11958v1)
- **Published**: 2020-04-24 19:36:37+00:00
- **Updated**: 2020-04-24 19:36:37+00:00
- **Authors**: Ranjita Thapa, Noah Snavely, Serge Belongie, Awais Khan
- **Comment**: 11 pages, 5 figures, Kaggle competition website:
  https://www.kaggle.com/c/plant-pathology-2020-fgvc7, CVPR fine-grained visual
  categorization website: https://sites.google.com/view/fgvc7/competitions
- **Journal**: None
- **Summary**: Apple orchards in the U.S. are under constant threat from a large number of pathogens and insects. Appropriate and timely deployment of disease management depends on early disease detection. Incorrect and delayed diagnosis can result in either excessive or inadequate use of chemicals, with increased production costs, environmental, and health impacts. We have manually captured 3,651 high-quality, real-life symptom images of multiple apple foliar diseases, with variable illumination, angles, surfaces, and noise. A subset, expert-annotated to create a pilot dataset for apple scab, cedar apple rust, and healthy leaves, was made available to the Kaggle community for 'Plant Pathology Challenge'; part of the Fine-Grained Visual Categorization (FGVC) workshop at CVPR 2020 (Computer Vision and Pattern Recognition). We also trained an off-the-shelf convolutional neural network (CNN) on this data for disease classification and achieved 97% accuracy on a held-out test set. This dataset will contribute towards development and deployment of machine learning-based automated plant disease classification algorithms to ultimately realize fast and accurate disease detection. We will continue to add images to the pilot dataset for a larger, more comprehensive expert-annotated dataset for future Kaggle competitions and to explore more advanced methods for disease classification and quantification.



### DeepMerge: Classifying High-redshift Merging Galaxies with Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2004.11981v1
- **DOI**: None
- **Categories**: **astro-ph.GA**, astro-ph.IM, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.11981v1)
- **Published**: 2020-04-24 20:36:06+00:00
- **Updated**: 2020-04-24 20:36:06+00:00
- **Authors**: A. Ćiprijanović, G. F. Snyder, B. Nord, J. E. G. Peek
- **Comment**: 17 pages, 8 figures, submitted to Astronomy & Computing
- **Journal**: None
- **Summary**: We investigate and demonstrate the use of convolutional neural networks (CNNs) for the task of distinguishing between merging and non-merging galaxies in simulated images, and for the first time at high redshifts (i.e. $z=2$). We extract images of merging and non-merging galaxies from the Illustris-1 cosmological simulation and apply observational and experimental noise that mimics that from the Hubble Space Telescope; the data without noise form a "pristine" data set and that with noise form a "noisy" data set. The test set classification accuracy of the CNN is $79\%$ for pristine and $76\%$ for noisy. The CNN outperforms a Random Forest classifier, which was shown to be superior to conventional one- or two-dimensional statistical methods (Concentration, Asymmetry, the Gini, $M_{20}$ statistics etc.), which are commonly used when classifying merging galaxies. We also investigate the selection effects of the classifier with respect to merger state and star formation rate, finding no bias. Finally, we extract Grad-CAMs (Gradient-weighted Class Activation Mapping) from the results to further assess and interrogate the fidelity of the classification model.



### Spectral Data Augmentation Techniques to quantify Lung Pathology from CT-images
- **Arxiv ID**: http://arxiv.org/abs/2004.11989v1
- **DOI**: 10.1109/ISBI45749.2020.9098581
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.11989v1)
- **Published**: 2020-04-24 20:57:50+00:00
- **Updated**: 2020-04-24 20:57:50+00:00
- **Authors**: Subhradeep Kayal, Florian Dubost, Harm A. W. M. Tiddens, Marleen de Bruijne
- **Comment**: 5 pages including references, accepted as Oral presentation at IEEE
  ISBI 2020
- **Journal**: None
- **Summary**: Data augmentation is of paramount importance in biomedical image processing tasks, characterized by inadequate amounts of labelled data, to best use all of the data that is present. In-use techniques range from intensity transformations and elastic deformations, to linearly combining existing data points to make new ones. In this work, we propose the use of spectral techniques for data augmentation, using the discrete cosine and wavelet transforms. We empirically evaluate our approaches on a CT texture analysis task to detect abnormal lung-tissue in patients with cystic fibrosis. Empirical experiments show that the proposed spectral methods perform favourably as compared to the existing methods. When used in combination with existing methods, our proposed approach can increase the relative minor class segmentation performance by 44.1% over a simple replication baseline.



### Extending and Analyzing Self-Supervised Learning Across Domains
- **Arxiv ID**: http://arxiv.org/abs/2004.11992v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.11992v2)
- **Published**: 2020-04-24 21:18:02+00:00
- **Updated**: 2020-08-17 16:13:46+00:00
- **Authors**: Bram Wallace, Bharath Hariharan
- **Comment**: None
- **Journal**: None
- **Summary**: Self-supervised representation learning has achieved impressive results in recent years, with experiments primarily coming on ImageNet or other similarly large internet imagery datasets. There has been little to no work with these methods on other smaller domains, such as satellite, textural, or biological imagery. We experiment with several popular methods on an unprecedented variety of domains. We discover, among other findings, that Rotation is by far the most semantically meaningful task, with much of the performance of Jigsaw and Instance Discrimination being attributable to the nature of their induced distribution rather than semantic understanding. Additionally, there are several areas, such as fine-grain classification, where all tasks underperform. We quantitatively and qualitatively diagnose the reasons for these failures and successes via novel experiments studying pretext generalization, random labelings, and implicit dimensionality. Code and models are available at https://github.com/BramSW/Extending_SSRL_Across_Domains/.



### Explicit Domain Adaptation with Loosely Coupled Samples
- **Arxiv ID**: http://arxiv.org/abs/2004.11995v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.11995v1)
- **Published**: 2020-04-24 21:23:45+00:00
- **Updated**: 2020-04-24 21:23:45+00:00
- **Authors**: Oliver Scheel, Loren Schwarz, Nassir Navab, Federico Tombari
- **Comment**: Submitted to IROS 2020
- **Journal**: None
- **Summary**: Transfer learning is an important field of machine learning in general, and particularly in the context of fully autonomous driving, which needs to be solved simultaneously for many different domains, such as changing weather conditions and country-specific driving behaviors. Traditional transfer learning methods often focus on image data and are black-box models. In this work we propose a transfer learning framework, core of which is learning an explicit mapping between domains. Due to its interpretability, this is beneficial for safety-critical applications, like autonomous driving. We show its general applicability by considering image classification problems and then move on to time-series data, particularly predicting lane changes. In our evaluation we adapt a pre-trained model to a dataset exhibiting different driving and sensory characteristics.



### Neural Head Reenactment with Latent Pose Descriptors
- **Arxiv ID**: http://arxiv.org/abs/2004.12000v2
- **DOI**: 10.1109/CVPR42600.2020.01380
- **Categories**: **cs.CV**, cs.LG, I.2.10; I.3.3; I.4.7; I.4.10
- **Links**: [PDF](http://arxiv.org/pdf/2004.12000v2)
- **Published**: 2020-04-24 21:37:52+00:00
- **Updated**: 2020-10-30 13:37:15+00:00
- **Authors**: Egor Burkov, Igor Pasechnik, Artur Grigorev, Victor Lempitsky
- **Comment**: None
- **Journal**: 2020 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR). pp. 13783-13792
- **Summary**: We propose a neural head reenactment system, which is driven by a latent pose representation and is capable of predicting the foreground segmentation alongside the RGB image. The latent pose representation is learned as a part of the entire reenactment system, and the learning process is based solely on image reconstruction losses. We show that despite its simplicity, with a large and diverse enough training dataset, such learning successfully decomposes pose from identity. The resulting system can then reproduce mimics of the driving person and, furthermore, can perform cross-person reenactment. Additionally, we show that the learned descriptors are useful for other pose-related tasks, such as keypoint prediction and pose-based retrieval.



### ML-driven Malware that Targets AV Safety
- **Arxiv ID**: http://arxiv.org/abs/2004.13004v2
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2004.13004v2)
- **Published**: 2020-04-24 22:29:59+00:00
- **Updated**: 2020-06-13 01:42:56+00:00
- **Authors**: Saurabh Jha, Shengkun Cui, Subho S. Banerjee, Timothy Tsai, Zbigniew Kalbarczyk, Ravi Iyer
- **Comment**: Accepted for DSN 2020
- **Journal**: 2020 50th Annual IEEE/IFIP International Conference on Dependable
  Systems and Networks
- **Summary**: Ensuring the safety of autonomous vehicles (AVs) is critical for their mass deployment and public adoption. However, security attacks that violate safety constraints and cause accidents are a significant deterrent to achieving public trust in AVs, and that hinders a vendor's ability to deploy AVs. Creating a security hazard that results in a severe safety compromise (for example, an accident) is compelling from an attacker's perspective. In this paper, we introduce an attack model, a method to deploy the attack in the form of smart malware, and an experimental evaluation of its impact on production-grade autonomous driving software. We find that determining the time interval during which to launch the attack is{ critically} important for causing safety hazards (such as collisions) with a high degree of success. For example, the smart malware caused 33X more forced emergency braking than random attacks did, and accidents in 52.6% of the driving simulations.



