# Arxiv Papers in cs.CV on 2020-04-18
### BReG-NeXt: Facial Affect Computing Using Adaptive Residual Networks With Bounded Gradient
- **Arxiv ID**: http://arxiv.org/abs/2004.08495v1
- **DOI**: 10.1109/TAFFC.2020.2986440
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.08495v1)
- **Published**: 2020-04-18 00:26:36+00:00
- **Updated**: 2020-04-18 00:26:36+00:00
- **Authors**: Behzad Hasani, Pooran Singh Negi, Mohammad H. Mahoor
- **Comment**: To appear in IEEE Transactions on Affective Computing journal
- **Journal**: 2020 IEEE Transactions on Affective Computing
- **Summary**: This paper introduces BReG-NeXt, a residual-based network architecture using a function wtih bounded derivative instead of a simple shortcut path (a.k.a. identity mapping) in the residual units for automatic recognition of facial expressions based on the categorical and dimensional models of affect. Compared to ResNet, our proposed adaptive complex mapping results in a shallower network with less numbers of training parameters and floating point operations per second (FLOPs). Adding trainable parameters to the bypass function further improves fitting and training the network and hence recognizing subtle facial expressions such as contempt with a higher accuracy. We conducted comprehensive experiments on the categorical and dimensional models of affect on the challenging in-the-wild databases of AffectNet, FER2013, and Affect-in-Wild. Our experimental results show that our adaptive complex mapping approach outperforms the original ResNet consisting of a simple identity mapping as well as other state-of-the-art methods for Facial Expression Recognition (FER). Various metrics are reported in both affect models to provide a comprehensive evaluation of our method. In the categorical model, BReG-NeXt-50 with only 3.1M training parameters and 15 MFLOPs, achieves 68.50% and 71.53% accuracy on AffectNet and FER2013 databases, respectively. In the dimensional model, BReG-NeXt achieves 0.2577 and 0.2882 RMSE value on AffectNet and Affect-in-Wild databases, respectively.



### Finding Berries: Segmentation and Counting of Cranberries using Point Supervision and Shape Priors
- **Arxiv ID**: http://arxiv.org/abs/2004.08501v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.08501v2)
- **Published**: 2020-04-18 01:08:57+00:00
- **Updated**: 2020-04-24 16:22:23+00:00
- **Authors**: Peri Akiva, Kristin Dana, Peter Oudemans, Michael Mars
- **Comment**: to be published in proceeding of CVPR 2020 in the Agriculture Vision
  Workshop
- **Journal**: None
- **Summary**: Precision agriculture has become a key factor for increasing crop yields by providing essential information to decision makers. In this work, we present a deep learning method for simultaneous segmentation and counting of cranberries to aid in yield estimation and sun exposure predictions. Notably, supervision is done using low cost center point annotations. The approach, named Triple-S Network, incorporates a three-part loss with shape priors to promote better fitting to objects of known shape typical in agricultural scenes. Our results improve overall segmentation performance by more than 6.74% and counting results by 22.91% when compared to state-of-the-art. To train and evaluate the network, we have collected the CRanberry Aerial Imagery Dataset (CRAID), the largest dataset of aerial drone imagery from cranberry fields. This dataset will be made publicly available.



### ImagePairs: Realistic Super Resolution Dataset via Beam Splitter Camera Rig
- **Arxiv ID**: http://arxiv.org/abs/2004.08513v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.08513v1)
- **Published**: 2020-04-18 03:06:41+00:00
- **Updated**: 2020-04-18 03:06:41+00:00
- **Authors**: Hamid Reza Vaezi Joze, Ilya Zharkov, Karlton Powell, Carl Ringler, Luming Liang, Andy Roulston, Moshe Lutz, Vivek Pradeep
- **Comment**: None
- **Journal**: The IEEE Conference on Computer Vision and Pattern Recognition
  Workshop (CVPRW), 2020
- **Summary**: Super Resolution is the problem of recovering a high-resolution image from a single or multiple low-resolution images of the same scene. It is an ill-posed problem since high frequency visual details of the scene are completely lost in low-resolution images. To overcome this, many machine learning approaches have been proposed aiming at training a model to recover the lost details in the new scenes. Such approaches include the recent successful effort in utilizing deep learning techniques to solve super resolution problem. As proven, data itself plays a significant role in the machine learning process especially deep learning approaches which are data hungry. Therefore, to solve the problem, the process of gathering data and its formation could be equally as vital as the machine learning technique used. Herein, we are proposing a new data acquisition technique for gathering real image data set which could be used as an input for super resolution, noise cancellation and quality enhancement techniques. We use a beam-splitter to capture the same scene by a low resolution camera and a high resolution camera. Since we also release the raw images, this large-scale dataset could be used for other tasks such as ISP generation. Unlike current small-scale dataset used for these tasks, our proposed dataset includes 11,421 pairs of low-resolution high-resolution images of diverse scenes. To our knowledge this is the most complete dataset for super resolution, ISP and image quality enhancement. The benchmarking result shows how the new dataset can be successfully used to significantly improve the quality of real-world image super resolution.



### DMT: Dynamic Mutual Training for Semi-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2004.08514v4
- **DOI**: 10.1016/j.patcog.2022.108777
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.08514v4)
- **Published**: 2020-04-18 03:12:55+00:00
- **Updated**: 2022-05-11 10:23:22+00:00
- **Authors**: Zhengyang Feng, Qianyu Zhou, Qiqi Gu, Xin Tan, Guangliang Cheng, Xuequan Lu, Jianping Shi, Lizhuang Ma
- **Comment**: Published at Pattern Recognition, see
  https://www.sciencedirect.com/science/article/abs/pii/S0031320322002588
- **Journal**: None
- **Summary**: Recent semi-supervised learning methods use pseudo supervision as core idea, especially self-training methods that generate pseudo labels. However, pseudo labels are unreliable. Self-training methods usually rely on single model prediction confidence to filter low-confidence pseudo labels, thus remaining high-confidence errors and wasting many low-confidence correct labels. In this paper, we point out it is difficult for a model to counter its own errors. Instead, leveraging inter-model disagreement between different models is a key to locate pseudo label errors. With this new viewpoint, we propose mutual training between two different models by a dynamically re-weighted loss function, called Dynamic Mutual Training (DMT). We quantify inter-model disagreement by comparing predictions from two different models to dynamically re-weight loss in training, where a larger disagreement indicates a possible error and corresponds to a lower loss value. Extensive experiments show that DMT achieves state-of-the-art performance in both image classification and semantic segmentation. Our codes are released at https://github.com/voldemortX/DST-CBC .



### JL-DCF: Joint Learning and Densely-Cooperative Fusion Framework for RGB-D Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2004.08515v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.08515v1)
- **Published**: 2020-04-18 03:22:40+00:00
- **Updated**: 2020-04-18 03:22:40+00:00
- **Authors**: Keren Fu, Deng-Ping Fan, Ge-Peng Ji, Qijun Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a novel joint learning and densely-cooperative fusion (JL-DCF) architecture for RGB-D salient object detection. Existing models usually treat RGB and depth as independent information and design separate networks for feature extraction from each. Such schemes can easily be constrained by a limited amount of training data or over-reliance on an elaborately-designed training process. In contrast, our JL-DCF learns from both RGB and depth inputs through a Siamese network. To this end, we propose two effective components: joint learning (JL), and densely-cooperative fusion (DCF). The JL module provides robust saliency feature learning, while the latter is introduced for complementary feature discovery. Comprehensive experiments on four popular metrics show that the designed framework yields a robust RGB-D saliency detector with good generalization. As a result, JL-DCF significantly advances the top-1 D3Net model by an average of ~1.9% (S-measure) across six challenging datasets, showing that the proposed framework offers a potential solution for real-world applications and could provide more insight into the cross-modality complementarity task. The code will be available at https://github.com/kerenfu/JLDCF/.



### Super-Resolution-based Snake Model -- An Unsupervised Method for Large-Scale Building Extraction using Airborne LiDAR Data and Optical Image
- **Arxiv ID**: http://arxiv.org/abs/2004.08522v1
- **DOI**: 10.3390/rs12111702
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.08522v1)
- **Published**: 2020-04-18 05:00:07+00:00
- **Updated**: 2020-04-18 05:00:07+00:00
- **Authors**: Thanh Huy Nguyen, Sylvie Daniel, Didier Gueriot, Christophe Sintes, Jean-Marc Le Caillec
- **Comment**: 30 pages, 15 figures. Submitted to the MDPI Remote Sensing
- **Journal**: Remote Sensing 2020, 12(11), 1702
- **Summary**: Automatic extraction of buildings in urban and residential scenes has become a subject of growing interest in the domain of photogrammetry and remote sensing, particularly since mid-1990s. Active contour model, colloquially known as snake model, has been studied to extract buildings from aerial and satellite imagery. However, this task is still very challenging due to the complexity of building size, shape, and its surrounding environment. This complexity leads to a major obstacle for carrying out a reliable large-scale building extraction, since the involved prior information and assumptions on building such as shape, size, and color cannot be generalized over large areas. This paper presents an efficient snake model to overcome such challenge, called Super-Resolution-based Snake Model (SRSM). The SRSM operates on high-resolution LiDAR-based elevation images -- called z-images -- generated by a super-resolution process applied to LiDAR data. The involved balloon force model is also improved to shrink or inflate adaptively, instead of inflating the snake continuously. This method is applicable for a large scale such as city scale and even larger, while having a high level of automation and not requiring any prior knowledge nor training data from the urban scenes (hence unsupervised). It achieves high overall accuracy when tested on various datasets. For instance, the proposed SRSM yields an average area-based Quality of 86.57% and object-based Quality of 81.60% on the ISPRS Vaihingen benchmark datasets. Compared to other methods using this benchmark dataset, this level of accuracy is highly desirable even for a supervised method. Similarly desirable outcomes are obtained when carrying out the proposed SRSM on the whole City of Quebec (total area of 656 km2), yielding an area-based Quality of 62.37% and an object-based Quality of 63.21%.



### Effect of Text Color on Word Embeddings
- **Arxiv ID**: http://arxiv.org/abs/2004.08526v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.08526v1)
- **Published**: 2020-04-18 05:14:18+00:00
- **Updated**: 2020-04-18 05:14:18+00:00
- **Authors**: Masaya Ikoma, Brian Kenji Iwana, Seiichi Uchida
- **Comment**: to appear at the 14th International Workshop on Document Analysis
  Systems (DAS) 2020
- **Journal**: None
- **Summary**: In natural scenes and documents, we can find the correlation between a text and its color. For instance, the word, "hot", is often printed in red, while "cold" is often in blue. This correlation can be thought of as a feature that represents the semantic difference between the words. Based on this observation, we propose the idea of using text color for word embeddings. While text-only word embeddings (e.g. word2vec) have been extremely successful, they often represent antonyms as similar since they are often interchangeable in sentences. In this paper, we try two tasks to verify the usefulness of text color in understanding the meanings of words, especially in identifying synonyms and antonyms. First, we quantify the color distribution of words from the book cover images and analyze the correlation between the color and meaning of the word. Second, we try to retrain word embeddings with the color distribution of words as a constraint. By observing the changes in the word embeddings of synonyms and antonyms before and after re-training, we aim to understand the kind of words that have positive or negative effects in their word embeddings when incorporating text color information.



### Moire Image Restoration using Multi Level Hyper Vision Net
- **Arxiv ID**: http://arxiv.org/abs/2004.08541v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.08541v1)
- **Published**: 2020-04-18 06:54:54+00:00
- **Updated**: 2020-04-18 06:54:54+00:00
- **Authors**: D. Sabari Nathan, M. Parisa Beham, S. M. Md Mansoor Roomi
- **Comment**: 8 pages , 5 figures
- **Journal**: None
- **Summary**: A moire pattern in the images is resulting from high frequency patterns captured by the image sensor (colour filter array) that appear after demosaicing. These Moire patterns would appear in natural images of scenes with high frequency content. The Moire pattern can also vary intensely due to a minimal change in the camera direction/positioning. Thus the Moire pattern depreciates the quality of photographs. An important issue in demoireing pattern is that the Moireing patterns have dynamic structure with varying colors and forms. These challenges makes the demoireing more difficult than many other image restoration tasks. Inspired by these challenges in demoireing, a multilevel hyper vision net is proposed to remove the Moire pattern to improve the quality of the images. As a key aspect, in this network we involved residual channel attention block that can be used to extract and adaptively fuse hierarchical features from all the layers efficiently. The proposed algorithms has been tested with the NTIRE 2020 challenge dataset and thus achieved 36.85 and 0.98 Peak to Signal Noise Ratio (PSNR) and Structural Similarity (SSIM) Index respectively.



### Towards Non-I.I.D. and Invisible Data with FedNAS: Federated Deep Learning via Neural Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/2004.08546v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.DC, cs.MA, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.08546v4)
- **Published**: 2020-04-18 08:04:44+00:00
- **Updated**: 2021-01-04 02:18:08+00:00
- **Authors**: Chaoyang He, Murali Annavaram, Salman Avestimehr
- **Comment**: accepted to CVPR 2020 workshop on neural architecture search and
  beyond for representation learning. Code is released at https://fedml.ai
- **Journal**: None
- **Summary**: Federated Learning (FL) has been proved to be an effective learning framework when data cannot be centralized due to privacy, communication costs, and regulatory restrictions. When training deep learning models under an FL setting, people employ the predefined model architecture discovered in the centralized environment. However, this predefined architecture may not be the optimal choice because it may not fit data with non-identical and independent distribution (non-IID). Thus, we advocate automating federated learning (AutoFL) to improve model accuracy and reduce the manual design effort. We specifically study AutoFL via Neural Architecture Search (NAS), which can automate the design process. We propose a Federated NAS (FedNAS) algorithm to help scattered workers collaboratively searching for a better architecture with higher accuracy. We also build a system based on FedNAS. Our experiments on non-IID dataset show that the architecture searched by FedNAS can outperform the manually predefined architecture.



### Color Image Segmentation using Adaptive Particle Swarm Optimization and Fuzzy C-means
- **Arxiv ID**: http://arxiv.org/abs/2004.08547v1
- **DOI**: None
- **Categories**: **cs.CV**, 68U10, I.4.6
- **Links**: [PDF](http://arxiv.org/pdf/2004.08547v1)
- **Published**: 2020-04-18 08:11:33+00:00
- **Updated**: 2020-04-18 08:11:33+00:00
- **Authors**: Narayana Reddy A, Ranjita Das
- **Comment**: 4 pages, 2 figures, Included in conference proceedings of
  "International conference in Recent Trends on Electronics & Computer Science
  (ICRTECS-2019)" organised by National Institute of Technology, Silchar
- **Journal**: None
- **Summary**: Segmentation partitions an image into different regions containing pixels with similar attributes. A standard non-contextual variant of Fuzzy C-means clustering algorithm (FCM), considering its simplicity is generally used in image segmentation. Using FCM has its disadvantages like it is dependent on the initial guess of the number of clusters and highly sensitive to noise. Satisfactory visual segments cannot be obtained using FCM. Particle Swarm Optimization (PSO) belongs to the class of evolutionary algorithms and has good convergence speed and fewer parameters compared to Genetic Algorithms (GAs). An optimized version of PSO can be combined with FCM to act as a proper initializer for the algorithm thereby reducing its sensitivity to initial guess. A hybrid PSO algorithm named Adaptive Particle Swarm Optimization (APSO) which improves in the calculation of various hyper parameters like inertia weight, learning factors over standard PSO, using insights from swarm behaviour, leading to improvement in cluster quality can be used. This paper presents a new image segmentation algorithm called Adaptive Particle Swarm Optimization and Fuzzy C-means Clustering Algorithm (APSOF), which is based on Adaptive Particle Swarm Optimization (APSO) and Fuzzy C-means clustering. Experimental results show that APSOF algorithm has edge over FCM in correctly identifying the optimum cluster centers, there by leading to accurate classification of the image pixels. Hence, APSOF algorithm has superior performance in comparison with classic Particle Swarm Optimization (PSO) and Fuzzy C-means clustering algorithm (FCM) for image segmentation.



### Accurate Tumor Tissue Region Detection with Accelerated Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2004.08552v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.08552v1)
- **Published**: 2020-04-18 08:24:27+00:00
- **Updated**: 2020-04-18 08:24:27+00:00
- **Authors**: Gabriel Tjio, Xulei Yang, Jia Mei Hong, Sum Thai Wong, Vanessa Ding, Andre Choo, Yi Su
- **Comment**: 9 pages, 6 figures, 3 tables
- **Journal**: None
- **Summary**: Manual annotation of pathology slides for cancer diagnosis is laborious and repetitive. Therefore, much effort has been devoted to develop computer vision solutions. Our approach, (FLASH), is based on a Deep Convolutional Neural Network (DCNN) architecture. It reduces computational costs and is faster than typical deep learning approaches by two orders of magnitude, making high throughput processing a possibility. In computer vision approaches using deep learning methods, the input image is subdivided into patches which are separately passed through the neural network. Features extracted from these patches are used by the classifier to annotate the corresponding region. Our approach aggregates all the extracted features into a single matrix before passing them to the classifier. Previously, the features are extracted from overlapping patches. Aggregating the features eliminates the need for processing overlapping patches, which reduces the computations required. DCCN and FLASH demonstrate high sensitivity (~ 0.96), good precision (~0.78) and high F1 scores (~0.84). The average time taken to process each sample for FLASH and DCNN is 96.6 seconds and 9489.20 seconds, respectively. Our approach was approximately 100 times faster than the original DCNN approach while simultaneously preserving high accuracy and precision.



### Realistic Large-Scale Fine-Depth Dehazing Dataset from 3D Videos
- **Arxiv ID**: http://arxiv.org/abs/2004.08554v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.08554v3)
- **Published**: 2020-04-18 08:25:25+00:00
- **Updated**: 2022-12-15 09:09:19+00:00
- **Authors**: Ruoteng Li, Xiaoyi Zhang, Shaodi You, Yu Li
- **Comment**: None
- **Journal**: None
- **Summary**: Image dehazing is one of the important and popular topics in computer vision and machine learning. A reliable real-time dehazing method with reliable performance is highly desired for many applications such as autonomous driving, security surveillance, etc. While recent learning-based methods require datasets containing pairs of hazy images and clean ground truth, it is impossible to capture them in real scenes. Many existing works compromise this difficulty to generate hazy images by rendering the haze from depth on common RGBD datasets using the haze imaging model. However, there is still a gap between the synthetic datasets and real hazy images as large datasets with high-quality depth are mostly indoor and depth maps for outdoor are imprecise. In this paper, we complement the existing datasets with a new, large, and diverse dehazing dataset containing real outdoor scenes from High-Definition (HD) 3D movies. We select a large number of high-quality frames of real outdoor scenes and render haze on them using depth from stereo. Our dataset is clearly more realistic and more diversified with better visual quality than existing ones. More importantly, we demonstrate that using this dataset greatly improves the dehazing performance on real scenes. In addition to the dataset, we also evaluate a series state of the art methods on the proposed benchmarking datasets.



### DriftNet: Aggressive Driving Behavior Classification using 3D EfficientNet Architecture
- **Arxiv ID**: http://arxiv.org/abs/2004.11970v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.11970v1)
- **Published**: 2020-04-18 08:36:04+00:00
- **Updated**: 2020-04-18 08:36:04+00:00
- **Authors**: Alam Noor, Bilel Benjdira, Adel Ammar, Anis Koubaa
- **Comment**: None
- **Journal**: None
- **Summary**: Aggressive driving (i.e., car drifting) is a dangerous behavior that puts human safety and life into a significant risk. This behavior is considered as an anomaly concerning the regular traffic in public transportation roads. Recent techniques in deep learning proposed new approaches for anomaly detection in different contexts such as pedestrian monitoring, street fighting, and threat detection. In this paper, we propose a new anomaly detection framework applied to the detection of aggressive driving behavior. Our contribution consists in the development of a 3D neural network architecture, based on the state-of-the-art EfficientNet 2D image classifier, for the aggressive driving detection in videos. We propose an EfficientNet3D CNN feature extractor for video analysis, and we compare it with existing feature extractors. We also created a dataset of car drifting in Saudi Arabian context https://www.youtube.com/watch?v=vLzgye1-d1k . To the best of our knowledge, this is the first work that addresses the problem of aggressive driving behavior using deep learning.



### On the Synergies between Machine Learning and Binocular Stereo for Depth Estimation from Images: a Survey
- **Arxiv ID**: http://arxiv.org/abs/2004.08566v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.08566v2)
- **Published**: 2020-04-18 09:14:08+00:00
- **Updated**: 2021-03-31 14:00:23+00:00
- **Authors**: Matteo Poggi, Fabio Tosi, Konstantinos Batsos, Philippos Mordohai, Stefano Mattoccia
- **Comment**: Accepted to TPAMI. Paper version of our CVPR 2019 tutorial:
  "Learning-based depth estimation from stereo and monocular images: successes,
  limitations and future challenges"
  (https://sites.google.com/view/cvpr-2019-depth-from-image/home)
- **Journal**: None
- **Summary**: Stereo matching is one of the longest-standing problems in computer vision with close to 40 years of studies and research. Throughout the years the paradigm has shifted from local, pixel-level decision to various forms of discrete and continuous optimization to data-driven, learning-based methods. Recently, the rise of machine learning and the rapid proliferation of deep learning enhanced stereo matching with new exciting trends and applications unthinkable until a few years ago. Interestingly, the relationship between these two worlds is two-way. While machine, and especially deep, learning advanced the state-of-the-art in stereo matching, stereo itself enabled new ground-breaking methodologies such as self-supervised monocular depth estimation based on deep networks. In this paper, we review recent research in the field of learning-based depth estimation from single and binocular images highlighting the synergies, the successes achieved so far and the open challenges the community is going to face in the immediate future.



### Automatic Grading of Knee Osteoarthritis on the Kellgren-Lawrence Scale from Radiographs Using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2004.08572v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.08572v1)
- **Published**: 2020-04-18 09:46:55+00:00
- **Updated**: 2020-04-18 09:46:55+00:00
- **Authors**: Sudeep Kondal, Viraj Kulkarni, Ashrika Gaikwad, Amit Kharat, Aniruddha Pant
- **Comment**: 5 pages, 3 figures, 5 tables
- **Journal**: None
- **Summary**: The severity of knee osteoarthritis is graded using the 5-point Kellgren-Lawrence (KL) scale where healthy knees are assigned grade 0, and the subsequent grades 1-4 represent increasing severity of the affliction. Although several methods have been proposed in recent years to develop models that can automatically predict the KL grade from a given radiograph, most models have been developed and evaluated on datasets not sourced from India. These models fail to perform well on the radiographs of Indian patients. In this paper, we propose a novel method using convolutional neural networks to automatically grade knee radiographs on the KL scale. Our method works in two connected stages: in the first stage, an object detection model segments individual knees from the rest of the image; in the second stage, a regression model automatically grades each knee separately on the KL scale. We train our model using the publicly available Osteoarthritis Initiative (OAI) dataset and demonstrate that fine-tuning the model before evaluating it on a dataset from a private hospital significantly improves the mean absolute error from 1.09 (95% CI: 1.03-1.15) to 0.28 (95% CI: 0.25-0.32). Additionally, we compare classification and regression models built for the same task and demonstrate that regression outperforms classification.



### Self-Supervised Representation Learning on Document Images
- **Arxiv ID**: http://arxiv.org/abs/2004.10605v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML, 68T05
- **Links**: [PDF](http://arxiv.org/pdf/2004.10605v2)
- **Published**: 2020-04-18 10:14:06+00:00
- **Updated**: 2020-05-27 08:48:48+00:00
- **Authors**: Adrian Cosma, Mihai Ghidoveanu, Michael Panaitescu-Liess, Marius Popescu
- **Comment**: 15 pages, 5 figures. Accepted at DAS 2020: IAPR International
  Workshop on Document Analysis Systems
- **Journal**: None
- **Summary**: This work analyses the impact of self-supervised pre-training on document images in the context of document image classification. While previous approaches explore the effect of self-supervision on natural images, we show that patch-based pre-training performs poorly on document images because of their different structural properties and poor intra-sample semantic information. We propose two context-aware alternatives to improve performance on the Tobacco-3482 image classification task. We also propose a novel method for self-supervision, which makes use of the inherent multi-modality of documents (image and text), which performs better than other popular self-supervised methods, including supervised ImageNet pre-training, on document image classification scenarios with a limited amount of data.



### BiFNet: Bidirectional Fusion Network for Road Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2004.08582v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.08582v1)
- **Published**: 2020-04-18 10:24:43+00:00
- **Updated**: 2020-04-18 10:24:43+00:00
- **Authors**: Haoran Li, Yaran Chen, Qichao Zhang, Dongbin Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-sensor fusion-based road segmentation plays an important role in the intelligent driving system since it provides a drivable area. The existing mainstream fusion method is mainly to feature fusion in the image space domain which causes the perspective compression of the road and damages the performance of the distant road. Considering the bird's eye views(BEV) of the LiDAR remains the space structure in horizontal plane, this paper proposes a bidirectional fusion network(BiFNet) to fuse the image and BEV of the point cloud. The network consists of two modules: 1) Dense space transformation module, which solves the mutual conversion between camera image space and BEV space. 2) Context-based feature fusion module, which fuses the different sensors information based on the scenes from corresponding features.This method has achieved competitive results on KITTI dataset.



### Dynamic Feature Integration for Simultaneous Detection of Salient Object, Edge and Skeleton
- **Arxiv ID**: http://arxiv.org/abs/2004.08595v1
- **DOI**: 10.1109/TIP.2020.3017352
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.08595v1)
- **Published**: 2020-04-18 11:10:11+00:00
- **Updated**: 2020-04-18 11:10:11+00:00
- **Authors**: Jiang-Jiang Liu, Qibin Hou, Ming-Ming Cheng
- **Comment**: None
- **Journal**: IEEE TIP 2020
- **Summary**: In this paper, we solve three low-level pixel-wise vision problems, including salient object segmentation, edge detection, and skeleton extraction, within a unified framework. We first show some similarities shared by these tasks and then demonstrate how they can be leveraged for developing a unified framework that can be trained end-to-end. In particular, we introduce a selective integration module that allows each task to dynamically choose features at different levels from the shared backbone based on its own characteristics. Furthermore, we design a task-adaptive attention module, aiming at intelligently allocating information for different tasks according to the image content priors. To evaluate the performance of our proposed network on these tasks, we conduct exhaustive experiments on multiple representative datasets. We will show that though these tasks are naturally quite different, our network can work well on all of them and even perform better than current single-purpose state-of-the-art methods. In addition, we also conduct adequate ablation analyses that provide a full understanding of the design principles of the proposed framework. To facilitate future research, source code will be released.



### DAPnet: A Double Self-attention Convolutional Network for Point Cloud Semantic Labeling
- **Arxiv ID**: http://arxiv.org/abs/2004.08596v2
- **DOI**: 10.1109/JSTARS.2021.3113047
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.08596v2)
- **Published**: 2020-04-18 11:14:28+00:00
- **Updated**: 2021-09-15 07:20:14+00:00
- **Authors**: Li Chen, Zewei Xu, Yongjian Fu, Haozhe Huang, Shaowen Wang, Haifeng Li
- **Comment**: 12 pages, 7 figures
- **Journal**: IEEE Journal of Selected Topics in Applied Earth Observations and
  Remote Sensing, 2021
- **Summary**: Airborne Laser Scanning (ALS) point clouds have complex structures, and their 3D semantic labeling has been a challenging task. It has three problems: (1) the difficulty of classifying point clouds around boundaries of objects from different classes, (2) the diversity of shapes within the same class, and (3) the scale differences between classes. In this study, we propose a novel double self-attention convolutional network called the DAPnet. The double self-attention includes the point attention module (PAM) and the group attention module (GAM). For problem (1), the PAM can effectively assign different weights based on the relevance of point clouds in adjacent areas. Meanwhile, for the problem (2), the GAM enhances the correlation between groups, i.e., grouped features within the same classes. To solve the problem (3), we adopt a multiscale radius to construct the groups and concatenate extracted hierarchical features with the output of the corresponding upsampling process. Under the ISPRS 3D Semantic Labeling Contest dataset, the DAPnet outperforms the benchmark by 85.2\% with an overall accuracy of 90.7\%. By conducting ablation comparisons, we find that the PAM effectively improves the model than the GAM. The incorporation of the double self-attention module has an average of 7\% improvement on the pre-class accuracy. Plus, the DAPnet consumes a similar training time to those without the attention modules for model convergence. The DAPnet can assign different weights to features based on the relevance between point clouds and their neighbors, which effectively improves classification performance. The source codes are available at: https://github.com/RayleighChen/point-attention.



### Feathers dataset for Fine-Grained Visual Categorization
- **Arxiv ID**: http://arxiv.org/abs/2004.08606v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.08606v1)
- **Published**: 2020-04-18 12:40:43+00:00
- **Updated**: 2020-04-18 12:40:43+00:00
- **Authors**: Alina Belko, Konstantin Dobratulin, Andrey Kuznetsov
- **Comment**: 6 pages, 6 figures, 3 tables
- **Journal**: None
- **Summary**: This paper introduces a novel dataset FeatherV1, containing 28,272 images of feathers categorized by 595 bird species. It was created to perform taxonomic identification of bird species by a single feather, which can be applied in amateur and professional ornithology. FeatherV1 is the first publicly available bird's plumage dataset for machine learning, and it can raise interest for a new task in fine-grained visual recognition domain. The latest version of the dataset can be downloaded at https://github.com/feathers-dataset/feathersv1-dataset. We also present feathers classification task results. We selected several deep learning architectures (DenseNet based) for categorical crossentropy values comparison on the provided dataset.



### Underwater image enhancement with Image Colorfulness Measure
- **Arxiv ID**: http://arxiv.org/abs/2004.08609v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.08609v1)
- **Published**: 2020-04-18 12:44:57+00:00
- **Updated**: 2020-04-18 12:44:57+00:00
- **Authors**: Hui Li, Xi Yang, ZhenMing Li, TianLun Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Due to the absorption and scattering effects of the water, underwater images tend to suffer from many severe problems, such as low contrast, grayed out colors and blurring content. To improve the visual quality of underwater images, we proposed a novel enhancement model, which is a trainable end-to-end neural model. Two parts constitute the overall model. The first one is a non-parameter layer for the preliminary color correction, then the second part is consisted of parametric layers for a self-adaptive refinement, namely the channel-wise linear shift. For better details, contrast and colorfulness, this enhancement network is jointly optimized by the pixel-level and characteristiclevel training criteria. Through extensive experiments on natural underwater scenes, we show that the proposed method can get high quality enhancement results.



### Halluci-Net: Scene Completion by Exploiting Object Co-occurrence Relationships
- **Arxiv ID**: http://arxiv.org/abs/2004.08614v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.08614v2)
- **Published**: 2020-04-18 13:12:59+00:00
- **Updated**: 2021-05-21 03:04:53+00:00
- **Authors**: Kuldeep Kulkarni, Tejas Gokhale, Rajhans Singh, Pavan Turaga, Aswin Sankaranarayanan
- **Comment**: Accepted to AI for Content Creation Workshop @CVPR 2021
- **Journal**: None
- **Summary**: Recently, there has been substantial progress in image synthesis from semantic labelmaps. However, methods used for this task assume the availability of complete and unambiguous labelmaps, with instance boundaries of objects, and class labels for each pixel. This reliance on heavily annotated inputs restricts the application of image synthesis techniques to real-world applications, especially under uncertainty due to weather, occlusion, or noise. On the other hand, algorithms that can synthesize images from sparse labelmaps or sketches are highly desirable as tools that can guide content creators and artists to quickly generate scenes by simply specifying locations of a few objects. In this paper, we address the problem of complex scene completion from sparse labelmaps. Under this setting, very few details about the scene (30\% of object instances) are available as input for image synthesis. We propose a two-stage deep network based method, called `Halluci-Net', that learns co-occurence relationships between objects in scenes, and then exploits these relationships to produce a dense and complete labelmap. The generated dense labelmap can then be used as input by state-of-the-art image synthesis techniques like pix2pixHD to obtain the final image. The proposed method is evaluated on the Cityscapes dataset and it outperforms two baselines methods on performance metrics like Fr\'echet Inception Distance (FID), semantic segmentation accuracy, and similarity in object co-occurrences. We also show qualitative results on a subset of ADE20K dataset that contains bedroom images.



### Single-step Adversarial training with Dropout Scheduling
- **Arxiv ID**: http://arxiv.org/abs/2004.08628v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.08628v1)
- **Published**: 2020-04-18 14:14:00+00:00
- **Updated**: 2020-04-18 14:14:00+00:00
- **Authors**: Vivek B. S., R. Venkatesh Babu
- **Comment**: CVPR 2020
- **Journal**: None
- **Summary**: Deep learning models have shown impressive performance across a spectrum of computer vision applications including medical diagnosis and autonomous driving. One of the major concerns that these models face is their susceptibility to adversarial attacks. Realizing the importance of this issue, more researchers are working towards developing robust models that are less affected by adversarial attacks. Adversarial training method shows promising results in this direction. In adversarial training regime, models are trained with mini-batches augmented with adversarial samples. Fast and simple methods (e.g., single-step gradient ascent) are used for generating adversarial samples, in order to reduce computational complexity. It is shown that models trained using single-step adversarial training method (adversarial samples are generated using non-iterative method) are pseudo robust. Further, this pseudo robustness of models is attributed to the gradient masking effect. However, existing works fail to explain when and why gradient masking effect occurs during single-step adversarial training. In this work, (i) we show that models trained using single-step adversarial training method learn to prevent the generation of single-step adversaries, and this is due to over-fitting of the model during the initial stages of training, and (ii) to mitigate this effect, we propose a single-step adversarial training method with dropout scheduling. Unlike models trained using existing single-step adversarial training methods, models trained using the proposed single-step adversarial training method are robust against both single-step and multi-step adversarial attacks, and the performance is on par with models trained using computationally expensive multi-step adversarial training methods, in white-box and black-box settings.



### Motion Segmentation using Frequency Domain Transformer Networks
- **Arxiv ID**: http://arxiv.org/abs/2004.08638v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.08638v1)
- **Published**: 2020-04-18 15:05:11+00:00
- **Updated**: 2020-04-18 15:05:11+00:00
- **Authors**: Hafez Farazi, Sven Behnke
- **Comment**: 28th European Symposium on Artificial Neural Networks, Computational
  Intelligence and Machine Learning (ESANN), Bruges, Belgium, 2020
- **Journal**: None
- **Summary**: Self-supervised prediction is a powerful mechanism to learn representations that capture the underlying structure of the data. Despite recent progress, the self-supervised video prediction task is still challenging. One of the critical factors that make the task hard is motion segmentation, which is segmenting individual objects and the background and estimating their motion separately. In video prediction, the shape, appearance, and transformation of each object should be understood only by predicting the next frame in pixel space. To address this task, we propose a novel end-to-end learnable architecture that predicts the next frame by modeling foreground and background separately while simultaneously estimating and predicting the foreground motion using Frequency Domain Transformer Networks. Experimental evaluations show that this yields interpretable representations and that our approach can outperform some widely used video prediction methods like Video Ladder Network and Predictive Gated Pyramids on synthetic data.



### A Deep Learning Approach to Object Affordance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2004.08644v1
- **DOI**: 10.1109/ICASSP40776.2020.9054167
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.08644v1)
- **Published**: 2020-04-18 15:34:41+00:00
- **Updated**: 2020-04-18 15:34:41+00:00
- **Authors**: Spyridon Thermos, Petros Daras, Gerasimos Potamianos
- **Comment**: 5 pages, 4 figures, ICASSP 2020
- **Journal**: None
- **Summary**: Learning to understand and infer object functionalities is an important step towards robust visual intelligence. Significant research efforts have recently focused on segmenting the object parts that enable specific types of human-object interaction, the so-called "object affordances". However, most works treat it as a static semantic segmentation problem, focusing solely on object appearance and relying on strong supervision and object detection. In this paper, we propose a novel approach that exploits the spatio-temporal nature of human-object interaction for affordance segmentation. In particular, we design an autoencoder that is trained using ground-truth labels of only the last frame of the sequence, and is able to infer pixel-wise affordance labels in both videos and static images. Our model surpasses the need for object labels and bounding boxes by using a soft-attention mechanism that enables the implicit localization of the interaction hotspot. For evaluation purposes, we introduce the SOR3D-AFF corpus, which consists of human-object interaction sequences and supports 9 types of affordances in terms of pixel-wise annotation, covering typical manipulations of tool-like objects. We show that our model achieves competitive results compared to strongly supervised methods on SOR3D-AFF, while being able to predict affordances for similar unseen objects in two affordance image-only datasets.



### Occluded Prohibited Items Detection: an X-ray Security Inspection Benchmark and De-occlusion Attention Module
- **Arxiv ID**: http://arxiv.org/abs/2004.08656v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.08656v4)
- **Published**: 2020-04-18 16:10:55+00:00
- **Updated**: 2020-08-13 13:41:24+00:00
- **Authors**: Yanlu Wei, Renshuai Tao, Zhangjie Wu, Yuqing Ma, Libo Zhang, Xianglong Liu
- **Comment**: 9 pages, 7 figures, accepted by ACM Multimedia 2020, data and code
  are available at https://github.com/OPIXray-author/OPIXray
- **Journal**: None
- **Summary**: Security inspection often deals with a piece of baggage or suitcase where objects are heavily overlapped with each other, resulting in an unsatisfactory performance for prohibited items detection in X-ray images. In the literature, there have been rare studies and datasets touching this important topic. In this work, we contribute the first high-quality object detection dataset for security inspection, named Occluded Prohibited Items X-ray (OPIXray) image benchmark. OPIXray focused on the widely-occurred prohibited item "cutter", annotated manually by professional inspectors from the international airport. The test set is further divided into three occlusion levels to better understand the performance of detectors. Furthermore, to deal with the occlusion in X-ray images detection, we propose the De-occlusion Attention Module (DOAM), a plug-and-play module that can be easily inserted into and thus promote most popular detectors. Despite the heavy occlusion in X-ray imaging, shape appearance of objects can be preserved well, and meanwhile different materials visually appear with different colors and textures. Motivated by these observations, our DOAM simultaneously leverages the different appearance information of the prohibited item to generate the attention map, which helps refine feature maps for the general detectors. We comprehensively evaluate our module on the OPIXray dataset, and demonstrate that our module can consistently improve the performance of the state-of-the-art detection methods such as SSD, FCOS, etc, and significantly outperforms several widely-used attention mechanisms. In particular, the advantages of DOAM are more significant in the scenarios with higher levels of occlusion, which demonstrates its potential application in real-world inspections. The OPIXray benchmark and our model are released at https://github.com/OPIXray-author/OPIXray.



### Dual Embedding Expansion for Vehicle Re-identification
- **Arxiv ID**: http://arxiv.org/abs/2004.08665v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.08665v1)
- **Published**: 2020-04-18 17:14:07+00:00
- **Updated**: 2020-04-18 17:14:07+00:00
- **Authors**: Clint Sebastian, Raffaele Imbriaco, Egor Bondarev, Peter H. N. de With
- **Comment**: None
- **Journal**: None
- **Summary**: Vehicle re-identification plays a crucial role in the management of transportation infrastructure and traffic flow. However, this is a challenging task due to the large view-point variations in appearance, environmental and instance-related factors. Modern systems deploy CNNs to produce unique representations from the images of each vehicle instance. Most work focuses on leveraging new losses and network architectures to improve the descriptiveness of these representations. In contrast, our work concentrates on re-ranking and embedding expansion techniques. We propose an efficient approach for combining the outputs of multiple models at various scales while exploiting tracklet and neighbor information, called dual embedding expansion (DEx). Additionally, a comparative study of several common image retrieval techniques is presented in the context of vehicle re-ID. Our system yields competitive performance in the 2020 NVIDIA AI City Challenge with promising results. We demonstrate that DEx when combined with other re-ranking techniques, can produce an even larger gain without any additional attribute labels or manual supervision.



### Example-Guided Image Synthesis across Arbitrary Scenes using Masked Spatial-Channel Attention and Self-Supervision
- **Arxiv ID**: http://arxiv.org/abs/2004.10024v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.10024v1)
- **Published**: 2020-04-18 18:17:40+00:00
- **Updated**: 2020-04-18 18:17:40+00:00
- **Authors**: Haitian Zheng, Haofu Liao, Lele Chen, Wei Xiong, Tianlang Chen, Jiebo Luo
- **Comment**: 24 pages. arXiv admin note: substantial text overlap with
  arXiv:1911.12362
- **Journal**: None
- **Summary**: Example-guided image synthesis has recently been attempted to synthesize an image from a semantic label map and an exemplary image. In the task, the additional exemplar image provides the style guidance that controls the appearance of the synthesized output. Despite the controllability advantage, the existing models are designed on datasets with specific and roughly aligned objects. In this paper, we tackle a more challenging and general task, where the exemplar is an arbitrary scene image that is semantically different from the given label map. To this end, we first propose a Masked Spatial-Channel Attention (MSCA) module which models the correspondence between two arbitrary scenes via efficient decoupled attention. Next, we propose an end-to-end network for joint global and local feature alignment and synthesis. Finally, we propose a novel self-supervision task to enable training. Experiments on the large-scale and more diverse COCO-stuff dataset show significant improvements over the existing methods. Moreover, our approach provides interpretability and can be readily extended to other content manipulation tasks including style and spatial interpolation or extrapolation.



### A Large Dataset of Historical Japanese Documents with Complex Layouts
- **Arxiv ID**: http://arxiv.org/abs/2004.08686v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.08686v1)
- **Published**: 2020-04-18 18:38:25+00:00
- **Updated**: 2020-04-18 18:38:25+00:00
- **Authors**: Zejiang Shen, Kaixuan Zhang, Melissa Dell
- **Comment**: 8 pages, 8 figures, accepted at CVPR2020 Workshop on Text and
  Documents in the Deep Learning Era
- **Journal**: None
- **Summary**: Deep learning-based approaches for automatic document layout analysis and content extraction have the potential to unlock rich information trapped in historical documents on a large scale. One major hurdle is the lack of large datasets for training robust models. In particular, little training data exist for Asian languages. To this end, we present HJDataset, a Large Dataset of Historical Japanese Documents with Complex Layouts. It contains over 250,000 layout element annotations of seven types. In addition to bounding boxes and masks of the content regions, it also includes the hierarchical structures and reading orders for layout elements. The dataset is constructed using a combination of human and machine efforts. A semi-rule based method is developed to extract the layout elements, and the results are checked by human inspectors. The resulting large-scale dataset is used to provide baseline performance analyses for text region detection using state-of-the-art deep learning models. And we demonstrate the usefulness of the dataset on real-world document digitization tasks. The dataset is available at https://dell-research-harvard.github.io/HJDataset/.



### A fast semi-automatic method for classification and counting the number and types of blood cells in an image
- **Arxiv ID**: http://arxiv.org/abs/2004.08690v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.08690v1)
- **Published**: 2020-04-18 19:23:56+00:00
- **Updated**: 2020-04-18 19:23:56+00:00
- **Authors**: Hamed Sadeghi, Shahram Shirani, David W. Capson
- **Comment**: None
- **Journal**: None
- **Summary**: A novel and fast semi-automatic method for segmentation, locating and counting blood cells in an image is proposed. In this method, thresholding is used to separate the nucleus from the other parts. We also use Hough transform for circles to locate the center of white cells. Locating and counting of red cells is performed using template matching. We make use of finding local maxima, labeling and mean value computation in order to shrink the areas obtained after applying Hough transform or template matching, to a single pixel as representative of location of each region. The proposed method is very fast and computes the number and location of white cells accurately. It is also capable of locating and counting the red cells with a small error.



### A Spatio-temporal Transformer for 3D Human Motion Prediction
- **Arxiv ID**: http://arxiv.org/abs/2004.08692v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.08692v3)
- **Published**: 2020-04-18 19:49:28+00:00
- **Updated**: 2021-11-29 15:13:04+00:00
- **Authors**: Emre Aksan, Manuel Kaufmann, Peng Cao, Otmar Hilliges
- **Comment**: 3DV 2021 camera-ready
- **Journal**: International Conference on 3D Vision 2021 (3DV)
- **Summary**: We propose a novel Transformer-based architecture for the task of generative modelling of 3D human motion. Previous work commonly relies on RNN-based models considering shorter forecast horizons reaching a stationary and often implausible state quickly. Recent studies show that implicit temporal representations in the frequency domain are also effective in making predictions for a predetermined horizon. Our focus lies on learning spatio-temporal representations autoregressively and hence generation of plausible future developments over both short and long term. The proposed model learns high dimensional embeddings for skeletal joints and how to compose a temporally coherent pose via a decoupled temporal and spatial self-attention mechanism. Our dual attention concept allows the model to access current and past information directly and to capture both the structural and the temporal dependencies explicitly. We show empirically that this effectively learns the underlying motion dynamics and reduces error accumulation over time observed in auto-regressive models. Our model is able to make accurate short-term predictions and generate plausible motion sequences over long horizons. We make our code publicly available at https://github.com/eth-ait/motion-transformer.



### Adaptive Attention Span in Computer Vision
- **Arxiv ID**: http://arxiv.org/abs/2004.08708v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.08708v1)
- **Published**: 2020-04-18 21:32:47+00:00
- **Updated**: 2020-04-18 21:32:47+00:00
- **Authors**: Jerrod Parker, Shakti Kumar, Joe Roussy
- **Comment**: 6 pages with 1 Algorithm, 4 figures, 1 Table and 1 page appendix
- **Journal**: None
- **Summary**: Recent developments in Transformers for language modeling have opened new areas of research in computer vision. Results from late 2019 showed vast performance increases in both object detection and recognition when convolutions are replaced by local self-attention kernels. Models using local self-attention kernels were also shown to have less parameters and FLOPS compared to equivalent architectures that only use convolutions. In this work we propose a novel method for learning the local self-attention kernel size. We then compare its performance to fixed-size local attention and convolution kernels. The code for all our experiments and models is available at https://github.com/JoeRoussy/adaptive-attention-in-cv



