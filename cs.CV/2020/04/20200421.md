# Arxiv Papers in cs.CV on 2020-04-21
### Alleviating the Incompatibility between Cross Entropy Loss and Episode Training for Few-shot Skin Disease Classification
- **Arxiv ID**: http://arxiv.org/abs/2004.09694v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.09694v1)
- **Published**: 2020-04-21 00:57:11+00:00
- **Updated**: 2020-04-21 00:57:11+00:00
- **Authors**: Wei Zhu, Haofu Liao, Wenbin Li, Weijian Li, Jiebo Luo
- **Comment**: None
- **Journal**: None
- **Summary**: Skin disease classification from images is crucial to dermatological diagnosis. However, identifying skin lesions involves a variety of aspects in terms of size, color, shape, and texture. To make matters worse, many categories only contain very few samples, posing great challenges to conventional machine learning algorithms and even human experts. Inspired by the recent success of Few-Shot Learning (FSL) in natural image classification, we propose to apply FSL to skin disease identification to address the extreme scarcity of training sample problem. However, directly applying FSL to this task does not work well in practice, and we find that the problem can be largely attributed to the incompatibility between Cross Entropy (CE) and episode training, which are both commonly used in FSL. Based on a detailed analysis, we propose the Query-Relative (QR) loss, which proves superior to CE under episode training and is closely related to recently proposed mutual information estimation. Moreover, we further strengthen the proposed QR loss with a novel adaptive hard margin strategy. Comprehensive experiments validate the effectiveness of the proposed FSL scheme and the possibility to diagnosis rare skin disease with a few labeled samples.



### Image Retrieval using Multi-scale CNN Features Pooling
- **Arxiv ID**: http://arxiv.org/abs/2004.09695v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.09695v2)
- **Published**: 2020-04-21 00:57:52+00:00
- **Updated**: 2020-04-24 11:19:31+00:00
- **Authors**: Federico Vaccaro, Marco Bertini, Tiberio Uricchio, Alberto Del Bimbo
- **Comment**: Accepted at ICMR 2020
- **Journal**: None
- **Summary**: In this paper, we address the problem of image retrieval by learning images representation based on the activations of a Convolutional Neural Network. We present an end-to-end trainable network architecture that exploits a novel multi-scale local pooling based on NetVLAD and a triplet mining procedure based on samples difficulty to obtain an effective image representation. Extensive experiments show that our approach is able to reach state-of-the-art results on three standard datasets.



### M^3VSNet: Unsupervised Multi-metric Multi-view Stereo Network
- **Arxiv ID**: http://arxiv.org/abs/2004.09722v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.09722v2)
- **Published**: 2020-04-21 02:45:25+00:00
- **Updated**: 2020-05-28 11:29:16+00:00
- **Authors**: Baichuan Huang, Hongwei Yi, Can Huang, Yijia He, Jingbin Liu, Xiao Liu
- **Comment**: Welcome to communicate with the author by the repo
  https://github.com/whubaichuan/M3VSNet
- **Journal**: None
- **Summary**: The present Multi-view stereo (MVS) methods with supervised learning-based networks have an impressive performance comparing with traditional MVS methods. However, the ground-truth depth maps for training are hard to be obtained and are within limited kinds of scenarios. In this paper, we propose a novel unsupervised multi-metric MVS network, named M^3VSNet, for dense point cloud reconstruction without any supervision. To improve the robustness and completeness of point cloud reconstruction, we propose a novel multi-metric loss function that combines pixel-wise and feature-wise loss function to learn the inherent constraints from different perspectives of matching correspondences. Besides, we also incorporate the normal-depth consistency in the 3D point cloud format to improve the accuracy and continuity of the estimated depth maps. Experimental results show that M3VSNet establishes the state-of-the-arts unsupervised method and achieves comparable performance with previous supervised MVSNet on the DTU dataset and demonstrates the powerful generalization ability on the Tanks and Temples benchmark with effective improvement. Our code is available at https://github.com/whubaichuan/M3VSNet.



### TrueBranch: Metric Learning-based Verification of Forest Conservation Projects
- **Arxiv ID**: http://arxiv.org/abs/2004.09725v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2004.09725v1)
- **Published**: 2020-04-21 02:52:27+00:00
- **Updated**: 2020-04-21 02:52:27+00:00
- **Authors**: Simona Santamaria, David Dao, Björn Lütjens, Ce Zhang
- **Comment**: *Authors have contributed equally. Published as Spotlight
  Presentation at ICLR 2020 Workshop on Tackling Climate Change with Machine
  Learning
- **Journal**: None
- **Summary**: International stakeholders increasingly invest in offsetting carbon emissions, for example, via issuing Payments for Ecosystem Services (PES) to forest conservation projects. Issuing trusted payments requires a transparent monitoring, reporting, and verification (MRV) process of the ecosystem services (e.g., carbon stored in forests). The current MRV process, however, is either too expensive (on-ground inspection of forest) or inaccurate (satellite). Recent works propose low-cost and accurate MRV via automatically determining forest carbon from drone imagery, collected by the landowners. The automation of MRV, however, opens up the possibility that landowners report untruthful drone imagery. To be robust against untruthful reporting, we propose TrueBranch, a metric learning-based algorithm that verifies the truthfulness of drone imagery from forest conservation projects. TrueBranch aims to detect untruthfully reported drone imagery by matching it with public satellite imagery. Preliminary results suggest that nominal distance metrics are not sufficient to reliably detect untruthfully reported imagery. TrueBranch leverages metric learning to create a feature embedding in which truthfully and untruthfully collected imagery is easily distinguishable by distance thresholding.



### YOLO and K-Means Based 3D Object Detection Method on Image and Point Cloud
- **Arxiv ID**: http://arxiv.org/abs/2004.11465v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.11465v1)
- **Published**: 2020-04-21 03:08:46+00:00
- **Updated**: 2020-04-21 03:08:46+00:00
- **Authors**: Xuanyu YIN, Yoko SASAKI, Weimin WANG, Kentaro SHIMIZU
- **Comment**: 5 pages
- **Journal**: None
- **Summary**: Lidar based 3D object detection and classification tasks are essential for automated driving(AD). A Lidar sensor can provide the 3D point coud data reconstruction of the surrounding environment. But the detection in 3D point cloud still needs a strong algorithmic challenge. This paper consists of three parts.(1)Lidar-camera calib. (2)YOLO, based detection and PointCloud extraction, (3) k-means based point cloud segmentation. In our research, Camera can capture the image to make the Real-time 2D Object Detection by using YOLO, I transfer the bounding box to node whose function is making 3d object detection on point cloud data from Lidar. By comparing whether 2D coordinate transferred from the 3D point is in the object bounding box or not, and doing a k-means clustering can achieve High-speed 3D object recognition function in GPU.



### 3D Object Detection Method Based on YOLO and K-Means for Image and Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2005.02132v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.02132v1)
- **Published**: 2020-04-21 04:32:36+00:00
- **Updated**: 2020-04-21 04:32:36+00:00
- **Authors**: Xuanyu Yin, Yoko Sasaki, Weimin Wang, Kentaro Shimizu
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2004.11465
- **Journal**: None
- **Summary**: Lidar based 3D object detection and classification tasks are essential for autonomous driving(AD). A lidar sensor can provide the 3D point cloud data reconstruction of the surrounding environment. However, real time detection in 3D point clouds still needs a strong algorithmic. This paper proposes a 3D object detection method based on point cloud and image which consists of there parts.(1)Lidar-camera calibration and undistorted image transformation. (2)YOLO-based detection and PointCloud extraction, (3)K-means based point cloud segmentation and detection experiment test and evaluation in depth image. In our research, camera can capture the image to make the Real-time 2D object detection by using YOLO, we transfer the bounding box to node whose function is making 3d object detection on point cloud data from Lidar. By comparing whether 2D coordinate transferred from the 3D point is in the object bounding box or not can achieve High-speed 3D object recognition function in GPU. The accuracy and precision get imporved after k-means clustering in point cloud. The speed of our detection method is a advantage faster than PointNet.



### MiniSeg: An Extremely Minimum Network for Efficient COVID-19 Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2004.09750v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.09750v3)
- **Published**: 2020-04-21 04:51:37+00:00
- **Updated**: 2021-03-21 14:47:25+00:00
- **Authors**: Yu Qiu, Yun Liu, Shijie Li, Jing Xu
- **Comment**: None
- **Journal**: None
- **Summary**: The rapid spread of the new pandemic, i.e., COVID-19, has severely threatened global health. Deep-learning-based computer-aided screening, e.g., COVID-19 infected CT area segmentation, has attracted much attention. However, the publicly available COVID-19 training data are limited, easily causing overfitting for traditional deep learning methods that are usually data-hungry with millions of parameters. On the other hand, fast training/testing and low computational cost are also necessary for quick deployment and development of COVID-19 screening systems, but traditional deep learning methods are usually computationally intensive. To address the above problems, we propose MiniSeg, a lightweight deep learning model for efficient COVID-19 segmentation. Compared with traditional segmentation methods, MiniSeg has several significant strengths: i) it only has 83K parameters and is thus not easy to overfit; ii) it has high computational efficiency and is thus convenient for practical deployment; iii) it can be fast retrained by other users using their private COVID-19 data for further improving performance. In addition, we build a comprehensive COVID-19 segmentation benchmark for comparing MiniSeg to traditional methods.



### The 1st Agriculture-Vision Challenge: Methods and Results
- **Arxiv ID**: http://arxiv.org/abs/2004.09754v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.09754v2)
- **Published**: 2020-04-21 05:02:31+00:00
- **Updated**: 2020-04-23 17:24:31+00:00
- **Authors**: Mang Tik Chiu, Xingqian Xu, Kai Wang, Jennifer Hobbs, Naira Hovakimyan, Thomas S. Huang, Honghui Shi, Yunchao Wei, Zilong Huang, Alexander Schwing, Robert Brunner, Ivan Dozier, Wyatt Dozier, Karen Ghandilyan, David Wilson, Hyunseong Park, Junhee Kim, Sungho Kim, Qinghui Liu, Michael C. Kampffmeyer, Robert Jenssen, Arnt B. Salberg, Alexandre Barbosa, Rodrigo Trevisan, Bingchen Zhao, Shaozuo Yu, Siwei Yang, Yin Wang, Hao Sheng, Xiao Chen, Jingyi Su, Ram Rajagopal, Andrew Ng, Van Thong Huynh, Soo-Hyung Kim, In-Seop Na, Ujjwal Baid, Shubham Innani, Prasad Dutande, Bhakti Baheti, Sanjay Talbar, Jianyu Tang
- **Comment**: CVPR 2020 Workshop
- **Journal**: None
- **Summary**: The first Agriculture-Vision Challenge aims to encourage research in developing novel and effective algorithms for agricultural pattern recognition from aerial images, especially for the semantic segmentation task associated with our challenge dataset. Around 57 participating teams from various countries compete to achieve state-of-the-art in aerial agriculture semantic segmentation. The Agriculture-Vision Challenge Dataset was employed, which comprises of 21,061 aerial and multi-spectral farmland images. This paper provides a summary of notable methods and results in the challenge. Our submission server and leaderboard will continue to open for researchers that are interested in this challenge dataset and task; the link can be found here.



### Take a NAP: Non-Autoregressive Prediction for Pedestrian Trajectories
- **Arxiv ID**: http://arxiv.org/abs/2004.09760v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.09760v1)
- **Published**: 2020-04-21 05:32:30+00:00
- **Updated**: 2020-04-21 05:32:30+00:00
- **Authors**: Hao Xue, Du. Q. Huynh, Mark Reynolds
- **Comment**: None
- **Journal**: None
- **Summary**: Pedestrian trajectory prediction is a challenging task as there are three properties of human movement behaviors which need to be addressed, namely, the social influence from other pedestrians, the scene constraints, and the multimodal (multiroute) nature of predictions. Although existing methods have explored these key properties, the prediction process of these methods is autoregressive. This means they can only predict future locations sequentially. In this paper, we present NAP, a non-autoregressive method for trajectory prediction. Our method comprises specifically designed feature encoders and a latent variable generator to handle the three properties above. It also has a time-agnostic context generator and a time-specific context generator for non-autoregressive prediction. Through extensive experiments that compare NAP against several recent methods, we show that NAP has state-of-the-art trajectory prediction performance.



### Fine-Grained Expression Manipulation via Structured Latent Space
- **Arxiv ID**: http://arxiv.org/abs/2004.09769v2
- **DOI**: 10.1109/ICME46284.2020.9102852
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.09769v2)
- **Published**: 2020-04-21 06:18:34+00:00
- **Updated**: 2020-05-10 08:11:12+00:00
- **Authors**: Junshu Tang, Zhiwen Shao, Lizhuang Ma
- **Comment**: None
- **Journal**: None
- **Summary**: Fine-grained facial expression manipulation is a challenging problem, as fine-grained expression details are difficult to be captured. Most existing expression manipulation methods resort to discrete expression labels, which mainly edit global expressions and ignore the manipulation of fine details. To tackle this limitation, we propose an end-to-end expression-guided generative adversarial network (EGGAN), which utilizes structured latent codes and continuous expression labels as input to generate images with expected expressions. Specifically, we adopt an adversarial autoencoder to map a source image into a structured latent space. Then, given the source latent code and the target expression label, we employ a conditional GAN to generate a new image with the target expression. Moreover, we introduce a perceptual loss and a multi-scale structural similarity loss to preserve identity and global shape during generation. Extensive experiments show that our method can manipulate fine-grained expressions, and generate continuous intermediate expressions between source and target expressions.



### Decoupling Video and Human Motion: Towards Practical Event Detection in Athlete Recordings
- **Arxiv ID**: http://arxiv.org/abs/2004.09776v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.09776v2)
- **Published**: 2020-04-21 07:06:12+00:00
- **Updated**: 2020-04-22 15:52:25+00:00
- **Authors**: Moritz Einfalt, Rainer Lienhart
- **Comment**: Accepted at 2020 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition Workshop (CVPRW)
- **Journal**: None
- **Summary**: In this paper we address the problem of motion event detection in athlete recordings from individual sports. In contrast to recent end-to-end approaches, we propose to use 2D human pose sequences as an intermediate representation that decouples human motion from the raw video information. Combined with domain-adapted athlete tracking, we describe two approaches to event detection on pose sequences and evaluate them in complementary domains: swimming and athletics. For swimming, we show how robust decision rules on pose statistics can detect different motion events during swim starts, with a F1 score of over 91% despite limited data. For athletics, we use a convolutional sequence model to infer stride-related events in long and triple jump recordings, leading to highly accurate detections with 96% in F1 score at only +/- 5ms temporal deviation. Our approach is not limited to these domains and shows the flexibility of pose-based motion event detection.



### Deep Cerebellar Nuclei Segmentation via Semi-Supervised Deep Context-Aware Learning from 7T Diffusion MRI
- **Arxiv ID**: http://arxiv.org/abs/2004.09788v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.09788v3)
- **Published**: 2020-04-21 07:30:07+00:00
- **Updated**: 2020-05-31 01:47:33+00:00
- **Authors**: Jinyoung Kim, Remi Patriat, Jordan Kaplan, Oren Solomon, Noam Harel
- **Comment**: 56 pages (one column), 13 figures, 5 tables, supplementary materials,
  Accepted for publication in IEEE Access
- **Journal**: None
- **Summary**: Deep cerebellar nuclei are a key structure of the cerebellum that are involved in processing motor and sensory information. It is thus a crucial step to accurately segment deep cerebellar nuclei for the understanding of the cerebellum system and its utility in deep brain stimulation treatment. However, it is challenging to clearly visualize such small nuclei under standard clinical magnetic resonance imaging (MRI) protocols and therefore precise segmentation is not feasible. Recent advances in 7 Tesla (T) MRI technology and great potential of deep neural networks facilitate automatic patient-specific segmentation. In this paper, we propose a novel deep learning framework (referred to as DCN-Net) for fast, accurate, and robust patient-specific segmentation of deep cerebellar dentate and interposed nuclei on 7T diffusion MRI. DCN-Net effectively encodes contextual information on the patch images without consecutive pooling operations and adding complexity via proposed dilated dense blocks. During the end-to-end training, label probabilities of dentate and interposed nuclei are independently learned with a hybrid loss, handling highly imbalanced data. Finally, we utilize self-training strategies to cope with the problem of limited labeled data. To this end, auxiliary dentate and interposed nuclei labels are created on unlabeled data by using DCN-Net trained on manual labels. We validate the proposed framework using 7T B0 MRIs from 60 subjects. Experimental results demonstrate that DCN-Net provides better segmentation than atlas-based deep cerebellar nuclei segmentation tools and other state-of-the-art deep neural networks in terms of accuracy and consistency. We further prove the effectiveness of the proposed components within DCN-Net in dentate and interposed nuclei segmentation.



### A CNN Framenwork Based on Line Annotations for Detecting Nematodes in Microscopic Images
- **Arxiv ID**: http://arxiv.org/abs/2004.09795v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.09795v1)
- **Published**: 2020-04-21 07:48:02+00:00
- **Updated**: 2020-04-21 07:48:02+00:00
- **Authors**: Long Chen, Martin Strauch, Matthias Daub, Xiaochen Jiang, Marcus Jansen, Hans-Georg Luigs, Susanne Schultz-Kuhlmann, Stefan Krüssel, Dorif Merhof
- **Comment**: ISBI 2020
- **Journal**: None
- **Summary**: Plant parasitic nematodes cause damage to crop plants on a global scale. Robust detection on image data is a prerequisite for monitoring such nematodes, as well as for many biological studies involving the nematode C. elegans, a common model organism. Here, we propose a framework for detecting worm-shaped objects in microscopic images that is based on convolutional neural networks (CNNs). We annotate nematodes with curved lines along the body, which is more suitable for worm-shaped objects than bounding boxes. The trained model predicts worm skeletons and body endpoints. The endpoints serve to untangle the skeletons from which segmentation masks are reconstructed by estimating the body width at each location along the skeleton. With light-weight backbone networks, we achieve 75.85 % precision, 73.02 % recall on a potato cyst nematode data set and 84.20 % precision, 85.63 % recall on a public C. elegans data set.



### Spatio-Temporal Dual Affine Differential Invariant for Skeleton-based Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2004.09802v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.09802v1)
- **Published**: 2020-04-21 08:00:45+00:00
- **Updated**: 2020-04-21 08:00:45+00:00
- **Authors**: Qi Li, Hanlin Mo, Jinghan Zhao, Hongxiang Hao, Hua Li
- **Comment**: None
- **Journal**: None
- **Summary**: The dynamics of human skeletons have significant information for the task of action recognition. The similarity between trajectories of corresponding joints is an indicating feature of the same action, while this similarity may subject to some distortions that can be modeled as the combination of spatial and temporal affine transformations. In this work, we propose a novel feature called spatio-temporal dual affine differential invariant (STDADI). Furthermore, in order to improve the generalization ability of neural networks, a channel augmentation method is proposed. On the large scale action recognition dataset NTU-RGB+D, and its extended version NTU-RGB+D 120, it achieves remarkable improvements over previous state-of-the-art methods.



### CovidAID: COVID-19 Detection Using Chest X-Ray
- **Arxiv ID**: http://arxiv.org/abs/2004.09803v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.09803v1)
- **Published**: 2020-04-21 08:02:52+00:00
- **Updated**: 2020-04-21 08:02:52+00:00
- **Authors**: Arpan Mangal, Surya Kalia, Harish Rajgopal, Krithika Rangarajan, Vinay Namboodiri, Subhashis Banerjee, Chetan Arora
- **Comment**: None
- **Journal**: None
- **Summary**: The exponential increase in COVID-19 patients is overwhelming healthcare systems across the world. With limited testing kits, it is impossible for every patient with respiratory illness to be tested using conventional techniques (RT-PCR). The tests also have long turn-around time, and limited sensitivity. Detecting possible COVID-19 infections on Chest X-Ray may help quarantine high risk patients while test results are awaited. X-Ray machines are already available in most healthcare systems, and with most modern X-Ray systems already digitized, there is no transportation time involved for the samples either. In this work we propose the use of chest X-Ray to prioritize the selection of patients for further RT-PCR testing. This may be useful in an inpatient setting where the present systems are struggling to decide whether to keep the patient in the ward along with other patients or isolate them in COVID-19 areas. It would also help in identifying patients with high likelihood of COVID with a false negative RT-PCR who would need repeat testing. Further, we propose the use of modern AI techniques to detect the COVID-19 patients using X-Ray images in an automated manner, particularly in settings where radiologists are not available, and help make the proposed testing technology scalable. We present CovidAID: COVID-19 AI Detector, a novel deep neural network based model to triage patients for appropriate testing. On the publicly available covid-chestxray-dataset [2], our model gives 90.5% accuracy with 100% sensitivity (recall) for the COVID-19 infection. We significantly improve upon the results of Covid-Net [10] on the same dataset.



### AMC-Loss: Angular Margin Contrastive Loss for Improved Explainability in Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2004.09805v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.09805v1)
- **Published**: 2020-04-21 08:03:14+00:00
- **Updated**: 2020-04-21 08:03:14+00:00
- **Authors**: Hongjun Choi, Anirudh Som, Pavan Turaga
- **Comment**: None
- **Journal**: None
- **Summary**: Deep-learning architectures for classification problems involve the cross-entropy loss sometimes assisted with auxiliary loss functions like center loss, contrastive loss and triplet loss. These auxiliary loss functions facilitate better discrimination between the different classes of interest. However, recent studies hint at the fact that these loss functions do not take into account the intrinsic angular distribution exhibited by the low-level and high-level feature representations. This results in less compactness between samples from the same class and unclear boundary separations between data clusters of different classes. In this paper, we address this issue by proposing the use of geometric constraints, rooted in Riemannian geometry. Specifically, we propose Angular Margin Contrastive Loss (AMC-Loss), a new loss function to be used along with the traditional cross-entropy loss. The AMC-Loss employs the discriminative angular distance metric that is equivalent to geodesic distance on a hypersphere manifold such that it can serve a clear geometric interpretation. We demonstrate the effectiveness of AMC-Loss by providing quantitative and qualitative results. We find that although the proposed geometrically constrained loss-function improves quantitative results modestly, it has a qualitatively surprisingly beneficial effect on increasing the interpretability of deep-net decisions as seen by the visual explanations generated by techniques such as the Grad-CAM. Our code is available at https://github.com/hchoi71/AMC-Loss.



### Fast and Robust Registration of Aerial Images and LiDAR data Based on Structrual Features and 3D Phase Correlation
- **Arxiv ID**: http://arxiv.org/abs/2004.09811v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.09811v1)
- **Published**: 2020-04-21 08:19:56+00:00
- **Updated**: 2020-04-21 08:19:56+00:00
- **Authors**: Bai Zhu, Yuanxin Ye, Chao Yang, Liang Zhou, Huiyu Liu, Yungang Cao
- **Comment**: None
- **Journal**: None
- **Summary**: Co-Registration of aerial imagery and Light Detection and Ranging (LiDAR) data is quilt challenging because the different imaging mechanism causes significant geometric and radiometric distortions between such data. To tackle the problem, this paper proposes an automatic registration method based on structural features and three-dimension (3D) phase correlation. In the proposed method, the LiDAR point cloud data is first transformed into the intensity map, which is used as the reference image. Then, we employ the Fast operator to extract uniformly distributed interest points in the aerial image by a partition strategy and perform a local geometric correction by using the collinearity equation to eliminate scale and rotation difference between images. Subsequently, a robust structural feature descriptor is build based on dense gradient features, and the 3D phase correlation is used to detect control points (CPs) between aerial images and LiDAR data in the frequency domain, where the image matching is accelerated by the 3D Fast Fourier Transform (FFT). Finally, the obtained CPs are employed to correct the exterior orientation elements, which is used to achieve co-registration of aerial images and LiDAR data. Experiments with two datasets of aerial images and LiDAR data show that the proposed method is much faster and more robust than state of the art methods



### Instance Segmentation of Biomedical Images with an Object-aware Embedding Learned with Local Constraints
- **Arxiv ID**: http://arxiv.org/abs/2004.09821v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.09821v1)
- **Published**: 2020-04-21 08:33:29+00:00
- **Updated**: 2020-04-21 08:33:29+00:00
- **Authors**: Long Chen, Martin Strauch, Dorit Merhof
- **Comment**: MICCAI 2019
- **Journal**: vol 11764, pp 451-459, MICCAI 2019
- **Summary**: Automatic instance segmentation is a problem that occurs in many biomedical applications. State-of-the-art approaches either perform semantic segmentation or refine object bounding boxes obtained from detection methods. Both suffer from crowded objects to varying degrees, merging adjacent objects or suppressing a valid object. In this work, we assign an embedding vector to each pixel through a deep neural network. The network is trained to output embedding vectors of similar directions for pixels from the same object, while adjacent objects are orthogonal in the embedding space, which effectively avoids the fusion of objects in a crowd. Our method yields state-of-the-art results even with a light-weighted backbone network on a cell segmentation (BBBC006 + DSB2018) and a leaf segmentation data set (CVPPP2017). The code and model weights are public available.



### Robust Motion Averaging under Maximum Correntropy Criterion
- **Arxiv ID**: http://arxiv.org/abs/2004.09829v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.09829v2)
- **Published**: 2020-04-21 08:52:38+00:00
- **Updated**: 2020-05-10 09:07:07+00:00
- **Authors**: Jihua Zhu, Jie Hu, Huimin Lu, Badong Chen, Zhongyu Li
- **Comment**: None
- **Journal**: IEEE ICRA 2021
- **Summary**: Recently, the motion averaging method has been introduced as an effective means to solve the multi-view registration problem. This method aims to recover global motions from a set of relative motions, where the original method is sensitive to outliers due to using the Frobenius norm error in the optimization. Accordingly, this paper proposes a novel robust motion averaging method based on the maximum correntropy criterion (MCC). Specifically, the correntropy measure is used instead of utilizing Frobenius norm error to improve the robustness of motion averaging against outliers. According to the half-quadratic technique, the correntropy measure based optimization problem can be solved by the alternating minimization procedure, which includes operations of weight assignment and weighted motion averaging. Further, we design a selection strategy of adaptive kernel width to take advantage of correntropy. Experimental results on benchmark data sets illustrate that the new method has superior performance on accuracy and robustness for multi-view registration.



### MixNet: Multi-modality Mix Network for Brain Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2004.09832v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.09832v1)
- **Published**: 2020-04-21 08:55:55+00:00
- **Updated**: 2020-04-21 08:55:55+00:00
- **Authors**: Long Chen, Dorit Merhof
- **Comment**: BrainLes, MICCAI 2018
- **Journal**: pp 267-376, MICCAI 2018 Brainlesion: Glioma, Multiple Sclerosis,
  Stroke and Traumatic Brain Injuries
- **Summary**: Automated brain structure segmentation is important to many clinical quantitative analysis and diagnoses. In this work, we introduce MixNet, a 2D semantic-wise deep convolutional neural network to segment brain structure in multi-modality MRI images. The network is composed of our modified deep residual learning units. In the unit, we replace the traditional convolution layer with the dilated convolutional layer, which avoids the use of pooling layers and deconvolutional layers, reducing the number of network parameters. Final predictions are made by aggregating information from multiple scales and modalities. A pyramid pooling module is used to capture spatial information of the anatomical structures at the output end. In addition, we test three architectures (MixNetv1, MixNetv2 and MixNetv3) which fuse the modalities differently to see the effect on the results. Our network achieves the state-of-the-art performance. MixNetv2 was submitted to the MRBrainS challenge at MICCAI 2018 and won the 3rd place in the 3-label task. On the MRBrainS2018 dataset, which includes subjects with a variety of pathologies, the overall DSC (Dice Coefficient) of 84.7% (gray matter), 87.3% (white matter) and 83.4% (cerebrospinal fluid) were obtained with only 7 subjects as training data.



### Multispectral Video Fusion for Non-contact Monitoring of Respiratory Rate and Apnea
- **Arxiv ID**: http://arxiv.org/abs/2004.09834v1
- **DOI**: 10.1109/TBME.2020.2993649
- **Categories**: **eess.SP**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.09834v1)
- **Published**: 2020-04-21 09:07:09+00:00
- **Updated**: 2020-04-21 09:07:09+00:00
- **Authors**: Gaetano Scebba, Giulia Da Poian, Walter Karlen
- **Comment**: None
- **Journal**: None
- **Summary**: Continuous monitoring of respiratory activity is desirable in many clinical applications to detect respiratory events. Non-contact monitoring of respiration can be achieved with near- and far-infrared spectrum cameras. However, current technologies are not sufficiently robust to be used in clinical applications. For example, they fail to estimate an accurate respiratory rate (RR) during apnea. We present a novel algorithm based on multispectral data fusion that aims at estimating RR also during apnea. The algorithm independently addresses the RR estimation and apnea detection tasks. Respiratory information is extracted from multiple sources and fed into an RR estimator and an apnea detector whose results are fused into a final respiratory activity estimation. We evaluated the system retrospectively using data from 30 healthy adults who performed diverse controlled breathing tasks while lying supine in a dark room and reproduced central and obstructive apneic events. Combining multiple respiratory information from multispectral cameras improved the root mean square error (RMSE) accuracy of the RR estimation from up to 4.64 monospectral data down to 1.60 breaths/min. The median F1 scores for classifying obstructive (0.75 to 0.86) and central apnea (0.75 to 0.93) also improved. Furthermore, the independent consideration of apnea detection led to a more robust system (RMSE of 4.44 vs. 7.96 breaths/min). Our findings may represent a step towards the use of cameras for vital sign monitoring in medical applications.



### LRTD: Long-Range Temporal Dependency based Active Learning for Surgical Workflow Recognition
- **Arxiv ID**: http://arxiv.org/abs/2004.09845v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.09845v2)
- **Published**: 2020-04-21 09:21:22+00:00
- **Updated**: 2020-04-23 05:57:47+00:00
- **Authors**: Xueying Shi, Yueming Jin, Qi Dou, Pheng-Ann Heng
- **Comment**: Accepted as a conference paper in IPCAI 2020
- **Journal**: None
- **Summary**: Automatic surgical workflow recognition in video is an essentially fundamental yet challenging problem for developing computer-assisted and robotic-assisted surgery. Existing approaches with deep learning have achieved remarkable performance on analysis of surgical videos, however, heavily relying on large-scale labelled datasets. Unfortunately, the annotation is not often available in abundance, because it requires the domain knowledge of surgeons. In this paper, we propose a novel active learning method for cost-effective surgical video analysis. Specifically, we propose a non-local recurrent convolutional network (NL-RCNet), which introduces non-local block to capture the long-range temporal dependency (LRTD) among continuous frames. We then formulate an intra-clip dependency score to represent the overall dependency within this clip. By ranking scores among clips in unlabelled data pool, we select the clips with weak dependencies to annotate, which indicates the most informative ones to better benefit network training. We validate our approach on a large surgical video dataset (Cholec80) by performing surgical workflow recognition task. By using our LRTD based selection strategy, we can outperform other state-of-the-art active learning methods. Using only up to 50% of samples, our approach can exceed the performance of full-data training.



### TAL EmotioNet Challenge 2020 Rethinking the Model Chosen Problem in Multi-Task Learning
- **Arxiv ID**: http://arxiv.org/abs/2004.09862v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.09862v1)
- **Published**: 2020-04-21 09:39:38+00:00
- **Updated**: 2020-04-21 09:39:38+00:00
- **Authors**: Pengcheng Wang, Zihao Wang, Zhilong Ji, Xiao Liu, Songfan Yang, Zhongqin Wu
- **Comment**: 4 pages, 2 figures, 3 tables, CVPRW2020
- **Journal**: None
- **Summary**: This paper introduces our approach to the EmotioNet Challenge 2020. We pose the AU recognition problem as a multi-task learning problem, where the non-rigid facial muscle motion (mainly the first 17 AUs) and the rigid head motion (the last 6 AUs) are modeled separately. The co-occurrence of the expression features and the head pose features are explored. We observe that different AUs converge at various speed. By choosing the optimal checkpoint for each AU, the recognition results are improved. We are able to obtain a final score of 0.746 in validation set and 0.7306 in the test set of the challenge.



### Rice grain disease identification using dual phase convolutional neural network based system aimed at small dataset
- **Arxiv ID**: http://arxiv.org/abs/2004.09870v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.09870v2)
- **Published**: 2020-04-21 09:56:27+00:00
- **Updated**: 2021-05-07 18:42:32+00:00
- **Authors**: Tashin Ahmed, Chowdhury Rafeed Rahman, Md. Faysal Mahmud Abid
- **Comment**: None
- **Journal**: None
- **Summary**: Although Convolutional neural networks (CNNs) are widely used for plant disease detection, they require a large number of training samples when dealing with wide variety of heterogeneous background. In this work, a CNN based dual phase method has been proposed which can work effectively on small rice grain disease dataset with heterogeneity. At the first phase, Faster RCNN method is applied for cropping out the significant portion (rice grain) from the image. This initial phase results in a secondary dataset of rice grains devoid of heterogeneous background. Disease classification is performed on such derived and simplified samples using CNN architecture. Comparison of the dual phase approach with straight forward application of CNN on the small grain dataset shows the effectiveness of the proposed method which provides a 5 fold cross validation accuracy of 88.07%.



### TTNet: Real-time temporal and spatial video analysis of table tennis
- **Arxiv ID**: http://arxiv.org/abs/2004.09927v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.09927v1)
- **Published**: 2020-04-21 11:57:51+00:00
- **Updated**: 2020-04-21 11:57:51+00:00
- **Authors**: Roman Voeikov, Nikolay Falaleev, Ruslan Baikulov
- **Comment**: 6th International Workshop on Computer Vision in Sports (CVsports) at
  CVPR 2020
- **Journal**: None
- **Summary**: We present a neural network TTNet aimed at real-time processing of high-resolution table tennis videos, providing both temporal (events spotting) and spatial (ball detection and semantic segmentation) data. This approach gives core information for reasoning score updates by an auto-referee system.   We also publish a multi-task dataset OpenTTGames with videos of table tennis games in 120 fps labeled with events, semantic segmentation masks, and ball coordinates for evaluation of multi-task approaches, primarily oriented on spotting of quick events and small objects tracking. TTNet demonstrated 97.0% accuracy in game events spotting along with 2 pixels RMSE in ball detection with 97.5% accuracy on the test part of the presented dataset.   The proposed network allows the processing of downscaled full HD videos with inference time below 6 ms per input tensor on a machine with a single consumer-grade GPU. Thus, we are contributing to the development of real-time multi-task deep learning applications and presenting approach, which is potentially capable of substituting manual data collection by sports scouts, providing support for referees' decision-making, and gathering extra information about the game process.



### Single Pair Cross-Modality Super Resolution
- **Arxiv ID**: http://arxiv.org/abs/2004.09965v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.09965v4)
- **Published**: 2020-04-21 12:57:51+00:00
- **Updated**: 2021-01-22 16:17:27+00:00
- **Authors**: Guy Shacht, Sharon Fogel, Dov Danon, Daniel Cohen-Or, Ilya Leizerson
- **Comment**: None
- **Journal**: None
- **Summary**: Non-visual imaging sensors are widely used in the industry for different purposes. Those sensors are more expensive than visual (RGB) sensors, and usually produce images with lower resolution. To this end, Cross-Modality Super-Resolution methods were introduced, where an RGB image of a high-resolution assists in increasing the resolution of the low-resolution modality. However, fusing images from different modalities is not a trivial task; the output must be artifact-free and remain loyal to the characteristics of the target modality. Moreover, the input images are never perfectly aligned, which results in further artifacts during the fusion process.   We present CMSR, a deep network for Cross-Modality Super-Resolution, which unlike previous methods, is designed to deal with weakly aligned images. The network is trained on the two input images only, learns their internal statistics and correlations, and applies them to up-sample the target modality. CMSR contains an internal transformer that is trained on-the-fly together with the up-sampling process itself, without explicit supervision. We show that CMSR succeeds to increase the resolution of the input image, gaining valuable information from its RGB counterpart, yet in a conservative way, without introducing artifacts or irrelevant details.



### Towards Generalization of 3D Human Pose Estimation In The Wild
- **Arxiv ID**: http://arxiv.org/abs/2004.09989v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.09989v1)
- **Published**: 2020-04-21 13:31:58+00:00
- **Updated**: 2020-04-21 13:31:58+00:00
- **Authors**: Renato Baptista, Alexandre Saint, Kassem Al Ismaeil, Djamila Aouada
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose 3DBodyTex.Pose, a dataset that addresses the task of 3D human pose estimation in-the-wild. Generalization to in-the-wild images remains limited due to the lack of adequate datasets. Existent ones are usually collected in indoor controlled environments where motion capture systems are used to obtain the 3D ground-truth annotations of humans. 3DBodyTex.Pose offers high quality and rich data containing 405 different real subjects in various clothing and poses, and 81k image samples with ground-truth 2D and 3D pose annotations. These images are generated from 200 viewpoints among which 70 challenging extreme viewpoints. This data was created starting from high resolution textured 3D body scans and by incorporating various realistic backgrounds. Retraining a state-of-the-art 3D pose estimation approach using data augmented with 3DBodyTex.Pose showed promising improvement in the overall performance, and a sensible decrease in the per joint position error when testing on challenging viewpoints. The 3DBodyTex.Pose is expected to offer the research community with new possibilities for generalizing 3D pose estimation from monocular in-the-wild images.



### Learning Local Neighboring Structure for Robust 3D Shape Representation
- **Arxiv ID**: http://arxiv.org/abs/2004.09995v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.09995v3)
- **Published**: 2020-04-21 13:40:03+00:00
- **Updated**: 2020-12-21 13:32:12+00:00
- **Authors**: Zhongpai Gao, Junchi Yan, Guangtao Zhai, Juyong Zhang, Yiyan Yang, Xiaokang Yang
- **Comment**: None
- **Journal**: The Thirty-Fifth AAAI Conference on Artificial Intelligence (AAAI
  2021) (https://ojs.aaai.org/index.php/AAAI/article/view/16229)
- **Summary**: Mesh is a powerful data structure for 3D shapes. Representation learning for 3D meshes is important in many computer vision and graphics applications. The recent success of convolutional neural networks (CNNs) for structured data (e.g., images) suggests the value of adapting insight from CNN for 3D shapes. However, 3D shape data are irregular since each node's neighbors are unordered. Various graph neural networks for 3D shapes have been developed with isotropic filters or predefined local coordinate systems to overcome the node inconsistency on graphs. However, isotropic filters or predefined local coordinate systems limit the representation power. In this paper, we propose a local structure-aware anisotropic convolutional operation (LSA-Conv) that learns adaptive weighting matrices for each node according to the local neighboring structure and performs shared anisotropic filters. In fact, the learnable weighting matrix is similar to the attention matrix in the random synthesizer -- a new Transformer model for natural language processing (NLP). Comprehensive experiments demonstrate that our model produces significant improvement in 3D shape reconstruction compared to state-of-the-art methods.



### Unsupervised Domain Adaptation through Inter-modal Rotation for RGB-D Object Recognition
- **Arxiv ID**: http://arxiv.org/abs/2004.10016v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2004.10016v1)
- **Published**: 2020-04-21 13:53:55+00:00
- **Updated**: 2020-04-21 13:53:55+00:00
- **Authors**: Mohammad Reza Loghmani, Luca Robbiano, Mirco Planamente, Kiru Park, Barbara Caputo, Markus Vincze
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised Domain Adaptation (DA) exploits the supervision of a label-rich source dataset to make predictions on an unlabeled target dataset by aligning the two data distributions. In robotics, DA is used to take advantage of automatically generated synthetic data, that come with "free" annotation, to make effective predictions on real data. However, existing DA methods are not designed to cope with the multi-modal nature of RGB-D data, which are widely used in robotic vision. We propose a novel RGB-D DA method that reduces the synthetic-to-real domain shift by exploiting the inter-modal relation between the RGB and depth image. Our method consists of training a convolutional neural network to solve, in addition to the main recognition task, the pretext task of predicting the relative rotation between the RGB and depth image. To evaluate our method and encourage further research in this area, we define two benchmark datasets for object categorization and instance recognition. With extensive experiments, we show the benefits of leveraging the inter-modal relations for RGB-D DA.



### Importance of Data Loading Pipeline in Training Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2005.02130v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.02130v1)
- **Published**: 2020-04-21 14:19:48+00:00
- **Updated**: 2020-04-21 14:19:48+00:00
- **Authors**: Mahdi Zolnouri, Xinlin Li, Vahid Partovi Nia
- **Comment**: None
- **Journal**: None
- **Summary**: Training large-scale deep neural networks is a long, time-consuming operation, often requiring many GPUs to accelerate. In large models, the time spent loading data takes a significant portion of model training time. As GPU servers are typically expensive, tricks that can save training time are valuable.Slow training is observed especially on real-world applications where exhaustive data augmentation operations are required. Data augmentation techniques include: padding, rotation, adding noise, down sampling, up sampling, etc. These additional operations increase the need to build an efficient data loading pipeline, and to explore existing tools to speed up training time. We focus on the comparison of two main tools designed for this task, namely binary data format to accelerate data reading, and NVIDIA DALI to accelerate data augmentation. Our study shows improvement on the order of 20% to 40% if such dedicated tools are used.



### Towards Analysis-friendly Face Representation with Scalable Feature and Texture Compression
- **Arxiv ID**: http://arxiv.org/abs/2004.10043v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.10043v2)
- **Published**: 2020-04-21 14:32:49+00:00
- **Updated**: 2021-04-19 16:40:09+00:00
- **Authors**: Shurun Wang, Shiqi Wang, Wenhan Yang, Xinfeng Zhang, Shanshe Wang, Siwei Ma, Wen Gao
- **Comment**: None
- **Journal**: None
- **Summary**: It plays a fundamental role to compactly represent the visual information towards the optimization of the ultimate utility in myriad visual data centered applications. With numerous approaches proposed to efficiently compress the texture and visual features serving human visual perception and machine intelligence respectively, much less work has been dedicated to studying the interactions between them. Here we investigate the integration of feature and texture compression, and show that a universal and collaborative visual information representation can be achieved in a hierarchical way. In particular, we study the feature and texture compression in a scalable coding framework, where the base layer serves as the deep learning feature and enhancement layer targets to perfectly reconstruct the texture. Based on the strong generative capability of deep neural networks, the gap between the base feature layer and enhancement layer is further filled with the feature level texture reconstruction, aiming to further construct texture representation from feature. As such, the residuals between the original and reconstructed texture could be further conveyed in the enhancement layer. To improve the efficiency of the proposed framework, the base layer neural network is trained in a multi-task manner such that the learned features enjoy both high quality reconstruction and high accuracy analysis. We further demonstrate the framework and optimization strategies in face image compression, and promising coding performance has been achieved in terms of both rate-fidelity and rate-accuracy.



### Frequency-Weighted Robust Tensor Principal Component Analysis
- **Arxiv ID**: http://arxiv.org/abs/2004.10068v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.10068v2)
- **Published**: 2020-04-21 14:58:21+00:00
- **Updated**: 2020-11-10 02:06:22+00:00
- **Authors**: Shenghan Wang, Yipeng Liu, Lanlan Feng, Ce Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: Robust tensor principal component analysis (RTPCA) can separate the low-rank component and sparse component from multidimensional data, which has been used successfully in several image applications. Its performance varies with different kinds of tensor decompositions, and the tensor singular value decomposition (t-SVD) is a popularly selected one. The standard t-SVD takes the discrete Fourier transform to exploit the residual in the 3rd mode in the decomposition. When minimizing the tensor nuclear norm related to t-SVD, all the frontal slices in frequency domain are optimized equally. In this paper, we incorporate frequency component analysis into t-SVD to enhance the RTPCA performance. Specially, different frequency bands are unequally weighted with respect to the corresponding physical meanings, and the frequency-weighted tensor nuclear norm can be obtained. Accordingly we rigorously deduce the frequency-weighted tensor singular value threshold operator, and apply it for low rank approximation subproblem in RTPCA. The newly obtained frequency-weighted RTPCA can be solved by alternating direction method of multipliers, and it is the first time that frequency analysis is taken in tensor principal component analysis. Numerical experiments on synthetic 3D data, color image denoising and background modeling verify that the proposed method outperforms the state-of-the-art algorithms both in accuracy and computational complexity.



### Tensor Networks for Medical Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2004.10076v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.10076v1)
- **Published**: 2020-04-21 15:02:58+00:00
- **Updated**: 2020-04-21 15:02:58+00:00
- **Authors**: Raghavendra Selvan, Erik B Dam
- **Comment**: Accepted for publication at International Conference on Medical
  Imaging with Deep Learning (MIDL), 2020. Reviews on Openreview here:
  https://openreview.net/forum?id=jjk6bxk07G
- **Journal**: None
- **Summary**: With the increasing adoption of machine learning tools like neural networks across several domains, interesting connections and comparisons to concepts from other domains are coming to light. In this work, we focus on the class of Tensor Networks, which has been a work horse for physicists in the last two decades to analyse quantum many-body systems. Building on the recent interest in tensor networks for machine learning, we extend the Matrix Product State tensor networks (which can be interpreted as linear classifiers operating in exponentially high dimensional spaces) to be useful in medical image analysis tasks. We focus on classification problems as a first step where we motivate the use of tensor networks and propose adaptions for 2D images using classical image domain concepts such as local orderlessness of images. With the proposed locally orderless tensor network model (LoTeNet), we show that tensor networks are capable of attaining performance that is comparable to state-of-the-art deep learning methods. We evaluate the model on two publicly available medical imaging datasets and show performance improvements with fewer model hyperparameters and lesser computational resources compared to relevant baseline methods.



### AMP-Net: Denoising based Deep Unfolding for Compressive Image Sensing
- **Arxiv ID**: http://arxiv.org/abs/2004.10078v2
- **DOI**: 10.1109/TIP.2020.3044472
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.10078v2)
- **Published**: 2020-04-21 15:04:17+00:00
- **Updated**: 2021-01-22 02:59:40+00:00
- **Authors**: Zhonghao Zhang, Yipeng Liu, Jiani Liu, Fei Wen, Ce Zhu
- **Comment**: None
- **Journal**: IEEE Transactions on Image Processing 2021
- **Summary**: Most compressive sensing (CS) reconstruction methods can be divided into two categories, i.e. model-based methods and classical deep network methods. By unfolding the iterative optimization algorithm for model-based methods onto networks, deep unfolding methods have the good interpretation of model-based methods and the high speed of classical deep network methods. In this paper, to solve the visual image CS problem, we propose a deep unfolding model dubbed AMP-Net. Rather than learning regularization terms, it is established by unfolding the iterative denoising process of the well-known approximate message passing algorithm. Furthermore, AMP-Net integrates deblocking modules in order to eliminate the blocking artifacts that usually appear in CS of visual images. In addition, the sampling matrix is jointly trained with other network parameters to enhance the reconstruction performance. Experimental results show that the proposed AMP-Net has better reconstruction accuracy than other state-of-the-art methods with high reconstruction speed and a small number of network parameters.



### Spatio-Temporal Deep Learning Methods for Motion Estimation Using 4D OCT Image Data
- **Arxiv ID**: http://arxiv.org/abs/2004.10114v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.10114v1)
- **Published**: 2020-04-21 15:43:01+00:00
- **Updated**: 2020-04-21 15:43:01+00:00
- **Authors**: Marcel Bengs, Nils Gessert, Matthias Schlüter, Alexander Schlaefer
- **Comment**: Accepted for publication in the International Journal of Computer
  Assisted Radiology and Surgery (IJCARS)
- **Journal**: None
- **Summary**: Purpose. Localizing structures and estimating the motion of a specific target region are common problems for navigation during surgical interventions. Optical coherence tomography (OCT) is an imaging modality with a high spatial and temporal resolution that has been used for intraoperative imaging and also for motion estimation, for example, in the context of ophthalmic surgery or cochleostomy. Recently, motion estimation between a template and a moving OCT image has been studied with deep learning methods to overcome the shortcomings of conventional, feature-based methods.   Methods. We investigate whether using a temporal stream of OCT image volumes can improve deep learning-based motion estimation performance. For this purpose, we design and evaluate several 3D and 4D deep learning methods and we propose a new deep learning approach. Also, we propose a temporal regularization strategy at the model output.   Results. Using a tissue dataset without additional markers, our deep learning methods using 4D data outperform previous approaches. The best performing 4D architecture achieves an correlation coefficient (aCC) of 98.58% compared to 85.0% of a previous 3D deep learning method. Also, our temporal regularization strategy at the output further improves 4D model performance to an aCC of 99.06%. In particular, our 4D method works well for larger motion and is robust towards image rotations and motion distortions.   Conclusions. We propose 4D spatio-temporal deep learning for OCT-based motion estimation. On a tissue dataset, we find that using 4D information for the model input improves performance while maintaining reasonable inference times. Our regularization strategy demonstrates that additional temporal information is also beneficial at the model output.



### A Deep Learning Approach for Motion Forecasting Using 4D OCT Data
- **Arxiv ID**: http://arxiv.org/abs/2004.10121v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.10121v2)
- **Published**: 2020-04-21 15:59:53+00:00
- **Updated**: 2020-06-01 10:53:43+00:00
- **Authors**: Marcel Bengs, Nils Gessert, Alexander Schlaefer
- **Comment**: Accepted for publication at MIDL 2020:
  https://openreview.net/forum?id=WVd56kgRV
- **Journal**: None
- **Summary**: Forecasting motion of a specific target object is a common problem for surgical interventions, e.g. for localization of a target region, guidance for surgical interventions, or motion compensation. Optical coherence tomography (OCT) is an imaging modality with a high spatial and temporal resolution. Recently, deep learning methods have shown promising performance for OCT-based motion estimation based on two volumetric images. We extend this approach and investigate whether using a time series of volumes enables motion forecasting. We propose 4D spatio-temporal deep learning for end-to-end motion forecasting and estimation using a stream of OCT volumes. We design and evaluate five different 3D and 4D deep learning methods using a tissue data set. Our best performing 4D method achieves motion forecasting with an overall average correlation coefficient of 97.41%, while also improving motion estimation performance by a factor of 2.5 compared to a previous 3D approach.



### Have you forgotten? A method to assess if machine learning models have forgotten data
- **Arxiv ID**: http://arxiv.org/abs/2004.10129v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.10129v2)
- **Published**: 2020-04-21 16:13:45+00:00
- **Updated**: 2020-07-12 12:50:14+00:00
- **Authors**: Xiao Liu, Sotirios A Tsaftaris
- **Comment**: Accepted by MICCAI 2020
- **Journal**: None
- **Summary**: In the era of deep learning, aggregation of data from several sources is a common approach to ensuring data diversity. Let us consider a scenario where several providers contribute data to a consortium for the joint development of a classification model (hereafter the target model), but, now one of the providers decides to leave. This provider requests that their data (hereafter the query dataset) be removed from the databases but also that the model `forgets' their data. In this paper, for the first time, we want to address the challenging question of whether data have been forgotten by a model. We assume knowledge of the query dataset and the distribution of a model's output. We establish statistical methods that compare the target's outputs with outputs of models trained with different datasets. We evaluate our approach on several benchmark datasets (MNIST, CIFAR-10 and SVHN) and on a cardiac pathology diagnosis task using data from the Automated Cardiac Diagnosis Challenge (ACDC). We hope to encourage studies on what information a model retains and inspire extensions in more complex settings.



### Natural Disaster Classification using Aerial Photography Explainable for Typhoon Damaged Feature
- **Arxiv ID**: http://arxiv.org/abs/2004.10130v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML, K.4.2; J.m
- **Links**: [PDF](http://arxiv.org/pdf/2004.10130v5)
- **Published**: 2020-04-21 16:21:52+00:00
- **Updated**: 2020-11-16 14:29:26+00:00
- **Authors**: Takato Yasuno, Masazumi Amakata, Masahiro Okano
- **Comment**: 10pages, 5figures
- **Journal**: ICPR2020 Workshops and Challenges (forthcoming on 10 January 2021)
- **Summary**: Recent years, typhoon damages has become social problem owing to climate change. In 9 September 2019, Typhoon Faxai passed on the Chiba in Japan, whose damages included with electric provision stop because of strong wind recorded on the maximum 45 meter per second. A large amount of tree fell down, and the neighbor electric poles also fell down at the same time. These disaster features have caused that it took 18 days for recovery longer than past ones. Immediate responses are important for faster recovery. As long as we can, aerial survey for global screening of devastated region would be required for decision support to respond where to recover ahead. This paper proposes a practical method to visualize the damaged areas focused on the typhoon disaster features using aerial photography. This method can classify eight classes which contains land covers without damages and areas with disaster. Using target feature class probabilities, we can visualize disaster feature map to scale a color range. Furthermore, we can realize explainable map on each unit grid images to compute the convolutional activation map using Grad-CAM. We demonstrate case studies applied to aerial photographs recorded at the Chiba region after typhoon.



### TAEN: Temporal Aware Embedding Network for Few-Shot Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2004.10141v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.10141v2)
- **Published**: 2020-04-21 16:32:10+00:00
- **Updated**: 2021-07-17 10:54:57+00:00
- **Authors**: Rami Ben-Ari, Mor Shpigel, Ophir Azulai, Udi Barzelay, Daniel Rotman
- **Comment**: None
- **Journal**: Published in Learning from Limited and Imperfect Data (L2ID)
  Workshop - CVPR 2021
- **Summary**: Classification of new class entities requires collecting and annotating hundreds or thousands of samples that is often prohibitively costly. Few-shot learning suggests learning to classify new classes using just a few examples. Only a small number of studies address the challenge of few-shot learning on spatio-temporal patterns such as videos. In this paper, we present the Temporal Aware Embedding Network (TAEN) for few-shot action recognition, that learns to represent actions, in a metric space as a trajectory, conveying both short term semantics and longer term connectivity between action parts. We demonstrate the effectiveness of TAEN on two few shot tasks, video classification and temporal action detection and evaluate our method on the Kinetics-400 and on ActivityNet 1.2 few-shot benchmarks. With training of just a few fully connected layers we reach comparable results to prior art on both few shot video classification and temporal detection tasks, while reaching state-of-the-art in certain scenarios.



### Spatio-spectral deep learning methods for in-vivo hyperspectral laryngeal cancer detection
- **Arxiv ID**: http://arxiv.org/abs/2004.10159v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.10159v1)
- **Published**: 2020-04-21 17:07:18+00:00
- **Updated**: 2020-04-21 17:07:18+00:00
- **Authors**: Marcel Bengs, Stephan Westermann, Nils Gessert, Dennis Eggert, Andreas O. H. Gerstner, Nina A. Mueller, Christian Betz, Wiebke Laffers, Alexander Schlaefer
- **Comment**: Accepted at SPIE Medical Imaging 2020
- **Journal**: None
- **Summary**: Early detection of head and neck tumors is crucial for patient survival. Often, diagnoses are made based on endoscopic examination of the larynx followed by biopsy and histological analysis, leading to a high inter-observer variability due to subjective assessment. In this regard, early non-invasive diagnostics independent of the clinician would be a valuable tool. A recent study has shown that hyperspectral imaging (HSI) can be used for non-invasive detection of head and neck tumors, as precancerous or cancerous lesions show specific spectral signatures that distinguish them from healthy tissue. However, HSI data processing is challenging due to high spectral variations, various image interferences, and the high dimensionality of the data. Therefore, performance of automatic HSI analysis has been limited and so far, mostly ex-vivo studies have been presented with deep learning. In this work, we analyze deep learning techniques for in-vivo hyperspectral laryngeal cancer detection. For this purpose we design and evaluate convolutional neural networks (CNNs) with 2D spatial or 3D spatio-spectral convolutions combined with a state-of-the-art Densenet architecture. For evaluation, we use an in-vivo data set with HSI of the oral cavity or oropharynx. Overall, we present multiple deep learning techniques for in-vivo laryngeal cancer detection based on HSI and we show that jointly learning from the spatial and spectral domain improves classification accuracy notably. Our 3D spatio-spectral Densenet achieves an average accuracy of 81%.



### EMPIR: Ensembles of Mixed Precision Deep Networks for Increased Robustness against Adversarial Attacks
- **Arxiv ID**: http://arxiv.org/abs/2004.10162v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.10162v1)
- **Published**: 2020-04-21 17:17:09+00:00
- **Updated**: 2020-04-21 17:17:09+00:00
- **Authors**: Sanchari Sen, Balaraman Ravindran, Anand Raghunathan
- **Comment**: Published as a conference paper at ICLR 2020
- **Journal**: None
- **Summary**: Ensuring robustness of Deep Neural Networks (DNNs) is crucial to their adoption in safety-critical applications such as self-driving cars, drones, and healthcare. Notably, DNNs are vulnerable to adversarial attacks in which small input perturbations can produce catastrophic misclassifications. In this work, we propose EMPIR, ensembles of quantized DNN models with different numerical precisions, as a new approach to increase robustness against adversarial attacks. EMPIR is based on the observation that quantized neural networks often demonstrate much higher robustness to adversarial attacks than full precision networks, but at the cost of a substantial loss in accuracy on the original (unperturbed) inputs. EMPIR overcomes this limitation to achieve the 'best of both worlds', i.e., the higher unperturbed accuracies of the full precision models combined with the higher robustness of the low precision models, by composing them in an ensemble. Further, as low precision DNN models have significantly lower computational and storage requirements than full precision models, EMPIR models only incur modest compute and memory overheads compared to a single full-precision model (<25% in our evaluations). We evaluate EMPIR across a suite of DNNs for 3 different image recognition tasks (MNIST, CIFAR-10 and ImageNet) and under 4 different adversarial attacks. Our results indicate that EMPIR boosts the average adversarial accuracies by 42.6%, 15.2% and 10.5% for the DNN models trained on the MNIST, CIFAR-10 and ImageNet datasets respectively, when compared to single full-precision models, without sacrificing accuracy on the unperturbed inputs.



### 4D Spatio-Temporal Deep Learning with 4D fMRI Data for Autism Spectrum Disorder Classification
- **Arxiv ID**: http://arxiv.org/abs/2004.10165v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.10165v1)
- **Published**: 2020-04-21 17:19:06+00:00
- **Updated**: 2020-04-21 17:19:06+00:00
- **Authors**: Marcel Bengs, Nils Gessert, Alexander Schlaefer
- **Comment**: Accepted at MIDL 2019
- **Journal**: None
- **Summary**: Autism spectrum disorder (ASD) is associated with behavioral and communication problems. Often, functional magnetic resonance imaging (fMRI) is used to detect and characterize brain changes related to the disorder. Recently, machine learning methods have been employed to reveal new patterns by trying to classify ASD from spatio-temporal fMRI images. Typically, these methods have either focused on temporal or spatial information processing. Instead, we propose a 4D spatio-temporal deep learning approach for ASD classification where we jointly learn from spatial and temporal data. We employ 4D convolutional neural networks and convolutional-recurrent models which outperform a previous approach with an F1-score of 0.71 compared to an F1-score of 0.65.



### Never Stop Learning: The Effectiveness of Fine-Tuning in Robotic Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/2004.10190v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.10190v2)
- **Published**: 2020-04-21 17:57:04+00:00
- **Updated**: 2020-07-31 13:43:53+00:00
- **Authors**: Ryan Julian, Benjamin Swanson, Gaurav S. Sukhatme, Sergey Levine, Chelsea Finn, Karol Hausman
- **Comment**: 8.5 pages, 9 figures. See video overview and experiments at
  https://youtu.be/pPDVewcSpdc and project website at
  https://ryanjulian.me/continual-fine-tuning
- **Journal**: None
- **Summary**: One of the great promises of robot learning systems is that they will be able to learn from their mistakes and continuously adapt to ever-changing environments. Despite this potential, most of the robot learning systems today are deployed as a fixed policy and they are not being adapted after their deployment. Can we efficiently adapt previously learned behaviors to new environments, objects and percepts in the real world? In this paper, we present a method and empirical evidence towards a robot learning framework that facilitates continuous adaption. In particular, we demonstrate how to adapt vision-based robotic manipulation policies to new variations by fine-tuning via off-policy reinforcement learning, including changes in background, object shape and appearance, lighting conditions, and robot morphology. Further, this adaptation uses less than 0.2% of the data necessary to learn the task from scratch. We find that our approach of adapting pre-trained policies leads to substantial performance gains over the course of fine-tuning, and that pre-training via RL is essential: training from scratch or adapting from supervised ImageNet features are both unsuccessful with such small amounts of data. We also find that these positive results hold in a limited continual learning setting, in which we repeatedly fine-tune a single lineage of policies using data from a succession of new tasks. Our empirical conclusions are consistently supported by experiments on simulated manipulation tasks, and by 52 unique fine-tuning experiments on a real robotic grasping system pre-trained on 580,000 grasps.



### Partial Volume Segmentation of Brain MRI Scans of any Resolution and Contrast
- **Arxiv ID**: http://arxiv.org/abs/2004.10221v3
- **DOI**: 10.1007/978-3-030-59728-3_18
- **Categories**: **cs.CV**, cs.LG, eess.IV, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2004.10221v3)
- **Published**: 2020-04-21 18:04:44+00:00
- **Updated**: 2021-04-08 12:56:24+00:00
- **Authors**: Benjamin Billot, Eleanor D. Robinson, Adrian V. Dalca, Juan Eugenio Iglesias
- **Comment**: 12 pages, 7 figures
- **Journal**: International Conference on Medical Image Computing and
  Computer-Assisted Intervention (MICCAI) 2020, pp. 177-187
- **Summary**: Partial voluming (PV) is arguably the last crucial unsolved problem in Bayesian segmentation of brain MRI with probabilistic atlases. PV occurs when voxels contain multiple tissue classes, giving rise to image intensities that may not be representative of any one of the underlying classes. PV is particularly problematic for segmentation when there is a large resolution gap between the atlas and the test scan, e.g., when segmenting clinical scans with thick slices, or when using a high-resolution atlas. In this work, we present PV-SynthSeg, a convolutional neural network (CNN) that tackles this problem by directly learning a mapping between (possibly multi-modal) low resolution (LR) scans and underlying high resolution (HR) segmentations. PV-SynthSeg simulates LR images from HR label maps with a generative model of PV, and can be trained to segment scans of any desired target contrast and resolution, even for previously unseen modalities where neither images nor segmentations are available at training. PV-SynthSeg does not require any preprocessing, and runs in seconds. We demonstrate the accuracy and flexibility of the method with extensive experiments on three datasets and 2,680 scans. The code is available at https://github.com/BBillot/SynthSeg.



### ParaCNN: Visual Paragraph Generation via Adversarial Twin Contextual CNNs
- **Arxiv ID**: http://arxiv.org/abs/2004.10258v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.10258v1)
- **Published**: 2020-04-21 19:54:18+00:00
- **Updated**: 2020-04-21 19:54:18+00:00
- **Authors**: Shiyang Yan, Yang Hua, Neil Robertson
- **Comment**: None
- **Journal**: None
- **Summary**: Image description generation plays an important role in many real-world applications, such as image retrieval, automatic navigation, and disabled people support. A well-developed task of image description generation is image captioning, which usually generates a short captioning sentence and thus neglects many of fine-grained properties, e.g., the information of subtle objects and their relationships. In this paper, we study the visual paragraph generation, which can describe the image with a long paragraph containing rich details. Previous research often generates the paragraph via a hierarchical Recurrent Neural Network (RNN)-like model, which has complex memorising, forgetting and coupling mechanism. Instead, we propose a novel pure CNN model, ParaCNN, to generate visual paragraph using hierarchical CNN architecture with contextual information between sentences within one paragraph. The ParaCNN can generate an arbitrary length of a paragraph, which is more applicable in many real-world applications. Furthermore, to enable the ParaCNN to model paragraph comprehensively, we also propose an adversarial twin net training scheme. During training, we force the forwarding network's hidden features to be close to that of the backwards network by using adversarial training. During testing, we only use the forwarding network, which already includes the knowledge of the backwards network, to generate a paragraph. We conduct extensive experiments on the Stanford Visual Paragraph dataset and achieve state-of-the-art performance.



### SynthMorph: learning contrast-invariant registration without acquired images
- **Arxiv ID**: http://arxiv.org/abs/2004.10282v4
- **DOI**: 10.1109/TMI.2021.3116879
- **Categories**: **eess.IV**, cs.CV, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/2004.10282v4)
- **Published**: 2020-04-21 20:29:39+00:00
- **Updated**: 2022-03-03 14:46:48+00:00
- **Authors**: Malte Hoffmann, Benjamin Billot, Douglas N. Greve, Juan Eugenio Iglesias, Bruce Fischl, Adrian V. Dalca
- **Comment**: 16 pages, 15 figures, 3 tables, deformable image registration, data
  independence, deep learning, MRI-contrast invariance, anatomy agnosticism,
  final published version
- **Journal**: IEEE Trans Med Imaging, 41 (3), 2022, 543-558
- **Summary**: We introduce a strategy for learning image registration without acquired imaging data, producing powerful networks agnostic to contrast introduced by magnetic resonance imaging (MRI). While classical registration methods accurately estimate the spatial correspondence between images, they solve an optimization problem for every new image pair. Learning-based techniques are fast at test time but limited to registering images with contrasts and geometric content similar to those seen during training. We propose to remove this dependency on training data by leveraging a generative strategy for diverse synthetic label maps and images that exposes networks to a wide range of variability, forcing them to learn more invariant features. This approach results in powerful networks that accurately generalize to a broad array of MRI contrasts. We present extensive experiments with a focus on 3D neuroimaging, showing that this strategy enables robust and accurate registration of arbitrary MRI contrasts even if the target contrast is not seen by the networks during training. We demonstrate registration accuracy surpassing the state of the art both within and across contrasts, using a single model. Critically, training on arbitrary shapes synthesized from noise distributions results in competitive performance, removing the dependency on acquired data of any kind. Additionally, since anatomical label maps are often available for the anatomy of interest, we show that synthesizing images from these dramatically boosts performance, while still avoiding the need for real intensity images. Our code is available at https://w3id.org/synthmorph.



### Panoptic-based Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2004.10289v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.10289v1)
- **Published**: 2020-04-21 20:40:53+00:00
- **Updated**: 2020-04-21 20:40:53+00:00
- **Authors**: Aysegul Dundar, Karan Sapra, Guilin Liu, Andrew Tao, Bryan Catanzaro
- **Comment**: CVPR 2020
- **Journal**: None
- **Summary**: Conditional image synthesis for generating photorealistic images serves various applications for content editing to content generation. Previous conditional image synthesis algorithms mostly rely on semantic maps, and often fail in complex environments where multiple instances occlude each other. We propose a panoptic aware image synthesis network to generate high fidelity and photorealistic images conditioned on panoptic maps which unify semantic and instance information. To achieve this, we efficiently use panoptic maps in convolution and upsampling layers. We show that with the proposed changes to the generator, we can improve on the previous state-of-the-art methods by generating images in complex instance interaction environments in higher fidelity and tiny objects in more details. Furthermore, our proposed method also outperforms the previous state-of-the-art methods in metrics of mean IoU (Intersection over Union), and detAP (Detection Average Precision).



### M-LVC: Multiple Frames Prediction for Learned Video Compression
- **Arxiv ID**: http://arxiv.org/abs/2004.10290v1
- **DOI**: 10.1109/CVPR42600.2020.00360
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.10290v1)
- **Published**: 2020-04-21 20:42:02+00:00
- **Updated**: 2020-04-21 20:42:02+00:00
- **Authors**: Jianping Lin, Dong Liu, Houqiang Li, Feng Wu
- **Comment**: Accepted to appear in CVPR2020; camera-ready
- **Journal**: None
- **Summary**: We propose an end-to-end learned video compression scheme for low-latency scenarios. Previous methods are limited in using the previous one frame as reference. Our method introduces the usage of the previous multiple frames as references. In our scheme, the motion vector (MV) field is calculated between the current frame and the previous one. With multiple reference frames and associated multiple MV fields, our designed network can generate more accurate prediction of the current frame, yielding less residual. Multiple reference frames also help generate MV prediction, which reduces the coding cost of MV field. We use two deep auto-encoders to compress the residual and the MV, respectively. To compensate for the compression error of the auto-encoders, we further design a MV refinement network and a residual refinement network, taking use of the multiple reference frames as well. All the modules in our scheme are jointly optimized through a single rate-distortion loss function. We use a step-by-step training strategy to optimize the entire scheme. Experimental results show that the proposed method outperforms the existing learned video compression methods for low-latency mode. Our method also performs better than H.265 in both PSNR and MS-SSIM. Our code and models are publicly available.



### Group Activity Detection from Trajectory and Video Data in Soccer
- **Arxiv ID**: http://arxiv.org/abs/2004.10299v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.10299v1)
- **Published**: 2020-04-21 21:11:30+00:00
- **Updated**: 2020-04-21 21:11:30+00:00
- **Authors**: Ryan Sanford, Siavash Gorji, Luiz G. Hafemann, Bahareh Pourbabaee, Mehrsan Javan
- **Comment**: Accepted to the 6th International Workshop on Computer Vision in
  Sports (CVsports) at CVPR 2020
- **Journal**: None
- **Summary**: Group activity detection in soccer can be done by using either video data or player and ball trajectory data. In current soccer activity datasets, activities are labelled as atomic events without a duration. Given that the state-of-the-art activity detection methods are not well-defined for atomic actions, these methods cannot be used. In this work, we evaluated the effectiveness of activity recognition models for detecting such events, by using an intuitive non-maximum suppression process and evaluation metrics. We also considered the problem of explicitly modeling interactions between players and ball. For this, we propose self-attention models to learn and extract relevant information from a group of soccer players for activity detection from both trajectory and video data. We conducted an extensive study on the use of visual features and trajectory data for group activity detection in sports using a large scale soccer dataset provided by Sportlogiq. Our results show that most events can be detected using either vision or trajectory-based approaches with a temporal resolution of less than 0.5 seconds, and that each approach has unique challenges.



### Keep It Real: a Window to Real Reality in Virtual Reality
- **Arxiv ID**: http://arxiv.org/abs/2004.10313v3
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.10313v3)
- **Published**: 2020-04-21 21:33:14+00:00
- **Updated**: 2020-11-12 07:02:43+00:00
- **Authors**: Baihan Lin
- **Comment**: IJCAI 2020
- **Journal**: None
- **Summary**: This paper proposed a new interaction paradigm in the virtual reality (VR) environments, which consists of a virtual mirror or window projected onto a virtual surface, representing the correct perspective geometry of a mirror or window reflecting the real world. This technique can be applied to various videos, live streaming apps, augmented and virtual reality settings to provide an interactive and immersive user experience. To support such a perspective-accurate representation, we implemented computer vision algorithms for feature detection and correspondence matching. To constrain the solutions, we incorporated an automatically tuning scaling factor upon the homography transform matrix such that each image frame follows a smooth transition with the user in sight. The system is a real-time rendering framework where users can engage their real-life presence with the virtual space.



### Combining Deep Learning Classifiers for 3D Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2004.10314v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2004.10314v1)
- **Published**: 2020-04-21 21:36:25+00:00
- **Updated**: 2020-04-21 21:36:25+00:00
- **Authors**: Jan Sedmidubsky, Pavel Zezula
- **Comment**: Submitted to Pattern Recognition Letters
- **Journal**: None
- **Summary**: The popular task of 3D human action recognition is almost exclusively solved by training deep-learning classifiers. To achieve a high recognition accuracy, the input 3D actions are often pre-processed by various normalization or augmentation techniques. However, it is not computationally feasible to train a classifier for each possible variant of training data in order to select the best-performing subset of pre-processing techniques for a given dataset. In this paper, we propose to train an independent classifier for each available pre-processing technique and fuse the classification results based on a strict majority vote rule. Together with a proposed evaluation procedure, we can very efficiently determine the best combination of normalization and augmentation techniques for a specific dataset. For the best-performing combination, we can retrospectively apply the normalized/augmented variants of input data to train only a single classifier. This also allows us to decide whether it is better to train a single model, or rather a set of independent classifiers.



### Multi-view Self-Constructing Graph Convolutional Networks with Adaptive Class Weighting Loss for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2004.10327v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.10327v1)
- **Published**: 2020-04-21 22:18:16+00:00
- **Updated**: 2020-04-21 22:18:16+00:00
- **Authors**: Qinghui Liu, Michael Kampffmeyer, Robert Jenssen, Arnt-Børre Salberg
- **Comment**: 7-page, MSCG-Net, CVPRW-2020
- **Journal**: None
- **Summary**: We propose a novel architecture called the Multi-view Self-Constructing Graph Convolutional Networks (MSCG-Net) for semantic segmentation. Building on the recently proposed Self-Constructing Graph (SCG) module, which makes use of learnable latent variables to self-construct the underlying graphs directly from the input features without relying on manually built prior knowledge graphs, we leverage multiple views in order to explicitly exploit the rotational invariance in airborne images. We further develop an adaptive class weighting loss to address the class imbalance. We demonstrate the effectiveness and flexibility of the proposed method on the Agriculture-Vision challenge dataset and our model achieves very competitive results (0.547 mIoU) with much fewer parameters and at a lower computational cost compared to related pure-CNN based work. Code will be available at: github.com/samleoqh/MSCG-Net



### How to track your dragon: A Multi-Attentional Framework for real-time RGB-D 6-DOF Object Pose Tracking
- **Arxiv ID**: http://arxiv.org/abs/2004.10335v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.10335v3)
- **Published**: 2020-04-21 23:00:06+00:00
- **Updated**: 2020-09-15 11:33:55+00:00
- **Authors**: Isidoros Marougkas, Petros Koutras, Nikos Kardaris, Georgios Retsinas, Georgia Chalvatzaki, Petros Maragos
- **Comment**: 14 pages, accepted at the 6th Workshop on Recovering 6D Object Pose
  of the ECCV 2020
- **Journal**: None
- **Summary**: We present a novel multi-attentional convolutional architecture to tackle the problem of real-time RGB-D 6D object pose tracking of single, known objects. Such a problem poses multiple challenges originating both from the objects' nature and their interaction with their environment, which previous approaches have failed to fully address. The proposed framework encapsulates methods for background clutter and occlusion handling by integrating multiple parallel soft spatial attention modules into a multitask Convolutional Neural Network (CNN) architecture. Moreover, we consider the special geometrical properties of both the object's 3D model and the pose space, and we use a more sophisticated approach for data augmentation during training. The provided experimental results confirm the effectiveness of the proposed multi-attentional architecture, as it improves the State-of-the-Art (SoA) tracking performance by an average score of 34.03% for translation and 40.01% for rotation, when tested on the most complete dataset designed, up to date,for the problem of RGB-D object tracking.



### The iWildCam 2020 Competition Dataset
- **Arxiv ID**: http://arxiv.org/abs/2004.10340v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.10340v1)
- **Published**: 2020-04-21 23:25:13+00:00
- **Updated**: 2020-04-21 23:25:13+00:00
- **Authors**: Sara Beery, Elijah Cole, Arvi Gjoka
- **Comment**: Fine-Grained Visual Categorization Workshop at CVPR 2020
- **Journal**: None
- **Summary**: Camera traps enable the automatic collection of large quantities of image data. Biologists all over the world use camera traps to monitor animal populations. We have recently been making strides towards automatic species classification in camera trap images. However, as we try to expand the geographic scope of these models we are faced with an interesting question: how do we train models that perform well on new (unseen during training) camera trap locations? Can we leverage data from other modalities, such as citizen science data and remote sensing data? In order to tackle this problem, we have prepared a challenge where the training data and test data are from different cameras spread across the globe. For each camera, we provide a series of remote sensing imagery that is tied to the location of the camera. We also provide citizen science imagery from the set of species seen in our data. The challenge is to correctly classify species in the test camera traps.



### Textual Visual Semantic Dataset for Text Spotting
- **Arxiv ID**: http://arxiv.org/abs/2004.10349v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2004.10349v1)
- **Published**: 2020-04-21 23:58:16+00:00
- **Updated**: 2020-04-21 23:58:16+00:00
- **Authors**: Ahmed Sabir, Francesc Moreno-Noguer, Lluís Padró
- **Comment**: None
- **Journal**: None
- **Summary**: Text Spotting in the wild consists of detecting and recognizing text appearing in images (e.g. signboards, traffic signals or brands in clothing or objects). This is a challenging problem due to the complexity of the context where texts appear (uneven backgrounds, shading, occlusions, perspective distortions, etc.). Only a few approaches try to exploit the relation between text and its surrounding environment to better recognize text in the scene. In this paper, we propose a visual context dataset for Text Spotting in the wild, where the publicly available dataset COCO-text [Veit et al. 2016] has been extended with information about the scene (such as objects and places appearing in the image) to enable researchers to include semantic relations between texts and scene in their Text Spotting systems, and to offer a common framework for such approaches. For each text in an image, we extract three kinds of context information: objects in the scene, image location label and a textual image description (caption). We use state-of-the-art out-of-the-box available tools to extract this additional information. Since this information has textual form, it can be used to leverage text similarity or semantic relation methods into Text Spotting systems, either as a post-processing or in an end-to-end training strategy. Our data is publicly available at https://git.io/JeZTb.



### L-CO-Net: Learned Condensation-Optimization Network for Clinical Parameter Estimation from Cardiac Cine MRI
- **Arxiv ID**: http://arxiv.org/abs/2004.11253v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.11253v1)
- **Published**: 2020-04-21 23:59:07+00:00
- **Updated**: 2020-04-21 23:59:07+00:00
- **Authors**: S. M. Kamrul Hasan, Cristian A. Linte
- **Comment**: 6 pages, 5 figures, IEEE Conference. arXiv admin note: text overlap
  with arXiv:2004.02249
- **Journal**: None
- **Summary**: In this work, we implement a fully convolutional segmenter featuring both a learned group structure and a regularized weight-pruner to reduce the high computational cost in volumetric image segmentation. We validated our framework on the ACDC dataset featuring one healthy and four pathology groups imaged throughout the cardiac cycle. Our technique achieved Dice scores of 96.8% (LV blood-pool), 93.3% (RV blood-pool) and 90.0% (LV Myocardium) with five-fold cross-validation and yielded similar clinical parameters as those estimated from the ground truth segmentation data. Based on these results, this technique has the potential to become an efficient and competitive cardiac image segmentation tool that may be used for cardiac computer-aided diagnosis, planning, and guidance applications.



