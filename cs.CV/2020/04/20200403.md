# Arxiv Papers in cs.CV on 2020-04-03
### Towards Better Generalization: Joint Depth-Pose Learning without PoseNet
- **Arxiv ID**: http://arxiv.org/abs/2004.01314v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2004.01314v2)
- **Published**: 2020-04-03 00:28:09+00:00
- **Updated**: 2021-09-03 09:45:39+00:00
- **Authors**: Wang Zhao, Shaohui Liu, Yezhi Shu, Yong-Jin Liu
- **Comment**: To appear in CVPR 2020
- **Journal**: None
- **Summary**: In this work, we tackle the essential problem of scale inconsistency for self-supervised joint depth-pose learning. Most existing methods assume that a consistent scale of depth and pose can be learned across all input samples, which makes the learning problem harder, resulting in degraded performance and limited generalization in indoor environments and long-sequence visual odometry application. To address this issue, we propose a novel system that explicitly disentangles scale from the network estimation. Instead of relying on PoseNet architecture, our method recovers relative pose by directly solving fundamental matrix from dense optical flow correspondence and makes use of a two-view triangulation module to recover an up-to-scale 3D structure. Then, we align the scale of the depth prediction with the triangulated point cloud and use the transformed depth map for depth error computation and dense reprojection check. Our whole system can be jointly trained end-to-end. Extensive experiments show that our system not only reaches state-of-the-art performance on KITTI depth and flow estimation, but also significantly improves the generalization ability of existing self-supervised depth-pose learning methods under a variety of challenging scenarios, and achieves state-of-the-art results among self-supervised learning-based methods on KITTI Odometry and NYUv2 dataset. Furthermore, we present some interesting findings on the limitation of PoseNet-based relative pose estimation methods in terms of generalization ability. Code is available at https://github.com/B1ueber2y/TrianFlow.



### From Paris to Berlin: Discovering Fashion Style Influences Around the World
- **Arxiv ID**: http://arxiv.org/abs/2004.01316v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.SI
- **Links**: [PDF](http://arxiv.org/pdf/2004.01316v2)
- **Published**: 2020-04-03 00:54:23+00:00
- **Updated**: 2020-08-09 02:34:18+00:00
- **Authors**: Ziad Al-Halah, Kristen Grauman
- **Comment**: CVPR 2020. Project page:
  https://www.cs.utexas.edu/~ziad/fashion_influence.html
- **Journal**: None
- **Summary**: The evolution of clothing styles and their migration across the world is intriguing, yet difficult to describe quantitatively. We propose to discover and quantify fashion influences from everyday images of people wearing clothes. We introduce an approach that detects which cities influence which other cities in terms of propagating their styles. We then leverage the discovered influence patterns to inform a forecasting model that predicts the popularity of any given style at any given city into the future. Demonstrating our idea with GeoStyle---a large-scale dataset of 7.7M images covering 44 major world cities, we present the discovered influence relationships, revealing how cities exert and receive fashion influence for an array of 50 observed visual styles. Furthermore, the proposed forecasting model achieves state-of-the-art results for a challenging style forecasting task, showing the advantage of grounding visual style evolution both spatially and temporally.



### A Fast Fully Octave Convolutional Neural Network for Document Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2004.01317v1
- **DOI**: 10.1109/IJCNN48605.2020.9206711
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.01317v1)
- **Published**: 2020-04-03 00:57:33+00:00
- **Updated**: 2020-04-03 00:57:33+00:00
- **Authors**: Ricardo Batista das Neves Junior, Luiz Felipe Verçosa, David Macêdo, Byron Leite Dantas Bezerra, Cleber Zanchettin
- **Comment**: This paper was accepted for IJCNN 2020 Conference
- **Journal**: 2020 International Joint Conference on Neural Networks (IJCNN)
- **Summary**: The Know Your Customer (KYC) and Anti Money Laundering (AML) are worldwide practices to online customer identification based on personal identification documents, similarity and liveness checking, and proof of address. To answer the basic regulation question: are you whom you say you are? The customer needs to upload valid identification documents (ID). This task imposes some computational challenges since these documents are diverse, may present different and complex backgrounds, some occlusion, partial rotation, poor quality, or damage. Advanced text and document segmentation algorithms were used to process the ID images. In this context, we investigated a method based on U-Net to detect the document edges and text regions in ID images. Besides the promising results on image segmentation, the U-Net based approach is computationally expensive for a real application, since the image segmentation is a customer device task. We propose a model optimization based on Octave Convolutions to qualify the method to situations where storage, processing, and time resources are limited, such as in mobile and robotic applications. We conducted the evaluation experiments in two new datasets CDPhotoDataset and DTDDataset, which are composed of real ID images of Brazilian documents. Our results showed that the proposed models are efficient to document segmentation tasks and portable.



### Learning Pose-invariant 3D Object Reconstruction from Single-view Images
- **Arxiv ID**: http://arxiv.org/abs/2004.01347v2
- **DOI**: 10.1016/j.neucom.2020.10.089
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.01347v2)
- **Published**: 2020-04-03 02:47:35+00:00
- **Updated**: 2020-07-27 08:11:32+00:00
- **Authors**: Bo Peng, Wei Wang, Jing Dong, Tieniu Tan
- **Comment**: under review, code available at https://github.com/bomb2peng/learn3D
- **Journal**: Neurocomputing Volume 423, 29 January 2021, Pages 407-418
- **Summary**: Learning to reconstruct 3D shapes using 2D images is an active research topic, with benefits of not requiring expensive 3D data. However, most work in this direction requires multi-view images for each object instance as training supervision, which oftentimes does not apply in practice. In this paper, we relax the common multi-view assumption and explore a more challenging yet more realistic setup of learning 3D shape from only single-view images. The major difficulty lies in insufficient constraints that can be provided by single view images, which leads to the problem of pose entanglement in learned shape space. As a result, reconstructed shapes vary along input pose and have poor accuracy. We address this problem by taking a novel domain adaptation perspective, and propose an effective adversarial domain confusion method to learn pose-disentangled compact shape space. Experiments on single-view reconstruction show effectiveness in solving pose entanglement, and the proposed method achieves on-par reconstruction accuracy with state-of-the-art with higher efficiency.



### Context-Aware Multi-Task Learning for Traffic Scene Recognition in Autonomous Vehicles
- **Arxiv ID**: http://arxiv.org/abs/2004.01351v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.01351v1)
- **Published**: 2020-04-03 03:09:26+00:00
- **Updated**: 2020-04-03 03:09:26+00:00
- **Authors**: Younkwan Lee, Jihyo Jeon, Jongmin Yu, Moongu Jeon
- **Comment**: Accepted to IV 2020
- **Journal**: None
- **Summary**: Traffic scene recognition, which requires various visual classification tasks, is a critical ingredient in autonomous vehicles. However, most existing approaches treat each relevant task independently from one another, never considering the entire system as a whole. Because of this, they are limited to utilizing a task-specific set of features for all possible tasks of inference-time, which ignores the capability to leverage common task-invariant contextual knowledge for the task at hand. To address this problem, we propose an algorithm to jointly learn the task-specific and shared representations by adopting a multi-task learning network. Specifically, we present a lower bound for the mutual information constraint between shared feature embedding and input that is considered to be able to extract common contextual information across tasks while preserving essential information of each task jointly. The learned representations capture richer contextual information without additional task-specific network. Extensive experiments on the large-scale dataset HSD demonstrate the effectiveness and superiority of our network over state-of-the-art methods.



### Deep White-Balance Editing
- **Arxiv ID**: http://arxiv.org/abs/2004.01354v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.01354v1)
- **Published**: 2020-04-03 03:18:42+00:00
- **Updated**: 2020-04-03 03:18:42+00:00
- **Authors**: Mahmoud Afifi, Michael S. Brown
- **Comment**: Accepted as Oral at CVPR 2020
- **Journal**: None
- **Summary**: We introduce a deep learning approach to realistically edit an sRGB image's white balance. Cameras capture sensor images that are rendered by their integrated signal processor (ISP) to a standard RGB (sRGB) color space encoding. The ISP rendering begins with a white-balance procedure that is used to remove the color cast of the scene's illumination. The ISP then applies a series of nonlinear color manipulations to enhance the visual quality of the final sRGB image. Recent work by [3] showed that sRGB images that were rendered with the incorrect white balance cannot be easily corrected due to the ISP's nonlinear rendering. The work in [3] proposed a k-nearest neighbor (KNN) solution based on tens of thousands of image pairs. We propose to solve this problem with a deep neural network (DNN) architecture trained in an end-to-end manner to learn the correct white balance. Our DNN maps an input image to two additional white-balance settings corresponding to indoor and outdoor illuminations. Our solution not only is more accurate than the KNN approach in terms of correcting a wrong white-balance setting but also provides the user the freedom to edit the white balance in the sRGB image to other illumination settings.



### FairALM: Augmented Lagrangian Method for Training Fair Models with Little Regret
- **Arxiv ID**: http://arxiv.org/abs/2004.01355v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.01355v2)
- **Published**: 2020-04-03 03:18:53+00:00
- **Updated**: 2020-06-24 00:17:37+00:00
- **Authors**: Vishnu Suresh Lokhande, Aditya Kumar Akash, Sathya N. Ravi, Vikas Singh
- **Comment**: None
- **Journal**: None
- **Summary**: Algorithmic decision making based on computer vision and machine learning technologies continue to permeate our lives. But issues related to biases of these models and the extent to which they treat certain segments of the population unfairly, have led to concern in the general public. It is now accepted that because of biases in the datasets we present to the models, a fairness-oblivious training will lead to unfair models. An interesting topic is the study of mechanisms via which the de novo design or training of the model can be informed by fairness measures. Here, we study mechanisms that impose fairness concurrently while training the model. While existing fairness based approaches in vision have largely relied on training adversarial modules together with the primary classification/regression task, in an effort to remove the influence of the protected attribute or variable, we show how ideas based on well-known optimization concepts can provide a simpler alternative. In our proposed scheme, imposing fairness just requires specifying the protected attribute and utilizing our optimization routine. We provide a detailed technical analysis and present experiments demonstrating that various fairness measures from the literature can be reliably imposed on a number of training tasks in vision in a manner that is interpretable.



### Characterization of Multiple 3D LiDARs for Localization and Mapping using Normal Distributions Transform
- **Arxiv ID**: http://arxiv.org/abs/2004.01374v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.01374v1)
- **Published**: 2020-04-03 05:05:36+00:00
- **Updated**: 2020-04-03 05:05:36+00:00
- **Authors**: Alexander Carballo, Abraham Monrroy, David Wong, Patiphon Narksri, Jacob Lambert, Yuki Kitsukawa, Eijiro Takeuchi, Shinpei Kato, Kazuya Takeda
- **Comment**: Submitted to IEEE International Conference on Intelligent
  Transportation Systems(ITSC) 2020 LIBRE dataset is available at
  https://sites.google.com/g.sp.m.is.nagoya-u.ac.jp/libre-dataset
- **Journal**: None
- **Summary**: In this work, we present a detailed comparison of ten different 3D LiDAR sensors, covering a range of manufacturers, models, and laser configurations, for the tasks of mapping and vehicle localization, using as common reference the Normal Distributions Transform (NDT) algorithm implemented in the self-driving open source platform Autoware. LiDAR data used in this study is a subset of our LiDAR Benchmarking and Reference (LIBRE) dataset, captured independently from each sensor, from a vehicle driven on public urban roads multiple times, at different times of the day. In this study, we analyze the performance and characteristics of each LiDAR for the tasks of (1) 3D mapping including an assessment map quality based on mean map entropy, and (2) 6-DOF localization using a ground truth reference map.



### Sequential Learning for Domain Generalization
- **Arxiv ID**: http://arxiv.org/abs/2004.01377v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.01377v1)
- **Published**: 2020-04-03 05:10:33+00:00
- **Updated**: 2020-04-03 05:10:33+00:00
- **Authors**: Da Li, Yongxin Yang, Yi-Zhe Song, Timothy Hospedales
- **Comment**: tech report
- **Journal**: None
- **Summary**: In this paper we propose a sequential learning framework for Domain Generalization (DG), the problem of training a model that is robust to domain shift by design. Various DG approaches have been proposed with different motivating intuitions, but they typically optimize for a single step of domain generalization -- training on one set of domains and generalizing to one other. Our sequential learning is inspired by the idea lifelong learning, where accumulated experience means that learning the $n^{th}$ thing becomes easier than the $1^{st}$ thing. In DG this means encountering a sequence of domains and at each step training to maximise performance on the next domain. The performance at domain $n$ then depends on the previous $n-1$ learning problems. Thus backpropagating through the sequence means optimizing performance not just for the next domain, but all following domains. Training on all such sequences of domains provides dramatically more `practice' for a base DG learner compared to existing approaches, thus improving performance on a true testing domain. This strategy can be instantiated for different base DG algorithms, but we focus on its application to the recently proposed Meta-Learning Domain generalization (MLDG). We show that for MLDG it leads to a simple to implement and fast algorithm that provides consistent performance improvement on a variety of DG benchmarks.



### Effective Fusion of Deep Multitasking Representations for Robust Visual Tracking
- **Arxiv ID**: http://arxiv.org/abs/2004.01382v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.01382v2)
- **Published**: 2020-04-03 05:33:59+00:00
- **Updated**: 2021-09-20 09:24:50+00:00
- **Authors**: Seyed Mojtaba Marvasti-Zadeh, Hossein Ghanei-Yakhdan, Shohreh Kasaei, Kamal Nasrollahi, Thomas B. Moeslund
- **Comment**: To be appeared in The Visual Computer (International Journal of
  Computer Graphics), Springer, 2021
- **Journal**: None
- **Summary**: Visual object tracking remains an active research field in computer vision due to persisting challenges with various problem-specific factors in real-world scenes. Many existing tracking methods based on discriminative correlation filters (DCFs) employ feature extraction networks (FENs) to model the target appearance during the learning process. However, using deep feature maps extracted from FENs based on different residual neural networks (ResNets) has not previously been investigated. This paper aims to evaluate the performance of twelve state-of-the-art ResNet-based FENs in a DCF-based framework to determine the best for visual tracking purposes. First, it ranks their best feature maps and explores the generalized adoption of the best ResNet-based FEN into another DCF-based method. Then, the proposed method extracts deep semantic information from a fully convolutional FEN and fuses it with the best ResNet-based feature maps to strengthen the target representation in the learning process of continuous convolution filters. Finally, it introduces a new and efficient semantic weighting method (using semantic segmentation feature maps on each video frame) to reduce the drift problem. Extensive experimental results on the well-known OTB-2013, OTB-2015, TC-128 and VOT-2018 visual tracking datasets demonstrate that the proposed method effectively outperforms state-of-the-art methods in terms of precision and robustness of visual tracking.



### LiDAR-based Online 3D Video Object Detection with Graph-based Message Passing and Spatiotemporal Transformer Attention
- **Arxiv ID**: http://arxiv.org/abs/2004.01389v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.01389v1)
- **Published**: 2020-04-03 06:06:52+00:00
- **Updated**: 2020-04-03 06:06:52+00:00
- **Authors**: Junbo Yin, Jianbing Shen, Chenye Guan, Dingfu Zhou, Ruigang Yang
- **Comment**: Accepted to CVPR 2020. Code: https://github.com/yinjunbo/3DVID
- **Journal**: None
- **Summary**: Existing LiDAR-based 3D object detectors usually focus on the single-frame detection, while ignoring the spatiotemporal information in consecutive point cloud frames. In this paper, we propose an end-to-end online 3D video object detector that operates on point cloud sequences. The proposed model comprises a spatial feature encoding component and a spatiotemporal feature aggregation component. In the former component, a novel Pillar Message Passing Network (PMPNet) is proposed to encode each discrete point cloud frame. It adaptively collects information for a pillar node from its neighbors by iterative message passing, which effectively enlarges the receptive field of the pillar feature. In the latter component, we propose an Attentive Spatiotemporal Transformer GRU (AST-GRU) to aggregate the spatiotemporal information, which enhances the conventional ConvGRU with an attentive memory gating mechanism. AST-GRU contains a Spatial Transformer Attention (STA) module and a Temporal Transformer Attention (TTA) module, which can emphasize the foreground objects and align the dynamic objects, respectively. Experimental results demonstrate that the proposed 3D video object detector achieves state-of-the-art performance on the large-scale nuScenes benchmark.



### Crossover-Net: Leveraging the Vertical-Horizontal Crossover Relation for Robust Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2004.01397v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, 14J60 (Primary) 14F05, 14J26 (Secondary), I.4.6
- **Links**: [PDF](http://arxiv.org/pdf/2004.01397v1)
- **Published**: 2020-04-03 06:49:31+00:00
- **Updated**: 2020-04-03 06:49:31+00:00
- **Authors**: Qian Yu, Yinghuan Shi, Yefeng Zheng, Yang Gao, Jianbing Zhu, Yakang Dai
- **Comment**: 11 pages, 14 figures
- **Journal**: None
- **Summary**: Robust segmentation for non-elongated tissues in medical images is hard to realize due to the large variation of the shape, size, and appearance of these tissues in different patients. In this paper, we present an end-to-end trainable deep segmentation model termed Crossover-Net for robust segmentation in medical images. Our proposed model is inspired by an insightful observation: during segmentation, the representation from the horizontal and vertical directions can provide different local appearance and orthogonality context information, which helps enhance the discrimination between different tissues by simultaneously learning from these two directions. Specifically, by converting the segmentation task to a pixel/voxel-wise prediction problem, firstly, we originally propose a cross-shaped patch, namely crossover-patch, which consists of a pair of (orthogonal and overlapped) vertical and horizontal patches, to capture the orthogonal vertical and horizontal relation. Then, we develop the Crossover-Net to learn the vertical-horizontal crossover relation captured by our crossover-patches. To achieve this goal, for learning the representation on a typical crossover-patch, we design a novel loss function to (1) impose the consistency on the overlap region of the vertical and horizontal patches and (2) preserve the diversity on their non-overlap regions. We have extensively evaluated our method on CT kidney tumor, MR cardiac, and X-ray breast mass segmentation tasks. Promising results are achieved according to our extensive evaluation and comparison with the state-of-the-art segmentation models.



### TEA: Temporal Excitation and Aggregation for Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2004.01398v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.01398v1)
- **Published**: 2020-04-03 06:53:30+00:00
- **Updated**: 2020-04-03 06:53:30+00:00
- **Authors**: Yan Li, Bin Ji, Xintian Shi, Jianguo Zhang, Bin Kang, Limin Wang
- **Comment**: CVPR2020. The code is available at
  https://github.com/Phoenix1327/tea-action-recognition
- **Journal**: None
- **Summary**: Temporal modeling is key for action recognition in videos. It normally considers both short-range motions and long-range aggregations. In this paper, we propose a Temporal Excitation and Aggregation (TEA) block, including a motion excitation (ME) module and a multiple temporal aggregation (MTA) module, specifically designed to capture both short- and long-range temporal evolution. In particular, for short-range motion modeling, the ME module calculates the feature-level temporal differences from spatiotemporal features. It then utilizes the differences to excite the motion-sensitive channels of the features. The long-range temporal aggregations in previous works are typically achieved by stacking a large number of local temporal convolutions. Each convolution processes a local temporal window at a time. In contrast, the MTA module proposes to deform the local convolution to a group of sub-convolutions, forming a hierarchical residual architecture. Without introducing additional parameters, the features will be processed with a series of sub-convolutions, and each frame could complete multiple temporal aggregations with neighborhoods. The final equivalent receptive field of temporal dimension is accordingly enlarged, which is capable of modeling the long-range temporal relationship over distant frames. The two components of the TEA block are complementary in temporal modeling. Finally, our approach achieves impressive results at low FLOPs on several action recognition benchmarks, such as Kinetics, Something-Something, HMDB51, and UCF101, which confirms its effectiveness and efficiency.



### Demographic Bias: A Challenge for Fingervein Recognition Systems?
- **Arxiv ID**: http://arxiv.org/abs/2004.01418v1
- **DOI**: 10.23919/Eusipco47968.2020.9287722
- **Categories**: **cs.CV**, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/2004.01418v1)
- **Published**: 2020-04-03 07:53:11+00:00
- **Updated**: 2020-04-03 07:53:11+00:00
- **Authors**: P. Drozdowski, B. Prommegger, G. Wimmer, R. Schraml, C. Rathgeb, A. Uhl, C. Busch
- **Comment**: 5 pages, 2 figures, 8 tables. Submitted to European Signal Processing
  Conference (EUSIPCO) -- special session on bias in biometrics
- **Journal**: None
- **Summary**: Recently, concerns regarding potential biases in the underlying algorithms of many automated systems (including biometrics) have been raised. In this context, a biased algorithm produces statistically different outcomes for different groups of individuals based on certain (often protected by anti-discrimination legislation) attributes such as sex and age. While several preliminary studies investigating this matter for facial recognition algorithms do exist, said topic has not yet been addressed for vascular biometric characteristics. Accordingly, in this paper, several popular types of recognition algorithms are benchmarked to ascertain the matter for fingervein recognition. The experimental evaluation suggests lack of bias for the tested algorithms, although future works with larger datasets are needed to validate and confirm those preliminary results.



### Error-Corrected Margin-Based Deep Cross-Modal Hashing for Facial Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2004.03378v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.03378v1)
- **Published**: 2020-04-03 08:20:08+00:00
- **Updated**: 2020-04-03 08:20:08+00:00
- **Authors**: Fariborz Taherkhani, Veeru Talreja, Matthew C. Valenti, Nasser M. Nasrabadi
- **Comment**: IEEE TRANSACTIONS ON BIOMETRICS, BEHAVIOR, AND IDENTITY SCIENCE, 2020
- **Journal**: None
- **Summary**: Cross-modal hashing facilitates mapping of heterogeneous multimedia data into a common Hamming space, which can beutilized for fast and flexible retrieval across different modalities. In this paper, we propose a novel cross-modal hashingarchitecture-deep neural decoder cross-modal hashing (DNDCMH), which uses a binary vector specifying the presence of certainfacial attributes as an input query to retrieve relevant face images from a database. The DNDCMH network consists of two separatecomponents: an attribute-based deep cross-modal hashing (ADCMH) module, which uses a margin (m)-based loss function toefficiently learn compact binary codes to preserve similarity between modalities in the Hamming space, and a neural error correctingdecoder (NECD), which is an error correcting decoder implemented with a neural network. The goal of NECD network in DNDCMH isto error correct the hash codes generated by ADCMH to improve the retrieval efficiency. The NECD network is trained such that it hasan error correcting capability greater than or equal to the margin (m) of the margin-based loss function. This results in NECD cancorrect the corrupted hash codes generated by ADCMH up to the Hamming distance of m. We have evaluated and comparedDNDCMH with state-of-the-art cross-modal hashing methods on standard datasets to demonstrate the superiority of our method.



### Disassembling Object Representations without Labels
- **Arxiv ID**: http://arxiv.org/abs/2004.01426v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.01426v1)
- **Published**: 2020-04-03 08:23:09+00:00
- **Updated**: 2020-04-03 08:23:09+00:00
- **Authors**: Zunlei Feng, Xinchao Wang, Yongming He, Yike Yuan, Xin Gao, Mingli Song
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we study a new representation-learning task, which we termed as disassembling object representations. Given an image featuring multiple objects, the goal of disassembling is to acquire a latent representation, of which each part corresponds to one category of objects. Disassembling thus finds its application in a wide domain such as image editing and few- or zero-shot learning, as it enables category-specific modularity in the learned representations. To this end, we propose an unsupervised approach to achieving disassembling, named Unsupervised Disassembling Object Representation (UDOR). UDOR follows a double auto-encoder architecture, in which a fuzzy classification and an object-removing operation are imposed. The fuzzy classification constrains each part of the latent representation to encode features of up to one object category, while the object-removing, combined with a generative adversarial network, enforces the modularity of the representations and integrity of the reconstructed image. Furthermore, we devise two metrics to respectively measure the modularity of disassembled representations and the visual integrity of reconstructed images. Experimental results demonstrate that the proposed UDOR, despited unsupervised, achieves truly encouraging results on par with those of supervised methods.



### Self-Paced Deep Regression Forests with Consideration on Underrepresented Examples
- **Arxiv ID**: http://arxiv.org/abs/2004.01459v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.01459v4)
- **Published**: 2020-04-03 10:18:05+00:00
- **Updated**: 2020-08-06 01:06:32+00:00
- **Authors**: Lili Pan, Shijie Ai, Yazhou Ren, Zenglin Xu
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: Deep discriminative models (e.g. deep regression forests, deep neural decision forests) have achieved remarkable success recently to solve problems such as facial age estimation and head pose estimation. Most existing methods pursue robust and unbiased solutions either through learning discriminative features, or reweighting samples. We argue what is more desirable is learning gradually to discriminate like our human beings, and hence we resort to self-paced learning (SPL). Then, a natural question arises: can self-paced regime lead deep discriminative models to achieve more robust and less biased solutions? To this end, this paper proposes a new deep discriminative model--self-paced deep regression forests with consideration on underrepresented examples (SPUDRFs). It tackles the fundamental ranking and selecting problem in SPL from a new perspective: fairness. This paradigm is fundamental and could be easily combined with a variety of deep discriminative models (DDMs). Extensive experiments on two computer vision tasks, i.e., facial age estimation and head pose estimation, demonstrate the efficacy of SPUDRFs, where state-of-the-art performances are achieved.



### Gradient Centralization: A New Optimization Technique for Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2004.01461v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.01461v2)
- **Published**: 2020-04-03 10:25:00+00:00
- **Updated**: 2020-04-08 03:40:44+00:00
- **Authors**: Hongwei Yong, Jianqiang Huang, Xiansheng Hua, Lei Zhang
- **Comment**: 20 pages, 7 figures, conference
- **Journal**: None
- **Summary**: Optimization techniques are of great importance to effectively and efficiently train a deep neural network (DNN). It has been shown that using the first and second order statistics (e.g., mean and variance) to perform Z-score standardization on network activations or weight vectors, such as batch normalization (BN) and weight standardization (WS), can improve the training performance. Different from these existing methods that mostly operate on activations or weights, we present a new optimization technique, namely gradient centralization (GC), which operates directly on gradients by centralizing the gradient vectors to have zero mean. GC can be viewed as a projected gradient descent method with a constrained loss function. We show that GC can regularize both the weight space and output feature space so that it can boost the generalization performance of DNNs. Moreover, GC improves the Lipschitzness of the loss function and its gradient so that the training process becomes more efficient and stable. GC is very simple to implement and can be easily embedded into existing gradient based DNN optimizers with only one line of code. It can also be directly used to fine-tune the pre-trained DNNs. Our experiments on various applications, including general image classification, fine-grained image classification, detection and segmentation, demonstrate that GC can consistently improve the performance of DNN learning. The code of GC can be found at https://github.com/Yonghongwei/Gradient-Centralization.



### Composition of Saliency Metrics for Channel Pruning with a Myopic Oracle
- **Arxiv ID**: http://arxiv.org/abs/2004.03376v2
- **DOI**: 10.1109/SSCI47803.2020.9308157
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.03376v2)
- **Published**: 2020-04-03 11:29:41+00:00
- **Updated**: 2021-06-24 12:56:50+00:00
- **Authors**: Kaveena Persand, Andrew Anderson, David Gregg
- **Comment**: None
- **Journal**: 2020 IEEE Symposium Series on Computational Intelligence (SSCI)
- **Summary**: The computation and memory needed for Convolutional Neural Network (CNN) inference can be reduced by pruning weights from the trained network. Pruning is guided by a pruning saliency, which heuristically approximates the change in the loss function associated with the removal of specific weights. Many pruning signals have been proposed, but the performance of each heuristic depends on the particular trained network. This leaves the data scientist with a difficult choice. When using any one saliency metric for the entire pruning process, we run the risk of the metric assumptions being invalidated, leading to poor decisions being made by the metric. Ideally we could combine the best aspects of different saliency metrics. However, despite an extensive literature review, we are unable to find any prior work on composing different saliency metrics. The chief difficulty lies in combining the numerical output of different saliency metrics, which are not directly comparable.   We propose a method to compose several primitive pruning saliencies, to exploit the cases where each saliency measure does well. Our experiments show that the composition of saliencies avoids many poor pruning choices identified by individual saliencies. In most cases our method finds better selections than even the best individual pruning saliency.



### Cell Segmentation and Tracking using CNN-Based Distance Predictions and a Graph-Based Matching Strategy
- **Arxiv ID**: http://arxiv.org/abs/2004.01486v4
- **DOI**: 10.1371/journal.pone.0243219
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.01486v4)
- **Published**: 2020-04-03 11:55:28+00:00
- **Updated**: 2020-10-22 14:51:01+00:00
- **Authors**: Tim Scherr, Katharina Löffler, Moritz Böhland, Ralf Mikut
- **Comment**: 25 pages, 14 figures, methods of the team KIT-Sch-GE for the IEEE
  ISBI 2020 Cell Tracking Challenge
- **Journal**: PLoS ONE (2020): e0243219
- **Summary**: The accurate segmentation and tracking of cells in microscopy image sequences is an important task in biomedical research, e.g., for studying the development of tissues, organs or entire organisms. However, the segmentation of touching cells in images with a low signal-to-noise-ratio is still a challenging problem. In this paper, we present a method for the segmentation of touching cells in microscopy images. By using a novel representation of cell borders, inspired by distance maps, our method is capable to utilize not only touching cells but also close cells in the training process. Furthermore, this representation is notably robust to annotation errors and shows promising results for the segmentation of microscopy images containing in the training data underrepresented or not included cell types. For the prediction of the proposed neighbor distances, an adapted U-Net convolutional neural network (CNN) with two decoder paths is used. In addition, we adapt a graph-based cell tracking algorithm to evaluate our proposed method on the task of cell tracking. The adapted tracking algorithm includes a movement estimation in the cost function to re-link tracks with missing segmentation masks over a short sequence of frames. Our combined tracking by detection method has proven its potential in the IEEE ISBI 2020 Cell Tracking Challenge (http://celltrackingchallenge.net/) where we achieved as team KIT-Sch-GE multiple top three rankings including two top performances using a single segmentation model for the diverse data sets.



### Two-Stream AMTnet for Action Detection
- **Arxiv ID**: http://arxiv.org/abs/2004.01494v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.01494v1)
- **Published**: 2020-04-03 12:16:45+00:00
- **Updated**: 2020-04-03 12:16:45+00:00
- **Authors**: Suman Saha, Gurkirt Singh, Fabio Cuzzolin
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: In this paper, we propose Two-Stream AMTnet, which leverages recent advances in video-based action representation[1] and incremental action tube generation[2]. Majority of the present action detectors follow a frame-based representation, a late-fusion followed by an offline action tube building steps. These are sub-optimal as: frame-based features barely encode the temporal relations; late-fusion restricts the network to learn robust spatiotemporal features; and finally, an offline action tube generation is not suitable for many real-world problems such as autonomous driving, human-robot interaction to name a few. The key contributions of this work are: (1) combining AMTnet's 3D proposal architecture with an online action tube generation technique which allows the model to learn stronger temporal features needed for accurate action detection and facilitates running inference online; (2) an efficient fusion technique allowing the deep network to learn strong spatiotemporal action representations. This is achieved by augmenting the previous Action Micro-Tube (AMTnet) action detection framework in three distinct ways: by adding a parallel motion stIn this paper, we propose a new deep neural network architecture for online action detection, termed ream to the original appearance one in AMTnet; (2) in opposition to state-of-the-art action detectors which train appearance and motion streams separately, and use a test time late fusion scheme to fuse RGB and flow cues, by jointly training both streams in an end-to-end fashion and merging RGB and optical flow features at training time; (3) by introducing an online action tube generation algorithm which works at video-level, and in real-time (when exploiting only appearance features). Two-Stream AMTnet exhibits superior action detection performance over state-of-the-art approaches on the standard action detection benchmarks.



### RANSAC-Flow: generic two-stage image alignment
- **Arxiv ID**: http://arxiv.org/abs/2004.01526v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.01526v2)
- **Published**: 2020-04-03 12:37:58+00:00
- **Updated**: 2020-07-17 13:51:18+00:00
- **Authors**: Xi Shen, François Darmon, Alexei A. Efros, Mathieu Aubry
- **Comment**: Accepted to ECCV 2020 as a spotlight. Project page:
  http://imagine.enpc.fr/~shenx/RANSAC-Flow/
- **Journal**: None
- **Summary**: This paper considers the generic problem of dense alignment between two images, whether they be two frames of a video, two widely different views of a scene, two paintings depicting similar content, etc. Whereas each such task is typically addressed with a domain-specific solution, we show that a simple unsupervised approach performs surprisingly well across a range of tasks. Our main insight is that parametric and non-parametric alignment methods have complementary strengths. We propose a two-stage process: first, a feature-based parametric coarse alignment using one or more homographies, followed by non-parametric fine pixel-wise alignment. Coarse alignment is performed using RANSAC on off-the-shelf deep features. Fine alignment is learned in an unsupervised way by a deep network which optimizes a standard structural similarity metric (SSIM) between the two images, plus cycle-consistency. Despite its simplicity, our method shows competitive results on a range of tasks and datasets, including unsupervised optical flow on KITTI, dense correspondences on Hpatches, two-view geometry estimation on YFCC100M, localization on Aachen Day-Night, and, for the first time, fine alignment of artworks on the Brughel dataset. Our code and data are available at http://imagine.enpc.fr/~shenx/RANSAC-Flow/



### Exploring the ability of CNNs to generalise to previously unseen scales over wide scale ranges
- **Arxiv ID**: http://arxiv.org/abs/2004.01536v7
- **DOI**: 10.1109/ICPR48806.2021.9413276
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.01536v7)
- **Published**: 2020-04-03 13:00:35+00:00
- **Updated**: 2021-05-18 09:27:23+00:00
- **Authors**: Ylva Jansson, Tony Lindeberg
- **Comment**: 14 pages, 6 figures, 3 tables
- **Journal**: Shortened version in International Conference on Pattern
  Recognition (ICPR 2020), pages 1181-1188, Jan 2021
- **Summary**: The ability to handle large scale variations is crucial for many real world visual tasks. A straightforward approach for handling scale in a deep network is to process an image at several scales simultaneously in a set of scale channels. Scale invariance can then, in principle, be achieved by using weight sharing between the scale channels together with max or average pooling over the outputs from the scale channels. The ability of such scale channel networks to generalise to scales not present in the training set over significant scale ranges has, however, not previously been explored. We, therefore, present a theoretical analysis of invariance and covariance properties of scale channel networks and perform an experimental evaluation of the ability of different types of scale channel networks to generalise to previously unseen scales. We identify limitations of previous approaches and propose a new type of foveated scale channel architecture, where the scale channels process increasingly larger parts of the image with decreasing resolution. Our proposed FovMax and FovAvg networks perform almost identically over a scale range of 8, also when training on single scale training data, and do also give improvements in the small sample regime.



### Context Prior for Scene Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2004.01547v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.01547v1)
- **Published**: 2020-04-03 13:16:32+00:00
- **Updated**: 2020-04-03 13:16:32+00:00
- **Authors**: Changqian Yu, Jingbo Wang, Changxin Gao, Gang Yu, Chunhua Shen, Nong Sang
- **Comment**: Accepted to CVPR 2020. Code is available at
  https://git.io/ContextPrior
- **Journal**: IEEE Conference on Computer Vision and Pattern Recognition 2020
- **Summary**: Recent works have widely explored the contextual dependencies to achieve more accurate segmentation results. However, most approaches rarely distinguish different types of contextual dependencies, which may pollute the scene understanding. In this work, we directly supervise the feature aggregation to distinguish the intra-class and inter-class context clearly. Specifically, we develop a Context Prior with the supervision of the Affinity Loss. Given an input image and corresponding ground truth, Affinity Loss constructs an ideal affinity map to supervise the learning of Context Prior. The learned Context Prior extracts the pixels belonging to the same category, while the reversed prior focuses on the pixels of different classes. Embedded into a conventional deep CNN, the proposed Context Prior Layer can selectively capture the intra-class and inter-class contextual dependencies, leading to robust feature representation. To validate the effectiveness, we design an effective Context Prior Network (CPNet). Extensive quantitative and qualitative evaluations demonstrate that the proposed model performs favorably against state-of-the-art semantic segmentation approaches. More specifically, our algorithm achieves 46.3% mIoU on ADE20K, 53.9% mIoU on PASCAL-Context, and 81.3% mIoU on Cityscapes. Code is available at https://git.io/ContextPrior.



### Sparse Concept Coded Tetrolet Transform for Unconstrained Odia Character Recognition
- **Arxiv ID**: http://arxiv.org/abs/2004.01551v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.01551v1)
- **Published**: 2020-04-03 13:20:12+00:00
- **Updated**: 2020-04-03 13:20:12+00:00
- **Authors**: Kalyan S Dash, N B Puhan, G Panda
- **Comment**: None
- **Journal**: None
- **Summary**: Feature representation in the form of spatio-spectral decomposition is one of the robust techniques adopted in automatic handwritten character recognition systems. In this regard, we propose a new image representation approach for unconstrained handwritten alphanumeric characters using sparse concept coded Tetrolets. Tetrolets, which does not use fixed dyadic square blocks for spectral decomposition like conventional wavelets, preserve the localized variations in handwritings by adopting tetrominoes those capture the shape geometry. The sparse concept coding of low entropy Tetrolet representation is found to extract the important hidden information (concept) for superior pattern discrimination. Large scale experimentation using ten databases in six different scripts (Bangla, Devanagari, Odia, English, Arabic and Telugu) has been performed. The proposed feature representation along with standard classifiers such as random forest, support vector machine (SVM), nearest neighbor and modified quadratic discriminant function (MQDF) is found to achieve state-of-the-art recognition performance in all the databases, viz. 99.40% (MNIST); 98.72% and 93.24% (IITBBS); 99.38% and 99.22% (ISI Kolkata). The proposed OCR system is shown to perform better than other sparse based techniques such as PCA, SparsePCA and SparseLDA, as well as better than existing transforms (Wavelet, Slantlet and Stockwell).



### Complete CVDL Methodology for Investigating Hydrodynamic Instabilities
- **Arxiv ID**: http://arxiv.org/abs/2004.03374v2
- **DOI**: None
- **Categories**: **physics.flu-dyn**, cs.CV, cs.LG, physics.comp-ph
- **Links**: [PDF](http://arxiv.org/pdf/2004.03374v2)
- **Published**: 2020-04-03 13:52:04+00:00
- **Updated**: 2020-04-26 06:52:01+00:00
- **Authors**: Re'em Harel, Matan Rusanovsky, Yehonatan Fridman, Assaf Shimony, Gal Oren
- **Comment**: 21 pages
- **Journal**: None
- **Summary**: In fluid dynamics, one of the most important research fields is hydrodynamic instabilities and their evolution in different flow regimes. The investigation of said instabilities is concerned with the highly non-linear dynamics. Currently, three main methods are used for understanding of such phenomenon - namely analytical models, experiments and simulations - and all of them are primarily investigated and correlated using human expertise. In this work we claim and demonstrate that a major portion of this research effort could and should be analysed using recent breakthrough advancements in the field of Computer Vision with Deep Learning (CVDL, or Deep Computer-Vision). Specifically, we target and evaluate specific state-of-the-art techniques - such as Image Retrieval, Template Matching, Parameters Regression and Spatiotemporal Prediction - for the quantitative and qualitative benefits they provide. In order to do so we focus in this research on one of the most representative instabilities, the Rayleigh-Taylor one, simulate its behaviour and create an open-sourced state-of-the-art annotated database (RayleAI). Finally, we use adjusted experimental results and novel physical loss methodologies to validate the correspondence of the predicted results to actual physical reality to prove the models efficiency. The techniques which were developed and proved in this work can be served as essential tools for physicists in the field of hydrodynamics for investigating a variety of physical systems, and also could be used via Transfer Learning to other instabilities research. A part of the techniques can be easily applied on already exist simulation results. All models as well as the data-set that was created for this work, are publicly available at: https://github.com/scientific-computing-nrcn/SimulAI.



### DFNet: Discriminative feature extraction and integration network for salient object detection
- **Arxiv ID**: http://arxiv.org/abs/2004.01573v1
- **DOI**: 10.1016/j.engappai.2019.103419
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.01573v1)
- **Published**: 2020-04-03 13:56:41+00:00
- **Updated**: 2020-04-03 13:56:41+00:00
- **Authors**: Mehrdad Noori, Sina Mohammadi, Sina Ghofrani Majelan, Ali Bahri, Mohammad Havaei
- **Comment**: Accepted by Engineering Applications of Artificial Intelligence. 22
  pages, 8 figures
- **Journal**: Engineering Applications of Artificial Intelligence, Volume 89,
  2020, 103419, ISSN 0952-1976
- **Summary**: Despite the powerful feature extraction capability of Convolutional Neural Networks, there are still some challenges in saliency detection. In this paper, we focus on two aspects of challenges: i) Since salient objects appear in various sizes, using single-scale convolution would not capture the right size. Moreover, using multi-scale convolutions without considering their importance may confuse the model. ii) Employing multi-level features helps the model use both local and global context. However, treating all features equally results in information redundancy. Therefore, there needs to be a mechanism to intelligently select which features in different levels are useful. To address the first challenge, we propose a Multi-scale Attention Guided Module. This module not only extracts multi-scale features effectively but also gives more attention to more discriminative feature maps corresponding to the scale of the salient object. To address the second challenge, we propose an Attention-based Multi-level Integrator Module to give the model the ability to assign different weights to multi-level feature maps. Furthermore, our Sharpening Loss function guides our network to output saliency maps with higher certainty and less blurry salient objects, and it has far better performance than the Cross-entropy loss. For the first time, we adopt four different backbones to show the generalization of our method. Experiments on five challenging datasets prove that our method achieves the state-of-the-art performance. Our approach is fast as well and can run at a real-time speed.



### Retinopathy of Prematurity Stage Diagnosis Using Object Segmentation and Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2004.01582v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.01582v1)
- **Published**: 2020-04-03 14:07:41+00:00
- **Updated**: 2020-04-03 14:07:41+00:00
- **Authors**: Alexander Ding, Qilei Chen, Yu Cao, Benyuan Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Retinopathy of Prematurity (ROP) is an eye disorder primarily affecting premature infants with lower weights. It causes proliferation of vessels in the retina and could result in vision loss and, eventually, retinal detachment, leading to blindness. While human experts can easily identify severe stages of ROP, the diagnosis of earlier stages, which are the most relevant to determining treatment choice, are much more affected by variability in subjective interpretations of human experts. In recent years, there has been a significant effort to automate the diagnosis using deep learning. This paper builds upon the success of previous models and develops a novel architecture, which combines object segmentation and convolutional neural networks (CNN) to construct an effective classifier of ROP stages 1-3 based on neonatal retinal images. Motivated by the fact that the formation and shape of a demarcation line in the retina is the distinguishing feature between earlier ROP stages, our proposed system first trains an object segmentation model to identify the demarcation line at a pixel level and adds the resulting mask as an additional "color" channel in the original image. Then, the system trains a CNN classifier based on the processed images to leverage information from both the original image and the mask, which helps direct the model's attention to the demarcation line. In a number of careful experiments comparing its performance to previous object segmentation systems and CNN-only systems trained on our dataset, our novel architecture significantly outperforms previous systems in accuracy, demonstrating the effectiveness of our proposed pipeline.



### HandVoxNet: Deep Voxel-Based Network for 3D Hand Shape and Pose Estimation from a Single Depth Map
- **Arxiv ID**: http://arxiv.org/abs/2004.01588v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.01588v1)
- **Published**: 2020-04-03 14:27:16+00:00
- **Updated**: 2020-04-03 14:27:16+00:00
- **Authors**: Jameel Malik, Ibrahim Abdelaziz, Ahmed Elhayek, Soshi Shimada, Sk Aziz Ali, Vladislav Golyanik, Christian Theobalt, Didier Stricker
- **Comment**: 10 pages, 8 figures, 5 tables, CVPR
- **Journal**: None
- **Summary**: 3D hand shape and pose estimation from a single depth map is a new and challenging computer vision problem with many applications. The state-of-the-art methods directly regress 3D hand meshes from 2D depth images via 2D convolutional neural networks, which leads to artefacts in the estimations due to perspective distortions in the images. In contrast, we propose a novel architecture with 3D convolutions trained in a weakly-supervised manner. The input to our method is a 3D voxelized depth map, and we rely on two hand shape representations. The first one is the 3D voxelized grid of the shape which is accurate but does not preserve the mesh topology and the number of mesh vertices. The second representation is the 3D hand surface which is less accurate but does not suffer from the limitations of the first representation. We combine the advantages of these two representations by registering the hand surface to the voxelized hand shape. In the extensive experiments, the proposed approach improves over the state of the art by 47.8% on the SynHand5M dataset. Moreover, our augmentation policy for voxelized depth maps further enhances the accuracy of 3D hand pose estimation on real data. Our method produces visually more reasonable and realistic hand shapes on NYU and BigHand2.2M datasets compared to the existing approaches.



### Detection of Perineural Invasion in Prostate Needle Biopsies with Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2004.01589v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.01589v1)
- **Published**: 2020-04-03 14:27:53+00:00
- **Updated**: 2020-04-03 14:27:53+00:00
- **Authors**: Peter Ström, Kimmo Kartasalo, Pekka Ruusuvuori, Henrik Grönberg, Hemamali Samaratunga, Brett Delahunt, Toyonori Tsuzuki, Lars Egevad, Martin Eklund
- **Comment**: 20 pages, 5 figures
- **Journal**: None
- **Summary**: Background: The detection of perineural invasion (PNI) by carcinoma in prostate biopsies has been shown to be associated with poor prognosis. The assessment and quantification of PNI is; however, labor intensive. In the study we aimed to develop an algorithm based on deep neural networks to aid pathologists in this task.   Methods: We collected, digitized and pixel-wise annotated the PNI findings in each of the approximately 80,000 biopsy cores from the 7,406 men who underwent biopsy in the prospective and diagnostic STHLM3 trial between 2012 and 2014. In total, 485 biopsy cores showed PNI. We also digitized more than 10% (n=8,318) of the PNI negative biopsy cores. Digitized biopsies from a random selection of 80% of the men were used to build deep neural networks, and the remaining 20% were used to evaluate the performance of the algorithm.   Results: For the detection of PNI in prostate biopsy cores the network had an estimated area under the receiver operating characteristics curve of 0.98 (95% CI 0.97-0.99) based on 106 PNI positive cores and 1,652 PNI negative cores in the independent test set. For the pre-specified operating point this translates to sensitivity of 0.87 and specificity of 0.97. The corresponding positive and negative predictive values were 0.67 and 0.99, respectively. For localizing the regions of PNI within a slide we estimated an average intersection over union of 0.50 (CI: 0.46-0.55).   Conclusion: We have developed an algorithm based on deep neural networks for detecting PNI in prostate biopsies with apparently acceptable diagnostic properties. These algorithms have the potential to aid pathologists in the day-to-day work by drastically reducing the number of biopsy cores that need to be assessed for PNI and by highlighting regions of diagnostic interest.



### Cell Segmentation by Combining Marker-Controlled Watershed and Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2004.01607v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.01607v1)
- **Published**: 2020-04-03 14:51:43+00:00
- **Updated**: 2020-04-03 14:51:43+00:00
- **Authors**: Filip Lux, Petr Matula
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a cell segmentation method for analyzing images of densely clustered cells. The method combines the strengths of marker-controlled watershed transformation and a convolutional neural network (CNN). We demonstrate the method universality and high performance on three Cell Tracking Challenge (CTC) datasets of clustered cells captured by different acquisition techniques. For all tested datasets, our method reached the top performance in both cell detection and segmentation. Based on a series of experiments, we observed: (1) Predicting both watershed marker function and segmentation function significantly improves the accuracy of the segmentation. (2) Both functions can be learned independently. (3) Training data augmentation by scaling and rigid geometric transformations is superior to augmentation that involves elastic transformations. Our method is simple to use, and it generalizes well for various data with state-of-the-art performance.



### Interpreting Medical Image Classifiers by Optimization Based Counterfactual Impact Analysis
- **Arxiv ID**: http://arxiv.org/abs/2004.01610v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.01610v1)
- **Published**: 2020-04-03 14:59:08+00:00
- **Updated**: 2020-04-03 14:59:08+00:00
- **Authors**: David Major, Dimitrios Lenis, Maria Wimmer, Gert Sluiter, Astrid Berg, Katja Bühler
- **Comment**: Accepted for publication at IEEE International Symposium on
  Biomedical Imaging (ISBI) 2020
- **Journal**: None
- **Summary**: Clinical applicability of automated decision support systems depends on a robust, well-understood classification interpretation. Artificial neural networks while achieving class-leading scores fall short in this regard. Therefore, numerous approaches have been proposed that map a salient region of an image to a diagnostic classification. Utilizing heuristic methodology, like blurring and noise, they tend to produce diffuse, sometimes misleading results, hindering their general adoption. In this work we overcome these issues by presenting a model agnostic saliency mapping framework tailored to medical imaging. We replace heuristic techniques with a strong neighborhood conditioned inpainting approach, which avoids anatomically implausible artefacts. We formulate saliency attribution as a map-quality optimization task, enforcing constrained and focused attributions. Experiments on public mammography data show quantitatively and qualitatively more precise localization and clearer conveying results than existing state-of-the-art methods.



### Deep Learning for Image Search and Retrieval in Large Remote Sensing Archives
- **Arxiv ID**: http://arxiv.org/abs/2004.01613v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.01613v2)
- **Published**: 2020-04-03 15:03:41+00:00
- **Updated**: 2020-07-03 08:52:07+00:00
- **Authors**: Gencer Sumbul, Jian Kang, Begüm Demir
- **Comment**: To appear as a book chapter in "Deep Learning for the Earth
  Sciences", John Wiley & Sons, 2020
- **Journal**: None
- **Summary**: This chapter presents recent advances in content based image search and retrieval (CBIR) systems in remote sensing (RS) for fast and accurate information discovery from massive data archives. Initially, we analyze the limitations of the traditional CBIR systems that rely on the hand-crafted RS image descriptors. Then, we focus our attention on the advances in RS CBIR systems for which deep learning (DL) models are at the forefront. In particular, we present the theoretical properties of the most recent DL based CBIR systems for the characterization of the complex semantic content of RS images. After discussing their strengths and limitations, we present the deep hashing based CBIR systems that have high time-efficient search capability within huge data archives. Finally, the most promising research directions in RS CBIR are discussed.



### Deep Transfer Learning for Texture Classification in Colorectal Cancer Histology
- **Arxiv ID**: http://arxiv.org/abs/2004.01614v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.01614v1)
- **Published**: 2020-04-03 15:05:36+00:00
- **Updated**: 2020-04-03 15:05:36+00:00
- **Authors**: Srinath Jayachandran, Ashlin Ghosh
- **Comment**: None
- **Journal**: None
- **Summary**: Microscopic examination of tissues or histopathology is one of the diagnostic procedures for detecting colorectal cancer. The pathologist involved in such an examination usually identifies tissue type based on texture analysis, especially focusing on tumour-stroma ratio. In this work, we automate the task of tissue classification within colorectal cancer histology samples using deep transfer learning. We use discriminative fine-tuning with one-cycle-policy and apply structure-preserving colour normalization to boost our results. We also provide visual explanations of the deep neural network's decision on texture classification. With achieving state-of-the-art test accuracy of 96.2% we also embark on using deployment friendly architecture called SqueezeNet for memory-limited hardware.



### Robust Self-Supervised Convolutional Neural Network for Subspace Clustering and Classification
- **Arxiv ID**: http://arxiv.org/abs/2004.03375v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.03375v1)
- **Published**: 2020-04-03 16:07:58+00:00
- **Updated**: 2020-04-03 16:07:58+00:00
- **Authors**: Dario Sitnik, Ivica Kopriva
- **Comment**: 15 pages, 3 tables, 3 figures
- **Journal**: None
- **Summary**: Insufficient capability of existing subspace clustering methods to handle data coming from nonlinear manifolds, data corruptions, and out-of-sample data hinders their applicability to address real-world clustering and classification problems. This paper proposes the robust formulation of the self-supervised convolutional subspace clustering network ($S^2$ConvSCN) that incorporates the fully connected (FC) layer and, thus, it is capable for handling out-of-sample data by classifying them using a softmax classifier. $S^2$ConvSCN clusters data coming from nonlinear manifolds by learning the linear self-representation model in the feature space. Robustness to data corruptions is achieved by using the correntropy induced metric (CIM) of the error. Furthermore, the block-diagonal (BD) structure of the representation matrix is enforced explicitly through BD regularization. In a truly unsupervised training environment, Robust $S^2$ConvSCN outperforms its baseline version by a significant amount for both seen and unseen data on four well-known datasets. Arguably, such an ablation study has not been reported before.



### Quantifying Data Augmentation for LiDAR based 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2004.01643v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.01643v2)
- **Published**: 2020-04-03 16:09:14+00:00
- **Updated**: 2022-07-29 11:43:02+00:00
- **Authors**: Martin Hahner, Dengxin Dai, Alexander Liniger, Luc Van Gool
- **Comment**: 2022 Update
- **Journal**: None
- **Summary**: In this work, we shed light on different data augmentation techniques commonly used in Light Detection and Ranging (LiDAR) based 3D Object Detection. For the bulk of our experiments, we utilize the well known PointPillars pipeline and the well established KITTI dataset. We investigate a variety of global and local augmentation techniques, where global augmentation techniques are applied to the entire point cloud of a scene and local augmentation techniques are only applied to points belonging to individual objects in the scene. Our findings show that both types of data augmentation can lead to performance increases, but it also turns out, that some augmentation techniques, such as individual object translation, for example, can be counterproductive and can hurt the overall performance. We show that these findings transfer and generalize well to other state of the art 3D Object Detection methods and the challenging STF dataset. On the KITTI dataset we can gain up to 1.5% and on the STF dataset up to 1.7% in 3D mAP on the moderate car class.



### Deep Learning based detection of Acute Aortic Syndrome in contrast CT images
- **Arxiv ID**: http://arxiv.org/abs/2004.01648v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.01648v1)
- **Published**: 2020-04-03 16:12:04+00:00
- **Updated**: 2020-04-03 16:12:04+00:00
- **Authors**: Manikanta Srikar Yellapragada, Yiting Xie, Benedikt Graf, David Richmond, Arun Krishnan, Arkadiusz Sitek
- **Comment**: None
- **Journal**: None
- **Summary**: Acute aortic syndrome (AAS) is a group of life threatening conditions of the aorta. We have developed an end-to-end automatic approach to detect AAS in computed tomography (CT) images. Our approach consists of two steps. At first, we extract N cross sections along the segmented aorta centerline for each CT scan. These cross sections are stacked together to form a new volume which is then classified using two different classifiers, a 3D convolutional neural network (3D CNN) and a multiple instance learning (MIL). We trained, validated, and compared two models on 2291 contrast CT volumes. We tested on a set aside cohort of 230 normal and 50 positive CT volumes. Our models detected AAS with an Area under Receiver Operating Characteristic curve (AUC) of 0.965 and 0.985 using 3DCNN and MIL, respectively.



### PointGroup: Dual-Set Point Grouping for 3D Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2004.01658v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.01658v1)
- **Published**: 2020-04-03 16:26:37+00:00
- **Updated**: 2020-04-03 16:26:37+00:00
- **Authors**: Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, Jiaya Jia
- **Comment**: CVPR 2020
- **Journal**: None
- **Summary**: Instance segmentation is an important task for scene understanding. Compared to the fully-developed 2D, 3D instance segmentation for point clouds have much room to improve. In this paper, we present PointGroup, a new end-to-end bottom-up architecture, specifically focused on better grouping the points by exploring the void space between objects. We design a two-branch network to extract point features and predict semantic labels and offsets, for shifting each point towards its respective instance centroid. A clustering component is followed to utilize both the original and offset-shifted point coordinate sets, taking advantage of their complementary strength. Further, we formulate the ScoreNet to evaluate the candidate instances, followed by the Non-Maximum Suppression (NMS) to remove duplicates. We conduct extensive experiments on two challenging datasets, ScanNet v2 and S3DIS, on which our method achieves the highest performance, 63.6% and 64.0%, compared to 54.9% and 54.4% achieved by former best solutions in terms of mAP with IoU threshold 0.5.



### Intrinsic Point Cloud Interpolation via Dual Latent Space Navigation
- **Arxiv ID**: http://arxiv.org/abs/2004.01661v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2004.01661v1)
- **Published**: 2020-04-03 16:28:55+00:00
- **Updated**: 2020-04-03 16:28:55+00:00
- **Authors**: Marie-Julie Rakotosaona, Maks Ovsjanikov
- **Comment**: None
- **Journal**: ECCV 2020
- **Summary**: We present a learning-based method for interpolating and manipulating 3D shapes represented as point clouds, that is explicitly designed to preserve intrinsic shape properties. Our approach is based on constructing a dual encoding space that enables shape synthesis and, at the same time, provides links to the intrinsic shape information, which is typically not available on point cloud data. Our method works in a single pass and avoids expensive optimization, employed by existing techniques. Furthermore, the strong regularization provided by our dual latent space approach also helps to improve shape recovery in challenging settings from noisy point clouds across different datasets. Extensive experiments show that our method results in more realistic and smoother interpolations compared to baselines.



### Attribution in Scale and Space
- **Arxiv ID**: http://arxiv.org/abs/2004.03383v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2004.03383v2)
- **Published**: 2020-04-03 17:04:16+00:00
- **Updated**: 2020-04-08 16:41:12+00:00
- **Authors**: Shawn Xu, Subhashini Venugopalan, Mukund Sundararajan
- **Comment**: CVPR 2020 camera-ready. Code is available at
  https://github.com/PAIR-code/saliency
- **Journal**: None
- **Summary**: We study the attribution problem [28] for deep networks applied to perception tasks. For vision tasks, attribution techniques attribute the prediction of a network to the pixels of the input image. We propose a new technique called \emph{Blur Integrated Gradients}. This technique has several advantages over other methods. First, it can tell at what scale a network recognizes an object. It produces scores in the scale/frequency dimension, that we find captures interesting phenomena. Second, it satisfies the scale-space axioms [14], which imply that it employs perturbations that are free of artifact. We therefore produce explanations that are cleaner and consistent with the operation of deep networks. Third, it eliminates the need for a 'baseline' parameter for Integrated Gradients [31] for perception tasks. This is desirable because the choice of baseline has a significant effect on the explanations. We compare the proposed technique against previous techniques and demonstrate application on three tasks: ImageNet object recognition, Diabetic Retinopathy prediction, and AudioSet audio event identification.



### S2DNet: Learning Accurate Correspondences for Sparse-to-Dense Feature Matching
- **Arxiv ID**: http://arxiv.org/abs/2004.01673v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.01673v1)
- **Published**: 2020-04-03 17:04:34+00:00
- **Updated**: 2020-04-03 17:04:34+00:00
- **Authors**: Hugo Germain, Guillaume Bourmaud, Vincent Lepetit
- **Comment**: None
- **Journal**: None
- **Summary**: Establishing robust and accurate correspondences is a fundamental backbone to many computer vision algorithms. While recent learning-based feature matching methods have shown promising results in providing robust correspondences under challenging conditions, they are often limited in terms of precision. In this paper, we introduce S2DNet, a novel feature matching pipeline, designed and trained to efficiently establish both robust and accurate correspondences. By leveraging a sparse-to-dense matching paradigm, we cast the correspondence learning problem as a supervised classification task to learn to output highly peaked correspondence maps. We show that S2DNet achieves state-of-the-art results on the HPatches benchmark, as well as on several long-term visual localization datasets.



### Near-chip Dynamic Vision Filtering for Low-Bandwidth Pedestrian Detection
- **Arxiv ID**: http://arxiv.org/abs/2004.01689v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AR, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.01689v1)
- **Published**: 2020-04-03 17:36:26+00:00
- **Updated**: 2020-04-03 17:36:26+00:00
- **Authors**: Anthony Bisulco, Fernando Cladera Ojeda, Volkan Isler, Daniel D. Lee
- **Comment**: 6 pages, 5 figures
- **Journal**: None
- **Summary**: This paper presents a novel end-to-end system for pedestrian detection using Dynamic Vision Sensors (DVSs). We target applications where multiple sensors transmit data to a local processing unit, which executes a detection algorithm. Our system is composed of (i) a near-chip event filter that compresses and denoises the event stream from the DVS, and (ii) a Binary Neural Network (BNN) detection module that runs on a low-computation edge computing device (in our case a STM32F4 microcontroller). We present the system architecture and provide an end-to-end implementation for pedestrian detection in an office environment. Our implementation reduces transmission size by up to 99.6% compared to transmitting the raw event stream. The average packet size in our system is only 1397 bits, while 307.2 kb are required to send an uncompressed DVS time window. Our detector is able to perform a detection every 450 ms, with an overall testing F1 score of 83%. The low bandwidth and energy properties of our system make it ideal for IoT applications.



### Unsupervised Domain Adaptation with Progressive Domain Augmentation
- **Arxiv ID**: http://arxiv.org/abs/2004.01735v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.01735v2)
- **Published**: 2020-04-03 18:45:39+00:00
- **Updated**: 2020-04-24 01:45:24+00:00
- **Authors**: Kevin Hua, Yuhong Guo
- **Comment**: None
- **Journal**: None
- **Summary**: Domain adaptation aims to exploit a label-rich source domain for learning classifiers in a different label-scarce target domain. It is particularly challenging when there are significant divergences between the two domains. In the paper, we propose a novel unsupervised domain adaptation method based on progressive domain augmentation. The proposed method generates virtual intermediate domains via domain interpolation, progressively augments the source domain and bridges the source-target domain divergence by conducting multiple subspace alignment on the Grassmann manifold. We conduct experiments on multiple domain adaptation tasks and the results shows the proposed method achieves the state-of-the-art performance.



### Analysis of Deep Complex-Valued Convolutional Neural Networks for MRI Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2004.01738v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2004.01738v4)
- **Published**: 2020-04-03 19:00:23+00:00
- **Updated**: 2020-05-12 01:21:36+00:00
- **Authors**: Elizabeth K. Cole, Joseph Y. Cheng, John M. Pauly, Shreyas S. Vasanawala
- **Comment**: None
- **Journal**: None
- **Summary**: Many real-world signal sources are complex-valued, having real and imaginary components. However, the vast majority of existing deep learning platforms and network architectures do not support the use of complex-valued data. MRI data is inherently complex-valued, so existing approaches discard the richer algebraic structure of the complex data. In this work, we investigate end-to-end complex-valued convolutional neural networks - specifically, for image reconstruction in lieu of two-channel real-valued networks. We apply this to magnetic resonance imaging reconstruction for the purpose of accelerating scan times and determine the performance of various promising complex-valued activation functions. We find that complex-valued CNNs with complex-valued convolutions provide superior reconstructions compared to real-valued convolutions with the same number of trainable parameters, over a variety of network architectures and datasets.



### TensorFI: A Flexible Fault Injection Framework for TensorFlow Applications
- **Arxiv ID**: http://arxiv.org/abs/2004.01743v1
- **DOI**: None
- **Categories**: **cs.DC**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.01743v1)
- **Published**: 2020-04-03 19:26:23+00:00
- **Updated**: 2020-04-03 19:26:23+00:00
- **Authors**: Zitao Chen, Niranjhana Narayanan, Bo Fang, Guanpeng Li, Karthik Pattabiraman, Nathan DeBardeleben
- **Comment**: A preliminary version of this work was published in a workshop
- **Journal**: None
- **Summary**: As machine learning (ML) has seen increasing adoption in safety-critical domains (e.g., autonomous vehicles), the reliability of ML systems has also grown in importance. While prior studies have proposed techniques to enable efficient error-resilience techniques (e.g., selective instruction duplication), a fundamental requirement for realizing these techniques is a detailed understanding of the application's resilience.   In this work, we present TensorFI, a high-level fault injection (FI) framework for TensorFlow-based applications. TensorFI is able to inject both hardware and software faults in general TensorFlow programs. TensorFI is a configurable FI tool that is flexible, easy to use, and portable. It can be integrated into existing TensorFlow programs to assess their resilience for different fault types (e.g., faults in particular operators). We use TensorFI to evaluate the resilience of 12 ML programs, including DNNs used in the autonomous vehicle domain. Our tool is publicly available at https://github.com/DependableSystemsLab/TensorFI.



### A white-box analysis on the writer-independent dichotomy transformation applied to offline handwritten signature verification
- **Arxiv ID**: http://arxiv.org/abs/2004.03370v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.03370v2)
- **Published**: 2020-04-03 19:59:34+00:00
- **Updated**: 2020-04-14 17:51:28+00:00
- **Authors**: Victor L. F. Souza, Adriano L. I. Oliveira, Rafael M. O. Cruz, Robert Sabourin
- **Comment**: None
- **Journal**: None
- **Summary**: High number of writers, small number of training samples per writer with high intra-class variability and heavily imbalanced class distributions are among the challenges and difficulties of the offline Handwritten Signature Verification (HSV) problem. A good alternative to tackle these issues is to use a writer-independent (WI) framework. In WI systems, a single model is trained to perform signature verification for all writers from a dissimilarity space generated by the dichotomy transformation. Among the advantages of this framework is its scalability to deal with some of these challenges and its ease in managing new writers, and hence of being used in a transfer learning context. In this work, we present a white-box analysis of this approach highlighting how it handles the challenges, the dynamic selection of references through fusion function, and its application for transfer learning. All the analyses are carried out at the instance level using the instance hardness (IH) measure. The experimental results show that, using the IH analysis, we were able to characterize "good" and "bad" quality skilled forgeries as well as the frontier region between positive and negative samples. This enables futures investigations on methods for improving discrimination between genuine signatures and skilled forgeries by considering these characterizations.



### Differential 3D Facial Recognition: Adding 3D to Your State-of-the-Art 2D Method
- **Arxiv ID**: http://arxiv.org/abs/2004.03385v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.03385v1)
- **Published**: 2020-04-03 20:17:14+00:00
- **Updated**: 2020-04-03 20:17:14+00:00
- **Authors**: J. Matias Di Martino, Fernando Suzacq, Mauricio Delbracio, Qiang Qiu, Guillermo Sapiro
- **Comment**: None
- **Journal**: None
- **Summary**: Active illumination is a prominent complement to enhance 2D face recognition and make it more robust, e.g., to spoofing attacks and low-light conditions. In the present work we show that it is possible to adopt active illumination to enhance state-of-the-art 2D face recognition approaches with 3D features, while bypassing the complicated task of 3D reconstruction. The key idea is to project over the test face a high spatial frequency pattern, which allows us to simultaneously recover real 3D information plus a standard 2D facial image. Therefore, state-of-the-art 2D face recognition solution can be transparently applied, while from the high frequency component of the input image, complementary 3D facial features are extracted. Experimental results on ND-2006 dataset show that the proposed ideas can significantly boost face recognition performance and dramatically improve the robustness to spoofing attacks.



### Privacy-Preserving Eye Videos using Rubber Sheet Model
- **Arxiv ID**: http://arxiv.org/abs/2004.01792v1
- **DOI**: 10.1145/3379156.3391375
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.01792v1)
- **Published**: 2020-04-03 21:59:38+00:00
- **Updated**: 2020-04-03 21:59:38+00:00
- **Authors**: Aayush K Chaudhary, Jeff B. Pelz
- **Comment**: Will be published in ETRA 20 Short Papers, June 2-5, 2020, Stuttgart,
  Germany Copyright 2020 Association for Computing Machinery
- **Journal**: None
- **Summary**: Video-based eye trackers estimate gaze based on eye images/videos. As security and privacy concerns loom over technological advancements, tackling such challenges is crucial. We present a new approach to handle privacy issues in eye videos by replacing the current identifiable iris texture with a different iris template in the video capture pipeline based on the Rubber Sheet Model. We extend to image blending and median-value representations to demonstrate that videos can be manipulated without significantly degrading segmentation and pupil detection accuracy.



### Self-Supervised Viewpoint Learning From Image Collections
- **Arxiv ID**: http://arxiv.org/abs/2004.01793v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.01793v1)
- **Published**: 2020-04-03 22:01:41+00:00
- **Updated**: 2020-04-03 22:01:41+00:00
- **Authors**: Siva Karthik Mustikovela, Varun Jampani, Shalini De Mello, Sifei Liu, Umar Iqbal, Carsten Rother, Jan Kautz
- **Comment**: Accepted at CVPR 20
- **Journal**: None
- **Summary**: Training deep neural networks to estimate the viewpoint of objects requires large labeled training datasets. However, manually labeling viewpoints is notoriously hard, error-prone, and time-consuming. On the other hand, it is relatively easy to mine many unlabelled images of an object category from the internet, e.g., of cars or faces. We seek to answer the research question of whether such unlabeled collections of in-the-wild images can be successfully utilized to train viewpoint estimation networks for general object categories purely via self-supervision. Self-supervision here refers to the fact that the only true supervisory signal that the network has is the input image itself. We propose a novel learning framework which incorporates an analysis-by-synthesis paradigm to reconstruct images in a viewpoint aware manner with a generative network, along with symmetry and adversarial constraints to successfully supervise our viewpoint estimation network. We show that our approach performs competitively to fully-supervised approaches for several object categories like human faces, cars, buses, and trains. Our work opens up further research in self-supervised viewpoint learning and serves as a robust baseline for it. We open-source our code at https://github.com/NVlabs/SSV.



### Temporally Distributed Networks for Fast Video Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2004.01800v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.01800v2)
- **Published**: 2020-04-03 22:43:32+00:00
- **Updated**: 2020-04-07 00:44:51+00:00
- **Authors**: Ping Hu, Fabian Caba Heilbron, Oliver Wang, Zhe Lin, Stan Sclaroff, Federico Perazzi
- **Comment**: [CVPR2020] Project: https://github.com/feinanshan/TDNet
- **Journal**: None
- **Summary**: We present TDNet, a temporally distributed network designed for fast and accurate video semantic segmentation. We observe that features extracted from a certain high-level layer of a deep CNN can be approximated by composing features extracted from several shallower sub-networks. Leveraging the inherent temporal continuity in videos, we distribute these sub-networks over sequential frames. Therefore, at each time step, we only need to perform a lightweight computation to extract a sub-features group from a single sub-network. The full features used for segmentation are then recomposed by application of a novel attention propagation module that compensates for geometry deformation between frames. A grouped knowledge distillation loss is also introduced to further improve the representation power at both full and sub-feature levels. Experiments on Cityscapes, CamVid, and NYUD-v2 demonstrate that our method achieves state-of-the-art accuracy with significantly faster speed and lower latency.



### On-board Deep-learning-based Unmanned Aerial Vehicle Fault Cause Detection and Identification
- **Arxiv ID**: http://arxiv.org/abs/2005.00336v2
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2005.00336v2)
- **Published**: 2020-04-03 22:46:34+00:00
- **Updated**: 2020-05-06 18:55:28+00:00
- **Authors**: Vidyasagar Sadhu, Saman Zonouz, Dario Pompili
- **Comment**: IEEE International Conference on Robotics and Automation (ICRA), May
  2020, 6+1 pages
- **Journal**: None
- **Summary**: With the increase in use of Unmanned Aerial Vehicles (UAVs)/drones, it is important to detect and identify causes of failure in real time for proper recovery from a potential crash-like scenario or post incident forensics analysis. The cause of crash could be either a fault in the sensor/actuator system, a physical damage/attack, or a cyber attack on the drone's software. In this paper, we propose novel architectures based on deep Convolutional and Long Short-Term Memory Neural Networks (CNNs and LSTMs) to detect (via Autoencoder) and classify drone mis-operations based on sensor data. The proposed architectures are able to learn high-level features automatically from the raw sensor data and learn the spatial and temporal dynamics in the sensor data. We validate the proposed deep-learning architectures via simulations and experiments on a real drone. Empirical results show that our solution is able to detect with over 90% accuracy and classify various types of drone mis-operations (with about 99% accuracy (simulation data) and upto 88% accuracy (experimental data)).



### SqueezeSegV3: Spatially-Adaptive Convolution for Efficient Point-Cloud Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2004.01803v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.01803v2)
- **Published**: 2020-04-03 22:47:56+00:00
- **Updated**: 2021-04-13 09:42:51+00:00
- **Authors**: Chenfeng Xu, Bichen Wu, Zining Wang, Wei Zhan, Peter Vajda, Kurt Keutzer, Masayoshi Tomizuka
- **Comment**: Accepted by ECCV 2020. Code and data are available at:
  https://github.com/chenfengxu714/SqueezeSegV3.git
- **Journal**: None
- **Summary**: LiDAR point-cloud segmentation is an important problem for many applications. For large-scale point cloud segmentation, the \textit{de facto} method is to project a 3D point cloud to get a 2D LiDAR image and use convolutions to process it. Despite the similarity between regular RGB and LiDAR images, we discover that the feature distribution of LiDAR images changes drastically at different image locations. Using standard convolutions to process such LiDAR images is problematic, as convolution filters pick up local features that are only active in specific regions in the image. As a result, the capacity of the network is under-utilized and the segmentation performance decreases. To fix this, we propose Spatially-Adaptive Convolution (SAC) to adopt different filters for different locations according to the input image. SAC can be computed efficiently since it can be implemented as a series of element-wise multiplications, im2col, and standard convolution. It is a general framework such that several previous methods can be seen as special cases of SAC. Using SAC, we build SqueezeSegV3 for LiDAR point-cloud segmentation and outperform all previous published methods by at least 3.7% mIoU on the SemanticKITTI benchmark with comparable inference speed.



### Google Landmarks Dataset v2 -- A Large-Scale Benchmark for Instance-Level Recognition and Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2004.01804v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.01804v2)
- **Published**: 2020-04-03 22:52:17+00:00
- **Updated**: 2020-11-02 18:30:45+00:00
- **Authors**: Tobias Weyand, Andre Araujo, Bingyi Cao, Jack Sim
- **Comment**: CVPR20 camera-ready (oral) + appendices
- **Journal**: None
- **Summary**: While image retrieval and instance recognition techniques are progressing rapidly, there is a need for challenging datasets to accurately measure their performance -- while posing novel challenges that are relevant for practical applications. We introduce the Google Landmarks Dataset v2 (GLDv2), a new benchmark for large-scale, fine-grained instance recognition and image retrieval in the domain of human-made and natural landmarks. GLDv2 is the largest such dataset to date by a large margin, including over 5M images and 200k distinct instance labels. Its test set consists of 118k images with ground truth annotations for both the retrieval and recognition tasks. The ground truth construction involved over 800 hours of human annotator work. Our new dataset has several challenging properties inspired by real world applications that previous datasets did not consider: An extremely long-tailed class distribution, a large fraction of out-of-domain test photos and large intra-class variability. The dataset is sourced from Wikimedia Commons, the world's largest crowdsourced collection of landmark photos. We provide baseline results for both recognition and retrieval tasks based on state-of-the-art methods as well as competitive results from a public challenge. We further demonstrate the suitability of the dataset for transfer learning by showing that image embeddings trained on it achieve competitive retrieval performance on independent datasets. The dataset images, ground-truth and metric scoring code are available at https://github.com/cvdfoundation/google-landmark.



### TimeGate: Conditional Gating of Segments in Long-range Activities
- **Arxiv ID**: http://arxiv.org/abs/2004.01808v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.01808v1)
- **Published**: 2020-04-03 23:14:35+00:00
- **Updated**: 2020-04-03 23:14:35+00:00
- **Authors**: Noureldien Hussein, Mihir Jain, Babak Ehteshami Bejnordi
- **Comment**: None
- **Journal**: None
- **Summary**: When recognizing a long-range activity, exploring the entire video is exhaustive and computationally expensive, as it can span up to a few minutes. Thus, it is of great importance to sample only the salient parts of the video. We propose TimeGate, along with a novel conditional gating module, for sampling the most representative segments from the long-range activity. TimeGate has two novelties that address the shortcomings of previous sampling methods, as SCSampler. First, it enables a differentiable sampling of segments. Thus, TimeGate can be fitted with modern CNNs and trained end-to-end as a single and unified model.Second, the sampling is conditioned on both the segments and their context. Consequently, TimeGate is better suited for long-range activities, where the importance of a segment heavily depends on the video context.TimeGate reduces the computation of existing CNNs on three benchmarks for long-range activities: Charades, Breakfast and MultiThumos. In particular, TimeGate reduces the computation of I3D by 50% while maintaining the classification accuracy.



