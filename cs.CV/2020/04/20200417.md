# Arxiv Papers in cs.CV on 2020-04-17
### Quantization Guided JPEG Artifact Correction
- **Arxiv ID**: http://arxiv.org/abs/2004.09320v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.09320v2)
- **Published**: 2020-04-17 00:10:08+00:00
- **Updated**: 2020-07-16 14:28:51+00:00
- **Authors**: Max Ehrlich, Larry Davis, Ser-Nam Lim, Abhinav Shrivastava
- **Comment**: Published in the proceedings of ECCV 2020, please see our released
  code and models at https://gitlab.com/Queuecumber/quantization-guided-ac
- **Journal**: None
- **Summary**: The JPEG image compression algorithm is the most popular method of image compression because of its ability for large compression ratios. However, to achieve such high compression, information is lost. For aggressive quantization settings, this leads to a noticeable reduction in image quality. Artifact correction has been studied in the context of deep neural networks for some time, but the current state-of-the-art methods require a different model to be trained for each quality setting, greatly limiting their practical application. We solve this problem by creating a novel architecture which is parameterized by the JPEG files quantization matrix. This allows our single model to achieve state-of-the-art performance over models trained for specific quality settings.



### DepthNet Nano: A Highly Compact Self-Normalizing Neural Network for Monocular Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/2004.08008v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.08008v1)
- **Published**: 2020-04-17 00:41:35+00:00
- **Updated**: 2020-04-17 00:41:35+00:00
- **Authors**: Linda Wang, Mahmoud Famouri, Alexander Wong
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: Depth estimation is an active area of research in the field of computer vision, and has garnered significant interest due to its rising demand in a large number of applications ranging from robotics and unmanned aerial vehicles to autonomous vehicles. A particularly challenging problem in this area is monocular depth estimation, where the goal is to infer depth from a single image. An effective strategy that has shown considerable promise in recent years for tackling this problem is the utilization of deep convolutional neural networks. Despite these successes, the memory and computational requirements of such networks have made widespread deployment in embedded scenarios very challenging. In this study, we introduce DepthNet Nano, a highly compact self normalizing network for monocular depth estimation designed using a human machine collaborative design strategy, where principled network design prototyping based on encoder-decoder design principles are coupled with machine-driven design exploration. The result is a compact deep neural network with highly customized macroarchitecture and microarchitecture designs, as well as self-normalizing characteristics, that are highly tailored for the task of embedded depth estimation. The proposed DepthNet Nano possesses a highly efficient network architecture (e.g., 24X smaller and 42X fewer MAC operations than Alhashim et al. on KITTI), while still achieving comparable performance with state-of-the-art networks on the NYU-Depth V2 and KITTI datasets. Furthermore, experiments on inference speed and energy efficiency on a Jetson AGX Xavier embedded module further illustrate the efficacy of DepthNet Nano at different resolutions and power budgets (e.g., ~14 FPS and >0.46 images/sec/watt at 384 X 1280 at a 30W power budget on KITTI).



### CPARR: Category-based Proposal Analysis for Referring Relationships
- **Arxiv ID**: http://arxiv.org/abs/2004.08028v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.08028v1)
- **Published**: 2020-04-17 01:54:01+00:00
- **Updated**: 2020-04-17 01:54:01+00:00
- **Authors**: Chuanzi He, Haidong Zhu, Jiyang Gao, Kan Chen, Ram Nevatia
- **Comment**: CVPR 2020 Workshop on Multimodal Learning
- **Journal**: None
- **Summary**: The task of referring relationships is to localize subject and object entities in an image satisfying a relationship query, which is given in the form of \texttt{<subject, predicate, object>}. This requires simultaneous localization of the subject and object entities in a specified relationship. We introduce a simple yet effective proposal-based method for referring relationships. Different from the existing methods such as SSAS, our method can generate a high-resolution result while reducing its complexity and ambiguity. Our method is composed of two modules: a category-based proposal generation module to select the proposals related to the entities and a predicate analysis module to score the compatibility of pairs of selected proposals. We show state-of-the-art performance on the referring relationship task on two public datasets: Visual Relationship Detection and Visual Genome.



### Conservative Plane Releasing for Spatial Privacy Protection in Mixed Reality
- **Arxiv ID**: http://arxiv.org/abs/2004.08029v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2004.08029v1)
- **Published**: 2020-04-17 01:57:58+00:00
- **Updated**: 2020-04-17 01:57:58+00:00
- **Authors**: Jaybie A. de Guzman, Kanchana Thilakarathna, Aruna Seneviratne
- **Comment**: 15 pages
- **Journal**: None
- **Summary**: Augmented reality (AR) or mixed reality (MR) platforms require spatial understanding to detect objects or surfaces, often including their structural (i.e. spatial geometry) and photometric (e.g. color, and texture) attributes, to allow applications to place virtual or synthetic objects seemingly "anchored" on to real world objects; in some cases, even allowing interactions between the physical and virtual objects. These functionalities require AR/MR platforms to capture the 3D spatial information with high resolution and frequency; however, these pose unprecedented risks to user privacy. Aside from objects being detected, spatial information also reveals the location of the user with high specificity, e.g. in which part of the house the user is. In this work, we propose to leverage spatial generalizations coupled with conservative releasing to provide spatial privacy while maintaining data utility. We designed an adversary that builds up on existing place and shape recognition methods over 3D data as attackers to which the proposed spatial privacy approach can be evaluated against. Then, we simulate user movement within spaces which reveals more of their space as they move around utilizing 3D point clouds collected from Microsoft HoloLens. Results show that revealing no more than 11 generalized planes--accumulated from successively revealed spaces with large enough radius, i.e. $r\leq1.0m$--can make an adversary fail in identifying the spatial location of the user for at least half of the time. Furthermore, if the accumulated spaces are of smaller radius, i.e. each successively revealed space is $r\leq 0.5m$, we can release up to 29 generalized planes while enjoying both better data utility and privacy.



### Smartphone camera based pointer
- **Arxiv ID**: http://arxiv.org/abs/2004.08030v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2004.08030v1)
- **Published**: 2020-04-17 01:59:23+00:00
- **Updated**: 2020-04-17 01:59:23+00:00
- **Authors**: Predrag Lazic
- **Comment**: 13 pages, 8 figures
- **Journal**: None
- **Summary**: Large screen displays are omnipresent today as a part of infrastructure for presentations and entertainment. Also powerful smartphones with integrated camera(s) are ubiquitous. However, there are not many ways in which smartphones and screens can interact besides casting the video from a smartphone. In this paper, we present a novel idea that turns a smartphone into a direct virtual pointer on the screen using the phone's camera. The idea and its implementation are simple, robust, efficient and fun to use. Besides the mathematical concepts of the idea we accompany the paper with a small javascript project (www.mobiletvgames.com) which demonstrates the possibility of the new interaction technique presented as a massive multiplayer game in the HTML5 framework.



### Knowledge-Based Visual Question Answering in Videos
- **Arxiv ID**: http://arxiv.org/abs/2004.08385v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2004.08385v1)
- **Published**: 2020-04-17 02:06:26+00:00
- **Updated**: 2020-04-17 02:06:26+00:00
- **Authors**: Noa Garcia, Mayu Otani, Chenhui Chu, Yuta Nakashima
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1910.10706
- **Journal**: None
- **Summary**: We propose a novel video understanding task by fusing knowledge-based and video question answering. First, we introduce KnowIT VQA, a video dataset with 24,282 human-generated question-answer pairs about a popular sitcom. The dataset combines visual, textual and temporal coherence reasoning together with knowledge-based questions, which need of the experience obtained from the viewing of the series to be answered. Second, we propose a video understanding model by combining the visual and textual video content with specific knowledge about the show. Our main findings are: (i) the incorporation of knowledge produces outstanding improvements for VQA in video, and (ii) the performance on KnowIT VQA still lags well behind human accuracy, indicating its usefulness for studying current video modelling limitations.



### Approximate Inverse Reinforcement Learning from Vision-based Imitation Learning
- **Arxiv ID**: http://arxiv.org/abs/2004.08051v3
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.08051v3)
- **Published**: 2020-04-17 03:36:50+00:00
- **Updated**: 2021-04-08 19:52:37+00:00
- **Authors**: Keuntaek Lee, Bogdan Vlahov, Jason Gibson, James M. Rehg, Evangelos A. Theodorou
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we present a method for obtaining an implicit objective function for vision-based navigation. The proposed methodology relies on Imitation Learning, Model Predictive Control (MPC), and an interpretation technique used in Deep Neural Networks. We use Imitation Learning as a means to do Inverse Reinforcement Learning in order to create an approximate cost function generator for a visual navigation challenge. The resulting cost function, the costmap, is used in conjunction with MPC for real-time control and outperforms other state-of-the-art costmap generators in novel environments. The proposed process allows for simple training and robustness to out-of-sample data. We apply our method to the task of vision-based autonomous driving in multiple real and simulated environments and show its generalizability.



### A modified deep convolutional neural network for detecting COVID-19 and pneumonia from chest X-ray images based on the concatenation of Xception and ResNet50V2
- **Arxiv ID**: http://arxiv.org/abs/2004.08052v2
- **DOI**: 10.1016/j.imu.2020.100360
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.08052v2)
- **Published**: 2020-04-17 03:38:39+00:00
- **Updated**: 2021-05-04 01:14:11+00:00
- **Authors**: Mohammad Rahimzadeh, Abolfazl Attar
- **Comment**: This is a preprint of an article published in Informatics in Medicine
  Unlocked journal. The final authenticated version is available online at
  https://doi.org/10.1016/j.imu.2020.100360. The Code is available at
  https://github.com/mr7495/covid19
- **Journal**: None
- **Summary**: In this paper, we have trained several deep convolutional networks with introduced training techniques for classifying X-ray images into three classes: normal, pneumonia, and COVID-19, based on two open-source datasets. Our data contains 180 X-ray images that belong to persons infected with COVID-19, and we attempted to apply methods to achieve the best possible results. In this research, we introduce some training techniques that help the network learn better when we have an unbalanced dataset (fewer cases of COVID-19 along with more cases from other classes). We also propose a neural network that is a concatenation of the Xception and ResNet50V2 networks. This network achieved the best accuracy by utilizing multiple features extracted by two robust networks. For evaluating our network, we have tested it on 11302 images to report the actual accuracy achievable in real circumstances. The average accuracy of the proposed network for detecting COVID-19 cases is 99.50%, and the overall average accuracy for all classes is 91.4%.



### Self-Learning with Rectification Strategy for Human Parsing
- **Arxiv ID**: http://arxiv.org/abs/2004.08055v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.08055v1)
- **Published**: 2020-04-17 03:51:30+00:00
- **Updated**: 2020-04-17 03:51:30+00:00
- **Authors**: Tao Li, Zhiyuan Liang, Sanyuan Zhao, Jiahao Gong, Jianbing Shen
- **Comment**: Accepted to CVPR 2020
- **Journal**: None
- **Summary**: In this paper, we solve the sample shortage problem in the human parsing task. We begin with the self-learning strategy, which generates pseudo-labels for unlabeled data to retrain the model. However, directly using noisy pseudo-labels will cause error amplification and accumulation. Considering the topology structure of human body, we propose a trainable graph reasoning method that establishes internal structural connections between graph nodes to correct two typical errors in the pseudo-labels, i.e., the global structural error and the local consistency error. For the global error, we first transform category-wise features into a high-level graph model with coarse-grained structural information, and then decouple the high-level graph to reconstruct the category features. The reconstructed features have a stronger ability to represent the topology structure of the human body. Enlarging the receptive field of features can effectively reducing the local error. We first project feature pixels into a local graph model to capture pixel-wise relations in a hierarchical graph manner, then reverse the relation information back to the pixels. With the global structural and local consistency modules, these errors are rectified and confident pseudo-labels are generated for retraining. Extensive experiments on the LIP and the ATR datasets demonstrate the effectiveness of our global and local rectification modules. Our method outperforms other state-of-the-art methods in supervised human parsing tasks.



### Generative Adversarial Networks for Video-to-Video Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2004.08058v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.08058v1)
- **Published**: 2020-04-17 04:16:37+00:00
- **Updated**: 2020-04-17 04:16:37+00:00
- **Authors**: Jiawei Chen, Yuexiang Li, Kai Ma, Yefeng Zheng
- **Comment**: Accepted by AAAI 2020
- **Journal**: None
- **Summary**: Endoscopic videos from multicentres often have different imaging conditions, e.g., color and illumination, which make the models trained on one domain usually fail to generalize well to another. Domain adaptation is one of the potential solutions to address the problem. However, few of existing works focused on the translation of video-based data. In this work, we propose a novel generative adversarial network (GAN), namely VideoGAN, to transfer the video-based data across different domains. As the frames of a video may have similar content and imaging conditions, the proposed VideoGAN has an X-shape generator to preserve the intra-video consistency during translation. Furthermore, a loss function, namely color histogram loss, is proposed to tune the color distribution of each translated frame. Two colonoscopic datasets from different centres, i.e., CVC-Clinic and ETIS-Larib, are adopted to evaluate the performance of domain adaptation of our VideoGAN. Experimental results demonstrate that the adapted colonoscopic video generated by our VideoGAN can significantly boost the segmentation accuracy, i.e., an improvement of 5%, of colorectal polyps on multicentre datasets. As our VideoGAN is a general network architecture, we also evaluate its performance with the CamVid driving video dataset on the cloudy-to-sunny translation task. Comprehensive experiments show that the domain gap could be substantially narrowed down by our VideoGAN.



### YuruGAN: Yuru-Chara Mascot Generator Using Generative Adversarial Networks With Clustering Small Dataset
- **Arxiv ID**: http://arxiv.org/abs/2004.08066v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.08066v1)
- **Published**: 2020-04-17 05:18:49+00:00
- **Updated**: 2020-04-17 05:18:49+00:00
- **Authors**: Yuki Hagiwara, Toshihisa Tanaka
- **Comment**: conference
- **Journal**: None
- **Summary**: A yuru-chara is a mascot character created by local governments and companies for publicizing information on areas and products. Because it takes various costs to create a yuruchara, the utilization of machine learning techniques such as generative adversarial networks (GANs) can be expected. In recent years, it has been reported that the use of class conditions in a dataset for GANs training stabilizes learning and improves the quality of the generated images. However, it is difficult to apply class conditional GANs when the amount of original data is small and when a clear class is not given, such as a yuruchara image. In this paper, we propose a class conditional GAN based on clustering and data augmentation. Specifically, first, we performed clustering based on K-means++ on the yuru-chara image dataset and converted it into a class conditional dataset. Next, data augmentation was performed on the class conditional dataset so that the amount of data was increased five times. In addition, we built a model that incorporates ResBlock and self-attention into a network based on class conditional GAN and trained the class conditional yuru-chara dataset. As a result of evaluating the generated images, the effect on the generated images by the difference of the clustering method was confirmed.



### One-vs-Rest Network-based Deep Probability Model for Open Set Recognition
- **Arxiv ID**: http://arxiv.org/abs/2004.08067v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.08067v2)
- **Published**: 2020-04-17 05:24:34+00:00
- **Updated**: 2020-05-28 06:37:47+00:00
- **Authors**: Jaeyeon Jang, Chang Ouk Kim
- **Comment**: 16 pages, 11 figures
- **Journal**: None
- **Summary**: Unknown examples that are unseen during training often appear in real-world computer vision tasks, and an intelligent self-learning system should be able to differentiate between known and unknown examples. Open set recognition, which addresses this problem, has been studied for approximately a decade. However, conventional open set recognition methods based on deep neural networks (DNNs) lack a foundation for post recognition score analysis. In this paper, we propose a DNN structure in which multiple one-vs-rest sigmoid networks follow a convolutional neural network feature extractor. A one-vs-rest network, which is composed of rectified linear unit activation functions for the hidden layers and a single sigmoid target class output node, can maximize the ability to learn information from nonmatch examples. Furthermore, the network yields a sophisticated nonlinear features-to-output mapping that is explainable in the feature space. By introducing extreme value theory-based calibration techniques, the nonlinear and explainable mapping provides a well-grounded class membership probability models. Our experiments show that one-vs-rest networks can provide more informative hidden representations for unknown examples than the commonly used SoftMax layer. In addition, the proposed probability model outperformed the state-of-the art methods in open set classification scenarios.



### Transform and Tell: Entity-Aware News Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/2004.08070v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, I.4.0; I.2.7
- **Links**: [PDF](http://arxiv.org/pdf/2004.08070v2)
- **Published**: 2020-04-17 05:44:37+00:00
- **Updated**: 2020-06-13 01:21:14+00:00
- **Authors**: Alasdair Tran, Alexander Mathews, Lexing Xie
- **Comment**: Published in CVPR 2020. Code is available at
  https://github.com/alasdairtran/transform-and-tell and demo is available at
  https://transform-and-tell.ml
- **Journal**: The IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR), 2020, pp. 13035-13045
- **Summary**: We propose an end-to-end model which generates captions for images embedded in news articles. News images present two key challenges: they rely on real-world knowledge, especially about named entities; and they typically have linguistically rich captions that include uncommon words. We address the first challenge by associating words in the caption with faces and objects in the image, via a multi-modal, multi-head attention mechanism. We tackle the second challenge with a state-of-the-art transformer language model that uses byte-pair-encoding to generate captions as a sequence of word parts. On the GoodNews dataset, our model outperforms the previous state of the art by a factor of four in CIDEr score (13 to 54). This performance gain comes from a unique combination of language models, word representation, image embeddings, face embeddings, object embeddings, and improvements in neural network design. We also introduce the NYTimes800k dataset which is 70% larger than GoodNews, has higher article quality, and includes the locations of images within articles as an additional contextual cue.



### Adaptive Neuron-wise Discriminant Criterion and Adaptive Center Loss at Hidden Layer for Deep Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2004.08074v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.08074v1)
- **Published**: 2020-04-17 05:52:24+00:00
- **Updated**: 2020-04-17 05:52:24+00:00
- **Authors**: Motoshi Abe, Junichi Miyao, Takio Kurita
- **Comment**: Accepted to IJCNN 2020
- **Journal**: None
- **Summary**: A deep convolutional neural network (CNN) has been widely used in image classification and gives better classification accuracy than the other techniques. The softmax cross-entropy loss function is often used for classification tasks. There are some works to introduce the additional terms in the objective function for training to make the features of the output layer more discriminative. The neuron-wise discriminant criterion makes the input feature of each neuron in the output layer discriminative by introducing the discriminant criterion to each of the features. Similarly, the center loss was introduced to the features before the softmax activation function for face recognition to make the deep features discriminative. The ReLU function is often used for the network as an active function in the hidden layers of the CNN. However, it is observed that the deep features trained by using the ReLU function are not discriminative enough and show elongated shapes. In this paper, we propose to use the neuron-wise discriminant criterion at the output layer and the center-loss at the hidden layer. Also, we introduce the online computation of the means of each class with the exponential forgetting. We named them adaptive neuron-wise discriminant criterion and adaptive center loss, respectively. The effectiveness of the integration of the adaptive neuron-wise discriminant criterion and the adaptive center loss is shown by the experiments with MNSIT, FashionMNIST, CIFAR10, CIFAR100, and STL10. Source code is at https://github.com/i13abe/Adaptive-discriminant-and-center



### Image Processing Based Scene-Text Detection and Recognition with Tesseract
- **Arxiv ID**: http://arxiv.org/abs/2004.08079v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.08079v1)
- **Published**: 2020-04-17 06:58:35+00:00
- **Updated**: 2020-04-17 06:58:35+00:00
- **Authors**: Ebin Zacharias, Martin Teuchler, Bénédicte Bernier
- **Comment**: 6 pages, 4 figures
- **Journal**: None
- **Summary**: Text Recognition is one of the challenging tasks of computer vision with considerable practical interest. Optical character recognition (OCR) enables different applications for automation. This project focuses on word detection and recognition in natural images. In comparison to reading text in scanned documents, the targeted problem is significantly more challenging. The use case in focus facilitates the possibility to detect the text area in natural scenes with greater accuracy because of the availability of images under constraints. This is achieved using a camera mounted on a truck capturing likewise images round-the-clock. The detected text area is then recognized using Tesseract OCR engine. Even though it benefits low computational power requirements, the model is limited to only specific use cases. This paper discusses a critical false positive case scenario occurred while testing and elaborates the strategy used to alleviate the problem. The project achieved a correct character recognition rate of more than 80\%. This paper outlines the stages of development, the major challenges and some of the interesting findings of the project.



### Meta-Meta Classification for One-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2004.08083v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.08083v4)
- **Published**: 2020-04-17 07:05:03+00:00
- **Updated**: 2020-06-14 01:02:11+00:00
- **Authors**: Arkabandhu Chowdhury, Dipak Chaudhari, Swarat Chaudhuri, Chris Jermaine
- **Comment**: 10 pages without references, 3 figures
- **Journal**: None
- **Summary**: We present a new approach, called meta-meta classification, to learning in small-data settings. In this approach, one uses a large set of learning problems to design an ensemble of learners, where each learner has high bias and low variance and is skilled at solving a specific type of learning problem. The meta-meta classifier learns how to examine a given learning problem and combine the various learners to solve the problem. The meta-meta learning approach is especially suited to solving few-shot learning tasks, as it is easier to learn to classify a new learning problem with little data than it is to apply a learning algorithm to a small data set. We evaluate the approach on a one-shot, one-class-versus-all classification task and show that it is able to outperform traditional meta-learning as well as ensembling approaches.



### Fast Soft Color Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2004.08096v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2004.08096v1)
- **Published**: 2020-04-17 07:43:33+00:00
- **Updated**: 2020-04-17 07:43:33+00:00
- **Authors**: Naofumi Akimoto, Huachun Zhu, Yanghua Jin, Yoshimitsu Aoki
- **Comment**: Accepted at CVPR 2020
- **Journal**: None
- **Summary**: We address the problem of soft color segmentation, defined as decomposing a given image into several RGBA layers, each containing only homogeneous color regions. The resulting layers from decomposition pave the way for applications that benefit from layer-based editing, such as recoloring and compositing of images and videos. The current state-of-the-art approach for this problem is hindered by slow processing time due to its iterative nature, and consequently does not scale to certain real-world scenarios. To address this issue, we propose a neural network based method for this task that decomposes a given image into multiple layers in a single forward pass. Furthermore, our method separately decomposes the color layers and the alpha channel layers. By leveraging a novel training objective, our method achieves proper assignment of colors amongst layers. As a consequence, our method achieve promising quality without existing issue of inference speed for iterative approaches. Our thorough experimental analysis shows that our method produces qualitative and quantitative results comparable to previous methods while achieving a 300,000x speed improvement. Finally, we utilize our proposed method on several applications, and demonstrate its speed advantage, especially in video editing.



### Cascaded Context Enhancement Network for Automatic Skin Lesion Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2004.08107v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.08107v3)
- **Published**: 2020-04-17 08:25:17+00:00
- **Updated**: 2021-06-07 08:01:03+00:00
- **Authors**: Ruxin Wang, Shuyuan Chen, Chaojie Ji, Ye Li
- **Comment**: None
- **Journal**: None
- **Summary**: Skin lesion segmentation is an important step for automatic melanoma diagnosis. Due to the non-negligible diversity of lesions from different patients, extracting powerful context for fine-grained semantic segmentation is still challenging today. Although the deep convolutional neural network (CNNs) have made significant improvements on skin lesion segmentation, they often fail to reserve the spatial details and long-range dependencies context due to consecutive convolution striding and pooling operations inside CNNs. In this paper, we formulate a cascaded context enhancement neural network for automatic skin lesion segmentation. A new cascaded context aggregation (CCA) module with a gate-based information integration approach is proposed to sequentially and selectively aggregate original image and multi-level features from the encoder sub-network. The generated context is further utilized to guide discriminative features extraction by the designed context-guided local affinity (CGL) module. Furthermore, an auxiliary loss is added to the CCA module for refining the prediction. In our work, we evaluate our approach on four public skin dermoscopy image datasets. The proposed method achieves the Jaccard Index (JA) of 87.1%, 80.3%, 83.4%, and 86.6% on ISIC-2016, ISIC-2017, ISIC-2018, and PH2 datasets, which are higher than other state-of-the-art models respectively.



### LiteDenseNet: A Lightweight Network for Hyperspectral Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2004.08112v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.08112v2)
- **Published**: 2020-04-17 08:38:52+00:00
- **Updated**: 2020-04-26 14:15:17+00:00
- **Authors**: Rui Li, Chenxi Duan
- **Comment**: The random split among training, test, and validation is not
  acceptable in cube-based methods, which may lead to test data leakage
- **Journal**: None
- **Summary**: Hyperspectral Image (HSI) classification based on deep learning has been an attractive area in recent years. However, as a kind of data-driven algorithm, deep learning method usually requires numerous computational resources and high-quality labelled dataset, while the cost of high-performance computing and data annotation is expensive. In this paper, to reduce dependence on massive calculation and labelled samples, we propose a lightweight network architecture (LiteDenseNet) based on DenseNet for Hyperspectral Image Classification. Inspired by GoogLeNet and PeleeNet, we design a 3D two-way dense layer to capture the local and global features of the input. As convolution is a computationally intensive operation, we introduce group convolution to decrease calculation cost and parameter size further. Thus, the number of parameters and the consumptions of calculation are observably less than contrapositive deep learning methods, which means LiteDenseNet owns simpler architecture and higher efficiency. A series of quantitative experiences on 6 widely used hyperspectral datasets show that the proposed LiteDenseNet obtains the state-of-the-art performance, even though when the absence of labelled samples is severe.



### Triplet Loss for Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/2004.08116v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.08116v1)
- **Published**: 2020-04-17 08:48:29+00:00
- **Updated**: 2020-04-17 08:48:29+00:00
- **Authors**: Hideki Oki, Motoshi Abe, Junichi Miyao, Takio Kurita
- **Comment**: Accepted to IJCNN 2020, Source code is at
  https://github.com/i13abe/Triplet-Loss-for-Knowledge-Distillation
- **Journal**: None
- **Summary**: In recent years, deep learning has spread rapidly, and deeper, larger models have been proposed. However, the calculation cost becomes enormous as the size of the models becomes larger. Various techniques for compressing the size of the models have been proposed to improve performance while reducing computational costs. One of the methods to compress the size of the models is knowledge distillation (KD). Knowledge distillation is a technique for transferring knowledge of deep or ensemble models with many parameters (teacher model) to smaller shallow models (student model). Since the purpose of knowledge distillation is to increase the similarity between the teacher model and the student model, we propose to introduce the concept of metric learning into knowledge distillation to make the student model closer to the teacher model using pairs or triplets of the training samples. In metric learning, the researchers are developing the methods to build a model that can increase the similarity of outputs for similar samples. Metric learning aims at reducing the distance between similar and increasing the distance between dissimilar. The functionality of the metric learning to reduce the differences between similar outputs can be used for the knowledge distillation to reduce the differences between the outputs of the teacher model and the student model. Since the outputs of the teacher model for different objects are usually different, the student model needs to distinguish them. We think that metric learning can clarify the difference between the different outputs, and the performance of the student model could be improved. We have performed experiments to compare the proposed method with state-of-the-art knowledge distillation methods.



### Object Detection and Recognition of Swap-Bodies using Camera mounted on a Vehicle
- **Arxiv ID**: http://arxiv.org/abs/2004.08118v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.08118v1)
- **Published**: 2020-04-17 08:49:54+00:00
- **Updated**: 2020-04-17 08:49:54+00:00
- **Authors**: Ebin Zacharias, Didier Stricker, Martin Teuchler, Kripasindhu Sarkar
- **Comment**: 13 pages 9 figures 2 tables
- **Journal**: None
- **Summary**: Object detection and identification is a challenging area of computer vision and a fundamental requirement for autonomous cars. This project aims to jointly perform object detection of a swap-body and to find the type of swap-body by reading an ILU code using an efficient optical character recognition (OCR) method. Recent research activities have drastically improved deep learning techniques which proves to enhance the field of computer vision. Collecting enough images for training the model is a critical step towards achieving good results. The data for training were collected from different locations with maximum possible variations and the details are explained. In addition, data augmentation methods applied for training has proved to be effective in improving the performance of the trained model. Training the model achieved good results and the test results are also provided. The final model was tested with images and videos. Finally, this paper also draws attention to some of the major challenges faced during various stages of the project and the possible solutions applied.



### A Cross-Stitch Architecture for Joint Registration and Segmentation in Adaptive Radiotherapy
- **Arxiv ID**: http://arxiv.org/abs/2004.08122v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.08122v1)
- **Published**: 2020-04-17 08:55:23+00:00
- **Updated**: 2020-04-17 08:55:23+00:00
- **Authors**: Laurens Beljaards, Mohamed S. Elmahdy, Fons Verbeek, Marius Staring
- **Comment**: Accepted to MIDL 2020
- **Journal**: None
- **Summary**: Recently, joint registration and segmentation has been formulated in a deep learning setting, by the definition of joint loss functions. In this work, we investigate joining these tasks at the architectural level. We propose a registration network that integrates segmentation propagation between images, and a segmentation network to predict the segmentation directly. These networks are connected into a single joint architecture via so-called cross-stitch units, allowing information to be exchanged between the tasks in a learnable manner. The proposed method is evaluated in the context of adaptive image-guided radiotherapy, using daily prostate CT imaging. Two datasets from different institutes and manufacturers were involved in the study. The first dataset was used for training (12 patients) and validation (6 patients), while the second dataset was used as an independent test set (14 patients). In terms of mean surface distance, our approach achieved $1.06 \pm 0.3$ mm, $0.91 \pm 0.4$ mm, $1.27 \pm 0.4$ mm, and $1.76 \pm 0.8$ mm on the validation set and $1.82 \pm 2.4$ mm, $2.45 \pm 2.4$ mm, $2.45 \pm 5.0$ mm, and $2.57 \pm 2.3$ mm on the test set for the prostate, bladder, seminal vesicles, and rectum, respectively. The proposed multi-task network outperformed single-task networks, as well as a network only joined through the loss function, thus demonstrating the capability to leverage the individual strengths of the segmentation and registration tasks. The obtained performance as well as the inference speed make this a promising candidate for daily re-contouring in adaptive radiotherapy, potentially reducing treatment-related side effects and improving quality-of-life after treatment.



### Non-Blocking Simultaneous Multithreading: Embracing the Resiliency of Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2004.09309v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AR, cs.CV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2004.09309v2)
- **Published**: 2020-04-17 09:29:56+00:00
- **Updated**: 2020-09-17 20:54:37+00:00
- **Authors**: Gil Shomron, Uri Weiser
- **Comment**: MICRO-53
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) are known for their inability to utilize underlying hardware resources due to hardware susceptibility to sparse activations and weights. Even in finer granularities, many of the non-zero values hold a portion of zero-valued bits that may cause inefficiencies when executed on hardware. Inspired by conventional CPU simultaneous multithreading (SMT) that increases computer resource utilization by sharing them across several threads, we propose non-blocking SMT (NB-SMT) designated for DNN accelerators. Like conventional SMT, NB-SMT shares hardware resources among several execution flows. Yet, unlike SMT, NB-SMT is non-blocking, as it handles structural hazards by exploiting the algorithmic resiliency of DNNs. Instead of opportunistically dispatching instructions while they wait in a reservation station for available hardware, NB-SMT temporarily reduces the computation precision to accommodate all threads at once, enabling a non-blocking operation. We demonstrate NB-SMT applicability using SySMT, an NB-SMT-enabled output-stationary systolic array (OS-SA). Compared with a conventional OS-SA, a 2-threaded SySMT consumes 1.4x the area and delivers 2x speedup with 33% energy savings and less than 1% accuracy degradation of state-of-the-art CNNs with ImageNet. A 4-threaded SySMT consumes 2.5x the area and delivers, for example, 3.4x speedup and 39% energy savings with 1% accuracy degradation of 40%-pruned ResNet-18.



### Modeling Extent-of-Texture Information for Ground Terrain Recognition
- **Arxiv ID**: http://arxiv.org/abs/2004.08141v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.08141v2)
- **Published**: 2020-04-17 09:39:35+00:00
- **Updated**: 2020-10-27 10:03:12+00:00
- **Authors**: Shuvozit Ghose, Pinaki Nath Chowdhury, Partha Pratim Roy, Umapada Pal
- **Comment**: Accepted in ICPR 2020
- **Journal**: None
- **Summary**: Ground Terrain Recognition is a difficult task as the context information varies significantly over the regions of a ground terrain image. In this paper, we propose a novel approach towards ground-terrain recognition via modeling the Extent-of-Texture information to establish a balance between the order-less texture component and ordered-spatial information locally. At first, the proposed method uses a CNN backbone feature extractor network to capture meaningful information of a ground terrain image, and model the extent of texture and shape information locally. Then, the order-less texture information and ordered shape information are encoded in a patch-wise manner, which is utilized by intra-domain message passing module to make every patch aware of each other for rich feature learning. Next, the Extent-of-Texture (EoT) Guided Inter-domain Message Passing module combines the extent of texture and shape information with the encoded texture and shape information in a patch-wise fashion for sharing knowledge to balance out the order-less texture information with ordered shape information. Further, Bilinear model generates a pairwise correlation between the order-less texture information and ordered shape information. Finally, the ground-terrain image classification is performed by a fully connected layer. The experimental results indicate superior performance of the proposed model over existing state-of-the-art techniques on publicly available datasets like DTD, MINC and GTOS-mobile.



### Detailed 2D-3D Joint Representation for Human-Object Interaction
- **Arxiv ID**: http://arxiv.org/abs/2004.08154v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.08154v2)
- **Published**: 2020-04-17 10:22:12+00:00
- **Updated**: 2020-05-21 04:51:52+00:00
- **Authors**: Yong-Lu Li, Xinpeng Liu, Han Lu, Shiyi Wang, Junqi Liu, Jiefeng Li, Cewu Lu
- **Comment**: Accepted to CVPR 2020, supplementary materials included, code
  available:https://github.com/DirtyHarryLYL/DJ-RN
- **Journal**: None
- **Summary**: Human-Object Interaction (HOI) detection lies at the core of action understanding. Besides 2D information such as human/object appearance and locations, 3D pose is also usually utilized in HOI learning since its view-independence. However, rough 3D body joints just carry sparse body information and are not sufficient to understand complex interactions. Thus, we need detailed 3D body shape to go further. Meanwhile, the interacted object in 3D is also not fully studied in HOI learning. In light of these, we propose a detailed 2D-3D joint representation learning method. First, we utilize the single-view human body capture method to obtain detailed 3D body, face and hand shapes. Next, we estimate the 3D object location and size with reference to the 2D human-object spatial configuration and object category priors. Finally, a joint learning framework and cross-modal consistency tasks are proposed to learn the joint HOI representation. To better evaluate the 2D ambiguity processing capacity of models, we propose a new benchmark named Ambiguous-HOI consisting of hard ambiguous images. Extensive experiments in large-scale HOI benchmark and Ambiguous-HOI show impressive effectiveness of our method. Code and data are available at https://github.com/DirtyHarryLYL/DJ-RN.



### Multi-Modal Face Anti-Spoofing Based on Central Difference Networks
- **Arxiv ID**: http://arxiv.org/abs/2004.08388v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.08388v1)
- **Published**: 2020-04-17 11:42:23+00:00
- **Updated**: 2020-04-17 11:42:23+00:00
- **Authors**: Zitong Yu, Yunxiao Qin, Xiaobai Li, Zezheng Wang, Chenxu Zhao, Zhen Lei, Guoying Zhao
- **Comment**: 1st place in "Track Multi-Modal" of ChaLearn Face Anti-spoofing
  Attack Detection Challenge@CVPR2020; Accepted by CVPR2020 Media Forensics
  Workshop. arXiv admin note: text overlap with arXiv:2003.04092
- **Journal**: None
- **Summary**: Face anti-spoofing (FAS) plays a vital role in securing face recognition systems from presentation attacks. Existing multi-modal FAS methods rely on stacked vanilla convolutions, which is weak in describing detailed intrinsic information from modalities and easily being ineffective when the domain shifts (e.g., cross attack and cross ethnicity). In this paper, we extend the central difference convolutional networks (CDCN) \cite{yu2020searching} to a multi-modal version, intending to capture intrinsic spoofing patterns among three modalities (RGB, depth and infrared). Meanwhile, we also give an elaborate study about single-modal based CDCN. Our approach won the first place in "Track Multi-Modal" as well as the second place in "Track Single-Modal (RGB)" of ChaLearn Face Anti-spoofing Attack Detection Challenge@CVPR2020 \cite{liu2020cross}. Our final submission obtains 1.02$\pm$0.59\% and 4.84$\pm$1.79\% ACER in "Track Multi-Modal" and "Track Single-Modal (RGB)", respectively. The codes are available at{https://github.com/ZitongYu/CDCN}.



### MOPT: Multi-Object Panoptic Tracking
- **Arxiv ID**: http://arxiv.org/abs/2004.08189v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2004.08189v2)
- **Published**: 2020-04-17 11:45:28+00:00
- **Updated**: 2020-05-27 14:57:01+00:00
- **Authors**: Juana Valeria Hurtado, Rohit Mohan, Wolfram Burgard, Abhinav Valada
- **Comment**: Code & models are available at
  http://rl.uni-freiburg.de/research/panoptictracking
- **Journal**: The IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR) Workshop on Scalability in Autonomous Driving, 2020
- **Summary**: Comprehensive understanding of dynamic scenes is a critical prerequisite for intelligent robots to autonomously operate in their environment. Research in this domain, which encompasses diverse perception problems, has primarily been focused on addressing specific tasks individually rather than modeling the ability to understand dynamic scenes holistically. In this paper, we introduce a novel perception task denoted as multi-object panoptic tracking (MOPT), which unifies the conventionally disjoint tasks of semantic segmentation, instance segmentation, and multi-object tracking. MOPT allows for exploiting pixel-level semantic information of 'thing' and 'stuff' classes, temporal coherence, and pixel-level associations over time, for the mutual benefit of each of the individual sub-problems. To facilitate quantitative evaluations of MOPT in a unified manner, we propose the soft panoptic tracking quality (sPTQ) metric. As a first step towards addressing this task, we propose the novel PanopticTrackNet architecture that builds upon the state-of-the-art top-down panoptic segmentation network EfficientPS by adding a new tracking head to simultaneously learn all sub-tasks in an end-to-end manner. Additionally, we present several strong baselines that combine predictions from state-of-the-art panoptic segmentation and multi-object tracking models for comparison. We present extensive quantitative and qualitative evaluations of both vision-based and LiDAR-based MOPT that demonstrate encouraging results.



### Structured Landmark Detection via Topology-Adapting Deep Graph Learning
- **Arxiv ID**: http://arxiv.org/abs/2004.08190v6
- **DOI**: 10.1007/978-3-030-58545-7_16
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.08190v6)
- **Published**: 2020-04-17 11:55:03+00:00
- **Updated**: 2020-07-23 17:00:51+00:00
- **Authors**: Weijian Li, Yuhang Lu, Kang Zheng, Haofu Liao, Chihung Lin, Jiebo Luo, Chi-Tung Cheng, Jing Xiao, Le Lu, Chang-Fu Kuo, Shun Miao
- **Comment**: Accepted to ECCV-20. Camera-ready with supplementary material
- **Journal**: None
- **Summary**: Image landmark detection aims to automatically identify the locations of predefined fiducial points. Despite recent success in this field, higher-ordered structural modeling to capture implicit or explicit relationships among anatomical landmarks has not been adequately exploited. In this work, we present a new topology-adapting deep graph learning approach for accurate anatomical facial and medical (e.g., hand, pelvis) landmark detection. The proposed method constructs graph signals leveraging both local image features and global shape features. The adaptive graph topology naturally explores and lands on task-specific structures which are learned end-to-end with two Graph Convolutional Networks (GCNs). Extensive experiments are conducted on three public facial image datasets (WFLW, 300W, and COFW-68) as well as three real-world X-ray medical datasets (Cephalometric (public), Hand and Pelvis). Quantitative results comparing with the previous state-of-the-art approaches across all studied datasets indicating the superior performance in both robustness and accuracy. Qualitative visualizations of the learned graph topologies demonstrate a physically plausible connectivity laying behind the landmarks.



### The FaceChannel: A Light-weight Deep Neural Network for Facial Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/2004.08195v1
- **DOI**: 10.1109/FG47880.2020.00070
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.08195v1)
- **Published**: 2020-04-17 12:03:14+00:00
- **Updated**: 2020-04-17 12:03:14+00:00
- **Authors**: Pablo Barros, Nikhil Churamani, Alessandra Sciutti
- **Comment**: Accepted at the Workshop on Affect Recognition in-the-wild:
  Uni/Multi-Modal Analysis & VA-AU-Expression Challenges, FG2020
- **Journal**: None
- **Summary**: Current state-of-the-art models for automatic FER are based on very deep neural networks that are difficult to train. This makes it challenging to adapt these models to changing conditions, a requirement from FER models given the subjective nature of affect perception and understanding. In this paper, we address this problem by formalizing the FaceChannel, a light-weight neural network that has much fewer parameters than common deep neural networks. We perform a series of experiments on different benchmark datasets to demonstrate how the FaceChannel achieves a comparable, if not better, performance, as compared to the current state-of-the-art in FER.



### Vehicle Position Estimation with Aerial Imagery from Unmanned Aerial Vehicles
- **Arxiv ID**: http://arxiv.org/abs/2004.08206v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.08206v2)
- **Published**: 2020-04-17 12:29:40+00:00
- **Updated**: 2020-05-13 11:42:30+00:00
- **Authors**: Friedrich Kruber, Eduardo Sánchez Morales, Samarjit Chakraborty, Michael Botsch
- **Comment**: Copyright 20xx IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works
- **Journal**: 2020 IEEE Intelligent Vehicles Symposium (IV)
- **Summary**: The availability of real-world data is a key element for novel developments in the fields of automotive and traffic research. Aerial imagery has the major advantage of recording multiple objects simultaneously and overcomes limitations such as occlusions. However, there are only few data sets available. This work describes a process to estimate a precise vehicle position from aerial imagery. A robust object detection is crucial for reliable results, hence the state-of-the-art deep neural network Mask-RCNN is applied for that purpose. Two training data sets are employed: The first one is optimized for detecting the test vehicle, while the second one consists of randomly selected images recorded on public roads. To reduce errors, several aspects are accounted for, such as the drone movement and the perspective projection from a photograph. The estimated position is comapared with a reference system installed in the test vehicle. It is shown, that a mean accuracy of 20 cm can be achieved with flight altitudes up to 100 m, Full-HD resolution and a frame-by-frame detection. A reliable position estimation is the basis for further data processing, such as obtaining additional vehicle state variables. The source code, training weights, labeled data and example videos are made publicly available. This supports researchers to create new traffic data sets with specific local conditions.



### Learning to Predict Context-adaptive Convolution for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2004.08222v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.08222v2)
- **Published**: 2020-04-17 13:09:17+00:00
- **Updated**: 2020-08-26 03:49:13+00:00
- **Authors**: Jianbo Liu, Junjun He, Jimmy S. Ren, Yu Qiao, Hongsheng Li
- **Comment**: Accepted in ECCV 2020
- **Journal**: None
- **Summary**: Long-range contextual information is essential for achieving high-performance semantic segmentation. Previous feature re-weighting methods demonstrate that using global context for re-weighting feature channels can effectively improve the accuracy of semantic segmentation. However, the globally-sharing feature re-weighting vector might not be optimal for regions of different classes in the input image. In this paper, we propose a Context-adaptive Convolution Network (CaC-Net) to predict a spatially-varying feature weighting vector for each spatial location of the semantic feature maps. In CaC-Net, a set of context-adaptive convolution kernels are predicted from the global contextual information in a parameter-efficient manner. When used for convolution with the semantic feature maps, the predicted convolutional kernels can generate the spatially-varying feature weighting factors capturing both global and local contextual information. Comprehensive experimental results show that our CaC-Net achieves superior segmentation performance on three public datasets, PASCAL Context, PASCAL VOC 2012 and ADE20K.



### Weakly Supervised Geodesic Segmentation of Egyptian Mummy CT Scans
- **Arxiv ID**: http://arxiv.org/abs/2004.08270v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.08270v1)
- **Published**: 2020-04-17 14:35:00+00:00
- **Updated**: 2020-04-17 14:35:00+00:00
- **Authors**: Avik Hati, Matteo Bustreo, Diego Sona, Vittorio Murino, Alessio Del Bue
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we tackle the task of automatically analyzing 3D volumetric scans obtained from computed tomography (CT) devices. In particular, we address a particular task for which data is very limited: the segmentation of ancient Egyptian mummies CT scans. We aim at digitally unwrapping the mummy and identify different segments such as body, bandages and jewelry. The problem is complex because of the lack of annotated data for the different semantic regions to segment, thus discouraging the use of strongly supervised approaches. We, therefore, propose a weakly supervised and efficient interactive segmentation method to solve this challenging problem. After segmenting the wrapped mummy from its exterior region using histogram analysis and template matching, we first design a voxel distance measure to find an approximate solution for the body and bandage segments. Here, we use geodesic distances since voxel features as well as spatial relationship among voxels is incorporated in this measure. Next, we refine the solution using a GrabCut based segmentation together with a tracking method on the slices of the scan that assigns labels to different regions in the volume, using limited supervision in the form of scribbles drawn by the user. The efficiency of the proposed method is demonstrated using visualizations and validated through quantitative measures and qualitative unwrapping of the mummy.



### IDDA: a large-scale multi-domain dataset for autonomous driving
- **Arxiv ID**: http://arxiv.org/abs/2004.08298v2
- **DOI**: 10.1109/LRA.2020.3009075
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2004.08298v2)
- **Published**: 2020-04-17 15:22:38+00:00
- **Updated**: 2021-10-22 08:59:37+00:00
- **Authors**: Emanuele Alberti, Antonio Tavera, Carlo Masone, Barbara Caputo
- **Comment**: Accepted at IROS 2020 and RA-L. Download at:
  https://idda-dataset.github.io/home/
- **Journal**: IEEE Robotics and Automation Letters, vol. 5, no. 4, pp.
  5526-5533, Oct. 2020
- **Summary**: Semantic segmentation is key in autonomous driving. Using deep visual learning architectures is not trivial in this context, because of the challenges in creating suitable large scale annotated datasets. This issue has been traditionally circumvented through the use of synthetic datasets, that have become a popular resource in this field. They have been released with the need to develop semantic segmentation algorithms able to close the visual domain shift between the training and test data. Although exacerbated by the use of artificial data, the problem is extremely relevant in this field even when training on real data. Indeed, weather conditions, viewpoint changes and variations in the city appearances can vary considerably from car to car, and even at test time for a single, specific vehicle. How to deal with domain adaptation in semantic segmentation, and how to leverage effectively several different data distributions (source domains) are important research questions in this field. To support work in this direction, this paper contributes a new large scale, synthetic dataset for semantic segmentation with more than 100 different source visual domains. The dataset has been created to explicitly address the challenges of domain shift between training and test data in various weather and view point conditions, in seven different city types. Extensive benchmark experiments assess the dataset, showcasing open challenges for the current state of the art. The dataset will be available at: https://idda-dataset.github.io/home/ .



### Data-driven Flood Emulation: Speeding up Urban Flood Predictions by Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2004.08340v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.08340v2)
- **Published**: 2020-04-17 16:44:46+00:00
- **Updated**: 2020-05-13 10:19:29+00:00
- **Authors**: Zifeng Guo, Joao P. Leitao, Nuno E. Simoes, Vahid Moosavi
- **Comment**: None
- **Journal**: None
- **Summary**: Computational complexity has been the bottleneck of applying physically-based simulations on large urban areas with high spatial resolution for efficient and systematic flooding analyses and risk assessments. To address this issue of long computational time, this paper proposes that the prediction of maximum water depth rasters can be considered as an image-to-image translation problem where the results are generated from input elevation rasters using the information learned from data rather than by conducting simulations, which can significantly accelerate the prediction process. The proposed approach was implemented by a deep convolutional neural network trained on flood simulation data of 18 designed hyetographs on three selected catchments. Multiple tests with both designed and real rainfall events were performed and the results show that the flood predictions by neural network uses only 0.5 % of time comparing with physically-based approaches, with promising accuracy and ability of generalizations. The proposed neural network can also potentially be applied to different but relevant problems including flood predictions for urban layout planning.



### Complexity Analysis of an Edge Preserving CNN SAR Despeckling Algorithm
- **Arxiv ID**: http://arxiv.org/abs/2004.08345v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.08345v2)
- **Published**: 2020-04-17 17:02:01+00:00
- **Updated**: 2020-06-17 17:58:48+00:00
- **Authors**: Sergio Vitale, Giampaolo Ferraioli, Vito Pascazio
- **Comment**: Accpeted to International Geoscience and Remote Sensing Symposium,
  IGARSS 2020
- **Journal**: None
- **Summary**: SAR images are affected by multiplicative noise that impairs their interpretations. In the last decades several methods for SAR denoising have been proposed and in the last years great attention has moved towards deep learning based solutions. Based on our last proposed convolutional neural network for SAR despeckling, here we exploit the effect of the complexity of the network. More precisely, once a dataset has been fixed, we carry out an analysis of the network performance with respect to the number of layers and numbers of features the network is composed of. Evaluation on simulated and real data are carried out. The results show that deeper networks better generalize on both simulated and real images.



### An integrated light management system with real-time light measurement and human perception
- **Arxiv ID**: http://arxiv.org/abs/2004.08346v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.08346v1)
- **Published**: 2020-04-17 17:03:19+00:00
- **Updated**: 2020-04-17 17:03:19+00:00
- **Authors**: Theodore Tsesmelis, Irtiza Hasan, Marco Cristani, Alessio Del Bue, Fabio Galasso
- **Comment**: None
- **Journal**: None
- **Summary**: Illumination is important for well-being, productivity and safety across several environments, including offices, retail shops and industrial warehouses. Current techniques for setting up lighting require extensive and expert support and need to be repeated if the scene changes. Here we propose the first fully-automated light management system (LMS) which measures lighting in real-time, leveraging an RGBD sensor and a radiosity-based light propagation model. Thanks to the integration of light distribution and perception curves into the radiosity, we outperform a commercial software (Relux) on a newly introduced dataset. Furthermore, our proposed LMS is the first to estimate both the presence and the attention of the people in the environment, as well as their light perception. Our new LMS adapts therefore lighting to the scene and human activity and it is capable of saving up to 66%, as we experimentally quantify,without compromising the lighting quality.



### Motion and Region Aware Adversarial Learning for Fall Detection with Thermal Imaging
- **Arxiv ID**: http://arxiv.org/abs/2004.08352v2
- **DOI**: 10.1109/ICPR48806.2021.9412632
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.08352v2)
- **Published**: 2020-04-17 17:17:29+00:00
- **Updated**: 2020-10-24 22:06:49+00:00
- **Authors**: Vineet Mehta, Abhinav Dhall, Sujata Pal, Shehroz S. Khan
- **Comment**: 8 pages,7 figures
- **Journal**: None
- **Summary**: Automatic fall detection is a vital technology for ensuring the health and safety of people. Home-based camera systems for fall detection often put people's privacy at risk. Thermal cameras can partially or fully obfuscate facial features, thus preserving the privacy of a person. Another challenge is the less occurrence of falls in comparison to the normal activities of daily living. As fall occurs rarely, it is non-trivial to learn algorithms due to class imbalance. To handle these problems, we formulate fall detection as an anomaly detection within an adversarial framework using thermal imaging. We present a novel adversarial network that comprises of two-channel 3D convolutional autoencoders which reconstructs the thermal data and the optical flow input sequences respectively. We introduce a technique to track the region of interest, a region-based difference constraint, and a joint discriminator to compute the reconstruction error. A larger reconstruction error indicates the occurrence of a fall. The experiments on a publicly available thermal fall dataset show the superior results obtained compared to the standard baseline.



### Robotic Room Traversal using Optical Range Finding
- **Arxiv ID**: http://arxiv.org/abs/2004.08368v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.08368v1)
- **Published**: 2020-04-17 17:48:40+00:00
- **Updated**: 2020-04-17 17:48:40+00:00
- **Authors**: Cole Smith, Eric Lin, Dennis Shasha
- **Comment**: Technical Report TR2018-991
- **Journal**: None
- **Summary**: Consider the goal of visiting every part of a room that is not blocked by obstacles. Doing so efficiently requires both sensors and planning. Our findings suggest a method of inexpensive optical range finding for robotic room traversal. Our room traversal algorithm relies upon the approximate distance from the robot to the nearest obstacle in 360 degrees. We then choose the path with the furthest approximate distance. Since millimeter-precision is not required for our problem, we have opted to develop our own laser range finding solution, in lieu of using more common, but also expensive solutions like light detection and ranging (LIDAR). Rather, our solution uses a laser that casts a visible dot on the target and a common camera (an iPhone, for example). Based upon where in the camera frame the laser dot is detected, we may calculate an angle between our target and the laser aperture. Using this angle and the known distance between the camera eye and the laser aperture, we may solve all sides of a trigonometric model which provides the distance between the robot and the target.



### Fitting the Search Space of Weight-sharing NAS with Graph Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/2004.08423v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.08423v2)
- **Published**: 2020-04-17 19:12:39+00:00
- **Updated**: 2020-12-15 09:47:03+00:00
- **Authors**: Xin Chen, Lingxi Xie, Jun Wu, Longhui Wei, Yuhui Xu, Qi Tian
- **Comment**: Accepted to AAAI 2021
- **Journal**: None
- **Summary**: Neural architecture search has attracted wide attentions in both academia and industry. To accelerate it, researchers proposed weight-sharing methods which first train a super-network to reuse computation among different operators, from which exponentially many sub-networks can be sampled and efficiently evaluated. These methods enjoy great advantages in terms of computational costs, but the sampled sub-networks are not guaranteed to be estimated precisely unless an individual training process is taken. This paper owes such inaccuracy to the inevitable mismatch between assembled network layers, so that there is a random error term added to each estimation. We alleviate this issue by training a graph convolutional network to fit the performance of sampled sub-networks so that the impact of random errors becomes minimal. With this strategy, we achieve a higher rank correlation coefficient in the selected set of candidates, which consequently leads to better performance of the final architecture. In addition, our approach also enjoys the flexibility of being used under different hardware constraints, since the graph convolutional network has provided an efficient lookup table of the performance of architectures in the entire search space.



### Organ at Risk Segmentation for Head and Neck Cancer using Stratified Learning and Neural Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/2004.08426v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.08426v1)
- **Published**: 2020-04-17 19:15:48+00:00
- **Updated**: 2020-04-17 19:15:48+00:00
- **Authors**: Dazhou Guo, Dakai Jin, Zhuotun Zhu, Tsung-Ying Ho, Adam P. Harrison, Chun-Hung Chao, Jing Xiao, Alan Yuille, Chien-Yu Lin, Le Lu
- **Comment**: None
- **Journal**: IEEE CVPR 2020
- **Summary**: OAR segmentation is a critical step in radiotherapy of head and neck (H&N) cancer, where inconsistencies across radiation oncologists and prohibitive labor costs motivate automated approaches. However, leading methods using standard fully convolutional network workflows that are challenged when the number of OARs becomes large, e.g. > 40. For such scenarios, insights can be gained from the stratification approaches seen in manual clinical OAR delineation. This is the goal of our work, where we introduce stratified organ at risk segmentation (SOARS), an approach that stratifies OARs into anchor, mid-level, and small & hard (S&H) categories. SOARS stratifies across two dimensions. The first dimension is that distinct processing pipelines are used for each OAR category. In particular, inspired by clinical practices, anchor OARs are used to guide the mid-level and S&H categories. The second dimension is that distinct network architectures are used to manage the significant contrast, size, and anatomy variations between different OARs. We use differentiable neural architecture search (NAS), allowing the network to choose among 2D, 3D or Pseudo-3D convolutions. Extensive 4-fold cross-validation on 142 H&N cancer patients with 42 manually labeled OARs, the most comprehensive OAR dataset to date, demonstrates that both pipeline- and NAS-stratification significantly improves quantitative performance over the state-of-the-art (from 69.52% to 73.68% in absolute Dice scores). Thus, SOARS provides a powerful and principled means to manage the highly complex segmentation space of OARs.



### Adversarial Attack on Deep Learning-Based Splice Localization
- **Arxiv ID**: http://arxiv.org/abs/2004.08443v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.08443v1)
- **Published**: 2020-04-17 20:31:38+00:00
- **Updated**: 2020-04-17 20:31:38+00:00
- **Authors**: Andras Rozsa, Zheng Zhong, Terrance E. Boult
- **Comment**: This is a pre-print of the original paper accepted at the CVPR
  Workshop on Media Forensics 2020
- **Journal**: None
- **Summary**: Regarding image forensics, researchers have proposed various approaches to detect and/or localize manipulations, such as splices. Recent best performing image-forensics algorithms greatly benefit from the application of deep learning, but such tools can be vulnerable to adversarial attacks. Due to the fact that most of the proposed adversarial example generation techniques can be used only on end-to-end classifiers, the adversarial robustness of image-forensics methods that utilize deep learning only for feature extraction has not been studied yet. Using a novel algorithm capable of directly adjusting the underlying representations of patches we demonstrate on three non end-to-end deep learning-based splice localization tools that hiding manipulations of images is feasible via adversarial attacks. While the tested image-forensics methods, EXIF-SC, SpliceRadar, and Noiseprint, rely on feature extractors that were trained on different surrogate tasks, we find that the formed adversarial perturbations can be transferable among them regarding the deterioration of their localization performance.



