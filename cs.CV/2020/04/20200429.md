# Arxiv Papers in cs.CV on 2020-04-29
### An Auto-Encoder Strategy for Adaptive Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2004.13903v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.13903v1)
- **Published**: 2020-04-29 00:53:24+00:00
- **Updated**: 2020-04-29 00:53:24+00:00
- **Authors**: Evan M. Yu, Juan Eugenio Iglesias, Adrian V. Dalca, Mert R. Sabuncu
- **Comment**: MIDL 2020
- **Journal**: None
- **Summary**: Deep neural networks are powerful tools for biomedical image segmentation. These models are often trained with heavy supervision, relying on pairs of images and corresponding voxel-level labels. However, obtaining segmentations of anatomical regions on a large number of cases can be prohibitively expensive. Thus there is a strong need for deep learning-based segmentation tools that do not require heavy supervision and can continuously adapt. In this paper, we propose a novel perspective of segmentation as a discrete representation learning problem, and present a variational autoencoder segmentation strategy that is flexible and adaptive. Our method, called Segmentation Auto-Encoder (SAE), leverages all available unlabeled scans and merely requires a segmentation prior, which can be a single unpaired segmentation image. In experiments, we apply SAE to brain MRI scans. Our results show that SAE can produce good quality segmentations, particularly when the prior is good. We demonstrate that a Markov Random Field prior can yield significantly better results than a spatially independent prior. Our code is freely available at https://github.com/evanmy/sae.



### Span-based Localizing Network for Natural Language Video Localization
- **Arxiv ID**: http://arxiv.org/abs/2004.13931v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.13931v2)
- **Published**: 2020-04-29 02:47:04+00:00
- **Updated**: 2020-06-14 08:49:07+00:00
- **Authors**: Hao Zhang, Aixin Sun, Wei Jing, Joey Tianyi Zhou
- **Comment**: To appear at ACL 2020
- **Journal**: None
- **Summary**: Given an untrimmed video and a text query, natural language video localization (NLVL) is to locate a matching span from the video that semantically corresponds to the query. Existing solutions formulate NLVL either as a ranking task and apply multimodal matching architecture, or as a regression task to directly regress the target video span. In this work, we address NLVL task with a span-based QA approach by treating the input video as text passage. We propose a video span localizing network (VSLNet), on top of the standard span-based QA framework, to address NLVL. The proposed VSLNet tackles the differences between NLVL and span-based QA through a simple yet effective query-guided highlighting (QGH) strategy. The QGH guides VSLNet to search for matching video span within a highlighted region. Through extensive experiments on three benchmark datasets, we show that the proposed VSLNet outperforms the state-of-the-art methods; and adopting span-based QA framework is a promising direction to solve NLVL.



### Stereotype-Free Classification of Fictitious Faces
- **Arxiv ID**: http://arxiv.org/abs/2005.02157v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2005.02157v1)
- **Published**: 2020-04-29 04:37:54+00:00
- **Updated**: 2020-04-29 04:37:54+00:00
- **Authors**: Mohammadhossein Toutiaee, Soheyla Amirian, John A. Miller, Sheng Li
- **Comment**: None
- **Journal**: None
- **Summary**: Equal Opportunity and Fairness are receiving increasing attention in artificial intelligence. Stereotyping is another source of discrimination, which yet has been unstudied in literature. GAN-made faces would be exposed to such discrimination, if they are classified by human perception. It is possible to eliminate the human impact on fictitious faces classification task by the use of statistical approaches. We present a novel approach through penalized regression to label stereotype-free GAN-generated synthetic unlabeled images. The proposed approach aids labeling new data (fictitious output images) by minimizing a penalized version of the least squares cost function between realistic pictures and target pictures.



### Video Contents Understanding using Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2004.13959v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.13959v1)
- **Published**: 2020-04-29 05:18:40+00:00
- **Updated**: 2020-04-29 05:18:40+00:00
- **Authors**: Mohammadhossein Toutiaee, Abbas Keshavarzi, Abolfazl Farahani, John A. Miller
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel application of Transfer Learning to classify video-frame sequences over multiple classes. This is a pre-weighted model that does not require to train a fresh CNN. This representation is achieved with the advent of "deep neural network" (DNN), which is being studied these days by many researchers. We utilize the classical approaches for video classification task using object detection techniques for comparison, such as "Google Video Intelligence API" and this study will run experiments as to how those architectures would perform in foggy or rainy weather conditions. Experimental evaluation on video collections shows that the new proposed classifier achieves superior performance over existing solutions.



### Deep Transfer Learning For Plant Center Localization
- **Arxiv ID**: http://arxiv.org/abs/2004.13973v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.13973v1)
- **Published**: 2020-04-29 06:29:49+00:00
- **Updated**: 2020-04-29 06:29:49+00:00
- **Authors**: Enyu Cai, Sriram Baireddy, Changye Yang, Melba Crawford, Edward J. Delp
- **Comment**: None
- **Journal**: None
- **Summary**: Plant phenotyping focuses on the measurement of plant characteristics throughout the growing season, typically with the goal of evaluating genotypes for plant breeding. Estimating plant location is important for identifying genotypes which have low emergence, which is also related to the environment and management practices such as fertilizer applications. The goal of this paper is to investigate methods that estimate plant locations for a field-based crop using RGB aerial images captured using Unmanned Aerial Vehicles (UAVs). Deep learning approaches provide promising capability for locating plants observed in RGB images, but they require large quantities of labeled data (ground truth) for training. Using a deep learning architecture fine-tuned on a single field or a single type of crop on fields in other geographic areas or with other crops may not have good results. The problem of generating ground truth for each new field is labor-intensive and tedious. In this paper, we propose a method for estimating plant centers by transferring an existing model to a new scenario using limited ground truth data. We describe the use of transfer learning using a model fine-tuned for a single field or a single type of plant on a varied set of similar crops and fields. We show that transfer learning provides promising results for detecting plant locations.



### MatriVasha: A Multipurpose Comprehensive Database for Bangla Handwritten Compound Characters
- **Arxiv ID**: http://arxiv.org/abs/2005.02155v2
- **DOI**: 10.1007/978-981-15-9774-9_74
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.02155v2)
- **Published**: 2020-04-29 06:38:12+00:00
- **Updated**: 2020-05-06 07:59:45+00:00
- **Authors**: Jannatul Ferdous, Suvrajit Karmaker, A K M Shahariar Azad Rabby, Syed Akhter Hossain
- **Comment**: 19 fig, 2 table
- **Journal**: None
- **Summary**: At present, recognition of the Bangla handwriting compound character has been an essential issue for many years. In recent years there have been application-based researches in machine learning, and deep learning, which is gained interest, and most notably is handwriting recognition because it has a tremendous application such as Bangla OCR. MatrriVasha, the project which can recognize Bangla, handwritten several compound characters. Currently, compound character recognition is an important topic due to its variant application, and helps to create old forms, and information digitization with reliability. But unfortunately, there is a lack of a comprehensive dataset that can categorize all types of Bangla compound characters. MatrriVasha is an attempt to align compound character, and it's challenging because each person has a unique style of writing shapes. After all, MatrriVasha has proposed a dataset that intends to recognize Bangla 120(one hundred twenty) compound characters that consist of 2552(two thousand five hundred fifty-two) isolated handwritten characters written unique writers which were collected from within Bangladesh. This dataset faced problems in terms of the district, age, and gender-based written related research because the samples were collected that includes a verity of the district, age group, and the equal number of males, and females. As of now, our proposed dataset is so far the most extensive dataset for Bangla compound characters. It is intended to frame the acknowledgment technique for handwritten Bangla compound character. In the future, this dataset will be made publicly available to help to widen the research.



### Effective Human Activity Recognition Based on Small Datasets
- **Arxiv ID**: http://arxiv.org/abs/2004.13977v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.13977v1)
- **Published**: 2020-04-29 06:38:23+00:00
- **Updated**: 2020-04-29 06:38:23+00:00
- **Authors**: Bruce X. B. Yu, Yan Liu, Keith C. C. Chan
- **Comment**: 7 pages
- **Journal**: None
- **Summary**: Most recent work on vision-based human activity recognition (HAR) focuses on designing complex deep learning models for the task. In so doing, there is a requirement for large datasets to be collected. As acquiring and processing large training datasets are usually very expensive, the problem of how dataset size can be reduced without affecting recognition accuracy has to be tackled. To do so, we propose a HAR method that consists of three steps: (i) data transformation involving the generation of new features based on transforming of raw data, (ii) feature extraction involving the learning of a classifier based on the AdaBoost algorithm and the use of training data consisting of the transformed features, and (iii) parameter determination and pattern recognition involving the determination of parameters based on the features generated in (ii) and the use of the parameters as training data for deep learning algorithms to be used to recognize human activities. Compared to existing approaches, this proposed approach has the advantageous characteristics that it is simple and robust. The proposed approach has been tested with a number of experiments performed on a relatively small real dataset. The experimental results indicate that using the proposed method, human activities can be more accurately recognized even with smaller training data size.



### Skeleton Focused Human Activity Recognition in RGB Video
- **Arxiv ID**: http://arxiv.org/abs/2004.13979v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.13979v1)
- **Published**: 2020-04-29 06:40:42+00:00
- **Updated**: 2020-04-29 06:40:42+00:00
- **Authors**: Bruce X. B. Yu, Yan Liu, Keith C. C. Chan
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: The data-driven approach that learns an optimal representation of vision features like skeleton frames or RGB videos is currently a dominant paradigm for activity recognition. While great improvements have been achieved from existing single modal approaches with increasingly larger datasets, the fusion of various data modalities at the feature level has seldom been attempted. In this paper, we propose a multimodal feature fusion model that utilizes both skeleton and RGB modalities to infer human activity. The objective is to improve the activity recognition accuracy by effectively utilizing the mutual complemental information among different data modalities. For the skeleton modality, we propose to use a graph convolutional subnetwork to learn the skeleton representation. Whereas for the RGB modality, we will use the spatial-temporal region of interest from RGB videos and take the attention features from the skeleton modality to guide the learning process. The model could be either individually or uniformly trained by the back-propagation algorithm in an end-to-end manner. The experimental results for the NTU-RGB+D and Northwestern-UCLA Multiview datasets achieved state-of-the-art performance, which indicates that the proposed skeleton-driven attention mechanism for the RGB modality increases the mutual communication between different data modalities and brings more discriminative features for inferring human activities.



### Motion Guided 3D Pose Estimation from Videos
- **Arxiv ID**: http://arxiv.org/abs/2004.13985v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.13985v1)
- **Published**: 2020-04-29 06:59:30+00:00
- **Updated**: 2020-04-29 06:59:30+00:00
- **Authors**: Jingbo Wang, Sijie Yan, Yuanjun Xiong, Dahua Lin
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a new loss function, called motion loss, for the problem of monocular 3D Human pose estimation from 2D pose. In computing motion loss, a simple yet effective representation for keypoint motion, called pairwise motion encoding, is introduced. We design a new graph convolutional network architecture, U-shaped GCN (UGCN). It captures both short-term and long-term motion information to fully leverage the additional supervision from the motion loss. We experiment training UGCN with the motion loss on two large scale benchmarks: Human3.6M and MPI-INF-3DHP. Our model surpasses other state-of-the-art models by a large margin. It also demonstrates strong capacity in producing smooth 3D sequences and recovering keypoint motion.



### Retinal vessel segmentation by probing adaptive to lighting variations
- **Arxiv ID**: http://arxiv.org/abs/2004.13992v1
- **DOI**: 10.1109/ISBI45749.2020.9098332
- **Categories**: **cs.CV**, cs.NA, eess.SP, math.NA, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2004.13992v1)
- **Published**: 2020-04-29 07:10:17+00:00
- **Updated**: 2020-04-29 07:10:17+00:00
- **Authors**: Guillaume Noyel, Christine Vartin, Peter Boyle, Laurent Kodjikian
- **Comment**: Proceedings of 2020 IEEE 17th International Symposium on Biomedical
  Imaging (ISBI).To appear in https://ieeexplore.ieee.org
- **Journal**: 2020 IEEE 17th International Symposium on Biomedical Imaging
  (ISBI), IEEE, Apr 2020, Iowa City, United States. pp.1246-1249
- **Summary**: We introduce a novel method to extract the vessels in eye fun-dus images which is adaptive to lighting variations. In the Logarithmic Image Processing framework, a 3-segment probe detects the vessels by probing the topographic surface of an image from below. A map of contrasts between the probe and the image allows to detect the vessels by a threshold. In a lowly contrasted image, results show that our method better extract the vessels than another state-of the-art method. In a highly contrasted image database (DRIVE) with a reference , ours has an accuracy of 0.9454 which is similar or better than three state-of-the-art methods and below three others. The three best methods have a higher accuracy than a manual segmentation by another expert. Importantly, our method automatically adapts to the lighting conditions of the image acquisition.



### The International Workshop on Osteoarthritis Imaging Knee MRI Segmentation Challenge: A Multi-Institute Evaluation and Analysis Framework on a Standardized Dataset
- **Arxiv ID**: http://arxiv.org/abs/2004.14003v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.14003v2)
- **Published**: 2020-04-29 07:54:26+00:00
- **Updated**: 2020-05-26 19:24:14+00:00
- **Authors**: Arjun D. Desai, Francesco Caliva, Claudia Iriondo, Naji Khosravan, Aliasghar Mortazi, Sachin Jambawalikar, Drew Torigian, Jutta Ellermann, Mehmet Akcakaya, Ulas Bagci, Radhika Tibrewala, Io Flament, Matthew O`Brien, Sharmila Majumdar, Mathias Perslev, Akshay Pai, Christian Igel, Erik B. Dam, Sibaji Gaj, Mingrui Yang, Kunio Nakamura, Xiaojuan Li, Cem M. Deniz, Vladimir Juras, Ravinder Regatte, Garry E. Gold, Brian A. Hargreaves, Valentina Pedoia, Akshay S. Chaudhari
- **Comment**: Submitted to Radiology: Artificial Intelligence; Fixed typos
- **Journal**: None
- **Summary**: Purpose: To organize a knee MRI segmentation challenge for characterizing the semantic and clinical efficacy of automatic segmentation methods relevant for monitoring osteoarthritis progression.   Methods: A dataset partition consisting of 3D knee MRI from 88 subjects at two timepoints with ground-truth articular (femoral, tibial, patellar) cartilage and meniscus segmentations was standardized. Challenge submissions and a majority-vote ensemble were evaluated using Dice score, average symmetric surface distance, volumetric overlap error, and coefficient of variation on a hold-out test set. Similarities in network segmentations were evaluated using pairwise Dice correlations. Articular cartilage thickness was computed per-scan and longitudinally. Correlation between thickness error and segmentation metrics was measured using Pearson's coefficient. Two empirical upper bounds for ensemble performance were computed using combinations of model outputs that consolidated true positives and true negatives.   Results: Six teams (T1-T6) submitted entries for the challenge. No significant differences were observed across all segmentation metrics for all tissues (p=1.0) among the four top-performing networks (T2, T3, T4, T6). Dice correlations between network pairs were high (>0.85). Per-scan thickness errors were negligible among T1-T4 (p=0.99) and longitudinal changes showed minimal bias (<0.03mm). Low correlations (<0.41) were observed between segmentation metrics and thickness error. The majority-vote ensemble was comparable to top performing networks (p=1.0). Empirical upper bound performances were similar for both combinations (p=1.0).   Conclusion: Diverse networks learned to segment the knee similarly where high segmentation accuracy did not correlate to cartilage thickness accuracy. Voting ensembles did not outperform individual networks but may help regularize individual models.



### Counting of Grapevine Berries in Images via Semantic Segmentation using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2004.14010v1
- **DOI**: 10.1016/j.isprsjprs.2020.04.002
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.14010v1)
- **Published**: 2020-04-29 08:10:19+00:00
- **Updated**: 2020-04-29 08:10:19+00:00
- **Authors**: Laura Zabawa, Anna Kicherer, Lasse Klingbeil, Reinhard Töpfer, Heiner Kuhlmann, Ribana Roscher
- **Comment**: None
- **Journal**: Journal of Photogrammetry and Remote Sensing, vol. 164,
  pp.73-83,2020
- **Summary**: The extraction of phenotypic traits is often very time and labour intensive. Especially the investigation in viticulture is restricted to an on-site analysis due to the perennial nature of grapevine. Traditionally skilled experts examine small samples and extrapolate the results to a whole plot. Thereby different grapevine varieties and training systems, e.g. vertical shoot positioning (VSP) and semi minimal pruned hedges (SMPH) pose different challenges. In this paper we present an objective framework based on automatic image analysis which works on two different training systems. The images are collected semi automatic by a camera system which is installed in a modified grape harvester. The system produces overlapping images from the sides of the plants. Our framework uses a convolutional neural network to detect single berries in images by performing a semantic segmentation. Each berry is then counted with a connected component algorithm. We compare our results with the Mask-RCNN, a state-of-the-art network for instance segmentation and with a regression approach for counting. The experiments presented in this paper show that we are able to detect green berries in images despite of different training systems. We achieve an accuracy for the berry detection of 94.0% in the VSP and 85.6% in the SMPH.



### Improving Target-driven Visual Navigation with Attention on 3D Spatial Relationships
- **Arxiv ID**: http://arxiv.org/abs/2005.02153v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2005.02153v1)
- **Published**: 2020-04-29 08:46:38+00:00
- **Updated**: 2020-04-29 08:46:38+00:00
- **Authors**: Yunlian Lv, Ning Xie, Yimin Shi, Zijiao Wang, Heng Tao Shen
- **Comment**: 12 pages, 9 figures
- **Journal**: None
- **Summary**: Embodied artificial intelligence (AI) tasks shift from tasks focusing on internet images to active settings involving embodied agents that perceive and act within 3D environments. In this paper, we investigate the target-driven visual navigation using deep reinforcement learning (DRL) in 3D indoor scenes, whose navigation task aims to train an agent that can intelligently make a series of decisions to arrive at a pre-specified target location from any possible starting positions only based on egocentric views. However, most navigation methods currently struggle against several challenging problems, such as data efficiency, automatic obstacle avoidance, and generalization. Generalization problem means that agent does not have the ability to transfer navigation skills learned from previous experience to unseen targets and scenes. To address these issues, we incorporate two designs into classic DRL framework: attention on 3D knowledge graph (KG) and target skill extension (TSE) module. On the one hand, our proposed method combines visual features and 3D spatial representations to learn navigation policy. On the other hand, TSE module is used to generate sub-targets which allow agent to learn from failures. Specifically, our 3D spatial relationships are encoded through recently popular graph convolutional network (GCN). Considering the real world settings, our work also considers open action and adds actionable targets into conventional navigation situations. Those more difficult settings are applied to test whether DRL agent really understand its task, navigating environment, and can carry out reasoning. Our experiments, performed in the AI2-THOR, show that our model outperforms the baselines in both SR and SPL metrics, and improves generalization ability across targets and scenes.



### Multiresolution and Multimodal Speech Recognition with Transformers
- **Arxiv ID**: http://arxiv.org/abs/2004.14840v1
- **DOI**: None
- **Categories**: **eess.AS**, cs.CV, cs.LG, cs.SD, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.14840v1)
- **Published**: 2020-04-29 09:32:11+00:00
- **Updated**: 2020-04-29 09:32:11+00:00
- **Authors**: Georgios Paraskevopoulos, Srinivas Parthasarathy, Aparna Khare, Shiva Sundaram
- **Comment**: Accepted for ACL 2020
- **Journal**: None
- **Summary**: This paper presents an audio visual automatic speech recognition (AV-ASR) system using a Transformer-based architecture. We particularly focus on the scene context provided by the visual information, to ground the ASR. We extract representations for audio features in the encoder layers of the transformer and fuse video features using an additional crossmodal multihead attention layer. Additionally, we incorporate a multitask training criterion for multiresolution ASR, where we train the model to generate both character and subword level transcriptions.   Experimental results on the How2 dataset, indicate that multiresolution training can speed up convergence by around 50% and relatively improves word error rate (WER) performance by upto 18% over subword prediction models. Further, incorporating visual information improves performance with relative gains upto 3.76% over audio only models.   Our results are comparable to state-of-the-art Listen, Attend and Spell-based architectures.



### Single-Side Domain Generalization for Face Anti-Spoofing
- **Arxiv ID**: http://arxiv.org/abs/2004.14043v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.14043v1)
- **Published**: 2020-04-29 09:32:54+00:00
- **Updated**: 2020-04-29 09:32:54+00:00
- **Authors**: Yunpei Jia, Jie Zhang, Shiguang Shan, Xilin Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Existing domain generalization methods for face anti-spoofing endeavor to extract common differentiation features to improve the generalization. However, due to large distribution discrepancies among fake faces of different domains, it is difficult to seek a compact and generalized feature space for the fake faces. In this work, we propose an end-to-end single-side domain generalization framework (SSDG) to improve the generalization ability of face anti-spoofing. The main idea is to learn a generalized feature space, where the feature distribution of the real faces is compact while that of the fake ones is dispersed among domains but compact within each domain. Specifically, a feature generator is trained to make only the real faces from different domains undistinguishable, but not for the fake ones, thus forming a single-side adversarial learning. Moreover, an asymmetric triplet loss is designed to constrain the fake faces of different domains separated while the real ones aggregated. The above two points are integrated into a unified framework in an end-to-end training manner, resulting in a more generalized class boundary, especially good for samples from novel domains. Feature and weight normalization is incorporated to further improve the generalization ability. Extensive experiments show that our proposed approach is effective and outperforms the state-of-the-art methods on four public databases.



### Minimal Rolling Shutter Absolute Pose with Unknown Focal Length and Radial Distortion
- **Arxiv ID**: http://arxiv.org/abs/2004.14052v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.14052v1)
- **Published**: 2020-04-29 10:03:03+00:00
- **Updated**: 2020-04-29 10:03:03+00:00
- **Authors**: Zuzana Kukelova, Cenek Albl, Akihiro Sugimoto, Konrad Schindler, Tomas Pajdla
- **Comment**: None
- **Journal**: None
- **Summary**: The internal geometry of most modern consumer cameras is not adequately described by the perspective projection. Almost all cameras exhibit some radial lens distortion and are equipped with an electronic rolling shutter that induces distortions when the camera moves during the image capture. When focal length has not been calibrated offline, the parameters that describe the radial and rolling shutter distortions are usually unknown. While for global shutter cameras, minimal solvers for the absolute camera pose and unknown focal length and radial distortion are available, solvers for the rolling shutter were missing. We present the first minimal solutions for the absolute pose of a rolling shutter camera with unknown rolling shutter parameters, focal length, and radial distortion. Our new minimal solvers combine iterative schemes designed for calibrated rolling shutter cameras with fast generalized eigenvalue and Groebner basis solvers. In a series of experiments, with both synthetic and real data, we show that our new solvers provide accurate estimates of the camera pose, rolling shutter parameters, focal length, and radial distortion parameters.



### Image Morphing with Perceptual Constraints and STN Alignment
- **Arxiv ID**: http://arxiv.org/abs/2004.14071v1
- **DOI**: 10.1111/cgf.14027
- **Categories**: **cs.GR**, cs.CV, cs.LG, I.3.3
- **Links**: [PDF](http://arxiv.org/pdf/2004.14071v1)
- **Published**: 2020-04-29 10:49:10+00:00
- **Updated**: 2020-04-29 10:49:10+00:00
- **Authors**: Noa Fish, Richard Zhang, Lilach Perry, Daniel Cohen-Or, Eli Shechtman, Connelly Barnes
- **Comment**: None
- **Journal**: None
- **Summary**: In image morphing, a sequence of plausible frames are synthesized and composited together to form a smooth transformation between given instances. Intermediates must remain faithful to the input, stand on their own as members of the set, and maintain a well-paced visual transition from one to the next. In this paper, we propose a conditional GAN morphing framework operating on a pair of input images. The network is trained to synthesize frames corresponding to temporal samples along the transformation, and learns a proper shape prior that enhances the plausibility of intermediate frames. While individual frame plausibility is boosted by the adversarial setup, a special training protocol producing sequences of frames, combined with a perceptual similarity loss, promote smooth transformation over time. Explicit stating of correspondences is replaced with a grid-based freeform deformation spatial transformer that predicts the geometric warp between the inputs, instituting the smooth geometric effect by bringing the shapes into an initial alignment. We provide comparisons to classic as well as latent space morphing techniques, and demonstrate that, given a set of images for self-supervision, our network learns to generate visually pleasing morphing effects featuring believable in-betweens, with robustness to changes in shape and texture, requiring no correspondence annotation.



### DR-SPAAM: A Spatial-Attention and Auto-regressive Model for Person Detection in 2D Range Data
- **Arxiv ID**: http://arxiv.org/abs/2004.14079v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.14079v2)
- **Published**: 2020-04-29 11:01:44+00:00
- **Updated**: 2020-07-31 16:43:53+00:00
- **Authors**: Dan Jia, Alexander Hermans, Bastian Leibe
- **Comment**: None
- **Journal**: None
- **Summary**: Detecting persons using a 2D LiDAR is a challenging task due to the low information content of 2D range data. To alleviate the problem caused by the sparsity of the LiDAR points, current state-of-the-art methods fuse multiple previous scans and perform detection using the combined scans. The downside of such a backward looking fusion is that all the scans need to be aligned explicitly, and the necessary alignment operation makes the whole pipeline more expensive -- often too expensive for real-world applications. In this paper, we propose a person detection network which uses an alternative strategy to combine scans obtained at different times. Our method, Distance Robust SPatial Attention and Auto-regressive Model (DR-SPAAM), follows a forward looking paradigm. It keeps the intermediate features from the backbone network as a template and recurrently updates the template when a new scan becomes available. The updated feature template is in turn used for detecting persons currently in the scene. On the DROW dataset, our method outperforms the existing state-of-the-art, while being approximately four times faster, running at 87.2 FPS on a laptop with a dedicated GPU and at 22.6 FPS on an NVIDIA Jetson AGX embedded GPU. We release our code in PyTorch and a ROS node including pre-trained models.



### Informative Scene Decomposition for Crowd Analysis, Comparison and Simulation Guidance
- **Arxiv ID**: http://arxiv.org/abs/2004.14107v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV, cs.LG, cs.MA
- **Links**: [PDF](http://arxiv.org/pdf/2004.14107v1)
- **Published**: 2020-04-29 12:03:32+00:00
- **Updated**: 2020-04-29 12:03:32+00:00
- **Authors**: Feixiang He, Yuanhang Xiang, Xi Zhao, He Wang
- **Comment**: accepted in SIGGRAPH 2020
- **Journal**: None
- **Summary**: Crowd simulation is a central topic in several fields including graphics. To achieve high-fidelity simulations, data has been increasingly relied upon for analysis and simulation guidance. However, the information in real-world data is often noisy, mixed and unstructured, making it difficult for effective analysis, therefore has not been fully utilized. With the fast-growing volume of crowd data, such a bottleneck needs to be addressed. In this paper, we propose a new framework which comprehensively tackles this problem. It centers at an unsupervised method for analysis. The method takes as input raw and noisy data with highly mixed multi-dimensional (space, time and dynamics) information, and automatically structure it by learning the correlations among these dimensions. The dimensions together with their correlations fully describe the scene semantics which consists of recurring activity patterns in a scene, manifested as space flows with temporal and dynamics profiles. The effectiveness and robustness of the analysis have been tested on datasets with great variations in volume, duration, environment and crowd dynamics. Based on the analysis, new methods for data visualization, simulation evaluation and simulation guidance are also proposed. Together, our framework establishes a highly automated pipeline from raw data to crowd analysis, comparison and simulation guidance. Extensive experiments and evaluations have been conducted to show the flexibility, versatility and intuitiveness of our framework.



### Zero-Shot Learning and its Applications from Autonomous Vehicles to COVID-19 Diagnosis: A Review
- **Arxiv ID**: http://arxiv.org/abs/2004.14143v3
- **DOI**: 10.1016/j.ibmed.2020.100005
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.14143v3)
- **Published**: 2020-04-29 12:45:35+00:00
- **Updated**: 2020-11-29 03:27:44+00:00
- **Authors**: Mahdi Rezaei, Mahsa Shahidi
- **Comment**: Accepted in Journal of Intelligence-Based Medicine (Elsevier)
- **Journal**: Journal of Intelligence-Based Medicine, Volumes 4, 2020
- **Summary**: The challenge of learning a new concept, object, or a new medical disease recognition without receiving any examples beforehand is called Zero-Shot Learning (ZSL). One of the major issues in deep learning based methodologies such as in Medical Imaging and other real-world applications is the requirement of large annotated datasets prepared by clinicians or experts to train the model. ZSL is known for having minimal human intervention by relying only on previously known or trained concepts plus currently existing auxiliary information. This makes the ZSL applicable in many real-world scenarios, from unknown object detection in autonomous vehicles to medical imaging and unforeseen diseases such as COVID-19 Chest X-Ray (CXR) based diagnosis. We introduce a novel and broaden solution called Few/one-shot learning, and present the definition of the ZSL problem as an extreme case of the few-shot learning. We review over fundamentals and the challenging steps of Zero-Shot Learning, including state-of-the-art categories of solutions, as well as our recommended solution, motivations behind each approach, their advantages over each category to guide both clinicians and AI researchers to proceed with the best techniques and practices based on their applications. We then review through different datasets inducing medical and non-medical images, the variety of splits, and the evaluation protocols proposed so far. Finally, we discuss the recent applications and future directions of ZSL. We aim to convey a useful intuition through this paper towards the goal of handling complex learning tasks more similar to the way humans learn. We mainly focus on two applications in the current modern yet challenging era: coping with an early and fast diagnosis of COVID-19 cases, and also encouraging the readers to develop other similar AI-based automated detection/recognition systems using ZSL.



### A Fast 3D CNN for Hyperspectral Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2004.14152v1
- **DOI**: 10.1109/LGRS.2020.3043710
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.14152v1)
- **Published**: 2020-04-29 12:57:36+00:00
- **Updated**: 2020-04-29 12:57:36+00:00
- **Authors**: Muhammad Ahmad
- **Comment**: 5 pages, 8 figures, (IEEE GRSL)
- **Journal**: None
- **Summary**: Hyperspectral imaging (HSI) has been extensively utilized for a number of real-world applications. HSI classification (HSIC) is a challenging task due to high inter-class similarity, high intra-class variability, overlapping, and nested regions. A 2D Convolutional Neural Network (CNN) is a viable approach whereby HSIC highly depends on both Spectral-Spatial information, therefore, 3D CNN can be an alternative but highly computational complex due to the volume and spectral dimensions. Furthermore, these models do not extract quality feature maps and may underperform over the regions having similar textures. Therefore, this work proposed a 3D CNN model that utilizes both spatial-spectral feature maps to attain good performance. In order to achieve the said performance, the HSI cube is first divided into small overlapping 3D patches. Later these patches are processed to generate 3D feature maps using a 3D kernel function over multiple contiguous bands that persevere the spectral information as well. Benchmark HSI datasets (Pavia University, Salinas and Indian Pines) are considered to validate the performance of our proposed method. The results are further compared with several state-of-the-art methods.



### Assessing Car Damage using Mask R-CNN
- **Arxiv ID**: http://arxiv.org/abs/2004.14173v3
- **DOI**: 10.35940/ijeat.C5302.029320
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2004.14173v3)
- **Published**: 2020-04-29 13:19:25+00:00
- **Updated**: 2020-05-05 03:29:43+00:00
- **Authors**: Sarath P, Soorya M, Shaik Abdul Rahman A, S Suresh Kumar, K Devaki
- **Comment**: None
- **Journal**: None
- **Summary**: Picture based vehicle protection handling is a significant region with enormous degree for mechanization. In this paper we consider the issue of vehicle harm characterization, where a portion of the classifications can be fine-granular. We investigate profound learning based procedures for this reason. At first, we attempt legitimately preparing a CNN. In any case, because of little arrangement of marked information, it doesn't function admirably. At that point, we investigate the impact of space explicit pre-preparing followed by tweaking. At last, we explore different avenues regarding move learning and outfit learning. Trial results show that move learning works superior to space explicit tweaking. We accomplish precision of 89.5% with blend of move and gathering learning.



### Deepfake Video Forensics based on Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/2004.14178v1
- **DOI**: 10.35940/ijrte.F9747.038620
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.14178v1)
- **Published**: 2020-04-29 13:21:28+00:00
- **Updated**: 2020-04-29 13:21:28+00:00
- **Authors**: Rahul U, Ragul M, Raja Vignesh K, Tejeswinee K
- **Comment**: This submission has been removed by arXiv administrators due to
  copyright infringement
- **Journal**: None
- **Summary**: Deeplearning has been used to solve complex problems in various domains. As it advances, it also creates applications which become a major threat to our privacy, security and even to our Democracy. Such an application which is being developed recently is the "Deepfake". Deepfake models can create fake images and videos that humans cannot differentiate them from the genuine ones. Therefore, the counter application to automatically detect and analyze the digital visual media is necessary in today world. This paper details retraining the image classification models to apprehend the features from each deepfake video frames. After feeding different sets of deepfake clips of video fringes through a pretrained layer of bottleneck in the neural network is made for every video frame, already stated layer contains condense data for all images and exposes artificial manipulations in Deepfake videos. When checking Deepfake videos, this technique received more than 87 per cent accuracy. This technique has been tested on the Face Forensics dataset and obtained good accuracy in detection.



### Image Captioning through Image Transformer
- **Arxiv ID**: http://arxiv.org/abs/2004.14231v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.14231v2)
- **Published**: 2020-04-29 14:30:57+00:00
- **Updated**: 2020-10-02 19:26:14+00:00
- **Authors**: Sen He, Wentong Liao, Hamed R. Tavakoli, Michael Yang, Bodo Rosenhahn, Nicolas Pugeault
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic captioning of images is a task that combines the challenges of image analysis and text generation. One important aspect in captioning is the notion of attention: How to decide what to describe and in which order. Inspired by the successes in text analysis and translation, previous work have proposed the \textit{transformer} architecture for image captioning. However, the structure between the \textit{semantic units} in images (usually the detected regions from object detection model) and sentences (each single word) is different. Limited work has been done to adapt the transformer's internal architecture to images. In this work, we introduce the \textbf{\textit{image transformer}}, which consists of a modified encoding transformer and an implicit decoding transformer, motivated by the relative spatial relationship between image regions. Our design widen the original transformer layer's inner architecture to adapt to the structure of images. With only regions feature as inputs, our model achieves new state-of-the-art performance on both MSCOCO offline and online testing benchmarks.



### Action Sequence Predictions of Vehicles in Urban Environments using Map and Social Context
- **Arxiv ID**: http://arxiv.org/abs/2004.14251v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2004.14251v1)
- **Published**: 2020-04-29 14:59:58+00:00
- **Updated**: 2020-04-29 14:59:58+00:00
- **Authors**: Jan-Nico Zaech, Dengxin Dai, Alexander Liniger, Luc Van Gool
- **Comment**: None
- **Journal**: None
- **Summary**: This work studies the problem of predicting the sequence of future actions for surround vehicles in real-world driving scenarios. To this aim, we make three main contributions. The first contribution is an automatic method to convert the trajectories recorded in real-world driving scenarios to action sequences with the help of HD maps. The method enables automatic dataset creation for this task from large-scale driving data. Our second contribution lies in applying the method to the well-known traffic agent tracking and prediction dataset Argoverse, resulting in 228,000 action sequences. Additionally, 2,245 action sequences were manually annotated for testing. The third contribution is to propose a novel action sequence prediction method by integrating past positions and velocities of the traffic agents, map information and social context into a single end-to-end trainable neural network. Our experiments prove the merit of the data creation method and the value of the created dataset - prediction performance improves consistently with the size of the dataset and shows that our action prediction method outperforms comparing models.



### Augmented Semantic Signatures of Airborne LiDAR Point Clouds for Comparison
- **Arxiv ID**: http://arxiv.org/abs/2005.02152v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, eess.IV, 68U05, 68U10, 68U20, 65C50, G.1.3; I.4.10; I.4.8; J.2
- **Links**: [PDF](http://arxiv.org/pdf/2005.02152v2)
- **Published**: 2020-04-29 15:27:07+00:00
- **Updated**: 2020-08-08 04:32:43+00:00
- **Authors**: Jaya Sreevalsan-Nair, Pragyan Mohapatra
- **Comment**: 18 pages, 6 figures, 1 table
- **Journal**: None
- **Summary**: LiDAR point clouds provide rich geometric information, which is particularly useful for the analysis of complex scenes of urban regions. Finding structural and semantic differences between two different three-dimensional point clouds, say, of the same region but acquired at different time instances is an important problem. A comparison of point clouds involves computationally expensive registration and segmentation. We are interested in capturing the relative differences in the geometric uncertainty and semantic content of the point cloud without the registration process. Hence, we propose an orientation-invariant geometric signature of the point cloud, which integrates its probabilistic geometric and semantic classifications. We study different properties of the geometric signature, which are an image-based encoding of geometric uncertainty and semantic content. We explore different metrics to determine differences between these signatures, which in turn compare point clouds without performing point-to-point registration. Our results show that the differences in the signatures corroborate with the geometric and semantic differences of the point clouds.



### Tensor train rank minimization with nonlocal self-similarity for tensor completion
- **Arxiv ID**: http://arxiv.org/abs/2004.14273v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.14273v1)
- **Published**: 2020-04-29 15:39:39+00:00
- **Updated**: 2020-04-29 15:39:39+00:00
- **Authors**: Meng Ding, Ting-Zhu Huang, Xi-Le Zhao, Michael K. Ng, Tian-Hui Ma
- **Comment**: None
- **Journal**: None
- **Summary**: The tensor train (TT) rank has received increasing attention in tensor completion due to its ability to capture the global correlation of high-order tensors ($\textrm{order} >3$). For third order visual data, direct TT rank minimization has not exploited the potential of TT rank for high-order tensors. The TT rank minimization accompany with \emph{ket augmentation}, which transforms a lower-order tensor (e.g., visual data) into a higher-order tensor, suffers from serious block-artifacts. To tackle this issue, we suggest the TT rank minimization with nonlocal self-similarity for tensor completion by simultaneously exploring the spatial, temporal/spectral, and nonlocal redundancy in visual data. More precisely, the TT rank minimization is performed on a formed higher-order tensor called group by stacking similar cubes, which naturally and fully takes advantage of the ability of TT rank for high-order tensors. Moreover, the perturbation analysis for the TT low-rankness of each group is established. We develop the alternating direction method of multipliers tailored for the specific structure to solve the proposed model. Extensive experiments demonstrate that the proposed method is superior to several existing state-of-the-art methods in terms of both qualitative and quantitative measures.



### Seeing voices and hearing voices: learning discriminative embeddings using cross-modal self-supervision
- **Arxiv ID**: http://arxiv.org/abs/2004.14326v2
- **DOI**: 10.21437/Interspeech.2020-1113
- **Categories**: **cs.SD**, cs.CV, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2004.14326v2)
- **Published**: 2020-04-29 16:51:50+00:00
- **Updated**: 2020-05-06 14:56:36+00:00
- **Authors**: Soo-Whan Chung, Hong Goo Kang, Joon Son Chung
- **Comment**: Under submission as a conference paper
- **Journal**: None
- **Summary**: The goal of this work is to train discriminative cross-modal embeddings without access to manually annotated data. Recent advances in self-supervised learning have shown that effective representations can be learnt from natural cross-modal synchrony. We build on earlier work to train embeddings that are more discriminative for uni-modal downstream tasks. To this end, we propose a novel training strategy that not only optimises metrics across modalities, but also enforces intra-class feature separation within each of the modalities. The effectiveness of the method is demonstrated on two downstream tasks: lip reading using the features trained on audio-visual synchronisation, and speaker recognition using the features trained for cross-modal biometric matching. The proposed method outperforms state-of-the-art self-supervised baselines by a signficant margin.



### Beyond Instructional Videos: Probing for More Diverse Visual-Textual Grounding on YouTube
- **Arxiv ID**: http://arxiv.org/abs/2004.14338v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.14338v2)
- **Published**: 2020-04-29 17:10:10+00:00
- **Updated**: 2020-10-16 17:30:51+00:00
- **Authors**: Jack Hessel, Zhenhai Zhu, Bo Pang, Radu Soricut
- **Comment**: 11 pages including supplementary materials
- **Journal**: Published in EMNLP 2020
- **Summary**: Pretraining from unlabelled web videos has quickly become the de-facto means of achieving high performance on many video understanding tasks. Features are learned via prediction of grounded relationships between visual content and automatic speech recognition (ASR) tokens. However, prior pretraining work has been limited to only instructional videos; a priori, we expect this domain to be relatively "easy:" speakers in instructional videos will often reference the literal objects/actions being depicted. We ask: can similar models be trained on more diverse video corpora? And, if so, what types of videos are "grounded" and what types are not? We fit a representative pretraining model to the diverse YouTube8M dataset, and study its success and failure cases. We find that visual-textual grounding is indeed possible across previously unexplored video categories, and that pretraining on a more diverse set results in representations that generalize to both non-instructional and instructional domains.



### Editing in Style: Uncovering the Local Semantics of GANs
- **Arxiv ID**: http://arxiv.org/abs/2004.14367v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.14367v2)
- **Published**: 2020-04-29 17:45:56+00:00
- **Updated**: 2020-05-21 13:01:07+00:00
- **Authors**: Edo Collins, Raja Bala, Bob Price, Sabine Süsstrunk
- **Comment**: IEEE Conference on Computer Vision and Patten Recognition (CVPR),
  2020. Code: https://github.com/IVRL/GANLocalEditing
- **Journal**: None
- **Summary**: While the quality of GAN image synthesis has improved tremendously in recent years, our ability to control and condition the output is still limited. Focusing on StyleGAN, we introduce a simple and effective method for making local, semantically-aware edits to a target output image. This is accomplished by borrowing elements from a source image, also a GAN output, via a novel manipulation of style vectors. Our method requires neither supervision from an external model, nor involves complex spatial morphing operations. Instead, it relies on the emergent disentanglement of semantic objects that is learned by StyleGAN during its training. Semantic editing is demonstrated on GANs producing human faces, indoor scenes, cats, and cars. We measure the locality and photorealism of the edits produced by our method, and find that it accomplishes both.



### VGGSound: A Large-scale Audio-Visual Dataset
- **Arxiv ID**: http://arxiv.org/abs/2004.14368v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2004.14368v2)
- **Published**: 2020-04-29 17:46:54+00:00
- **Updated**: 2020-09-25 00:26:52+00:00
- **Authors**: Honglie Chen, Weidi Xie, Andrea Vedaldi, Andrew Zisserman
- **Comment**: ICASSP2020
- **Journal**: None
- **Summary**: Our goal is to collect a large-scale audio-visual dataset with low label noise from videos in the wild using computer vision techniques. The resulting dataset can be used for training and evaluating audio recognition models. We make three contributions. First, we propose a scalable pipeline based on computer vision techniques to create an audio dataset from open-source media. Our pipeline involves obtaining videos from YouTube; using image classification algorithms to localize audio-visual correspondence; and filtering out ambient noise using audio verification. Second, we use this pipeline to curate the VGGSound dataset consisting of more than 210k videos for 310 audio classes. Third, we investigate various Convolutional Neural Network~(CNN) architectures and aggregation approaches to establish audio recognition baselines for our new dataset. Compared to existing audio datasets, VGGSound ensures audio-visual correspondence and is collected under unconstrained conditions. Code and the dataset are available at http://www.robots.ox.ac.uk/~vgg/data/vggsound/



### UAV and Machine Learning Based Refinement of a Satellite-Driven Vegetation Index for Precision Agriculture
- **Arxiv ID**: http://arxiv.org/abs/2004.14421v1
- **DOI**: 10.3390/s20092530
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.14421v1)
- **Published**: 2020-04-29 18:34:48+00:00
- **Updated**: 2020-04-29 18:34:48+00:00
- **Authors**: Vittorio Mazzia, Lorenzo Comba, Aleem Khaliq, Marcello Chiaberge, Paolo Gay
- **Comment**: None
- **Journal**: Sensors 2020, 20(9), 2530
- **Summary**: Precision agriculture is considered to be a fundamental approach in pursuing a low-input, high-efficiency, and sustainable kind of agriculture when performing site-specific management practices. To achieve this objective, a reliable and updated description of the local status of crops is required. Remote sensing, and in particular satellite-based imagery, proved to be a valuable tool in crop mapping, monitoring, and diseases assessment. However, freely available satellite imagery with low or moderate resolutions showed some limits in specific agricultural applications, e.g., where crops are grown by rows. Indeed, in this framework, the satellite's output could be biased by intra-row covering, giving inaccurate information about crop status. This paper presents a novel satellite imagery refinement framework, based on a deep learning technique which exploits information properly derived from high resolution images acquired by unmanned aerial vehicle (UAV) airborne multispectral sensors. To train the convolutional neural network, only a single UAV-driven dataset is required, making the proposed approach simple and cost-effective. A vineyard in Serralunga d'Alba (Northern Italy) was chosen as a case study for validation purposes. Refined satellite-driven normalized difference vegetation index (NDVI) maps, acquired in four different periods during the vine growing season, were shown to better describe crop status with respect to raw datasets by correlation analysis and ANOVA. In addition, using a K-means based classifier, 3-class vineyard vigor maps were profitably derived from the NDVI maps, which are a valuable tool for growers.



### Pragmatic Issue-Sensitive Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/2004.14451v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.14451v2)
- **Published**: 2020-04-29 20:00:53+00:00
- **Updated**: 2020-10-05 23:24:41+00:00
- **Authors**: Allen Nie, Reuben Cohn-Gordon, Christopher Potts
- **Comment**: 15 pages, 7 figures. EMNLP 2020 Findings Accepted
- **Journal**: None
- **Summary**: Image captioning systems have recently improved dramatically, but they still tend to produce captions that are insensitive to the communicative goals that captions should meet. To address this, we propose Issue-Sensitive Image Captioning (ISIC). In ISIC, a captioning system is given a target image and an issue, which is a set of images partitioned in a way that specifies what information is relevant. The goal of the captioner is to produce a caption that resolves this issue. To model this task, we use an extension of the Rational Speech Acts model of pragmatic language use. Our extension is built on top of state-of-the-art pretrained neural image captioners and explicitly reasons about issues in our sense. We establish experimentally that these models generate captions that are both highly descriptive and issue-sensitive, and we show how ISIC can complement and enrich the related task of Visual Question Answering.



### Teaching Cameras to Feel: Estimating Tactile Physical Properties of Surfaces From Images
- **Arxiv ID**: http://arxiv.org/abs/2004.14487v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.14487v3)
- **Published**: 2020-04-29 21:27:26+00:00
- **Updated**: 2021-09-20 19:03:56+00:00
- **Authors**: Matthew Purri, Kristin Dana
- **Comment**: 19 pages, 5 figures, 6 tables
- **Journal**: None
- **Summary**: The connection between visual input and tactile sensing is critical for object manipulation tasks such as grasping and pushing. In this work, we introduce the challenging task of estimating a set of tactile physical properties from visual information. We aim to build a model that learns the complex mapping between visual information and tactile physical properties. We construct a first of its kind image-tactile dataset with over 400 multiview image sequences and the corresponding tactile properties. A total of fifteen tactile physical properties across categories including friction, compliance, adhesion, texture, and thermal conductance are measured and then estimated by our models. We develop a cross-modal framework comprised of an adversarial objective and a novel visuo-tactile joint classification loss. Additionally, we develop a neural architecture search framework capable of selecting optimal combinations of viewing angles for estimating a given physical property.



### Interactive Video Stylization Using Few-Shot Patch-Based Training
- **Arxiv ID**: http://arxiv.org/abs/2004.14489v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.14489v1)
- **Published**: 2020-04-29 21:33:28+00:00
- **Updated**: 2020-04-29 21:33:28+00:00
- **Authors**: Ondřej Texler, David Futschik, Michal Kučera, Ondřej Jamriška, Šárka Sochorová, Menglei Chai, Sergey Tulyakov, Daniel Sýkora
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present a learning-based method to the keyframe-based video stylization that allows an artist to propagate the style from a few selected keyframes to the rest of the sequence. Its key advantage is that the resulting stylization is semantically meaningful, i.e., specific parts of moving objects are stylized according to the artist's intention. In contrast to previous style transfer techniques, our approach does not require any lengthy pre-training process nor a large training dataset. We demonstrate how to train an appearance translation network from scratch using only a few stylized exemplars while implicitly preserving temporal consistency. This leads to a video stylization framework that supports real-time inference, parallel processing, and random access to an arbitrary output frame. It can also merge the content from multiple keyframes without the need to perform an explicit blending operation. We demonstrate its practical utility in various interactive scenarios, where the user paints over a selected keyframe and sees her style transferred to an existing recorded sequence or a live video stream.



### Detecting Deep-Fake Videos from Appearance and Behavior
- **Arxiv ID**: http://arxiv.org/abs/2004.14491v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.14491v1)
- **Published**: 2020-04-29 21:38:22+00:00
- **Updated**: 2020-04-29 21:38:22+00:00
- **Authors**: Shruti Agarwal, Tarek El-Gaaly, Hany Farid, Ser-Nam Lim
- **Comment**: None
- **Journal**: IEEE Workshop on Image Forensics and Security, 2020
- **Summary**: Synthetically-generated audios and videos -- so-called deep fakes -- continue to capture the imagination of the computer-graphics and computer-vision communities. At the same time, the democratization of access to technology that can create sophisticated manipulated video of anybody saying anything continues to be of concern because of its power to disrupt democratic elections, commit small to large-scale fraud, fuel dis-information campaigns, and create non-consensual pornography. We describe a biometric-based forensic technique for detecting face-swap deep fakes. This technique combines a static biometric based on facial recognition with a temporal, behavioral biometric based on facial expressions and head movements, where the behavioral embedding is learned using a CNN with a metric-learning objective function. We show the efficacy of this approach across several large-scale video datasets, as well as in-the-wild deep fakes.



### Rethinking Class-Discrimination Based CNN Channel Pruning
- **Arxiv ID**: http://arxiv.org/abs/2004.14492v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.14492v1)
- **Published**: 2020-04-29 21:40:23+00:00
- **Updated**: 2020-04-29 21:40:23+00:00
- **Authors**: Yuchen Liu, David Wentzlaff, S. Y. Kung
- **Comment**: None
- **Journal**: None
- **Summary**: Channel pruning has received ever-increasing focus on network compression. In particular, class-discrimination based channel pruning has made major headway, as it fits seamlessly with the classification objective of CNNs and provides good explainability. Prior works singly propose and evaluate their discriminant functions, while further study on the effectiveness of the adopted metrics is absent. To this end, we initiate the first study on the effectiveness of a broad range of discriminant functions on channel pruning. Conventional single-variate binary-class statistics like Student's T-Test are also included in our study via an intuitive generalization. The winning metric of our study has a greater ability to select informative channels over other state-of-the-art methods, which is substantiated by our qualitative and quantitative analysis. Moreover, we develop a FLOP-normalized sensitivity analysis scheme to automate the structural pruning procedure. On CIFAR-10, CIFAR-100, and ILSVRC-2012 datasets, our pruned models achieve higher accuracy with less inference cost compared to state-of-the-art results. For example, on ILSVRC-2012, our 44.3% FLOPs-pruned ResNet-50 has only a 0.3% top-1 accuracy drop, which significantly outperforms the state of the art.



### The EPIC-KITCHENS Dataset: Collection, Challenges and Baselines
- **Arxiv ID**: http://arxiv.org/abs/2005.00343v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.00343v1)
- **Published**: 2020-04-29 21:57:04+00:00
- **Updated**: 2020-04-29 21:57:04+00:00
- **Authors**: Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Sanja Fidler, Antonino Furnari, Evangelos Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, Michael Wray
- **Comment**: Preprint for paper at IEEE TPAMI. arXiv admin note: substantial text
  overlap with arXiv:1804.02748
- **Journal**: None
- **Summary**: Since its introduction in 2018, EPIC-KITCHENS has attracted attention as the largest egocentric video benchmark, offering a unique viewpoint on people's interaction with objects, their attention, and even intention. In this paper, we detail how this large-scale dataset was captured by 32 participants in their native kitchen environments, and densely annotated with actions and object interactions. Our videos depict nonscripted daily activities, as recording is started every time a participant entered their kitchen. Recording took place in 4 countries by participants belonging to 10 different nationalities, resulting in highly diverse kitchen habits and cooking styles. Our dataset features 55 hours of video consisting of 11.5M frames, which we densely labelled for a total of 39.6K action segments and 454.2K object bounding boxes. Our annotation is unique in that we had the participants narrate their own videos after recording, thus reflecting true intention, and we crowd-sourced ground-truths based on these. We describe our object, action and. anticipation challenges, and evaluate several baselines over two test splits, seen and unseen kitchens. We introduce new baselines that highlight the multimodal nature of the dataset and the importance of explicit temporal modelling to discriminate fine-grained actions e.g. 'closing a tap' from 'opening' it up.



