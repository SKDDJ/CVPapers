# Arxiv Papers in cs.CV on 2020-04-13
### A Comparison of Deep Learning Convolution Neural Networks for Liver Segmentation in Radial Turbo Spin Echo Images
- **Arxiv ID**: http://arxiv.org/abs/2004.05731v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.05731v1)
- **Published**: 2020-04-13 00:19:02+00:00
- **Updated**: 2020-04-13 00:19:02+00:00
- **Authors**: Lavanya Umapathy, Mahesh Bharath Keerthivasan, Jean-Phillipe Galons, Wyatt Unger, Diego Martin, Maria I Altbach, Ali Bilgin
- **Comment**: 3 pages, 4 figures, 1 table. Published in Proceedings of
  International Society for Magnetic Resonance in Medicine 2018
- **Journal**: None
- **Summary**: Motion-robust 2D Radial Turbo Spin Echo (RADTSE) pulse sequence can provide a high-resolution composite image, T2-weighted images at multiple echo times (TEs), and a quantitative T2 map, all from a single k-space acquisition. In this work, we use a deep-learning convolutional neural network (CNN) for the segmentation of liver in abdominal RADTSE images. A modified UNET architecture with generalized dice loss objective function was implemented. Three 2D CNNs were trained, one for each image type obtained from the RADTSE sequence. On evaluating the performance of the CNNs on the validation set, we found that CNNs trained on TE images or the T2 maps had higher average dice scores than the composite images. This, in turn, implies that the information regarding T2 variation in tissues aids in improving the segmentation performance.



### Deep Siamese Domain Adaptation Convolutional Neural Network for Cross-domain Change Detection in Multispectral Images
- **Arxiv ID**: http://arxiv.org/abs/2004.05745v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.05745v1)
- **Published**: 2020-04-13 02:15:04+00:00
- **Updated**: 2020-04-13 02:15:04+00:00
- **Authors**: Hongruixuan Chen, Chen Wu, Bo Du, Liangepei Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, deep learning has achieved promising performance in the change detection task. However, the deep models are task-specific and data set bias often exists, thus it is difficult to transfer a network trained on one multi-temporal data set (source domain) to another multi-temporal data set with very limited (even no) labeled data (target domain). In this paper, we propose a novel deep siamese domain adaptation convolutional neural network (DSDANet) architecture for cross-domain change detection. In DSDANet, a siamese convolutional neural network first extracts spatial-spectral features from multi-temporal images. Then, through multiple kernel maximum mean discrepancy (MK-MMD), the learned feature representation is embedded into a reproducing kernel Hilbert space (RKHS), in which the distribution of two domains can be explicitly matched. By optimizing the network parameters and kernel coefficients with the source labeled data and target unlabeled data, the DSDANet can learn transferrable feature representation that can bridge the discrepancy between two domains. To the best of our knowledge, it is the first time that such a domain adaptation-based deep network is proposed for change detection. The theoretical analysis and experimental results demonstrate the effectiveness and potential of the proposed method.



### Enabling Incremental Knowledge Transfer for Object Detection at the Edge
- **Arxiv ID**: http://arxiv.org/abs/2004.05746v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.05746v2)
- **Published**: 2020-04-13 02:19:18+00:00
- **Updated**: 2020-06-07 13:31:22+00:00
- **Authors**: Mohammad Farhadi Bajestani, Mehdi Ghasemi, Sarma Vrudhula, Yezhou Yang
- **Comment**: 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition
  Workshop (CVPRW)
- **Journal**: None
- **Summary**: Object detection using deep neural networks (DNNs) involves a huge amount of computation which impedes its implementation on resource/energy-limited user-end devices. The reason for the success of DNNs is due to having knowledge over all different domains of observed environments. However, we need a limited knowledge of the observed environment at inference time which can be learned using a shallow neural network (SHNN). In this paper, a system-level design is proposed to improve the energy consumption of object detection on the user-end device. An SHNN is deployed on the user-end device to detect objects in the observing environment. Also, a knowledge transfer mechanism is implemented to update the SHNN model using the DNN knowledge when there is a change in the object domain. DNN knowledge can be obtained from a powerful edge device connected to the user-end device through LAN or Wi-Fi. Experiments demonstrate that the energy consumption of the user-end device and the inference time can be improved by 78% and 71% compared with running the deep model on the user-end device.



### Self-supervised Feature Learning by Cross-modality and Cross-view Correspondences
- **Arxiv ID**: http://arxiv.org/abs/2004.05749v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.05749v1)
- **Published**: 2020-04-13 02:57:25+00:00
- **Updated**: 2020-04-13 02:57:25+00:00
- **Authors**: Longlong Jing, Yucheng Chen, Ling Zhang, Mingyi He, Yingli Tian
- **Comment**: None
- **Journal**: None
- **Summary**: The success of supervised learning requires large-scale ground truth labels which are very expensive, time-consuming, or may need special skills to annotate. To address this issue, many self- or un-supervised methods are developed. Unlike most existing self-supervised methods to learn only 2D image features or only 3D point cloud features, this paper presents a novel and effective self-supervised learning approach to jointly learn both 2D image features and 3D point cloud features by exploiting cross-modality and cross-view correspondences without using any human annotated labels. Specifically, 2D image features of rendered images from different views are extracted by a 2D convolutional neural network, and 3D point cloud features are extracted by a graph convolution neural network. Two types of features are fed into a two-layer fully connected neural network to estimate the cross-modality correspondence. The three networks are jointly trained (i.e. cross-modality) by verifying whether two sampled data of different modalities belong to the same object, meanwhile, the 2D convolutional neural network is additionally optimized through minimizing intra-object distance while maximizing inter-object distance of rendered images in different views (i.e. cross-view). The effectiveness of the learned 2D and 3D features is evaluated by transferring them on five different tasks including multi-view 2D shape recognition, 3D shape recognition, multi-view 2D shape retrieval, 3D shape retrieval, and 3D part-segmentation. Extensive evaluations on all the five different tasks across different datasets demonstrate strong generalization and effectiveness of the learned 2D and 3D features by the proposed self-supervised method.



### Deep Learning COVID-19 Features on CXR using Limited Training Data Sets
- **Arxiv ID**: http://arxiv.org/abs/2004.05758v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.05758v2)
- **Published**: 2020-04-13 03:44:42+00:00
- **Updated**: 2020-05-05 16:07:25+00:00
- **Authors**: Yujin Oh, Sangjoon Park, Jong Chul Ye
- **Comment**: Accepted for IEEE Trans. on Medical Imaging Special Issue on
  Imaging-based Diagnosis of COVID-19
- **Journal**: None
- **Summary**: Under the global pandemic of COVID-19, the use of artificial intelligence to analyze chest X-ray (CXR) image for COVID-19 diagnosis and patient triage is becoming important. Unfortunately, due to the emergent nature of the COVID-19 pandemic, a systematic collection of the CXR data set for deep neural network training is difficult. To address this problem, here we propose a patch-based convolutional neural network approach with a relatively small number of trainable parameters for COVID-19 diagnosis. The proposed method is inspired by our statistical analysis of the potential imaging biomarkers of the CXR radiographs. Experimental results show that our method achieves state-of-the-art performance and provides clinically interpretable saliency maps, which are useful for COVID-19 diagnosis and patient triage.



### UC-Net: Uncertainty Inspired RGB-D Saliency Detection via Conditional Variational Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/2004.05763v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.05763v1)
- **Published**: 2020-04-13 04:12:59+00:00
- **Updated**: 2020-04-13 04:12:59+00:00
- **Authors**: Jing Zhang, Deng-Ping Fan, Yuchao Dai, Saeed Anwar, Fatemeh Sadat Saleh, Tong Zhang, Nick Barnes
- **Comment**: Accepted by IEEE CVPR 2020 (ORAL). Code:
  https://github.com/JingZhang617/UCNet
- **Journal**: None
- **Summary**: In this paper, we propose the first framework (UCNet) to employ uncertainty for RGB-D saliency detection by learning from the data labeling process. Existing RGB-D saliency detection methods treat the saliency detection task as a point estimation problem, and produce a single saliency map following a deterministic learning pipeline. Inspired by the saliency data labeling process, we propose probabilistic RGB-D saliency detection network via conditional variational autoencoders to model human annotation uncertainty and generate multiple saliency maps for each input image by sampling in the latent space. With the proposed saliency consensus process, we are able to generate an accurate saliency map based on these multiple predictions. Quantitative and qualitative evaluations on six challenging benchmark datasets against 18 competing algorithms demonstrate the effectiveness of our approach in learning the distribution of saliency maps, leading to a new state-of-the-art in RGB-D saliency detection.



### Towards Transferable Adversarial Attack against Deep Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2004.05790v2
- **DOI**: 10.1109/TIFS.2020.3036801
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.05790v2)
- **Published**: 2020-04-13 06:44:33+00:00
- **Updated**: 2020-11-23 14:07:04+00:00
- **Authors**: Yaoyao Zhong, Weihong Deng
- **Comment**: This article has been accepted by IEEE Transactions on Information
  Forensics and Security. TALFW database is available at
  http://www.whdeng.cn/TALFW/index.html
- **Journal**: None
- **Summary**: Face recognition has achieved great success in the last five years due to the development of deep learning methods. However, deep convolutional neural networks (DCNNs) have been found to be vulnerable to adversarial examples. In particular, the existence of transferable adversarial examples can severely hinder the robustness of DCNNs since this type of attacks can be applied in a fully black-box manner without queries on the target system. In this work, we first investigate the characteristics of transferable adversarial attacks in face recognition by showing the superiority of feature-level methods over label-level methods. Then, to further improve transferability of feature-level adversarial examples, we propose DFANet, a dropout-based method used in convolutional layers, which can increase the diversity of surrogate models and obtain ensemble-like effects. Extensive experiments on state-of-the-art face models with various training databases, loss functions and network architectures show that the proposed method can significantly enhance the transferability of existing attack methods. Finally, by applying DFANet to the LFW database, we generate a new set of adversarial face pairs that can successfully attack four commercial APIs without any queries. This TALFW database is available to facilitate research on the robustness and defense of deep face recognition.



### Learning Event-Based Motion Deblurring
- **Arxiv ID**: http://arxiv.org/abs/2004.05794v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.05794v1)
- **Published**: 2020-04-13 07:01:06+00:00
- **Updated**: 2020-04-13 07:01:06+00:00
- **Authors**: Zhe Jiang, Yu Zhang, Dongqing Zou, Jimmy Ren, Jiancheng Lv, Yebin Liu
- **Comment**: Accepted to CVPR 2020
- **Journal**: None
- **Summary**: Recovering sharp video sequence from a motion-blurred image is highly ill-posed due to the significant loss of motion information in the blurring process. For event-based cameras, however, fast motion can be captured as events at high time rate, raising new opportunities to exploring effective solutions. In this paper, we start from a sequential formulation of event-based motion deblurring, then show how its optimization can be unfolded with a novel end-to-end deep architecture. The proposed architecture is a convolutional recurrent neural network that integrates visual and temporal knowledge of both global and local scales in principled manner. To further improve the reconstruction, we propose a differentiable directional event filtering module to effectively extract rich boundary prior from the stream of events. We conduct extensive experiments on the synthetic GoPro dataset and a large newly introduced dataset captured by a DAVIS240C camera. The proposed approach achieves state-of-the-art reconstruction quality, and generalizes better to handling real-world motion blur.



### Rethinking Differentiable Search for Mixed-Precision Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2004.05795v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2004.05795v1)
- **Published**: 2020-04-13 07:02:23+00:00
- **Updated**: 2020-04-13 07:02:23+00:00
- **Authors**: Zhaowei Cai, Nuno Vasconcelos
- **Comment**: accepted by CVPR 2020
- **Journal**: None
- **Summary**: Low-precision networks, with weights and activations quantized to low bit-width, are widely used to accelerate inference on edge devices. However, current solutions are uniform, using identical bit-width for all filters. This fails to account for the different sensitivities of different filters and is suboptimal. Mixed-precision networks address this problem, by tuning the bit-width to individual filter requirements. In this work, the problem of optimal mixed-precision network search (MPS) is considered. To circumvent its difficulties of discrete search space and combinatorial optimization, a new differentiable search architecture is proposed, with several novel contributions to advance the efficiency by leveraging the unique properties of the MPS problem. The resulting Efficient differentiable MIxed-Precision network Search (EdMIPS) method is effective at finding the optimal bit allocation for multiple popular networks, and can search a large model, e.g. Inception-V3, directly on ImageNet without proxy task in a reasonable amount of time. The learned mixed-precision networks significantly outperform their uniform counterparts.



### Multi-modal Datasets for Super-resolution
- **Arxiv ID**: http://arxiv.org/abs/2004.05804v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.05804v1)
- **Published**: 2020-04-13 07:39:52+00:00
- **Updated**: 2020-04-13 07:39:52+00:00
- **Authors**: Haoran Li, Weihong Quan, Meijun Yan, Jin zhang, Xiaoli Gong, Jin Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: Nowdays, most datasets used to train and evaluate super-resolution models are single-modal simulation datasets. However, due to the variety of image degradation types in the real world, models trained on single-modal simulation datasets do not always have good robustness and generalization ability in different degradation scenarios. Previous work tended to focus only on true-color images. In contrast, we first proposed real-world black-and-white old photo datasets for super-resolution (OID-RW), which is constructed using two methods of manually filling pixels and shooting with different cameras. The dataset contains 82 groups of images, including 22 groups of character type and 60 groups of landscape and architecture. At the same time, we also propose a multi-modal degradation dataset (MDD400) to solve the super-resolution reconstruction in real-life image degradation scenarios. We managed to simulate the process of generating degraded images by the following four methods: interpolation algorithm, CNN network, GAN network and capturing videos with different bit rates. Our experiments demonstrate that not only the models trained on our dataset have better generalization capability and robustness, but also the trained images can maintain better edge contours and texture features.



### Diversity Helps: Unsupervised Few-shot Learning via Distribution Shift-based Data Augmentation
- **Arxiv ID**: http://arxiv.org/abs/2004.05805v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.05805v2)
- **Published**: 2020-04-13 07:41:56+00:00
- **Updated**: 2020-09-17 06:00:36+00:00
- **Authors**: Tiexin Qin, Wenbin Li, Yinghuan Shi, Yang Gao
- **Comment**: 10 pages, 8 figures. Code: https://github.com/WonderSeven/ULDA
- **Journal**: None
- **Summary**: Few-shot learning aims to learn a new concept when only a few training examples are available, which has been extensively explored in recent years. However, most of the current works heavily rely on a large-scale labeled auxiliary set to train their models in an episodic-training paradigm. Such a kind of supervised setting basically limits the widespread use of few-shot learning algorithms. Instead, in this paper, we develop a novel framework called Unsupervised Few-shot Learning via Distribution Shift-based Data Augmentation (ULDA), which pays attention to the distribution diversity inside each constructed pretext few-shot task when using data augmentation. Importantly, we highlight the value and importance of the distribution diversity in the augmentation-based pretext few-shot tasks, which can effectively alleviate the overfitting problem and make the few-shot model learn more robust feature representations. In ULDA, we systemically investigate the effects of different augmentation techniques and propose to strengthen the distribution diversity (or difference) between the query set and support set in each few-shot task, by augmenting these two sets diversely (i.e., distribution shifting). In this way, even incorporated with simple augmentation techniques (e.g., random crop, color jittering, or rotation), our ULDA can produce a significant improvement. In the experiments, few-shot models learned by ULDA can achieve superior generalization performance and obtain state-of-the-art results in a variety of established few-shot learning tasks on Omniglot and miniImageNet. The source code is available in https://github.com/WonderSeven/ULDA.



### MulayCap: Multi-layer Human Performance Capture Using A Monocular Video Camera
- **Arxiv ID**: http://arxiv.org/abs/2004.05815v3
- **DOI**: 10.1109/TVCG.2020.3027763
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.05815v3)
- **Published**: 2020-04-13 08:13:37+00:00
- **Updated**: 2020-10-01 08:00:34+00:00
- **Authors**: Zhaoqi Su, Weilin Wan, Tao Yu, Lingjie Liu, Lu Fang, Wenping Wang, Yebin Liu
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce MulayCap, a novel human performance capture method using a monocular video camera without the need for pre-scanning. The method uses "multi-layer" representations for geometry reconstruction and texture rendering, respectively. For geometry reconstruction, we decompose the clothed human into multiple geometry layers, namely a body mesh layer and a garment piece layer. The key technique behind is a Garment-from-Video (GfV) method for optimizing the garment shape and reconstructing the dynamic cloth to fit the input video sequence, based on a cloth simulation model which is effectively solved with gradient descent. For texture rendering, we decompose each input image frame into a shading layer and an albedo layer, and propose a method for fusing a fixed albedo map and solving for detailed garment geometry using the shading layer. Compared with existing single view human performance capture systems, our "multi-layer" approach bypasses the tedious and time consuming scanning step for obtaining a human specific mesh template. Experimental results demonstrate that MulayCap produces realistic rendering of dynamically changing details that has not been achieved in any previous monocular video camera systems. Benefiting from its fully semantic modeling, MulayCap can be applied to various important editing applications, such as cloth editing, re-targeting, relighting, and AR applications.



### Monocular Depth Estimation with Self-supervised Instance Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2004.05821v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.05821v1)
- **Published**: 2020-04-13 08:32:03+00:00
- **Updated**: 2020-04-13 08:32:03+00:00
- **Authors**: Robert McCraith, Lukas Neumann, Andrew Zisserman, Andrea Vedaldi
- **Comment**: IROS submission, 7 pages
- **Journal**: None
- **Summary**: Recent advances in self-supervised learning havedemonstrated that it is possible to learn accurate monoculardepth reconstruction from raw video data, without using any 3Dground truth for supervision. However, in robotics applications,multiple views of a scene may or may not be available, depend-ing on the actions of the robot, switching between monocularand multi-view reconstruction. To address this mixed setting,we proposed a new approach that extends any off-the-shelfself-supervised monocular depth reconstruction system to usemore than one image at test time. Our method builds on astandard prior learned to perform monocular reconstruction,but uses self-supervision at test time to further improve thereconstruction accuracy when multiple images are available.When used to update the correct components of the model, thisapproach is highly-effective. On the standard KITTI bench-mark, our self-supervised method consistently outperformsall the previous methods with an average 25% reduction inabsolute error for the three common setups (monocular, stereoand monocular+stereo), and comes very close in accuracy whencompared to the fully-supervised state-of-the-art methods.



### From Inference to Generation: End-to-end Fully Self-supervised Generation of Human Face from Speech
- **Arxiv ID**: http://arxiv.org/abs/2004.05830v1
- **DOI**: None
- **Categories**: **eess.AS**, cs.CV, cs.LG, cs.SD, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.05830v1)
- **Published**: 2020-04-13 09:01:49+00:00
- **Updated**: 2020-04-13 09:01:49+00:00
- **Authors**: Hyeong-Seok Choi, Changdae Park, Kyogu Lee
- **Comment**: 18 pages, 12 figures, Published as a conference paper at
  International Conference on Learning Representations (ICLR) 2020.
  (camera-ready version)
- **Journal**: None
- **Summary**: This work seeks the possibility of generating the human face from voice solely based on the audio-visual data without any human-labeled annotations. To this end, we propose a multi-modal learning framework that links the inference stage and generation stage. First, the inference networks are trained to match the speaker identity between the two different modalities. Then the trained inference networks cooperate with the generation network by giving conditional information about the voice. The proposed method exploits the recent development of GANs techniques and generates the human face directly from the speech waveform making our system fully end-to-end. We analyze the extent to which the network can naturally disentangle two latent factors that contribute to the generation of a face image - one that comes directly from a speech signal and the other that is not related to it - and explore whether the network can learn to generate natural human face image distribution by modeling these factors. Experimental results show that the proposed network can not only match the relationship between the human face and speech, but can also generate the high-quality human face sample conditioned on its speech. Finally, the correlation between the generated face and the corresponding speech is quantitatively measured to analyze the relationship between the two modalities.



### SPCNet:Spatial Preserve and Content-aware Network for Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2004.05834v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.05834v1)
- **Published**: 2020-04-13 09:14:00+00:00
- **Updated**: 2020-04-13 09:14:00+00:00
- **Authors**: Yabo Xiao, Dongdong Yu, Xiaojuan Wang, Tianqi Lv, Yiqi Fan, Lingrui Wu
- **Comment**: 8 pages,6 figures, accepted for presentation at the 24th European
  Conference on Artificial Intelligence (ECAI 2020)
- **Journal**: None
- **Summary**: Human pose estimation is a fundamental yet challenging task in computer vision. Although deep learning techniques have made great progress in this area, difficult scenarios (e.g., invisible keypoints, occlusions, complex multi-person scenarios, and abnormal poses) are still not well-handled. To alleviate these issues, we propose a novel Spatial Preserve and Content-aware Network(SPCNet), which includes two effective modules: Dilated Hourglass Module(DHM) and Selective Information Module(SIM). By using the Dilated Hourglass Module, we can preserve the spatial resolution along with large receptive field. Similar to Hourglass Network, we stack the DHMs to get the multi-stage and multi-scale information. Then, a Selective Information Module is designed to select relatively important features from different levels under a sufficient consideration of spatial content-aware mechanism and thus considerably improves the performance. Extensive experiments on MPII, LSP and FLIC human pose estimation benchmarks demonstrate the effectiveness of our network. In particular, we exceed previous methods and achieve the state-of-the-art performance on three aforementioned benchmark datasets.



### SSP: Single Shot Future Trajectory Prediction
- **Arxiv ID**: http://arxiv.org/abs/2004.05846v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2004.05846v2)
- **Published**: 2020-04-13 09:56:38+00:00
- **Updated**: 2020-11-09 01:37:26+00:00
- **Authors**: Isht Dwivedi, Srikanth Malla, Behzad Dariush, Chiho Choi
- **Comment**: Accepted at IROS 2020
- **Journal**: None
- **Summary**: We propose a robust solution to future trajectory forecast, which can be practically applicable to autonomous agents in highly crowded environments. For this, three aspects are particularly addressed in this paper. First, we use composite fields to predict future locations of all road agents in a single-shot, which results in a constant time complexity, regardless of the number of agents in the scene. Second, interactions between agents are modeled as a non-local response, enabling spatial relationships between different locations to be captured temporally as well (i.e., in spatio-temporal interactions). Third, the semantic context of the scene are modeled and take into account the environmental constraints that potentially influence the future motion. To this end, we validate the robustness of the proposed approach using the ETH, UCY, and SDD datasets and highlight its practical functionality compared to the current state-of-the-art methods.



### Regularizing Meta-Learning via Gradient Dropout
- **Arxiv ID**: http://arxiv.org/abs/2004.05859v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.05859v1)
- **Published**: 2020-04-13 10:47:02+00:00
- **Updated**: 2020-04-13 10:47:02+00:00
- **Authors**: Hung-Yu Tseng, Yi-Wen Chen, Yi-Hsuan Tsai, Sifei Liu, Yen-Yu Lin, Ming-Hsuan Yang
- **Comment**: Code: https://github.com/hytseng0509/DropGrad
- **Journal**: None
- **Summary**: With the growing attention on learning-to-learn new tasks using only a few examples, meta-learning has been widely used in numerous problems such as few-shot classification, reinforcement learning, and domain generalization. However, meta-learning models are prone to overfitting when there are no sufficient training tasks for the meta-learners to generalize. Although existing approaches such as Dropout are widely used to address the overfitting problem, these methods are typically designed for regularizing models of a single task in supervised training. In this paper, we introduce a simple yet effective method to alleviate the risk of overfitting for gradient-based meta-learning. Specifically, during the gradient-based adaptation stage, we randomly drop the gradient in the inner-loop optimization of each parameter in deep neural networks, such that the augmented gradients improve generalization to new tasks. We present a general form of the proposed gradient dropout regularization and show that this term can be sampled from either the Bernoulli or Gaussian distribution. To validate the proposed method, we conduct extensive experiments and analysis on numerous computer vision tasks, demonstrating that the gradient dropout regularization mitigates the overfitting problem and improves the performance upon various gradient-based meta-learning frameworks.



### Analysis of The Ratio of $\ell_1$ and $\ell_2$ Norms in Compressed Sensing
- **Arxiv ID**: http://arxiv.org/abs/2004.05873v2
- **DOI**: None
- **Categories**: **math.NA**, cs.CV, cs.NA, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/2004.05873v2)
- **Published**: 2020-04-13 11:35:41+00:00
- **Updated**: 2021-01-28 00:09:48+00:00
- **Authors**: Yiming Xu, Akil Narayan, Hoang Tran, Clayton G. Webster
- **Comment**: 25 pages, 7 figures
- **Journal**: None
- **Summary**: We first propose a novel criterion that guarantees that an $s$-sparse signal is the local minimizer of the $\ell_1/\ell_2$ objective; our criterion is interpretable and useful in practice. We also give the first uniform recovery condition using a geometric characterization of the null space of the measurement matrix, and show that this condition is easily satisfied for a class of random matrices. We also present analysis on the robustness of the procedure when noise pollutes data. Numerical experiments are provided that compare $\ell_1/\ell_2$ with some other popular non-convex methods in compressed sensing. Finally, we propose a novel initialization approach to accelerate the numerical optimization procedure. We call this initialization approach \emph{support selection}, and we demonstrate that it empirically improves the performance of existing $\ell_1/\ell_2$ algorithms.



### Adversarial Weight Perturbation Helps Robust Generalization
- **Arxiv ID**: http://arxiv.org/abs/2004.05884v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.05884v2)
- **Published**: 2020-04-13 12:05:01+00:00
- **Updated**: 2020-10-13 13:46:09+00:00
- **Authors**: Dongxian Wu, Shu-tao Xia, Yisen Wang
- **Comment**: To appear in NeurIPS 2020
- **Journal**: None
- **Summary**: The study on improving the robustness of deep neural networks against adversarial examples grows rapidly in recent years. Among them, adversarial training is the most promising one, which flattens the input loss landscape (loss change with respect to input) via training on adversarially perturbed examples. However, how the widely used weight loss landscape (loss change with respect to weight) performs in adversarial training is rarely explored. In this paper, we investigate the weight loss landscape from a new perspective, and identify a clear correlation between the flatness of weight loss landscape and robust generalization gap. Several well-recognized adversarial training improvements, such as early stopping, designing new objective functions, or leveraging unlabeled data, all implicitly flatten the weight loss landscape. Based on these observations, we propose a simple yet effective Adversarial Weight Perturbation (AWP) to explicitly regularize the flatness of weight loss landscape, forming a double-perturbation mechanism in the adversarial training framework that adversarially perturbs both inputs and weights. Extensive experiments demonstrate that AWP indeed brings flatter weight loss landscape and can be easily incorporated into various existing adversarial training methods to further boost their adversarial robustness.



### Unsupervised Facial Action Unit Intensity Estimation via Differentiable Optimization
- **Arxiv ID**: http://arxiv.org/abs/2004.05908v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.05908v1)
- **Published**: 2020-04-13 12:56:28+00:00
- **Updated**: 2020-04-13 12:56:28+00:00
- **Authors**: Xinhui Song, Tianyang Shi, Tianjia Shao, Yi Yuan, Zunlei Feng, Changjie Fan
- **Comment**: None
- **Journal**: None
- **Summary**: The automatic intensity estimation of facial action units (AUs) from a single image plays a vital role in facial analysis systems. One big challenge for data-driven AU intensity estimation is the lack of sufficient AU label data. Due to the fact that AU annotation requires strong domain expertise, it is expensive to construct an extensive database to learn deep models. The limited number of labeled AUs as well as identity differences and pose variations further increases the estimation difficulties. Considering all these difficulties, we propose an unsupervised framework GE-Net for facial AU intensity estimation from a single image, without requiring any annotated AU data. Our framework performs differentiable optimization, which iteratively updates the facial parameters (i.e., head pose, AU parameters and identity parameters) to match the input image. GE-Net consists of two modules: a generator and a feature extractor. The generator learns to "render" a face image from a set of facial parameters in a differentiable way, and the feature extractor extracts deep features for measuring the similarity of the rendered image and input real image. After the two modules are trained and fixed, the framework searches optimal facial parameters by minimizing the differences of the extracted features between the rendered image and the input image. Experimental results demonstrate that our method can achieve state-of-the-art results compared with existing methods.



### kDecay: Just adding k-decay items on Learning-Rate Schedule to improve Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2004.05909v5
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.05909v5)
- **Published**: 2020-04-13 12:58:45+00:00
- **Updated**: 2022-03-22 02:05:24+00:00
- **Authors**: Tao Zhang, Wei Li
- **Comment**: None
- **Journal**: None
- **Summary**: Recent work has shown that optimizing the Learning Rate (LR) schedule can be a very accurate and efficient way to train deep neural networks. We observe that the rate of change (ROC) of LR has correlation with the training process, but how to use this relationship to control the training to achieve the purpose of improving accuracy? We propose a new method, k-decay, just add an extra item to the commonly used and easy LR schedule(exp, cosine and polynomial), is effectively improves the performance of these schedule, also better than the state-of-the-art algorithms of LR shcedule such as SGDR, CLR and AutoLRS. In the k-decay, by adjusting the hyper-parameter \(k\), to generate different LR schedule, when k increases, the performance is improved. We evaluate the k-decay method on CIFAR And ImageNet datasets with different neural networks (ResNet, Wide ResNet). Our experiments show that this method can improve on most of them. The accuracy has been improved by 1.08\% on the CIFAR-10 dataset and by 2.07 \% on the CIFAR-100 dataset. On the ImageNet, accuracy is improved by 1.25\%. Our method is not only a general method to be applied other LR Shcedule, but also has no additional computational cost.



### Knowledge Distillation and Student-Teacher Learning for Visual Intelligence: A Review and New Outlooks
- **Arxiv ID**: http://arxiv.org/abs/2004.05937v7
- **DOI**: 10.1109/TPAMI.2021.3055564
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.05937v7)
- **Published**: 2020-04-13 13:45:38+00:00
- **Updated**: 2021-06-17 07:17:50+00:00
- **Authors**: Lin Wang, Kuk-Jin Yoon
- **Comment**: Accepted to IEEE Transactions on Pattern Analysis and Machine
  Intelligence(TPAMI),2021. Some references are updated in this version
- **Journal**: IEEE Transactions on Pattern Analysis and Machine Intelligence,
  2021
- **Summary**: Deep neural models in recent years have been successful in almost every field, including extremely complex problem statements. However, these models are huge in size, with millions (and even billions) of parameters, thus demanding more heavy computation power and failing to be deployed on edge devices. Besides, the performance boost is highly dependent on redundant labeled data. To achieve faster speeds and to handle the problems caused by the lack of data, knowledge distillation (KD) has been proposed to transfer information learned from one model to another. KD is often characterized by the so-called `Student-Teacher' (S-T) learning framework and has been broadly applied in model compression and knowledge transfer. This paper is about KD and S-T learning, which are being actively studied in recent years. First, we aim to provide explanations of what KD is and how/why it works. Then, we provide a comprehensive survey on the recent progress of KD methods together with S-T frameworks typically for vision tasks. In general, we consider some fundamental questions that have been driving this research area and thoroughly generalize the research progress and technical details. Additionally, we systematically analyze the research status of KD in vision applications. Finally, we discuss the potentials and open challenges of existing methods and prospect the future directions of KD and S-T learning.



### Dense Registration and Mosaicking of Fingerprints by Training an End-to-End Network
- **Arxiv ID**: http://arxiv.org/abs/2004.05972v1
- **DOI**: 10.1109/TIFS.2020.3017926
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.05972v1)
- **Published**: 2020-04-13 14:47:00+00:00
- **Updated**: 2020-04-13 14:47:00+00:00
- **Authors**: Zhe Cui, Jianjiang Feng, Jie Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: Dense registration of fingerprints is a challenging task due to elastic skin distortion, low image quality, and self-similarity of ridge pattern. To overcome the limitation of handcraft features, we propose to train an end-to-end network to directly output pixel-wise displacement field between two fingerprints. The proposed network includes a siamese network for feature embedding, and a following encoder-decoder network for regressing displacement field. By applying displacement fields reliably estimated by tracing high quality fingerprint videos to challenging fingerprints, we synthesize a large number of training fingerprint pairs with ground truth displacement fields. In addition, based on the proposed registration algorithm, we propose a fingerprint mosaicking method based on optimal seam selection. Registration and matching experiments on FVC2004 databases, Tsinghua Distorted Fingerprint (TDF) database, and NIST SD27 latent fingerprint database show that our registration method outperforms previous dense registration methods in accuracy and efficiency. Mosaicking experiment on FVC2004 DB1 demonstrates that the proposed algorithm produced higher quality fingerprints than other algorithms which also validates the performance of our registration algorithm.



### Speak2Label: Using Domain Knowledge for Creating a Large Scale Driver Gaze Zone Estimation Dataset
- **Arxiv ID**: http://arxiv.org/abs/2004.05973v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.05973v4)
- **Published**: 2020-04-13 14:47:34+00:00
- **Updated**: 2021-10-18 04:37:58+00:00
- **Authors**: Shreya Ghosh, Abhinav Dhall, Garima Sharma, Sarthak Gupta, Nicu Sebe
- **Comment**: None
- **Journal**: None
- **Summary**: Labelling of human behavior analysis data is a complex and time consuming task. In this paper, a fully automatic technique for labelling an image based gaze behavior dataset for driver gaze zone estimation is proposed. Domain knowledge is added to the data recording paradigm and later labels are generated in an automatic manner using Speech To Text conversion (STT). In order to remove the noise in the STT process due to different illumination and ethnicity of subjects in our data, the speech frequency and energy are analysed. The resultant Driver Gaze in the Wild (DGW) dataset contains 586 recordings, captured during different times of the day including evenings. The large scale dataset contains 338 subjects with an age range of 18-63 years. As the data is recorded in different lighting conditions, an illumination robust layer is proposed in the Convolutional Neural Network (CNN). The extensive experiments show the variance in the dataset resembling real-world conditions and the effectiveness of the proposed CNN pipeline. The proposed network is also fine-tuned for the eye gaze prediction task, which shows the discriminativeness of the representation learnt by our network on the proposed DGW dataset. Project Page: https://sites.google.com/view/drivergazeprediction/home



### A Survey of Single-Scene Video Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/2004.05993v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.05993v2)
- **Published**: 2020-04-13 15:09:50+00:00
- **Updated**: 2020-08-14 18:09:34+00:00
- **Authors**: Bharathkumar Ramachandra, Michael J. Jones, Ranga Raju Vatsavai
- **Comment**: None
- **Journal**: None
- **Summary**: This survey article summarizes research trends on the topic of anomaly detection in video feeds of a single scene. We discuss the various problem formulations, publicly available datasets and evaluation criteria. We categorize and situate past research into an intuitive taxonomy and provide a comprehensive comparison of the accuracy of many algorithms on standard test sets. Finally, we also provide best practices and suggest some possible directions for future research.



### Dynamic R-CNN: Towards High Quality Object Detection via Dynamic Training
- **Arxiv ID**: http://arxiv.org/abs/2004.06002v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.06002v2)
- **Published**: 2020-04-13 15:20:25+00:00
- **Updated**: 2020-07-26 07:28:39+00:00
- **Authors**: Hongkai Zhang, Hong Chang, Bingpeng Ma, Naiyan Wang, Xilin Chen
- **Comment**: Accepted by ECCV2020
- **Journal**: None
- **Summary**: Although two-stage object detectors have continuously advanced the state-of-the-art performance in recent years, the training process itself is far from crystal. In this work, we first point out the inconsistency problem between the fixed network settings and the dynamic training procedure, which greatly affects the performance. For example, the fixed label assignment strategy and regression loss function cannot fit the distribution change of proposals and thus are harmful to training high quality detectors. Consequently, we propose Dynamic R-CNN to adjust the label assignment criteria (IoU threshold) and the shape of regression loss function (parameters of SmoothL1 Loss) automatically based on the statistics of proposals during training. This dynamic design makes better use of the training samples and pushes the detector to fit more high quality samples. Specifically, our method improves upon ResNet-50-FPN baseline with 1.9% AP and 5.5% AP$_{90}$ on the MS COCO dataset with no extra overhead. Codes and models are available at https://github.com/hkzhang95/DynamicRCNN.



### Quantification of MagLIF morphology using the Mallat Scattering Transformation
- **Arxiv ID**: http://arxiv.org/abs/2005.01600v2
- **DOI**: 10.1063/5.0010781
- **Categories**: **physics.plasm-ph**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.01600v2)
- **Published**: 2020-04-13 15:41:14+00:00
- **Updated**: 2020-10-15 21:00:25+00:00
- **Authors**: Michael E. Glinsky, Thomas W. Moore, William E. Lewis, Matthew R. Weis, Christopher A. Jennings, David J. Ampleford, Patrick F. Knapp, Eric C. Harding, Matthew R. Gomez, Adam J. Harvey-Thompson
- **Comment**: 19 pages, 18 figures, 3 tables, 4 animations, accepted for
  publication in Physics of Plasmas; arXiv admin note: substantial text overlap
  with arXiv:1911.02359
- **Journal**: Phys. Plasmas 27, 112703 (2020)
- **Summary**: The morphology of the stagnated plasma resulting from Magnetized Liner Inertial Fusion (MagLIF) is measured by imaging the self-emission x-rays coming from the multi-keV plasma. Equivalent diagnostic response can be generated by integrated radiation-magnetohydrodynamic (rad-MHD) simulations from programs such as HYDRA and GORGON. There have been only limited quantitative ways to compare the image morphology, that is the texture, of simulations and experiments. We have developed a metric of image morphology based on the Mallat Scattering Transformation (MST), a transformation that has proved to be effective at distinguishing textures, sounds, and written characters. This metric is designed, demonstrated, and refined by classifying ensembles (i.e., classes) of synthetic stagnation images, and by regressing an ensemble of synthetic stagnation images to the morphology (i.e., model) parameters used to generate the synthetic images. We use this metric to quantitatively compare simulations to experimental images, experimental images to each other, and to estimate the morphological parameters of the experimental images with uncertainty. This coordinate space has proved very adept at doing a sophisticated relative background subtraction in the MST space. This was needed to compare the experimental self emission images to the rad-MHD simulation images.



### Compositional Visual Generation and Inference with Energy Based Models
- **Arxiv ID**: http://arxiv.org/abs/2004.06030v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.06030v3)
- **Published**: 2020-04-13 16:01:40+00:00
- **Updated**: 2020-12-17 09:26:00+00:00
- **Authors**: Yilun Du, Shuang Li, Igor Mordatch
- **Comment**: NeurIPS 2020 Spotlight; Website at
  https://energy-based-model.github.io/compositional-generation-inference/
- **Journal**: None
- **Summary**: A vital aspect of human intelligence is the ability to compose increasingly complex concepts out of simpler ideas, enabling both rapid learning and adaptation of knowledge. In this paper we show that energy-based models can exhibit this ability by directly combining probability distributions. Samples from the combined distribution correspond to compositions of concepts. For example, given a distribution for smiling faces, and another for male faces, we can combine them to generate smiling male faces. This allows us to generate natural images that simultaneously satisfy conjunctions, disjunctions, and negations of concepts. We evaluate compositional generation abilities of our model on the CelebA dataset of natural faces and synthetic 3D scene images. We also demonstrate other unique advantages of our model, such as the ability to continually learn and incorporate new concepts, or infer compositions of concept properties underlying an image.



### Learning a low dimensional manifold of real cancer tissue with PathologyGAN
- **Arxiv ID**: http://arxiv.org/abs/2004.06517v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.06517v1)
- **Published**: 2020-04-13 16:18:00+00:00
- **Updated**: 2020-04-13 16:18:00+00:00
- **Authors**: Adalberto Claudio Quiros, Roderick Murray-Smith, Ke Yuan
- **Comment**: None
- **Journal**: None
- **Summary**: Application of deep learning in digital pathology shows promise on improving disease diagnosis and understanding. We present a deep generative model that learns to simulate high-fidelity cancer tissue images while mapping the real images onto an interpretable low dimensional latent space. The key to the model is an encoder trained by a previously developed generative adversarial network, PathologyGAN. We study the latent space using 249K images from two breast cancer cohorts. We find that the latent space encodes morphological characteristics of tissues (e.g. patterns of cancer, lymphocytes, and stromal cells). In addition, the latent space reveals distinctly enriched clusters of tissue architectures in the high-risk patient group.



### Adversarial Style Mining for One-Shot Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2004.06042v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.06042v1)
- **Published**: 2020-04-13 16:18:46+00:00
- **Updated**: 2020-04-13 16:18:46+00:00
- **Authors**: Yawei Luo, Ping Liu, Tao Guan, Junqing Yu, Yi Yang
- **Comment**: Preprint
- **Journal**: None
- **Summary**: We aim at the problem named One-Shot Unsupervised Domain Adaptation. Unlike traditional Unsupervised Domain Adaptation, it assumes that only one unlabeled target sample can be available when learning to adapt. This setting is realistic but more challenging, in which conventional adaptation approaches are prone to failure due to the scarce of unlabeled target data. To this end, we propose a novel Adversarial Style Mining approach, which combines the style transfer module and task-specific module into an adversarial manner. Specifically, the style transfer module iteratively searches for harder stylized images around the one-shot target sample according to the current learning state, leading the task model to explore the potential styles that are difficult to solve in the almost unseen target domain, thus boosting the adaptation performance in a data-scarce scenario. The adversarial learning framework makes the style transfer module and task-specific module benefit each other during the competition. Extensive experiments on both cross-domain classification and segmentation benchmarks verify that ASM achieves state-of-the-art adaptation performance under the challenging one-shot setting.



### SpeedNet: Learning the Speediness in Videos
- **Arxiv ID**: http://arxiv.org/abs/2004.06130v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.06130v2)
- **Published**: 2020-04-13 18:00:27+00:00
- **Updated**: 2020-07-26 14:33:04+00:00
- **Authors**: Sagie Benaim, Ariel Ephrat, Oran Lang, Inbar Mosseri, William T. Freeman, Michael Rubinstein, Michal Irani, Tali Dekel
- **Comment**: Accepted to CVPR 2020 (oral). Project webpage:
  http://speednet-cvpr20.github.io
- **Journal**: None
- **Summary**: We wish to automatically predict the "speediness" of moving objects in videos---whether they move faster, at, or slower than their "natural" speed. The core component in our approach is SpeedNet---a novel deep network trained to detect if a video is playing at normal rate, or if it is sped up. SpeedNet is trained on a large corpus of natural videos in a self-supervised manner, without requiring any manual annotations. We show how this single, binary classification network can be used to detect arbitrary rates of speediness of objects. We demonstrate prediction results by SpeedNet on a wide range of videos containing complex natural motions, and examine the visual cues it utilizes for making those predictions. Importantly, we show that through predicting the speed of videos, the model learns a powerful and meaningful space-time representation that goes beyond simple motion cues. We demonstrate how those learned features can boost the performance of self-supervised action recognition, and can be used for video retrieval. Furthermore, we also apply SpeedNet for generating time-varying, adaptive video speedups, which can allow viewers to watch videos faster, but with less of the jittery, unnatural motions typical to videos that are sped up uniformly.



### An Efficient UAV-based Artificial Intelligence Framework for Real-Time Visual Tasks
- **Arxiv ID**: http://arxiv.org/abs/2004.06154v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2004.06154v1)
- **Published**: 2020-04-13 18:53:12+00:00
- **Updated**: 2020-04-13 18:53:12+00:00
- **Authors**: Enkhtogtokh Togootogtokh, Christian Micheloni, Gian Luca Foresti, Niki Martinel
- **Comment**: None
- **Journal**: None
- **Summary**: Modern Unmanned Aerial Vehicles equipped with state of the art artificial intelligence (AI) technologies are opening to a wide plethora of novel and interesting applications. While this field received a strong impact from the recent AI breakthroughs, most of the provided solutions either entirely rely on commercial software or provide a weak integration interface which denies the development of additional techniques. This leads us to propose a novel and efficient framework for the UAV-AI joint technology. Intelligent UAV systems encounter complex challenges to be tackled without human control. One of these complex challenges is to be able to carry out computer vision tasks in real-time use cases. In this paper we focus on this challenge and introduce a multi-layer AI (MLAI) framework to allow easy integration of ad-hoc visual-based AI applications. To show its features and its advantages, we implemented and evaluated different modern visual-based deep learning models for object detection, target tracking and target handover.



### Blind Quality Assessment for Image Superresolution Using Deep Two-Stream Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/2004.06163v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2004.06163v1)
- **Published**: 2020-04-13 19:14:28+00:00
- **Updated**: 2020-04-13 19:14:28+00:00
- **Authors**: Wei Zhou, Qiuping Jiang, Yuwang Wang, Zhibo Chen, Weiping Li
- **Comment**: None
- **Journal**: None
- **Summary**: Numerous image superresolution (SR) algorithms have been proposed for reconstructing high-resolution (HR) images from input images with lower spatial resolutions. However, effectively evaluating the perceptual quality of SR images remains a challenging research problem. In this paper, we propose a no-reference/blind deep neural network-based SR image quality assessor (DeepSRQ). To learn more discriminative feature representations of various distorted SR images, the proposed DeepSRQ is a two-stream convolutional network including two subcomponents for distorted structure and texture SR images. Different from traditional image distortions, the artifacts of SR images cause both image structure and texture quality degradation. Therefore, we choose the two-stream scheme that captures different properties of SR inputs instead of directly learning features from one image stream. Considering the human visual system (HVS) characteristics, the structure stream focuses on extracting features in structural degradations, while the texture stream focuses on the change in textural distributions. In addition, to augment the training data and ensure the category balance, we propose a stride-based adaptive cropping approach for further improvement. Experimental results on three publicly available SR image quality databases demonstrate the effectiveness and generalization ability of our proposed DeepSRQ method compared with state-of-the-art image quality assessment algorithms.



### Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks
- **Arxiv ID**: http://arxiv.org/abs/2004.06165v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.IR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.06165v5)
- **Published**: 2020-04-13 19:18:10+00:00
- **Updated**: 2020-07-26 00:46:46+00:00
- **Authors**: Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, Yejin Choi, Jianfeng Gao
- **Comment**: ECCV 2020, Code and pre-trained models are released:
  https://github.com/microsoft/Oscar
- **Journal**: None
- **Summary**: Large-scale pre-training methods of learning cross-modal representations on image-text pairs are becoming popular for vision-language tasks. While existing methods simply concatenate image region features and text features as input to the model to be pre-trained and use self-attention to learn image-text semantic alignments in a brute force manner, in this paper, we propose a new learning method Oscar (Object-Semantics Aligned Pre-training), which uses object tags detected in images as anchor points to significantly ease the learning of alignments. Our method is motivated by the observation that the salient objects in an image can be accurately detected, and are often mentioned in the paired text. We pre-train an Oscar model on the public corpus of 6.5 million text-image pairs, and fine-tune it on downstream tasks, creating new state-of-the-arts on six well-established vision-language understanding and generation tasks.



### Event detection in coarsely annotated sports videos via parallel multi receptive field 1D convolutions
- **Arxiv ID**: http://arxiv.org/abs/2004.06172v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.06172v1)
- **Published**: 2020-04-13 19:51:25+00:00
- **Updated**: 2020-04-13 19:51:25+00:00
- **Authors**: Kanav Vats, Mehrnaz Fani, Pascale Walters, David A. Clausi, John Zelek
- **Comment**: None
- **Journal**: None
- **Summary**: In problems such as sports video analytics, it is difficult to obtain accurate frame level annotations and exact event duration because of the lengthy videos and sheer volume of video data. This issue is even more pronounced in fast-paced sports such as ice hockey. Obtaining annotations on a coarse scale can be much more practical and time efficient. We propose the task of event detection in coarsely annotated videos. We introduce a multi-tower temporal convolutional network architecture for the proposed task. The network, with the help of multiple receptive fields, processes information at various temporal scales to account for the uncertainty with regard to the exact event location and duration. We demonstrate the effectiveness of the multi-receptive field architecture through appropriate ablation studies. The method is evaluated on two tasks - event detection in coarsely annotated hockey videos in the NHL dataset and event spotting in soccer on the SoccerNet dataset. The two datasets lack frame-level annotations and have very distinct event frequencies. Experimental results demonstrate the effectiveness of the network by obtaining a 55% average F1 score on the NHL dataset and by achieving competitive performance compared to the state of the art on the SoccerNet dataset. We believe our approach will help develop more practical pipelines for event detection in sports video.



### Challenges and Opportunities for Computer Vision in Real-life Soccer Analytics
- **Arxiv ID**: http://arxiv.org/abs/2004.06180v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.06180v1)
- **Published**: 2020-04-13 20:06:23+00:00
- **Updated**: 2020-04-13 20:06:23+00:00
- **Authors**: Neha Bhargava, Fabio Cuzzolin
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we explore some of the applications of computer vision to sports analytics. Sport analytics deals with understanding and discovering patterns from a corpus of sports data. Analysing such data provides important performance metrics for the players, for instance in soccer matches, that could be useful for estimating their fitness and strengths. Team level statistics can also be estimated from such analysis. This paper mainly focuses on some the challenges and opportunities presented by sport video analysis in computer vision. Specifically, we use our multi-camera setup as a framework to discuss some of the real-life challenges for machine learning algorithms.



### Relation Transformer Network
- **Arxiv ID**: http://arxiv.org/abs/2004.06193v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.06193v2)
- **Published**: 2020-04-13 20:47:01+00:00
- **Updated**: 2021-07-20 21:10:56+00:00
- **Authors**: Rajat Koner, Suprosanna Shit, Volker Tresp
- **Comment**: None
- **Journal**: None
- **Summary**: The extraction of a scene graph with objects as nodes and mutual relationships as edges is the basis for a deep understanding of image content. Despite recent advances, such as message passing and joint classification, the detection of visual relationships remains a challenging task due to sub-optimal exploration of the mutual interaction among the visual objects. In this work, we propose a novel transformer formulation for scene graph generation and relation prediction. We leverage the encoder-decoder architecture of the transformer for rich feature embedding of nodes and edges. Specifically, we model the node-to-node interaction with the self-attention of the transformer encoder and the edge-to-node interaction with the cross-attention of the transformer decoder. Further, we introduce a novel positional embedding suitable to handle edges in the decoder. Finally, our relation prediction module classifies the directed relation from the learned node and edge embedding. We name this architecture as Relation Transformer Network (RTN). On the Visual Genome and GQA dataset, we have achieved an overall mean of 4.85% and 3.1% point improvement in comparison with state-of-the-art methods. Our experiments show that Relation Transformer can efficiently model context across various datasets with small, medium, and large-scale relation classification.



### Embedded Large-Scale Handwritten Chinese Character Recognition
- **Arxiv ID**: http://arxiv.org/abs/2004.06209v1
- **DOI**: 10.1109/ICASSP40776.2020.9053084
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.06209v1)
- **Published**: 2020-04-13 21:21:34+00:00
- **Updated**: 2020-04-13 21:21:34+00:00
- **Authors**: Youssouf Chherawala, Hans J. G. A. Dolfing, Ryan S. Dixon, Jerome R. Bellegarda
- **Comment**: 5 pages, 7 figures
- **Journal**: None
- **Summary**: As handwriting input becomes more prevalent, the large symbol inventory required to support Chinese handwriting recognition poses unique challenges. This paper describes how the Apple deep learning recognition system can accurately handle up to 30,000 Chinese characters while running in real-time across a range of mobile devices. To achieve acceptable accuracy, we paid particular attention to data collection conditions, representativeness of writing styles, and training regimen. We found that, with proper care, even larger inventories are within reach. Our experiments show that accuracy only degrades slowly as the inventory increases, as long as we use training data of sufficient quality and in sufficient quantity.



### Imitation Learning for Fashion Style Based on Hierarchical Multimodal Representation
- **Arxiv ID**: http://arxiv.org/abs/2004.06229v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.06229v1)
- **Published**: 2020-04-13 23:02:25+00:00
- **Updated**: 2020-04-13 23:02:25+00:00
- **Authors**: Shizhu Liu, Shanglin Yang, Hui Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: Fashion is a complex social phenomenon. People follow fashion styles from demonstrations by experts or fashion icons. However, for machine agent, learning to imitate fashion experts from demonstrations can be challenging, especially for complex styles in environments with high-dimensional, multimodal observations. Most existing research regarding fashion outfit composition utilizes supervised learning methods to mimic the behaviors of style icons. These methods suffer from distribution shift: because the agent greedily imitates some given outfit demonstrations, it can drift away from one style to another styles given subtle differences. In this work, we propose an adversarial inverse reinforcement learning formulation to recover reward functions based on hierarchical multimodal representation (HM-AIRL) during the imitation process. The hierarchical joint representation can more comprehensively model the expert composited outfit demonstrations to recover the reward function. We demonstrate that the proposed HM-AIRL model is able to recover reward functions that are robust to changes in multimodal observations, enabling us to learn policies under significant variation between different styles.



### VoxelPose: Towards Multi-Camera 3D Human Pose Estimation in Wild Environment
- **Arxiv ID**: http://arxiv.org/abs/2004.06239v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.06239v4)
- **Published**: 2020-04-13 23:50:01+00:00
- **Updated**: 2020-08-24 11:01:32+00:00
- **Authors**: Hanyue Tu, Chunyu Wang, Wenjun Zeng
- **Comment**: None
- **Journal**: None
- **Summary**: We present an approach to estimate 3D poses of multiple people from multiple camera views. In contrast to the previous efforts which require to establish cross-view correspondence based on noisy and incomplete 2D pose estimations, we present an end-to-end solution which directly operates in the $3$D space, therefore avoids making incorrect decisions in the 2D space. To achieve this goal, the features in all camera views are warped and aggregated in a common 3D space, and fed into Cuboid Proposal Network (CPN) to coarsely localize all people. Then we propose Pose Regression Network (PRN) to estimate a detailed 3D pose for each proposal. The approach is robust to occlusion which occurs frequently in practice. Without bells and whistles, it outperforms the state-of-the-arts on the public datasets. Code will be released at https://github.com/microsoft/multiperson-pose-estimation-pytorch.



