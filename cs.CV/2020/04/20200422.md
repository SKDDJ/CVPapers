# Arxiv Papers in cs.CV on 2020-04-22
### Weakly Supervised Learning Guided by Activation Mapping Applied to a Novel Citrus Pest Benchmark
- **Arxiv ID**: http://arxiv.org/abs/2004.11252v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.11252v1)
- **Published**: 2020-04-22 01:26:50+00:00
- **Updated**: 2020-04-22 01:26:50+00:00
- **Authors**: Edson Bollis, Helio Pedrini, Sandra Avila
- **Comment**: Accepted to The 1st International Workshop on Agriculture-Vision
  Workshop - CVPR 2020
- **Journal**: None
- **Summary**: Pests and diseases are relevant factors for production losses in agriculture and, therefore, promote a huge investment in the prevention and detection of its causative agents. In many countries, Integrated Pest Management is the most widely used process to prevent and mitigate the damages caused by pests and diseases in citrus crops. However, its results are credited by humans who visually inspect the orchards in order to identify the disease symptoms, insects and mite pests. In this context, we design a weakly supervised learning process guided by saliency maps to automatically select regions of interest in the images, significantly reducing the annotation task. In addition, we create a large citrus pest benchmark composed of positive samples (six classes of mite species) and negative samples. Experiments conducted on two large datasets demonstrate that our results are very promising for the problem of pest and disease classification in the agriculture field.



### Yoga-82: A New Dataset for Fine-grained Classification of Human Poses
- **Arxiv ID**: http://arxiv.org/abs/2004.10362v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.10362v1)
- **Published**: 2020-04-22 01:43:44+00:00
- **Updated**: 2020-04-22 01:43:44+00:00
- **Authors**: Manisha Verma, Sudhakar Kumawat, Yuta Nakashima, Shanmuganathan Raman
- **Comment**: Accepted CVPR Workshops 2020
- **Journal**: None
- **Summary**: Human pose estimation is a well-known problem in computer vision to locate joint positions. Existing datasets for the learning of poses are observed to be not challenging enough in terms of pose diversity, object occlusion, and viewpoints. This makes the pose annotation process relatively simple and restricts the application of the models that have been trained on them. To handle more variety in human poses, we propose the concept of fine-grained hierarchical pose classification, in which we formulate the pose estimation as a classification task, and propose a dataset, Yoga-82, for large-scale yoga pose recognition with 82 classes. Yoga-82 consists of complex poses where fine annotations may not be possible. To resolve this, we provide hierarchical labels for yoga poses based on the body configuration of the pose. The dataset contains a three-level hierarchy including body positions, variations in body positions, and the actual pose names. We present the classification accuracy of the state-of-the-art convolutional neural network architectures on Yoga-82. We also present several hierarchical variants of DenseNet in order to utilize the hierarchical labels.



### Automatic exposure selection and fusion for high-dynamic-range photography via smartphones
- **Arxiv ID**: http://arxiv.org/abs/2004.10365v1
- **DOI**: 10.1007/s11760-017-1104-9
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.10365v1)
- **Published**: 2020-04-22 01:48:17+00:00
- **Updated**: 2020-04-22 01:48:17+00:00
- **Authors**: Reza Pourreza, Nasser Kehtarnavaz
- **Comment**: None
- **Journal**: Signal, Image and Video Processing volume 11, pages 1437-1444
  (2017)
- **Summary**: High-dynamic-range (HDR) photography involves fusing a bracket of images taken at different exposure settings in order to compensate for the low dynamic range of digital cameras such as the ones used in smartphones. In this paper, a method for automatically selecting the exposure settings of such images is introduced based on the camera characteristic function. In addition, a new fusion method is introduced based on an optimization formulation and weighted averaging. Both of these methods are implemented on a smartphone platform as an HDR app to demonstrate the practicality of the introduced methods. Comparison results with several existing methods are presented indicating the effectiveness as well as the computational efficiency of the introduced solution.



### Graph-based Kinship Reasoning Network
- **Arxiv ID**: http://arxiv.org/abs/2004.10375v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.10375v1)
- **Published**: 2020-04-22 02:55:38+00:00
- **Updated**: 2020-04-22 02:55:38+00:00
- **Authors**: Wanhua Li, Yingqiang Zhang, Kangchen Lv, Jiwen Lu, Jianjiang Feng, Jie Zhou
- **Comment**: Accepted to ICME 2020(IEEE International Conference on Multimedia &
  Expo 2020) as an Oral Presentation
- **Journal**: None
- **Summary**: In this paper, we propose a graph-based kinship reasoning (GKR) network for kinship verification, which aims to effectively perform relational reasoning on the extracted features of an image pair. Unlike most existing methods which mainly focus on how to learn discriminative features, our method considers how to compare and fuse the extracted feature pair to reason about the kin relations. The proposed GKR constructs a star graph called kinship relational graph where each peripheral node represents the information comparison in one feature dimension and the central node is used as a bridge for information communication among peripheral nodes. Then the GKR performs relational reasoning on this graph with recursive message passing. Extensive experimental results on the KinFaceW-I and KinFaceW-II datasets show that the proposed GKR outperforms the state-of-the-art methods.



### Towards Real-Time DNN Inference on Mobile Platforms with Model Pruning and Compiler Optimization
- **Arxiv ID**: http://arxiv.org/abs/2004.11250v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2004.11250v1)
- **Published**: 2020-04-22 03:18:23+00:00
- **Updated**: 2020-04-22 03:18:23+00:00
- **Authors**: Wei Niu, Pu Zhao, Zheng Zhan, Xue Lin, Yanzhi Wang, Bin Ren
- **Comment**: accepted by the IJCAI-PRICAI 2020 Demonstrations Track
- **Journal**: None
- **Summary**: High-end mobile platforms rapidly serve as primary computing devices for a wide range of Deep Neural Network (DNN) applications. However, the constrained computation and storage resources on these devices still pose significant challenges for real-time DNN inference executions. To address this problem, we propose a set of hardware-friendly structured model pruning and compiler optimization techniques to accelerate DNN executions on mobile devices. This demo shows that these optimizations can enable real-time mobile execution of multiple DNN applications, including style transfer, DNN coloring and super resolution.



### Image Processing Failure and Deep Learning Success in Lawn Measurement
- **Arxiv ID**: http://arxiv.org/abs/2004.10382v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.10382v1)
- **Published**: 2020-04-22 03:20:16+00:00
- **Updated**: 2020-04-22 03:20:16+00:00
- **Authors**: J. Wilkins, M. V. Nguyen, B. Rahmani
- **Comment**: None
- **Journal**: None
- **Summary**: Lawn area measurement is an application of image processing and deep learning. Researchers have been used hierarchical networks, segmented images and many other methods to measure lawn area. Methods effectiveness and accuracy varies. In this project Image processing and deep learning methods has been compared to find the best way to measure the lawn area. Three Image processing methods using OpenCV has been compared to Convolutional Neural network which is one of the most famous and effective deep learning methods. We used Keras and TensorFlow to estimate the lawn area. Convolutional Neural Network or shortly CNN shows very high accuracy (94-97%) for this purpose. In image processing methods, Thresholding with 80-87% accuracy and Edge detection are effective methods to measure the lawn area but Contouring with 26-31% accuracy does not calculate the lawn area successfully. We may conclude that deep learning methods especially CNN could be the best detective method comparing to image processing and other deep learning techniques.



### Recursive Social Behavior Graph for Trajectory Prediction
- **Arxiv ID**: http://arxiv.org/abs/2004.10402v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.10402v1)
- **Published**: 2020-04-22 06:01:48+00:00
- **Updated**: 2020-04-22 06:01:48+00:00
- **Authors**: Jianhua Sun, Qinhong Jiang, Cewu Lu
- **Comment**: None
- **Journal**: None
- **Summary**: Social interaction is an important topic in human trajectory prediction to generate plausible paths. In this paper, we present a novel insight of group-based social interaction model to explore relationships among pedestrians. We recursively extract social representations supervised by group-based annotations and formulate them into a social behavior graph, called Recursive Social Behavior Graph. Our recursive mechanism explores the representation power largely. Graph Convolutional Neural Network then is used to propagate social interaction information in such a graph. With the guidance of Recursive Social Behavior Graph, we surpass state-of-the-art method on ETH and UCY dataset for 11.1% in ADE and 10.8% in FDE in average, and successfully predict complex social behaviors.



### Inf-Net: Automatic COVID-19 Lung Infection Segmentation from CT Images
- **Arxiv ID**: http://arxiv.org/abs/2004.14133v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.14133v4)
- **Published**: 2020-04-22 07:30:56+00:00
- **Updated**: 2020-05-21 18:23:37+00:00
- **Authors**: Deng-Ping Fan, Tao Zhou, Ge-Peng Ji, Yi Zhou, Geng Chen, Huazhu Fu, Jianbing Shen, Ling Shao
- **Comment**: To appear in IEEE TMI. The code is released in:
  https://github.com/DengPingFan/Inf-Net
- **Journal**: None
- **Summary**: Coronavirus Disease 2019 (COVID-19) spread globally in early 2020, causing the world to face an existential health crisis. Automated detection of lung infections from computed tomography (CT) images offers a great potential to augment the traditional healthcare strategy for tackling COVID-19. However, segmenting infected regions from CT slices faces several challenges, including high variation in infection characteristics, and low intensity contrast between infections and normal tissues. Further, collecting a large amount of data is impractical within a short time period, inhibiting the training of a deep model. To address these challenges, a novel COVID-19 Lung Infection Segmentation Deep Network (Inf-Net) is proposed to automatically identify infected regions from chest CT slices. In our Inf-Net, a parallel partial decoder is used to aggregate the high-level features and generate a global map. Then, the implicit reverse attention and explicit edge-attention are utilized to model the boundaries and enhance the representations. Moreover, to alleviate the shortage of labeled data, we present a semi-supervised segmentation framework based on a randomly selected propagation strategy, which only requires a few labeled images and leverages primarily unlabeled data. Our semi-supervised framework can improve the learning ability and achieve a higher performance. Extensive experiments on our COVID-SemiSeg and real CT volumes demonstrate that the proposed Inf-Net outperforms most cutting-edge segmentation models and advances the state-of-the-art performance.



### A Cycle GAN Approach for Heterogeneous Domain Adaptation in Land Use Classification
- **Arxiv ID**: http://arxiv.org/abs/2004.11245v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.11245v1)
- **Published**: 2020-04-22 08:16:18+00:00
- **Updated**: 2020-04-22 08:16:18+00:00
- **Authors**: Claire Voreiter, Jean-Christophe Burnel, Pierre Lassalle, Marc Spigai, Romain Hugues, Nicolas Courty
- **Comment**: None
- **Journal**: None
- **Summary**: In the field of remote sensing and more specifically in Earth Observation, new data are available every day, coming from different sensors. Leveraging on those data in classification tasks comes at the price of intense labelling tasks that are not realistic in operational settings. While domain adaptation could be useful to counterbalance this problem, most of the usual methods assume that the data to adapt are comparable (they belong to the same metric space), which is not the case when multiple sensors are at stake. Heterogeneous domain adaptation methods are a particular solution to this problem. We present a novel method to deal with such cases, based on a modified cycleGAN version that incorporates classification losses and a metric space alignment term. We demonstrate its power on a land use classification tasks, with images from both Google Earth and Sentinel-2.



### Learning an Adaptive Model for Extreme Low-light Raw Image Processing
- **Arxiv ID**: http://arxiv.org/abs/2004.10447v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.10447v1)
- **Published**: 2020-04-22 09:01:07+00:00
- **Updated**: 2020-04-22 09:01:07+00:00
- **Authors**: Qingxu Fu, Xiaoguang Di, Yu Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Low-light images suffer from severe noise and low illumination. Current deep learning models that are trained with real-world images have excellent noise reduction, but a ratio parameter must be chosen manually to complete the enhancement pipeline. In this work, we propose an adaptive low-light raw image enhancement network to avoid parameter-handcrafting and to improve image quality. The proposed method can be divided into two sub-models: Brightness Prediction (BP) and Exposure Shifting (ES). The former is designed to control the brightness of the resulting image by estimating a guideline exposure time $t_1$. The latter learns to approximate an exposure-shifting operator $ES$, converting a low-light image with real exposure time $t_0$ to a noise-free image with guideline exposure time $t_1$. Additionally, structural similarity (SSIM) loss and Image Enhancement Vector (IEV) are introduced to promote image quality, and a new Campus Image Dataset (CID) is proposed to overcome the limitations of the existing datasets and to supervise the training of the proposed model. Using the proposed model, we can achieve high-quality low-light image enhancement from a single raw image. In quantitative tests, it is shown that the proposed method has the lowest Noise Level Estimation (NLE) score compared with the state-of-the-art low-light algorithms, suggesting a superior denoising performance. Furthermore, those tests illustrate that the proposed method is able to adaptively control the global image brightness according to the content of the image scene. Lastly, the potential application in video processing is briefly discussed.



### DeepFake Detection by Analyzing Convolutional Traces
- **Arxiv ID**: http://arxiv.org/abs/2004.10448v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.10448v1)
- **Published**: 2020-04-22 09:02:55+00:00
- **Updated**: 2020-04-22 09:02:55+00:00
- **Authors**: Luca Guarnera, Oliver Giudice, Sebastiano Battiato
- **Comment**: None
- **Journal**: IEEE Conference on Computer Vision and Pattern Recognition
  Workshops 2020
- **Summary**: The Deepfake phenomenon has become very popular nowadays thanks to the possibility to create incredibly realistic images using deep learning tools, based mainly on ad-hoc Generative Adversarial Networks (GAN). In this work we focus on the analysis of Deepfakes of human faces with the objective of creating a new detection method able to detect a forensics trace hidden in images: a sort of fingerprint left in the image generation process. The proposed technique, by means of an Expectation Maximization (EM) algorithm, extracts a set of local features specifically addressed to model the underlying convolutional generative process. Ad-hoc validation has been employed through experimental tests with naive classifiers on five different architectures (GDWCT, STARGAN, ATTGAN, STYLEGAN, STYLEGAN2) against the CELEBA dataset as ground-truth for non-fakes. Results demonstrated the effectiveness of the technique in distinguishing the different architectures and the corresponding generation process.



### Smart Attendance System Usign CNN
- **Arxiv ID**: http://arxiv.org/abs/2004.14289v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.14289v1)
- **Published**: 2020-04-22 09:04:33+00:00
- **Updated**: 2020-04-22 09:04:33+00:00
- **Authors**: Shailesh Arya, Hrithik Mesariya, Vishal Parekh
- **Comment**: 4 Pages, 9 Figures
- **Journal**: None
- **Summary**: The research on the attendance system has been going for a very long time, numerous arrangements have been proposed in the last decade to make this system efficient and less time consuming, but all those systems have several flaws. In this paper, we are introducing a smart and efficient system for attendance using face detection and face recognition. This system can be used to take attendance in colleges or offices using real-time face recognition with the help of the Convolution Neural Network(CNN). The conventional methods like Eigenfaces and Fisher faces are sensitive to lighting, noise, posture, obstruction, illumination etc. Hence, we have used CNN to recognize the face and overcome such difficulties. The attendance records will be updated automatically and stored in an excel sheet as well as in a database. We have used MongoDB as a backend database for attendance records.



### Warwick Image Forensics Dataset for Device Fingerprinting In Multimedia Forensics
- **Arxiv ID**: http://arxiv.org/abs/2004.10469v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.MM, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.10469v2)
- **Published**: 2020-04-22 09:54:27+00:00
- **Updated**: 2020-05-07 10:55:05+00:00
- **Authors**: Yijun Quan, Chang-Tsun Li, Yujue Zhou, Li Li
- **Comment**: Paper accepted to IEEE International Conference on Multimedia and
  Expo 2020 (ICME 2020)
- **Journal**: None
- **Summary**: Device fingerprints like sensor pattern noise (SPN) are widely used for provenance analysis and image authentication. Over the past few years, the rapid advancement in digital photography has greatly reshaped the pipeline of image capturing process on consumer-level mobile devices. The flexibility of camera parameter settings and the emergence of multi-frame photography algorithms, especially high dynamic range (HDR) imaging, bring new challenges to device fingerprinting. The subsequent study on these topics requires a new purposefully built image dataset. In this paper, we present the Warwick Image Forensics Dataset, an image dataset of more than 58,600 images captured using 14 digital cameras with various exposure settings. Special attention to the exposure settings allows the images to be adopted by different multi-frame computational photography algorithms and for subsequent device fingerprinting. The dataset is released as an open-source, free for use for the digital forensic community.



### Graph Convolutional Subspace Clustering: A Robust Subspace Clustering Framework for Hyperspectral Image
- **Arxiv ID**: http://arxiv.org/abs/2004.10476v1
- **DOI**: 10.1109/TGRS.2020.3018135
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.10476v1)
- **Published**: 2020-04-22 10:09:19+00:00
- **Updated**: 2020-04-22 10:09:19+00:00
- **Authors**: Yaoming Cai, Zijia Zhang, Zhihua Cai, Xiaobo Liu, Xinwei Jiang, Qin Yan
- **Comment**: This paper is submitted to IEEE TGRS
- **Journal**: None
- **Summary**: Hyperspectral image (HSI) clustering is a challenging task due to the high complexity of HSI data. Subspace clustering has been proven to be powerful for exploiting the intrinsic relationship between data points. Despite the impressive performance in the HSI clustering, traditional subspace clustering methods often ignore the inherent structural information among data. In this paper, we revisit the subspace clustering with graph convolution and present a novel subspace clustering framework called Graph Convolutional Subspace Clustering (GCSC) for robust HSI clustering. Specifically, the framework recasts the self-expressiveness property of the data into the non-Euclidean domain, which results in a more robust graph embedding dictionary. We show that traditional subspace clustering models are the special forms of our framework with the Euclidean data. Basing on the framework, we further propose two novel subspace clustering models by using the Frobenius norm, namely Efficient GCSC (EGCSC) and Efficient Kernel GCSC (EKGCSC). Both models have a globally optimal closed-form solution, which makes them easier to implement, train, and apply in practice. Extensive experiments on three popular HSI datasets demonstrate that EGCSC and EKGCSC can achieve state-of-the-art clustering performance and dramatically outperforms many existing methods with significant margins.



### SensitiveLoss: Improving Accuracy and Fairness of Face Representations with Discrimination-Aware Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2004.11246v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/2004.11246v2)
- **Published**: 2020-04-22 10:32:16+00:00
- **Updated**: 2020-12-02 16:22:01+00:00
- **Authors**: Ignacio Serna, Aythami Morales, Julian Fierrez, Manuel Cebrian, Nick Obradovich, Iyad Rahwan
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1912.01842
- **Journal**: None
- **Summary**: We propose a discrimination-aware learning method to improve both accuracy and fairness of biased face recognition algorithms. The most popular face recognition benchmarks assume a distribution of subjects without paying much attention to their demographic attributes. In this work, we perform a comprehensive discrimination-aware experimentation of deep learning-based face recognition. We also propose a general formulation of algorithmic discrimination with application to face biometrics. The experiments include tree popular face recognition models and three public databases composed of 64,000 identities from different demographic groups characterized by gender and ethnicity. We experimentally show that learning processes based on the most used face databases have led to popular pre-trained deep face models that present a strong algorithmic discrimination. We finally propose a discrimination-aware learning method, Sensitive Loss, based on the popular triplet loss function and a sensitive triplet generator. Our approach works as an add-on to pre-trained networks and is used to improve their performance in terms of average accuracy and fairness. The method shows results comparable to state-of-the-art de-biasing networks and represents a step forward to prevent discriminatory effects by automatic systems.



### Understanding Integrated Gradients with SmoothTaylor for Deep Neural Network Attribution
- **Arxiv ID**: http://arxiv.org/abs/2004.10484v2
- **DOI**: 10.1109/ICPR48806.2021.9413242
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.10484v2)
- **Published**: 2020-04-22 10:43:19+00:00
- **Updated**: 2021-09-02 17:57:56+00:00
- **Authors**: Gary S. W. Goh, Sebastian Lapuschkin, Leander Weber, Wojciech Samek, Alexander Binder
- **Comment**: 8 pages, 3 figures. Accepted in 25th International Conference on
  Pattern Recognition, (ICPR) 2020. In Proceedings: pp. 4949-4956
- **Journal**: None
- **Summary**: Integrated Gradients as an attribution method for deep neural network models offers simple implementability. However, it suffers from noisiness of explanations which affects the ease of interpretability. The SmoothGrad technique is proposed to solve the noisiness issue and smoothen the attribution maps of any gradient-based attribution method. In this paper, we present SmoothTaylor as a novel theoretical concept bridging Integrated Gradients and SmoothGrad, from the Taylor's theorem perspective. We apply the methods to the image classification problem, using the ILSVRC2012 ImageNet object recognition dataset, and a couple of pretrained image models to generate attribution maps. These attribution maps are empirically evaluated using quantitative measures for sensitivity and noise level. We further propose adaptive noising to optimize for the noise scale hyperparameter value. From our experiments, we find that the SmoothTaylor approach together with adaptive noising is able to generate better quality saliency maps with lesser noise and higher sensitivity to the relevant points in the input space as compared to Integrated Gradients.



### Stabilizing Training of Generative Adversarial Nets via Langevin Stein Variational Gradient Descent
- **Arxiv ID**: http://arxiv.org/abs/2004.10495v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.10495v1)
- **Published**: 2020-04-22 11:20:04+00:00
- **Updated**: 2020-04-22 11:20:04+00:00
- **Authors**: Dong Wang, Xiaoqian Qin, Fengyi Song, Li Cheng
- **Comment**: None
- **Journal**: None
- **Summary**: Generative adversarial networks (GANs), famous for the capability of learning complex underlying data distribution, are however known to be tricky in the training process, which would probably result in mode collapse or performance deterioration. Current approaches of dealing with GANs' issues almost utilize some practical training techniques for the purpose of regularization, which on the other hand undermines the convergence and theoretical soundness of GAN. In this paper, we propose to stabilize GAN training via a novel particle-based variational inference -- Langevin Stein variational gradient descent (LSVGD), which not only inherits the flexibility and efficiency of original SVGD but aims to address its instability issues by incorporating an extra disturbance into the update dynamics. We further demonstrate that by properly adjusting the noise variance, LSVGD simulates a Langevin process whose stationary distribution is exactly the target distribution. We also show that LSVGD dynamics has an implicit regularization which is able to enhance particles' spread-out and diversity. At last we present an efficient way of applying particle-based variational inference on a general GAN training procedure no matter what loss function is adopted. Experimental results on one synthetic dataset and three popular benchmark datasets -- Cifar-10, Tiny-ImageNet and CelebA validate that LSVGD can remarkably improve the performance and stability of various GAN models.



### Distributed Learning and Inference with Compressed Images
- **Arxiv ID**: http://arxiv.org/abs/2004.10497v2
- **DOI**: 10.1109/TIP.2021.3058545
- **Categories**: **cs.CV**, eess.IV, I.4.2
- **Links**: [PDF](http://arxiv.org/pdf/2004.10497v2)
- **Published**: 2020-04-22 11:20:53+00:00
- **Updated**: 2021-02-05 11:45:05+00:00
- **Authors**: Sudeep Katakol, Basem Elbarashy, Luis Herranz, Joost van de Weijer, Antonio M. Lopez
- **Comment**: Accepted for publication in IEEE Transactions on Image Processing; 15
  pages, 15 figures
- **Journal**: None
- **Summary**: Modern computer vision requires processing large amounts of data, both while training the model and/or during inference, once the model is deployed. Scenarios where images are captured and processed in physically separated locations are increasingly common (e.g. autonomous vehicles, cloud computing). In addition, many devices suffer from limited resources to store or transmit data (e.g. storage space, channel capacity). In these scenarios, lossy image compression plays a crucial role to effectively increase the number of images collected under such constraints. However, lossy compression entails some undesired degradation of the data that may harm the performance of the downstream analysis task at hand, since important semantic information may be lost in the process. Moreover, we may only have compressed images at training time but are able to use original images at inference time, or vice versa, and in such a case, the downstream model suffers from covariate shift. In this paper, we analyze this phenomenon, with a special focus on vision-based perception for autonomous driving as a paradigmatic scenario. We see that loss of semantic information and covariate shift do indeed exist, resulting in a drop in performance that depends on the compression rate. In order to address the problem, we propose dataset restoration, based on image restoration with generative adversarial networks (GANs). Our method is agnostic to both the particular image compression method and the downstream task; and has the advantage of not adding additional cost to the deployed models, which is particularly important in resource-limited devices. The presented experiments focus on semantic segmentation as a challenging use case, cover a broad range of compression rates and diverse datasets, and show how our method is able to significantly alleviate the negative effects of compression on the downstream visual task.



### Deep Learning for Screening COVID-19 using Chest X-Ray Images
- **Arxiv ID**: http://arxiv.org/abs/2004.10507v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.10507v4)
- **Published**: 2020-04-22 11:41:50+00:00
- **Updated**: 2020-08-21 20:17:35+00:00
- **Authors**: Sanhita Basu, Sushmita Mitra, Nilanjan Saha
- **Comment**: None
- **Journal**: None
- **Summary**: With the ever increasing demand for screening millions of prospective "novel coronavirus" or COVID-19 cases, and due to the emergence of high false negatives in the commonly used PCR tests, the necessity for probing an alternative simple screening mechanism of COVID-19 using radiological images (like chest X-Rays) assumes importance. In this scenario, machine learning (ML) and deep learning (DL) offer fast, automated, effective strategies to detect abnormalities and extract key features of the altered lung parenchyma, which may be related to specific signatures of the COVID-19 virus. However, the available COVID-19 datasets are inadequate to train deep neural networks. Therefore, we propose a new concept called domain extension transfer learning (DETL). We employ DETL, with pre-trained deep convolutional neural network, on a related large chest X-Ray dataset that is tuned for classifying between four classes \textit{viz.} $normal$, $pneumonia$, $other\_disease$, and $Covid-19$. A 5-fold cross validation is performed to estimate the feasibility of using chest X-Rays to diagnose COVID-19. The initial results show promise, with the possibility of replication on bigger and more diverse data sets. The overall accuracy was measured as $90.13\% \pm 0.14$. In order to get an idea about the COVID-19 detection transparency, we employed the concept of Gradient Class Activation Map (Grad-CAM) for detecting the regions where the model paid more attention during the classification. This was found to strongly correlate with clinical findings, as validated by experts.



### Human and Machine Action Prediction Independent of Object Information
- **Arxiv ID**: http://arxiv.org/abs/2004.10518v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.10518v1)
- **Published**: 2020-04-22 12:13:25+00:00
- **Updated**: 2020-04-22 12:13:25+00:00
- **Authors**: Fatemeh Ziaeetabar, Jennifer Pomp, Stefan Pfeiffer, Nadiya El-Sourani, Ricarda I. Schubotz, Minija Tamosiunaite, Florentin Wörgötter
- **Comment**: This paper includes 31 pages, 11 figures and 1 table
- **Journal**: None
- **Summary**: Predicting other people's action is key to successful social interactions, enabling us to adjust our own behavior to the consequence of the others' future actions. Studies on action recognition have focused on the importance of individual visual features of objects involved in an action and its context. Humans, however, recognize actions on unknown objects or even when objects are imagined (pantomime). Other cues must thus compensate the lack of recognizable visual object features. Here, we focus on the role of inter-object relations that change during an action. We designed a virtual reality setup and tested recognition speed for 10 different manipulation actions on 50 subjects. All objects were abstracted by emulated cubes so the actions could not be inferred using object information. Instead, subjects had to rely only on the information that comes from the changes in the spatial relations that occur between those cubes. In spite of these constraints, our results show the subjects were able to predict actions in, on average, less than 64% of the action's duration. We employed a computational model -an enriched Semantic Event Chain (eSEC)- incorporating the information of spatial relations, specifically (a) objects' touching/untouching, (b) static spatial relations between objects and (c) dynamic spatial relations between objects. Trained on the same actions as those observed by subjects, the model successfully predicted actions even better than humans. Information theoretical analysis shows that eSECs optimally use individual cues, whereas humans presumably mostly rely on a mixed-cue strategy, which takes longer until recognition. Providing a better cognitive basis of action recognition may, on one hand improve our understanding of related human pathologies and, on the other hand, also help to build robots for conflict-free human-robot cooperation. Our results open new avenues here.



### TetraTSDF: 3D human reconstruction from a single image with a tetrahedral outer shell
- **Arxiv ID**: http://arxiv.org/abs/2004.10534v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.10534v1)
- **Published**: 2020-04-22 12:47:24+00:00
- **Updated**: 2020-04-22 12:47:24+00:00
- **Authors**: Hayato Onizuka, Zehra Hayirci, Diego Thomas, Akihiro Sugimoto, Hideaki Uchiyama, Rin-ichiro Taniguchi
- **Comment**: None
- **Journal**: None
- **Summary**: Recovering the 3D shape of a person from its 2D appearance is ill-posed due to ambiguities. Nevertheless, with the help of convolutional neural networks (CNN) and prior knowledge on the 3D human body, it is possible to overcome such ambiguities to recover detailed 3D shapes of human bodies from single images. Current solutions, however, fail to reconstruct all the details of a person wearing loose clothes. This is because of either (a) huge memory requirement that cannot be maintained even on modern GPUs or (b) the compact 3D representation that cannot encode all the details. In this paper, we propose the tetrahedral outer shell volumetric truncated signed distance function (TetraTSDF) model for the human body, and its corresponding part connection network (PCN) for 3D human body shape regression. Our proposed model is compact, dense, accurate, and yet well suited for CNN-based regression task. Our proposed PCN allows us to learn the distribution of the TSDF in the tetrahedral volume from a single image in an end-to-end manner. Results show that our proposed method allows to reconstruct detailed shapes of humans wearing loose clothes from single RGB images.



### Multi-Domain Learning and Identity Mining for Vehicle Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2004.10547v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.10547v2)
- **Published**: 2020-04-22 13:03:52+00:00
- **Updated**: 2020-04-24 14:08:35+00:00
- **Authors**: Shuting He, Hao Luo, Weihua Chen, Miao Zhang, Yuqi Zhang, Fan Wang, Hao Li, Wei Jiang
- **Comment**: Solution for AI City Challenge, CVPR2020 Workshop. Codes are at
  https://github.com/heshuting555/AICITY2020_DMT_VehicleReID
- **Journal**: None
- **Summary**: This paper introduces our solution for the Track2 in AI City Challenge 2020 (AICITY20). The Track2 is a vehicle re-identification (ReID) task with both the real-world data and synthetic data. Our solution is based on a strong baseline with bag of tricks (BoT-BS) proposed in person ReID. At first, we propose a multi-domain learning method to joint the real-world and synthetic data to train the model. Then, we propose the Identity Mining method to automatically generate pseudo labels for a part of the testing data, which is better than the k-means clustering. The tracklet-level re-ranking strategy with weighted features is also used to post-process the results. Finally, with multiple-model ensemble, our method achieves 0.7322 in the mAP score which yields third place in the competition. The codes are available at https://github.com/heshuting555/AICITY2020_DMT_VehicleReID.



### Real-time Simultaneous 3D Head Modeling and Facial Motion Capture with an RGB-D camera
- **Arxiv ID**: http://arxiv.org/abs/2004.10557v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.10557v1)
- **Published**: 2020-04-22 13:22:21+00:00
- **Updated**: 2020-04-22 13:22:21+00:00
- **Authors**: Diego Thomas
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a method to build in real-time animated 3D head models using a consumer-grade RGB-D camera. Our proposed method is the first one to provide simultaneously comprehensive facial motion tracking and a detailed 3D model of the user's head. Anyone's head can be instantly reconstructed and his facial motion captured without requiring any training or pre-scanning. The user starts facing the camera with a neutral expression in the first frame, but is free to move, talk and change his face expression as he wills otherwise. The facial motion is captured using a blendshape animation model while geometric details are captured using a Deviation image mapped over the template mesh. We contribute with an efficient algorithm to grow and refine the deforming 3D model of the head on-the-fly and in real-time. We demonstrate robust and high-fidelity simultaneous facial motion capture and 3D head modeling results on a wide range of subjects with various head poses and facial expressions.



### Efficient Neighbourhood Consensus Networks via Submanifold Sparse Convolutions
- **Arxiv ID**: http://arxiv.org/abs/2004.10566v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.10566v1)
- **Published**: 2020-04-22 13:37:36+00:00
- **Updated**: 2020-04-22 13:37:36+00:00
- **Authors**: Ignacio Rocco, Relja Arandjelović, Josef Sivic
- **Comment**: None
- **Journal**: None
- **Summary**: In this work we target the problem of estimating accurately localised correspondences between a pair of images. We adopt the recent Neighbourhood Consensus Networks that have demonstrated promising performance for difficult correspondence problems and propose modifications to overcome their main limitations: large memory consumption, large inference time and poorly localised correspondences. Our proposed modifications can reduce the memory footprint and execution time more than $10\times$, with equivalent results. This is achieved by sparsifying the correlation tensor containing tentative matches, and its subsequent processing with a 4D CNN using submanifold sparse convolutions. Localisation accuracy is significantly improved by processing the input images in higher resolution, which is possible due to the reduced memory footprint, and by a novel two-stage correspondence relocalisation module. The proposed Sparse-NCNet method obtains state-of-the-art results on the HPatches Sequences and InLoc visual localisation benchmarks, and competitive results in the Aachen Day-Night benchmark.



### Up or Down? Adaptive Rounding for Post-Training Quantization
- **Arxiv ID**: http://arxiv.org/abs/2004.10568v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.10568v2)
- **Published**: 2020-04-22 13:44:28+00:00
- **Updated**: 2020-06-30 09:51:23+00:00
- **Authors**: Markus Nagel, Rana Ali Amjad, Mart van Baalen, Christos Louizos, Tijmen Blankevoort
- **Comment**: Published as a conference paper at ICML 2020
- **Journal**: None
- **Summary**: When quantizing neural networks, assigning each floating-point weight to its nearest fixed-point value is the predominant approach. We find that, perhaps surprisingly, this is not the best we can do. In this paper, we propose AdaRound, a better weight-rounding mechanism for post-training quantization that adapts to the data and the task loss. AdaRound is fast, does not require fine-tuning of the network, and only uses a small amount of unlabelled data. We start by theoretically analyzing the rounding problem for a pre-trained neural network. By approximating the task loss with a Taylor series expansion, the rounding task is posed as a quadratic unconstrained binary optimization problem. We simplify this to a layer-wise local loss and propose to optimize this loss with a soft relaxation. AdaRound not only outperforms rounding-to-nearest by a significant margin but also establishes a new state-of-the-art for post-training quantization on several networks and tasks. Without fine-tuning, we can quantize the weights of Resnet18 and Resnet50 to 4 bits while staying within an accuracy loss of 1%.



### MangaGAN: Unpaired Photo-to-Manga Translation Based on The Methodology of Manga Drawing
- **Arxiv ID**: http://arxiv.org/abs/2004.10634v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.10634v2)
- **Published**: 2020-04-22 15:23:42+00:00
- **Updated**: 2020-12-17 17:21:42+00:00
- **Authors**: Hao Su, Jianwei Niu, Xuefeng Liu, Qingfeng Li, Jiahe Cui, Ji Wan
- **Comment**: 17 pages
- **Journal**: None
- **Summary**: Manga is a world popular comic form originated in Japan, which typically employs black-and-white stroke lines and geometric exaggeration to describe humans' appearances, poses, and actions. In this paper, we propose MangaGAN, the first method based on Generative Adversarial Network (GAN) for unpaired photo-to-manga translation. Inspired by how experienced manga artists draw manga, MangaGAN generates the geometric features of manga face by a designed GAN model and delicately translates each facial region into the manga domain by a tailored multi-GANs architecture. For training MangaGAN, we construct a new dataset collected from a popular manga work, containing manga facial features, landmarks, bodies, and so on. Moreover, to produce high-quality manga faces, we further propose a structural smoothing loss to smooth stroke-lines and avoid noisy pixels, and a similarity preserving module to improve the similarity between domains of photo and manga. Extensive experiments show that MangaGAN can produce high-quality manga faces which preserve both the facial similarity and a popular manga style, and outperforms other related state-of-the-art methods.



### Automatic Detection of Coronavirus Disease (COVID-19) in X-ray and CT Images: A Machine Learning-Based Approach
- **Arxiv ID**: http://arxiv.org/abs/2004.10641v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.10641v1)
- **Published**: 2020-04-22 15:34:45+00:00
- **Updated**: 2020-04-22 15:34:45+00:00
- **Authors**: Sara Hosseinzadeh Kassani, Peyman Hosseinzadeh Kassasni, Michal J. Wesolowski, Kevin A. Schneider, Ralph Deters
- **Comment**: None
- **Journal**: None
- **Summary**: The newly identified Coronavirus pneumonia, subsequently termed COVID-19, is highly transmittable and pathogenic with no clinically approved antiviral drug or vaccine available for treatment. The most common symptoms of COVID-19 are dry cough, sore throat, and fever. Symptoms can progress to a severe form of pneumonia with critical complications, including septic shock, pulmonary edema, acute respiratory distress syndrome and multi-organ failure. While medical imaging is not currently recommended in Canada for primary diagnosis of COVID-19, computer-aided diagnosis systems could assist in the early detection of COVID-19 abnormalities and help to monitor the progression of the disease, potentially reduce mortality rates. In this study, we compare popular deep learning-based feature extraction frameworks for automatic COVID-19 classification. To obtain the most accurate feature, which is an essential component of learning, MobileNet, DenseNet, Xception, ResNet, InceptionV3, InceptionResNetV2, VGGNet, NASNet were chosen amongst a pool of deep convolutional neural networks. The extracted features were then fed into several machine learning classifiers to classify subjects as either a case of COVID-19 or a control. This approach avoided task-specific data pre-processing methods to support a better generalization ability for unseen data. The performance of the proposed method was validated on a publicly available COVID-19 dataset of chest X-ray and CT images. The DenseNet121 feature extractor with Bagging tree classifier achieved the best performance with 99% classification accuracy. The second-best learner was a hybrid of the a ResNet50 feature extractor trained by LightGBM with an accuracy of 98%.



### QUANOS- Adversarial Noise Sensitivity Driven Hybrid Quantization of Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2004.11233v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.11233v2)
- **Published**: 2020-04-22 15:56:31+00:00
- **Updated**: 2020-06-27 13:14:58+00:00
- **Authors**: Priyadarshini Panda
- **Comment**: Accepted in ACM/IEEE International Symposium on Low Power Electronics
  and Design (ISLPED), 2020
- **Journal**: None
- **Summary**: Deep Neural Networks (DNNs) have been shown to be vulnerable to adversarial attacks, wherein, a model gets fooled by applying slight perturbations on the input. With the advent of Internet-of-Things and the necessity to enable intelligence in embedded devices, low-power and secure hardware implementation of DNNs is vital. In this paper, we investigate the use of quantization to potentially resist adversarial attacks. Several recent studies have reported remarkable results in reducing the energy requirement of a DNN through quantization. However, no prior work has considered the relationship between adversarial sensitivity of a DNN and its effect on quantization. We propose QUANOS- a framework that performs layer-specific hybrid quantization based on Adversarial Noise Sensitivity (ANS). We identify a novel noise stability metric (ANS) for DNNs, i.e., the sensitivity of each layer's computation to adversarial noise. ANS allows for a principled way of determining optimal bit-width per layer that incurs adversarial robustness as well as energy-efficiency with minimal loss in accuracy. Essentially, QUANOS assigns layer significance based on its contribution to adversarial perturbation and accordingly scales the precision of the layers. A key advantage of QUANOS is that it does not rely on a pre-trained model and can be applied in the initial stages of training. We evaluate the benefits of QUANOS on precision scalable Multiply and Accumulate (MAC) hardware architectures with data gating and subword parallelism capabilities. Our experiments on CIFAR10, CIFAR100 datasets show that QUANOS outperforms homogenously quantized 8-bit precision baseline in terms of adversarial robustness (3%-4% higher) while yielding improved compression (>5x) and energy savings (>2x) at iso-accuracy.



### A review: Deep learning for medical image segmentation using multi-modality fusion
- **Arxiv ID**: http://arxiv.org/abs/2004.10664v2
- **DOI**: 10.1016/j.array.2019.100004
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.10664v2)
- **Published**: 2020-04-22 16:00:53+00:00
- **Updated**: 2020-07-16 15:33:31+00:00
- **Authors**: Tongxue Zhou, Su Ruan, Stéphane Canu
- **Comment**: 26 pages, 8 figures
- **Journal**: Array, Volumes 3-4, September-December 2019, Article 100004
- **Summary**: Multi-modality is widely used in medical imaging, because it can provide multiinformation about a target (tumor, organ or tissue). Segmentation using multimodality consists of fusing multi-information to improve the segmentation. Recently, deep learning-based approaches have presented the state-of-the-art performance in image classification, segmentation, object detection and tracking tasks. Due to their self-learning and generalization ability over large amounts of data, deep learning recently has also gained great interest in multi-modal medical image segmentation. In this paper, we give an overview of deep learning-based approaches for multi-modal medical image segmentation task. Firstly, we introduce the general principle of deep learning and multi-modal medical image segmentation. Secondly, we present different deep learning network architectures, then analyze their fusion strategies and compare their results. The earlier fusion is commonly used, since it's simple and it focuses on the subsequent segmentation network architecture. However, the later fusion gives more attention on fusion strategy to learn the complex relationship between different modalities. In general, compared to the earlier fusion, the later fusion can give more accurate result if the fusion method is effective enough. We also discuss some common problems in medical image segmentation. Finally, we summarize and provide some perspectives on the future research.



### Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
- **Arxiv ID**: http://arxiv.org/abs/2004.10681v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.10681v3)
- **Published**: 2020-04-22 16:31:59+00:00
- **Updated**: 2020-08-07 05:15:31+00:00
- **Authors**: Lokender Tiwari, Pan Ji, Quoc-Huy Tran, Bingbing Zhuang, Saket Anand, Manmohan Chandraker
- **Comment**: ECCV 2020, Project Page:
  https://lokender.github.io/self-improving-SLAM.html
- **Journal**: None
- **Summary**: Classical monocular Simultaneous Localization And Mapping (SLAM) and the recently emerging convolutional neural networks (CNNs) for monocular depth prediction represent two largely disjoint approaches towards building a 3D map of the surrounding environment. In this paper, we demonstrate that the coupling of these two by leveraging the strengths of each mitigates the other's shortcomings. Specifically, we propose a joint narrow and wide baseline based self-improving framework, where on the one hand the CNN-predicted depth is leveraged to perform pseudo RGB-D feature-based SLAM, leading to better accuracy and robustness than the monocular RGB SLAM baseline. On the other hand, the bundle-adjusted 3D scene structures and camera poses from the more principled geometric SLAM are injected back into the depth network through novel wide baseline losses proposed for improving the depth prediction network, which then continues to contribute towards better pose and 3D structure estimation in the next iteration. We emphasize that our framework only requires unlabeled monocular videos in both training and inference stages, and yet is able to outperform state-of-the-art self-supervised monocular and stereo depth prediction networks (e.g, Monodepth2) and feature-based monocular SLAM system (i.e, ORB-SLAM). Extensive experiments on KITTI and TUM RGB-D datasets verify the superiority of our self-improving geometry-CNN framework.



### DyNet: Dynamic Convolution for Accelerating Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2004.10694v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.10694v1)
- **Published**: 2020-04-22 16:58:05+00:00
- **Updated**: 2020-04-22 16:58:05+00:00
- **Authors**: Yikang Zhang, Jian Zhang, Qiang Wang, Zhao Zhong
- **Comment**: None
- **Journal**: None
- **Summary**: Convolution operator is the core of convolutional neural networks (CNNs) and occupies the most computation cost. To make CNNs more efficient, many methods have been proposed to either design lightweight networks or compress models. Although some efficient network structures have been proposed, such as MobileNet or ShuffleNet, we find that there still exists redundant information between convolution kernels. To address this issue, we propose a novel dynamic convolution method to adaptively generate convolution kernels based on image contents. To demonstrate the effectiveness, we apply dynamic convolution on multiple state-of-the-art CNNs. On one hand, we can reduce the computation cost remarkably while maintaining the performance. For ShuffleNetV2/MobileNetV2/ResNet18/ResNet50, DyNet can reduce 37.0/54.7/67.2/71.3% FLOPs without loss of accuracy. On the other hand, the performance can be largely boosted if the computation cost is maintained. Based on the architecture MobileNetV3-Small/Large, DyNet achieves 70.3/77.1% Top-1 accuracy on ImageNet with an improvement of 2.9/1.9%. To verify the scalability, we also apply DyNet on segmentation task, the results show that DyNet can reduce 69.3% FLOPs while maintaining Mean IoU on segmentation task.



### Per-pixel Classification Rebar Exposures in Bridge Eye-inspection
- **Arxiv ID**: http://arxiv.org/abs/2004.12805v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, I.2.6; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/2004.12805v1)
- **Published**: 2020-04-22 17:28:42+00:00
- **Updated**: 2020-04-22 17:28:42+00:00
- **Authors**: Takato Yasuno, Nakajima Michihiro, Noda Kazuhiro
- **Comment**: 4 pages, 3 figures
- **Journal**: None
- **Summary**: Efficient inspection and accurate diagnosis are required for civil infrastructures with 50 years since completion. Especially in municipalities, the shortage of technical staff and budget constraints on repair expenses have become a critical problem. If we can detect damaged photos automatically per-pixels from the record of the inspection record in addition to the 5-step judgment and countermeasure classification of eye-inspection vision, then it is possible that countermeasure information can be provided more flexibly, whether we need to repair and how large the expose of damage interest. A piece of damage photo is often sparse as long as it is not zoomed around damage, exactly the range where the detection target is photographed, is at most only 1%. Generally speaking, rebar exposure is frequently occurred, and there are many opportunities to judge repair measure. In this paper, we propose three damage detection methods of transfer learning which enables semantic segmentation in an image with low pixels using damaged photos of human eye-inspection. Also, we tried to create a deep convolutional network from scratch with the preprocessing that random crops with rotations are generated. In fact, we show the results applied this method using the 208 rebar exposed images on the 106 real-world bridges. Finally, future tasks of damage detection modeling are mentioned.



### Red-GAN: Attacking class imbalance via conditioned generation. Yet another perspective on medical image synthesis for skin lesion dermoscopy and brain tumor MRI
- **Arxiv ID**: http://arxiv.org/abs/2004.10734v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.10734v4)
- **Published**: 2020-04-22 17:38:48+00:00
- **Updated**: 2021-03-28 00:15:19+00:00
- **Authors**: Ahmad B Qasim, Ivan Ezhov, Suprosanna Shit, Oliver Schoppe, Johannes C Paetzold, Anjany Sekuboyina, Florian Kofler, Jana Lipkova, Hongwei Li, Bjoern Menze
- **Comment**: None
- **Journal**: Published in Proceedings of the 3rd edition of Medical Imaging
  with Deep Learning, Montr\'eal, Canada, PMLR 121, 2020
- **Summary**: Exploiting learning algorithms under scarce data regimes is a limitation and a reality of the medical imaging field. In an attempt to mitigate the problem, we propose a data augmentation protocol based on generative adversarial networks. We condition the networks at a pixel-level (segmentation mask) and at a global-level information (acquisition environment or lesion type). Such conditioning provides immediate access to the image-label pairs while controlling global class specific appearance of the synthesized images. To stimulate synthesis of the features relevant for the segmentation task, an additional passive player in a form of segmentor is introduced into the adversarial game. We validate the approach on two medical datasets: BraTS, ISIC. By controlling the class distribution through injection of synthetic images into the training set we achieve control over the accuracy levels of the datasets' classes.



### Action recognition in real-world videos
- **Arxiv ID**: http://arxiv.org/abs/2004.10774v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.10774v1)
- **Published**: 2020-04-22 18:02:50+00:00
- **Updated**: 2020-04-22 18:02:50+00:00
- **Authors**: Waqas Sultani, Qazi Ammar Arshad, Chen Chen
- **Comment**: None
- **Journal**: None
- **Summary**: The goal of human action recognition is to temporally or spatially localize the human action of interest in video sequences. Temporal localization (i.e. indicating the start and end frames of the action in a video) is referred to as frame-level detection. Spatial localization, which is more challenging, means to identify the pixels within each action frame that correspond to the action. This setting is usually referred to as pixel-level detection. In this chapter, we are using action, activity, event interchangeably.



### Diagram Image Retrieval using Sketch-Based Deep Learning and Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/2004.10780v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.10780v1)
- **Published**: 2020-04-22 18:27:46+00:00
- **Updated**: 2020-04-22 18:27:46+00:00
- **Authors**: Manish Bhattarai, Diane Oyen, Juan Castorena, Liping Yang, Brendt Wohlberg
- **Comment**: None
- **Journal**: None
- **Summary**: Resolution of the complex problem of image retrieval for diagram images has yet to be reached. Deep learning methods continue to excel in the fields of object detection and image classification applied to natural imagery. However, the application of such methodologies applied to binary imagery remains limited due to lack of crucial features such as textures,color and intensity information. This paper presents a deep learning based method for image-based search for binary patent images by taking advantage of existing large natural image repositories for image search and sketch-based methods (Sketches are not identical to diagrams, but they do share some characteristics; for example, both imagery types are gray scale (binary), composed of contours, and are lacking in texture).   We begin by using deep learning to generate sketches from natural images for image retrieval and then train a second deep learning model on the sketches. We then use our small set of manually labeled patent diagram images via transfer learning to adapt the image search from sketches of natural images to diagrams. Our experiment results show the effectiveness of deep learning with transfer learning for detecting near-identical copies in patent images and querying similar images based on content.



### Automatic Polyp Segmentation Using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2004.10792v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.10792v1)
- **Published**: 2020-04-22 18:54:29+00:00
- **Updated**: 2020-04-22 18:54:29+00:00
- **Authors**: Sara Hosseinzadeh Kassani, Peyman Hosseinzadeh Kassani, Michal J. Wesolowski, Kevin A. Schneider, Ralph Deters
- **Comment**: None
- **Journal**: None
- **Summary**: Colorectal cancer is the third most common cancer-related death after lung cancer and breast cancer worldwide. The risk of developing colorectal cancer could be reduced by early diagnosis of polyps during a colonoscopy. Computer-aided diagnosis systems have the potential to be applied for polyp screening and reduce the number of missing polyps. In this paper, we compare the performance of different deep learning architectures as feature extractors, i.e. ResNet, DenseNet, InceptionV3, InceptionResNetV2 and SE-ResNeXt in the encoder part of a U-Net architecture. We validated the performance of presented ensemble models on the CVC-Clinic (GIANA 2018) dataset. The DenseNet169 feature extractor combined with U-Net architecture outperformed the other counterparts and achieved an accuracy of 99.15\%, Dice similarity coefficient of 90.87%, and Jaccard index of 83.82%.



### VisualCOMET: Reasoning about the Dynamic Context of a Still Image
- **Arxiv ID**: http://arxiv.org/abs/2004.10796v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2004.10796v3)
- **Published**: 2020-04-22 19:02:20+00:00
- **Updated**: 2020-08-01 13:11:10+00:00
- **Authors**: Jae Sung Park, Chandra Bhagavatula, Roozbeh Mottaghi, Ali Farhadi, Yejin Choi
- **Comment**: Project Page: http://visualcomet.xyz (ECCV 2020 Spotlight)
- **Journal**: None
- **Summary**: Even from a single frame of a still image, people can reason about the dynamic story of the image before, after, and beyond the frame. For example, given an image of a man struggling to stay afloat in water, we can reason that the man fell into the water sometime in the past, the intent of that man at the moment is to stay alive, and he will need help in the near future or else he will get washed away. We propose VisualComet, the novel framework of visual commonsense reasoning tasks to predict events that might have happened before, events that might happen next, and the intents of the people at present. To support research toward visual commonsense reasoning, we introduce the first large-scale repository of Visual Commonsense Graphs that consists of over 1.4 million textual descriptions of visual commonsense inferences carefully annotated over a diverse set of 60,000 images, each paired with short video summaries of before and after. In addition, we provide person-grounding (i.e., co-reference links) between people appearing in the image and people mentioned in the textual commonsense descriptions, allowing for tighter integration between images and text. We establish strong baseline performances on this task and demonstrate that integration between visual and textual commonsense reasoning is the key and wins over non-integrative alternatives.



### Continual Learning of Object Instances
- **Arxiv ID**: http://arxiv.org/abs/2004.10862v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.10862v1)
- **Published**: 2020-04-22 21:09:02+00:00
- **Updated**: 2020-04-22 21:09:02+00:00
- **Authors**: Kishan Parshotam, Mert Kilickaya
- **Comment**: Accepted to CVPR 2020: Workshop on Continual Learning in Computer
  Vision
- **Journal**: None
- **Summary**: We propose continual instance learning - a method that applies the concept of continual learning to the task of distinguishing instances of the same object category. We specifically focus on the car object, and incrementally learn to distinguish car instances from each other with metric learning. We begin our paper by evaluating current techniques. Establishing that catastrophic forgetting is evident in existing methods, we then propose two remedies. Firstly, we regularise metric learning via Normalised Cross-Entropy. Secondly, we augment existing models with synthetic data transfer. Our extensive experiments on three large-scale datasets, using two different architectures for five different continual learning methods, reveal that Normalised cross-entropy and synthetic transfer leads to less forgetting in existing techniques.



### Microscopy Image Restoration using Deep Learning on W2S
- **Arxiv ID**: http://arxiv.org/abs/2004.10884v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.10884v1)
- **Published**: 2020-04-22 22:14:19+00:00
- **Updated**: 2020-04-22 22:14:19+00:00
- **Authors**: Martin Chatton
- **Comment**: None
- **Journal**: None
- **Summary**: We leverage deep learning techniques to jointly denoise and super-resolve biomedical images acquired with fluorescence microscopy. We develop a deep learning algorithm based on the networks and method described in the recent W2S paper to solve a joint denoising and super-resolution problem. Specifically, we address the restoration of SIM images from widefield images. Our TensorFlow model is trained on the W2S dataset of cell images and is made accessible online in this repository: https://github.com/mchatton/w2s-tensorflow. On test images, the model shows a visually-convincing denoising and increases the resolution by a factor of two compared to the input image. For a 512 $\times$ 512 image, the inference takes less than 1 second on a Titan X GPU and about 15 seconds on a common CPU. We further present the results of different variations of losses used in training.



### Through the Looking Glass: Neural 3D Reconstruction of Transparent Shapes
- **Arxiv ID**: http://arxiv.org/abs/2004.10904v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2004.10904v2)
- **Published**: 2020-04-22 23:51:30+00:00
- **Updated**: 2020-07-23 05:54:49+00:00
- **Authors**: Zhengqin Li, Yu-Ying Yeh, Manmohan Chandraker
- **Comment**: Accepted by CVPR 2020 as an oral presentation
- **Journal**: None
- **Summary**: Recovering the 3D shape of transparent objects using a small number of unconstrained natural images is an ill-posed problem. Complex light paths induced by refraction and reflection have prevented both traditional and deep multiview stereo from solving this challenge. We propose a physically-based network to recover 3D shape of transparent objects using a few images acquired with a mobile phone camera, under a known but arbitrary environment map. Our novel contributions include a normal representation that enables the network to model complex light transport through local computation, a rendering layer that models refractions and reflections, a cost volume specifically designed for normal refinement of transparent shapes and a feature mapping based on predicted normals for 3D point cloud reconstruction. We render a synthetic dataset to encourage the model to learn refractive light transport across different views. Our experiments show successful recovery of high-quality 3D geometry for complex transparent shapes using as few as 5-12 natural images. Code and data are publicly released.



