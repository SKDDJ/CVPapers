# Arxiv Papers in cs.CV on 2020-04-26
### TPNet: Trajectory Proposal Network for Motion Prediction
- **Arxiv ID**: http://arxiv.org/abs/2004.12255v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.12255v2)
- **Published**: 2020-04-26 00:01:49+00:00
- **Updated**: 2021-02-07 08:04:58+00:00
- **Authors**: Liangji Fang, Qinhong Jiang, Jianping Shi, Bolei Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: Making accurate motion prediction of the surrounding traffic agents such as pedestrians, vehicles, and cyclists is crucial for autonomous driving. Recent data-driven motion prediction methods have attempted to learn to directly regress the exact future position or its distribution from massive amount of trajectory data. However, it remains difficult for these methods to provide multimodal predictions as well as integrate physical constraints such as traffic rules and movable areas. In this work we propose a novel two-stage motion prediction framework, Trajectory Proposal Network (TPNet). TPNet first generates a candidate set of future trajectories as hypothesis proposals, then makes the final predictions by classifying and refining the proposals which meets the physical constraints. By steering the proposal generation process, safe and multimodal predictions are realized. Thus this framework effectively mitigates the complexity of motion prediction problem while ensuring the multimodal output. Experiments on four large-scale trajectory prediction datasets, i.e. the ETH, UCY, Apollo and Argoverse datasets, show that TPNet achieves the state-of-the-art results both quantitatively and qualitatively.



### Learning to Autofocus
- **Arxiv ID**: http://arxiv.org/abs/2004.12260v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.12260v3)
- **Published**: 2020-04-26 00:39:21+00:00
- **Updated**: 2020-05-02 23:23:12+00:00
- **Authors**: Charles Herrmann, Richard Strong Bowen, Neal Wadhwa, Rahul Garg, Qiurui He, Jonathan T. Barron, Ramin Zabih
- **Comment**: CVPR 2020
- **Journal**: None
- **Summary**: Autofocus is an important task for digital cameras, yet current approaches often exhibit poor performance. We propose a learning-based approach to this problem, and provide a realistic dataset of sufficient size for effective learning. Our dataset is labeled with per-pixel depths obtained from multi-view stereo, following "Learning single camera depth estimation using dual-pixels". Using this dataset, we apply modern deep classification models and an ordinal regression loss to obtain an efficient learning-based autofocus technique. We demonstrate that our approach provides a significant improvement compared with previous learned and non-learned methods: our model reduces the mean absolute error by a factor of 3.6 over the best comparable baseline algorithm. Our dataset and code are publicly available.



### TRAKO: Efficient Transmission of Tractography Data for Visualization
- **Arxiv ID**: http://arxiv.org/abs/2004.13630v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.GR, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2004.13630v1)
- **Published**: 2020-04-26 01:19:50+00:00
- **Updated**: 2020-04-26 01:19:50+00:00
- **Authors**: Daniel Haehn, Loraine Franke, Fan Zhang, Suheyla Cetin Karayumak, Steve Pieper, Lauren O'Donnell, Yogesh Rathi
- **Comment**: None
- **Journal**: None
- **Summary**: Fiber tracking produces large tractography datasets that are tens of gigabytes in size consisting of millions of streamlines. Such vast amounts of data require formats that allow for efficient storage, transfer, and visualization. We present TRAKO, a new data format based on the Graphics Layer Transmission Format (glTF) that enables immediate graphical and hardware-accelerated processing. We integrate a state-of-the-art compression technique for vertices, streamlines, and attached scalar and property data. We then compare TRAKO to existing tractography storage methods and provide a detailed evaluation on eight datasets. TRAKO can achieve data reductions of over 28x without loss of statistical significance when used to replicate analysis from previously published studies.



### Development of a High Fidelity Simulator for Generalised Photometric Based Space Object Classification using Machine Learning
- **Arxiv ID**: http://arxiv.org/abs/2004.12270v1
- **DOI**: None
- **Categories**: **physics.space-ph**, astro-ph.IM, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.12270v1)
- **Published**: 2020-04-26 02:10:15+00:00
- **Updated**: 2020-04-26 02:10:15+00:00
- **Authors**: James Allworth, Lloyd Windrim, Jeffrey Wardman, Daniel Kucharski, James Bennett, Mitch Bryson
- **Comment**: This paper is a pre-print that appeared in Proceedings of 70th
  International Astronautical Congress (IAC), 2019
- **Journal**: Proceedings of the 70th International Astronautical Congress, 2019
- **Summary**: This paper presents the initial stages in the development of a deep learning classifier for generalised Resident Space Object (RSO) characterisation that combines high-fidelity simulated light curves with transfer learning to improve the performance of object characterisation models that are trained on real data. The classification and characterisation of RSOs is a significant goal in Space Situational Awareness (SSA) in order to improve the accuracy of orbital predictions. The specific focus of this paper is the development of a high-fidelity simulation environment for generating realistic light curves. The simulator takes in a textured geometric model of an RSO as well as the objects ephemeris and uses Blender to generate photo-realistic images of the RSO that are then processed to extract the light curve. Simulated light curves have been compared with real light curves extracted from telescope imagery to provide validation for the simulation environment. Future work will involve further validation and the use of the simulator to generate a dataset of realistic light curves for the purpose of training neural networks.



### Show, Describe and Conclude: On Exploiting the Structure Information of Chest X-Ray Reports
- **Arxiv ID**: http://arxiv.org/abs/2004.12274v2
- **DOI**: 10.18653/v1/P19-1657
- **Categories**: **cs.CL**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.12274v2)
- **Published**: 2020-04-26 02:29:20+00:00
- **Updated**: 2020-07-23 17:44:44+00:00
- **Authors**: Baoyu Jing, Zeya Wang, Eric Xing
- **Comment**: ACL 2019
- **Journal**: None
- **Summary**: Chest X-Ray (CXR) images are commonly used for clinical screening and diagnosis. Automatically writing reports for these images can considerably lighten the workload of radiologists for summarizing descriptive findings and conclusive impressions. The complex structures between and within sections of the reports pose a great challenge to the automatic report generation. Specifically, the section Impression is a diagnostic summarization over the section Findings; and the appearance of normality dominates each section over that of abnormality. Existing studies rarely explore and consider this fundamental structure information. In this work, we propose a novel framework that exploits the structure information between and within report sections for generating CXR imaging reports. First, we propose a two-stage strategy that explicitly models the relationship between Findings and Impression. Second, we design a novel cooperative multi-agent system that implicitly captures the imbalanced distribution between abnormality and normality. Experiments on two CXR report datasets show that our method achieves state-of-the-art performance in terms of various evaluation metrics. Our results expose that the proposed approach is able to generate high-quality medical reports through integrating the structure information.



### Fashionpedia: Ontology, Segmentation, and an Attribute Localization Dataset
- **Arxiv ID**: http://arxiv.org/abs/2004.12276v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.12276v2)
- **Published**: 2020-04-26 02:38:26+00:00
- **Updated**: 2020-07-18 21:02:49+00:00
- **Authors**: Menglin Jia, Mengyun Shi, Mikhail Sirotenko, Yin Cui, Claire Cardie, Bharath Hariharan, Hartwig Adam, Serge Belongie
- **Comment**: eccv2020
- **Journal**: None
- **Summary**: In this work we explore the task of instance segmentation with attribute localization, which unifies instance segmentation (detect and segment each object instance) and fine-grained visual attribute categorization (recognize one or multiple attributes). The proposed task requires both localizing an object and describing its properties. To illustrate the various aspects of this task, we focus on the domain of fashion and introduce Fashionpedia as a step toward mapping out the visual aspects of the fashion world. Fashionpedia consists of two parts: (1) an ontology built by fashion experts containing 27 main apparel categories, 19 apparel parts, 294 fine-grained attributes and their relationships; (2) a dataset with everyday and celebrity event fashion images annotated with segmentation masks and their associated per-mask fine-grained attributes, built upon the Fashionpedia ontology. In order to solve this challenging task, we propose a novel Attribute-Mask RCNN model to jointly perform instance segmentation and localized attribute recognition, and provide a novel evaluation metric for the task. We also demonstrate instance segmentation models pre-trained on Fashionpedia achieve better transfer learning performance on other fashion datasets than ImageNet pre-training. Fashionpedia is available at: https://fashionpedia.github.io/home/index.html.



### An Extension of LIME with Improvement of Interpretability and Fidelity
- **Arxiv ID**: http://arxiv.org/abs/2004.12277v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.12277v1)
- **Published**: 2020-04-26 02:54:27+00:00
- **Updated**: 2020-04-26 02:54:27+00:00
- **Authors**: Sheng Shi, Yangzhou Du, Wei Fan
- **Comment**: None
- **Journal**: None
- **Summary**: While deep learning makes significant achievements in Artificial Intelligence (AI), the lack of transparency has limited its broad application in various vertical domains. Explainability is not only a gateway between AI and real world, but also a powerful feature to detect flaw of the models and bias of the data. Local Interpretable Model-agnostic Explanation (LIME) is a widely-accepted technique that explains the prediction of any classifier faithfully by learning an interpretable model locally around the predicted instance. As an extension of LIME, this paper proposes an high-interpretability and high-fidelity local explanation method, known as Local Explanation using feature Dependency Sampling and Nonlinear Approximation (LEDSNA). Given an instance being explained, LEDSNA enhances interpretability by feature sampling with intrinsic dependency. Besides, LEDSNA improves the local explanation fidelity by approximating nonlinear boundary of local decision. We evaluate our method with classification tasks in both image domain and text domain. Experiments show that LEDSNA's explanation of the back-box model achieves much better performance than original LIME in terms of interpretability and fidelity.



### Harnessing adversarial examples with a surprisingly simple defense
- **Arxiv ID**: http://arxiv.org/abs/2004.13013v3
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.13013v3)
- **Published**: 2020-04-26 03:09:42+00:00
- **Updated**: 2020-06-03 02:52:54+00:00
- **Authors**: Ali Borji
- **Comment**: None
- **Journal**: None
- **Summary**: I introduce a very simple method to defend against adversarial examples. The basic idea is to raise the slope of the ReLU function at the test time. Experiments over MNIST and CIFAR-10 datasets demonstrate the effectiveness of the proposed defense against a number of strong attacks in both untargeted and targeted settings. While perhaps not as effective as the state of the art adversarial defenses, this approach can provide insights to understand and mitigate adversarial attacks. It can also be used in conjunction with other defenses.



### Attention Based Real Image Restoration
- **Arxiv ID**: http://arxiv.org/abs/2004.13524v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.13524v2)
- **Published**: 2020-04-26 04:21:49+00:00
- **Updated**: 2020-10-01 06:52:45+00:00
- **Authors**: Saeed Anwar, Nick Barnes, Lars Petersson
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1904.07396
- **Journal**: None
- **Summary**: Deep convolutional neural networks perform better on images containing spatially invariant degradations, also known as synthetic degradations; however, their performance is limited on real-degraded photographs and requires multiple-stage network modeling. To advance the practicability of restoration algorithms, this paper proposes a novel single-stage blind real image restoration network (R$^2$Net) by employing a modular architecture. We use a residual on the residual structure to ease the flow of low-frequency information and apply feature attention to exploit the channel dependencies. Furthermore, the evaluation in terms of quantitative metrics and visual quality for four restoration tasks i.e. Denoising, Super-resolution, Raindrop Removal, and JPEG Compression on 11 real degraded datasets against more than 30 state-of-the-art algorithms demonstrate the superiority of our R$^2$Net. We also present the comparison on three synthetically generated degraded datasets for denoising to showcase the capability of our method on synthetics denoising. The codes, trained models, and results are available on https://github.com/saeed-anwar/R2Net.



### Identity Enhanced Residual Image Denoising
- **Arxiv ID**: http://arxiv.org/abs/2004.13523v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.13523v1)
- **Published**: 2020-04-26 04:52:22+00:00
- **Updated**: 2020-04-26 04:52:22+00:00
- **Authors**: Saeed Anwar, Cong Phuoc Huynh, Fatih Porikli
- **Comment**: Accepted in CVPRW. arXiv admin note: substantial text overlap with
  arXiv:1712.02933
- **Journal**: None
- **Summary**: We propose to learn a fully-convolutional network model that consists of a Chain of Identity Mapping Modules and residual on the residual architecture for image denoising. Our network structure possesses three distinctive features that are important for the noise removal task. Firstly, each unit employs identity mappings as the skip connections and receives pre-activated input to preserve the gradient magnitude propagated in both the forward and backward directions. Secondly, by utilizing dilated kernels for the convolution layers in the residual branch, each neuron in the last convolution layer of each module can observe the full receptive field of the first layer. Lastly, we employ the residual on the residual architecture to ease the propagation of the high-level information. Contrary to current state-of-the-art real denoising networks, we also present a straightforward and single-stage network for real image denoising. The proposed network produces remarkably higher numerical accuracy and better visual image quality than the classical state-of-the-art and CNN algorithms when being evaluated on the three conventional benchmark and three real-world datasets.



### Stomach 3D Reconstruction Based on Virtual Chromoendoscopic Image Generation
- **Arxiv ID**: http://arxiv.org/abs/2004.12288v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.12288v1)
- **Published**: 2020-04-26 04:55:50+00:00
- **Updated**: 2020-04-26 04:55:50+00:00
- **Authors**: Aji Resindra Widya, Yusuke Monno, Masatoshi Okutomi, Sho Suzuki, Takuji Gotoda, Kenji Miki
- **Comment**: Accepted for main conference in EMBC 2020
- **Journal**: None
- **Summary**: Gastric endoscopy is a standard clinical process that enables medical practitioners to diagnose various lesions inside a patient's stomach. If any lesion is found, it is very important to perceive the location of the lesion relative to the global view of the stomach. Our previous research showed that this could be addressed by reconstructing the whole stomach shape from chromoendoscopic images using a structure-from-motion (SfM) pipeline, in which indigo carmine (IC) blue dye sprayed images were used to increase feature matches for SfM by enhancing stomach surface's textures. However, spraying the IC dye to the whole stomach requires additional time, labor, and cost, which is not desirable for patients and practitioners. In this paper, we propose an alternative way to achieve whole stomach 3D reconstruction without the need of the IC dye by generating virtual IC-sprayed (VIC) images based on image-to-image style translation trained on unpaired real no-IC and IC-sprayed images. We have specifically investigated the effect of input and output color channel selection for generating the VIC images and found that translating no-IC green-channel images to IC-sprayed red-channel images gives the best SfM reconstruction result.



### AutoHR: A Strong End-to-end Baseline for Remote Heart Rate Measurement with Neural Searching
- **Arxiv ID**: http://arxiv.org/abs/2004.12292v1
- **DOI**: 10.1109/LSP.2020.3007086
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.12292v1)
- **Published**: 2020-04-26 05:43:21+00:00
- **Updated**: 2020-04-26 05:43:21+00:00
- **Authors**: Zitong Yu, Xiaobai Li, Xuesong Niu, Jingang Shi, Guoying Zhao
- **Comment**: Submitted to IEEE Signal Processing Letters
- **Journal**: None
- **Summary**: Remote photoplethysmography (rPPG), which aims at measuring heart activities without any contact, has great potential in many applications (e.g., remote healthcare). Existing end-to-end rPPG and heart rate (HR) measurement methods from facial videos are vulnerable to the less-constrained scenarios (e.g., with head movement and bad illumination). In this letter, we explore the reason why existing end-to-end networks perform poorly in challenging conditions and establish a strong end-to-end baseline (AutoHR) for remote HR measurement with neural architecture search (NAS). The proposed method includes three parts: 1) a powerful searched backbone with novel Temporal Difference Convolution (TDC), intending to capture intrinsic rPPG-aware clues between frames; 2) a hybrid loss function considering constraints from both time and frequency domains; and 3) spatio-temporal data augmentation strategies for better representation learning. Comprehensive experiments are performed on three benchmark datasets to show our superior performance on both intra- and cross-dataset testing.



### A Spontaneous Driver Emotion Facial Expression (DEFE) Dataset for Intelligent Vehicles
- **Arxiv ID**: http://arxiv.org/abs/2005.08626v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.08626v1)
- **Published**: 2020-04-26 07:15:50+00:00
- **Updated**: 2020-04-26 07:15:50+00:00
- **Authors**: Wenbo Li, Yaodong Cui, Yintao Ma, Xingxin Chen, Guofa Li, Gang Guo, Dongpu Cao
- **Comment**: 13 pages, 7 figures
- **Journal**: None
- **Summary**: In this paper, we introduce a new dataset, the driver emotion facial expression (DEFE) dataset, for driver spontaneous emotions analysis. The dataset includes facial expression recordings from 60 participants during driving. After watching a selected video-audio clip to elicit a specific emotion, each participant completed the driving tasks in the same driving scenario and rated their emotional responses during the driving processes from the aspects of dimensional emotion and discrete emotion. We also conducted classification experiments to recognize the scales of arousal, valence, dominance, as well as the emotion category and intensity to establish baseline results for the proposed dataset. Besides, this paper compared and discussed the differences in facial expressions between driving and non-driving scenarios. The results show that there were significant differences in AUs (Action Units) presence of facial expressions between driving and non-driving scenarios, indicating that human emotional expressions in driving scenarios were different from other life scenarios. Therefore, publishing a human emotion dataset specifically for the driver is necessary for traffic safety improvement. The proposed dataset will be publicly available so that researchers worldwide can use it to develop and examine their driver emotion analysis methods. To the best of our knowledge, this is currently the only public driver facial expression dataset.



### IROS 2019 Lifelong Robotic Vision Challenge -- Lifelong Object Recognition Report
- **Arxiv ID**: http://arxiv.org/abs/2004.14774v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.14774v1)
- **Published**: 2020-04-26 08:33:55+00:00
- **Updated**: 2020-04-26 08:33:55+00:00
- **Authors**: Qi She, Fan Feng, Qi Liu, Rosa H. M. Chan, Xinyue Hao, Chuanlin Lan, Qihan Yang, Vincenzo Lomonaco, German I. Parisi, Heechul Bae, Eoin Brophy, Baoquan Chen, Gabriele Graffieti, Vidit Goel, Hyonyoung Han, Sathursan Kanagarajah, Somesh Kumar, Siew-Kei Lam, Tin Lun Lam, Liang Ma, Davide Maltoni, Lorenzo Pellegrini, Duvindu Piyasena, Shiliang Pu, Debdoot Sheet, Soonyong Song, Youngsung Son, Zhengwei Wang, Tomas E. Ward, Jianwen Wu, Meiqing Wu, Di Xie, Yangsheng Xu, Lin Yang, Qiaoyong Zhong, Liguang Zhou
- **Comment**: 9 pages, 11 figures, 3 tables, accepted into IEEE Robotics and
  Automation Magazine. arXiv admin note: text overlap with arXiv:1911.06487
- **Journal**: None
- **Summary**: This report summarizes IROS 2019-Lifelong Robotic Vision Competition (Lifelong Object Recognition Challenge) with methods and results from the top $8$ finalists (out of over~$150$ teams). The competition dataset (L)ifel(O)ng (R)obotic V(IS)ion (OpenLORIS) - Object Recognition (OpenLORIS-object) is designed for driving lifelong/continual learning research and application in robotic vision domain, with everyday objects in home, office, campus, and mall scenarios. The dataset explicitly quantifies the variants of illumination, object occlusion, object size, camera-object distance/angles, and clutter information. Rules are designed to quantify the learning capability of the robotic vision system when faced with the objects appearing in the dynamic environments in the contest. Individual reports, dataset information, rules, and released source code can be found at the project homepage: "https://lifelong-robotic-vision.github.io/competition/".



### A Global Benchmark of Algorithms for Segmenting Late Gadolinium-Enhanced Cardiac Magnetic Resonance Imaging
- **Arxiv ID**: http://arxiv.org/abs/2004.12314v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.12314v3)
- **Published**: 2020-04-26 08:49:17+00:00
- **Updated**: 2020-05-07 14:05:14+00:00
- **Authors**: Zhaohan Xiong, Qing Xia, Zhiqiang Hu, Ning Huang, Cheng Bian, Yefeng Zheng, Sulaiman Vesal, Nishant Ravikumar, Andreas Maier, Xin Yang, Pheng-Ann Heng, Dong Ni, Caizi Li, Qianqian Tong, Weixin Si, Elodie Puybareau, Younes Khoudli, Thierry Geraud, Chen Chen, Wenjia Bai, Daniel Rueckert, Lingchao Xu, Xiahai Zhuang, Xinzhe Luo, Shuman Jia, Maxime Sermesant, Yashu Liu, Kuanquan Wang, Davide Borra, Alessandro Masci, Cristiana Corsi, Coen de Vente, Mitko Veta, Rashed Karim, Chandrakanth Jayachandran Preetha, Sandy Engelhardt, Menyun Qiao, Yuanyuan Wang, Qian Tao, Marta Nunez-Garcia, Oscar Camara, Nicolo Savioli, Pablo Lamata, Jichao Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: Segmentation of cardiac images, particularly late gadolinium-enhanced magnetic resonance imaging (LGE-MRI) widely used for visualizing diseased cardiac structures, is a crucial first step for clinical diagnosis and treatment. However, direct segmentation of LGE-MRIs is challenging due to its attenuated contrast. Since most clinical studies have relied on manual and labor-intensive approaches, automatic methods are of high interest, particularly optimized machine learning approaches. To address this, we organized the "2018 Left Atrium Segmentation Challenge" using 154 3D LGE-MRIs, currently the world's largest cardiac LGE-MRI dataset, and associated labels of the left atrium segmented by three medical experts, ultimately attracting the participation of 27 international teams. In this paper, extensive analysis of the submitted algorithms using technical and biological metrics was performed by undergoing subgroup analysis and conducting hyper-parameter analysis, offering an overall picture of the major design choices of convolutional neural networks (CNNs) and practical considerations for achieving state-of-the-art left atrium segmentation. Results show the top method achieved a dice score of 93.2% and a mean surface to a surface distance of 0.7 mm, significantly outperforming prior state-of-the-art. Particularly, our analysis demonstrated that double, sequentially used CNNs, in which a first CNN is used for automatic region-of-interest localization and a subsequent CNN is used for refined regional segmentation, achieved far superior results than traditional methods and pipelines containing single CNNs. This large-scale benchmarking study makes a significant step towards much-improved segmentation methods for cardiac LGE-MRIs, and will serve as an important benchmark for evaluating and comparing the future works in the field.



### DeepSeg: Deep Neural Network Framework for Automatic Brain Tumor Segmentation using Magnetic Resonance FLAIR Images
- **Arxiv ID**: http://arxiv.org/abs/2004.12333v1
- **DOI**: 10.1007/s11548-020-02186-z
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.12333v1)
- **Published**: 2020-04-26 09:50:02+00:00
- **Updated**: 2020-04-26 09:50:02+00:00
- **Authors**: Ramy A. Zeineldin, Mohamed E. Karar, Jan Coburger, Christian R. Wirtz, Oliver Burgert
- **Comment**: Accepted to International Journal of Computer Assisted Radiology and
  Surgery
- **Journal**: None
- **Summary**: Purpose: Gliomas are the most common and aggressive type of brain tumors due to their infiltrative nature and rapid progression. The process of distinguishing tumor boundaries from healthy cells is still a challenging task in the clinical routine. Fluid-Attenuated Inversion Recovery (FLAIR) MRI modality can provide the physician with information about tumor infiltration. Therefore, this paper proposes a new generic deep learning architecture; namely DeepSeg for fully automated detection and segmentation of the brain lesion using FLAIR MRI data.   Methods: The developed DeepSeg is a modular decoupling framework. It consists of two connected core parts based on an encoding and decoding relationship. The encoder part is a convolutional neural network (CNN) responsible for spatial information extraction. The resulting semantic map is inserted into the decoder part to get the full resolution probability map. Based on modified U-Net architecture, different CNN models such as Residual Neural Network (ResNet), Dense Convolutional Network (DenseNet), and NASNet have been utilized in this study.   Results: The proposed deep learning architectures have been successfully tested and evaluated on-line based on MRI datasets of Brain Tumor Segmentation (BraTS 2019) challenge, including s336 cases as training data and 125 cases for validation data. The dice and Hausdorff distance scores of obtained segmentation results are about 0.81 to 0.84 and 9.8 to 19.7 correspondingly.   Conclusion: This study showed successful feasibility and comparative performance of applying different deep learning models in a new DeepSeg framework for automated brain tumor segmentation in FLAIR MR images. The proposed DeepSeg is open-source and freely available at https://github.com/razeineldin/DeepSeg/.



### KrakN: Transfer Learning framework for thin crack detection in infrastructure maintenance
- **Arxiv ID**: http://arxiv.org/abs/2004.12337v2
- **DOI**: 10.1016/j.softx.2021.100893
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2004.12337v2)
- **Published**: 2020-04-26 09:57:36+00:00
- **Updated**: 2020-10-11 17:15:12+00:00
- **Authors**: Mateusz Żarski, Bartosz Wójcik, Jarosław Adam Miszczak
- **Comment**: 23 pages, 15 figures and flowcharts, software available at
  https://github.com/MatZar01/KrakN, https://doi.org/10.5281/zenodo.3764697,
  and https://doi.org/10.5281/zenodo.3755452, dataset available from
  https://doi.org/10.5281/zenodo.3759845
- **Journal**: SoftwareX, Vol 16, 100893 (2021)
- **Summary**: Monitoring the technical condition of infrastructure is a crucial element to its maintenance. Currently applied methods are outdated, labour-intensive and inaccurate. At the same time, the latest methods using Artificial Intelligence techniques are severely limited in their application due to two main factors -- labour-intensive gathering of new datasets and high demand for computing power. We propose to utilize custom made framework -- KrakN, to overcome these limiting factors. It enables the development of unique infrastructure defects detectors on digital images, achieving the accuracy of above 90%. The framework supports semi-automatic creation of new datasets and has modest computing power requirements. It is implemented in the form of a ready-to-use software package openly distributed to the public. Thus, it can be used to immediately implement the methods proposed in this paper in the process of infrastructure management by government units, regardless of their financial capabilities.



### Climate Adaptation: Reliably Predicting from Imbalanced Satellite Data
- **Arxiv ID**: http://arxiv.org/abs/2004.12344v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, I.2.6; I.2.10; I.2.9
- **Links**: [PDF](http://arxiv.org/pdf/2004.12344v1)
- **Published**: 2020-04-26 10:41:08+00:00
- **Updated**: 2020-04-26 10:41:08+00:00
- **Authors**: Ruchit Rawal, Prabhu Pradhan
- **Comment**: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
  Workshops: Agriculture-Vision 2020
- **Journal**: None
- **Summary**: The utility of aerial imagery (Satellite, Drones) has become an invaluable information source for cross-disciplinary applications, especially for crisis management. Most of the mapping and tracking efforts are manual which is resource-intensive and often lead to delivery delays. Deep Learning methods have boosted the capacity of relief efforts via recognition, detection, and are now being used for non-trivial applications. However the data commonly available is highly imbalanced (similar to other real-life applications) which severely hampers the neural network's capabilities, this reduces robustness and trust. We give an overview on different kinds of techniques being used for handling such extreme settings and present solutions aimed at maximizing performance on minority classes using a diverse set of methods (ranging from architectural tuning to augmentation) which as a combination generalizes for all minority classes. We hope to amplify cross-disciplinary efforts by enhancing model reliability.



### When CNNs Meet Random RNNs: Towards Multi-Level Analysis for RGB-D Object and Scene Recognition
- **Arxiv ID**: http://arxiv.org/abs/2004.12349v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.12349v2)
- **Published**: 2020-04-26 10:58:27+00:00
- **Updated**: 2022-01-11 07:41:19+00:00
- **Authors**: Ali Caglayan, Nevrez Imamoglu, Ahmet Burak Can, Ryosuke Nakamura
- **Comment**: 15 pages, 9 figures, 5 tables (To appear in Computer Vision and Image
  Understanding, Elsevier)
- **Journal**: None
- **Summary**: Recognizing objects and scenes are two challenging but essential tasks in image understanding. In particular, the use of RGB-D sensors in handling these tasks has emerged as an important area of focus for better visual understanding. Meanwhile, deep neural networks, specifically convolutional neural networks (CNNs), have become widespread and have been applied to many visual tasks by replacing hand-crafted features with effective deep features. However, it is an open problem how to exploit deep features from a multi-layer CNN model effectively. In this paper, we propose a novel two-stage framework that extracts discriminative feature representations from multi-modal RGB-D images for object and scene recognition tasks. In the first stage, a pretrained CNN model has been employed as a backbone to extract visual features at multiple levels. The second stage maps these features into high level representations with a fully randomized structure of recursive neural networks (RNNs) efficiently. To cope with the high dimensionality of CNN activations, a random weighted pooling scheme has been proposed by extending the idea of randomness in RNNs. Multi-modal fusion has been performed through a soft voting approach by computing weights based on individual recognition confidences (i.e. SVM scores) of RGB and depth streams separately. This produces consistent class label estimation in final RGB-D classification performance. Extensive experiments verify that fully randomized structure in RNN stage encodes CNN activations to discriminative solid features successfully. Comparative experimental results on the popular Washington RGB-D Object and SUN RGB-D Scene datasets show that the proposed approach achieves superior or on-par performance compared to state-of-the-art methods both in object and scene recognition tasks. Code is available at https://github.com/acaglayan/CNN_randRNN.



### Joint Liver Lesion Segmentation and Classification via Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/2004.12352v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.12352v2)
- **Published**: 2020-04-26 11:06:23+00:00
- **Updated**: 2020-05-24 16:08:17+00:00
- **Authors**: Michal Heker, Hayit Greenspan
- **Comment**: Accepted to MIDL 2020
- **Journal**: None
- **Summary**: Transfer learning and joint learning approaches are extensively used to improve the performance of Convolutional Neural Networks (CNNs). In medical imaging applications in which the target dataset is typically very small, transfer learning improves feature learning while joint learning has shown effectiveness in improving the network's generalization and robustness. In this work, we study the combination of these two approaches for the problem of liver lesion segmentation and classification. For this purpose, 332 abdominal CT slices containing lesion segmentation and classification of three lesion types are evaluated. For feature learning, the dataset of MICCAI 2017 Liver Tumor Segmentation (LiTS) Challenge is used. Joint learning shows improvement in both segmentation and classification results. We show that a simple joint framework outperforms the commonly used multi-task architecture (Y-Net), achieving an improvement of 10% in classification accuracy, compared to a 3% improvement with Y-Net.



### Evaluation Metrics for Conditional Image Generation
- **Arxiv ID**: http://arxiv.org/abs/2004.12361v2
- **DOI**: 10.1007/s11263-020-01424-w
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.12361v2)
- **Published**: 2020-04-26 12:15:16+00:00
- **Updated**: 2021-02-08 12:36:48+00:00
- **Authors**: Yaniv Benny, Tomer Galanti, Sagie Benaim, Lior Wolf
- **Comment**: To be published in "INTERNATIONAL JOURNAL OF COMPUTER VISION"
- **Journal**: None
- **Summary**: We present two new metrics for evaluating generative models in the class-conditional image generation setting. These metrics are obtained by generalizing the two most popular unconditional metrics: the Inception Score (IS) and the Fre'chet Inception Distance (FID). A theoretical analysis shows the motivation behind each proposed metric and links the novel metrics to their unconditional counterparts. The link takes the form of a product in the case of IS or an upper bound in the FID case. We provide an extensive empirical evaluation, comparing the metrics to their unconditional variants and to other metrics, and utilize them to analyze existing generative models, thus providing additional insights about their performance, from unlearned classes to mode collapse.



### Hyperspectral Images Classification Based on Multi-scale Residual Network
- **Arxiv ID**: http://arxiv.org/abs/2004.12381v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.12381v2)
- **Published**: 2020-04-26 13:46:52+00:00
- **Updated**: 2020-05-12 01:56:40+00:00
- **Authors**: Xiangdong Zhang, Tengjun Wang, Yun Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Because hyperspectral remote sensing images contain a lot of redundant information and the data structure is highly non-linear, leading to low classification accuracy of traditional machine learning methods. The latest research shows that hyperspectral image classification based on deep convolutional neural network has high accuracy. However, when a small amount of data is used for training, the classification accuracy of deep learning methods is greatly reduced. In order to solve the problem of low classification accuracy of existing algorithms on small samples of hyperspectral images, a multi-scale residual network is proposed. The multi-scale extraction and fusion of spatial and spectral features is realized by adding a branch structure into the residual block and using convolution kernels of different sizes in the branch. The spatial and spectral information contained in hyperspectral images are fully utilized to improve the classification accuracy. In addition, in order to improve the speed and prevent overfitting, the model uses dynamic learning rate, BN and Dropout strategies. The experimental results show that the overall classification accuracy of this method is 99.07% and 99.96% respectively in the data set of Indian Pines and Pavia University, which is better than other algorithms.



### Towards Feature Space Adversarial Attack
- **Arxiv ID**: http://arxiv.org/abs/2004.12385v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.12385v2)
- **Published**: 2020-04-26 13:56:31+00:00
- **Updated**: 2020-12-16 03:47:44+00:00
- **Authors**: Qiuling Xu, Guanhong Tao, Siyuan Cheng, Xiangyu Zhang
- **Comment**: AAAI 2021
- **Journal**: None
- **Summary**: We propose a new adversarial attack to Deep Neural Networks for image classification. Different from most existing attacks that directly perturb input pixels, our attack focuses on perturbing abstract features, more specifically, features that denote styles, including interpretable styles such as vivid colors and sharp outlines, and uninterpretable ones. It induces model misclassfication by injecting imperceptible style changes through an optimization procedure. We show that our attack can generate adversarial samples that are more natural-looking than the state-of-the-art unbounded attacks. The experiment also supports that existing pixel-space adversarial attack detection and defense techniques can hardly ensure robustness in the style related feature space.



### Disentangled Image Generation Through Structured Noise Injection
- **Arxiv ID**: http://arxiv.org/abs/2004.12411v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.12411v2)
- **Published**: 2020-04-26 15:15:19+00:00
- **Updated**: 2020-05-05 15:41:16+00:00
- **Authors**: Yazeed Alharbi, Peter Wonka
- **Comment**: CVPR2020 Oral. Project page:
  https://github.com/yalharbi/StructuredNoiseInjection
- **Journal**: None
- **Summary**: We explore different design choices for injecting noise into generative adversarial networks (GANs) with the goal of disentangling the latent space. Instead of traditional approaches, we propose feeding multiple noise codes through separate fully-connected layers respectively. The aim is restricting the influence of each noise code to specific parts of the generated image. We show that disentanglement in the first layer of the generator network leads to disentanglement in the generated image. Through a grid-based structure, we achieve several aspects of disentanglement without complicating the network architecture and without requiring labels. We achieve spatial disentanglement, scale-space disentanglement, and disentanglement of the foreground object from the background style allowing fine-grained control over the generated images. Examples include changing facial expressions in face images, changing beak length in bird images, and changing car dimensions in car images. This empirically leads to better disentanglement scores than state-of-the-art methods on the FFHQ dataset.



### Cross-Domain Structure Preserving Projection for Heterogeneous Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2004.12427v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.12427v3)
- **Published**: 2020-04-26 16:22:28+00:00
- **Updated**: 2021-10-09 03:27:41+00:00
- **Authors**: Qian Wang, Toby P. Breckon
- **Comment**: Technical Report
- **Journal**: None
- **Summary**: Heterogeneous Domain Adaptation (HDA) addresses the transfer learning problems where data from the source and target domains are of different modalities (e.g., texts and images) or feature dimensions (e.g., features extracted with different methods). It is useful for multi-modal data analysis. Traditional domain adaptation algorithms assume that the representations of source and target samples reside in the same feature space, hence are likely to fail in solving the heterogeneous domain adaptation problem. Contemporary state-of-the-art HDA approaches are usually composed of complex optimization objectives for favourable performance and are therefore computationally expensive and less generalizable. To address these issues, we propose a novel Cross-Domain Structure Preserving Projection (CDSPP) algorithm for HDA. As an extension of the classic LPP to heterogeneous domains, CDSPP aims to learn domain-specific projections to map sample features from source and target domains into a common subspace such that the class consistency is preserved and data distributions are sufficiently aligned. CDSPP is simple and has deterministic solutions by solving a generalized eigenvalue problem. It is naturally suitable for supervised HDA but has also been extended for semi-supervised HDA where the unlabelled target domain samples are available. Extensive experiments have been conducted on commonly used benchmark datasets (i.e. Office-Caltech, Multilingual Reuters Collection, NUS-WIDE-ImageNet) for HDA as well as the Office-Home dataset firstly introduced for HDA by ourselves due to its significantly larger number of classes than the existing ones (65 vs 10, 6 and 8). The experimental results of both supervised and semi-supervised HDA demonstrate the superior performance of our proposed method against contemporary state-of-the-art methods.



### Dynamic Scale Training for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2004.12432v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.12432v2)
- **Published**: 2020-04-26 16:48:17+00:00
- **Updated**: 2021-03-14 05:22:59+00:00
- **Authors**: Yukang Chen, Peizhen Zhang, Zeming Li, Yanwei Li, Xiangyu Zhang, Lu Qi, Jian Sun, Jiaya Jia
- **Comment**: Technical Report
- **Journal**: None
- **Summary**: We propose a Dynamic Scale Training paradigm (abbreviated as DST) to mitigate scale variation challenge in object detection. Previous strategies like image pyramid, multi-scale training, and their variants are aiming at preparing scale-invariant data for model optimization. However, the preparation procedure is unaware of the following optimization process that restricts their capability in handling the scale variation. Instead, in our paradigm, we use feedback information from the optimization process to dynamically guide the data preparation. The proposed method is surprisingly simple yet obtains significant gains (2%+ Average Precision on MS COCO dataset), outperforming previous methods. Experimental results demonstrate the efficacy of our proposed DST method towards scale variation handling. It could also generalize to various backbones, benchmarks, and other challenging downstream tasks like instance segmentation. It does not introduce inference overhead and could serve as a free lunch for general detection configurations. Besides, it also facilitates efficient training due to fast convergence. Code and models are available at github.com/yukang2017/Stitcher.



### All you need is a second look: Towards Tighter Arbitrary shape text detection
- **Arxiv ID**: http://arxiv.org/abs/2004.12436v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.12436v1)
- **Published**: 2020-04-26 17:03:41+00:00
- **Updated**: 2020-04-26 17:03:41+00:00
- **Authors**: Meng Cao, Yuexian Zou
- **Comment**: 5 pages, 6 figures
- **Journal**: None
- **Summary**: Deep learning-based scene text detection methods have progressed substantially over the past years. However, there remain several problems to be solved. Generally, long curve text instances tend to be fragmented because of the limited receptive field size of CNN. Besides, simple representations using rectangle or quadrangle bounding boxes fall short when dealing with more challenging arbitrary-shaped texts. In addition, the scale of text instances varies greatly which leads to the difficulty of accurate prediction through a single segmentation network. To address these problems, we innovatively propose a two-stage segmentation based arbitrary text detector named \textit{NASK} (\textbf{N}eed \textbf{A} \textbf{S}econd loo\textbf{K}). Specifically, \textit{NASK} consists of a Text Instance Segmentation network namely \textit{TIS} (\(1^{st}\) stage), a Text RoI Pooling module and a Fiducial pOint eXpression module termed as \textit{FOX} (\(2^{nd}\) stage). Firstly, \textit{TIS} conducts instance segmentation to obtain rectangle text proposals with a proposed Group Spatial and Channel Attention module (\textit{GSCA}) to augment the feature expression. Then, Text RoI Pooling transforms these rectangles to the fixed size. Finally, \textit{FOX} is introduced to reconstruct text instances with a more tighter representation using the predicted geometrical attributes including text center line, text line orientation, character scale and character orientation. Experimental results on two public benchmarks including \textit{Total-Text} and \textit{SCUT-CTW1500} have demonstrated that the proposed \textit{NASK} achieves state-of-the-art results.



### One-Shot Identity-Preserving Portrait Reenactment
- **Arxiv ID**: http://arxiv.org/abs/2004.12452v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.12452v1)
- **Published**: 2020-04-26 18:30:33+00:00
- **Updated**: 2020-04-26 18:30:33+00:00
- **Authors**: Sitao Xiang, Yuming Gu, Pengda Xiang, Mingming He, Koki Nagano, Haiwei Chen, Hao Li
- **Comment**: 29 pages, 14 figures
- **Journal**: None
- **Summary**: We present a deep learning-based framework for portrait reenactment from a single picture of a target (one-shot) and a video of a driving subject. Existing facial reenactment methods suffer from identity mismatch and produce inconsistent identities when a target and a driving subject are different (cross-subject), especially in one-shot settings. In this work, we aim to address identity preservation in cross-subject portrait reenactment from a single picture. We introduce a novel technique that can disentangle identity from expressions and poses, allowing identity preserving portrait reenactment even when the driver's identity is very different from that of the target. This is achieved by a novel landmark disentanglement network (LD-Net), which predicts personalized facial landmarks that combine the identity of the target with expressions and poses from a different subject. To handle portrait reenactment from unseen subjects, we also introduce a feature dictionary-based generative adversarial network (FD-GAN), which locally translates 2D landmarks into a personalized portrait, enabling one-shot portrait reenactment under large pose and expression variations. We validate the effectiveness of our identity disentangling capabilities via an extensive ablation study, and our method produces consistent identities for cross-subject portrait reenactment. Our comprehensive experiments show that our method significantly outperforms the state-of-the-art single-image facial reenactment methods. We will release our code and models for academic use.



### Designing a physically-feasible colour filter to make a camera more colorimetric
- **Arxiv ID**: http://arxiv.org/abs/2004.12464v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.12464v1)
- **Published**: 2020-04-26 19:45:54+00:00
- **Updated**: 2020-04-26 19:45:54+00:00
- **Authors**: Yuteng Zhu
- **Comment**: 4 pages, 2 figures
- **Journal**: None
- **Summary**: Previously, a method has been developed to find the best colour filter for a given camera which results in the new effective camera sensitivities that best meet the Luther condition. That is, the new sensitivities are approximately linearly related to the XYZ colour matching functions. However, with no constraint, the filter derived from this Luther-condition based optimisation can be rather non-smooth and transmit very little light which are impractical for fabrication.   In this paper, we extend the Luther-condition filter optimisation method to allow us to incorporate both the smoothness and transmittance bounds of the recovered filter which are key practical concerns. Experiments demonstrate that we can find physically realisable filters which are smooth and reasonably transmissive with which the effective "camera+filter" becomes significantly more colorimetric.



### On the Limits to Multi-Modal Popularity Prediction on Instagram -- A New Robust, Efficient and Explainable Baseline
- **Arxiv ID**: http://arxiv.org/abs/2004.12482v2
- **DOI**: 10.5220/0010377112001209
- **Categories**: **cs.CV**, cs.SI
- **Links**: [PDF](http://arxiv.org/pdf/2004.12482v2)
- **Published**: 2020-04-26 21:21:50+00:00
- **Updated**: 2021-02-20 13:39:45+00:00
- **Authors**: Christoffer Riis, Damian Konrad Kowalczyk, Lars Kai Hansen
- **Comment**: Presented at ICAART 2021
- **Journal**: Proceedings of the 13th International Conference on Agents and
  Artificial Intelligence - Volume 2: ICAART, ISBN 978-989-758-484-8, pages
  1200-1209, 2021
- **Summary**: Our global population contributes visual content on platforms like Instagram, attempting to express themselves and engage their audiences, at an unprecedented and increasing rate. In this paper, we revisit the popularity prediction on Instagram. We present a robust, efficient, and explainable baseline for population-based popularity prediction, achieving strong ranking performance. We employ the latest methods in computer vision to maximize the information extracted from the visual modality. We use transfer learning to extract visual semantics such as concepts, scenes, and objects, allowing a new level of scrutiny in an extensive, explainable ablation study. We inform feature selection towards a robust and scalable model, but also illustrate feature interactions, offering new directions for further inquiry in computational social science. Our strongest models inform a lower limit to population-based predictability of popularity on Instagram. The models are immediately applicable to social media monitoring and influencer identification.



### Weakly Supervised Semantic Segmentation in 3D Graph-Structured Point Clouds of Wild Scenes
- **Arxiv ID**: http://arxiv.org/abs/2004.12498v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.12498v2)
- **Published**: 2020-04-26 23:02:23+00:00
- **Updated**: 2020-05-17 21:14:07+00:00
- **Authors**: Haiyan Wang, Xuejian Rong, Liang Yang, Jinglun Feng, Jizhong Xiao, Yingli Tian
- **Comment**: 13 pages, 8 figures, Under review as a journal paper at CVIU
- **Journal**: None
- **Summary**: The deficiency of 3D segmentation labels is one of the main obstacles to effective point cloud segmentation, especially for scenes in the wild with varieties of different objects. To alleviate this issue, we propose a novel deep graph convolutional network-based framework for large-scale semantic scene segmentation in point clouds with sole 2D supervision. Different with numerous preceding multi-view supervised approaches focusing on single object point clouds, we argue that 2D supervision is capable of providing sufficient guidance information for training 3D semantic segmentation models of natural scene point clouds while not explicitly capturing their inherent structures, even with only single view per training sample. Specifically, a Graph-based Pyramid Feature Network (GPFN) is designed to implicitly infer both global and local features of point sets and an Observability Network (OBSNet) is introduced to further solve object occlusion problem caused by complicated spatial relations of objects in 3D scenes. During the projection process, perspective rendering and semantic fusion modules are proposed to provide refined 2D supervision signals for training along with a 2D-3D joint optimization strategy. Extensive experimental results demonstrate the effectiveness of our 2D supervised framework, which achieves comparable results with the state-of-the-art approaches trained with full 3D labels, for semantic point cloud segmentation on the popular SUNCG synthetic dataset and S3DIS real-world dataset.



