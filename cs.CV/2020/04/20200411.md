# Arxiv Papers in cs.CV on 2020-04-11
### Towards Anomaly Detection in Dashcam Videos
- **Arxiv ID**: http://arxiv.org/abs/2004.05261v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.05261v2)
- **Published**: 2020-04-11 00:10:40+00:00
- **Updated**: 2020-05-12 02:04:09+00:00
- **Authors**: Sanjay Haresh, Sateesh Kumar, M. Zeeshan Zia, Quoc-Huy Tran
- **Comment**: To appear at IV 2020
- **Journal**: None
- **Summary**: Inexpensive sensing and computation, as well as insurance innovations, have made smart dashboard cameras ubiquitous. Increasingly, simple model-driven computer vision algorithms focused on lane departures or safe following distances are finding their way into these devices. Unfortunately, the long-tailed distribution of road hazards means that these hand-crafted pipelines are inadequate for driver safety systems. We propose to apply data-driven anomaly detection ideas from deep learning to dashcam videos, which hold the promise of bridging this gap. Unfortunately, there exists almost no literature applying anomaly understanding to moving cameras, and correspondingly there is also a lack of relevant datasets. To counter this issue, we present a large and diverse dataset of truck dashcam videos, namely RetroTrucks, that includes normal and anomalous driving scenes. We apply: (i) one-class classification loss and (ii) reconstruction-based loss, for anomaly detection on RetroTrucks as well as on existing static-camera datasets. We introduce formulations for modeling object interactions in this context as priors. Our experiments indicate that our dataset is indeed more challenging than standard anomaly detection datasets, and previous anomaly detection methods do not perform well here out-of-the-box. In addition, we share insights into the behavior of these two important families of anomaly detection approaches on dashcam data.



### Multi-View Matching (MVM): Facilitating Multi-Person 3D Pose Estimation Learning with Action-Frozen People Video
- **Arxiv ID**: http://arxiv.org/abs/2004.05275v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.05275v1)
- **Published**: 2020-04-11 01:09:50+00:00
- **Updated**: 2020-04-11 01:09:50+00:00
- **Authors**: Yeji Shen, C. -C. Jay Kuo
- **Comment**: 16 pages, 6 figures, submitted JVCI
- **Journal**: None
- **Summary**: To tackle the challeging problem of multi-person 3D pose estimation from a single image, we propose a multi-view matching (MVM) method in this work. The MVM method generates reliable 3D human poses from a large-scale video dataset, called the Mannequin dataset, that contains action-frozen people immitating mannequins. With a large amount of in-the-wild video data labeled by 3D supervisions automatically generated by MVM, we are able to train a neural network that takes a single image as the input for multi-person 3D pose estimation. The core technology of MVM lies in effective alignment of 2D poses obtained from multiple views of a static scene that has a strong geometric constraint. Our objective is to maximize mutual consistency of 2D poses estimated in multiple frames, where geometric constraints as well as appearance similarities are taken into account simultaneously. To demonstrate the effectiveness of 3D supervisions provided by the MVM method, we conduct experiments on the 3DPW and the MSCOCO datasets and show that our proposed solution offers the state-of-the-art performance.



### From Quantized DNNs to Quantizable DNNs
- **Arxiv ID**: http://arxiv.org/abs/2004.05284v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.05284v1)
- **Published**: 2020-04-11 02:25:46+00:00
- **Updated**: 2020-04-11 02:25:46+00:00
- **Authors**: Kunyuan Du, Ya Zhang, Haibing Guan
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes Quantizable DNNs, a special type of DNNs that can flexibly quantize its bit-width (denoted as `bit modes' thereafter) during execution without further re-training. To simultaneously optimize for all bit modes, a combinational loss of all bit modes is proposed, which enforces consistent predictions ranging from low-bit mode to 32-bit mode. This Consistency-based Loss may also be viewed as certain form of regularization during training. Because outputs of matrix multiplication in different bit modes have different distributions, we introduce Bit-Specific Batch Normalization so as to reduce conflicts among different bit modes. Experiments on CIFAR100 and ImageNet have shown that compared to quantized DNNs, Quantizable DNNs not only have much better flexibility, but also achieve even higher classification accuracy. Ablation studies further verify that the regularization through the consistency-based loss indeed improves the model's generalization performance.



### Detached Error Feedback for Distributed SGD with Random Sparsification
- **Arxiv ID**: http://arxiv.org/abs/2004.05298v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.05298v3)
- **Published**: 2020-04-11 03:50:59+00:00
- **Updated**: 2022-06-13 13:19:06+00:00
- **Authors**: An Xu, Heng Huang
- **Comment**: None
- **Journal**: None
- **Summary**: The communication bottleneck has been a critical problem in large-scale distributed deep learning. In this work, we study distributed SGD with random block-wise sparsification as the gradient compressor, which is ring-allreduce compatible and highly computation-efficient but leads to inferior performance. To tackle this important issue, we improve the communication-efficient distributed SGD from a novel aspect, that is, the trade-off between the variance and second moment of the gradient. With this motivation, we propose a new detached error feedback (DEF) algorithm, which shows better convergence bound than error feedback for non-convex problems. We also propose DEF-A to accelerate the generalization of DEF at the early stages of the training, which shows better generalization bounds than DEF. Furthermore, we establish the connection between communication-efficient distributed SGD and SGD with iterate averaging (SGD-IA) for the first time. Extensive deep learning experiments show significant empirical improvement of the proposed methods under various settings.



### Object-oriented SLAM using Quadrics and Symmetry Properties for Indoor Environments
- **Arxiv ID**: http://arxiv.org/abs/2004.05303v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.05303v1)
- **Published**: 2020-04-11 04:15:25+00:00
- **Updated**: 2020-04-11 04:15:25+00:00
- **Authors**: Ziwei Liao, Wei Wang, Xianyu Qi, Xiaoyu Zhang, Lin Xue, Jianzhen Jiao, Ran Wei
- **Comment**: Submission to IROS 2020. Video: https://youtu.be/u9zRBp4TPIs
- **Journal**: None
- **Summary**: Aiming at the application environment of indoor mobile robots, this paper proposes a sparse object-level SLAM algorithm based on an RGB-D camera. A quadric representation is used as a landmark to compactly model objects, including their position, orientation, and occupied space. The state-of-art quadric-based SLAM algorithm faces the observability problem caused by the limited perspective under the plane trajectory of the mobile robot. To solve the problem, the proposed algorithm fuses both object detection and point cloud data to estimate the quadric parameters. It finishes the quadric initialization based on a single frame of RGB-D data, which significantly reduces the requirements for perspective changes. As objects are often observed locally, the proposed algorithm uses the symmetrical properties of indoor artificial objects to estimate the occluded parts to obtain more accurate quadric parameters. Experiments have shown that compared with the state-of-art algorithm, especially on the forward trajectory of mobile robots, the proposed algorithm significantly improves the accuracy and convergence speed of quadric reconstruction. Finally, we made available an opensource implementation to replicate the experiments.



### Inter-Region Affinity Distillation for Road Marking Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2004.05304v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.05304v1)
- **Published**: 2020-04-11 04:26:37+00:00
- **Updated**: 2020-04-11 04:26:37+00:00
- **Authors**: Yuenan Hou, Zheng Ma, Chunxiao Liu, Tak-Wai Hui, Chen Change Loy
- **Comment**: 10 pages, 10 figures; This paper is accepted by CVPR 2020; Our code
  is available at https://github.com/cardwing/Codes-for-IntRA-KD
- **Journal**: None
- **Summary**: We study the problem of distilling knowledge from a large deep teacher network to a much smaller student network for the task of road marking segmentation. In this work, we explore a novel knowledge distillation (KD) approach that can transfer 'knowledge' on scene structure more effectively from a teacher to a student model. Our method is known as Inter-Region Affinity KD (IntRA-KD). It decomposes a given road scene image into different regions and represents each region as a node in a graph. An inter-region affinity graph is then formed by establishing pairwise relationships between nodes based on their similarity in feature distribution. To learn structural knowledge from the teacher network, the student is required to match the graph generated by the teacher. The proposed method shows promising results on three large-scale road marking segmentation benchmarks, i.e., ApolloScape, CULane and LLAMAS, by taking various lightweight models as students and ResNet-101 as the teacher. IntRA-KD consistently brings higher performance gains on all lightweight models, compared to previous distillation methods. Our code is available at https://github.com/cardwing/Codes-for-IntRA-KD.



### Probabilistic Oriented Object Detection in Automotive Radar
- **Arxiv ID**: http://arxiv.org/abs/2004.05310v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2004.05310v2)
- **Published**: 2020-04-11 05:29:32+00:00
- **Updated**: 2020-04-18 03:49:39+00:00
- **Authors**: Xu Dong, Pengluo Wang, Pengyue Zhang, Langechuan Liu
- **Comment**: Accepted to CVPR2020
- **Journal**: None
- **Summary**: Autonomous radar has been an integral part of advanced driver assistance systems due to its robustness to adverse weather and various lighting conditions. Conventional automotive radars use digital signal processing (DSP) algorithms to process raw data into sparse radar pins that do not provide information regarding the size and orientation of the objects. In this paper, we propose a deep-learning based algorithm for radar object detection. The algorithm takes in radar data in its raw tensor representation and places probabilistic oriented bounding boxes around the detected objects in bird's-eye-view space. We created a new multimodal dataset with 102544 frames of raw radar and synchronized LiDAR data. To reduce human annotation effort we developed a scalable pipeline to automatically annotate ground truth using LiDAR as reference. Based on this dataset we developed a vehicle detection pipeline using raw radar data as the only input. Our best performing radar detection model achieves 77.28\% AP under oriented IoU of 0.3. To the best of our knowledge, this is the first attempt to investigate object detection with raw radar data for conventional corner automotive radars.



### Domain Adaptive Transfer Attack (DATA)-based Segmentation Networks for Building Extraction from Aerial Images
- **Arxiv ID**: http://arxiv.org/abs/2004.11819v2
- **DOI**: 10.1109/TGRS.2020.3010055
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.11819v2)
- **Published**: 2020-04-11 06:17:13+00:00
- **Updated**: 2020-04-29 06:12:03+00:00
- **Authors**: Younghwan Na, Jun Hee Kim, Kyungsu Lee, Juhum Park, Jae Youn Hwang, Jihwan P. Choi
- **Comment**: 11pages, 12 figures
- **Journal**: None
- **Summary**: Semantic segmentation models based on convolutional neural networks (CNNs) have gained much attention in relation to remote sensing and have achieved remarkable performance for the extraction of buildings from high-resolution aerial images. However, the issue of limited generalization for unseen images remains. When there is a domain gap between the training and test datasets, CNN-based segmentation models trained by a training dataset fail to segment buildings for the test dataset. In this paper, we propose segmentation networks based on a domain adaptive transfer attack (DATA) scheme for building extraction from aerial images. The proposed system combines the domain transfer and adversarial attack concepts. Based on the DATA scheme, the distribution of the input images can be shifted to that of the target images while turning images into adversarial examples against a target network. Defending adversarial examples adapted to the target domain can overcome the performance degradation due to the domain gap and increase the robustness of the segmentation model. Cross-dataset experiments and the ablation study are conducted for the three different datasets: the Inria aerial image labeling dataset, the Massachusetts building dataset, and the WHU East Asia dataset. Compared to the performance of the segmentation network without the DATA scheme, the proposed method shows improvements in the overall IoU. Moreover, it is verified that the proposed method outperforms even when compared to feature adaptation (FA) and output space adaptation (OSA).



### KD-MRI: A knowledge distillation framework for image reconstruction and image restoration in MRI workflow
- **Arxiv ID**: http://arxiv.org/abs/2004.05319v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.05319v1)
- **Published**: 2020-04-11 06:21:28+00:00
- **Updated**: 2020-04-11 06:21:28+00:00
- **Authors**: Balamurali Murugesan, Sricharan Vijayarangan, Kaushik Sarveswaran, Keerthi Ram, Mohanasankar Sivaprakasam
- **Comment**: Accepted in MIDL 2020. Code available
- **Journal**: None
- **Summary**: Deep learning networks are being developed in every stage of the MRI workflow and have provided state-of-the-art results. However, this has come at the cost of increased computation requirement and storage. Hence, replacing the networks with compact models at various stages in the MRI workflow can significantly reduce the required storage space and provide considerable speedup. In computer vision, knowledge distillation is a commonly used method for model compression. In our work, we propose a knowledge distillation (KD) framework for the image to image problems in the MRI workflow in order to develop compact, low-parameter models without a significant drop in performance. We propose a combination of the attention-based feature distillation method and imitation loss and demonstrate its effectiveness on the popular MRI reconstruction architecture, DC-CNN. We conduct extensive experiments using Cardiac, Brain, and Knee MRI datasets for 4x, 5x and 8x accelerations. We observed that the student network trained with the assistance of the teacher using our proposed KD framework provided significant improvement over the student network trained without assistance across all the datasets and acceleration factors. Specifically, for the Knee dataset, the student network achieves $65\%$ parameter reduction, 2x faster CPU running time, and 1.5x faster GPU running time compared to the teacher. Furthermore, we compare our attention-based feature distillation method with other feature distillation methods. We also conduct an ablative study to understand the significance of attention-based distillation and imitation loss. We also extend our KD framework for MRI super-resolution and show encouraging results.



### Improving Semantic Segmentation through Spatio-Temporal Consistency Learned from Videos
- **Arxiv ID**: http://arxiv.org/abs/2004.05324v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.05324v2)
- **Published**: 2020-04-11 07:09:29+00:00
- **Updated**: 2020-05-20 23:55:36+00:00
- **Authors**: Ankita Pasad, Ariel Gordon, Tsung-Yi Lin, Anelia Angelova
- **Comment**: Learning from Unlabeled Videos, CVPR Workshop, 2020
- **Journal**: None
- **Summary**: We leverage unsupervised learning of depth, egomotion, and camera intrinsics to improve the performance of single-image semantic segmentation, by enforcing 3D-geometric and temporal consistency of segmentation masks across video frames. The predicted depth, egomotion, and camera intrinsics are used to provide an additional supervision signal to the segmentation model, significantly enhancing its quality, or, alternatively, reducing the number of labels the segmentation model needs. Our experiments were performed on the ScanNet dataset.



### Spatially-Attentive Patch-Hierarchical Network for Adaptive Motion Deblurring
- **Arxiv ID**: http://arxiv.org/abs/2004.05343v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.05343v1)
- **Published**: 2020-04-11 09:24:00+00:00
- **Updated**: 2020-04-11 09:24:00+00:00
- **Authors**: Maitreya Suin, Kuldeep Purohit, A. N. Rajagopalan
- **Comment**: Accepted at CVPR2020
- **Journal**: None
- **Summary**: This paper tackles the problem of motion deblurring of dynamic scenes. Although end-to-end fully convolutional designs have recently advanced the state-of-the-art in non-uniform motion deblurring, their performance-complexity trade-off is still sub-optimal. Existing approaches achieve a large receptive field by increasing the number of generic convolution layers and kernel-size, but this comes at the expense of of the increase in model size and inference speed. In this work, we propose an efficient pixel adaptive and feature attentive design for handling large blur variations across different spatial locations and process each test image adaptively. We also propose an effective content-aware global-local filtering module that significantly improves performance by considering not only global dependencies but also by dynamically exploiting neighbouring pixel information. We use a patch-hierarchical attentive architecture composed of the above module that implicitly discovers the spatial variations in the blur present in the input image and in turn, performs local and global modulation of intermediate features. Extensive qualitative and quantitative comparisons with prior art on deblurring benchmarks demonstrate that our design offers significant improvements over the state-of-the-art in accuracy as well as speed.



### Exploring The Spatial Reasoning Ability of Neural Models in Human IQ Tests
- **Arxiv ID**: http://arxiv.org/abs/2004.05352v1
- **DOI**: 10.1016/j.neunet.2021.02.018
- **Categories**: **cs.AI**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.05352v1)
- **Published**: 2020-04-11 09:41:46+00:00
- **Updated**: 2020-04-11 09:41:46+00:00
- **Authors**: Hyunjae Kim, Yookyung Koh, Jinheon Baek, Jaewoo Kang
- **Comment**: None
- **Journal**: Neural Networks, Volume 140, August 2021, Pages 27-38
- **Summary**: Although neural models have performed impressively well on various tasks such as image recognition and question answering, their reasoning ability has been measured in only few studies. In this work, we focus on spatial reasoning and explore the spatial understanding of neural models. First, we describe the following two spatial reasoning IQ tests: rotation and shape composition. Using well-defined rules, we constructed datasets that consist of various complexity levels. We designed a variety of experiments in terms of generalization, and evaluated six different baseline models on the newly generated datasets. We provide an analysis of the results and factors that affect the generalization abilities of models. Also, we analyze how neural models solve spatial reasoning tests with visual aids. Our findings would provide valuable insights into understanding a machine and the difference between a machine and human.



### Bayesian Surprise in Indoor Environments
- **Arxiv ID**: http://arxiv.org/abs/2004.05381v1
- **DOI**: 10.1145/3347146.3359358
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.05381v1)
- **Published**: 2020-04-11 12:09:51+00:00
- **Updated**: 2020-04-11 12:09:51+00:00
- **Authors**: Sebastian Feld, Andreas Sedlmeier, Markus Friedrich, Jan Franz, Lenz Belzner
- **Comment**: 10 pages, 16 figures
- **Journal**: Proceedings of the 27th ACM SIGSPATIAL International Conference on
  Advances in Geographic Information Systems (SIGSPATIAL '19), 2019, p. 129-138
- **Summary**: This paper proposes a novel method to identify unexpected structures in 2D floor plans using the concept of Bayesian Surprise. Taking into account that a person's expectation is an important aspect of the perception of space, we exploit the theory of Bayesian Surprise to robustly model expectation and thus surprise in the context of building structures. We use Isovist Analysis, which is a popular space syntax technique, to turn qualitative object attributes into quantitative environmental information. Since isovists are location-specific patterns of visibility, a sequence of isovists describes the spatial perception during a movement along multiple points in space. We then use Bayesian Surprise in a feature space consisting of these isovist readings. To demonstrate the suitability of our approach, we take "snapshots" of an agent's local environment to provide a short list of images that characterize a traversed trajectory through a 2D indoor environment. Those fingerprints represent surprising regions of a tour, characterize the traversed map and enable indoor LBS to focus more on important regions. Given this idea, we propose to use "surprise" as a new dimension of context in indoor location-based services (LBS). Agents of LBS, such as mobile robots or non-player characters in computer games, may use the context surprise to focus more on important regions of a map for a better use or understanding of the floor plan.



### Trajectory annotation using sequences of spatial perception
- **Arxiv ID**: http://arxiv.org/abs/2004.05383v1
- **DOI**: 10.1145/3274895.3274968
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.05383v1)
- **Published**: 2020-04-11 12:22:27+00:00
- **Updated**: 2020-04-11 12:22:27+00:00
- **Authors**: Sebastian Feld, Steffen Illium, Andreas Sedlmeier, Lenz Belzner
- **Comment**: 10 pages, 17 figures
- **Journal**: Proceedings of the 26th ACM SIGSPATIAL International Conference on
  Advances in Geographic Information Systems (SIGSPATIAL '18), 2018, p. 329-338
- **Summary**: In the near future, more and more machines will perform tasks in the vicinity of human spaces or support them directly in their spatially bound activities. In order to simplify the verbal communication and the interaction between robotic units and/or humans, reliable and robust systems w.r.t. noise and processing results are needed. This work builds a foundation to address this task. By using a continuous representation of spatial perception in interiors learned from trajectory data, our approach clusters movement in dependency to its spatial context. We propose an unsupervised learning approach based on a neural autoencoding that learns semantically meaningful continuous encodings of spatio-temporal trajectory data. This learned encoding can be used to form prototypical representations. We present promising results that clear the path for future applications.



### Unveiling COVID-19 from Chest X-ray with deep learning: a hurdles race with small data
- **Arxiv ID**: http://arxiv.org/abs/2004.05405v1
- **DOI**: 10.3390/ijerph17186933
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.05405v1)
- **Published**: 2020-04-11 13:58:17+00:00
- **Updated**: 2020-04-11 13:58:17+00:00
- **Authors**: Enzo Tartaglione, Carlo Alberto Barbano, Claudio Berzovini, Marco Calandri, Marco Grangetto
- **Comment**: None
- **Journal**: Int. J. Environ. Res. Public Health 2020, 17(18), 6933
- **Summary**: The possibility to use widespread and simple chest X-ray (CXR) imaging for early screening of COVID-19 patients is attracting much interest from both the clinical and the AI community. In this study we provide insights and also raise warnings on what is reasonable to expect by applying deep-learning to COVID classification of CXR images. We provide a methodological guide and critical reading of an extensive set of statistical results that can be obtained using currently available datasets. In particular, we take the challenge posed by current small size COVID data and show how significant can be the bias introduced by transfer-learning using larger public non-COVID CXR datasets. We also contribute by providing results on a medium size COVID CXR dataset, just collected by one of the major emergency hospitals in Northern Italy during the peak of the COVID pandemic. These novel data allow us to contribute to validate the generalization capacity of preliminary results circulating in the scientific community. Our conclusions shed some light into the possibility to effectively discriminate COVID using CXR.



### The Role of Stem Noise in Visual Perception and Image Quality Measurement
- **Arxiv ID**: http://arxiv.org/abs/2004.05422v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.05422v1)
- **Published**: 2020-04-11 15:10:21+00:00
- **Updated**: 2020-04-11 15:10:21+00:00
- **Authors**: Arash Ashtari
- **Comment**: 16 pages,19 figures
- **Journal**: None
- **Summary**: This paper considers reference free quality assessment of distorted and noisy images. Specifically, it considers the first and second order statistics of stem noise that can be evaluated given any image. In the research field of Image quality Assessment (IQA), the stem noise is defined as the input of an Auto Regressive (AR) process, from which a low-energy and de-correlated version of the image can be recovered. To estimate the AR model parameters and associated stem noise energy, the Yule-walker equations are used such that the accompanying Auto Correlation Function (ACF) coefficients can be treated as model parameters for image reconstruction. To characterize systematic signal dependent and signal independent distortions, the mean and variance of stem noise can be evaluated over the image. Crucially, this paper shows that these statistics have a predictive validity in relation to human ratings of image quality. Furthermore, under certain kinds of image distortion, stem noise statistics show very significant correlations with established measures of image quality.



### Underwater Image Enhancement Based on Structure-Texture Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2004.05430v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.05430v1)
- **Published**: 2020-04-11 15:52:07+00:00
- **Updated**: 2020-04-11 15:52:07+00:00
- **Authors**: Sen Lin, Kaichen Chi
- **Comment**: None
- **Journal**: None
- **Summary**: Aiming at the problems of color distortion, blur and excessive noise of underwater image, an underwater image enhancement algorithm based on structure-texture reconstruction is proposed. Firstly, the color equalization of the degraded image is realized by the automatic color enhancement algorithm; Secondly, the relative total variation is introduced to decompose the image into the structure layer and texture layer; Then, the best background light point is selected based on brightness, gradient discrimination, and hue judgment, the transmittance of the backscatter component is obtained by the red dark channel prior, which is substituted into the imaging model to remove the fogging phenomenon in the structure layer. Enhancement of effective details in the texture layer by multi scale detail enhancement algorithm and binary mask; Finally, the structure layer and texture layer are reconstructed to get the final image. The experimental results show that the algorithm can effectively balance the hue, saturation, and clarity of underwater image, and has good performance in different underwater environments.



### Detection of Covid-19 From Chest X-ray Images Using Artificial Intelligence: An Early Review
- **Arxiv ID**: http://arxiv.org/abs/2004.05436v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.05436v1)
- **Published**: 2020-04-11 16:15:53+00:00
- **Updated**: 2020-04-11 16:15:53+00:00
- **Authors**: Muhammad Ilyas, Hina Rehman, Amine Nait-ali
- **Comment**: None
- **Journal**: None
- **Summary**: In 2019, the entire world is facing a situation of health emergency due to a newly emerged coronavirus (COVID-19). Almost 196 countries are affected by covid-19, while USA, Italy, China, Spain, Iran, and France have the maximum active cases of COVID-19. The issues, medical and healthcare departments are facing in delay of detecting the COVID-19. Several artificial intelligence based system are designed for the automatic detection of COVID-19 using chest x-rays. In this article we will discuss the different approaches used for the detection of COVID-19 and the challenges we are facing. It is mandatory to develop an automatic detection system to prevent the transfer of the virus through contact. Several deep learning architecture are deployed for the detection of COVID-19 such as ResNet, Inception, Googlenet etc. All these approaches are detecting the subjects suffering with pneumonia while its hard to decide whether the pneumonia is caused by COVID-19 or due to any other bacterial or fungal attack.



### Robust Generalised Quadratic Discriminant Analysis
- **Arxiv ID**: http://arxiv.org/abs/2004.06568v1
- **DOI**: None
- **Categories**: **stat.ME**, cs.CV, cs.LG, stat.CO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.06568v1)
- **Published**: 2020-04-11 18:21:06+00:00
- **Updated**: 2020-04-11 18:21:06+00:00
- **Authors**: Abhik Ghosh, Rita SahaRay, Sayan Chakrabarty, Sayan Bhadra
- **Comment**: Pre-print. Under Review
- **Journal**: None
- **Summary**: Quadratic discriminant analysis (QDA) is a widely used statistical tool to classify observations from different multivariate Normal populations. The generalized quadratic discriminant analysis (GQDA) classification rule/classifier, which generalizes the QDA and the minimum Mahalanobis distance (MMD) classifiers to discriminate between populations with underlying elliptically symmetric distributions competes quite favorably with the QDA classifier when it is optimal and performs much better when QDA fails under non-Normal underlying distributions, e.g. Cauchy distribution. However, the classification rule in GQDA is based on the sample mean vector and the sample dispersion matrix of a training sample, which are extremely non-robust under data contamination. In real world, since it is quite common to face data highly vulnerable to outliers, the lack of robustness of the classical estimators of the mean vector and the dispersion matrix reduces the efficiency of the GQDA classifier significantly, increasing the misclassification errors. The present paper investigates the performance of the GQDA classifier when the classical estimators of the mean vector and the dispersion matrix used therein are replaced by various robust counterparts. Applications to various real data sets as well as simulation studies reveal far better performance of the proposed robust versions of the GQDA classifier. A Comparative study has been made to advocate the appropriate choice of the robust estimators to be used in a specific situation of the degree of contamination of the data sets.



### Farmland Parcel Delineation Using Spatio-temporal Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/2004.05471v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.05471v2)
- **Published**: 2020-04-11 19:49:09+00:00
- **Updated**: 2020-04-20 16:34:41+00:00
- **Authors**: Han Lin Aung, Burak Uzkent, Marshall Burke, David Lobell, Stefano Ermon
- **Comment**: None
- **Journal**: None
- **Summary**: Farm parcel delineation provides cadastral data that is important in developing and managing climate change policies. Specifically, farm parcel delineation informs applications in downstream governmental policies of land allocation, irrigation, fertilization, green-house gases (GHG's), etc. This data can also be useful for the agricultural insurance sector for assessing compensations following damages associated with extreme weather events - a growing trend related to climate change. Using satellite imaging can be a scalable and cost effective manner to perform the task of farm parcel delineation to collect this valuable data. In this paper, we break down this task using satellite imaging into two approaches: 1) Segmentation of parcel boundaries, and 2) Segmentation of parcel areas. We implemented variations of UNets, one of which takes into account temporal information, which achieved the best results on our dataset on farmland parcels in France in 2017.



### Learning to Manipulate Individual Objects in an Image
- **Arxiv ID**: http://arxiv.org/abs/2004.05495v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.05495v1)
- **Published**: 2020-04-11 21:50:20+00:00
- **Updated**: 2020-04-11 21:50:20+00:00
- **Authors**: Yanchao Yang, Yutong Chen, Stefano Soatto
- **Comment**: None
- **Journal**: None
- **Summary**: We describe a method to train a generative model with latent factors that are (approximately) independent and localized. This means that perturbing the latent variables affects only local regions of the synthesized image, corresponding to objects. Unlike other unsupervised generative models, ours enables object-centric manipulation, without requiring object-level annotations, or any form of annotation for that matter. The key to our method is the combination of spatial disentanglement, enforced by a Contextual Information Separation loss, and perceptual cycle-consistency, enforced by a loss that penalizes changes in the image partition in response to perturbations of the latent factors. We test our method's ability to allow independent control of spatial and semantic factors of variability on existing datasets and also introduce two new ones that highlight the limitations of current methods.



### FDA: Fourier Domain Adaptation for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2004.05498v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.05498v1)
- **Published**: 2020-04-11 22:20:48+00:00
- **Updated**: 2020-04-11 22:20:48+00:00
- **Authors**: Yanchao Yang, Stefano Soatto
- **Comment**: None
- **Journal**: None
- **Summary**: We describe a simple method for unsupervised domain adaptation, whereby the discrepancy between the source and target distributions is reduced by swapping the low-frequency spectrum of one with the other. We illustrate the method in semantic segmentation, where densely annotated images are aplenty in one domain (synthetic data), but difficult to obtain in another (real images). Current state-of-the-art methods are complex, some requiring adversarial optimization to render the backbone of a neural network invariant to the discrete domain selection variable. Our method does not require any training to perform the domain alignment, just a simple Fourier Transform and its inverse. Despite its simplicity, it achieves state-of-the-art performance in the current benchmarks, when integrated into a relatively standard semantic segmentation model. Our results indicate that even simple procedures can discount nuisance variability in the data that more sophisticated methods struggle to learn away.



### A Pose Proposal and Refinement Network for Better Object Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2004.05507v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.05507v2)
- **Published**: 2020-04-11 23:13:54+00:00
- **Updated**: 2020-10-07 15:41:11+00:00
- **Authors**: Ameni Trabelsi, Mohamed Chaabane, Nathaniel Blanchard, Ross Beveridge
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present a novel, end-to-end 6D object pose estimation method that operates on RGB inputs. Our approach is composed of 2 main components: the first component classifies the objects in the input image and proposes an initial 6D pose estimate through a multi-task, CNN-based encoder/multi-decoder module. The second component, a refinement module, includes a renderer and a multi-attentional pose refinement network, which iteratively refines the estimated poses by utilizing both appearance features and flow vectors. Our refiner takes advantage of the hybrid representation of the initial pose estimates to predict the relative errors with respect to the target poses. It is further augmented by a spatial multi-attention block that emphasizes objects' discriminative feature parts. Experiments on three benchmarks for 6D pose estimation show that our proposed pipeline outperforms state-of-the-art RGB-based methods with competitive runtime performance.



### MetaIQA: Deep Meta-learning for No-Reference Image Quality Assessment
- **Arxiv ID**: http://arxiv.org/abs/2004.05508v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.05508v1)
- **Published**: 2020-04-11 23:36:36+00:00
- **Updated**: 2020-04-11 23:36:36+00:00
- **Authors**: Hancheng Zhu, Leida Li, Jinjian Wu, Weisheng Dong, Guangming Shi
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, increasing interest has been drawn in exploiting deep convolutional neural networks (DCNNs) for no-reference image quality assessment (NR-IQA). Despite of the notable success achieved, there is a broad consensus that training DCNNs heavily relies on massive annotated data. Unfortunately, IQA is a typical small sample problem. Therefore, most of the existing DCNN-based IQA metrics operate based on pre-trained networks. However, these pre-trained networks are not designed for IQA task, leading to generalization problem when evaluating different types of distortions. With this motivation, this paper presents a no-reference IQA metric based on deep meta-learning. The underlying idea is to learn the meta-knowledge shared by human when evaluating the quality of images with various distortions, which can then be adapted to unknown distortions easily. Specifically, we first collect a number of NR-IQA tasks for different distortions. Then meta-learning is adopted to learn the prior knowledge shared by diversified distortions. Finally, the quality prior model is fine-tuned on a target NR-IQA task for quickly obtaining the quality model. Extensive experiments demonstrate that the proposed metric outperforms the state-of-the-arts by a large margin. Furthermore, the meta-model learned from synthetic distortions can also be easily generalized to authentic distortions, which is highly desired in real-world applications of IQA metrics.



