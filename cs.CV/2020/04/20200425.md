# Arxiv Papers in cs.CV on 2020-04-25
### Deepfakes Detection with Automatic Face Weighting
- **Arxiv ID**: http://arxiv.org/abs/2004.12027v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.12027v2)
- **Published**: 2020-04-25 00:47:42+00:00
- **Updated**: 2020-05-04 19:44:49+00:00
- **Authors**: Daniel Mas Montserrat, Hanxiang Hao, S. K. Yarlagadda, Sriram Baireddy, Ruiting Shao, János Horváth, Emily Bartusiak, Justin Yang, David Güera, Fengqing Zhu, Edward J. Delp
- **Comment**: None
- **Journal**: None
- **Summary**: Altered and manipulated multimedia is increasingly present and widely distributed via social media platforms. Advanced video manipulation tools enable the generation of highly realistic-looking altered multimedia. While many methods have been presented to detect manipulations, most of them fail when evaluated with data outside of the datasets used in research environments. In order to address this problem, the Deepfake Detection Challenge (DFDC) provides a large dataset of videos containing realistic manipulations and an evaluation system that ensures that methods work quickly and accurately, even when faced with challenging data. In this paper, we introduce a method based on convolutional neural networks (CNNs) and recurrent neural networks (RNNs) that extracts visual and temporal features from faces present in videos to accurately detect manipulations. The method is evaluated with the DFDC dataset, providing competitive results compared to other techniques.



### Computer Vision-Based Health Monitoring of Mecklenburg Bridge Using 3D Digital Image Correlation
- **Arxiv ID**: http://arxiv.org/abs/2005.02120v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.02120v1)
- **Published**: 2020-04-25 00:56:06+00:00
- **Updated**: 2020-04-25 00:56:06+00:00
- **Authors**: Mehrdad S. Dizaji, Devin K. Harris, Bernie Kassner, Jeffrey C. Hill
- **Comment**: 19 pages, 6 figures
- **Journal**: None
- **Summary**: A collaborative investigation between the University of Virginia (UVA) and the Virginia Transportation Research Council was performed on the Mecklenburg Bridge (I-85 over Route 1 in Mecklenburg County). The research team aided the Virginia Department of Transportation - Richmond District in the characterization of the bridge behavior of one of the bridge beams that had been repaired due to a previous web buckling and crippling failure. The investigation focused on collecting full-field three-dimensional digital image correlation (3D-DIC) deformation measurements during the dropping sequence (removal of jacking to support beam on bearing/pier). Additionally, measurements were taken of the section prior to and after dropping using a handheld laser scanner to assess the potential of lateral deformation or out-of-plane buckling. Results from the study demonstrated that buckling of the tested beam did not occur, but did provided a series of approaches that can be used to evaluate the effectiveness of repaired steel beam ends. Specifically, the results provided an approach that could estimate the dead load distribution through back-calculation.



### On the Role of Visual Cues in Audiovisual Speech Enhancement
- **Arxiv ID**: http://arxiv.org/abs/2004.12031v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CL, cs.CV, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2004.12031v4)
- **Published**: 2020-04-25 01:00:03+00:00
- **Updated**: 2021-02-25 15:56:42+00:00
- **Authors**: Zakaria Aldeneh, Anushree Prasanna Kumar, Barry-John Theobald, Erik Marchi, Sachin Kajarekar, Devang Naik, Ahmed Hussen Abdelaziz
- **Comment**: ICASSP 2021
- **Journal**: None
- **Summary**: We present an introspection of an audiovisual speech enhancement model. In particular, we focus on interpreting how a neural audiovisual speech enhancement model uses visual cues to improve the quality of the target speech signal. We show that visual cues provide not only high-level information about speech activity, i.e., speech/silence, but also fine-grained visual information about the place of articulation. One byproduct of this finding is that the learned visual embeddings can be used as features for other visual speech applications. We demonstrate the effectiveness of the learned visual embeddings for classifying visemes (the visual analogy to phonemes). Our results provide insight into important aspects of audiovisual speech enhancement and demonstrate how such models can be used for self-supervision tasks for visual speech applications.



### StRDAN: Synthetic-to-Real Domain Adaptation Network for Vehicle Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2004.12032v2
- **DOI**: 10.1109/CVPRW50498.2020.00312
- **Categories**: **cs.CV**, cs.LG, eess.IV, I.2.10; I.5.1; I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/2004.12032v2)
- **Published**: 2020-04-25 01:00:30+00:00
- **Updated**: 2020-07-17 07:44:10+00:00
- **Authors**: Sangrok Lee, Eunsoo Park, Hongsuk Yi, Sang Hun Lee
- **Comment**: 7 pages, 2 figures, CVPR Workshop Paper (Revised)
- **Journal**: 2020 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition Workshops (CVPRW), pp. 2590-2597
- **Summary**: Vehicle re-identification aims to obtain the same vehicles from vehicle images. This is challenging but essential for analyzing and predicting traffic flow in the city. Although deep learning methods have achieved enormous progress for this task, their large data requirement is a critical shortcoming. Therefore, we propose a synthetic-to-real domain adaptation network (StRDAN) framework, which can be trained with inexpensive large-scale synthetic and real data to improve performance. The StRDAN training method combines domain adaptation and semi-supervised learning methods and their associated losses. StRDAN offers significant improvement over the baseline model, which can only be trained using real data, for VeRi and CityFlow-ReID datasets, achieving 3.1% and 12.9% improved mean average precision, respectively.



### Deep convolutional neural networks for face and iris presentation attack detection: Survey and case study
- **Arxiv ID**: http://arxiv.org/abs/2004.12040v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.12040v2)
- **Published**: 2020-04-25 02:06:19+00:00
- **Updated**: 2020-04-29 03:51:10+00:00
- **Authors**: Yomna Safaa El-Din, Mohamed N. Moustafa, Hani Mahdi
- **Comment**: A preprint of a paper accepted by IET Biometrics journal and is
  subject to Institution of Engineering and Technology Copyright
- **Journal**: None
- **Summary**: Biometric presentation attack detection is gaining increasing attention. Users of mobile devices find it more convenient to unlock their smart applications with finger, face or iris recognition instead of passwords. In this paper, we survey the approaches presented in the recent literature to detect face and iris presentation attacks. Specifically, we investigate the effectiveness of fine tuning very deep convolutional neural networks to the task of face and iris antispoofing. We compare two different fine tuning approaches on six publicly available benchmark datasets. Results show the effectiveness of these deep models in learning discriminative features that can tell apart real from fake biometric images with very low error rate. Cross-dataset evaluation on face PAD showed better generalization than state of the art. We also performed cross-dataset testing on iris PAD datasets in terms of equal error rate which was not reported in literature before. Additionally, we propose the use of a single deep network trained to detect both face and iris attacks. We have not noticed accuracy degradation compared to networks trained for only one biometric separately. Finally, we analyzed the learned features by the network, in correlation with the image frequency components, to justify its prediction decision.



### GPO: Global Plane Optimization for Fast and Accurate Monocular SLAM Initialization
- **Arxiv ID**: http://arxiv.org/abs/2004.12051v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.12051v2)
- **Published**: 2020-04-25 03:57:50+00:00
- **Updated**: 2020-05-24 01:59:57+00:00
- **Authors**: Sicong Du, Hengkai Guo, Yao Chen, Yilun Lin, Xiangbing Meng, Linfu Wen, Fei-Yue Wang
- **Comment**: Revised some minor errors. Accepted by ICRA 2020
- **Journal**: None
- **Summary**: Initialization is essential to monocular Simultaneous Localization and Mapping (SLAM) problems. This paper focuses on a novel initialization method for monocular SLAM based on planar features. The algorithm starts by homography estimation in a sliding window. It then proceeds to a global plane optimization (GPO) to obtain camera poses and the plane normal. 3D points can be recovered using planar constraints without triangulation. The proposed method fully exploits the plane information from multiple frames and avoids the ambiguities in homography decomposition. We validate our algorithm on the collected chessboard dataset against baseline implementations and present extensive analysis. Experimental results show that our method outperforms the fine-tuned baselines in both accuracy and real-time.



### SAIA: Split Artificial Intelligence Architecture for Mobile Healthcare System
- **Arxiv ID**: http://arxiv.org/abs/2004.12059v2
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2004.12059v2)
- **Published**: 2020-04-25 05:06:51+00:00
- **Updated**: 2020-05-09 05:00:59+00:00
- **Authors**: Di Zhuang, Nam Nguyen, Keyu Chen, J. Morris Chang
- **Comment**: 17 pages, 9 figures, 2 tables
- **Journal**: None
- **Summary**: As the advancement of deep learning (DL), the Internet of Things and cloud computing techniques for biomedical and healthcare problems, mobile healthcare systems have received unprecedented attention. Since DL techniques usually require enormous amount of computation, most of them cannot be directly deployed on the resource-constrained mobile and IoT devices. Hence, most of the mobile healthcare systems leverage the cloud computing infrastructure, where the data collected by the mobile and IoT devices would be transmitted to the cloud computing platforms for analysis. However, in the contested environments, relying on the cloud might not be practical at all times. For instance, the satellite communication might be denied or disrupted. We propose SAIA, a Split Artificial Intelligence Architecture for mobile healthcare systems. Unlike traditional approaches for artificial intelligence (AI) which solely exploits the computational power of the cloud server, SAIA could not only relies on the cloud computing infrastructure while the wireless communication is available, but also utilizes the lightweight AI solutions that work locally on the client side, hence, it can work even when the communication is impeded. In SAIA, we propose a meta-information based decision unit, that could tune whether a sample captured by the client should be operated by the embedded AI (i.e., keeping on the client) or the networked AI (i.e., sending to the server), under different conditions. In our experimental evaluation, extensive experiments have been conducted on two popular healthcare datasets. Our results show that SAIA consistently outperforms its baselines in terms of both effectiveness and efficiency.



### CS-AF: A Cost-sensitive Multi-classifier Active Fusion Framework for Skin Lesion Classification
- **Arxiv ID**: http://arxiv.org/abs/2004.12064v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.12064v2)
- **Published**: 2020-04-25 05:48:14+00:00
- **Updated**: 2020-09-09 04:37:03+00:00
- **Authors**: Di Zhuang, Keyu Chen, J. Morris Chang
- **Comment**: 16 pages, 8 figures, 2 table
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) have achieved the state-of-the-art performance in skin lesion analysis. Compared with single CNN classifier, combining the results of multiple classifiers via fusion approaches shows to be more effective and robust. Since the skin lesion datasets are usually limited and statistically biased, while designing an effective fusion approach, it is important to consider not only the performance of each classifier on the training/validation dataset, but also the relative discriminative power (e.g., confidence) of each classifier regarding an individual sample in the testing phase, which calls for an active fusion approach. Furthermore, in skin lesion analysis, the data of certain classes (e.g., the benign lesions) is usually abundant making them an over-represented majority, while the data of some other classes (e.g., the cancerous lesions) is deficient, making them an underrepresented minority. It is more crucial to precisely identify the samples from an underrepresented (i.e., in terms of the amount of data) but more important minority class (e.g., certain cancerous lesion). In other words, misclassifying a more severe lesion to a benign or less severe lesion should have relative more cost (e.g., money, time and even lives). To address such challenges, we present CS-AF, a cost-sensitive multi-classifier active fusion framework for skin lesion classification. In the experimental evaluation, we prepared 96 base classifiers (of 12 CNN architectures) on the ISIC research datasets. Our experimental results show that our framework consistently outperforms the static fusion competitors.



### Deep Multimodal Neural Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/2004.12070v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.12070v2)
- **Published**: 2020-04-25 07:00:32+00:00
- **Updated**: 2020-10-11 03:28:08+00:00
- **Authors**: Zhou Yu, Yuhao Cui, Jun Yu, Meng Wang, Dacheng Tao, Qi Tian
- **Comment**: Accept to ACM MM2020, code available at
  https://github.com/MILVLG/mmnas/
- **Journal**: None
- **Summary**: Designing effective neural networks is fundamentally important in deep multimodal learning. Most existing works focus on a single task and design neural architectures manually, which are highly task-specific and hard to generalize to different tasks. In this paper, we devise a generalized deep multimodal neural architecture search (MMnas) framework for various multimodal learning tasks. Given multimodal input, we first define a set of primitive operations, and then construct a deep encoder-decoder based unified backbone, where each encoder or decoder block corresponds to an operation searched from a predefined operation pool. On top of the unified backbone, we attach task-specific heads to tackle different multimodal learning tasks. By using a gradient-based NAS algorithm, the optimal architectures for different tasks are learned efficiently. Extensive ablation studies, comprehensive analysis, and comparative experimental results show that the obtained MMnasNet significantly outperforms existing state-of-the-art approaches across three multimodal learning tasks (over five datasets), including visual question answering, image-text matching, and visual grounding.



### POCOVID-Net: Automatic Detection of COVID-19 From a New Lung Ultrasound Imaging Dataset (POCUS)
- **Arxiv ID**: http://arxiv.org/abs/2004.12084v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.12084v4)
- **Published**: 2020-04-25 08:41:24+00:00
- **Updated**: 2021-01-24 13:37:44+00:00
- **Authors**: Jannis Born, Gabriel Brändle, Manuel Cossio, Marion Disdier, Julie Goulet, Jérémie Roulin, Nina Wiedemann
- **Comment**: 7 pages, 4 figures
- **Journal**: ISMB TransMed COSI 2020
- **Summary**: With the rapid development of COVID-19 into a global pandemic, there is an ever more urgent need for cheap, fast and reliable tools that can assist physicians in diagnosing COVID-19. Medical imaging such as CT can take a key role in complementing conventional diagnostic tools from molecular biology, and, using deep learning techniques, several automatic systems were demonstrated promising performances using CT or X-ray data. Here, we advocate a more prominent role of point-of-care ultrasound imaging to guide COVID-19 detection. Ultrasound is non-invasive and ubiquitous in medical facilities around the globe. Our contribution is threefold. First, we gather a lung ultrasound (POCUS) dataset consisting of 1103 images (654 COVID-19, 277 bacterial pneumonia and 172 healthy controls), sampled from 64 videos. This dataset was assembled from various online sources, processed specifically for deep learning models and is intended to serve as a starting point for an open-access initiative. Second, we train a deep convolutional neural network (POCOVID-Net) on this 3-class dataset and achieve an accuracy of 89% and, by a majority vote, a video accuracy of 92% . For detecting COVID-19 in particular, the model performs with a sensitivity of 0.96, a specificity of 0.79 and F1-score of 0.92 in a 5-fold cross validation. Third, we provide an open-access web service (POCOVIDScreen) that is available at: https://pocovidscreen.org. The website deploys the predictive model, allowing to perform predictions on ultrasound lung images. In addition, it grants medical staff the option to (bulk) upload their own screenings in order to contribute to the growing public database of pathological lung ultrasound images.   Dataset and code are available from: https://github.com/jannisborn/covid19_pocus_ultrasound.   NOTE: This preprint is superseded by our paper in Applied Sciences: https://doi.org/10.3390/app11020672



### Clustering by Constructing Hyper-Planes
- **Arxiv ID**: http://arxiv.org/abs/2004.12087v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.12087v1)
- **Published**: 2020-04-25 08:52:21+00:00
- **Updated**: 2020-04-25 08:52:21+00:00
- **Authors**: Luhong Diao, Jinying Gao1, Manman Deng
- **Comment**: None
- **Journal**: None
- **Summary**: As a kind of basic machine learning method, clustering algorithms group data points into different categories based on their similarity or distribution. We present a clustering algorithm by finding hyper-planes to distinguish the data points. It relies on the marginal space between the points. Then we combine these hyper-planes to determine centers and numbers of clusters. Because the algorithm is based on linear structures, it can approximate the distribution of datasets accurately and flexibly. To evaluate its performance, we compared it with some famous clustering algorithms by carrying experiments on different kinds of benchmark datasets. It outperforms other methods clearly.



### PF-cpGAN: Profile to Frontal Coupled GAN for Face Recognition in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2005.02166v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.02166v1)
- **Published**: 2020-04-25 09:01:54+00:00
- **Updated**: 2020-04-25 09:01:54+00:00
- **Authors**: Fariborz Taherkhani, Veeru Talreja, Jeremy Dawson, Matthew C. Valenti, Nasser M. Nasrabadi
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, due to the emergence of deep learning, face recognition has achieved exceptional success. However, many of these deep face recognition models perform relatively poorly in handling profile faces compared to frontal faces. The major reason for this poor performance is that it is inherently difficult to learn large pose invariant deep representations that are useful for profile face recognition. In this paper, we hypothesize that the profile face domain possesses a gradual connection with the frontal face domain in the deep feature space. We look to exploit this connection by projecting the profile faces and frontal faces into a common latent space and perform verification or retrieval in the latent domain. We leverage a coupled generative adversarial network (cpGAN) structure to find the hidden relationship between the profile and frontal images in a latent common embedding subspace. Specifically, the cpGAN framework consists of two GAN-based sub-networks, one dedicated to the frontal domain and the other dedicated to the profile domain. Each sub-network tends to find a projection that maximizes the pair-wise correlation between two feature domains in a common embedding feature subspace. The efficacy of our approach compared with the state-of-the-art is demonstrated using the CFP, CMU MultiPIE, IJB-A, and IJB-C datasets.



### How to read faces without looking at them
- **Arxiv ID**: http://arxiv.org/abs/2004.12103v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.12103v1)
- **Published**: 2020-04-25 10:17:38+00:00
- **Updated**: 2020-04-25 10:17:38+00:00
- **Authors**: Suyash Shandilya, Waris Quamer
- **Comment**: 6 pages
- **Journal**: None
- **Summary**: Face reading is the most intuitive aspect of emotion recognition. Unfortunately, digital analysis of facial expression requires digitally recording personal faces. As emotional analysis is particularly required in a more poised scenario, capturing faces becomes a gross violation of privacy. In this paper, we use the concept of compressive analysis to conceptualise a system which compressively acquires faces in order to ascertain unusable reconstruction, while allowing for acceptable (and adjustable) accuracy in inference.



### Offline Signature Verification on Real-World Documents
- **Arxiv ID**: http://arxiv.org/abs/2004.12104v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.12104v1)
- **Published**: 2020-04-25 10:28:03+00:00
- **Updated**: 2020-04-25 10:28:03+00:00
- **Authors**: Deniz Engin, Alperen Kantarcı, Seçil Arslan, Hazım Kemal Ekenel
- **Comment**: CVPR 2020 Biometrics Workshop
- **Journal**: None
- **Summary**: Research on offline signature verification has explored a large variety of methods on multiple signature datasets, which are collected under controlled conditions. However, these datasets may not fully reflect the characteristics of the signatures in some practical use cases. Real-world signatures extracted from the formal documents may contain different types of occlusions, for example, stamps, company seals, ruling lines, and signature boxes. Moreover, they may have very high intra-class variations, where even genuine signatures resemble forgeries. In this paper, we address a real-world writer independent offline signature verification problem, in which, a bank's customers' transaction request documents that contain their occluded signatures are compared with their clean reference signatures. Our proposed method consists of two main components, a stamp cleaning method based on CycleGAN and signature representation based on CNNs. We extensively evaluate different verification setups, fine-tuning strategies, and signature representation approaches to have a thorough analysis of the problem. Moreover, we conduct a human evaluation to show the challenging nature of the problem. We run experiments both on our custom dataset, as well as on the publicly available Tobacco-800 dataset. The experimental results validate the difficulty of offline signature verification on real-world documents. However, by employing the stamp cleaning process, we improve the signature verification performance significantly.



### AutoTune: Automatically Tuning Convolutional Neural Networks for Improved Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/2005.02165v2
- **DOI**: 10.1016/j.neunet.2020.10.009
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.02165v2)
- **Published**: 2020-04-25 10:42:06+00:00
- **Updated**: 2020-12-03 05:35:23+00:00
- **Authors**: S. H. Shabbeer Basha, Sravan Kumar Vinakota, Viswanath Pulabaigari, Snehasis Mukherjee, Shiv Ram Dubey
- **Comment**: This paper is published in Neural Networks journal
- **Journal**: None
- **Summary**: Transfer learning enables solving a specific task having limited data by using the pre-trained deep networks trained on large-scale datasets. Typically, while transferring the learned knowledge from source task to the target task, the last few layers are fine-tuned (re-trained) over the target dataset. However, these layers are originally designed for the source task that might not be suitable for the target task. In this paper, we introduce a mechanism for automatically tuning the Convolutional Neural Networks (CNN) for improved transfer learning. The pre-trained CNN layers are tuned with the knowledge from target data using Bayesian Optimization. First, we train the final layer of the base CNN model by replacing the number of neurons in the softmax layer with the number of classes involved in the target task. Next, the pre-trained CNN is tuned automatically by observing the classification performance on the validation data (greedy criteria). To evaluate the performance of the proposed method, experiments are conducted on three benchmark datasets, e.g., CalTech-101, CalTech-256, and Stanford Dogs. The classification results obtained through the proposed AutoTune method outperforms the standard baseline transfer learning methods over the three datasets by achieving $95.92\%$, $86.54\%$, and $84.67\%$ accuracy over CalTech-101, CalTech-256, and Stanford Dogs, respectively. The experimental results obtained in this study depict that tuning of the pre-trained CNN layers with the knowledge from the target dataset confesses better transfer learning ability. The source codes are available at https://github.com/JekyllAndHyde8999/AutoTune_CNN_TransferLearning.



### Detecting Electric Devices in 3D Images of Bags
- **Arxiv ID**: http://arxiv.org/abs/2005.02163v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2005.02163v1)
- **Published**: 2020-04-25 11:30:42+00:00
- **Updated**: 2020-04-25 11:30:42+00:00
- **Authors**: Anthony Bagnall, Paul Southam, James Large, Richard Harvey
- **Comment**: None
- **Journal**: None
- **Summary**: The aviation and transport security industries face the challenge of screening high volumes of baggage for threats and contraband in the minimum time possible. Automation and semi-automation of this procedure offers the potential to increase security by detecting more threats and improve the customer experience by speeding up the process. Traditional 2D x-ray images are often extremely difficult to examine due to the fact that they are tightly packed and contain a wide variety of cluttered and occluded objects. Because of these limitations, major airports are introducing 3D x-ray Computed Tomography (CT) baggage scanning. We investigate whether we can automate the process of detecting electric devices in these 3D images of luggage. Detecting electrical devices is of particular concern as they can be used to conceal explosives. Given the massive volume of luggage that needs to be screened for this threat, the best way to automate the detection is to first filter whether a bag contains an electric device or not, and if it does, to identify the number of devices and their location. We present an algorithm, Unpack, Predict, eXtract, Repack (UXPR), which involves unpacking through segmenting the data at a range of scales using an algorithm known as the Sieve, predicting whether a segment is electrical or not based on the histogram of voxel intensities, then repacking the bag by ensembling the segments and predictions to identify the devices in bags. Through a range of experiments using data provided by ALERT (Awareness and Localization of Explosives-Related Threats) we show that this system can find a high proportion of devices with unsupervised segmentation if a similar device has been seen before, and shows promising results for detecting devices not seen at all based on the properties of its constituent parts.



### Global Wheat Head Detection (GWHD) dataset: a large and diverse dataset of high resolution RGB labelled images to develop and benchmark wheat head detection methods
- **Arxiv ID**: http://arxiv.org/abs/2005.02162v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2005.02162v2)
- **Published**: 2020-04-25 14:20:26+00:00
- **Updated**: 2020-06-30 07:34:36+00:00
- **Authors**: E. David, S. Madec, P. Sadeghi-Tehran, H. Aasen, B. Zheng, S. Liu, N. Kirchgessner, G. Ishikawa, K. Nagasawa, M. A. Badhon, C. Pozniak, B. de Solan, A. Hund, S. C. Chapman, F. Baret, I. Stavness, W. Guo
- **Comment**: 16 pages, 7 figures, Dataset paper
- **Journal**: None
- **Summary**: Detection of wheat heads is an important task allowing to estimate pertinent traits including head population density and head characteristics such as sanitary state, size, maturity stage and the presence of awns. Several studies developed methods for wheat head detection from high-resolution RGB imagery. They are based on computer vision and machine learning and are generally calibrated and validated on limited datasets. However, variability in observational conditions, genotypic differences, development stages, head orientation represents a challenge in computer vision. Further, possible blurring due to motion or wind and overlap between heads for dense populations make this task even more complex. Through a joint international collaborative effort, we have built a large, diverse and well-labelled dataset, the Global Wheat Head detection (GWHD) dataset. It contains 4,700 high-resolution RGB images and 190,000 labelled wheat heads collected from several countries around the world at different growth stages with a wide range of genotypes. Guidelines for image acquisition, associating minimum metadata to respect FAIR principles and consistent head labelling methods are proposed when developing new head detection datasets. The GWHD is publicly available at http://www.global-wheat.com/ and aimed at developing and benchmarking methods for wheat head detection.



### A Survey on Incorporating Domain Knowledge into Deep Learning for Medical Image Analysis
- **Arxiv ID**: http://arxiv.org/abs/2004.12150v4
- **DOI**: 10.1016/j.media.2021.101985
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.12150v4)
- **Published**: 2020-04-25 14:27:47+00:00
- **Updated**: 2021-02-08 06:55:26+00:00
- **Authors**: Xiaozheng Xie, Jianwei Niu, Xuefeng Liu, Zhengsu Chen, Shaojie Tang, Shui Yu
- **Comment**: 27 pages, 18 figures
- **Journal**: Medical Image Analysis 2021
- **Summary**: Although deep learning models like CNNs have achieved great success in medical image analysis, the small size of medical datasets remains a major bottleneck in this area. To address this problem, researchers have started looking for external information beyond current available medical datasets. Traditional approaches generally leverage the information from natural images via transfer learning. More recent works utilize the domain knowledge from medical doctors, to create networks that resemble how medical doctors are trained, mimic their diagnostic patterns, or focus on the features or areas they pay particular attention to. In this survey, we summarize the current progress on integrating medical domain knowledge into deep learning models for various tasks, such as disease diagnosis, lesion, organ and abnormality detection, lesion and organ segmentation. For each task, we systematically categorize different kinds of medical domain knowledge that have been utilized and their corresponding integrating methods. We also provide current challenges and directions for future research.



### Semi-Lexical Languages -- A Formal Basis for Unifying Machine Learning and Symbolic Reasoning in Computer Vision
- **Arxiv ID**: http://arxiv.org/abs/2004.12152v2
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.12152v2)
- **Published**: 2020-04-25 14:37:24+00:00
- **Updated**: 2020-12-17 12:20:08+00:00
- **Authors**: Briti Gangopadhyay, Somnath Hazra, Pallab Dasgupta
- **Comment**: The paper is under consideration at Pattern Recognition Letters
- **Journal**: None
- **Summary**: Human vision is able to compensate imperfections in sensory inputs from the real world by reasoning based on prior knowledge about the world. Machine learning has had a significant impact on computer vision due to its inherent ability in handling imprecision, but the absence of a reasoning framework based on domain knowledge limits its ability to interpret complex scenarios. We propose semi-lexical languages as a formal basis for dealing with imperfect tokens provided by the real world. The power of machine learning is used to map the imperfect tokens into the alphabet of the language and symbolic reasoning is used to determine the membership of input in the language. Semi-lexical languages also have bindings that prevent the variations in which a semi-lexical token is interpreted in different parts of the input, thereby leaning on deduction to enhance the quality of recognition of individual tokens. We present case studies that demonstrate the advantage of using such a framework over pure machine learning and pure symbolic methods.



### CNN based Road User Detection using the 3D Radar Cube
- **Arxiv ID**: http://arxiv.org/abs/2004.12165v2
- **DOI**: 10.1109/LRA.2020.2967272
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.12165v2)
- **Published**: 2020-04-25 15:07:03+00:00
- **Updated**: 2020-07-16 10:06:15+00:00
- **Authors**: Andras Palffy, Jiaao Dong, Julian F. P. Kooij, Dariu M. Gavrila
- **Comment**: None
- **Journal**: IEEE Robotics and Automation Letters (RAL), vol. 5, nr. 2, pp.
  1263-1270, 2020
- **Summary**: This letter presents a novel radar based, single-frame, multi-class detection method for moving road users (pedestrian, cyclist, car), which utilizes low-level radar cube data. The method provides class information both on the radar target- and object-level. Radar targets are classified individually after extending the target features with a cropped block of the 3D radar cube around their positions, thereby capturing the motion of moving parts in the local velocity distribution. A Convolutional Neural Network (CNN) is proposed for this classification step. Afterwards, object proposals are generated with a clustering step, which not only considers the radar targets' positions and velocities, but their calculated class scores as well. In experiments on a real-life dataset we demonstrate that our method outperforms the state-of-the-art methods both target- and object-wise by reaching an average of 0.70 (baseline: 0.68) target-wise and 0.56 (baseline: 0.48) object-wise F1 score. Furthermore, we examine the importance of the used features in an ablation study.



### Revisiting Sequence-to-Sequence Video Object Segmentation with Multi-Task Loss and Skip-Memory
- **Arxiv ID**: http://arxiv.org/abs/2004.12170v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.12170v1)
- **Published**: 2020-04-25 15:38:09+00:00
- **Updated**: 2020-04-25 15:38:09+00:00
- **Authors**: Fatemeh Azimi, Benjamin Bischke, Sebastian Palacio, Federico Raue, Joern Hees, Andreas Dengel
- **Comment**: None
- **Journal**: None
- **Summary**: Video Object Segmentation (VOS) is an active research area of the visual domain. One of its fundamental sub-tasks is semi-supervised / one-shot learning: given only the segmentation mask for the first frame, the task is to provide pixel-accurate masks for the object over the rest of the sequence. Despite much progress in the last years, we noticed that many of the existing approaches lose objects in longer sequences, especially when the object is small or briefly occluded. In this work, we build upon a sequence-to-sequence approach that employs an encoder-decoder architecture together with a memory module for exploiting the sequential data. We further improve this approach by proposing a model that manipulates multi-scale spatio-temporal information using memory-equipped skip connections. Furthermore, we incorporate an auxiliary task based on distance classification which greatly enhances the quality of edges in segmentation masks. We compare our approach to the state of the art and show considerable improvement in the contour accuracy metric and the overall segmentation accuracy.



### Cheaper Pre-training Lunch: An Efficient Paradigm for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2004.12178v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.12178v2)
- **Published**: 2020-04-25 16:09:46+00:00
- **Updated**: 2020-08-31 09:14:45+00:00
- **Authors**: Dongzhan Zhou, Xinchi Zhou, Hongwen Zhang, Shuai Yi, Wanli Ouyang
- **Comment**: Accepted by ECCV2020
- **Journal**: None
- **Summary**: In this paper, we propose a general and efficient pre-training paradigm, Montage pre-training, for object detection. Montage pre-training needs only the target detection dataset while taking only 1/4 computational resources compared to the widely adopted ImageNet pre-training.To build such an efficient paradigm, we reduce the potential redundancy by carefully extracting useful samples from the original images, assembling samples in a Montage manner as input, and using an ERF-adaptive dense classification strategy for model pre-training. These designs include not only a new input pattern to improve the spatial utilization but also a novel learning objective to expand the effective receptive field of the pretrained model. The efficiency and effectiveness of Montage pre-training are validated by extensive experiments on the MS-COCO dataset, where the results indicate that the models using Montage pre-training are able to achieve on-par or even better detection performances compared with the ImageNet pre-training.



### EfficientPose: Scalable single-person pose estimation
- **Arxiv ID**: http://arxiv.org/abs/2004.12186v2
- **DOI**: 10.1007/s10489-020-01918-7
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.12186v2)
- **Published**: 2020-04-25 16:50:46+00:00
- **Updated**: 2020-12-04 09:27:44+00:00
- **Authors**: Daniel Groos, Heri Ramampiaro, Espen A. F. Ihlen
- **Comment**: Published in Applied Intelligence Journal (APIN)
- **Journal**: Applied Intelligence 51 (2021) 2518-2533
- **Summary**: Single-person human pose estimation facilitates markerless movement analysis in sports, as well as in clinical applications. Still, state-of-the-art models for human pose estimation generally do not meet the requirements of real-life applications. The proliferation of deep learning techniques has resulted in the development of many advanced approaches. However, with the progresses in the field, more complex and inefficient models have also been introduced, which have caused tremendous increases in computational demands. To cope with these complexity and inefficiency challenges, we propose a novel convolutional neural network architecture, called EfficientPose, which exploits recently proposed EfficientNets in order to deliver efficient and scalable single-person pose estimation. EfficientPose is a family of models harnessing an effective multi-scale feature extractor and computationally efficient detection blocks using mobile inverted bottleneck convolutions, while at the same time ensuring that the precision of the pose configurations is still improved. Due to its low complexity and efficiency, EfficientPose enables real-world applications on edge devices by limiting the memory footprint and computational cost. The results from our experiments, using the challenging MPII single-person benchmark, show that the proposed EfficientPose models substantially outperform the widely-used OpenPose model both in terms of accuracy and computational efficiency. In particular, our top-performing model achieves state-of-the-art accuracy on single-person MPII, with low-complexity ConvNets.



### Detective: An Attentive Recurrent Model for Sparse Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2004.12197v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.12197v1)
- **Published**: 2020-04-25 17:41:52+00:00
- **Updated**: 2020-04-25 17:41:52+00:00
- **Authors**: Amine Kechaou, Manuel Martinez, Monica Haurilet, Rainer Stiefelhagen
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we present Detective - an attentive object detector that identifies objects in images in a sequential manner. Our network is based on an encoder-decoder architecture, where the encoder is a convolutional neural network, and the decoder is a convolutional recurrent neural network coupled with an attention mechanism. At each iteration, our decoder focuses on the relevant parts of the image using an attention mechanism, and then estimates the object's class and the bounding box coordinates. Current object detection models generate dense predictions and rely on post-processing to remove duplicate predictions. Detective is a sparse object detector that generates a single bounding box per object instance. However, training a sparse object detector is challenging, as it requires the model to reason at the instance level and not just at the class and spatial levels. We propose a training mechanism based on the Hungarian algorithm and a loss that balances the localization and classification tasks. This allows Detective to achieve promising results on the PASCAL VOC object detection dataset. Our experiments demonstrate that sparse object detection is possible and has a great potential for future developments in applications where the order of the objects to be predicted is of interest.



### On the safety of vulnerable road users by cyclist orientation detection using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2004.11909v1
- **DOI**: 10.1007/s00138-021-01231-4
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.11909v1)
- **Published**: 2020-04-25 18:10:26+00:00
- **Updated**: 2020-04-25 18:10:26+00:00
- **Authors**: Marichelo Garcia-Venegas, Diego A. Mercado-Ravell, Carlos A. Carballo-Monsivais
- **Comment**: "This paper is a preprint of a paper submitted to IET Intelligent
  Transport Systems. If accepted, the copy of record will be available at the
  IET Digital Library"
- **Journal**: Machine Vision and Applications 32, 109 (2021)
- **Summary**: In this work, orientation detection using Deep Learning is acknowledged for a particularly vulnerable class of road users,the cyclists. Knowing the cyclists' orientation is of great relevance since it provides a good notion about their future trajectory, which is crucial to avoid accidents in the context of intelligent transportation systems. Using Transfer Learning with pre-trained models and TensorFlow, we present a performance comparison between the main algorithms reported in the literature for object detection,such as SSD, Faster R-CNN and R-FCN along with MobilenetV2, InceptionV2, ResNet50, ResNet101 feature extractors. Moreover, we propose multi-class detection with eight different classes according to orientations. To do so, we introduce a new dataset called "Detect-Bike", containing 20,229 cyclist instances over 11,103 images, which has been labeled based on cyclist's orientation. Then, the same Deep Learning methods used for detection are trained to determine the target's heading. Our experimental results and vast evaluation showed satisfactory performance of all of the studied methods for the cyclists and their orientation detection, especially using Faster R-CNN with ResNet50 proved to be precise but significantly slower. Meanwhile, SSD using InceptionV2 provided good trade-off between precision and execution time, and is to be preferred for real-time embedded applications.



### Explainable Deep CNNs for MRI-Based Diagnosis of Alzheimer's Disease
- **Arxiv ID**: http://arxiv.org/abs/2004.12204v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.12204v1)
- **Published**: 2020-04-25 18:14:49+00:00
- **Updated**: 2020-04-25 18:14:49+00:00
- **Authors**: Eduardo Nigri, Nivio Ziviani, Fabio Cappabianco, Augusto Antunes, Adriano Veloso
- **Comment**: Accepted for the IEEE International Joint Conference on Neural
  Networks (IJCNN) 2020
- **Journal**: None
- **Summary**: Deep Convolutional Neural Networks (CNNs) are becoming prominent models for semi-automated diagnosis of Alzheimer's Disease (AD) using brain Magnetic Resonance Imaging (MRI). Although being highly accurate, deep CNN models lack transparency and interpretability, precluding adequate clinical reasoning and not complying with most current regulatory demands. One popular choice for explaining deep image models is occluding regions of the image to isolate their influence on the prediction. However, existing methods for occluding patches of brain scans generate images outside the distribution to which the model was trained for, thus leading to unreliable explanations. In this paper, we propose an alternative explanation method that is specifically designed for the brain scan task. Our method, which we refer to as Swap Test, produces heatmaps that depict the areas of the brain that are most indicative of AD, providing interpretability for the model's decisions in a format understandable to clinicians. Experimental results using an axiomatic evaluation show that the proposed method is more suitable for explaining the diagnosis of AD using MRI while the opposite trend was observed when using a typical occlusion test. Therefore, we believe our method may address the inherent black-box nature of deep neural networks that are capable of diagnosing AD.



### Deep DIH : Statistically Inferred Reconstruction of Digital In-Line Holography by Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2004.12231v2
- **DOI**: 10.1109/ACCESS.2020.3036380
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.12231v2)
- **Published**: 2020-04-25 20:39:25+00:00
- **Updated**: 2020-06-24 22:08:02+00:00
- **Authors**: Huayu Li, Xiwen Chen, Haiyu Wu, Zaoyi Chi, Christopher Mann, Abolfazl Razi
- **Comment**: None
- **Journal**: None
- **Summary**: Digital in-line holography is commonly used to reconstruct 3D images from 2D holograms for microscopic objects. One of the technical challenges that arise in the signal processing stage is removing the twin image that is caused by the phase-conjugate wavefront from the recorded holograms. Twin image removal is typically formulated as a non-linear inverse problem due to the irreversible scattering process when generating the hologram. Recently, end-to-end deep learning-based methods have been utilized to reconstruct the object wavefront (as a surrogate for the 3D structure of the object) directly from a single-shot in-line digital hologram. However, massive data pairs are required to train deep learning models for acceptable reconstruction precision. In contrast to typical image processing problems, well-curated datasets for in-line digital holography does not exist. Also, the trained model highly influenced by the morphological properties of the object and hence can vary for different applications. Therefore, data collection can be prohibitively cumbersome in practice as a major hindrance to using deep learning for digital holography. In this paper, we proposed a novel implementation of autoencoder-based deep learning architecture for single-shot hologram reconstruction solely based on the current sample without the need for massive datasets to train the model. The simulations results demonstrate the superior performance of the proposed method compared to the state of the art single-shot compressive digital in-line hologram reconstruction method.



### Reconstruct, Rasterize and Backprop: Dense shape and pose estimation from a single image
- **Arxiv ID**: http://arxiv.org/abs/2004.12232v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.12232v1)
- **Published**: 2020-04-25 20:53:43+00:00
- **Updated**: 2020-04-25 20:53:43+00:00
- **Authors**: Aniket Pokale, Aditya Aggarwal, K. Madhava Krishna
- **Comment**: 8 pages, 5 figures, 2 tables
- **Journal**: None
- **Summary**: This paper presents a new system to obtain dense object reconstructions along with 6-DoF poses from a single image. Geared towards high fidelity reconstruction, several recent approaches leverage implicit surface representations and deep neural networks to estimate a 3D mesh of an object, given a single image. However, all such approaches recover only the shape of an object; the reconstruction is often in a canonical frame, unsuitable for downstream robotics tasks. To this end, we leverage recent advances in differentiable rendering (in particular, rasterization) to close the loop with 3D reconstruction in camera frame. We demonstrate that our approach---dubbed reconstruct, rasterize and backprop (RRB) achieves significantly lower pose estimation errors compared to prior art, and is able to recover dense object shapes and poses from imagery. We further extend our results to an (offline) setup, where we demonstrate a dense monocular object-centric egomotion estimation system.



### Congestion-aware Evacuation Routing using Augmented Reality Devices
- **Arxiv ID**: http://arxiv.org/abs/2004.12246v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.12246v1)
- **Published**: 2020-04-25 22:54:35+00:00
- **Updated**: 2020-04-25 22:54:35+00:00
- **Authors**: Zeyu Zhang, Hangxin Liu, Ziyuan Jiao, Yixin Zhu, Song-Chun Zhu
- **Comment**: Accepted to ICRA 2020
- **Journal**: None
- **Summary**: We present a congestion-aware routing solution for indoor evacuation, which produces real-time individual-customized evacuation routes among multiple destinations while keeping tracks of all evacuees' locations. A population density map, obtained on-the-fly by aggregating locations of evacuees from user-end Augmented Reality (AR) devices, is used to model the congestion distribution inside a building. To efficiently search the evacuation route among all destinations, a variant of A* algorithm is devised to obtain the optimal solution in a single pass. In a series of simulated studies, we show that the proposed algorithm is more computationally optimized compared to classic path planning algorithms; it generates a more time-efficient evacuation route for each individual that minimizes the overall congestion. A complete system using AR devices is implemented for a pilot study in real-world environments, demonstrating the efficacy of the proposed approach.



### Joint Inference of States, Robot Knowledge, and Human (False-)Beliefs
- **Arxiv ID**: http://arxiv.org/abs/2004.12248v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2004.12248v1)
- **Published**: 2020-04-25 23:02:04+00:00
- **Updated**: 2020-04-25 23:02:04+00:00
- **Authors**: Tao Yuan, Hangxin Liu, Lifeng Fan, Zilong Zheng, Tao Gao, Yixin Zhu, Song-Chun Zhu
- **Comment**: Accepted to ICRA 2020
- **Journal**: None
- **Summary**: Aiming to understand how human (false-)belief--a core socio-cognitive ability--would affect human interactions with robots, this paper proposes to adopt a graphical model to unify the representation of object states, robot knowledge, and human (false-)beliefs. Specifically, a parse graph (pg) is learned from a single-view spatiotemporal parsing by aggregating various object states along the time; such a learned representation is accumulated as the robot's knowledge. An inference algorithm is derived to fuse individual pg from all robots across multi-views into a joint pg, which affords more effective reasoning and inference capability to overcome the errors originated from a single view. In the experiments, through the joint inference over pg-s, the system correctly recognizes human (false-)belief in various settings and achieves better cross-view accuracy on a challenging small object tracking dataset.



