# Arxiv Papers in cs.CV on 2020-04-30
### MobileDets: Searching for Object Detection Architectures for Mobile Accelerators
- **Arxiv ID**: http://arxiv.org/abs/2004.14525v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.14525v3)
- **Published**: 2020-04-30 00:21:30+00:00
- **Updated**: 2021-03-31 01:21:42+00:00
- **Authors**: Yunyang Xiong, Hanxiao Liu, Suyog Gupta, Berkin Akin, Gabriel Bender, Yongzhe Wang, Pieter-Jan Kindermans, Mingxing Tan, Vikas Singh, Bo Chen
- **Comment**: Accepted at CVPR 2021; Code and models are available in the
  TensorFlow Object Detection API:
  https://github.com/tensorflow/models/tree/master/research/object_detection
- **Journal**: None
- **Summary**: Inverted bottleneck layers, which are built upon depthwise convolutions, have been the predominant building blocks in state-of-the-art object detection models on mobile devices. In this work, we investigate the optimality of this design pattern over a broad range of mobile accelerators by revisiting the usefulness of regular convolutions. We discover that regular convolutions are a potent component to boost the latency-accuracy trade-off for object detection on accelerators, provided that they are placed strategically in the network via neural architecture search. By incorporating regular convolutions in the search space and directly optimizing the network architectures for object detection, we obtain a family of object detection models, MobileDets, that achieve state-of-the-art results across mobile accelerators. On the COCO object detection task, MobileDets outperform MobileNetV3+SSDLite by 1.7 mAP at comparable mobile CPU inference latencies. MobileDets also outperform MobileNetV2+SSDLite by 1.9 mAP on mobile CPUs, 3.7 mAP on Google EdgeTPU, 3.4 mAP on Qualcomm Hexagon DSP and 2.7 mAP on Nvidia Jetson GPU without increasing latency. Moreover, MobileDets are comparable with the state-of-the-art MnasFPN on mobile CPUs even without using the feature pyramid, and achieve better mAP scores on both EdgeTPUs and DSPs with up to 2x speedup. Code and models are available in the TensorFlow Object Detection API: https://github.com/tensorflow/models/tree/master/research/object_detection.



### Bias-corrected estimator for intrinsic dimension and differential entropy--a visual multiscale approach
- **Arxiv ID**: http://arxiv.org/abs/2004.14528v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.IT, cs.LG, eess.SP, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/2004.14528v1)
- **Published**: 2020-04-30 00:29:28+00:00
- **Updated**: 2020-04-30 00:29:28+00:00
- **Authors**: Jugurta Montalvão, Jânio Canuto, Luiz Miranda
- **Comment**: 10 pages, 11 figures
- **Journal**: None
- **Summary**: Intrinsic dimension and differential entropy estimators are studied in this paper, including their systematic bias. A pragmatic approach for joint estimation and bias correction of these two fundamental measures is proposed. Shared steps on both estimators are highlighted, along with their useful consequences to data analysis. It is shown that both estimators can be complementary parts of a single approach, and that the simultaneous estimation of differential entropy and intrinsic dimension give meaning to each other, where estimates at different observation scales convey different perspectives of underlying manifolds. Experiments with synthetic and real datasets are presented to illustrate how to extract meaning from visual inspections, and how to compensate for biases.



### Physarum Powered Differentiable Linear Programming Layers and Applications
- **Arxiv ID**: http://arxiv.org/abs/2004.14539v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.14539v2)
- **Published**: 2020-04-30 01:50:37+00:00
- **Updated**: 2021-05-10 17:21:06+00:00
- **Authors**: Zihang Meng, Sathya N. Ravi, Vikas Singh
- **Comment**: None
- **Journal**: None
- **Summary**: Consider a learning algorithm, which involves an internal call to an optimization routine such as a generalized eigenvalue problem, a cone programming problem or even sorting. Integrating such a method as a layer(s) within a trainable deep neural network (DNN) in an efficient and numerically stable way is not straightforward -- for instance, only recently, strategies have emerged for eigendecomposition and differentiable sorting. We propose an efficient and differentiable solver for general linear programming problems which can be used in a plug and play manner within DNNs as a layer. Our development is inspired by a fascinating but not widely used link between dynamics of slime mold (physarum) and optimization schemes such as steepest descent. We describe our development and show the use of our solver in a video segmentation task and meta-learning for few-shot learning. We review the existing results and provide a technical analysis describing its applicability for our use cases. Our solver performs comparably with a customized projected gradient descent method on the first task and outperforms the differentiable CVXPY-SCS solver on the second task. Experiments show that our solver converges quickly without the need for a feasible initial point. Our proposal is easy to implement and can easily serve as layers whenever a learning procedure needs a fast approximate solution to a LP, within a larger network.



### Salient Object Detection Combining a Self-attention Module and a Feature Pyramid Network
- **Arxiv ID**: http://arxiv.org/abs/2004.14552v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.14552v1)
- **Published**: 2020-04-30 03:08:34+00:00
- **Updated**: 2020-04-30 03:08:34+00:00
- **Authors**: Guangyu Ren, Tianhong Dai, Panagiotis Barmpoutis, Tania Stathaki
- **Comment**: None
- **Journal**: None
- **Summary**: Salient object detection has achieved great improvement by using the Fully Convolution Network (FCN). However, the FCN-based U-shape architecture may cause the dilution problem in the high-level semantic information during the up-sample operations in the top-down pathway. Thus, it can weaken the ability of salient object localization and produce degraded boundaries. To this end, in order to overcome this limitation, we propose a novel pyramid self-attention module (PSAM) and the adoption of an independent feature-complementing strategy. In PSAM, self-attention layers are equipped after multi-scale pyramid features to capture richer high-level features and bring larger receptive fields to the model. In addition, a channel-wise attention module is also employed to reduce the redundant features of the FPN and provide refined results. Experimental analysis shows that the proposed PSAM effectively contributes to the whole model so that it outperforms state-of-the-art results over five challenging datasets. Finally, quantitative results show that PSAM generates clear and integral salient maps which can provide further help to other computer vision tasks, such as object detection and semantic segmentation.



### COVID-DA: Deep Domain Adaptation from Typical Pneumonia to COVID-19
- **Arxiv ID**: http://arxiv.org/abs/2005.01577v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.01577v1)
- **Published**: 2020-04-30 03:13:40+00:00
- **Updated**: 2020-04-30 03:13:40+00:00
- **Authors**: Yifan Zhang, Shuaicheng Niu, Zhen Qiu, Ying Wei, Peilin Zhao, Jianhua Yao, Junzhou Huang, Qingyao Wu, Mingkui Tan
- **Comment**: None
- **Journal**: None
- **Summary**: The outbreak of novel coronavirus disease 2019 (COVID-19) has already infected millions of people and is still rapidly spreading all over the globe. Most COVID-19 patients suffer from lung infection, so one important diagnostic method is to screen chest radiography images, e.g., X-Ray or CT images. However, such examinations are time-consuming and labor-intensive, leading to limited diagnostic efficiency. To solve this issue, AI-based technologies, such as deep learning, have been used recently as effective computer-aided means to improve diagnostic efficiency. However, one practical and critical difficulty is the limited availability of annotated COVID-19 data, due to the prohibitive annotation costs and urgent work of doctors to fight against the pandemic. This makes the learning of deep diagnosis models very challenging. To address this, motivated by that typical pneumonia has similar characteristics with COVID-19 and many pneumonia datasets are publicly available, we propose to conduct domain knowledge adaptation from typical pneumonia to COVID-19. There are two main challenges: 1) the discrepancy of data distributions between domains; 2) the task difference between the diagnosis of typical pneumonia and COVID-19. To address them, we propose a new deep domain adaptation method for COVID-19 diagnosis, namely COVID-DA. Specifically, we alleviate the domain discrepancy via feature adversarial adaptation and handle the task difference issue via a novel classifier separation scheme. In this way, COVID-DA is able to diagnose COVID-19 effectively with only a small number of COVID-19 annotations. Extensive experiments verify the effectiveness of COVID-DA and its great potential for real-world applications.



### Learning Deformable Image Registration from Optimization: Perspective, Modules, Bilevel Training and Beyond
- **Arxiv ID**: http://arxiv.org/abs/2004.14557v4
- **DOI**: 10.1109/TPAMI.2021.3115825
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.14557v4)
- **Published**: 2020-04-30 03:23:45+00:00
- **Updated**: 2021-09-30 01:12:11+00:00
- **Authors**: Risheng Liu, Zi Li, Xin Fan, Chenying Zhao, Hao Huang, Zhongxuan Luo
- **Comment**: None
- **Journal**: IEEE Transactions on Pattern Analysis and Machine Intelligence
  2021
- **Summary**: Conventional deformable registration methods aim at solving an optimization model carefully designed on image pairs and their computational costs are exceptionally high. In contrast, recent deep learning based approaches can provide fast deformation estimation. These heuristic network architectures are fully data-driven and thus lack explicit geometric constraints, e.g., topology-preserving, which are indispensable to generate plausible deformations. We design a new deep learning based framework to optimize a diffeomorphic model via multi-scale propagation in order to integrate advantages and avoid limitations of these two categories of approaches. Specifically, we introduce a generic optimization model to formulate diffeomorphic registration and develop a series of learnable architectures to obtain propagative updating in the coarse-to-fine feature space. Moreover, we propose a novel bilevel self-tuned training strategy, allowing efficient search of task-specific hyper-parameters. This training strategy increases the flexibility to various types of data while reduces computational and human burdens. We conduct two groups of image registration experiments on 3D volume datasets including image-to-atlas registration on brain MRI data and image-to-image registration on liver CT data. Extensive results demonstrate the state-of-the-art performance of the proposed method with diffeomorphic guarantee and extreme efficiency. We also apply our framework to challenging multi-modal image registration, and investigate how our registration to support the down-streaming tasks for medical image analysis including multi-modal fusion and image segmentation.



### TRP: Trained Rank Pruning for Efficient Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2004.14566v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.14566v1)
- **Published**: 2020-04-30 03:37:36+00:00
- **Updated**: 2020-04-30 03:37:36+00:00
- **Authors**: Yuhui Xu, Yuxi Li, Shuai Zhang, Wei Wen, Botao Wang, Yingyong Qi, Yiran Chen, Weiyao Lin, Hongkai Xiong
- **Comment**: Accepted by IJCAI2020, An extension version of arXiv:1812.02402
- **Journal**: None
- **Summary**: To enable DNNs on edge devices like mobile phones, low-rank approximation has been widely adopted because of its solid theoretical rationale and efficient implementations. Several previous works attempted to directly approximate a pretrained model by low-rank decomposition; however, small approximation errors in parameters can ripple over a large prediction loss. As a result, performance usually drops significantly and a sophisticated effort on fine-tuning is required to recover accuracy. Apparently, it is not optimal to separate low-rank approximation from training. Unlike previous works, this paper integrates low rank approximation and regularization into the training process. We propose Trained Rank Pruning (TRP), which alternates between low rank approximation and training. TRP maintains the capacity of the original network while imposing low-rank constraints during training. A nuclear regularization optimized by stochastic sub-gradient descent is utilized to further promote low rank in TRP. The TRP trained network inherently has a low-rank structure, and is approximated with negligible performance loss, thus eliminating the fine-tuning process after low rank decomposition. The proposed method is comprehensively evaluated on CIFAR-10 and ImageNet, outperforming previous compression methods using low rank approximation.



### APB2Face: Audio-guided face reenactment with auxiliary pose and blink signals
- **Arxiv ID**: http://arxiv.org/abs/2004.14569v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.14569v1)
- **Published**: 2020-04-30 03:44:35+00:00
- **Updated**: 2020-04-30 03:44:35+00:00
- **Authors**: Jiangning Zhang, Liang Liu, Zhucun Xue, Yong Liu
- **Comment**: ICASSP 2020
- **Journal**: None
- **Summary**: Audio-guided face reenactment aims at generating photorealistic faces using audio information while maintaining the same facial movement as when speaking to a real person. However, existing methods can not generate vivid face images or only reenact low-resolution faces, which limits the application value. To solve those problems, we propose a novel deep neural network named APB2Face, which consists of GeometryPredictor and FaceReenactor modules. GeometryPredictor uses extra head pose and blink state signals as well as audio to predict the latent landmark geometry information, while FaceReenactor inputs the face landmark image to reenact the photorealistic face. A new dataset AnnVI collected from YouTube is presented to support the approach, and experimental results indicate the superiority of our method than state-of-the-arts, whether in authenticity or controllability.



### Feedback U-net for Cell Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2004.14581v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.14581v1)
- **Published**: 2020-04-30 04:16:26+00:00
- **Updated**: 2020-04-30 04:16:26+00:00
- **Authors**: Eisuke Shibuya, Kazuhiro Hotta
- **Comment**: Accepted by CVPR2020 Workshop "Computer Vision for Microscopy Image
  Analysis (CVMI)"
- **Journal**: None
- **Summary**: Human brain is a layered structure, and performs not only a feedforward process from a lower layer to an upper layer but also a feedback process from an upper layer to a lower layer. The layer is a collection of neurons, and neural network is a mathematical model of the function of neurons. Although neural network imitates the human brain, everyone uses only feedforward process from the lower layer to the upper layer, and feedback process from the upper layer to the lower layer is not used. Therefore, in this paper, we propose Feedback U-Net using Convolutional LSTM which is the segmentation method using Convolutional LSTM and feedback process. The output of U-net gave feedback to the input, and the second round is performed. By using Convolutional LSTM, the features in the second round are extracted based on the features acquired in the first round. On both of the Drosophila cell image and Mouse cell image datasets, our method outperformed conventional U-Net which uses only feedforward process.



### Bilateral Attention Network for RGB-D Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2004.14582v1
- **DOI**: 10.1109/TIP.2021.3049959
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.14582v1)
- **Published**: 2020-04-30 04:23:32+00:00
- **Updated**: 2020-04-30 04:23:32+00:00
- **Authors**: Zhao Zhang, Zheng Lin, Jun Xu, Wenda Jin, Shao-Ping Lu, Deng-Ping Fan
- **Comment**: None
- **Journal**: None
- **Summary**: Most existing RGB-D salient object detection (SOD) methods focus on the foreground region when utilizing the depth images. However, the background also provides important information in traditional SOD methods for promising performance. To better explore salient information in both foreground and background regions, this paper proposes a Bilateral Attention Network (BiANet) for the RGB-D SOD task. Specifically, we introduce a Bilateral Attention Module (BAM) with a complementary attention mechanism: foreground-first (FF) attention and background-first (BF) attention. The FF attention focuses on the foreground region with a gradual refinement style, while the BF one recovers potentially useful salient information in the background region. Benefitted from the proposed BAM module, our BiANet can capture more meaningful foreground and background cues, and shift more attention to refining the uncertain details between foreground and background regions. Additionally, we extend our BAM by leveraging the multi-scale techniques for better SOD performance. Extensive experiments on six benchmark datasets demonstrate that our BiANet outperforms other state-of-the-art RGB-D SOD methods in terms of objective metrics and subjective visual comparison. Our BiANet can run up to 80fps on $224\times224$ RGB-D images, with an NVIDIA GeForce RTX 2080Ti GPU. Comprehensive ablation studies also validate our contributions.



### Out-of-the-box channel pruned networks
- **Arxiv ID**: http://arxiv.org/abs/2004.14584v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.14584v1)
- **Published**: 2020-04-30 04:40:47+00:00
- **Updated**: 2020-04-30 04:40:47+00:00
- **Authors**: Ragav Venkatesan, Gurumurthy Swaminathan, Xiong Zhou, Anna Luo
- **Comment**: Under review at ECCV 2020
- **Journal**: None
- **Summary**: In the last decade convolutional neural networks have become gargantuan. Pre-trained models, when used as initializers are able to fine-tune ever larger networks on small datasets. Consequently, not all the convolutional features that these fine-tuned models detect are requisite for the end-task. Several works of channel pruning have been proposed to prune away compute and memory from models that were trained already. Typically, these involve policies that decide which and how many channels to remove from each layer leading to channel-wise and/or layer-wise pruning profiles, respectively. In this paper, we conduct several baseline experiments and establish that profiles from random channel-wise pruning policies are as good as metric-based ones. We also establish that there may exist profiles from some layer-wise pruning policies that are measurably better than common baselines. We then demonstrate that the top layer-wise pruning profiles found using an exhaustive random search from one datatset are also among the top profiles for other datasets. This implies that we could identify out-of-the-box layer-wise pruning profiles using benchmark datasets and use these directly for new datasets. Furthermore, we develop a Reinforcement Learning (RL) policy-based search algorithm with a direct objective of finding transferable layer-wise pruning profiles using many models for the same architecture. We use a novel reward formulation that drives this RL search towards an expected compression while maximizing accuracy. Our results show that our transferred RL-based profiles are as good or better than best profiles found on the original dataset via exhaustive search. We then demonstrate that if we found the profiles using a mid-sized dataset such as Cifar10/100, we are able to transfer them to even a large dataset such as Imagenet.



### EXACT: A collaboration toolset for algorithm-aided annotation of images with annotation version control
- **Arxiv ID**: http://arxiv.org/abs/2004.14595v3
- **DOI**: 10.1038/s41598-021-83827-4
- **Categories**: **cs.HC**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.14595v3)
- **Published**: 2020-04-30 06:07:21+00:00
- **Updated**: 2021-07-19 12:29:32+00:00
- **Authors**: Christian Marzahl, Marc Aubreville, Christof A. Bertram, Jennifer Maier, Christian Bergler, Christine Kröger, Jörn Voigt, Katharina Breininger, Robert Klopfleisch, Andreas Maier
- **Comment**: None
- **Journal**: Scientific Reports 2021
- **Summary**: In many research areas, scientific progress is accelerated by multidisciplinary access to image data and their interdisciplinary annotation. However, keeping track of these annotations to ensure a high-quality multi-purpose data set is a challenging and labour intensive task. We developed the open-source online platform EXACT (EXpert Algorithm Collaboration Tool) that enables the collaborative interdisciplinary analysis of images from different domains online and offline. EXACT supports multi-gigapixel medical whole slide images as well as image series with thousands of images. The software utilises a flexible plugin system that can be adapted to diverse applications such as counting mitotic figures with a screening mode, finding false annotations on a novel validation view, or using the latest deep learning image analysis technologies. This is combined with a version control system which makes it possible to keep track of changes in the data sets and, for example, to link the results of deep learning experiments to specific data set versions. EXACT is freely available and has already been successfully applied to a broad range of annotation tasks, including highly diverse applications like deep learning supported cytology scoring, interdisciplinary multi-centre whole slide image tumour annotation, and highly specialised whale sound spectroscopy clustering.



### Dynamic Language Binding in Relational Visual Reasoning
- **Arxiv ID**: http://arxiv.org/abs/2004.14603v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.14603v3)
- **Published**: 2020-04-30 06:26:20+00:00
- **Updated**: 2021-02-18 03:35:24+00:00
- **Authors**: Thao Minh Le, Vuong Le, Svetha Venkatesh, Truyen Tran
- **Comment**: Early version accepted by IJCAI20, Code available at
  https://github.com/thaolmk54/LOGNet-VQA
- **Journal**: None
- **Summary**: We present Language-binding Object Graph Network, the first neural reasoning method with dynamic relational structures across both visual and textual domains with applications in visual question answering. Relaxing the common assumption made by current models that the object predicates pre-exist and stay static, passive to the reasoning process, we propose that these dynamic predicates expand across the domain borders to include pair-wise visual-linguistic object binding. In our method, these contextualized object links are actively found within each recurrent reasoning step without relying on external predicative priors. These dynamic structures reflect the conditional dual-domain object dependency given the evolving context of the reasoning through co-attention. Such discovered dynamic graphs facilitate multi-step knowledge combination and refinements that iteratively deduce the compact representation of the final answer. The effectiveness of this model is demonstrated on image question answering demonstrating favorable performance on major VQA datasets. Our method outperforms other methods in sophisticated question-answering tasks wherein multiple object relations are involved. The graph structure effectively assists the progress of training, and therefore the network learns efficiently compared to other reasoning models.



### Vision-based techniques for gait recognition
- **Arxiv ID**: http://arxiv.org/abs/2005.02148v1
- **DOI**: 10.1007/s11042-013-1574-x
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.02148v1)
- **Published**: 2020-04-30 06:33:01+00:00
- **Updated**: 2020-04-30 06:33:01+00:00
- **Authors**: Tracey K. M. Lee, Mohammed Belkhatir, Saeid Sanei
- **Comment**: None
- **Journal**: None
- **Summary**: Global security concerns have raised a proliferation of video surveillance devices. Intelligent surveillance systems seek to discover possible threats automatically and raise alerts. Being able to identify the surveyed object can help determine its threat level. The current generation of devices provide digital video data to be analysed for time varying features to assist in the identification process. Commonly, people queue up to access a facility and approach a video camera in full frontal view. In this environment, a variety of biometrics are available - for example, gait which includes temporal features like stride period. Gait can be measured unobtrusively at a distance. The video data will also include face features, which are short-range biometrics. In this way, one can combine biometrics naturally using one set of data. In this paper we survey current techniques of gait recognition and modelling with the environment in which the research was conducted. We also discuss in detail the issues arising from deriving gait data, such as perspective and occlusion effects, together with the associated computer vision challenges of reliable tracking of human movement. Then, after highlighting these issues and challenges related to gait processing, we proceed to discuss the frameworks combining gait with other biometrics. We then provide motivations for a novel paradigm in biometrics-based human recognition, i.e. the use of the fronto-normal view of gait as a far-range biometrics combined with biometrics operating at a near distance.



### Generative Adversarial Data Programming
- **Arxiv ID**: http://arxiv.org/abs/2005.00364v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.00364v1)
- **Published**: 2020-04-30 07:06:44+00:00
- **Updated**: 2020-04-30 07:06:44+00:00
- **Authors**: Arghya Pal, Vineeth N Balasubramanian
- **Comment**: arXiv admin note: text overlap with arXiv:1803.05137
- **Journal**: None
- **Summary**: The paucity of large curated hand-labeled training data forms a major bottleneck in the deployment of machine learning models in computer vision and other fields. Recent work (Data Programming) has shown how distant supervision signals in the form of labeling functions can be used to obtain labels for given data in near-constant time. In this work, we present Adversarial Data Programming (ADP), which presents an adversarial methodology to generate data as well as a curated aggregated label, given a set of weak labeling functions. More interestingly, such labeling functions are often easily generalizable, thus allowing our framework to be extended to different setups, including self-supervised labeled image generation, zero-shot text to labeled image generation, transfer learning, and multi-task learning.



### The 4th AI City Challenge
- **Arxiv ID**: http://arxiv.org/abs/2004.14619v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.DB, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.14619v1)
- **Published**: 2020-04-30 07:47:14+00:00
- **Updated**: 2020-04-30 07:47:14+00:00
- **Authors**: Milind Naphade, Shuo Wang, David Anastasiu, Zheng Tang, Ming-Ching Chang, Xiaodong Yang, Liang Zheng, Anuj Sharma, Rama Chellappa, Pranamesh Chakraborty
- **Comment**: Organization summary of the 4th AI City Challenge Workshop @ CVPR
  2020
- **Journal**: None
- **Summary**: The AI City Challenge was created to accelerate intelligent video analysis that helps make cities smarter and safer. Transportation is one of the largest segments that can benefit from actionable insights derived from data captured by sensors, where computer vision and deep learning have shown promise in achieving large-scale practical deployment. The 4th annual edition of the AI City Challenge has attracted 315 participating teams across 37 countries, who leveraged city-scale real traffic data and high-quality synthetic data to compete in four challenge tracks. Track 1 addressed video-based automatic vehicle counting, where the evaluation is conducted on both algorithmic effectiveness and computational efficiency. Track 2 addressed city-scale vehicle re-identification with augmented synthetic data to substantially increase the training set for the task. Track 3 addressed city-scale multi-target multi-camera vehicle tracking. Track 4 addressed traffic anomaly detection. The evaluation system shows two leader boards, in which a general leader board shows all submitted results, and a public leader board shows results limited to our contest participation rules, that teams are not allowed to use external data in their work. The public leader board shows results more close to real-world situations where annotated data are limited. Our results show promise that AI technology can enable smarter and safer transportation systems.



### Towards Embodied Scene Description
- **Arxiv ID**: http://arxiv.org/abs/2004.14638v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.14638v2)
- **Published**: 2020-04-30 08:50:25+00:00
- **Updated**: 2020-05-07 09:14:37+00:00
- **Authors**: Sinan Tan, Huaping Liu, Di Guo, Xinyu Zhang, Fuchun Sun
- **Comment**: Accepted in Robotics: Science and Systems 2020
- **Journal**: None
- **Summary**: Embodiment is an important characteristic for all intelligent agents (creatures and robots), while existing scene description tasks mainly focus on analyzing images passively and the semantic understanding of the scenario is separated from the interaction between the agent and the environment. In this work, we propose the Embodied Scene Description, which exploits the embodiment ability of the agent to find an optimal viewpoint in its environment for scene description tasks. A learning framework with the paradigms of imitation learning and reinforcement learning is established to teach the intelligent agent to generate corresponding sensorimotor activities. The proposed framework is tested on both the AI2Thor dataset and a real world robotic platform demonstrating the effectiveness and extendability of the developed method.



### DIABLO: Dictionary-based Attention Block for Deep Metric Learning
- **Arxiv ID**: http://arxiv.org/abs/2004.14644v1
- **DOI**: 10.1016/j.patrec.2020.03.020
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.14644v1)
- **Published**: 2020-04-30 09:05:56+00:00
- **Updated**: 2020-04-30 09:05:56+00:00
- **Authors**: Pierre Jacob, David Picard, Aymeric Histace, Edouard Klein
- **Comment**: Pre-print. Accepted for publication at Pattern Recognition Letters
- **Journal**: None
- **Summary**: Recent breakthroughs in representation learning of unseen classes and examples have been made in deep metric learning by training at the same time the image representations and a corresponding metric with deep networks. Recent contributions mostly address the training part (loss functions, sampling strategies, etc.), while a few works focus on improving the discriminative power of the image representation. In this paper, we propose DIABLO, a dictionary-based attention method for image embedding. DIABLO produces richer representations by aggregating only visually-related features together while being easier to train than other attention-based methods in deep metric learning. This is experimentally confirmed on four deep metric learning datasets (Cub-200-2011, Cars-196, Stanford Online Products, and In-Shop Clothes Retrieval) for which DIABLO shows state-of-the-art performances.



### M^3VSNet: Unsupervised Multi-metric Multi-view Stereo Network
- **Arxiv ID**: http://arxiv.org/abs/2005.00363v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.00363v2)
- **Published**: 2020-04-30 09:26:51+00:00
- **Updated**: 2020-06-06 03:07:12+00:00
- **Authors**: Baichuan Huang, Hongwei Yi, Can Huang, Yijia He, Jingbin Liu, Xiao Liu
- **Comment**: The original top-level version is arXiv:2004.09722v2 but I upload the
  similar version to arXiv:2005.00363 mistakenly, which is overlapped with
  arXiv:2004.09722v2. So the submission is to make the two addresses keeping
  the same version
- **Journal**: None
- **Summary**: The present Multi-view stereo (MVS) methods with supervised learning-based networks have an impressive performance comparing with traditional MVS methods. However, the ground-truth depth maps for training are hard to be obtained and are within limited kinds of scenarios. In this paper, we propose a novel unsupervised multi-metric MVS network, named M^3VSNet, for dense point cloud reconstruction without any supervision. To improve the robustness and completeness of point cloud reconstruction, we propose a novel multi-metric loss function that combines pixel-wise and feature-wise loss function to learn the inherent constraints from different perspectives of matching correspondences. Besides, we also incorporate the normal-depth consistency in the 3D point cloud format to improve the accuracy and continuity of the estimated depth maps. Experimental results show that M3VSNet establishes the state-of-the-arts unsupervised method and achieves comparable performance with previous supervised MVSNet on the DTU dataset and demonstrates the powerful generalization ability on the Tanks and Temples benchmark with effective improvement. Our code is available at https://github.com/whubaichuan/M3VSNet



### Attentive Weakly Supervised land cover mapping for object-based satellite image time series data with spatial interpretation
- **Arxiv ID**: http://arxiv.org/abs/2004.14672v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.14672v1)
- **Published**: 2020-04-30 10:23:12+00:00
- **Updated**: 2020-04-30 10:23:12+00:00
- **Authors**: Dino Ienco, Yawogan Jean Eudes Gbodjo, Roberto Interdonato, Raffaele Gaetano
- **Comment**: Under submission to Elsevier journal
- **Journal**: None
- **Summary**: Nowadays, modern Earth Observation systems continuously collect massive amounts of satellite information. The unprecedented possibility to acquire high resolution Satellite Image Time Series (SITS) data (series of images with high revisit time period on the same geographical area) is opening new opportunities to monitor the different aspects of the Earth Surface but, at the same time, it is raising up new challenges in term of suitable methods to analyze and exploit such huge amount of rich and complex image data. One of the main task associated to SITS data analysis is related to land cover mapping where satellite data are exploited via learning methods to recover the Earth Surface status aka the corresponding land cover classes. Due to operational constraints, the collected label information, on which machine learning strategies are trained, is often limited in volume and obtained at coarse granularity carrying out inexact and weak knowledge that can affect the whole process. To cope with such issues, in the context of object-based SITS land cover mapping, we propose a new deep learning framework, named TASSEL (aTtentive weAkly Supervised Satellite image time sEries cLassifier), that is able to intelligently exploit the weak supervision provided by the coarse granularity labels. Furthermore, our framework also produces an additional side-information that supports the model interpretability with the aim to make the black box gray. Such side-information allows to associate spatial interpretation to the model decision via visual inspection.



### SS3D: Single Shot 3D Object Detector
- **Arxiv ID**: http://arxiv.org/abs/2004.14674v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.14674v2)
- **Published**: 2020-04-30 10:28:08+00:00
- **Updated**: 2020-05-02 12:33:08+00:00
- **Authors**: Aniket Limaye, Manu Mathew, Soyeb Nagori, Pramod Kumar Swami, Debapriya Maji, Kumar Desappan
- **Comment**: None
- **Journal**: None
- **Summary**: Single stage deep learning algorithm for 2D object detection was made popular by Single Shot MultiBox Detector (SSD) and it was heavily adopted in several embedded applications. PointPillars is a state of the art 3D object detection algorithm that uses a Single Shot Detector adapted for 3D object detection. The main downside of PointPillars is that it has a two stage approach with learned input representation based on fully connected layers followed by the Single Shot Detector for 3D detection. In this paper we present Single Shot 3D Object Detection (SS3D) - a single stage 3D object detection algorithm which combines straight forward, statistically computed input representation and a Single Shot Detector (based on PointPillars). Computing the input representation is straight forward, does not involve learning and does not have much computational cost. We also extend our method to stereo input and show that, aided by additional semantic segmentation input; our method produces similar accuracy as state of the art stereo based detectors. Achieving the accuracy of two stage detectors using a single stage approach is important as single stage approaches are simpler to implement in embedded, real-time applications. With LiDAR as well as stereo input, our method outperforms PointPillars. When using LiDAR input, our input representation is able to improve the AP3D of Cars objects in the moderate category from 74.99 to 76.84. When using stereo input, our input representation is able to improve the AP3D of Cars objects in the moderate category from 38.13 to 45.13. Our results are also better than other popular 3D object detectors such as AVOD and F-PointNet.



### Multi-View Spectral Clustering Tailored Tensor Low-Rank Representation
- **Arxiv ID**: http://arxiv.org/abs/2004.14705v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.14705v2)
- **Published**: 2020-04-30 11:52:12+00:00
- **Updated**: 2020-08-01 13:35:01+00:00
- **Authors**: Yuheng Jia, Hui Liu, Junhui Hou, Sam Kwong, Qingfu Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: This paper explores the problem of multi-view spectral clustering (MVSC) based on tensor low-rank modeling. Unlike the existing methods that all adopt an off-the-shelf tensor low-rank norm without considering the special characteristics of the tensor in MVSC, we design a novel structured tensor low-rank norm tailored to MVSC. Specifically, we explicitly impose a symmetric low-rank constraint and a structured sparse low-rank constraint on the frontal and horizontal slices of the tensor to characterize the intra-view and inter-view relationships, respectively. Moreover, the two constraints could be jointly optimized to achieve mutual refinement. On the basis of the novel tensor low-rank norm, we formulate MVSC as a convex low-rank tensor recovery problem, which is then efficiently solved with an augmented Lagrange multiplier based method iteratively. Extensive experimental results on five benchmark datasets show that the proposed method outperforms state-of-the-art methods to a significant extent. Impressively, our method is able to produce perfect clustering. In addition, the parameters of our method can be easily tuned, and the proposed model is robust to different datasets, demonstrating its potential in practice.



### Inability of spatial transformations of CNN feature maps to support invariant recognition
- **Arxiv ID**: http://arxiv.org/abs/2004.14716v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.14716v1)
- **Published**: 2020-04-30 12:12:58+00:00
- **Updated**: 2020-04-30 12:12:58+00:00
- **Authors**: Ylva Jansson, Maksim Maydanskiy, Lukas Finnveden, Tony Lindeberg
- **Comment**: 22 pages, 3 figures
- **Journal**: None
- **Summary**: A large number of deep learning architectures use spatial transformations of CNN feature maps or filters to better deal with variability in object appearance caused by natural image transformations. In this paper, we prove that spatial transformations of CNN feature maps cannot align the feature maps of a transformed image to match those of its original, for general affine transformations, unless the extracted features are themselves invariant. Our proof is based on elementary analysis for both the single- and multi-layer network case. The results imply that methods based on spatial transformations of CNN feature maps or filters cannot replace image alignment of the input and cannot enable invariant recognition for general affine transformations, specifically not for scaling transformations or shear transformations. For rotations and reflections, spatially transforming feature maps or filters can enable invariance but only for networks with learnt or hardcoded rotation- or reflection-invariant features



### Pedestrian Path, Pose and Intention Prediction through Gaussian Process Dynamical Models and Pedestrian Activity Recognition
- **Arxiv ID**: http://arxiv.org/abs/2004.14747v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.14747v1)
- **Published**: 2020-04-30 13:08:46+00:00
- **Updated**: 2020-04-30 13:08:46+00:00
- **Authors**: Raul Quintero, Ignacio Parra, David Fernandez Llorca, Miguel Angel Sotelo
- **Comment**: 12 pages
- **Journal**: None
- **Summary**: According to several reports published by worldwide organisations, thousands of pedestrians die in road accidents every year. Due to this fact, vehicular technologies have been evolving with the intent of reducing these fatalities. This evolution has not finished yet since, for instance, the predictions of pedestrian paths could improve the current Automatic Emergency Braking Systems (AEBS). For this reason, this paper proposes a method to predict future pedestrian paths, poses and intentions up to 1s in advance. This method is based on Balanced Gaussian Process Dynamical Models (B-GPDMs), which reduce the 3D time-related information extracted from keypoints or joints placed along pedestrian bodies into low-dimensional spaces. The B-GPDM is also capable of inferring future latent positions and reconstruct their associated observations. However, learning a generic model for all kind of pedestrian activities normally provides less ccurate predictions. For this reason, the proposed method obtains multiple models of four types of activity, i.e. walking, stopping, starting and standing, and selects the most similar model to estimate future pedestrian states. This method detects starting activities 125ms after the gait initiation with an accuracy of 80% and recognises stopping intentions 58.33ms before the event with an accuracy of 70%. Concerning the path prediction, the mean error for stopping activities at a Time-To-Event (TTE) of 1s is 238.01mm and, for starting actions, the mean error at a TTE of 0s is 331.93mm.



### A Deep Convolutional Neural Network for COVID-19 Detection Using Chest X-Rays
- **Arxiv ID**: http://arxiv.org/abs/2005.01578v4
- **DOI**: 10.1007/s42600-021-00132-9
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.01578v4)
- **Published**: 2020-04-30 13:20:42+00:00
- **Updated**: 2021-01-13 04:08:10+00:00
- **Authors**: Pedro R. A. S. Bassi, Romis Attux
- **Comment**: None
- **Journal**: None
- **Summary**: Purpose: We present image classifiers based on Dense Convolutional Networks and transfer learning to classify chest X-ray images according to three labels: COVID-19, pneumonia and normal.   Methods: We fine-tuned neural networks pretrained on ImageNet and applied a twice transfer learning approach, using NIH ChestX-ray14 dataset as an intermediate step. We also suggested a novelty called output neuron keeping, which changes the twice transfer learning technique. In order to clarify the modus operandi of the models, we used Layer-wise Relevance Propagation (LRP) to generate heatmaps.   Results: We were able to reach test accuracy of 100% on our test dataset. Twice transfer learning and output neuron keeping showed promising results improving performances, mainly in the beginning of the training process. Although LRP revealed that words on the X-rays can influence the networks' predictions, we discovered this had only a very small effect on accuracy.   Conclusion: Although clinical studies and larger datasets are still needed to further ensure good generalization, the state-of-the-art performances we achieved show that, with the help of artificial intelligence, chest X-rays can become a cheap and accurate auxiliary method for COVID-19 diagnosis. Heatmaps generated by LRP improve the interpretability of the deep neural networks and indicate an analytical path for future research on diagnosis. Twice transfer learning with output neuron keeping improved performances.



### A Novel Perspective to Zero-shot Learning: Towards an Alignment of Manifold Structures via Semantic Feature Expansion
- **Arxiv ID**: http://arxiv.org/abs/2004.14795v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.14795v1)
- **Published**: 2020-04-30 14:08:10+00:00
- **Updated**: 2020-04-30 14:08:10+00:00
- **Authors**: Jingcai Guo, Song Guo
- **Comment**: None
- **Journal**: None
- **Summary**: Zero-shot learning aims at recognizing unseen classes (no training example) with knowledge transferred from seen classes. This is typically achieved by exploiting a semantic feature space shared by both seen and unseen classes, i.e., attribute or word vector, as the bridge. One common practice in zero-shot learning is to train a projection between the visual and semantic feature spaces with labeled seen classes examples. When inferring, this learned projection is applied to unseen classes and recognizes the class labels by some metrics. However, the visual and semantic feature spaces are mutually independent and have quite different manifold structures. Under such a paradigm, most existing methods easily suffer from the domain shift problem and weaken the performance of zero-shot recognition. To address this issue, we propose a novel model called AMS-SFE. It considers the alignment of manifold structures by semantic feature expansion. Specifically, we build upon an autoencoder-based model to expand the semantic features from the visual inputs. Additionally, the expansion is jointly guided by an embedded manifold extracted from the visual feature space of the data. Our model is the first attempt to align both feature spaces by expanding semantic features and derives two benefits: first, we expand some auxiliary features that enhance the semantic feature space; second and more importantly, we implicitly align the manifold structures between the visual and semantic feature spaces; thus, the projection can be better trained and mitigate the domain shift problem. Extensive experiments show significant performance improvement, which verifies the effectiveness of our model.



### Progressive Transformers for End-to-End Sign Language Production
- **Arxiv ID**: http://arxiv.org/abs/2004.14874v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.14874v2)
- **Published**: 2020-04-30 15:20:25+00:00
- **Updated**: 2020-07-20 10:20:03+00:00
- **Authors**: Ben Saunders, Necati Cihan Camgoz, Richard Bowden
- **Comment**: None
- **Journal**: None
- **Summary**: The goal of automatic Sign Language Production (SLP) is to translate spoken language to a continuous stream of sign language video at a level comparable to a human translator. If this was achievable, then it would revolutionise Deaf hearing communications. Previous work on predominantly isolated SLP has shown the need for architectures that are better suited to the continuous domain of full sign sequences.   In this paper, we propose Progressive Transformers, a novel architecture that can translate from discrete spoken language sentences to continuous 3D skeleton pose outputs representing sign language. We present two model configurations, an end-to-end network that produces sign direct from text and a stacked network that utilises a gloss intermediary.   Our transformer network architecture introduces a counter that enables continuous sequence generation at training and inference. We also provide several data augmentation processes to overcome the problem of drift and improve the performance of SLP models. We propose a back translation evaluation mechanism for SLP, presenting benchmark quantitative results on the challenging RWTH-PHOENIX-Weather-2014T(PHOENIX14T) dataset and setting baselines for future research.



### Polygonal Building Segmentation by Frame Field Learning
- **Arxiv ID**: http://arxiv.org/abs/2004.14875v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.14875v2)
- **Published**: 2020-04-30 15:21:56+00:00
- **Updated**: 2021-03-31 13:30:18+00:00
- **Authors**: Nicolas Girard, Dmitriy Smirnov, Justin Solomon, Yuliya Tarabalka
- **Comment**: CVPR 2021 - IEEE Conference on Computer Vision and Pattern
  Recognition, Jun 2021, Pittsburg / Virtual, United States
- **Journal**: None
- **Summary**: While state of the art image segmentation models typically output segmentations in raster format, applications in geographic information systems often require vector polygons. To help bridge the gap between deep network output and the format used in downstream tasks, we add a frame field output to a deep segmentation model for extracting buildings from remote sensing images. We train a deep neural network that aligns a predicted frame field to ground truth contours. This additional objective improves segmentation quality by leveraging multi-task learning and provides structural information that later facilitates polygonization; we also introduce a polygonization algorithm that utilizes the frame field along with the raster segmentation. Our code is available at https://github.com/Lydorn/Polygonization-by-Frame-Field-Learning.



### PreCNet: Next-Frame Video Prediction Based on Predictive Coding
- **Arxiv ID**: http://arxiv.org/abs/2004.14878v3
- **DOI**: 10.1109/TNNLS.2023.3240857
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2004.14878v3)
- **Published**: 2020-04-30 15:31:24+00:00
- **Updated**: 2023-02-08 11:50:42+00:00
- **Authors**: Zdenek Straka, Tomas Svoboda, Matej Hoffmann
- **Comment**: Accepted by IEEE Transactions on Neural Networks and Learning Systems
  (TNNLS)
- **Journal**: None
- **Summary**: Predictive coding, currently a highly influential theory in neuroscience, has not been widely adopted in machine learning yet. In this work, we transform the seminal model of Rao and Ballard (1999) into a modern deep learning framework while remaining maximally faithful to the original schema. The resulting network we propose (PreCNet) is tested on a widely used next frame video prediction benchmark, which consists of images from an urban environment recorded from a car-mounted camera, and achieves state-of-the-art performance. Performance on all measures (MSE, PSNR, SSIM) was further improved when a larger training set (2M images from BDD100k), pointing to the limitations of the KITTI training set. This work demonstrates that an architecture carefully based in a neuroscience model, without being explicitly tailored to the task at hand, can exhibit exceptional performance.



### MuSe 2020 -- The First International Multimodal Sentiment Analysis in Real-life Media Challenge and Workshop
- **Arxiv ID**: http://arxiv.org/abs/2004.14858v3
- **DOI**: None
- **Categories**: **cs.MM**, cs.CL, cs.CV, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2004.14858v3)
- **Published**: 2020-04-30 15:54:22+00:00
- **Updated**: 2020-07-09 08:37:43+00:00
- **Authors**: Lukas Stappen, Alice Baird, Georgios Rizos, Panagiotis Tzirakis, Xinchen Du, Felix Hafner, Lea Schumann, Adria Mallol-Ragolta, Björn W. Schuller, Iulia Lefter, Erik Cambria, Ioannis Kompatsiaris
- **Comment**: Baseline Paper MuSe 2020, MuSe Workshop Challenge, ACM Multimedia
- **Journal**: None
- **Summary**: Multimodal Sentiment Analysis in Real-life Media (MuSe) 2020 is a Challenge-based Workshop focusing on the tasks of sentiment recognition, as well as emotion-target engagement and trustworthiness detection by means of more comprehensively integrating the audio-visual and language modalities. The purpose of MuSe 2020 is to bring together communities from different disciplines; mainly, the audio-visual emotion recognition community (signal-based), and the sentiment analysis community (symbol-based). We present three distinct sub-challenges: MuSe-Wild, which focuses on continuous emotion (arousal and valence) prediction; MuSe-Topic, in which participants recognise domain-specific topics as the target of 3-class (low, medium, high) emotions; and MuSe-Trust, in which the novel aspect of trustworthiness is to be predicted. In this paper, we provide detailed information on MuSe-CaR, the first of its kind in-the-wild database, which is utilised for the challenge, as well as the state-of-the-art features and modelling approaches applied. For each sub-challenge, a competitive baseline for participants is set; namely, on test we report for MuSe-Wild a combined (valence and arousal) CCC of .2568, for MuSe-Topic a score (computed as 0.34$\cdot$ UAR + 0.66$\cdot$F1) of 76.78 % on the 10-class topic and 40.64 % on the 3-class emotion prediction, and for MuSe-Trust a CCC of .4359.



### Polarization Human Shape and Pose Dataset
- **Arxiv ID**: http://arxiv.org/abs/2004.14899v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.14899v2)
- **Published**: 2020-04-30 15:58:21+00:00
- **Updated**: 2020-07-29 23:48:40+00:00
- **Authors**: Shihao Zou, Xinxin Zuo, Yiming Qian, Sen Wang, Chuan Guo, Chi Xu, Minglun Gong, Li Cheng
- **Comment**: None
- **Journal**: None
- **Summary**: Polarization images are known to be able to capture polarized reflected lights that preserve rich geometric cues of an object, which has motivated its recent applications in reconstructing detailed surface normal of the objects of interest. Meanwhile, inspired by the recent breakthroughs in human shape estimation from a single color image, we attempt to investigate the new question of whether the geometric cues from polarization camera could be leveraged in estimating detailed human body shapes. This has led to the curation of Polarization Human Shape and Pose Dataset (PHSPD), our home-grown polarization image dataset of various human shapes and poses.



### Survey on Reliable Deep Learning-Based Person Re-Identification Models: Are We There Yet?
- **Arxiv ID**: http://arxiv.org/abs/2005.00355v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.00355v1)
- **Published**: 2020-04-30 16:09:16+00:00
- **Updated**: 2020-04-30 16:09:16+00:00
- **Authors**: Bahram Lavi, Ihsan Ullah, Mehdi Fatan, Anderson Rocha
- **Comment**: 24 pages, 6 figures, and 2 tables, considered over than 100 papers.
  arXiv admin note: substantial text overlap with arXiv:1807.05284
- **Journal**: None
- **Summary**: Intelligent video-surveillance (IVS) is currently an active research field in computer vision and machine learning and provides useful tools for surveillance operators and forensic video investigators. Person re-identification (PReID) is one of the most critical problems in IVS, and it consists of recognizing whether or not an individual has already been observed over a camera in a network. Solutions to PReID have myriad applications including retrieval of video-sequences showing an individual of interest or even pedestrian tracking over multiple camera views. Different techniques have been proposed to increase the performance of PReID in the literature, and more recently researchers utilized deep neural networks (DNNs) given their compelling performance on similar vision problems and fast execution at test time. Given the importance and wide range of applications of re-identification solutions, our objective herein is to discuss the work carried out in the area and come up with a survey of state-of-the-art DNN models being used for this task. We present descriptions of each model along with their evaluation on a set of benchmark datasets. Finally, we show a detailed comparison among these models, which are followed by some discussions on their limitations that can work as guidelines for future research.



### Generative Adversarial Networks in Digital Pathology: A Survey on Trends and Future Potential
- **Arxiv ID**: http://arxiv.org/abs/2004.14936v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.14936v2)
- **Published**: 2020-04-30 16:38:06+00:00
- **Updated**: 2020-05-07 06:05:07+00:00
- **Authors**: Maximilian Ernst Tschuchnig, Gertie Janneke Oostingh, Michael Gadermayr
- **Comment**: None
- **Journal**: None
- **Summary**: Image analysis in the field of digital pathology has recently gained increased popularity. The use of high-quality whole slide scanners enables the fast acquisition of large amounts of image data, showing extensive context and microscopic detail at the same time. Simultaneously, novel machine learning algorithms have boosted the performance of image analysis approaches. In this paper, we focus on a particularly powerful class of architectures, called Generative Adversarial Networks (GANs), applied to histological image data. Besides improving performance, GANs also enable application scenarios in this field, which were previously intractable. However, GANs could exhibit a potential for introducing bias. Hereby, we summarize the recent state-of-the-art developments in a generalizing notation, present the main applications of GANs and give an outlook of some chosen promising approaches and their possible future applications. In addition, we identify currently unavailable methods with potential for future applications.



### Improving Semantic Segmentation via Self-Training
- **Arxiv ID**: http://arxiv.org/abs/2004.14960v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.14960v2)
- **Published**: 2020-04-30 17:09:17+00:00
- **Updated**: 2020-05-06 16:57:02+00:00
- **Authors**: Yi Zhu, Zhongyue Zhang, Chongruo Wu, Zhi Zhang, Tong He, Hang Zhang, R. Manmatha, Mu Li, Alexander Smola
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning usually achieves the best results with complete supervision. In the case of semantic segmentation, this means that large amounts of pixelwise annotations are required to learn accurate models. In this paper, we show that we can obtain state-of-the-art results using a semi-supervised approach, specifically a self-training paradigm. We first train a teacher model on labeled data, and then generate pseudo labels on a large set of unlabeled data. Our robust training framework can digest human-annotated and pseudo labels jointly and achieve top performances on Cityscapes, CamVid and KITTI datasets while requiring significantly less supervision. We also demonstrate the effectiveness of self-training on a challenging cross-domain generalization task, outperforming conventional finetuning method by a large margin. Lastly, to alleviate the computational burden caused by the large amount of pseudo labels, we propose a fast training schedule to accelerate the training of segmentation models by up to 2x without performance degradation.



### Improving Vision-and-Language Navigation with Image-Text Pairs from the Web
- **Arxiv ID**: http://arxiv.org/abs/2004.14973v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.14973v2)
- **Published**: 2020-04-30 17:22:40+00:00
- **Updated**: 2020-05-01 17:16:50+00:00
- **Authors**: Arjun Majumdar, Ayush Shrivastava, Stefan Lee, Peter Anderson, Devi Parikh, Dhruv Batra
- **Comment**: None
- **Journal**: None
- **Summary**: Following a navigation instruction such as 'Walk down the stairs and stop at the brown sofa' requires embodied AI agents to ground scene elements referenced via language (e.g. 'stairs') to visual content in the environment (pixels corresponding to 'stairs').   We ask the following question -- can we leverage abundant 'disembodied' web-scraped vision-and-language corpora (e.g. Conceptual Captions) to learn visual groundings (what do 'stairs' look like?) that improve performance on a relatively data-starved embodied perception task (Vision-and-Language Navigation)? Specifically, we develop VLN-BERT, a visiolinguistic transformer-based model for scoring the compatibility between an instruction ('...stop at the brown sofa') and a sequence of panoramic RGB images captured by the agent. We demonstrate that pretraining VLN-BERT on image-text pairs from the web before fine-tuning on embodied path-instruction data significantly improves performance on VLN -- outperforming the prior state-of-the-art in the fully-observed setting by 4 absolute percentage points on success rate. Ablations of our pretraining curriculum show each stage to be impactful -- with their combination resulting in further positive synergistic effects.



### CNN Explainer: Learning Convolutional Neural Networks with Interactive Visualization
- **Arxiv ID**: http://arxiv.org/abs/2004.15004v3
- **DOI**: 10.1109/TVCG.2020.3030418
- **Categories**: **cs.HC**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.15004v3)
- **Published**: 2020-04-30 17:49:44+00:00
- **Updated**: 2020-08-28 18:42:23+00:00
- **Authors**: Zijie J. Wang, Robert Turko, Omar Shaikh, Haekyu Park, Nilaksh Das, Fred Hohman, Minsuk Kahng, Duen Horng Chau
- **Comment**: 11 pages, 14 figures, to be presented at IEEE VIS 2020. For a demo
  video, see https://youtu.be/HnWIHWFbuUQ . For a live demo, visit
  https://poloclub.github.io/cnn-explainer/
- **Journal**: None
- **Summary**: Deep learning's great success motivates many practitioners and students to learn about this exciting technology. However, it is often challenging for beginners to take their first step due to the complexity of understanding and applying deep learning. We present CNN Explainer, an interactive visualization tool designed for non-experts to learn and examine convolutional neural networks (CNNs), a foundational deep learning model architecture. Our tool addresses key challenges that novices face while learning about CNNs, which we identify from interviews with instructors and a survey with past students. CNN Explainer tightly integrates a model overview that summarizes a CNN's structure, and on-demand, dynamic visual explanation views that help users understand the underlying components of CNNs. Through smooth transitions across levels of abstraction, our tool enables users to inspect the interplay between low-level mathematical operations and high-level model structures. A qualitative user study shows that CNN Explainer helps users more easily understand the inner workings of CNNs, and is engaging and enjoyable to use. We also derive design lessons from our study. Developed using modern web technologies, CNN Explainer runs locally in users' web browsers without the need for installation or specialized hardware, broadening the public's education access to modern deep learning techniques.



### SimPropNet: Improved Similarity Propagation for Few-shot Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2004.15014v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.15014v2)
- **Published**: 2020-04-30 17:56:48+00:00
- **Updated**: 2020-05-02 08:00:15+00:00
- **Authors**: Siddhartha Gairola, Mayur Hemani, Ayush Chopra, Balaji Krishnamurthy
- **Comment**: An updated version of this work was accepted at IJCAI 2020
- **Journal**: None
- **Summary**: Few-shot segmentation (FSS) methods perform image segmentation for a particular object class in a target (query) image, using a small set of (support) image-mask pairs. Recent deep neural network based FSS methods leverage high-dimensional feature similarity between the foreground features of the support images and the query image features. In this work, we demonstrate gaps in the utilization of this similarity information in existing methods, and present a framework - SimPropNet, to bridge those gaps. We propose to jointly predict the support and query masks to force the support features to share characteristics with the query features. We also propose to utilize similarities in the background regions of the query and support images using a novel foreground-background attentive fusion mechanism. Our method achieves state-of-the-art results for one-shot and five-shot segmentation on the PASCAL-5i dataset. The paper includes detailed analysis and ablation studies for the proposed improvements and quantitative comparisons with contemporary methods.



### Consistent Video Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/2004.15021v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.15021v2)
- **Published**: 2020-04-30 17:59:26+00:00
- **Updated**: 2020-08-26 20:11:33+00:00
- **Authors**: Xuan Luo, Jia-Bin Huang, Richard Szeliski, Kevin Matzen, Johannes Kopf
- **Comment**: SIGGRAPH 2020. Video: https://www.youtube.com/watch?v=5Tia2oblJAg
  Project page: https://roxanneluo.github.io/Consistent-Video-Depth-Estimation/
  Code: https://github.com/facebookresearch/consistent_depth
- **Journal**: None
- **Summary**: We present an algorithm for reconstructing dense, geometrically consistent depth for all pixels in a monocular video. We leverage a conventional structure-from-motion reconstruction to establish geometric constraints on pixels in the video. Unlike the ad-hoc priors in classical reconstruction, we use a learning-based prior, i.e., a convolutional neural network trained for single-image depth estimation. At test time, we fine-tune this network to satisfy the geometric constraints of a particular input video, while retaining its ability to synthesize plausible depth details in parts of the video that are less constrained. We show through quantitative validation that our method achieves higher accuracy and a higher degree of geometric consistency than previous monocular reconstruction methods. Visually, our results appear more stable. Our algorithm is able to handle challenging hand-held captured input videos with a moderate degree of dynamic motion. The improved quality of the reconstruction enables several applications, such as scene reconstruction and advanced video-based visual effects.



### Unsupervised Lesion Detection via Image Restoration with a Normative Prior
- **Arxiv ID**: http://arxiv.org/abs/2005.00031v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.00031v1)
- **Published**: 2020-04-30 18:03:18+00:00
- **Updated**: 2020-04-30 18:03:18+00:00
- **Authors**: Xiaoran Chen, Suhang You, Kerem Can Tezcan, Ender Konukoglu
- **Comment**: Extended version of 'Unsupervised Lesion Detection via Image
  Restoration with a Normative Prior' (MIDL2019)
- **Journal**: None
- **Summary**: Unsupervised lesion detection is a challenging problem that requires accurately estimating normative distributions of healthy anatomy and detecting lesions as outliers without training examples. Recently, this problem has received increased attention from the research community following the advances in unsupervised learning with deep learning. Such advances allow the estimation of high-dimensional distributions, such as normative distributions, with higher accuracy than previous methods.The main approach of the recently proposed methods is to learn a latent-variable model parameterized with networks to approximate the normative distribution using example images showing healthy anatomy, perform prior-projection, i.e. reconstruct the image with lesions using the latent-variable model, and determine lesions based on the differences between the reconstructed and original images. While being promising, the prior-projection step often leads to a large number of false positives. In this work, we approach unsupervised lesion detection as an image restoration problem and propose a probabilistic model that uses a network-based prior as the normative distribution and detect lesions pixel-wise using MAP estimation. The probabilistic model punishes large deviations between restored and original images, reducing false positives in pixel-wise detections. Experiments with gliomas and stroke lesions in brain MRI using publicly available datasets show that the proposed approach outperforms the state-of-the-art unsupervised methods by a substantial margin, +0.13 (AUC), for both glioma and stroke detection. Extensive model analysis confirms the effectiveness of MAP-based image restoration.



### CP-NAS: Child-Parent Neural Architecture Search for Binary Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2005.00057v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.00057v2)
- **Published**: 2020-04-30 19:09:55+00:00
- **Updated**: 2020-05-17 15:38:02+00:00
- **Authors**: Li'an Zhuo, Baochang Zhang, Hanlin Chen, Linlin Yang, Chen Chen, Yanjun Zhu, David Doermann
- **Comment**: 7 pages, 6 figures
- **Journal**: None
- **Summary**: Neural architecture search (NAS) proves to be among the best approaches for many tasks by generating an application-adaptive neural architecture, which is still challenged by high computational cost and memory consumption. At the same time, 1-bit convolutional neural networks (CNNs) with binarized weights and activations show their potential for resource-limited embedded devices. One natural approach is to use 1-bit CNNs to reduce the computation and memory cost of NAS by taking advantage of the strengths of each in a unified framework. To this end, a Child-Parent (CP) model is introduced to a differentiable NAS to search the binarized architecture (Child) under the supervision of a full-precision model (Parent). In the search stage, the Child-Parent model uses an indicator generated by the child and parent model accuracy to evaluate the performance and abandon operations with less potential. In the training stage, a kernel-level CP loss is introduced to optimize the binarized network. Extensive experiments demonstrate that the proposed CP-NAS achieves a comparable accuracy with traditional NAS on both the CIFAR and ImageNet databases. It achieves the accuracy of $95.27\%$ on CIFAR-10, $64.3\%$ on ImageNet with binarized weights and activations, and a $30\%$ faster search than prior arts.



### Bridging Mode Connectivity in Loss Landscapes and Adversarial Robustness
- **Arxiv ID**: http://arxiv.org/abs/2005.00060v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2005.00060v2)
- **Published**: 2020-04-30 19:12:50+00:00
- **Updated**: 2020-07-03 03:49:28+00:00
- **Authors**: Pu Zhao, Pin-Yu Chen, Payel Das, Karthikeyan Natesan Ramamurthy, Xue Lin
- **Comment**: accepted by ICLR 2020
- **Journal**: None
- **Summary**: Mode connectivity provides novel geometric insights on analyzing loss landscapes and enables building high-accuracy pathways between well-trained neural networks. In this work, we propose to employ mode connectivity in loss landscapes to study the adversarial robustness of deep neural networks, and provide novel methods for improving this robustness. Our experiments cover various types of adversarial attacks applied to different network architectures and datasets. When network models are tampered with backdoor or error-injection attacks, our results demonstrate that the path connection learned using limited amount of bonafide data can effectively mitigate adversarial effects while maintaining the original accuracy on clean data. Therefore, mode connectivity provides users with the power to repair backdoored or error-injected models. We also use mode connectivity to investigate the loss landscapes of regular and robust models against evasion attacks. Experiments show that there exists a barrier in adversarial robustness loss on the path connecting regular and adversarially-trained models. A high correlation is observed between the adversarial robustness loss and the largest eigenvalue of the input Hessian matrix, for which theoretical justifications are provided. Our results suggest that mode connectivity offers a holistic tool and practical means for evaluating and improving adversarial robustness.



### Occlusion resistant learning of intuitive physics from videos
- **Arxiv ID**: http://arxiv.org/abs/2005.00069v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2005.00069v1)
- **Published**: 2020-04-30 19:35:54+00:00
- **Updated**: 2020-04-30 19:35:54+00:00
- **Authors**: Ronan Riochet, Josef Sivic, Ivan Laptev, Emmanuel Dupoux
- **Comment**: None
- **Journal**: None
- **Summary**: To reach human performance on complex tasks, a key ability for artificial systems is to understand physical interactions between objects, and predict future outcomes of a situation. This ability, often referred to as intuitive physics, has recently received attention and several methods were proposed to learn these physical rules from video sequences. Yet, most of these methods are restricted to the case where no, or only limited, occlusions occur. In this work we propose a probabilistic formulation of learning intuitive physics in 3D scenes with significant inter-object occlusions. In our formulation, object positions are modeled as latent variables enabling the reconstruction of the scene. We then propose a series of approximations that make this problem tractable. Object proposals are linked across frames using a combination of a recurrent interaction network, modeling the physics in object space, and a compositional renderer, modeling the way in which objects project onto pixel space. We demonstrate significant improvements over state-of-the-art in the intuitive physics benchmark of IntPhys. We apply our method to a second dataset with increasing levels of occlusions, showing it realistically predicts segmentation masks up to 30 frames in the future. Finally, we also show results on predicting motion of objects in real videos.



### Importance Driven Continual Learning for Segmentation Across Domains
- **Arxiv ID**: http://arxiv.org/abs/2005.00079v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.00079v1)
- **Published**: 2020-04-30 19:58:18+00:00
- **Updated**: 2020-04-30 19:58:18+00:00
- **Authors**: Sinan Özgür Özgün, Anne-Marie Rickmann, Abhijit Guha Roy, Christian Wachinger
- **Comment**: None
- **Journal**: None
- **Summary**: The ability of neural networks to continuously learn and adapt to new tasks while retaining prior knowledge is crucial for many applications. However, current neural networks tend to forget previously learned tasks when trained on new ones, i.e., they suffer from Catastrophic Forgetting (CF). The objective of Continual Learning (CL) is to alleviate this problem, which is particularly relevant for medical applications, where it may not be feasible to store and access previously used sensitive patient data. In this work, we propose a Continual Learning approach for brain segmentation, where a single network is consecutively trained on samples from different domains. We build upon an importance driven approach and adapt it for medical image segmentation. Particularly, we introduce learning rate regularization to prevent the loss of the network's knowledge. Our results demonstrate that directly restricting the adaptation of important network parameters clearly reduces Catastrophic Forgetting for segmentation across domains.



### Domain Siamese CNNs for Sparse Multispectral Disparity Estimation
- **Arxiv ID**: http://arxiv.org/abs/2005.00088v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2005.00088v1)
- **Published**: 2020-04-30 20:29:59+00:00
- **Updated**: 2020-04-30 20:29:59+00:00
- **Authors**: David-Alexandre Beaupre, Guillaume-Alexandre Bilodeau
- **Comment**: None
- **Journal**: None
- **Summary**: Multispectral disparity estimation is a difficult task for many reasons: it has all the same challenges as traditional visible-visible disparity estimation (occlusions, repetitive patterns, textureless surfaces), in addition of having very few common visual information between images (e.g. color information vs. thermal information). In this paper, we propose a new CNN architecture able to do disparity estimation between images from different spectrum, namely thermal and visible in our case. Our proposed model takes two patches as input and proceeds to do domain feature extraction for each of them. Features from both domains are then merged with two fusion operations, namely correlation and concatenation. These merged vectors are then forwarded to their respective classification heads, which are responsible for classifying the inputs as being same or not. Using two merging operations gives more robustness to our feature extraction process, which leads to more precise disparity estimation. Our method was tested using the publicly available LITIV 2014 and LITIV 2018 datasets, and showed best results when compared to other state of the art methods.



### Intra-model Variability in COVID-19 Classification Using Chest X-ray Images
- **Arxiv ID**: http://arxiv.org/abs/2005.02167v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, 68T01, I.2.0
- **Links**: [PDF](http://arxiv.org/pdf/2005.02167v1)
- **Published**: 2020-04-30 21:20:32+00:00
- **Updated**: 2020-04-30 21:20:32+00:00
- **Authors**: Brian D Goodwin, Corey Jaskolski, Can Zhong, Herick Asmani
- **Comment**: 7 pages, 5 figures; Writing, analysis, and design carried out by
  authors Brian and Corey; experiments carried out by authors Can and Herick;
  results and code located at
  https://github.com/synthetaic/COVID19-IntraModel-Variability and
  https://covidresearch.ai/datasets/dataset?id=2
- **Journal**: None
- **Summary**: X-ray and computed tomography (CT) scanning technologies for COVID-19 screening have gained significant traction in AI research since the start of the coronavirus pandemic. Despite these continuous advancements for COVID-19 screening, many concerns remain about model reliability when used in a clinical setting. Much has been published, but with limited transparency in expected model performance. We set out to address this limitation through a set of experiments to quantify baseline performance metrics and variability for COVID-19 detection in chest x-ray for 12 common deep learning architectures. Specifically, we adopted an experimental paradigm controlling for train-validation-test split and model architecture where the source of prediction variability originates from model weight initialization, random data augmentation transformations, and batch shuffling. Each model architecture was trained 5 separate times on identical train-validation-test splits of a publicly available x-ray image dataset provided by Cohen et al. (2020). Results indicate that even within model architectures, model behavior varies in a meaningful way between trained models. Best performing models achieve a false negative rate of 3 out of 20 for detecting COVID-19 in a hold-out set. While these results show promise in using AI for COVID-19 screening, they further support the urgent need for diverse medical imaging datasets for model training in a way that yields consistent prediction outcomes. It is our hope that these modeling results accelerate work in building a more robust dataset and a viable screening tool for COVID-19.



### Sequence Information Channel Concatenation for Improving Camera Trap Image Burst Classification
- **Arxiv ID**: http://arxiv.org/abs/2005.00116v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, I.4.9; I.4.10; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2005.00116v2)
- **Published**: 2020-04-30 21:47:14+00:00
- **Updated**: 2020-06-06 02:57:41+00:00
- **Authors**: Bhuvan Malladihalli Shashidhara, Darshan Mehta, Yash Kale, Dan Morris, Megan Hazen
- **Comment**: 8 pages, 4 figures, 2 tables. Git repository can be found at:
  https://github.com/bhuvi3/camera_trap_animal_classification
- **Journal**: None
- **Summary**: Camera Traps are extensively used to observe wildlife in their natural habitat without disturbing the ecosystem. This could help in the early detection of natural or human threats to animals, and help towards ecological conservation. Currently, a massive number of such camera traps have been deployed at various ecological conservation areas around the world, collecting data for decades, thereby requiring automation to detect images containing animals. Existing systems perform classification to detect if images contain animals by considering a single image. However, due to challenging scenes with animals camouflaged in their natural habitat, it sometimes becomes difficult to identify the presence of animals from merely a single image. We hypothesize that a short burst of images instead of a single image, assuming that the animal moves, makes it much easier for a human as well as a machine to detect the presence of animals. In this work, we explore a variety of approaches, and measure the impact of using short image sequences (burst of 3 images) on improving the camera trap image classification. We show that concatenating masks containing sequence information and the images from the 3-image-burst across channels, improves the ROC AUC by 20% on a test-set from unseen camera-sites, as compared to an equivalent model that learns from a single image.



### Suspicious Behavior Detection on Shoplifting Cases for Crime Prevention by Using 3D Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2005.02142v1
- **DOI**: 10.3390/computation9020024
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2005.02142v1)
- **Published**: 2020-04-30 22:06:16+00:00
- **Updated**: 2020-04-30 22:06:16+00:00
- **Authors**: Guillermo A. Martínez-Mascorro, José R. Abreu-Pederzini, José C. Ortiz-Bayliss, Hugo Terashima-Marín
- **Comment**: None
- **Journal**: Computation 2021, 9(2), 24
- **Summary**: Crime generates significant losses, both human and economic. Every year, billions of dollars are lost due to attacks, crimes, and scams. Surveillance video camera networks are generating vast amounts of data, and the surveillance staff can not process all the information in real-time. The human sight has its limitations, where the visual focus is among the most critical ones when dealing with surveillance. A crime can occur in a different screen segment or on a distinct monitor, and the staff may not notice it. Our proposal focuses on shoplifting crimes by analyzing special situations that an average person will consider as typical conditions, but may lead to a crime. While other approaches identify the crime itself, we instead model suspicious behavior -- the one that may occur before a person commits a crime -- by detecting precise segments of a video with a high probability to contain a shoplifting crime. By doing so, we provide the staff with more opportunities to act and prevent crime. We implemented a 3DCNN model as a video feature extractor and tested its performance on a dataset composed of daily-action and shoplifting samples. The results are encouraging since it correctly identifies 75% of the cases where a crime is about to happen.



### Conceptual Design of Human-Drone Communication in Collaborative Environments
- **Arxiv ID**: http://arxiv.org/abs/2005.00127v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2005.00127v1)
- **Published**: 2020-04-30 22:20:56+00:00
- **Updated**: 2020-04-30 22:20:56+00:00
- **Authors**: Hans Dermot Doran, Monika Reif, Marco Oehler, Curdin Stoehr, Pierluigi Capone
- **Comment**: 4 pages, 4 figures
- **Journal**: None
- **Summary**: Autonomous robots and drones will work collaboratively and cooperatively in tomorrow's industry and agriculture. Before this becomes a reality, some form of standardised communication between man and machine must be established that specifically facilitates communication between autonomous machines and both trained and untrained human actors in the working environment. We present preliminary results on a human-drone and a drone-human language situated in the agricultural industry where interactions with trained and untrained workers and visitors can be expected. We present basic visual indicators enhanced with flight patterns for drone-human interaction and human signaling based on aircraft marshaling for humane-drone interaction. We discuss preliminary results on image recognition and future work.



