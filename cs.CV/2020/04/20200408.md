# Arxiv Papers in cs.CV on 2020-04-08
### Understanding Knowledge Gaps in Visual Question Answering: Implications for Gap Identification and Testing
- **Arxiv ID**: http://arxiv.org/abs/2004.03755v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.03755v2)
- **Published**: 2020-04-08 00:27:43+00:00
- **Updated**: 2020-06-03 21:53:59+00:00
- **Authors**: Goonmeet Bajaj, Bortik Bandyopadhyay, Daniel Schmidt, Pranav Maneriker, Christopher Myers, Srinivasan Parthasarathy
- **Comment**: None
- **Journal**: None
- **Summary**: Visual Question Answering (VQA) systems are tasked with answering natural language questions corresponding to a presented image. Traditional VQA datasets typically contain questions related to the spatial information of objects, object attributes, or general scene questions. Recently, researchers have recognized the need to improve the balance of such datasets to reduce the system's dependency on memorized linguistic features and statistical biases, while aiming for enhanced visual understanding. However, it is unclear whether any latent patterns exist to quantify and explain these failures. As an initial step towards better quantifying our understanding of the performance of VQA models, we use a taxonomy of Knowledge Gaps (KGs) to tag questions with one or more types of KGs. Each Knowledge Gap (KG) describes the reasoning abilities needed to arrive at a resolution. After identifying KGs for each question, we examine the skew in the distribution of questions for each KG. We then introduce a targeted question generation model to reduce this skew, which allows us to generate new types of questions for an image. These new questions can be added to existing VQA datasets to increase the diversity of questions and reduce the skew.



### DashCam Pay: A System for In-vehicle Payments Using Face and Voice
- **Arxiv ID**: http://arxiv.org/abs/2004.03756v2
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.03756v2)
- **Published**: 2020-04-08 00:28:33+00:00
- **Updated**: 2020-09-08 22:28:41+00:00
- **Authors**: Cori Tymoszek, Sunpreet S. Arora, Kim Wagner, Anil K. Jain
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: We present our ongoing work on developing a system, called DashCam Pay, that enables in-vehicle payments in a seamless and secure manner using face and voice biometrics. A plug-and-play device (dashcam) mounted in the vehicle is used to capture face images and voice commands of passengers. Privacy-preserving biometric comparison techniques are used to compare the biometric data captured by the dashcam with the biometric data enrolled on the users' mobile devices over a wireless interface (e.g., Bluetooth or Wi-Fi Direct) to determine the payer. Once the payer is identified, payment is conducted using the enrolled payment credential on the mobile device of the payer. We conduct preliminary analysis on data collected using a commercially available dashcam to show the feasibility of building the proposed system. A prototype of the proposed system is also developed in Android. DashCam Pay can be integrated as a software solution by dashcam or vehicle manufacturers to enable open loop in-vehicle payments.



### Facial Expression Recognition with Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2004.11823v1
- **DOI**: None
- **Categories**: **cs.CV**, I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2004.11823v1)
- **Published**: 2020-04-08 03:12:49+00:00
- **Updated**: 2020-04-08 03:12:49+00:00
- **Authors**: Amil Khanzada, Charles Bai, Ferhat Turker Celepcikay
- **Comment**: 6 pages
- **Journal**: None
- **Summary**: One of the most universal ways that people communicate is through facial expressions. In this paper, we take a deep dive, implementing multiple deep learning models for facial expression recognition (FER). Our goals are twofold: we aim not only to maximize accuracy, but also to apply our results to the real-world. By leveraging numerous techniques from recent research, we demonstrate a state-of-the-art 75.8% accuracy on the FER2013 test set, outperforming all existing publications. Additionally, we showcase a mobile web app which runs our FER models on-device in real time.



### Learning A Single Network for Scale-Arbitrary Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2004.03791v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.03791v2)
- **Published**: 2020-04-08 03:40:15+00:00
- **Updated**: 2021-07-23 14:01:58+00:00
- **Authors**: Longguang Wang, Yingqian Wang, Zaiping Lin, Jungang Yang, Wei An, Yulan Guo
- **Comment**: Accepted by ICCV 2021
- **Journal**: None
- **Summary**: Recently, the performance of single image super-resolution (SR) has been significantly improved with powerful networks. However, these networks are developed for image SR with a single specific integer scale (e.g., x2;x3,x4), and cannot be used for non-integer and asymmetric SR. In this paper, we propose to learn a scale-arbitrary image SR network from scale-specific networks. Specifically, we propose a plug-in module for existing SR networks to perform scale-arbitrary SR, which consists of multiple scale-aware feature adaption blocks and a scale-aware upsampling layer. Moreover, we introduce a scale-aware knowledge transfer paradigm to transfer knowledge from scale-specific networks to the scale-arbitrary network. Our plug-in module can be easily adapted to existing networks to achieve scale-arbitrary SR. These networks plugged with our module can achieve promising results for non-integer and asymmetric SR while maintaining state-of-the-art performance for SR with integer scale factors. Besides, the additional computational and memory cost of our module is very small.



### DMLO: Deep Matching LiDAR Odometry
- **Arxiv ID**: http://arxiv.org/abs/2004.03796v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2004.03796v2)
- **Published**: 2020-04-08 03:52:49+00:00
- **Updated**: 2020-04-09 02:25:38+00:00
- **Authors**: Zhichao Li, Naiyan Wang
- **Comment**: None
- **Journal**: None
- **Summary**: LiDAR odometry is a fundamental task for various areas such as robotics, autonomous driving. This problem is difficult since it requires the systems to be highly robust running in noisy real-world data. Existing methods are mostly local iterative methods. Feature-based global registration methods are not preferred since extracting accurate matching pairs in the nonuniform and sparse LiDAR data remains challenging. In this paper, we present Deep Matching LiDAR Odometry (DMLO), a novel learning-based framework which makes the feature matching method applicable to LiDAR odometry task. Unlike many recent learning-based methods, DMLO explicitly enforces geometry constraints in the framework. Specifically, DMLO decomposes the 6-DoF pose estimation into two parts, a learning-based matching network which provides accurate correspondences between two scans and rigid transformation estimation with a close-formed solution by Singular Value Decomposition (SVD). Comprehensive experimental results on real-world datasets KITTI and Argoverse demonstrate that our DMLO dramatically outperforms existing learning-based methods and comparable with the state-of-the-art geometry based approaches.



### HybridDNN: A Framework for High-Performance Hybrid DNN Accelerator Design and Implementation
- **Arxiv ID**: http://arxiv.org/abs/2004.03804v1
- **DOI**: None
- **Categories**: **cs.AR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.03804v1)
- **Published**: 2020-04-08 04:28:38+00:00
- **Updated**: 2020-04-08 04:28:38+00:00
- **Authors**: Hanchen Ye, Xiaofan Zhang, Zhize Huang, Gengsheng Chen, Deming Chen
- **Comment**: Published as a conference paper at Design Automation Conference 2020
  (DAC'20)
- **Journal**: None
- **Summary**: To speedup Deep Neural Networks (DNN) accelerator design and enable effective implementation, we propose HybridDNN, a framework for building high-performance hybrid DNN accelerators and delivering FPGA-based hardware implementations. Novel techniques include a highly flexible and scalable architecture with a hybrid Spatial/Winograd convolution (CONV) Processing Engine (PE), a comprehensive design space exploration tool, and a complete design flow to fully support accelerator design and implementation. Experimental results show that the accelerators generated by HybridDNN can deliver 3375.7 and 83.3 GOPS on a high-end FPGA (VU9P) and an embedded FPGA (PYNQ-Z1), respectively, which achieve a 1.8x higher performance improvement compared to the state-of-art accelerator designs. This demonstrates that HybridDNN is flexible and scalable and can target both cloud and embedded hardware platforms with vastly different resource constraints.



### State of the Art on Neural Rendering
- **Arxiv ID**: http://arxiv.org/abs/2004.03805v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2004.03805v1)
- **Published**: 2020-04-08 04:36:31+00:00
- **Updated**: 2020-04-08 04:36:31+00:00
- **Authors**: Ayush Tewari, Ohad Fried, Justus Thies, Vincent Sitzmann, Stephen Lombardi, Kalyan Sunkavalli, Ricardo Martin-Brualla, Tomas Simon, Jason Saragih, Matthias Nießner, Rohit Pandey, Sean Fanello, Gordon Wetzstein, Jun-Yan Zhu, Christian Theobalt, Maneesh Agrawala, Eli Shechtman, Dan B Goldman, Michael Zollhöfer
- **Comment**: Eurographics 2020 survey paper
- **Journal**: None
- **Summary**: Efficient rendering of photo-realistic virtual worlds is a long standing effort of computer graphics. Modern graphics techniques have succeeded in synthesizing photo-realistic images from hand-crafted scene representations. However, the automatic generation of shape, materials, lighting, and other aspects of scenes remains a challenging problem that, if solved, would make photo-realistic computer graphics more widely accessible. Concurrently, progress in computer vision and machine learning have given rise to a new approach to image synthesis and editing, namely deep generative models. Neural rendering is a new and rapidly emerging field that combines generative machine learning techniques with physical knowledge from computer graphics, e.g., by the integration of differentiable rendering into network training. With a plethora of applications in computer graphics and vision, neural rendering is poised to become a new area in the graphics community, yet no survey of this emerging field exists. This state-of-the-art report summarizes the recent trends and applications of neural rendering. We focus on approaches that combine classic computer graphics techniques with deep generative models to obtain controllable and photo-realistic outputs. Starting with an overview of the underlying computer graphics and machine learning concepts, we discuss critical aspects of neural rendering approaches. This state-of-the-art report is focused on the many important use cases for the described algorithms such as novel view synthesis, semantic photo manipulation, facial and body reenactment, relighting, free-viewpoint video, and the creation of photo-realistic avatars for virtual and augmented reality telepresence. Finally, we conclude with a discussion of the social implications of such technology and investigate open research problems.



### MirrorNet: A Deep Bayesian Approach to Reflective 2D Pose Estimation from Human Images
- **Arxiv ID**: http://arxiv.org/abs/2004.03811v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.03811v1)
- **Published**: 2020-04-08 05:02:48+00:00
- **Updated**: 2020-04-08 05:02:48+00:00
- **Authors**: Takayuki Nakatsuka, Kazuyoshi Yoshii, Yuki Koyama, Satoru Fukayama, Masataka Goto, Shigeo Morishima
- **Comment**: 19 pages
- **Journal**: None
- **Summary**: This paper proposes a statistical approach to 2D pose estimation from human images. The main problems with the standard supervised approach, which is based on a deep recognition (image-to-pose) model, are that it often yields anatomically implausible poses, and its performance is limited by the amount of paired data. To solve these problems, we propose a semi-supervised method that can make effective use of images with and without pose annotations. Specifically, we formulate a hierarchical generative model of poses and images by integrating a deep generative model of poses from pose features with that of images from poses and image features. We then introduce a deep recognition model that infers poses from images. Given images as observed data, these models can be trained jointly in a hierarchical variational autoencoding (image-to-pose-to-feature-to-pose-to-image) manner. The results of experiments show that the proposed reflective architecture makes estimated poses anatomically plausible, and the performance of pose estimation improved by integrating the recognition and generative models and also by feeding non-annotated images.



### Feature Re-Learning with Data Augmentation for Video Relevance Prediction
- **Arxiv ID**: http://arxiv.org/abs/2004.03815v1
- **DOI**: 10.1109/TKDE.2019.2947442
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/2004.03815v1)
- **Published**: 2020-04-08 05:22:41+00:00
- **Updated**: 2020-04-08 05:22:41+00:00
- **Authors**: Jianfeng Dong, Xun Wang, Leimin Zhang, Chaoxi Xu, Gang Yang, Xirong Li
- **Comment**: accepted by IEEE Transactions on Knowledge and Data Engineering
  (TKDE)
- **Journal**: None
- **Summary**: Predicting the relevance between two given videos with respect to their visual content is a key component for content-based video recommendation and retrieval. Thanks to the increasing availability of pre-trained image and video convolutional neural network models, deep visual features are widely used for video content representation. However, as how two videos are relevant is task-dependent, such off-the-shelf features are not always optimal for all tasks. Moreover, due to varied concerns including copyright, privacy and security, one might have access to only pre-computed video features rather than original videos. We propose in this paper feature re-learning for improving video relevance prediction, with no need of revisiting the original video content. In particular, re-learning is realized by projecting a given deep feature into a new space by an affine transformation. We optimize the re-learning process by a novel negative-enhanced triplet ranking loss. In order to generate more training data, we propose a new data augmentation strategy which works directly on frame-level and video-level features. Extensive experiments in the context of the Hulu Content-based Video Relevance Prediction Challenge 2018 justify the effectiveness of the proposed method and its state-of-the-art performance for content-based video relevance prediction.



### Attentive Normalization for Conditional Image Generation
- **Arxiv ID**: http://arxiv.org/abs/2004.03828v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.03828v1)
- **Published**: 2020-04-08 06:12:25+00:00
- **Updated**: 2020-04-08 06:12:25+00:00
- **Authors**: Yi Wang, Ying-Cong Chen, Xiangyu Zhang, Jian Sun, Jiaya Jia
- **Comment**: CVPR 2020
- **Journal**: None
- **Summary**: Traditional convolution-based generative adversarial networks synthesize images based on hierarchical local operations, where long-range dependency relation is implicitly modeled with a Markov chain. It is still not sufficient for categories with complicated structures. In this paper, we characterize long-range dependence with attentive normalization (AN), which is an extension to traditional instance normalization. Specifically, the input feature map is softly divided into several regions based on its internal semantic similarity, which are respectively normalized. It enhances consistency between distant regions with semantic correspondence. Compared with self-attention GAN, our attentive normalization does not need to measure the correlation of all locations, and thus can be directly applied to large-size feature maps without much computational burden. Extensive experiments on class-conditional image generation and semantic inpainting verify the efficacy of our proposed module.



### Change Detection in Heterogeneous Optical and SAR Remote Sensing Images via Deep Homogeneous Feature Fusion
- **Arxiv ID**: http://arxiv.org/abs/2004.03830v1
- **DOI**: 10.1109/JSTARS.2020.2983993
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.03830v1)
- **Published**: 2020-04-08 06:27:37+00:00
- **Updated**: 2020-04-08 06:27:37+00:00
- **Authors**: Xiao Jiang, Gang Li, Yu Liu, Xiao-Ping Zhang, You He
- **Comment**: 15 pages, 14 figures, Accepted by IEEE Journal of Selected Topics in
  Applied Earth Observations and Remote Sensing
- **Journal**: None
- **Summary**: Change detection in heterogeneous remote sensing images is crucial for disaster damage assessment. Recent methods use homogenous transformation, which transforms the heterogeneous optical and SAR remote sensing images into the same feature space, to achieve change detection. Such transformations mainly operate on the low-level feature space and may corrupt the semantic content, deteriorating the performance of change detection. To solve this problem, this paper presents a new homogeneous transformation model termed deep homogeneous feature fusion (DHFF) based on image style transfer (IST). Unlike the existing methods, the DHFF method segregates the semantic content and the style features in the heterogeneous images to perform homogeneous transformation. The separation of the semantic content and the style in homogeneous transformation prevents the corruption of image semantic content, especially in the regions of change. In this way, the detection performance is improved with accurate homogeneous transformation. Furthermore, we present a new iterative IST (IIST) strategy, where the cost function in each IST iteration measures and thus maximizes the feature homogeneity in additional new feature subspaces for change detection. After that, change detection is accomplished accurately on the original and the transformed images that are in the same feature space. Real remote sensing images acquired by SAR and optical satellites are utilized to evaluate the performance of the proposed method. The experiments demonstrate that the proposed DHFF method achieves significant improvement for change detection in heterogeneous optical and SAR remote sensing images, in terms of both accuracy rate and Kappa index.



### Multi-Head Attention based Probabilistic Vehicle Trajectory Prediction
- **Arxiv ID**: http://arxiv.org/abs/2004.03842v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2004.03842v3)
- **Published**: 2020-04-08 06:58:51+00:00
- **Updated**: 2020-07-04 09:47:35+00:00
- **Authors**: Hayoung Kim, Dongchan Kim, Gihoon Kim, Jeongmin Cho, Kunsoo Huh
- **Comment**: 6 pages, 5 figures, 2020 IEEE Intelligent Vehicles Symposium (IV)
- **Journal**: None
- **Summary**: This paper presents online-capable deep learning model for probabilistic vehicle trajectory prediction. We propose a simple encoder-decoder architecture based on multi-head attention. The proposed model generates the distribution of the predicted trajectories for multiple vehicles in parallel. Our approach to model the interactions can learn to attend to a few influential vehicles in an unsupervised manner, which can improve the interpretability of the network. The experiments using naturalistic trajectories at highway show the clear improvement in terms of positional error on both longitudinal and lateral direction.



### MNIST-MIX: A Multi-language Handwritten Digit Recognition Dataset
- **Arxiv ID**: http://arxiv.org/abs/2004.03848v1
- **DOI**: 10.1088/2633-1357/abad0e
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.03848v1)
- **Published**: 2020-04-08 07:17:32+00:00
- **Updated**: 2020-04-08 07:17:32+00:00
- **Authors**: Weiwei Jiang
- **Comment**: 3 pages, 1 figure, 2 tables
- **Journal**: IOP SciNotes, 2020
- **Summary**: In this letter, we contribute a multi-language handwritten digit recognition dataset named MNIST-MIX, which is the largest dataset of the same type in terms of both languages and data samples. With the same data format with MNIST, MNIST-MIX can be seamlessly applied in existing studies for handwritten digit recognition. By introducing digits from 10 different languages, MNIST-MIX becomes a more challenging dataset and its imbalanced classification requires a better design of models. We also present the results of applying a LeNet model which is pre-trained on MNIST as the baseline.



### A Robust Method for Image Stitching
- **Arxiv ID**: http://arxiv.org/abs/2004.03860v3
- **DOI**: 10.1007/s10044-021-01005-8
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.03860v3)
- **Published**: 2020-04-08 07:53:31+00:00
- **Updated**: 2021-07-28 04:52:35+00:00
- **Authors**: Matti Pellikka, Valtteri Lahtinen
- **Comment**: None
- **Journal**: Pattern Analysis and Applications, 2021
- **Summary**: We propose a novel method for large-scale image stitching that is robust against repetitive patterns and featureless regions in the imagery. In such cases, state-of-the-art image stitching methods easily produce image alignment artifacts, since they may produce false pairwise image registrations that are in conflict within the global connectivity graph. Our method augments the current methods by collecting all the plausible pairwise image registration candidates, among which globally consistent candidates are chosen. This enables the stitching process to determine the correct pairwise registrations by utilizing all the available information from the whole imagery, such as unambiguous registrations outside the repeating pattern and featureless regions. We formalize the method as a weighted multigraph whose nodes represent the individual image transformations from the composite image, and whose sets of multiple edges between two nodes represent all the plausible transformations between the pixel coordinates of the two images. The edge weights represent the plausibility of the transformations. The image transformations and the edge weights are solved from a non-linear minimization problem with linear constraints, for which a projection method is used. As an example, we apply the method in a large-scale scanning application where the transformations are primarily translations with only slight rotation and scaling component. Despite these simplifications, the state-of-the-art methods do not produce adequate results in such applications, since the image overlap is small, which can be featureless or repetitive, and misalignment artifacts and their concealment are unacceptable.



### S2A: Wasserstein GAN with Spatio-Spectral Laplacian Attention for Multi-Spectral Band Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2004.03867v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.03867v1)
- **Published**: 2020-04-08 08:07:00+00:00
- **Updated**: 2020-04-08 08:07:00+00:00
- **Authors**: Litu Rout, Indranil Misra, S Manthira Moorthi, Debajyoti Dhar
- **Comment**: Computer Vision and Pattern Recognition (CVPR) Workshop on Large
  Scale Computer Vision for Remote Sensing Imagery
- **Journal**: None
- **Summary**: Intersection of adversarial learning and satellite image processing is an emerging field in remote sensing. In this study, we intend to address synthesis of high resolution multi-spectral satellite imagery using adversarial learning. Guided by the discovery of attention mechanism, we regulate the process of band synthesis through spatio-spectral Laplacian attention. Further, we use Wasserstein GAN with gradient penalty norm to improve training and stability of adversarial learning. In this regard, we introduce a new cost function for the discriminator based on spatial attention and domain adaptation loss. We critically analyze the qualitative and quantitative results compared with state-of-the-art methods using widely adopted evaluation metrics. Our experiments on datasets of three different sensors, namely LISS-3, LISS-4, and WorldView-2 show that attention learning performs favorably against state-of-the-art methods. Using the proposed method we provide an additional data product in consistent with existing high resolution bands. Furthermore, we synthesize over 4000 high resolution scenes covering various terrains to analyze scientific fidelity. At the end, we demonstrate plausible large scale real world applications of the synthesized band.



### Monte-Carlo Siamese Policy on Actor for Satellite Image Super Resolution
- **Arxiv ID**: http://arxiv.org/abs/2004.03879v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.03879v1)
- **Published**: 2020-04-08 08:39:08+00:00
- **Updated**: 2020-04-08 08:39:08+00:00
- **Authors**: Litu Rout, Saumyaa Shah, S Manthira Moorthi, Debajyoti Dhar
- **Comment**: Computer Vision and Pattern Recognition (CVPR) Workshop on Large
  Scale Computer Vision for Remote Sensing Imagery
- **Journal**: None
- **Summary**: In the past few years supervised and adversarial learning have been widely adopted in various complex computer vision tasks. It seems natural to wonder whether another branch of artificial intelligence, commonly known as Reinforcement Learning (RL) can benefit such complex vision tasks. In this study, we explore the plausible usage of RL in super resolution of remote sensing imagery. Guided by recent advances in super resolution, we propose a theoretical framework that leverages the benefits of supervised and reinforcement learning. We argue that a straightforward implementation of RL is not adequate to address ill-posed super resolution as the action variables are not fully known. To tackle this issue, we propose to parameterize action variables by matrices, and train our policy network using Monte-Carlo sampling. We study the implications of parametric action space in a model-free environment from theoretical and empirical perspective. Furthermore, we analyze the quantitative and qualitative results on both remote sensing and non-remote sensing datasets. Based on our experiments, we report considerable improvement over state-of-the-art methods by encapsulating supervised models in a reinforcement learning framework.



### CNN in CT Image Segmentation: Beyound Loss Function for Expoliting Ground Truth Images
- **Arxiv ID**: http://arxiv.org/abs/2004.03882v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.03882v1)
- **Published**: 2020-04-08 08:44:39+00:00
- **Updated**: 2020-04-08 08:44:39+00:00
- **Authors**: Youyi Song, Zhen Yu, Teng Zhou, Jeremy Yuen-Chun Teoh, Baiying Lei, Kup-Sze Choi, Jing Qin
- **Comment**: 4 pages, 3 figures, and having been accepted by ISBI 2020
- **Journal**: None
- **Summary**: Exploiting more information from ground truth (GT) images now is a new research direction for further improving CNN's performance in CT image segmentation. Previous methods focus on devising the loss function for fulfilling such a purpose. However, it is rather difficult to devise a general and optimization-friendly loss function. We here present a novel and practical method that exploits GT images beyond the loss function. Our insight is that feature maps of two CNNs trained respectively on GT and CT images should be similar on some metric space, because they both are used to describe the same objects for the same purpose. We hence exploit GT images by enforcing such two CNNs' feature maps to be consistent. We assess the proposed method on two data sets, and compare its performance to several competitive methods. Extensive experimental results show that the proposed method is effective, outperforming all the compared methods.



### Normalizing Flows with Multi-Scale Autoregressive Priors
- **Arxiv ID**: http://arxiv.org/abs/2004.03891v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.03891v1)
- **Published**: 2020-04-08 09:07:11+00:00
- **Updated**: 2020-04-08 09:07:11+00:00
- **Authors**: Shweta Mahajan, Apratim Bhattacharyya, Mario Fritz, Bernt Schiele, Stefan Roth
- **Comment**: To appear in CVPR 2020
- **Journal**: None
- **Summary**: Flow-based generative models are an important class of exact inference models that admit efficient inference and sampling for image synthesis. Owing to the efficiency constraints on the design of the flow layers, e.g. split coupling flow layers in which approximately half the pixels do not undergo further transformations, they have limited expressiveness for modeling long-range data dependencies compared to autoregressive models that rely on conditional pixel-wise generation. In this work, we improve the representational power of flow-based models by introducing channel-wise dependencies in their latent space through multi-scale autoregressive priors (mAR). Our mAR prior for models with split coupling flow layers (mAR-SCF) can better capture dependencies in complex multimodal data. The resulting model achieves state-of-the-art density estimation results on MNIST, CIFAR-10, and ImageNet. Furthermore, we show that mAR-SCF allows for improved image generation quality, with gains in FID and Inception scores compared to state-of-the-art flow-based models.



### Constrained Multi-shape Evolution for Overlapping Cytoplasm Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2004.03892v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.03892v2)
- **Published**: 2020-04-08 09:08:07+00:00
- **Updated**: 2020-04-15 12:00:02+00:00
- **Authors**: Youyi Song, Lei Zhu, Baiying Lei, Bin Sheng, Qi Dou, Jing Qin, Kup-Sze Choi
- **Comment**: 12 pages and 6 figures
- **Journal**: None
- **Summary**: Segmenting overlapping cytoplasm of cells in cervical smear images is a clinically essential task, for quantitatively measuring cell-level features in order to diagnose cervical cancer. This task, however, remains rather challenging, mainly due to the deficiency of intensity (or color) information in the overlapping region. Although shape prior-based models that compensate intensity deficiency by introducing prior shape information (shape priors) about cytoplasm are firmly established, they often yield visually implausible results, mainly because they model shape priors only by limited shape hypotheses about cytoplasm, exploit cytoplasm-level shape priors alone, and impose no shape constraint on the resulting shape of the cytoplasm. In this paper, we present a novel and effective shape prior-based approach, called constrained multi-shape evolution, that segments all overlapping cytoplasms in the clump simultaneously by jointly evolving each cytoplasm's shape guided by the modeled shape priors. We model local shape priors (cytoplasm--level) by an infinitely large shape hypothesis set which contains all possible shapes of the cytoplasm. In the shape evolution, we compensate intensity deficiency for the segmentation by introducing not only the modeled local shape priors but also global shape priors (clump--level) modeled by considering mutual shape constraints of cytoplasms in the clump. We also constrain the resulting shape in each evolution to be in the built shape hypothesis set, for further reducing implausible segmentation results. We evaluated the proposed method in two typical cervical smear datasets, and the extensive experimental results show that the proposed method is effective to segment overlapping cytoplasm, consistently outperforming the state-of-the-art methods.



### Towards Reusable Network Components by Learning Compatible Representations
- **Arxiv ID**: http://arxiv.org/abs/2004.03898v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.03898v3)
- **Published**: 2020-04-08 09:21:37+00:00
- **Updated**: 2020-12-16 13:31:27+00:00
- **Authors**: Michael Gygli, Jasper Uijlings, Vittorio Ferrari
- **Comment**: Preprint; To be presented at AAAI 2021
- **Journal**: None
- **Summary**: This paper proposes to make a first step towards compatible and hence reusable network components. Rather than training networks for different tasks independently, we adapt the training process to produce network components that are compatible across tasks. In particular, we split a network into two components, a features extractor and a target task head, and propose various approaches to accomplish compatibility between them. We systematically analyse these approaches on the task of image classification on standard datasets. We demonstrate that we can produce components which are directly compatible without any fine-tuning or compromising accuracy on the original tasks. Afterwards, we demonstrate the use of compatible components on three applications: Unsupervised domain adaptation, transferring classifiers across feature extractors with different architectures, and increasing the computational efficiency of transfer learning.



### Deep Adaptive Inference Networks for Single Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2004.03915v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.03915v1)
- **Published**: 2020-04-08 10:08:20+00:00
- **Updated**: 2020-04-08 10:08:20+00:00
- **Authors**: Ming Liu, Zhilu Zhang, Liya Hou, Wangmeng Zuo, Lei Zhang
- **Comment**: Code can be found at https://github.com/csmliu/AdaDSR
- **Journal**: None
- **Summary**: Recent years have witnessed tremendous progress in single image super-resolution (SISR) owing to the deployment of deep convolutional neural networks (CNNs). For most existing methods, the computational cost of each SISR model is irrelevant to local image content, hardware platform and application scenario. Nonetheless, content and resource adaptive model is more preferred, and it is encouraging to apply simpler and efficient networks to the easier regions with less details and the scenarios with restricted efficiency constraints. In this paper, we take a step forward to address this issue by leveraging the adaptive inference networks for deep SISR (AdaDSR). In particular, our AdaDSR involves an SISR model as backbone and a lightweight adapter module which takes image features and resource constraint as input and predicts a map of local network depth. Adaptive inference can then be performed with the support of efficient sparse convolution, where only a fraction of the layers in the backbone is performed at a given position according to its predicted depth. The network learning can be formulated as the joint optimization of reconstruction and network depth losses. In the inference stage, the average depth can be flexibly tuned to meet a range of efficiency constraints. Experiments demonstrate the effectiveness and adaptability of our AdaDSR in contrast to its counterparts (e.g., EDSR and RCAN).



### Image super-resolution reconstruction based on attention mechanism and feature fusion
- **Arxiv ID**: http://arxiv.org/abs/2004.03939v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.03939v1)
- **Published**: 2020-04-08 11:20:10+00:00
- **Updated**: 2020-04-08 11:20:10+00:00
- **Authors**: Jiawen Lyn, Sen Yan
- **Comment**: None
- **Journal**: None
- **Summary**: Aiming at the problems that the convolutional neural networks neglect to capture the inherent attributes of natural images and extract features only in a single scale in the field of image super-resolution reconstruction, a network structure based on attention mechanism and multi-scale feature fusion is proposed. By using the attention mechanism, the network can effectively integrate the non-local information and second-order features of the image, so as to improve the feature expression ability of the network. At the same time, the convolution kernel of different scales is used to extract the multi-scale information of the image, so as to preserve the complete information characteristics at different scales. Experimental results show that the proposed method can achieve better performance over other representative super-resolution reconstruction algorithms in objective quantitative metrics and visual quality.



### Improved YOLOv3 Object Classification in Intelligent Transportation System
- **Arxiv ID**: http://arxiv.org/abs/2004.03948v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.03948v1)
- **Published**: 2020-04-08 11:45:13+00:00
- **Updated**: 2020-04-08 11:45:13+00:00
- **Authors**: Yang Zhang, Changhui Hu, Xiaobo Lu
- **Comment**: None
- **Journal**: None
- **Summary**: The technology of vehicle and driver detection in Intelligent Transportation System(ITS) is a hot topic in recent years. In particular, the driver detection is still a challenging problem which is conductive to supervising traffic order and maintaining public safety. In this paper, an algorithm based on YOLOv3 is proposed to realize the detection and classification of vehicles, drivers, and people on the highway, so as to achieve the purpose of distinguishing driver and passenger and form a one-to-one correspondence between vehicles and drivers. The proposed model and contrast experiment are conducted on our self-build traffic driver's face database. The effectiveness of our proposed algorithm is validated by extensive experiments and verified under various complex highway conditions. Compared with other advanced vehicle and driver detection technologies, the model has a good performance and is robust to road blocking, different attitudes, and extreme lighting.



### Dendrite Net: A White-Box Module for Classification, Regression, and System Identification
- **Arxiv ID**: http://arxiv.org/abs/2004.03955v6
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2004.03955v6)
- **Published**: 2020-04-08 12:02:16+00:00
- **Updated**: 2021-11-19 03:24:11+00:00
- **Authors**: Gang Liu, Jing Wang
- **Comment**: Dendrite (DD)---A new "block" since decades. Published by IEEE
  Transactions on Cybernetics
- **Journal**: None
- **Summary**: The simulation of biological dendrite computations is vital for the development of artificial intelligence (AI). This paper presents a basic machine learning algorithm, named Dendrite Net or DD, just like Support Vector Machine (SVM) or Multilayer Perceptron (MLP). DD's main concept is that the algorithm can recognize this class after learning, if the output's logical expression contains the corresponding class's logical relationship among inputs (and$\backslash$or$\backslash$not). Experiments and main results: DD, a white-box machine learning algorithm, showed excellent system identification performance for the black-box system. Secondly, it was verified by nine real-world applications that DD brought better generalization capability relative to MLP architecture that imitated neurons' cell body (Cell body Net) for regression. Thirdly, by MNIST and FASHION-MNIST datasets, it was verified that DD showed higher testing accuracy under greater training loss than Cell body Net for classification. The number of modules can effectively adjust DD's logical expression capacity, which avoids over-fitting and makes it easy to get a model with outstanding generalization capability. Finally, repeated experiments in MATLAB and PyTorch (Python) demonstrated that DD was faster than Cell body Net both in epoch and forward-propagation. The main contribution of this paper is the basic machine learning algorithm (DD) with a white-box attribute, controllable precision for better generalization capability, and lower computational complexity. Not only can DD be used for generalized engineering, but DD has vast development potential as a module for deep learning. DD code is available at GitHub: https://github.com/liugang1234567/Gang-neuron .



### Adversary Helps: Gradient-based Device-Free Domain-Independent Gesture Recognition
- **Arxiv ID**: http://arxiv.org/abs/2004.03961v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.03961v1)
- **Published**: 2020-04-08 12:20:44+00:00
- **Updated**: 2020-04-08 12:20:44+00:00
- **Authors**: Jianwei Liu, Jinsong Han, Feng Lin, Kui Ren
- **Comment**: None
- **Journal**: None
- **Summary**: Wireless signal-based gesture recognition has promoted the developments of VR game, smart home, etc. However, traditional approaches suffer from the influence of the domain gap. Low recognition accuracy occurs when the recognition model is trained in one domain but is used in another domain. Though some solutions, such as adversarial learning, transfer learning and body-coordinate velocity profile, have been proposed to achieve cross-domain recognition, these solutions more or less have flaws. In this paper, we define the concept of domain gap and then propose a more promising solution, namely DI, to eliminate domain gap and further achieve domain-independent gesture recognition. DI leverages the sign map of the gradient map as the domain gap eliminator to improve the recognition accuracy. We conduct experiments with ten domains and ten gestures. The experiment results show that DI can achieve the recognition accuracies of 87.13%, 90.12% and 94.45% on KNN, SVM and CNN, which outperforms existing solutions.



### Learning 3D Semantic Scene Graphs from 3D Indoor Reconstructions
- **Arxiv ID**: http://arxiv.org/abs/2004.03967v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.03967v1)
- **Published**: 2020-04-08 12:25:25+00:00
- **Updated**: 2020-04-08 12:25:25+00:00
- **Authors**: Johanna Wald, Helisa Dhamo, Nassir Navab, Federico Tombari
- **Comment**: first two authors contributed equally, CVPR 2020, video
  https://youtu.be/8D3HjYf6cYw
- **Journal**: None
- **Summary**: Scene understanding has been of high interest in computer vision. It encompasses not only identifying objects in a scene, but also their relationships within the given context. With this goal, a recent line of works tackles 3D semantic segmentation and scene layout prediction. In our work we focus on scene graphs, a data structure that organizes the entities of a scene in a graph, where objects are nodes and their relationships modeled as edges. We leverage inference on scene graphs as a way to carry out 3D scene understanding, mapping objects and their relationships. In particular, we propose a learned method that regresses a scene graph from the point cloud of a scene. Our novel architecture is based on PointNet and Graph Convolutional Networks (GCN). In addition, we introduce 3DSSG, a semi-automatically generated dataset, that contains semantically rich scene graphs of 3D scenes. We show the application of our method in a domain-agnostic retrieval task, where graphs serve as an intermediate representation for 3D-3D and 2D-3D matching.



### Multi-Person Absolute 3D Human Pose Estimation with Weak Depth Supervision
- **Arxiv ID**: http://arxiv.org/abs/2004.03989v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.03989v1)
- **Published**: 2020-04-08 13:29:22+00:00
- **Updated**: 2020-04-08 13:29:22+00:00
- **Authors**: Marton Veges, Andras Lorincz
- **Comment**: None
- **Journal**: None
- **Summary**: In 3D human pose estimation one of the biggest problems is the lack of large, diverse datasets. This is especially true for multi-person 3D pose estimation, where, to our knowledge, there are only machine generated annotations available for training. To mitigate this issue, we introduce a network that can be trained with additional RGB-D images in a weakly supervised fashion. Due to the existence of cheap sensors, videos with depth maps are widely available, and our method can exploit a large, unannotated dataset. Our algorithm is a monocular, multi-person, absolute pose estimator. We evaluate the algorithm on several benchmarks, showing a consistent improvement in error rates. Also, our model achieves state-of-the-art results on the MuPoTS-3D dataset by a considerable margin.



### ARCH: Animatable Reconstruction of Clothed Humans
- **Arxiv ID**: http://arxiv.org/abs/2004.04572v2
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2004.04572v2)
- **Published**: 2020-04-08 14:23:08+00:00
- **Updated**: 2020-04-10 19:14:39+00:00
- **Authors**: Zeng Huang, Yuanlu Xu, Christoph Lassner, Hao Li, Tony Tung
- **Comment**: 10 pages, 10 figures, CVPR2020
- **Journal**: None
- **Summary**: In this paper, we propose ARCH (Animatable Reconstruction of Clothed Humans), a novel end-to-end framework for accurate reconstruction of animation-ready 3D clothed humans from a monocular image. Existing approaches to digitize 3D humans struggle to handle pose variations and recover details. Also, they do not produce models that are animation ready. In contrast, ARCH is a learned pose-aware model that produces detailed 3D rigged full-body human avatars from a single unconstrained RGB image. A Semantic Space and a Semantic Deformation Field are created using a parametric 3D body estimator. They allow the transformation of 2D/3D clothed humans into a canonical space, reducing ambiguities in geometry caused by pose variations and occlusions in training data. Detailed surface geometry and appearance are learned using an implicit function representation with spatial local features. Furthermore, we propose additional per-pixel supervision on the 3D reconstruction using opacity-aware differentiable rendering. Our experiments indicate that ARCH increases the fidelity of the reconstructed humans. We obtain more than 50% lower reconstruction errors for standard metrics compared to state-of-the-art methods on public datasets. We also show numerous qualitative examples of animated, high-quality reconstructed avatars unseen in the literature so far.



### A Deep Learning Approach for Determining Effects of Tuta Absoluta in Tomato Plants
- **Arxiv ID**: http://arxiv.org/abs/2004.04023v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.04023v1)
- **Published**: 2020-04-08 14:41:38+00:00
- **Updated**: 2020-04-08 14:41:38+00:00
- **Authors**: Denis P. Rubanga, Loyani K. Loyani, Mgaya Richard, Sawahiko Shimada
- **Comment**: Paper presented at the ICLR 2020 Workshop on Computer Vision for
  Agriculture (CV4A)
- **Journal**: None
- **Summary**: Early quantification of Tuta absoluta pest's effects in tomato plants is a very important factor in controlling and preventing serious damages of the pest. The invasion of Tuta absoluta is considered a major threat to tomato production causing heavy loss ranging from 80 to 100 percent when not properly managed. Therefore, real-time and early quantification of tomato leaf miner Tuta absoluta, can play an important role in addressing the issue of pest management and enhance farmers' decisions. In this study, we propose a Convolutional Neural Network (CNN) approach in determining the effects of Tuta absoluta in tomato plants. Four CNN pre-trained architectures (VGG16, VGG19, ResNet and Inception-V3) were used in training classifiers on a dataset containing health and infested tomato leaves collected from real field experiments. Among the pre-trained architectures, experimental results showed that Inception-V3 yielded the best results with an average accuracy of 87.2 percent in estimating the severity status of Tuta absoluta in tomato plants. The pre-trained models could also easily identify High Tuta severity status compared to other severity status (Low tuta and No tuta)



### Convolutional neural net face recognition works in non-human-like ways
- **Arxiv ID**: http://arxiv.org/abs/2004.04069v2
- **DOI**: None
- **Categories**: **cs.CV**, I.4; I.5
- **Links**: [PDF](http://arxiv.org/pdf/2004.04069v2)
- **Published**: 2020-04-08 15:42:14+00:00
- **Updated**: 2020-06-23 11:59:06+00:00
- **Authors**: P. J. B. Hancock, R. S. Somai, V. R. Mileva
- **Comment**: 8 pages, 2 figures. Submitted to Royal Society Open Science
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) give state of the art performance in many pattern recognition problems but can be fooled by carefully crafted patterns of noise. We report that CNN face recognition systems also make surprising "errors". We tested six commercial face recognition CNNs and found that they outperform typical human participants on standard face matching tasks. However, they also declare matches that humans would not, where one image from the pair has been transformed to look a different sex or race. This is not due to poor performance; the best CNNs perform almost perfectly on the human face matching tasks, but also declare the most matches for faces of a different apparent race or sex. Although differing on the salience of sex and race, humans and computer systems are not working in completely different ways. They tend to find the same pairs of images difficult, suggesting some agreement about the underlying similarity space.



### Satellite-based Prediction of Forage Conditions for Livestock in Northern Kenya
- **Arxiv ID**: http://arxiv.org/abs/2004.04081v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.04081v2)
- **Published**: 2020-04-08 16:03:50+00:00
- **Updated**: 2020-04-23 18:12:23+00:00
- **Authors**: Andrew Hobbs, Stacey Svetlichnaya
- **Comment**: Paper presented at the ICLR 2020 Workshop on Computer Vision for
  Agriculture (CV4A)
- **Journal**: None
- **Summary**: This paper introduces the first dataset of satellite images labeled with forage quality by on-the-ground experts and provides proof of concept for applying computer vision methods to index-based drought insurance. We also present the results of a collaborative benchmark tool used to crowdsource an accurate machine learning model on the dataset. Our methods significantly outperform the existing technology for an insurance program in Northern Kenya, suggesting that a computer vision-based approach could substantially benefit pastoralists, whose exposure to droughts is severe and worsening with climate change.



### Beyond Photometric Consistency: Gradient-based Dissimilarity for Improving Visual Odometry and Stereo Matching
- **Arxiv ID**: http://arxiv.org/abs/2004.04090v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.04090v1)
- **Published**: 2020-04-08 16:13:25+00:00
- **Updated**: 2020-04-08 16:13:25+00:00
- **Authors**: Jan Quenzel, Radu Alexandru Rosu, Thomas Läbe, Cyrill Stachniss, Sven Behnke
- **Comment**: Accepted for International Conference on Robotic and Automation
  (ICRA), 2020
- **Journal**: None
- **Summary**: Pose estimation and map building are central ingredients of autonomous robots and typically rely on the registration of sensor data. In this paper, we investigate a new metric for registering images that builds upon on the idea of the photometric error. Our approach combines a gradient orientation-based metric with a magnitude-dependent scaling term. We integrate both into stereo estimation as well as visual odometry systems and show clear benefits for typical disparity and direct image registration tasks when using our proposed metric. Our experimental evaluation indicats that our metric leads to more robust and more accurate estimates of the scene depth as well as camera trajectory. Thus, the metric improves camera pose estimation and in turn the mapping capabilities of mobile robots. We believe that a series of existing visual odometry and visual SLAM systems can benefit from the findings reported in this paper.



### Weakly Supervised Semantic Point Cloud Segmentation:Towards 10X Fewer Labels
- **Arxiv ID**: http://arxiv.org/abs/2004.04091v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.04091v1)
- **Published**: 2020-04-08 16:14:41+00:00
- **Updated**: 2020-04-08 16:14:41+00:00
- **Authors**: Xun Xu, Gim Hee Lee
- **Comment**: CVPR2020
- **Journal**: None
- **Summary**: Point cloud analysis has received much attention recently; and segmentation is one of the most important tasks. The success of existing approaches is attributed to deep network design and large amount of labelled training data, where the latter is assumed to be always available. However, obtaining 3d point cloud segmentation labels is often very costly in practice. In this work, we propose a weakly supervised point cloud segmentation approach which requires only a tiny fraction of points to be labelled in the training stage. This is made possible by learning gradient approximation and exploitation of additional spatial and color smoothness constraints. Experiments are done on three public datasets with different degrees of weak supervision. In particular, our proposed method can produce results that are close to and sometimes even better than its fully supervised counterpart with 10$\times$ fewer labels.



### Time accelerated image super-resolution using shallow residual feature representative network
- **Arxiv ID**: http://arxiv.org/abs/2004.04093v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.04093v1)
- **Published**: 2020-04-08 16:17:42+00:00
- **Updated**: 2020-04-08 16:17:42+00:00
- **Authors**: Meenu Ajith, Aswathy Rajendra Kurup, Manel Martínez-Ramón
- **Comment**: None
- **Journal**: None
- **Summary**: The recent advances in deep learning indicate significant progress in the field of single image super-resolution. With the advent of these techniques, high-resolution image with high peak signal to noise ratio (PSNR) and excellent perceptual quality can be reconstructed. The major challenges associated with existing deep convolutional neural networks are their computational complexity and time; the increasing depth of the networks, often result in high space complexity. To alleviate these issues, we developed an innovative shallow residual feature representative network (SRFRN) that uses a bicubic interpolated low-resolution image as input and residual representative units (RFR) which include serially stacked residual non-linear convolutions. Furthermore, the reconstruction of the high-resolution image is done by combining the output of the RFR units and the residual output from the bicubic interpolated LR image. Finally, multiple experiments have been performed on the benchmark datasets and the proposed model illustrates superior performance for higher scales. Besides, this model also exhibits faster execution time compared to all the existing approaches.



### Skin Diseases Detection using LBP and WLD- An Ensembling Approach
- **Arxiv ID**: http://arxiv.org/abs/2004.04122v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.04122v1)
- **Published**: 2020-04-08 17:09:59+00:00
- **Updated**: 2020-04-08 17:09:59+00:00
- **Authors**: Arnab Banerjee, Nibaran Das, Mita Nasipuri
- **Comment**: None
- **Journal**: None
- **Summary**: In all developing and developed countries in the world, skin diseases are becoming a very frequent health problem for the humans of all age groups. Skin problems affect mental health, develop addiction to alcohol and drugs and sometimes causes social isolation. Considering the importance, we propose an automatic technique to detect three popular skin diseases- Leprosy, Tinea versicolor and Vitiligofrom the images of skin lesions. The proposed technique involves Weber local descriptor and Local binary pattern to represent texture pattern of the affected skin regions. This ensemble technique achieved 91.38% accuracy using multi-level support vector machine classifier, where features are extracted from different regions that are based on center of gravity. We have also applied some popular deep learn-ing networks such as MobileNet, ResNet_152, GoogLeNet,DenseNet_121, and ResNet_101. We get 89% accuracy using ResNet_101. The ensemble approach clearly outperform all of the used deep learning networks. This imaging tool will be useful for early skin disease screening.



### CURL: Contrastive Unsupervised Representations for Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/2004.04136v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.04136v4)
- **Published**: 2020-04-08 17:40:43+00:00
- **Updated**: 2020-09-21 15:34:30+00:00
- **Authors**: Aravind Srinivas, Michael Laskin, Pieter Abbeel
- **Comment**: First two authors contributed equally, website:
  https://mishalaskin.github.io/curl code: https://github.com/MishaLaskin/curl
- **Journal**: None
- **Summary**: We present CURL: Contrastive Unsupervised Representations for Reinforcement Learning. CURL extracts high-level features from raw pixels using contrastive learning and performs off-policy control on top of the extracted features. CURL outperforms prior pixel-based methods, both model-based and model-free, on complex tasks in the DeepMind Control Suite and Atari Games showing 1.9x and 1.2x performance gains at the 100K environment and interaction steps benchmarks respectively. On the DeepMind Control Suite, CURL is the first image-based algorithm to nearly match the sample-efficiency of methods that use state-based features. Our code is open-sourced and available at https://github.com/MishaLaskin/curl.



### Empirical Perspectives on One-Shot Semi-supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2004.04141v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.04141v1)
- **Published**: 2020-04-08 17:51:06+00:00
- **Updated**: 2020-04-08 17:51:06+00:00
- **Authors**: Leslie N. Smith, Adam Conovaloff
- **Comment**: Short paper with interesting results pointing to further
  investigation
- **Journal**: None
- **Summary**: One of the greatest obstacles in the adoption of deep neural networks for new applications is that training the network typically requires a large number of manually labeled training samples. We empirically investigate the scenario where one has access to large amounts of unlabeled data but require labeling only a single prototypical sample per class in order to train a deep network (i.e., one-shot semi-supervised learning). Specifically, we investigate the recent results reported in FixMatch for one-shot semi-supervised learning to understand the factors that affect and impede high accuracies and reliability for one-shot semi-supervised learning of Cifar-10. For example, we discover that one barrier to one-shot semi-supervised learning for high-performance image classification is the unevenness of class accuracy during the training. These results point to solutions that might enable more widespread adoption of one-shot semi-supervised training methods for new applications.



### Self-Supervised Monocular Scene Flow Estimation
- **Arxiv ID**: http://arxiv.org/abs/2004.04143v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2004.04143v2)
- **Published**: 2020-04-08 17:55:54+00:00
- **Updated**: 2020-04-15 22:17:10+00:00
- **Authors**: Junhwa Hur, Stefan Roth
- **Comment**: To appear at CVPR 2020 (Oral); a typo corrected in the reference
  section
- **Journal**: None
- **Summary**: Scene flow estimation has been receiving increasing attention for 3D environment perception. Monocular scene flow estimation -- obtaining 3D structure and 3D motion from two temporally consecutive images -- is a highly ill-posed problem, and practical solutions are lacking to date. We propose a novel monocular scene flow method that yields competitive accuracy and real-time performance. By taking an inverse problem view, we design a single convolutional neural network (CNN) that successfully estimates depth and 3D motion simultaneously from a classical optical flow cost volume. We adopt self-supervised learning with 3D loss functions and occlusion reasoning to leverage unlabeled data. We validate our design choices, including the proxy loss and augmentation setup. Our model achieves state-of-the-art accuracy among unsupervised/self-supervised learning approaches to monocular scene flow, and yields competitive results for the optical flow and monocular depth estimation sub-tasks. Semi-supervised fine-tuning further improves the accuracy and yields promising results in real-time.



### Slicing and dicing soccer: automatic detection of complex events from spatio-temporal data
- **Arxiv ID**: http://arxiv.org/abs/2004.04147v2
- **DOI**: 10.1007/978-3-030-50347-5_11
- **Categories**: **cs.CV**, cs.LO
- **Links**: [PDF](http://arxiv.org/pdf/2004.04147v2)
- **Published**: 2020-04-08 17:57:50+00:00
- **Updated**: 2020-04-10 07:30:55+00:00
- **Authors**: Lia Morra, Francesco Manigrasso, Giuseppe Canto, Claudio Gianfrate, Enrico Guarino, Fabrizio Lamberti
- **Comment**: accepted at 17th International Conference on Image Analysis and
  Recognition ICIAR 2020
- **Journal**: None
- **Summary**: The automatic detection of events in sport videos has im-portant applications for data analytics, as well as for broadcasting andmedia companies. This paper presents a comprehensive approach for de-tecting a wide range of complex events in soccer videos starting frompositional data. The event detector is designed as a two-tier system thatdetectsatomicandcomplex events. Atomic events are detected basedon temporal and logical combinations of the detected objects, their rel-ative distances, as well as spatio-temporal features such as velocity andacceleration. Complex events are defined as temporal and logical com-binations of atomic and complex events, and are expressed by meansof a declarative Interval Temporal Logic (ITL). The effectiveness of theproposed approach is demonstrated over 16 different events, includingcomplex situations such as tackles and filtering passes. By formalizingevents based on principled ITL, it is possible to easily perform reason-ing tasks, such as understanding which passes or crosses result in a goalbeing scored. To counterbalance the lack of suitable, annotated publicdatasets, we built on an open source soccer simulation engine to re-lease the synthetic SoccER (Soccer Event Recognition) dataset, whichincludes complete positional data and annotations for more than 1.6 mil-lion atomic events and 9,000 complex events. The dataset and code areavailable at https://gitlab.com/grains2/slicing-and-dicing-soccer



### Inpainting via Generative Adversarial Networks for CMB data analysis
- **Arxiv ID**: http://arxiv.org/abs/2004.04177v2
- **DOI**: 10.1088/1475-7516/2021/03/012
- **Categories**: **astro-ph.CO**, cs.CV, cs.LG, stat.CO, I.2.6
- **Links**: [PDF](http://arxiv.org/pdf/2004.04177v2)
- **Published**: 2020-04-08 18:00:10+00:00
- **Updated**: 2020-04-21 07:38:15+00:00
- **Authors**: Alireza Vafaei Sadr, Farida Farsian
- **Comment**: 19 pages, 21 figures. Prepared for submission to JCAP. All codes will
  be published after acceptance
- **Journal**: None
- **Summary**: In this work, we propose a new method to inpaint the CMB signal in regions masked out following a point source extraction process. We adopt a modified Generative Adversarial Network (GAN) and compare different combinations of internal (hyper-)parameters and training strategies. We study the performance using a suitable $\mathcal{C}_r$ variable in order to estimate the performance regarding the CMB power spectrum recovery. We consider a test set where one point source is masked out in each sky patch with a 1.83 $\times$ 1.83 squared degree extension, which, in our gridding, corresponds to 64 $\times$ 64 pixels. The GAN is optimized for estimating performance on Planck 2018 total intensity simulations. The training makes the GAN effective in reconstructing a masking corresponding to about 1500 pixels with $1\%$ error down to angular scales corresponding to about 5 arcminutes.



### Leveraging 2D Data to Learn Textured 3D Mesh Generation
- **Arxiv ID**: http://arxiv.org/abs/2004.04180v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.04180v1)
- **Published**: 2020-04-08 18:00:37+00:00
- **Updated**: 2020-04-08 18:00:37+00:00
- **Authors**: Paul Henderson, Vagia Tsiminaki, Christoph H. Lampert
- **Comment**: CVPR 2020 (oral)
- **Journal**: None
- **Summary**: Numerous methods have been proposed for probabilistic generative modelling of 3D objects. However, none of these is able to produce textured objects, which renders them of limited use for practical tasks. In this work, we present the first generative model of textured 3D meshes. Training such a model would traditionally require a large dataset of textured meshes, but unfortunately, existing datasets of meshes lack detailed textures. We instead propose a new training methodology that allows learning from collections of 2D images without any 3D information. To do so, we train our model to explain a distribution of images by modelling each image as a 3D foreground object placed in front of a 2D background. Thus, it learns to generate meshes that when rendered, produce images similar to those in its training set.   A well-known problem when generating meshes with deep networks is the emergence of self-intersections, which are problematic for many use-cases. As a second contribution we therefore introduce a new generation process for 3D meshes that guarantees no self-intersections arise, based on the physical intuition that faces should push one another out of the way as they move.   We conduct extensive experiments on our approach, reporting quantitative and qualitative results on both synthetic data and natural images. These show our method successfully learns to generate plausible and diverse textured 3D samples for five challenging object classes.



### The GeoLifeCLEF 2020 Dataset
- **Arxiv ID**: http://arxiv.org/abs/2004.04192v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.04192v1)
- **Published**: 2020-04-08 18:30:00+00:00
- **Updated**: 2020-04-08 18:30:00+00:00
- **Authors**: Elijah Cole, Benjamin Deneu, Titouan Lorieul, Maximilien Servajean, Christophe Botella, Dan Morris, Nebojsa Jojic, Pierre Bonnet, Alexis Joly
- **Comment**: 10 pages, 4 figures
- **Journal**: None
- **Summary**: Understanding the geographic distribution of species is a key concern in conservation. By pairing species occurrences with environmental features, researchers can model the relationship between an environment and the species which may be found there. To facilitate research in this area, we present the GeoLifeCLEF 2020 dataset, which consists of 1.9 million species observations paired with high-resolution remote sensing imagery, land cover data, and altitude, in addition to traditional low-resolution climate and soil variables. We also discuss the GeoLifeCLEF 2020 competition, which aims to use this dataset to advance the state-of-the-art in location-based species recommendation.



### Transferable, Controllable, and Inconspicuous Adversarial Attacks on Person Re-identification With Deep Mis-Ranking
- **Arxiv ID**: http://arxiv.org/abs/2004.04199v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.04199v1)
- **Published**: 2020-04-08 18:48:29+00:00
- **Updated**: 2020-04-08 18:48:29+00:00
- **Authors**: Hongjun Wang, Guangrun Wang, Ya Li, Dongyu Zhang, Liang Lin
- **Comment**: Accepted in CVPR 2020. To attack ReID, we propose a
  learning-to-mis-rank formulation to perturb the ranking of the system output
- **Journal**: None
- **Summary**: The success of DNNs has driven the extensive applications of person re-identification (ReID) into a new era. However, whether ReID inherits the vulnerability of DNNs remains unexplored. To examine the robustness of ReID systems is rather important because the insecurity of ReID systems may cause severe losses, e.g., the criminals may use the adversarial perturbations to cheat the CCTV systems. In this work, we examine the insecurity of current best-performing ReID models by proposing a learning-to-mis-rank formulation to perturb the ranking of the system output. As the cross-dataset transferability is crucial in the ReID domain, we also perform a back-box attack by developing a novel multi-stage network architecture that pyramids the features of different levels to extract general and transferable features for the adversarial perturbations. Our method can control the number of malicious pixels by using differentiable multi-shot sampling. To guarantee the inconspicuousness of the attack, we also propose a new perception loss to achieve better visual quality. Extensive experiments on four of the largest ReID benchmarks (i.e., Market1501 [45], CUHK03 [18], DukeMTMC [33], and MSMT17 [40]) not only show the effectiveness of our method, but also provides directions of the future improvement in the robustness of ReID systems. For example, the accuracy of one of the best-performing ReID systems drops sharply from 91.8% to 1.4% after being attacked by our method. Some attack results are shown in Fig. 1. The code is available at https://github.com/whj363636/Adversarial-attack-on-Person-ReID-With-Deep-Mis-Ranking.



### A single image deep learning approach to restoration of corrupted remote sensing products
- **Arxiv ID**: http://arxiv.org/abs/2004.04209v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.04209v1)
- **Published**: 2020-04-08 19:11:32+00:00
- **Updated**: 2020-04-08 19:11:32+00:00
- **Authors**: Anna Petrovskaia, Raghavendra B. Jana, Ivan V. Oseledets
- **Comment**: Paper presented at the ICLR 2020 Workshop on Computer Vision for
  Agriculture (CV4A)
- **Journal**: None
- **Summary**: Remote sensing images are used for a variety of analyses, from agricultural monitoring, to disaster relief, to resource planning, among others. The images can be corrupted due to a number of reasons, including instrument errors and natural obstacles such as clouds. We present here a novel approach for reconstruction of missing information in such cases using only the corrupted image as the input. The Deep Image Prior methodology eliminates the need for a pre-trained network or an image database. It is shown that the approach easily beats the performance of traditional single-image methods.



### Deep Manifold Prior
- **Arxiv ID**: http://arxiv.org/abs/2004.04242v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2004.04242v1)
- **Published**: 2020-04-08 20:47:56+00:00
- **Updated**: 2020-04-08 20:47:56+00:00
- **Authors**: Matheus Gadelha, Rui Wang, Subhransu Maji
- **Comment**: 22 pages, 12 figures
- **Journal**: None
- **Summary**: We present a prior for manifold structured data, such as surfaces of 3D shapes, where deep neural networks are adopted to reconstruct a target shape using gradient descent starting from a random initialization. We show that surfaces generated this way are smooth, with limiting behavior characterized by Gaussian processes, and we mathematically derive such properties for fully-connected as well as convolutional networks. We demonstrate our method in a variety of manifold reconstruction applications, such as point cloud denoising and interpolation, achieving considerably better results against competitive baselines while requiring no training data. We also show that when training data is available, our method allows developing alternate parametrizations of surfaces under the framework of AtlasNet, leading to a compact network architecture and better reconstruction results on standard image to shape reconstruction benchmarks.



### Variable Rate Video Compression using a Hybrid Recurrent Convolutional Learning Framework
- **Arxiv ID**: http://arxiv.org/abs/2004.04244v2
- **DOI**: 10.1109/ICCCI48352.2020.9104085
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2004.04244v2)
- **Published**: 2020-04-08 20:49:25+00:00
- **Updated**: 2020-08-21 20:21:24+00:00
- **Authors**: Aishwarya Jadhav
- **Comment**: None
- **Journal**: 2020 International Conference on Computer Communication and
  Informatics (ICCCI)
- **Summary**: In recent years, neural network-based image compression techniques have been able to outperform traditional codecs and have opened the gates for the development of learning-based video codecs. However, to take advantage of the high temporal correlation in videos, more sophisticated architectures need to be employed. This paper presents PredEncoder, a hybrid video compression framework based on the concept of predictive auto-encoding that models the temporal correlations between consecutive video frames using a prediction network which is then combined with a progressive encoder network to exploit the spatial redundancies. A variable-rate block encoding scheme has been proposed in the paper that leads to remarkably high quality to bit-rate ratios. By joint training and fine-tuning of this hybrid architecture, PredEncoder has been able to gain significant improvement over the MPEG-4 codec and has achieved bit-rate savings over the H.264 codec in the low to medium bit-rate range for HD videos and comparable results over most bit-rates for non-HD videos. This paper serves to demonstrate how neural architectures can be leveraged to perform at par with the highly optimized traditional methodologies in the video compression domain.



### GeneCAI: Genetic Evolution for Acquiring Compact AI
- **Arxiv ID**: http://arxiv.org/abs/2004.04249v2
- **DOI**: 10.1145/3377930.3390226
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2004.04249v2)
- **Published**: 2020-04-08 20:56:37+00:00
- **Updated**: 2020-04-14 04:35:42+00:00
- **Authors**: Mojan Javaheripi, Mohammad Samragh, Tara Javidi, Farinaz Koushanfar
- **Comment**: None
- **Journal**: None
- **Summary**: In the contemporary big data realm, Deep Neural Networks (DNNs) are evolving towards more complex architectures to achieve higher inference accuracy. Model compression techniques can be leveraged to efficiently deploy such compute-intensive architectures on resource-limited mobile devices. Such methods comprise various hyper-parameters that require per-layer customization to ensure high accuracy. Choosing such hyper-parameters is cumbersome as the pertinent search space grows exponentially with model layers. This paper introduces GeneCAI, a novel optimization method that automatically learns how to tune per-layer compression hyper-parameters. We devise a bijective translation scheme that encodes compressed DNNs to the genotype space. The optimality of each genotype is measured using a multi-objective score based on accuracy and number of floating point operations. We develop customized genetic operations to iteratively evolve the non-dominated solutions towards the optimal Pareto front, thus, capturing the optimal trade-off between model accuracy and complexity. GeneCAI optimization method is highly scalable and can achieve a near-linear performance boost on distributed multi-GPU platforms. Our extensive evaluations demonstrate that GeneCAI outperforms existing rule-based and reinforcement learning methods in DNN compression by finding models that lie on a better accuracy-complexity Pareto curve.



### Estimating Grape Yield on the Vine from Multiple Images
- **Arxiv ID**: http://arxiv.org/abs/2004.04278v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2004.04278v1)
- **Published**: 2020-04-08 21:58:58+00:00
- **Updated**: 2020-04-08 21:58:58+00:00
- **Authors**: Daniel L. Silver, Jabun Nasa
- **Comment**: Paper presented at the ICLR 2020 Workshop on Computer Vision for
  Agriculture (CV4A), 4 pages, 4 figures
- **Journal**: None
- **Summary**: Estimating grape yield prior to harvest is important to commercial vineyard production as it informs many vineyard and winery decisions. Currently, the process of yield estimation is time consuming and varies in its accuracy from 75-90\% depending on the experience of the viticulturist. This paper proposes a multiple task learning (MTL) convolutional neural network (CNN) approach that uses images captured by inexpensive smart phones secured in a simple tripod arrangement. The CNN models use MTL transfer from autoencoders to achieve 85\% accuracy from image data captured 6 days prior to harvest.



