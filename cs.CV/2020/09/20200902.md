# Arxiv Papers in cs.CV on 2020-09-02
### Monocular 3D Detection with Geometric Constraints Embedding and Semi-supervised Training
- **Arxiv ID**: http://arxiv.org/abs/2009.00764v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.00764v1)
- **Published**: 2020-09-02 00:51:51+00:00
- **Updated**: 2020-09-02 00:51:51+00:00
- **Authors**: Peixuan Li
- **Comment**: 16 pages
- **Journal**: None
- **Summary**: In this work, we propose a novel single-shot and keypoints-based framework for monocular 3D objects detection using only RGB images, called KM3D-Net. We design a fully convolutional model to predict object keypoints, dimension, and orientation, and then combine these estimations with perspective geometry constraints to compute position attribute. Further, we reformulate the geometric constraints as a differentiable version and embed it into the network to reduce running time while maintaining the consistency of model outputs in an end-to-end fashion. Benefiting from this simple structure, we then propose an effective semi-supervised training strategy for the setting where labeled training data is scarce. In this strategy, we enforce a consensus prediction of two shared-weights KM3D-Net for the same unlabeled image under different input augmentation conditions and network regularization. In particular, we unify the coordinate-dependent augmentations as the affine transformation for the differential recovering position of objects and propose a keypoints-dropout module for the network regularization. Our model only requires RGB images without synthetic data, instance segmentation, CAD model, or depth generator. Nevertheless, extensive experiments on the popular KITTI 3D detection dataset indicate that the KM3D-Net surpasses all previous state-of-the-art methods in both efficiency and accuracy by a large margin. And also, to the best of our knowledge, this is the first time that semi-supervised learning is applied in monocular 3D objects detection. We even surpass most of the previous fully supervised methods with only 13\% labeled data on KITTI.



### LSMVOS: Long-Short-Term Similarity Matching for Video Object
- **Arxiv ID**: http://arxiv.org/abs/2009.00771v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.00771v1)
- **Published**: 2020-09-02 01:32:05+00:00
- **Updated**: 2020-09-02 01:32:05+00:00
- **Authors**: Zhang Xuerui, Yuan Xia
- **Comment**: None
- **Journal**: None
- **Summary**: Objective Semi-supervised video object segmentation refers to segmenting the object in subsequent frames given the object label in the first frame. Existing algorithms are mostly based on the objectives of matching and propagation strategies, which often make use of the previous frame with masking or optical flow. This paper explores a new propagation method, uses short-term matching modules to extract the information of the previous frame and apply it in propagation, and proposes the network of Long-Short-Term similarity matching for video object segmentation (LSMOVS) Method: By conducting pixel-level matching and correlation between long-term matching module and short-term matching module with the first frame and previous frame, global similarity map and local similarity map are obtained, as well as feature pattern of current frame and masking of previous frame. After two refine networks, final results are obtained through segmentation network. Results: According to the experiments on the two data sets DAVIS 2016 and 2017, the method of this paper achieves favorable average of region similarity and contour accuracy without online fine tuning, which achieves 86.5% and 77.4% in terms of single target and multiple targets. Besides, the count of segmented frames per second reached 21. Conclusion: The short-term matching module proposed in this paper is more conducive to extracting the information of the previous frame than only the mask. By combining the long-term matching module with the short-term matching module, the whole network can achieve efficient video object segmentation without online fine tuning



### A perception centred self-driving system without HD Maps
- **Arxiv ID**: http://arxiv.org/abs/2009.00782v2
- **DOI**: 10.14569/IJACSA.2020.0111081
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.00782v2)
- **Published**: 2020-09-02 02:06:29+00:00
- **Updated**: 2020-09-11 02:13:27+00:00
- **Authors**: Alan Sun
- **Comment**: None
- **Journal**: None
- **Summary**: Building a fully autonomous self-driving system has been discussed for more than 20 years yet remains unsolved. Previous systems have limited ability to scale. Their localization subsystem needs labor-intensive map recording for running in a new area, and the accuracy decreases after the changes occur in the environment. In this paper, a new localization method is proposed to solve the scalability problems, with a new method for detecting and making sense of diverse traffic lines. Like the way human drives, a self-driving system should not rely on an exact position to travel in most scenarios. As a result, without HD Maps, GPS or IMU, the proposed localization subsystem relies only on detecting driving-related features around (like lane lines, stop lines, and merging lane lines). For spotting and reasoning all these features, a new line detector is proposed and tested against multiple datasets.



### CLOCs: Camera-LiDAR Object Candidates Fusion for 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2009.00784v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.00784v1)
- **Published**: 2020-09-02 02:07:00+00:00
- **Updated**: 2020-09-02 02:07:00+00:00
- **Authors**: Su Pang, Daniel Morris, Hayder Radha
- **Comment**: None
- **Journal**: None
- **Summary**: There have been significant advances in neural networks for both 3D object detection using LiDAR and 2D object detection using video. However, it has been surprisingly difficult to train networks to effectively use both modalities in a way that demonstrates gain over single-modality networks. In this paper, we propose a novel Camera-LiDAR Object Candidates (CLOCs) fusion network. CLOCs fusion provides a low-complexity multi-modal fusion framework that significantly improves the performance of single-modality detectors. CLOCs operates on the combined output candidates before Non-Maximum Suppression (NMS) of any 2D and any 3D detector, and is trained to leverage their geometric and semantic consistencies to produce more accurate final 3D and 2D detection results. Our experimental evaluation on the challenging KITTI object detection benchmark, including 3D and bird's eye view metrics, shows significant improvements, especially at long distance, over the state-of-the-art fusion based methods. At time of submission, CLOCs ranks the highest among all the fusion-based methods in the official KITTI leaderboard. We will release our code upon acceptance.



### Dual Precision Deep Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2009.02191v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.02191v1)
- **Published**: 2020-09-02 02:56:51+00:00
- **Updated**: 2020-09-02 02:56:51+00:00
- **Authors**: Jae Hyun Park, Ji Sub Choi, Jong Hwan Ko
- **Comment**: 5 pages, 4 figures, 2 tables
- **Journal**: None
- **Summary**: On-line Precision scalability of the deep neural networks(DNNs) is a critical feature to support accuracy and complexity trade-off during the DNN inference. In this paper, we propose dual-precision DNN that includes two different precision modes in a single model, thereby supporting an on-line precision switch without re-training. The proposed two-phase training process optimizes both low- and high-precision modes.



### Estimating the Brittleness of AI: Safety Integrity Levels and the Need for Testing Out-Of-Distribution Performance
- **Arxiv ID**: http://arxiv.org/abs/2009.00802v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.CY, cs.SE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2009.00802v1)
- **Published**: 2020-09-02 03:33:40+00:00
- **Updated**: 2020-09-02 03:33:40+00:00
- **Authors**: Andrew J. Lohn
- **Comment**: None
- **Journal**: None
- **Summary**: Test, Evaluation, Verification, and Validation (TEVV) for Artificial Intelligence (AI) is a challenge that threatens to limit the economic and societal rewards that AI researchers have devoted themselves to producing. A central task of TEVV for AI is estimating brittleness, where brittleness implies that the system functions well within some bounds and poorly outside of those bounds. This paper argues that neither of those criteria are certain of Deep Neural Networks. First, highly touted AI successes (eg. image classification and speech recognition) are orders of magnitude more failure-prone than are typically certified in critical systems even within design bounds (perfectly in-distribution sampling). Second, performance falls off only gradually as inputs become further Out-Of-Distribution (OOD). Enhanced emphasis is needed on designing systems that are resilient despite failure-prone AI components as well as on evaluating and improving OOD performance in order to get AI to where it can clear the challenging hurdles of TEVV and certification.



### Open-set Adversarial Defense
- **Arxiv ID**: http://arxiv.org/abs/2009.00814v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2009.00814v1)
- **Published**: 2020-09-02 04:35:33+00:00
- **Updated**: 2020-09-02 04:35:33+00:00
- **Authors**: Rui Shao, Pramuditha Perera, Pong C. Yuen, Vishal M. Patel
- **Comment**: Accepted by ECCV 2020
- **Journal**: None
- **Summary**: Open-set recognition and adversarial defense study two key aspects of deep learning that are vital for real-world deployment. The objective of open-set recognition is to identify samples from open-set classes during testing, while adversarial defense aims to defend the network against images with imperceptible adversarial perturbations. In this paper, we show that open-set recognition systems are vulnerable to adversarial attacks. Furthermore, we show that adversarial defense mechanisms trained on known classes do not generalize well to open-set samples. Motivated by this observation, we emphasize the need of an Open-Set Adversarial Defense (OSAD) mechanism. This paper proposes an Open-Set Defense Network (OSDN) as a solution to the OSAD problem. The proposed network uses an encoder with feature-denoising layers coupled with a classifier to learn a noise-free latent feature representation. Two techniques are employed to obtain an informative latent feature space with the objective of improving open-set performance. First, a decoder is used to ensure that clean images can be reconstructed from the obtained latent features. Then, self-supervision is used to ensure that the latent features are informative enough to carry out an auxiliary task. We introduce a testing protocol to evaluate OSAD performance and show the effectiveness of the proposed method in multiple object classification datasets. The implementation code of the proposed method is available at: https://github.com/rshaojimmy/ECCV2020-OSAD.



### On the Structures of Representation for the Robustness of Semantic Segmentation to Input Corruption
- **Arxiv ID**: http://arxiv.org/abs/2009.00817v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2009.00817v1)
- **Published**: 2020-09-02 04:49:27+00:00
- **Updated**: 2020-09-02 04:49:27+00:00
- **Authors**: Charles Lehman, Dogancan Temel, Ghassan AlRegib
- **Comment**: None
- **Journal**: None
- **Summary**: Semantic segmentation is a scene understanding task at the heart of safety-critical applications where robustness to corrupted inputs is essential. Implicit Background Estimation (IBE) has demonstrated to be a promising technique to improve the robustness to out-of-distribution inputs for semantic segmentation models for little to no cost. In this paper, we provide analysis comparing the structures learned as a result of optimization objectives that use Softmax, IBE, and Sigmoid in order to improve understanding their relationship to robustness. As a result of this analysis, we propose combining Sigmoid with IBE (SCrIBE) to improve robustness. Finally, we demonstrate that SCrIBE exhibits superior segmentation performance aggregated across all corruptions and severity levels with a mIOU of 42.1 compared to both IBE 40.3 and the Softmax Baseline 37.5.



### A new heuristic algorithm for fast k-segmentation
- **Arxiv ID**: http://arxiv.org/abs/2009.05148v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2009.05148v1)
- **Published**: 2020-09-02 04:50:17+00:00
- **Updated**: 2020-09-02 04:50:17+00:00
- **Authors**: Sabarish Vadarevu, Vijay Karamcheti
- **Comment**: 10 pages, 10 figures, 5 tables, and 1 pseudo-code. Submitted to IEEE
  BigData 2020. Supplementary material (200 segmented videos) at
  https://figshare.com/articles/media/7-segmentation of 200 scenes from
  BDD100k/12859493/1
- **Journal**: None
- **Summary**: The $k$-segmentation of a video stream is used to partition it into $k$ piecewise-linear segments, so that each linear segment has a meaningful interpretation. Such segmentation may be used to summarize large videos using a small set of images, to identify anomalies within segments and change points between segments, and to select critical subsets for training machine learning models. Exact and approximate segmentation methods for $k$-segmentation exist in the literature. Each of these algorithms occupies a different spot in the trade-off between computational complexity and accuracy. A novel heuristic algorithm is proposed in this paper to improve upon existing methods. It is empirically found to provide accuracies competitive with exact methods at a fraction of the computational expense.   The new algorithm is inspired by Lloyd's algorithm for K-Means and Lloyd-Max algorithm for scalar quantization, and is called the LM algorithm for convenience. It works by iteratively minimizing a cost function from any given initialisation; the commonly used $L_2$ cost is chosen in this paper. While the greedy minimization makes the algorithm sensitive to initialisation, the ability to converge from any initial guess to a local optimum allows the algorithm to be integrated into other existing algorithms. Three variants of the algorithm are tested over a large number of synthetic datasets, one being a standalone LM implementation, and two others that combine with existing algorithms. One of the latter two -- LM-enhanced-Bottom-Up segmentation -- is found to have the best accuracy and the lowest computational complexity among all algorithms. This variant of LM can provide $k$-segmentations over data sets with up to a million image frames within several seconds.



### Tangent Space Based Alternating Projections for Nonnegative Low Rank Matrix Approximation
- **Arxiv ID**: http://arxiv.org/abs/2009.03998v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2009.03998v1)
- **Published**: 2020-09-02 05:25:16+00:00
- **Updated**: 2020-09-02 05:25:16+00:00
- **Authors**: Guangjing Song, Michael K. Ng, Tai-Xiang Jiang
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we develop a new alternating projection method to compute nonnegative low rank matrix approximation for nonnegative matrices. In the nonnegative low rank matrix approximation method, the projection onto the manifold of fixed rank matrices can be expensive as the singular value decomposition is required. We propose to use the tangent space of the point in the manifold to approximate the projection onto the manifold in order to reduce the computational cost. We show that the sequence generated by the alternating projections onto the tangent spaces of the fixed rank matrices manifold and the nonnegative matrix manifold, converge linearly to a point in the intersection of the two manifolds where the convergent point is sufficiently close to optimal solutions. This convergence result based inexact projection onto the manifold is new and is not studied in the literature. Numerical examples in data clustering, pattern recognition and hyperspectral data analysis are given to demonstrate that the performance of the proposed method is better than that of nonnegative matrix factorization methods in terms of computational time and accuracy.



### Convolutional Nonlinear Dictionary with Cascaded Structure Filter Banks
- **Arxiv ID**: http://arxiv.org/abs/2009.00831v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4.4
- **Links**: [PDF](http://arxiv.org/pdf/2009.00831v1)
- **Published**: 2020-09-02 05:40:01+00:00
- **Updated**: 2020-09-02 05:40:01+00:00
- **Authors**: Ruiki Kobayashi, Shogo Muramatsu
- **Comment**: 4 pages, 5 figures
- **Journal**: None
- **Summary**: This study proposes a convolutional nonlinear dictionary (CNLD) for image restoration using cascaded filter banks. Generally, convolutional neural networks (CNN) demonstrate their practicality in image restoration applications; however, existing CNNs are constructed without considering the relationship among atomic images (convolution kernels). As a result, there remains room for discussing the role of design spaces. To provide a framework for constructing an effective and structured convolutional network, this study proposes the CNLD. The backpropagation learning procedure is derived from certain image restoration experiments, and thereby the significance of CNLD is verified. It is demonstrated that the number of parameters is reduced while preserving the restoration performance.



### Intrinsic Relationship Reasoning for Small Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2009.00833v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.00833v1)
- **Published**: 2020-09-02 06:03:05+00:00
- **Updated**: 2020-09-02 06:03:05+00:00
- **Authors**: Kui Fu, Jia Li, Lin Ma, Kai Mu, Yonghong Tian
- **Comment**: None
- **Journal**: None
- **Summary**: The small objects in images and videos are usually not independent individuals. Instead, they more or less present some semantic and spatial layout relationships with each other. Modeling and inferring such intrinsic relationships can thereby be beneficial for small object detection. In this paper, we propose a novel context reasoning approach for small object detection which models and infers the intrinsic semantic and spatial layout relationships between objects. Specifically, we first construct a semantic module to model the sparse semantic relationships based on the initial regional features, and a spatial layout module to model the sparse spatial layout relationships based on their position and shape information, respectively. Both of them are then fed into a context reasoning module for integrating the contextual information with respect to the objects and their relationships, which is further fused with the original regional visual features for classification and regression. Experimental results reveal that the proposed approach can effectively boost the small object detection performance.



### Retaining Image Feature Matching Performance Under Low Light Conditions
- **Arxiv ID**: http://arxiv.org/abs/2009.00842v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.00842v1)
- **Published**: 2020-09-02 06:44:45+00:00
- **Updated**: 2020-09-02 06:44:45+00:00
- **Authors**: Pranjay Shyam, Antyanta Bangunharcana, Kyung-Soo Kim
- **Comment**: Accepted in ICCAS 2020 - 20th International Conference on Control,
  Robotics, and Systems
- **Journal**: None
- **Summary**: Poor image quality in low light images may result in a reduced number of feature matching between images. In this paper, we investigate the performance of feature extraction algorithms in low light environments. To find an optimal setting to retain feature matching performance in low light images, we look into the effect of changing feature acceptance threshold for feature detector and adding pre-processing in the form of Low Light Image Enhancement (LLIE) prior to feature detection. We observe that even in low light images, feature matching using traditional hand-crafted feature detectors still performs reasonably well by lowering the threshold parameter. We also show that applying Low Light Image Enhancement (LLIE) algorithms can improve feature matching even more when paired with the right feature extraction algorithm.



### e-TLD: Event-based Framework for Dynamic Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/2009.00855v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.00855v1)
- **Published**: 2020-09-02 07:08:56+00:00
- **Updated**: 2020-09-02 07:08:56+00:00
- **Authors**: Bharath Ramesh, Shihao Zhang, Hong Yang, Andres Ussa, Matthew Ong, Garrick Orchard, Cheng Xiang
- **Comment**: 11 pages, 10 figures
- **Journal**: None
- **Summary**: This paper presents a long-term object tracking framework with a moving event camera under general tracking conditions. A first of its kind for these revolutionary cameras, the tracking framework uses a discriminative representation for the object with online learning, and detects and re-tracks the object when it comes back into the field-of-view. One of the key novelties is the use of an event-based local sliding window technique that tracks reliably in scenes with cluttered and textured background. In addition, Bayesian bootstrapping is used to assist real-time processing and boost the discriminative power of the object representation. On the other hand, when the object re-enters the field-of-view of the camera, a data-driven, global sliding window detector locates the object for subsequent tracking. Extensive experiments demonstrate the ability of the proposed framework to track and detect arbitrary objects of various shapes and sizes, including dynamic objects such as a human. This is a significant improvement compared to earlier works that simply track objects as long as they are visible under simpler background settings. Using the ground truth locations for five different objects under three motion settings, namely translation, rotation and 6-DOF, quantitative measurement is reported for the event-based tracking framework with critical insights on various performance issues. Finally, real-time implementation in C++ highlights tracking ability under scale, rotation, view-point and occlusion scenarios in a lab setting.



### Breast mass detection in digital mammography based on anchor-free architecture
- **Arxiv ID**: http://arxiv.org/abs/2009.00857v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.00857v1)
- **Published**: 2020-09-02 07:11:16+00:00
- **Updated**: 2020-09-02 07:11:16+00:00
- **Authors**: Haichao Cao
- **Comment**: 26 pages, 12 figures
- **Journal**: None
- **Summary**: Background and Objective: Accurate detection of breast masses in mammography images is critical to diagnose early breast cancer, which can greatly improve the patients survival rate. However, it is still a big challenge due to the heterogeneity of breast masses and the complexity of their surrounding environment.Methods: To address these problems, we propose a one-stage object detection architecture, called Breast Mass Detection Network (BMassDNet), based on anchor-free and feature pyramid which makes the detection of breast masses of different sizes well adapted. We introduce a truncation normalization method and combine it with adaptive histogram equalization to enhance the contrast between the breast mass and the surrounding environment. Meanwhile, to solve the overfitting problem caused by small data size, we propose a natural deformation data augmentation method and mend the train data dynamic updating method based on the data complexity to effectively utilize the limited data. Finally, we use transfer learning to assist the training process and to improve the robustness of the model ulteriorly.Results: On the INbreast dataset, each image has an average of 0.495 false positives whilst the recall rate is 0.930; On the DDSM dataset, when each image has 0.599 false positives, the recall rate reaches 0.943.Conclusions: The experimental results on datasets INbreast and DDSM show that the proposed BMassDNet can obtain competitive detection performance over the current top ranked methods.



### ALEX: Active Learning based Enhancement of a Model's Explainability
- **Arxiv ID**: http://arxiv.org/abs/2009.00859v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2009.00859v1)
- **Published**: 2020-09-02 07:15:39+00:00
- **Updated**: 2020-09-02 07:15:39+00:00
- **Authors**: Ishani Mondal, Debasis Ganguly
- **Comment**: CIKM 2020
- **Journal**: None
- **Summary**: An active learning (AL) algorithm seeks to construct an effective classifier with a minimal number of labeled examples in a bootstrapping manner. While standard AL heuristics, such as selecting those points for annotation for which a classification model yields least confident predictions, there has been no empirical investigation to see if these heuristics lead to models that are more interpretable to humans. In the era of data-driven learning, this is an important research direction to pursue. This paper describes our work-in-progress towards developing an AL selection function that in addition to model effectiveness also seeks to improve on the interpretability of a model during the bootstrapping steps. Concretely speaking, our proposed selection function trains an `explainer' model in addition to the classifier model, and favours those instances where a different part of the data is used, on an average, to explain the predicted class. Initial experiments exhibited encouraging trends in showing that such a heuristic can lead to developing more effective and more explainable end-to-end data-driven classifiers.



### Efficient, high-performance pancreatic segmentation using multi-scale feature extraction
- **Arxiv ID**: http://arxiv.org/abs/2009.00872v2
- **DOI**: 10.1371/journal.pone.0255397
- **Categories**: **eess.IV**, cs.CV, cs.LG, I.4.6; J.3
- **Links**: [PDF](http://arxiv.org/pdf/2009.00872v2)
- **Published**: 2020-09-02 07:47:39+00:00
- **Updated**: 2021-01-12 14:24:18+00:00
- **Authors**: Moritz Knolle, Georgios Kaissis, Friederike Jungmann, Sebastian Ziegelmayer, Daniel Sasse, Marcus Makowski, Daniel Rueckert, Rickmer Braren
- **Comment**: None
- **Journal**: None
- **Summary**: For artificial intelligence-based image analysis methods to reach clinical applicability, the development of high-performance algorithms is crucial. For example, existent segmentation algorithms based on natural images are neither efficient in their parameter use nor optimized for medical imaging. Here we present MoNet, a highly optimized neural-network-based pancreatic segmentation algorithm focused on achieving high performance by efficient multi-scale image feature utilization.



### GAIT: Gradient Adjusted Unsupervised Image-to-Image Translation
- **Arxiv ID**: http://arxiv.org/abs/2009.00878v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2009.00878v1)
- **Published**: 2020-09-02 08:04:00+00:00
- **Updated**: 2020-09-02 08:04:00+00:00
- **Authors**: Ibrahim Batuhan Akkaya, Ugur Halici
- **Comment**: Accepted by ICIP2020
- **Journal**: None
- **Summary**: Image-to-image translation (IIT) has made much progress recently with the development of adversarial learning. In most of the recent work, an adversarial loss is utilized to match the distributions of the translated and target image sets. However, this may create artifacts if two domains have different marginal distributions, for example, in uniform areas. In this work, we propose an unsupervised IIT method that preserves the uniform regions after the translation. The gradient adjustment loss, which is the L2 norm between the Sobel response of the target image and the adjusted Sobel response of the source images, is utilized. The proposed method is validated on the jellyfish-to-Haeckel dataset, which is prepared to demonstrate the mentioned problem, which contains images with different background distributions. We demonstrate that our method obtained a performance gain compared to the baseline method qualitatively and quantitatively, showing the effectiveness of the proposed method.



### PCPL: Predicate-Correlation Perception Learning for Unbiased Scene Graph Generation
- **Arxiv ID**: http://arxiv.org/abs/2009.00893v1
- **DOI**: 10.1145/3394171.3413722
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.00893v1)
- **Published**: 2020-09-02 08:30:09+00:00
- **Updated**: 2020-09-02 08:30:09+00:00
- **Authors**: Shaotian Yan, Chen Shen, Zhongming Jin, Jianqiang Huang, Rongxin Jiang, Yaowu Chen, Xian-Sheng Hua
- **Comment**: To be appeared on ACMMM 2020
- **Journal**: None
- **Summary**: Today, scene graph generation(SGG) task is largely limited in realistic scenarios, mainly due to the extremely long-tailed bias of predicate annotation distribution. Thus, tackling the class imbalance trouble of SGG is critical and challenging. In this paper, we first discover that when predicate labels have strong correlation with each other, prevalent re-balancing strategies(e.g., re-sampling and re-weighting) will give rise to either over-fitting the tail data(e.g., bench sitting on sidewalk rather than on), or still suffering the adverse effect from the original uneven distribution(e.g., aggregating varied parked on/standing on/sitting on into on). We argue the principal reason is that re-balancing strategies are sensitive to the frequencies of predicates yet blind to their relatedness, which may play a more important role to promote the learning of predicate features. Therefore, we propose a novel Predicate-Correlation Perception Learning(PCPL for short) scheme to adaptively seek out appropriate loss weights by directly perceiving and utilizing the correlation among predicate classes. Moreover, our PCPL framework is further equipped with a graph encoder module to better extract context features. Extensive experiments on the benchmark VG150 dataset show that the proposed PCPL performs markedly better on tail classes while well-preserving the performance on head ones, which significantly outperforms previous state-of-the-art methods.



### Multi-domain semantic segmentation with pyramidal fusion
- **Arxiv ID**: http://arxiv.org/abs/2009.01636v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.01636v5)
- **Published**: 2020-09-02 08:37:14+00:00
- **Updated**: 2021-10-07 13:55:02+00:00
- **Authors**: Petra Bevandić, Marin Oršić, Ivan Grubišić, Josip Šarić, Siniša Šegvić
- **Comment**: 2 pages, 2 tables, no figures
- **Journal**: None
- **Summary**: We present our submission to the semantic segmentation contest of the Robust Vision Challenge held at ECCV 2020. The contest requires submitting the same model to seven benchmarks from three different domains. Our approach is based on the SwiftNet architecture with pyramidal fusion. We address inconsistent taxonomies with a single-level 193-dimensional softmax output. We strive to train with large batches in order to stabilize optimization of a hard recognition problem, and to favour smooth evolution of batchnorm statistics. We achieve this by implementing a custom backward step through log-sum-prob loss, and by using small crops before freezing the population statistics. Our model ranks first on the RVC semantic segmentation challenge as well as on the WildDash 2 leaderboard. This suggests that pyramidal fusion is competitive not only for efficient inference with lightweight backbones, but also in large-scale setups for multi-domain application.



### Adversarially Robust Neural Architectures
- **Arxiv ID**: http://arxiv.org/abs/2009.00902v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.00902v2)
- **Published**: 2020-09-02 08:52:15+00:00
- **Updated**: 2023-02-02 02:52:04+00:00
- **Authors**: Minjing Dong, Yanxi Li, Yunhe Wang, Chang Xu
- **Comment**: 13 pages, 5 figures, 8 tables
- **Journal**: None
- **Summary**: Deep Neural Networks (DNNs) are vulnerable to adversarial attacks. Existing methods are devoted to developing various robust training strategies or regularizations to update the weights of the neural network. But beyond the weights, the overall structure and information flow in the network are explicitly determined by the neural architecture, which remains unexplored. This paper thus aims to improve the adversarial robustness of the network from the architecture perspective. We explore the relationship among adversarial robustness, Lipschitz constant, and architecture parameters and show that an appropriate constraint on architecture parameters could reduce the Lipschitz constant to further improve the robustness. The importance of architecture parameters could vary from operation to operation or connection to connection. We approximate the Lipschitz constant of the entire network through a univariate log-normal distribution, whose mean and variance are related to architecture parameters. The confidence can be fulfilled through formulating a constraint on the distribution parameters based on the cumulative function. Compared with adversarially trained neural architectures searched by various NAS algorithms as well as efficient human-designed models, our algorithm empirically achieves the best performance among all the models under various attacks on different datasets.



### Neural Crossbreed: Neural Based Image Metamorphosis
- **Arxiv ID**: http://arxiv.org/abs/2009.00905v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2009.00905v1)
- **Published**: 2020-09-02 08:56:47+00:00
- **Updated**: 2020-09-02 08:56:47+00:00
- **Authors**: Sanghun Park, Kwanggyoon Seo, Junyong Noh
- **Comment**: 16 pages
- **Journal**: ACM Transactions on Graphics (Proceeding of SIGGRAPH Asia), 2020
- **Summary**: We propose Neural Crossbreed, a feed-forward neural network that can learn a semantic change of input images in a latent space to create the morphing effect. Because the network learns a semantic change, a sequence of meaningful intermediate images can be generated without requiring the user to specify explicit correspondences. In addition, the semantic change learning makes it possible to perform the morphing between the images that contain objects with significantly different poses or camera views. Furthermore, just as in conventional morphing techniques, our morphing network can handle shape and appearance transitions separately by disentangling the content and the style transfer for rich usability. We prepare a training dataset for morphing using a pre-trained BigGAN, which generates an intermediate image by interpolating two latent vectors at an intended morphing value. This is the first attempt to address image morphing using a pre-trained generative model in order to learn semantic transformation. The experiments show that Neural Crossbreed produces high quality morphed images, overcoming various limitations associated with conventional approaches. In addition, Neural Crossbreed can be further extended for diverse applications such as multi-image morphing, appearance transfer, and video frame interpolation.



### DARWIN: A Highly Flexible Platform for Imaging Research in Radiology
- **Arxiv ID**: http://arxiv.org/abs/2009.00908v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.00908v1)
- **Published**: 2020-09-02 09:19:40+00:00
- **Updated**: 2020-09-02 09:19:40+00:00
- **Authors**: Lufan Chang, Wenjing Zhuang, Richeng Wu, Sai Feng, Hao Liu, Jing Yu, Jia Ding, Ziteng Wang, Jiaqi Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: To conduct a radiomics or deep learning research experiment, the radiologists or physicians need to grasp the needed programming skills, which, however, could be frustrating and costly when they have limited coding experience. In this paper, we present DARWIN, a flexible research platform with a graphical user interface for medical imaging research. Our platform is consists of a radiomics module and a deep learning module. The radiomics module can extract more than 1000 dimension features(first-, second-, and higher-order) and provided many draggable supervised and unsupervised machine learning models. Our deep learning module integrates state of the art architectures of classification, detection, and segmentation tasks. It allows users to manually select hyperparameters, or choose an algorithm to automatically search for the best ones. DARWIN also offers the possibility for users to define a custom pipeline for their experiment. These flexibilities enable radiologists to carry out various experiments easily.



### A Survey on Negative Transfer
- **Arxiv ID**: http://arxiv.org/abs/2009.00909v4
- **DOI**: 10.1109/JAS.2022.106004
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2009.00909v4)
- **Published**: 2020-09-02 09:20:20+00:00
- **Updated**: 2021-08-09 12:38:51+00:00
- **Authors**: Wen Zhang, Lingfei Deng, Lei Zhang, Dongrui Wu
- **Comment**: None
- **Journal**: IEEE/CAA Journal of Automatica Sinica, 2022, 1-25
- **Summary**: Transfer learning (TL) utilizes data or knowledge from one or more source domains to facilitate the learning in a target domain. It is particularly useful when the target domain has very few or no labeled data, due to annotation expense, privacy concerns, etc. Unfortunately, the effectiveness of TL is not always guaranteed. Negative transfer (NT), i.e., leveraging source domain data/knowledge undesirably reduces the learning performance in the target domain, has been a long-standing and challenging problem in TL. Various approaches have been proposed in the literature to handle it. However, there does not exist a systematic survey on the formulation of NT, the factors leading to NT, and the algorithms that mitigate NT. This paper fills this gap, by first introducing the definition of NT and its factors, then reviewing about fifty representative approaches for overcoming NT, according to four categories: secure transfer, domain similarity estimation, distant transfer, and NT mitigation. NT in related fields, e.g., multi-task learning, lifelong learning, and adversarial attacks, are also discussed.



### Continual Prototype Evolution: Learning Online from Non-Stationary Data Streams
- **Arxiv ID**: http://arxiv.org/abs/2009.00919v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2009.00919v4)
- **Published**: 2020-09-02 09:39:26+00:00
- **Updated**: 2021-04-06 10:40:42+00:00
- **Authors**: Matthias De Lange, Tinne Tuytelaars
- **Comment**: 10 pages, code publicly available
- **Journal**: Proceedings of the IEEE/CVF International Conference on Computer
  Vision (ICCV), 2021, pp. 8250-8259
- **Summary**: Attaining prototypical features to represent class distributions is well established in representation learning. However, learning prototypes online from streaming data proves a challenging endeavor as they rapidly become outdated, caused by an ever-changing parameter space during the learning process. Additionally, continual learning does not assume the data stream to be stationary, typically resulting in catastrophic forgetting of previous knowledge. As a first, we introduce a system addressing both problems, where prototypes evolve continually in a shared latent space, enabling learning and prediction at any point in time. In contrast to the major body of work in continual learning, data streams are processed in an online fashion, without additional task-information, and an efficient memory scheme provides robustness to imbalanced data streams. Besides nearest neighbor based prediction, learning is facilitated by a novel objective function, encouraging cluster density about the class prototype and increased inter-class variance. Furthermore, the latent space quality is elevated by pseudo-prototypes in each batch, constituted by replay of exemplars from memory. As an additional contribution, we generalize the existing paradigms in continual learning to incorporate data incremental learning from data streams by formalizing a two-agent learner-evaluator framework. We obtain state-of-the-art performance by a significant margin on eight benchmarks, including three highly imbalanced data streams.



### Going beyond Free Viewpoint: Creating Animatable Volumetric Video of Human Performances
- **Arxiv ID**: http://arxiv.org/abs/2009.00922v1
- **DOI**: 10.1049/iet-cvi.2019.0786
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.00922v1)
- **Published**: 2020-09-02 09:46:12+00:00
- **Updated**: 2020-09-02 09:46:12+00:00
- **Authors**: Anna Hilsmann, Philipp Fechteler, Wieland Morgenstern, Wolfgang Paier, Ingo Feldmann, Oliver Schreer, Peter Eisert
- **Comment**: None
- **Journal**: ET Computer Vision, Special Issue on Computer Vision for the
  Creative Industries (2020)
- **Summary**: In this paper, we present an end-to-end pipeline for the creation of high-quality animatable volumetric video content of human performances. Going beyond the application of free-viewpoint volumetric video, we allow re-animation and alteration of an actor's performance through (i) the enrichment of the captured data with semantics and animation properties and (ii) applying hybrid geometry- and video-based animation methods that allow a direct animation of the high-quality data itself instead of creating an animatable model that resembles the captured data. Semantic enrichment and geometric animation ability are achieved by establishing temporal consistency in the 3D data, followed by an automatic rigging of each frame using a parametric shape-adaptive full human body model. Our hybrid geometry- and video-based animation approaches combine the flexibility of classical CG animation with the realism of real captured data. For pose editing, we exploit the captured data as much as possible and kinematically deform the captured frames to fit a desired pose. Further, we treat the face differently from the body in a hybrid geometry- and video-based animation approach where coarse movements and poses are modeled in the geometry only, while very fine and subtle details in the face, often lacking in purely geometric methods, are captured in video-based textures. These are processed to be interactively combined to form new facial expressions. On top of that, we learn the appearance of regions that are challenging to synthesize, such as the teeth or the eyes, and fill in missing regions realistically in an autoencoder-based approach. This paper covers the full pipeline from capturing and producing high-quality video content, over the enrichment with semantics and deformation properties for re-animation and processing of the data for the final hybrid animation.



### Deep Learning to Detect Bacterial Colonies for the Production of Vaccines
- **Arxiv ID**: http://arxiv.org/abs/2009.00926v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2009.00926v1)
- **Published**: 2020-09-02 10:10:43+00:00
- **Updated**: 2020-09-02 10:10:43+00:00
- **Authors**: Thomas Beznik, Paul Smyth, Gaël de Lannoy, John A. Lee
- **Comment**: 6 pages, 2 figures, accepted at ESANN 2020 (European Symposium on
  Artificial Neural Networks, Computational Intelligence and Machine Learning)
- **Journal**: None
- **Summary**: During the development of vaccines, bacterial colony forming units (CFUs) are counted in order to quantify the yield in the fermentation process. This manual task is time-consuming and error-prone. In this work we test multiple segmentation algorithms based on the U-Net CNN architecture and show that these offer robust, automated CFU counting. We show that the multiclass generalisation with a bespoke loss function allows distinguishing virulent and avirulent colonies with acceptable accuracy. While many possibilities are left to explore, our results show the potential of deep learning for separating and classifying bacterial colonies.



### Real-time 3D Facial Tracking via Cascaded Compositional Learning
- **Arxiv ID**: http://arxiv.org/abs/2009.00935v1
- **DOI**: 10.1109/TIP.2021.3065819
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.00935v1)
- **Published**: 2020-09-02 10:27:36+00:00
- **Updated**: 2020-09-02 10:27:36+00:00
- **Authors**: Jianwen Lou, Xiaoxu Cai, Junyu Dong, Hui Yu
- **Comment**: None
- **Journal**: None
- **Summary**: We propose to learn a cascade of globally-optimized modular boosted ferns (GoMBF) to solve multi-modal facial motion regression for real-time 3D facial tracking from a monocular RGB camera. GoMBF is a deep composition of multiple regression models with each is a boosted ferns initially trained to predict partial motion parameters of the same modality, and then concatenated together via a global optimization step to form a singular strong boosted ferns that can effectively handle the whole regression target. It can explicitly cope with the modality variety in output variables, while manifesting increased fitting power and a faster learning speed comparing against the conventional boosted ferns. By further cascading a sequence of GoMBFs (GoMBF-Cascade) to regress facial motion parameters, we achieve competitive tracking performance on a variety of in-the-wild videos comparing to the state-of-the-art methods, which require much more training data or have higher computational complexity. It provides a robust and highly elegant solution to real-time 3D facial tracking using a small set of training data and hence makes it more practical in real-world applications.



### 3D Facial Geometry Recovery from a Depth View with Attention Guided Generative Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/2009.00938v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.00938v1)
- **Published**: 2020-09-02 10:35:26+00:00
- **Updated**: 2020-09-02 10:35:26+00:00
- **Authors**: Xiaoxu Cai, Hui Yu, Jianwen Lou, Xuguang Zhang, Gongfa Li, Junyu Dong
- **Comment**: None
- **Journal**: None
- **Summary**: We present to recover the complete 3D facial geometry from a single depth view by proposing an Attention Guided Generative Adversarial Networks (AGGAN). In contrast to existing work which normally requires two or more depth views to recover a full 3D facial geometry, the proposed AGGAN is able to generate a dense 3D voxel grid of the face from a single unconstrained depth view. Specifically, AGGAN encodes the 3D facial geometry within a voxel space and utilizes an attention-guided GAN to model the illposed 2.5D depth-3D mapping. Multiple loss functions, which enforce the 3D facial geometry consistency, together with a prior distribution of facial surface points in voxel space are incorporated to guide the training process. Both qualitative and quantitative comparisons show that AGGAN recovers a more complete and smoother 3D facial shape, with the capability to handle a much wider range of view angles and resist to noise in the depth view than conventional methods



### Structure-Aware Generation Network for Recipe Generation from Images
- **Arxiv ID**: http://arxiv.org/abs/2009.00944v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.00944v1)
- **Published**: 2020-09-02 10:54:25+00:00
- **Updated**: 2020-09-02 10:54:25+00:00
- **Authors**: Hao Wang, Guosheng Lin, Steven C. H. Hoi, Chunyan Miao
- **Comment**: Published at ECCV 2020
- **Journal**: None
- **Summary**: Sharing food has become very popular with the development of social media. For many real-world applications, people are keen to know the underlying recipes of a food item. In this paper, we are interested in automatically generating cooking instructions for food. We investigate an open research task of generating cooking instructions based on only food images and ingredients, which is similar to the image captioning task. However, compared with image captioning datasets, the target recipes are long-length paragraphs and do not have annotations on structure information. To address the above limitations, we propose a novel framework of Structure-aware Generation Network (SGN) to tackle the food recipe generation task. Our approach brings together several novel ideas in a systematic framework: (1) exploiting an unsupervised learning approach to obtain the sentence-level tree structure labels before training; (2) generating trees of target recipes from images with the supervision of tree structure labels learned from (1); and (3) integrating the inferred tree structures with the recipe generation procedure. Our proposed model can produce high-quality and coherent recipes, and achieve the state-of-the-art performance on the benchmark Recipe1M dataset.



### Mutual Teaching for Graph Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/2009.00952v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2009.00952v1)
- **Published**: 2020-09-02 11:10:55+00:00
- **Updated**: 2020-09-02 11:10:55+00:00
- **Authors**: Kun Zhan, Chaoxi Niu
- **Comment**: GCN, 8 pages, 1 figures
- **Journal**: Future Generation Computer Systems, 2021
- **Summary**: Graph convolutional networks produce good predictions of unlabeled samples due to its transductive label propagation. Since samples have different predicted confidences, we take high-confidence predictions as pseudo labels to expand the label set so that more samples are selected for updating models. We propose a new training method named as mutual teaching, i.e., we train dual models and let them teach each other during each batch. First, each network feeds forward all samples and selects samples with high-confidence predictions. Second, each model is updated by samples selected by its peer network. We view the high-confidence predictions as useful knowledge, and the useful knowledge of one network teaches the peer network with model updating in each batch. In mutual teaching, the pseudo-label set of a network is from its peer network. Since we use the new strategy of network training, performance improves significantly. Extensive experimental results demonstrate that our method achieves superior performance over state-of-the-art methods under very low label rates.



### Unsupervised Feature Learning by Autoencoder and Prototypical Contrastive Learning for Hyperspectral Classification
- **Arxiv ID**: http://arxiv.org/abs/2009.00953v1
- **DOI**: 10.1016/j.neucom.2021.07.015
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.00953v1)
- **Published**: 2020-09-02 11:17:48+00:00
- **Updated**: 2020-09-02 11:17:48+00:00
- **Authors**: Zeyu Cao, Xiaorun Li, Liaoying Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised learning methods for feature extraction are becoming more and more popular. We combine the popular contrastive learning method (prototypical contrastive learning) and the classic representation learning method (autoencoder) to design an unsupervised feature learning network for hyperspectral classification. Experiments have proved that our two proposed autoencoder networks have good feature learning capabilities by themselves, and the contrastive learning network we designed can better combine the features of the two to learn more representative features. As a result, our method surpasses other comparison methods in the hyperspectral classification experiments, including some supervised methods. Moreover, our method maintains a fast feature extraction speed than baseline methods. In addition, our method reduces the requirements for huge computing resources, separates feature extraction and contrastive learning, and allows more researchers to conduct research and experiments on unsupervised contrastive learning.



### Simulating Unknown Target Models for Query-Efficient Black-box Attacks
- **Arxiv ID**: http://arxiv.org/abs/2009.00960v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/2009.00960v2)
- **Published**: 2020-09-02 11:30:40+00:00
- **Updated**: 2021-04-06 14:01:46+00:00
- **Authors**: Chen Ma, Li Chen, Jun-Hai Yong
- **Comment**: Accepted at CVPR 2021. Code and models are available at
  https://github.com/machanic/SimulatorAttack
- **Journal**: None
- **Summary**: Many adversarial attacks have been proposed to investigate the security issues of deep neural networks. In the black-box setting, current model stealing attacks train a substitute model to counterfeit the functionality of the target model. However, the training requires querying the target model. Consequently, the query complexity remains high, and such attacks can be defended easily. This study aims to train a generalized substitute model called "Simulator", which can mimic the functionality of any unknown target model. To this end, we build the training data with the form of multiple tasks by collecting query sequences generated during the attacks of various existing networks. The learning process uses a mean square error-based knowledge-distillation loss in the meta-learning to minimize the difference between the Simulator and the sampled networks. The meta-gradients of this loss are then computed and accumulated from multiple tasks to update the Simulator and subsequently improve generalization. When attacking a target model that is unseen in training, the trained Simulator can accurately simulate its functionality using its limited feedback. As a result, a large fraction of queries can be transferred to the Simulator, thereby reducing query complexity. Results of the comprehensive experiments conducted using the CIFAR-10, CIFAR-100, and TinyImageNet datasets demonstrate that the proposed approach reduces query complexity by several orders of magnitude compared to the baseline method. The implementation source code is released at https://github.com/machanic/SimulatorAttack.



### DARTS-: Robustly Stepping out of Performance Collapse Without Indicators
- **Arxiv ID**: http://arxiv.org/abs/2009.01027v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2009.01027v2)
- **Published**: 2020-09-02 12:54:13+00:00
- **Updated**: 2021-01-15 07:58:11+00:00
- **Authors**: Xiangxiang Chu, Xiaoxing Wang, Bo Zhang, Shun Lu, Xiaolin Wei, Junchi Yan
- **Comment**: Accepted to ICLR2021
- **Journal**: None
- **Summary**: Despite the fast development of differentiable architecture search (DARTS), it suffers from long-standing performance instability, which extremely limits its application. Existing robustifying methods draw clues from the resulting deteriorated behavior instead of finding out its causing factor. Various indicators such as Hessian eigenvalues are proposed as a signal to stop searching before the performance collapses. However, these indicator-based methods tend to easily reject good architectures if the thresholds are inappropriately set, let alone the searching is intrinsically noisy. In this paper, we undertake a more subtle and direct approach to resolve the collapse. We first demonstrate that skip connections have a clear advantage over other candidate operations, where it can easily recover from a disadvantageous state and become dominant. We conjecture that this privilege is causing degenerated performance. Therefore, we propose to factor out this benefit with an auxiliary skip connection, ensuring a fairer competition for all operations. We call this approach DARTS-. Extensive experiments on various datasets verify that it can substantially improve robustness. Our code is available at https://github.com/Meituan-AutoML/DARTS- .



### Privacy Leakage of SIFT Features via Deep Generative Model based Image Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2009.01030v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/2009.01030v1)
- **Published**: 2020-09-02 12:59:12+00:00
- **Updated**: 2020-09-02 12:59:12+00:00
- **Authors**: Haiwei Wu, Jiantao Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: Many practical applications, e.g., content based image retrieval and object recognition, heavily rely on the local features extracted from the query image. As these local features are usually exposed to untrustworthy parties, the privacy leakage problem of image local features has received increasing attention in recent years. In this work, we thoroughly evaluate the privacy leakage of Scale Invariant Feature Transform (SIFT), which is one of the most widely-used image local features. We first consider the case that the adversary can fully access the SIFT features, i.e., both the SIFT descriptors and the coordinates are available. We propose a novel end-to-end, coarse-to-fine deep generative model for reconstructing the latent image from its SIFT features. The designed deep generative model consists of two networks, where the first one attempts to learn the structural information of the latent image by transforming from SIFT features to Local Binary Pattern (LBP) features, while the second one aims to reconstruct the pixel values guided by the learned LBP. Compared with the state-of-the-art algorithms, the proposed deep generative model produces much improved reconstructed results over three public datasets. Furthermore, we address more challenging cases that only partial SIFT features (either SIFT descriptors or coordinates) are accessible to the adversary. It is shown that, if the adversary can only have access to the SIFT descriptors while not their coordinates, then the modest success of reconstructing the latent image can be achieved for highly-structured images (e.g., faces) and would fail in general settings. In addition, the latent image can be reconstructed with reasonably good quality solely from the SIFT coordinates. Our results would suggest that the privacy leakage problem can be largely avoided if the SIFT coordinates can be well protected.



### Deep Generative Model for Image Inpainting with Local Binary Pattern Learning and Spatial Attention
- **Arxiv ID**: http://arxiv.org/abs/2009.01031v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.01031v1)
- **Published**: 2020-09-02 12:59:28+00:00
- **Updated**: 2020-09-02 12:59:28+00:00
- **Authors**: Haiwei Wu, Jiantao Zhou, Yuanman Li
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning (DL) has demonstrated its powerful capabilities in the field of image inpainting. The DL-based image inpainting approaches can produce visually plausible results, but often generate various unpleasant artifacts, especially in the boundary and highly textured regions. To tackle this challenge, in this work, we propose a new end-to-end, two-stage (coarse-to-fine) generative model through combining a local binary pattern (LBP) learning network with an actual inpainting network. Specifically, the first LBP learning network using U-Net architecture is designed to accurately predict the structural information of the missing region, which subsequently guides the second image inpainting network for better filling the missing pixels. Furthermore, an improved spatial attention mechanism is integrated in the image inpainting network, by considering the consistency not only between the known region with the generated one, but also within the generated region itself. Extensive experiments on public datasets including CelebA-HQ, Places and Paris StreetView demonstrate that our model generates better inpainting results than the state-of-the-art competing algorithms, both quantitatively and qualitatively. The source code and trained models will be made available at https://github.com/HighwayWu/ImageInpainting.



### IAUnet: Global Context-Aware Feature Learning for Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2009.01035v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.01035v1)
- **Published**: 2020-09-02 13:07:10+00:00
- **Updated**: 2020-09-02 13:07:10+00:00
- **Authors**: Ruibing Hou, Bingpeng Ma, Hong Chang, Xinqian Gu, Shiguang Shan, Xilin Chen
- **Comment**: 14 pages, 9 figures. Accepted by IEEE Transactions on Neural Networks
  and Learning Systems (TNNLS)
- **Journal**: None
- **Summary**: Person re-identification (reID) by CNNs based networks has achieved favorable performance in recent years. However, most of existing CNNs based methods do not take full advantage of spatial-temporal context modeling. In fact, the global spatial-temporal context can greatly clarify local distractions to enhance the target feature representation. To comprehensively leverage the spatial-temporal context information, in this work, we present a novel block, Interaction-Aggregation-Update (IAU), for high-performance person reID. Firstly, Spatial-Temporal IAU (STIAU) module is introduced. STIAU jointly incorporates two types of contextual interactions into a CNN framework for target feature learning. Here the spatial interactions learn to compute the contextual dependencies between different body parts of a single frame. While the temporal interactions are used to capture the contextual dependencies between the same body parts across all frames. Furthermore, a Channel IAU (CIAU) module is designed to model the semantic contextual interactions between channel features to enhance the feature representation, especially for small-scale visual cues and body parts. Therefore, the IAU block enables the feature to incorporate the globally spatial, temporal, and channel context. It is lightweight, end-to-end trainable, and can be easily plugged into existing CNNs to form IAUnet. The experiments show that IAUnet performs favorably against state-of-the-art on both image and video reID tasks and achieves compelling results on a general object categorization task. The source code is available at https://github.com/blue-blue272/ImgReID-IAnet.



### Zero-Shot Human-Object Interaction Recognition via Affordance Graphs
- **Arxiv ID**: http://arxiv.org/abs/2009.01039v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.01039v1)
- **Published**: 2020-09-02 13:14:44+00:00
- **Updated**: 2020-09-02 13:14:44+00:00
- **Authors**: Alessio Sarullo, Tingting Mu
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a new approach for Zero-Shot Human-Object Interaction Recognition in the challenging setting that involves interactions with unseen actions (as opposed to just unseen combinations of seen actions and objects). Our approach makes use of knowledge external to the image content in the form of a graph that models affordance relations between actions and objects, i.e., whether an action can be performed on the given object or not. We propose a loss function with the aim of distilling the knowledge contained in the graph into the model, while also using the graph to regularise learnt representations by imposing a local structure on the latent space. We evaluate our approach on several datasets (including the popular HICO and HICO-DET) and show that it outperforms the current state of the art.



### Exploiting Latent Codes: Interactive Fashion Product Generation, Similar Image Retrieval, and Cross-Category Recommendation using Variational Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/2009.01053v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/2009.01053v1)
- **Published**: 2020-09-02 13:27:30+00:00
- **Updated**: 2020-09-02 13:27:30+00:00
- **Authors**: James-Andrew Sarmiento
- **Comment**: None
- **Journal**: None
- **Summary**: The rise of deep learning applications in the fashion industry has fueled advances in curating large-scale datasets to build applications for product design, image retrieval, and recommender systems. In this paper, the author proposes using Variational Autoencoder (VAE) to build an interactive fashion product application framework that allows the users to generate products with attributes according to their liking, retrieve similar styles for the same product category, and receive content-based recommendations from other categories. Fashion product images dataset containing eyewear, footwear, and bags are appropriate to illustrate that this pipeline is applicable in the booming industry of e-commerce enabling direct user interaction in specifying desired products paired with new methods for data matching, and recommendation systems by using VAE and exploiting its generated latent codes.



### Decentralized Source Localization without Sensor Parameters in Wireless Sensor Networks
- **Arxiv ID**: http://arxiv.org/abs/2009.01062v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.SP, stat.ML, H.4.3; I.6.5
- **Links**: [PDF](http://arxiv.org/pdf/2009.01062v3)
- **Published**: 2020-09-02 13:34:55+00:00
- **Updated**: 2020-10-31 13:18:23+00:00
- **Authors**: Akram Hussain, Yuan Luo
- **Comment**: 16 pages, 14 figures
- **Journal**: None
- **Summary**: This paper studies the source (event) localization problem in decentralized wireless sensor networks (WSNs) under the fault model without knowing the sensor parameters. Event localizations have many applications such as localizing intruders, Wifi hotspots and users, and faults in power systems. Previous studies assume the true knowledge (or good estimates) of sensor parameters (e.g., fault model probability or Region of Influence (ROI) of the source) for source localization. However, we propose two methods to estimate the source location in this paper under the fault model: hitting set approach and feature selection method, which only utilize the noisy data set at the fusion center for estimation of the source location without knowing the sensor parameters. The proposed methods have been shown to localize the source effectively. We also study the lower bound on the sample complexity requirement for hitting set method. These methods have also been extended for multiple sources localizations. In addition, we modify the proposed feature selection approach to use maximum likelihood. Finally, extensive simulations are carried out for different settings (i.e., the number of sensor nodes and sample complexity) to validate our proposed methods in comparison to centroid, maximum likelihood, FTML, SNAP estimators.



### Video Captioning Using Weak Annotation
- **Arxiv ID**: http://arxiv.org/abs/2009.01067v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2009.01067v1)
- **Published**: 2020-09-02 13:45:01+00:00
- **Updated**: 2020-09-02 13:45:01+00:00
- **Authors**: Jingyi Hou, Yunde Jia, Xinxiao wu, Yayun Qi
- **Comment**: None
- **Journal**: None
- **Summary**: Video captioning has shown impressive progress in recent years. One key reason of the performance improvements made by existing methods lie in massive paired video-sentence data, but collecting such strong annotation, i.e., high-quality sentences, is time-consuming and laborious. It is the fact that there now exist an amazing number of videos with weak annotation that only contains semantic concepts such as actions and objects. In this paper, we investigate using weak annotation instead of strong annotation to train a video captioning model. To this end, we propose a progressive visual reasoning method that progressively generates fine sentences from weak annotations by inferring more semantic concepts and their dependency relationships for video captioning. To model concept relationships, we use dependency trees that are spanned by exploiting external knowledge from large sentence corpora. Through traversing the dependency trees, the sentences are generated to train the captioning model. Accordingly, we develop an iterative refinement algorithm that refines sentences via spanning dependency trees and fine-tunes the captioning model using the refined sentences in an alternative training manner. Experimental results demonstrate that our method using weak annotation is very competitive to the state-of-the-art methods using strong annotation.



### Unsupervised Domain Adaptation For Plant Organ Counting
- **Arxiv ID**: http://arxiv.org/abs/2009.01081v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.01081v1)
- **Published**: 2020-09-02 13:57:09+00:00
- **Updated**: 2020-09-02 13:57:09+00:00
- **Authors**: Tewodros Ayalew, Jordan Ubbens, Ian Stavness
- **Comment**: To be published in Computer Vision Problems in Plant Phenotyping
  (CVPPP) in conjunction with ECCV 2020
- **Journal**: None
- **Summary**: Supervised learning is often used to count objects in images, but for counting small, densely located objects, the required image annotations are burdensome to collect. Counting plant organs for image-based plant phenotyping falls within this category. Object counting in plant images is further challenged by having plant image datasets with significant domain shift due to different experimental conditions, e.g. applying an annotated dataset of indoor plant images for use on outdoor images, or on a different plant species. In this paper, we propose a domain-adversarial learning approach for domain adaptation of density map estimation for the purposes of object counting. The approach does not assume perfectly aligned distributions between the source and target datasets, which makes it more broadly applicable within general object counting and plant organ counting tasks. Evaluation on two diverse object counting tasks (wheat spikelets, leaves) demonstrates consistent performance on the target datasets across different classes of domain shift: from indoor-to-outdoor images and from species-to-species adaptation.



### Face Image Quality Assessment: A Literature Survey
- **Arxiv ID**: http://arxiv.org/abs/2009.01103v3
- **DOI**: 10.1145/3507901
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.01103v3)
- **Published**: 2020-09-02 14:26:12+00:00
- **Updated**: 2021-10-25 13:06:18+00:00
- **Authors**: Torsten Schlett, Christian Rathgeb, Olaf Henniger, Javier Galbally, Julian Fierrez, Christoph Busch
- **Comment**: None
- **Journal**: None
- **Summary**: The performance of face analysis and recognition systems depends on the quality of the acquired face data, which is influenced by numerous factors. Automatically assessing the quality of face data in terms of biometric utility can thus be useful to detect low-quality data and make decisions accordingly. This survey provides an overview of the face image quality assessment literature, which predominantly focuses on visible wavelength face image input. A trend towards deep learning based methods is observed, including notable conceptual differences among the recent approaches, such as the integration of quality assessment into face recognition models. Besides image selection, face image quality assessment can also be used in a variety of other application scenarios, which are discussed herein. Open issues and challenges are pointed out, i.a. highlighting the importance of comparability for algorithm evaluations, and the challenge for future work to create deep learning approaches that are interpretable in addition to providing accurate utility predictions.



### Perceptual Deep Neural Networks: Adversarial Robustness through Input Recreation
- **Arxiv ID**: http://arxiv.org/abs/2009.01110v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.01110v4)
- **Published**: 2020-09-02 14:36:36+00:00
- **Updated**: 2020-11-30 10:36:03+00:00
- **Authors**: Danilo Vasconcellos Vargas, Bingli Liao, Takahiro Kanzaki
- **Comment**: None
- **Journal**: None
- **Summary**: Adversarial examples have shown that albeit highly accurate, models learned by machines, differently from humans, have many weaknesses. However, humans' perception is also fundamentally different from machines, because we do not see the signals which arrive at the retina but a rather complex recreation of them. In this paper, we explore how machines could recreate the input as well as investigate the benefits of such an augmented perception. In this regard, we propose Perceptual Deep Neural Networks ($\varphi$DNN) which also recreate their own input before further processing. The concept is formalized mathematically and two variations of it are developed (one based on inpainting the whole image and the other based on a noisy resized super resolution recreation). Experiments reveal that $\varphi$DNNs and their adversarial training variations can increase the robustness substantially, surpassing both state-of-the-art defenses and pre-processing types of defenses in 100% of the tests. $\varphi$DNNs are shown to scale well to bigger image sizes, keeping a similar high accuracy throughout; while the state-of-the-art worsen up to 35%. Moreover, the recreation process intentionally corrupts the input image. Interestingly, we show by ablation tests that corrupting the input is, although counter-intuitive, beneficial. Thus, $\varphi$DNNs reveal that input recreation has strong benefits for artificial neural networks similar to biological ones, shedding light into the importance of purposely corrupting the input as well as pioneering an area of perception models based on GANs and autoencoders for robust recognition in artificial intelligence.



### Lifelong Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2009.01129v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.01129v1)
- **Published**: 2020-09-02 15:08:51+00:00
- **Updated**: 2020-09-02 15:08:51+00:00
- **Authors**: Wang Zhou, Shiyu Chang, Norma Sosa, Hendrik Hamann, David Cox
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advances in object detection have benefited significantly from rapid developments in deep neural networks. However, neural networks suffer from the well-known issue of catastrophic forgetting, which makes continual or lifelong learning problematic. In this paper, we leverage the fact that new training classes arrive in a sequential manner and incrementally refine the model so that it additionally detects new object classes in the absence of previous training data. Specifically, we consider the representative object detector, Faster R-CNN, for both accurate and efficient prediction. To prevent abrupt performance degradation due to catastrophic forgetting, we propose to apply knowledge distillation on both the region proposal network and the region classification network, to retain the detection of previously trained classes. A pseudo-positive-aware sampling strategy is also introduced for distillation sample selection. We evaluate the proposed method on PASCAL VOC 2007 and MS COCO benchmarks and show competitive mAP and 6x inference speed improvement, which makes the approach more suitable for real-time applications. Our implementation will be publicly available.



### Long-Term Anticipation of Activities with Cycle Consistency
- **Arxiv ID**: http://arxiv.org/abs/2009.01142v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.01142v1)
- **Published**: 2020-09-02 15:41:32+00:00
- **Updated**: 2020-09-02 15:41:32+00:00
- **Authors**: Yazan Abu Farha, Qiuhong Ke, Bernt Schiele, Juergen Gall
- **Comment**: GCPR 2020
- **Journal**: None
- **Summary**: With the success of deep learning methods in analyzing activities in videos, more attention has recently been focused towards anticipating future activities. However, most of the work on anticipation either analyzes a partially observed activity or predicts the next action class. Recently, new approaches have been proposed to extend the prediction horizon up to several minutes in the future and that anticipate a sequence of future activities including their durations. While these works decouple the semantic interpretation of the observed sequence from the anticipation task, we propose a framework for anticipating future activities directly from the features of the observed frames and train it in an end-to-end fashion. Furthermore, we introduce a cycle consistency loss over time by predicting the past activities given the predicted future. Our framework achieves state-of-the-art results on two datasets: the Breakfast dataset and 50Salads.



### Local-HDP: Interactive Open-Ended 3D Object Categorization in Real-Time Robotic Scenarios
- **Arxiv ID**: http://arxiv.org/abs/2009.01152v3
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2009.01152v3)
- **Published**: 2020-09-02 15:55:49+00:00
- **Updated**: 2021-04-11 17:58:49+00:00
- **Authors**: H. Ayoobi, H. Kasaei, M. Cao, R. Verbrugge, B. Verheij
- **Comment**: 13 pages
- **Journal**: None
- **Summary**: We introduce a non-parametric hierarchical Bayesian approach for open-ended 3D object categorization, named the Local Hierarchical Dirichlet Process (Local-HDP). This method allows an agent to learn independent topics for each category incrementally and to adapt to the environment in time. Hierarchical Bayesian approaches like Latent Dirichlet Allocation (LDA) can transform low-level features to high-level conceptual topics for 3D object categorization. However, the efficiency and accuracy of LDA-based approaches depend on the number of topics that is chosen manually. Moreover, fixing the number of topics for all categories can lead to overfitting or underfitting of the model. In contrast, the proposed Local-HDP can autonomously determine the number of topics for each category. Furthermore, the online variational inference method has been adapted for fast posterior approximation in the Local-HDP model. Experiments show that the proposed Local-HDP method outperforms other state-of-the-art approaches in terms of accuracy, scalability, and memory efficiency by a large margin. Moreover, two robotic experiments have been conducted to show the applicability of the proposed approach in real-time applications.



### Semantically Adaptive Image-to-image Translation for Domain Adaptation of Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2009.01166v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.01166v1)
- **Published**: 2020-09-02 16:16:50+00:00
- **Updated**: 2020-09-02 16:16:50+00:00
- **Authors**: Luigi Musto, Andrea Zinelli
- **Comment**: Paper will appear on BMVC 2020
- **Journal**: None
- **Summary**: Domain shift is a very challenging problem for semantic segmentation. Any model can be easily trained on synthetic data, where images and labels are artificially generated, but it will perform poorly when deployed on real environments. In this paper, we address the problem of domain adaptation for semantic segmentation of street scenes. Many state-of-the-art approaches focus on translating the source image while imposing that the result should be semantically consistent with the input. However, we advocate that the image semantics can also be exploited to guide the translation algorithm. To this end, we rethink the generative model to enforce this assumption and strengthen the connection between pixel-level and feature-level domain alignment. We conduct extensive experiments by training common semantic segmentation models with our method and show that the results we obtain on the synthetic-to-real benchmarks surpass the state-of-the-art.



### Transform Quantization for CNN (Convolutional Neural Network) Compression
- **Arxiv ID**: http://arxiv.org/abs/2009.01174v4
- **DOI**: 10.1109/TPAMI.2021.3084839
- **Categories**: **cs.CV**, cs.IT, cs.LG, eess.IV, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/2009.01174v4)
- **Published**: 2020-09-02 16:33:42+00:00
- **Updated**: 2021-11-07 17:29:26+00:00
- **Authors**: Sean I. Young, Wang Zhe, David Taubman, Bernd Girod
- **Comment**: To appear in IEEE Trans Pattern Anal Mach Intell
- **Journal**: None
- **Summary**: In this paper, we compress convolutional neural network (CNN) weights post-training via transform quantization. Previous CNN quantization techniques tend to ignore the joint statistics of weights and activations, producing sub-optimal CNN performance at a given quantization bit-rate, or consider their joint statistics during training only and do not facilitate efficient compression of already trained CNN models. We optimally transform (decorrelate) and quantize the weights post-training using a rate-distortion framework to improve compression at any given quantization bit-rate. Transform quantization unifies quantization and dimensionality reduction (decorrelation) techniques in a single framework to facilitate low bit-rate compression of CNNs and efficient inference in the transform domain. We first introduce a theory of rate and distortion for CNN quantization, and pose optimum quantization as a rate-distortion optimization problem. We then show that this problem can be solved using optimal bit-depth allocation following decorrelation by the optimal End-to-end Learned Transform (ELT) we derive in this paper. Experiments demonstrate that transform quantization advances the state of the art in CNN compression in both retrained and non-retrained quantization scenarios. In particular, we find that transform quantization with retraining is able to compress CNN models such as AlexNet, ResNet and DenseNet to very low bit-rates (1-2 bits).



### Evaluation of Deep Convolutional Generative Adversarial Networks for data augmentation of chest X-ray images
- **Arxiv ID**: http://arxiv.org/abs/2009.01181v1
- **DOI**: 10.3390/fi13010008
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.01181v1)
- **Published**: 2020-09-02 16:43:55+00:00
- **Updated**: 2020-09-02 16:43:55+00:00
- **Authors**: Sagar Kora Venu
- **Comment**: 5 pages, 4 figures
- **Journal**: None
- **Summary**: Medical image datasets are usually imbalanced, due to the high costs of obtaining the data and time-consuming annotations. Training deep neural network models on such datasets to accurately classify the medical condition does not yield desired results and often over-fits the data on majority class samples. In order to address this issue, data augmentation is often performed on training data by position augmentation techniques such as scaling, cropping, flipping, padding, rotation, translation, affine transformation, and color augmentation techniques such as brightness, contrast, saturation, and hue to increase the dataset sizes. These augmentation techniques are not guaranteed to be advantageous in domains with limited data, especially medical image data, and could lead to further overfitting. In this work, we performed data augmentation on the Chest X-rays dataset through generative modeling (deep convolutional generative adversarial network) which creates artificial instances retaining similar characteristics to the original data and evaluation of the model resulted in Fr\'echet Distance of Inception (FID) score of 1.289.



### Comprehensive Semantic Segmentation on High Resolution UAV Imagery for Natural Disaster Damage Assessment
- **Arxiv ID**: http://arxiv.org/abs/2009.01193v2
- **DOI**: None
- **Categories**: **cs.CV**, 68T45, I.4.6
- **Links**: [PDF](http://arxiv.org/pdf/2009.01193v2)
- **Published**: 2020-09-02 17:07:28+00:00
- **Updated**: 2020-09-06 22:03:20+00:00
- **Authors**: Maryam Rahnemoonfar, Tashnim Chowdhury, Robin Murphy, Odair Fernandes
- **Comment**: 10 pages, 7 figures
- **Journal**: None
- **Summary**: In this paper, we present a large-scale hurricane Michael dataset for visual perception in disaster scenarios, and analyze state-of-the-art deep neural network models for semantic segmentation. The dataset consists of around 2000 high-resolution aerial images, with annotated ground-truth data for semantic segmentation. We discuss the challenges of the dataset and train the state-of-the-art methods on this dataset to evaluate how well these methods can recognize the disaster situations. Finally, we discuss challenges for future research.



### Automatic cinematography for 360 video
- **Arxiv ID**: http://arxiv.org/abs/2009.05388v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.05388v1)
- **Published**: 2020-09-02 17:14:05+00:00
- **Updated**: 2020-09-02 17:14:05+00:00
- **Authors**: Hannes Fassold
- **Comment**: Accepted as demo paper for IEEE MMSP 2020
- **Journal**: None
- **Summary**: We describe our method for automatic generation of a visually interesting camera path (automatic cinematography)from a 360 video. Based on the information from the scene objects, multiple shot hypotheses for different shot types are constructed and the best one is rendered.



### Excavating "Excavating AI": The Elephant in the Gallery
- **Arxiv ID**: http://arxiv.org/abs/2009.01215v3
- **DOI**: 10.5281/zenodo.4037538
- **Categories**: **cs.CY**, cs.AI, cs.CV, cs.HC, cs.LG, 68T01, K.4.0
- **Links**: [PDF](http://arxiv.org/pdf/2009.01215v3)
- **Published**: 2020-09-02 17:42:06+00:00
- **Updated**: 2020-12-24 01:27:53+00:00
- **Authors**: Michael J. Lyons
- **Comment**: 15 pages, 4 figures
- **Journal**: None
- **Summary**: Two art exhibitions, "Training Humans" and "Making Faces," and the accompanying essay "Excavating AI: The politics of images in machine learning training sets" by Kate Crawford and Trevor Paglen, are making substantial impact on discourse taking place in the social and mass media networks, and some scholarly circles. Critical scrutiny reveals, however, a self-contradictory stance regarding informed consent for the use of facial images, as well as serious flaws in their critique of ML training sets. Our analysis underlines the non-negotiability of informed consent when using human data in artistic and other contexts, and clarifies issues relating to the description of ML training sets.



### Seeing wake words: Audio-visual Keyword Spotting
- **Arxiv ID**: http://arxiv.org/abs/2009.01225v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2009.01225v1)
- **Published**: 2020-09-02 17:57:38+00:00
- **Updated**: 2020-09-02 17:57:38+00:00
- **Authors**: Liliane Momeni, Triantafyllos Afouras, Themos Stafylakis, Samuel Albanie, Andrew Zisserman
- **Comment**: None
- **Journal**: None
- **Summary**: The goal of this work is to automatically determine whether and when a word of interest is spoken by a talking face, with or without the audio. We propose a zero-shot method suitable for in the wild videos. Our key contributions are: (1) a novel convolutional architecture, KWS-Net, that uses a similarity map intermediate representation to separate the task into (i) sequence matching, and (ii) pattern detection, to decide whether the word is there and when; (2) we demonstrate that if audio is available, visual keyword spotting improves the performance both for a clean and noisy audio signal. Finally, (3) we show that our method generalises to other languages, specifically French and German, and achieves a comparable performance to English with less language specific data, by fine-tuning the network pre-trained on English. The method exceeds the performance of the previous state-of-the-art visual keyword spotting architecture when trained and tested on the same benchmark, and also that of a state-of-the-art lip reading method.



### Lunar Crater Identification in Digital Images
- **Arxiv ID**: http://arxiv.org/abs/2009.01228v2
- **DOI**: 10.1007/s40295-021-00287-8
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.01228v2)
- **Published**: 2020-09-02 17:59:51+00:00
- **Updated**: 2020-09-14 16:25:05+00:00
- **Authors**: John A. Christian, Harm Derksen, Ryan Watkins
- **Comment**: None
- **Journal**: None
- **Summary**: It is often necessary to identify a pattern of observed craters in a single image of the lunar surface and without any prior knowledge of the camera's location. This so-called "lost-in-space" crater identification problem is common in both crater-based terrain relative navigation (TRN) and in automatic registration of scientific imagery. Past work on crater identification has largely been based on heuristic schemes, with poor performance outside of a narrowly defined operating regime (e.g., nadir pointing images, small search areas). This work provides the first mathematically rigorous treatment of the general crater identification problem. It is shown when it is (and when it is not) possible to recognize a pattern of elliptical crater rims in an image formed by perspective projection. For the cases when it is possible to recognize a pattern, descriptors are developed using invariant theory that provably capture all of the viewpoint invariant information. These descriptors may be pre-computed for known crater patterns and placed in a searchable index for fast recognition. New techniques are also developed for computing pose from crater rim observations and for evaluating crater rim correspondences. These techniques are demonstrated on both synthetic and real images.



### Efficiency in Real-time Webcam Gaze Tracking
- **Arxiv ID**: http://arxiv.org/abs/2009.01270v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.HC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.01270v1)
- **Published**: 2020-09-02 18:07:41+00:00
- **Updated**: 2020-09-02 18:07:41+00:00
- **Authors**: Amogh Gudi, Xin Li, Jan van Gemert
- **Comment**: Awarded Best Paper at European Conference on Computer Vision (ECCV)
  Workshop on Eye Gaze in AR, VR, and in the Wild (OpenEyes) 2020
- **Journal**: None
- **Summary**: Efficiency and ease of use are essential for practical applications of camera based eye/gaze-tracking. Gaze tracking involves estimating where a person is looking on a screen based on face images from a computer-facing camera. In this paper we investigate two complementary forms of efficiency in gaze tracking: 1. The computational efficiency of the system which is dominated by the inference speed of a CNN predicting gaze-vectors; 2. The usability efficiency which is determined by the tediousness of the mandatory calibration of the gaze-vector to a computer screen. To do so, we evaluate the computational speed/accuracy trade-off for the CNN and the calibration effort/accuracy trade-off for screen calibration. For the CNN, we evaluate the full face, two-eyes, and single eye input. For screen calibration, we measure the number of calibration points needed and evaluate three types of calibration: 1. pure geometry, 2. pure machine learning, and 3. hybrid geometric regression. Results suggest that a single eye input and geometric regression calibration achieve the best trade-off.



### Unsupervised Feedforward Feature (UFF) Learning for Point Cloud Classification and Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2009.01280v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.01280v1)
- **Published**: 2020-09-02 18:25:25+00:00
- **Updated**: 2020-09-02 18:25:25+00:00
- **Authors**: Min Zhang, Pranav Kadam, Shan Liu, C. -C. Jay Kuo
- **Comment**: 7 pages, 2 figures, the final version is accepted by VCIP 2020
- **Journal**: None
- **Summary**: In contrast to supervised backpropagation-based feature learning in deep neural networks (DNNs), an unsupervised feedforward feature (UFF) learning scheme for joint classification and segmentation of 3D point clouds is proposed in this work. The UFF method exploits statistical correlations of points in a point cloud set to learn shape and point features in a one-pass feedforward manner through a cascaded encoder-decoder architecture. It learns global shape features through the encoder and local point features through the concatenated encoder-decoder architecture. The extracted features of an input point cloud are fed to classifiers for shape classification and part segmentation. Experiments are conducted to evaluate the performance of the UFF method. For shape classification, the UFF is superior to existing unsupervised methods and on par with state-of-the-art DNNs. For part segmentation, the UFF outperforms semi-supervised methods and performs slightly worse than DNNs.



### Unsupervised Point Cloud Registration via Salient Points Analysis (SPA)
- **Arxiv ID**: http://arxiv.org/abs/2009.01293v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.01293v1)
- **Published**: 2020-09-02 18:40:37+00:00
- **Updated**: 2020-09-02 18:40:37+00:00
- **Authors**: Pranav Kadam, Min Zhang, Shan Liu, C. -C. Jay Kuo
- **Comment**: 7 pages, 5 figures, final version is accepted by IEEE International
  Conference on Visual Communications and Image Processing (VCIP) 2020
- **Journal**: None
- **Summary**: An unsupervised point cloud registration method, called salient points analysis (SPA), is proposed in this work. The proposed SPA method can register two point clouds effectively using only a small subset of salient points. It first applies the PointHop++ method to point clouds, finds corresponding salient points in two point clouds based on the local surface characteristics of points and performs registration by matching the corresponding salient points. The SPA method offers several advantages over the recent deep learning based solutions for registration. Deep learning methods such as PointNetLK and DCP train end-to-end networks and rely on full supervision (namely, ground truth transformation matrix and class label). In contrast, the SPA is completely unsupervised. Furthermore, SPA's training time and model size are much less. The effectiveness of the SPA method is demonstrated by experiments on seen and unseen classes and noisy point clouds from the ModelNet-40 dataset.



### When Image Decomposition Meets Deep Learning: A Novel Infrared and Visible Image Fusion Method
- **Arxiv ID**: http://arxiv.org/abs/2009.01315v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.01315v2)
- **Published**: 2020-09-02 19:32:28+00:00
- **Updated**: 2021-04-14 12:24:44+00:00
- **Authors**: Zixiang Zhao, Jiangshe Zhang, Shuang Xu, Kai Sun, Chunxia Zhang, Junmin Liu
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2003.09210
- **Journal**: None
- **Summary**: Infrared and visible image fusion, as a hot topic in image processing and image enhancement, aims to produce fused images retaining the detail texture information in visible images and the thermal radiation information in infrared images. A critical step for this issue is to decompose features in different scales and to merge them separately. In this paper, we propose a novel dual-stream auto-encoder (AE) based fusion network. The core idea is that the encoder decomposes an image into base and detail feature maps with low- and high-frequency information, respectively, and that the decoder is responsible for the original image reconstruction. To this end, a well-designed loss function is established to make the base/detail feature maps similar/dissimilar. In the test phase, base and detail feature maps are respectively merged via an additional fusion layer, which contains a saliency weighted-based spatial attention module and a channel attention module to adaptively preserve more information from source images and to highlight the objects. Then the fused image is recovered by the decoder. Qualitative and quantitative results demonstrate that our method can generate fusion images containing highlighted targets and abundant detail texture information with strong reproducibility and meanwhile is superior to the state-of-the-art (SOTA) approaches.



### An Internal Cluster Validity Index Using a Distance-based Separability Measure
- **Arxiv ID**: http://arxiv.org/abs/2009.01328v2
- **DOI**: 10.1109/ICTAI50040.2020.00131
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2009.01328v2)
- **Published**: 2020-09-02 20:20:29+00:00
- **Updated**: 2021-01-04 21:22:03+00:00
- **Authors**: Shuyue Guan, Murray Loew
- **Comment**: 8 pages, 4 figures. Accepted by IEEE ICTAI 2020 (Long Paper & Oral
  Presentation)
- **Journal**: IEEE 32nd International Conference on Tools with Artificial
  Intelligence (ICTAI), 2020, pp. 827-834
- **Summary**: To evaluate clustering results is a significant part of cluster analysis. There are no true class labels for clustering in typical unsupervised learning. Thus, a number of internal evaluations, which use predicted labels and data, have been created. They are also named internal cluster validity indices (CVIs). Without true labels, to design an effective CVI is not simple because it is similar to create a clustering method. And, to have more CVIs is crucial because there is no universal CVI that can be used to measure all datasets, and no specific method for selecting a proper CVI for clusters without true labels. Therefore, to apply more CVIs to evaluate clustering results is necessary. In this paper, we propose a novel CVI - called Distance-based Separability Index (DSI), based on a data separability measure. We applied the DSI and eight other internal CVIs including early studies from Dunn (1974) to most recent studies CVDD (2019) as comparison. We used an external CVI as ground truth for clustering results of five clustering algorithms on 12 real and 97 synthetic datasets. Results show DSI is an effective, unique, and competitive CVI to other compared CVIs. In addition, we summarized the general process to evaluate CVIs and created a new method - rank difference - to compare the results of CVIs.



### Robust Object Classification Approach using Spherical Harmonics
- **Arxiv ID**: http://arxiv.org/abs/2009.01369v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.01369v1)
- **Published**: 2020-09-02 22:29:06+00:00
- **Updated**: 2020-09-02 22:29:06+00:00
- **Authors**: Ayman Mukhaimar, Ruwan Tennakoon, Chow Yin Lai, Reza Hoseinnezhad, Alireza Bab-Hadiashar
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present a robust spherical harmonics approach for the classification of point cloud-based objects. Spherical harmonics have been used for classification over the years, with several frameworks existing in the literature. These approaches use variety of spherical harmonics based descriptors to classify objects. We first investigated these frameworks robustness against data augmentation, such as outliers and noise, as it has not been studied before. Then we propose a spherical convolution neural network framework for robust object classification. The proposed framework uses the voxel grid of concentric spheres to learn features over the unit ball. Our proposed model learn features that are less sensitive to data augmentation due to the selected sampling strategy and the designed convolution operation. We tested our proposed model against several types of data augmentation, such as noise and outliers. Our results show that the proposed model outperforms the state of art networks in terms of robustness to data augmentation.



### Real Image Super Resolution Via Heterogeneous Model Ensemble using GP-NAS
- **Arxiv ID**: http://arxiv.org/abs/2009.01371v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.01371v2)
- **Published**: 2020-09-02 22:33:23+00:00
- **Updated**: 2021-01-22 18:48:30+00:00
- **Authors**: Zhihong Pan, Baopu Li, Teng Xi, Yanwen Fan, Gang Zhang, Jingtuo Liu, Junyu Han, Errui Ding
- **Comment**: This is a manuscript related to our algorithm that won the ECCV AIM
  2020 Real Image Super-Resolution Challenge
- **Journal**: None
- **Summary**: With advancement in deep neural network (DNN), recent state-of-the-art (SOTA) image superresolution (SR) methods have achieved impressive performance using deep residual network with dense skip connections. While these models perform well on benchmark dataset where low-resolution (LR) images are constructed from high-resolution (HR) references with known blur kernel, real image SR is more challenging when both images in the LR-HR pair are collected from real cameras. Based on existing dense residual networks, a Gaussian process based neural architecture search (GP-NAS) scheme is utilized to find candidate network architectures using a large search space by varying the number of dense residual blocks, the block size and the number of features. A suite of heterogeneous models with diverse network structure and hyperparameter are selected for model-ensemble to achieve outstanding performance in real image SR. The proposed method won the first place in all three tracks of the AIM 2020 Real Image Super-Resolution Challenge.



### NITES: A Non-Parametric Interpretable Texture Synthesis Method
- **Arxiv ID**: http://arxiv.org/abs/2009.01376v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.01376v1)
- **Published**: 2020-09-02 22:52:44+00:00
- **Updated**: 2020-09-02 22:52:44+00:00
- **Authors**: Xuejing Lei, Ganning Zhao, C. -C. Jay Kuo
- **Comment**: None
- **Journal**: None
- **Summary**: A non-parametric interpretable texture synthesis method, called the NITES method, is proposed in this work. Although automatic synthesis of visually pleasant texture can be achieved by deep neural networks nowadays, the associated generation models are mathematically intractable and their training demands higher computational cost. NITES offers a new texture synthesis solution to address these shortcomings. NITES is mathematically transparent and efficient in training and inference. The input is a single exemplary texture image. The NITES method crops out patches from the input and analyzes the statistical properties of these texture patches to obtain their joint spatial-spectral representations. Then, the probabilistic distributions of samples in the joint spatial-spectral spaces are characterized. Finally, numerous texture images that are visually similar to the exemplary texture image can be generated automatically. Experimental results are provided to show the superior quality of generated texture images and efficiency of the proposed NITES method in terms of both training and inference time.



### Towards Practical Implementations of Person Re-Identification from Full Video Frames
- **Arxiv ID**: http://arxiv.org/abs/2009.01377v1
- **DOI**: 10.1016/j.patrec.2020.08.023
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.01377v1)
- **Published**: 2020-09-02 22:53:46+00:00
- **Updated**: 2020-09-02 22:53:46+00:00
- **Authors**: Felix O. Sumari, Luigy Machaca, Jose Huaman, Esteban W. G. Clua, Joris Guérin
- **Comment**: 7 pages, 9 figures, This paper is under consideration at Pattern
  Recognition Letters
- **Journal**: None
- **Summary**: With the major adoption of automation for cities security, person re-identification (Re-ID) has been extensively studied recently. In this paper, we argue that the current way of studying person re-identification, i.e. by trying to re-identify a person within already detected and pre-cropped images of people, is not sufficient to implement practical security applications, where the inputs to the system are the full frames of the video streams. To support this claim, we introduce the Full Frame Person Re-ID setting (FF-PRID) and define specific metrics to evaluate FF-PRID implementations. To improve robustness, we also formalize the hybrid human-machine collaboration framework, which is inherent to any Re-ID security applications. To demonstrate the importance of considering the FF-PRID setting, we build an experiment showing that combining a good people detection network with a good Re-ID model does not necessarily produce good results for the final application. This underlines a failure of the current formulation in assessing the quality of a Re-ID model and justifies the use of different metrics. We hope that this work will motivate the research community to consider the full problem in order to develop algorithms that are better suited to real-world scenarios.



### Noise-Aware Texture-Preserving Low-Light Enhancement
- **Arxiv ID**: http://arxiv.org/abs/2009.01385v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2009.01385v1)
- **Published**: 2020-09-02 23:30:03+00:00
- **Updated**: 2020-09-02 23:30:03+00:00
- **Authors**: Zohreh Azizi, Xuejing Lei, C. -C Jay Kuo
- **Comment**: Accepted by IEEE VCIP 2020. The final version will appear in IEEE
  VCIP 2020
- **Journal**: None
- **Summary**: A simple and effective low-light image enhancement method based on a noise-aware texture-preserving retinex model is proposed in this work. The new method, called NATLE, attempts to strike a balance between noise removal and natural texture preservation through a low-complexity solution. Its cost function includes an estimated piece-wise smooth illumination map and a noise-free texture-preserving reflectance map. Afterwards, illumination is adjusted to form the enhanced image together with the reflectance map. Extensive experiments are conducted on common low-light image enhancement datasets to demonstrate the superior performance of NATLE.



