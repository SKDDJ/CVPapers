# Arxiv Papers in cs.CV on 2020-09-30
### Finding It at Another Side: A Viewpoint-Adapted Matching Encoder for Change Captioning
- **Arxiv ID**: http://arxiv.org/abs/2009.14352v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.14352v1)
- **Published**: 2020-09-30 00:13:49+00:00
- **Updated**: 2020-09-30 00:13:49+00:00
- **Authors**: Xiangxi Shi, Xu Yang, Jiuxiang Gu, Shafiq Joty, Jianfei Cai
- **Comment**: None
- **Journal**: ECCV2020
- **Summary**: Change Captioning is a task that aims to describe the difference between images with natural language. Most existing methods treat this problem as a difference judgment without the existence of distractors, such as viewpoint changes. However, in practice, viewpoint changes happen often and can overwhelm the semantic difference to be described. In this paper, we propose a novel visual encoder to explicitly distinguish viewpoint changes from semantic changes in the change captioning task. Moreover, we further simulate the attention preference of humans and propose a novel reinforcement learning process to fine-tune the attention directly with language evaluation rewards. Extensive experimental results show that our method outperforms the state-of-the-art approaches by a large margin in both Spot-the-Diff and CLEVR-Change datasets.



### Toward Privacy and Utility Preserving Image Representation
- **Arxiv ID**: http://arxiv.org/abs/2009.14376v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/2009.14376v2)
- **Published**: 2020-09-30 01:25:00+00:00
- **Updated**: 2020-10-17 16:27:59+00:00
- **Authors**: Ahmadreza Mosallanezhad, Yasin N. Silva, Michelle V. Mancenido, Huan Liu
- **Comment**: Accepted as a working paper in SBP-BRiMS 2020
- **Journal**: None
- **Summary**: Face images are rich data items that are useful and can easily be collected in many applications, such as in 1-to-1 face verification tasks in the domain of security and surveillance systems. Multiple methods have been proposed to protect an individual's privacy by perturbing the images to remove traces of identifiable information, such as gender or race. However, significantly less attention has been given to the problem of protecting images while maintaining optimal task utility. In this paper, we study the novel problem of creating privacy-preserving image representations with respect to a given utility task by proposing a principled framework called the Adversarial Image Anonymizer (AIA). AIA first creates an image representation using a generative model, then enhances the learned image representations using adversarial learning to preserve privacy and utility for a given task. Experiments were conducted on a publicly available data set to demonstrate the effectiveness of AIA as a privacy-preserving mechanism for face images.



### AttendNets: Tiny Deep Image Recognition Neural Networks for the Edge via Visual Attention Condensers
- **Arxiv ID**: http://arxiv.org/abs/2009.14385v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.14385v1)
- **Published**: 2020-09-30 01:53:17+00:00
- **Updated**: 2020-09-30 01:53:17+00:00
- **Authors**: Alexander Wong, Mahmoud Famouri, Mohammad Javad Shafiee
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: While significant advances in deep learning has resulted in state-of-the-art performance across a large number of complex visual perception tasks, the widespread deployment of deep neural networks for TinyML applications involving on-device, low-power image recognition remains a big challenge given the complexity of deep neural networks. In this study, we introduce AttendNets, low-precision, highly compact deep neural networks tailored for on-device image recognition. More specifically, AttendNets possess deep self-attention architectures based on visual attention condensers, which extends on the recently introduced stand-alone attention condensers to improve spatial-channel selective attention. Furthermore, AttendNets have unique machine-designed macroarchitecture and microarchitecture designs achieved via a machine-driven design exploration strategy. Experimental results on ImageNet$_{50}$ benchmark dataset for the task of on-device image recognition showed that AttendNets have significantly lower architectural and computational complexity when compared to several deep neural networks in research literature designed for efficiency while achieving highest accuracies (with the smallest AttendNet achieving $\sim$7.2% higher accuracy, while requiring $\sim$3$\times$ fewer multiply-add operations, $\sim$4.17$\times$ fewer parameters, and $\sim$16.7$\times$ lower weight memory requirements than MobileNet-V1). Based on these promising results, AttendNets illustrate the effectiveness of visual attention condensers as building blocks for enabling various on-device visual perception tasks for TinyML applications.



### Teacher-Critical Training Strategies for Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/2009.14405v1
- **DOI**: 10.1016/j.neucom.2022.07.068
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2009.14405v1)
- **Published**: 2020-09-30 03:15:12+00:00
- **Updated**: 2020-09-30 03:15:12+00:00
- **Authors**: Yiqing Huang, Jiansheng Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Existing image captioning models are usually trained by cross-entropy (XE) loss and reinforcement learning (RL), which set ground-truth words as hard targets and force the captioning model to learn from them. However, the widely adopted training strategies suffer from misalignment in XE training and inappropriate reward assignment in RL training. To tackle these problems, we introduce a teacher model that serves as a bridge between the ground-truth caption and the caption model by generating some easier-to-learn word proposals as soft targets. The teacher model is constructed by incorporating the ground-truth image attributes into the baseline caption model. To effectively learn from the teacher model, we propose Teacher-Critical Training Strategies (TCTS) for both XE and RL training to facilitate better learning processes for the caption model. Experimental evaluations of several widely adopted caption models on the benchmark MSCOCO dataset show the proposed TCTS comprehensively enhances most evaluation metrics, especially the Bleu and Rouge-L scores, in both training stages. TCTS is able to achieve to-date the best published single model Bleu-4 and Rouge-L performances of 40.2% and 59.4% on the MSCOCO Karpathy test split. Our codes and pre-trained models will be open-sourced.



### Bilateral Asymmetry Guided Counterfactual Generating Network for Mammogram Classification
- **Arxiv ID**: http://arxiv.org/abs/2009.14406v1
- **DOI**: 10.1109/TIP.2021.3112053
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.14406v1)
- **Published**: 2020-09-30 03:15:30+00:00
- **Updated**: 2020-09-30 03:15:30+00:00
- **Authors**: Chu-ran Wang, Jing Li, Fandong Zhang, Xinwei Sun, Hao Dong, Yizhou Yu, Yizhou Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Mammogram benign or malignant classification with only image-level labels is challenging due to the absence of lesion annotations. Motivated by the symmetric prior that the lesions on one side of breasts rarely appear in the corresponding areas on the other side, given a diseased image, we can explore a counterfactual problem that how would the features have behaved if there were no lesions in the image, so as to identify the lesion areas. We derive a new theoretical result for counterfactual generation based on the symmetric prior. By building a causal model that entails such a prior for bilateral images, we obtain two optimization goals for counterfactual generation, which can be accomplished via our newly proposed counterfactual generative network. Our proposed model is mainly composed of Generator Adversarial Network and a \emph{prediction feedback mechanism}, they are optimized jointly and prompt each other. Specifically, the former can further improve the classification performance by generating counterfactual features to calculate lesion areas. On the other hand, the latter helps counterfactual generation by the supervision of classification loss. The utility of our method and the effectiveness of each module in our model can be verified by state-of-the-art performance on INBreast and an in-house dataset and ablation studies.



### Pruning Filter in Filter
- **Arxiv ID**: http://arxiv.org/abs/2009.14410v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.14410v3)
- **Published**: 2020-09-30 03:35:16+00:00
- **Updated**: 2020-12-09 08:35:21+00:00
- **Authors**: Fanxu Meng, Hao Cheng, Ke Li, Huixiang Luo, Xiaowei Guo, Guangming Lu, Xing Sun
- **Comment**: Accepted by NeurIPS2020
- **Journal**: None
- **Summary**: Pruning has become a very powerful and effective technique to compress and accelerate modern neural networks. Existing pruning methods can be grouped into two categories: filter pruning (FP) and weight pruning (WP). FP wins at hardware compatibility but loses at the compression ratio compared with WP. To converge the strength of both methods, we propose to prune the filter in the filter. Specifically, we treat a filter $F \in \mathbb{R}^{C\times K\times K}$ as $K \times K$ stripes, i.e., $1\times 1$ filters $\in \mathbb{R}^{C}$, then by pruning the stripes instead of the whole filter, we can achieve finer granularity than traditional FP while being hardware friendly. We term our method as SWP (\emph{Stripe-Wise Pruning}). SWP is implemented by introducing a novel learnable matrix called Filter Skeleton, whose values reflect the shape of each filter. As some recent work has shown that the pruned architecture is more crucial than the inherited important weights, we argue that the architecture of a single filter, i.e., the shape, also matters. Through extensive experiments, we demonstrate that SWP is more effective compared to the previous FP-based methods and achieves the state-of-art pruning ratio on CIFAR-10 and ImageNet datasets without obvious accuracy drop. Code is available at https://github.com/fxmeng/Pruning-Filter-in-Filter



### Uncertainty Estimation and Sample Selection for Crowd Counting
- **Arxiv ID**: http://arxiv.org/abs/2009.14411v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.14411v2)
- **Published**: 2020-09-30 03:40:07+00:00
- **Updated**: 2020-10-04 18:41:49+00:00
- **Authors**: Viresh Ranjan, Boyu Wang, Mubarak Shah, Minh Hoai
- **Comment**: ACCV 2020
- **Journal**: None
- **Summary**: We present a method for image-based crowd counting, one that can predict a crowd density map together with the uncertainty values pertaining to the predicted density map. To obtain prediction uncertainty, we model the crowd density values using Gaussian distributions and develop a convolutional neural network architecture to predict these distributions. A key advantage of our method over existing crowd counting methods is its ability to quantify the uncertainty of its predictions. We illustrate the benefits of knowing the prediction uncertainty by developing a method to reduce the human annotation effort needed to adapt counting networks to a new domain. We present sample selection strategies which make use of the density and uncertainty of predictions from the networks trained on one domain to select the informative images from a target domain of interest to acquire human annotation. We show that our sample selection strategy drastically reduces the amount of labeled data from the target domain needed to adapt a counting network trained on a source domain to the target domain. Empirically, the networks trained on UCF-QNRF dataset can be adapted to surpass the performance of the previous state-of-the-art results on NWPU dataset and Shanghaitech dataset using only 17$\%$ of the labeled training samples from the target domain.



### Improved Knowledge Distillation via Full Kernel Matrix Transfer
- **Arxiv ID**: http://arxiv.org/abs/2009.14416v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2009.14416v2)
- **Published**: 2020-09-30 04:03:09+00:00
- **Updated**: 2022-03-29 18:14:55+00:00
- **Authors**: Qi Qian, Hao Li, Juhua Hu
- **Comment**: accepted by SDM'22
- **Journal**: None
- **Summary**: Knowledge distillation is an effective way for model compression in deep learning. Given a large model (i.e., teacher model), it aims to improve the performance of a compact model (i.e., student model) by transferring the information from the teacher. Various information for distillation has been studied. Recently, a number of works propose to transfer the pairwise similarity between examples to distill relative information. However, most of efforts are devoted to developing different similarity measurements, while only a small matrix consisting of examples within a mini-batch is transferred at each iteration that can be inefficient for optimizing the pairwise similarity over the whole data set. In this work, we aim to transfer the full similarity matrix effectively. The main challenge is from the size of the full matrix that is quadratic to the number of examples. To address the challenge, we decompose the original full matrix with Nystr{\"{o}}m method. By selecting appropriate landmark points, our theoretical analysis indicates that the loss for transfer can be further simplified. Concretely, we find that the difference between the original full kernel matrices between teacher and student can be well bounded by that of the corresponding partial matrices, which only consists of similarities between original examples and landmark points. Compared with the full matrix, the size of the partial matrix is linear in the number of examples, which improves the efficiency of optimization significantly. The empirical study on benchmark data sets demonstrates the effectiveness of the proposed algorithm. Code is available at \url{https://github.com/idstcv/KDA}.



### Towards Adaptive Semantic Segmentation by Progressive Feature Refinement
- **Arxiv ID**: http://arxiv.org/abs/2009.14420v1
- **DOI**: 10.1109/ICIP40778.2020.9190829
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.14420v1)
- **Published**: 2020-09-30 04:17:48+00:00
- **Updated**: 2020-09-30 04:17:48+00:00
- **Authors**: Bin Zhang, Shengjie Zhao, Rongqing Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: As one of the fundamental tasks in computer vision, semantic segmentation plays an important role in real world applications. Although numerous deep learning models have made notable progress on several mainstream datasets with the rapid development of convolutional networks, they still encounter various challenges in practical scenarios. Unsupervised adaptive semantic segmentation aims to obtain a robust classifier trained with source domain data, which is able to maintain stable performance when deployed to a target domain with different data distribution. In this paper, we propose an innovative progressive feature refinement framework, along with domain adversarial learning to boost the transferability of segmentation networks. Specifically, we firstly align the multi-stage intermediate feature maps of source and target domain images, and then a domain classifier is adopted to discriminate the segmentation output. As a result, the segmentation models trained with source domain images can be transferred to a target domain without significant performance degradation. Experimental results verify the efficiency of our proposed method compared with state-of-the-art methods.



### Ask-n-Learn: Active Learning via Reliable Gradient Representations for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2009.14448v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.14448v1)
- **Published**: 2020-09-30 05:19:56+00:00
- **Updated**: 2020-09-30 05:19:56+00:00
- **Authors**: Bindya Venkatesh, Jayaraman J. Thiagarajan
- **Comment**: None
- **Journal**: None
- **Summary**: Deep predictive models rely on human supervision in the form of labeled training data. Obtaining large amounts of annotated training data can be expensive and time consuming, and this becomes a critical bottleneck while building such models in practice. In such scenarios, active learning (AL) strategies are used to achieve faster convergence in terms of labeling efforts. Existing active learning employ a variety of heuristics based on uncertainty and diversity to select query samples. Despite their wide-spread use, in practice, their performance is limited by a number of factors including non-calibrated uncertainties, insufficient trade-off between data exploration and exploitation, presence of confirmation bias etc. In order to address these challenges, we propose Ask-n-Learn, an active learning approach based on gradient embeddings obtained using the pesudo-labels estimated in each iteration of the algorithm. More importantly, we advocate the use of prediction calibration to obtain reliable gradient embeddings, and propose a data augmentation strategy to alleviate the effects of confirmation bias during pseudo-labeling. Through empirical studies on benchmark image classification tasks (CIFAR-10, SVHN, Fashion-MNIST, MNIST), we demonstrate significant improvements over state-of-the-art baselines, including the recently proposed BADGE algorithm.



### Learning Image-adaptive 3D Lookup Tables for High Performance Photo Enhancement in Real-time
- **Arxiv ID**: http://arxiv.org/abs/2009.14468v1
- **DOI**: 10.1109/TPAMI.2020.3026740
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.14468v1)
- **Published**: 2020-09-30 06:34:57+00:00
- **Updated**: 2020-09-30 06:34:57+00:00
- **Authors**: Hui Zeng, Jianrui Cai, Lida Li, Zisheng Cao, Lei Zhang
- **Comment**: High quality adaptive photo enhancement in real-time (<2ms for 4K
  resolution images)! Accepted by IEEE Transactions on Pattern Analysis and
  Machine Intelligence
- **Journal**: None
- **Summary**: Recent years have witnessed the increasing popularity of learning based methods to enhance the color and tone of photos. However, many existing photo enhancement methods either deliver unsatisfactory results or consume too much computational and memory resources, hindering their application to high-resolution images (usually with more than 12 megapixels) in practice. In this paper, we learn image-adaptive 3-dimensional lookup tables (3D LUTs) to achieve fast and robust photo enhancement. 3D LUTs are widely used for manipulating color and tone of photos, but they are usually manually tuned and fixed in camera imaging pipeline or photo editing tools. We, for the first time to our best knowledge, propose to learn 3D LUTs from annotated data using pairwise or unpaired learning. More importantly, our learned 3D LUT is image-adaptive for flexible photo enhancement. We learn multiple basis 3D LUTs and a small convolutional neural network (CNN) simultaneously in an end-to-end manner. The small CNN works on the down-sampled version of the input image to predict content-dependent weights to fuse the multiple basis 3D LUTs into an image-adaptive one, which is employed to transform the color and tone of source images efficiently. Our model contains less than 600K parameters and takes less than 2 ms to process an image of 4K resolution using one Titan RTX GPU. While being highly efficient, our model also outperforms the state-of-the-art photo enhancement methods by a large margin in terms of PSNR, SSIM and a color difference metric on two publically available benchmark datasets.



### The Utility of Decorrelating Colour Spaces in Vector Quantised Variational Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/2009.14487v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.14487v1)
- **Published**: 2020-09-30 07:44:01+00:00
- **Updated**: 2020-09-30 07:44:01+00:00
- **Authors**: Arash Akbarinia, Raquel Gil-Rodríguez, Alban Flachot, Matteo Toscani
- **Comment**: None
- **Journal**: None
- **Summary**: Vector quantised variational autoencoders (VQ-VAE) are characterised by three main components: 1) encoding visual data, 2) assigning $k$ different vectors in the so-called embedding space, and 3) decoding the learnt features. While images are often represented in RGB colour space, the specific organisation of colours in other spaces also offer interesting features, e.g. CIE L*a*b* decorrelates chromaticity into opponent axes. In this article, we propose colour space conversion, a simple quasi-unsupervised task, to enforce a network learning structured representations. To this end, we trained several instances of VQ-VAE whose input is an image in one colour space, and its output in another, e.g. from RGB to CIE L*a*b* (in total five colour spaces were considered). We examined the finite embedding space of trained networks in order to disentangle the colour representation in VQ-VAE models. Our analysis suggests that certain vectors encode hue and others luminance information. We further evaluated the quality of reconstructed images at low-level using pixel-wise colour metrics, and at high-level by inputting them to image classification and scene segmentation networks. We conducted experiments in three benchmark datasets: ImageNet, COCO and CelebA. Our results show, with respect to the baseline network (whose input and output are RGB), colour conversion to decorrelated spaces obtains 1-2 Delta-E lower colour difference and 5-10% higher classification accuracy. We also observed that the learnt embedding space is easier to interpret in colour opponent models.



### Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2009.14524v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.14524v1)
- **Published**: 2020-09-30 09:21:43+00:00
- **Updated**: 2020-09-30 09:21:43+00:00
- **Authors**: Deniz Beker, Hiroharu Kato, Mihai Adrian Morariu, Takahiro Ando, Toru Matsuoka, Wadim Kehl, Adrien Gaidon
- **Comment**: 20 pages, Supplementary material included, Published in ECCV 2020
- **Journal**: None
- **Summary**: 3D object detection from monocular images is an ill-posed problem due to the projective entanglement of depth and scale. To overcome this ambiguity, we present a novel self-supervised method for textured 3D shape reconstruction and pose estimation of rigid objects with the help of strong shape priors and 2D instance masks. Our method predicts the 3D location and meshes of each object in an image using differentiable rendering and a self-supervised objective derived from a pretrained monocular depth estimation network. We use the KITTI 3D object detection dataset to evaluate the accuracy of the method. Experiments demonstrate that we can effectively use noisy monocular depth and differentiable rendering as an alternative to expensive 3D ground-truth labels or LiDAR information.



### Asymmetric Contextual Modulation for Infrared Small Target Detection
- **Arxiv ID**: http://arxiv.org/abs/2009.14530v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.14530v1)
- **Published**: 2020-09-30 09:30:08+00:00
- **Updated**: 2020-09-30 09:30:08+00:00
- **Authors**: Yimian Dai, Yiquan Wu, Fei Zhou, Kobus Barnard
- **Comment**: None
- **Journal**: None
- **Summary**: Single-frame infrared small target detection remains a challenge not only due to the scarcity of intrinsic target characteristics but also because of lacking a public dataset. In this paper, we first contribute an open dataset with high-quality annotations to advance the research in this field. We also propose an asymmetric contextual modulation module specially designed for detecting infrared small targets. To better highlight small targets, besides a top-down global contextual feedback, we supplement a bottom-up modulation pathway based on point-wise channel attention for exchanging high-level semantics and subtle low-level details. We report ablation studies and comparisons to state-of-the-art methods, where we find that our approach performs significantly better. Our dataset and code are available online.



### Demographic Influences on Contemporary Art with Unsupervised Style Embeddings
- **Arxiv ID**: http://arxiv.org/abs/2009.14545v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.SI
- **Links**: [PDF](http://arxiv.org/pdf/2009.14545v2)
- **Published**: 2020-09-30 10:13:18+00:00
- **Updated**: 2020-12-01 10:34:15+00:00
- **Authors**: Nikolai Huckle, Noa Garcia, Yuta Nakashima
- **Comment**: To be published in Proceedings of the European Conference in Computer
  Vision Workshops 2020
- **Journal**: None
- **Summary**: Computational art analysis has, through its reliance on classification tasks, prioritised historical datasets in which the artworks are already well sorted with the necessary annotations. Art produced today, on the other hand, is numerous and easily accessible, through the internet and social networks that are used by professional and amateur artists alike to display their work. Although this art, yet unsorted in terms of style and genre, is less suited for supervised analysis, the data sources come with novel information that may help frame the visual content in equally novel ways. As a first step in this direction, we present contempArt, a multi-modal dataset of exclusively contemporary artworks. contempArt is a collection of paintings and drawings, a detailed graph network based on social connections on Instagram and additional socio-demographic information; all attached to 442 artists at the beginning of their career. We evaluate three methods suited for generating unsupervised style embeddings of images and correlate them with the remaining data. We find no connections between visual style on the one hand and social proximity, gender, and nationality on the other.



### FAN: Frequency Aggregation Network for Real Image Super-resolution
- **Arxiv ID**: http://arxiv.org/abs/2009.14547v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.14547v1)
- **Published**: 2020-09-30 10:18:41+00:00
- **Updated**: 2020-09-30 10:18:41+00:00
- **Authors**: Yingxue Pang, Xin Li, Xin Jin, Yaojun Wu, Jianzhao Liu, Sen Liu, Zhibo Chen
- **Comment**: 14 pages, 7 figures, presented as a workshop paper at AIM 2020
  Challenge @ ECCV 2020
- **Journal**: None
- **Summary**: Single image super-resolution (SISR) aims to recover the high-resolution (HR) image from its low-resolution (LR) input image. With the development of deep learning, SISR has achieved great progress. However, It is still a challenge to restore the real-world LR image with complicated authentic degradations. Therefore, we propose FAN, a frequency aggregation network, to address the real-world image super-resolu-tion problem. Specifically, we extract different frequencies of the LR image and pass them to a channel attention-grouped residual dense network (CA-GRDB) individually to output corresponding feature maps. And then aggregating these residual dense feature maps adaptively to recover the HR image with enhanced details and textures. We conduct extensive experiments quantitatively and qualitatively to verify that our FAN performs well on the real image super-resolution task of AIM 2020 challenge. According to the released final results, our team SR-IM achieves the fourth place on the X4 track with PSNR of 31.1735 and SSIM of 0.8728.



### Learning Object Detection from Captions via Textual Scene Attributes
- **Arxiv ID**: http://arxiv.org/abs/2009.14558v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.14558v1)
- **Published**: 2020-09-30 10:59:20+00:00
- **Updated**: 2020-09-30 10:59:20+00:00
- **Authors**: Achiya Jerbi, Roei Herzig, Jonathan Berant, Gal Chechik, Amir Globerson
- **Comment**: None
- **Journal**: None
- **Summary**: Object detection is a fundamental task in computer vision, requiring large annotated datasets that are difficult to collect, as annotators need to label objects and their bounding boxes. Thus, it is a significant challenge to use cheaper forms of supervision effectively. Recent work has begun to explore image captions as a source for weak supervision, but to date, in the context of object detection, captions have only been used to infer the categories of the objects in the image. In this work, we argue that captions contain much richer information about the image, including attributes of objects and their relations. Namely, the text represents a scene of the image, as described recently in the literature. We present a method that uses the attributes in this "textual scene graph" to train object detectors. We empirically demonstrate that the resulting model achieves state-of-the-art results on several challenging object detection datasets, outperforming recent approaches.



### Restoring Spatially-Heterogeneous Distortions using Mixture of Experts Network
- **Arxiv ID**: http://arxiv.org/abs/2009.14563v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.14563v1)
- **Published**: 2020-09-30 11:06:38+00:00
- **Updated**: 2020-09-30 11:06:38+00:00
- **Authors**: Sijin Kim, Namhyuk Ahn, Kyung-Ah Sohn
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, deep learning-based methods have been successfully applied to the image distortion restoration tasks. However, scenarios that assume a single distortion only may not be suitable for many real-world applications. To deal with such cases, some studies have proposed sequentially combined distortions datasets. Viewing in a different point of combining, we introduce a spatially-heterogeneous distortion dataset in which multiple corruptions are applied to the different locations of each image. In addition, we also propose a mixture of experts network to effectively restore a multi-distortion image. Motivated by the multi-task learning, we design our network to have multiple paths that learn both common and distortion-specific representations. Our model is effective for restoring real-world distortions and we experimentally verify that our method outperforms other models designed to manage both single distortion and multiple distortions.



### A robustness measure for singular point and index estimation in discretized orientation and vector fields
- **Arxiv ID**: http://arxiv.org/abs/2009.14570v1
- **DOI**: 10.1002/pamm.202000261
- **Categories**: **cs.CV**, cond-mat.soft, math.AT, 55M25, 26B20, 68U10, I.4.7; I.5.2
- **Links**: [PDF](http://arxiv.org/pdf/2009.14570v1)
- **Published**: 2020-09-30 11:21:19+00:00
- **Updated**: 2020-09-30 11:21:19+00:00
- **Authors**: Karl B. Hoffmann, Ivo F. Sbalzarini
- **Comment**: 4 pages, 1 figure
- **Journal**: None
- **Summary**: The identification of singular points or topological defects in discretized vector fields occurs in diverse areas ranging from the polarization of the cosmic microwave background to liquid crystals to fingerprint recognition and bio-medical imaging. Due to their discrete nature, defects and their topological charge cannot depend continuously on each single vector, but they discontinuously change as soon as a vector changes by more than a threshold. Considering this threshold of admissible change at the level of vectors, we develop a robustness measure for discrete defect estimators. Here, we compare different template paths for defect estimation in discretized vector or orientation fields. Sampling prototypical vector field patterns around defects shows that the robustness increases with the length of template path, but less so in the presence of noise on the vectors. We therefore find an optimal trade-off between resolution and robustness against noise for relatively small templates, except for the "single pixel" defect analysis, which cannot exclude zero robustness. The presented robustness measure paves the way for uncertainty quantification of defects in discretized vector fields.



### Adversarial Semi-Supervised Multi-Domain Tracking
- **Arxiv ID**: http://arxiv.org/abs/2009.14635v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.14635v1)
- **Published**: 2020-09-30 12:47:28+00:00
- **Updated**: 2020-09-30 12:47:28+00:00
- **Authors**: Kourosh Meshgi, Maryam Sadat Mirzaei
- **Comment**: Accepted for ACCV 2020
- **Journal**: None
- **Summary**: Neural networks for multi-domain learning empowers an effective combination of information from different domains by sharing and co-learning the parameters. In visual tracking, the emerging features in shared layers of a multi-domain tracker, trained on various sequences, are crucial for tracking in unseen videos. Yet, in a fully shared architecture, some of the emerging features are useful only in a specific domain, reducing the generalization of the learned feature representation. We propose a semi-supervised learning scheme to separate domain-invariant and domain-specific features using adversarial learning, to encourage mutual exclusion between them, and to leverage self-supervised learning for enhancing the shared features using the unlabeled reservoir. By employing these features and training dedicated layers for each sequence, we build a tracker that performs exceptionally on different types of videos.



### Dissected 3D CNNs: Temporal Skip Connections for Efficient Online Video Processing
- **Arxiv ID**: http://arxiv.org/abs/2009.14639v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2009.14639v2)
- **Published**: 2020-09-30 12:48:52+00:00
- **Updated**: 2021-10-18 13:47:49+00:00
- **Authors**: Okan Köpüklü, Stefan Hörmann, Fabian Herzog, Hakan Cevikalp, Gerhard Rigoll
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional Neural Networks with 3D kernels (3D-CNNs) currently achieve state-of-the-art results in video recognition tasks due to their supremacy in extracting spatiotemporal features within video frames. There have been many successful 3D-CNN architectures surpassing the state-of-the-art results successively. However, nearly all of them are designed to operate offline creating several serious handicaps during online operation. Firstly, conventional 3D-CNNs are not dynamic since their output features represent the complete input clip instead of the most recent frame in the clip. Secondly, they are not temporal resolution-preserving due to their inherent temporal downsampling. Lastly, 3D-CNNs are constrained to be used with fixed temporal input size limiting their flexibility. In order to address these drawbacks, we propose dissected 3D-CNNs, where the intermediate volumes of the network are dissected and propagated over depth (time) dimension for future calculations, substantially reducing the number of computations at online operation. For action classification, the dissected version of ResNet models performs 77-90% fewer computations at online operation while achieving ~5% better classification accuracy on the Kinetics-600 dataset than conventional 3D-ResNet models. Moreover, the advantages of dissected 3D-CNNs are demonstrated by deploying our approach onto several vision tasks, which consistently improved the performance.



### Driver Anomaly Detection: A Dataset and Contrastive Learning Approach
- **Arxiv ID**: http://arxiv.org/abs/2009.14660v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2009.14660v2)
- **Published**: 2020-09-30 13:23:21+00:00
- **Updated**: 2020-11-30 15:00:06+00:00
- **Authors**: Okan Köpüklü, Jiapeng Zheng, Hang Xu, Gerhard Rigoll
- **Comment**: Accepted to IEEE Winter Conference on Applications of Computer Vision
  (WACV 2021)
- **Journal**: None
- **Summary**: Distracted drivers are more likely to fail to anticipate hazards, which result in car accidents. Therefore, detecting anomalies in drivers' actions (i.e., any action deviating from normal driving) contains the utmost importance to reduce driver-related accidents. However, there are unbounded many anomalous actions that a driver can do while driving, which leads to an 'open set recognition' problem. Accordingly, instead of recognizing a set of anomalous actions that are commonly defined by previous dataset providers, in this work, we propose a contrastive learning approach to learn a metric to differentiate normal driving from anomalous driving. For this task, we introduce a new video-based benchmark, the Driver Anomaly Detection (DAD) dataset, which contains normal driving videos together with a set of anomalous actions in its training set. In the test set of the DAD dataset, there are unseen anomalous actions that still need to be winnowed out from normal driving. Our method reaches 0.9673 AUC on the test set, demonstrating the effectiveness of the contrastive learning approach on the anomaly detection task. Our dataset, codes and pre-trained models are publicly available.



### Encode the Unseen: Predictive Video Hashing for Scalable Mid-Stream Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2009.14661v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.14661v2)
- **Published**: 2020-09-30 13:25:59+00:00
- **Updated**: 2020-10-02 13:11:34+00:00
- **Authors**: Tong Yu, Nicolas Padoy
- **Comment**: Accepted at ACCV 2020
- **Journal**: None
- **Summary**: This paper tackles a new problem in computer vision: mid-stream video-to-video retrieval. This task, which consists in searching a database for content similar to a video right as it is playing, e.g. from a live stream, exhibits challenging characteristics. Only the beginning part of the video is available as query and new frames are constantly added as the video plays out. To perform retrieval in this demanding situation, we propose an approach based on a binary encoder that is both predictive and incremental in order to (1) account for the missing video content at query time and (2) keep up with repeated, continuously evolving queries throughout the streaming. In particular, we present the first hashing framework that infers the unseen future content of a currently playing video. Experiments on FCVID and ActivityNet demonstrate the feasibility of this task. Our approach also yields a significant mAP@20 performance increase compared to a baseline adapted from the literature for this task, for instance 7.4% (2.6%) increase at 20% (50%) of elapsed runtime on FCVID using bitcodes of size 192 bits.



### Benchmark for Anonymous Video Analytics
- **Arxiv ID**: http://arxiv.org/abs/2009.14684v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.14684v3)
- **Published**: 2020-09-30 14:01:16+00:00
- **Updated**: 2021-10-03 12:25:16+00:00
- **Authors**: Ricardo Sanchez-Matilla, Andrea Cavallaro
- **Comment**: None
- **Journal**: None
- **Summary**: Out-of-home audience measurement aims to count and characterize the people exposed to advertising content in the physical world. While audience measurement solutions based on computer vision are of increasing interest, no commonly accepted benchmark exists to evaluate and compare their performance. In this paper, we propose the first benchmark for digital out-of-home audience measurement that evaluates the vision-based tasks of audience localization and counting, and audience demographics. The benchmark is composed of a novel, dataset captured at multiple locations and a set of performance measures. Using the benchmark, we present an in-depth comparison of eight open-source algorithms on four hardware platforms with GPU and CPU-optimized inferences and of two commercial off-the-shelf solutions for localization, count, age, and gender estimation. This benchmark and related open-source codes are available at http://ava.eecs.qmul.ac.uk.



### Where Does Trust Break Down? A Quantitative Trust Analysis of Deep Neural Networks via Trust Matrix and Conditional Trust Densities
- **Arxiv ID**: http://arxiv.org/abs/2009.14701v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2009.14701v1)
- **Published**: 2020-09-30 14:33:43+00:00
- **Updated**: 2020-09-30 14:33:43+00:00
- **Authors**: Andrew Hryniowski, Xiao Yu Wang, Alexander Wong
- **Comment**: 5 pages
- **Journal**: None
- **Summary**: The advances and successes in deep learning in recent years have led to considerable efforts and investments into its widespread ubiquitous adoption for a wide variety of applications, ranging from personal assistants and intelligent navigation to search and product recommendation in e-commerce. With this tremendous rise in deep learning adoption comes questions about the trustworthiness of the deep neural networks that power these applications. Motivated to answer such questions, there has been a very recent interest in trust quantification. In this work, we introduce the concept of trust matrix, a novel trust quantification strategy that leverages the recently introduced question-answer trust metric by Wong et al. to provide deeper, more detailed insights into where trust breaks down for a given deep neural network given a set of questions. More specifically, a trust matrix defines the expected question-answer trust for a given actor-oracle answer scenario, allowing one to quickly spot areas of low trust that needs to be addressed to improve the trustworthiness of a deep neural network. The proposed trust matrix is simple to calculate, humanly interpretable, and to the best of the authors' knowledge is the first to study trust at the actor-oracle answer level. We further extend the concept of trust densities with the notion of conditional trust densities. We experimentally leverage trust matrices to study several well-known deep neural network architectures for image recognition, and further study the trust density and conditional trust densities for an interesting actor-oracle answer scenario. The results illustrate that trust matrices, along with conditional trust densities, can be useful tools in addition to the existing suite of trust quantification metrics for guiding practitioners and regulators in creating and certifying deep learning solutions for trusted operation.



### Investigating Cultural Aspects in the Fundamental Diagram using Convolutional Neural Networks and Simulation
- **Arxiv ID**: http://arxiv.org/abs/2010.11995v1
- **DOI**: 10.1002/cav.1899
- **Categories**: **cs.OH**, cs.CV, cs.GR, physics.soc-ph
- **Links**: [PDF](http://arxiv.org/pdf/2010.11995v1)
- **Published**: 2020-09-30 14:44:04+00:00
- **Updated**: 2020-09-30 14:44:04+00:00
- **Authors**: Rodolfo M. Favaretto, Roberto R. Santos, Marcio Ballotin, Paulo Knob, Soraia R. Musse, Felipe Vilanova, Angelo B. Costa
- **Comment**: Computer Animation and Virtual Worlds, 2019
- **Journal**: None
- **Summary**: This paper presents a study regarding group behavior in a controlled experiment focused on differences in an important attribute that vary across cultures -- the personal spaces -- in two Countries: Brazil and Germany. In order to coherently compare Germany and Brazil evolutions with same population applying same task, we performed the pedestrian Fundamental Diagram experiment in Brazil, as performed in Germany. We use CNNs to detect and track people in video sequences. With this data, we use Voronoi Diagrams to find out the neighbor relation among people and then compute the walking distances to find out the personal spaces. Based on personal spaces analyses, we found out that people behavior is more similar, in terms of their behaviours, in high dense populations and vary more in low and medium densities. So, we focused our study on cultural differences between the two Countries in low and medium densities. Results indicate that personal space analyses can be a relevant feature in order to understand cultural aspects in video sequences. In addition to the cultural differences, we also investigate the personality model in crowds, using OCEAN. We also proposed a way to simulate the FD experiment from other countries using the OCEAN psychological traits model as input. The simulated countries were consistent with the literature.



### S3K: Self-Supervised Semantic Keypoints for Robotic Manipulation via Multi-View Consistency
- **Arxiv ID**: http://arxiv.org/abs/2009.14711v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.14711v2)
- **Published**: 2020-09-30 14:44:54+00:00
- **Updated**: 2020-10-13 10:42:41+00:00
- **Authors**: Mel Vecerik, Jean-Baptiste Regli, Oleg Sushkov, David Barker, Rugile Pevceviciute, Thomas Rothörl, Christopher Schuster, Raia Hadsell, Lourdes Agapito, Jonathan Scholz
- **Comment**: 11 pages, supplementary material available at:
  https://sites.google.com/view/2020-s3k/home
- **Journal**: None
- **Summary**: A robot's ability to act is fundamentally constrained by what it can perceive. Many existing approaches to visual representation learning utilize general-purpose training criteria, e.g. image reconstruction, smoothness in latent space, or usefulness for control, or else make use of large datasets annotated with specific features (bounding boxes, segmentations, etc.). However, both approaches often struggle to capture the fine-detail required for precision tasks on specific objects, e.g. grasping and mating a plug and socket. We argue that these difficulties arise from a lack of geometric structure in these models. In this work we advocate semantic 3D keypoints as a visual representation, and present a semi-supervised training objective that can allow instance or category-level keypoints to be trained to 1-5 millimeter-accuracy with minimal supervision. Furthermore, unlike local texture-based approaches, our model integrates contextual information from a large area and is therefore robust to occlusion, noise, and lack of discernible texture. We demonstrate that this ability to locate semantic keypoints enables high level scripting of human understandable behaviours. Finally we show that these keypoints provide a good way to define reward functions for reinforcement learning and are a good representation for training agents.



### Deep Learning-based Pipeline for Module Power Prediction from EL Measurements
- **Arxiv ID**: http://arxiv.org/abs/2009.14712v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2009.14712v2)
- **Published**: 2020-09-30 14:46:47+00:00
- **Updated**: 2020-11-26 10:25:54+00:00
- **Authors**: Mathis Hoffmann, Claudia Buerhop-Lutz, Luca Reeb, Tobias Pickel, Thilo Winkler, Bernd Doll, Tobias Würfl, Ian Marius Peters, Christoph Brabec, Andreas Maier, Vincent Christlein
- **Comment**: None
- **Journal**: None
- **Summary**: Automated inspection plays an important role in monitoring large-scale photovoltaic power plants. Commonly, electroluminescense measurements are used to identify various types of defects on solar modules but have not been used to determine the power of a module. However, knowledge of the power at maximum power point is important as well, since drops in the power of a single module can affect the performance of an entire string. By now, this is commonly determined by measurements that require to discontact or even dismount the module, rendering a regular inspection of individual modules infeasible. In this work, we bridge the gap between electroluminescense measurements and the power determination of a module. We compile a large dataset of 719 electroluminescense measurementsof modules at various stages of degradation, especially cell cracks and fractures, and the corresponding power at maximum power point. Here,we focus on inactive regions and cracks as the predominant type of defect. We set up a baseline regression model to predict the power from electroluminescense measurements with a mean absolute error of 9.0+/-3.7$W_P$ (4.0+/-8.4%). Then, we show that deep-learning can be used to train a model that performs significantly better (7.3+/-2.7$W_P$ or 3.2+/-6.5%) and propose a variant of class activation maps to obtain the per cell power loss, as predicted by the model. With this work, we aim to open a new research topic. Therefore, we publicly release the dataset, the code and trained models to empower other researchers to compare against our results. Finally, we present a thorough evaluation of certain boundary conditions like the dataset size and an automated preprocessing pipeline for on-site measurements showing multiple modules at once.



### Efficient texture-aware multi-GAN for image inpainting
- **Arxiv ID**: http://arxiv.org/abs/2009.14721v2
- **DOI**: 10.1016/j.knosys.2021.106789
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.14721v2)
- **Published**: 2020-09-30 14:58:03+00:00
- **Updated**: 2021-02-13 15:19:43+00:00
- **Authors**: Mohamed Abbas Hedjazi, Yakup Genc
- **Comment**: 25 pages, 15 figures, 11 tables
- **Journal**: Knowledge-Based Systems, Volume 217, 6 April 2021, 106789
- **Summary**: Recent GAN-based (Generative adversarial networks) inpainting methods show remarkable improvements and generate plausible images using multi-stage networks or Contextual Attention Modules (CAM). However, these techniques increase the model complexity limiting their application in low-resource environments. Furthermore, they fail in generating high-resolution images with realistic texture details due to the GAN stability problem. Motivated by these observations, we propose a multi-GAN architecture improving both the performance and rendering efficiency. Our training schema optimizes the parameters of four progressive efficient generators and discriminators in an end-to-end manner. Filling in low-resolution images is less challenging for GANs due to the small dimensional space. Meanwhile, it guides higher resolution generators to learn the global structure consistency of the image. To constrain the inpainting task and ensure fine-grained textures, we adopt an LBP-based loss function to minimize the difference between the generated and the ground truth textures. We conduct our experiments on Places2 and CelebHQ datasets. Qualitative and quantitative results show that the proposed method not only performs favorably against state-of-the-art algorithms but also speeds up the inference time.



### Training general representations for remote sensing using in-domain knowledge
- **Arxiv ID**: http://arxiv.org/abs/2010.00332v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.00332v1)
- **Published**: 2020-09-30 15:00:07+00:00
- **Updated**: 2020-09-30 15:00:07+00:00
- **Authors**: Maxim Neumann, André Susano Pinto, Xiaohua Zhai, Neil Houlsby
- **Comment**: Accepted at the IEEE International Geoscience and Remote Sensing
  Symposium (IGARSS) 2020. arXiv admin note: substantial text overlap with
  arXiv:1911.06721
- **Journal**: None
- **Summary**: Automatically finding good and general remote sensing representations allows to perform transfer learning on a wide range of applications - improving the accuracy and reducing the required number of training samples. This paper investigates development of generic remote sensing representations, and explores which characteristics are important for a dataset to be a good source for representation learning. For this analysis, five diverse remote sensing datasets are selected and used for both, disjoint upstream representation learning and downstream model training and evaluation. A common evaluation protocol is used to establish baselines for these datasets that achieve state-of-the-art performance. As the results indicate, especially with a low number of available training samples a significant performance enhancement can be observed when including additionally in-domain data in comparison to training models from scratch or fine-tuning only on ImageNet (up to 11% and 40%, respectively, at 100 training samples). All datasets and pretrained representation models are published online.



### Improving Auto-Augment via Augmentation-Wise Weight Sharing
- **Arxiv ID**: http://arxiv.org/abs/2009.14737v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2009.14737v2)
- **Published**: 2020-09-30 15:23:12+00:00
- **Updated**: 2020-10-22 15:12:47+00:00
- **Authors**: Keyu Tian, Chen Lin, Ming Sun, Luping Zhou, Junjie Yan, Wanli Ouyang
- **Comment**: Accepted to NeurIPS 2020 (Poster)
- **Journal**: None
- **Summary**: The recent progress on automatically searching augmentation policies has boosted the performance substantially for various tasks. A key component of automatic augmentation search is the evaluation process for a particular augmentation policy, which is utilized to return reward and usually runs thousands of times. A plain evaluation process, which includes full model training and validation, would be time-consuming. To achieve efficiency, many choose to sacrifice evaluation reliability for speed. In this paper, we dive into the dynamics of augmented training of the model. This inspires us to design a powerful and efficient proxy task based on the Augmentation-Wise Weight Sharing (AWS) to form a fast yet accurate evaluation process in an elegant way. Comprehensive analysis verifies the superiority of this approach in terms of effectiveness and efficiency. The augmentation policies found by our method achieve superior accuracies compared with existing auto-augmentation search methods. On CIFAR-10, we achieve a top-1 error rate of 1.24%, which is currently the best performing single model without extra training data. On ImageNet, we get a top-1 error rate of 20.36% for ResNet-50, which leads to 3.34% absolute error rate reduction over the baseline augmentation.



### Multi-channel Deep 3D Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2009.14743v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CG
- **Links**: [PDF](http://arxiv.org/pdf/2009.14743v1)
- **Published**: 2020-09-30 15:29:05+00:00
- **Updated**: 2020-09-30 15:29:05+00:00
- **Authors**: Zhiqian You, Tingting Yang, Miao Jin
- **Comment**: None
- **Journal**: None
- **Summary**: Face recognition has been of great importance in many applications as a biometric for its throughput, convenience, and non-invasiveness. Recent advancements in deep Convolutional Neural Network (CNN) architectures have boosted significantly the performance of face recognition based on two-dimensional (2D) facial texture images and outperformed the previous state of the art using conventional methods. However, the accuracy of 2D face recognition is still challenged by the change of pose, illumination, make-up, and expression. On the other hand, the geometric information contained in three-dimensional (3D) face data has the potential to overcome the fundamental limitations of 2D face data.   We propose a multi-Channel deep 3D face network for face recognition based on 3D face data. We compute the geometric information of a 3D face based on its piecewise-linear triangular mesh structure and then conformally flatten geometric information along with the color from 3D to 2D plane to leverage the state-of-the-art deep CNN architectures. We modify the input layer of the network to take images with nine channels instead of three only such that more geometric information can be explicitly fed to it. We pre-train the network using images from the VGG-Face \cite{Parkhi2015} and then fine-tune it with the generated multi-channel face images. The face recognition accuracy of the multi-Channel deep 3D face network has achieved 98.6. The experimental results also clearly show that the network performs much better when a 9-channel image is flattened to plane based on the conformal map compared with the orthographic projection.



### GraphXCOVID: Explainable Deep Graph Diffusion Pseudo-Labelling for Identifying COVID-19 on Chest X-rays
- **Arxiv ID**: http://arxiv.org/abs/2010.00378v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2010.00378v2)
- **Published**: 2020-09-30 15:38:24+00:00
- **Updated**: 2021-07-04 18:30:43+00:00
- **Authors**: Angelica I Aviles-Rivero, Philip Sellars, Carola-Bibiane Schönlieb, Nicolas Papadakis
- **Comment**: None
- **Journal**: None
- **Summary**: Can one learn to diagnose COVID-19 under extreme minimal supervision? Since the outbreak of the novel COVID-19 there has been a rush for developing Artificial Intelligence techniques for expert-level disease identification on Chest X-ray data. In particular, the use of deep supervised learning has become the go-to paradigm. However, the performance of such models is heavily dependent on the availability of a large and representative labelled dataset. The creation of which is a heavily expensive and time consuming task, and especially imposes a great challenge for a novel disease. Semi-supervised learning has shown the ability to match the incredible performance of supervised models whilst requiring a small fraction of the labelled examples. This makes the semi-supervised paradigm an attractive option for identifying COVID-19. In this work, we introduce a graph based deep semi-supervised framework for classifying COVID-19 from chest X-rays. Our framework introduces an optimisation model for graph diffusion that reinforces the natural relation among the tiny labelled set and the vast unlabelled data. We then connect the diffusion prediction output as pseudo-labels that are used in an iterative scheme in a deep net. We demonstrate, through our experiments, that our model is able to outperform the current leading supervised model with a tiny fraction of the labelled examples. Finally, we provide attention maps to accommodate the radiologist's mental model, better fitting their perceptual and cognitive abilities. These visualisation aims to assist the radiologist in judging whether the diagnostic is correct or not, and in consequence to accelerate the decision.



### Enhanced Standard Compatible Image Compression Framework based on Auxiliary Codec Networks
- **Arxiv ID**: http://arxiv.org/abs/2009.14754v2
- **DOI**: 10.1109/TIP.2021.3134473
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.14754v2)
- **Published**: 2020-09-30 15:42:06+00:00
- **Updated**: 2021-12-15 05:59:04+00:00
- **Authors**: Hanbin Son, Taeoh Kim, Hyeongmin Lee, Sangyoun Lee
- **Comment**: Accepted by IEEE Transactions on image processing
- **Journal**: None
- **Summary**: To enhance image compression performance, recent deep neural network-based research can be divided into three categories: a learnable codec, a postprocessing network, and a compact representation network. The learnable codec has been designed for an end-to-end learning beyond the conventional compression modules. The postprocessing network increases the quality of decoded images using an example-based learning. The compact representation network is learned to reduce the capacity of an input image to reduce the bitrate while keeping the quality of the decoded image. However, these approaches are not compatible with the existing codecs or not optimal to increase the coding efficiency. Specifically, it is difficult to achieve optimal learning in the previous studies using the compact representation network, due to the inaccurate consideration of the codecs. In this paper, we propose a novel standard compatible image compression framework based on Auxiliary Codec Networks (ACNs). ACNs are designed to imitate image degradation operations of the existing codec, which delivers more accurate gradients to the compact representation network. Therefore, the compact representation and the postprocessing networks can be learned effectively and optimally. We demonstrate that our proposed framework based on JPEG and High Efficiency Video Coding (HEVC) standard substantially outperforms existing image compression algorithms in a standard compatible manner.



### Attention-Aware Noisy Label Learning for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2009.14757v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.14757v1)
- **Published**: 2020-09-30 15:45:36+00:00
- **Updated**: 2020-09-30 15:45:36+00:00
- **Authors**: Zhenzhen Wang, Chunyan Xu, Yap-Peng Tan, Junsong Yuan
- **Comment**: 10 pages, 8 figures
- **Journal**: None
- **Summary**: Deep convolutional neural networks (CNNs) learned on large-scale labeled samples have achieved remarkable progress in computer vision, such as image/video classification. The cheapest way to obtain a large body of labeled visual data is to crawl from websites with user-supplied labels, such as Flickr. However, these samples often tend to contain incorrect labels (i.e. noisy labels), which will significantly degrade the network performance. In this paper, the attention-aware noisy label learning approach ($A^2NL$) is proposed to improve the discriminative capability of the network trained on datasets with potential label noise. Specifically, a Noise-Attention model, which contains multiple noise-specific units, is designed to better capture noisy information. Each unit is expected to learn a specific noisy distribution for a subset of images so that different disturbances are more precisely modeled. Furthermore, a recursive learning process is introduced to strengthen the learning ability of the attention network by taking advantage of the learned high-level knowledge. To fully evaluate the proposed method, we conduct experiments from two aspects: manually flipped label noise on large-scale image classification datasets, including CIFAR-10, SVHN; and real-world label noise on an online crawled clothing dataset with multiple attributes. The superior results over state-of-the-art methods validate the effectiveness of our proposed approach.



### Joint Contrastive Learning with Infinite Possibilities
- **Arxiv ID**: http://arxiv.org/abs/2009.14776v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.14776v2)
- **Published**: 2020-09-30 16:24:21+00:00
- **Updated**: 2020-10-10 13:27:10+00:00
- **Authors**: Qi Cai, Yu Wang, Yingwei Pan, Ting Yao, Tao Mei
- **Comment**: NeurIPS 2020 Spotlight; Code is publicly available at:
  https://github.com/caiqi/Joint-Contrastive-Learning
- **Journal**: None
- **Summary**: This paper explores useful modifications of the recent development in contrastive learning via novel probabilistic modeling. We derive a particular form of contrastive loss named Joint Contrastive Learning (JCL). JCL implicitly involves the simultaneous learning of an infinite number of query-key pairs, which poses tighter constraints when searching for invariant features. We derive an upper bound on this formulation that allows analytical solutions in an end-to-end training manner. While JCL is practically effective in numerous computer vision applications, we also theoretically unveil the certain mechanisms that govern the behavior of JCL. We demonstrate that the proposed formulation harbors an innate agency that strongly favors similarity within each instance-specific class, and therefore remains advantageous when searching for discriminative features among distinct instances. We evaluate these proposals on multiple benchmarks, demonstrating considerable improvements over existing algorithms. Code is publicly available at: https://github.com/caiqi/Joint-Contrastive-Learning.



### 3D Dense Geometry-Guided Facial Expression Synthesis by Adversarial Learning
- **Arxiv ID**: http://arxiv.org/abs/2009.14798v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.14798v1)
- **Published**: 2020-09-30 17:12:35+00:00
- **Updated**: 2020-09-30 17:12:35+00:00
- **Authors**: Rumeysa Bodur, Binod Bhattarai, Tae-Kyun Kim
- **Comment**: None
- **Journal**: None
- **Summary**: Manipulating facial expressions is a challenging task due to fine-grained shape changes produced by facial muscles and the lack of input-output pairs for supervised learning. Unlike previous methods using Generative Adversarial Networks (GAN), which rely on cycle-consistency loss or sparse geometry (landmarks) loss for expression synthesis, we propose a novel GAN framework to exploit 3D dense (depth and surface normals) information for expression manipulation. However, a large-scale dataset containing RGB images with expression annotations and their corresponding depth maps is not available. To this end, we propose to use an off-the-shelf state-of-the-art 3D reconstruction model to estimate the depth and create a large-scale RGB-Depth dataset after a manual data clean-up process. We utilise this dataset to minimise the novel depth consistency loss via adversarial learning (note we do not have ground truth depth maps for generated face images) and the depth categorical loss of synthetic data on the discriminator. In addition, to improve the generalisation and lower the bias of the depth parameters, we propose to use a novel confidence regulariser on the discriminator side of the framework. We extensively performed both quantitative and qualitative evaluations on two publicly available challenging facial expression benchmarks: AffectNet and RaFD. Our experiments demonstrate that the proposed method outperforms the competitive baseline and existing arts by a large margin.



### RG-Flow: A hierarchical and explainable flow model based on renormalization group and sparse prior
- **Arxiv ID**: http://arxiv.org/abs/2010.00029v5
- **DOI**: 10.1088/2632-2153/ac8393
- **Categories**: **cs.LG**, cond-mat.dis-nn, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2010.00029v5)
- **Published**: 2020-09-30 18:04:04+00:00
- **Updated**: 2022-08-15 09:50:27+00:00
- **Authors**: Hong-Ye Hu, Dian Wu, Yi-Zhuang You, Bruno Olshausen, Yubei Chen
- **Comment**: 31 pages, 20 figures, 3 tables
- **Journal**: Mach. Learn.: Sci. Technol. 3 035009 (2022)
- **Summary**: Flow-based generative models have become an important class of unsupervised learning approaches. In this work, we incorporate the key ideas of renormalization group (RG) and sparse prior distribution to design a hierarchical flow-based generative model, RG-Flow, which can separate information at different scales of images and extract disentangled representations at each scale. We demonstrate our method on synthetic multi-scale image datasets and the CelebA dataset, showing that the disentangled representations enable semantic manipulation and style mixing of the images at different scales. To visualize the latent representations, we introduce receptive fields for flow-based models and show that the receptive fields of RG-Flow are similar to those of convolutional neural networks. In addition, we replace the widely adopted isotropic Gaussian prior distribution by the sparse Laplacian distribution to further enhance the disentanglement of representations. From a theoretical perspective, our proposed method has $O(\log L)$ complexity for inpainting of an image with edge length $L$, compared to previous generative models with $O(L^2)$ complexity.



### Sampling possible reconstructions of undersampled acquisitions in MR imaging
- **Arxiv ID**: http://arxiv.org/abs/2010.00042v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.AP
- **Links**: [PDF](http://arxiv.org/pdf/2010.00042v3)
- **Published**: 2020-09-30 18:20:06+00:00
- **Updated**: 2022-02-09 10:10:22+00:00
- **Authors**: Kerem C. Tezcan, Neerav Karani, Christian F. Baumgartner, Ender Konukoglu
- **Comment**: Accepted to IEEE Transactions in Medical Imaging. Main article and
  appendix together. GIFs and code can be found on
  https://github.com/kctezcan/sampling
- **Journal**: None
- **Summary**: Undersampling the k-space during MR acquisitions saves time, however results in an ill-posed inversion problem, leading to an infinite set of images as possible solutions. Traditionally, this is tackled as a reconstruction problem by searching for a single "best" image out of this solution set according to some chosen regularization or prior. This approach, however, misses the possibility of other solutions and hence ignores the uncertainty in the inversion process. In this paper, we propose a method that instead returns multiple images which are possible under the acquisition model and the chosen prior to capture the uncertainty in the inversion process. To this end, we introduce a low dimensional latent space and model the posterior distribution of the latent vectors given the acquisition data in k-space, from which we can sample in the latent space and obtain the corresponding images. We use a variational autoencoder for the latent model and the Metropolis adjusted Langevin algorithm for the sampling. We evaluate our method on two datasets; with images from the Human Connectome Project and in-house measured multi-coil images. We compare to five alternative methods. Results indicate that the proposed method produces images that match the measured k-space data better than the alternatives, while showing realistic structural variability. Furthermore, in contrast to the compared methods, the proposed method yields higher uncertainty in the undersampled phase encoding direction, as expected.   Keywords: Magnetic Resonance image reconstruction, uncertainty estimation, inverse problems, sampling, MCMC, deep learning, unsupervised learning.



### DOT: Dynamic Object Tracking for Visual SLAM
- **Arxiv ID**: http://arxiv.org/abs/2010.00052v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.00052v1)
- **Published**: 2020-09-30 18:36:28+00:00
- **Updated**: 2020-09-30 18:36:28+00:00
- **Authors**: Irene Ballester, Alejandro Fontan, Javier Civera, Klaus H. Strobl, Rudolph Triebel
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we present DOT (Dynamic Object Tracking), a front-end that added to existing SLAM systems can significantly improve their robustness and accuracy in highly dynamic environments. DOT combines instance segmentation and multi-view geometry to generate masks for dynamic objects in order to allow SLAM systems based on rigid scene models to avoid such image areas in their optimizations.   To determine which objects are actually moving, DOT segments first instances of potentially dynamic objects and then, with the estimated camera motion, tracks such objects by minimizing the photometric reprojection error. This short-term tracking improves the accuracy of the segmentation with respect to other approaches. In the end, only actually dynamic masks are generated. We have evaluated DOT with ORB-SLAM 2 in three public datasets. Our results show that our approach improves significantly the accuracy and robustness of ORB-SLAM 2, especially in highly dynamic scenes.



### Depth Estimation from Monocular Images and Sparse Radar Data
- **Arxiv ID**: http://arxiv.org/abs/2010.00058v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.00058v1)
- **Published**: 2020-09-30 19:01:33+00:00
- **Updated**: 2020-09-30 19:01:33+00:00
- **Authors**: Juan-Ting Lin, Dengxin Dai, Luc Van Gool
- **Comment**: 9 pages, 6 figures, Accepted to 2020 IEEE International Conference on
  Intelligent Robots and Systems (IROS 2020)
- **Journal**: None
- **Summary**: In this paper, we explore the possibility of achieving a more accurate depth estimation by fusing monocular images and Radar points using a deep neural network. We give a comprehensive study of the fusion between RGB images and Radar measurements from different aspects and proposed a working solution based on the observations. We find that the noise existing in Radar measurements is one of the main key reasons that prevents one from applying the existing fusion methods developed for LiDAR data and images to the new fusion problem between Radar data and images. The experiments are conducted on the nuScenes dataset, which is one of the first datasets which features Camera, Radar, and LiDAR recordings in diverse scenes and weather conditions. Extensive experiments demonstrate that our method outperforms existing fusion methods. We also provide detailed ablation studies to show the effectiveness of each component in our method.



### GCNNMatch: Graph Convolutional Neural Networks for Multi-Object Tracking via Sinkhorn Normalization
- **Arxiv ID**: http://arxiv.org/abs/2010.00067v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.00067v4)
- **Published**: 2020-09-30 19:18:44+00:00
- **Updated**: 2021-04-16 16:36:09+00:00
- **Authors**: Ioannis Papakis, Abhijit Sarkar, Anuj Karpatne
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a novel method for online Multi-Object Tracking (MOT) using Graph Convolutional Neural Network (GCNN) based feature extraction and end-to-end feature matching for object association. The Graph based approach incorporates both appearance and geometry of objects at past frames as well as the current frame into the task of feature learning. This new paradigm enables the network to leverage the "context" information of the geometry of objects and allows us to model the interactions among the features of multiple objects. Another central innovation of our proposed framework is the use of the Sinkhorn algorithm for end-to-end learning of the associations among objects during model training. The network is trained to predict object associations by taking into account constraints specific to the MOT task. Experimental results demonstrate the efficacy of the proposed approach in achieving top performance on the MOT '15, '16, '17 and '20 Challenges among state-of-the-art online approaches. The code is available at https://github.com/IPapakis/GCNNMatch.



### The Importance of Balanced Data Sets: Analyzing a Vehicle Trajectory Prediction Model based on Neural Networks and Distributed Representations
- **Arxiv ID**: http://arxiv.org/abs/2010.00084v1
- **DOI**: 10.1109/IJCNN48605.2020.9206627
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2010.00084v1)
- **Published**: 2020-09-30 20:00:11+00:00
- **Updated**: 2020-09-30 20:00:11+00:00
- **Authors**: Florian Mirus, Terrence C. Stewart, Jorg Conradt
- **Comment**: None
- **Journal**: None
- **Summary**: Predicting future behavior of other traffic participants is an essential task that needs to be solved by automated vehicles and human drivers alike to achieve safe and situationaware driving. Modern approaches to vehicles trajectory prediction typically rely on data-driven models like neural networks, in particular LSTMs (Long Short-Term Memorys), achieving promising results. However, the question of optimal composition of the underlying training data has received less attention. In this paper, we expand on previous work on vehicle trajectory prediction based on neural network models employing distributed representations to encode automotive scenes in a semantic vector substrate. We analyze the influence of variations in the training data on the performance of our prediction models. Thereby, we show that the models employing our semantic vector representation outperform the numerical model when trained on an adequate data set and thereby, that the composition of training data in vehicle trajectory prediction is crucial for successful training. We conduct our analysis on challenging real-world driving data.



### MaterialGAN: Reflectance Capture using a Generative SVBRDF Model
- **Arxiv ID**: http://arxiv.org/abs/2010.00114v1
- **DOI**: 10.1145/3414685.3417779
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.00114v1)
- **Published**: 2020-09-30 21:33:00+00:00
- **Updated**: 2020-09-30 21:33:00+00:00
- **Authors**: Yu Guo, Cameron Smith, Miloš Hašan, Kalyan Sunkavalli, Shuang Zhao
- **Comment**: 13 pages, 16 figures. Siggraph Asia 2020
- **Journal**: None
- **Summary**: We address the problem of reconstructing spatially-varying BRDFs from a small set of image measurements. This is a fundamentally under-constrained problem, and previous work has relied on using various regularization priors or on capturing many images to produce plausible results. In this work, we present MaterialGAN, a deep generative convolutional network based on StyleGAN2, trained to synthesize realistic SVBRDF parameter maps. We show that MaterialGAN can be used as a powerful material prior in an inverse rendering framework: we optimize in its latent representation to generate material maps that match the appearance of the captured images when rendered. We demonstrate this framework on the task of reconstructing SVBRDFs from images captured under flash illumination using a hand-held mobile phone. Our method succeeds in producing plausible material maps that accurately reproduce the target images, and outperforms previous state-of-the-art material capture methods in evaluations on both synthetic and real data. Furthermore, our GAN-based latent space allows for high-level semantic material editing operations such as generating material variations and material morphing.



### Self-Guided Multiple Instance Learning for Weakly Supervised Disease Classification and Localization in Chest Radiographs
- **Arxiv ID**: http://arxiv.org/abs/2010.00127v1
- **DOI**: 10.1007/978-3-030-69541-5_37
- **Categories**: **cs.CV**, cs.LG, eess.IV, I.4.0
- **Links**: [PDF](http://arxiv.org/pdf/2010.00127v1)
- **Published**: 2020-09-30 22:19:40+00:00
- **Updated**: 2020-09-30 22:19:40+00:00
- **Authors**: Constantin Seibold, Jens Kleesiek, Heinz-Peter Schlemmer, Rainer Stiefelhagen
- **Comment**: None
- **Journal**: None
- **Summary**: The lack of fine-grained annotations hinders the deployment of automated diagnosis systems, which require human-interpretable justification for their decision process. In this paper, we address the problem of weakly supervised identification and localization of abnormalities in chest radiographs. To that end, we introduce a novel loss function for training convolutional neural networks increasing the \emph{localization confidence} and assisting the overall \emph{disease identification}. The loss leverages both image- and patch-level predictions to generate auxiliary supervision. Rather than forming strictly binary from the predictions as done in previous loss formulations, we create targets in a more customized manner, which allows the loss to account for possible misclassification. We show that the supervision provided within the proposed learning scheme leads to better performance and more precise predictions on prevalent datasets for multiple-instance learning as well as on the NIH~ChestX-Ray14 benchmark for disease recognition than previously used losses.



### A study on using image based machine learning methods to develop the surrogate models of stamp forming simulations
- **Arxiv ID**: http://arxiv.org/abs/2010.03370v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2010.03370v1)
- **Published**: 2020-09-30 22:46:56+00:00
- **Updated**: 2020-09-30 22:46:56+00:00
- **Authors**: Haosu Zhou, Qingfeng Xu, Nan Li
- **Comment**: 16 pages, 14 figures
- **Journal**: None
- **Summary**: In the design optimization of metal forming, it is increasingly significant to use surrogate models to analyse the finite element analysis (FEA) simulations. However, traditional surrogate models using scalar based machine learning methods (SBMLMs) fall in short of accuracy and generalizability. This is because SBMLMs fail to harness the location information of the simulations. To overcome these shortcomings, image based machine learning methods (IBMLMs) are leveraged in this paper. The underlying theory of location information, which supports the advantages of IBMLM, is qualitatively interpreted. Based on this theory, a Res-SE-U-Net IBMLM surrogate model is developed and compared with a multi-layer perceptron (MLP) as a referencing SBMLM surrogate model. It is demonstrated that the IBMLM model is advantageous over the MLP SBMLM model in accuracy, generalizability, robustness, and informativeness. This paper presents a promising methodology of leveraging IBMLMs in surrogate models to make maximum use of info from FEA results. Future prospective studies that inspired by this paper are also discussed.



### DEEPMIR: A DEEP neural network for differential detection of cerebral Microbleeds and IRon deposits in MRI
- **Arxiv ID**: http://arxiv.org/abs/2010.00148v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.00148v3)
- **Published**: 2020-09-30 23:50:14+00:00
- **Updated**: 2021-06-07 05:40:41+00:00
- **Authors**: Tanweer Rashid, Ahmed Abdulkadir, Ilya M. Nasrallah, Jeffrey B. Ware, Hangfan Liu, Pascal Spincemaille, J. Rafael Romero, R. Nick Bryan, Susan R. Heckbert, Mohamad Habes
- **Comment**: None
- **Journal**: None
- **Summary**: Lobar cerebral microbleeds (CMBs) and localized non-hemorrhage iron deposits in the basal ganglia have been associated with brain aging, vascular disease and neurodegenerative disorders. Particularly, CMBs are small lesions and require multiple neuroimaging modalities for accurate detection. Quantitative susceptibility mapping (QSM) derived from in vivo magnetic resonance imaging (MRI) is necessary to differentiate between iron content and mineralization. We set out to develop a deep learning-based segmentation method suitable for segmenting both CMBs and iron deposits. We included a convenience sample of 24 participants from the MESA cohort and used T2-weighted images, susceptibility weighted imaging (SWI), and QSM to segment the two types of lesions. We developed a protocol for simultaneous manual annotation of CMBs and non-hemorrhage iron deposits in the basal ganglia. This manual annotation was then used to train a deep convolution neural network (CNN). Specifically, we adapted the U-Net model with a higher number of resolution layers to be able to detect small lesions such as CMBs from standard resolution MRI. We tested different combinations of the three modalities to determine the most informative data sources for the detection tasks. In the detection of CMBs using single class and multiclass models, we achieved an average sensitivity and precision of between 0.84-0.88 and 0.40-0.59, respectively. The same framework detected non-hemorrhage iron deposits with an average sensitivity and precision of about 0.75-0.81 and 0.62-0.75, respectively. Our results showed that deep learning could automate the detection of small vessel disease lesions and including multimodal MR data (particularly QSM) can improve the detection of CMB and non-hemorrhage iron deposits with sensitivity and precision that is compatible with use in large-scale research studies.



