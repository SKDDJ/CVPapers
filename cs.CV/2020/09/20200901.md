# Arxiv Papers in cs.CV on 2020-09-01
### A Review of Single-Source Deep Unsupervised Visual Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2009.00155v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2009.00155v3)
- **Published**: 2020-09-01 00:06:50+00:00
- **Updated**: 2020-09-19 00:46:27+00:00
- **Authors**: Sicheng Zhao, Xiangyu Yue, Shanghang Zhang, Bo Li, Han Zhao, Bichen Wu, Ravi Krishna, Joseph E. Gonzalez, Alberto L. Sangiovanni-Vincentelli, Sanjit A. Seshia, Kurt Keutzer
- **Comment**: None
- **Journal**: None
- **Summary**: Large-scale labeled training datasets have enabled deep neural networks to excel across a wide range of benchmark vision tasks. However, in many applications, it is prohibitively expensive and time-consuming to obtain large quantities of labeled data. To cope with limited labeled training data, many have attempted to directly apply models trained on a large-scale labeled source domain to another sparsely labeled or unlabeled target domain. Unfortunately, direct transfer across domains often performs poorly due to the presence of domain shift or dataset bias. Domain adaptation is a machine learning paradigm that aims to learn a model from a source domain that can perform well on a different (but related) target domain. In this paper, we review the latest single-source deep unsupervised domain adaptation methods focused on visual tasks and discuss new perspectives for future research. We begin with the definitions of different domain adaptation strategies and the descriptions of existing benchmark datasets. We then summarize and compare different categories of single-source unsupervised domain adaptation methods, including discrepancy-based methods, adversarial discriminative methods, adversarial generative methods, and self-supervision-based methods. Finally, we discuss future research directions with challenges and possible solutions.



### LodoNet: A Deep Neural Network with 2D Keypoint Matchingfor 3D LiDAR Odometry Estimation
- **Arxiv ID**: http://arxiv.org/abs/2009.00164v1
- **DOI**: 10.1145/3394171.3413771
- **Categories**: **cs.CV**, I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/2009.00164v1)
- **Published**: 2020-09-01 01:09:41+00:00
- **Updated**: 2020-09-01 01:09:41+00:00
- **Authors**: Ce Zheng, Yecheng Lyu, Ming Li, Ziming Zhang
- **Comment**: In 28th ACM International Conference on Multimedia, 9 pages
- **Journal**: None
- **Summary**: Deep learning based LiDAR odometry (LO) estimation attracts increasing research interests in the field of autonomous driving and robotics. Existing works feed consecutive LiDAR frames into neural networks as point clouds and match pairs in the learned feature space. In contrast, motivated by the success of image based feature extractors, we propose to transfer the LiDAR frames to image space and reformulate the problem as image feature extraction. With the help of scale-invariant feature transform (SIFT) for feature extraction, we are able to generate matched keypoint pairs (MKPs) that can be precisely returned to the 3D space. A convolutional neural network pipeline is designed for LiDAR odometry estimation by extracted MKPs. The proposed scheme, namely LodoNet, is then evaluated in the KITTI odometry estimation benchmark, achieving on par with or even better results than the state-of-the-art.



### Automatic Radish Wilt Detection Using Image Processing Based Techniques and Machine Learning Algorithm
- **Arxiv ID**: http://arxiv.org/abs/2009.00173v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.00173v1)
- **Published**: 2020-09-01 01:37:01+00:00
- **Updated**: 2020-09-01 01:37:01+00:00
- **Authors**: Asif Ashraf Patankar, Hyeonjoon Moon
- **Comment**: None
- **Journal**: None
- **Summary**: Image processing, computer vision, and pattern recognition have been playing a vital role in diverse agricultural applications, such as species detection, recognition, classification, identification, plant growth stages, plant disease detection, and many more. On the other hand, there is a growing need to capture high resolution images using unmanned aerial vehicles (UAV) and to develop better algorithms in order to find highly accurate and to the point results. In this paper, we propose a segmentation and extraction-based technique to detect fusarium wilt in radish crops. Recent wilt detection algorithms are either based on image processing techniques or conventional machine learning algorithms. However, our methodology is based on a hybrid algorithm, which combines image processing and machine learning. First, the crop image is divided into three segments, which include viz., healthy vegetation, ground and packing material. Based on the HSV decision tree algorithm, all the three segments are segregated from the image. Second, the extracted segments are summed together into an empty canvas of the same resolution as the image and one new image is produced. Third, this new image is compared with the original image, and a final noisy image, which contains traces of wilt is extracted. Finally, a k-means algorithm is applied to eliminate the noise and to extract the accurate wilt from it. Moreover, the extracted wilt is mapped on the original image using the contouring method. The proposed combination of algorithms detects the wilt appropriately, which surpasses the traditional practice of separately using the image processing techniques or machine learning.



### Utilizing Satellite Imagery Datasets and Machine Learning Data Models to Evaluate Infrastructure Change in Undeveloped Regions
- **Arxiv ID**: http://arxiv.org/abs/2009.00185v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.00185v1)
- **Published**: 2020-09-01 02:11:14+00:00
- **Updated**: 2020-09-01 02:11:14+00:00
- **Authors**: Kyle McCullough, Andrew Feng, Meida Chen, Ryan McAlinden
- **Comment**: None
- **Journal**: Interservice/Industry Training, Simulation, and Education
  Conference (I/ITSEC) 2020
- **Summary**: In the globalized economic world, it has become important to understand the purpose behind infrastructural and construction initiatives occurring within developing regions of the earth. This is critical when the financing for such projects must be coming from external sources, as is occurring throughout major portions of the African continent. When it comes to imagery analysis to research these regions, ground and aerial coverage is either non-existent or not commonly acquired. However, imagery from a large number of commercial, private, and government satellites have produced enormous datasets with global coverage, compiling geospatial resources that can be mined and processed using machine learning algorithms and neural networks. The downside is that a majority of these geospatial data resources are in a state of technical stasis, as it is difficult to quickly parse and determine a plan for request and processing when acquiring satellite image data. A goal of this research is to allow automated monitoring for largescale infrastructure projects, such as railways, to determine reliable metrics that define and predict the direction construction initiatives could take, allowing for a directed monitoring via narrowed and targeted satellite imagery requests. By utilizing photogrammetric techniques on available satellite data to create 3D Meshes and Digital Surface Models (DSM) we hope to effectively predict transport routes. In understanding the potential directions that largescale transport infrastructure will take through predictive modeling, it becomes much easier to track, understand, and monitor progress, especially in areas with limited imagery coverage.



### Object Detection-Based Variable Quantization Processing
- **Arxiv ID**: http://arxiv.org/abs/2009.00189v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.00189v1)
- **Published**: 2020-09-01 02:40:56+00:00
- **Updated**: 2020-09-01 02:40:56+00:00
- **Authors**: Likun Liu, Hua Qi
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a preprocessing method for conventional image and video encoders that can make these existing encoders content-aware. By going through our process, a higher quality parameter could be set on a traditional encoder without increasing the output size. A still frame or an image will firstly go through an object detector. Either the properties of the detection result will decide the parameters of the following procedures, or the system will be bypassed if no object is detected in the given frame. The processing method utilizes an adaptive quantization process to determine the portion of data to be dropped. This method is primarily based on the JPEG compression theory and is optimum for JPEG-based encoders such as JPEG encoders and the Motion JPEG encoders. However, other DCT-based encoders like MPEG part 2, H.264, etc. can also benefit from this method. In the experiments, we compare the MS-SSIM under the same bitrate as well as similar MS-SSIM but enhanced bitrate. As this method is based on human perception, even with similar MS-SSIM, the overall watching experience will be better than the direct encoded ones.



### Deep Ice Layer Tracking and Thickness Estimation using Fully Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/2009.00191v3
- **DOI**: 10.1109/BigData50022.2020.9378070
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2009.00191v3)
- **Published**: 2020-09-01 02:43:59+00:00
- **Updated**: 2021-01-13 08:30:02+00:00
- **Authors**: Debvrat Varshney, Maryam Rahnemoonfar, Masoud Yari, John Paden
- **Comment**: None
- **Journal**: None
- **Summary**: Global warming is rapidly reducing glaciers and ice sheets across the world. Real time assessment of this reduction is required so as to monitor its global climatic impact. In this paper, we introduce a novel way of estimating the thickness of each internal ice layer using Snow Radar images and Fully Convolutional Networks. The estimated thickness can be used to understand snow accumulation each year. To understand the depth and structure of each internal ice layer, we perform multi-class semantic segmentation on radar images, which hasn't been performed before. As the radar images lack good training labels, we carry out a pre-processing technique to get a clean set of labels. After detecting each ice layer uniquely, we calculate its thickness and compare it with the processed ground truth. This is the first time that each ice layer is detected separately and its thickness calculated through automated techniques. Through this procedure we were able to estimate the ice-layer thicknesses within a Mean Absolute Error of approximately 3.6 pixels. Such a Deep Learning based method can be used with ever-increasing datasets to make accurate assessments for cryospheric studies.



### RangeRCNN: Towards Fast and Accurate 3D Object Detection with Range Image Representation
- **Arxiv ID**: http://arxiv.org/abs/2009.00206v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.00206v2)
- **Published**: 2020-09-01 03:28:13+00:00
- **Updated**: 2021-03-23 06:53:11+00:00
- **Authors**: Zhidong Liang, Ming Zhang, Zehan Zhang, Xian Zhao, Shiliang Pu
- **Comment**: None
- **Journal**: None
- **Summary**: We present RangeRCNN, a novel and effective 3D object detection framework based on the range image representation. Most existing methods are voxel-based or point-based. Though several optimizations have been introduced to ease the sparsity issue and speed up the running time, the two representations are still computationally inefficient. Compared to them, the range image representation is dense and compact which can exploit powerful 2D convolution. Even so, the range image is not preferred in 3D object detection due to scale variation and occlusion. In this paper, we utilize the dilated residual block (DRB) to better adapt different object scales and obtain a more flexible receptive field. Considering scale variation and occlusion, we propose the RV-PV-BEV (range view-point view-bird's eye view) module to transfer features from RV to BEV. The anchor is defined in BEV which avoids scale variation and occlusion. Neither RV nor BEV can provide enough information for height estimation; therefore, we propose a two-stage RCNN for better 3D detection performance. The aforementioned point view not only serves as a bridge from RV to BEV but also provides pointwise features for RCNN. Experiments show that RangeRCNN achieves state-of-the-art performance on the KITTI dataset and the Waymo Open dataset, and provides more possibilities for real-time 3D object detection. We further introduce and discuss the data augmentation strategy for the range image based method, which will be very valuable for future research on range image.



### Semantics-aware Adaptive Knowledge Distillation for Sensor-to-Vision Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2009.00210v5
- **DOI**: 10.1109/TIP.2021.3086590
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.00210v5)
- **Published**: 2020-09-01 03:38:31+00:00
- **Updated**: 2021-05-27 07:16:45+00:00
- **Authors**: Yang Liu, Keze Wang, Guanbin Li, Liang Lin
- **Comment**: This paper focuses on the sensor-to-vision heterogenous action
  recognition problem. Code is available at
  https://github.com/YangLiu9208/SAKDN
- **Journal**: None
- **Summary**: Existing vision-based action recognition is susceptible to occlusion and appearance variations, while wearable sensors can alleviate these challenges by capturing human motion with one-dimensional time-series signal. For the same action, the knowledge learned from vision sensors and wearable sensors, may be related and complementary. However, there exists significantly large modality difference between action data captured by wearable-sensor and vision-sensor in data dimension, data distribution and inherent information content. In this paper, we propose a novel framework, named Semantics-aware Adaptive Knowledge Distillation Networks (SAKDN), to enhance action recognition in vision-sensor modality (videos) by adaptively transferring and distilling the knowledge from multiple wearable sensors. The SAKDN uses multiple wearable-sensors as teacher modalities and uses RGB videos as student modality. To preserve local temporal relationship and facilitate employing visual deep learning model, we transform one-dimensional time-series signals of wearable sensors to two-dimensional images by designing a gramian angular field based virtual image generation model. Then, we build a novel Similarity-Preserving Adaptive Multi-modal Fusion Module to adaptively fuse intermediate representation knowledge from different teacher networks. Finally, to fully exploit and transfer the knowledge of multiple well-trained teacher networks to the student network, we propose a novel Graph-guided Semantically Discriminative Mapping loss, which utilizes graph-guided ablation analysis to produce a good visual explanation highlighting the important regions across modalities and concurrently preserving the interrelations of original data. Experimental results on Berkeley-MHAD, UTD-MHAD and MMAct datasets well demonstrate the effectiveness of our proposed SAKDN.



### On The Usage Of Average Hausdorff Distance For Segmentation Performance Assessment: Hidden Bias When Used For Ranking
- **Arxiv ID**: http://arxiv.org/abs/2009.00215v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.00215v2)
- **Published**: 2020-09-01 03:58:16+00:00
- **Updated**: 2020-09-13 13:37:14+00:00
- **Authors**: Orhun Utku Aydin, Abdel Aziz Taha, Adam Hilbert, Ahmed A. Khalil, Ivana Galinovic, Jochen B. Fiebach, Dietmar Frey, Vince Istvan Madai
- **Comment**: Added Disclosures, changed typo in Discussion
- **Journal**: None
- **Summary**: Average Hausdorff Distance (AVD) is a widely used performance measure to calculate the distance between two point sets. In medical image segmentation, AVD is used to compare ground truth images with segmentation results allowing their ranking. We identified, however, a ranking bias of AVD making it less suitable for segmentation ranking. To mitigate this bias, we present a modified calculation of AVD that we have coined balanced AVD (bAVD). To simulate segmentations for ranking, we manually created non-overlapping segmentation errors common in cerebral vessel segmentation as our use-case. Adding the created errors consecutively and randomly to the ground truth, we created sets of simulated segmentations with increasing number of errors. Each set of simulated segmentations was ranked using AVD and bAVD. We calculated the Kendall-rank-correlation-coefficient between the segmentation ranking and the number of errors in each simulated segmentation. The rankings produced by bAVD had a significantly higher average correlation (0.969) than those of AVD (0.847). In 200 total rankings, bAVD misranked 52 and AVD misranked 179 segmentations. Our proposed evaluation measure, bAVD, alleviates AVDs ranking bias making it more suitable for rankings and quality assessment of segmentations.



### Practical Cross-modal Manifold Alignment for Grounded Language
- **Arxiv ID**: http://arxiv.org/abs/2009.05147v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2009.05147v1)
- **Published**: 2020-09-01 04:16:48+00:00
- **Updated**: 2020-09-01 04:16:48+00:00
- **Authors**: Andre T. Nguyen, Luke E. Richards, Gaoussou Youssouf Kebe, Edward Raff, Kasra Darvish, Frank Ferraro, Cynthia Matuszek
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a cross-modality manifold alignment procedure that leverages triplet loss to jointly learn consistent, multi-modal embeddings of language-based concepts of real-world items. Our approach learns these embeddings by sampling triples of anchor, positive, and negative data points from RGB-depth images and their natural language descriptions. We show that our approach can benefit from, but does not require, post-processing steps such as Procrustes analysis, in contrast to some of our baselines which require it for reasonable performance. We demonstrate the effectiveness of our approach on two datasets commonly used to develop robotic-based grounded language learning systems, where our approach outperforms four baselines, including a state-of-the-art approach, across five evaluation metrics.



### Gaussian Process Gradient Maps for Loop-Closure Detection in Unstructured Planetary Environments
- **Arxiv ID**: http://arxiv.org/abs/2009.00221v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.00221v1)
- **Published**: 2020-09-01 04:41:40+00:00
- **Updated**: 2020-09-01 04:41:40+00:00
- **Authors**: Cedric Le Gentil, Mallikarjuna Vayugundla, Riccardo Giubilato, Wolfgang Stürzl, Teresa Vidal-Calleja, Rudolph Triebel
- **Comment**: This work is accepted for presentation at the 2020 IEEE/RSJ
  International Conference on Intelligent Robots and Systems (IROS). Please
  find IEEE's copyright statement at the bottom of the first page. Cedric Le
  Gentil and Mallikarjuna Vayugundla share the first authorship of this paper
- **Journal**: None
- **Summary**: The ability to recognize previously mapped locations is an essential feature for autonomous systems. Unstructured planetary-like environments pose a major challenge to these systems due to the similarity of the terrain. As a result, the ambiguity of the visual appearance makes state-of-the-art visual place recognition approaches less effective than in urban or man-made environments. This paper presents a method to solve the loop closure problem using only spatial information. The key idea is to use a novel continuous and probabilistic representations of terrain elevation maps. Given 3D point clouds of the environment, the proposed approach exploits Gaussian Process (GP) regression with linear operators to generate continuous gradient maps of the terrain elevation information. Traditional image registration techniques are then used to search for potential matches. Loop closures are verified by leveraging both the spatial characteristic of the elevation maps (SE(2) registration) and the probabilistic nature of the GP representation. A submap-based localization and mapping framework is used to demonstrate the validity of the proposed approach. The performance of this pipeline is evaluated and benchmarked using real data from a rover that is equipped with a stereo camera and navigates in challenging, unstructured planetary-like environments in Morocco and on Mt. Etna.



### Heatmap Regression via Randomized Rounding
- **Arxiv ID**: http://arxiv.org/abs/2009.00225v2
- **DOI**: 10.1109/TPAMI.2021.3103980
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.00225v2)
- **Published**: 2020-09-01 04:54:22+00:00
- **Updated**: 2021-08-26 09:34:41+00:00
- **Authors**: Baosheng Yu, Dacheng Tao
- **Comment**: To appear in TPAMI
- **Journal**: None
- **Summary**: Heatmap regression has become the mainstream methodology for deep learning-based semantic landmark localization, including in facial landmark localization and human pose estimation. Though heatmap regression is robust to large variations in pose, illumination, and occlusion in unconstrained settings, it usually suffers from a sub-pixel localization problem. Specifically, considering that the activation point indices in heatmaps are always integers, quantization error thus appears when using heatmaps as the representation of numerical coordinates. Previous methods to overcome the sub-pixel localization problem usually rely on high-resolution heatmaps. As a result, there is always a trade-off between achieving localization accuracy and computational cost, where the computational complexity of heatmap regression depends on the heatmap resolution in a quadratic manner. In this paper, we formally analyze the quantization error of vanilla heatmap regression and propose a simple yet effective quantization system to address the sub-pixel localization problem. The proposed quantization system induced by the randomized rounding operation 1) encodes the fractional part of numerical coordinates into the ground truth heatmap using a probabilistic approach during training; and 2) decodes the predicted numerical coordinates from a set of activation points during testing. We prove that the proposed quantization system for heatmap regression is unbiased and lossless. Experimental results on popular facial landmark localization datasets (WFLW, 300W, COFW, and AFLW) and human pose estimation datasets (MPII and COCO) demonstrate the effectiveness of the proposed method for efficient and accurate semantic landmark localization. Code is available at http://github.com/baoshengyu/H3R.



### Temporal Continuity Based Unsupervised Learning for Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2009.00242v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.00242v1)
- **Published**: 2020-09-01 05:29:30+00:00
- **Updated**: 2020-09-01 05:29:30+00:00
- **Authors**: Usman Ali, Bayram Bayramli, Hongtao Lu
- **Comment**: None
- **Journal**: None
- **Summary**: Person re-identification (re-id) aims to match the same person from images taken across multiple cameras. Most existing person re-id methods generally require a large amount of identity labeled data to act as discriminative guideline for representation learning. Difficulty in manually collecting identity labeled data leads to poor adaptability in practical scenarios. To overcome this problem, we propose an unsupervised center-based clustering approach capable of progressively learning and exploiting the underlying re-id discriminative information from temporal continuity within a camera. We call our framework Temporal Continuity based Unsupervised Learning (TCUL). Specifically, TCUL simultaneously does center based clustering of unlabeled (target) dataset and fine-tunes a convolutional neural network (CNN) pre-trained on irrelevant labeled (source) dataset to enhance discriminative capability of the CNN for the target dataset. Furthermore, it exploits temporally continuous nature of images within-camera jointly with spatial similarity of feature maps across-cameras to generate reliable pseudo-labels for training a re-identification model. As the training progresses, number of reliable samples keep on growing adaptively which in turn boosts representation ability of the CNN. Extensive experiments on three large-scale person re-id benchmark datasets are conducted to compare our framework with state-of-the-art techniques, which demonstrate superiority of TCUL over existing methods.



### Distinctive 3D local deep descriptors
- **Arxiv ID**: http://arxiv.org/abs/2009.00258v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.00258v2)
- **Published**: 2020-09-01 06:25:06+00:00
- **Updated**: 2020-12-28 14:01:52+00:00
- **Authors**: Fabio Poiesi, Davide Boscaini
- **Comment**: IEEE International Conference on Pattern Recognition 2020
- **Journal**: None
- **Summary**: We present a simple but yet effective method for learning distinctive 3D local deep descriptors (DIPs) that can be used to register point clouds without requiring an initial alignment. Point cloud patches are extracted, canonicalised with respect to their estimated local reference frame and encoded into rotation-invariant compact descriptors by a PointNet-based deep neural network. DIPs can effectively generalise across different sensor modalities because they are learnt end-to-end from locally and randomly sampled points. Because DIPs encode only local geometric information, they are robust to clutter, occlusions and missing regions. We evaluate and compare DIPs against alternative hand-crafted and deep descriptors on several indoor and outdoor datasets consisting of point clouds reconstructed using different sensors. Results show that DIPs (i) achieve comparable results to the state-of-the-art on RGB-D indoor scenes (3DMatch dataset), (ii) outperform state-of-the-art by a large margin on laser-scanner outdoor scenes (ETH dataset), and (iii) generalise to indoor scenes reconstructed with the Visual-SLAM system of Android ARCore. Source code: https://github.com/fabiopoiesi/dip.



### Personalization in Human Activity Recognition
- **Arxiv ID**: http://arxiv.org/abs/2009.00268v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.00268v1)
- **Published**: 2020-09-01 06:59:17+00:00
- **Updated**: 2020-09-01 06:59:17+00:00
- **Authors**: Anna Ferrari, Daniela Micucci, Marco Mobilio, Paolo Napoletano
- **Comment**: 2 pages
- **Journal**: None
- **Summary**: In the recent years there has been a growing interest in techniques able to automatically recognize activities performed by people. This field is known as Human Activity recognition (HAR). HAR can be crucial in monitoring the wellbeing of the people, with special regard to the elder population and those people affected by degenerative conditions. One of the main challenges concerns the diversity of the population and how the same activities can be performed in different ways due to physical characteristics and life-style. In this paper we explore the possibility of exploiting physical characteristics and signal similarity to achieve better results with respect to deep learning classifiers that do not rely on this information.



### Classification of Diabetic Retinopathy Using Unlabeled Data and Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/2009.00982v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.00982v1)
- **Published**: 2020-09-01 07:18:39+00:00
- **Updated**: 2020-09-01 07:18:39+00:00
- **Authors**: Sajjad Abbasi, Mohsen Hajabdollahi, Pejman Khadivi, Nader Karimi, Roshanak Roshandel, Shahram Shirani, Shadrokh Samavi
- **Comment**: 21 pages, 6 figures, 7 tables. arXiv admin note: substantial text
  overlap with arXiv:2002.03321
- **Journal**: None
- **Summary**: Knowledge distillation allows transferring knowledge from a pre-trained model to another. However, it suffers from limitations, and constraints related to the two models need to be architecturally similar. Knowledge distillation addresses some of the shortcomings associated with transfer learning by generalizing a complex model to a lighter model. However, some parts of the knowledge may not be distilled by knowledge distillation sufficiently. In this paper, a novel knowledge distillation approach using transfer learning is proposed. The proposed method transfers the entire knowledge of a model to a new smaller one. To accomplish this, unlabeled data are used in an unsupervised manner to transfer the maximum amount of knowledge to the new slimmer model. The proposed method can be beneficial in medical image analysis, where labeled data are typically scarce. The proposed approach is evaluated in the context of classification of images for diagnosing Diabetic Retinopathy on two publicly available datasets, including Messidor and EyePACS. Simulation results demonstrate that the approach is effective in transferring knowledge from a complex model to a lighter one. Furthermore, experimental results illustrate that the performance of different small models is improved significantly using unlabeled data and knowledge distillation.



### ZooBuilder: 2D and 3D Pose Estimation for Quadrupeds Using Synthetic Data
- **Arxiv ID**: http://arxiv.org/abs/2009.05389v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.05389v1)
- **Published**: 2020-09-01 07:41:20+00:00
- **Updated**: 2020-09-01 07:41:20+00:00
- **Authors**: Abassin Sourou Fangbemi, Yi Fei Lu, Mao Yuan Xu, Xiao Wu Luo, Alexis Rolland, Chedy Raissi
- **Comment**: None
- **Journal**: None
- **Summary**: This work introduces a novel strategy for generating synthetic training data for 2D and 3D pose estimation of animals using keyframe animations. With the objective to automate the process of creating animations for wildlife, we train several 2D and 3D pose estimation models with synthetic data, and put in place an end-to-end pipeline called ZooBuilder. The pipeline takes as input a video of an animal in the wild, and generates the corresponding 2D and 3D coordinates for each joint of the animal's skeleton. With this approach, we produce motion capture data that can be used to create animations for wildlife.



### Recognition Oriented Iris Image Quality Assessment in the Feature Space
- **Arxiv ID**: http://arxiv.org/abs/2009.00294v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.00294v2)
- **Published**: 2020-09-01 08:58:18+00:00
- **Updated**: 2020-09-27 06:47:49+00:00
- **Authors**: Leyuan Wang, Kunbo Zhang, Min Ren, Yunlong Wang, Zhenan Sun
- **Comment**: None
- **Journal**: None
- **Summary**: A large portion of iris images captured in real world scenarios are poor quality due to the uncontrolled environment and the non-cooperative subject. To ensure that the recognition algorithm is not affected by low-quality images, traditional hand-crafted factors based methods discard most images, which will cause system timeout and disrupt user experience. In this paper, we propose a recognition-oriented quality metric and assessment method for iris image to deal with the problem. The method regards the iris image embeddings Distance in Feature Space (DFS) as the quality metric and the prediction is based on deep neural networks with the attention mechanism. The quality metric proposed in this paper can significantly improve the performance of the recognition algorithm while reducing the number of images discarded for recognition, which is advantageous over hand-crafted factors based iris quality assessment methods. The relationship between Image Rejection Rate (IRR) and Equal Error Rate (EER) is proposed to evaluate the performance of the quality assessment algorithm under the same image quality distribution and the same recognition algorithm. Compared with hand-crafted factors based methods, the proposed method is a trial to bridge the gap between the image quality assessment and biometric recognition. The code is available at https://github.com/Debatrix/DFSNet.



### Multi-channel Transformers for Multi-articulatory Sign Language Translation
- **Arxiv ID**: http://arxiv.org/abs/2009.00299v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.00299v1)
- **Published**: 2020-09-01 09:10:55+00:00
- **Updated**: 2020-09-01 09:10:55+00:00
- **Authors**: Necati Cihan Camgoz, Oscar Koller, Simon Hadfield, Richard Bowden
- **Comment**: None
- **Journal**: None
- **Summary**: Sign languages use multiple asynchronous information channels (articulators), not just the hands but also the face and body, which computational approaches often ignore. In this paper we tackle the multi-articulatory sign language translation task and propose a novel multi-channel transformer architecture. The proposed architecture allows both the inter and intra contextual relationships between different sign articulators to be modelled within the transformer network itself, while also maintaining channel specific information. We evaluate our approach on the RWTH-PHOENIX-Weather-2014T dataset and report competitive translation performance. Importantly, we overcome the reliance on gloss annotations which underpin other state-of-the-art approaches, thereby removing future need for expensive curated datasets.



### To augment or not to augment? Data augmentation in user identification based on motion sensors
- **Arxiv ID**: http://arxiv.org/abs/2009.00300v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2009.00300v1)
- **Published**: 2020-09-01 09:11:12+00:00
- **Updated**: 2020-09-01 09:11:12+00:00
- **Authors**: Cezara Benegui, Radu Tudor Ionescu
- **Comment**: Extended version (12 pages, 2 figures) of our paper (9 pages)
  accepted at ICONIP 2020
- **Journal**: None
- **Summary**: Nowadays, commonly-used authentication systems for mobile device users, e.g. password checking, face recognition or fingerprint scanning, are susceptible to various kinds of attacks. In order to prevent some of the possible attacks, these explicit authentication systems can be enhanced by considering a two-factor authentication scheme, in which the second factor is an implicit authentication system based on analyzing motion sensor data captured by accelerometers or gyroscopes. In order to avoid any additional burdens to the user, the registration process of the implicit authentication system must be performed quickly, i.e. the number of data samples collected from the user is typically small. In the context of designing a machine learning model for implicit user authentication based on motion signals, data augmentation can play an important role. In this paper, we study several data augmentation techniques in the quest of finding useful augmentation methods for motion sensor data. We propose a set of four research questions related to data augmentation in the context of few-shot user identification based on motion sensor signals. We conduct experiments on a benchmark data set, using two deep learning architectures, convolutional neural networks and Long Short-Term Memory networks, showing which and when data augmentation methods bring accuracy improvements. Interestingly, we find that data augmentation is not very helpful, most likely because the signal patterns useful to discriminate users are too sensitive to the transformations brought by certain data augmentation techniques. This result is somewhat contradictory to the common belief that data augmentation is expected to increase the accuracy of machine learning models.



### Generalized Zero-Shot Learning via VAE-Conditioned Generative Flow
- **Arxiv ID**: http://arxiv.org/abs/2009.00303v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.00303v1)
- **Published**: 2020-09-01 09:12:31+00:00
- **Updated**: 2020-09-01 09:12:31+00:00
- **Authors**: Yu-Chao Gu, Le Zhang, Yun Liu, Shao-Ping Lu, Ming-Ming Cheng
- **Comment**: None
- **Journal**: None
- **Summary**: Generalized zero-shot learning (GZSL) aims to recognize both seen and unseen classes by transferring knowledge from semantic descriptions to visual representations. Recent generative methods formulate GZSL as a missing data problem, which mainly adopts GANs or VAEs to generate visual features for unseen classes. However, GANs often suffer from instability, and VAEs can only optimize the lower bound on the log-likelihood of observed data. To overcome the above limitations, we resort to generative flows, a family of generative models with the advantage of accurate likelihood estimation. More specifically, we propose a conditional version of generative flows for GZSL, i.e., VAE-Conditioned Generative Flow (VAE-cFlow). By using VAE, the semantic descriptions are firstly encoded into tractable latent distributions, conditioned on that the generative flow optimizes the exact log-likelihood of the observed visual features. We ensure the conditional latent distribution to be both semantic meaningful and inter-class discriminative by i) adopting the VAE reconstruction objective, ii) releasing the zero-mean constraint in VAE posterior regularization, and iii) adding a classification regularization on the latent variables. Our method achieves state-of-the-art GZSL results on five well-known benchmark datasets, especially for the significant improvement in the large-scale setting. Code is released at https://github.com/guyuchao/VAE-cFlow-ZSL.



### PIDNet: An Efficient Network for Dynamic Pedestrian Intrusion Detection
- **Arxiv ID**: http://arxiv.org/abs/2009.00312v1
- **DOI**: 10.1145/3394171.3413837
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.00312v1)
- **Published**: 2020-09-01 09:34:43+00:00
- **Updated**: 2020-09-01 09:34:43+00:00
- **Authors**: Jingchen Sun, Jiming Chen, Tao Chen, Jiayuan Fan, Shibo He
- **Comment**: Proceedings of the 28th ACM International Conference on Multimedia
  (MM '20), October 12--16, 2020, Seattle, WA, USA
- **Journal**: None
- **Summary**: Vision-based dynamic pedestrian intrusion detection (PID), judging whether pedestrians intrude an area-of-interest (AoI) by a moving camera, is an important task in mobile surveillance. The dynamically changing AoIs and a number of pedestrians in video frames increase the difficulty and computational complexity of determining whether pedestrians intrude the AoI, which makes previous algorithms incapable of this task. In this paper, we propose a novel and efficient multi-task deep neural network, PIDNet, to solve this problem. PIDNet is mainly designed by considering two factors: accurately segmenting the dynamically changing AoIs from a video frame captured by the moving camera and quickly detecting pedestrians from the generated AoI-contained areas. Three efficient network designs are proposed and incorporated into PIDNet to reduce the computational complexity: 1) a special PID task backbone for feature sharing, 2) a feature cropping module for feature cropping, and 3) a lighter detection branch network for feature compression. In addition, considering there are no public datasets and benchmarks in this field, we establish a benchmark dataset to evaluate the proposed network and give the corresponding evaluation metrics for the first time. Experimental results show that PIDNet can achieve 67.1% PID accuracy and 9.6 fps inference speed on the proposed dataset, which serves as a good baseline for the future vision-based dynamic PID study.



### Active Deep Densely Connected Convolutional Network for Hyperspectral Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2009.00320v2
- **DOI**: 10.1080/01431161.2021.1931542
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.00320v2)
- **Published**: 2020-09-01 09:53:38+00:00
- **Updated**: 2020-11-24 11:46:16+00:00
- **Authors**: Bing Liu, Anzhu Yu, Pengqiang Zhang, Lei Ding, Wenyue Guo, Kuiliang Gao, Xibing Zuo
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning based methods have seen a massive rise in popularity for hyperspectral image classification over the past few years. However, the success of deep learning is attributed greatly to numerous labeled samples. It is still very challenging to use only a few labeled samples to train deep learning models to reach a high classification accuracy. An active deep-learning framework trained by an end-to-end manner is, therefore, proposed by this paper in order to minimize the hyperspectral image classification costs. First, a deep densely connected convolutional network is considered for hyperspectral image classification. Different from the traditional active learning methods, an additional network is added to the designed deep densely connected convolutional network to predict the loss of input samples. Then, the additional network could be used to suggest unlabeled samples that the deep densely connected convolutional network is more likely to produce a wrong label. Note that the additional network uses the intermediate features of the deep densely connected convolutional network as input. Therefore, the proposed method is an end-to-end framework. Subsequently, a few of the selected samples are labelled manually and added to the training samples. The deep densely connected convolutional network is therefore trained using the new training set. Finally, the steps above are repeated to train the whole framework iteratively. Extensive experiments illustrates that the method proposed could reach a high accuracy in classification after selecting just a few samples.



### Uncovering Hidden Challenges in Query-Based Video Moment Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2009.00325v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.00325v2)
- **Published**: 2020-09-01 10:07:23+00:00
- **Updated**: 2020-10-07 10:15:13+00:00
- **Authors**: Mayu Otani, Yuta Nakashima, Esa Rahtu, Janne Heikkilä
- **Comment**: British Machine Vision Conference (BMVC), 2020. (v2) added references
- **Journal**: None
- **Summary**: The query-based moment retrieval is a problem of localising a specific clip from an untrimmed video according a query sentence. This is a challenging task that requires interpretation of both the natural language query and the video content. Like in many other areas in computer vision and machine learning, the progress in query-based moment retrieval is heavily driven by the benchmark datasets and, therefore, their quality has significant impact on the field. In this paper, we present a series of experiments assessing how well the benchmark results reflect the true progress in solving the moment retrieval task. Our results indicate substantial biases in the popular datasets and unexpected behaviour of the state-of-the-art models. Moreover, we present new sanity check experiments and approaches for visualising the results. Finally, we suggest possible directions to improve the temporal sentence grounding in the future. Our code for this paper is available at https://mayu-ot.github.io/hidden-challenges-MR .



### Perceiving Humans: from Monocular 3D Localization to Social Distancing
- **Arxiv ID**: http://arxiv.org/abs/2009.00984v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.00984v2)
- **Published**: 2020-09-01 10:12:30+00:00
- **Updated**: 2021-03-24 10:30:19+00:00
- **Authors**: Lorenzo Bertoni, Sven Kreiss, Alexandre Alahi
- **Comment**: IEEE Transactions on Intelligent Transportation Systems
- **Journal**: None
- **Summary**: Perceiving humans in the context of Intelligent Transportation Systems (ITS) often relies on multiple cameras or expensive LiDAR sensors. In this work, we present a new cost-effective vision-based method that perceives humans' locations in 3D and their body orientation from a single image. We address the challenges related to the ill-posed monocular 3D tasks by proposing a neural network architecture that predicts confidence intervals in contrast to point estimates. Our neural network estimates human 3D body locations and their orientation with a measure of uncertainty. Our proposed solution (i) is privacy-safe, (ii) works with any fixed or moving cameras, and (iii) does not rely on ground plane estimation. We demonstrate the performance of our method with respect to three applications: locating humans in 3D, detecting social interactions, and verifying the compliance of recent safety measures due to the COVID-19 outbreak. We show that it is possible to rethink the concept of "social distancing" as a form of social interaction in contrast to a simple location-based rule. We publicly share the source code towards an open science mission.



### 3D-DEEP: 3-Dimensional Deep-learning based on elevation patterns forroad scene interpretation
- **Arxiv ID**: http://arxiv.org/abs/2009.00330v2
- **DOI**: 10.1109/IV47402.2020.9304601
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.00330v2)
- **Published**: 2020-09-01 10:18:08+00:00
- **Updated**: 2021-01-27 12:05:54+00:00
- **Authors**: A. Hernández, S. Woo, H. Corrales, I. Parra, E. Kim, D. F. Llorca, M. A. Sotelo
- **Comment**: "This work has been accepted for publication at IEEE Intelligent
  Vehicle Symposium 2020"
- **Journal**: None
- **Summary**: Road detection and segmentation is a crucial task in computer vision for safe autonomous driving. With this in mind, a new net architecture (3D-DEEP) and its end-to-end training methodology for CNN-based semantic segmentation are described along this paper for. The method relies on disparity filtered and LiDAR projected images for three-dimensional information and image feature extraction through fully convolutional networks architectures. The developed models were trained and validated over Cityscapes dataset using just fine annotation examples with 19 different training classes, and over KITTI road dataset. 72.32% mean intersection over union(mIoU) has been obtained for the 19 Cityscapes training classes using the validation images. On the other hand, over KITTIdataset the model has achieved an F1 error value of 97.85% invalidation and 96.02% using the test images.



### LiftFormer: 3D Human Pose Estimation using attention models
- **Arxiv ID**: http://arxiv.org/abs/2009.00348v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.00348v1)
- **Published**: 2020-09-01 11:05:45+00:00
- **Updated**: 2020-09-01 11:05:45+00:00
- **Authors**: Adrian Llopart
- **Comment**: 9 pages, 2 figures
- **Journal**: None
- **Summary**: Estimating the 3D position of human joints has become a widely researched topic in the last years. Special emphasis has gone into defining novel methods that extrapolate 2-dimensional data (keypoints) into 3D, namely predicting the root-relative coordinates of joints associated to human skeletons. The latest research trends have proven that the Transformer Encoder blocks aggregate temporal information significantly better than previous approaches. Thus, we propose the usage of these models to obtain more accurate 3D predictions by leveraging temporal information using attention mechanisms on ordered sequences human poses in videos.   Our method consistently outperforms the previous best results from the literature when using both 2D keypoint predictors by 0.3 mm (44.8 MPJPE, 0.7% improvement) and ground truth inputs by 2mm (MPJPE: 31.9, 8.4% improvement) on Human3.6M. It also achieves state-of-the-art performance on the HumanEva-I dataset with 10.5 P-MPJPE (22.2% reduction). The number of parameters in our model is easily tunable and is smaller (9.5M) than current methodologies (16.95M and 11.25M) whilst still having better performance. Thus, our 3D lifting model's accuracy exceeds that of other end-to-end or SMPL approaches and is comparable to many multi-view methods.



### Operational vs Convolutional Neural Networks for Image Denoising
- **Arxiv ID**: http://arxiv.org/abs/2009.00612v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2009.00612v1)
- **Published**: 2020-09-01 12:15:28+00:00
- **Updated**: 2020-09-01 12:15:28+00:00
- **Authors**: Junaid Malik, Serkan Kiranyaz, Moncef Gabbouj
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) have recently become a favored technique for image denoising due to its adaptive learning ability, especially with a deep configuration. However, their efficacy is inherently limited owing to their homogenous network formation with the unique use of linear convolution. In this study, we propose a heterogeneous network model which allows greater flexibility for embedding additional non-linearity at the core of the data transformation. To this end, we propose the idea of an operational neuron or Operational Neural Networks (ONN), which enables a flexible non-linear and heterogeneous configuration employing both inter and intra-layer neuronal diversity. Furthermore, we propose a robust operator search strategy inspired by the Hebbian theory, called the Synaptic Plasticity Monitoring (SPM) which can make data-driven choices for non-linearities in any architecture. An extensive set of comparative evaluations of ONNs and CNNs over two severe image denoising problems yield conclusive evidence that ONNs enriched by non-linear operators can achieve a superior denoising performance against CNNs with both equivalent and well-known deep configurations.



### Image Super-Resolution using Explicit Perceptual Loss
- **Arxiv ID**: http://arxiv.org/abs/2009.00382v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.00382v1)
- **Published**: 2020-09-01 12:22:39+00:00
- **Updated**: 2020-09-01 12:22:39+00:00
- **Authors**: Tomoki Yoshida, Kazutoshi Akita, Muhammad Haris, Norimichi Ukita
- **Comment**: 9 pages, 5 figures
- **Journal**: None
- **Summary**: This paper proposes an explicit way to optimize the super-resolution network for generating visually pleasing images. The previous approaches use several loss functions which is hard to interpret and has the implicit relationships to improve the perceptual score. We show how to exploit the machine learning based model which is directly trained to provide the perceptual score on generated images. It is believed that these models can be used to optimizes the super-resolution network which is easier to interpret. We further analyze the characteristic of the existing loss and our proposed explicit perceptual loss for better interpretation. The experimental results show the explicit approach has a higher perceptual score than other approaches. Finally, we demonstrate the relation of explicit perceptual loss and visually pleasing images using subjective evaluation.



### Multimodal Aggregation Approach for Memory Vision-Voice Indoor Navigation with Meta-Learning
- **Arxiv ID**: http://arxiv.org/abs/2009.00402v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.00402v1)
- **Published**: 2020-09-01 13:12:27+00:00
- **Updated**: 2020-09-01 13:12:27+00:00
- **Authors**: Liqi Yan, Dongfang Liu, Yaoxian Song, Changbin Yu
- **Comment**: 8 pages, 6 figures, 2 tables, accepted at 2020 IEEE/RSJ International
  Conference on Intelligent Robots and Systems (IROS 2020)
- **Journal**: None
- **Summary**: Vision and voice are two vital keys for agents' interaction and learning. In this paper, we present a novel indoor navigation model called Memory Vision-Voice Indoor Navigation (MVV-IN), which receives voice commands and analyzes multimodal information of visual observation in order to enhance robots' environment understanding. We make use of single RGB images taken by a first-view monocular camera. We also apply a self-attention mechanism to keep the agent focusing on key areas. Memory is important for the agent to avoid repeating certain tasks unnecessarily and in order for it to adapt adequately to new scenes, therefore, we make use of meta-learning. We have experimented with various functional features extracted from visual observation. Comparative experiments prove that our methods outperform state-of-the-art baselines.



### Single-shot Hyperspectral-Depth Imaging with Learned Diffractive Optics
- **Arxiv ID**: http://arxiv.org/abs/2009.00463v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, I.2.10; I.4.1; I.5
- **Links**: [PDF](http://arxiv.org/pdf/2009.00463v3)
- **Published**: 2020-09-01 14:19:35+00:00
- **Updated**: 2021-08-15 11:26:30+00:00
- **Authors**: Seung-Hwan Baek, Hayato Ikoma, Daniel S. Jeon, Yuqi Li, Wolfgang Heidrich, Gordon Wetzstein, Min H. Kim
- **Comment**: None
- **Journal**: International Conference on Computer Vision (ICCV) 2021
- **Summary**: Imaging depth and spectrum have been extensively studied in isolation from each other for decades. Recently, hyperspectral-depth (HS-D) imaging emerges to capture both information simultaneously by combining two different imaging systems; one for depth, the other for spectrum. While being accurate, this combinational approach induces increased form factor, cost, capture time, and alignment/registration problems. In this work, departing from the combinational principle, we propose a compact single-shot monocular HS-D imaging method. Our method uses a diffractive optical element (DOE), the point spread function of which changes with respect to both depth and spectrum. This enables us to reconstruct spectrum and depth from a single captured image. To this end, we develop a differentiable simulator and a neural-network-based reconstruction that are jointly optimized via automatic differentiation. To facilitate learning the DOE, we present a first HS-D dataset by building a benchtop HS-D imager that acquires high-quality ground truth. We evaluate our method with synthetic and real experiments by building an experimental prototype and achieve state-of-the-art HS-D imaging results.



### A High-Level Description and Performance Evaluation of Pupil Invisible
- **Arxiv ID**: http://arxiv.org/abs/2009.00508v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.00508v1)
- **Published**: 2020-09-01 15:10:10+00:00
- **Updated**: 2020-09-01 15:10:10+00:00
- **Authors**: Marc Tonsen, Chris Kay Baumann, Kai Dierkes
- **Comment**: None
- **Journal**: None
- **Summary**: Head-mounted eye trackers promise convenient access to reliable gaze data in unconstrained environments. Due to several limitations, however, often they can only partially deliver on this promise.   Among those are the following: (i) the necessity of performing a device setup and calibration prior to every use of the eye tracker, (ii) a lack of robustness of gaze-estimation results against perturbations, such as outdoor lighting conditions and unavoidable slippage of the eye tracker on the head of the subject, and (iii) behavioral distortion resulting from social awkwardness, due to the unnatural appearance of current head-mounted eye trackers.   Recently, Pupil Labs released Pupil Invisible glasses, a head-mounted eye tracker engineered to tackle these limitations. Here, we present an extensive evaluation of its gaze-estimation capabilities. To this end, we designed a data-collection protocol and evaluation scheme geared towards providing a faithful portrayal of the real-world usage of Pupil Invisible glasses.   In particular, we develop a geometric framework for gauging gaze-estimation accuracy that goes beyond reporting mean angular accuracy. We demonstrate that Pupil Invisible glasses, without the need of a calibration, provide gaze estimates which are robust to perturbations, including outdoor lighting conditions and slippage of the headset.



### Unsupervised Domain Adaptation with Progressive Adaptation of Subspaces
- **Arxiv ID**: http://arxiv.org/abs/2009.00520v1
- **DOI**: 10.1016/j.patcog.2022.108918
- **Categories**: **cs.LG**, cs.CV, stat.ML, 68T10, I.2.6
- **Links**: [PDF](http://arxiv.org/pdf/2009.00520v1)
- **Published**: 2020-09-01 15:40:50+00:00
- **Updated**: 2020-09-01 15:40:50+00:00
- **Authors**: Weikai Li, Songcan Chen
- **Comment**: None
- **Journal**: Pattern Recognition, 132, 108918 (2022)
- **Summary**: Unsupervised Domain Adaptation (UDA) aims to classify unlabeled target domain by transferring knowledge from labeled source domain with domain shift. Most of the existing UDA methods try to mitigate the adverse impact induced by the shift via reducing domain discrepancy. However, such approaches easily suffer a notorious mode collapse issue due to the lack of labels in target domain. Naturally, one of the effective ways to mitigate this issue is to reliably estimate the pseudo labels for target domain, which itself is hard. To overcome this, we propose a novel UDA method named Progressive Adaptation of Subspaces approach (PAS) in which we utilize such an intuition that appears much reasonable to gradually obtain reliable pseudo labels. Speci fically, we progressively and steadily refine the shared subspaces as bridge of knowledge transfer by adaptively anchoring/selecting and leveraging those target samples with reliable pseudo labels. Subsequently, the refined subspaces can in turn provide more reliable pseudo-labels of the target domain, making the mode collapse highly mitigated. Our thorough evaluation demonstrates that PAS is not only effective for common UDA, but also outperforms the state-of-the arts for more challenging Partial Domain Adaptation (PDA) situation, where the source label set subsumes the target one.



### Training Deep Neural Networks with Constrained Learning Parameters
- **Arxiv ID**: http://arxiv.org/abs/2009.00540v1
- **DOI**: 10.1109/ICRC2020.2020.00018
- **Categories**: **cs.LG**, cs.CV, stat.ML, 68T07, 90C27, I.2.6
- **Links**: [PDF](http://arxiv.org/pdf/2009.00540v1)
- **Published**: 2020-09-01 16:20:11+00:00
- **Updated**: 2020-09-01 16:20:11+00:00
- **Authors**: Prasanna Date, Christopher D. Carothers, John E. Mitchell, James A. Hendler, Malik Magdon-Ismail
- **Comment**: None
- **Journal**: None
- **Summary**: Today's deep learning models are primarily trained on CPUs and GPUs. Although these models tend to have low error, they consume high power and utilize large amount of memory owing to double precision floating point learning parameters. Beyond the Moore's law, a significant portion of deep learning tasks would run on edge computing systems, which will form an indispensable part of the entire computation fabric. Subsequently, training deep learning models for such systems will have to be tailored and adopted to generate models that have the following desirable characteristics: low error, low memory, and low power. We believe that deep neural networks (DNNs), where learning parameters are constrained to have a set of finite discrete values, running on neuromorphic computing systems would be instrumental for intelligent edge computing systems having these desirable characteristics. To this extent, we propose the Combinatorial Neural Network Training Algorithm (CoNNTrA), that leverages a coordinate gradient descent-based approach for training deep learning models with finite discrete learning parameters. Next, we elaborate on the theoretical underpinnings and evaluate the computational complexity of CoNNTrA. As a proof of concept, we use CoNNTrA to train deep learning models with ternary learning parameters on the MNIST, Iris and ImageNet data sets and compare their performance to the same models trained using Backpropagation. We use following performance metrics for the comparison: (i) Training error; (ii) Validation error; (iii) Memory usage; and (iv) Training time. Our results indicate that CoNNTrA models use 32x less memory and have errors at par with the Backpropagation models.



### A Primer on Motion Capture with Deep Learning: Principles, Pitfalls and Perspectives
- **Arxiv ID**: http://arxiv.org/abs/2009.00564v2
- **DOI**: 10.1016/j.neuron.2020.09.017
- **Categories**: **cs.CV**, cs.LG, q-bio.NC, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2009.00564v2)
- **Published**: 2020-09-01 16:51:33+00:00
- **Updated**: 2020-09-02 20:29:12+00:00
- **Authors**: Alexander Mathis, Steffen Schneider, Jessy Lauer, Mackenzie W. Mathis
- **Comment**: Review, 21 pages, 8 figures and 5 boxes
- **Journal**: Neuron Volume 108, Issue 1, 14 October 2020, Pages 44-65
- **Summary**: Extracting behavioral measurements non-invasively from video is stymied by the fact that it is a hard computational problem. Recent advances in deep learning have tremendously advanced predicting posture from videos directly, which quickly impacted neuroscience and biology more broadly. In this primer we review the budding field of motion capture with deep learning. In particular, we will discuss the principles of those novel algorithms, highlight their potential as well as pitfalls for experimentalists, and provide a glimpse into the future.



### MORPH-DSLAM: Model Order Reduction for PHysics-based Deformable SLAM
- **Arxiv ID**: http://arxiv.org/abs/2009.00576v2
- **DOI**: 10.1109/TPAMI.2021.3118802
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.00576v2)
- **Published**: 2020-09-01 17:06:41+00:00
- **Updated**: 2021-10-06 10:03:13+00:00
- **Authors**: Alberto Badias, Iciar Alfaro, David Gonzalez, Francisco Chinesta, Elias Cueto
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a new methodology to estimate the 3D displacement field of deformable objects from video sequences using standard monocular cameras. We solve in real time the complete (possibly visco-)hyperelasticity problem to properly describe the strain and stress fields that are consistent with the displacements captured by the images, constrained by real physics. We do not impose any ad-hoc prior or energy minimization in the external surface, since the real and complete mechanics problem is solved. This means that we can also estimate the internal state of the objects, even in occluded areas, just by observing the external surface and the knowledge of material properties and geometry. Solving this problem in real time using a realistic constitutive law, usually non-linear, is out of reach for current systems. To overcome this difficulty, we solve off-line a parametrized problem that considers each source of variability in the problem as a new parameter and, consequently, as a new dimension in the formulation. Model Order Reduction methods allow us to reduce the dimensionality of the problem, and therefore, its computational cost, while preserving the visualization of the solution in the high-dimensionality space. This allows an accurate estimation of the object deformations, improving also the robustness in the 3D points estimation.



### A Short Review on Data Modelling for Vector Fields
- **Arxiv ID**: http://arxiv.org/abs/2009.00577v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.00577v1)
- **Published**: 2020-09-01 17:07:29+00:00
- **Updated**: 2020-09-01 17:07:29+00:00
- **Authors**: Jun Li, Wanrong Hong, Yusheng Xiang
- **Comment**: 18 pages, 0 figures
- **Journal**: None
- **Summary**: Machine learning methods based on statistical principles have proven highly successful in dealing with a wide variety of data analysis and analytics tasks. Traditional data models are mostly concerned with independent identically distributed data. The recent success of end-to-end modelling scheme using deep neural networks equipped with effective structures such as convolutional layers or skip connections allows the extension to more sophisticated and structured practical data, such as natural language, images, videos, etc. On the application side, vector fields are an extremely useful type of data in empirical sciences, as well as signal processing, e.g. non-parametric transformations of 3D point clouds using 3D vector fields, the modelling of the fluid flow in earth science, and the modelling of physical fields.   This review article is dedicated to recent computational tools of vector fields, including vector data representations, predictive model of spatial data, as well as applications in computer vision, signal processing, and empirical sciences.



### A Deep 2-Dimensional Dynamical Spiking Neuronal Network for Temporal Encoding trained with STDP
- **Arxiv ID**: http://arxiv.org/abs/2009.00581v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.00581v1)
- **Published**: 2020-09-01 17:12:18+00:00
- **Updated**: 2020-09-01 17:12:18+00:00
- **Authors**: Matthew Evanusa, Cornelia Fermuller, Yiannis Aloimonos
- **Comment**: None
- **Journal**: None
- **Summary**: The brain is known to be a highly complex, asynchronous dynamical system that is highly tailored to encode temporal information. However, recent deep learning approaches to not take advantage of this temporal coding. Spiking Neural Networks (SNNs) can be trained using biologically-realistic learning mechanisms, and can have neuronal activation rules that are biologically relevant. This type of network is also structured fundamentally around accepting temporal information through a time-decaying voltage update, a kind of input that current rate-encoding networks have difficulty with. Here we show that a large, deep layered SNN with dynamical, chaotic activity mimicking the mammalian cortex with biologically-inspired learning rules, such as STDP, is capable of encoding information from temporal data. We argue that the randomness inherent in the network weights allow the neurons to form groups that encode the temporal data being inputted after self-organizing with STDP. We aim to show that precise timing of input stimulus is critical in forming synchronous neural groups in a layered network. We analyze the network in terms of network entropy as a metric of information transfer. We hope to tackle two problems at once: the creation of artificial temporal neural systems for artificial intelligence, as well as solving coding mechanisms in the brain.



### Quality-aware semi-supervised learning for CMR segmentation
- **Arxiv ID**: http://arxiv.org/abs/2009.00584v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.00584v1)
- **Published**: 2020-09-01 17:18:22+00:00
- **Updated**: 2020-09-01 17:18:22+00:00
- **Authors**: Bram Ruijsink, Esther Puyol-Anton, Ye Li, Wenja Bai, Eric Kerfoot, Reza Razavi, Andrew P. King
- **Comment**: MICCAI STACOM 2020
- **Journal**: None
- **Summary**: One of the challenges in developing deep learning algorithms for medical image segmentation is the scarcity of annotated training data. To overcome this limitation, data augmentation and semi-supervised learning (SSL) methods have been developed. However, these methods have limited effectiveness as they either exploit the existing data set only (data augmentation) or risk negative impact by adding poor training examples (SSL). Segmentations are rarely the final product of medical image analysis - they are typically used in downstream tasks to infer higher-order patterns to evaluate diseases. Clinicians take into account a wealth of prior knowledge on biophysics and physiology when evaluating image analysis results. We have used these clinical assessments in previous works to create robust quality-control (QC) classifiers for automated cardiac magnetic resonance (CMR) analysis. In this paper, we propose a novel scheme that uses QC of the downstream task to identify high quality outputs of CMR segmentation networks, that are subsequently utilised for further network training. In essence, this provides quality-aware augmentation of training data in a variant of SSL for segmentation networks (semiQCSeg). We evaluate our approach in two CMR segmentation tasks (aortic and short axis cardiac volume segmentation) using UK Biobank data and two commonly used network architectures (U-net and a Fully Convolutional Network) and compare against supervised and SSL strategies. We show that semiQCSeg improves training of the segmentation networks. It decreases the need for labelled data, while outperforming the other methods in terms of Dice and clinical metrics. SemiQCSeg can be an efficient approach for training segmentation networks for medical image data when labelled datasets are scarce.



### Inducing Predictive Uncertainty Estimation for Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2009.00603v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.00603v1)
- **Published**: 2020-09-01 17:52:00+00:00
- **Updated**: 2020-09-01 17:52:00+00:00
- **Authors**: Weidi Xie, Jeffrey Byrne, Andrew Zisserman
- **Comment**: To Appear at the British Machine Vision Conference (BMVC), 2020
- **Journal**: None
- **Summary**: Knowing when an output can be trusted is critical for reliably using face recognition systems. While there has been enormous effort in recent research on improving face verification performance, understanding when a model's predictions should or should not be trusted has received far less attention. Our goal is to assign a confidence score for a face image that reflects its quality in terms of recognizable information. To this end, we propose a method for generating image quality training data automatically from 'mated-pairs' of face images, and use the generated data to train a lightweight Predictive Confidence Network, termed as PCNet, for estimating the confidence score of a face image. We systematically evaluate the usefulness of PCNet with its error versus reject performance, and demonstrate that it can be universally paired with and improve the robustness of any verification model. We describe three use cases on the public IJB-C face verification benchmark: (i) to improve 1:1 image-based verification error rates by rejecting low-quality face images; (ii) to improve quality score based fusion performance on the 1:1 set-based verification benchmark; and (iii) its use as a quality measure for selecting high quality (unblurred, good lighting, more frontal) faces from a collection, e.g. for automatic enrolment or display.



### NPRportrait 1.0: A Three-Level Benchmark for Non-Photorealistic Rendering of Portraits
- **Arxiv ID**: http://arxiv.org/abs/2009.00633v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.00633v1)
- **Published**: 2020-09-01 18:04:19+00:00
- **Updated**: 2020-09-01 18:04:19+00:00
- **Authors**: Paul L. Rosin, Yu-Kun Lai, David Mould, Ran Yi, Itamar Berger, Lars Doyle, Seungyong Lee, Chuan Li, Yong-Jin Liu, Amir Semmo, Ariel Shamir, Minjung Son, Holger Winnemoller
- **Comment**: 17 pages, 15 figures
- **Journal**: None
- **Summary**: Despite the recent upsurge of activity in image-based non-photorealistic rendering (NPR), and in particular portrait image stylisation, due to the advent of neural style transfer, the state of performance evaluation in this field is limited, especially compared to the norms in the computer vision and machine learning communities. Unfortunately, the task of evaluating image stylisation is thus far not well defined, since it involves subjective, perceptual and aesthetic aspects. To make progress towards a solution, this paper proposes a new structured, three level, benchmark dataset for the evaluation of stylised portrait images. Rigorous criteria were used for its construction, and its consistency was validated by user studies. Moreover, a new methodology has been developed for evaluating portrait stylisation algorithms, which makes use of the different benchmark levels as well as annotations provided by user studies regarding the characteristics of the faces. We perform evaluation for a wide variety of image stylisation methods (both portrait-specific and general purpose, and also both traditional NPR approaches and neural style transfer) using the new benchmark dataset.



### View-invariant action recognition
- **Arxiv ID**: http://arxiv.org/abs/2009.00638v1
- **DOI**: 10.1007/978-3-030-03243-2_878-1
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.00638v1)
- **Published**: 2020-09-01 18:08:46+00:00
- **Updated**: 2020-09-01 18:08:46+00:00
- **Authors**: Yogesh S Rawat, Shruti Vyas
- **Comment**: None
- **Journal**: None
- **Summary**: Human action recognition is an important problem in computer vision. It has a wide range of applications in surveillance, human-computer interaction, augmented reality, video indexing, and retrieval. The varying pattern of spatio-temporal appearance generated by human action is key for identifying the performed action. We have seen a lot of research exploring this dynamics of spatio-temporal appearance for learning a visual representation of human actions. However, most of the research in action recognition is focused on some common viewpoints, and these approaches do not perform well when there is a change in viewpoint. Human actions are performed in a 3-dimensional environment and are projected to a 2-dimensional space when captured as a video from a given viewpoint. Therefore, an action will have a different spatio-temporal appearance from different viewpoints. The research in view-invariant action recognition addresses this problem and focuses on recognizing human actions from unseen viewpoints.



### Fed-Sim: Federated Simulation for Medical Imaging
- **Arxiv ID**: http://arxiv.org/abs/2009.00668v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2009.00668v1)
- **Published**: 2020-09-01 19:17:46+00:00
- **Updated**: 2020-09-01 19:17:46+00:00
- **Authors**: Daiqing Li, Amlan Kar, Nishant Ravikumar, Alejandro F Frangi, Sanja Fidler
- **Comment**: MICCAI 2020 (Early Accept)
- **Journal**: None
- **Summary**: Labelling data is expensive and time consuming especially for domains such as medical imaging that contain volumetric imaging data and require expert knowledge. Exploiting a larger pool of labeled data available across multiple centers, such as in federated learning, has also seen limited success since current deep learning approaches do not generalize well to images acquired with scanners from different manufacturers. We aim to address these problems in a common, learning-based image simulation framework which we refer to as Federated Simulation. We introduce a physics-driven generative approach that consists of two learnable neural modules: 1) a module that synthesizes 3D cardiac shapes along with their materials, and 2) a CT simulator that renders these into realistic 3D CT Volumes, with annotations. Since the model of geometry and material is disentangled from the imaging sensor, it can effectively be trained across multiple medical centers. We show that our data synthesis framework improves the downstream segmentation performance on several datasets. Project Page: https://nv-tlabs.github.io/fed-sim/ .



### Applying a random projection algorithm to optimize machine learning model for predicting peritoneal metastasis in gastric cancer patients using CT images
- **Arxiv ID**: http://arxiv.org/abs/2009.00675v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2009.00675v1)
- **Published**: 2020-09-01 19:53:09+00:00
- **Updated**: 2020-09-01 19:53:09+00:00
- **Authors**: Seyedehnafiseh Mirniaharikandehei, Morteza Heidari, Gopichandh Danala, Sivaramakrishnan Lakshmivarahan, Bin Zheng
- **Comment**: 24 pages, 7 figures
- **Journal**: None
- **Summary**: Background and Objective: Non-invasively predicting the risk of cancer metastasis before surgery plays an essential role in determining optimal treatment methods for cancer patients (including who can benefit from neoadjuvant chemotherapy). Although developing radiomics based machine learning (ML) models has attracted broad research interest for this purpose, it often faces a challenge of how to build a highly performed and robust ML model using small and imbalanced image datasets. Methods: In this study, we explore a new approach to build an optimal ML model. A retrospective dataset involving abdominal computed tomography (CT) images acquired from 159 patients diagnosed with gastric cancer is assembled. Among them, 121 cases have peritoneal metastasis (PM), while 38 cases do not have PM. A computer-aided detection (CAD) scheme is first applied to segment primary gastric tumor volumes and initially computes 315 image features. Then, two Gradient Boosting Machine (GBM) models embedded with two different feature dimensionality reduction methods, namely, the principal component analysis (PCA) and a random projection algorithm (RPA) and a synthetic minority oversampling technique, are built to predict the risk of the patients having PM. All GBM models are trained and tested using a leave-one-case-out cross-validation method. Results: Results show that the GBM embedded with RPA yielded a significantly higher prediction accuracy (71.2%) than using PCA (65.2%) (p<0.05). Conclusions: The study demonstrated that CT images of the primary gastric tumors contain discriminatory information to predict the risk of PM, and RPA is a promising method to generate optimal feature vector, improving the performance of ML models of medical images.



### Text and Style Conditioned GAN for Generation of Offline Handwriting Lines
- **Arxiv ID**: http://arxiv.org/abs/2009.00678v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.00678v1)
- **Published**: 2020-09-01 20:19:42+00:00
- **Updated**: 2020-09-01 20:19:42+00:00
- **Authors**: Brian Davis, Chris Tensmeyer, Brian Price, Curtis Wigington, Bryan Morse, Rajiv Jain
- **Comment**: Includes Supplementary Material. Accepted at BMVC 2020. 32 pages, 30
  figures
- **Journal**: None
- **Summary**: This paper presents a GAN for generating images of handwritten lines conditioned on arbitrary text and latent style vectors. Unlike prior work, which produce stroke points or single-word images, this model generates entire lines of offline handwriting. The model produces variable-sized images by using style vectors to determine character widths. A generator network is trained with GAN and autoencoder techniques to learn style, and uses a pre-trained handwriting recognition network to induce legibility. A study using human evaluators demonstrates that the model produces images that appear to be written by a human. After training, the encoder network can extract a style vector from an image, allowing images in a similar style to be generated, but with arbitrary text.



### Aggregating Long-Term Context for Learning Laparoscopic and Robot-Assisted Surgical Workflows
- **Arxiv ID**: http://arxiv.org/abs/2009.00681v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2009.00681v4)
- **Published**: 2020-09-01 20:29:14+00:00
- **Updated**: 2021-05-10 20:02:18+00:00
- **Authors**: Yutong Ban, Guy Rosman, Thomas Ward, Daniel Hashimoto, Taisei Kondo, Hidekazu Iwaki, Ozanan Meireles, Daniela Rus
- **Comment**: None
- **Journal**: None
- **Summary**: Analyzing surgical workflow is crucial for surgical assistance robots to understand surgeries. With the understanding of the complete surgical workflow, the robots are able to assist the surgeons in intra-operative events, such as by giving a warning when the surgeon is entering specific keys or high-risk phases. Deep learning techniques have recently been widely applied to recognizing surgical workflows. Many of the existing temporal neural network models are limited in their capability to handle long-term dependencies in the data, instead, relying upon the strong performance of the underlying per-frame visual models. We propose a new temporal network structure that leverages task-specific network representation to collect long-term sufficient statistics that are propagated by a sufficient statistics model (SSM). We implement our approach within an LSTM backbone for the task of surgical phase recognition and explore several choices for propagated statistics. We demonstrate superior results over existing and novel state-of-the-art segmentation techniques on two laparoscopic cholecystectomy datasets: the publicly available Cholec80 dataset and MGH100, a novel dataset with more challenging and clinically meaningful segment labels.



### Unsupervised Single-Image Reflection Separation Using Perceptual Deep Image Priors
- **Arxiv ID**: http://arxiv.org/abs/2009.00702v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM, I.5.4; I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/2009.00702v1)
- **Published**: 2020-09-01 21:08:30+00:00
- **Updated**: 2020-09-01 21:08:30+00:00
- **Authors**: Suhong Kim, Hamed RahmaniKhezri, Seyed Mohammad Nourbakhsh, Mohamed Hefeeda
- **Comment**: 11 pages, 13 figures
- **Journal**: None
- **Summary**: Reflections often degrade the quality of the image by obstructing the background scene. This is not desirable for everyday users, and it negatively impacts the performance of multimedia applications that process images with reflections. Most current methods for removing reflections utilize supervised-learning models. However, these models require an extensive number of image pairs to perform well, especially on natural images with reflection, which is difficult to achieve in practice. In this paper, we propose a novel unsupervised framework for single-image reflection separation. Instead of learning from a large dataset, we optimize the parameters of two cross-coupled deep convolutional networks on a target image to generate two exclusive background and reflection layers. In particular, we design a new architecture of the network to embed semantic features extracted from a pre-trained deep classification network, which gives more meaningful separation similar to human perception. Quantitative and qualitative results on commonly used datasets in the literature show that our method's performance is at least on par with the state-of-the-art supervised methods and, occasionally, better without requiring large training datasets. Our results also show that our method significantly outperforms the closest unsupervised method in the literature for removing reflections from single images.



### SPAN: Spatial Pyramid Attention Network forImage Manipulation Localization
- **Arxiv ID**: http://arxiv.org/abs/2009.00726v2
- **DOI**: 10.1007/978-3-030-58589-1_19 10.1007/978-3-030-58589-1_19
  10.1007/978-3-030-58589-1_19 10.1007/978-3-030-58589-1_19
- **Categories**: **cs.CV**, I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/2009.00726v2)
- **Published**: 2020-09-01 21:59:35+00:00
- **Updated**: 2021-01-14 01:43:21+00:00
- **Authors**: Xuefeng Hu, Zhihan Zhang, Zhenye Jiang, Syomantak Chaudhuri, Zhenheng Yang, Ram Nevatia
- **Comment**: Accepted at ECCV 2020
  (https://link.springer.com/chapter/10.1007%2F978-3-030-58589-1_19) Code
  Available at https://github.com/ZhiHanZ/IRIS0-SPAN/
- **Journal**: None
- **Summary**: We present a novel framework, Spatial Pyramid Attention Network (SPAN) for detection and localization of multiple types of image manipulations. The proposed architecture efficiently and effectively models the relationship between image patches at multiple scales by constructing a pyramid of local self-attention blocks. The design includes a novel position projection to encode the spatial positions of the patches. SPAN is trained on a generic, synthetic dataset but can also be fine tuned for specific datasets; The proposed method shows significant gains in performance on standard datasets over previous state-of-the-art methods.



### Defending against substitute model black box adversarial attacks with the 01 loss
- **Arxiv ID**: http://arxiv.org/abs/2009.09803v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.09803v1)
- **Published**: 2020-09-01 22:32:51+00:00
- **Updated**: 2020-09-01 22:32:51+00:00
- **Authors**: Yunzhe Xue, Meiyan Xie, Usman Roshan
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2006.07800;
  text overlap with arXiv:2008.09148
- **Journal**: None
- **Summary**: Substitute model black box attacks can create adversarial examples for a target model just by accessing its output labels. This poses a major challenge to machine learning models in practice, particularly in security sensitive applications. The 01 loss model is known to be more robust to outliers and noise than convex models that are typically used in practice. Motivated by these properties we present 01 loss linear and 01 loss dual layer neural network models as a defense against transfer based substitute model black box attacks. We compare the accuracy of adversarial examples from substitute model black box attacks targeting our 01 loss models and their convex counterparts for binary classification on popular image benchmarks. Our 01 loss dual layer neural network has an adversarial accuracy of 66.2%, 58%, 60.5%, and 57% on MNIST, CIFAR10, STL10, and ImageNet respectively whereas the sigmoid activated logistic loss counterpart has accuracies of 63.5%, 19.3%, 14.9%, and 27.6%. Except for MNIST the convex counterparts have substantially lower adversarial accuracies. We show practical applications of our models to deter traffic sign and facial recognition adversarial attacks. On GTSRB street sign and CelebA facial detection our 01 loss network has 34.6% and 37.1% adversarial accuracy respectively whereas the convex logistic counterpart has accuracy 24% and 1.9%. Finally we show that our 01 loss network can attain robustness on par with simple convolutional neural networks and much higher than its convex counterpart even when attacked with a convolutional network substitute model. Our work shows that 01 loss models offer a powerful defense against substitute model black box attacks.



### On Open and Strong-Scaling Tools for Atom Probe Crystallography: High-Throughput Methods for Indexing Crystal Structure and Orientation
- **Arxiv ID**: http://arxiv.org/abs/2009.00735v1
- **DOI**: None
- **Categories**: **physics.comp-ph**, cond-mat.mtrl-sci, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.00735v1)
- **Published**: 2020-09-01 22:50:03+00:00
- **Updated**: 2020-09-01 22:50:03+00:00
- **Authors**: Markus Kühbach, Matthew Kasemer, Baptiste Gault, Andrew Breen
- **Comment**: 36 pages, 19 figures, preprint
- **Journal**: None
- **Summary**: Volumetric crystal structure indexing and orientation mapping are key data processing steps for virtually any quantitative study of spatial correlations between the local chemistry and the microstructure of a material. For electron and X-ray diffraction methods it is possible to develop indexing tools which compare measured and analytically computed patterns to decode the structure and relative orientation within local regions of interest. Consequently, a number of numerically efficient and automated software tools exist to solve the above characterisation tasks.   For atom probe tomography (APT) experiments, however, the strategy of making comparisons between measured and analytically computed patterns is less robust because many APT datasets may contain substantial noise. Given that general enough predictive models for such noise remain elusive, crystallography tools for APT face several limitations: Their robustness to noise, and therefore, their capability to identify and distinguish different crystal structures and orientation is limited. In addition, the tools are sequential and demand substantial manual interaction. In combination, this makes robust uncertainty quantifying with automated high-throughput studies of the latent crystallographic information a difficult task with APT data.   To improve the situation, we review the existent methods and discuss how they link to those in the diffraction communities. With this we modify some of the APT methods to yield more robust descriptors of the atomic arrangement. We report how this enables the development of an open-source software tool for strong-scaling and automated identifying of crystal structure and mapping crystal orientation in nanocrystalline APT datasets with multiple phases.



### Bidirectional Attention Network for Monocular Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/2009.00743v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2009.00743v2)
- **Published**: 2020-09-01 23:14:05+00:00
- **Updated**: 2021-03-25 18:43:01+00:00
- **Authors**: Shubhra Aich, Jean Marie Uwabeza Vianney, Md Amirul Islam, Mannat Kaur, Bingbing Liu
- **Comment**: Camera-ready for IEEE International Conference on Robotics and
  Automation (ICRA) 2021
- **Journal**: None
- **Summary**: In this paper, we propose a Bidirectional Attention Network (BANet), an end-to-end framework for monocular depth estimation (MDE) that addresses the limitation of effectively integrating local and global information in convolutional neural networks. The structure of this mechanism derives from a strong conceptual foundation of neural machine translation, and presents a light-weight mechanism for adaptive control of computation similar to the dynamic nature of recurrent neural networks. We introduce bidirectional attention modules that utilize the feed-forward feature maps and incorporate the global context to filter out ambiguity. Extensive experiments reveal the high degree of capability of this bidirectional attention model over feed-forward baselines and other state-of-the-art methods for monocular depth estimation on two challenging datasets -- KITTI and DIODE. We show that our proposed approach either outperforms or performs at least on a par with the state-of-the-art monocular depth estimation methods with less memory and computational complexity.



### Iris Liveness Detection Competition (LivDet-Iris) -- The 2020 Edition
- **Arxiv ID**: http://arxiv.org/abs/2009.00749v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.00749v1)
- **Published**: 2020-09-01 23:43:19+00:00
- **Updated**: 2020-09-01 23:43:19+00:00
- **Authors**: Priyanka Das, Joseph McGrath, Zhaoyuan Fang, Aidan Boyd, Ganghee Jang, Amir Mohammadi, Sandip Purnapatra, David Yambay, Sébastien Marcel, Mateusz Trokielewicz, Piotr Maciejewicz, Kevin Bowyer, Adam Czajka, Stephanie Schuckers, Juan Tapia, Sebastian Gonzalez, Meiling Fang, Naser Damer, Fadi Boutros, Arjan Kuijper, Renu Sharma, Cunjian Chen, Arun Ross
- **Comment**: 9 pages, 3 figures, 3 tables, Accepted for presentation at
  International Joint Conference on Biometrics (IJCB 2020)
- **Journal**: None
- **Summary**: Launched in 2013, LivDet-Iris is an international competition series open to academia and industry with the aim to assess and report advances in iris Presentation Attack Detection (PAD). This paper presents results from the fourth competition of the series: LivDet-Iris 2020. This year's competition introduced several novel elements: (a) incorporated new types of attacks (samples displayed on a screen, cadaver eyes and prosthetic eyes), (b) initiated LivDet-Iris as an on-going effort, with a testing protocol available now to everyone via the Biometrics Evaluation and Testing (BEAT)(https://www.idiap.ch/software/beat/) open-source platform to facilitate reproducibility and benchmarking of new algorithms continuously, and (c) performance comparison of the submitted entries with three baseline methods (offered by the University of Notre Dame and Michigan State University), and three open-source iris PAD methods available in the public domain. The best performing entry to the competition reported a weighted average APCER of 59.10\% and a BPCER of 0.46\% over all five attack types. This paper serves as the latest evaluation of iris PAD on a large spectrum of presentation attack instruments.



