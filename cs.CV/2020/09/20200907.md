# Arxiv Papers in cs.CV on 2020-09-07
### 3D Room Layout Estimation Beyond the Manhattan World Assumption
- **Arxiv ID**: http://arxiv.org/abs/2009.02857v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.02857v1)
- **Published**: 2020-09-07 02:14:29+00:00
- **Updated**: 2020-09-07 02:14:29+00:00
- **Authors**: Dongho Choi
- **Comment**: 3rd Place @ ECCV 2020 Holistic Scene Structures for 3D Vision
  Workshop Challenges Track 1; 6 pages with 3 figures and 2 tables
- **Journal**: None
- **Summary**: Predicting 3D room layout from single image is a challenging task with many applications. In this paper, we propose a new training and post-processing method for 3D room layout estimation, built on a recent state-of-the-art 3D room layout estimation model. Experimental results show our method outperforms state-of-the-art approaches by a large margin in predicting visible room layout. Our method has obtained the 3rd place in 2020 Holistic Scene Structures for 3D Vision Workshop.



### Channel-wise Alignment for Adaptive Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2009.02862v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.02862v1)
- **Published**: 2020-09-07 02:42:18+00:00
- **Updated**: 2020-09-07 02:42:18+00:00
- **Authors**: Hang Yang, Shan Jiang, Xinge Zhu, Mingyang Huang, Zhiqiang Shen, Chunxiao Liu, Jianping Shi
- **Comment**: First two authors contributed equally
- **Journal**: None
- **Summary**: Generic object detection has been immensely promoted by the development of deep convolutional neural networks in the past decade. However, in the domain shift circumstance, the changes in weather, illumination, etc., often cause domain gap, and thus performance drops substantially when detecting objects from one domain to another. Existing methods on this task usually draw attention on the high-level alignment based on the whole image or object of interest, which naturally, cannot fully utilize the fine-grained channel information. In this paper, we realize adaptation from a thoroughly different perspective, i.e., channel-wise alignment. Motivated by the finding that each channel focuses on a specific pattern (e.g., on special semantic regions, such as car), we aim to align the distribution of source and target domain on the channel level, which is finer for integration between discrepant domains. Our method mainly consists of self channel-wise and cross channel-wise alignment. These two parts explore the inner-relation and cross-relation of attention regions implicitly from the view of channels. Further more, we also propose a RPN domain classifier module to obtain a domain-invariant RPN network. Extensive experiments show that the proposed method performs notably better than existing methods with about 5% improvement under various domain-shift settings. Experiments on different task (e.g. instance segmentation) also demonstrate its good scalability.



### Frontier Detection and Reachability Analysis for Efficient 2D Graph-SLAM Based Active Exploration
- **Arxiv ID**: http://arxiv.org/abs/2009.02869v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.02869v1)
- **Published**: 2020-09-07 03:13:47+00:00
- **Updated**: 2020-09-07 03:13:47+00:00
- **Authors**: Zezhou Sun, Banghe Wu, Cheng-Zhong Xu, Sanjay E. Sarma, Jian Yang, Hui Kong
- **Comment**: None
- **Journal**: None
- **Summary**: We propose an integrated approach to active exploration by exploiting the Cartographer method as the base SLAM module for submap creation and performing efficient frontier detection in the geometrically co-aligned submaps induced by graph optimization. We also carry out analysis on the reachability of frontiers and their clusters to ensure that the detected frontier can be reached by robot. Our method is tested on a mobile robot in real indoor scene to demonstrate the effectiveness and efficiency of our approach.



### Benchmarking off-the-shelf statistical shape modeling tools in clinical applications
- **Arxiv ID**: http://arxiv.org/abs/2009.02878v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.02878v1)
- **Published**: 2020-09-07 03:51:35+00:00
- **Updated**: 2020-09-07 03:51:35+00:00
- **Authors**: Anupama Goparaju, Alexandre Bone, Nan Hu, Heath B. Henninger, Andrew E. Anderson, Stanley Durrleman, Matthijs Jacxsens, Alan Morris, Ibolya Csecs, Nassir Marrouche, Shireen Y. Elhabian
- **Comment**: 22 pages, 22 figures
- **Journal**: None
- **Summary**: Statistical shape modeling (SSM) is widely used in biology and medicine as a new generation of morphometric approaches for the quantitative analysis of anatomical shapes. Technological advancements of in vivo imaging have led to the development of open-source computational tools that automate the modeling of anatomical shapes and their population-level variability. However, little work has been done on the evaluation and validation of such tools in clinical applications that rely on morphometric quantifications (e.g., implant design and lesion screening). Here, we systematically assess the outcome of widely used, state-of-the-art SSM tools, namely ShapeWorks, Deformetrica, and SPHARM-PDM. We use both quantitative and qualitative metrics to evaluate shape models from different tools. We propose validation frameworks for anatomical landmark/measurement inference and lesion screening. We also present a lesion screening method to objectively characterize subtle abnormal shape changes with respect to learned population-level statistics of controls. Results demonstrate that SSM tools display different levels of consistencies, where ShapeWorks and Deformetrica models are more consistent compared to models from SPHARM-PDM due to the groupwise approach of estimating surface correspondences. Furthermore, ShapeWorks and Deformetrica shape models are found to capture clinically relevant population-level variability compared to SPHARM-PDM models.



### Quantifying Explainability of Saliency Methods in Deep Neural Networks with a Synthetic Dataset
- **Arxiv ID**: http://arxiv.org/abs/2009.02899v6
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2009.02899v6)
- **Published**: 2020-09-07 05:55:24+00:00
- **Updated**: 2022-12-10 13:44:07+00:00
- **Authors**: Erico Tjoa, Cuntai Guan
- **Comment**: None
- **Journal**: None
- **Summary**: Post-hoc analysis is a popular category in eXplainable artificial intelligence (XAI) study. In particular, methods that generate heatmaps have been used to explain the deep neural network (DNN), a black-box model. Heatmaps can be appealing due to the intuitive and visual ways to understand them but assessing their qualities might not be straightforward. Different ways to assess heatmaps' quality have their own merits and shortcomings. This paper introduces a synthetic dataset that can be generated adhoc along with the ground-truth heatmaps for more objective quantitative assessment. Each sample data is an image of a cell with easily recognized features that are distinguished from localization ground-truth mask, hence facilitating a more transparent assessment of different XAI methods. Comparison and recommendations are made, shortcomings are clarified along with suggestions for future research directions to handle the finer details of select post-hoc analysis methods. Furthermore, mabCAM is introduced as the heatmap generation method compatible with our ground-truth heatmaps. The framework is easily generalizable and uses only standard deep learning components.



### DV-ConvNet: Fully Convolutional Deep Learning on Point Clouds with Dynamic Voxelization and 3D Group Convolution
- **Arxiv ID**: http://arxiv.org/abs/2009.02918v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.02918v2)
- **Published**: 2020-09-07 07:45:05+00:00
- **Updated**: 2021-07-27 10:10:04+00:00
- **Authors**: Zhaoyu Su, Pin Siang Tan, Junkang Chow, Jimmy Wu, Yehur Cheong, Yu-Hsing Wang
- **Comment**: None
- **Journal**: None
- **Summary**: 3D point cloud interpretation is a challenging task due to the randomness and sparsity of the component points. Many of the recently proposed methods like PointNet and PointCNN have been focusing on learning shape descriptions from point coordinates as point-wise input features, which usually involves complicated network architectures. In this work, we draw attention back to the standard 3D convolutions towards an efficient 3D point cloud interpretation. Instead of converting the entire point cloud into voxel representations like the other volumetric methods, we voxelize the sub-portions of the point cloud only at necessary locations within each convolution layer on-the-fly, using our dynamic voxelization operation with self-adaptive voxelization resolution. In addition, we incorporate 3D group convolution into our dense convolution kernel implementation to further exploit the rotation invariant features of point cloud. Benefiting from its simple fully-convolutional architecture, our network is able to run and converge at a considerably fast speed, while yields on-par or even better performance compared with the state-of-the-art methods on several benchmark datasets.



### Going deeper with brain morphometry using neural networks
- **Arxiv ID**: http://arxiv.org/abs/2009.03303v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.03303v1)
- **Published**: 2020-09-07 07:57:13+00:00
- **Updated**: 2020-09-07 07:57:13+00:00
- **Authors**: Rodrigo Santa Cruz, Léo Lebrat, Pierrick Bourgeat, Vincent Doré, Jason Dowling, Jurgen Fripp, Clinton Fookes, Olivier Salvado
- **Comment**: None
- **Journal**: None
- **Summary**: Brain morphometry from magnetic resonance imaging (MRI) is a consolidated biomarker for many neurodegenerative diseases. Recent advances in this domain indicate that deep convolutional neural networks can infer morphometric measurements within a few seconds. Nevertheless, the accuracy of the devised model for insightful bio-markers (mean curvature and thickness) remains unsatisfactory. In this paper, we propose a more accurate and efficient neural network model for brain morphometry named HerstonNet. More specifically, we develop a 3D ResNet-based neural network to learn rich features directly from MRI, design a multi-scale regression scheme by predicting morphometric measures at feature maps of different resolutions, and leverage a robust optimization method to avoid poor quality minima and reduce the prediction variance. As a result, HerstonNet improves the existing approach by 24.30% in terms of intraclass correlation coefficient (agreement measure) to FreeSurfer silver-standards while maintaining a competitive run-time.



### Stochastic-YOLO: Efficient Probabilistic Object Detection under Dataset Shifts
- **Arxiv ID**: http://arxiv.org/abs/2009.02967v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2009.02967v2)
- **Published**: 2020-09-07 09:28:17+00:00
- **Updated**: 2020-11-06 12:57:02+00:00
- **Authors**: Tiago Azevedo, René de Jong, Matthew Mattina, Partha Maji
- **Comment**: To appear in the Workshop on Machine Learning for Autonomous Driving
  (ML4AD) at NeurIPS 2020. 9 pages, 7 figures
- **Journal**: None
- **Summary**: In image classification tasks, the evaluation of models' robustness to increased dataset shifts with a probabilistic framework is very well studied. However, object detection (OD) tasks pose other challenges for uncertainty estimation and evaluation. For example, one needs to evaluate both the quality of the label uncertainty (i.e., what?) and spatial uncertainty (i.e., where?) for a given bounding box, but that evaluation cannot be performed with more traditional average precision metrics (e.g., mAP). In this paper, we adapt the well-established YOLOv3 architecture to generate uncertainty estimations by introducing stochasticity in the form of Monte Carlo Dropout (MC-Drop), and evaluate it across different levels of dataset shift. We call this novel architecture Stochastic-YOLO, and provide an efficient implementation to effectively reduce the burden of the MC-Drop sampling mechanism at inference time. Finally, we provide some sensitivity analyses, while arguing that Stochastic-YOLO is a sound approach that improves different components of uncertainty estimations, in particular spatial uncertainties.



### Light Field View Synthesis via Aperture Disparity and Warping Confidence Map
- **Arxiv ID**: http://arxiv.org/abs/2009.02978v2
- **DOI**: 10.1109/TIP.2021.3066293
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.02978v2)
- **Published**: 2020-09-07 09:46:01+00:00
- **Updated**: 2021-04-03 05:00:06+00:00
- **Authors**: Nan Meng, Kai Li, Jianzhuang Liu, Edmund Y. Lam
- **Comment**: 14 pages, 14 figures
- **Journal**: IEEE Transactions on Image Processing 30 (2021): 3908-3921
- **Summary**: This paper presents a learning-based approach to synthesize the view from an arbitrary camera position given a sparse set of images. A key challenge for this novel view synthesis arises from the reconstruction process, when the views from different input images may not be consistent due to obstruction in the light path. We overcome this by jointly modeling the epipolar property and occlusion in designing a convolutional neural network. We start by defining and computing the aperture disparity map, which approximates the parallax and measures the pixel-wise shift between two views. While this relates to free-space rendering and can fail near the object boundaries, we further develop a warping confidence map to address pixel occlusion in these challenging regions. The proposed method is evaluated on diverse real-world and synthetic light field scenes, and it shows better performance over several state-of-the-art techniques.



### Towards learned optimal q-space sampling in diffusion MRI
- **Arxiv ID**: http://arxiv.org/abs/2009.03008v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.03008v1)
- **Published**: 2020-09-07 10:46:12+00:00
- **Updated**: 2020-09-07 10:46:12+00:00
- **Authors**: Tomer Weiss, Sanketh Vedula, Ortal Senouf, Oleg Michailovich, AlexBronstein
- **Comment**: CDMRI 2020
- **Journal**: None
- **Summary**: Fiber tractography is an important tool of computational neuroscience that enables reconstructing the spatial connectivity and organization of white matter of the brain. Fiber tractography takes advantage of diffusion Magnetic Resonance Imaging (dMRI) which allows measuring the apparent diffusivity of cerebral water along different spatial directions. Unfortunately, collecting such data comes at the price of reduced spatial resolution and substantially elevated acquisition times, which limits the clinical applicability of dMRI. This problem has been thus far addressed using two principal strategies. Most of the efforts have been extended towards improving the quality of signal estimation for any, yet fixed sampling scheme (defined through the choice of diffusion-encoding gradients). On the other hand, optimization over the sampling scheme has also proven to be effective. Inspired by the previous results, the present work consolidates the above strategies into a unified estimation framework, in which the optimization is carried out with respect to both estimation model and sampling design {\it concurrently}. The proposed solution offers substantial improvements in the quality of signal estimation as well as the accuracy of ensuing analysis by means of fiber tractography. While proving the optimality of the learned estimation models would probably need more extensive evaluation, we nevertheless claim that the learned sampling schemes can be of immediate use, offering a way to improve the dMRI analysis without the necessity of deploying the neural network used for their estimation. We present a comprehensive comparative analysis based on the Human Connectome Project data. Code and learned sampling designs aviliable at https://github.com/tomer196/Learned_dMRI.



### Real-Time Segmentation of Non-Rigid Surgical Tools based on Deep Learning and Tracking
- **Arxiv ID**: http://arxiv.org/abs/2009.03016v1
- **DOI**: 10.1007/978-3-319-54057-3_8
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2009.03016v1)
- **Published**: 2020-09-07 11:06:14+00:00
- **Updated**: 2020-09-07 11:06:14+00:00
- **Authors**: Luis C. García-Peraza-Herrera, Wenqi Li, Caspar Gruijthuijsen, Alain Devreker, George Attilakos, Jan Deprest, Emmanuel Vander Poorten, Danail Stoyanov, Tom Vercauteren, Sébastien Ourselin
- **Comment**: Accepted in CARE Workshop, held in conjunction with MICCAI 2016
- **Journal**: None
- **Summary**: Real-time tool segmentation is an essential component in computer-assisted surgical systems. We propose a novel real-time automatic method based on Fully Convolutional Networks (FCN) and optical flow tracking. Our method exploits the ability of deep neural networks to produce accurate segmentations of highly deformable parts along with the high speed of optical flow. Furthermore, the pre-trained FCN can be fine-tuned on a small amount of medical images without the need to hand-craft features. We validated our method using existing and new benchmark datasets, covering both ex vivo and in vivo real clinical cases where different surgical instruments are employed. Two versions of the method are presented, non-real-time and real-time. The former, using only deep learning, achieves a balanced accuracy of 89.6% on a real clinical dataset, outperforming the (non-real-time) state of the art by 3.8% points. The latter, a combination of deep learning with optical flow tracking, yields an average balanced accuracy of 78.2% across all the validated datasets.



### Deep Cyclic Generative Adversarial Residual Convolutional Networks for Real Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2009.03693v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.03693v1)
- **Published**: 2020-09-07 11:11:18+00:00
- **Updated**: 2020-09-07 11:11:18+00:00
- **Authors**: Rao Muhammad Umer, Christian Micheloni
- **Comment**: In proceedings of European Conference on Computer Vision (ECCV)
  Workshops. arXiv admin note: substantial text overlap with arXiv:2005.00953
- **Journal**: None
- **Summary**: Recent deep learning based single image super-resolution (SISR) methods mostly train their models in a clean data domain where the low-resolution (LR) and the high-resolution (HR) images come from noise-free settings (same domain) due to the bicubic down-sampling assumption. However, such degradation process is not available in real-world settings. We consider a deep cyclic network structure to maintain the domain consistency between the LR and HR data distributions, which is inspired by the recent success of CycleGAN in the image-to-image translation applications. We propose the Super-Resolution Residual Cyclic Generative Adversarial Network (SRResCycGAN) by training with a generative adversarial network (GAN) framework for the LR to HR domain translation in an end-to-end manner. We demonstrate our proposed approach in the quantitative and qualitative experiments that generalize well to the real image super-resolution and it is easy to deploy for the mobile/embedded devices. In addition, our SR results on the AIM 2020 Real Image SR Challenge datasets demonstrate that the proposed SR approach achieves comparable results as the other state-of-art methods.



### Localization and classification of intracranialhemorrhages in CT data
- **Arxiv ID**: http://arxiv.org/abs/2009.03046v1
- **DOI**: None
- **Categories**: **physics.med-ph**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2009.03046v1)
- **Published**: 2020-09-07 12:07:27+00:00
- **Updated**: 2020-09-07 12:07:27+00:00
- **Authors**: Jakub Nemcek, Roman Jakubicek, Jiri Chmelik
- **Comment**: Submitted on EMBEC 2020, paper has not been reviewed yet, 7 pages
- **Journal**: None
- **Summary**: Intracranial hemorrhages (ICHs) are life-threatening brain injures with a relatively high incidence. In this paper, the automatic algorithm for the detection and classification of ICHs, including localization, is present. The set of binary convolutional neural network-based classifiers with a designed cascade-parallel architecture is used. This automatic system may lead to a distinct decrease in the diagnostic process's duration in acute cases. An average Jaccard coefficient of 53.7 % is achieved on the data from the publicly available head CT dataset CQ500.



### A Light-Weight Object Detection Framework with FPA Module for Optical Remote Sensing Imagery
- **Arxiv ID**: http://arxiv.org/abs/2009.03063v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.03063v1)
- **Published**: 2020-09-07 12:41:17+00:00
- **Updated**: 2020-09-07 12:41:17+00:00
- **Authors**: Xi Gu, Lingbin Kong, Zhicheng Wang, Jie Li, Zhaohui Yu, Gang Wei
- **Comment**: None
- **Journal**: None
- **Summary**: With the development of remote sensing technology, the acquisition of remote sensing images is easier and easier, which provides sufficient data resources for the task of detecting remote sensing objects. However, how to detect objects quickly and accurately from many complex optical remote sensing images is a challenging hot issue. In this paper, we propose an efficient anchor free object detector, CenterFPANet. To pursue speed, we use a lightweight backbone and introduce the asymmetric revolution block. To improve the accuracy, we designed the FPA module, which links the feature maps of different levels, and introduces the attention mechanism to dynamically adjust the weights of each level of feature maps, which solves the problem of detection difficulty caused by large size range of remote sensing objects. This strategy can improve the accuracy of remote sensing image object detection without reducing the detection speed. On the DOTA dataset, CenterFPANet mAP is 64.00%, and FPS is 22.2, which is close to the accuracy of the anchor-based methods currently used and much faster than them. Compared with Faster RCNN, mAP is 6.76% lower but 60.87% faster. All in all, CenterFPANet achieves a balance between speed and accuracy in large-scale optical remote sensing object detection.



### Deep Iterative Residual Convolutional Network for Single Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2009.04809v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.04809v1)
- **Published**: 2020-09-07 12:54:14+00:00
- **Updated**: 2020-09-07 12:54:14+00:00
- **Authors**: Rao Muhammad Umer, Gian Luca Foresti, Christian Micheloni
- **Comment**: To be appeared in proceedings of the 25th IEEE International
  Conference on Pattern Recognition (ICPR). arXiv admin note: text overlap with
  arXiv:2005.00953, arXiv:2009.03693
- **Journal**: None
- **Summary**: Deep convolutional neural networks (CNNs) have recently achieved great success for single image super-resolution (SISR) task due to their powerful feature representation capabilities. The most recent deep learning based SISR methods focus on designing deeper / wider models to learn the non-linear mapping between low-resolution (LR) inputs and high-resolution (HR) outputs. These existing SR methods do not take into account the image observation (physical) model and thus require a large number of network's trainable parameters with a great volume of training data. To address these issues, we propose a deep Iterative Super-Resolution Residual Convolutional Network (ISRResCNet) that exploits the powerful image regularization and large-scale optimization techniques by training the deep network in an iterative manner with a residual learning approach. Extensive experimental results on various super-resolution benchmarks demonstrate that our method with a few trainable parameters improves the results for different scaling factors in comparison with the state-of-art methods.



### Uncertainty Inspired RGB-D Saliency Detection
- **Arxiv ID**: http://arxiv.org/abs/2009.03075v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.03075v1)
- **Published**: 2020-09-07 13:01:45+00:00
- **Updated**: 2020-09-07 13:01:45+00:00
- **Authors**: Jing Zhang, Deng-Ping Fan, Yuchao Dai, Saeed Anwar, Fatemeh Saleh, Sadegh Aliakbarian, Nick Barnes
- **Comment**: None
- **Journal**: None
- **Summary**: We propose the first stochastic framework to employ uncertainty for RGB-D saliency detection by learning from the data labeling process. Existing RGB-D saliency detection models treat this task as a point estimation problem by predicting a single saliency map following a deterministic learning pipeline. We argue that, however, the deterministic solution is relatively ill-posed. Inspired by the saliency data labeling process, we propose a generative architecture to achieve probabilistic RGB-D saliency detection which utilizes a latent variable to model the labeling variations. Our framework includes two main models: 1) a generator model, which maps the input image and latent variable to stochastic saliency prediction, and 2) an inference model, which gradually updates the latent variable by sampling it from the true or approximate posterior distribution. The generator model is an encoder-decoder saliency network. To infer the latent variable, we introduce two different solutions: i) a Conditional Variational Auto-encoder with an extra encoder to approximate the posterior distribution of the latent variable; and ii) an Alternating Back-Propagation technique, which directly samples the latent variable from the true posterior distribution. Qualitative and quantitative results on six challenging RGB-D benchmark datasets show our approach's superior performance in learning the distribution of saliency maps. The source code is publicly available via our project page: https://github.com/JingZhang617/UCNet.



### Progressive Bilateral-Context Driven Model for Post-Processing Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2009.03098v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2009.03098v1)
- **Published**: 2020-09-07 13:35:09+00:00
- **Updated**: 2020-09-07 13:35:09+00:00
- **Authors**: Min Cao, Chen Chen, Hao Dou, Xiyuan Hu, Silong Peng, Arjan Kuijper
- **Comment**: None
- **Journal**: Transactions on Multimedia 2020
- **Summary**: Most existing person re-identification methods compute pairwise similarity by extracting robust visual features and learning the discriminative metric. Owing to visual ambiguities, these content-based methods that determine the pairwise relationship only based on the similarity between them, inevitably produce a suboptimal ranking list. Instead, the pairwise similarity can be estimated more accurately along the geodesic path of the underlying data manifold by exploring the rich contextual information of the sample. In this paper, we propose a lightweight post-processing person re-identification method in which the pairwise measure is determined by the relationship between the sample and the counterpart's context in an unsupervised way. We translate the point-to-point comparison into the bilateral point-to-set comparison. The sample's context is composed of its neighbor samples with two different definition ways: the first order context and the second order context, which are used to compute the pairwise similarity in sequence, resulting in a progressive post-processing model. The experiments on four large-scale person re-identification benchmark datasets indicate that (1) the proposed method can consistently achieve higher accuracies by serving as a post-processing procedure after the content-based person re-identification methods, showing its state-of-the-art results, (2) the proposed lightweight method only needs about 6 milliseconds for optimizing the ranking results of one sample, showing its high-efficiency. Code is available at: https://github.com/123ci/PBCmodel.



### Stem-leaf segmentation and phenotypic trait extraction of maize shoots from three-dimensional point cloud
- **Arxiv ID**: http://arxiv.org/abs/2009.03108v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.03108v1)
- **Published**: 2020-09-07 13:58:09+00:00
- **Updated**: 2020-09-07 13:58:09+00:00
- **Authors**: Chao Zhu, Teng Miao, Tongyu Xu, Tao Yang, Na Li
- **Comment**: None
- **Journal**: None
- **Summary**: Nowadays, there are many approaches to acquire three-dimensional (3D) point clouds of maize plants. However, automatic stem-leaf segmentation of maize shoots from three-dimensional (3D) point clouds remains challenging, especially for new emerging leaves that are very close and wrapped together during the seedling stage. To address this issue, we propose an automatic segmentation method consisting of three main steps: skeleton extraction, coarse segmentation based on the skeleton, fine segmentation based on stem-leaf classification. The segmentation method was tested on 30 maize seedlings and compared with manually obtained ground truth. The mean precision, mean recall, mean micro F1 score and mean over accuracy of our segmentation algorithm were 0.964, 0.966, 0.963 and 0.969. Using the segmentation results, two applications were also developed in this paper, including phenotypic trait extraction and skeleton optimization. Six phenotypic parameters can be accurately and automatically measured, including plant height, crown diameter, stem height and diameter, leaf width and length. Furthermore, the values of R2 for the six phenotypic traits were all above 0.94. The results indicated that the proposed algorithm could automatically and precisely segment not only the fully expanded leaves, but also the new leaves wrapped together and close together. The proposed approach may play an important role in further maize research and applications, such as genotype-to-phenotype study, geometric reconstruction and dynamic growth animation. We released the source code and test data at the web site https://github.com/syau-miao/seg4maize.git



### Interpretable Deep Multimodal Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2009.03118v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.03118v1)
- **Published**: 2020-09-07 14:08:35+00:00
- **Updated**: 2020-09-07 14:08:35+00:00
- **Authors**: Iman Marivani, Evaggelia Tsiligianni, Bruno Cornelis, Nikos Deligiannis
- **Comment**: in Proceedings of iTWIST'20, Paper-ID: 41, Nantes, France, December,
  2-4, 2020
- **Journal**: None
- **Summary**: Multimodal image super-resolution (SR) is the reconstruction of a high resolution image given a low-resolution observation with the aid of another image modality. While existing deep multimodal models do not incorporate domain knowledge about image SR, we present a multimodal deep network design that integrates coupled sparse priors and allows the effective fusion of information from another modality into the reconstruction process. Our method is inspired by a novel iterative algorithm for coupled convolutional sparse coding, resulting in an interpretable network by design. We apply our model to the super-resolution of near-infrared image guided by RGB images. Experimental results show that our model outperforms state-of-the-art methods.



### Towards Semantic Segmentation of Urban-Scale 3D Point Clouds: A Dataset, Benchmarks and Challenges
- **Arxiv ID**: http://arxiv.org/abs/2009.03137v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2009.03137v3)
- **Published**: 2020-09-07 14:47:07+00:00
- **Updated**: 2021-04-06 04:19:34+00:00
- **Authors**: Qingyong Hu, Bo Yang, Sheikh Khalid, Wen Xiao, Niki Trigoni, Andrew Markham
- **Comment**: CVPR 2021, Code: https://github.com/QingyongHu/SensatUrban
- **Journal**: None
- **Summary**: An essential prerequisite for unleashing the potential of supervised deep learning algorithms in the area of 3D scene understanding is the availability of large-scale and richly annotated datasets. However, publicly available datasets are either in relative small spatial scales or have limited semantic annotations due to the expensive cost of data acquisition and data annotation, which severely limits the development of fine-grained semantic understanding in the context of 3D point clouds. In this paper, we present an urban-scale photogrammetric point cloud dataset with nearly three billion richly annotated points, which is three times the number of labeled points than the existing largest photogrammetric point cloud dataset. Our dataset consists of large areas from three UK cities, covering about 7.6 km^2 of the city landscape. In the dataset, each 3D point is labeled as one of 13 semantic classes. We extensively evaluate the performance of state-of-the-art algorithms on our dataset and provide a comprehensive analysis of the results. In particular, we identify several key challenges towards urban-scale point cloud understanding. The dataset is available at https://github.com/QingyongHu/SensatUrban.



### Deepfake detection: humans vs. machines
- **Arxiv ID**: http://arxiv.org/abs/2009.03155v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2009.03155v1)
- **Published**: 2020-09-07 15:20:37+00:00
- **Updated**: 2020-09-07 15:20:37+00:00
- **Authors**: Pavel Korshunov, Sébastien Marcel
- **Comment**: None
- **Journal**: None
- **Summary**: Deepfake videos, where a person's face is automatically swapped with a face of someone else, are becoming easier to generate with more realistic results. In response to the threat such manipulations can pose to our trust in video evidence, several large datasets of deepfake videos and many methods to detect them were proposed recently. However, it is still unclear how realistic deepfake videos are for an average person and whether the algorithms are significantly better than humans at detecting them. In this paper, we present a subjective study conducted in a crowdsourcing-like scenario, which systematically evaluates how hard it is for humans to see if the video is deepfake or not. For the evaluation, we used 120 different videos (60 deepfakes and 60 originals) manually pre-selected from the Facebook deepfake database, which was provided in the Kaggle's Deepfake Detection Challenge 2020. For each video, a simple question: "Is face of the person in the video real of fake?" was answered on average by 19 na\"ive subjects. The results of the subjective evaluation were compared with the performance of two different state of the art deepfake detection methods, based on Xception and EfficientNets (B4 variant) neural networks, which were pre-trained on two other large public databases: the Google's subset from FaceForensics++ and the recent Celeb-DF dataset. The evaluation demonstrates that while the human perception is very different from the perception of a machine, both successfully but in different ways are fooled by deepfakes. Specifically, algorithms struggle to detect those deepfake videos, which human subjects found to be very easy to spot.



### Improving colonoscopy lesion classification using semi-supervised deep learning
- **Arxiv ID**: http://arxiv.org/abs/2009.03162v1
- **DOI**: 10.1109/ACCESS.2020.3047544
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.03162v1)
- **Published**: 2020-09-07 15:25:35+00:00
- **Updated**: 2020-09-07 15:25:35+00:00
- **Authors**: Mayank Golhar, Taylor L. Bobrow, MirMilad Pourmousavi Khoshknab, Simran Jit, Saowanee Ngamruengphong, Nicholas J. Durr
- **Comment**: 10 pages, 5 figures
- **Journal**: IEEE Access, vol. 9, pp. 631-640, 2021
- **Summary**: While data-driven approaches excel at many image analysis tasks, the performance of these approaches is often limited by a shortage of annotated data available for training. Recent work in semi-supervised learning has shown that meaningful representations of images can be obtained from training with large quantities of unlabeled data, and that these representations can improve the performance of supervised tasks. Here, we demonstrate that an unsupervised jigsaw learning task, in combination with supervised training, results in up to a 9.8% improvement in correctly classifying lesions in colonoscopy images when compared to a fully-supervised baseline. We additionally benchmark improvements in domain adaptation and out-of-distribution detection, and demonstrate that semi-supervised learning outperforms supervised learning in both cases. In colonoscopy applications, these metrics are important given the skill required for endoscopic assessment of lesions, the wide variety of endoscopy systems in use, and the homogeneity that is typical of labeled datasets.



### Are Deep Neural Architectures Losing Information? Invertibility Is Indispensable
- **Arxiv ID**: http://arxiv.org/abs/2009.03173v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.IT, cs.LG, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/2009.03173v2)
- **Published**: 2020-09-07 15:39:24+00:00
- **Updated**: 2020-09-29 00:15:58+00:00
- **Authors**: Yang Liu, Zhenyue Qin, Saeed Anwar, Sabrina Caldwell, Tom Gedeon
- **Comment**: ICONIP 2020
- **Journal**: None
- **Summary**: Ever since the advent of AlexNet, designing novel deep neural architectures for different tasks has consistently been a productive research direction. Despite the exceptional performance of various architectures in practice, we study a theoretical question: what is the condition for deep neural architectures to preserve all the information of the input data? Identifying the information lossless condition for deep neural architectures is important, because tasks such as image restoration require keep the detailed information of the input data as much as possible. Using the definition of mutual information, we show that: a deep neural architecture can preserve maximum details about the given data if and only if the architecture is invertible. We verify the advantages of our Invertible Restoring Autoencoder (IRAE) network by comparing it with competitive models on three perturbed image restoration tasks: image denoising, jpeg image decompression and image inpainting. Experimental results show that IRAE consistently outperforms non-invertible ones. Our model even contains far fewer parameters. Thus, it may be worthwhile to try replacing standard components of deep neural architectures, such as residual blocks and ReLU, with their invertible counterparts. We believe our work provides a unique perspective and direction for future deep learning research.



### VisCode: Embedding Information in Visualization Images using Encoder-Decoder Network
- **Arxiv ID**: http://arxiv.org/abs/2009.03817v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, 68U99
- **Links**: [PDF](http://arxiv.org/pdf/2009.03817v1)
- **Published**: 2020-09-07 15:48:48+00:00
- **Updated**: 2020-09-07 15:48:48+00:00
- **Authors**: Peiying Zhang, Chenhui Li, Changbo Wang
- **Comment**: 11 pages, 16 figures, conference
- **Journal**: None
- **Summary**: We present an approach called VisCode for embedding information into visualization images. This technology can implicitly embed data information specified by the user into a visualization while ensuring that the encoded visualization image is not distorted. The VisCode framework is based on a deep neural network. We propose to use visualization images and QR codes data as training data and design a robust deep encoder-decoder network. The designed model considers the salient features of visualization images to reduce the explicit visual loss caused by encoding. To further support large-scale encoding and decoding, we consider the characteristics of information visualization and propose a saliency-based QR code layout algorithm. We present a variety of practical applications of VisCode in the context of information visualization and conduct a comprehensive evaluation of the perceptual quality of encoding, decoding success rate, anti-attack capability, time performance, etc. The evaluation results demonstrate the effectiveness of VisCode.



### Attention based Writer Independent Handwriting Verification
- **Arxiv ID**: http://arxiv.org/abs/2009.04532v3
- **DOI**: 10.1109/ICFHR2020.2020.00074
- **Categories**: **cs.CV**, cs.AI, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2009.04532v3)
- **Published**: 2020-09-07 16:28:16+00:00
- **Updated**: 2020-10-01 00:59:36+00:00
- **Authors**: Mohammad Abuzar Shaikh, Tiehang Duan, Mihir Chauhan, Sargur Srihari
- **Comment**: 7 pages, 6 figures, Published in 2020 17th International Conference
  on Frontiers in Handwriting Recognition (ICFHR)
- **Journal**: None
- **Summary**: The task of writer verification is to provide a likelihood score for whether the queried and known handwritten image samples belong to the same writer or not. Such a task calls for the neural network to make it's outcome interpretable, i.e. provide a view into the network's decision making process. We implement and integrate cross-attention and soft-attention mechanisms to capture the highly correlated and salient points in feature space of 2D inputs. The attention maps serve as an explanation premise for the network's output likelihood score. The attention mechanism also allows the network to focus more on relevant areas of the input, thus improving the classification performance. Our proposed approach achieves a precision of 86\% for detecting intra-writer cases in CEDAR cursive "AND" dataset. Furthermore, we generate meaningful explanations for the provided decision by extracting attention maps from multiple levels of the network.



### A Review on Near Duplicate Detection of Images using Computer Vision Techniques
- **Arxiv ID**: http://arxiv.org/abs/2009.03224v1
- **DOI**: 10.1007/s11831-020-09400-w
- **Categories**: **cs.CV**, I.4.7; I.4.9; I.4.10; I.5.2; I.5.3; I.5.4; I.2.4; I.2.6; I.2.10;
  H.3.1; H.3.3
- **Links**: [PDF](http://arxiv.org/pdf/2009.03224v1)
- **Published**: 2020-09-07 16:41:46+00:00
- **Updated**: 2020-09-07 16:41:46+00:00
- **Authors**: K. K. Thyagharajan, G. Kalaiarasi
- **Comment**: 37 Pages, 7 figures, "For online first version, see
  https://link.springer.com/article/10.1007/s11831-020-09400-w"
- **Journal**: None
- **Summary**: Nowadays, digital content is widespread and simply redistributable, either lawfully or unlawfully. For example, after images are posted on the internet, other web users can modify them and then repost their versions, thereby generating near-duplicate images. The presence of near-duplicates affects the performance of the search engines critically. Computer vision is concerned with the automatic extraction, analysis and understanding of useful information from digital images. The main application of computer vision is image understanding. There are several tasks in image understanding such as feature extraction, object detection, object recognition, image cleaning, image transformation, etc. There is no proper survey in literature related to near duplicate detection of images. In this paper, we review the state-of-the-art computer vision-based approaches and feature extraction methods for the detection of near duplicate images. We also discuss the main challenges in this field and how other researchers addressed those challenges. This review provides research directions to the fellow researchers who are interested to work in this field.



### Integrating Egocentric Localization for More Realistic Point-Goal Navigation Agents
- **Arxiv ID**: http://arxiv.org/abs/2009.03231v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.03231v1)
- **Published**: 2020-09-07 16:52:47+00:00
- **Updated**: 2020-09-07 16:52:47+00:00
- **Authors**: Samyak Datta, Oleksandr Maksymets, Judy Hoffman, Stefan Lee, Dhruv Batra, Devi Parikh
- **Comment**: None
- **Journal**: None
- **Summary**: Recent work has presented embodied agents that can navigate to point-goal targets in novel indoor environments with near-perfect accuracy. However, these agents are equipped with idealized sensors for localization and take deterministic actions. This setting is practically sterile by comparison to the dirty reality of noisy sensors and actuations in the real world -- wheels can slip, motion sensors have error, actuations can rebound. In this work, we take a step towards this noisy reality, developing point-goal navigation agents that rely on visual estimates of egomotion under noisy action dynamics. We find these agents outperform naive adaptions of current point-goal agents to this setting as well as those incorporating classic localization baselines. Further, our model conceptually divides learning agent dynamics or odometry (where am I?) from task-specific navigation policy (where do I want to go?). This enables a seamless adaption to changing dynamics (a different robot or floor type) by simply re-calibrating the visual odometry model -- circumventing the expense of re-training of the navigation policy. Our agent was the runner-up in the PointNav track of CVPR 2020 Habitat Challenge.



### Implicit Multidimensional Projection of Local Subspaces
- **Arxiv ID**: http://arxiv.org/abs/2009.03259v2
- **DOI**: 10.1109/TVCG.2020.3030368
- **Categories**: **cs.LG**, cs.CV, cs.GR, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2009.03259v2)
- **Published**: 2020-09-07 17:27:27+00:00
- **Updated**: 2023-07-20 12:11:56+00:00
- **Authors**: Rongzheng Bian, Yumeng Xue, Liang Zhou, Jian Zhang, Baoquan Chen, Daniel Weiskopf, Yunhai Wang
- **Comment**: None
- **Journal**: in IEEE Transactions on Visualization and Computer Graphics, vol.
  27, no. 2, pp. 1558-1568, Feb. 2021
- **Summary**: We propose a visualization method to understand the effect of multidimensional projection on local subspaces, using implicit function differentiation. Here, we understand the local subspace as the multidimensional local neighborhood of data points. Existing methods focus on the projection of multidimensional data points, and the neighborhood information is ignored. Our method is able to analyze the shape and directional information of the local subspace to gain more insights into the global structure of the data through the perception of local structures. Local subspaces are fitted by multidimensional ellipses that are spanned by basis vectors. An accurate and efficient vector transformation method is proposed based on analytical differentiation of multidimensional projections formulated as implicit functions. The results are visualized as glyphs and analyzed using a full set of specifically-designed interactions supported in our efficient web-based visualization tool. The usefulness of our method is demonstrated using various multi- and high-dimensional benchmark datasets. Our implicit differentiation vector transformation is evaluated through numerical comparisons; the overall method is evaluated through exploration examples and use cases.



### User-assisted Video Reflection Removal
- **Arxiv ID**: http://arxiv.org/abs/2009.03281v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2009.03281v1)
- **Published**: 2020-09-07 17:42:40+00:00
- **Updated**: 2020-09-07 17:42:40+00:00
- **Authors**: Amgad Ahmed, Suhong Kim, Mohamed Elgharib, Mohamed Hefeeda
- **Comment**: None
- **Journal**: None
- **Summary**: Reflections in videos are obstructions that often occur when videos are taken behind reflective surfaces like glass. These reflections reduce the quality of such videos, lead to information loss and degrade the accuracy of many computer vision algorithms. A video containing reflections is a combination of background and reflection layers. Thus, reflection removal is equivalent to decomposing the video into two layers. This, however, is a challenging and ill-posed problem as there is an infinite number of valid decompositions. To address this problem, we propose a user-assisted method for video reflection removal. We rely on both spatial and temporal information and utilize sparse user hints to help improve separation. The key idea of the proposed method is to use motion cues to separate the background layer from the reflection layer with minimal user assistance. We show that user-assistance significantly improves the layer separation results. We implement and evaluate the proposed method through quantitative and qualitative results on real and synthetic videos. Our experiments show that the proposed method successfully removes reflection from video sequences, does not introduce visual distortions, and significantly outperforms the state-of-the-art reflection removal methods in the literature.



### A novel action recognition system for smart monitoring of elderly people using Action Pattern Image and Series CNN with transfer learning
- **Arxiv ID**: http://arxiv.org/abs/2009.03285v1
- **DOI**: None
- **Categories**: **cs.CV**, H.3.3; I.2.1; I.2.6; I.2.10; I.4.3; I.4.6; I.4.7; I.4.8; I.4.9;
  I.5.1; I.5.2; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/2009.03285v1)
- **Published**: 2020-09-07 17:51:27+00:00
- **Updated**: 2020-09-07 17:51:27+00:00
- **Authors**: L. Aneesh Euprazia, K. K. Thyagharajan
- **Comment**: 30 pages, 19 figures, "Submitted to Pattern Recognition Journal,
  Elsevier"
- **Journal**: None
- **Summary**: Falling of elderly people who are staying alone at home leads to health risks. If they are not attended immediately even it may lead to fatal danger to their life. In this paper a novel computer vision-based system for smart monitoring of elderly people using Series Convolutional Neural Network (SCNN) with transfer learning is proposed. When CNN is trained by the frames of the videos directly, it learns from all pixels including the background pixels. Generally, the background in a video does not contribute anything in identifying the action and actually it will mislead the action classification. So, we propose a novel action recognition system and our contributions are 1) to generate more general action patterns which are not affected by illumination and background variations of the video sequences and eliminate the obligation of image augmentation in CNN training 2) to design SCNN architecture and enhance the feature extraction process to learn large amount of data, 3) to present the patterns learnt by the neurons in the layers and analyze how these neurons capture the action when the input pattern is passing through these neurons, and 4) to extend the capability of the trained SCNN for recognizing fall actions using transfer learning.



### Improved Modeling of 3D Shapes with Multi-view Depth Maps
- **Arxiv ID**: http://arxiv.org/abs/2009.03298v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.03298v1)
- **Published**: 2020-09-07 17:58:27+00:00
- **Updated**: 2020-09-07 17:58:27+00:00
- **Authors**: Kamal Gupta, Susmija Jabbireddy, Ketul Shah, Abhinav Shrivastava, Matthias Zwicker
- **Comment**: None
- **Journal**: None
- **Summary**: We present a simple yet effective general-purpose framework for modeling 3D shapes by leveraging recent advances in 2D image generation using CNNs. Using just a single depth image of the object, we can output a dense multi-view depth map representation of 3D objects. Our simple encoder-decoder framework, comprised of a novel identity encoder and class-conditional viewpoint generator, generates 3D consistent depth maps. Our experimental results demonstrate the two-fold advantage of our approach. First, we can directly borrow architectures that work well in the 2D image domain to 3D. Second, we can effectively generate high-resolution 3D shapes with low computational memory. Our quantitative evaluations show that our method is superior to existing depth map methods for reconstructing and synthesizing 3D objects and is competitive with other representations, such as point clouds, voxel grids, and implicit functions.



### Adversarial attacks on deep learning models for fatty liver disease classification by modification of ultrasound image reconstruction method
- **Arxiv ID**: http://arxiv.org/abs/2009.03364v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2009.03364v1)
- **Published**: 2020-09-07 18:35:35+00:00
- **Updated**: 2020-09-07 18:35:35+00:00
- **Authors**: Michal Byra, Grzegorz Styczynski, Cezary Szmigielski, Piotr Kalinowski, Lukasz Michalowski, Rafal Paluszkiewicz, Bogna Ziarkiewicz-Wroblewska, Krzysztof Zieniewicz, Andrzej Nowicki
- **Comment**: 4 pages, 3 figures
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) have achieved remarkable success in medical image analysis tasks. In ultrasound (US) imaging, CNNs have been applied to object classification, image reconstruction and tissue characterization. However, CNNs can be vulnerable to adversarial attacks, even small perturbations applied to input data may significantly affect model performance and result in wrong output. In this work, we devise a novel adversarial attack, specific to ultrasound (US) imaging. US images are reconstructed based on radio-frequency signals. Since the appearance of US images depends on the applied image reconstruction method, we explore the possibility of fooling deep learning model by perturbing US B-mode image reconstruction method. We apply zeroth order optimization to find small perturbations of image reconstruction parameters, related to attenuation compensation and amplitude compression, which can result in wrong output. We illustrate our approach using a deep learning model developed for fatty liver disease diagnosis, where the proposed adversarial attack achieved success rate of 48%.



### ePointDA: An End-to-End Simulation-to-Real Domain Adaptation Framework for LiDAR Point Cloud Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2009.03456v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2009.03456v2)
- **Published**: 2020-09-07 23:46:08+00:00
- **Updated**: 2021-02-23 16:52:41+00:00
- **Authors**: Sicheng Zhao, Yezhen Wang, Bo Li, Bichen Wu, Yang Gao, Pengfei Xu, Trevor Darrell, Kurt Keutzer
- **Comment**: Accepted by AAAI 2021
- **Journal**: None
- **Summary**: Due to its robust and precise distance measurements, LiDAR plays an important role in scene understanding for autonomous driving. Training deep neural networks (DNNs) on LiDAR data requires large-scale point-wise annotations, which are time-consuming and expensive to obtain. Instead, simulation-to-real domain adaptation (SRDA) trains a DNN using unlimited synthetic data with automatically generated labels and transfers the learned model to real scenarios. Existing SRDA methods for LiDAR point cloud segmentation mainly employ a multi-stage pipeline and focus on feature-level alignment. They require prior knowledge of real-world statistics and ignore the pixel-level dropout noise gap and the spatial feature gap between different domains. In this paper, we propose a novel end-to-end framework, named ePointDA, to address the above issues. Specifically, ePointDA consists of three modules: self-supervised dropout noise rendering, statistics-invariant and spatially-adaptive feature alignment, and transferable segmentation learning. The joint optimization enables ePointDA to bridge the domain shift at the pixel-level by explicitly rendering dropout noise for synthetic LiDAR and at the feature-level by spatially aligning the features between different domains, without requiring the real-world statistics. Extensive experiments adapting from synthetic GTA-LiDAR to real KITTI and SemanticKITTI demonstrate the superiority of ePointDA for LiDAR point cloud segmentation.



### Horus: Using Sensor Fusion to Combine Infrastructure and On-board Sensing to Improve Autonomous Vehicle Safety
- **Arxiv ID**: http://arxiv.org/abs/2009.03458v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.NI, I.2.9; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2009.03458v1)
- **Published**: 2020-09-07 23:52:57+00:00
- **Updated**: 2020-09-07 23:52:57+00:00
- **Authors**: Sanjay Seshan
- **Comment**: Presented at Intel ISEF 2019
- **Journal**: None
- **Summary**: Studies predict that demand for autonomous vehicles will increase tenfold between 2019 and 2026. However, recent high-profile accidents have significantly impacted consumer confidence in this technology. The cause for many of these accidents can be traced back to the inability of these vehicles to correctly sense the impending danger. In response, manufacturers have been improving the already extensive on-vehicle sensor packages to ensure that the system always has access to the data necessary to ensure safe navigation. However, these sensor packages only provide a view from the vehicle's perspective and, as a result, autonomous vehicles still require frequent human intervention to ensure safety.   To address this issue, I developed a system, called Horus, that combines on-vehicle and infrastructure-based sensors to provide a more complete view of the environment, including areas not visible from the vehicle. I built a small-scale experimental testbed as a proof of concept. My measurements of the impact of sensor failures showed that even short outages (1 second) at slow speeds (25 km/hr scaled velocity) prevents vehicles that rely on on-vehicle sensors from navigating properly. My experiments also showed that Horus dramatically improves driving safety and that the sensor fusion algorithm selected plays a significant role in the quality of the navigation. With just a pair of infrastructure sensors, Horus could tolerate sensors that fail 40% of the time and still navigate safely. These results are a promising first step towards safer autonomous vehicles.



