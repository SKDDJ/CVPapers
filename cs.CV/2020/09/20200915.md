# Arxiv Papers in cs.CV on 2020-09-15
### Optimality of short-term synaptic plasticity in modelling certain dynamic environments
- **Arxiv ID**: http://arxiv.org/abs/2009.06808v2
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, cs.LG, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/2009.06808v2)
- **Published**: 2020-09-15 01:04:28+00:00
- **Updated**: 2021-06-15 22:14:34+00:00
- **Authors**: Timoleon Moraitis, Abu Sebastian, Evangelos Eleftheriou
- **Comment**: Main paper: 12 pages, 4 figures. Supplementary Information: 13 pages,
  4 figures
- **Journal**: None
- **Summary**: Biological neurons and their in-silico emulations for neuromorphic artificial intelligence (AI) use extraordinarily energy-efficient mechanisms, such as spike-based communication and local synaptic plasticity. It remains unclear whether these neuronal mechanisms only offer efficiency or also underlie the superiority of biological intelligence. Here, we prove rigorously that, indeed, the Bayes-optimal prediction and inference of randomly but continuously transforming environments, a common natural setting, relies on short-term spike-timing-dependent plasticity, a hallmark of biological synapses. Further, this dynamic Bayesian inference through plasticity enables circuits of the cerebral cortex in simulations to recognize previously unseen, highly distorted dynamic stimuli. Strikingly, this also introduces a biologically-modelled AI, the first to overcome multiple limitations of deep learning and outperform artificial neural networks in a visual task. The cortical-like network is spiking and event-based, trained only with unsupervised and local plasticity, on a small, narrow, and static training dataset, but achieves recognition of unseen, transformed, and dynamic data better than deep neural networks with continuous activations, trained with supervised backpropagation on the transforming data. These results link short-term plasticity to high-level cortical function, suggest optimality of natural intelligence for natural environments, and repurpose neuromorphic AI from mere efficiency to computational supremacy altogether.



### Microscope Based HER2 Scoring System
- **Arxiv ID**: http://arxiv.org/abs/2009.06816v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.SY, eess.SY
- **Links**: [PDF](http://arxiv.org/pdf/2009.06816v1)
- **Published**: 2020-09-15 01:44:39+00:00
- **Updated**: 2020-09-15 01:44:39+00:00
- **Authors**: Jun Zhang, Kuan Tian, Pei Dong, Haocheng Shen, Kezhou Yan, Jianhua Yao, Junzhou Huang, Xiao Han
- **Comment**: 11 pages, 5 figures, and 4 tables
- **Journal**: None
- **Summary**: The overexpression of human epidermal growth factor receptor 2 (HER2) has been established as a therapeutic target in multiple types of cancers, such as breast and gastric cancers. Immunohistochemistry (IHC) is employed as a basic HER2 test to identify the HER2-positive, borderline, and HER2-negative patients. However, the reliability and accuracy of HER2 scoring are affected by many factors, such as pathologists' experience. Recently, artificial intelligence (AI) has been used in various disease diagnosis to improve diagnostic accuracy and reliability, but the interpretation of diagnosis results is still an open problem. In this paper, we propose a real-time HER2 scoring system, which follows the HER2 scoring guidelines to complete the diagnosis, and thus each step is explainable. Unlike the previous scoring systems based on whole-slide imaging, our HER2 scoring system is integrated into an augmented reality (AR) microscope that can feedback AI results to the pathologists while reading the slide. The pathologists can help select informative fields of view (FOVs), avoiding the confounding regions, such as DCIS. Importantly, we illustrate the intermediate results with membrane staining condition and cell classification results, making it possible to evaluate the reliability of the diagnostic results. Also, we support the interactive modification of selecting regions-of-interest, making our system more flexible in clinical practice. The collaboration of AI and pathologists can significantly improve the robustness of our system. We evaluate our system with 285 breast IHC HER2 slides, and the classification accuracy of 95\% shows the effectiveness of our HER2 scoring system.



### Fused Deep Convolutional Neural Network for Precision Diagnosis of COVID-19 Using Chest X-Ray Images
- **Arxiv ID**: http://arxiv.org/abs/2009.08831v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.08831v1)
- **Published**: 2020-09-15 02:27:20+00:00
- **Updated**: 2020-09-15 02:27:20+00:00
- **Authors**: Hussin K. Ragb, Ian T. Dover, Redha Ali
- **Comment**: 9 Pages, 14 figures
- **Journal**: None
- **Summary**: With a Coronavirus disease (COVID-19) case count exceeding 10 million worldwide, there is an increased need for a diagnostic capability. The main variables in increasing diagnostic capability are reduced cost, turnaround or diagnosis time, and upfront equipment cost and accessibility. Two candidates for machine learning COVID-19 diagnosis are Computed Tomography (CT) scans and plain chest X-rays. While CT scans score higher in sensitivity, they have a higher cost, maintenance requirement, and turnaround time as compared to plain chest X-rays. The use of portable chest X-radiograph (CXR) is recommended by the American College of Radiology (ACR) since using CT places a massive burden on radiology services. Therefore, X-ray imagery paired with machine learning techniques is proposed a first-line triage tool for COVID-19 diagnostics. In this paper we propose a computer-aided diagnosis (CAD) to accurately classify chest X-ray scans of COVID-19 and normal subjects by fine-tuning several neural networks (ResNet18, ResNet50, DenseNet201) pre-trained on the ImageNet dataset. These neural networks are fused in a parallel architecture and the voting criteria are applied in the final classification decision between the candidate object classes where the output of each neural network is representing a single vote. Several experiments are conducted on the weakly labeled COVID-19-CT-CXR dataset consisting of 263 COVID-19 CXR images extracted from PubMed Central Open Access subsets combined with 25 normal classification CXR images. These experiments show an optimistic result and a capability of the proposed model to outperforming many state-of-the-art algorithms on several measures. Using k-fold cross-validation and a bagging classifier ensemble, we achieve an accuracy of 99.7% and a sensitivity of 100%.



### Toward Deep Supervised Anomaly Detection: Reinforcement Learning from Partially Labeled Anomaly Data
- **Arxiv ID**: http://arxiv.org/abs/2009.06847v2
- **DOI**: 10.1145/3447548.3467417
- **Categories**: **cs.LG**, cs.AI, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2009.06847v2)
- **Published**: 2020-09-15 03:05:39+00:00
- **Updated**: 2021-06-10 13:40:11+00:00
- **Authors**: Guansong Pang, Anton van den Hengel, Chunhua Shen, Longbing Cao
- **Comment**: Accepted to KDD 2021
- **Journal**: None
- **Summary**: We consider the problem of anomaly detection with a small set of partially labeled anomaly examples and a large-scale unlabeled dataset. This is a common scenario in many important applications. Existing related methods either exclusively fit the limited anomaly examples that typically do not span the entire set of anomalies, or proceed with unsupervised learning from the unlabeled data. We propose here instead a deep reinforcement learning-based approach that enables an end-to-end optimization of the detection of both labeled and unlabeled anomalies. This approach learns the known abnormality by automatically interacting with an anomaly-biased simulation environment, while continuously extending the learned abnormality to novel classes of anomaly (i.e., unknown anomalies) by actively exploring possible anomalies in the unlabeled data. This is achieved by jointly optimizing the exploitation of the small labeled anomaly data and the exploration of the rare unlabeled anomalies. Extensive experiments on 48 real-world datasets show that our model significantly outperforms five state-of-the-art competing methods.



### Ensemble learning of diffractive optical networks
- **Arxiv ID**: http://arxiv.org/abs/2009.06869v1
- **DOI**: 10.1038/s41377-020-00446-w
- **Categories**: **cs.NE**, cs.CV, cs.LG, eess.IV, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/2009.06869v1)
- **Published**: 2020-09-15 05:02:50+00:00
- **Updated**: 2020-09-15 05:02:50+00:00
- **Authors**: Md Sadman Sakib Rahman, Jingxi Li, Deniz Mengu, Yair Rivenson, Aydogan Ozcan
- **Comment**: 22 Pages, 4 Figures, 1 Table
- **Journal**: Light: Science & Applications (2021)
- **Summary**: A plethora of research advances have emerged in the fields of optics and photonics that benefit from harnessing the power of machine learning. Specifically, there has been a revival of interest in optical computing hardware, due to its potential advantages for machine learning tasks in terms of parallelization, power efficiency and computation speed. Diffractive Deep Neural Networks (D2NNs) form such an optical computing framework, which benefits from deep learning-based design of successive diffractive layers to all-optically process information as the input light diffracts through these passive layers. D2NNs have demonstrated success in various tasks, including e.g., object classification, spectral-encoding of information, optical pulse shaping and imaging, among others. Here, we significantly improve the inference performance of diffractive optical networks using feature engineering and ensemble learning. After independently training a total of 1252 D2NNs that were diversely engineered with a variety of passive input filters, we applied a pruning algorithm to select an optimized ensemble of D2NNs that collectively improve their image classification accuracy. Through this pruning, we numerically demonstrated that ensembles of N=14 and N=30 D2NNs achieve blind testing accuracies of 61.14% and 62.13%, respectively, on the classification of CIFAR-10 test images, providing an inference improvement of >16% compared to the average performance of the individual D2NNs within each ensemble. These results constitute the highest inference accuracies achieved to date by any diffractive optical neural network design on the same dataset and might provide a significant leapfrog to extend the application space of diffractive optical image classification and machine vision systems.



### Attention-SLAM: A Visual Monocular SLAM Learning from Human Gaze
- **Arxiv ID**: http://arxiv.org/abs/2009.06886v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2009.06886v1)
- **Published**: 2020-09-15 06:59:12+00:00
- **Updated**: 2020-09-15 06:59:12+00:00
- **Authors**: Jinquan Li, Ling Pei, Danping Zou, Songpengcheng Xia, Qi Wu, Tao Li, Zhen Sun, Wenxian Yu
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a novel simultaneous localization and mapping (SLAM) approach, namely Attention-SLAM, which simulates human navigation mode by combining a visual saliency model (SalNavNet) with traditional monocular visual SLAM. Most SLAM methods treat all the features extracted from the images as equal importance during the optimization process. However, the salient feature points in scenes have more significant influence during the human navigation process. Therefore, we first propose a visual saliency model called SalVavNet in which we introduce a correlation module and propose an adaptive Exponential Moving Average (EMA) module. These modules mitigate the center bias to enable the saliency maps generated by SalNavNet to pay more attention to the same salient object. Moreover, the saliency maps simulate the human behavior for the refinement of SLAM results. The feature points extracted from the salient regions have greater importance in optimization process. We add semantic saliency information to the Euroc dataset to generate an open-source saliency SLAM dataset. Comprehensive test results prove that Attention-SLAM outperforms benchmarks such as Direct Sparse Odometry (DSO), ORB-SLAM, and Salient DSO in terms of efficiency, accuracy, and robustness in most test cases.



### 3DPVNet: Patch-level 3D Hough Voting Network for 6D Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2009.06887v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.06887v1)
- **Published**: 2020-09-15 06:59:57+00:00
- **Updated**: 2020-09-15 06:59:57+00:00
- **Authors**: Yuanpeng Liu, Jun Zhou, Yuqi Zhang, Chao Ding, Jun Wang
- **Comment**: 9 pages, 5 figures
- **Journal**: None
- **Summary**: In this paper, we focus on estimating the 6D pose of objects in point clouds. Although the topic has been widely studied, pose estimation in point clouds remains a challenging problem due to the noise and occlusion. To address the problem, a novel 3DPVNet is presented in this work, which utilizes 3D local patches to vote for the object 6D poses. 3DPVNet is comprised of three modules. In particular, a Patch Unification (\textbf{PU}) module is first introduced to normalize the input patch, and also create a standard local coordinate frame on it to generate a reliable vote. We then devise a Weight-guided Neighboring Feature Fusion (\textbf{WNFF}) module in the network, which fuses the neighboring features to yield a semi-global feature for the center patch. WNFF module mines the neighboring information of a local patch, such that the representation capability to local geometric characteristics is significantly enhanced, making the method robust to a certain level of noise. Moreover, we present a Patch-level Voting (\textbf{PV}) module to regress transformations and generates pose votes. After the aggregation of all votes from patches and a refinement step, the final pose of the object can be obtained. Compared to recent voting-based methods, 3DPVNet is patch-level, and directly carried out on point clouds. Therefore, 3DPVNet achieves less computation than point/pixel-level voting scheme, and has robustness to partial data. Experiments on several datasets demonstrate that 3DPVNet achieves the state-of-the-art performance, and is also robust against noise and occlusions.



### BSN++: Complementary Boundary Regressor with Scale-Balanced Relation Modeling for Temporal Action Proposal Generation
- **Arxiv ID**: http://arxiv.org/abs/2009.07641v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.07641v5)
- **Published**: 2020-09-15 07:08:59+00:00
- **Updated**: 2021-03-01 08:01:49+00:00
- **Authors**: Haisheng Su, Weihao Gan, Wei Wu, Yu Qiao, Junjie Yan
- **Comment**: Accepted by AAAI 2021. Ranked 1st place in the CVPR19 - ActivityNet
  Challenge leaderboard on Temporal Action Localization task. arXiv admin note:
  substantial text overlap with arXiv:2007.09883
- **Journal**: None
- **Summary**: Generating human action proposals in untrimmed videos is an important yet challenging task with wide applications. Current methods often suffer from the noisy boundary locations and the inferior quality of confidence scores used for proposal retrieving. In this paper, we present BSN++, a new framework which exploits complementary boundary regressor and relation modeling for temporal proposal generation. First, we propose a novel boundary regressor based on the complementary characteristics of both starting and ending boundary classifiers. Specifically, we utilize the U-shaped architecture with nested skip connections to capture rich contexts and introduce bi-directional boundary matching mechanism to improve boundary precision. Second, to account for the proposal-proposal relations ignored in previous methods, we devise a proposal relation block to which includes two self-attention modules from the aspects of position and channel. Furthermore, we find that there inevitably exists data imbalanced problems in the positive/negative proposals and temporal durations, which harm the model performance on tail distributions. To relieve this issue, we introduce the scale-balanced re-sampling strategy. Extensive experiments are conducted on two popular benchmarks: ActivityNet-1.3 and THUMOS14, which demonstrate that BSN++ achieves the state-of-the-art performance. Not surprisingly, the proposed BSN++ ranked 1st place in the CVPR19 - ActivityNet challenge leaderboard on temporal action localization task.



### Collaborative Distillation in the Parameter and Spectrum Domains for Video Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2009.06902v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.06902v1)
- **Published**: 2020-09-15 07:29:57+00:00
- **Updated**: 2020-09-15 07:29:57+00:00
- **Authors**: Haisheng Su, Jing Su, Dongliang Wang, Weihao Gan, Wei Wu, Mengmeng Wang, Junjie Yan, Yu Qiao
- **Comment**: Submmited to AAAI21
- **Journal**: None
- **Summary**: Recent years have witnessed the significant progress of action recognition task with deep networks. However, most of current video networks require large memory and computational resources, which hinders their applications in practice. Existing knowledge distillation methods are limited to the image-level spatial domain, ignoring the temporal and frequency information which provide structural knowledge and are important for video analysis. This paper explores how to train small and efficient networks for action recognition. Specifically, we propose two distillation strategies in the frequency domain, namely the feature spectrum and parameter distribution distillations respectively. Our insight is that appealing performance of action recognition requires \textit{explicitly} modeling the temporal frequency spectrum of video features. Therefore, we introduce a spectrum loss that enforces the student network to mimic the temporal frequency spectrum from the teacher network, instead of \textit{implicitly} distilling features as many previous works. Second, the parameter frequency distribution is further adopted to guide the student network to learn the appearance modeling process from the teacher. Besides, a collaborative learning strategy is presented to optimize the training process from a probabilistic view. Extensive experiments are conducted on several action recognition benchmarks, such as Kinetics, Something-Something, and Jester, which consistently verify effectiveness of our approach, and demonstrate that our method can achieve higher performance than state-of-the-art methods with the same backbone.



### A Robust and Reliable Point Cloud Recognition Network Under Rigid Transformation
- **Arxiv ID**: http://arxiv.org/abs/2009.06903v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.06903v2)
- **Published**: 2020-09-15 07:30:16+00:00
- **Updated**: 2021-12-29 03:10:33+00:00
- **Authors**: Dongrui Liu, Chuanchuan Chen, Changqing Xu, Qi Cai, Lei Chu, Fei Wen, Robert Caiming Qiu
- **Comment**: 13 pages, 11 figures
- **Journal**: None
- **Summary**: Point cloud recognition is an essential task in industrial robotics and autonomous driving. Recently, several point cloud processing models have achieved state-of-the-art performances. However, these methods lack rotation robustness, and their performances degrade severely under random rotations, failing to extend to real-world scenarios with varying orientations. To this end, we propose a method named Self Contour-based Transformation (SCT), which can be flexibly integrated into various existing point cloud recognition models against arbitrary rotations. SCT provides efficient rotation and translation invariance by introducing Contour-Aware Transformation (CAT), which linearly transforms Cartesian coordinates of points to translation and rotation-invariant representations. We prove that CAT is a rotation and translation-invariant transformation based on the theoretical analysis. Furthermore, the Frame Alignment module is proposed to enhance discriminative feature extraction by capturing contours and transforming self contour-based frames into intra-class frames. Extensive experimental results show that SCT outperforms the state-of-the-art approaches under arbitrary rotations in effectiveness and efficiency on synthetic and real-world benchmarks. Furthermore, the robustness and generality evaluations indicate that SCT is robust and is applicable to various point cloud processing models, which highlights the superiority of SCT in industrial applications.



### Multi-scale Attention U-Net (MsAUNet): A Modified U-Net Architecture for Scene Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2009.06911v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.06911v1)
- **Published**: 2020-09-15 08:03:41+00:00
- **Updated**: 2020-09-15 08:03:41+00:00
- **Authors**: Soham Chattopadhyay, Hritam Basak
- **Comment**: 12 Pages, 7 figures
- **Journal**: None
- **Summary**: Despite the growing success of Convolution neural networks (CNN) in the recent past in the task of scene segmentation, the standard models lack some of the important features that might result in sub-optimal segmentation outputs. The widely used encoder-decoder architecture extracts and uses several redundant and low-level features at different steps and different scales. Also, these networks fail to map the long-range dependencies of local features, which results in discriminative feature maps corresponding to each semantic class in the resulting segmented image. In this paper, we propose a novel multi-scale attention network for scene segmentation purposes by using the rich contextual information from an image. Different from the original UNet architecture we have used attention gates which take the features from the encoder and the output of the pyramid pool as input and produced out-put is further concatenated with the up-sampled output of the previous pyramid-pool layer and mapped to the next subsequent layer. This network can map local features with their global counterparts with improved accuracy and emphasize on discriminative image regions by focusing on relevant local features only. We also propose a compound loss function by optimizing the IoU loss and fusing Dice Loss and Weighted Cross-entropy loss with it to achieve an optimal solution at a faster convergence rate. We have evaluated our model on two standard datasets named PascalVOC2012 and ADE20k and was able to achieve mean IoU of 79.88% and 44.88% on the two datasets respectively, and compared our result with the widely known models to prove the superiority of our model over them.



### Learning a Single Model with a Wide Range of Quality Factors for JPEG Image Artifacts Removal
- **Arxiv ID**: http://arxiv.org/abs/2009.06912v1
- **DOI**: 10.1109/TIP.2020.3020389
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.06912v1)
- **Published**: 2020-09-15 08:16:58+00:00
- **Updated**: 2020-09-15 08:16:58+00:00
- **Authors**: Jianwei Li, Yongtao Wang, Haihua Xie, Kai-Kuang Ma
- **Comment**: Accepted for publication in the IEEE Transactions on Image Processing
- **Journal**: IEEE Transactions on Image Processing, vol. 29, pp.8842-8854, 2020
- **Summary**: Lossy compression brings artifacts into the compressed image and degrades the visual quality. In recent years, many compression artifacts removal methods based on convolutional neural network (CNN) have been developed with great success. However, these methods usually train a model based on one specific value or a small range of quality factors. Obviously, if the test image's quality factor does not match to the assumed value range, then degraded performance will be resulted. With this motivation and further consideration of practical usage, a highly robust compression artifacts removal network is proposed in this paper. Our proposed network is a single model approach that can be trained for handling a wide range of quality factors while consistently delivering superior or comparable image artifacts removal performance. To demonstrate, we focus on the JPEG compression with quality factors, ranging from 1 to 60. Note that a turnkey success of our proposed network lies in the novel utilization of the quantization tables as part of the training data. Furthermore, it has two branches in parallel---i.e., the restoration branch and the global branch. The former effectively removes the local artifacts, such as ringing artifacts removal. On the other hand, the latter extracts the global features of the entire image that provides highly instrumental image quality improvement, especially effective on dealing with the global artifacts, such as blocking, color shifting. Extensive experimental results performed on color and grayscale images have clearly demonstrated the effectiveness and efficacy of our proposed single-model approach on the removal of compression artifacts from the decoded image.



### 360-Degree Gaze Estimation in the Wild Using Multiple Zoom Scales
- **Arxiv ID**: http://arxiv.org/abs/2009.06924v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.06924v3)
- **Published**: 2020-09-15 08:45:12+00:00
- **Updated**: 2021-10-26 11:30:10+00:00
- **Authors**: Ashesh, Chu-Song Chen, Hsuan-Tien Lin
- **Comment**: accepted at BMVC 2021
- **Journal**: None
- **Summary**: Gaze estimation involves predicting where the person is looking at within an image or video. Technically, the gaze information can be inferred from two different magnification levels: face orientation and eye orientation. The inference is not always feasible for gaze estimation in the wild, given the lack of clear eye patches in conditions like extreme left/right gazes or occlusions. In this work, we design a model that mimics humans' ability to estimate the gaze by aggregating from focused looks, each at a different magnification level of the face area. The model avoids the need to extract clear eye patches and at the same time addresses another important issue of face-scale variation for gaze estimation in the wild. We further extend the model to handle the challenging task of 360-degree gaze estimation by encoding the backward gazes in the polar representation along with a robust averaging scheme. Experiment results on the ETH-XGaze dataset, which does not contain scale-varying faces, demonstrate the model's effectiveness to assimilate information from multiple scales. For other benchmark datasets with many scale-varying faces (Gaze360 and RT-GENE), the proposed model achieves state-of-the-art performance for gaze estimation when using either images or videos. Our code and pretrained models can be accessed at https://github.com/ashesh-0/MultiZoomGaze.



### The FaceChannel: A Fast & Furious Deep Neural Network for Facial Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/2009.07635v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2009.07635v1)
- **Published**: 2020-09-15 09:25:37+00:00
- **Updated**: 2020-09-15 09:25:37+00:00
- **Authors**: Pablo Barros, Nikhil Churamani, Alessandra Sciutti
- **Comment**: Accepted for publication at SN Computer Science. arXiv admin note:
  substantial text overlap with arXiv:2004.08195
- **Journal**: None
- **Summary**: Current state-of-the-art models for automatic Facial Expression Recognition (FER) are based on very deep neural networks that are effective but rather expensive to train. Given the dynamic conditions of FER, this characteristic hinders such models of been used as a general affect recognition. In this paper, we address this problem by formalizing the FaceChannel, a light-weight neural network that has much fewer parameters than common deep neural networks. We introduce an inhibitory layer that helps to shape the learning of facial features in the last layer of the network and thus improving performance while reducing the number of trainable parameters. To evaluate our model, we perform a series of experiments on different benchmark datasets and demonstrate how the FaceChannel achieves a comparable, if not better, performance to the current state-of-the-art in FER. Our experiments include cross-dataset analysis, to estimate how our model behaves on different affective recognition conditions. We conclude our paper with an analysis of how FaceChannel learns and adapt the learned facial features towards the different datasets.



### AIM 2020 Challenge on Efficient Super-Resolution: Methods and Results
- **Arxiv ID**: http://arxiv.org/abs/2009.06943v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.06943v1)
- **Published**: 2020-09-15 09:25:51+00:00
- **Updated**: 2020-09-15 09:25:51+00:00
- **Authors**: Kai Zhang, Martin Danelljan, Yawei Li, Radu Timofte, Jie Liu, Jie Tang, Gangshan Wu, Yu Zhu, Xiangyu He, Wenjie Xu, Chenghua Li, Cong Leng, Jian Cheng, Guangyang Wu, Wenyi Wang, Xiaohong Liu, Hengyuan Zhao, Xiangtao Kong, Jingwen He, Yu Qiao, Chao Dong, Xiaotong Luo, Liang Chen, Jiangtao Zhang, Maitreya Suin, Kuldeep Purohit, A. N. Rajagopalan, Xiaochuan Li, Zhiqiang Lang, Jiangtao Nie, Wei Wei, Lei Zhang, Abdul Muqeet, Jiwon Hwang, Subin Yang, JungHeum Kang, Sung-Ho Bae, Yongwoo Kim, Liang Chen, Jiangtao Zhang, Xiaotong Luo, Yanyun Qu, Geun-Woo Jeon, Jun-Ho Choi, Jun-Hyuk Kim, Jong-Seok Lee, Steven Marty, Eric Marty, Dongliang Xiong, Siang Chen, Lin Zha, Jiande Jiang, Xinbo Gao, Wen Lu, Haicheng Wang, Vineeth Bhaskara, Alex Levinshtein, Stavros Tsogkas, Allan Jepson, Xiangzhen Kong, Tongtong Zhao, Shanshan Zhao, Hrishikesh P S, Densen Puthussery, Jiji C V, Nan Nan, Shuai Liu, Jie Cai, Zibo Meng, Jiaming Ding, Chiu Man Ho, Xuehui Wang, Qiong Yan, Yuzhi Zhao, Long Chen, Jiangtao Zhang, Xiaotong Luo, Liang Chen, Yanyun Qu, Long Sun, Wenhao Wang, Zhenbing Liu, Rushi Lan, Rao Muhammad Umer, Christian Micheloni
- **Comment**: None
- **Journal**: None
- **Summary**: This paper reviews the AIM 2020 challenge on efficient single image super-resolution with focus on the proposed solutions and results. The challenge task was to super-resolve an input image with a magnification factor x4 based on a set of prior examples of low and corresponding high resolution images. The goal is to devise a network that reduces one or several aspects such as runtime, parameter count, FLOPs, activations, and memory consumption while at least maintaining PSNR of MSRResNet. The track had 150 registered participants, and 25 teams submitted the final results. They gauge the state-of-the-art in efficient single image super-resolution.



### Puzzle Mix: Exploiting Saliency and Local Statistics for Optimal Mixup
- **Arxiv ID**: http://arxiv.org/abs/2009.06962v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2009.06962v2)
- **Published**: 2020-09-15 10:10:23+00:00
- **Updated**: 2020-12-30 10:45:39+00:00
- **Authors**: Jang-Hyun Kim, Wonho Choo, Hyun Oh Song
- **Comment**: Published at ICML 2020
- **Journal**: None
- **Summary**: While deep neural networks achieve great performance on fitting the training distribution, the learned networks are prone to overfitting and are susceptible to adversarial attacks. In this regard, a number of mixup based augmentation methods have been recently proposed. However, these approaches mainly focus on creating previously unseen virtual examples and can sometimes provide misleading supervisory signal to the network. To this end, we propose Puzzle Mix, a mixup method for explicitly utilizing the saliency information and the underlying statistics of the natural examples. This leads to an interesting optimization problem alternating between the multi-label objective for optimal mixing mask and saliency discounted optimal transport objective. Our experiments show Puzzle Mix achieves the state of the art generalization and the adversarial robustness results compared to other mixup methods on CIFAR-100, Tiny-ImageNet, and ImageNet datasets. The source code is available at https://github.com/snu-mllab/PuzzleMix.



### Gravitational Models Explain Shifts on Human Visual Attention
- **Arxiv ID**: http://arxiv.org/abs/2009.06963v1
- **DOI**: 10.1038/s41598-020-73494-2
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.06963v1)
- **Published**: 2020-09-15 10:12:41+00:00
- **Updated**: 2020-09-15 10:12:41+00:00
- **Authors**: Dario Zanca, Marco Gori, Stefano Melacci, Alessandra Rufa
- **Comment**: None
- **Journal**: None
- **Summary**: Visual attention refers to the human brain's ability to select relevant sensory information for preferential processing, improving performance in visual and cognitive tasks. It proceeds in two phases. One in which visual feature maps are acquired and processed in parallel. Another where the information from these maps is merged in order to select a single location to be attended for further and more complex computations and reasoning. Its computational description is challenging, especially if the temporal dynamics of the process are taken into account. Numerous methods to estimate saliency have been proposed in the last three decades. They achieve almost perfect performance in estimating saliency at the pixel level, but the way they generate shifts in visual attention fully depends on winner-take-all (WTA) circuitry. WTA is implemented} by the biological hardware in order to select a location with maximum saliency, towards which to direct overt attention. In this paper we propose a gravitational model (GRAV) to describe the attentional shifts. Every single feature acts as an attractor and {the shifts are the result of the joint effects of the attractors. In the current framework, the assumption of a single, centralized saliency map is no longer necessary, though still plausible. Quantitative results on two large image datasets show that this model predicts shifts more accurately than winner-take-all.



### Contrastive Cross-site Learning with Redesigned Net for COVID-19 CT Classification
- **Arxiv ID**: http://arxiv.org/abs/2009.07652v1
- **DOI**: 10.1109/JBHI.2020.3023246
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.07652v1)
- **Published**: 2020-09-15 11:09:04+00:00
- **Updated**: 2020-09-15 11:09:04+00:00
- **Authors**: Zhao Wang, Quande Liu, Qi Dou
- **Comment**: Published as a journal paper at IEEE J-BHI; code and dataset are
  available at https://github.com/med-air/Contrastive-COVIDNet
- **Journal**: None
- **Summary**: The pandemic of coronavirus disease 2019 (COVID-19) has lead to a global public health crisis spreading hundreds of countries. With the continuous growth of new infections, developing automated tools for COVID-19 identification with CT image is highly desired to assist the clinical diagnosis and reduce the tedious workload of image interpretation. To enlarge the datasets for developing machine learning methods, it is essentially helpful to aggregate the cases from different medical systems for learning robust and generalizable models. This paper proposes a novel joint learning framework to perform accurate COVID-19 identification by effectively learning with heterogeneous datasets with distribution discrepancy. We build a powerful backbone by redesigning the recently proposed COVID-Net in aspects of network architecture and learning strategy to improve the prediction accuracy and learning efficiency. On top of our improved backbone, we further explicitly tackle the cross-site domain shift by conducting separate feature normalization in latent space. Moreover, we propose to use a contrastive training objective to enhance the domain invariance of semantic embeddings for boosting the classification performance on each dataset. We develop and evaluate our method with two public large-scale COVID-19 diagnosis datasets made up of CT images. Extensive experiments show that our approach consistently improves the performances on both datasets, outperforming the original COVID-Net trained on each dataset by 12.16% and 14.23% in AUC respectively, also exceeding existing state-of-the-art multi-site learning methods.



### Light Can Hack Your Face! Black-box Backdoor Attack on Face Recognition Systems
- **Arxiv ID**: http://arxiv.org/abs/2009.06996v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.06996v1)
- **Published**: 2020-09-15 11:50:29+00:00
- **Updated**: 2020-09-15 11:50:29+00:00
- **Authors**: Haoliang Li, Yufei Wang, Xiaofei Xie, Yang Liu, Shiqi Wang, Renjie Wan, Lap-Pui Chau, Alex C. Kot
- **Comment**: First two authors contributed equally
- **Journal**: None
- **Summary**: Deep neural networks (DNN) have shown great success in many computer vision applications. However, they are also known to be susceptible to backdoor attacks. When conducting backdoor attacks, most of the existing approaches assume that the targeted DNN is always available, and an attacker can always inject a specific pattern to the training data to further fine-tune the DNN model. However, in practice, such attack may not be feasible as the DNN model is encrypted and only available to the secure enclave.   In this paper, we propose a novel black-box backdoor attack technique on face recognition systems, which can be conducted without the knowledge of the targeted DNN model. To be specific, we propose a backdoor attack with a novel color stripe pattern trigger, which can be generated by modulating LED in a specialized waveform. We also use an evolutionary computing strategy to optimize the waveform for backdoor attack. Our backdoor attack can be conducted in a very mild condition: 1) the adversary cannot manipulate the input in an unnatural way (e.g., injecting adversarial noise); 2) the adversary cannot access the training database; 3) the adversary has no knowledge of the training model as well as the training set used by the victim party.   We show that the backdoor trigger can be quite effective, where the attack success rate can be up to $88\%$ based on our simulation study and up to $40\%$ based on our physical-domain study by considering the task of face recognition and verification based on at most three-time attempts during authentication. Finally, we evaluate several state-of-the-art potential defenses towards backdoor attacks, and find that our attack can still be effective. We highlight that our study revealed a new physical backdoor attack, which calls for the attention of the security issue of the existing face recognition/verification techniques.



### Optimal Use of Multi-spectral Satellite Data with Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2009.07000v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2009.07000v1)
- **Published**: 2020-09-15 11:55:45+00:00
- **Updated**: 2020-09-15 11:55:45+00:00
- **Authors**: Sagar Vaze, James Foley, Mohamed Seddiq, Alexey Unagaev, Natalia Efremova
- **Comment**: AI for Social Good workshop - Harvard CRCS
- **Journal**: None
- **Summary**: The analysis of satellite imagery will prove a crucial tool in the pursuit of sustainable development. While Convolutional Neural Networks (CNNs) have made large gains in natural image analysis, their application to multi-spectral satellite images (wherein input images have a large number of channels) remains relatively unexplored. In this paper, we compare different methods of leveraging multi-band information with CNNs, demonstrating the performance of all compared methods on the task of semantic segmentation of agricultural vegetation (vineyards). We show that standard industry practice of using bands selected by a domain expert leads to a significantly worse test accuracy than the other methods compared. Specifically, we compare: using bands specified by an expert; using all available bands; learning attention maps over the input bands; and leveraging Bayesian optimisation to dictate band choice. We show that simply using all available band information already increases test time performance, and show that the Bayesian optimisation, first applied to band selection in this work, can be used to further boost accuracy.



### Promoting Connectivity of Network-Like Structures by Enforcing Region Separation
- **Arxiv ID**: http://arxiv.org/abs/2009.07011v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.07011v1)
- **Published**: 2020-09-15 12:21:35+00:00
- **Updated**: 2020-09-15 12:21:35+00:00
- **Authors**: Doruk Oner, Mateusz Koziński, Leonardo Citraro, Nathan C. Dadap, Alexandra G. Konings, Pascal Fua
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel, connectivity-oriented loss function for training deep convolutional networks to reconstruct network-like structures, like roads and irrigation canals, from aerial images. The main idea behind our loss is to express the connectivity of roads, or canals, in terms of disconnections that they create between background regions of the image. In simple terms, a gap in the predicted road causes two background regions, that lie on the opposite sides of a ground truth road, to touch in prediction. Our loss function is designed to prevent such unwanted connections between background regions, and therefore close the gaps in predicted roads. It also prevents predicting false positive roads and canals by penalizing unwarranted disconnections of background regions. In order to capture even short, dead-ending road segments, we evaluate the loss in small image crops. We show, in experiments on two standard road benchmarks and a new data set of irrigation canals, that convnets trained with our loss function recover road connectivity so well, that it suffices to skeletonize their output to produce state of the art maps. A distinct advantage of our approach is that the loss can be plugged in to any existing training setup without further modifications.



### Group-Level Emotion Recognition Using a Unimodal Privacy-Safe Non-Individual Approach
- **Arxiv ID**: http://arxiv.org/abs/2009.07013v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2009.07013v1)
- **Published**: 2020-09-15 12:25:33+00:00
- **Updated**: 2020-09-15 12:25:33+00:00
- **Authors**: Anastasia Petrova, Dominique Vaufreydaz, Philippe Dessus
- **Comment**: None
- **Journal**: EmotiW2020 Challenge at the 22nd ACM International Conference on
  Multimodal Interaction (ICMI2020), Oct 2020, Utrecht, Netherlands
- **Summary**: This article presents our unimodal privacy-safe and non-individual proposal for the audio-video group emotion recognition subtask at the Emotion Recognition in the Wild (EmotiW) Challenge 2020 1. This sub challenge aims to classify in the wild videos into three categories: Positive, Neutral and Negative. Recent deep learning models have shown tremendous advances in analyzing interactions between people, predicting human behavior and affective evaluation. Nonetheless, their performance comes from individual-based analysis, which means summing up and averaging scores from individual detections, which inevitably leads to some privacy issues. In this research, we investigated a frugal approach towards a model able to capture the global moods from the whole image without using face or pose detection, or any individual-based feature as input. The proposed methodology mixes state-of-the-art and dedicated synthetic corpora as training sources. With an in-depth exploration of neural network architectures for group-level emotion recognition, we built a VGG-based model achieving 59.13% accuracy on the VGAF test set (eleventh place of the challenge). Given that the analysis is unimodal based only on global features and that the performance is evaluated on a real-world dataset, these results are promising and let us envision extending this model to multimodality for classroom ambiance evaluation, our final target application.



### Decision-based Universal Adversarial Attack
- **Arxiv ID**: http://arxiv.org/abs/2009.07024v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.07024v4)
- **Published**: 2020-09-15 12:49:03+00:00
- **Updated**: 2021-01-05 11:01:01+00:00
- **Authors**: Jing Wu, Mingyi Zhou, Shuaicheng Liu, Yipeng Liu, Ce Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: A single perturbation can pose the most natural images to be misclassified by classifiers. In black-box setting, current universal adversarial attack methods utilize substitute models to generate the perturbation, then apply the perturbation to the attacked model. However, this transfer often produces inferior results. In this study, we directly work in the black-box setting to generate the universal adversarial perturbation. Besides, we aim to design an adversary generating a single perturbation having texture like stripes based on orthogonal matrix, as the top convolutional layers are sensitive to stripes. To this end, we propose an efficient Decision-based Universal Attack (DUAttack). With few data, the proposed adversary computes the perturbation based solely on the final inferred labels, but good transferability has been realized not only across models but also span different vision tasks. The effectiveness of DUAttack is validated through comparisons with other state-of-the-art attacks. The efficiency of DUAttack is also demonstrated on real world settings including the Microsoft Azure. In addition, several representative defense methods are struggling with DUAttack, indicating the practicability of the proposed method.



### RaLL: End-to-end Radar Localization on Lidar Map Using Differentiable Measurement Model
- **Arxiv ID**: http://arxiv.org/abs/2009.07061v3
- **DOI**: 10.1109/TITS.2021.3061165
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.07061v3)
- **Published**: 2020-09-15 13:13:38+00:00
- **Updated**: 2021-03-06 03:17:49+00:00
- **Authors**: Huan Yin, Runjian Chen, Yue Wang, Rong Xiong
- **Comment**: This paper has been accepted for publication by IEEE TITS. The
  published version is available at
  https://ieeexplore.ieee.org/document/9370010 . A demonstration video is
  available at https://youtu.be/a3wEv-eVlcg . For open source code, see
  https://github.com/ZJUYH/RaLL
- **Journal**: None
- **Summary**: Compared to the onboard camera and laser scanner, radar sensor provides lighting and weather invariant sensing, which is naturally suitable for long-term localization under adverse conditions. However, radar data is sparse and noisy, resulting in challenges for radar mapping. On the other hand, the most popular available map currently is built by lidar. In this paper, we propose an end-to-end deep learning framework for Radar Localization on Lidar Map (RaLL) to bridge the gap, which not only achieves the robust radar localization but also exploits the mature lidar mapping technique, thus reducing the cost of radar mapping. We first embed both sensor modals into a common feature space by a neural network. Then multiple offsets are added to the map modal for exhaustive similarity evaluation against the current radar modal, yielding the regression of the current pose. Finally, we apply this differentiable measurement model to a Kalman Filter (KF) to learn the whole sequential localization process in an end-to-end manner. \textit{The whole learning system is differentiable with the network based measurement model at the front-end and KF at the back-end.} To validate the feasibility and effectiveness, we employ multi-session multi-scene datasets collected from the real world, and the results demonstrate that our proposed system achieves superior performance over $90km$ driving, even in generalization scenarios where the model training is in UK, while testing in South Korea. We also release the source code publicly.



### Multi-structure bone segmentation in pediatric MR images with combined regularization from shape priors and adversarial network
- **Arxiv ID**: http://arxiv.org/abs/2009.07092v5
- **DOI**: 10.1016/j.artmed.2022.102364
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.07092v5)
- **Published**: 2020-09-15 13:39:53+00:00
- **Updated**: 2022-07-12 08:45:15+00:00
- **Authors**: Arnaud Boutillon, Bhushan Borotikar, Valérie Burdin, Pierre-Henri Conze
- **Comment**: 21 pages, 11 figures, 7 tables, Accepted for publication in the
  Journal of Artificial Intelligence in Medicine
- **Journal**: None
- **Summary**: Morphological and diagnostic evaluation of pediatric musculoskeletal system is crucial in clinical practice. However, most segmentation models do not perform well on scarce pediatric imaging data. We propose a new pre-trained regularized convolutional encoder-decoder network for the challenging task of segmenting heterogeneous pediatric magnetic resonance (MR) images. To this end, we have conceived a novel optimization scheme for the segmentation network which comprises additional regularization terms to the loss function. In order to obtain globally consistent predictions, we incorporate a shape priors based regularization, derived from a non-linear shape representation learnt by an auto-encoder. Additionally, an adversarial regularization computed by a discriminator is integrated to encourage precise delineations. The proposed method is evaluated for the task of multi-bone segmentation on two scarce pediatric imaging datasets from ankle and shoulder joints, comprising pathological as well as healthy examinations. The proposed method performed either better or at par with previously proposed approaches for Dice, sensitivity, specificity, maximum symmetric surface distance, average symmetric surface distance, and relative absolute volume difference metrics. We illustrate that the proposed approach can be easily integrated into various bone segmentation strategies and can improve the prediction accuracy of models pre-trained on large non-medical images databases. The obtained results bring new perspectives for the management of pediatric musculoskeletal disorders.



### CSI2Image: Image Reconstruction from Channel State Information Using Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2009.07100v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.07100v2)
- **Published**: 2020-09-15 13:49:07+00:00
- **Updated**: 2020-09-16 23:58:49+00:00
- **Authors**: Sorachi Kato, Takeru Fukushima, Tomoki Murakami, Hirantha Abeysekera, Yusuke Iwasaki, Takuya Fujihashi, Takashi Watanabe, Shunsuke Saruwatari
- **Comment**: 12 pages, 19 figures
- **Journal**: None
- **Summary**: This study aims to find the upper limit of the wireless sensing capability of acquiring physical space information. This is a challenging objective, because at present, wireless sensing studies continue to succeed in acquiring novel phenomena. Thus, although a complete answer cannot be obtained yet, a step is taken towards it here. To achieve this, CSI2Image, a novel channel-state-information (CSI)-to-image conversion method based on generative adversarial networks (GANs), is proposed. The type of physical information acquired using wireless sensing can be estimated by checking wheth\-er the reconstructed image captures the desired physical space information. Three types of learning methods are demonstrated: gen\-er\-a\-tor-only learning, GAN-only learning, and hybrid learning. Evaluating the performance of CSI2Image is difficult, because both the clarity of the image and the presence of the desired physical space information must be evaluated. To solve this problem, a quantitative evaluation methodology using an object detection library is also proposed. CSI2Image was implemented using IEEE 802.11ac compressed CSI, and the evaluation results show that the image was successfully reconstructed. The results demonstrate that gen\-er\-a\-tor-only learning is sufficient for simple wireless sensing problems, but in complex wireless sensing problems, GANs are important for reconstructing generalized images with more accurate physical space information.



### Polyp-artifact relationship analysis using graph inductive learned representations
- **Arxiv ID**: http://arxiv.org/abs/2009.07109v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2009.07109v1)
- **Published**: 2020-09-15 13:56:39+00:00
- **Updated**: 2020-09-15 13:56:39+00:00
- **Authors**: Roger D. Soberanis-Mukul, Shadi Albarqouni, Nassir Navab
- **Comment**: None
- **Journal**: None
- **Summary**: The diagnosis process of colorectal cancer mainly focuses on the localization and characterization of abnormal growths in the colon tissue known as polyps. Despite recent advances in deep object localization, the localization of polyps remains challenging due to the similarities between tissues, and the high level of artifacts. Recent studies have shown the negative impact of the presence of artifacts in the polyp detection task, and have started to take them into account within the training process. However, the use of prior knowledge related to the spatial interaction of polyps and artifacts has not yet been considered. In this work, we incorporate artifact knowledge in a post-processing step. Our method models this task as an inductive graph representation learning problem, and is composed of training and inference steps. Detected bounding boxes around polyps and artifacts are considered as nodes connected by a defined criterion. The training step generates a node classifier with ground truth bounding boxes. In inference, we use this classifier to analyze a second graph, generated from artifact and polyp predictions given by region proposal networks. We evaluate how the choices in the connectivity and artifacts affect the performance of our method and show that it has the potential to reduce the false positives in the results of a region proposal network.



### A Mobile App for Wound Localization using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2009.07133v1
- **DOI**: 10.1109/ACCESS.2022.3179137
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.07133v1)
- **Published**: 2020-09-15 14:35:29+00:00
- **Updated**: 2020-09-15 14:35:29+00:00
- **Authors**: D. M. Anisuzzaman, Yash Patel, Jeffrey Niezgoda, Sandeep Gopalakrishnan, Zeyun Yu
- **Comment**: 8 pages, 5 figures, 1 table
- **Journal**: IEEE Access. 30 May 2022
- **Summary**: We present an automated wound localizer from 2D wound and ulcer images by using deep neural network, as the first step towards building an automated and complete wound diagnostic system. The wound localizer has been developed by using YOLOv3 model, which is then turned into an iOS mobile application. The developed localizer can detect the wound and its surrounding tissues and isolate the localized wounded region from images, which would be very helpful for future processing such as wound segmentation and classification due to the removal of unnecessary regions from wound images. For Mobile App development with video processing, a lighter version of YOLOv3 named tiny-YOLOv3 has been used. The model is trained and tested on our own image dataset in collaboration with AZH Wound and Vascular Center, Milwaukee, Wisconsin. The YOLOv3 model is compared with SSD model, showing that YOLOv3 gives a mAP value of 93.9%, which is much better than the SSD model (86.4%). The robustness and reliability of these models are also tested on a publicly available dataset named Medetec and shows a very good performance as well.



### HGCN-GJS: Hierarchical Graph Convolutional Network with Groupwise Joint Sampling for Trajectory Prediction
- **Arxiv ID**: http://arxiv.org/abs/2009.07140v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.07140v2)
- **Published**: 2020-09-15 14:51:10+00:00
- **Updated**: 2023-08-16 15:40:23+00:00
- **Authors**: Yuying Chen, Congcong Liu, Xiaodong Mei, Bertram E. Shi, Ming Liu
- **Comment**: 8 pages, 8 figures, accepted by IROS 2022
- **Journal**: None
- **Summary**: Accurate pedestrian trajectory prediction is of great importance for downstream tasks such as autonomous driving and mobile robot navigation. Fully investigating the social interactions within the crowd is crucial for accurate pedestrian trajectory prediction. However, most existing methods do not capture group level interactions well, focusing only on pairwise interactions and neglecting group-wise interactions. In this work, we propose a hierarchical graph convolutional network, HGCN-GJS, for trajectory prediction which well leverages group level interactions within the crowd. Furthermore, we introduce a novel joint sampling scheme for modeling the joint distribution of multiple pedestrians in the future trajectories. Based on the group information, this scheme associates the trajectory of one person with the trajectory of other people in the group, but maintains the independence of the trajectories of outsiders. We demonstrate the performance of our network on several trajectory prediction datasets, achieving state-of-the-art results on all datasets considered.



### Image Based Artificial Intelligence in Wound Assessment: A Systematic Review
- **Arxiv ID**: http://arxiv.org/abs/2009.07141v1
- **DOI**: 10.1089/wound.2021.0091
- **Categories**: **cs.CY**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.07141v1)
- **Published**: 2020-09-15 14:52:14+00:00
- **Updated**: 2020-09-15 14:52:14+00:00
- **Authors**: D. M. Anisuzzaman, Chuanbo Wang, Behrouz Rostami, Sandeep Gopalakrishnan, Jeffrey Niezgoda, Zeyun Yu
- **Comment**: 18 pages, 9 tables, 1 figure
- **Journal**: Adv Wound Care (New Rochelle). 2021 Sep 21. PMID: 34544270
- **Summary**: Efficient and effective assessment of acute and chronic wounds can help wound care teams in clinical practice to greatly improve wound diagnosis, optimize treatment plans, ease the workload and achieve health related quality of life to the patient population. While artificial intelligence (AI) has found wide applications in health-related sciences and technology, AI-based systems remain to be developed clinically and computationally for high-quality wound care. To this end, we have carried out a systematic review of intelligent image-based data analysis and system developments for wound assessment. Specifically, we provide an extensive review of research methods on wound measurement (segmentation) and wound diagnosis (classification). We also reviewed recent work on wound assessment systems (including hardware, software, and mobile apps). More than 250 articles were retrieved from various publication databases and online resources, and 115 of them were carefully selected to cover the breadth and depth of most recent and relevant work to convey the current review to its fulfillment.



### F3RNet: Full-Resolution Residual Registration Network for Deformable Image Registration
- **Arxiv ID**: http://arxiv.org/abs/2009.07151v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.07151v3)
- **Published**: 2020-09-15 15:05:54+00:00
- **Updated**: 2020-12-07 03:08:38+00:00
- **Authors**: Zhe Xu, Jie Luo, Jiangpeng Yan, Xiu Li, Jagadeesan Jayender
- **Comment**: None
- **Journal**: None
- **Summary**: Deformable image registration (DIR) is essential for many image-guided therapies. Recently, deep learning approaches have gained substantial popularity and success in DIR. Most deep learning approaches use the so-called mono-stream "high-to-low, low-to-high" network structure, and can achieve satisfactory overall registration results. However, accurate alignments for some severely deformed local regions, which are crucial for pinpointing surgical targets, are often overlooked. Consequently, these approaches are not sensitive to some hard-to-align regions, e.g., intra-patient registration of deformed liver lobes. In this paper, we propose a novel unsupervised registration network, namely the Full-Resolution Residual Registration Network (F3RNet), for deformable registration of severely deformed organs. The proposed method combines two parallel processing streams in a residual learning fashion. One stream takes advantage of the full-resolution information that facilitates accurate voxel-level registration. The other stream learns the deep multi-scale residual representations to obtain robust recognition. We also factorize the 3D convolution to reduce the training parameters and enhance network efficiency. We validate the proposed method on a clinically acquired intra-patient abdominal CT-MRI dataset and a public inspiratory and expiratory thorax CT dataset. Experiments on both multimodal and unimodal registration demonstrate promising results compared to state-of-the-art approaches.



### PESAO: Psychophysical Experimental Setup for Active Observers
- **Arxiv ID**: http://arxiv.org/abs/2009.09933v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2009.09933v2)
- **Published**: 2020-09-15 15:06:48+00:00
- **Updated**: 2020-09-30 17:12:50+00:00
- **Authors**: Markus D. Solbach, John K. Tsotsos
- **Comment**: http://data.nvision2.eecs.yorku.ca/PESAO/, technical report, 20
  pages, 21 figures
- **Journal**: None
- **Summary**: Most past and present research in computer vision involves passively observed data. Humans, however, are active observers outside the lab; they explore, search, select what and how to look. Nonetheless, how exactly active observation occurs in humans so that it can inform the design of active computer vision systems is an open problem. PESAO is designed for investigating active, visual observation in a 3D world. The goal was to build an experimental setup for various active perception tasks with human subjects (active observers) in mind that is capable of tracking the head and gaze. While many studies explore human performances, usually, they use line drawings portrayed in 2D, and no active observer is involved. PESAO allows us to bring many studies to the three-dimensional world, even involving active observers. In our instantiation, it spans an area of 400cm x 300cm and can track active observers at a frequency of 120Hz. Furthermore, PESAO provides tracking and recording of 6D head motion, gaze, eye movement-type, first-person video, head-mounted IMU sensor, birds-eye video, and experimenter notes. All are synchronized at microsecond resolution.



### AMRNet: Chips Augmentation in Aerial Images Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2009.07168v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.07168v2)
- **Published**: 2020-09-15 15:16:06+00:00
- **Updated**: 2020-10-25 08:38:25+00:00
- **Authors**: Zhiwei Wei, Chenzhen Duan, Xinghao Song, Ye Tian, Hongpeng Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Object detection in aerial images is a challenging task due to the following reasons: (1) objects are small and dense relative to images; (2) the object scale varies in a wide range; (3) the number of object in different classes is imbalanced. Many current methods adopt cropping idea: splitting high resolution images into serials subregions (chips) and detecting on them. However, some problems such as scale variation, object sparsity, and class imbalance exist in the process of training network with chips. In this work, three augmentation methods are introduced to relieve these problems. Specifically, we propose a scale adaptive module, which dynamically adjusts chip size to balance object scale, narrowing scale variation in training. In addtion, we introduce mosaic to augment datasets, relieving object sparity problem. To balance catgory, we present mask resampling to paste object in chips with panoramic segmentation. Our model achieves state-of-the-art perfomance on two popular aerial image datasets of VisDrone and UAVDT. Remarkably, three methods can be independently applied to detectiors, increasing performance steady without the sacrifice of inference efficiency.



### ResNet-like Architecture with Low Hardware Requirements
- **Arxiv ID**: http://arxiv.org/abs/2009.07190v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.07190v2)
- **Published**: 2020-09-15 15:54:28+00:00
- **Updated**: 2020-10-21 16:28:07+00:00
- **Authors**: Elena Limonova, Daniil Alfonso, Dmitry Nikolaev, Vladimir V. Arlazarov
- **Comment**: Accepted to ICPR 2020. Corrected typos and bibliography formatting
- **Journal**: None
- **Summary**: One of the most computationally intensive parts in modern recognition systems is an inference of deep neural networks that are used for image classification, segmentation, enhancement, and recognition. The growing popularity of edge computing makes us look for ways to reduce its time for mobile and embedded devices. One way to decrease the neural network inference time is to modify a neuron model to make it moreefficient for computations on a specific device. The example ofsuch a model is a bipolar morphological neuron model. The bipolar morphological neuron is based on the idea of replacing multiplication with addition and maximum operations. This model has been demonstrated for simple image classification with LeNet-like architectures [1]. In the paper, we introduce a bipolar morphological ResNet (BM-ResNet) model obtained from a much more complex ResNet architecture by converting its layers to bipolar morphological ones. We apply BM-ResNet to image classification on MNIST and CIFAR-10 datasets with only a moderate accuracy decrease from 99.3% to 99.1% and from 85.3% to 85.1%. We also estimate the computational complexity of the resulting model. We show that for the majority of ResNet layers, the considered model requires 2.1-2.9 times fewer logic gates for implementation and 15-30% lower latency.



### Switching Transferable Gradient Directions for Query-Efficient Black-Box Adversarial Attacks
- **Arxiv ID**: http://arxiv.org/abs/2009.07191v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/2009.07191v2)
- **Published**: 2020-09-15 15:55:08+00:00
- **Updated**: 2021-05-13 12:47:32+00:00
- **Authors**: Chen Ma, Shuyu Cheng, Li Chen, Jun Zhu, Junhai Yong
- **Comment**: 18 pages, including the supplementary material after the reference
  section
- **Journal**: None
- **Summary**: We propose a simple and highly query-efficient black-box adversarial attack named SWITCH, which has a state-of-the-art performance in the score-based setting. SWITCH features a highly efficient and effective utilization of the gradient of a surrogate model $\hat{\mathbf{g}}$ w.r.t. the input image, i.e., the transferable gradient. In each iteration, SWITCH first tries to update the current sample along the direction of $\hat{\mathbf{g}}$, but considers switching to its opposite direction $-\hat{\mathbf{g}}$ if our algorithm detects that it does not increase the value of the attack objective function. We justify the choice of switching to the opposite direction by a local approximate linearity assumption. In SWITCH, only one or two queries are needed per iteration, but it is still effective due to the rich information provided by the transferable gradient, thereby resulting in unprecedented query efficiency. To improve the robustness of SWITCH, we further propose SWITCH$_\text{RGF}$ in which the update follows the direction of a random gradient-free (RGF) estimate when neither $\hat{\mathbf{g}}$ nor its opposite direction can increase the objective, while maintaining the advantage of SWITCH in terms of query efficiency. Experimental results conducted on CIFAR-10, CIFAR-100 and TinyImageNet show that compared with other methods, SWITCH achieves a satisfactory attack success rate using much fewer queries, and SWITCH$_\text{RGF}$ achieves the state-of-the-art attack success rate with fewer queries overall. Our approach can serve as a strong baseline for future black-box attacks because of its simplicity. The PyTorch source code is released on https://github.com/machanic/SWITCH.



### 3D_DEN: Open-ended 3D Object Recognition using Dynamically Expandable Networks
- **Arxiv ID**: http://arxiv.org/abs/2009.07213v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2009.07213v2)
- **Published**: 2020-09-15 16:44:18+00:00
- **Updated**: 2021-03-15 19:41:06+00:00
- **Authors**: Sudhakaran Jain, Hamidreza Kasaei
- **Comment**: None
- **Journal**: None
- **Summary**: Service robots, in general, have to work independently and adapt to the dynamic changes happening in the environment in real-time. One important aspect in such scenarios is to continually learn to recognize newer object categories when they become available. This combines two main research problems namely continual learning and 3D object recognition. Most of the existing research approaches include the use of deep Convolutional Neural Networks (CNNs) focusing on image datasets. A modified approach might be needed for continually learning 3D object categories. A major concern in using CNNs is the problem of catastrophic forgetting when a model tries to learn a new task. Despite various proposed solutions to mitigate this problem, there still exist some downsides of such solutions, e.g., computational complexity, especially when learning substantial number of tasks. These downsides can pose major problems in robotic scenarios where real-time response plays an essential role. Towards addressing this challenge, we propose a new deep transfer learning approach based on a dynamic architectural method to make robots capable of open-ended learning about new 3D object categories. Furthermore, we make sure that the mentioned downsides are minimized to a great extent. Experimental results showed that the proposed model outperformed state-of-the-art approaches with regards to accuracy and also substantially minimizes computational overhead.



### PointIso: Point Cloud Based Deep Learning Model for Detecting Arbitrary-Precision Peptide Features in LC-MS Map through Attention Based Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2009.07250v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2009.07250v1)
- **Published**: 2020-09-15 17:34:14+00:00
- **Updated**: 2020-09-15 17:34:14+00:00
- **Authors**: Fatema Tuz Zohora, M Ziaur Rahman, Ngoc Hieu Tran, Lei Xin, Baozhen Shan, Ming Li
- **Comment**: 16 pages (main text) with 10 figures, then supplementary material of
  about 5 pages. preprint of journal submission
- **Journal**: None
- **Summary**: A promising technique of discovering disease biomarkers is to measure the relative protein abundance in multiple biofluid samples through liquid chromatography with tandem mass spectrometry (LC-MS/MS) based quantitative proteomics. The key step involves peptide feature detection in LC-MS map, along with its charge and intensity. Existing heuristic algorithms suffer from inaccurate parameters since different settings of the parameters result in significantly different outcomes. Therefore, we propose PointIso, to serve the necessity of an automated system for peptide feature detection that is able to find out the proper parameters itself, and is easily adaptable to different types of datasets. It consists of an attention based scanning step for segmenting the multi-isotopic pattern of peptide features along with charge and a sequence classification step for grouping those isotopes into potential peptide features. PointIso is the first point cloud based, arbitrary-precision deep learning network to address the problem and achieves 98% detection of high quality MS/MS identifications in a benchmark dataset, which is higher than several other widely used algorithms. Besides contributing to the proteomics study, we believe our novel segmentation technique should serve the general image processing domain as well.



### Understanding Deformable Alignment in Video Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2009.07265v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.07265v1)
- **Published**: 2020-09-15 17:55:06+00:00
- **Updated**: 2020-09-15 17:55:06+00:00
- **Authors**: Kelvin C. K. Chan, Xintao Wang, Ke Yu, Chao Dong, Chen Change Loy
- **Comment**: Tech report, 15 pages, 19 figures
- **Journal**: None
- **Summary**: Deformable convolution, originally proposed for the adaptation to geometric variations of objects, has recently shown compelling performance in aligning multiple frames and is increasingly adopted for video super-resolution. Despite its remarkable performance, its underlying mechanism for alignment remains unclear. In this study, we carefully investigate the relation between deformable alignment and the classic flow-based alignment. We show that deformable convolution can be decomposed into a combination of spatial warping and convolution. This decomposition reveals the commonality of deformable alignment and flow-based alignment in formulation, but with a key difference in their offset diversity. We further demonstrate through experiments that the increased diversity in deformable alignment yields better-aligned features, and hence significantly improves the quality of video super-resolution output. Based on our observations, we propose an offset-fidelity loss that guides the offset learning with optical flow. Experiments show that our loss successfully avoids the overflow of offsets and alleviates the instability problem of deformable alignment. Aside from the contributions to deformable alignment, our formulation inspires a more flexible approach to introduce offset diversity to flow-based alignment, improving its performance.



### Generative models with kernel distance in data space
- **Arxiv ID**: http://arxiv.org/abs/2009.07327v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2009.07327v1)
- **Published**: 2020-09-15 19:11:47+00:00
- **Updated**: 2020-09-15 19:11:47+00:00
- **Authors**: Szymon Knop, Marcin Mazur, Przemysław Spurek, Jacek Tabor, Igor Podolak
- **Comment**: None
- **Journal**: None
- **Summary**: Generative models dealing with modeling a~joint data distribution are generally either autoencoder or GAN based. Both have their pros and cons, generating blurry images or being unstable in training or prone to mode collapse phenomenon, respectively. The objective of this paper is to construct a~model situated between above architectures, one that does not inherit their main weaknesses. The proposed LCW generator (Latent Cramer-Wold generator) resembles a classical GAN in transforming Gaussian noise into data space. What is of utmost importance, instead of a~discriminator, LCW generator uses kernel distance. No adversarial training is utilized, hence the name generator. It is trained in two phases. First, an autoencoder based architecture, using kernel measures, is built to model a manifold of data. We propose a Latent Trick mapping a Gaussian to latent in order to get the final model. This results in very competitive FID values.



### Video captioning with stacked attention and semantic hard pull
- **Arxiv ID**: http://arxiv.org/abs/2009.07335v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.07335v3)
- **Published**: 2020-09-15 19:34:37+00:00
- **Updated**: 2021-07-16 18:06:58+00:00
- **Authors**: Md. Mushfiqur Rahman, Thasin Abedin, Khondokar S. S. Prottoy, Ayana Moshruba, Fazlul Hasan Siddiqui
- **Comment**: None
- **Journal**: None
- **Summary**: Video captioning, i.e. the task of generating captions from video sequences creates a bridge between the Natural Language Processing and Computer Vision domains of computer science. The task of generating a semantically accurate description of a video is quite complex. Considering the complexity, of the problem, the results obtained in recent research works are praiseworthy. However, there is plenty of scope for further investigation. This paper addresses this scope and proposes a novel solution. Most video captioning models comprise two sequential/recurrent layers - one as a video-to-context encoder and the other as a context-to-caption decoder. This paper proposes a novel architecture, namely Semantically Sensible Video Captioning (SSVC) which modifies the context generation mechanism by using two novel approaches - "stacked attention" and "spatial hard pull". As there are no exclusive metrics for evaluating video captioning models, we emphasize both quantitative and qualitative analysis of our model. Hence, we have used the BLEU scoring metric for quantitative analysis and have proposed a human evaluation metric for qualitative analysis, namely the Semantic Sensibility (SS) scoring metric. SS Score overcomes the shortcomings of common automated scoring metrics. This paper reports that the use of the aforementioned novelties improves the performance of state-of-the-art architectures.



### Comparison of Spatiotemporal Networks for Learning Video Related Tasks
- **Arxiv ID**: http://arxiv.org/abs/2009.07338v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.07338v1)
- **Published**: 2020-09-15 19:57:50+00:00
- **Updated**: 2020-09-15 19:57:50+00:00
- **Authors**: Logan Courtney, Ramavarapu Sreenivas
- **Comment**: None
- **Journal**: None
- **Summary**: Many methods for learning from video sequences involve temporally processing 2D CNN features from the individual frames or directly utilizing 3D convolutions within high-performing 2D CNN architectures. The focus typically remains on how to incorporate the temporal processing within an already stable spatial architecture. This work constructs an MNIST-based video dataset with parameters controlling relevant facets of common video-related tasks: classification, ordering, and speed estimation. Models trained on this dataset are shown to differ in key ways depending on the task and their use of 2D convolutions, 3D convolutions, or convolutional LSTMs. An empirical analysis indicates a complex, interdependent relationship between the spatial and temporal dimensions with design choices having a large impact on a network's ability to learn the appropriate spatiotemporal features.



### BOP Challenge 2020 on 6D Object Localization
- **Arxiv ID**: http://arxiv.org/abs/2009.07378v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2009.07378v2)
- **Published**: 2020-09-15 22:35:14+00:00
- **Updated**: 2020-10-13 12:09:44+00:00
- **Authors**: Tomas Hodan, Martin Sundermeyer, Bertram Drost, Yann Labbe, Eric Brachmann, Frank Michel, Carsten Rother, Jiri Matas
- **Comment**: In ECCV 2020 Workshops Proceedings
- **Journal**: None
- **Summary**: This paper presents the evaluation methodology, datasets, and results of the BOP Challenge 2020, the third in a series of public competitions organized with the goal to capture the status quo in the field of 6D object pose estimation from an RGB-D image. In 2020, to reduce the domain gap between synthetic training and real test RGB images, the participants were provided 350K photorealistic training images generated by BlenderProc4BOP, a new open-source and light-weight physically-based renderer (PBR) and procedural data generator. Methods based on deep neural networks have finally caught up with methods based on point pair features, which were dominating previous editions of the challenge. Although the top-performing methods rely on RGB-D image channels, strong results were achieved when only RGB channels were used at both training and test time - out of the 26 evaluated methods, the third method was trained on RGB channels of PBR and real images, while the fifth on RGB channels of PBR images only. Strong data augmentation was identified as a key component of the top-performing CosyPose method, and the photorealism of PBR images was demonstrated effective despite the augmentation. The online evaluation system stays open and is available on the project website: bop.felk.cvut.cz.



### Creation and Validation of a Chest X-Ray Dataset with Eye-tracking and Report Dictation for AI Development
- **Arxiv ID**: http://arxiv.org/abs/2009.07386v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.07386v3)
- **Published**: 2020-09-15 23:12:49+00:00
- **Updated**: 2020-10-08 05:54:40+00:00
- **Authors**: Alexandros Karargyris, Satyananda Kashyap, Ismini Lourentzou, Joy Wu, Arjun Sharma, Matthew Tong, Shafiq Abedin, David Beymer, Vandana Mukherjee, Elizabeth A Krupinski, Mehdi Moradi
- **Comment**: None
- **Journal**: None
- **Summary**: We developed a rich dataset of Chest X-Ray (CXR) images to assist investigators in artificial intelligence. The data were collected using an eye tracking system while a radiologist reviewed and reported on 1,083 CXR images. The dataset contains the following aligned data: CXR image, transcribed radiology report text, radiologist's dictation audio and eye gaze coordinates data. We hope this dataset can contribute to various areas of research particularly towards explainable and multimodal deep learning / machine learning methods. Furthermore, investigators in disease classification and localization, automated radiology report generation, and human-machine interaction can benefit from these data. We report deep learning experiments that utilize the attention maps produced by eye gaze dataset to show the potential utility of this data.



