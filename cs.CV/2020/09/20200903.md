# Arxiv Papers in cs.CV on 2020-09-03
### Interactive Visual Study of Multiple Attributes Learning Model of X-Ray Scattering Images
- **Arxiv ID**: http://arxiv.org/abs/2009.02256v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.02256v1)
- **Published**: 2020-09-03 00:38:45+00:00
- **Updated**: 2020-09-03 00:38:45+00:00
- **Authors**: Xinyi Huang, Suphanut Jamonnak, Ye Zhao, Boyu Wang, Minh Hoai, Kevin Yager, Wei Xu
- **Comment**: IEEE SciVis Conference 2020
- **Journal**: IEEE Transactions on Visualization & Computer Graphics 2020
- **Summary**: Existing interactive visualization tools for deep learning are mostly applied to the training, debugging, and refinement of neural network models working on natural images. However, visual analytics tools are lacking for the specific application of x-ray image classification with multiple structural attributes. In this paper, we present an interactive system for domain scientists to visually study the multiple attributes learning models applied to x-ray scattering images. It allows domain scientists to interactively explore this important type of scientific images in embedded spaces that are defined on the model prediction output, the actual labels, and the discovered feature space of neural networks. Users are allowed to flexibly select instance images, their clusters, and compare them regarding the specified visual representation of attributes. The exploration is guided by the manifestation of model performance related to mutual relationships among attributes, which often affect the learning accuracy and effectiveness. The system thus supports domain scientists to improve the training dataset and model, find questionable attributes labels, and identify outlier images or spurious data clusters. Case studies and scientists feedback demonstrate its functionalities and usefulness.



### Spatial Transformer Point Convolution
- **Arxiv ID**: http://arxiv.org/abs/2009.01427v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.01427v1)
- **Published**: 2020-09-03 03:12:25+00:00
- **Updated**: 2020-09-03 03:12:25+00:00
- **Authors**: Yuan Fang, Chunyan Xu, Zhen Cui, Yuan Zong, Jian Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Point clouds are unstructured and unordered in the embedded 3D space. In order to produce consistent responses under different permutation layouts, most existing methods aggregate local spatial points through maximum or summation operation. But such an aggregation essentially belongs to the isotropic filtering on all operated points therein, which tends to lose the information of geometric structures. In this paper, we propose a spatial transformer point convolution (STPC) method to achieve anisotropic convolution filtering on point clouds. To capture and represent implicit geometric structures, we specifically introduce spatial direction dictionary to learn those latent geometric components. To better encode unordered neighbor points, we design sparse deformer to transform them into the canonical ordered dictionary space by using direction dictionary learning. In the transformed space, the standard image-like convolution can be leveraged to generate anisotropic filtering, which is more robust to express those finer variances of local regions. Dictionary learning and encoding processes are encapsulated into a network module and jointly learnt in an end-to-end manner. Extensive experiments on several public datasets (including S3DIS, Semantic3D, SemanticKITTI) demonstrate the effectiveness of our proposed method in point clouds semantic segmentation task.



### Tasks Integrated Networks: Joint Detection and Retrieval for Image Search
- **Arxiv ID**: http://arxiv.org/abs/2009.01438v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2009.01438v1)
- **Published**: 2020-09-03 03:57:50+00:00
- **Updated**: 2020-09-03 03:57:50+00:00
- **Authors**: Lei Zhang, Zhenwei He, Yi Yang, Liang Wang, Xinbo Gao
- **Comment**: To appear in IEEE TPAMI, 18 pages
- **Journal**: None
- **Summary**: The traditional object retrieval task aims to learn a discriminative feature representation with intra-similarity and inter-dissimilarity, which supposes that the objects in an image are manually or automatically pre-cropped exactly. However, in many real-world searching scenarios (e.g., video surveillance), the objects (e.g., persons, vehicles, etc.) are seldom accurately detected or annotated. Therefore, object-level retrieval becomes intractable without bounding-box annotation, which leads to a new but challenging topic, i.e. image-level search. In this paper, to address the image search issue, we first introduce an end-to-end Integrated Net (I-Net), which has three merits: 1) A Siamese architecture and an on-line pairing strategy for similar and dissimilar objects in the given images are designed. 2) A novel on-line pairing (OLP) loss is introduced with a dynamic feature dictionary, which alleviates the multi-task training stagnation problem, by automatically generating a number of negative pairs to restrict the positives. 3) A hard example priority (HEP) based softmax loss is proposed to improve the robustness of classification task by selecting hard categories. With the philosophy of divide and conquer, we further propose an improved I-Net, called DC-I-Net, which makes two new contributions: 1) two modules are tailored to handle different tasks separately in the integrated framework, such that the task specification is guaranteed. 2) A class-center guided HEP loss (C2HEP) by exploiting the stored class centers is proposed, such that the intra-similarity and inter-dissimilarity can be captured for ultimate retrieval. Extensive experiments on famous image-level search oriented benchmark datasets demonstrate that the proposed DC-I-Net outperforms the state-of-the-art tasks-integrated and tasks-separated image search models.



### Learning Dexterous Grasping with Object-Centric Visual Affordances
- **Arxiv ID**: http://arxiv.org/abs/2009.01439v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.01439v2)
- **Published**: 2020-09-03 04:00:40+00:00
- **Updated**: 2021-06-16 22:28:15+00:00
- **Authors**: Priyanka Mandikal, Kristen Grauman
- **Comment**: Accepted at ICRA 2021
- **Journal**: None
- **Summary**: Dexterous robotic hands are appealing for their agility and human-like morphology, yet their high degree of freedom makes learning to manipulate challenging. We introduce an approach for learning dexterous grasping. Our key idea is to embed an object-centric visual affordance model within a deep reinforcement learning loop to learn grasping policies that favor the same object regions favored by people. Unlike traditional approaches that learn from human demonstration trajectories (e.g., hand joint sequences captured with a glove), the proposed prior is object-centric and image-based, allowing the agent to anticipate useful affordance regions for objects unseen during policy learning. We demonstrate our idea with a 30-DoF five-fingered robotic hand simulator on 40 objects from two datasets, where it successfully and efficiently learns policies for stable functional grasps. Our affordance-guided policies are significantly more effective, generalize better to novel objects, train 3 X faster than the baselines, and are more robust to noisy sensor readings and actuation. Our work offers a step towards manipulation agents that learn by watching how people use objects, without requiring state and action information about the human body. Project website: http://vision.cs.utexas.edu/projects/graff-dexterous-affordance-grasp



### Ref-NMS: Breaking Proposal Bottlenecks in Two-Stage Referring Expression Grounding
- **Arxiv ID**: http://arxiv.org/abs/2009.01449v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2009.01449v3)
- **Published**: 2020-09-03 05:04:12+00:00
- **Updated**: 2021-03-10 01:25:59+00:00
- **Authors**: Long Chen, Wenbo Ma, Jun Xiao, Hanwang Zhang, Shih-Fu Chang
- **Comment**: Camera ready version at AAAI 2021, Codes are available at:
  https://github.com/ChopinSharp/ref-nms
- **Journal**: None
- **Summary**: The prevailing framework for solving referring expression grounding is based on a two-stage process: 1) detecting proposals with an object detector and 2) grounding the referent to one of the proposals. Existing two-stage solutions mostly focus on the grounding step, which aims to align the expressions with the proposals. In this paper, we argue that these methods overlook an obvious mismatch between the roles of proposals in the two stages: they generate proposals solely based on the detection confidence (i.e., expression-agnostic), hoping that the proposals contain all right instances in the expression (i.e., expression-aware). Due to this mismatch, current two-stage methods suffer from a severe performance drop between detected and ground-truth proposals. To this end, we propose Ref-NMS, which is the first method to yield expression-aware proposals at the first stage. Ref-NMS regards all nouns in the expression as critical objects, and introduces a lightweight module to predict a score for aligning each box with a critical object. These scores can guide the NMS operation to filter out the boxes irrelevant to the expression, increasing the recall of critical objects, resulting in a significantly improved grounding performance. Since Ref- NMS is agnostic to the grounding step, it can be easily integrated into any state-of-the-art two-stage method. Extensive ablation studies on several backbones, benchmarks, and tasks consistently demonstrate the superiority of Ref-NMS. Codes are available at: https://github.com/ChopinSharp/ref-nms.



### Adherent Mist and Raindrop Removal from a Single Image Using Attentive Convolutional Network
- **Arxiv ID**: http://arxiv.org/abs/2009.01466v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.01466v2)
- **Published**: 2020-09-03 06:17:53+00:00
- **Updated**: 2020-11-26 12:38:39+00:00
- **Authors**: Da He, Xiaoyu Shang, Jiajia Luo
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Temperature difference-induced mist adhered to the glass, such as windshield, camera lens, is often inhomogeneous and obscure, easily obstructing the vision and severely degrading the image. Together with adherent raindrops, they bring considerable challenges to various vision systems but without enough attention. Recent methods for other similar problems typically use hand-crafted priors to generate spatial attention maps. In this work, we newly present a problem of image degradation caused by adherent mist and raindrops. An attentive convolutional network is adopted to visually remove the adherent mist and raindrop from a single image. A baseline architecture with general channel-wise attention, spatial attention, and multilevel feature fusion is used. Considering the variations and regional characteristics of adherent mist and raindrops, we apply interpolation-based pyramid-attention blocks to perceive spatial information at different scales. Experiments show that the proposed method can improve severely degraded images' visibility, both qualitatively and quantitatively. More applied experiments show that this underrated practical problem is critical to high-level vision scenes. Our method also achieves state-of-the-art performance on conventional dehazing and pure de-raindrop problems, in addition to our task of handling adherent mist and raindrops.



### Modeling Global Body Configurations in American Sign Language
- **Arxiv ID**: http://arxiv.org/abs/2009.01468v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.01468v1)
- **Published**: 2020-09-03 06:20:10+00:00
- **Updated**: 2020-09-03 06:20:10+00:00
- **Authors**: Nicholas Wilkins, Beck Cordes Galbraith, Ifeoma Nwogu
- **Comment**: None
- **Journal**: None
- **Summary**: American Sign Language (ASL) is the fourth most commonly used language in the United States and is the language most commonly used by Deaf people in the United States and the English-speaking regions of Canada. Unfortunately, until recently, ASL received little research. This is due, in part, to its delayed recognition as a language until William C. Stokoe's publication in 1960. Limited data has been a long-standing obstacle to ASL research and computational modeling. The lack of large-scale datasets has prohibited many modern machine-learning techniques, such as Neural Machine Translation, from being applied to ASL. In addition, the modality required to capture sign language (i.e. video) is complex in natural settings (as one must deal with background noise, motion blur, and the curse of dimensionality). Finally, when compared with spoken languages, such as English, there has been limited research conducted into the linguistics of ASL.   We realize a simplified version of Liddell and Johnson's Movement-Hold (MH) Model using a Probabilistic Graphical Model (PGM). We trained our model on ASLing, a dataset collected from three fluent ASL signers. We evaluate our PGM against other models to determine its ability to model ASL. Finally, we interpret various aspects of the PGM and draw conclusions about ASL phonetics. The main contributions of this paper are



### TAP-Net: Transport-and-Pack using Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/2009.01469v1
- **DOI**: 10.1145/3414685.3417796
- **Categories**: **cs.GR**, cs.CV, I.3.5; I.2.9
- **Links**: [PDF](http://arxiv.org/pdf/2009.01469v1)
- **Published**: 2020-09-03 06:20:17+00:00
- **Updated**: 2020-09-03 06:20:17+00:00
- **Authors**: Ruizhen Hu, Juzhan Xu, Bin Chen, Minglun Gong, Hao Zhang, Hui Huang
- **Comment**: None
- **Journal**: ACM Transactions on Graphics 2020
- **Summary**: We introduce the transport-and-pack(TAP) problem, a frequently encountered instance of real-world packing, and develop a neural optimization solution based on reinforcement learning. Given an initial spatial configuration of boxes, we seek an efficient method to iteratively transport and pack the boxes compactly into a target container. Due to obstruction and accessibility constraints, our problem has to add a new search dimension, i.e., finding an optimal transport sequence, to the already immense search space for packing alone. Using a learning-based approach, a trained network can learn and encode solution patterns to guide the solution of new problem instances instead of executing an expensive online search. In our work, we represent the transport constraints using a precedence graph and train a neural network, coined TAP-Net, using reinforcement learning to reward efficient and stable packing. The network is built on an encoder-decoder architecture, where the encoder employs convolution layers to encode the box geometry and precedence graph and the decoder is a recurrent neural network (RNN) which inputs the current encoder output, as well as the current box packing state of the target container, and outputs the next box to pack, as well as its orientation. We train our network on randomly generated initial box configurations, without supervision, via policy gradients to learn optimal TAP policies to maximize packing efficiency and stability. We demonstrate the performance of TAP-Net on a variety of examples, evaluating the network through ablation studies and comparisons to baselines and alternative network designs. We also show that our network generalizes well to larger problem instances, when trained on small-sized inputs.



### SAC: Semantic Attention Composition for Text-Conditioned Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2009.01485v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2009.01485v2)
- **Published**: 2020-09-03 06:55:23+00:00
- **Updated**: 2021-10-19 19:02:15+00:00
- **Authors**: Surgan Jandial, Pinkesh Badjatiya, Pranit Chawla, Ayush Chopra, Mausoom Sarkar, Balaji Krishnamurthy
- **Comment**: Surgan Jandial, Pinkesh Badjatiya, Pranit Chawla, and Ayush Chopra
  contributed equally to this work. Work accepted at WACV 2022
- **Journal**: None
- **Summary**: The ability to efficiently search for images is essential for improving the user experiences across various products. Incorporating user feedback, via multi-modal inputs, to navigate visual search can help tailor retrieved results to specific user queries. We focus on the task of text-conditioned image retrieval that utilizes support text feedback alongside a reference image to retrieve images that concurrently satisfy constraints imposed by both inputs. The task is challenging since it requires learning composite image-text features by incorporating multiple cross-granular semantic edits from text feedback and then applying the same to visual features. To address this, we propose a novel framework SAC which resolves the above in two major steps: "where to see" (Semantic Feature Attention) and "how to change" (Semantic Feature Modification). We systematically show how our architecture streamlines the generation of text-aware image features by removing the need for various modules required by other state-of-art techniques. We present extensive quantitative, qualitative analysis, and ablation studies, to show that our architecture SAC outperforms existing techniques by achieving state-of-the-art performance on 3 benchmark datasets: FashionIQ, Shoes, and Birds-to-Words, while supporting natural language feedback of varying lengths.



### TopoMap: A 0-dimensional Homology Preserving Projection of High-Dimensional Data
- **Arxiv ID**: http://arxiv.org/abs/2009.01512v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CG, cs.CV, cs.HC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.01512v1)
- **Published**: 2020-09-03 08:30:02+00:00
- **Updated**: 2020-09-03 08:30:02+00:00
- **Authors**: Harish Doraiswamy, Julien Tierny, Paulo J. S. Silva, Luis Gustavo Nonato, Claudio Silva
- **Comment**: None
- **Journal**: None
- **Summary**: Multidimensional Projection is a fundamental tool for high-dimensional data analytics and visualization. With very few exceptions, projection techniques are designed to map data from a high-dimensional space to a visual space so as to preserve some dissimilarity (similarity) measure, such as the Euclidean distance for example. In fact, although adopting distinct mathematical formulations designed to favor different aspects of the data, most multidimensional projection methods strive to preserve dissimilarity measures that encapsulate geometric properties such as distances or the proximity relation between data objects. However, geometric relations are not the only interesting property to be preserved in a projection. For instance, the analysis of particular structures such as clusters and outliers could be more reliably performed if the mapping process gives some guarantee as to topological invariants such as connected components and loops. This paper introduces TopoMap, a novel projection technique which provides topological guarantees during the mapping process. In particular, the proposed method performs the mapping from a high-dimensional space to a visual space, while preserving the 0-dimensional persistence diagram of the Rips filtration of the high-dimensional data, ensuring that the filtrations generate the same connected components when applied to the original as well as projected data. The presented case studies show that the topological guarantee provided by TopoMap not only brings confidence to the visual analytic process but also can be used to assist in the assessment of other projection methods.



### A Comparison of Pre-trained Vision-and-Language Models for Multimodal Representation Learning across Medical Images and Reports
- **Arxiv ID**: http://arxiv.org/abs/2009.01523v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.01523v1)
- **Published**: 2020-09-03 09:00:47+00:00
- **Updated**: 2020-09-03 09:00:47+00:00
- **Authors**: Yikuan Li, Hanyin Wang, Yuan Luo
- **Comment**: 10 pages, 3 figures, submitted to BIBM2020
- **Journal**: None
- **Summary**: Joint image-text embedding extracted from medical images and associated contextual reports is the bedrock for most biomedical vision-and-language (V+L) tasks, including medical visual question answering, clinical image-text retrieval, clinical report auto-generation. In this study, we adopt four pre-trained V+L models: LXMERT, VisualBERT, UNIER and PixelBERT to learn multimodal representation from MIMIC-CXR radiographs and associated reports. The extrinsic evaluation on OpenI dataset shows that in comparison to the pioneering CNN-RNN model, the joint embedding learned by pre-trained V+L models demonstrate performance improvement in the thoracic findings classification task. We conduct an ablation study to analyze the contribution of certain model components and validate the advantage of joint embedding over text-only embedding. We also visualize attention maps to illustrate the attention mechanism of V+L models.



### Multi-Attention-Network for Semantic Segmentation of Fine Resolution Remote Sensing Images
- **Arxiv ID**: http://arxiv.org/abs/2009.02130v4
- **DOI**: 10.1109/TGRS.2021.3093977
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.02130v4)
- **Published**: 2020-09-03 09:08:02+00:00
- **Updated**: 2020-11-23 12:56:55+00:00
- **Authors**: Rui Li, Shunyi Zheng, Chenxi Duan, Ce Zhang, Jianlin Su, P. M. Atkinson
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2007.14902
- **Journal**: None
- **Summary**: Semantic segmentation of remote sensing images plays an important role in a wide range of applications including land resource management, biosphere monitoring and urban planning. Although the accuracy of semantic segmentation in remote sensing images has been increased significantly by deep convolutional neural networks, several limitations exist in standard models. First, for encoder-decoder architectures such as U-Net, the utilization of multi-scale features causes the underuse of information, where low-level features and high-level features are concatenated directly without any refinement. Second, long-range dependencies of feature maps are insufficiently explored, resulting in sub-optimal feature representations associated with each semantic class. Third, even though the dot-product attention mechanism has been introduced and utilized in semantic segmentation to model long-range dependencies, the large time and space demands of attention impede the actual usage of attention in application scenarios with large-scale input. This paper proposed a Multi-Attention-Network (MANet) to address these issues by extracting contextual dependencies through multiple efficient attention modules. A novel attention mechanism of kernel attention with linear complexity is proposed to alleviate the large computational demand in attention. Based on kernel attention and channel attention, we integrate local feature maps extracted by ResNeXt-101 with their corresponding global dependencies and reweight interdependent channel maps adaptively. Numerical experiments on three large-scale fine resolution remote sensing images captured by different satellite sensors demonstrate the superior performance of the proposed MANet, outperforming the DeepLab V3+, PSPNet, FastFCN, DANet, OCRNet, and other benchmark approaches.



### Physics-based Shading Reconstruction for Intrinsic Image Decomposition
- **Arxiv ID**: http://arxiv.org/abs/2009.01540v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.01540v1)
- **Published**: 2020-09-03 09:30:17+00:00
- **Updated**: 2020-09-03 09:30:17+00:00
- **Authors**: Anil S. Baslamisli, Yang Liu, Sezer Karaoglu, Theo Gevers
- **Comment**: Submitted to Computer Vision and Image Understanding (CVIU)
- **Journal**: None
- **Summary**: We investigate the use of photometric invariance and deep learning to compute intrinsic images (albedo and shading). We propose albedo and shading gradient descriptors which are derived from physics-based models. Using the descriptors, albedo transitions are masked out and an initial sparse shading map is calculated directly from the corresponding RGB image gradients in a learning-free unsupervised manner. Then, an optimization method is proposed to reconstruct the full dense shading map. Finally, we integrate the generated shading map into a novel deep learning framework to refine it and also to predict corresponding albedo image to achieve intrinsic image decomposition. By doing so, we are the first to directly address the texture and intensity ambiguity problems of the shading estimations. Large scale experiments show that our approach steered by physics-based invariant descriptors achieve superior results on MIT Intrinsics, NIR-RGB Intrinsics, Multi-Illuminant Intrinsic Images, Spectral Intrinsic Images, As Realistic As Possible, and competitive results on Intrinsic Images in the Wild datasets while achieving state-of-the-art shading estimations.



### Fundus Image Analysis for Age Related Macular Degeneration: ADAM-2020 Challenge Report
- **Arxiv ID**: http://arxiv.org/abs/2009.01548v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.01548v1)
- **Published**: 2020-09-03 09:46:32+00:00
- **Updated**: 2020-09-03 09:46:32+00:00
- **Authors**: Sharath M Shankaranarayana
- **Comment**: None
- **Journal**: None
- **Summary**: Age related macular degeneration (AMD) is one of the major causes for blindness in the elderly population. In this report, we propose deep learning based methods for retinal analysis using color fundus images for computer aided diagnosis of AMD. We leverage the recent state of the art deep networks for building a single fundus image based AMD classification pipeline. We also propose methods for the other directly relevant and auxiliary tasks such as lesions detection and segmentation, fovea detection and optic disc segmentation. We propose the use of generative adversarial networks (GANs) for the tasks of segmentation and detection. We also propose a novel method of fovea detection using GANs.



### 1st Place Solution of LVIS Challenge 2020: A Good Box is not a Guarantee of a Good Mask
- **Arxiv ID**: http://arxiv.org/abs/2009.01559v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.01559v1)
- **Published**: 2020-09-03 10:09:44+00:00
- **Updated**: 2020-09-03 10:09:44+00:00
- **Authors**: Jingru Tan, Gang Zhang, Hanming Deng, Changbao Wang, Lewei Lu, Quanquan Li, Jifeng Dai
- **Comment**: Winner of LVIS challenge 2020
- **Journal**: None
- **Summary**: This article introduces the solutions of the team lvisTraveler for LVIS Challenge 2020. In this work, two characteristics of LVIS dataset are mainly considered: the long-tailed distribution and high quality instance segmentation mask. We adopt a two-stage training pipeline. In the first stage, we incorporate EQL and self-training to learn generalized representation. In the second stage, we utilize Balanced GroupSoftmax to promote the classifier, and propose a novel proposal assignment strategy and a new balanced mask loss for mask head to get more precise mask predictions. Finally, we achieve 41.5 and 41.2 AP on LVIS v1.0 val and test-dev splits respectively, outperforming the baseline based on X101-FPN-MaskRCNN by a large margin.



### Detection-Aware Trajectory Generation for a Drone Cinematographer
- **Arxiv ID**: http://arxiv.org/abs/2009.01565v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.01565v1)
- **Published**: 2020-09-03 10:27:56+00:00
- **Updated**: 2020-09-03 10:27:56+00:00
- **Authors**: Boseong Felipe Jeon, Dongseok Shim, H. Jin Kim
- **Comment**: 8 pages, IROS 2020 accepted
- **Journal**: None
- **Summary**: This work investigates an efficient trajectory generation for chasing a dynamic target, which incorporates the detectability objective. The proposed method actively guides the motion of a cinematographer drone so that the color of a target is well-distinguished against the colors of the background in the view of the drone. For the objective, we define a measure of color detectability given a chasing path. After computing a discrete path optimized for the metric, we generate a dynamically feasible trajectory. The whole pipeline can be updated on-the-fly to respond to the motion of the target. For the efficient discrete path generation, we construct a directed acyclic graph (DAG) for which a topological sorting can be determined analytically without the depth-first search. The smooth path is obtained in quadratic programming (QP) framework. We validate the enhanced performance of state-of-the-art object detection and tracking algorithms when the camera drone executes the trajectory obtained from the proposed method.



### Auto-Classifier: A Robust Defect Detector Based on an AutoML Head
- **Arxiv ID**: http://arxiv.org/abs/2009.01573v1
- **DOI**: 10.1007/978-3-030-63830-6_12
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2009.01573v1)
- **Published**: 2020-09-03 10:39:02+00:00
- **Updated**: 2020-09-03 10:39:02+00:00
- **Authors**: Vasco Lopes, Luís A. Alexandre
- **Comment**: 12 pages, 2 figures. Published in ICONIP2020, proceedings published
  in the Springer's series of Lecture Notes in Computer Science
- **Journal**: None
- **Summary**: The dominant approach for surface defect detection is the use of hand-crafted feature-based methods. However, this falls short when conditions vary that affect extracted images. So, in this paper, we sought to determine how well several state-of-the-art Convolutional Neural Networks perform in the task of surface defect detection. Moreover, we propose two methods: CNN-Fusion, that fuses the prediction of all the networks into a final one, and Auto-Classifier, which is a novel proposal that improves a Convolutional Neural Network by modifying its classification component using AutoML. We carried out experiments to evaluate the proposed methods in the task of surface defect detection using different datasets from DAGM2007. We show that the use of Convolutional Neural Networks achieves better results than traditional methods, and also, that Auto-Classifier out-performs all other methods, by achieving 100% accuracy and 100% AUC results throughout all the datasets.



### LiPo-LCD: Combining Lines and Points for Appearance-based Loop Closure Detection
- **Arxiv ID**: http://arxiv.org/abs/2009.09897v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2009.09897v1)
- **Published**: 2020-09-03 10:43:16+00:00
- **Updated**: 2020-09-03 10:43:16+00:00
- **Authors**: Joan P. Company-Corcoles, Emilio Garcia-Fidalgo, Alberto Ortiz
- **Comment**: British Machine Vision Conference (BMVC) 2020
- **Journal**: None
- **Summary**: Visual SLAM approaches typically depend on loop closure detection to correct the inconsistencies that may arise during the map and camera trajectory calculations, typically making use of point features for detecting and closing the existing loops. In low-textured scenarios, however, it is difficult to find enough point features and, hence, the performance of these solutions drops drastically. An alternative for human-made scenarios, due to their structural regularity, is the use of geometrical cues such as straight segments, frequently present within these environments. Under this context, in this paper we introduce LiPo-LCD, a novel appearance-based loop closure detection method that integrates lines and points. Adopting the idea of incremental Bag-of-Binary-Words schemes, we build separate BoW models for each feature, and use them to retrieve previously seen images using a late fusion strategy. Additionally, a simple but effective mechanism, based on the concept of island, groups similar images close in time to reduce the image candidate search effort. A final step validates geometrically the loop candidates by incorporating the detected lines by means of a process comprising a line feature matching stage, followed by a robust spatial verification stage, now combining both lines and points. As it is reported in the paper, LiPo-LCD compares well with several state-of-the-art solutions for a number of datasets involving different environmental conditions.



### DESC: Domain Adaptation for Depth Estimation via Semantic Consistency
- **Arxiv ID**: http://arxiv.org/abs/2009.01579v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.01579v1)
- **Published**: 2020-09-03 10:54:05+00:00
- **Updated**: 2020-09-03 10:54:05+00:00
- **Authors**: Adrian Lopez-Rodriguez, Krystian Mikolajczyk
- **Comment**: BMVC20 (Oral). Code: https://github.com/alopezgit/DESC
- **Journal**: None
- **Summary**: Accurate real depth annotations are difficult to acquire, needing the use of special devices such as a LiDAR sensor. Self-supervised methods try to overcome this problem by processing video or stereo sequences, which may not always be available. Instead, in this paper, we propose a domain adaptation approach to train a monocular depth estimation model using a fully-annotated source dataset and a non-annotated target dataset. We bridge the domain gap by leveraging semantic predictions and low-level edge features to provide guidance for the target domain. We enforce consistency between the main model and a second model trained with semantic segmentation and edge maps, and introduce priors in the form of instance heights. Our approach is evaluated on standard domain adaptation benchmarks for monocular depth estimation and show consistent improvement upon the state-of-the-art.



### Layer-specific Optimization for Mixed Data Flow with Mixed Precision in FPGA Design for CNN-based Object Detectors
- **Arxiv ID**: http://arxiv.org/abs/2009.01588v1
- **DOI**: 10.1109/TCSVT.2020.3020569
- **Categories**: **cs.CV**, cs.AR, cs.DC
- **Links**: [PDF](http://arxiv.org/pdf/2009.01588v1)
- **Published**: 2020-09-03 11:27:40+00:00
- **Updated**: 2020-09-03 11:27:40+00:00
- **Authors**: Duy Thanh Nguyen, Hyun Kim, Hyuk-Jae Lee
- **Comment**: Accepted for publication in IEEE Transaction on Circuit and System
  for Video Technology
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) require both intensive computation and frequent memory access, which lead to a low processing speed and large power dissipation. Although the characteristics of the different layers in a CNN are frequently quite different, previous hardware designs have employed common optimization schemes for them. This paper proposes a layer-specific design that employs different organizations that are optimized for the different layers. The proposed design employs two layer-specific optimizations: layer-specific mixed data flow and layer-specific mixed precision. The mixed data flow aims to minimize the off-chip access while demanding a minimal on-chip memory (BRAM) resource of an FPGA device. The mixed precision quantization is to achieve both a lossless accuracy and an aggressive model compression, thereby further reducing the off-chip access. A Bayesian optimization approach is used to select the best sparsity for each layer, achieving the best trade-off between the accuracy and compression. This mixing scheme allows the entire network model to be stored in BRAMs of the FPGA to aggressively reduce the off-chip access, and thereby achieves a significant performance enhancement. The model size is reduced by 22.66-28.93 times compared to that in a full-precision network with a negligible degradation of accuracy on VOC, COCO, and ImageNet datasets. Furthermore, the combination of mixed dataflow and mixed precision significantly outperforms the previous works in terms of both throughput, off-chip access, and on-chip memory requirement.



### Multimodal brain tumor classification
- **Arxiv ID**: http://arxiv.org/abs/2009.01592v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.01592v2)
- **Published**: 2020-09-03 11:41:50+00:00
- **Updated**: 2020-10-06 16:05:22+00:00
- **Authors**: Marvin Lerousseau, Eric Deutsh, Nikos Paragios
- **Comment**: None
- **Journal**: None
- **Summary**: Cancer is a complex disease that provides various types of information depending on the scale of observation. While most tumor diagnostics are performed by observing histopathological slides, radiology images should yield additional knowledge towards the efficacy of cancer diagnostics. This work investigates a deep learning method combining whole slide images and magnetic resonance images to classify tumors. In particular, our solution comprises a powerful, generic and modular architecture for whole slide image classification. Experiments are prospectively conducted on the 2020 Computational Precision Medicine challenge, in a 3-classes unbalanced classification task. We report cross-validation (resp. validation) balanced-accuracy, kappa and f1 of 0.913, 0.897 and 0.951 (resp. 0.91, 0.90 and 0.94). For research purposes, including reproducibility and direct performance comparisons, our finale submitted models are usable off-the-shelf in a Docker image available at https://hub.docker.com/repository/docker/marvinler/cpm_2020_marvinler.



### SCG-Net: Self-Constructing Graph Neural Networks for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2009.01599v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.01599v2)
- **Published**: 2020-09-03 12:13:09+00:00
- **Updated**: 2021-01-03 14:59:24+00:00
- **Authors**: Qinghui Liu, Michael Kampffmeyer, Robert Jenssen, Arnt-Børre Salberg
- **Comment**: 11 pages, 5 figs. code will be open soon
- **Journal**: None
- **Summary**: Capturing global contextual representations by exploiting long-range pixel-pixel dependencies has shown to improve semantic segmentation performance. However, how to do this efficiently is an open question as current approaches of utilising attention schemes or very deep models to increase the models field of view, result in complex models with large memory consumption. Inspired by recent work on graph neural networks, we propose the Self-Constructing Graph (SCG) module that learns a long-range dependency graph directly from the image and uses it to propagate contextual information efficiently to improve semantic segmentation. The module is optimised via a novel adaptive diagonal enhancement method and a variational lower bound that consists of a customized graph reconstruction term and a Kullback-Leibler divergence regularization term. When incorporated into a neural network (SCG-Net), semantic segmentation is performed in an end-to-end manner and competitive performance (mean F1-scores of 92.0% and 89.8% respectively) on the publicly available ISPRS Potsdam and Vaihingen datasets is achieved, with much fewer parameters, and at a lower computational cost compared to related pure convolutional neural network (CNN) based models.



### Heightmap Reconstruction of Macula on Color Fundus Images Using Conditional Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2009.01601v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.01601v4)
- **Published**: 2020-09-03 12:13:51+00:00
- **Updated**: 2020-12-28 07:27:35+00:00
- **Authors**: Peyman Tahghighi, Reza A. Zoroofi, Sare Safi, Alireza Ramezani
- **Comment**: None
- **Journal**: None
- **Summary**: For screening, 3D shape of the eye retina often provides structural information and can assist ophthalmologists to diagnose diseases. However, fundus images which are one the most common screening modalities for retina diagnosis lack this information due to their 2D nature. Hence, in this work, we try to infer about this 3D information or more specifically its heights. Recent approaches have used shading information for reconstructing the heights but their output is not accurate since the utilized information is not sufficient. Additionally, other methods were dependent on the availability of more than one image of the eye which is not available in practice. In this paper, motivated by the success of Conditional Generative Adversarial Networks(cGANs) and deeply supervised networks, we propose a novel architecture for the generator which enhances the details in a sequence of steps. Comparisons on our dataset illustrate that the proposed method outperforms all of the state-of-the-art methods in image translation and medical image translation on this particular task. Additionally, clinical studies also indicate that the proposed method can provide additional information for ophthalmologists for diagnosis.



### Few-shot Object Detection with Feature Attention Highlight Module in Remote Sensing Images
- **Arxiv ID**: http://arxiv.org/abs/2009.01616v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.01616v1)
- **Published**: 2020-09-03 12:38:49+00:00
- **Updated**: 2020-09-03 12:38:49+00:00
- **Authors**: Zixuan Xiao, Ping Zhong, Yuan Quan, Xuping Yin, Wei Xue
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, there are many applications of object detection in remote sensing field, which demands a great number of labeled data. However, in many cases, data is extremely rare. In this paper, we proposed a few-shot object detector which is designed for detecting novel objects based on only a few examples. Through fully leveraging labeled base classes, our model that is composed of a feature-extractor, a feature attention highlight module as well as a two-stage detection backend can quickly adapt to novel classes. The pre-trained feature extractor whose parameters are shared produces general features. While the feature attention highlight module is designed to be light-weighted and simple in order to fit the few-shot cases. Although it is simple, the information provided by it in a serial way is helpful to make the general features to be specific for few-shot objects. Then the object-specific features are delivered to the two-stage detection backend for the detection results. The experiments demonstrate the effectiveness of the proposed method for few-shot cases.



### Modification method for single-stage object detectors that allows to exploit the temporal behaviour of a scene to improve detection accuracy
- **Arxiv ID**: http://arxiv.org/abs/2009.01617v1
- **DOI**: None
- **Categories**: **cs.CV**, 68T45, 68T07, I.4.8; I.4.9; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2009.01617v1)
- **Published**: 2020-09-03 12:38:55+00:00
- **Updated**: 2020-09-03 12:38:55+00:00
- **Authors**: Menua Gevorgyan
- **Comment**: 5 pages, 5 figures
- **Journal**: None
- **Summary**: A simple modification method for single-stage generic object detection neural networks, such as YOLO and SSD, is proposed, which allows for improving the detection accuracy on video data by exploiting the temporal behavior of the scene in the detection pipeline. It is shown that, using this method, the detection accuracy of the base network can be considerably improved, especially for occluded and hidden objects. It is shown that a modified network is more prone to detect hidden objects with more confidence than an unmodified one. A weakly supervised training method is proposed, which allows for training a modified network without requiring any additional annotated data.



### Multi-Loss Weighting with Coefficient of Variations
- **Arxiv ID**: http://arxiv.org/abs/2009.01717v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, 68T45, I.4
- **Links**: [PDF](http://arxiv.org/pdf/2009.01717v2)
- **Published**: 2020-09-03 14:51:19+00:00
- **Updated**: 2020-11-10 10:41:03+00:00
- **Authors**: Rick Groenendijk, Sezer Karaoglu, Theo Gevers, Thomas Mensink
- **Comment**: Paper was accepted at the IEEE Winter Conference on Applications of
  Computer Vision 2021 (WACV2021)
- **Journal**: None
- **Summary**: Many interesting tasks in machine learning and computer vision are learned by optimising an objective function defined as a weighted linear combination of multiple losses. The final performance is sensitive to choosing the correct (relative) weights for these losses. Finding a good set of weights is often done by adopting them into the set of hyper-parameters, which are set using an extensive grid search. This is computationally expensive. In this paper, we propose a weighting scheme based on the coefficient of variations and set the weights based on properties observed while training the model. The proposed method incorporates a measure of uncertainty to balance the losses, and as a result the loss weights evolve during training without requiring another (learning based) optimisation. In contrast to many loss weighting methods in literature, we focus on single-task multi-loss problems, such as monocular depth estimation and semantic segmentation, and show that multi-task approaches for loss weighting do not work on those single-tasks. The validity of the approach is shown empirically for depth estimation and semantic segmentation on multiple datasets.



### MIPGAN -- Generating Strong and High Quality Morphing Attacks Using Identity Prior Driven GAN
- **Arxiv ID**: http://arxiv.org/abs/2009.01729v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/2009.01729v3)
- **Published**: 2020-09-03 15:08:38+00:00
- **Updated**: 2021-04-07 11:02:03+00:00
- **Authors**: Haoyu Zhang, Sushma Venkatesh, Raghavendra Ramachandra, Kiran Raja, Naser Damer, Christoph Busch
- **Comment**: Revised version. Submitted to IEEE T-BIOM 2020
- **Journal**: None
- **Summary**: Face morphing attacks target to circumvent Face Recognition Systems (FRS) by employing face images derived from multiple data subjects (e.g., accomplices and malicious actors). Morphed images can be verified against contributing data subjects with a reasonable success rate, given they have a high degree of facial resemblance. The success of morphing attacks is directly dependent on the quality of the generated morph images. We present a new approach for generating strong attacks extending our earlier framework for generating face morphs. We present a new approach using an Identity Prior Driven Generative Adversarial Network, which we refer to as MIPGAN (Morphing through Identity Prior driven GAN). The proposed MIPGAN is derived from the StyleGAN with a newly formulated loss function exploiting perceptual quality and identity factor to generate a high quality morphed facial image with minimal artefacts and with high resolution. We demonstrate the proposed approach's applicability to generate strong morphing attacks by evaluating its vulnerability against both commercial and deep learning based Face Recognition System (FRS) and demonstrate the success rate of attacks. Extensive experiments are carried out to assess the FRS's vulnerability against the proposed morphed face generation technique on three types of data such as digital images, re-digitized (printed and scanned) images, and compressed images after re-digitization from newly generated MIPGAN Face Morph Dataset. The obtained results demonstrate that the proposed approach of morph generation poses a high threat to FRS.



### Synthetic-to-Real Unsupervised Domain Adaptation for Scene Text Detection in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2009.01766v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2009.01766v1)
- **Published**: 2020-09-03 16:16:34+00:00
- **Updated**: 2020-09-03 16:16:34+00:00
- **Authors**: Weijia Wu, Ning Lu, Enze Xie
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning-based scene text detection can achieve preferable performance, powered with sufficient labeled training data. However, manual labeling is time consuming and laborious. At the extreme, the corresponding annotated data are unavailable. Exploiting synthetic data is a very promising solution except for domain distribution mismatches between synthetic datasets and real datasets. To address the severe domain distribution mismatch, we propose a synthetic-to-real domain adaptation method for scene text detection, which transfers knowledge from synthetic data (source domain) to real data (target domain). In this paper, a text self-training (TST) method and adversarial text instance alignment (ATA) for domain adaptive scene text detection are introduced. ATA helps the network learn domain-invariant features by training a domain classifier in an adversarial manner. TST diminishes the adverse effects of false positives~(FPs) and false negatives~(FNs) from inaccurate pseudo-labels. Two components have positive effects on improving the performance of scene text detectors when adapting from synthetic-to-real scenes. We evaluate the proposed method by transferring from SynthText, VISD to ICDAR2015, ICDAR2013. The results demonstrate the effectiveness of the proposed method with up to 10% improvement, which has important exploration significance for domain adaptive scene text detection. Code is available at https://github.com/weijiawu/SyntoReal_STD



### Limited View Tomographic Reconstruction Using a Deep Recurrent Framework with Residual Dense Spatial-Channel Attention Network and Sinogram Consistency
- **Arxiv ID**: http://arxiv.org/abs/2009.01782v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.01782v1)
- **Published**: 2020-09-03 16:39:48+00:00
- **Updated**: 2020-09-03 16:39:48+00:00
- **Authors**: Bo Zhou, S. Kevin Zhou, James S. Duncan, Chi Liu
- **Comment**: Submitted to the IEEE for possible publication
- **Journal**: None
- **Summary**: Limited view tomographic reconstruction aims to reconstruct a tomographic image from a limited number of sinogram or projection views arising from sparse view or limited angle acquisitions that reduce radiation dose or shorten scanning time. However, such a reconstruction suffers from high noise and severe artifacts due to the incompleteness of sinogram. To derive quality reconstruction, previous state-of-the-art methods use UNet-like neural architectures to directly predict the full view reconstruction from limited view data; but these methods leave the deep network architecture issue largely intact and cannot guarantee the consistency between the sinogram of the reconstructed image and the acquired sinogram, leading to a non-ideal reconstruction. In this work, we propose a novel recurrent reconstruction framework that stacks the same block multiple times. The recurrent block consists of a custom-designed residual dense spatial-channel attention network. Further, we develop a sinogram consistency layer interleaved in our recurrent framework in order to ensure that the sampled sinogram is consistent with the sinogram of the intermediate outputs of the recurrent blocks. We evaluate our methods on two datasets. Our experimental results on AAPM Low Dose CT Grand Challenge datasets demonstrate that our algorithm achieves a consistent and significant improvement over the existing state-of-the-art neural methods on both limited angle reconstruction (over 5dB better in terms of PSNR) and sparse view reconstruction (about 4dB better in term of PSNR). In addition, our experimental results on Deep Lesion datasets demonstrate that our method is able to generate high-quality reconstruction for 8 major lesion types.



### Computational Analysis of Deformable Manifolds: from Geometric Modelling to Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2009.01786v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NA, math.NA
- **Links**: [PDF](http://arxiv.org/pdf/2009.01786v1)
- **Published**: 2020-09-03 16:50:48+00:00
- **Updated**: 2020-09-03 16:50:48+00:00
- **Authors**: Stefan C Schonsheck
- **Comment**: PhD Thesis, Versions of several chapters have previously appeard or
  been submitted under different titles
- **Journal**: None
- **Summary**: Leo Tolstoy opened his monumental novel Anna Karenina with the now famous words: Happy families are all alike; every unhappy family is unhappy in its own way A similar notion also applies to mathematical spaces: Every flat space is alike; every unflat space is unflat in its own way. However, rather than being a source of unhappiness, we will show that the diversity of non-flat spaces provides a rich area of study. The genesis of the so-called big data era and the proliferation of social and scientific databases of increasing size has led to a need for algorithms that can efficiently process, analyze and, even generate high dimensional data. However, the curse of dimensionality leads to the fact that many classical approaches do not scale well with respect to the size of these problems. One technique to avoid some of these ill-effects is to exploit the geometric structure of coherent data. In this thesis, we will explore geometric methods for shape processing and data analysis. More specifically, we will study techniques for representing manifolds and signals supported on them through a variety of mathematical tools including, but not limited to, computational differential geometry, variational PDE modeling, and deep learning. First, we will explore non-isometric shape matching through variational modeling. Next, we will use ideas from parallel transport on manifolds to generalize convolution and convolutional neural networks to deformable manifolds. Finally, we conclude by proposing a novel auto-regressive model for capturing the intrinsic geometry and topology of data. Throughout this work, we will use the idea of computing correspondences as a though-line to both motivate our work and analyze our results.



### CNN-Based Ultrasound Image Reconstruction for Ultrafast Displacement Tracking
- **Arxiv ID**: http://arxiv.org/abs/2009.01816v2
- **DOI**: 10.1109/TMI.2020.3046700
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.01816v2)
- **Published**: 2020-09-03 17:31:44+00:00
- **Updated**: 2020-12-21 17:56:40+00:00
- **Authors**: Dimitris Perdios, Manuel Vonlanthen, Florian Martinez, Marcel Arditi, Jean-Philippe Thiran
- **Comment**: 11 pages, 4 figures. Animation and slideshow of Figure 4 are provided
  as ancillary files. This version has been accepted for publication in the
  IEEE Transactions on Medical Imaging
- **Journal**: None
- **Summary**: Thanks to its capability of acquiring full-view frames at multiple kilohertz, ultrafast ultrasound imaging unlocked the analysis of rapidly changing physical phenomena in the human body, with pioneering applications such as ultrasensitive flow imaging in the cardiovascular system or shear-wave elastography. The accuracy achievable with these motion estimation techniques is strongly contingent upon two contradictory requirements: a high quality of consecutive frames and a high frame rate. Indeed, the image quality can usually be improved by increasing the number of steered ultrafast acquisitions, but at the expense of a reduced frame rate and possible motion artifacts. To achieve accurate motion estimation at uncompromised frame rates and immune to motion artifacts, the proposed approach relies on single ultrafast acquisitions to reconstruct high-quality frames and on only two consecutive frames to obtain 2-D displacement estimates. To this end, we deployed a convolutional neural network-based image reconstruction method combined with a speckle tracking algorithm based on cross-correlation. Numerical and in vivo experiments, conducted in the context of plane-wave imaging, demonstrate that the proposed approach is capable of estimating displacements in regions where the presence of side lobe and grating lobe artifacts prevents any displacement estimation with a state-of-the-art technique that relies on conventional delay-and-sum beamforming. The proposed approach may therefore unlock the full potential of ultrafast ultrasound, in applications such as ultrasensitive cardiovascular motion and flow analysis or shear-wave elastography.



### Flow-edge Guided Video Completion
- **Arxiv ID**: http://arxiv.org/abs/2009.01835v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.01835v1)
- **Published**: 2020-09-03 17:59:42+00:00
- **Updated**: 2020-09-03 17:59:42+00:00
- **Authors**: Chen Gao, Ayush Saraf, Jia-Bin Huang, Johannes Kopf
- **Comment**: ECCV 2020. Project: http://chengao.vision/FGVC/
- **Journal**: None
- **Summary**: We present a new flow-based video completion algorithm. Previous flow completion methods are often unable to retain the sharpness of motion boundaries. Our method first extracts and completes motion edges, and then uses them to guide piecewise-smooth flow completion with sharp edges. Existing methods propagate colors among local flow connections between adjacent frames. However, not all missing regions in a video can be reached in this way because the motion boundaries form impenetrable barriers. Our method alleviates this problem by introducing non-local flow connections to temporally distant frames, enabling propagating video content over motion boundaries. We validate our approach on the DAVIS dataset. Both visual and quantitative results show that our method compares favorably against the state-of-the-art algorithms.



### A general approach to bridge the reality-gap
- **Arxiv ID**: http://arxiv.org/abs/2009.01865v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.01865v1)
- **Published**: 2020-09-03 18:19:28+00:00
- **Updated**: 2020-09-03 18:19:28+00:00
- **Authors**: Michael Lomnitz, Zigfried Hampel-Arias, Nina Lopatina, Felipe A. Mejia
- **Comment**: 8 pages, 4 figures, 2 tables
- **Journal**: None
- **Summary**: Employing machine learning models in the real world requires collecting large amounts of data, which is both time consuming and costly to collect. A common approach to circumvent this is to leverage existing, similar data-sets with large amounts of labelled data. However, models trained on these canonical distributions do not readily transfer to real-world ones. Domain adaptation and transfer learning are often used to breach this "reality gap", though both require a substantial amount of real-world data. In this paper we discuss a more general approach: we propose learning a general transformation to bring arbitrary images towards a canonical distribution where we can naively apply the trained machine learning models. This transformation is trained in an unsupervised regime, leveraging data augmentation to generate off-canonical examples of images and training a Deep Learning model to recover their original counterpart. We quantify the performance of this transformation using pre-trained ImageNet classifiers, demonstrating that this procedure can recover half of the loss in performance on the distorted data-set. We then validate the effectiveness of this approach on a series of pre-trained ImageNet models on a real world data set collected by printing and photographing images in different lighting conditions.



### ESMFL: Efficient and Secure Models for Federated Learning
- **Arxiv ID**: http://arxiv.org/abs/2009.01867v2
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.01867v2)
- **Published**: 2020-09-03 18:27:32+00:00
- **Updated**: 2021-03-03 19:45:00+00:00
- **Authors**: Sheng Lin, Chenghong Wang, Hongjia Li, Jieren Deng, Yanzhi Wang, Caiwen Ding
- **Comment**: 7 pages, 3 figures, accepted by NeurIPS Workshop 2020, SpicyFL
- **Journal**: None
- **Summary**: Nowadays, Deep Neural Networks are widely applied to various domains. However, massive data collection required for deep neural network reveals the potential privacy issues and also consumes large mounts of communication bandwidth. To address these problems, we propose a privacy-preserving method for the federated learning distributed system, operated on Intel Software Guard Extensions, a set of instructions that increase the security of application code and data. Meanwhile, the encrypted models make the transmission overhead larger. Hence, we reduce the commutation cost by sparsification and it can achieve reasonable accuracy with different model architectures.



### Federated Learning for Breast Density Classification: A Real-World Implementation
- **Arxiv ID**: http://arxiv.org/abs/2009.01871v3
- **DOI**: 10.1007/978-3-030-60548-3_18
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.01871v3)
- **Published**: 2020-09-03 18:34:59+00:00
- **Updated**: 2020-10-20 13:46:55+00:00
- **Authors**: Holger R. Roth, Ken Chang, Praveer Singh, Nir Neumark, Wenqi Li, Vikash Gupta, Sharut Gupta, Liangqiong Qu, Alvin Ihsani, Bernardo C. Bizzo, Yuhong Wen, Varun Buch, Meesam Shah, Felipe Kitamura, Matheus Mendonça, Vitor Lavor, Ahmed Harouni, Colin Compas, Jesse Tetreault, Prerna Dogra, Yan Cheng, Selnur Erdal, Richard White, Behrooz Hashemian, Thomas Schultz, Miao Zhang, Adam McCarthy, B. Min Yun, Elshaimaa Sharaf, Katharina V. Hoebel, Jay B. Patel, Bryan Chen, Sean Ko, Evan Leibovitz, Etta D. Pisano, Laura Coombs, Daguang Xu, Keith J. Dreyer, Ittai Dayan, Ram C. Naidu, Mona Flores, Daniel Rubin, Jayashree Kalpathy-Cramer
- **Comment**: Accepted at the 1st MICCAI Workshop on "Distributed And Collaborative
  Learning"; add citation to Fig. 1 & 2 and update Fig. 5; fix typo in
  affiliations
- **Journal**: In: Albarqouni S. et al. (eds) Domain Adaptation and
  Representation Transfer, and Distributed and Collaborative Learning. DART
  2020, DCL 2020. Lecture Notes in Computer Science, vol 12444. Springer, Cham
- **Summary**: Building robust deep learning-based models requires large quantities of diverse training data. In this study, we investigate the use of federated learning (FL) to build medical imaging classification models in a real-world collaborative setting. Seven clinical institutions from across the world joined this FL effort to train a model for breast density classification based on Breast Imaging, Reporting & Data System (BI-RADS). We show that despite substantial differences among the datasets from all sites (mammography system, class distribution, and data set size) and without centralizing data, we can successfully train AI models in federation. The results show that models trained using FL perform 6.3% on average better than their counterparts trained on an institute's local data alone. Furthermore, we show a 45.8% relative improvement in the models' generalizability when evaluated on the other participating sites' testing data.



### Depth Completion via Inductive Fusion of Planar LIDAR and Monocular Camera
- **Arxiv ID**: http://arxiv.org/abs/2009.01875v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2009.01875v1)
- **Published**: 2020-09-03 18:39:57+00:00
- **Updated**: 2020-09-03 18:39:57+00:00
- **Authors**: Chen Fu, Chiyu Dong, Christoph Mertz, John M. Dolan
- **Comment**: Accepted at IROS 2020
- **Journal**: None
- **Summary**: Modern high-definition LIDAR is expensive for commercial autonomous driving vehicles and small indoor robots. An affordable solution to this problem is fusion of planar LIDAR with RGB images to provide a similar level of perception capability. Even though state-of-the-art methods provide approaches to predict depth information from limited sensor input, they are usually a simple concatenation of sparse LIDAR features and dense RGB features through an end-to-end fusion architecture. In this paper, we introduce an inductive late-fusion block which better fuses different sensor modalities inspired by a probability model. The proposed demonstration and aggregation network propagates the mixed context and depth features to the prediction network and serves as a prior knowledge of the depth completion. This late-fusion block uses the dense context features to guide the depth prediction based on demonstrations by sparse depth features. In addition to evaluating the proposed method on benchmark depth completion datasets including NYUDepthV2 and KITTI, we also test the proposed method on a simulated planar LIDAR dataset. Our method shows promising results compared to previous approaches on both the benchmark datasets and simulated dataset with various 3D densities.



### The Little W-Net That Could: State-of-the-Art Retinal Vessel Segmentation with Minimalistic Models
- **Arxiv ID**: http://arxiv.org/abs/2009.01907v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.01907v1)
- **Published**: 2020-09-03 19:59:51+00:00
- **Updated**: 2020-09-03 19:59:51+00:00
- **Authors**: Adrian Galdran, André Anjos, José Dolz, Hadi Chakor, Hervé Lombaert, Ismail Ben Ayed
- **Comment**: None
- **Journal**: None
- **Summary**: The segmentation of the retinal vasculature from eye fundus images represents one of the most fundamental tasks in retinal image analysis. Over recent years, increasingly complex approaches based on sophisticated Convolutional Neural Network architectures have been slowly pushing performance on well-established benchmark datasets. In this paper, we take a step back and analyze the real need of such complexity. Specifically, we demonstrate that a minimalistic version of a standard U-Net with several orders of magnitude less parameters, carefully trained and rigorously evaluated, closely approximates the performance of current best techniques. In addition, we propose a simple extension, dubbed W-Net, which reaches outstanding performance on several popular datasets, still using orders of magnitude less learnable weights than any previously published approach. Furthermore, we provide the most comprehensive cross-dataset performance analysis to date, involving up to 10 different databases. Our analysis demonstrates that the retinal vessel segmentation problem is far from solved when considering test images that differ substantially from the training data, and that this task represents an ideal scenario for the exploration of domain adaptation techniques. In this context, we experiment with a simple self-labeling strategy that allows us to moderately enhance cross-dataset performance, indicating that there is still much room for improvement in this area. Finally, we also test our approach on the Artery/Vein segmentation problem, where we again achieve results well-aligned with the state-of-the-art, at a fraction of the model complexity in recent literature. All the code to reproduce the results in this paper is released.



### Compression-aware Continual Learning using Singular Value Decomposition
- **Arxiv ID**: http://arxiv.org/abs/2009.01956v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.01956v2)
- **Published**: 2020-09-03 23:29:50+00:00
- **Updated**: 2020-09-14 22:29:21+00:00
- **Authors**: Varigonda Pavan Teja, Priyadarshini Panda
- **Comment**: 13 pages, 2 figures, 4 tables
- **Journal**: None
- **Summary**: We propose a compression based continual task learning method that can dynamically grow a neural network. Inspired from the recent model compression techniques, we employ compression-aware training and perform low-rank weight approximations using singular value decomposition (SVD) to achieve network compaction. By encouraging the network to learn low-rank weight filters, our method achieves compressed representations with minimal performance degradation without the need for costly fine-tuning. Specifically, we decompose the weight filters using SVD and train the network on incremental tasks in its factorized form. Such a factorization allows us to directly impose sparsity-inducing regularizers over the singular values and allows us to use fewer number of parameters for each task. We further introduce a novel shared representational space based learning between tasks. This promotes the incoming tasks to only learn residual task-specific information on top of the previously learnt weight filters and greatly helps in learning under fixed capacity constraints. Our method significantly outperforms prior continual learning approaches on three benchmark datasets, demonstrating accuracy improvements of 10.3%, 12.3%, 15.6% on 20-split CIFAR-100, miniImageNet and a 5-sequence dataset, respectively, over state-of-the-art. Further, our method yields compressed models that have ~3.64x, 2.88x, 5.91x fewer number of parameters respectively, on the above mentioned datasets in comparison to baseline individual task models. Our source code is available at https://github.com/pavanteja295/CACL.



