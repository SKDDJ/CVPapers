# Arxiv Papers in cs.CV on 2020-09-25
### Going to Extremes: Weakly Supervised Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2009.11988v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.11988v1)
- **Published**: 2020-09-25 00:28:10+00:00
- **Updated**: 2020-09-25 00:28:10+00:00
- **Authors**: Holger R Roth, Dong Yang, Ziyue Xu, Xiaosong Wang, Daguang Xu
- **Comment**: 13 pages, 6 figures, 1 table
- **Journal**: None
- **Summary**: Medical image annotation is a major hurdle for developing precise and robust machine learning models. Annotation is expensive, time-consuming, and often requires expert knowledge, particularly in the medical field. Here, we suggest using minimal user interaction in the form of extreme point clicks to train a segmentation model which, in effect, can be used to speed up medical image annotation. An initial segmentation is generated based on the extreme points utilizing the random walker algorithm. This initial segmentation is then used as a noisy supervision signal to train a fully convolutional network that can segment the organ of interest, based on the provided user clicks. Through experimentation on several medical imaging datasets, we show that the predictions of the network can be refined using several rounds of training with the prediction from the same weakly annotated data. Further improvements are shown utilizing the clicked points within a custom-designed loss and attention mechanism. Our approach has the potential to speed up the process of generating new training datasets for the development of new machine learning and deep learning-based models for, but not exclusively, medical image analysis.



### G-SimCLR : Self-Supervised Contrastive Learning with Guided Projection via Pseudo Labelling
- **Arxiv ID**: http://arxiv.org/abs/2009.12007v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2009.12007v1)
- **Published**: 2020-09-25 02:25:37+00:00
- **Updated**: 2020-09-25 02:25:37+00:00
- **Authors**: Souradip Chakraborty, Aritra Roy Gosthipaty, Sayak Paul
- **Comment**: Code available at this URL: https://github.com/ariG23498/G-SimCLR.
  This paper is accepeted as a workshop paper at
  https://fuzhenzhuang.github.io/DLKT2020/index.html
- **Journal**: None
- **Summary**: In the realms of computer vision, it is evident that deep neural networks perform better in a supervised setting with a large amount of labeled data. The representations learned with supervision are not only of high quality but also helps the model in enhancing its accuracy. However, the collection and annotation of a large dataset are costly and time-consuming. To avoid the same, there has been a lot of research going on in the field of unsupervised visual representation learning especially in a self-supervised setting. Amongst the recent advancements in self-supervised methods for visual recognition, in SimCLR Chen et al. shows that good quality representations can indeed be learned without explicit supervision. In SimCLR, the authors maximize the similarity of augmentations of the same image and minimize the similarity of augmentations of different images. A linear classifier trained with the representations learned using this approach yields 76.5% top-1 accuracy on the ImageNet ILSVRC-2012 dataset. In this work, we propose that, with the normalized temperature-scaled cross-entropy (NT-Xent) loss function (as used in SimCLR), it is beneficial to not have images of the same category in the same batch. In an unsupervised setting, the information of images pertaining to the same category is missing. We use the latent space representation of a denoising autoencoder trained on the unlabeled dataset and cluster them with k-means to obtain pseudo labels. With this apriori information we batch images, where no two images from the same category are to be found. We report comparable performance enhancements on the CIFAR10 dataset and a subset of the ImageNet dataset. We refer to our method as G-SimCLR.



### Influence of segmentation accuracy in structural MR head scans on electric field computation for TMS and tES
- **Arxiv ID**: http://arxiv.org/abs/2009.12015v2
- **DOI**: 10.1088/1361-6560/abe223
- **Categories**: **physics.med-ph**, cs.CV, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2009.12015v2)
- **Published**: 2020-09-25 03:38:24+00:00
- **Updated**: 2021-01-29 00:54:14+00:00
- **Authors**: Essam A. Rashed, Jose Gomez-Tames, Akimasa Hirata
- **Comment**: Phys. Med. Biol
- **Journal**: Physics in Medicine and Biology, 2021
- **Summary**: In several diagnosis and therapy procedures based on electrostimulation effect, the internal physical quantity related to the stimulation is the induced electric field. To estimate the induced electric field in an individual human model, the segmentation of anatomical imaging, such as (magnetic resonance image (MRI) scans, of the corresponding body parts into tissues is required. Then, electrical properties associated with different annotated tissues are assigned to the digital model to generate a volume conductor. An open question is how segmentation accuracy of different tissues would influence the distribution of the induced electric field. In this study, we applied parametric segmentation of different tissues to exploit the segmentation of available MRI to generate different quality of head models using deep learning neural network architecture, named ForkNet. Then, the induced electric field are compared to assess the effect of model segmentation variations. Computational results indicate that the influence of segmentation error is tissue-dependent. In brain, sensitivity to segmentation accuracy is relatively high in cerebrospinal fluid (CSF), moderate in gray matter (GM) and low in white matter for transcranial magnetic stimulation (TMS) and transcranial electrical stimulation (tES). A CSF segmentation accuracy reduction of 10% in terms of Dice coefficient (DC) lead to decrease up to 4% in normalized induced electric field in both applications. However, a GM segmentation accuracy reduction of 5.6% DC leads to increase of normalized induced electric field up to 6%. Opposite trend of electric field variation was found between CSF and GM for both TMS and tES. The finding obtained here would be useful to quantify potential uncertainty of computational results.



### Tied Block Convolution: Leaner and Better CNNs with Shared Thinner Filters
- **Arxiv ID**: http://arxiv.org/abs/2009.12021v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.12021v1)
- **Published**: 2020-09-25 03:58:40+00:00
- **Updated**: 2020-09-25 03:58:40+00:00
- **Authors**: Xudong Wang, Stella X. Yu
- **Comment**: 13 pages
- **Journal**: None
- **Summary**: Convolution is the main building block of convolutional neural networks (CNN). We observe that an optimized CNN often has highly correlated filters as the number of channels increases with depth, reducing the expressive power of feature representations. We propose Tied Block Convolution (TBC) that shares the same thinner filters over equal blocks of channels and produces multiple responses with a single filter. The concept of TBC can also be extended to group convolution and fully connected layers, and can be applied to various backbone networks and attention modules. Our extensive experimentation on classification, detection, instance segmentation, and attention demonstrates TBC's significant across-the-board gain over standard convolution and group convolution. The proposed TiedSE attention module can even use 64 times fewer parameters than the SE module to achieve comparable performance. In particular, standard CNNs often fail to accurately aggregate information in the presence of occlusion and result in multiple redundant partial object proposals. By sharing filters across channels, TBC reduces correlation and can effectively handle highly overlapping instances. TBC increases the average precision for object detection on MS-COCO by 6% when the occlusion ratio is 80%. Our code will be released.



### A Unified Plug-and-Play Framework for Effective Data Denoising and Robust Abstention
- **Arxiv ID**: http://arxiv.org/abs/2009.12027v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2009.12027v1)
- **Published**: 2020-09-25 04:18:08+00:00
- **Updated**: 2020-09-25 04:18:08+00:00
- **Authors**: Krishanu Sarker, Xiulong Yang, Yang Li, Saeid Belkasim, Shihao Ji
- **Comment**: Under review
- **Journal**: None
- **Summary**: The success of Deep Neural Networks (DNNs) highly depends on data quality. Moreover, predictive uncertainty makes high performing DNNs risky for real-world deployment. In this paper, we aim to address these two issues by proposing a unified filtering framework leveraging underlying data density, that can effectively denoise training data as well as avoid predicting uncertain test data points. Our proposed framework leverages underlying data distribution to differentiate between noise and clean data samples without requiring any modification to existing DNN architectures or loss functions. Extensive experiments on multiple image classification datasets and multiple CNN architectures demonstrate that our simple yet effective framework can outperform the state-of-the-art techniques in denoising training data and abstaining uncertain test data.



### Deep Adversarial Transition Learning using Cross-Grafted Generative Stacks
- **Arxiv ID**: http://arxiv.org/abs/2009.12028v1
- **DOI**: 10.1016/j.neunet.2022.02.011
- **Categories**: **cs.CV**, cs.LG, I.2.10; I.2.6
- **Links**: [PDF](http://arxiv.org/pdf/2009.12028v1)
- **Published**: 2020-09-25 04:25:27+00:00
- **Updated**: 2020-09-25 04:25:27+00:00
- **Authors**: Jinyong Hou, Xuejie Ding, Stephen Cranefield, Jeremiah D. Deng
- **Comment**: 12 pages, 8 figures
- **Journal**: Neural Networks, Volume 149, 2022, Pages 172-183
- **Summary**: Current deep domain adaptation methods used in computer vision have mainly focused on learning discriminative and domain-invariant features across different domains. In this paper, we present a novel "deep adversarial transition learning" (DATL) framework that bridges the domain gap by projecting the source and target domains into intermediate, transitional spaces through the employment of adjustable, cross-grafted generative network stacks and effective adversarial learning between transitions. Specifically, we construct variational auto-encoders (VAE) for the two domains, and form bidirectional transitions by cross-grafting the VAEs' decoder stacks. Furthermore, generative adversarial networks (GAN) are employed for domain adaptation, mapping the target domain data to the known label space of the source domain. The overall adaptation process hence consists of three phases: feature representation learning by VAEs, transitions generation, and transitions alignment by GANs. Experimental results demonstrate that our method outperforms the state-of-the art on a number of unsupervised domain adaptation benchmarks.



### DPN: Detail-Preserving Network with High Resolution Representation for Efficient Segmentation of Retinal Vessels
- **Arxiv ID**: http://arxiv.org/abs/2009.12053v2
- **DOI**: 10.1007/s12652-021-03422-3
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.12053v2)
- **Published**: 2020-09-25 06:38:18+00:00
- **Updated**: 2021-08-29 13:00:05+00:00
- **Authors**: Song Guo
- **Comment**: 23 pages
- **Journal**: J Ambient Intell Human Comput (2021)
- **Summary**: Retinal vessels are important biomarkers for many ophthalmological and cardiovascular diseases. Hence, it is of great significance to develop automatic models for computer-aided diagnosis. Existing methods, such as U-Net follow the encoder-decoder pipeline, where detailed information is lost in the encoder in order to achieve a large field of view. Although spatial detailed information could be recovered partly in the decoder, while there is noise in the high-resolution feature maps of the encoder. And, we argue this encoder-decoder architecture is inefficient for vessel segmentation. In this paper, we present the detail-preserving network (DPN), which avoids the encoder-decoder pipeline. To preserve detailed information and learn structural information simultaneously, we designed the detail-preserving block (DP-Block). Further, we stacked eight DP-Blocks together to form the DPN. More importantly, there are no down-sampling operations among these blocks. Therefore, the DPN could maintain a high/full resolution during processing, avoiding the loss of detailed information. To illustrate the effectiveness of DPN, we conducted experiments over three public datasets. Experimental results show, compared to state-of-the-art methods, DPN shows competitive/better performance in terms of segmentation accuracy, segmentation speed, and model size. Specifically, 1) Our method achieves comparable segmentation performance on the DRIVE, CHASE_DB1, and HRF datasets. 2) The segmentation speed of DPN is over 20-160 times faster than other methods on the DRIVE dataset. 3) The number of parameters of DPN is1 around 120k, far less than all comparison methods.



### In-sample Contrastive Learning and Consistent Attention for Weakly Supervised Object Localization
- **Arxiv ID**: http://arxiv.org/abs/2009.12063v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.12063v1)
- **Published**: 2020-09-25 07:24:46+00:00
- **Updated**: 2020-09-25 07:24:46+00:00
- **Authors**: Minsong Ki, Youngjung Uh, Wonyoung Lee, Hyeran Byun
- **Comment**: To appear at ACCV2020 Oral
- **Journal**: None
- **Summary**: Weakly supervised object localization (WSOL) aims to localize the target object using only the image-level supervision. Recent methods encourage the model to activate feature maps over the entire object by dropping the most discriminative parts. However, they are likely to induce excessive extension to the backgrounds which leads to over-estimated localization. In this paper, we consider the background as an important cue that guides the feature activation to cover the sophisticated object region and propose contrastive attention loss. The loss promotes similarity between foreground and its dropped version, and, dissimilarity between the dropped version and background. Furthermore, we propose foreground consistency loss that penalizes earlier layers producing noisy attention regarding the later layer as a reference to provide them with a sense of backgroundness. It guides the early layers to activate on objects rather than locally distinctive backgrounds so that their attentions to be similar to the later layer. For better optimizing the above losses, we use the non-local attention blocks to replace channel-pooled attention leading to enhanced attention maps considering the spatial similarity. Last but not least, we propose to drop background regions in addition to the most discriminative region. Our method achieves state-of-theart performance on CUB-200-2011 and ImageNet benchmark datasets regarding top-1 localization accuracy and MaxBoxAccV2, and we provide detailed analysis on our individual components. The code will be publicly available online for reproducibility.



### AIM 2020 Challenge on Real Image Super-Resolution: Methods and Results
- **Arxiv ID**: http://arxiv.org/abs/2009.12072v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.12072v1)
- **Published**: 2020-09-25 07:42:55+00:00
- **Updated**: 2020-09-25 07:42:55+00:00
- **Authors**: Pengxu Wei, Hannan Lu, Radu Timofte, Liang Lin, Wangmeng Zuo, Zhihong Pan, Baopu Li, Teng Xi, Yanwen Fan, Gang Zhang, Jingtuo Liu, Junyu Han, Errui Ding, Tangxin Xie, Liang Cao, Yan Zou, Yi Shen, Jialiang Zhang, Yu Jia, Kaihua Cheng, Chenhuan Wu, Yue Lin, Cen Liu, Yunbo Peng, Xueyi Zou, Zhipeng Luo, Yuehan Yao, Zhenyu Xu, Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, Keon-Hee Ahn, Jun-Hyuk Kim, Jun-Ho Choi, Jong-Seok Lee, Tongtong Zhao, Shanshan Zhao, Yoseob Han, Byung-Hoon Kim, JaeHyun Baek, Haoning Wu, Dejia Xu, Bo Zhou, Wei Guan, Xiaobo Li, Chen Ye, Hao Li, Haoyu Zhong, Yukai Shi, Zhijing Yang, Xiaojun Yang, Haoyu Zhong, Xin Li, Xin Jin, Yaojun Wu, Yingxue Pang, Sen Liu, Zhi-Song Liu, Li-Wen Wang, Chu-Tak Li, Marie-Paule Cani, Wan-Chi Siu, Yuanbo Zhou, Rao Muhammad Umer, Christian Micheloni, Xiaofeng Cong, Rajat Gupta, Keon-Hee Ahn, Jun-Hyuk Kim, Jun-Ho Choi, Jong-Seok Lee, Feras Almasri, Thomas Vandamme, Olivier Debeir
- **Comment**: None
- **Journal**: European Conference on Computer Vision Workshops, 2020
- **Summary**: This paper introduces the real image Super-Resolution (SR) challenge that was part of the Advances in Image Manipulation (AIM) workshop, held in conjunction with ECCV 2020. This challenge involves three tracks to super-resolve an input image for $\times$2, $\times$3 and $\times$4 scaling factors, respectively. The goal is to attract more attention to realistic image degradation for the SR task, which is much more complicated and challenging, and contributes to real-world image super-resolution applications. 452 participants were registered for three tracks in total, and 24 teams submitted their results. They gauge the state-of-the-art approaches for real image SR in terms of PSNR and SSIM.



### Training CNNs in Presence of JPEG Compression: Multimedia Forensics vs Computer Vision
- **Arxiv ID**: http://arxiv.org/abs/2009.12088v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2009.12088v1)
- **Published**: 2020-09-25 08:47:21+00:00
- **Updated**: 2020-09-25 08:47:21+00:00
- **Authors**: Sara Mandelli, Nicolò Bonettini, Paolo Bestagini, Stefano Tubaro
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) have proved very accurate in multiple computer vision image classification tasks that required visual inspection in the past (e.g., object recognition, face detection, etc.). Motivated by these astonishing results, researchers have also started using CNNs to cope with image forensic problems (e.g., camera model identification, tampering detection, etc.). However, in computer vision, image classification methods typically rely on visual cues easily detectable by human eyes. Conversely, forensic solutions rely on almost invisible traces that are often very subtle and lie in the fine details of the image under analysis. For this reason, training a CNN to solve a forensic task requires some special care, as common processing operations (e.g., resampling, compression, etc.) can strongly hinder forensic traces. In this work, we focus on the effect that JPEG has on CNN training considering different computer vision and forensic image classification problems. Specifically, we consider the issues that rise from JPEG compression and misalignment of the JPEG grid. We show that it is necessary to consider these effects when generating a training dataset in order to properly train a forensic detector not losing generalization capability, whereas it is almost possible to ignore these effects for computer vision tasks.



### Enhancing MRI Brain Tumor Segmentation with an Additional Classification Network
- **Arxiv ID**: http://arxiv.org/abs/2009.12111v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.12111v2)
- **Published**: 2020-09-25 10:05:12+00:00
- **Updated**: 2020-10-28 04:00:45+00:00
- **Authors**: Hieu T. Nguyen, Tung T. Le, Thang V. Nguyen, Nhan T. Nguyen
- **Comment**: None
- **Journal**: None
- **Summary**: Brain tumor segmentation plays an essential role in medical image analysis. In recent studies, deep convolution neural networks (DCNNs) are extremely powerful to tackle tumor segmentation tasks. We propose in this paper a novel training method that enhances the segmentation results by adding an additional classification branch to the network. The whole network was trained end-to-end on the Multimodal Brain Tumor Segmentation Challenge (BraTS) 2020 training dataset. On the BraTS's validation set, it achieved an average Dice score of 78.43%, 89.99%, and 84.22% respectively for the enhancing tumor, the whole tumor, and the tumor core.



### A Possible Method of Carbon Deposit Mapping on Plasma Facing Components Using Infrared Thermography
- **Arxiv ID**: http://arxiv.org/abs/2010.06374v1
- **DOI**: 10.1016/j.jnucmat.2007.01.00
- **Categories**: **physics.ins-det**, cs.CV, physics.class-ph, physics.plasm-ph
- **Links**: [PDF](http://arxiv.org/pdf/2010.06374v1)
- **Published**: 2020-09-25 12:04:50+00:00
- **Updated**: 2020-09-25 12:04:50+00:00
- **Authors**: R Mitteau, J Spruytte, S Vallet, J Travère, D Guilhem, C Brosset
- **Comment**: Journal of Nuclear Materials, Elsevier, 2007
- **Journal**: None
- **Summary**: The material eroded from the surface of plasma facing components is redeposited partly close to high heat flux areas. At these locations, the deposit is heated by the plasma and the deposition pattern evolves depending on the operation parameters. The mapping of the deposit is still a matter of intense scientific activity, especially during the course of experimental campaigns. A method based on the comparison of surface temperature maps, obtained in situ by infrared cameras and by theoretical modelling is proposed. The difference between the two is attributed to the thermal resistance added by deposited material, and expressed as a deposit thickness. The method benefits of elaborated imaging techniques such as possibility theory and fuzzy logics. The results are consistent with deposit maps obtained by visual inspection during shutdowns.



### Tarsier: Evolving Noise Injection in Super-Resolution GANs
- **Arxiv ID**: http://arxiv.org/abs/2009.12177v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2009.12177v1)
- **Published**: 2020-09-25 12:29:16+00:00
- **Updated**: 2020-09-25 12:29:16+00:00
- **Authors**: Baptiste Roziere, Nathanal Carraz Rakotonirina, Vlad Hosu, Andry Rasoanaivo, Hanhe Lin, Camille Couprie, Olivier Teytaud
- **Comment**: None
- **Journal**: None
- **Summary**: Super-resolution aims at increasing the resolution and level of detail within an image. The current state of the art in general single-image super-resolution is held by NESRGAN+, which injects a Gaussian noise after each residual layer at training time. In this paper, we harness evolutionary methods to improve NESRGAN+ by optimizing the noise injection at inference time. More precisely, we use Diagonal CMA to optimize the injected noise according to a novel criterion combining quality assessment and realism. Our results are validated by the PIRM perceptual score and a human study. Our method outperforms NESRGAN+ on several standard super-resolution datasets. More generally, our approach can be used to optimize any method based on noise injection.



### Improved Dimensionality Reduction of various Datasets using Novel Multiplicative Factoring Principal Component Analysis (MPCA)
- **Arxiv ID**: http://arxiv.org/abs/2009.12179v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.12179v1)
- **Published**: 2020-09-25 12:30:15+00:00
- **Updated**: 2020-09-25 12:30:15+00:00
- **Authors**: Chisom Ezinne Ogbuanya
- **Comment**: None
- **Journal**: None
- **Summary**: Principal Component Analysis (PCA) is known to be the most widely applied dimensionality reduction approach. A lot of improvements have been done on the traditional PCA, in order to obtain optimal results in the dimensionality reduction of various datasets. In this paper, we present an improvement to the traditional PCA approach called Multiplicative factoring Principal Component Analysis (MPCA). The advantage of MPCA over the traditional PCA is that a penalty is imposed on the occurrence space through a multiplier to make negligible the effect of outliers in seeking out projections. Here we apply two multiplier approaches, total distance and cosine similarity metrics. These two approaches can learn the relationship that exists between each of the data points and the principal projections in the feature space. As a result of this, improved low-rank projections are gotten through multiplying the data iteratively to make negligible the effect of corrupt data in the training set. Experiments were carried out on YaleB, MNIST, AR, and Isolet datasets and the results were compared to results gotten from some popular dimensionality reduction methods such as traditional PCA, RPCA-OM, and also some recently published methods such as IFPCA-1 and IFPCA-2.



### From Pixel to Patch: Synthesize Context-aware Features for Zero-shot Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2009.12232v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.12232v4)
- **Published**: 2020-09-25 13:26:30+00:00
- **Updated**: 2022-01-21 12:38:13+00:00
- **Authors**: Zhangxuan Gu, Siyuan Zhou, Li Niu, Zihan Zhao, Liqing Zhang
- **Comment**: accepted by TNNLS
- **Journal**: None
- **Summary**: Zero-shot learning has been actively studied for image classification task to relieve the burden of annotating image labels. Interestingly, semantic segmentation task requires more labor-intensive pixel-wise annotation, but zero-shot semantic segmentation has only attracted limited research interest. Thus, we focus on zero-shot semantic segmentation, which aims to segment unseen objects with only category-level semantic representations provided for unseen categories. In this paper, we propose a novel Context-aware feature Generation Network (CaGNet), which can synthesize context-aware pixel-wise visual features for unseen categories based on category-level semantic representations and pixel-wise contextual information. The synthesized features are used to finetune the classifier to enable segmenting unseen objects. Furthermore, we extend pixel-wise feature generation and finetuning to patch-wise feature generation and finetuning, which additionally considers inter-pixel relationship. Experimental results on Pascal-VOC, Pascal-Context, and COCO-stuff show that our method significantly outperforms the existing zero-shot semantic segmentation methods. Code is available at https://github.com/bcmi/CaGNetv2-Zero-Shot-Semantic-Segmentation.



### Database Annotation with few Examples: An Atlas-based Framework using Diffeomorphic Registration of 3D Trees
- **Arxiv ID**: http://arxiv.org/abs/2009.12252v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2009.12252v1)
- **Published**: 2020-09-25 14:10:52+00:00
- **Updated**: 2020-09-25 14:10:52+00:00
- **Authors**: Pierre-Louis Antonsanti, Thomas Benseghir, Vincent Jugnon, Joan Glaunès
- **Comment**: Medical Image Computing and Computer Assisted Intervention (MICCAI)
  conference, 2020
- **Journal**: None
- **Summary**: Automatic annotation of anatomical structures can help simplify workflow during interventions in numerous clinical applications but usually involves a large amount of annotated data. The complexity of the labeling task, together with the lack of representative data, slows down the development of robust solutions. In this paper, we propose a solution requiring very few annotated cases to label 3D pelvic arterial trees of patients with benign prostatic hyperplasia. We take advantage of Large Deformation Diffeomorphic Metric Mapping (LDDMM) to perform registration based on meaningful deformations from which we build an atlas. Branch pairing is then computed from the atlas to new cases using optimal transport to ensure one-to-one correspondence during the labeling process. To tackle topological variations in the tree, which usually degrades the performance of atlas-based techniques, we propose a simple bottom-up label assignment adapted to the pelvic anatomy. The proposed method achieves 97.6\% labeling precision with only 5 cases for training, while in comparison learning-based methods only reach 82.2\% on such small training sets.



### SemanticVoxels: Sequential Fusion for 3D Pedestrian Detection using LiDAR Point Cloud and Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2009.12276v1
- **DOI**: 10.1109/MFI49285.2020.9235240
- **Categories**: **cs.RO**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2009.12276v1)
- **Published**: 2020-09-25 14:52:32+00:00
- **Updated**: 2020-09-25 14:52:32+00:00
- **Authors**: Juncong Fei, Wenbo Chen, Philipp Heidenreich, Sascha Wirges, Christoph Stiller
- **Comment**: Accepted to present in the 2020 IEEE International Conference on
  Multisensor Fusion and Integration (MFI 2020)
- **Journal**: None
- **Summary**: 3D pedestrian detection is a challenging task in automated driving because pedestrians are relatively small, frequently occluded and easily confused with narrow vertical objects. LiDAR and camera are two commonly used sensor modalities for this task, which should provide complementary information. Unexpectedly, LiDAR-only detection methods tend to outperform multisensor fusion methods in public benchmarks. Recently, PointPainting has been presented to eliminate this performance drop by effectively fusing the output of a semantic segmentation network instead of the raw image information. In this paper, we propose a generalization of PointPainting to be able to apply fusion at different levels. After the semantic augmentation of the point cloud, we encode raw point data in pillars to get geometric features and semantic point data in voxels to get semantic features and fuse them in an effective way. Experimental results on the KITTI test set show that SemanticVoxels achieves state-of-the-art performance in both 3D and bird's eye view pedestrian detection benchmarks. In particular, our approach demonstrates its strength in detecting challenging pedestrian cases and outperforms current state-of-the-art approaches.



### Locally orderless tensor networks for classifying two- and three-dimensional medical images
- **Arxiv ID**: http://arxiv.org/abs/2009.12280v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2009.12280v2)
- **Published**: 2020-09-25 15:05:02+00:00
- **Updated**: 2021-03-24 20:45:47+00:00
- **Authors**: Raghavendra Selvan, Silas Ørting, Erik B Dam
- **Comment**: Accepted for publication at the Journal of Machine Learning for
  Biomedical Imaging (MELBA) (see https://melba-journal.org). Source code at
  https://github.com/raghavian/LoTeNet_pytorch/
- **Journal**: Journal of Machine Learning for Biomedical Imaging. 2021:5. pp
  1-21. Special Issue: Medical Imaging with Deep Learning (MIDL) 2020
- **Summary**: Tensor networks are factorisations of high rank tensors into networks of lower rank tensors and have primarily been used to analyse quantum many-body problems. Tensor networks have seen a recent surge of interest in relation to supervised learning tasks with a focus on image classification. In this work, we improve upon the matrix product state (MPS) tensor networks that can operate on one-dimensional vectors to be useful for working with 2D and 3D medical images. We treat small image regions as orderless, squeeze their spatial information into feature dimensions and then perform MPS operations on these locally orderless regions. These local representations are then aggregated in a hierarchical manner to retain global structure. The proposed locally orderless tensor network (LoTeNet) is compared with relevant methods on three datasets. The architecture of LoTeNet is fixed in all experiments and we show it requires lesser computational resources to attain performance on par or superior to the compared methods.



### CAD2Real: Deep learning with domain randomization of CAD data for 3D pose estimation of electronic control unit housings
- **Arxiv ID**: http://arxiv.org/abs/2009.12312v1
- **DOI**: None
- **Categories**: **cs.CV**, I.2.10; I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/2009.12312v1)
- **Published**: 2020-09-25 16:08:16+00:00
- **Updated**: 2020-09-25 16:08:16+00:00
- **Authors**: Simon Baeuerle, Jonas Barth, Elton Renato Tavares de Menezes, Andreas Steimer, Ralf Mikut
- **Comment**: Proc. 30. Workshop Computational Intelligence, Berlin, 2020
- **Journal**: None
- **Summary**: Electronic control units (ECUs) are essential for many automobile components, e.g. engine, anti-lock braking system (ABS), steering and airbags. For some products, the 3D pose of each single ECU needs to be determined during series production. Deep learning approaches can not easily be applied to this problem, because labeled training data is not available in sufficient numbers. Thus, we train state-of-the-art artificial neural networks (ANNs) on purely synthetic training data, which is automatically created from a single CAD file. By randomizing parameters during rendering of training images, we enable inference on RGB images of a real sample part. In contrast to classic image processing approaches, this data-driven approach poses only few requirements regarding the measurement setup and transfers to related use cases with little development effort.



### Are scene graphs good enough to improve Image Captioning?
- **Arxiv ID**: http://arxiv.org/abs/2009.12313v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, 68T50, 68T45, I.2.7; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2009.12313v2)
- **Published**: 2020-09-25 16:09:08+00:00
- **Updated**: 2020-10-27 17:55:55+00:00
- **Authors**: Victor Milewski, Marie-Francine Moens, Iacer Calixto
- **Comment**: Published at AACL-IJCNLP 2020. 12 pages, 5 figures
- **Journal**: None
- **Summary**: Many top-performing image captioning models rely solely on object features computed with an object detection model to generate image descriptions. However, recent studies propose to directly use scene graphs to introduce information about object relations into captioning, hoping to better describe interactions between objects. In this work, we thoroughly investigate the use of scene graphs in image captioning. We empirically study whether using additional scene graph encoders can lead to better image descriptions and propose a conditional graph attention network (C-GAT), where the image captioning decoder state is used to condition the graph updates. Finally, we determine to what extent noise in the predicted scene graphs influence caption quality. Overall, we find no significant difference between models that use scene graph features and models that only use object detection features across different captioning metrics, which suggests that existing scene graph generation models are still too noisy to be useful in image captioning. Moreover, although the quality of predicted scene graphs is very low in general, when using high quality scene graphs we obtain gains of up to 3.3 CIDEr compared to a strong Bottom-Up Top-Down baseline. We open source code to reproduce all our experiments in https://github.com/iacercalixto/butd-image-captioning.



### Predicting galaxy spectra from images with hybrid convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/2009.12318v2
- **DOI**: None
- **Categories**: **astro-ph.IM**, astro-ph.GA, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.12318v2)
- **Published**: 2020-09-25 16:16:16+00:00
- **Updated**: 2020-11-30 18:21:16+00:00
- **Authors**: John F. Wu, J. E. G. Peek
- **Comment**: 5 pages, 2 figures, accepted to the Machine Learning and the Physical
  Sciences workshop at NeurIPS 2020. Code available at
  https://github.com/jwuphysics/predicting-spectra-from-images/
- **Journal**: None
- **Summary**: Galaxies can be described by features of their optical spectra such as oxygen emission lines, or morphological features such as spiral arms. Although spectroscopy provides a rich description of the physical processes that govern galaxy evolution, spectroscopic data are observationally expensive to obtain. For the first time, we are able to robustly predict galaxy spectra directly from broad-band imaging. We present a powerful new approach using a hybrid convolutional neural network with deconvolution instead of batch normalization; this hybrid CNN outperforms other models in our tests. The learned mapping between galaxy imaging and spectra will be transformative for future wide-field surveys, such as with the Vera C. Rubin Observatory and Nancy Grace Roman Space Telescope, by multiplying the scientific returns for spectroscopically-limited galaxy samples.



### SuPEr-SAM: Using the Supervision Signal from a Pose Estimator to Train a Spatial Attention Module for Personal Protective Equipment Recognition
- **Arxiv ID**: http://arxiv.org/abs/2009.12339v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.12339v2)
- **Published**: 2020-09-25 16:58:18+00:00
- **Updated**: 2020-11-02 12:59:03+00:00
- **Authors**: Adrian Sandru, Georgian-Emilian Duta, Mariana-Iuliana Georgescu, Radu Tudor Ionescu
- **Comment**: Accepted at WACV 2021
- **Journal**: None
- **Summary**: We propose a deep learning method to automatically detect personal protective equipment (PPE), such as helmets, surgical masks, reflective vests, boots and so on, in images of people. Typical approaches for PPE detection based on deep learning are (i) to train an object detector for items such as those listed above or (ii) to train a person detector and a classifier that takes the bounding boxes predicted by the detector and discriminates between people wearing and people not wearing the corresponding PPE items. We propose a novel and accurate approach that uses three components: a person detector, a body pose estimator and a classifier. Our novelty consists in using the pose estimator only at training time, to improve the prediction performance of the classifier. We modify the neural architecture of the classifier by adding a spatial attention mechanism, which is trained using supervision signal from the pose estimator. In this way, the classifier learns to focus on PPE items, using knowledge from the pose estimator with almost no computational overhead during inference.



### Semi-Supervised Image Deraining using Gaussian Processes
- **Arxiv ID**: http://arxiv.org/abs/2009.13075v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.13075v1)
- **Published**: 2020-09-25 17:16:16+00:00
- **Updated**: 2020-09-25 17:16:16+00:00
- **Authors**: Rajeev Yasarla, V. A. Sindagi, V. M. Patel
- **Comment**: arXiv admin note: substantial text overlap with 2006.05580
- **Journal**: None
- **Summary**: Recent CNN-based methods for image deraining have achieved excellent performance in terms of reconstruction error as well as visual quality. However, these methods are limited in the sense that they can be trained only on fully labeled data. Due to various challenges in obtaining real world fully-labeled image deraining datasets, existing methods are trained only on synthetically generated data and hence, generalize poorly to real-world images. The use of real-world data in training image deraining networks is relatively less explored in the literature. We propose a Gaussian Process-based semi-supervised learning framework which enables the network in learning to derain using synthetic dataset while generalizing better using unlabeled real-world images. More specifically, we model the latent space vectors of unlabeled data using Gaussian Processes, which is then used to compute pseudo-ground-truth for supervising the network on unlabeled data. Through extensive experiments and ablations on several challenging datasets (such as Rain800, Rain200L and DDN-SIRR), we show that the proposed method is able to effectively leverage unlabeled data thereby resulting in significantly better performance as compared to labeled-only training. Additionally, we demonstrate that using unlabeled real-world images in the proposed GP-based framework results



### High Definition image classification in Geoscience using Machine Learning
- **Arxiv ID**: http://arxiv.org/abs/2010.03965v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.03965v1)
- **Published**: 2020-09-25 17:30:03+00:00
- **Updated**: 2020-09-25 17:30:03+00:00
- **Authors**: Yajun An, Zachary Golden, Tarka Wilcox, Renzhi Cao
- **Comment**: 8 pages, 14 figures
- **Journal**: None
- **Summary**: High Definition (HD) digital photos taken with drones are widely used in the study of Geoscience. However, blurry images are often taken in collected data, and it takes a lot of time and effort to distinguish clear images from blurry ones. In this work, we apply Machine learning techniques, such as Support Vector Machine (SVM) and Neural Network (NN) to classify HD images in Geoscience as clear and blurry, and therefore automate data cleaning in Geoscience. We compare the results of classification based on features abstracted from several mathematical models. Some of the implementation of our machine learning tool is freely available at: https://github.com/zachgolden/geoai.



### SceneGen: Generative Contextual Scene Augmentation using Scene Graph Priors
- **Arxiv ID**: http://arxiv.org/abs/2009.12395v2
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.12395v2)
- **Published**: 2020-09-25 18:36:27+00:00
- **Updated**: 2020-09-30 17:06:05+00:00
- **Authors**: Mohammad Keshavarzi, Aakash Parikh, Xiyu Zhai, Melody Mao, Luisa Caldas, Allen Y. Yang
- **Comment**: 19 pages, 19 figures
- **Journal**: None
- **Summary**: Spatial computing experiences are constrained by the real-world surroundings of the user. In such experiences, augmenting virtual objects to existing scenes require a contextual approach, where geometrical conflicts are avoided, and functional and plausible relationships to other objects are maintained in the target environment. Yet, due to the complexity and diversity of user environments, automatically calculating ideal positions of virtual content that is adaptive to the context of the scene is considered a challenging task. Motivated by this problem, in this paper we introduce SceneGen, a generative contextual augmentation framework that predicts virtual object positions and orientations within existing scenes. SceneGen takes a semantically segmented scene as input, and outputs positional and orientational probability maps for placing virtual content. We formulate a novel spatial Scene Graph representation, which encapsulates explicit topological properties between objects, object groups, and the room. We believe providing explicit and intuitive features plays an important role in informative content creation and user interaction of spatial computing settings, a quality that is not captured in implicit models. We use kernel density estimation (KDE) to build a multivariate conditional knowledge model trained using prior spatial Scene Graphs extracted from real-world 3D scanned data. To further capture orientational properties, we develop a fast pose annotation tool to extend current real-world datasets with orientational labels. Finally, to demonstrate our system in action, we develop an Augmented Reality application, in which objects can be contextually augmented in real-time.



### Visually Grounded Compound PCFGs
- **Arxiv ID**: http://arxiv.org/abs/2009.12404v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.12404v1)
- **Published**: 2020-09-25 19:07:00+00:00
- **Updated**: 2020-09-25 19:07:00+00:00
- **Authors**: Yanpeng Zhao, Ivan Titov
- **Comment**: Accepted to EMNLP 2020. Our code is available at
  https://github.com/zhaoyanpeng/vpcfg
- **Journal**: None
- **Summary**: Exploiting visual groundings for language understanding has recently been drawing much attention. In this work, we study visually grounded grammar induction and learn a constituency parser from both unlabeled text and its visual groundings. Existing work on this task (Shi et al., 2019) optimizes a parser via Reinforce and derives the learning signal only from the alignment of images and sentences. While their model is relatively accurate overall, its error distribution is very uneven, with low performance on certain constituents types (e.g., 26.2% recall on verb phrases, VPs) and high on others (e.g., 79.6% recall on noun phrases, NPs). This is not surprising as the learning signal is likely insufficient for deriving all aspects of phrase-structure syntax and gradient estimates are noisy. We show that using an extension of probabilistic context-free grammar model we can do fully-differentiable end-to-end visually grounded learning. Additionally, this enables us to complement the image-text alignment loss with a language modeling objective. On the MSCOCO test captions, our model establishes a new state of the art, outperforming its non-grounded version and, thus, confirming the effectiveness of visual groundings in constituency grammar induction. It also substantially outperforms the previous grounded model, with largest improvements on more `abstract' categories (e.g., +55.1% recall on VPs).



### Towards General Purpose Geometry-Preserving Single-View Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/2009.12419v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2009.12419v2)
- **Published**: 2020-09-25 20:06:13+00:00
- **Updated**: 2021-02-09 20:30:35+00:00
- **Authors**: Mikhail Romanov, Nikolay Patatkin, Anna Vorontsova, Sergey Nikolenko, Anton Konushin, Dmitry Senyushkin
- **Comment**: None
- **Journal**: None
- **Summary**: Single-view depth estimation (SVDE) plays a crucial role in scene understanding for AR applications, 3D modeling, and robotics, providing the geometry of a scene based on a single image. Recent works have shown that a successful solution strongly relies on the diversity and volume of training data. This data can be sourced from stereo movies and photos. However, they do not provide geometrically complete depth maps (as disparities contain unknown shift value). Therefore, existing models trained on this data are not able to recover correct 3D representations. Our work shows that a model trained on this data along with conventional datasets can gain accuracy while predicting correct scene geometry. Surprisingly, only a small portion of geometrically correct depth maps are required to train a model that performs equally to a model trained on the full geometrically correct dataset. After that, we train computationally efficient models on a mixture of datasets using the proposed method. Through quantitative comparison on completely unseen datasets and qualitative comparison of 3D point clouds, we show that our model defines the new state of the art in general-purpose SVDE.



### Deep Artifact-Free Residual Network for Single Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2009.12433v1
- **DOI**: 10.1007/s11760-019-01569-3
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.12433v1)
- **Published**: 2020-09-25 20:53:55+00:00
- **Updated**: 2020-09-25 20:53:55+00:00
- **Authors**: Hamdollah Nasrollahi, Kamran Farajzadeh, Vahid Hosseini, Esmaeil Zarezadeh, Milad Abdollahzadeh
- **Comment**: 8 pages
- **Journal**: SIViP 14, 407-415 (2020)
- **Summary**: Recently, convolutional neural networks have shown promising performance for single-image super-resolution. In this paper, we propose Deep Artifact-Free Residual (DAFR) network which uses the merits of both residual learning and usage of ground-truth image as target. Our framework uses a deep model to extract the high-frequency information which is necessary for high-quality image reconstruction. We use a skip-connection to feed the low-resolution image to the network before the image reconstruction. In this way, we are able to use the ground-truth images as target and avoid misleading the network due to artifacts in difference image. In order to extract clean high-frequency information, we train the network in two steps. The first step is a traditional residual learning which uses the difference image as target. Then, the trained parameters of this step are transferred to the main training in the second step. Our experimental results show that the proposed method achieves better quantitative and qualitative image quality compared to the existing methods.



### Online Learnable Keyframe Extraction in Videos and its Application with Semantic Word Vector in Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2009.12434v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.12434v1)
- **Published**: 2020-09-25 20:54:46+00:00
- **Updated**: 2020-09-25 20:54:46+00:00
- **Authors**: G M Mashrur E Elahi, Yee-Hong Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Video processing has become a popular research direction in computer vision due to its various applications such as video summarization, action recognition, etc. Recently, deep learning-based methods have achieved impressive results in action recognition. However, these methods need to process a full video sequence to recognize the action, even though most of these frames are similar and non-essential to recognizing a particular action. Additionally, these non-essential frames increase the computational cost and can confuse a method in action recognition. Instead, the important frames called keyframes not only are helpful in the recognition of an action but also can reduce the processing time of each video sequence for classification or in other applications, e.g. summarization. As well, current methods in video processing have not yet been demonstrated in an online fashion.   Motivated by the above, we propose an online learnable module for keyframe extraction. This module can be used to select key-shots in video and thus can be applied to video summarization. The extracted keyframes can be used as input to any deep learning-based classification model to recognize action. We also propose a plugin module to use the semantic word vector as input along with keyframes and a novel train/test strategy for the classification models. To our best knowledge, this is the first time such an online module and train/test strategy have been proposed.   The experimental results on many commonly used datasets in video summarization and in action recognition have shown impressive results using the proposed module.



### Democratizing Artificial Intelligence in Healthcare: A Study of Model Development Across Two Institutions Incorporating Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/2009.12437v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2009.12437v1)
- **Published**: 2020-09-25 21:12:50+00:00
- **Updated**: 2020-09-25 21:12:50+00:00
- **Authors**: Vikash Gupta1, Holger Roth, Varun Buch3, Marcio A. B. C. Rockenbach, Richard D White, Dong Yang, Olga Laur, Brian Ghoshhajra, Ittai Dayan, Daguang Xu, Mona G. Flores, Barbaros Selnur Erdal
- **Comment**: 8 pages, 5 figures, pre-print
- **Journal**: None
- **Summary**: The training of deep learning models typically requires extensive data, which are not readily available as large well-curated medical-image datasets for development of artificial intelligence (AI) models applied in Radiology. Recognizing the potential for transfer learning (TL) to allow a fully trained model from one institution to be fine-tuned by another institution using a much small local dataset, this report describes the challenges, methodology, and benefits of TL within the context of developing an AI model for a basic use-case, segmentation of Left Ventricular Myocardium (LVM) on images from 4-dimensional coronary computed tomography angiography. Ultimately, our results from comparisons of LVM segmentation predicted by a model locally trained using random initialization, versus one training-enhanced by TL, showed that a use-case model initiated by TL can be developed with sparse labels with acceptable performance. This process reduces the time required to build a new model in the clinical environment at a different institution.



### Blind Image Super-Resolution with Spatial Context Hallucination
- **Arxiv ID**: http://arxiv.org/abs/2009.12461v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.12461v1)
- **Published**: 2020-09-25 22:36:07+00:00
- **Updated**: 2020-09-25 22:36:07+00:00
- **Authors**: Dong Huo, Yee-Hong Yang
- **Comment**: 14 pages, 6 figures
- **Journal**: None
- **Summary**: Deep convolution neural networks (CNNs) play a critical role in single image super-resolution (SISR) since the amazing improvement of high performance computing. However, most of the super-resolution (SR) methods only focus on recovering bicubic degradation. Reconstructing high-resolution (HR) images from randomly blurred and noisy low-resolution (LR) images is still a challenging problem. In this paper, we propose a novel Spatial Context Hallucination Network (SCHN) for blind super-resolution without knowing the degradation kernel. We find that when the blur kernel is unknown, separate deblurring and super-resolution could limit the performance because of the accumulation of error. Thus, we integrate denoising, deblurring and super-resolution within one framework to avoid such a problem. We train our model on two high quality datasets, DIV2K and Flickr2K. Our method performs better than state-of-the-art methods when input images are corrupted with random blur and noise.



### Enhanced 3D Myocardial Strain Estimation from Multi-View 2D CMR Imaging
- **Arxiv ID**: http://arxiv.org/abs/2009.12466v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CE, cs.CV, I.3; I.4; I.6; J.3
- **Links**: [PDF](http://arxiv.org/pdf/2009.12466v2)
- **Published**: 2020-09-25 22:47:50+00:00
- **Updated**: 2020-11-30 02:58:47+00:00
- **Authors**: Mohamed Abdelkhalek, Heba Aguib, Mohamed Moustafa, Khalil Elkhodary
- **Comment**: This a preprint of original research work
- **Journal**: None
- **Summary**: In this paper, we propose an enhanced 3D myocardial strain estimation procedure, which combines complementary displacement information from multiple orientations of a single imaging modality (untagged CMR SSFP images). To estimate myocardial strain across the left ventricle, we register the sets of short-axis, four-chamber and two-chamber views via a 2D non-rigid registration algorithm implemented in a commercial software (Segment, Medviso). We then create a series of interpolating functions for the three orthogonal directions of motion and use them to deform a tetrahedral mesh representation of a patient-specific left ventricle. Additionally, we correct for overestimation of displacement by introducing a weighting scheme that is based on displacement along the long axis. The procedure was evaluated on the STACOM 2011 dataset containing CMR SSFP images for 16 healthy volunteers. We show increased accuracy in estimating the three strain components (radial, circumferential, longitudinal) compared to reported results in the challenge, for the imaging modality of interest (SSFP). Our peak strain estimates are also significantly closer to reported measurements from studies of a larger cohort in the literature and our own ground truth measurements using Segment Strain Analysis Module. Our proposed procedure provides a relatively fast and simple method to improve 2D tracking results, with the added flexibility in either deforming a reconstructed mesh model from other image modalities or using the built-in CMR mesh reconstruction procedure. Our, proposed scheme presents a deforming patient-specific model of the left ventricle, using the commonest imaging modality , routinely administered in clinical settings, without requiring additional or specialized imaging protocols.



### SIA-GCN: A Spatial Information Aware Graph Neural Network with 2D Convolutions for Hand Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2009.12473v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.12473v1)
- **Published**: 2020-09-25 23:23:09+00:00
- **Updated**: 2020-09-25 23:23:09+00:00
- **Authors**: Deying Kong, Haoyu Ma, Xiaohui Xie
- **Comment**: 31st British Machine Vision Conference (BMVC), oral presentation
- **Journal**: None
- **Summary**: Graph Neural Networks (GNNs) generalize neural networks from applications on regular structures to applications on arbitrary graphs, and have shown success in many application domains such as computer vision, social networks and chemistry. In this paper, we extend GNNs along two directions: a) allowing features at each node to be represented by 2D spatial confidence maps instead of 1D vectors; and b) proposing an efficient operation to integrate information from neighboring nodes through 2D convolutions with different learnable kernels at each edge. The proposed SIA-GCN can efficiently extract spatial information from 2D maps at each node and propagate them through graph convolution. By associating each edge with a designated convolution kernel, the SIA-GCN could capture different spatial relationships for different pairs of neighboring nodes. We demonstrate the utility of SIA-GCN on the task of estimating hand keypoints from single-frame images, where the nodes represent the 2D coordinate heatmaps of keypoints and the edges denote the kinetic relationships between keypoints. Experiments on multiple datasets show that SIA-GCN provides a flexible and yet powerful framework to account for structural constraints between keypoints, and can achieve state-of-the-art performance on the task of hand pose estimation.



