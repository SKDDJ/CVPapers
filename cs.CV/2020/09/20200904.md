# Arxiv Papers in cs.CV on 2020-09-04
### A New Screening Method for COVID-19 based on Ocular Feature Recognition by Machine Learning Tools
- **Arxiv ID**: http://arxiv.org/abs/2009.03184v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.03184v1)
- **Published**: 2020-09-04 00:50:27+00:00
- **Updated**: 2020-09-04 00:50:27+00:00
- **Authors**: Yanwei Fu, Feng Li, Wenxuan Wang, Haicheng Tang, Xuelin Qian, Mengwei Gu, Xiangyang Xue
- **Comment**: technical report
- **Journal**: None
- **Summary**: The Coronavirus disease 2019 (COVID-19) has affected several million people. With the outbreak of the epidemic, many researchers are devoting themselves to the COVID-19 screening system. The standard practices for rapid risk screening of COVID-19 are the CT imaging or RT-PCR (real-time polymerase chain reaction). However, these methods demand professional efforts of the acquisition of CT images and saliva samples, a certain amount of waiting time, and most importantly prohibitive examination fee in some countries. Recently, some literatures have shown that the COVID-19 patients usually accompanied by ocular manifestations consistent with the conjunctivitis, including conjunctival hyperemia, chemosis, epiphora, or increased secretions. After more than four months study, we found that the confirmed cases of COVID-19 present the consistent ocular pathological symbols; and we propose a new screening method of analyzing the eye-region images, captured by common CCD and CMOS cameras, could reliably make a rapid risk screening of COVID-19 with very high accuracy. We believe a system implementing such an algorithm should assist the triage management or the clinical diagnosis. To further evaluate our algorithm and approved by the Ethics Committee of Shanghai public health clinic center of Fudan University, we conduct a study of analyzing the eye-region images of 303 patients (104 COVID-19, 131 pulmonary, and 68 ocular patients), as well as 136 healthy people. Remarkably, our results of COVID-19 patients in testing set consistently present similar ocular pathological symbols; and very high testing results have been achieved in terms of sensitivity and specificity. We hope this study can be inspiring and helpful for encouraging more researches in this topic.



### Attribute Adaptive Margin Softmax Loss using Privileged Information
- **Arxiv ID**: http://arxiv.org/abs/2009.01972v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.01972v1)
- **Published**: 2020-09-04 01:04:12+00:00
- **Updated**: 2020-09-04 01:04:12+00:00
- **Authors**: Seyed Mehdi Iranmanesh, Ali Dabouei, Nasser M. Nasrabadi
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel framework to exploit privileged information for recognition which is provided only during the training phase. Here, we focus on recognition task where images are provided as the main view and soft biometric traits (attributes) are provided as the privileged data (only available during training phase). We demonstrate that more discriminative feature space can be learned by enforcing a deep network to adjust adaptive margins between classes utilizing attributes. This tight constraint also effectively reduces the class imbalance inherent in the local data neighborhood, thus carving more balanced class boundaries locally and using feature space more efficiently. Extensive experiments are performed on five different datasets and the results show the superiority of our method compared to the state-of-the-art models in both tasks of face recognition and person re-identification.



### A Hybrid Deep Learning Model for Arabic Text Recognition
- **Arxiv ID**: http://arxiv.org/abs/2009.01987v1
- **DOI**: 10.14569/issn.2156-5570
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.01987v1)
- **Published**: 2020-09-04 02:49:17+00:00
- **Updated**: 2020-09-04 02:49:17+00:00
- **Authors**: Mohammad Fasha, Bassam Hammo, Nadim Obeid, Jabir Widian
- **Comment**: 11 pages
- **Journal**: International Journal of Advanced Computer Science and
  Applications, Vol. 11, No. 8, 2020
- **Summary**: Arabic text recognition is a challenging task because of the cursive nature of Arabic writing system, its joint writing scheme, the large number of ligatures and many other challenges. Deep Learning DL models achieved significant progress in numerous domains including computer vision and sequence modelling. This paper presents a model that can recognize Arabic text that was printed using multiple font types including fonts that mimic Arabic handwritten scripts. The proposed model employs a hybrid DL network that can recognize Arabic printed text without the need for character segmentation. The model was tested on a custom dataset comprised of over two million word samples that were generated using 18 different Arabic font types. The objective of the testing process was to assess the model capability in recognizing a diverse set of Arabic fonts representing a varied cursive styles. The model achieved good results in recognizing characters and words and it also achieved promising results in recognizing characters when it was tested on unseen data. The prepared model, the custom datasets and the toolkit for generating similar datasets are made publicly available, these tools can be used to prepare models for recognizing other font types as well as to further extend and enhance the performance of the proposed model.



### SSP-Net: Scalable Sequential Pyramid Networks for Real-Time 3D Human Pose Regression
- **Arxiv ID**: http://arxiv.org/abs/2009.01998v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.01998v1)
- **Published**: 2020-09-04 03:43:24+00:00
- **Updated**: 2020-09-04 03:43:24+00:00
- **Authors**: Diogo Luvizon, Hedi Tabia, David Picard
- **Comment**: Under review at PR
- **Journal**: None
- **Summary**: In this paper we propose a highly scalable convolutional neural network, end-to-end trainable, for real-time 3D human pose regression from still RGB images. We call this approach the Scalable Sequential Pyramid Networks (SSP-Net) as it is trained with refined supervision at multiple scales in a sequential manner. Our network requires a single training procedure and is capable of producing its best predictions at 120 frames per second (FPS), or acceptable predictions at more than 200 FPS when cut at test time. We show that the proposed regression approach is invariant to the size of feature maps, allowing our method to perform multi-resolution intermediate supervisions and reaching results comparable to the state-of-the-art with very low resolution feature maps. We demonstrate the accuracy and the effectiveness of our method by providing extensive experiments on two of the most important publicly available datasets for 3D pose estimation, Human3.6M and MPI-INF-3DHP. Additionally, we provide relevant insights about our decisions on the network architecture and show its flexibility to meet the best precision-speed compromise.



### Real-Time Selfie Video Stabilization
- **Arxiv ID**: http://arxiv.org/abs/2009.02007v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.02007v2)
- **Published**: 2020-09-04 04:41:05+00:00
- **Updated**: 2021-06-16 22:04:42+00:00
- **Authors**: Jiyang Yu, Ravi Ramamoorthi, Keli Cheng, Michel Sarkis, Ning Bi
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel real-time selfie video stabilization method. Our method is completely automatic and runs at 26 fps. We use a 1D linear convolutional network to directly infer the rigid moving least squares warping which implicitly balances between the global rigidity and local flexibility. Our network structure is specifically designed to stabilize the background and foreground at the same time, while providing optional control of stabilization focus (relative importance of foreground vs. background) to the users. To train our network, we collect a selfie video dataset with 1005 videos, which is significantly larger than previous selfie video datasets. We also propose a grid approximation method to the rigid moving least squares warping that enables the real-time frame warping. Our method is fully automatic and produces visually and quantitatively better results than previous real-time general video stabilization methods. Compared to previous offline selfie video methods, our approach produces comparable quality with a speed improvement of orders of magnitude.



### S3NAS: Fast NPU-aware Neural Architecture Search Methodology
- **Arxiv ID**: http://arxiv.org/abs/2009.02009v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2009.02009v1)
- **Published**: 2020-09-04 04:45:50+00:00
- **Updated**: 2020-09-04 04:45:50+00:00
- **Authors**: Jaeseong Lee, Duseok Kang, Soonhoi Ha
- **Comment**: None
- **Journal**: None
- **Summary**: As the application area of convolutional neural networks (CNN) is growing in embedded devices, it becomes popular to use a hardware CNN accelerator, called neural processing unit (NPU), to achieve higher performance per watt than CPUs or GPUs. Recently, automated neural architecture search (NAS) emerges as the default technique to find a state-of-the-art CNN architecture with higher accuracy than manually-designed architectures for image classification. In this paper, we present a fast NPU-aware NAS methodology, called S3NAS, to find a CNN architecture with higher accuracy than the existing ones under a given latency constraint. It consists of three steps: supernet design, Single-Path NAS for fast architecture exploration, and scaling. To widen the search space of the supernet structure that consists of stages, we allow stages to have a different number of blocks and blocks to have parallel layers of different kernel sizes. For a fast neural architecture search, we apply a modified Single-Path NAS technique to the proposed supernet structure. In this step, we assume a shorter latency constraint than the required to reduce the search space and the search time. The last step is to scale up the network maximally within the latency constraint. For accurate latency estimation, an analytical latency estimator is devised, based on a cycle-level NPU simulator that runs an entire CNN considering the memory access overhead accurately. With the proposed methodology, we are able to find a network in 3 hours using TPUv3, which shows 82.72% top-1 accuracy on ImageNet with 11.66 ms latency. Code are released at https://github.com/cap-lab/S3NAS



### CLEANN: Accelerated Trojan Shield for Embedded Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2009.02326v1
- **DOI**: 10.1145/3400302.3415671
- **Categories**: **cs.LG**, cs.AR, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2009.02326v1)
- **Published**: 2020-09-04 05:29:38+00:00
- **Updated**: 2020-09-04 05:29:38+00:00
- **Authors**: Mojan Javaheripi, Mohammad Samragh, Gregory Fields, Tara Javidi, Farinaz Koushanfar
- **Comment**: None
- **Journal**: None
- **Summary**: We propose CLEANN, the first end-to-end framework that enables online mitigation of Trojans for embedded Deep Neural Network (DNN) applications. A Trojan attack works by injecting a backdoor in the DNN while training; during inference, the Trojan can be activated by the specific backdoor trigger. What differentiates CLEANN from the prior work is its lightweight methodology which recovers the ground-truth class of Trojan samples without the need for labeled data, model retraining, or prior assumptions on the trigger or the attack. We leverage dictionary learning and sparse approximation to characterize the statistical behavior of benign data and identify Trojan triggers. CLEANN is devised based on algorithm/hardware co-design and is equipped with specialized hardware to enable efficient real-time execution on resource-constrained embedded platforms. Proof of concept evaluations on CLEANN for the state-of-the-art Neural Trojan attacks on visual benchmarks demonstrate its competitive advantage in terms of attack resiliency and execution overhead.



### TiVGAN: Text to Image to Video Generation with Step-by-Step Evolutionary Generator
- **Arxiv ID**: http://arxiv.org/abs/2009.02018v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.02018v2)
- **Published**: 2020-09-04 06:33:08+00:00
- **Updated**: 2021-06-28 00:25:23+00:00
- **Authors**: Doyeon Kim, Donggyu Joo, Junmo Kim
- **Comment**: IEEE Access
- **Journal**: None
- **Summary**: Advances in technology have led to the development of methods that can create desired visual multimedia. In particular, image generation using deep learning has been extensively studied across diverse fields. In comparison, video generation, especially on conditional inputs, remains a challenging and less explored area. To narrow this gap, we aim to train our model to produce a video corresponding to a given text description. We propose a novel training framework, Text-to-Image-to-Video Generative Adversarial Network (TiVGAN), which evolves frame-by-frame and finally produces a full-length video. In the first phase, we focus on creating a high-quality single video frame while learning the relationship between the text and an image. As the steps proceed, our model is trained gradually on more number of consecutive frames.This step-by-step learning process helps stabilize the training and enables the creation of high-resolution video based on conditional text descriptions. Qualitative and quantitative experimental results on various datasets demonstrate the effectiveness of the proposed method.



### Looking for change? Roll the Dice and demand Attention
- **Arxiv ID**: http://arxiv.org/abs/2009.02062v2
- **DOI**: 10.3390/rs13183707
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.02062v2)
- **Published**: 2020-09-04 08:30:25+00:00
- **Updated**: 2021-03-23 08:15:48+00:00
- **Authors**: Foivos I. Diakogiannis, François Waldner, Peter Caccetta
- **Comment**: 28 pages, under review in ISPRS P&RS, 1st revision. Figures of low
  quality due to compression for arxiv. Reduced abstract in arxiv due to
  character limitations
- **Journal**: https://www.mdpi.com/2072-4292/13/18/3707
- **Summary**: Change detection, i.e. identification per pixel of changes for some classes of interest from a set of bi-temporal co-registered images, is a fundamental task in the field of remote sensing. It remains challenging due to unrelated forms of change that appear at different times in input images. Here, we propose a reliable deep learning framework for the task of semantic change detection in very high-resolution aerial images. Our framework consists of a new loss function, new attention modules, new feature extraction building blocks, and a new backbone architecture that is tailored for the task of semantic change detection. Specifically, we define a new form of set similarity, that is based on an iterative evaluation of a variant of the Dice coefficient. We use this similarity metric to define a new loss function as well as a new spatial and channel convolution Attention layer (the FracTAL). The new attention layer, designed specifically for vision tasks, is memory efficient, thus suitable for use in all levels of deep convolutional networks. Based on these, we introduce two new efficient self-contained feature extraction convolution units. We validate the performance of these feature extraction building blocks on the CIFAR10 reference data and compare the results with standard ResNet modules. Further, we introduce a new encoder/decoder scheme, a network macro-topology, that is tailored for the task of change detection. Our network moves away from any notion of subtraction of feature layers for identifying change. We validate our approach by showing excellent performance and achieving state of the art score (F1 and Intersection over Union-hereafter IoU) on two building change detection datasets, namely, the LEVIRCD (F1: 0.918, IoU: 0.848) and the WHU (F1: 0.938, IoU: 0.882) datasets.



### Visual Sentiment Analysis from Disaster Images in Social Media
- **Arxiv ID**: http://arxiv.org/abs/2009.03051v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2009.03051v1)
- **Published**: 2020-09-04 11:29:52+00:00
- **Updated**: 2020-09-04 11:29:52+00:00
- **Authors**: Syed Zohaib Hassan, Kashif Ahmad, Steven Hicks, Paal Halvorsen, Ala Al-Fuqaha, Nicola Conci, Michael Riegler
- **Comment**: 10 pages, 6 figures, 6 tables. arXiv admin note: substantial text
  overlap with arXiv:2002.03773
- **Journal**: None
- **Summary**: The increasing popularity of social networks and users' tendency towards sharing their feelings, expressions, and opinions in text, visual, and audio content, have opened new opportunities and challenges in sentiment analysis. While sentiment analysis of text streams has been widely explored in literature, sentiment analysis from images and videos is relatively new. This article focuses on visual sentiment analysis in a societal important domain, namely disaster analysis in social media. To this aim, we propose a deep visual sentiment analyzer for disaster related images, covering different aspects of visual sentiment analysis starting from data collection, annotation, model selection, implementation, and evaluations. For data annotation, and analyzing peoples' sentiments towards natural disasters and associated images in social media, a crowd-sourcing study has been conducted with a large number of participants worldwide. The crowd-sourcing study resulted in a large-scale benchmark dataset with four different sets of annotations, each aiming a separate task. The presented analysis and the associated dataset will provide a baseline/benchmark for future research in the domain. We believe the proposed system can contribute toward more livable communities by helping different stakeholders, such as news broadcasters, humanitarian organizations, as well as the general public.



### Speech Gesture Generation from the Trimodal Context of Text, Audio, and Speaker Identity
- **Arxiv ID**: http://arxiv.org/abs/2009.02119v1
- **DOI**: 10.1145/3414685.3417838
- **Categories**: **cs.GR**, cs.CV, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2009.02119v1)
- **Published**: 2020-09-04 11:42:45+00:00
- **Updated**: 2020-09-04 11:42:45+00:00
- **Authors**: Youngwoo Yoon, Bok Cha, Joo-Haeng Lee, Minsu Jang, Jaeyeon Lee, Jaehong Kim, Geehyuk Lee
- **Comment**: 16 pages; ACM Transactions on Graphics (SIGGRAPH Asia 2020)
- **Journal**: None
- **Summary**: For human-like agents, including virtual avatars and social robots, making proper gestures while speaking is crucial in human--agent interaction. Co-speech gestures enhance interaction experiences and make the agents look alive. However, it is difficult to generate human-like gestures due to the lack of understanding of how people gesture. Data-driven approaches attempt to learn gesticulation skills from human demonstrations, but the ambiguous and individual nature of gestures hinders learning. In this paper, we present an automatic gesture generation model that uses the multimodal context of speech text, audio, and speaker identity to reliably generate gestures. By incorporating a multimodal context and an adversarial training scheme, the proposed model outputs gestures that are human-like and that match with speech content and rhythm. We also introduce a new quantitative evaluation metric for gesture generation models. Experiments with the introduced metric and subjective human evaluation showed that the proposed gesture generation model is better than existing end-to-end generation models. We further confirm that our model is able to work with synthesized audio in a scenario where contexts are constrained, and show that different gesture styles can be generated for the same speech by specifying different speaker identities in the style embedding space that is learned from videos of various speakers. All the code and data is available at https://github.com/ai4r/Gesture-Generation-from-Trimodal-Context.



### Improving Self-Organizing Maps with Unsupervised Feature Extraction
- **Arxiv ID**: http://arxiv.org/abs/2009.02174v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.02174v1)
- **Published**: 2020-09-04 13:19:24+00:00
- **Updated**: 2020-09-04 13:19:24+00:00
- **Authors**: Lyes Khacef, Laurent Rodriguez, Benoit Miramond
- **Comment**: Accepted for publication in the International Conference on Neural
  Information Processing (ICONIP) 2020
- **Journal**: None
- **Summary**: The Self-Organizing Map (SOM) is a brain-inspired neural model that is very promising for unsupervised learning, especially in embedded applications. However, it is unable to learn efficient prototypes when dealing with complex datasets. We propose in this work to improve the SOM performance by using extracted features instead of raw data. We conduct a comparative study on the SOM classification accuracy with unsupervised feature extraction using two different approaches: a machine learning approach with Sparse Convolutional Auto-Encoders using gradient-based learning, and a neuroscience approach with Spiking Neural Networks using Spike Timing Dependant Plasticity learning. The SOM is trained on the extracted features, then very few labeled samples are used to label the neurons with their corresponding class. We investigate the impact of the feature maps, the SOM size and the labeled subset size on the classification accuracy using the different feature extraction methods. We improve the SOM classification by +6.09\% and reach state-of-the-art performance on unsupervised image classification.



### GPU-based Self-Organizing Maps for Post-Labeled Few-Shot Unsupervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2009.03665v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.03665v1)
- **Published**: 2020-09-04 13:22:28+00:00
- **Updated**: 2020-09-04 13:22:28+00:00
- **Authors**: Lyes Khacef, Vincent Gripon, Benoit Miramond
- **Comment**: Accepted for publication in the International Conference on Neural
  Information Processing (ICONIP) 2020. arXiv admin note: text overlap with
  arXiv:2009.02174
- **Journal**: None
- **Summary**: Few-shot classification is a challenge in machine learning where the goal is to train a classifier using a very limited number of labeled examples. This scenario is likely to occur frequently in real life, for example when data acquisition or labeling is expensive. In this work, we consider the problem of post-labeled few-shot unsupervised learning, a classification task where representations are learned in an unsupervised fashion, to be later labeled using very few annotated examples. We argue that this problem is very likely to occur on the edge, when the embedded device directly acquires the data, and the expert needed to perform labeling cannot be prompted often. To address this problem, we consider an algorithm consisting of the concatenation of transfer learning with clustering using Self-Organizing Maps (SOMs). We introduce a TensorFlow-based implementation to speed-up the process in multi-core CPUs and GPUs. Finally, we demonstrate the effectiveness of the method using standard off-the-shelf few-shot classification benchmarks.



### Naive Artificial Intelligence
- **Arxiv ID**: http://arxiv.org/abs/2009.02185v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.02185v1)
- **Published**: 2020-09-04 13:40:10+00:00
- **Updated**: 2020-09-04 13:40:10+00:00
- **Authors**: Tomer Barak, Yehonatan Avidan, Yonatan Loewenstein
- **Comment**: None
- **Journal**: None
- **Summary**: In the cognitive sciences, it is common to distinguish between crystal intelligence, the ability to utilize knowledge acquired through past learning or experience and fluid intelligence, the ability to solve novel problems without relying on prior knowledge. Using this cognitive distinction between the two types of intelligence, extensively-trained deep networks that can play chess or Go exhibit crystal but not fluid intelligence. In humans, fluid intelligence is typically studied and quantified using intelligence tests. Previous studies have shown that deep networks can solve some forms of intelligence tests, but only after extensive training. Here we present a computational model that solves intelligence tests without any prior training. This ability is based on continual inductive reasoning, and is implemented by deep unsupervised latent-prediction networks. Our work demonstrates the potential fluid intelligence of deep networks. Finally, we propose that the computational principles underlying our approach can be used to model fluid intelligence in the cognitive sciences.



### Imbalanced Image Classification with Complement Cross Entropy
- **Arxiv ID**: http://arxiv.org/abs/2009.02189v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.02189v4)
- **Published**: 2020-09-04 13:46:24+00:00
- **Updated**: 2021-08-04 10:52:44+00:00
- **Authors**: Yechan Kim, Younkwan Lee, Moongu Jeon
- **Comment**: 8 pages, Accepted to Pattern Recognition Letters (PRL), August 2021
- **Journal**: None
- **Summary**: Recently, deep learning models have achieved great success in computer vision applications, relying on large-scale class-balanced datasets. However, imbalanced class distributions still limit the wide applicability of these models due to degradation in performance. To solve this problem, in this paper, we concentrate on the study of cross entropy which mostly ignores output scores on incorrect classes. This work discovers that neutralizing predicted probabilities on incorrect classes improves the prediction accuracy for imbalanced image classification. This paper proposes a simple but effective loss named complement cross entropy based on this finding. The proposed loss makes the ground truth class overwhelm the other classes in terms of softmax probability, by neutralizing probabilities of incorrect classes, without additional training procedures. Along with it, this loss facilitates the models to learn key information especially from samples on minority classes. It ensures more accurate and robust classification results on imbalanced distributions. Extensive experiments on imbalanced datasets demonstrate the effectiveness of the proposed method.



### SketchPatch: Sketch Stylization via Seamless Patch-level Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2009.02216v1
- **DOI**: 10.1145/3414685.3417816
- **Categories**: **cs.GR**, cs.CV, cs.LG, I.3.3
- **Links**: [PDF](http://arxiv.org/pdf/2009.02216v1)
- **Published**: 2020-09-04 14:20:46+00:00
- **Updated**: 2020-09-04 14:20:46+00:00
- **Authors**: Noa Fish, Lilach Perry, Amit Bermano, Daniel Cohen-Or
- **Comment**: SIGGRAPH Asia 2020
- **Journal**: None
- **Summary**: The paradigm of image-to-image translation is leveraged for the benefit of sketch stylization via transfer of geometric textural details. Lacking the necessary volumes of data for standard training of translation systems, we advocate for operation at the patch level, where a handful of stylized sketches provide ample mining potential for patches featuring basic geometric primitives. Operating at the patch level necessitates special consideration of full sketch translation, as individual translation of patches with no regard to neighbors is likely to produce visible seams and artifacts at patch borders. Aligned pairs of styled and plain primitives are combined to form input hybrids containing styled elements around the border and plain elements within, and given as input to a seamless translation (ST) generator, whose output patches are expected to reconstruct the fully styled patch. An adversarial addition promotes generalization and robustness to diverse geometries at inference time, forming a simple and effective system for arbitrary sketch stylization, as demonstrated upon a variety of styles and sketches.



### Efficient Computation of Higher Order 2D Image Moments using the Discrete Radon Transform
- **Arxiv ID**: http://arxiv.org/abs/2009.09898v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.09898v1)
- **Published**: 2020-09-04 15:26:03+00:00
- **Updated**: 2020-09-04 15:26:03+00:00
- **Authors**: William Diggin, Michael Diggin
- **Comment**: None
- **Journal**: None
- **Summary**: Geometric moments and moment invariants of image artifacts have many uses in computer vision applications, e.g. shape classification or object position and orientation. Higher order moments are of interest to provide additional feature descriptors, to measure kurtosis or to resolve n-fold symmetry. This paper provides the method and practical application to extend an efficient algorithm, based on the Discrete Radon Transform, to generate moments greater than the 3rd order. The mathematical fundamentals are presented, followed by relevant implementation details. Results of scaling the algorithm based on image area and its computational comparison with a standard method demonstrate the efficacy of the approach.



### Improving axial resolution in SIM using deep learning
- **Arxiv ID**: http://arxiv.org/abs/2009.02264v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.bio-ph, I.4.5; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2009.02264v3)
- **Published**: 2020-09-04 15:48:48+00:00
- **Updated**: 2021-02-18 09:56:10+00:00
- **Authors**: Miguel Boland, Edward A. K. Cohen, Seth Flaxman, Mark A. A. Neil
- **Comment**: None
- **Journal**: None
- **Summary**: Structured Illumination Microscopy is a widespread methodology to image live and fixed biological structures smaller than the diffraction limits of conventional optical microscopy. Using recent advances in image up-scaling through deep learning models, we demonstrate a method to reconstruct 3D SIM image stacks with twice the axial resolution attainable through conventional SIM reconstructions. We further evaluate our method for robustness to noise & generalisability to varying observed specimens, and discuss potential adaptions of the method to further improvements in resolution.



### Witches' Brew: Industrial Scale Data Poisoning via Gradient Matching
- **Arxiv ID**: http://arxiv.org/abs/2009.02276v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.02276v2)
- **Published**: 2020-09-04 16:17:54+00:00
- **Updated**: 2021-05-10 15:58:21+00:00
- **Authors**: Jonas Geiping, Liam Fowl, W. Ronny Huang, Wojciech Czaja, Gavin Taylor, Michael Moeller, Tom Goldstein
- **Comment**: First two authors contributed equally. Last two authors contributed
  equally. 21 pages, 11 figures. Published at ICLR 2021
- **Journal**: None
- **Summary**: Data Poisoning attacks modify training data to maliciously control a model trained on such data. In this work, we focus on targeted poisoning attacks which cause a reclassification of an unmodified test image and as such breach model integrity. We consider a particularly malicious poisoning attack that is both "from scratch" and "clean label", meaning we analyze an attack that successfully works against new, randomly initialized models, and is nearly imperceptible to humans, all while perturbing only a small fraction of the training data. Previous poisoning attacks against deep neural networks in this setting have been limited in scope and success, working only in simplified settings or being prohibitively expensive for large datasets. The central mechanism of the new attack is matching the gradient direction of malicious examples. We analyze why this works, supplement with practical considerations. and show its threat to real-world practitioners, finding that it is the first poisoning method to cause targeted misclassification in modern deep networks trained from scratch on a full-sized, poisoned ImageNet dataset. Finally we demonstrate the limitations of existing defensive strategies against such an attack, concluding that data poisoning is a credible threat, even for large-scale deep learning systems.



### End-to-End Deep Learning Model for Cardiac Cycle Synchronization from Multi-View Angiographic Sequences
- **Arxiv ID**: http://arxiv.org/abs/2009.02345v1
- **DOI**: 10.1109/EMBC44109.2020.9175453
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.02345v1)
- **Published**: 2020-09-04 18:11:50+00:00
- **Updated**: 2020-09-04 18:11:50+00:00
- **Authors**: Raphaël Royer-Rivard, Fantin Girard, Nagib Dahdah, Farida Cheriet
- **Comment**: None
- **Journal**: None
- **Summary**: Dynamic reconstructions (3D+T) of coronary arteries could give important perfusion details to clinicians. Temporal matching of the different views, which may not be acquired simultaneously, is a prerequisite for an accurate stereo-matching of the coronary segments. In this paper, we show how a neural network can be trained from angiographic sequences to synchronize different views during the cardiac cycle using raw x-ray angiography videos exclusively. First, we train a neural network model with angiographic sequences to extract features describing the progression of the cardiac cycle. Then, we compute the distance between the feature vectors of every frame from the first view with those from the second view to generate distance maps that display stripe patterns. Using pathfinding, we extract the best temporally coherent associations between each frame of both videos. Finally, we compare the synchronized frames of an evaluation set with the ECG signals to show an alignment with 96.04% accuracy.



### Sensors, Safety Models and A System-Level Approach to Safe and Scalable Automated Vehicles
- **Arxiv ID**: http://arxiv.org/abs/2009.03301v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.03301v1)
- **Published**: 2020-09-04 20:14:59+00:00
- **Updated**: 2020-09-04 20:14:59+00:00
- **Authors**: Jack Weast
- **Comment**: None
- **Journal**: None
- **Summary**: When considering the accuracy of sensors in an automated vehicle (AV), it is not sufficient to evaluate the performance of any given sensor in isolation. Rather, the performance of any individual sensor must be considered in the context of the overall system design. Techniques like redundancy and different sensing modalities can reduce the chances of a sensing failure. Additionally, the use of safety models is essential to understanding whether any particular sensing failure is relevant. Only when the entire system design is taken into account can one properly understand the meaning of safety-relevant sensing failures in an AV. In this paper, we will consider what should actually constitute a sensing failure, how safety models play an important role in mitigating potential failures, how a system-level approach to safety will deliver a safe and scalable AV, and what an acceptable sensing failure rate should be considering the full picture of an AV's architecture.



### Don't miss the Mismatch: Investigating the Objective Function Mismatch for Unsupervised Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2009.02383v2
- **DOI**: 10.1007/s00521-022-07031-9
- **Categories**: **cs.CV**, cs.AI, cs.LG, I.4; I.5; I.2
- **Links**: [PDF](http://arxiv.org/pdf/2009.02383v2)
- **Published**: 2020-09-04 20:21:17+00:00
- **Updated**: 2022-02-28 20:15:42+00:00
- **Authors**: Bonifaz Stuhr, Jürgen Brauer
- **Comment**: 13 pages, 6 figures, Published in Neural Computing and Applications
- **Journal**: Neural Computing and Applications (2022)
- **Summary**: Finding general evaluation metrics for unsupervised representation learning techniques is a challenging open research question, which recently has become more and more necessary due to the increasing interest in unsupervised methods. Even though these methods promise beneficial representation characteristics, most approaches currently suffer from the objective function mismatch. This mismatch states that the performance on a desired target task can decrease when the unsupervised pretext task is learned too long - especially when both tasks are ill-posed. In this work, we build upon the widely used linear evaluation protocol and define new general evaluation metrics to quantitatively capture the objective function mismatch and the more generic metrics mismatch. We discuss the usability and stability of our protocols on a variety of pretext and target tasks and study mismatches in a wide range of experiments. Thereby we disclose dependencies of the objective function mismatch across several pretext and target tasks with respect to the pretext model's representation size, target model complexity, pretext and target augmentations as well as pretext and target task types. In our experiments, we find that the objective function mismatch reduces performance by ~0.1-5.0% for Cifar10, Cifar100 and PCam in many setups, and up to ~25-59% in extreme cases for the 3dshapes dataset.



### ACDC: Weight Sharing in Atom-Coefficient Decomposed Convolution
- **Arxiv ID**: http://arxiv.org/abs/2009.02386v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.02386v1)
- **Published**: 2020-09-04 20:41:47+00:00
- **Updated**: 2020-09-04 20:41:47+00:00
- **Authors**: Ze Wang, Xiuyuan Cheng, Guillermo Sapiro, Qiang Qiu
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) are known to be significantly over-parametrized, and difficult to interpret, train and adapt. In this paper, we introduce a structural regularization across convolutional kernels in a CNN. In our approach, each convolution kernel is first decomposed as 2D dictionary atoms linearly combined by coefficients. The widely observed correlation and redundancy in a CNN hint a common low-rank structure among the decomposed coefficients, which is here further supported by our empirical observations. We then explicitly regularize CNN kernels by enforcing decomposed coefficients to be shared across sub-structures, while leaving each sub-structure only its own dictionary atoms, a few hundreds of parameters typically, which leads to dramatic model reductions. We explore models with sharing across different sub-structures to cover a wide range of trade-offs between parameter reduction and expressiveness. Our proposed regularized network structures open the door to better interpreting, training and adapting deep models. We validate the flexibility and compatibility of our method by image classification experiments on multiple datasets and underlying network structures, and show that CNNs now maintain performance with dramatic reduction in parameters and computations, e.g., only 5\% parameters are used in a ResNet-18 to achieve comparable performance. Further experiments on few-shot classification show that faster and more robust task adaptation is obtained in comparison with models with standard convolutions.



### Class Interference Regularization
- **Arxiv ID**: http://arxiv.org/abs/2009.02396v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.02396v1)
- **Published**: 2020-09-04 21:03:32+00:00
- **Updated**: 2020-09-04 21:03:32+00:00
- **Authors**: Bharti Munjal, Sikandar Amin, Fabio Galasso
- **Comment**: Accepted at BMVC 2020
- **Journal**: None
- **Summary**: Contrastive losses yield state-of-the-art performance for person re-identification, face verification and few shot learning. They have recently outperformed the cross-entropy loss on classification at the ImageNet scale and outperformed all self-supervision prior results by a large margin (SimCLR). Simple and effective regularization techniques such as label smoothing and self-distillation do not apply anymore, because they act on multinomial label distributions, adopted in cross-entropy losses, and not on tuple comparative terms, which characterize the contrastive losses.   Here we propose a novel, simple and effective regularization technique, the Class Interference Regularization (CIR), which applies to cross-entropy losses but is especially effective on contrastive losses. CIR perturbs the output features by randomly moving them towards the average embeddings of the negative classes. To the best of our knowledge, CIR is the first regularization technique to act on the output features.   In experimental evaluation, the combination of CIR and a plain Siamese-net with triplet loss yields best few-shot learning performance on the challenging tieredImageNet. CIR also improves the state-of-the-art technique in person re-identification on the Market-1501 dataset, based on triplet loss, and the state-of-the-art technique in person search on the CUHK-SYSU dataset, based on a cross-entropy loss. Finally, on the task of classification CIR performs on par with the popular label smoothing, as demonstrated for CIFAR-10 and -100.



### A Deep Learning Approach to Tongue Detection for Pediatric Population
- **Arxiv ID**: http://arxiv.org/abs/2009.02397v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, cs.LG, 68T07, I.4.9; I.5.1
- **Links**: [PDF](http://arxiv.org/pdf/2009.02397v3)
- **Published**: 2020-09-04 21:04:57+00:00
- **Updated**: 2020-09-28 19:43:29+00:00
- **Authors**: Javad Rahimipour Anaraki, Silvia Orlandi, Tom Chau
- **Comment**: 7 pages, 1 figure
- **Journal**: None
- **Summary**: Children with severe disabilities and complex communication needs face limitations in the usage of access technology (AT) devices. Conventional ATs (e.g., mechanical switches) can be insufficient for nonverbal children and those with limited voluntary motion control. Automatic techniques for the detection of tongue gestures represent a promising pathway. Previous studies have shown the robustness of tongue detection algorithms on adult participants, but further research is needed to use these methods with children. In this study, a network architecture for tongue-out gesture recognition was implemented and evaluated on videos recorded in a naturalistic setting when children were playing a video-game. A cascade object detector algorithm was used to detect the participants' faces, and an automated classification scheme for tongue gesture detection was developed using a convolutional neural network (CNN). In evaluation experiments conducted, the network was trained using adults and children's images. The network classification accuracy was evaluated using leave-one-subject-out cross-validation. Preliminary classification results obtained from the analysis of videos of five typically developing children showed an accuracy of up to 99% in predicting tongue-out gestures. Moreover, we demonstrated that using only children data for training the classifier yielded better performance than adult's one supporting the need for pediatric tongue gesture datasets.



### Video Moment Retrieval via Natural Language Queries
- **Arxiv ID**: http://arxiv.org/abs/2009.02406v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.02406v2)
- **Published**: 2020-09-04 22:06:34+00:00
- **Updated**: 2020-09-10 14:49:04+00:00
- **Authors**: Xinli Yu, Mohsen Malmir, Cynthia He, Yue Liu, Rex Wu
- **Comment**: needs internal approval
- **Journal**: None
- **Summary**: In this paper, we propose a novel method for video moment retrieval (VMR) that achieves state of the arts (SOTA) performance on R@1 metrics and surpassing the SOTA on the high IoU metric (R@1, IoU=0.7).   First, we propose to use a multi-head self-attention mechanism, and further a cross-attention scheme to capture video/query interaction and long-range query dependencies from video context. The attention-based methods can develop frame-to-query interaction and query-to-frame interaction at arbitrary positions and the multi-head setting ensures the sufficient understanding of complicated dependencies. Our model has a simple architecture, which enables faster training and inference while maintaining .   Second, We also propose to use multiple task training objective consists of moment segmentation task, start/end distribution prediction and start/end location regression task. We have verified that start/end prediction are noisy due to annotator disagreement and joint training with moment segmentation task can provide richer information since frames inside the target clip are also utilized as positive training examples.   Third, we propose to use an early fusion approach, which achieves better performance at the cost of inference time. However, the inference time will not be a problem for our model since our model has a simple architecture which enables efficient training and inference.



### Explanation of Unintended Radiated Emission Classification via LIME
- **Arxiv ID**: http://arxiv.org/abs/2009.02418v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2009.02418v2)
- **Published**: 2020-09-04 23:14:50+00:00
- **Updated**: 2020-09-08 16:37:29+00:00
- **Authors**: Tom Grimes, Eric Church, William Pitts, Lynn Wood
- **Comment**: 7 pages, 11 Figures
- **Journal**: None
- **Summary**: Unintended radiated emissions arise during the use of electronic devices. Identifying and mitigating the effects of these emissions is a key element of modern power engineering and associated control systems. Signal processing of the electrical system can identify the sources of these emissions. A dataset known as Flaming Moes includes captured unintended radiated emissions from consumer electronics. This dataset was analyzed to construct next-generation methods for device identification. To this end, a neural network based on applying the ResNet-18 image classification architecture to the short time Fourier transforms of short segments of voltage signatures was constructed. Using this classifier, the 18 device classes and background class were identified with close to 100 percent accuracy. By applying LIME to this classifier and aggregating the results over many classifications for the same device, it was possible to determine the frequency bands used by the classifier to make decisions. Using ensembles of classifiers trained on very similar datasets from the same parent data distribution, it was possible to recover robust sets of features of device output useful for identification. The additional understanding provided by the application of LIME enhances the trainability, trustability, and transferability of URE analysis networks.



