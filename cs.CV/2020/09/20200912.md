# Arxiv Papers in cs.CV on 2020-09-12
### RGB2LIDAR: Towards Solving Large-Scale Cross-Modal Visual Localization
- **Arxiv ID**: http://arxiv.org/abs/2009.05695v1
- **DOI**: 10.1145/3394171.3413647
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2009.05695v1)
- **Published**: 2020-09-12 01:18:45+00:00
- **Updated**: 2020-09-12 01:18:45+00:00
- **Authors**: Niluthpol Chowdhury Mithun, Karan Sikka, Han-Pang Chiu, Supun Samarasekera, Rakesh Kumar
- **Comment**: ACM Multimedia 2020
- **Journal**: None
- **Summary**: We study an important, yet largely unexplored problem of large-scale cross-modal visual localization by matching ground RGB images to a geo-referenced aerial LIDAR 3D point cloud (rendered as depth images). Prior works were demonstrated on small datasets and did not lend themselves to scaling up for large-scale applications. To enable large-scale evaluation, we introduce a new dataset containing over 550K pairs (covering 143 km^2 area) of RGB and aerial LIDAR depth images. We propose a novel joint embedding based method that effectively combines the appearance and semantic cues from both modalities to handle drastic cross-modal variations. Experiments on the proposed dataset show that our model achieves a strong result of a median rank of 5 in matching across a large test set of 50K location pairs collected from a 14km^2 area. This represents a significant advancement over prior works in performance and scale. We conclude with qualitative results to highlight the challenging nature of this task and the benefits of the proposed model. Our work provides a foundation for further research in cross-modal visual localization.



### YOLObile: Real-Time Object Detection on Mobile Devices via Compression-Compilation Co-Design
- **Arxiv ID**: http://arxiv.org/abs/2009.05697v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.05697v2)
- **Published**: 2020-09-12 01:41:08+00:00
- **Updated**: 2020-12-30 15:55:43+00:00
- **Authors**: Yuxuan Cai, Hongjia Li, Geng Yuan, Wei Niu, Yanyu Li, Xulong Tang, Bin Ren, Yanzhi Wang
- **Comment**: None
- **Journal**: None
- **Summary**: The rapid development and wide utilization of object detection techniques have aroused attention on both accuracy and speed of object detectors. However, the current state-of-the-art object detection works are either accuracy-oriented using a large model but leading to high latency or speed-oriented using a lightweight model but sacrificing accuracy. In this work, we propose YOLObile framework, a real-time object detection on mobile devices via compression-compilation co-design. A novel block-punched pruning scheme is proposed for any kernel size. To improve computational efficiency on mobile devices, a GPU-CPU collaborative scheme is adopted along with advanced compiler-assisted optimizations. Experimental results indicate that our pruning scheme achieves 14$\times$ compression rate of YOLOv4 with 49.0 mAP. Under our YOLObile framework, we achieve 17 FPS inference speed using GPU on Samsung Galaxy S20. By incorporating our proposed GPU-CPU collaborative scheme, the inference speed is increased to 19.1 FPS, and outperforms the original YOLOv4 by 5$\times$ speedup. Source code is at: \url{https://github.com/nightsnack/YOLObile}.



### Short-Term and Long-Term Context Aggregation Network for Video Inpainting
- **Arxiv ID**: http://arxiv.org/abs/2009.05721v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.05721v1)
- **Published**: 2020-09-12 03:50:56+00:00
- **Updated**: 2020-09-12 03:50:56+00:00
- **Authors**: Ang Li, Shanshan Zhao, Xingjun Ma, Mingming Gong, Jianzhong Qi, Rui Zhang, Dacheng Tao, Ramamohanarao Kotagiri
- **Comment**: Accepted by ECCV 2020 as a spotlight paper
- **Journal**: None
- **Summary**: Video inpainting aims to restore missing regions of a video and has many applications such as video editing and object removal. However, existing methods either suffer from inaccurate short-term context aggregation or rarely explore long-term frame information. In this work, we present a novel context aggregation network to effectively exploit both short-term and long-term frame information for video inpainting. In the encoding stage, we propose boundary-aware short-term context aggregation, which aligns and aggregates, from neighbor frames, local regions that are closely related to the boundary context of missing regions into the target frame. Furthermore, we propose dynamic long-term context aggregation to globally refine the feature map generated in the encoding stage using long-term frame features, which are dynamically updated throughout the inpainting process. Experiments show that it outperforms state-of-the-art methods with better inpainting results and fast inpainting speed.



### Generator Versus Segmentor: Pseudo-healthy Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2009.05722v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.05722v3)
- **Published**: 2020-09-12 03:54:22+00:00
- **Updated**: 2021-07-15 13:59:39+00:00
- **Authors**: Zhang Yunlong, Li Chenxin, Lin Xin, Sun Liyan, Zhuang Yihong, Huang Yue, Ding Xinghao, Liu Xiaoqing, Yu Yizhou
- **Comment**: Accepted by MICCAI2021
- **Journal**: None
- **Summary**: This paper investigates the problem of pseudo-healthy synthesis that is defined as synthesizing a subject-specific pathology-free image from a pathological one. Recent approaches based on Generative Adversarial Network (GAN) have been developed for this task. However, these methods will inevitably fall into the trade-off between preserving the subject-specific identity and generating healthy-like appearances. To overcome this challenge, we propose a novel adversarial training regime, Generator versus Segmentor (GVS), to alleviate this trade-off by a divide-and-conquer strategy. We further consider the deteriorating generalization performance of the segmentor throughout the training and develop a pixel-wise weighted loss by muting the well-transformed pixels to promote it. Moreover, we propose a new metric to measure how healthy the synthetic images look. The qualitative and quantitative experiments on the public dataset BraTS demonstrate that the proposed method outperforms the existing methods. Besides, we also certify the effectiveness of our method on datasets LiTS. Our implementation and pre-trained networks are publicly available at https://github.com/Au3C2/Generator-Versus-Segmentor.



### Abstractive Information Extraction from Scanned Invoices (AIESI) using End-to-end Sequential Approach
- **Arxiv ID**: http://arxiv.org/abs/2009.05728v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.05728v1)
- **Published**: 2020-09-12 05:14:28+00:00
- **Updated**: 2020-09-12 05:14:28+00:00
- **Authors**: Shreeshiv Patel, Dvijesh Bhatt
- **Comment**: 6 pages, 7 images, to be published in upcoming relevant conference
- **Journal**: None
- **Summary**: Recent proliferation in the field of Machine Learning and Deep Learning allows us to generate OCR models with higher accuracy. Optical Character Recognition(OCR) is the process of extracting text from documents and scanned images. For document data streamlining, we are interested in data like, Payee name, total amount, address, and etc. Extracted information helps to get complete insight of data, which can be helpful for fast document searching, efficient indexing in databases, data analytics, and etc. Using AIESI we can eliminate human effort for key parameters extraction from scanned documents. Abstract Information Extraction from Scanned Invoices (AIESI) is a process of extracting information like, date, total amount, payee name, and etc from scanned receipts. In this paper we proposed an improved method to ensemble all visual and textual features from invoices to extract key invoice parameters using Word wise BiLSTM.



### Monitoring Spatial Sustainable Development: semi-automated analysis of Satellite and Aerial Images for Energy Transition and Sustainability Indicators
- **Arxiv ID**: http://arxiv.org/abs/2009.05738v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.05738v1)
- **Published**: 2020-09-12 07:09:59+00:00
- **Updated**: 2020-09-12 07:09:59+00:00
- **Authors**: Tim De Jong, Stefano Bromuri, Xi Chang, Marc Debusschere, Natalie Rosenski, Clara Schartner, Katharina Strauch, Marion Boehmer, Lyana Curier
- **Comment**: 81 pages, 12 figures
- **Journal**: None
- **Summary**: This report presents the results of the DeepSolaris project that was carried out under the ESS action 'Merging Geostatistics and Geospatial Information in Member States'. During the project several deep learning algorithms were evaluated to detect solar panels in remote sensing data. The aim of the project was to evaluate whether deep learning models could be developed, that worked across different member states in the European Union. Two remote sensing data sources were considered: aerial images on the one hand, and satellite images on the other. Two flavours of deep learning models were evaluated: classification models and object detection models. For the evaluation of the deep learning models we used a cross-site evaluation approach: the deep learning models where trained in one geographical area and then evaluated on a different geographical area, previously unseen by the algorithm. The cross-site evaluation was furthermore carried out twice: deep learning models trained on he Netherlands were evaluated on Germany and vice versa. While the deep learning models were able to detect solar panels successfully, false detection remained a problem. Moreover, model performance decreased dramatically when evaluated in a cross-border fashion. Hence, training a model that performs reliably across different countries in the European Union is a challenging task. That being said, the models detected quite a share of solar panels not present in current solar panel registers and therefore can already be used as-is to help reduced manual labor in checking these registers.



### Smoothness Sensor: Adaptive Smoothness-Transition Graph Convolutions for Attributed Graph Clustering
- **Arxiv ID**: http://arxiv.org/abs/2009.05743v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2009.05743v1)
- **Published**: 2020-09-12 08:12:27+00:00
- **Updated**: 2020-09-12 08:12:27+00:00
- **Authors**: Chaojie Ji, Hongwei Chen, Ruxin Wang, Yunpeng Cai, Hongyan Wu
- **Comment**: None
- **Journal**: None
- **Summary**: Clustering techniques attempt to group objects with similar properties into a cluster. Clustering the nodes of an attributed graph, in which each node is associated with a set of feature attributes, has attracted significant attention. Graph convolutional networks (GCNs) represent an effective approach for integrating the two complementary factors of node attributes and structural information for attributed graph clustering. However, oversmoothing of GCNs produces indistinguishable representations of nodes, such that the nodes in a graph tend to be grouped into fewer clusters, and poses a challenge due to the resulting performance drop. In this study, we propose a smoothness sensor for attributed graph clustering based on adaptive smoothness-transition graph convolutions, which senses the smoothness of a graph and adaptively terminates the current convolution once the smoothness is saturated to prevent oversmoothing. Furthermore, as an alternative to graph-level smoothness, a novel fine-gained node-wise level assessment of smoothness is proposed, in which smoothness is computed in accordance with the neighborhood conditions of a given node at a certain order of graph convolution. In addition, a self-supervision criterion is designed considering both the tightness within clusters and the separation between clusters to guide the whole neural network training process. Experiments show that the proposed methods significantly outperform 12 other state-of-the-art baselines in terms of three different metrics across four benchmark datasets. In addition, an extensive study reveals the reasons for their effectiveness and efficiency.



### Multi-Spectral Image Synthesis for Crop/Weed Segmentation in Precision Farming
- **Arxiv ID**: http://arxiv.org/abs/2009.05750v2
- **DOI**: 10.1016/j.robot.2021.103861
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2009.05750v2)
- **Published**: 2020-09-12 08:49:36+00:00
- **Updated**: 2021-09-06 12:33:43+00:00
- **Authors**: Mulham Fawakherji, Ciro Potena, Alberto Pretto, Domenico D. Bloisi, Daniele Nardi
- **Comment**: None
- **Journal**: Robotics and Autonomous Systems, Volume 146, December 2021, 103861
- **Summary**: An effective perception system is a fundamental component for farming robots, as it enables them to properly perceive the surrounding environment and to carry out targeted operations. The most recent methods make use of state-of-the-art machine learning techniques to learn a valid model for the target task. However, those techniques need a large amount of labeled data for training. A recent approach to deal with this issue is data augmentation through Generative Adversarial Networks (GANs), where entire synthetic scenes are added to the training data, thus enlarging and diversifying their informative content. In this work, we propose an alternative solution with respect to the common data augmentation methods, applying it to the fundamental problem of crop/weed segmentation in precision farming. Starting from real images, we create semi-artificial samples by replacing the most relevant object classes (i.e., crop and weeds) with their synthesized counterparts. To do that, we employ a conditional GAN (cGAN), where the generative model is trained by conditioning the shape of the generated object. Moreover, in addition to RGB data, we take into account also near-infrared (NIR) information, generating four channel multi-spectral synthetic images. Quantitative experiments, carried out on three publicly available datasets, show that (i) our model is capable of generating realistic multi-spectral images of plants and (ii) the usage of such synthetic images in the training process improves the segmentation performance of state-of-the-art semantic segmentation convolutional networks.



### Segmentation of Lungs in Chest X-Ray Image Using Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2009.05752v1
- **DOI**: 10.1109/ACCESS.2020.3017915
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.05752v1)
- **Published**: 2020-09-12 08:54:54+00:00
- **Updated**: 2020-09-12 08:54:54+00:00
- **Authors**: Faizan Munawar, Shoaib Azmat, Talha Iqbal, Christer Grönlund, Hazrat Ali
- **Comment**: Volume 8, August 2020, Pages 153535 - 153545
- **Journal**: in IEEE Access, vol. 8, pp. 153535-153545, 2020
- **Summary**: Chest X-ray (CXR) is a low-cost medical imaging technique. It is a common procedure for the identification of many respiratory diseases compared to MRI, CT, and PET scans. This paper presents the use of generative adversarial networks (GAN) to perform the task of lung segmentation on a given CXR. GANs are popular to generate realistic data by learning the mapping from one domain to another. In our work, the generator of the GAN is trained to generate a segmented mask of a given input CXR. The discriminator distinguishes between a ground truth and the generated mask, and updates the generator through the adversarial loss measure. The objective is to generate masks for the input CXR, which are as realistic as possible compared to the ground truth masks. The model is trained and evaluated using four different discriminators referred to as D1, D2, D3, and D4, respectively. Experimental results on three different CXR datasets reveal that the proposed model is able to achieve a dice-score of 0.9740, and IOU score of 0.943, which are better than other reported state-of-the art results.



### Supervised Learning with Projected Entangled Pair States
- **Arxiv ID**: http://arxiv.org/abs/2009.09932v1
- **DOI**: 10.1103/PhysRevB.103.125117
- **Categories**: **cs.CV**, cond-mat.str-el, cs.LG, quant-ph, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2009.09932v1)
- **Published**: 2020-09-12 09:15:00+00:00
- **Updated**: 2020-09-12 09:15:00+00:00
- **Authors**: Song Cheng, Lei Wang, Pan Zhang
- **Comment**: 7 pages, 4 figures, 1 table
- **Journal**: Phys. Rev. B 103, 125117 (2021)
- **Summary**: Tensor networks, a model that originated from quantum physics, has been gradually generalized as efficient models in machine learning in recent years. However, in order to achieve exact contraction, only tree-like tensor networks such as the matrix product states and tree tensor networks have been considered, even for modeling two-dimensional data such as images. In this work, we construct supervised learning models for images using the projected entangled pair states (PEPS), a two-dimensional tensor network having a similar structure prior to natural images. Our approach first performs a feature map, which transforms the image data to a product state on a grid, then contracts the product state to a PEPS with trainable parameters to predict image labels. The tensor elements of PEPS are trained by minimizing differences between training labels and predicted labels. The proposed model is evaluated on image classifications using the MNIST and the Fashion-MNIST datasets. We show that our model is significantly superior to existing models using tree-like tensor networks. Moreover, using the same input features, our method performs as well as the multilayer perceptron classifier, but with much fewer parameters and is more stable. Our results shed light on potential applications of two-dimensional tensor network models in machine learning.



### Enhancing Unsupervised Video Representation Learning by Decoupling the Scene and the Motion
- **Arxiv ID**: http://arxiv.org/abs/2009.05757v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.05757v3)
- **Published**: 2020-09-12 09:54:11+00:00
- **Updated**: 2020-12-16 10:45:17+00:00
- **Authors**: Jinpeng Wang, Yuting Gao, Ke Li, Jianguo Hu, Xinyang Jiang, Xiaowei Guo, Rongrong Ji, Xing Sun
- **Comment**: AAAI2021 camera-ready
- **Journal**: None
- **Summary**: One significant factor we expect the video representation learning to capture, especially in contrast with the image representation learning, is the object motion. However, we found that in the current mainstream video datasets, some action categories are highly related with the scene where the action happens, making the model tend to degrade to a solution where only the scene information is encoded. For example, a trained model may predict a video as playing football simply because it sees the field, neglecting that the subject is dancing as a cheerleader on the field. This is against our original intention towards the video representation learning and may bring scene bias on different dataset that can not be ignored. In order to tackle this problem, we propose to decouple the scene and the motion (DSM) with two simple operations, so that the model attention towards the motion information is better paid. Specifically, we construct a positive clip and a negative clip for each video. Compared to the original video, the positive/negative is motion-untouched/broken but scene-broken/untouched by Spatial Local Disturbance and Temporal Local Disturbance. Our objective is to pull the positive closer while pushing the negative farther to the original clip in the latent space. In this way, the impact of the scene is weakened while the temporal sensitivity of the network is further enhanced. We conduct experiments on two tasks with various backbones and different pre-training datasets, and find that our method surpass the SOTA methods with a remarkable 8.1% and 8.8% improvement towards action recognition task on the UCF101 and HMDB51 datasets respectively using the same backbone.



### Removing the Background by Adding the Background: Towards Background Robust Self-supervised Video Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2009.05769v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.05769v4)
- **Published**: 2020-09-12 11:25:13+00:00
- **Updated**: 2021-04-22 03:37:30+00:00
- **Authors**: Jinpeng Wang, Yuting Gao, Ke Li, Yiqi Lin, Andy J. Ma, Hao Cheng, Pai Peng, Feiyue Huang, Rongrong Ji, Xing Sun
- **Comment**: CVPR2021 camera ready
- **Journal**: None
- **Summary**: Self-supervised learning has shown great potentials in improving the video representation ability of deep neural networks by getting supervision from the data itself. However, some of the current methods tend to cheat from the background, i.e., the prediction is highly dependent on the video background instead of the motion, making the model vulnerable to background changes. To mitigate the model reliance towards the background, we propose to remove the background impact by adding the background. That is, given a video, we randomly select a static frame and add it to every other frames to construct a distracting video sample. Then we force the model to pull the feature of the distracting video and the feature of the original video closer, so that the model is explicitly restricted to resist the background influence, focusing more on the motion changes. We term our method as \emph{Background Erasing} (BE). It is worth noting that the implementation of our method is so simple and neat and can be added to most of the SOTA methods without much efforts. Specifically, BE brings 16.4% and 19.1% improvements with MoCo on the severely biased datasets UCF101 and HMDB51, and 14.5% improvement on the less biased dataset Diving48.



### Micro-Facial Expression Recognition Based on Deep-Rooted Learning Algorithm
- **Arxiv ID**: http://arxiv.org/abs/2009.05778v1
- **DOI**: 10.2991/IJCIS.D.190801.001
- **Categories**: **cs.CV**, cs.AI, cs.MM, I.2.1; I.2.6; I.2.10; I.4.3; I.4.7; I.4.8; I.4.9; I.4.10; I.5.3;
  I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/2009.05778v1)
- **Published**: 2020-09-12 12:23:27+00:00
- **Updated**: 2020-09-12 12:23:27+00:00
- **Authors**: S. D. Lalitha, K. K. Thyagharajan
- **Comment**: 20 pages, 7 figures, "for the published version of the article, see
  https://www.atlantis-press.com/journals/ijcis/125915627"
- **Journal**: 12 (2) 903 - 913 2019/08 International Journal of Computational
  Intelligence Systems
- **Summary**: Facial expressions are important cues to observe human emotions. Facial expression recognition has attracted many researchers for years, but it is still a challenging topic since expression features vary greatly with the head poses, environments, and variations in the different persons involved. In this work, three major steps are involved to improve the performance of micro-facial expression recognition. First, an Adaptive Homomorphic Filtering is used for face detection and rotation rectification processes. Secondly, Micro-facial features were used to extract the appearance variations of a testing image-spatial analysis. The features of motion information are used for expression recognition in a sequence of facial images. An effective Micro-Facial Expression Based Deep-Rooted Learning (MFEDRL) classifier is proposed in this paper to better recognize spontaneous micro-expressions by learning parameters on the optimal features. This proposed method includes two loss functions such as cross entropy loss function and centre loss function. Then the performance of the algorithm will be evaluated using recognition rate and false measures. Simulation results show that the predictive performance of the proposed method outperforms that of the existing classifiers such as Convolutional Neural Network (CNN), Deep Neural Network (DNN), Artificial Neural Network (ANN), Support Vector Machine (SVM), and k-Nearest Neighbours (KNN) in terms of accuracy and Mean Absolute Error (MAE).



### A CNN Based Approach for the Near-Field Photometric Stereo Problem
- **Arxiv ID**: http://arxiv.org/abs/2009.05792v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.05792v1)
- **Published**: 2020-09-12 13:28:28+00:00
- **Updated**: 2020-09-12 13:28:28+00:00
- **Authors**: Fotios Logothetis, Ignas Budvytis, Roberto Mecca, Roberto Cipolla
- **Comment**: None
- **Journal**: None
- **Summary**: Reconstructing the 3D shape of an object using several images under different light sources is a very challenging task, especially when realistic assumptions such as light propagation and attenuation, perspective viewing geometry and specular light reflection are considered. Many of works tackling Photometric Stereo (PS) problems often relax most of the aforementioned assumptions. Especially they ignore specular reflection and global illumination effects. In this work, we propose the first CNN based approach capable of handling these realistic assumptions in Photometric Stereo. We leverage recent improvements of deep neural networks for far-field Photometric Stereo and adapt them to near field setup. We achieve this by employing an iterative procedure for shape estimation which has two main steps. Firstly we train a per-pixel CNN to predict surface normals from reflectance samples. Secondly, we compute the depth by integrating the normal field in order to iteratively estimate light directions and attenuation which is used to compensate the input images to compute reflectance samples for the next iteration. To the best of our knowledge this is the first near-field framework which is able to accurately predict 3D shape from highly specular objects. Our method outperforms competing state-of-the-art near-field Photometric Stereo approaches on both synthetic and real experiments.



### Revisiting the Threat Space for Vision-based Keystroke Inference Attacks
- **Arxiv ID**: http://arxiv.org/abs/2009.05796v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/2009.05796v1)
- **Published**: 2020-09-12 14:03:55+00:00
- **Updated**: 2020-09-12 14:03:55+00:00
- **Authors**: John Lim, True Price, Fabian Monrose, Jan-Michael Frahm
- **Comment**: None
- **Journal**: None
- **Summary**: A vision-based keystroke inference attack is a side-channel attack in which an attacker uses an optical device to record users on their mobile devices and infer their keystrokes. The threat space for these attacks has been studied in the past, but we argue that the defining characteristics for this threat space, namely the strength of the attacker, are outdated. Previous works do not study adversaries with vision systems that have been trained with deep neural networks because these models require large amounts of training data and curating such a dataset is expensive. To address this, we create a large-scale synthetic dataset to simulate the attack scenario for a keystroke inference attack. We show that first pre-training on synthetic data, followed by adopting transfer learning techniques on real-life data, increases the performance of our deep learning models. This indicates that these models are able to learn rich, meaningful representations from our synthetic data and that training on the synthetic data can help overcome the issue of having small, real-life datasets for vision-based key stroke inference attacks. For this work, we focus on single keypress classification where the input is a frame of a keypress and the output is a predicted key. We are able to get an accuracy of 95.6% after pre-training a CNN on our synthetic data and training on a small set of real-life data in an adversarial domain adaptation framework. Source Code for Simulator: https://github.com/jlim13/keystroke-inference-attack-synthetic-dataset-generator-



### Learning semantic Image attributes using Image recognition and knowledge graph embeddings
- **Arxiv ID**: http://arxiv.org/abs/2009.05812v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2009.05812v1)
- **Published**: 2020-09-12 15:18:48+00:00
- **Updated**: 2020-09-12 15:18:48+00:00
- **Authors**: Ashutosh Tiwari, Sandeep Varma
- **Comment**: 7 Pages, 6 figures, Accepted at Future Technologies Conference (FTC)
  2020, Vancouver, Canada
- **Journal**: None
- **Summary**: Extracting structured knowledge from texts has traditionally been used for knowledge base generation. However, other sources of information, such as images can be leveraged into this process to build more complete and richer knowledge bases. Structured semantic representation of the content of an image and knowledge graph embeddings can provide a unique representation of semantic relationships between image entities. Linking known entities in knowledge graphs and learning open-world images using language models has attracted lots of interest over the years. In this paper, we propose a shared learning approach to learn semantic attributes of images by combining a knowledge graph embedding model with the recognized attributes of images. The proposed model premises to help us understand the semantic relationship between the entities of an image and implicitly provide a link for the extracted entities through a knowledge graph embedding model. Under the limitation of using a custom user-defined knowledge base with limited data, the proposed model presents significant accuracy and provides a new alternative to the earlier approaches. The proposed approach is a step towards bridging the gap between frameworks which learn from large amounts of data and frameworks which use a limited set of predicates to infer new knowledge.



### Multi-Channel Potts-Based Reconstruction for Multi-Spectral Computed Tomography
- **Arxiv ID**: http://arxiv.org/abs/2009.05814v2
- **DOI**: None
- **Categories**: **math.NA**, cs.CV, cs.NA, 94A08, 68U10, 65D18, 90C26, 90C39
- **Links**: [PDF](http://arxiv.org/pdf/2009.05814v2)
- **Published**: 2020-09-12 15:33:47+00:00
- **Updated**: 2021-03-10 09:31:27+00:00
- **Authors**: Lukas Kiefer, Stefania Petra, Martin Storath, Andreas Weinmann
- **Comment**: 37 pages, 12 figures
- **Journal**: None
- **Summary**: We consider reconstructing multi-channel images from measurements performed by photon-counting and energy-discriminating detectors in the setting of multi-spectral X-ray computed tomography (CT). Our aim is to exploit the strong structural correlation that is known to exist between the channels of multi-spectral CT images. To that end, we adopt the multi-channel Potts prior to jointly reconstruct all channels. This prior produces piecewise constant solutions with strongly correlated channels. In particular, edges are enforced to have the same spatial position across channels which is a benefit over TV-based methods. We consider the Potts prior in two frameworks: (a) in the context of a variational Potts model, and (b) in a Potts-superiorization approach that perturbs the iterates of a basic iterative least squares solver. We identify an alternating direction method of multipliers (ADMM) approach as well as a Potts-superiorized conjugate gradient method as particularly suitable. In numerical experiments, we compare the Potts prior based approaches to existing TV-type approaches on realistically simulated multi-spectral CT data and obtain improved reconstruction for compound solid bodies.



### Map-merging Algorithms for Visual SLAM: Feasibility Study and Empirical Evaluation
- **Arxiv ID**: http://arxiv.org/abs/2009.05819v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.05819v1)
- **Published**: 2020-09-12 16:15:16+00:00
- **Updated**: 2020-09-12 16:15:16+00:00
- **Authors**: Andrey Bokovoy, Kirill Muraviev, Konstantin Yakovlev
- **Comment**: Camera-ready version as submitted to RCAI-2020
- **Journal**: None
- **Summary**: Simultaneous localization and mapping, especially the one relying solely on video data (vSLAM), is a challenging problem that has been extensively studied in robotics and computer vision. State-of-the-art vSLAM algorithms are capable of constructing accurate-enough maps that enable a mobile robot to autonomously navigate an unknown environment. In this work, we are interested in an important problem related to vSLAM, i.e. map merging, that might appear in various practically important scenarios, e.g. in a multi-robot coverage scenario. This problem asks whether different vSLAM maps can be merged into a consistent single representation. We examine the existing 2D and 3D map-merging algorithms and conduct an extensive empirical evaluation in realistic simulated environment (Habitat). Both qualitative and quantitative comparison is carried out and the obtained results are reported and analyzed.



### Exploring the Hierarchy in Relation Labels for Scene Graph Generation
- **Arxiv ID**: http://arxiv.org/abs/2009.05834v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2009.05834v1)
- **Published**: 2020-09-12 17:36:53+00:00
- **Updated**: 2020-09-12 17:36:53+00:00
- **Authors**: Yi Zhou, Shuyang Sun, Chao Zhang, Yikang Li, Wanli Ouyang
- **Comment**: None
- **Journal**: None
- **Summary**: By assigning each relationship a single label, current approaches formulate the relationship detection as a classification problem. Under this formulation, predicate categories are treated as completely different classes. However, different from the object labels where different classes have explicit boundaries, predicates usually have overlaps in their semantic meanings. For example, sit\_on and stand\_on have common meanings in vertical relationships but different details of how these two objects are vertically placed. In order to leverage the inherent structures of the predicate categories, we propose to first build the language hierarchy and then utilize the Hierarchy Guided Feature Learning (HGFL) strategy to learn better region features of both the coarse-grained level and the fine-grained level. Besides, we also propose the Hierarchy Guided Module (HGM) to utilize the coarse-grained level to guide the learning of fine-grained level features. Experiments show that the proposed simple yet effective method can improve several state-of-the-art baselines by a large margin (up to $33\%$ relative gain) in terms of Recall@50 on the task of Scene Graph Generation in different datasets.



### How Much Can We Really Trust You? Towards Simple, Interpretable Trust Quantification Metrics for Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2009.05835v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.05835v3)
- **Published**: 2020-09-12 17:37:36+00:00
- **Updated**: 2021-04-03 15:08:50+00:00
- **Authors**: Alexander Wong, Xiao Yu Wang, Andrew Hryniowski
- **Comment**: 13 pages
- **Journal**: None
- **Summary**: A critical step to building trustworthy deep neural networks is trust quantification, where we ask the question: How much can we trust a deep neural network? In this study, we take a step towards simple, interpretable metrics for trust quantification by introducing a suite of metrics for assessing the overall trustworthiness of deep neural networks based on their behaviour when answering a set of questions. We conduct a thought experiment and explore two key questions about trust in relation to confidence: 1) How much trust do we have in actors who give wrong answers with great confidence? and 2) How much trust do we have in actors who give right answers hesitantly? Based on insights gained, we introduce the concept of question-answer trust to quantify trustworthiness of an individual answer based on confident behaviour under correct and incorrect answer scenarios, and the concept of trust density to characterize the distribution of overall trust for an individual answer scenario. We further introduce the concept of trust spectrum for representing overall trust with respect to the spectrum of possible answer scenarios across correctly and incorrectly answered questions. Finally, we introduce NetTrustScore, a scalar metric summarizing overall trustworthiness. The suite of metrics aligns with past social psychology studies that study the relationship between trust and confidence. Leveraging these metrics, we quantify the trustworthiness of several well-known deep neural network architectures for image recognition to get a deeper understanding of where trust breaks down. The proposed metrics are by no means perfect, but the hope is to push the conversation towards better metrics to help guide practitioners and regulators in producing, deploying, and certifying deep learning solutions that can be trusted to operate in real-world, mission-critical scenarios.



### FairCVtest Demo: Understanding Bias in Multimodal Learning with a Testbed in Fair Automatic Recruitment
- **Arxiv ID**: http://arxiv.org/abs/2009.07025v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.07025v1)
- **Published**: 2020-09-12 17:45:09+00:00
- **Updated**: 2020-09-12 17:45:09+00:00
- **Authors**: Alejandro Peña, Ignacio Serna, Aythami Morales, Julian Fierrez
- **Comment**: ACM Intl. Conf. on Multimodal Interaction (ICMI). arXiv admin note:
  substantial text overlap with arXiv:2004.07173
- **Journal**: None
- **Summary**: With the aim of studying how current multimodal AI algorithms based on heterogeneous sources of information are affected by sensitive elements and inner biases in the data, this demonstrator experiments over an automated recruitment testbed based on Curriculum Vitae: FairCVtest. The presence of decision-making algorithms in society is rapidly increasing nowadays, while concerns about their transparency and the possibility of these algorithms becoming new sources of discrimination are arising. This demo shows the capacity of the Artificial Intelligence (AI) behind a recruitment tool to extract sensitive information from unstructured data, and exploit it in combination to data biases in undesirable (unfair) ways. Aditionally, the demo includes a new algorithm (SensitiveNets) for discrimination-aware learning which eliminates sensitive information in our multimodal AI framework.



### A Unified Approach to Kinship Verification
- **Arxiv ID**: http://arxiv.org/abs/2009.05871v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.05871v1)
- **Published**: 2020-09-12 22:13:56+00:00
- **Updated**: 2020-09-12 22:13:56+00:00
- **Authors**: Eran Dahan, Yosi Keller
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we propose a deep learning-based approach for kin verification using a unified multi-task learning scheme where all kinship classes are jointly learned. This allows us to better utilize small training sets that are typical of kin verification. We introduce a novel approach for fusing the embeddings of kin images, to avoid overfitting, which is a common issue in training such networks. An adaptive sampling scheme is derived for the training set images to resolve the inherent imbalance in kin verification datasets. A thorough ablation study exemplifies the effectivity of our approach, which is experimentally shown to outperform contemporary state-of-the-art kin verification results when applied to the Families In the Wild, FG2018, and FG2020 datasets.



### An approach to human iris recognition using quantitative analysis of image features and machine learning
- **Arxiv ID**: http://arxiv.org/abs/2009.05880v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2009.05880v1)
- **Published**: 2020-09-12 23:23:33+00:00
- **Updated**: 2020-09-12 23:23:33+00:00
- **Authors**: Abolfazl Zargari Khuzani, Najmeh Mashhadi, Morteza Heidari, Donya Khaledyan
- **Comment**: None
- **Journal**: None
- **Summary**: The Iris pattern is a unique biological feature for each individual, making it a valuable and powerful tool for human identification. In this paper, an efficient framework for iris recognition is proposed in four steps. (1) Iris segmentation (using a relative total variation combined with Coarse Iris Localization), (2) feature extraction (using Shape&density, FFT, GLCM, GLDM, and Wavelet), (3) feature reduction (employing Kernel-PCA) and (4) classification (applying multi-layer neural network) to classify 2000 iris images of CASIA-Iris-Interval dataset obtained from 200 volunteers. The results confirm that the proposed scheme can provide a reliable prediction with an accuracy of up to 99.64%.



