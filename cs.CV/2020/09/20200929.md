# Arxiv Papers in cs.CV on 2020-09-29
### Mathematical derivation for Vora-Value based filter design method: Gradient and Hessian
- **Arxiv ID**: http://arxiv.org/abs/2009.13696v2
- **DOI**: None
- **Categories**: **math.OC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.13696v2)
- **Published**: 2020-09-29 00:14:56+00:00
- **Updated**: 2020-10-04 16:49:05+00:00
- **Authors**: Yuteng Zhu, Graham D. Finlayson
- **Comment**: 6 pages, correct typos and improve the mathematical proof
- **Journal**: None
- **Summary**: In this paper, we present the detailed mathematical derivation of the gradient and Hessian matrix for the Vora-Value based colorimetric filter optimization. We make a full recapitulation of the steps involved in differentiating the objective function and reveal the positive-definite Hessian matrix when a positive regularizer is applied. This paper serves as a supplementary material for our paper in the colorimetric filter design theory.



### Learn like a Pathologist: Curriculum Learning by Annotator Agreement for Histopathology Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2009.13698v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.13698v1)
- **Published**: 2020-09-29 00:25:21+00:00
- **Updated**: 2020-09-29 00:25:21+00:00
- **Authors**: Jerry Wei, Arief Suriawinata, Bing Ren, Xiaoying Liu, Mikhail Lisovsky, Louis Vaickus, Charles Brown, Michael Baker, Mustafa Nasir-Moin, Naofumi Tomita, Lorenzo Torresani, Jason Wei, Saeed Hassanpour
- **Comment**: None
- **Journal**: None
- **Summary**: Applying curriculum learning requires both a range of difficulty in data and a method for determining the difficulty of examples. In many tasks, however, satisfying these requirements can be a formidable challenge. In this paper, we contend that histopathology image classification is a compelling use case for curriculum learning. Based on the nature of histopathology images, a range of difficulty inherently exists among examples, and, since medical datasets are often labeled by multiple annotators, annotator agreement can be used as a natural proxy for the difficulty of a given example. Hence, we propose a simple curriculum learning method that trains on progressively-harder images as determined by annotator agreement. We evaluate our hypothesis on the challenging and clinically-important task of colorectal polyp classification. Whereas vanilla training achieves an AUC of 83.7% for this task, a model trained with our proposed curriculum learning approach achieves an AUC of 88.2%, an improvement of 4.5%. Our work aims to inspire researchers to think more creatively and rigorously when choosing contexts for applying curriculum learning.



### Cranial Implant Design via Virtual Craniectomy with Shape Priors
- **Arxiv ID**: http://arxiv.org/abs/2009.13704v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.13704v1)
- **Published**: 2020-09-29 00:35:44+00:00
- **Updated**: 2020-09-29 00:35:44+00:00
- **Authors**: Franco Matzkin, Virginia Newcombe, Ben Glocker, Enzo Ferrante
- **Comment**: None
- **Journal**: None
- **Summary**: Cranial implant design is a challenging task, whose accuracy is crucial in the context of cranioplasty procedures. This task is usually performed manually by experts using computer-assisted design software. In this work, we propose and evaluate alternative automatic deep learning models for cranial implant reconstruction from CT images. The models are trained and evaluated using the database released by the AutoImplant challenge, and compared to a baseline implemented by the organizers. We employ a simulated virtual craniectomy to train our models using complete skulls, and compare two different approaches trained with this procedure. The first one is a direct estimation method based on the UNet architecture. The second method incorporates shape priors to increase the robustness when dealing with out-of-distribution implant shapes. Our direct estimation method outperforms the baselines provided by the organizers, while the model with shape priors shows superior performance when dealing with out-of-distribution cases. Overall, our methods show promising results in the difficult task of cranial implant design.



### Learning to Generate Image Source-Agnostic Universal Adversarial Perturbations
- **Arxiv ID**: http://arxiv.org/abs/2009.13714v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2009.13714v4)
- **Published**: 2020-09-29 01:23:20+00:00
- **Updated**: 2022-08-17 23:00:11+00:00
- **Authors**: Pu Zhao, Parikshit Ram, Songtao Lu, Yuguang Yao, Djallel Bouneffouf, Xue Lin, Sijia Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Adversarial perturbations are critical for certifying the robustness of deep learning models. A universal adversarial perturbation (UAP) can simultaneously attack multiple images, and thus offers a more unified threat model, obviating an image-wise attack algorithm. However, the existing UAP generator is underdeveloped when images are drawn from different image sources (e.g., with different image resolutions). Towards an authentic universality across image sources, we take a novel view of UAP generation as a customized instance of few-shot learning, which leverages bilevel optimization and learning-to-optimize (L2O) techniques for UAP generation with improved attack success rate (ASR). We begin by considering the popular model agnostic meta-learning (MAML) framework to meta-learn a UAP generator. However, we see that the MAML framework does not directly offer the universal attack across image sources, requiring us to integrate it with another meta-learning framework of L2O. The resulting scheme for meta-learning a UAP generator (i) has better performance (50% higher ASR) than baselines such as Projected Gradient Descent, (ii) has better performance (37% faster) than the vanilla L2O and MAML frameworks (when applicable), and (iii) is able to simultaneously handle UAP generation for different victim models and image data sources.



### Grow-Push-Prune: aligning deep discriminants for effective structural network compression
- **Arxiv ID**: http://arxiv.org/abs/2009.13716v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.13716v3)
- **Published**: 2020-09-29 01:29:23+00:00
- **Updated**: 2021-10-02 01:03:05+00:00
- **Authors**: Qing Tian, Tal Arbel, James J. Clark
- **Comment**: None
- **Journal**: None
- **Summary**: Most of today's popular deep architectures are hand-engineered to be generalists. However, this design procedure usually leads to massive redundant, useless, or even harmful features for specific tasks. Unnecessarily high complexities render deep nets impractical for many real-world applications, especially those without powerful GPU support. In this paper, we attempt to derive task-dependent compact models from a deep discriminant analysis perspective. We propose an iterative and proactive approach for classification tasks which alternates between (1) a pushing step, with an objective to simultaneously maximize class separation, penalize co-variances, and push deep discriminants into alignment with a compact set of neurons, and (2) a pruning step, which discards less useful or even interfering neurons. Deconvolution is adopted to reverse 'unimportant' filters' effects and recover useful contributing sources. A simple network growing strategy based on the basic Inception module is proposed for challenging tasks requiring larger capacity than what the base net can offer. Experiments on the MNIST, CIFAR10, and ImageNet datasets demonstrate our approach's efficacy. On ImageNet, by pushing and pruning our grown Inception-88 model, we achieve more accurate models than Inception nets generated during growing, residual nets, and popular compact nets at similar sizes. We also show that our grown Inception nets (without hard-coded dimension alignment) clearly outperform residual nets of similar complexities.



### A Comprehensive Review for MRF and CRF Approaches in Pathology Image Analysis
- **Arxiv ID**: http://arxiv.org/abs/2009.13721v4
- **DOI**: 10.1007/s11831-021-09591-w
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.13721v4)
- **Published**: 2020-09-29 01:46:22+00:00
- **Updated**: 2021-05-09 02:42:16+00:00
- **Authors**: Yixin Li, Chen Li, Xiaoyan Li, Kai Wang, Md Mamunur Rahaman, Changhao Sun, Hao Chen, Xinran Wu, Hong Zhang, Qian Wang
- **Comment**: Arch Computat Methods Eng (2021)
- **Journal**: None
- **Summary**: Pathology image analysis is an essential procedure for clinical diagnosis of many diseases. To boost the accuracy and objectivity of detection, nowadays, an increasing number of computer-aided diagnosis (CAD) system is proposed. Among these methods, random field models play an indispensable role in improving the analysis performance. In this review, we present a comprehensive overview of pathology image analysis based on the markov random fields (MRFs) and conditional random fields (CRFs), which are two popular random field models. Firstly, we introduce the background of two random fields and pathology images. Secondly, we summarize the basic mathematical knowledge of MRFs and CRFs from modelling to optimization. Then, a thorough review of the recent research on the MRFs and CRFs of pathology images analysis is presented. Finally, we investigate the popular methodologies in the related works and discuss the method migration among CAD field.



### A Flow Base Bi-path Network for Cross-scene Video Crowd Understanding in Aerial View
- **Arxiv ID**: http://arxiv.org/abs/2009.13723v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.13723v1)
- **Published**: 2020-09-29 01:48:24+00:00
- **Updated**: 2020-09-29 01:48:24+00:00
- **Authors**: Zhiyuan Zhao, Tao Han, Junyu Gao, Qi Wang, Xuelong Li
- **Comment**: None
- **Journal**: None
- **Summary**: Drones shooting can be applied in dynamic traffic monitoring, object detecting and tracking, and other vision tasks. The variability of the shooting location adds some intractable challenges to these missions, such as varying scale, unstable exposure, and scene migration. In this paper, we strive to tackle the above challenges and automatically understand the crowd from the visual data collected from drones. First, to alleviate the background noise generated in cross-scene testing, a double-stream crowd counting model is proposed, which extracts optical flow and frame difference information as an additional branch. Besides, to improve the model's generalization ability at different scales and time, we randomly combine a variety of data transformation methods to simulate some unseen environments. To tackle the crowd density estimation problem under extreme dark environments, we introduce synthetic data generated by game Grand Theft Auto V(GTAV). Experiment results show the effectiveness of the virtual data. Our method wins the challenge with a mean absolute error (MAE) of 12.70. Moreover, a comprehensive ablation study is conducted to explore each component's contribution.



### Graph-based methods for analyzing orchard tree structure using noisy point cloud data
- **Arxiv ID**: http://arxiv.org/abs/2009.13727v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2009.13727v2)
- **Published**: 2020-09-29 02:07:30+00:00
- **Updated**: 2021-02-02 01:18:06+00:00
- **Authors**: Fredrik Westling, Dr James Underwood, Dr Mitch Bryson
- **Comment**: None
- **Journal**: None
- **Summary**: Digitisation of fruit trees using LiDAR enables analysis which can be used to better growing practices to improve yield. Sophisticated analysis requires geometric and semantic understanding of the data, including the ability to discern individual trees as well as identifying leafy and structural matter. Extraction of this information should be rapid, as should data capture, so that entire orchards can be processed, but existing methods for classification and segmentation rely on high-quality data or additional data sources like cameras. We present a method for analysis of LiDAR data specifically for individual tree location, segmentation and matter classification, which can operate on low-quality data captured by handheld or mobile LiDAR. Our methods for tree location and segmentation improved on existing methods with an F1 score of 0.774 and a v-measure of 0.915 respectively, while trunk matter classification performed poorly in absolute terms with an average F1 score of 0.490 on real data, though consistently outperformed existing methods and displayed a significantly shorter runtime.



### Fast and Incremental Loop Closure Detection with Deep Features and Proximity Graphs
- **Arxiv ID**: http://arxiv.org/abs/2010.11703v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2010.11703v2)
- **Published**: 2020-09-29 02:42:47+00:00
- **Updated**: 2022-01-02 13:49:52+00:00
- **Authors**: Shan An, Haogang Zhu, Dong Wei, Konstantinos A. Tsintotas, Antonios Gasteratos
- **Comment**: submitted to Journal of Field Robotics
- **Journal**: None
- **Summary**: In recent years, the robotics community has extensively examined methods concerning the place recognition task within the scope of simultaneous localization and mapping applications.This article proposes an appearance-based loop closure detection pipeline named ``FILD++" (Fast and Incremental Loop closure Detection).First, the system is fed by consecutive images and, via passing them twice through a single convolutional neural network, global and local deep features are extracted.Subsequently, a hierarchical navigable small-world graph incrementally constructs a visual database representing the robot's traversed path based on the computed global features.Finally, a query image, grabbed each time step, is set to retrieve similar locations on the traversed route.An image-to-image pairing follows, which exploits local features to evaluate the spatial information. Thus, in the proposed article, we propose a single network for global and local feature extraction in contrast to our previous work (FILD), while an exhaustive search for the verification process is adopted over the generated deep local features avoiding the utilization of hash codes. Exhaustive experiments on eleven publicly available datasets exhibit the system's high performance (achieving the highest recall score on eight of them) and low execution times (22.05 ms on average in New College, which is the largest one containing 52480 images) compared to other state-of-the-art approaches.



### MetaMix: Improved Meta-Learning with Interpolation-based Consistency Regularization
- **Arxiv ID**: http://arxiv.org/abs/2009.13735v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.13735v2)
- **Published**: 2020-09-29 02:44:13+00:00
- **Updated**: 2020-10-10 05:36:55+00:00
- **Authors**: Yangbin Chen, Yun Ma, Tom Ko, Jianping Wang, Qing Li
- **Comment**: 8 pages, 3 figures, 3 tables. Accepted by 25th International
  Conference on Pattern Recognition (ICPR) 2020
- **Journal**: None
- **Summary**: Model-Agnostic Meta-Learning (MAML) and its variants are popular few-shot classification methods. They train an initializer across a variety of sampled learning tasks (also known as episodes) such that the initialized model can adapt quickly to new tasks. However, current MAML-based algorithms have limitations in forming generalizable decision boundaries. In this paper, we propose an approach called MetaMix. It generates virtual feature-target pairs within each episode to regularize the backbone models. MetaMix can be integrated with any of the MAML-based algorithms and learn the decision boundaries generalizing better to new tasks. Experiments on the mini-ImageNet, CUB, and FC100 datasets show that MetaMix improves the performance of MAML-based algorithms and achieves state-of-the-art result when integrated with Meta-Transfer Learning.



### SwiftFace: Real-Time Face Detection
- **Arxiv ID**: http://arxiv.org/abs/2009.13743v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2009.13743v1)
- **Published**: 2020-09-29 03:09:29+00:00
- **Updated**: 2020-09-29 03:09:29+00:00
- **Authors**: Leonardo Ramos, Bernardo Morales
- **Comment**: None
- **Journal**: None
- **Summary**: Computer vision is a field of artificial intelligence that trains computers to interpret the visual world in a way similar to that of humans. Due to the rapid advancements in technology and the increasing availability of sufficiently large training datasets, the topics within computer vision have experienced a steep growth in the last decade. Among them, one of the most promising fields is face detection. Being used daily in a wide variety of fields; from mobile apps and augmented reality for entertainment purposes, to social studies and security cameras; designing high-performance models for face detection is crucial. On top of that, with the aforementioned growth in face detection technologies, precision and accuracy are no longer the only relevant factors: for real-time face detection, speed of detection is essential. SwiftFace is a novel deep learning model created solely to be a fast face detection model. By focusing only on detecting faces, SwiftFace performs 30% faster than current state-of-the-art face detection models. Code available at https://github.com/leo7r/swiftface



### Geometric Loss for Deep Multiple Sclerosis lesion Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2009.13755v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2009.13755v1)
- **Published**: 2020-09-29 03:49:28+00:00
- **Updated**: 2020-09-29 03:49:28+00:00
- **Authors**: Hang Zhang, Jinwei Zhang, Rongguang Wang, Qihao Zhang, Susan A. Gauthier, Pascal Spincemaille, Thanh D. Nguyen, Yi Wang
- **Comment**: 5 pages, three figures
- **Journal**: None
- **Summary**: Multiple sclerosis (MS) lesions occupy a small fraction of the brain volume, and are heterogeneous with regards to shape, size and locations, which poses a great challenge for training deep learning based segmentation models. We proposed a new geometric loss formula to address the data imbalance and exploit the geometric property of MS lesions. We showed that traditional region-based and boundary-aware loss functions can be associated with the formula. We further develop and instantiate two loss functions containing first- and second-order geometric information of lesion regions to enforce regularization on optimizing deep segmentation models. Experimental results on two MS lesion datasets with different scales, acquisition protocols and resolutions demonstrated the superiority of our proposed methods compared to other state-of-the-art methods.



### Motion Planning Combines Psychological Safety and Motion Prediction for a Sense Motive Robot
- **Arxiv ID**: http://arxiv.org/abs/2010.11671v2
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2010.11671v2)
- **Published**: 2020-09-29 04:19:53+00:00
- **Updated**: 2020-10-23 15:32:08+00:00
- **Authors**: Hejing Ling, Guoliang Liu, Guohui Tian
- **Comment**: submitted to RAL/ICRA2021
- **Journal**: None
- **Summary**: Human safety is the most important demand for human robot interaction and collaboration (HRIC), which not only refers to physical safety, but also includes psychological safety. Although many robots with different configurations have entered our living and working environments, the human safety problem is still an ongoing research problem in human-robot coexistence scenarios. This paper addresses the human safety issue by covering both the physical safety and psychological safety aspects. First, we introduce an adaptive robot velocity control and step size adjustment method according to human facial expressions, such that the robot can adjust its movement to keep safety when the human emotion is unusual. Second, we predict the human motion by detecting the suddenly changes of human head pose and gaze direction, such that the robot can infer whether the human attention is distracted, predict the next move of human and rebuild a repulsive force to avoid potential collision. Finally, we demonstrate our idea using a 7 DOF TIAGo robot in a dynamic HRIC environment, which shows that the robot becomes sense motive, and responds to human action and emotion changes quickly and efficiently.



### Knowledge Fusion Transformers for Video Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2009.13782v2
- **DOI**: None
- **Categories**: **cs.CV**, 65D19
- **Links**: [PDF](http://arxiv.org/pdf/2009.13782v2)
- **Published**: 2020-09-29 05:13:45+00:00
- **Updated**: 2020-09-30 03:53:44+00:00
- **Authors**: Ganesh Samarth, Sheetal Ojha, Nikhil Pareek
- **Comment**: 7 pages
- **Journal**: None
- **Summary**: We introduce Knowledge Fusion Transformers for video action classification. We present a self-attention based feature enhancer to fuse action knowledge in 3D inception based spatio-temporal context of the video clip intended to be classified. We show, how using only one stream networks and with little or, no pretraining can pave the way for a performance close to the current state-of-the-art. Additionally, we present how different self-attention architectures used at different levels of the network can be blended-in to enhance feature representation. Our architecture is trained and evaluated on UCF-101 and Charades dataset, where it is competitive with the state of the art. It also exceeds by a large gap from single stream networks with no to less pretraining.



### Micro-Facial Expression Recognition in Video Based on Optimal Convolutional Neural Network (MFEOCNN) Algorithm
- **Arxiv ID**: http://arxiv.org/abs/2009.13792v1
- **DOI**: 10.35940/ijeat.A9802.109119
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.MM, eess.IV, I.2.6; I.2.10; I.4.6; I.4.7; I.4.10; I.5
- **Links**: [PDF](http://arxiv.org/pdf/2009.13792v1)
- **Published**: 2020-09-29 05:56:26+00:00
- **Updated**: 2020-09-29 05:56:26+00:00
- **Authors**: S. D. Lalitha, K. K. Thyagharajan
- **Comment**: 19 pages, 10 figures, "for published version see
  https://www.ijeat.org/wp-content/uploads/papers/v9i1/A9802109119.pdf"
- **Journal**: None
- **Summary**: Facial expression is a standout amongst the most imperative features of human emotion recognition. For demonstrating the emotional states facial expressions are utilized by the people. In any case, recognition of facial expressions has persisted a testing and intriguing issue with regards to PC vision. Recognizing the Micro-Facial expression in video sequence is the main objective of the proposed approach. For efficient recognition, the proposed method utilizes the optimal convolution neural network. Here the proposed method considering the input dataset is the CK+ dataset. At first, by means of Adaptive median filtering preprocessing is performed in the input image. From the preprocessed output, the extracted features are Geometric features, Histogram of Oriented Gradients features and Local binary pattern features. The novelty of the proposed method is, with the help of Modified Lion Optimization (MLO) algorithm, the optimal features are selected from the extracted features. In a shorter computational time, it has the benefits of rapidly focalizing and effectively acknowledging with the aim of getting an overall arrangement or idea. Finally, the recognition is done by Convolution Neural network (CNN). Then the performance of the proposed MFEOCNN method is analysed in terms of false measures and recognition accuracy. This kind of emotion recognition is mainly used in medicine, marketing, E-learning, entertainment, law and monitoring. From the simulation, we know that the proposed approach achieves maximum recognition accuracy of 99.2% with minimum Mean Absolute Error (MAE) value. These results are compared with the existing for MicroFacial Expression Based Deep-Rooted Learning (MFEDRL), Convolutional Neural Network with Lion Optimization (CNN+LO) and Convolutional Neural Network (CNN) without optimization. The simulation of the proposed method is done in the working platform of MATLAB.



### A comparison of classical and variational autoencoders for anomaly detection
- **Arxiv ID**: http://arxiv.org/abs/2009.13793v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.13793v1)
- **Published**: 2020-09-29 05:58:31+00:00
- **Updated**: 2020-09-29 05:58:31+00:00
- **Authors**: Fabrizio Patuzzo
- **Comment**: None
- **Journal**: None
- **Summary**: This paper analyzes and compares a classical and a variational autoencoder in the context of anomaly detection. To better understand their architecture and functioning, describe their properties and compare their performance, it explores how they address a simple problem: reconstructing a line with a slope.



### Demonstration of a Cloud-based Software Framework for Video Analytics Application using Low-Cost IoT Devices
- **Arxiv ID**: http://arxiv.org/abs/2010.07680v1
- **DOI**: None
- **Categories**: **cs.CY**, cs.CV, cs.DC
- **Links**: [PDF](http://arxiv.org/pdf/2010.07680v1)
- **Published**: 2020-09-29 06:05:32+00:00
- **Updated**: 2020-09-29 06:05:32+00:00
- **Authors**: Bhavin Joshi, Tapan Pathak, Vatsal Patel, Sarth Kanani, Pankesh Patel, Muhammad Intizar Ali, John Breslin
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2009.09065
- **Journal**: None
- **Summary**: The design of products and services such as a Smart doorbell, demonstrating video analytics software/algorithm functionality, is expected to address a new kind of requirements such as designing a scalable solution while considering the trade-off between cost and accuracy; a flexible architecture to deploy new AI-based models or update existing models, as user requirements evolve; as well as seamlessly integrating different kinds of user interfaces and devices. To address these challenges, we propose a smart doorbell that orchestrates video analytics across Edge and Cloud resources. The proposal uses AWS as a base platform for implementation and leverages Commercially Available Off-The-Shelf(COTS) affordable devices such as Raspberry Pi in the form of an Edge device.



### MLOD: Awareness of Extrinsic Perturbation in Multi-LiDAR 3D Object Detection for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2010.11702v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2010.11702v1)
- **Published**: 2020-09-29 06:11:22+00:00
- **Updated**: 2020-09-29 06:11:22+00:00
- **Authors**: Jianhao Jiao, Peng Yun, Lei Tai, Ming Liu
- **Comment**: 8 pages, 6 figures
- **Journal**: IROS 2020
- **Summary**: Extrinsic perturbation always exists in multiple sensors. In this paper, we focus on the extrinsic uncertainty in multi-LiDAR systems for 3D object detection. We first analyze the influence of extrinsic perturbation on geometric tasks with two basic examples. To minimize the detrimental effect of extrinsic perturbation, we propagate an uncertainty prior on each point of input point clouds, and use this information to boost an approach for 3D geometric tasks. Then we extend our findings to propose a multi-LiDAR 3D object detector called MLOD. MLOD is a two-stage network where the multi-LiDAR information is fused through various schemes in stage one, and the extrinsic perturbation is handled in stage two. We conduct extensive experiments on a real-world dataset, and demonstrate both the accuracy and robustness improvement of MLOD. The code, data and supplementary materials are available at: https://ram-lab.com/file/site/mlod



### BAMSProd: A Step towards Generalizing the Adaptive Optimization Methods to Deep Binary Model
- **Arxiv ID**: http://arxiv.org/abs/2009.13799v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.13799v1)
- **Published**: 2020-09-29 06:12:32+00:00
- **Updated**: 2020-09-29 06:12:32+00:00
- **Authors**: Junjie Liu, Dongchao Wen, Deyu Wang, Wei Tao, Tse-Wei Chen, Kinya Osa, Masami Kato
- **Comment**: 10 pages, 4 figures, 2 tables
- **Journal**: 2020 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition Workshops (CVPRW)
- **Summary**: Recent methods have significantly reduced the performance degradation of Binary Neural Networks (BNNs), but guaranteeing the effective and efficient training of BNNs is an unsolved problem. The main reason is that the estimated gradients produced by the Straight-Through-Estimator (STE) mismatches with the gradients of the real derivatives. In this paper, we provide an explicit convex optimization example where training the BNNs with the traditionally adaptive optimization methods still faces the risk of non-convergence, and identify that constraining the range of gradients is critical for optimizing the deep binary model to avoid highly suboptimal solutions. For solving above issues, we propose a BAMSProd algorithm with a key observation that the convergence property of optimizing deep binary model is strongly related to the quantization errors. In brief, it employs an adaptive range constraint via an errors measurement for smoothing the gradients transition while follows the exponential moving strategy from AMSGrad to avoid errors accumulation during the optimization. The experiments verify the corollary of theoretical convergence analysis, and further demonstrate that our optimization method can speed up the convergence about 1:2x and boost the performance of BNNs to a significant level than the specific binary optimizer about 3:7%, even in a highly non-convex optimization problem.



### Self-grouping Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2009.13803v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.13803v1)
- **Published**: 2020-09-29 06:24:32+00:00
- **Updated**: 2020-09-29 06:24:32+00:00
- **Authors**: Qingbei Guo, Xiao-Jun Wu, Josef Kittler, Zhiquan Feng
- **Comment**: None
- **Journal**: None
- **Summary**: Although group convolution operators are increasingly used in deep convolutional neural networks to improve the computational efficiency and to reduce the number of parameters, most existing methods construct their group convolution architectures by a predefined partitioning of the filters of each convolutional layer into multiple regular filter groups with an equal spatial group size and data-independence, which prevents a full exploitation of their potential. To tackle this issue, we propose a novel method of designing self-grouping convolutional neural networks, called SG-CNN, in which the filters of each convolutional layer group themselves based on the similarity of their importance vectors. Concretely, for each filter, we first evaluate the importance value of their input channels to identify the importance vectors, and then group these vectors by clustering. Using the resulting \emph{data-dependent} centroids, we prune the less important connections, which implicitly minimizes the accuracy loss of the pruning, thus yielding a set of \emph{diverse} group convolution filters. Subsequently, we develop two fine-tuning schemes, i.e. (1) both local and global fine-tuning and (2) global only fine-tuning, which experimentally deliver comparable results, to recover the recognition capacity of the pruned network. Comprehensive experiments carried out on the CIFAR-10/100 and ImageNet datasets demonstrate that our self-grouping convolution method adapts to various state-of-the-art CNN architectures, such as ResNet and DenseNet, and delivers superior performance in terms of compression ratio, speedup and recognition accuracy. We demonstrate the ability of SG-CNN to generalise by transfer learning, including domain adaption and object detection, showing competitive results. Our source code is available at https://github.com/QingbeiGuo/SG-CNN.git.



### An Image Processing Pipeline for Automated Packaging Structure Recognition
- **Arxiv ID**: http://arxiv.org/abs/2009.13824v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.13824v1)
- **Published**: 2020-09-29 07:26:08+00:00
- **Updated**: 2020-09-29 07:26:08+00:00
- **Authors**: Laura Dörr, Felix Brandt, Martin Pouls, Alexander Naumann
- **Comment**: To be published in: "Forum Bildverarbeitung 2020", KIT Scientific
  Publishing, Karlsruhe
- **Journal**: None
- **Summary**: Dispatching and receiving logistics goods, as well as transportation itself, involve a high amount of manual efforts. The transported goods, including their packaging and labeling, need to be double-checked, verified or recognized at many supply chain network points. These processes hold automation potentials, which we aim to exploit using computer vision techniques. More precisely, we propose a cognitive system for the fully automated recognition of packaging structures for standardized logistics shipments based on single RGB images. Our contribution contains descriptions of a suitable system design and its evaluation on relevant real-world data. Further, we discuss our algorithmic choices.



### TinyGAN: Distilling BigGAN for Conditional Image Generation
- **Arxiv ID**: http://arxiv.org/abs/2009.13829v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.13829v1)
- **Published**: 2020-09-29 07:33:49+00:00
- **Updated**: 2020-09-29 07:33:49+00:00
- **Authors**: Ting-Yun Chang, Chi-Jen Lu
- **Comment**: accepted by ACCV 2020
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) have become a powerful approach for generative image modeling. However, GANs are notorious for their training instability, especially on large-scale, complex datasets. While the recent work of BigGAN has significantly improved the quality of image generation on ImageNet, it requires a huge model, making it hard to deploy on resource-constrained devices. To reduce the model size, we propose a black-box knowledge distillation framework for compressing GANs, which highlights a stable and efficient training process. Given BigGAN as the teacher network, we manage to train a much smaller student network to mimic its functionality, achieving competitive performance on Inception and FID scores with the generator having $16\times$ fewer parameters.



### SIR: Similar Image Retrieval for Product Search in E-Commerce
- **Arxiv ID**: http://arxiv.org/abs/2009.13836v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.IR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.13836v1)
- **Published**: 2020-09-29 07:53:03+00:00
- **Updated**: 2020-09-29 07:53:03+00:00
- **Authors**: Theban Stanley, Nihar Vanjara, Yanxin Pan, Ekaterina Pirogova, Swagata Chakraborty, Abon Chaudhuri
- **Comment**: Accepted in 13th International Conference on Similarity Search and
  Applications, SISAP 2020
- **Journal**: None
- **Summary**: We present a similar image retrieval (SIR) platform that is used to quickly discover visually similar products in a catalog of millions. Given the size, diversity, and dynamism of our catalog, product search poses many challenges. It can be addressed by building supervised models to tagging product images with labels representing themes and later retrieving them by labels. This approach suffices for common and perennial themes like "white shirt" or "lifestyle image of TV". It does not work for new themes such as "e-cigarettes", hard-to-define ones such as "image with a promotional badge", or the ones with short relevance span such as "Halloween costumes". SIR is ideal for such cases because it allows us to search by an example, not a pre-defined theme. We describe the steps - embedding computation, encoding, and indexing - that power the approximate nearest neighbor search back-end. We also highlight two applications of SIR. The first one is related to the detection of products with various types of potentially objectionable themes. This application is run with a sense of urgency, hence the typical time frame to train and bootstrap a model is not permitted. Also, these themes are often short-lived based on current trends, hence spending resources to build a lasting model is not justified. The second application is a variant item detection system where SIR helps discover visual variants that are hard to find through text search. We analyze the performance of SIR in the context of these applications.



### imdpGAN: Generating Private and Specific Data with Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2009.13839v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/2009.13839v1)
- **Published**: 2020-09-29 08:03:32+00:00
- **Updated**: 2020-09-29 08:03:32+00:00
- **Authors**: Saurabh Gupta, Arun Balaji Buduru, Ponnurangam Kumaraguru
- **Comment**: 9 pages, 7 figures, Accepted at IEEE TPS'2020
- **Journal**: None
- **Summary**: Generative Adversarial Network (GAN) and its variants have shown promising results in generating synthetic data. However, the issues with GANs are: (i) the learning happens around the training samples and the model often ends up remembering them, consequently, compromising the privacy of individual samples - this becomes a major concern when GANs are applied to training data including personally identifiable information, (ii) the randomness in generated data - there is no control over the specificity of generated samples. To address these issues, we propose imdpGAN - an information maximizing differentially private Generative Adversarial Network. It is an end-to-end framework that simultaneously achieves privacy protection and learns latent representations. With experiments on MNIST dataset, we show that imdpGAN preserves the privacy of the individual data point, and learns latent codes to control the specificity of the generated samples. We perform binary classification on digit pairs to show the utility versus privacy trade-off. The classification accuracy decreases as we increase privacy levels in the framework. We also experimentally show that the training process of imdpGAN is stable but experience a 10-fold time increase as compared with other GAN frameworks. Finally, we extend imdpGAN framework to CelebA dataset to show how the privacy and learned representations can be used to control the specificity of the output.



### Loop-box: Multi-Agent Direct SLAM Triggered by Single Loop Closure for Large-Scale Mapping
- **Arxiv ID**: http://arxiv.org/abs/2009.13851v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.13851v1)
- **Published**: 2020-09-29 08:26:43+00:00
- **Updated**: 2020-09-29 08:26:43+00:00
- **Authors**: M Usman Maqbool Bhutta, Manohar Kuse, Rui Fan, Yanan Liu, Ming Liu
- **Comment**: Material related to this work is available at
  https://usmanmaqbool.github.io/loop-box
- **Journal**: IEEE Transactions on Cybernetics, 2020
- **Summary**: In this paper, we present a multi-agent framework for real-time large-scale 3D reconstruction applications. In SLAM, researchers usually build and update a 3D map after applying non-linear pose graph optimization techniques. Moreover, many multi-agent systems are prevalently using odometry information from additional sensors. These methods generally involve intensive computer vision algorithms and are tightly coupled with various sensors. We develop a generic method for the keychallenging scenarios in multi-agent 3D mapping based on different camera systems. The proposed framework performs actively in terms of localizing each agent after the first loop closure between them. It is shown that the proposed system only uses monocular cameras to yield real-time multi-agent large-scale localization and 3D global mapping. Based on the initial matching, our system can calculate the optimal scale difference between multiple 3D maps and then estimate an accurate relative pose transformation for large-scale global mapping.



### Machine Learning for Semi-Automated Meteorite Recovery
- **Arxiv ID**: http://arxiv.org/abs/2009.13852v1
- **DOI**: 10.1111/maps.13593
- **Categories**: **astro-ph.EP**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.13852v1)
- **Published**: 2020-09-29 08:27:41+00:00
- **Updated**: 2020-09-29 08:27:41+00:00
- **Authors**: Seamus Anderson, Martin Towner, Phil Bland, Christopher Haikings, William Volante, Eleanor Sansom, Hadrien Devillepoix, Patrick Shober, Benjamin Hartig, Martin Cupak, Trent Jansen-Sturgeon, Robert Howie, Gretchen Benedix, Geoff Deacon
- **Comment**: 15 pages, 3 figures, 2 tables
- **Journal**: None
- **Summary**: We present a novel methodology for recovering meteorite falls observed and constrained by fireball networks, using drones and machine learning algorithms. This approach uses images of the local terrain for a given fall site to train an artificial neural network, designed to detect meteorite candidates. We have field tested our methodology to show a meteorite detection rate between 75-97%, while also providing an efficient mechanism to eliminate false-positives. Our tests at a number of locations within Western Australia also showcase the ability for this training scheme to generalize a model to learn localized terrain features. Our model-training approach was also able to correctly identify 3 meteorites in their native fall sites, that were found using traditional searching techniques. Our methodology will be used to recover meteorite falls in a wide range of locations within globe-spanning fireball networks.



### Neural Alignment for Face De-pixelization
- **Arxiv ID**: http://arxiv.org/abs/2009.13856v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR, cs.LG, I.3.3; I.3.8; I.4.3; I.4.7; I.4.5; I.4.9; I.4.10; I.2.6; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2009.13856v1)
- **Published**: 2020-09-29 08:29:15+00:00
- **Updated**: 2020-09-29 08:29:15+00:00
- **Authors**: Maayan Shuvi, Noa Fish, Kfir Aberman, Ariel Shamir, Daniel Cohen-Or
- **Comment**: None
- **Journal**: None
- **Summary**: We present a simple method to reconstruct a high-resolution video from a face-video, where the identity of a person is obscured by pixelization. This concealment method is popular because the viewer can still perceive a human face figure and the overall head motion. However, we show in our experiments that a fairly good approximation of the original video can be reconstructed in a way that compromises anonymity. Our system exploits the simultaneous similarity and small disparity between close-by video frames depicting a human face, and employs a spatial transformation component that learns the alignment between the pixelated frames. Each frame, supported by its aligned surrounding frames, is first encoded, then decoded to a higher resolution. Reconstruction and perceptual losses promote adherence to the ground-truth, and an adversarial loss assists in maintaining domain faithfulness. There is no need for explicit temporal coherency loss as it is maintained implicitly by the alignment of neighboring frames and reconstruction. Although simple, our framework synthesizes high-quality face reconstructions, demonstrating that given the statistical prior of a human face, multiple aligned pixelated frames contain sufficient information to reconstruct a high-quality approximation of the original signal.



### Where is the Model Looking At?--Concentrate and Explain the Network Attention
- **Arxiv ID**: http://arxiv.org/abs/2009.13862v1
- **DOI**: 10.1109/JSTSP.2020.2987729
- **Categories**: **cs.CV**, cs.AI, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2009.13862v1)
- **Published**: 2020-09-29 08:36:18+00:00
- **Updated**: 2020-09-29 08:36:18+00:00
- **Authors**: Wenjia Xu, Jiuniu Wang, Yang Wang, Guangluan Xu, Wei Dai, Yirong Wu
- **Comment**: None
- **Journal**: IEEE Journal of Selected Topics in Signal Processing, vol. 14, no.
  3, pp. 506-516, March 2020
- **Summary**: Image classification models have achieved satisfactory performance on many datasets, sometimes even better than human. However, The model attention is unclear since the lack of interpretability. This paper investigates the fidelity and interpretability of model attention. We propose an Explainable Attribute-based Multi-task (EAT) framework to concentrate the model attention on the discriminative image area and make the attention interpretable. We introduce attributes prediction to the multi-task learning network, helping the network to concentrate attention on the foreground objects. We generate attribute-based textual explanations for the network and ground the attributes on the image to show visual explanations. The multi-model explanation can not only improve user trust but also help to find the weakness of network and dataset. Our framework can be generalized to any basic model. We perform experiments on three datasets and five basic models. Results indicate that the EAT framework can give multi-modal explanations that interpret the network decision. The performance of several recognition approaches is improved by guiding network attention.



### TorchRadon: Fast Differentiable Routines for Computed Tomography
- **Arxiv ID**: http://arxiv.org/abs/2009.14788v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.14788v1)
- **Published**: 2020-09-29 09:20:22+00:00
- **Updated**: 2020-09-29 09:20:22+00:00
- **Authors**: Matteo Ronchetti
- **Comment**: None
- **Journal**: None
- **Summary**: This work presents TorchRadon -- an open source CUDA library which contains a set of differentiable routines for solving computed tomography (CT) reconstruction problems. The library is designed to help researchers working on CT problems to combine deep learning and model-based approaches. The package is developed as a PyTorch extension and can be seamlessly integrated into existing deep learning training code. Compared to the existing Astra Toolbox, TorchRadon is up to 125 faster. The operators implemented by TorchRadon allow the computation of gradients using PyTorch backward(), and can therefore be easily inserted inside existing neural networks architectures. Because of its speed and GPU support, TorchRadon can also be effectively used as a fast backend for the implementation of iterative algorithms. This paper presents the main functionalities of the library, compares results with existing libraries and provides examples of usage.



### A Multi-term and Multi-task Analyzing Framework for Affective Analysis in-the-wild
- **Arxiv ID**: http://arxiv.org/abs/2009.13885v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.13885v2)
- **Published**: 2020-09-29 09:24:29+00:00
- **Updated**: 2020-10-02 13:04:04+00:00
- **Authors**: Sachihiro Youoku, Yuushi Toyoda, Takahisa Yamamoto, Junya Saito, Ryosuke Kawamura, Xiaoyu Mi, Kentaro Murase
- **Comment**: 5 pages with 6 figures
- **Journal**: None
- **Summary**: Human affective recognition is an important factor in human-computer interaction. However, the method development with in-the-wild data is not yet accurate enough for practical usage. In this paper, we introduce the affective recognition method focusing on valence-arousal (VA) and expression (EXP) that was submitted to the Affective Behavior Analysis in-the-wild (ABAW) 2020 Contest. Since we considered that affective behaviors have many observable features that have their own time frames, we introduced multiple optimized time windows (short-term, middle-term, and long-term) into our analyzing framework for extracting feature parameters from video data. Moreover, multiple modality data are used, including action units, head poses, gaze, posture, and ResNet 50 or Efficient NET features, and are optimized during the extraction of these features. Then, we generated affective recognition models for each time window and ensembled these models together. Also, we fussed the valence, arousal, and expression models together to enable the multi-task learning, considering the fact that the basic psychological states behind facial expressions are closely related to each another. In the validation set, our model achieved a valence-arousal score of 0.498 and a facial expression score of 0.471. These verification results reveal that our proposed framework can improve estimation accuracy and robustness effectively.



### Weakly-supervised Salient Instance Detection
- **Arxiv ID**: http://arxiv.org/abs/2009.13898v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.13898v1)
- **Published**: 2020-09-29 09:47:23+00:00
- **Updated**: 2020-09-29 09:47:23+00:00
- **Authors**: Xin Tian, Ke Xu, Xin Yang, Baocai Yin, Rynson W. H. Lau
- **Comment**: BMVC 2020, best student paper runner-up
- **Journal**: None
- **Summary**: Existing salient instance detection (SID) methods typically learn from pixel-level annotated datasets. In this paper, we present the first weakly-supervised approach to the SID problem. Although weak supervision has been considered in general saliency detection, it is mainly based on using class labels for object localization. However, it is non-trivial to use only class labels to learn instance-aware saliency information, as salient instances with high semantic affinities may not be easily separated by the labels. We note that subitizing information provides an instant judgement on the number of salient items, which naturally relates to detecting salient instances and may help separate instances of the same class while grouping different parts of the same instance. Inspired by this insight, we propose to use class and subitizing labels as weak supervision for the SID problem. We propose a novel weakly-supervised network with three branches: a Saliency Detection Branch leveraging class consistency information to locate candidate objects; a Boundary Detection Branch exploiting class discrepancy information to delineate object boundaries; and a Centroid Detection Branch using subitizing information to detect salient instance centroids. This complementary information is further fused to produce salient instance maps. We conduct extensive experiments to demonstrate that the proposed method plays favorably against carefully designed baseline methods adapted from related tasks.



### A Comparative Study of Deep Learning Loss Functions for Multi-Label Remote Sensing Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2009.13935v1
- **DOI**: 10.1109/IGARSS39084.2020.9323583
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.13935v1)
- **Published**: 2020-09-29 11:35:26+00:00
- **Updated**: 2020-09-29 11:35:26+00:00
- **Authors**: Hichame Yessou, Gencer Sumbul, Begüm Demir
- **Comment**: Accepted at IEEE International Geoscience and Remote Sensing
  Symposium (IGARSS) 2020. For code visit:
  https://gitlab.tubit.tu-berlin.de/rsim/RS-MLC-Losses
- **Journal**: None
- **Summary**: This paper analyzes and compares different deep learning loss functions in the framework of multi-label remote sensing (RS) image scene classification problems. We consider seven loss functions: 1) cross-entropy loss; 2) focal loss; 3) weighted cross-entropy loss; 4) Hamming loss; 5) Huber loss; 6) ranking loss; and 7) sparseMax loss. All the considered loss functions are analyzed for the first time in RS. After a theoretical analysis, an experimental analysis is carried out to compare the considered loss functions in terms of their: 1) overall accuracy; 2) class imbalance awareness (for which the number of samples associated to each class significantly varies); 3) convexibility and differentiability; and 4) learning efficiency (i.e., convergence speed). On the basis of our analysis, some guidelines are derived for a proper selection of a loss function in multi-label RS scene classification problems.



### Tackling unsupervised multi-source domain adaptation with optimism and consistency
- **Arxiv ID**: http://arxiv.org/abs/2009.13939v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2009.13939v1)
- **Published**: 2020-09-29 11:55:14+00:00
- **Updated**: 2020-09-29 11:55:14+00:00
- **Authors**: Diogo Pernes, Jaime S. Cardoso
- **Comment**: None
- **Journal**: None
- **Summary**: It has been known for a while that the problem of multi-source domain adaptation can be regarded as a single source domain adaptation task where the source domain corresponds to a mixture of the original source domains. Nonetheless, how to adjust the mixture distribution weights remains an open question. Moreover, most existing work on this topic focuses only on minimizing the error on the source domains and achieving domain-invariant representations, which is insufficient to ensure low error on the target domain. In this work, we present a novel framework that addresses both problems and beats the current state of the art by using a mildly optimistic objective function and consistency regularization on the target samples.



### MS-RANAS: Multi-Scale Resource-Aware Neural Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/2009.13940v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, I.4.9; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/2009.13940v1)
- **Published**: 2020-09-29 11:56:01+00:00
- **Updated**: 2020-09-29 11:56:01+00:00
- **Authors**: Cristian Cioflan, Radu Timofte
- **Comment**: None
- **Journal**: None
- **Summary**: Neural Architecture Search (NAS) has proved effective in offering outperforming alternatives to handcrafted neural networks. In this paper we analyse the benefits of NAS for image classification tasks under strict computational constraints. Our aim is to automate the design of highly efficient deep neural networks, capable of offering fast and accurate predictions and that could be deployed on a low-memory, low-power system-on-chip. The task thus becomes a three-party trade-off between accuracy, computational complexity, and memory requirements. To address this concern, we propose Multi-Scale Resource-Aware Neural Architecture Search (MS-RANAS). We employ a one-shot architecture search approach in order to obtain a reduced search cost and we focus on an anytime prediction setting. Through the usage of multiple-scaled features and early classifiers, we achieved state-of-the-art results in terms of accuracy-speed trade-off.



### One-Shot learning based classification for segregation of plastic waste
- **Arxiv ID**: http://arxiv.org/abs/2009.13953v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2009.13953v1)
- **Published**: 2020-09-29 12:16:50+00:00
- **Updated**: 2020-09-29 12:16:50+00:00
- **Authors**: Shivaank Agarwal, Ravindra Gudi, Paresh Saxena
- **Comment**: Accepted in The International Conference on Digital Image Computing:
  Techniques and Applications, 2020
- **Journal**: None
- **Summary**: The problem of segregating recyclable waste is fairly daunting for many countries. This article presents an approach for image based classification of plastic waste using one-shot learning techniques. The proposed approach exploits discriminative features generated via the siamese and triplet loss convolutional neural networks to help differentiate between 5 types of plastic waste based on their resin codes. The approach achieves an accuracy of 99.74% on the WaDaBa Database



### A Prototype-Based Generalized Zero-Shot Learning Framework for Hand Gesture Recognition
- **Arxiv ID**: http://arxiv.org/abs/2009.13957v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.13957v1)
- **Published**: 2020-09-29 12:18:35+00:00
- **Updated**: 2020-09-29 12:18:35+00:00
- **Authors**: Jinting Wu, Yujia Zhang, Xiaoguang Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: Hand gesture recognition plays a significant role in human-computer interaction for understanding various human gestures and their intent. However, most prior works can only recognize gestures of limited labeled classes and fail to adapt to new categories. The task of Generalized Zero-Shot Learning (GZSL) for hand gesture recognition aims to address the above issue by leveraging semantic representations and detecting both seen and unseen class samples. In this paper, we propose an end-to-end prototype-based GZSL framework for hand gesture recognition which consists of two branches. The first branch is a prototype-based detector that learns gesture representations and determines whether an input sample belongs to a seen or unseen category. The second branch is a zero-shot label predictor which takes the features of unseen classes as input and outputs predictions through a learned mapping mechanism between the feature and the semantic space. We further establish a hand gesture dataset that specifically targets this GZSL task, and comprehensive experiments on this dataset demonstrate the effectiveness of our proposed approach on recognizing both seen and unseen gestures.



### Affect Expression Behaviour Analysis in the Wild using Spatio-Channel Attention and Complementary Context Information
- **Arxiv ID**: http://arxiv.org/abs/2009.14440v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2009.14440v2)
- **Published**: 2020-09-29 12:26:15+00:00
- **Updated**: 2020-10-10 06:24:19+00:00
- **Authors**: Darshan Gera, S Balasubramanian
- **Comment**: arXiv admin note: text overlap with arXiv:2007.10298 (ABAW2020
  challenge test set results added)
- **Journal**: None
- **Summary**: Facial expression recognition(FER) in the wild is crucial for building reliable human-computer interactive systems. However, current FER systems fail to perform well under various natural and un-controlled conditions. This report presents attention based framework used in our submission to expression recognition track of the Affective Behaviour Analysis in-the-wild (ABAW) 2020 competition. Spatial-channel attention net(SCAN) is used to extract local and global attentive features without seeking any information from landmark detectors. SCAN is complemented by a complementary context information(CCI) branch which uses efficient channel attention(ECA) to enhance the relevance of features. The performance of the model is validated on challenging Aff-Wild2 dataset for categorical expression classification.



### Deep Image Reconstruction using Unregistered Measurements without Groundtruth
- **Arxiv ID**: http://arxiv.org/abs/2009.13986v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.13986v1)
- **Published**: 2020-09-29 13:15:45+00:00
- **Updated**: 2020-09-29 13:15:45+00:00
- **Authors**: Weijie Gan, Yu Sun, Cihat Eldeniz, Jiaming Liu, Hongyu An, Ulugbek S. Kamilov
- **Comment**: None
- **Journal**: None
- **Summary**: One of the key limitations in conventional deep learning based image reconstruction is the need for registered pairs of training images containing a set of high-quality groundtruth images. This paper addresses this limitation by proposing a novel unsupervised deep registration-augmented reconstruction method (U-Dream) for training deep neural nets to reconstruct high-quality images by directly mapping pairs of unregistered and artifact-corrupted images. The ability of U-Dream to circumvent the need for accurately registered data makes it widely applicable to many biomedical image reconstruction tasks. We validate it in accelerated magnetic resonance imaging (MRI) by training an image reconstruction model directly on pairs of undersampled measurements from images that have undergone nonrigid deformations.



### MCW-Net: Single Image Deraining with Multi-level Connections and Wide Regional Non-local Blocks
- **Arxiv ID**: http://arxiv.org/abs/2009.13990v4
- **DOI**: 10.1016/j.image.2022.116701
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.13990v4)
- **Published**: 2020-09-29 13:21:31+00:00
- **Updated**: 2022-04-03 10:40:38+00:00
- **Authors**: Yeachan Park, Myeongho Jeon, Junho Lee, Myungjoo Kang
- **Comment**: None
- **Journal**: Signal Processing: Image Communication, Volume 105, 2022, 116701,
- **Summary**: A recent line of convolutional neural network-based works has succeeded in capturing rain streaks. However, difficulties in detailed recovery still remain. In this paper, we present a multi-level connection and wide regional non-local block network (MCW-Net) to properly restore the original background textures in rainy images. Unlike existing encoder-decoder-based image deraining models that improve performance with additional branches, MCW-Net improves performance by maximizing information utilization without additional branches through the following two proposed methods. The first method is a multi-level connection that repeatedly connects multi-level features of the encoder network to the decoder network. Multi-level connection encourages the decoding process to use the feature information of all levels. In multi-level connection, channel-wise attention is considered to learn which level of features is important in the decoding process of the current level. The second method is a wide regional non-local block. As rain streaks primarily exhibit a vertical distribution, we divide the grid of the image into horizontally-wide patches and apply a non-local operation to each region to explore the rich rain-free background information. Experimental results on both synthetic and real-world rainy datasets demonstrate that the proposed model significantly outperforms existing state-of-the-art models. Furthermore, the results of the joint deraining and segmentation experiment prove that our model contributes effectively to other vision tasks.



### Improving Interpretability for Computer-aided Diagnosis tools on Whole Slide Imaging with Multiple Instance Learning and Gradient-based Explanations
- **Arxiv ID**: http://arxiv.org/abs/2009.14001v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.14001v1)
- **Published**: 2020-09-29 13:39:27+00:00
- **Updated**: 2020-09-29 13:39:27+00:00
- **Authors**: Antoine Pirovano, Hippolyte Heuberger, Sylvain Berlemont, Saïd Ladjal, Isabelle Bloch
- **Comment**: 8 pages (references excluded), 3 figures, presented in iMIMIC
  Workshop at MICCAI 2020
- **Journal**: None
- **Summary**: Deep learning methods are widely used for medical applications to assist medical doctors in their daily routines. While performances reach expert's level, interpretability (highlight how and what a trained model learned and why it makes a specific decision) is the next important challenge that deep learning methods need to answer to be fully integrated in the medical field. In this paper, we address the question of interpretability in the context of whole slide images (WSI) classification. We formalize the design of WSI classification architectures and propose a piece-wise interpretability approach, relying on gradient-based methods, feature visualization and multiple instance learning context. We aim at explaining how the decision is made based on tile level scoring, how these tile scores are decided and which features are used and relevant for the task. After training two WSI classification architectures on Camelyon-16 WSI dataset, highlighting discriminative features learned, and validating our approach with pathologists, we propose a novel manner of computing interpretability slide-level heat-maps, based on the extracted features, that improves tile-level classification performances by more than 29% for AUC.



### Attentional Feature Fusion
- **Arxiv ID**: http://arxiv.org/abs/2009.14082v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.14082v2)
- **Published**: 2020-09-29 15:10:18+00:00
- **Updated**: 2020-11-09 17:41:20+00:00
- **Authors**: Yimian Dai, Fabian Gieseke, Stefan Oehmcke, Yiquan Wu, Kobus Barnard
- **Comment**: Accepted by WACV 2021
- **Journal**: None
- **Summary**: Feature fusion, the combination of features from different layers or branches, is an omnipresent part of modern network architectures. It is often implemented via simple operations, such as summation or concatenation, but this might not be the best choice. In this work, we propose a uniform and general scheme, namely attentional feature fusion, which is applicable for most common scenarios, including feature fusion induced by short and long skip connections as well as within Inception layers. To better fuse features of inconsistent semantics and scales, we propose a multi-scale channel attention module, which addresses issues that arise when fusing features given at different scales. We also demonstrate that the initial integration of feature maps can become a bottleneck and that this issue can be alleviated by adding another level of attention, which we refer to as iterative attentional feature fusion. With fewer layers or parameters, our models outperform state-of-the-art networks on both CIFAR-100 and ImageNet datasets, which suggests that more sophisticated attention mechanisms for feature fusion hold great potential to consistently yield better results compared to their direct counterparts. Our codes and trained models are available online.



### Localize to Classify and Classify to Localize: Mutual Guidance in Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2009.14085v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.14085v1)
- **Published**: 2020-09-29 15:15:26+00:00
- **Updated**: 2020-09-29 15:15:26+00:00
- **Authors**: Heng Zhang, Elisa Fromont, Sébastien Lefevre, Bruno Avignon
- **Comment**: Accepted by ACCV 2020
- **Journal**: None
- **Summary**: Most deep learning object detectors are based on the anchor mechanism and resort to the Intersection over Union (IoU) between predefined anchor boxes and ground truth boxes to evaluate the matching quality between anchors and objects. In this paper, we question this use of IoU and propose a new anchor matching criterion guided, during the training phase, by the optimization of both the localization and the classification tasks: the predictions related to one task are used to dynamically assign sample anchors and improve the model on the other task, and vice versa. Despite the simplicity of the proposed method, our experiments with different state-of-the-art deep learning architectures on PASCAL VOC and MS COCO datasets demonstrate the effectiveness and generality of our Mutual Guidance strategy.



### Learning to Compress Videos without Computing Motion
- **Arxiv ID**: http://arxiv.org/abs/2009.14110v3
- **DOI**: 10.1016/j.image.2022.116633
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.14110v3)
- **Published**: 2020-09-29 15:49:25+00:00
- **Updated**: 2022-03-27 03:52:39+00:00
- **Authors**: Meixu Chen, Todd Goodall, Anjul Patney, Alan C. Bovik
- **Comment**: None
- **Journal**: None
- **Summary**: With the development of higher resolution contents and displays, its significant volume poses significant challenges to the goals of acquiring, transmitting, compressing, and displaying high-quality video content. In this paper, we propose a new deep learning video compression architecture that does not require motion estimation, which is the most expensive element of modern hybrid video compression codecs like H.264 and HEVC. Our framework exploits the regularities inherent to video motion, which we capture by using displaced frame differences as video representations to train the neural network. In addition, we propose a new space-time reconstruction network based on both an LSTM model and a UNet model, which we call LSTM-UNet. The new video compression framework has three components: a Displacement Calculation Unit (DCU), a Displacement Compression Network (DCN), and a Frame Reconstruction Network (FRN). The DCU removes the need for motion estimation found in hybrid codecs and is less expensive. In the DCN, an RNN-based network is utilized to compress displaced frame differences as well as retain temporal information between frames. The LSTM-UNet is used in the FRN to learn space-time differential representations of videos. Our experimental results show that our compression model, which we call the MOtionless VIdeo Codec (MOVI-Codec), learns how to efficiently compress videos without computing motion. Our experiments show that MOVI-Codec outperforms the Low-Delay P veryfast setting of the video coding standard H.264 and exceeds the performance of the modern global standard HEVC codec, using the same setting, as measured by MS-SSIM, especially on higher resolution videos. In addition, our network outperforms the latest H.266 (VVC) codec at higher bitrates, when assessed using MS-SSIM, on high-resolution videos.



### CoKe: Localized Contrastive Learning for Robust Keypoint Detection
- **Arxiv ID**: http://arxiv.org/abs/2009.14115v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.14115v4)
- **Published**: 2020-09-29 16:00:43+00:00
- **Updated**: 2022-12-05 08:56:16+00:00
- **Authors**: Yutong Bai, Angtian Wang, Adam Kortylewski, Alan Yuille
- **Comment**: Accepted to WACV 2023
- **Journal**: None
- **Summary**: In this paper, we introduce a contrastive learning framework for keypoint detection (CoKe). Keypoint detection differs from other visual tasks where contrastive learning has been applied because the input is a set of images in which multiple keypoints are annotated. This requires the contrastive learning to be extended such that the keypoints are represented and detected independently, which enables the contrastive loss to make the keypoint features different from each other and from the background. Our approach has two benefits: It enables us to exploit contrastive learning for keypoint detection, and by detecting each keypoint independently the detection becomes more robust to occlusion compared to holistic methods, such as stacked hourglass networks, which attempt to detect all keypoints jointly. Our CoKe framework introduces several technical innovations. In particular, we introduce: (i) A clutter bank to represent non-keypoint features; (ii) a keypoint bank that stores prototypical representations of keypoints to approximate the contrastive loss between keypoints; and (iii) a cumulative moving average update to learn the keypoint prototypes while training the feature extractor. Our experiments on a range of diverse datasets (PASCAL3D+, MPII, ObjectNet3D) show that our approach works as well, or better than, alternative methods for keypoint detection, even for human keypoints, for which the literature is vast. Moreover, we observe that CoKe is exceptionally robust to partial occlusion and previously unseen object poses.



### Spatial Attention as an Interface for Image Captioning Models
- **Arxiv ID**: http://arxiv.org/abs/2010.11701v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2010.11701v1)
- **Published**: 2020-09-29 16:04:08+00:00
- **Updated**: 2020-09-29 16:04:08+00:00
- **Authors**: Philipp Sadler
- **Comment**: A thesis submitted in fulfillment of the requirements for the degree
  Master of Science in Cognitive Systems
- **Journal**: None
- **Summary**: The internal workings of modern deep learning models stay often unclear to an external observer, although spatial attention mechanisms are involved. The idea of this work is to translate these spatial attentions into natural language to provide a simpler access to the model's function. Thus, I took a neural image captioning model and measured the reactions to external modification in its spatial attention for three different interface methods: a fixation over the whole generation process, a fixation for the first time-steps and an addition to the generator's attention. The experimental results for bounding box based spatial attention vectors have shown that the captioning model reacts to method dependent changes in up to 52.65% and includes in 9.00% of the cases object categories, which were otherwise unmentioned. Afterwards, I established such a link to a hierarchical co-attention network for visual question answering by extraction of its word, phrase and question level spatial attentions. Here, generated captions for the word level included details of the question-answer pairs in up to 55.20% of the cases. This work indicates that spatial attention seen as an external interface for image caption generators is an useful method to access visual functions in natural language.



### Asymmetric Loss For Multi-Label Classification
- **Arxiv ID**: http://arxiv.org/abs/2009.14119v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, I.2.6; I.2.10; I.0; I.4.0
- **Links**: [PDF](http://arxiv.org/pdf/2009.14119v4)
- **Published**: 2020-09-29 16:08:19+00:00
- **Updated**: 2021-07-29 15:02:43+00:00
- **Authors**: Emanuel Ben-Baruch, Tal Ridnik, Nadav Zamir, Asaf Noy, Itamar Friedman, Matan Protter, Lihi Zelnik-Manor
- **Comment**: Accepted to ICCV 2021
- **Journal**: None
- **Summary**: In a typical multi-label setting, a picture contains on average few positive labels, and many negative ones. This positive-negative imbalance dominates the optimization process, and can lead to under-emphasizing gradients from positive labels during training, resulting in poor accuracy. In this paper, we introduce a novel asymmetric loss ("ASL"), which operates differently on positive and negative samples. The loss enables to dynamically down-weights and hard-thresholds easy negative samples, while also discarding possibly mislabeled samples. We demonstrate how ASL can balance the probabilities of different samples, and how this balancing is translated to better mAP scores. With ASL, we reach state-of-the-art results on multiple popular multi-label datasets: MS-COCO, Pascal-VOC, NUS-WIDE and Open Images. We also demonstrate ASL applicability for other tasks, such as single-label classification and object detection. ASL is effective, easy to implement, and does not increase the training time or complexity.   Implementation is available at: https://github.com/Alibaba-MIIL/ASL.



### Score-level Multi Cue Fusion for Sign Language Recognition
- **Arxiv ID**: http://arxiv.org/abs/2009.14139v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.14139v1)
- **Published**: 2020-09-29 16:32:51+00:00
- **Updated**: 2020-09-29 16:32:51+00:00
- **Authors**: Çağrı Gökçe, Oğulcan Özdemir, Ahmet Alp Kındıroğlu, Lale Akarun
- **Comment**: None
- **Journal**: None
- **Summary**: Sign Languages are expressed through hand and upper body gestures as well as facial expressions. Therefore, Sign Language Recognition (SLR) needs to focus on all such cues. Previous work uses hand-crafted mechanisms or network aggregation to extract the different cue features, to increase SLR performance. This is slow and involves complicated architectures. We propose a more straightforward approach that focuses on training separate cue models specializing on the dominant hand, hands, face, and upper body regions. We compare the performance of 3D Convolutional Neural Network (CNN) models specializing in these regions, combine them through score-level fusion, and use the weighted alternative. Our experimental results have shown the effectiveness of mixed convolutional models. Their fusion yields up to 19% accuracy improvement over the baseline using the full upper body. Furthermore, we include a discussion for fusion settings, which can help future work on Sign Language Translation (SLT).



### A Survey on Deep Learning Techniques for Video Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/2009.14146v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2009.14146v1)
- **Published**: 2020-09-29 16:40:46+00:00
- **Updated**: 2020-09-29 16:40:46+00:00
- **Authors**: Jessie James P. Suarez, Prospero C. Naval Jr
- **Comment**: None
- **Journal**: None
- **Summary**: Anomaly detection in videos is a problem that has been studied for more than a decade. This area has piqued the interest of researchers due to its wide applicability. Because of this, there has been a wide array of approaches that have been proposed throughout the years and these approaches range from statistical-based approaches to machine learning-based approaches. Numerous surveys have already been conducted on this area but this paper focuses on providing an overview on the recent advances in the field of anomaly detection using Deep Learning. Deep Learning has been applied successfully in many fields of artificial intelligence such as computer vision, natural language processing and more. This survey, however, focuses on how Deep Learning has improved and provided more insights to the area of video anomaly detection. This paper provides a categorization of the different Deep Learning approaches with respect to their objectives. Additionally, it also discusses the commonly used datasets along with the common evaluation metrics. Afterwards, a discussion synthesizing all of the recent approaches is made to provide direction and possible areas for future research.



### Multi-View Consistency Loss for Improved Single-Image 3D Reconstruction of Clothed People
- **Arxiv ID**: http://arxiv.org/abs/2009.14162v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.14162v1)
- **Published**: 2020-09-29 17:18:00+00:00
- **Updated**: 2020-09-29 17:18:00+00:00
- **Authors**: Akin Caliskan, Armin Mustafa, Evren Imre, Adrian Hilton
- **Comment**: Accepted to Asian Conference on Computer Vision 2020 (ACCV)
- **Journal**: None
- **Summary**: We present a novel method to improve the accuracy of the 3D reconstruction of clothed human shape from a single image. Recent work has introduced volumetric, implicit and model-based shape learning frameworks for reconstruction of objects and people from one or more images. However, the accuracy and completeness for reconstruction of clothed people is limited due to the large variation in shape resulting from clothing, hair, body size, pose and camera viewpoint. This paper introduces two advances to overcome this limitation: firstly a new synthetic dataset of realistic clothed people, 3DVH; and secondly, a novel multiple-view loss function for training of monocular volumetric shape estimation, which is demonstrated to significantly improve generalisation and reconstruction accuracy. The 3DVH dataset of realistic clothed 3D human models rendered with diverse natural backgrounds is demonstrated to allows transfer to reconstruction from real images of people. Comprehensive comparative performance evaluation on both synthetic and real images of people demonstrates that the proposed method significantly outperforms the previous state-of-the-art learning-based single image 3D human shape estimation approaches achieving significant improvement of reconstruction accuracy, completeness, and quality. An ablation study shows that this is due to both the proposed multiple-view training and the new 3DVH dataset. The code and the dataset can be found at the project website: https://akincaliskan3d.github.io/MV3DH/.



### Robust Detection of Objects under Periodic Motion with Gaussian Process Filtering
- **Arxiv ID**: http://arxiv.org/abs/2009.14178v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.14178v1)
- **Published**: 2020-09-29 17:45:21+00:00
- **Updated**: 2020-09-29 17:45:21+00:00
- **Authors**: Joris Guerin, Anne Magaly de Paula Canuto, Luiz Marcos Garcia Goncalves
- **Comment**: 8 pages, 11 figures, 1 table Accepted as a full paper at ICMLA 2020
  (19th IEEE International Conference On Machine Learning And Applications)
- **Journal**: None
- **Summary**: Object Detection (OD) is an important task in Computer Vision with many practical applications. For some use cases, OD must be done on videos, where the object of interest has a periodic motion. In this paper, we formalize the problem of periodic OD, which consists in improving the performance of an OD model in the specific case where the object of interest is repeating similar spatio-temporal trajectories with respect to the video frames. The proposed approach is based on training a Gaussian Process to model the periodic motion, and use it to filter out the erroneous predictions of the OD model. By simulating various OD models and periodic trajectories, we demonstrate that this filtering approach, which is entirely data-driven, improves the detection performance by a large margin.



### Uncertainty Sets for Image Classifiers using Conformal Prediction
- **Arxiv ID**: http://arxiv.org/abs/2009.14193v5
- **DOI**: None
- **Categories**: **cs.CV**, math.ST, stat.ML, stat.TH
- **Links**: [PDF](http://arxiv.org/pdf/2009.14193v5)
- **Published**: 2020-09-29 17:58:04+00:00
- **Updated**: 2022-09-03 05:45:19+00:00
- **Authors**: Anastasios Angelopoulos, Stephen Bates, Jitendra Malik, Michael I. Jordan
- **Comment**: ICLR 2021 Spotlight, https://openreview.net/forum?id=eNdiU_DbM9 .
  Project website at
  https://people.eecs.berkeley.edu/~angelopoulos/blog/posts/conformal-classification/
  . Codebase at https://github.com/aangelopoulos/conformal_classification
- **Journal**: None
- **Summary**: Convolutional image classifiers can achieve high predictive accuracy, but quantifying their uncertainty remains an unresolved challenge, hindering their deployment in consequential settings. Existing uncertainty quantification techniques, such as Platt scaling, attempt to calibrate the network's probability estimates, but they do not have formal guarantees. We present an algorithm that modifies any classifier to output a predictive set containing the true label with a user-specified probability, such as 90%. The algorithm is simple and fast like Platt scaling, but provides a formal finite-sample coverage guarantee for every model and dataset. Our method modifies an existing conformal prediction algorithm to give more stable predictive sets by regularizing the small scores of unlikely classes after Platt scaling. In experiments on both Imagenet and Imagenet-V2 with ResNet-152 and other classifiers, our scheme outperforms existing approaches, achieving coverage with sets that are often factors of 5 to 10 smaller than a stand-alone Platt scaling baseline.



### Deep Evolution for Facial Emotion Recognition
- **Arxiv ID**: http://arxiv.org/abs/2009.14194v2
- **DOI**: 10.1145/3410886.3410892
- **Categories**: **cs.NE**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.14194v2)
- **Published**: 2020-09-29 17:58:09+00:00
- **Updated**: 2020-10-13 13:21:18+00:00
- **Authors**: Emmanuel Dufourq, Bruce A. Bassett
- **Comment**: Conference of the South African Institute of Computer Scientists and
  Information Technologists 2020
- **Journal**: None
- **Summary**: Deep facial expression recognition faces two challenges that both stem from the large number of trainable parameters: long training times and a lack of interpretability. We propose a novel method based on evolutionary algorithms, that deals with both challenges by massively reducing the number of trainable parameters, whilst simultaneously retaining classification performance, and in some cases achieving superior performance. We are robustly able to reduce the number of parameters on average by 95% (e.g. from 2M to 100k parameters) with no loss in classification accuracy. The algorithm learns to choose small patches from the image, relative to the nose, which carry the most important information about emotion, and which coincide with typical human choices of important features. Our work implements a novel form attention and shows that evolutionary algorithms are a valuable addition to machine learning in the deep learning era, both for reducing the number of parameters for facial expression recognition and for providing interpretable features that can help reduce bias.



### Lip-reading with Densely Connected Temporal Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/2009.14233v3
- **DOI**: 10.1109/WACV48630.2021.00290
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.14233v3)
- **Published**: 2020-09-29 18:08:15+00:00
- **Updated**: 2022-09-29 14:50:00+00:00
- **Authors**: Pingchuan Ma, Yujiang Wang, Jie Shen, Stavros Petridis, Maja Pantic
- **Comment**: WACV 2021. An improved code implementation is available at:
  https://github.com/mpc001/Lipreading_using_Temporal_Convolutional_Networks
- **Journal**: 2021 IEEE Winter Conference on Applications of Computer Vision
  (WACV), pp. 2856-2865, 2021
- **Summary**: In this work, we present the Densely Connected Temporal Convolutional Network (DC-TCN) for lip-reading of isolated words. Although Temporal Convolutional Networks (TCN) have recently demonstrated great potential in many vision tasks, its receptive fields are not dense enough to model the complex temporal dynamics in lip-reading scenarios. To address this problem, we introduce dense connections into the network to capture more robust temporal features. Moreover, our approach utilises the Squeeze-and-Excitation block, a light-weight attention mechanism, to further enhance the model's classification power. Without bells and whistles, our DC-TCN method has achieved 88.36% accuracy on the Lip Reading in the Wild (LRW) dataset and 43.65% on the LRW-1000 dataset, which has surpassed all the baseline methods and is the new state-of-the-art on both datasets.



### Acceleration of Large Margin Metric Learning for Nearest Neighbor Classification Using Triplet Mining and Stratified Sampling
- **Arxiv ID**: http://arxiv.org/abs/2009.14244v1
- **DOI**: 10.15353/jcvis.v6i1.3534
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2009.14244v1)
- **Published**: 2020-09-29 18:24:34+00:00
- **Updated**: 2020-09-29 18:24:34+00:00
- **Authors**: Parisa Abdolrahim Poorheravi, Benyamin Ghojogh, Vincent Gaudet, Fakhri Karray, Mark Crowley
- **Comment**: The first two authors contributed equally to this work
- **Journal**: Journal of Computational Vision and Imaging Systems, Vol. 6, No.
  1, 2020, Special Issue: Proceedings of Conference on Vision and Intelligent
  Systems (CVIS) 2020
- **Summary**: Metric learning is one of the techniques in manifold learning with the goal of finding a projection subspace for increasing and decreasing the inter- and intra-class variances, respectively. Some of the metric learning methods are based on triplet learning with anchor-positive-negative triplets. Large margin metric learning for nearest neighbor classification is one of the fundamental methods to do this. Recently, Siamese networks have been introduced with the triplet loss. Many triplet mining methods have been developed for Siamese networks; however, these techniques have not been applied on the triplets of large margin metric learning for nearest neighbor classification. In this work, inspired by the mining methods for Siamese networks, we propose several triplet mining techniques for large margin metric learning. Moreover, a hierarchical approach is proposed, for acceleration and scalability of optimization, where triplets are selected by stratified sampling in hierarchical hyper-spheres. We analyze the proposed methods on three publicly available datasets, i.e., Fisher Iris, ORL faces, and MNIST datasets.



### Visually-Grounded Planning without Vision: Language Models Infer Detailed Plans from High-level Instructions
- **Arxiv ID**: http://arxiv.org/abs/2009.14259v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.14259v2)
- **Published**: 2020-09-29 18:52:39+00:00
- **Updated**: 2020-10-26 19:16:00+00:00
- **Authors**: Peter A. Jansen
- **Comment**: Accepted to Findings of EMNLP. V2: corrected typo Table 1; margins
  Table 3
- **Journal**: None
- **Summary**: The recently proposed ALFRED challenge task aims for a virtual robotic agent to complete complex multi-step everyday tasks in a virtual home environment from high-level natural language directives, such as "put a hot piece of bread on a plate". Currently, the best-performing models are able to complete less than 5% of these tasks successfully. In this work we focus on modeling the translation problem of converting natural language directives into detailed multi-step sequences of actions that accomplish those goals in the virtual environment. We empirically demonstrate that it is possible to generate gold multi-step plans from language directives alone without any visual input in 26% of unseen cases. When a small amount of visual information is incorporated, namely the starting location in the virtual environment, our best-performing GPT-2 model successfully generates gold command sequences in 58% of cases. Our results suggest that contextualized language models may provide strong visual semantic planning modules for grounded virtual agents.



### Trustworthy Convolutional Neural Networks: A Gradient Penalized-based Approach
- **Arxiv ID**: http://arxiv.org/abs/2009.14260v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2009.14260v1)
- **Published**: 2020-09-29 18:56:40+00:00
- **Updated**: 2020-09-29 18:56:40+00:00
- **Authors**: Nicholas Halliwell, Freddy Lecue
- **Comment**: 13pages
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) are commonly used for image classification. Saliency methods are examples of approaches that can be used to interpret CNNs post hoc, identifying the most relevant pixels for a prediction following the gradients flow. Even though CNNs can correctly classify images, the underlying saliency maps could be erroneous in many cases. This can result in skepticism as to the validity of the model or its interpretation. We propose a novel approach for training trustworthy CNNs by penalizing parameter choices that result in inaccurate saliency maps generated during training. We add a penalty term for inaccurate saliency maps produced when the predicted label is correct, a penalty term for accurate saliency maps produced when the predicted label is incorrect, and a regularization term penalizing overly confident saliency maps. Experiments show increased classification performance, user engagement, and trust.



### CrowdMOT: Crowdsourcing Strategies for Tracking Multiple Objects in Videos
- **Arxiv ID**: http://arxiv.org/abs/2009.14265v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2009.14265v1)
- **Published**: 2020-09-29 19:12:21+00:00
- **Updated**: 2020-09-29 19:12:21+00:00
- **Authors**: Samreen Anjum, Chi Lin, Danna Gurari
- **Comment**: CSCW 2020
- **Journal**: None
- **Summary**: Crowdsourcing is a valuable approach for tracking objects in videos in a more scalable manner than possible with domain experts. However, existing frameworks do not produce high quality results with non-expert crowdworkers, especially for scenarios where objects split. To address this shortcoming, we introduce a crowdsourcing platform called CrowdMOT, and investigate two micro-task design decisions: (1) whether to decompose the task so that each worker is in charge of annotating all objects in a sub-segment of the video versus annotating a single object across the entire video, and (2) whether to show annotations from previous workers to the next individuals working on the task. We conduct experiments on a diversity of videos which show both familiar objects (aka - people) and unfamiliar objects (aka - cells). Our results highlight strategies for efficiently collecting higher quality annotations than observed when using strategies employed by today's state-of-art crowdsourcing system.



### Learning an optimal PSF-pair for ultra-dense 3D localization microscopy
- **Arxiv ID**: http://arxiv.org/abs/2009.14303v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.bio-ph, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/2009.14303v1)
- **Published**: 2020-09-29 20:54:52+00:00
- **Updated**: 2020-09-29 20:54:52+00:00
- **Authors**: Elias Nehme, Boris Ferdman, Lucien E. Weiss, Tal Naor, Daniel Freedman, Tomer Michaeli, Yoav Shechtman
- **Comment**: 20 pages, 20 figures
- **Journal**: None
- **Summary**: A long-standing challenge in multiple-particle-tracking is the accurate and precise 3D localization of individual particles at close proximity. One established approach for snapshot 3D imaging is point-spread-function (PSF) engineering, in which the PSF is modified to encode the axial information. However, engineered PSFs are challenging to localize at high densities due to lateral PSF overlaps. Here we suggest using multiple PSFs simultaneously to help overcome this challenge, and investigate the problem of engineering multiple PSFs for dense 3D localization. We implement our approach using a bifurcated optical system that modifies two separate PSFs, and design the PSFs using three different approaches including end-to-end learning. We demonstrate our approach experimentally by volumetric imaging of fluorescently labelled telomeres in cells.



### Attention-Driven Body Pose Encoding for Human Activity Recognition
- **Arxiv ID**: http://arxiv.org/abs/2009.14326v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2009.14326v2)
- **Published**: 2020-09-29 22:17:17+00:00
- **Updated**: 2020-10-02 17:53:46+00:00
- **Authors**: B Debnath, M O'brien, S Kumar, A Behera
- **Comment**: This paper has been accepted for publication at the IAPR
  IEEE/Computer Society International Conference on Pattern Recognition (ICPR),
  Milan, 2021
- **Journal**: IAPR IEEE/Computer Society International Conference on Pattern
  Recognition (ICPR), Milan, 2021
- **Summary**: This article proposes a novel attention-based body pose encoding for human activity recognition that presents a enriched representation of body-pose that is learned. The enriched data complements the 3D body joint position data and improves model performance. In this paper, we propose a novel approach that learns enhanced feature representations from a given sequence of 3D body joints. To achieve this encoding, the approach exploits 1) a spatial stream which encodes the spatial relationship between various body joints at each time point to learn spatial structure involving the spatial distribution of different body joints 2) a temporal stream that learns the temporal variation of individual body joints over the entire sequence duration to present a temporally enhanced representation. Afterwards, these two pose streams are fused with a multi-head attention mechanism. % adapted from neural machine translation. We also capture the contextual information from the RGB video stream using a Inception-ResNet-V2 model combined with a multi-head attention and a bidirectional Long Short-Term Memory (LSTM) network. %Moreover, we whose performance is enhanced through the multi-head attention mechanism. Finally, the RGB video stream is combined with the fused body pose stream to give a novel end-to-end deep model for effective human activity recognition.



### Deep-3DAligner: Unsupervised 3D Point Set Registration Network With Optimizable Latent Vector
- **Arxiv ID**: http://arxiv.org/abs/2010.00321v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.00321v1)
- **Published**: 2020-09-29 22:44:38+00:00
- **Updated**: 2020-09-29 22:44:38+00:00
- **Authors**: Lingjing Wang, Xiang Li, Yi Fang
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2006.06200
- **Journal**: None
- **Summary**: Point cloud registration is the process of aligning a pair of point sets via searching for a geometric transformation. Unlike classical optimization-based methods, recent learning-based methods leverage the power of deep learning for registering a pair of point sets. In this paper, we propose to develop a novel model that organically integrates the optimization to learning, aiming to address the technical challenges in 3D registration. More specifically, in addition to the deep transformation decoding network, our framework introduce an optimizable deep \underline{S}patial \underline{C}orrelation \underline{R}epresentation (SCR) feature. The SCR feature and weights of the transformation decoder network are jointly updated towards the minimization of an unsupervised alignment loss. We further propose an adaptive Chamfer loss for aligning partial shapes. To verify the performance of our proposed method, we conducted extensive experiments on the ModelNet40 dataset. The results demonstrate that our method achieves significantly better performance than the previous state-of-the-art approaches in the full/partial point set registration task.



### Geometric Matrix Completion: A Functional View
- **Arxiv ID**: http://arxiv.org/abs/2009.14343v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.SI, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2009.14343v1)
- **Published**: 2020-09-29 23:23:04+00:00
- **Updated**: 2020-09-29 23:23:04+00:00
- **Authors**: Abhishek Sharma, Maks Ovsjanikov
- **Comment**: Accepted at GRL workshop, ICML'20. Code:
  \url{https://github.com/Not-IITian/functional-matrix-completion}
- **Journal**: None
- **Summary**: We propose a totally functional view of geometric matrix completion problem. Differently from existing work, we propose a novel regularization inspired from the functional map literature that is more interpretable and theoretically sound. On synthetic tasks with strong underlying geometric structure, our framework outperforms state of the art by a huge margin (two order of magnitude) demonstrating the potential of our approach. On real datasets, we achieve state-of-the-art results at a fraction of the computational effort of previous methods. Our code is publicly available at https://github.com/Not-IITian/functional-matrix-completion



