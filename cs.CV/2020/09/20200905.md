# Arxiv Papers in cs.CV on 2020-09-05
### Clustering COVID-19 Lung Scans
- **Arxiv ID**: http://arxiv.org/abs/2009.09899v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML, I.5.3
- **Links**: [PDF](http://arxiv.org/pdf/2009.09899v2)
- **Published**: 2020-09-05 00:21:13+00:00
- **Updated**: 2021-12-01 06:21:58+00:00
- **Authors**: Jacob Householder, Andrew Householder, John Paul Gomez-Reed, Fredrick Park, Shuai Zhang
- **Comment**: 11 pages, 7 figures
- **Journal**: None
- **Summary**: With the ongoing COVID-19 pandemic, understanding the characteristics of the virus has become an important and challenging task in the scientific community. While tests do exist for COVID-19, the goal of our research is to explore other methods of identifying infected individuals. Our group applied unsupervised clustering techniques to explore a dataset of lungscans of COVID-19 infected, Viral Pneumonia infected, and healthy individuals. This is an important area to explore as COVID-19 is a novel disease that is currently being studied in detail. Our methodology explores the potential that unsupervised clustering algorithms have to reveal important hidden differences between COVID-19 and other respiratory illnesses. Our experiments use: Principal Component Analysis (PCA), K-Means++ (KM++) and the recently developed Robust Continuous Clustering algorithm (RCC). We evaluate the performance of KM++ and RCC in clustering COVID-19 lung scans using the Adjusted Mutual Information (AMI) score.



### Player Identification in Hockey Broadcast Videos
- **Arxiv ID**: http://arxiv.org/abs/2009.02429v2
- **DOI**: 10.1016/j.eswa.2020.113891
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.02429v2)
- **Published**: 2020-09-05 01:30:15+00:00
- **Updated**: 2020-09-14 01:18:30+00:00
- **Authors**: Alvin Chan, Martin D. Levine, Mehrsan Javan
- **Comment**: None
- **Journal**: Volume 165, 1 March 2021, 113891
- **Summary**: We present a deep recurrent convolutional neural network (CNN) approach to solve the problem of hockey player identification in NHL broadcast videos. Player identification is a difficult computer vision problem mainly because of the players' similar appearance, occlusion, and blurry facial and physical features. However, we can observe players' jersey numbers over time by processing variable length image sequences of players (aka 'tracklets'). We propose an end-to-end trainable ResNet+LSTM network, with a residual network (ResNet) base and a long short-term memory (LSTM) layer, to discover spatio-temporal features of jersey numbers over time and learn long-term dependencies. For this work, we created a new hockey player tracklet dataset that contains sequences of hockey player bounding boxes. Additionally, we employ a secondary 1-dimensional convolutional neural network classifier as a late score-level fusion method to classify the output of the ResNet+LSTM network. This achieves an overall player identification accuracy score over 87% on the test split of our new dataset.



### GazeMAE: General Representations of Eye Movements using a Micro-Macro Autoencoder
- **Arxiv ID**: http://arxiv.org/abs/2009.02437v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.02437v2)
- **Published**: 2020-09-05 02:13:42+00:00
- **Updated**: 2020-10-25 06:02:31+00:00
- **Authors**: Louise Gillian C. Bautista, Prospero C. Naval Jr
- **Comment**: Accepted to 25th International Conference on Pattern Recognition
  (ICPR2020)
- **Journal**: None
- **Summary**: Eye movements are intricate and dynamic events that contain a wealth of information about the subject and the stimuli. We propose an abstract representation of eye movements that preserve the important nuances in gaze behavior while being stimuli-agnostic. We consider eye movements as raw position and velocity signals and train separate deep temporal convolutional autoencoders. The autoencoders learn micro-scale and macro-scale representations that correspond to the fast and slow features of eye movements. We evaluate the joint representations with a linear classifier fitted on various classification tasks. Our work accurately discriminates between gender and age groups, and outperforms previous works on biometrics and stimuli clasification. Further experiments highlight the validity and generalizability of this method, bringing eye tracking research closer to real-world applications.



### User-Guided Domain Adaptation for Rapid Annotation from User Interactions: A Study on Pathological Liver Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2009.02455v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.02455v1)
- **Published**: 2020-09-05 04:24:58+00:00
- **Updated**: 2020-09-05 04:24:58+00:00
- **Authors**: Ashwin Raju, Zhanghexuan Ji, Chi Tung Cheng, Jinzheng Cai, Junzhou Huang, Jing Xiao, Le Lu, ChienHung Liao, Adam P. Harrison
- **Comment**: None
- **Journal**: None
- **Summary**: Mask-based annotation of medical images, especially for 3D data, is a bottleneck in developing reliable machine learning models. Using minimal-labor user interactions (UIs) to guide the annotation is promising, but challenges remain on best harmonizing the mask prediction with the UIs. To address this, we propose the user-guided domain adaptation (UGDA) framework, which uses prediction-based adversarial domain adaptation (PADA) to model the combined distribution of UIs and mask predictions. The UIs are then used as anchors to guide and align the mask prediction. Importantly, UGDA can both learn from unlabelled data and also model the high-level semantic meaning behind different UIs. We test UGDA on annotating pathological livers using a clinically comprehensive dataset of 927 patient studies. Using only extreme-point UIs, we achieve a mean (worst-case) performance of 96.1%(94.9%), compared to 93.0% (87.0%) for deep extreme points (DEXTR). Furthermore, we also show UGDA can retain this state-of-the-art performance even when only seeing a fraction of available UIs, demonstrating an ability for robust and reliable UI-guided segmentation with extremely minimal labor demands.



### Dual Manifold Adversarial Robustness: Defense against Lp and non-Lp Adversarial Attacks
- **Arxiv ID**: http://arxiv.org/abs/2009.02470v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.02470v1)
- **Published**: 2020-09-05 06:00:28+00:00
- **Updated**: 2020-09-05 06:00:28+00:00
- **Authors**: Wei-An Lin, Chun Pong Lau, Alexander Levine, Rama Chellappa, Soheil Feizi
- **Comment**: None
- **Journal**: None
- **Summary**: Adversarial training is a popular defense strategy against attack threat models with bounded Lp norms. However, it often degrades the model performance on normal images and the defense does not generalize well to novel attacks. Given the success of deep generative models such as GANs and VAEs in characterizing the underlying manifold of images, we investigate whether or not the aforementioned problems can be remedied by exploiting the underlying manifold information. To this end, we construct an "On-Manifold ImageNet" (OM-ImageNet) dataset by projecting the ImageNet samples onto the manifold learned by StyleGSN. For this dataset, the underlying manifold information is exact. Using OM-ImageNet, we first show that adversarial training in the latent space of images improves both standard accuracy and robustness to on-manifold attacks. However, since no out-of-manifold perturbations are realized, the defense can be broken by Lp adversarial attacks. We further propose Dual Manifold Adversarial Training (DMAT) where adversarial perturbations in both latent and image spaces are used in robustifying the model. Our DMAT improves performance on normal images, and achieves comparable robustness to the standard adversarial training against Lp attacks. In addition, we observe that models defended by DMAT achieve improved robustness against novel attacks which manipulate images by global color shifts or various types of image filtering. Interestingly, similar improvements are also achieved when the defended models are tested on out-of-manifold natural images. These results demonstrate the potential benefits of using manifold information in enhancing robustness of deep learning models against various types of novel adversarial attacks.



### Reverse-engineering Bar Charts Using Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2009.02491v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.02491v1)
- **Published**: 2020-09-05 08:14:35+00:00
- **Updated**: 2020-09-05 08:14:35+00:00
- **Authors**: Fangfang Zhou, Yong Zhao, Wenjiang Chen, Yijing Tan, Yaqi Xu, Yi Chen, Chao Liu, Ying Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: Reverse-engineering bar charts extracts textual and numeric information from the visual representations of bar charts to support application scenarios that require the underlying information. In this paper, we propose a neural network-based method for reverse-engineering bar charts. We adopt a neural network-based object detection model to simultaneously localize and classify textual information. This approach improves the efficiency of textual information extraction. We design an encoder-decoder framework that integrates convolutional and recurrent neural networks to extract numeric information. We further introduce an attention mechanism into the framework to achieve high accuracy and robustness. Synthetic and real-world datasets are used to evaluate the effectiveness of the method. To the best of our knowledge, this work takes the lead in constructing a complete neural network-based method of reverse-engineering bar charts.



### Generalization on the Enhancement of Layerwise Relevance Interpretability of Deep Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2009.02516v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2009.02516v2)
- **Published**: 2020-09-05 11:26:53+00:00
- **Updated**: 2020-10-18 10:01:52+00:00
- **Authors**: Erico Tjoa, Guan Cuntai
- **Comment**: None
- **Journal**: None
- **Summary**: The practical application of deep neural networks are still limited by their lack of transparency. One of the efforts to provide explanation for decisions made by artificial intelligence (AI) is the use of saliency or heat maps highlighting relevant regions that contribute significantly to its prediction. A layer-wise amplitude filtering method was previously introduced to improve the quality of heatmaps, performing error corrections by noise-spike suppression. In this study, we generalize the layerwise error correction by considering any identifiable error and assuming there exists a groundtruth interpretable information. The forms of errors propagated through layerwise relevance methods are studied and we propose a filtering technique for interpretability signal rectification taylored to the trend of signal amplitude of the particular neural network used. Finally, we put forth arguments for the use of groundtruth interpretable information.



### Visual Object Tracking by Segmentation with Graph Convolutional Network
- **Arxiv ID**: http://arxiv.org/abs/2009.02523v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.02523v2)
- **Published**: 2020-09-05 12:43:21+00:00
- **Updated**: 2020-09-08 01:04:13+00:00
- **Authors**: Bo Jiang, Panpan Zhang, Lili Huang
- **Comment**: None
- **Journal**: None
- **Summary**: Segmentation-based tracking has been actively studied in computer vision and multimedia. Superpixel based object segmentation and tracking methods are usually developed for this task. However, they independently perform feature representation and learning of superpixels which may lead to sub-optimal results. In this paper, we propose to utilize graph convolutional network (GCN) model for superpixel based object tracking. The proposed model provides a general end-to-end framework which integrates i) label linear prediction, and ii) structure-aware feature information of each superpixel together to obtain object segmentation and further improves the performance of tracking. The main benefits of the proposed GCN method have two main aspects. First, it provides an effective end-to-end way to exploit both spatial and temporal consistency constraint for target object segmentation. Second, it utilizes a mixed graph convolution module to learn a context-aware and discriminative feature for superpixel representation and labeling. An effective algorithm has been developed to optimize the proposed model. Extensive experiments on five datasets demonstrate that our method obtains better performance against existing alternative methods.



### A Self-Supervised Gait Encoding Approach with Locality-Awareness for 3D Skeleton Based Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2009.03671v3
- **DOI**: 10.1109/TPAMI.2021.3092833
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.03671v3)
- **Published**: 2020-09-05 16:06:04+00:00
- **Updated**: 2021-07-05 02:37:09+00:00
- **Authors**: Haocong Rao, Siqi Wang, Xiping Hu, Mingkui Tan, Yi Guo, Jun Cheng, Xinwang Liu, Bin Hu
- **Comment**: Accepted by IEEE Transactions on Pattern Analysis and Machine
  Intelligence (T-PAMI). Journal version of
  https://www.ijcai.org/proceedings/2020/0125 (IJCAI 2020). Codes are available
  at https://github.com/Kali-Hac/Locality-Awareness-SGE. arXiv admin note: text
  overlap with arXiv:2008.09435
- **Journal**: None
- **Summary**: Person re-identification (Re-ID) via gait features within 3D skeleton sequences is a newly-emerging topic with several advantages. Existing solutions either rely on hand-crafted descriptors or supervised gait representation learning. This paper proposes a self-supervised gait encoding approach that can leverage unlabeled skeleton data to learn gait representations for person Re-ID. Specifically, we first create self-supervision by learning to reconstruct unlabeled skeleton sequences reversely, which involves richer high-level semantics to obtain better gait representations. Other pretext tasks are also explored to further improve self-supervised learning. Second, inspired by the fact that motion's continuity endows adjacent skeletons in one skeleton sequence and temporally consecutive skeleton sequences with higher correlations (referred as locality in 3D skeleton data), we propose a locality-aware attention mechanism and a locality-aware contrastive learning scheme, which aim to preserve locality-awareness on intra-sequence level and inter-sequence level respectively during self-supervised learning. Last, with context vectors learned by our locality-aware attention mechanism and contrastive learning scheme, a novel feature named Constrastive Attention-based Gait Encodings (CAGEs) is designed to represent gait effectively. Empirical evaluations show that our approach significantly outperforms skeleton-based counterparts by 15-40% Rank-1 accuracy, and it even achieves superior performance to numerous multi-modal methods with extra RGB or depth information. Our codes are available at https://github.com/Kali-Hac/Locality-Awareness-SGE.



### Semi-supervised Pathology Segmentation with Disentangled Representations
- **Arxiv ID**: http://arxiv.org/abs/2009.02564v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.02564v1)
- **Published**: 2020-09-05 17:07:59+00:00
- **Updated**: 2020-09-05 17:07:59+00:00
- **Authors**: Haochuan Jiang, Agisilaos Chartsias, Xinheng Zhang, Giorgos Papanastasiou, Scott Semple, Mark Dweck, David Semple, Rohan Dharmakumar, Sotirios A. Tsaftaris
- **Comment**: 12 Pages, 4 figures
- **Journal**: MICCAI-2020 DART workshop
- **Summary**: Automated pathology segmentation remains a valuable diagnostic tool in clinical practice. However, collecting training data is challenging. Semi-supervised approaches by combining labelled and unlabelled data can offer a solution to data scarcity. An approach to semi-supervised learning relies on reconstruction objectives (as self-supervision objectives) that learns in a joint fashion suitable representations for the task. Here, we propose Anatomy-Pathology Disentanglement Network (APD-Net), a pathology segmentation model that attempts to learn jointly for the first time: disentanglement of anatomy, modality, and pathology. The model is trained in a semi-supervised fashion with new reconstruction losses directly aiming to improve pathology segmentation with limited annotations. In addition, a joint optimization strategy is proposed to fully take advantage of the available annotations. We evaluate our methods with two private cardiac infarction segmentation datasets with LGE-MRI scans. APD-Net can perform pathology segmentation with few annotations, maintain performance with different amounts of supervision, and outperform related deep learning methods.



### An Efficient Technique for Image Captioning using Deep Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2009.02565v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.02565v1)
- **Published**: 2020-09-05 17:11:44+00:00
- **Updated**: 2020-09-05 17:11:44+00:00
- **Authors**: Borneel Bikash Phukan, Amiya Ranjan Panda
- **Comment**: 5 pages, 10 figures, Under review by an internationally recognized
  Scopus indexed journal 2020
- **Journal**: None
- **Summary**: With the huge expansion of internet and trillions of gigabytes of data generated every single day, the needs for the development of various tools has become mandatory in order to maintain system adaptability to rapid changes. One of these tools is known as Image Captioning. Every entity in internet must be properly identified and managed and therefore in the case of image data, automatic captioning for identification is required. Similarly, content generation for missing labels, image classification and artificial languages all requires the process of Image Captioning. This paper discusses an efficient and unique way to perform automatic image captioning on individual image and discusses strategies to improve its performances and functionalities.



### Multimodal Memorability: Modeling Effects of Semantics and Decay on Video Memorability
- **Arxiv ID**: http://arxiv.org/abs/2009.02568v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.02568v1)
- **Published**: 2020-09-05 17:24:02+00:00
- **Updated**: 2020-09-05 17:24:02+00:00
- **Authors**: Anelise Newman, Camilo Fosco, Vincent Casser, Allen Lee, Barry McNamara, Aude Oliva
- **Comment**: European Conference on Computer Vision
- **Journal**: None
- **Summary**: A key capability of an intelligent system is deciding when events from past experience must be remembered and when they can be forgotten. Towards this goal, we develop a predictive model of human visual event memory and how those memories decay over time. We introduce Memento10k, a new, dynamic video memorability dataset containing human annotations at different viewing delays. Based on our findings we propose a new mathematical formulation of memorability decay, resulting in a model that is able to produce the first quantitative estimation of how a video decays in memory over time. In contrast with previous work, our model can predict the probability that a video will be remembered at an arbitrary delay. Importantly, our approach combines visual and semantic information (in the form of textual captions) to fully represent the meaning of events. Our experiments on two video memorability benchmarks, including Memento10k, show that our model significantly improves upon the best prior approach (by 12% on average).



### Learning from Multiple Datasets with Heterogeneous and Partial Labels for Universal Lesion Detection in CT
- **Arxiv ID**: http://arxiv.org/abs/2009.02577v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.02577v3)
- **Published**: 2020-09-05 17:55:21+00:00
- **Updated**: 2021-01-03 18:55:59+00:00
- **Authors**: Ke Yan, Jinzheng Cai, Youjing Zheng, Adam P. Harrison, Dakai Jin, Youbao Tang, Yuxing Tang, Lingyun Huang, Jing Xiao, Le Lu
- **Comment**: IEEE Trans on Medical Imaging. Annotation and evaluation code in
  https://github.com/viggin/DeepLesion_manual_test_set
- **Journal**: None
- **Summary**: Large-scale datasets with high-quality labels are desired for training accurate deep learning models. However, due to the annotation cost, datasets in medical imaging are often either partially-labeled or small. For example, DeepLesion is such a large-scale CT image dataset with lesions of various types, but it also has many unlabeled lesions (missing annotations). When training a lesion detector on a partially-labeled dataset, the missing annotations will generate incorrect negative signals and degrade the performance. Besides DeepLesion, there are several small single-type datasets, such as LUNA for lung nodules and LiTS for liver tumors. These datasets have heterogeneous label scopes, i.e., different lesion types are labeled in different datasets with other types ignored. In this work, we aim to develop a universal lesion detection algorithm to detect a variety of lesions. The problem of heterogeneous and partial labels is tackled. First, we build a simple yet effective lesion detection framework named Lesion ENSemble (LENS). LENS can efficiently learn from multiple heterogeneous lesion datasets in a multi-task fashion and leverage their synergy by proposal fusion. Next, we propose strategies to mine missing annotations from partially-labeled datasets by exploiting clinical prior knowledge and cross-dataset knowledge transfer. Finally, we train our framework on four public lesion datasets and evaluate it on 800 manually-labeled sub-volumes in DeepLesion. Our method brings a relative improvement of 49% compared to the current state-of-the-art approach in the metric of average sensitivity. We have publicly released our manual 3D annotations of DeepLesion in https://github.com/viggin/DeepLesion_manual_test_set.



### Deep Sparse Light Field Refocusing
- **Arxiv ID**: http://arxiv.org/abs/2009.02582v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2009.02582v1)
- **Published**: 2020-09-05 18:34:55+00:00
- **Updated**: 2020-09-05 18:34:55+00:00
- **Authors**: Shachar Ben Dayan, David Mendlovic, Raja Giryes
- **Comment**: Published at BMVC 2020
- **Journal**: None
- **Summary**: Light field photography enables to record 4D images, containing angular information alongside spatial information of the scene. One of the important applications of light field imaging is post-capture refocusing. Current methods require for this purpose a dense field of angle views; those can be acquired with a micro-lens system or with a compressive system. Both techniques have major drawbacks to consider, including bulky structures and angular-spatial resolution trade-off. We present a novel implementation of digital refocusing based on sparse angular information using neural networks. This allows recording high spatial resolution in favor of the angular resolution, thus, enabling to design compact and simple devices with improved hardware as well as better performance of compressive systems. We use a novel convolutional neural network whose relatively small structure enables fast reconstruction with low memory consumption. Moreover, it allows handling without re-training various refocusing ranges and noise levels. Results show major improvement compared to existing methods.



