# Arxiv Papers in cs.CV on 2020-09-19
### Kernel Ridge Regression Using Importance Sampling with Application to Seismic Response Prediction
- **Arxiv ID**: http://arxiv.org/abs/2009.09136v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2009.09136v1)
- **Published**: 2020-09-19 01:44:56+00:00
- **Updated**: 2020-09-19 01:44:56+00:00
- **Authors**: Farhad Pourkamali-Anaraki, Mohammad Amin Hariri-Ardebili, Lydia Morawiec
- **Comment**: Accepted for publication in IEEE International Conference on Machine
  Learning and Applications (ICMLA)
- **Journal**: None
- **Summary**: Scalable kernel methods, including kernel ridge regression, often rely on low-rank matrix approximations using the Nystrom method, which involves selecting landmark points from large data sets. The existing approaches to selecting landmarks are typically computationally demanding as they require manipulating and performing computations with large matrices in the input or feature space. In this paper, our contribution is twofold. The first contribution is to propose a novel landmark selection method that promotes diversity using an efficient two-step approach. Our landmark selection technique follows a coarse to fine strategy, where the first step computes importance scores with a single pass over the whole data. The second step performs K-means clustering on the constructed coreset to use the obtained centroids as landmarks. Hence, the introduced method provides tunable trade-offs between accuracy and efficiency. Our second contribution is to investigate the performance of several landmark selection techniques using a novel application of kernel methods for predicting structural responses due to earthquake load and material uncertainties. Our experiments exhibit the merits of our proposed landmark selection scheme against baselines.



### Lossless White Balance For Improved Lossless CFA Image and Video Compression
- **Arxiv ID**: http://arxiv.org/abs/2009.09137v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.09137v1)
- **Published**: 2020-09-19 01:47:26+00:00
- **Updated**: 2020-09-19 01:47:26+00:00
- **Authors**: Yeejin Lee, Keigo Hirakawa
- **Comment**: None
- **Journal**: None
- **Summary**: Color filter array is spatial multiplexing of pixel-sized filters placed over pixel detectors in camera sensors. The state-of-the-art lossless coding techniques of raw sensor data captured by such sensors leverage spatial or cross-color correlation using lifting schemes. In this paper, we propose a lifting-based lossless white balance algorithm. When applied to the raw sensor data, the spatial bandwidth of the implied chrominance signals decreases. We propose to use this white balance as a pre-processing step to lossless CFA subsampled image/video compression, improving the overall coding efficiency of the raw sensor data.



### Introspective Learning by Distilling Knowledge from Online Self-explanation
- **Arxiv ID**: http://arxiv.org/abs/2009.09140v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.09140v1)
- **Published**: 2020-09-19 02:05:32+00:00
- **Updated**: 2020-09-19 02:05:32+00:00
- **Authors**: Jindong Gu, Zhiliang Wu, Volker Tresp
- **Comment**: None
- **Journal**: 15th Asian Conference on Computer Vision (ACCV) 2020
- **Summary**: In recent years, many explanation methods have been proposed to explain individual classifications of deep neural networks. However, how to leverage the created explanations to improve the learning process has been less explored. As the privileged information, the explanations of a model can be used to guide the learning process of the model itself. In the community, another intensively investigated privileged information used to guide the training of a model is the knowledge from a powerful teacher model. The goal of this work is to leverage the self-explanation to improve the learning process by borrowing ideas from knowledge distillation. We start by investigating the effective components of the knowledge transferred from the teacher network to the student network. Our investigation reveals that both the responses in non-ground-truth classes and class-similarity information in teacher's outputs contribute to the success of the knowledge distillation. Motivated by the conclusion, we propose an implementation of introspective learning by distilling knowledge from online self-explanations. The models trained with the introspective learning procedure outperform the ones trained with the standard learning procedure, as well as the ones trained with different regularization methods. When compared to the models learned from peer networks or teacher networks, our models also show competitive performance and requires neither peers nor teachers.



### BargainNet: Background-Guided Domain Translation for Image Harmonization
- **Arxiv ID**: http://arxiv.org/abs/2009.09169v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.09169v2)
- **Published**: 2020-09-19 05:14:08+00:00
- **Updated**: 2021-04-03 14:56:09+00:00
- **Authors**: Wenyan Cong, Li Niu, Jianfu Zhang, Jing Liang, Liqing Zhang
- **Comment**: Accepted by ICME2021 as Oral
- **Journal**: None
- **Summary**: Image composition is a fundamental operation in image editing field. However, unharmonious foreground and background downgrade the quality of composite image. Image harmonization, which adjusts the foreground to improve the consistency, is an essential yet challenging task. Previous deep learning based methods mainly focus on directly learning the mapping from composite image to real image, while ignoring the crucial guidance role that background plays. In this work, with the assumption that the foreground needs to be translated to the same domain as background, we formulate image harmonization task as background-guided domain translation. Therefore, we propose an image harmonization network with a novel domain code extractor and well-tailored triplet losses, which could capture the background domain information to guide the foreground harmonization. Extensive experiments on the existing image harmonization benchmark demonstrate the effectiveness of our proposed method. Code is available at https://github.com/bcmi/BargainNet.



### Few-shot learning using pre-training and shots, enriched by pre-trained samples
- **Arxiv ID**: http://arxiv.org/abs/2009.09172v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.09172v1)
- **Published**: 2020-09-19 06:08:07+00:00
- **Updated**: 2020-09-19 06:08:07+00:00
- **Authors**: Detlef Schmicker
- **Comment**: None
- **Journal**: None
- **Summary**: We use the EMNIST dataset of handwritten digits to test a simple approach for few-shot learning. A fully connected neural network is pre-trained with a subset of the 10 digits and used for few-shot learning with untrained digits. Two basic ideas are introduced: during few-shot learning the learning of the first layer is disabled, and for every shot a previously unknown digit is used together with four previously trained digits for the gradient descend, until a predefined threshold condition is fulfilled. This way we reach about 90% accuracy after 10 shots.



### Recognizing Micro-Expression in Video Clip with Adaptive Key-Frame Mining
- **Arxiv ID**: http://arxiv.org/abs/2009.09179v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.09179v3)
- **Published**: 2020-09-19 07:03:16+00:00
- **Updated**: 2021-03-15 07:53:54+00:00
- **Authors**: Min Peng, Chongyang Wang, Yuan Gao, Tao Bi, Tong Chen, Yu Shi, Xiang-Dong Zhou
- **Comment**: Submitted for Review in IEEE Transactions on Multimedia
- **Journal**: None
- **Summary**: As a spontaneous expression of emotion on face, micro-expression reveals the underlying emotion that cannot be controlled by human. In micro-expression, facial movement is transient and sparsely localized through time. However, the existing representation based on various deep learning techniques learned from a full video clip is usually redundant. In addition, methods utilizing the single apex frame of each video clip require expert annotations and sacrifice the temporal dynamics. To simultaneously localize and recognize such fleeting facial movements, we propose a novel end-to-end deep learning architecture, referred to as adaptive key-frame mining network (AKMNet). Operating on the video clip of micro-expression, AKMNet is able to learn discriminative spatio-temporal representation by combining spatial features of self-learned local key frames and their global-temporal dynamics. Theoretical analysis and empirical evaluation show that the proposed approach improved recognition accuracy in comparison with state-of-the-art methods on multiple benchmark datasets.



### ENAS4D: Efficient Multi-stage CNN Architecture Search for Dynamic Inference
- **Arxiv ID**: http://arxiv.org/abs/2009.09182v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.09182v1)
- **Published**: 2020-09-19 08:08:26+00:00
- **Updated**: 2020-09-19 08:08:26+00:00
- **Authors**: Zhihang Yuan, Xin Liu, Bingzhe Wu, Guangyu Sun
- **Comment**: 6 figures
- **Journal**: None
- **Summary**: Dynamic inference is a feasible way to reduce the computational cost of convolutional neural network(CNN), which can dynamically adjust the computation for each input sample. One of the ways to achieve dynamic inference is to use multi-stage neural network, which contains a sub-network with prediction layer at each stage. The inference of a input sample can exit from early stage if the prediction of the stage is confident enough. However, design a multi-stage CNN architecture is a non-trivial task. In this paper, we introduce a general framework, ENAS4D, which can efficiently search for optimal multi-stage CNN architecture for dynamic inference in a well-designed search space. Firstly, we propose a method to construct the search space with multi-stage convolution. The search space include different numbers of layers, different kernel sizes and different numbers of channels for each stage and the resolution of input samples. Then, we train a once-for-all network that supports to sample diverse multi-stage CNN architecture. A specialized multi-stage network can be obtained from the once-for-all network without additional training. Finally, we devise a method to efficiently search for the optimal multi-stage network that trades the accuracy off the computational cost taking the advantage of once-for-all network. The experiments on the ImageNet classification task demonstrate that the multi-stage CNNs searched by ENAS4D consistently outperform the state-of-the-art method for dyanmic inference. In particular, the network achieves 74.4% ImageNet top-1 accuracy under 185M average MACs.



### EI-MTD:Moving Target Defense for Edge Intelligence against Adversarial Attacks
- **Arxiv ID**: http://arxiv.org/abs/2009.10537v3
- **DOI**: None
- **Categories**: **cs.CR**, cs.AI, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2009.10537v3)
- **Published**: 2020-09-19 09:04:18+00:00
- **Updated**: 2020-11-25 01:13:39+00:00
- **Authors**: Yaguan Qian, Qiqi Shao, Jiamin Wang, Xiang Lin, Yankai Guo, Zhaoquan Gu, Bin Wang, Chunming Wu
- **Comment**: None
- **Journal**: None
- **Summary**: With the boom of edge intelligence, its vulnerability to adversarial attacks becomes an urgent problem. The so-called adversarial example can fool a deep learning model on the edge node to misclassify. Due to the property of transferability, the adversary can easily make a black-box attack using a local substitute model. Nevertheless, the limitation of resource of edge nodes cannot afford a complicated defense mechanism as doing on the cloud data center. To overcome the challenge, we propose a dynamic defense mechanism, namely EI-MTD. It first obtains robust member models with small size through differential knowledge distillation from a complicated teacher model on the cloud data center. Then, a dynamic scheduling policy based on a Bayesian Stackelberg game is applied to the choice of a target model for service. This dynamic defense can prohibit the adversary from selecting an optimal substitute model for black-box attacks. Our experimental result shows that this dynamic scheduling can effectively protect edge intelligence against adversarial attacks under the black-box setting.



### A Review of Visual Odometry Methods and Its Applications for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2009.09193v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2009.09193v1)
- **Published**: 2020-09-19 09:13:27+00:00
- **Updated**: 2020-09-19 09:13:27+00:00
- **Authors**: Kai Li Lim, Thomas Bräunl
- **Comment**: 15 pages, 2 figures, 2 tables
- **Journal**: None
- **Summary**: The research into autonomous driving applications has observed an increase in computer vision-based approaches in recent years. In attempts to develop exclusive vision-based systems, visual odometry is often considered as a key element to achieve motion estimation and self-localisation, in place of wheel odometry or inertial measurements. This paper presents a recent review to methods that are pertinent to visual odometry with an emphasis on autonomous driving. This review covers visual odometry in their monocular, stereoscopic and visual-inertial form, individually presenting them with analyses related to their applications. Discussions are drawn to outline the problems faced in the current state of research, and to summarise the works reviewed. This paper concludes with future work suggestions to aid prospective developments in visual odometry.



### Multi-Level Graph Convolutional Network with Automatic Graph Learning for Hyperspectral Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2009.09196v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.09196v1)
- **Published**: 2020-09-19 09:26:20+00:00
- **Updated**: 2020-09-19 09:26:20+00:00
- **Authors**: Sheng Wan, Chen Gong, Shirui Pan, Jie Yang, Jian Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Nowadays, deep learning methods, especially the Graph Convolutional Network (GCN), have shown impressive performance in hyperspectral image (HSI) classification. However, the current GCN-based methods treat graph construction and image classification as two separate tasks, which often results in suboptimal performance. Another defect of these methods is that they mainly focus on modeling the local pairwise importance between graph nodes while lack the capability to capture the global contextual information of HSI. In this paper, we propose a Multi-level GCN with Automatic Graph Learning method (MGCN-AGL) for HSI classification, which can automatically learn the graph information at both local and global levels. By employing attention mechanism to characterize the importance among spatially neighboring regions, the most relevant information can be adaptively incorporated to make decisions, which helps encode the spatial context to form the graph information at local level. Moreover, we utilize multiple pathways for local-level graph convolution, in order to leverage the merits from the diverse spatial context of HSI and to enhance the expressive power of the generated representations. To reconstruct the global contextual relations, our MGCN-AGL encodes the long range dependencies among image regions based on the expressive representations that have been produced at local level. Then inference can be performed along the reconstructed graph edges connecting faraway regions. Finally, the multi-level information is adaptively fused to generate the network output. In this means, the graph learning and image classification can be integrated into a unified framework and benefit each other. Extensive experiments have been conducted on three real-world hyperspectral datasets, which are shown to outperform the state-of-the-art methods.



### Weak-shot Fine-grained Classification via Similarity Transfer
- **Arxiv ID**: http://arxiv.org/abs/2009.09197v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.09197v2)
- **Published**: 2020-09-19 09:31:52+00:00
- **Updated**: 2021-10-23 02:57:36+00:00
- **Authors**: Junjie Chen, Li Niu, Liu Liu, Liqing Zhang
- **Comment**: accepted by NeurIPS2021
- **Journal**: None
- **Summary**: Recognizing fine-grained categories remains a challenging task, due to the subtle distinctions among different subordinate categories, which results in the need of abundant annotated samples. To alleviate the data-hungry problem, we consider the problem of learning novel categories from web data with the support of a clean set of base categories, which is referred to as weak-shot learning. In this setting, we propose a method called SimTrans to transfer pairwise semantic similarity from base categories to novel categories. Specifically, we firstly train a similarity net on clean data, and then leverage the transferred similarity to denoise web training data using two simple yet effective strategies. In addition, we apply adversarial loss on similarity net to enhance the transferability of similarity. Comprehensive experiments demonstrate the effectiveness of our weak-shot setting and our SimTrans method. Datasets and codes are available at https://github.com/bcmi/SimTrans-Weak-Shot-Classification.



### Adversarial Rain Attack and Defensive Deraining for DNN Perception
- **Arxiv ID**: http://arxiv.org/abs/2009.09205v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/2009.09205v2)
- **Published**: 2020-09-19 10:12:08+00:00
- **Updated**: 2022-02-03 06:32:48+00:00
- **Authors**: Liming Zhai, Felix Juefei-Xu, Qing Guo, Xiaofei Xie, Lei Ma, Wei Feng, Shengchao Qin, Yang Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Rain often poses inevitable threats to deep neural network (DNN) based perception systems, and a comprehensive investigation of the potential risks of the rain to DNNs is of great importance. However, it is rather difficult to collect or synthesize rainy images that can represent all rain situations that would possibly occur in the real world. To this end, in this paper, we start from a new perspective and propose to combine two totally different studies, i.e., rainy image synthesis and adversarial attack. We first present an adversarial rain attack, with which we could simulate various rain situations with the guidance of deployed DNNs and reveal the potential threat factors that can be brought by rain. In particular, we design a factor-aware rain generation that synthesizes rain streaks according to the camera exposure process and models the learnable rain factors for adversarial attack. With this generator, we perform the adversarial rain attack against the image classification and object detection. To defend the DNNs from the negative rain effect, we also present a defensive deraining strategy, for which we design an adversarial rain augmentation that uses mixed adversarial rain layers to enhance deraining models for downstream DNN perception. Our large-scale evaluation on various datasets demonstrates that our synthesized rainy images with realistic appearances not only exhibit strong adversarial capability against DNNs, but also boost the deraining models for defensive purposes, building the foundation for further rain-robust perception studies.



### MSR-DARTS: Minimum Stable Rank of Differentiable Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/2009.09209v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2009.09209v2)
- **Published**: 2020-09-19 11:03:39+00:00
- **Updated**: 2021-03-15 08:58:01+00:00
- **Authors**: Kengo Machida, Kuniaki Uto, Koichi Shinoda, Taiji Suzuki
- **Comment**: None
- **Journal**: None
- **Summary**: In neural architecture search (NAS), differentiable architecture search (DARTS) has recently attracted much attention due to its high efficiency. It defines an over-parameterized network with mixed edges, each of which represents all operator candidates, and jointly optimizes the weights of the network and its architecture in an alternating manner. However, this method finds a model with the weights converging faster than the others, and such a model with fastest convergence often leads to overfitting. Accordingly, the resulting model cannot always be well-generalized. To overcome this problem, we propose a method called minimum stable rank DARTS (MSR-DARTS), for finding a model with the best generalization error by replacing architecture optimization with the selection process using the minimum stable rank criterion. Specifically, a convolution operator is represented by a matrix, and MSR-DARTS selects the one with the smallest stable rank. We evaluated MSR-DARTS on CIFAR-10 and ImageNet datasets. It achieves an error rate of 2.54% with 4.0M parameters within 0.3 GPU-days on CIFAR-10, and a top-1 error rate of 23.9% on ImageNet. The official code is available at https://github.com/mtaecchhi/msrdarts.git.



### Dodging DeepFake Detection via Implicit Spatial-Domain Notch Filtering
- **Arxiv ID**: http://arxiv.org/abs/2009.09213v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.09213v4)
- **Published**: 2020-09-19 11:26:01+00:00
- **Updated**: 2023-03-15 09:33:01+00:00
- **Authors**: Yihao Huang, Felix Juefei-Xu, Qing Guo, Yang Liu, Geguang Pu
- **Comment**: 11 pages
- **Journal**: None
- **Summary**: The current high-fidelity generation and high-precision detection of DeepFake images are at an arms race. We believe that producing DeepFakes that are highly realistic and 'detection evasive' can serve the ultimate goal of improving future generation DeepFake detection capabilities. In this paper, we propose a simple yet powerful pipeline to reduce the artifact patterns of fake images without hurting image quality by performing implicit spatial-domain notch filtering. We first demonstrate that frequency-domain notch filtering, although famously shown to be effective in removing periodic noise in the spatial domain, is infeasible for our task at hand due to the manual designs required for the notch filters. We, therefore, resort to a learning-based approach to reproduce the notch filtering effects, but solely in the spatial domain. We adopt a combination of adding overwhelming spatial noise for breaking the periodic noise pattern and deep image filtering to reconstruct the noise-free fake images, and we name our method DeepNotch. Deep image filtering provides a specialized filter for each pixel in the noisy image, producing filtered images with high fidelity compared to their DeepFake counterparts. Moreover, we also use the semantic information of the image to generate an adversarial guidance map to add noise intelligently. Our large-scale evaluation on 3 representative state-of-the-art DeepFake detection methods (tested on 16 types of DeepFakes) has demonstrated that our technique significantly reduces the accuracy of these 3 fake image detection methods, 36.79% on average and up to 97.02% in the best case.



### Adversarial Exposure Attack on Diabetic Retinopathy Imagery
- **Arxiv ID**: http://arxiv.org/abs/2009.09231v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.09231v1)
- **Published**: 2020-09-19 13:47:33+00:00
- **Updated**: 2020-09-19 13:47:33+00:00
- **Authors**: Yupeng Cheng, Felix Juefei-Xu, Qing Guo, Huazhu Fu, Xiaofei Xie, Shang-Wei Lin, Weisi Lin, Yang Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Diabetic retinopathy (DR) is a leading cause of vision loss in the world and numerous cutting-edge works have built powerful deep neural networks (DNNs) to automatically classify the DR cases via the retinal fundus images (RFIs). However, RFIs are usually affected by the widely existing camera exposure while the robustness of DNNs to the exposure is rarely explored. In this paper, we study this problem from the viewpoint of adversarial attack and identify a totally new task, i.e., adversarial exposure attack generating adversarial images by tuning image exposure to mislead the DNNs with significantly high transferability. To this end, we first implement a straightforward method, i.e., multiplicative-perturbation-based exposure attack, and reveal the big challenges of this new task. Then, to make the adversarial image naturalness, we propose the adversarial bracketed exposure fusion that regards the exposure attack as an element-wise bracketed exposure fusion problem in the Laplacian-pyramid space. Moreover, to realize high transferability, we further propose the convolutional bracketed exposure fusion where the element-wise multiplicative operation is extended to the convolution. We validate our method on the real public DR dataset with the advanced DNNs, e.g., ResNet50, MobileNet, and EfficientNet, showing our method can achieve high image quality and success rate of the transfer attack. Our method reveals the potential threats to the DNN-based DR automated diagnosis and can definitely benefit the development of exposure-robust automated DR diagnosis method in the future.



### Open-Ended Fine-Grained 3D Object Categorization by Combining Shape and Texture Features in Multiple Colorspaces
- **Arxiv ID**: http://arxiv.org/abs/2009.09235v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2009.09235v3)
- **Published**: 2020-09-19 14:06:18+00:00
- **Updated**: 2021-05-28 19:54:03+00:00
- **Authors**: Nils Keunecke, S. Hamidreza Kasaei
- **Comment**: None
- **Journal**: None
- **Summary**: As a consequence of an ever-increasing number of service robots, there is a growing demand for highly accurate real-time 3D object recognition. Considering the expansion of robot applications in more complex and dynamic environments,it is evident that it is not possible to pre-program all object categories and anticipate all exceptions in advance. Therefore, robots should have the functionality to learn about new object categories in an open-ended fashion while working in the environment.Towards this goal, we propose a deep transfer learning approach to generate a scale- and pose-invariant object representation by considering shape and texture information in multiple colorspaces. The obtained global object representation is then fed to an instance-based object category learning and recognition,where a non-expert human user exists in the learning loop and can interactively guide the process of experience acquisition by teaching new object categories, or by correcting insufficient or erroneous categories. In this work, shape information encodes the common patterns of all categories, while texture information is used to describes the appearance of each instance in detail.Multiple color space combinations and network architectures are evaluated to find the most descriptive system. Experimental results showed that the proposed network architecture out-performed the selected state-of-the-art approaches in terms of object classification accuracy and scalability. Furthermore, we performed a real robot experiment in the context of serve-a-beer scenario to show the real-time performance of the proposed approach.



### AAA: Adaptive Aggregation of Arbitrary Online Trackers with Theoretical Performance Guarantee
- **Arxiv ID**: http://arxiv.org/abs/2009.09237v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.09237v2)
- **Published**: 2020-09-19 14:16:01+00:00
- **Updated**: 2020-09-24 04:22:11+00:00
- **Authors**: Heon Song, Daiki Suehiro, Seiichi Uchida
- **Comment**: None
- **Journal**: None
- **Summary**: For visual object tracking, it is difficult to realize an almighty online tracker due to the huge variations of target appearance depending on an image sequence. This paper proposes an online tracking method that adaptively aggregates arbitrary multiple online trackers. The performance of the proposed method is theoretically guaranteed to be comparable to that of the best tracker for any image sequence, although the best expert is unknown during tracking. The experimental study on the large variations of benchmark datasets and aggregated trackers demonstrates that the proposed method can achieve state-of-the-art performance. The code is available at https://github.com/songheony/AAA-journal.



### EfficientDeRain: Learning Pixel-wise Dilation Filtering for High-Efficiency Single-Image Deraining
- **Arxiv ID**: http://arxiv.org/abs/2009.09238v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.09238v1)
- **Published**: 2020-09-19 14:32:50+00:00
- **Updated**: 2020-09-19 14:32:50+00:00
- **Authors**: Qing Guo, Jingyang Sun, Felix Juefei-Xu, Lei Ma, Xiaofei Xie, Wei Feng, Yang Liu
- **Comment**: 9 pages, 9 figures
- **Journal**: None
- **Summary**: Single-image deraining is rather challenging due to the unknown rain model. Existing methods often make specific assumptions of the rain model, which can hardly cover many diverse circumstances in the real world, making them have to employ complex optimization or progressive refinement. This, however, significantly affects these methods' efficiency and effectiveness for many efficiency-critical applications. To fill this gap, in this paper, we regard the single-image deraining as a general image-enhancing problem and originally propose a model-free deraining method, i.e., EfficientDeRain, which is able to process a rainy image within 10~ms (i.e., around 6~ms on average), over 80 times faster than the state-of-the-art method (i.e., RCDNet), while achieving similar de-rain effects. We first propose the novel pixel-wise dilation filtering. In particular, a rainy image is filtered with the pixel-wise kernels estimated from a kernel prediction network, by which suitable multi-scale kernels for each pixel can be efficiently predicted. Then, to eliminate the gap between synthetic and real data, we further propose an effective data augmentation method (i.e., RainMix) that helps to train network for real rainy image handling.We perform comprehensive evaluation on both synthetic and real-world rainy datasets to demonstrate the effectiveness and efficiency of our method. We release the model and code in https://github.com/tsingqguo/efficientderain.git.



### Bias Field Poses a Threat to DNN-based X-Ray Recognition
- **Arxiv ID**: http://arxiv.org/abs/2009.09247v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.09247v2)
- **Published**: 2020-09-19 14:58:02+00:00
- **Updated**: 2021-05-03 04:00:36+00:00
- **Authors**: Binyu Tian, Qing Guo, Felix Juefei-Xu, Wen Le Chan, Yupeng Cheng, Xiaohong Li, Xiaofei Xie, Shengchao Qin
- **Comment**: 6 pages, 5 figures; This work has been accepted to ICME 2021 as the
  oral presentation
- **Journal**: None
- **Summary**: The chest X-ray plays a key role in screening and diagnosis of many lung diseases including the COVID-19. More recently, many works construct deep neural networks (DNNs) for chest X-ray images to realize automated and efficient diagnosis of lung diseases. However, bias field caused by the improper medical image acquisition process widely exists in the chest X-ray images while the robustness of DNNs to the bias field is rarely explored, which definitely poses a threat to the X-ray-based automated diagnosis system. In this paper, we study this problem based on the recent adversarial attack and propose a brand new attack, i.e., the adversarial bias field attack where the bias field instead of the additive noise works as the adversarial perturbations for fooling the DNNs. This novel attack posts a key problem: how to locally tune the bias field to realize high attack success rate while maintaining its spatial smoothness to guarantee high realisticity. These two goals contradict each other and thus has made the attack significantly challenging. To overcome this challenge, we propose the adversarial-smooth bias field attack that can locally tune the bias field with joint smooth & adversarial constraints. As a result, the adversarial X-ray images can not only fool the DNNs effectively but also retain very high level of realisticity. We validate our method on real chest X-ray datasets with powerful DNNs, e.g., ResNet50, DenseNet121, and MobileNet, and show different properties to the state-of-the-art attacks in both image realisticity and attack transferability. Our method reveals the potential threat to the DNN-based X-ray automated diagnosis and can definitely benefit the development of bias-field-robust automated diagnosis system.



### City-Scale Visual Place Recognition with Deep Local Features Based on Multi-Scale Ordered VLAD Pooling
- **Arxiv ID**: http://arxiv.org/abs/2009.09255v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.09255v2)
- **Published**: 2020-09-19 15:21:59+00:00
- **Updated**: 2023-05-01 06:34:50+00:00
- **Authors**: Duc Canh Le, Chan Hyun Youn
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: Visual place recognition is the task of recognizing a place depicted in an image based on its pure visual appearance without metadata. In visual place recognition, the challenges lie upon not only the changes in lighting conditions, camera viewpoint, and scale but also the characteristic of scene-level images and the distinct features of the area. To resolve these challenges, one must consider both the local discriminativeness and the global semantic context of images. On the other hand, the diversity of the datasets is also particularly important to develop more general models and advance the progress of the field. In this paper, we present a fully-automated system for place recognition at a city-scale based on content-based image retrieval. Our main contributions to the community lie in three aspects. Firstly, we take a comprehensive analysis of visual place recognition and sketch out the unique challenges of the task compared to general image retrieval tasks. Next, we propose yet a simple pooling approach on top of convolutional neural network activations to embed the spatial information into the image representation vector. Finally, we introduce new datasets for place recognition, which are particularly essential for application-based research. Furthermore, throughout extensive experiments, various issues in both image retrieval and place recognition are analyzed and discussed to give some insights into improving the performance of retrieval models in reality.   The dataset used in this paper can be found at https://github.com/canhld94/Daejeon520



### Can You Spot the Chameleon? Adversarially Camouflaging Images from Co-Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2009.09258v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.09258v5)
- **Published**: 2020-09-19 15:43:46+00:00
- **Updated**: 2022-04-18 02:39:41+00:00
- **Authors**: Ruijun Gao, Qing Guo, Felix Juefei-Xu, Hongkai Yu, Huazhu Fu, Wei Feng, Yang Liu, Song Wang
- **Comment**: Accepted to CVPR 2022
- **Journal**: None
- **Summary**: Co-salient object detection (CoSOD) has recently achieved significant progress and played a key role in retrieval-related tasks. However, it inevitably poses an entirely new safety and security issue, i.e., highly personal and sensitive content can potentially be extracting by powerful CoSOD methods. In this paper, we address this problem from the perspective of adversarial attacks and identify a novel task: adversarial co-saliency attack. Specially, given an image selected from a group of images containing some common and salient objects, we aim to generate an adversarial version that can mislead CoSOD methods to predict incorrect co-salient regions. Note that, compared with general white-box adversarial attacks for classification, this new task faces two additional challenges: (1) low success rate due to the diverse appearance of images in the group; (2) low transferability across CoSOD methods due to the considerable difference between CoSOD pipelines. To address these challenges, we propose the very first black-box joint adversarial exposure and noise attack (Jadena), where we jointly and locally tune the exposure and additive perturbations of the image according to a newly designed high-feature-level contrast-sensitive loss function. Our method, without any information on the state-of-the-art CoSOD methods, leads to significant performance degradation on various co-saliency detection datasets and makes the co-salient objects undetectable. This can have strong practical benefits in properly securing the large number of personal photos currently shared on the Internet. Moreover, our method is potential to be utilized as a metric for evaluating the robustness of CoSOD methods.



### Humans learn too: Better Human-AI Interaction using Optimized Human Inputs
- **Arxiv ID**: http://arxiv.org/abs/2009.09266v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.09266v1)
- **Published**: 2020-09-19 16:30:37+00:00
- **Updated**: 2020-09-19 16:30:37+00:00
- **Authors**: Johannes Schneider
- **Comment**: None
- **Journal**: None
- **Summary**: Humans rely more and more on systems with AI components. The AI community typically treats human inputs as a given and optimizes AI models only. This thinking is one-sided and it neglects the fact that humans can learn, too. In this work, human inputs are optimized for better interaction with an AI model while keeping the model fixed. The optimized inputs are accompanied by instructions on how to create them. They allow humans to save time and cut on errors, while keeping required changes to original inputs limited. We propose continuous and discrete optimization methods modifying samples in an iterative fashion. Our quantitative and qualitative evaluation including a human study on different hand-generated inputs shows that the generated proposals lead to lower error rates, require less effort to create and differ only modestly from the original samples.



### Reducing false-positive biopsies with deep neural networks that utilize local and global information in screening mammograms
- **Arxiv ID**: http://arxiv.org/abs/2009.09282v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.09282v1)
- **Published**: 2020-09-19 18:54:01+00:00
- **Updated**: 2020-09-19 18:54:01+00:00
- **Authors**: Nan Wu, Zhe Huang, Yiqiu Shen, Jungkyu Park, Jason Phang, Taro Makino, S. Gene Kim, Kyunghyun Cho, Laura Heacock, Linda Moy, Krzysztof J. Geras
- **Comment**: None
- **Journal**: None
- **Summary**: Breast cancer is the most common cancer in women, and hundreds of thousands of unnecessary biopsies are done around the world at a tremendous cost. It is crucial to reduce the rate of biopsies that turn out to be benign tissue. In this study, we build deep neural networks (DNNs) to classify biopsied lesions as being either malignant or benign, with the goal of using these networks as second readers serving radiologists to further reduce the number of false positive findings. We enhance the performance of DNNs that are trained to learn from small image patches by integrating global context provided in the form of saliency maps learned from the entire image into their reasoning, similar to how radiologists consider global context when evaluating areas of interest. Our experiments are conducted on a dataset of 229,426 screening mammography exams from 141,473 patients. We achieve an AUC of 0.8 on a test set consisting of 464 benign and 136 malignant lesions.



### Subverting Privacy-Preserving GANs: Hiding Secrets in Sanitized Images
- **Arxiv ID**: http://arxiv.org/abs/2009.09283v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CR, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2009.09283v1)
- **Published**: 2020-09-19 19:02:17+00:00
- **Updated**: 2020-09-19 19:02:17+00:00
- **Authors**: Kang Liu, Benjamin Tan, Siddharth Garg
- **Comment**: None
- **Journal**: None
- **Summary**: Unprecedented data collection and sharing have exacerbated privacy concerns and led to increasing interest in privacy-preserving tools that remove sensitive attributes from images while maintaining useful information for other tasks. Currently, state-of-the-art approaches use privacy-preserving generative adversarial networks (PP-GANs) for this purpose, for instance, to enable reliable facial expression recognition without leaking users' identity. However, PP-GANs do not offer formal proofs of privacy and instead rely on experimentally measuring information leakage using classification accuracy on the sensitive attributes of deep learning (DL)-based discriminators. In this work, we question the rigor of such checks by subverting existing privacy-preserving GANs for facial expression recognition. We show that it is possible to hide the sensitive identification data in the sanitized output images of such PP-GANs for later extraction, which can even allow for reconstruction of the entire input images, while satisfying privacy checks. We demonstrate our approach via a PP-GAN-based architecture and provide qualitative and quantitative evaluations using two public datasets. Our experimental results raise fundamental questions about the need for more rigorous privacy checks of PP-GANs, and we provide insights into the social impact of these.



### Adversarial Consistent Learning on Partial Domain Adaptation of PlantCLEF 2020 Challenge
- **Arxiv ID**: http://arxiv.org/abs/2009.09289v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.09289v1)
- **Published**: 2020-09-19 19:57:41+00:00
- **Updated**: 2020-09-19 19:57:41+00:00
- **Authors**: Youshan Zhang, Brian D. Davison
- **Comment**: None
- **Journal**: None
- **Summary**: Domain adaptation is one of the most crucial techniques to mitigate the domain shift problem, which exists when transferring knowledge from an abundant labeled sourced domain to a target domain with few or no labels. Partial domain adaptation addresses the scenario when target categories are only a subset of source categories. In this paper, to enable the efficient representation of cross-domain plant images, we first extract deep features from pre-trained models and then develop adversarial consistent learning ($ACL$) in a unified deep architecture for partial domain adaptation. It consists of source domain classification loss, adversarial learning loss, and feature consistency loss. Adversarial learning loss can maintain domain-invariant features between the source and target domains. Moreover, feature consistency loss can preserve the fine-grained feature transition between two domains. We also find the shared categories of two domains via down-weighting the irrelevant categories in the source domain. Experimental results demonstrate that training features from NASNetLarge model with proposed $ACL$ architecture yields promising results on the PlantCLEF 2020 Challenge.



### Features based Mammogram Image Classification using Weighted Feature Support Vector Machine
- **Arxiv ID**: http://arxiv.org/abs/2009.09300v1
- **DOI**: 10.1007/978-3-642-29216-3_35
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.MM, eess.IV, I.2.6; I.2.10; I.4.6; I.4.7; I.4.10; I.5
- **Links**: [PDF](http://arxiv.org/pdf/2009.09300v1)
- **Published**: 2020-09-19 21:28:31+00:00
- **Updated**: 2020-09-19 21:28:31+00:00
- **Authors**: S. Kavitha, K. K. Thyagharajan
- **Comment**: 9 pages, 3 figures, "submitted to International Conference on
  Computing and Communication Systems"
- **Journal**: Vol. 270, 2012, 320-329
- **Summary**: In the existing research of mammogram image classification, either clinical data or image features of a specific type is considered along with the supervised classifiers such as Neural Network (NN) and Support Vector Machine (SVM). This paper considers automated classification of breast tissue type as benign or malignant using Weighted Feature Support Vector Machine (WFSVM) through constructing the precomputed kernel function by assigning more weight to relevant features using the principle of maximizing deviations. Initially, MIAS dataset of mammogram images is divided into training and test set, then the preprocessing techniques such as noise removal and background removal are applied to the input images and the Region of Interest (ROI) is identified. The statistical features and texture features are extracted from the ROI and the clinical features are obtained directly from the dataset. The extracted features of the training dataset are used to construct the weighted features and precomputed linear kernel for training the WFSVM, from which the training model file is created. Using this model file the kernel matrix of test samples is classified as benign or malignant. This analysis shows that the texture features have resulted in better accuracy than the other features with WFSVM and SVM. However, the number of support vectors created in WFSVM is less than the SVM classifier.



### High-Resolution Augmentation for Automatic Template-Based Matching of Human Models
- **Arxiv ID**: http://arxiv.org/abs/2009.09312v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.09312v1)
- **Published**: 2020-09-19 22:41:24+00:00
- **Updated**: 2020-09-19 22:41:24+00:00
- **Authors**: Riccardo Marin, Simone Melzi, Emanuele Rodolà, Umberto Castellani
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a new approach for 3D shape matching of deformable human shapes. Our approach is based on the joint adoption of three different tools: an intrinsic spectral matching pipeline, a morphable model, and an extrinsic details refinement. By operating in conjunction, these tools allow us to greatly improve the quality of the matching while at the same time resolving the key issues exhibited by each tool individually. In this paper we present an innovative High-Resolution Augmentation (HRA) strategy that enables highly accurate correspondence even in the presence of significant mesh resolution mismatch between the input shapes. This augmentation provides an effective workaround for the resolution limitations imposed by the adopted morphable model. The HRA in its global and localized versions represents a novel refinement strategy for surface subdivision methods. We demonstrate the accuracy of the proposed pipeline on multiple challenging benchmarks, and showcase its effectiveness in surface registration and texture transfer.



### Efficient Certification of Spatial Robustness
- **Arxiv ID**: http://arxiv.org/abs/2009.09318v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2009.09318v2)
- **Published**: 2020-09-19 23:09:11+00:00
- **Updated**: 2021-01-31 00:24:32+00:00
- **Authors**: Anian Ruoss, Maximilian Baader, Mislav Balunović, Martin Vechev
- **Comment**: Conference Paper at AAAI 2021
- **Journal**: None
- **Summary**: Recent work has exposed the vulnerability of computer vision models to vector field attacks. Due to the widespread usage of such models in safety-critical applications, it is crucial to quantify their robustness against such spatial transformations. However, existing work only provides empirical robustness quantification against vector field deformations via adversarial attacks, which lack provable guarantees. In this work, we propose novel convex relaxations, enabling us, for the first time, to provide a certificate of robustness against vector field transformations. Our relaxations are model-agnostic and can be leveraged by a wide range of neural network verifiers. Experiments on various network architectures and different datasets demonstrate the effectiveness and scalability of our method.



### Learning a Lie Algebra from Unlabeled Data Pairs
- **Arxiv ID**: http://arxiv.org/abs/2009.09321v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.SD, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2009.09321v3)
- **Published**: 2020-09-19 23:23:52+00:00
- **Updated**: 2020-11-12 09:29:36+00:00
- **Authors**: Christopher Ick, Vincent Lostanlen
- **Comment**: 2 pages, 1 figure. Presented at the first DeepMath conference, New
  York City, NY, USA, November 2020
- **Journal**: None
- **Summary**: Deep convolutional networks (convnets) show a remarkable ability to learn disentangled representations. In recent years, the generalization of deep learning to Lie groups beyond rigid motion in $\mathbb{R}^n$ has allowed to build convnets over datasets with non-trivial symmetries, such as patterns over the surface of a sphere. However, one limitation of this approach is the need to explicitly define the Lie group underlying the desired invariance property before training the convnet. Whereas rotations on the sphere have a well-known symmetry group ($\mathrm{SO}(3)$), the same cannot be said of many real-world factors of variability. For example, the disentanglement of pitch, intensity dynamics, and playing technique remains a challenging task in music information retrieval.   This article proposes a machine learning method to discover a nonlinear transformation of the space $\mathbb{R}^n$ which maps a collection of $n$-dimensional vectors $(\boldsymbol{x}_i)_i$ onto a collection of target vectors $(\boldsymbol{y}_i)_i$. The key idea is to approximate every target $\boldsymbol{y}_i$ by a matrix--vector product of the form $\boldsymbol{\widetilde{y}}_i = \boldsymbol{\phi}(t_i) \boldsymbol{x}_i$, where the matrix $\boldsymbol{\phi}(t_i)$ belongs to a one-parameter subgroup of $\mathrm{GL}_n (\mathbb{R})$. Crucially, the value of the parameter $t_i \in \mathbb{R}$ may change between data pairs $(\boldsymbol{x}_i, \boldsymbol{y}_i)$ and does not need to be known in advance.



