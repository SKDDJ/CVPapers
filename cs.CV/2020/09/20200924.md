# Arxiv Papers in cs.CV on 2020-09-24
### FTN: Foreground-Guided Texture-Focused Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2009.11425v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.11425v1)
- **Published**: 2020-09-24 00:44:05+00:00
- **Updated**: 2020-09-24 00:44:05+00:00
- **Authors**: Donghaisheng Liu, Shoudong Han, Yang Chen, Chenfei Xia, Jun Zhao
- **Comment**: 9 pages,5 figures, 3 tables
- **Journal**: None
- **Summary**: Person re-identification (Re-ID) is a challenging task as persons are often in different backgrounds. Most recent Re-ID methods treat the foreground and background information equally for person discriminative learning, but can easily lead to potential false alarm problems when different persons are in similar backgrounds or the same person is in different backgrounds. In this paper, we propose a Foreground-Guided Texture-Focused Network (FTN) for Re-ID, which can weaken the representation of unrelated background and highlight the attributes person-related in an end-to-end manner. FTN consists of a semantic encoder (S-Enc) and a compact foreground attention module (CFA) for Re-ID task, and a texture-focused decoder (TF-Dec) for reconstruction task. Particularly, we build a foreground-guided semi-supervised learning strategy for TF-Dec because the reconstructed ground-truths are only the inputs of FTN weighted by the Gaussian mask and the attention mask generated by CFA. Moreover, a new gradient loss is introduced to encourage the network to mine the texture consistency between the inputs and the reconstructed outputs. Our FTN is computationally efficient and extensive experiments on three commonly used datasets Market1501, CUHK03 and MSMT17 demonstrate that the proposed method performs favorably against the state-of-the-art methods.



### Automatic identification of fossils and abiotic grains during carbonate microfacies analysis using deep convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/2009.11429v2
- **DOI**: 10.1016/j.sedgeo.2020.105790
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2009.11429v2)
- **Published**: 2020-09-24 00:58:48+00:00
- **Updated**: 2020-11-04 02:04:02+00:00
- **Authors**: Xiaokang Liu, Haijun Song
- **Comment**: This preprint has been accepted by Sedimentary Geology
- **Journal**: None
- **Summary**: Petrographic analysis based on microfacies identification in thin sections is widely used in sedimentary environment interpretation and paleoecological reconstruction. Fossil recognition from microfacies is an essential procedure for petrographers to complete this task. Distinguishing the morphological and microstructural diversity of skeletal fragments requires extensive prior knowledge of fossil morphotypes in microfacies and long training sessions under the microscope. This requirement engenders certain challenges for sedimentologists and paleontologists, especially novices. However, a machine classifier can help address this challenge. In this study, we collected a microfacies image dataset comprising both public data from 1,149 references and our own materials (including 30,815 images of 22 fossil and abiotic grain groups). We employed a high-performance workstation to implement four classic deep convolutional neural networks (DCNNs), which have proven to be highly efficient in computer vision over the last several years. Our framework uses a transfer learning technique, which reuses the pre-trained parameters that are trained on a larger ImageNet dataset as initialization for the network to achieve high accuracy with low computing costs. We obtained up to 95% of the top one and 99% of the top three test accuracies in the Inception ResNet v2 architecture. The machine classifier exhibited 0.99 precision on minerals, such as dolomite and pyrite. Although it had some difficulty on samples having similar morphologies, such as the bivalve, brachiopod, and ostracod, it nevertheless obtained 0.88 precision. Our machine learning framework demonstrated high accuracy with reproducibility and bias avoidance that was comparable to those of human classifiers. Its application can thus eliminate much of the tedious, manually intensive efforts by human experts conducting routine identification.



### Unifying data for fine-grained visual species classification
- **Arxiv ID**: http://arxiv.org/abs/2009.11433v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.11433v1)
- **Published**: 2020-09-24 01:04:18+00:00
- **Updated**: 2020-09-24 01:04:18+00:00
- **Authors**: Sayali Kulkarni, Tomer Gadot, Chen Luo, Tanya Birch, Eric Fegraus
- **Comment**: None
- **Journal**: None
- **Summary**: Wildlife monitoring is crucial to nature conservation and has been done by manual observations from motion-triggered camera traps deployed in the field. Widespread adoption of such in-situ sensors has resulted in unprecedented data volumes being collected over the last decade. A significant challenge exists to process and reliably identify what is in these images efficiently. Advances in computer vision are poised to provide effective solutions with custom AI models built to automatically identify images of interest and label the species in them. Here we outline the data unification effort for the Wildlife Insights platform from various conservation partners, and the challenges involved. Then we present an initial deep convolutional neural network model, trained on 2.9M images across 465 fine-grained species, with a goal to reduce the load on human experts to classify species in images manually. The long-term goal is to enable scientists to make conservation recommendations from near real-time analysis of species abundance and population health.



### 3D Object Localization Using 2D Estimates for Computer Vision Applications
- **Arxiv ID**: http://arxiv.org/abs/2009.11446v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2009.11446v2)
- **Published**: 2020-09-24 01:50:24+00:00
- **Updated**: 2021-08-21 09:37:25+00:00
- **Authors**: Taha Hasan Masood Siddique, Muhammad Usman
- **Comment**: Accepted for publication in MAJICC'21: Mohammad Ali Jinnah University
  Conference on Informatics and Computing 2021
- **Journal**: None
- **Summary**: A technique for object localization based on pose estimation and camera calibration is presented. The 3-dimensional (3D) coordinates are estimated by collecting multiple 2-dimensional (2D) images of the object and are utilized for the calibration of the camera. The calibration steps involving a number of parameter calculation including intrinsic and extrinsic parameters for the removal of lens distortion, computation of object's size and camera's position calculation are discussed. A transformation strategy to estimate the 3D pose using the 2D images is presented. The proposed method is implemented on MATLAB and validation experiments are carried out for both pose estimation and camera calibration.



### BWCFace: Open-set Face Recognition using Body-worn Camera
- **Arxiv ID**: http://arxiv.org/abs/2009.11458v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.11458v1)
- **Published**: 2020-09-24 02:45:29+00:00
- **Updated**: 2020-09-24 02:45:29+00:00
- **Authors**: Ali Almadan, Anoop Krishnan, Ajita Rattani
- **Comment**: None
- **Journal**: 19th IEEE International Conference On Machine Learning And
  Applications 2020 | Miami, Florida
- **Summary**: With computer vision reaching an inflection point in the past decade, face recognition technology has become pervasive in policing, intelligence gathering, and consumer applications. Recently, face recognition technology has been deployed on bodyworn cameras to keep officers safe, enabling situational awareness and providing evidence for trial. However, limited academic research has been conducted on this topic using traditional techniques on datasets with small sample size. This paper aims to bridge the gap in the state-of-the-art face recognition using bodyworn cameras (BWC). To this aim, the contribution of this work is two-fold: (1) collection of a dataset called BWCFace consisting of a total of 178K facial images of 132 subjects captured using the body-worn camera in in-door and daylight conditions, and (2) open-set evaluation of the latest deep-learning-based Convolutional Neural Network (CNN) architectures combined with five different loss functions for face identification, on the collected dataset. Experimental results on our BWCFace dataset suggest a maximum of 33.89% Rank-1 accuracy obtained when facial features are extracted using SENet-50 trained on a large scale VGGFace2 facial image dataset. However, performance improved up to a maximum of 99.00% Rank-1 accuracy when pretrained CNN models are fine-tuned on a subset of identities in our BWCFace dataset. Equivalent performances were obtained across body-worn camera sensor models used in existing face datasets. The collected BWCFace dataset and the pretrained/ fine-tuned algorithms are publicly available to promote further research and development in this area. A downloadable link of this dataset and the algorithms is available by contacting the authors.



### Understanding Fairness of Gender Classification Algorithms Across Gender-Race Groups
- **Arxiv ID**: http://arxiv.org/abs/2009.11491v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.11491v1)
- **Published**: 2020-09-24 04:56:10+00:00
- **Updated**: 2020-09-24 04:56:10+00:00
- **Authors**: Anoop Krishnan, Ali Almadan, Ajita Rattani
- **Comment**: 19th IEEE International Conference On Machine Learning And
  Applications 2020 | Miami, Florida
- **Journal**: None
- **Summary**: Automated gender classification has important applications in many domains, such as demographic research, law enforcement, online advertising, as well as human-computer interaction. Recent research has questioned the fairness of this technology across gender and race. Specifically, the majority of the studies raised the concern of higher error rates of the face-based gender classification system for darker-skinned people like African-American and for women. However, to date, the majority of existing studies were limited to African-American and Caucasian only. The aim of this paper is to investigate the differential performance of the gender classification algorithms across gender-race groups. To this aim, we investigate the impact of (a) architectural differences in the deep learning algorithms and (b) training set imbalance, as a potential source of bias causing differential performance across gender and race. Experimental investigations are conducted on two latest large-scale publicly available facial attribute datasets, namely, UTKFace and FairFace. The experimental results suggested that the algorithms with architectural differences varied in performance with consistency towards specific gender-race groups. For instance, for all the algorithms used, Black females (Black race in general) always obtained the least accuracy rates. Middle Eastern males and Latino females obtained higher accuracy rates most of the time. Training set imbalance further widens the gap in the unequal accuracy rates across all gender-race groups. Further investigations using facial landmarks suggested that facial morphological differences due to the bone structure influenced by genetic and environmental factors could be the cause of the least performance of Black females and Black race, in general.



### Adversarial Brain Multiplex Prediction From a Single Network for High-Order Connectional Gender-Specific Brain Mapping
- **Arxiv ID**: http://arxiv.org/abs/2009.11524v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.11524v1)
- **Published**: 2020-09-24 07:23:41+00:00
- **Updated**: 2020-09-24 07:23:41+00:00
- **Authors**: Ahmed Nebli, Islem Rekik
- **Comment**: None
- **Journal**: None
- **Summary**: Brain connectivity networks, derived from magnetic resonance imaging (MRI), non-invasively quantify the relationship in function, structure, and morphology between two brain regions of interest (ROIs) and give insights into gender-related connectional differences. However, to the best of our knowledge, studies on gender differences in brain connectivity were limited to investigating pairwise (i.e., low-order) relationship ROIs, overlooking the complex high-order interconnectedness of the brain as a network. To address this limitation, brain multiplexes have been introduced to model the relationship between at least two different brain networks. However, this inhibits their application to datasets with single brain networks such as functional networks. To fill this gap, we propose the first work on predicting brain multiplexes from a source network to investigate gender differences. Recently, generative adversarial networks (GANs) submerged the field of medical data synthesis. However, although conventional GANs work well on images, they cannot handle brain networks due to their non-Euclidean topological structure. Differently, in this paper, we tap into the nascent field of geometric-GANs (G-GAN) to design a deep multiplex prediction architecture comprising (i) a geometric source to target network translator mimicking a U-Net architecture with skip connections and (ii) a conditional discriminator which classifies predicted target intra-layers by conditioning on the multiplex source intra-layers. Such architecture simultaneously learns the latent source network representation and the deep non-linear mapping from the source to target multiplex intra-layers. Our experiments on a large dataset demonstrated that predicted multiplexes significantly boost gender classification accuracy compared with source networks and identifies both low and high-order gender-specific multiplex connections.



### MimicDet: Bridging the Gap Between One-Stage and Two-Stage Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2009.11528v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.11528v1)
- **Published**: 2020-09-24 07:36:58+00:00
- **Updated**: 2020-09-24 07:36:58+00:00
- **Authors**: Xin Lu, Quanquan Li, Buyu Li, Junjie Yan
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: Modern object detection methods can be divided into one-stage approaches and two-stage ones. One-stage detectors are more efficient owing to straightforward architectures, but the two-stage detectors still take the lead in accuracy. Although recent work try to improve the one-stage detectors by imitating the structural design of the two-stage ones, the accuracy gap is still significant. In this paper, we propose MimicDet, a novel and efficient framework to train a one-stage detector by directly mimic the two-stage features, aiming to bridge the accuracy gap between one-stage and two-stage detectors. Unlike conventional mimic methods, MimicDet has a shared backbone for one-stage and two-stage detectors, then it branches into two heads which are well designed to have compatible features for mimicking. Thus MimicDet can be end-to-end trained without the pre-train of the teacher network. And the cost does not increase much, which makes it practical to adopt large networks as backbones. We also make several specialized designs such as dual-path mimicking and staggered feature pyramid to facilitate the mimicking process. Experiments on the challenging COCO detection benchmark demonstrate the effectiveness of MimicDet. It achieves 46.1 mAP with ResNeXt-101 backbone on the COCO test-dev set, which significantly surpasses current state-of-the-art methods.



### Multi-Scale Profiling of Brain Multigraphs by Eigen-based Cross-Diffusion and Heat Tracing for Brain State Profiling
- **Arxiv ID**: http://arxiv.org/abs/2009.11534v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.11534v1)
- **Published**: 2020-09-24 07:51:44+00:00
- **Updated**: 2020-09-24 07:51:44+00:00
- **Authors**: Mustafa Saglam, Islem Rekik
- **Comment**: None
- **Journal**: None
- **Summary**: The individual brain can be viewed as a highly-complex multigraph (i.e. a set of graphs also called connectomes), where each graph represents a unique connectional view of pairwise brain region (node) relationships such as function or morphology. Due to its multifold complexity, understanding how brain disorders alter not only a single view of the brain graph, but its multigraph representation at the individual and population scales, remains one of the most challenging obstacles to profiling brain connectivity for ultimately disentangling a wide spectrum of brain states (e.g., healthy vs. disordered). In this work, while cross-pollinating the fields of spectral graph theory and diffusion models, we unprecedentedly propose an eigen-based cross-diffusion strategy for multigraph brain integration, comparison, and profiling. Specifically, we first devise a brain multigraph fusion model guided by eigenvector centrality to rely on most central nodes in the cross-diffusion process. Next, since the graph spectrum encodes its shape (or geometry) as if one can hear the shape of the graph, for the first time, we profile the fused multigraphs at several diffusion timescales by extracting the compact heat-trace signatures of their corresponding Laplacian matrices. Here, we reveal for the first time autistic and healthy profiles of morphological brain multigraphs, derived from T1-w magnetic resonance imaging (MRI), and demonstrate their discriminability in boosting the classification of unseen samples in comparison with state-of-the-art methods. This study presents the first step towards hearing the shape of the brain multigraph that can be leveraged for profiling and disentangling comorbid neurological disorders, thereby advancing precision medicine.



### Style-invariant Cardiac Image Segmentation with Test-time Augmentation
- **Arxiv ID**: http://arxiv.org/abs/2009.12193v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.12193v1)
- **Published**: 2020-09-24 08:27:40+00:00
- **Updated**: 2020-09-24 08:27:40+00:00
- **Authors**: Xiaoqiong Huang, Zejian Chen, Xin Yang, Zhendong Liu, Yuxin Zou, Mingyuan Luo, Wufeng Xue, Dong Ni
- **Comment**: Accepted by MICCAI STACOM 2020
- **Journal**: None
- **Summary**: Deep models often suffer from severe performance drop due to the appearance shift in the real clinical setting. Most of the existing learning-based methods rely on images from multiple sites/vendors or even corresponding labels. However, collecting enough unknown data to robustly model segmentation cannot always hold since the complex appearance shift caused by imaging factors in daily application. In this paper, we propose a novel style-invariant method for cardiac image segmentation. Based on the zero-shot style transfer to remove appearance shift and test-time augmentation to explore diverse underlying anatomy, our proposed method is effective in combating the appearance shift. Our contribution is three-fold. First, inspired by the spirit of universal style transfer, we develop a zero-shot stylization for content images to generate stylized images that appearance similarity to the style images. Second, we build up a robust cardiac segmentation model based on the U-Net structure. Our framework mainly consists of two networks during testing: the ST network for removing appearance shift and the segmentation network. Third, we investigate test-time augmentation to explore transformed versions of the stylized image for prediction and the results are merged. Notably, our proposed framework is fully test-time adaptation. Experiment results demonstrate that our methods are promising and generic for generalizing deep segmentation models.



### Residual Feature Distillation Network for Lightweight Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2009.11551v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.11551v1)
- **Published**: 2020-09-24 08:46:40+00:00
- **Updated**: 2020-09-24 08:46:40+00:00
- **Authors**: Jie Liu, Jie Tang, Gangshan Wu
- **Comment**: accepted by ECCV2020 AIM workshop
- **Journal**: None
- **Summary**: Recent advances in single image super-resolution (SISR) explored the power of convolutional neural network (CNN) to achieve a better performance. Despite the great success of CNN-based methods, it is not easy to apply these methods to edge devices due to the requirement of heavy computation. To solve this problem, various fast and lightweight CNN models have been proposed. The information distillation network is one of the state-of-the-art methods, which adopts the channel splitting operation to extract distilled features. However, it is not clear enough how this operation helps in the design of efficient SISR models. In this paper, we propose the feature distillation connection (FDC) that is functionally equivalent to the channel splitting operation while being more lightweight and flexible. Thanks to FDC, we can rethink the information multi-distillation network (IMDN) and propose a lightweight and accurate SISR model called residual feature distillation network (RFDN). RFDN uses multiple feature distillation connections to learn more discriminative feature representations. We also propose a shallow residual block (SRB) as the main building block of RFDN so that the network can benefit most from residual learning while still being lightweight enough. Extensive experimental results show that the proposed RFDN achieve a better trade-off against the state-of-the-art methods in terms of performance and model complexity. Moreover, we propose an enhanced RFDN (E-RFDN) and won the first place in the AIM 2020 efficient super-resolution challenge. Code will be available at https://github.com/njulj/RFDN.



### Multi-View Brain HyperConnectome AutoEncoder For Brain State Classification
- **Arxiv ID**: http://arxiv.org/abs/2009.11553v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.11553v1)
- **Published**: 2020-09-24 08:51:44+00:00
- **Updated**: 2020-09-24 08:51:44+00:00
- **Authors**: Alin Banka, Inis Buzi, Islem Rekik
- **Comment**: None
- **Journal**: None
- **Summary**: Graph embedding is a powerful method to represent graph neurological data (e.g., brain connectomes) in a low dimensional space for brain connectivity mapping, prediction and classification. However, existing embedding algorithms have two major limitations. First, they primarily focus on preserving one-to-one topological relationships between nodes (i.e., regions of interest (ROIs) in a connectome), but they have mostly ignored many-to-many relationships (i.e., set to set), which can be captured using a hyperconnectome structure. Second, existing graph embedding techniques cannot be easily adapted to multi-view graph data with heterogeneous distributions. In this paper, while cross-pollinating adversarial deep learning with hypergraph theory, we aim to jointly learn deep latent embeddings of subject0specific multi-view brain graphs to eventually disentangle different brain states. First, we propose a new simple strategy to build a hyperconnectome for each brain view based on nearest neighbour algorithm to preserve the connectivities across pairs of ROIs. Second, we design a hyperconnectome autoencoder (HCAE) framework which operates directly on the multi-view hyperconnectomes based on hypergraph convolutional layers to better capture the many-to-many relationships between brain regions (i.e., nodes). For each subject, we further regularize the hypergraph autoencoding by adversarial regularization to align the distribution of the learned hyperconnectome embeddings with that of the input hyperconnectomes. We formalize our hyperconnectome embedding within a geometric deep learning framework to optimize for a given subject, thereby designing an individual-based learning framework. Our experiments showed that the learned embeddings by HCAE yield to better results for brain state classification compared with other deep graph embedding methods methods.



### Local Context Attention for Salient Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2009.11562v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.11562v1)
- **Published**: 2020-09-24 09:20:06+00:00
- **Updated**: 2020-09-24 09:20:06+00:00
- **Authors**: Jing Tan, Pengfei Xiong, Yuwen He, Kuntao Xiao, Zhengyi Lv
- **Comment**: None
- **Journal**: None
- **Summary**: Salient object segmentation aims at distinguishing various salient objects from backgrounds. Despite the lack of semantic consistency, salient objects often have obvious texture and location characteristics in local area. Based on this priori, we propose a novel Local Context Attention Network (LCANet) to generate locally reinforcement feature maps in a uniform representational architecture. The proposed network introduces an Attentional Correlation Filter (ACF) module to generate explicit local attention by calculating the correlation feature map between coarse prediction and global context. Then it is expanded to a Local Context Block(LCB). Furthermore, an one-stage coarse-to-fine structure is implemented based on LCB to adaptively enhance the local context description ability. Comprehensive experiments are conducted on several salient object segmentation datasets, demonstrating the superior performance of the proposed LCANet against the state-of-the-art methods, especially with 0.883 max F-score and 0.034 MAE on DUTS-TE dataset.



### Cloud Cover Nowcasting with Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2009.11577v3
- **DOI**: 10.1109/IPTA50016.2020.9286606
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.11577v3)
- **Published**: 2020-09-24 09:57:29+00:00
- **Updated**: 2020-12-17 11:57:43+00:00
- **Authors**: Léa Berthomier, Bruno Pradel, Lior Perez
- **Comment**: 6 pages, 11 figures
- **Journal**: Proceedings of the 2020 Tenth International Conference on Image
  Processing Theory, Tools and Applications (IPTA), IEEE, Paris, France, 9-12
  November 2020
- **Summary**: Nowcasting is a field of meteorology which aims at forecasting weather on a short term of up to a few hours. In the meteorology landscape, this field is rather specific as it requires particular techniques, such as data extrapolation, where conventional meteorology is generally based on physical modeling. In this paper, we focus on cloud cover nowcasting, which has various application areas such as satellite shots optimisation and photovoltaic energy production forecast.   Following recent deep learning successes on multiple imagery tasks, we applied deep convolutionnal neural networks on Meteosat satellite images for cloud cover nowcasting. We present the results of several architectures specialized in image segmentation and time series prediction. We selected the best models according to machine learning metrics as well as meteorological metrics. All selected architectures showed significant improvements over persistence and the well-known U-Net surpasses AROME physical model.



### SoRC -- Evaluation of Computational Molecular Co-Localization Analysis in Mass Spectrometry Images
- **Arxiv ID**: http://arxiv.org/abs/2009.14677v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, q-bio.QM, stat.AP, stat.CO
- **Links**: [PDF](http://arxiv.org/pdf/2009.14677v1)
- **Published**: 2020-09-24 10:24:41+00:00
- **Updated**: 2020-09-24 10:24:41+00:00
- **Authors**: Karsten Wüllems, Tim W. Nattkemper
- **Comment**: None
- **Journal**: None
- **Summary**: The computational analysis of Mass Spectrometry Imaging (MSI) data aims at the identification of interesting mass co-localizations and the visualization of their lateral distribution in the sample, usually a tissue cross section. But as the morphological structure of tissues and the different kinds of mass co-localization naturally show a huge diversity, the selection and tuning of the computational method is a time-consuming effort. In this work we address the special problem of computationally grouping mass channel images according to their similarities in their lateral distribution patterns. Such an analysis is driven by the idea, that groups of molecules that feature a similar distribution pattern may have a functional relation. But the selection of the similarity function and other parameters is often done by a time-consuming and unsatsifactory trial and error. We propose a new flexible workflow scheme called SoRC (sum of ranked cluster indices) for automating this tuning step and making it much more efficient. We test SoRC using three different data sets acquired from the lab for three different kinds of samples (barley seed, mouse bladder tissue, human PXE skin). We show, that SORC can be applied to score and visualize the results obtained with the applied methods in short time without too much effort. In our application example, the SoRC results for the three data sets reveal that a) some well-known similarity functions are suited to achieve good results for all three data sets and b) for the MSI data featuring a higher degree of irregularity improved results can be achieved by applying non-standard similarity functions. The SoRC scores computed with our approach indicate that an automated testing and scoring of different methods for mass channel image grouping can improve the final outcome of a study by finally selecting the methods of the highest scores.



### Brain Tumor Segmentation using 3D-CNNs with Uncertainty Estimation
- **Arxiv ID**: http://arxiv.org/abs/2009.12188v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.12188v1)
- **Published**: 2020-09-24 10:50:12+00:00
- **Updated**: 2020-09-24 10:50:12+00:00
- **Authors**: Laura Mora Ballestar, Veronica Vilaplana
- **Comment**: Pre-conference paper. Brain Tumor Segmentation (BraTS) Challenge 2020
- **Journal**: None
- **Summary**: Automation of brain tumors in 3D magnetic resonance images (MRIs) is key to assess the diagnostic and treatment of the disease. In recent years, convolutional neural networks (CNNs) have shown improved results in the task. However, high memory consumption is still a problem in 3D-CNNs. Moreover, most methods do not include uncertainty information, which is specially critical in medical diagnosis. This work proposes a 3D encoder-decoder architecture, based on V-Net \cite{vnet} which is trained with patching techniques to reduce memory consumption and decrease the effect of unbalanced data. We also introduce voxel-wise uncertainty, both epistemic and aleatoric using test-time dropout and data-augmentation respectively. Uncertainty maps can provide extra information to expert neurologists, useful for detecting when the model is not confident on the provided segmentation.



### Interpreting and Boosting Dropout from a Game-Theoretic View
- **Arxiv ID**: http://arxiv.org/abs/2009.11729v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2009.11729v4)
- **Published**: 2020-09-24 14:39:42+00:00
- **Updated**: 2021-03-16 10:42:04+00:00
- **Authors**: Hao Zhang, Sen Li, Yinchao Ma, Mingjie Li, Yichen Xie, Quanshi Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: This paper aims to understand and improve the utility of the dropout operation from the perspective of game-theoretic interactions. We prove that dropout can suppress the strength of interactions between input variables of deep neural networks (DNNs). The theoretic proof is also verified by various experiments. Furthermore, we find that such interactions were strongly related to the over-fitting problem in deep learning. Thus, the utility of dropout can be regarded as decreasing interactions to alleviate the significance of over-fitting. Based on this understanding, we propose an interaction loss to further improve the utility of dropout. Experimental results have shown that the interaction loss can effectively improve the utility of dropout and boost the performance of DNNs.



### Learning Graph Normalization for Graph Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2009.11746v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.11746v1)
- **Published**: 2020-09-24 15:16:43+00:00
- **Updated**: 2020-09-24 15:16:43+00:00
- **Authors**: Yihao Chen, Xin Tang, Xianbiao Qi, Chun-Guang Li, Rong Xiao
- **Comment**: 15 pages, 3 figures, 6 tables
- **Journal**: None
- **Summary**: Graph Neural Networks (GNNs) have attracted considerable attention and have emerged as a new promising paradigm to process graph-structured data. GNNs are usually stacked to multiple layers and the node representations in each layer are computed through propagating and aggregating the neighboring node features with respect to the graph. By stacking to multiple layers, GNNs are able to capture the long-range dependencies among the data on the graph and thus bring performance improvements. To train a GNN with multiple layers effectively, some normalization techniques (e.g., node-wise normalization, batch-wise normalization) are necessary. However, the normalization techniques for GNNs are highly task-relevant and different application tasks prefer to different normalization techniques, which is hard to know in advance. To tackle this deficiency, in this paper, we propose to learn graph normalization by optimizing a weighted combination of normalization techniques at four different levels, including node-wise normalization, adjacency-wise normalization, graph-wise normalization, and batch-wise normalization, in which the adjacency-wise normalization and the graph-wise normalization are newly proposed in this paper to take into account the local structure and the global structure on the graph, respectively. By learning the optimal weights, we are able to automatically select a single best or a best combination of multiple normalizations for a specific task. We conduct extensive experiments on benchmark datasets for different tasks, including node classification, link prediction, graph classification and graph regression, and confirm that the learned graph normalization leads to competitive results and that the learned weights suggest the appropriate normalization techniques for the specific task. Source code is released here https://github.com/cyh1112/GraphNormalization.



### Attribute Propagation Network for Graph Zero-shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2009.11816v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.11816v1)
- **Published**: 2020-09-24 16:53:40+00:00
- **Updated**: 2020-09-24 16:53:40+00:00
- **Authors**: Lu Liu, Tianyi Zhou, Guodong Long, Jing Jiang, Chengqi Zhang
- **Comment**: AAAI 2020
- **Journal**: None
- **Summary**: The goal of zero-shot learning (ZSL) is to train a model to classify samples of classes that were not seen during training. To address this challenging task, most ZSL methods relate unseen test classes to seen(training) classes via a pre-defined set of attributes that can describe all classes in the same semantic space, so the knowledge learned on the training classes can be adapted to unseen classes. In this paper, we aim to optimize the attribute space for ZSL by training a propagation mechanism to refine the semantic attributes of each class based on its neighbors and related classes on a graph of classes. We show that the propagated attributes can produce classifiers for zero-shot classes with significantly improved performance in different ZSL settings. The graph of classes is usually free or very cheap to acquire such as WordNet or ImageNet classes. When the graph is not provided, given pre-defined semantic embeddings of the classes, we can learn a mechanism to generate the graph in an end-to-end manner along with the propagation mechanism. However, this graph-aided technique has not been well-explored in the literature. In this paper, we introduce the attribute propagation network (APNet), which is composed of 1) a graph propagation model generating attribute vector for each class and 2) a parameterized nearest neighbor (NN) classifier categorizing an image to the class with the nearest attribute vector to the image's embedding. For better generalization over unseen classes, different from previous methods, we adopt a meta-learning strategy to train the propagation mechanism and the similarity metric for the NN classifier on multiple sub-graphs, each associated with a classification task over a subset of training classes. In experiments with two zero-shot learning settings and five benchmark datasets, APNet achieves either compelling performance or new state-of-the-art results.



### A Gradient Flow Framework For Analyzing Network Pruning
- **Arxiv ID**: http://arxiv.org/abs/2009.11839v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2009.11839v4)
- **Published**: 2020-09-24 17:37:32+00:00
- **Updated**: 2021-09-23 07:47:56+00:00
- **Authors**: Ekdeep Singh Lubana, Robert P. Dick
- **Comment**: Accepted at ICLR, 2021
- **Journal**: None
- **Summary**: Recent network pruning methods focus on pruning models early-on in training. To estimate the impact of removing a parameter, these methods use importance measures that were originally designed to prune trained models. Despite lacking justification for their use early-on in training, such measures result in surprisingly low accuracy loss. To better explain this behavior, we develop a general framework that uses gradient flow to unify state-of-the-art importance measures through the norm of model parameters. We use this framework to determine the relationship between pruning measures and evolution of model parameters, establishing several results related to pruning models early-on in training: (i) magnitude-based pruning removes parameters that contribute least to reduction in loss, resulting in models that converge faster than magnitude-agnostic methods; (ii) loss-preservation based pruning preserves first-order model evolution dynamics and is therefore appropriate for pruning minimally trained models; and (iii) gradient-norm based pruning affects second-order model evolution dynamics, such that increasing gradient norm via pruning can produce poorly performing models. We validate our claims on several VGG-13, MobileNet-V1, and ResNet-56 models trained on CIFAR-10/CIFAR-100. Code available at https://github.com/EkdeepSLubana/flowandprune.



### How Neural Networks Extrapolate: From Feedforward to Graph Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2009.11848v5
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2009.11848v5)
- **Published**: 2020-09-24 17:48:59+00:00
- **Updated**: 2021-03-02 23:05:49+00:00
- **Authors**: Keyulu Xu, Mozhi Zhang, Jingling Li, Simon S. Du, Ken-ichi Kawarabayashi, Stefanie Jegelka
- **Comment**: None
- **Journal**: None
- **Summary**: We study how neural networks trained by gradient descent extrapolate, i.e., what they learn outside the support of the training distribution. Previous works report mixed empirical results when extrapolating with neural networks: while feedforward neural networks, a.k.a. multilayer perceptrons (MLPs), do not extrapolate well in certain simple tasks, Graph Neural Networks (GNNs) -- structured networks with MLP modules -- have shown some success in more complex tasks. Working towards a theoretical explanation, we identify conditions under which MLPs and GNNs extrapolate well. First, we quantify the observation that ReLU MLPs quickly converge to linear functions along any direction from the origin, which implies that ReLU MLPs do not extrapolate most nonlinear functions. But, they can provably learn a linear target function when the training distribution is sufficiently "diverse". Second, in connection to analyzing the successes and limitations of GNNs, these results suggest a hypothesis for which we provide theoretical and empirical evidence: the success of GNNs in extrapolating algorithmic tasks to new data (e.g., larger graphs or edge weights) relies on encoding task-specific non-linearities in the architecture or features. Our theoretical analysis builds on a connection of over-parameterized networks to the neural tangent kernel. Empirically, our theory holds across different training settings.



### ECOVNet: An Ensemble of Deep Convolutional Neural Networks Based on EfficientNet to Detect COVID-19 From Chest X-rays
- **Arxiv ID**: http://arxiv.org/abs/2009.11850v2
- **DOI**: 10.7717/peerj-cs.551
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.11850v2)
- **Published**: 2020-09-24 17:53:17+00:00
- **Updated**: 2020-10-16 01:24:48+00:00
- **Authors**: Nihad Karim Chowdhury, Muhammad Ashad Kabir, Md. Muhtadir Rahman, Noortaz Rezoana
- **Comment**: None
- **Journal**: Peer J Computer Science, 2021
- **Summary**: This paper proposed an ensemble of deep convolutional neural networks (CNN) based on EfficientNet, named ECOVNet, to detect COVID-19 using a large chest X-ray data set. At first, the open-access large chest X-ray collection is augmented, and then ImageNet pre-trained weights for EfficientNet is transferred with some customized fine-tuning top layers that are trained, followed by an ensemble of model snapshots to classify chest X-rays corresponding to COVID-19, normal, and pneumonia. The predictions of the model snapshots, which are created during a single training, are combined through two ensemble strategies, i.e., hard ensemble and soft ensemble to ameliorate classification performance and generalization in the related task of classifying chest X-rays.



### Multi-Frame to Single-Frame: Knowledge Distillation for 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2009.11859v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.11859v1)
- **Published**: 2020-09-24 17:59:12+00:00
- **Updated**: 2020-09-24 17:59:12+00:00
- **Authors**: Yue Wang, Alireza Fathi, Jiajun Wu, Thomas Funkhouser, Justin Solomon
- **Comment**: The Workshop on Perception for Autonomous Driving at ECCV2020
- **Journal**: None
- **Summary**: A common dilemma in 3D object detection for autonomous driving is that high-quality, dense point clouds are only available during training, but not testing. We use knowledge distillation to bridge the gap between a model trained on high-quality inputs at training time and another tested on low-quality inputs at inference time. In particular, we design a two-stage training pipeline for point cloud object detection. First, we train an object detection model on dense point clouds, which are generated from multiple frames using extra information only available at training time. Then, we train the model's identical counterpart on sparse single-frame point clouds with consistency regularization on features from both models. We show that this procedure improves performance on low-quality data during testing, without additional overhead.



### PK-GCN: Prior Knowledge Assisted Image Classification using Graph Convolution Networks
- **Arxiv ID**: http://arxiv.org/abs/2009.11892v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2009.11892v1)
- **Published**: 2020-09-24 18:31:35+00:00
- **Updated**: 2020-09-24 18:31:35+00:00
- **Authors**: Xueli Xiao, Chunyan Ji, Thosini Bamunu Mudiyanselage, Yi Pan
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning has gained great success in various classification tasks. Typically, deep learning models learn underlying features directly from data, and no underlying relationship between classes are included. Similarity between classes can influence the performance of classification. In this article, we propose a method that incorporates class similarity knowledge into convolutional neural networks models using a graph convolution layer. We evaluate our method on two benchmark image datasets: MNIST and CIFAR10, and analyze the results on different data and model sizes. Experimental results show that our model can improve classification accuracy, especially when the amount of available data is small.



### Exposing GAN-generated Faces Using Inconsistent Corneal Specular Highlights
- **Arxiv ID**: http://arxiv.org/abs/2009.11924v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.11924v2)
- **Published**: 2020-09-24 19:43:16+00:00
- **Updated**: 2020-10-12 19:28:14+00:00
- **Authors**: Shu Hu, Yuezun Li, Siwei Lyu
- **Comment**: None
- **Journal**: None
- **Summary**: Sophisticated generative adversary network (GAN) models are now able to synthesize highly realistic human faces that are difficult to discern from real ones visually. In this work, we show that GAN synthesized faces can be exposed with the inconsistent corneal specular highlights between two eyes. The inconsistency is caused by the lack of physical/physiological constraints in the GAN models. We show that such artifacts exist widely in high-quality GAN synthesized faces and further describe an automatic method to extract and compare corneal specular highlights from two eyes. Qualitative and quantitative evaluations of our method suggest its simplicity and effectiveness in distinguishing GAN synthesized faces.



### Image-Based Sorghum Head Counting When You Only Look Once
- **Arxiv ID**: http://arxiv.org/abs/2009.11929v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.11929v3)
- **Published**: 2020-09-24 19:50:08+00:00
- **Updated**: 2022-06-13 15:47:28+00:00
- **Authors**: Lawrence Mosley, Hieu Pham, Yogesh Bansal, Eric Hare
- **Comment**: None
- **Journal**: None
- **Summary**: Modern trends in digital agriculture have seen a shift towards artificial intelligence for crop quality assessment and yield estimation. In this work, we document how a parameter tuned single-shot object detection algorithm can be used to identify and count sorghum head from aerial drone images. Our approach involves a novel exploratory analysis that identified key structural elements of the sorghum images and motivated the selection of parameter-tuned anchor boxes that contributed significantly to performance. These insights led to the development of a deep learning model that outperformed the baseline model and achieved an out-of-sample mean average precision of 0.95.



### A Computer Vision Approach to Combat Lyme Disease
- **Arxiv ID**: http://arxiv.org/abs/2009.11931v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2009.11931v1)
- **Published**: 2020-09-24 20:00:02+00:00
- **Updated**: 2020-09-24 20:00:02+00:00
- **Authors**: Sina Akbarian, Tania Cawston, Laurent Moreno, Samir Patel, Vanessa Allen, Elham Dolatabadi
- **Comment**: Under review
- **Journal**: None
- **Summary**: Lyme disease is an infectious disease transmitted to humans by a bite from an infected Ixodes species (blacklegged ticks). It is one of the fastest growing vector-borne illness in North America and is expanding its geographic footprint. Lyme disease treatment is time-sensitive, and can be cured by administering an antibiotic (prophylaxis) to the patient within 72 hours after a tick bite by the Ixodes species. However, the laboratory-based identification of each tick that might carry the bacteria is time-consuming and labour intensive and cannot meet the maximum turn-around-time of 72 hours for an effective treatment. Early identification of blacklegged ticks using computer vision technologies is a potential solution in promptly identifying a tick and administering prophylaxis within a crucial window period. In this work, we build an automated detection tool that can differentiate blacklegged ticks from other ticks species using advanced deep learning and computer vision approaches. We demonstrate the classification of tick species using Convolution Neural Network (CNN) models, trained end-to-end from tick images directly. Advanced knowledge transfer techniques within teacher-student learning frameworks are adopted to improve the performance of classification of tick species. Our best CNN model achieves 92% accuracy on test set. The tool can be integrated with the geography of exposure to determine the risk of Lyme disease infection and need for prophylaxis treatment.



### daVinciNet: Joint Prediction of Motion and Surgical State in Robot-Assisted Surgery
- **Arxiv ID**: http://arxiv.org/abs/2009.11937v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2009.11937v1)
- **Published**: 2020-09-24 20:28:06+00:00
- **Updated**: 2020-09-24 20:28:06+00:00
- **Authors**: Yidan Qin, Seyedshams Feyzabadi, Max Allan, Joel W. Burdick, Mahdi Azizian
- **Comment**: Accepted to IROS 2020
- **Journal**: None
- **Summary**: This paper presents a technique to concurrently and jointly predict the future trajectories of surgical instruments and the future state(s) of surgical subtasks in robot-assisted surgeries (RAS) using multiple input sources. Such predictions are a necessary first step towards shared control and supervised autonomy of surgical subtasks. Minute-long surgical subtasks, such as suturing or ultrasound scanning, often have distinguishable tool kinematics and visual features, and can be described as a series of fine-grained states with transition schematics. We propose daVinciNet - an end-to-end dual-task model for robot motion and surgical state predictions. daVinciNet performs concurrent end-effector trajectory and surgical state predictions using features extracted from multiple data streams, including robot kinematics, endoscopic vision, and system events. We evaluate our proposed model on an extended Robotic Intra-Operative Ultrasound (RIOUS+) imaging dataset collected on a da Vinci Xi surgical system and the JHU-ISI Gesture and Skill Assessment Working Set (JIGSAWS). Our model achieves up to 93.85% short-term (0.5s) and 82.11% long-term (2s) state prediction accuracy, as well as 1.07mm short-term and 5.62mm long-term trajectory prediction error.



### Characterization of Covid-19 Dataset using Complex Networks and Image Processing
- **Arxiv ID**: http://arxiv.org/abs/2009.13302v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/2009.13302v1)
- **Published**: 2020-09-24 20:35:31+00:00
- **Updated**: 2020-09-24 20:35:31+00:00
- **Authors**: Josimar Chire, Esteban Wilfredo Vilca Zuniga
- **Comment**: None
- **Journal**: None
- **Summary**: This paper aims to explore the structure of pattern behind covid-19 dataset. The dataset includes medical images with positive and negative cases. A sample of 100 sample is chosen, 50 per each class. An histogram frequency is calculated to get features using statistical measurements, besides a feature extraction using Grey Level Co-Occurrence Matrix (GLCM). Using both features are build Complex Networks respectively to analyze the adjacency matrices and check the presence of patterns. Initial experiments introduces the evidence of hidden patterns in the dataset for each class, which are visible using Complex Networks representation.



### Deep Multi-Scale Feature Learning for Defocus Blur Estimation
- **Arxiv ID**: http://arxiv.org/abs/2009.11939v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2009.11939v2)
- **Published**: 2020-09-24 20:36:40+00:00
- **Updated**: 2021-11-07 17:29:16+00:00
- **Authors**: Ali Karaali, Naomi Harte, Claudio Rosito Jung
- **Comment**: under review
- **Journal**: None
- **Summary**: This paper presents an edge-based defocus blur estimation method from a single defocused image. We first distinguish edges that lie at depth discontinuities (called depth edges, for which the blur estimate is ambiguous) from edges that lie at approximately constant depth regions (called pattern edges, for which the blur estimate is well-defined). Then, we estimate the defocus blur amount at pattern edges only, and explore an interpolation scheme based on guided filters that prevents data propagation across the detected depth edges to obtain a dense blur map with well-defined object boundaries. Both tasks (edge classification and blur estimation) are performed by deep convolutional neural networks (CNNs) that share weights to learn meaningful local features from multi-scale patches centered at edge locations. Experiments on naturally defocused images show that the proposed method presents qualitative and quantitative results that outperform state-of-the-art (SOTA) methods, with a good compromise between running time and accuracy.



### Multidimensional TV-Stokes for image processing
- **Arxiv ID**: http://arxiv.org/abs/2009.11971v2
- **DOI**: None
- **Categories**: **math.NA**, cs.CV, cs.NA, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/2009.11971v2)
- **Published**: 2020-09-24 22:21:27+00:00
- **Updated**: 2020-09-28 21:16:30+00:00
- **Authors**: Bin Wu, Xue-Cheng Tai, Talal Rahman
- **Comment**: None
- **Journal**: None
- **Summary**: A complete multidimential TV-Stokes model is proposed based on smoothing a gradient field in the first step and reconstruction of the multidimensional image from the gradient field. It is the correct extension of the original two dimensional TV-Stokes to multidimensions. Numerical algorithm using the Chambolle's semi-implicit dual formula is proposed. Numerical results applied to denoising 3D images and movies are presented. They show excellent performance in avoiding the staircase effect, and preserving fine structures.



### Alternating minimization for a single step TV-Stokes model for image denoising
- **Arxiv ID**: http://arxiv.org/abs/2009.11973v2
- **DOI**: None
- **Categories**: **math.NA**, cs.CV, cs.NA, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/2009.11973v2)
- **Published**: 2020-09-24 22:31:15+00:00
- **Updated**: 2020-09-29 13:07:36+00:00
- **Authors**: Bin Wu, Xue-Cheng Tai, Talal Rahman
- **Comment**: None
- **Journal**: None
- **Summary**: The paper presents a fully coupled TV-Stokes model, and propose an algorithm based on alternating minimization of the objective functional whose first iteration is exactly the modified TV-Stokes model proposed earlier. The model is a generalization of the second order Total Generalized Variation model. A convergence analysis is given.



### CoFF: Cooperative Spatial Feature Fusion for 3D Object Detection on Autonomous Vehicles
- **Arxiv ID**: http://arxiv.org/abs/2009.11975v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2009.11975v1)
- **Published**: 2020-09-24 22:51:50+00:00
- **Updated**: 2020-09-24 22:51:50+00:00
- **Authors**: Jingda Guo, Dominic Carrillo, Sihai Tang, Qi Chen, Qing Yang, Song Fu, Xi Wang, Nannan Wang, Paparao Palacharla
- **Comment**: None
- **Journal**: None
- **Summary**: To reduce the amount of transmitted data, feature map based fusion is recently proposed as a practical solution to cooperative 3D object detection by autonomous vehicles. The precision of object detection, however, may require significant improvement, especially for objects that are far away or occluded. To address this critical issue for the safety of autonomous vehicles and human beings, we propose a cooperative spatial feature fusion (CoFF) method for autonomous vehicles to effectively fuse feature maps for achieving a higher 3D object detection performance. Specially, CoFF differentiates weights among feature maps for a more guided fusion, based on how much new semantic information is provided by the received feature maps. It also enhances the inconspicuous features corresponding to far/occluded objects to improve their detection precision. Experimental results show that CoFF achieves a significant improvement in terms of both detection precision and effective detection range for autonomous vehicles, compared to previous feature fusion solutions.



### Iterative regularization algorithms for image denoising with the TV-Stokes model
- **Arxiv ID**: http://arxiv.org/abs/2009.11976v1
- **DOI**: None
- **Categories**: **math.NA**, cs.CV, cs.NA, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/2009.11976v1)
- **Published**: 2020-09-24 22:55:18+00:00
- **Updated**: 2020-09-24 22:55:18+00:00
- **Authors**: Bin Wu, Leszek Marcinkowski, Xue-Cheng Tai, Talal Rahman
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a set of iterative regularization algorithms for the TV-Stokes model to restore images from noisy images with Gaussian noise. These are some extensions of the iterative regularization algorithm proposed for the classical Rudin-Osher-Fatemi (ROF) model for image reconstruction, a single step model involving a scalar field smoothing, to the TV-Stokes model for image reconstruction, a two steps model involving a vector field smoothing in the first and a scalar field smoothing in the second. The iterative regularization algorithms proposed here are Richardson's iteration like. We have experimental results that show improvement over the original method in the quality of the restored image. Convergence analysis and numerical experiments are presented.



### An original framework for Wheat Head Detection using Deep, Semi-supervised and Ensemble Learning within Global Wheat Head Detection (GWHD) Dataset
- **Arxiv ID**: http://arxiv.org/abs/2009.11977v1
- **DOI**: 10.1080/07038992.2021.1906213
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2009.11977v1)
- **Published**: 2020-09-24 22:58:40+00:00
- **Updated**: 2020-09-24 22:58:40+00:00
- **Authors**: Fares Fourati, Wided Souidene, Rabah Attia
- **Comment**: Canadian Journal of Remote Sensing (2021)
- **Journal**: None
- **Summary**: In this paper, we propose an original object detection methodology applied to Global Wheat Head Detection (GWHD) Dataset. We have been through two major architectures of object detection which are FasterRCNN and EfficientDet, in order to design a novel and robust wheat head detection model. We emphasize on optimizing the performance of our proposed final architectures. Furthermore, we have been through an extensive exploratory data analysis and adapted best data augmentation techniques to our context. We use semi supervised learning to boost previous supervised models of object detection. Moreover, we put much effort on ensemble to achieve higher performance. Finally we use specific post-processing techniques to optimize our wheat head detection results. Our results have been submitted to solve a research challenge launched on the GWHD Dataset which is led by nine research institutes from seven countries. Our proposed method was ranked within the top 6% in the above mentioned challenge.



