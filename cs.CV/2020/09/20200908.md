# Arxiv Papers in cs.CV on 2020-09-08
### LaSOT: A High-quality Large-scale Single Object Tracking Benchmark
- **Arxiv ID**: http://arxiv.org/abs/2009.03465v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.03465v3)
- **Published**: 2020-09-08 00:31:56+00:00
- **Updated**: 2020-09-12 03:53:45+00:00
- **Authors**: Heng Fan, Hexin Bai, Liting Lin, Fan Yang, Peng Chu, Ge Deng, Sijia Yu, Harshit, Mingzhen Huang, Juehuan Liu, Yong Xu, Chunyuan Liao, Lin Yuan, Haibin Ling
- **Comment**: Tech Report. Update project website
- **Journal**: None
- **Summary**: Despite great recent advances in visual tracking, its further development, including both algorithm design and evaluation, is limited due to lack of dedicated large-scale benchmarks. To address this problem, we present LaSOT, a high-quality Large-scale Single Object Tracking benchmark. LaSOT contains a diverse selection of 85 object classes, and offers 1,550 totaling more than 3.87 million frames. Each video frame is carefully and manually annotated with a bounding box. This makes LaSOT, to our knowledge, the largest densely annotated tracking benchmark. Our goal in releasing LaSOT is to provide a dedicated high quality platform for both training and evaluation of trackers. The average video length of LaSOT is around 2,500 frames, where each video contains various challenge factors that exist in real world video footage,such as the targets disappearing and re-appearing. These longer video lengths allow for the assessment of long-term trackers. To take advantage of the close connection between visual appearance and natural language, we provide language specification for each video in LaSOT. We believe such additions will allow for future research to use linguistic features to improve tracking. Two protocols, full-overlap and one-shot, are designated for flexible assessment of trackers. We extensively evaluate 48 baseline trackers on LaSOT with in-depth analysis, and results reveal that there still exists significant room for improvement. The complete benchmark, tracking results as well as analysis are available at http://vision.cs.stonybrook.edu/~lasot/.



### A Deep Neural Network Tool for Automatic Segmentation of Human Body Parts in Natural Scenes
- **Arxiv ID**: http://arxiv.org/abs/2009.09900v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2009.09900v1)
- **Published**: 2020-09-08 01:20:50+00:00
- **Updated**: 2020-09-08 01:20:50+00:00
- **Authors**: Patrick McClure, Gabrielle Reimann, Michal Ramot, Francisco Pereira
- **Comment**: None
- **Journal**: None
- **Summary**: This short article describes a deep neural network trained to perform automatic segmentation of human body parts in natural scenes. More specifically, we trained a Bayesian SegNet with concrete dropout on the Pascal-Parts dataset to predict whether each pixel in a given frame was part of a person's hair, head, ear, eyebrows, legs, arms, mouth, neck, nose, or torso.



### A Residual Solver and Its Unfolding Neural Network for Total Variation Regularized Models
- **Arxiv ID**: http://arxiv.org/abs/2009.03477v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/2009.03477v1)
- **Published**: 2020-09-08 01:44:34+00:00
- **Updated**: 2020-09-08 01:44:34+00:00
- **Authors**: Yuanhao Gong
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes to solve the Total Variation regularized models by finding the residual between the input and the unknown optimal solution. After analyzing a previous method, we developed a new iterative algorithm, named as Residual Solver, which implicitly solves the model in gradient domain. We theoretically prove the uniqueness of the gradient field in our algorithm. We further numerically confirm that the residual solver can reach the same global optimal solutions as the classical method on 500 natural images. Moreover, we unfold our iterative algorithm into a convolution neural network (named as Residual Solver Network). This network is unsupervised and can be considered as an "enhanced version" of our iterative algorithm. Finally, both the proposed algorithm and neural network are successfully applied on several problems to demonstrate their effectiveness and efficiency, including image smoothing, denoising, and biomedical image reconstruction. The proposed network is general and can be applied to solve other total variation regularized models.



### CNNPruner: Pruning Convolutional Neural Networks with Visual Analytics
- **Arxiv ID**: http://arxiv.org/abs/2009.09940v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2009.09940v1)
- **Published**: 2020-09-08 02:08:20+00:00
- **Updated**: 2020-09-08 02:08:20+00:00
- **Authors**: Guan Li, Junpeng Wang, Han-Wei Shen, Kaixin Chen, Guihua Shan, Zhonghua Lu
- **Comment**: 10 pages,15 figures, Accepted for presentation at IEEE VIS 2020
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) have demonstrated extraordinarily good performance in many computer vision tasks. The increasing size of CNN models, however, prevents them from being widely deployed to devices with limited computational resources, e.g., mobile/embedded devices. The emerging topic of model pruning strives to address this problem by removing less important neurons and fine-tuning the pruned networks to minimize the accuracy loss. Nevertheless, existing automated pruning solutions often rely on a numerical threshold of the pruning criteria, lacking the flexibility to optimally balance the trade-off between model size and accuracy. Moreover, the complicated interplay between the stages of neuron pruning and model fine-tuning makes this process opaque, and therefore becomes difficult to optimize. In this paper, we address these challenges through a visual analytics approach, named CNNPruner. It considers the importance of convolutional filters through both instability and sensitivity, and allows users to interactively create pruning plans according to a desired goal on model size or accuracy. Also, CNNPruner integrates state-of-the-art filter visualization techniques to help users understand the roles that different filters played and refine their pruning plans. Through comprehensive case studies on CNNs with real-world sizes, we validate the effectiveness of CNNPruner.



### Few-Shot Hyperspectral Image Classification With Unknown Classes Using Multitask Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2009.03508v1
- **DOI**: 10.1109/TGRS.2020.3018879
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.03508v1)
- **Published**: 2020-09-08 03:53:10+00:00
- **Updated**: 2020-09-08 03:53:10+00:00
- **Authors**: Shengjie Liu, Qian Shi, Liangpei Zhang
- **Comment**: Accepted by IEEE TGRS
- **Journal**: None
- **Summary**: Current hyperspectral image classification assumes that a predefined classification system is closed and complete, and there are no unknown or novel classes in the unseen data. However, this assumption may be too strict for the real world. Often, novel classes are overlooked when the classification system is constructed. The closed nature forces a model to assign a label given a new sample and may lead to overestimation of known land covers (e.g., crop area). To tackle this issue, we propose a multitask deep learning method that simultaneously conducts classification and reconstruction in the open world (named MDL4OW) where unknown classes may exist. The reconstructed data are compared with the original data; those failing to be reconstructed are considered unknown, based on the assumption that they are not well represented in the latent features due to the lack of labels. A threshold needs to be defined to separate the unknown and known classes; we propose two strategies based on the extreme value theory for few-shot and many-shot scenarios. The proposed method was tested on real-world hyperspectral images; state-of-the-art results were achieved, e.g., improving the overall accuracy by 4.94% for the Salinas data. By considering the existence of unknown classes in the open world, our method achieved more accurate hyperspectral image classification, especially under the few-shot context.



### Region Comparison Network for Interpretable Few-shot Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2009.03558v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2009.03558v1)
- **Published**: 2020-09-08 07:29:05+00:00
- **Updated**: 2020-09-08 07:29:05+00:00
- **Authors**: Zhiyu Xue, Lixin Duan, Wen Li, Lin Chen, Jiebo Luo
- **Comment**: None
- **Journal**: None
- **Summary**: While deep learning has been successfully applied to many real-world computer vision tasks, training robust classifiers usually requires a large amount of well-labeled data. However, the annotation is often expensive and time-consuming. Few-shot image classification has thus been proposed to effectively use only a limited number of labeled examples to train models for new classes. Recent works based on transferable metric learning methods have achieved promising classification performance through learning the similarity between the features of samples from the query and support sets. However, rare of them explicitly considers the model interpretability, which can actually be revealed during the training phase.   For that, in this work, we propose a metric learning based method named Region Comparison Network (RCN), which is able to reveal how few-shot learning works as in a neural network as well as to find out specific regions that are related to each other in images coming from the query and support sets. Moreover, we also present a visualization strategy named Region Activation Mapping (RAM) to intuitively explain what our method has learned by visualizing intermediate variables in our network. We also present a new way to generalize the interpretability from the level of tasks to categories, which can also be viewed as a method to find the prototypical parts for supporting the final decision of our RCN. Extensive experiments on four benchmark datasets clearly show the effectiveness of our method over existing baselines.



### Imbalanced Continual Learning with Partitioning Reservoir Sampling
- **Arxiv ID**: http://arxiv.org/abs/2009.03632v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2009.03632v1)
- **Published**: 2020-09-08 10:28:18+00:00
- **Updated**: 2020-09-08 10:28:18+00:00
- **Authors**: Chris Dongjoo Kim, Jinseo Jeong, Gunhee Kim
- **Comment**: Published to ECCV2020
- **Journal**: None
- **Summary**: Continual learning from a sequential stream of data is a crucial challenge for machine learning research. Most studies have been conducted on this topic under the single-label classification setting along with an assumption of balanced label distribution. This work expands this research horizon towards multi-label classification. In doing so, we identify unanticipated adversity innately existent in many multi-label datasets, the long-tailed distribution. We jointly address the two independently solved problems, Catastropic Forgetting and the long-tailed label distribution by first empirically showing a new challenge of destructive forgetting of the minority concepts on the tail. Then, we curate two benchmark datasets, COCOseq and NUS-WIDEseq, that allow the study of both intra- and inter-task imbalances. Lastly, we propose a new sampling strategy for replay-based approach named Partitioning Reservoir Sampling (PRS), which allows the model to maintain a balanced knowledge of both head and tail classes. We publicly release the dataset and the code in our project page.



### Learning more expressive joint distributions in multimodal variational methods
- **Arxiv ID**: http://arxiv.org/abs/2009.03651v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2009.03651v1)
- **Published**: 2020-09-08 11:45:27+00:00
- **Updated**: 2020-09-08 11:45:27+00:00
- **Authors**: Sasho Nedelkoski, Mihail Bogojeski, Odej Kao
- **Comment**: 12 pages, Accepted and presented at LOD 2020
- **Journal**: None
- **Summary**: Data often are formed of multiple modalities, which jointly describe the observed phenomena. Modeling the joint distribution of multimodal data requires larger expressive power to capture high-level concepts and provide better data representations. However, multimodal generative models based on variational inference are limited due to the lack of flexibility of the approximate posterior, which is obtained by searching within a known parametric family of distributions. We introduce a method that improves the representational capacity of multimodal variational methods using normalizing flows. It approximates the joint posterior with a simple parametric distribution and subsequently transforms into a more complex one. Through several experiments, we demonstrate that the model improves on state-of-the-art multimodal methods based on variational inference on various computer vision tasks such as colorization, edge and mask detection, and weakly supervised learning. We also show that learning more powerful approximate joint distributions improves the quality of the generated samples. The code of our model is publicly available at https://github.com/SashoNedelkoski/BPFDMVM.



### Label-Free Segmentation of COVID-19 Lesions in Lung CT
- **Arxiv ID**: http://arxiv.org/abs/2009.06456v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.06456v3)
- **Published**: 2020-09-08 12:38:34+00:00
- **Updated**: 2021-03-12 03:01:58+00:00
- **Authors**: Qingsong Yao, Li Xiao, Peihang Liu, S. Kevin Zhou
- **Comment**: Accepted by Transaction on Medical Imaging 2021
- **Journal**: None
- **Summary**: Scarcity of annotated images hampers the building of automated solution for reliable COVID-19 diagnosis and evaluation from CT. To alleviate the burden of data annotation, we herein present a label-free approach for segmenting COVID-19 lesions in CT via pixel-level anomaly modeling that mines out the relevant knowledge from normal CT lung scans. Our modeling is inspired by the observation that the parts of tracheae and vessels, which lay in the high-intensity range where lesions belong to, exhibit strong patterns. To facilitate the learning of such patterns at a pixel level, we synthesize `lesions' using a set of surprisingly simple operations and insert the synthesized `lesions' into normal CT lung scans to form training pairs, from which we learn a normalcy-converting network (NormNet) that turns an 'abnormal' image back to normal. Our experiments on three different datasets validate the effectiveness of NormNet, which conspicuously outperforms a variety of unsupervised anomaly detection (UAD) methods.



### Convolutional Neural Networks for Automatic Detection of Artifacts from Independent Components Represented in Scalp Topographies of EEG Signals
- **Arxiv ID**: http://arxiv.org/abs/2009.03696v1
- **DOI**: 10.1016/j.compbiomed.2021.104347
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2009.03696v1)
- **Published**: 2020-09-08 12:40:10+00:00
- **Updated**: 2020-09-08 12:40:10+00:00
- **Authors**: Giuseppe Placidi, Luigi Cinque, Matteo Polsinelli
- **Comment**: None
- **Journal**: Computers in Biology and Medicine. 132 (2021) 104347
- **Summary**: Electroencephalography (EEG) measures the electrical brain activity in real-time by using sensors placed on the scalp. Artifacts, due to eye movements and blink, muscular/cardiac activity and generic electrical disturbances, have to be recognized and eliminated to allow a correct interpretation of the useful brain signals (UBS) of EEG. Independent Component Analysis (ICA) is effective to split the signal into independent components (ICs) whose re-projections on 2D scalp topographies (images), also called topoplots, allow to recognize/separate artifacts and by UBS. Until now, IC topoplot analysis, a gold standard in EEG, has been carried on visually by human experts and, hence, not usable in automatic, fast-response EEG. We present a completely automatic and effective framework for EEG artifact recognition by IC topoplots, based on 2D Convolutional Neural Networks (CNNs), capable to divide topoplots in 4 classes: 3 types of artifacts and UBS. The framework setup is described and results are presented, discussed and compared with those obtained by other competitive strategies. Experiments, carried on public EEG datasets, have shown an overall accuracy of above 98%, employing 1.4 sec on a standard PC to classify 32 topoplots, that is to drive an EEG system of 32 sensors. Though not real-time, the proposed framework is efficient enough to be used in fast-response EEG-based Brain-Computer Interfaces (BCI) and faster than other automatic methods based on ICs.



### Adversarial Machine Learning in Image Classification: A Survey Towards the Defender's Perspective
- **Arxiv ID**: http://arxiv.org/abs/2009.03728v1
- **DOI**: 10.1145/3485133
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.03728v1)
- **Published**: 2020-09-08 13:21:55+00:00
- **Updated**: 2020-09-08 13:21:55+00:00
- **Authors**: Gabriel Resende Machado, Eugênio Silva, Ronaldo Ribeiro Goldschmidt
- **Comment**: None
- **Journal**: ACM Computing Surveys (CSUR), Volume 55, Issue 1, Article 8, 2021
- **Summary**: Deep Learning algorithms have achieved the state-of-the-art performance for Image Classification and have been used even in security-critical applications, such as biometric recognition systems and self-driving cars. However, recent works have shown those algorithms, which can even surpass the human capabilities, are vulnerable to adversarial examples. In Computer Vision, adversarial examples are images containing subtle perturbations generated by malicious optimization algorithms in order to fool classifiers. As an attempt to mitigate these vulnerabilities, numerous countermeasures have been constantly proposed in literature. Nevertheless, devising an efficient defense mechanism has proven to be a difficult task, since many approaches have already shown to be ineffective to adaptive attackers. Thus, this self-containing paper aims to provide all readerships with a review of the latest research progress on Adversarial Machine Learning in Image Classification, however with a defender's perspective. Here, novel taxonomies for categorizing adversarial attacks and defenses are introduced and discussions about the existence of adversarial examples are provided. Further, in contrast to exisiting surveys, it is also given relevant guidance that should be taken into consideration by researchers when devising and evaluating defenses. Finally, based on the reviewed literature, it is discussed some promising paths for future research.



### Self-Supervised Scale Recovery for Monocular Depth and Egomotion Estimation
- **Arxiv ID**: http://arxiv.org/abs/2009.03787v5
- **DOI**: 10.1109/IROS51168.2021.9635938
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.03787v5)
- **Published**: 2020-09-08 14:30:21+00:00
- **Updated**: 2022-05-01 16:54:55+00:00
- **Authors**: Brandon Wagstaff, Jonathan Kelly
- **Comment**: In Proceedings of the IEEE/RSJ International Conference on
  Intelligent Robots and Systems (IROS'21), Prague, Czech Republic, Sept. 27 -
  Oct. 1, 2021
- **Journal**: None
- **Summary**: The self-supervised loss formulation for jointly training depth and egomotion neural networks with monocular images is well studied and has demonstrated state-of-the-art accuracy. One of the main limitations of this approach, however, is that the depth and egomotion estimates are only determined up to an unknown scale. In this paper, we present a novel scale recovery loss that enforces consistency between a known camera height and the estimated camera height, generating metric (scaled) depth and egomotion predictions. We show that our proposed method is competitive with other scale recovery techniques that require more information. Further, we demonstrate that our method facilitates network retraining within new environments, whereas other scale-resolving approaches are incapable of doing so. Notably, our egomotion network is able to produce more accurate estimates than a similar method which recovers scale at test time only.



### Understanding Compositional Structures in Art Historical Images using Pose and Gaze Priors
- **Arxiv ID**: http://arxiv.org/abs/2009.03807v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2009.03807v1)
- **Published**: 2020-09-08 15:01:56+00:00
- **Updated**: 2020-09-08 15:01:56+00:00
- **Authors**: Prathmesh Madhu, Tilman Marquart, Ronak Kosti, Peter Bell, Andreas Maier, Vincent Christlein
- **Comment**: To be Published in ECCV 2020 Workshops (VISART V)
- **Journal**: None
- **Summary**: Image compositions as a tool for analysis of artworks is of extreme significance for art historians. These compositions are useful in analyzing the interactions in an image to study artists and their artworks. Max Imdahl in his work called Ikonik, along with other prominent art historians of the 20th century, underlined the aesthetic and semantic importance of the structural composition of an image. Understanding underlying compositional structures within images is challenging and a time consuming task. Generating these structures automatically using computer vision techniques (1) can help art historians towards their sophisticated analysis by saving lot of time; providing an overview and access to huge image repositories and (2) also provide an important step towards an understanding of man made imagery by machines. In this work, we attempt to automate this process using the existing state of the art machine learning techniques, without involving any form of training. Our approach, inspired by Max Imdahl's pioneering work, focuses on two central themes of image composition: (a) detection of action regions and action lines of the artwork; and (b) pose-based segmentation of foreground and background. Currently, our approach works for artworks comprising of protagonists (persons) in an image. In order to validate our approach qualitatively and quantitatively, we conduct a user study involving experts and non-experts. The outcome of the study highly correlates with our approach and also demonstrates its domain-agnostic capability. We have open-sourced the code at https://github.com/image-compostion-canvas-group/image-compostion-canvas.



### Understanding and Exploiting Dependent Variables with Deep Metric Learning
- **Arxiv ID**: http://arxiv.org/abs/2009.03820v1
- **DOI**: 10.1007/978-3-030-55180-3_8
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2009.03820v1)
- **Published**: 2020-09-08 15:30:45+00:00
- **Updated**: 2020-09-08 15:30:45+00:00
- **Authors**: Niall O' Mahony, Sean Campbell, Anderson Carvalho, Lenka Krpalkova, Gustavo Velasco-Hernandez, Daniel Riordan, Joseph Walsh
- **Comment**: None
- **Journal**: Proceedings of the 2020 Intelligent Systems Conference
  (IntelliSys) Volume 1, B. R. Arai K., Kapoor S., Ed. Springer, Cham, 2020,
  pp. 97 to 113
- **Summary**: Deep Metric Learning (DML) approaches learn to represent inputs to a lower-dimensional latent space such that the distance between representations in this space corresponds with a predefined notion of similarity. This paper investigates how the mapping element of DML may be exploited in situations where the salient features in arbitrary classification problems vary over time or due to changing underlying variables. Examples of such variable features include seasonal and time-of-day variations in outdoor scenes in place recognition tasks for autonomous navigation and age/gender variations in human/animal subjects in classification tasks for medical/ethological studies. Through the use of visualisation tools for observing the distribution of DML representations per each query variable for which prior information is available, the influence of each variable on the classification task may be better understood. Based on these relationships, prior information on these salient background variables may be exploited at the inference stage of the DML approach by using a clustering algorithm to improve classification performance. This research proposes such a methodology establishing the saliency of query background variables and formulating clustering algorithms for better separating latent-space representations at run-time. The paper also discusses online management strategies to preserve the quality and diversity of data and the representation of each class in the gallery of embeddings in the DML approach. We also discuss latent works towards understanding the relevance of underlying/multiple variables with DML.



### COVIDNet-CT: A Tailored Deep Convolutional Neural Network Design for Detection of COVID-19 Cases from Chest CT Images
- **Arxiv ID**: http://arxiv.org/abs/2009.05383v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.05383v1)
- **Published**: 2020-09-08 15:49:55+00:00
- **Updated**: 2020-09-08 15:49:55+00:00
- **Authors**: Hayden Gunraj, Linda Wang, Alexander Wong
- **Comment**: 12 pages
- **Journal**: None
- **Summary**: The coronavirus disease 2019 (COVID-19) pandemic continues to have a tremendous impact on patients and healthcare systems around the world. In the fight against this novel disease, there is a pressing need for rapid and effective screening tools to identify patients infected with COVID-19, and to this end CT imaging has been proposed as one of the key screening methods which may be used as a complement to RT-PCR testing, particularly in situations where patients undergo routine CT scans for non-COVID-19 related reasons, patients with worsening respiratory status or developing complications that require expedited care, and patients suspected to be COVID-19-positive but have negative RT-PCR test results. Motivated by this, in this study we introduce COVIDNet-CT, a deep convolutional neural network architecture that is tailored for detection of COVID-19 cases from chest CT images via a machine-driven design exploration approach. Additionally, we introduce COVIDx-CT, a benchmark CT image dataset derived from CT imaging data collected by the China National Center for Bioinformation comprising 104,009 images across 1,489 patient cases. Furthermore, in the interest of reliability and transparency, we leverage an explainability-driven performance validation strategy to investigate the decision-making behaviour of COVIDNet-CT, and in doing so ensure that COVIDNet-CT makes predictions based on relevant indicators in CT images. Both COVIDNet-CT and the COVIDx-CT dataset are available to the general public in an open-source and open access manner as part of the COVID-Net initiative. While COVIDNet-CT is not yet a production-ready screening solution, we hope that releasing the model and dataset will encourage researchers, clinicians, and citizen data scientists alike to leverage and build upon them.



### Binarized Neural Architecture Search for Efficient Object Recognition
- **Arxiv ID**: http://arxiv.org/abs/2009.04247v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.04247v1)
- **Published**: 2020-09-08 15:51:23+00:00
- **Updated**: 2020-09-08 15:51:23+00:00
- **Authors**: Hanlin Chen, Li'an Zhuo, Baochang Zhang, Xiawu Zheng, Jianzhuang Liu, Rongrong Ji, David Doermann, Guodong Guo
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1911.10862
- **Journal**: None
- **Summary**: Traditional neural architecture search (NAS) has a significant impact in computer vision by automatically designing network architectures for various tasks. In this paper, binarized neural architecture search (BNAS), with a search space of binarized convolutions, is introduced to produce extremely compressed models to reduce huge computational cost on embedded devices for edge computing. The BNAS calculation is more challenging than NAS due to the learning inefficiency caused by optimization requirements and the huge architecture space, and the performance loss when handling the wild data in various computing applications. To address these issues, we introduce operation space reduction and channel sampling into BNAS to significantly reduce the cost of searching. This is accomplished through a performance-based strategy that is robust to wild data, which is further used to abandon less potential operations. Furthermore, we introduce the Upper Confidence Bound (UCB) to solve 1-bit BNAS. Two optimization methods for binarized neural networks are used to validate the effectiveness of our BNAS. Extensive experiments demonstrate that the proposed BNAS achieves a comparable performance to NAS on both CIFAR and ImageNet databases. An accuracy of $96.53\%$ vs. $97.22\%$ is achieved on the CIFAR-10 dataset, but with a significantly compressed model, and a $40\%$ faster search than the state-of-the-art PC-DARTS. On the wild face recognition task, our binarized models achieve a performance similar to their corresponding full-precision models.



### TanhSoft -- a family of activation functions combining Tanh and Softplus
- **Arxiv ID**: http://arxiv.org/abs/2009.03863v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.03863v1)
- **Published**: 2020-09-08 16:59:28+00:00
- **Updated**: 2020-09-08 16:59:28+00:00
- **Authors**: Koushik Biswas, Sandeep Kumar, Shilpak Banerjee, Ashish Kumar Pandey
- **Comment**: 11 pages
- **Journal**: None
- **Summary**: Deep learning at its core, contains functions that are composition of a linear transformation with a non-linear function known as activation function. In past few years, there is an increasing interest in construction of novel activation functions resulting in better learning. In this work, we propose a family of novel activation functions, namely TanhSoft, with four undetermined hyper-parameters of the form tanh({\alpha}x+{\beta}e^{{\gamma}x})ln({\delta}+e^x) and tune these hyper-parameters to obtain activation functions which are shown to outperform several well known activation functions. For instance, replacing ReLU with xtanh(0.6e^x)improves top-1 classification accuracy on CIFAR-10 by 0.46% for DenseNet-169 and 0.7% for Inception-v3 while with tanh(0.87x)ln(1 +e^x) top-1 classification accuracy on CIFAR-100 improves by 1.24% for DenseNet-169 and 2.57% for SimpleNet model.



### Intraoperative Liver Surface Completion with Graph Convolutional VAE
- **Arxiv ID**: http://arxiv.org/abs/2009.03871v2
- **DOI**: 10.1007/978-3-030-60365-6_19
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.03871v2)
- **Published**: 2020-09-08 17:19:31+00:00
- **Updated**: 2021-07-12 18:28:56+00:00
- **Authors**: Simone Foti, Bongjin Koo, Thomas Dowrick, Joao Ramalhinho, Moustafa Allam, Brian Davidson, Danail Stoyanov, Matthew J. Clarkson
- **Comment**: None
- **Journal**: None
- **Summary**: In this work we propose a method based on geometric deep learning to predict the complete surface of the liver, given a partial point cloud of the organ obtained during the surgical laparoscopic procedure. We introduce a new data augmentation technique that randomly perturbs shapes in their frequency domain to compensate the limited size of our dataset. The core of our method is a variational autoencoder (VAE) that is trained to learn a latent space for complete shapes of the liver. At inference time, the generative part of the model is embedded in an optimisation procedure where the latent representation is iteratively updated to generate a model that matches the intraoperative partial point cloud. The effect of this optimisation is a progressive non-rigid deformation of the initially generated shape. Our method is qualitatively evaluated on real data and quantitatively evaluated on synthetic data. We compared with a state-of-the-art rigid registration algorithm, that our method outperformed in visible areas.



### Convolution Neural Networks for diagnosing colon and lung cancer histopathological images
- **Arxiv ID**: http://arxiv.org/abs/2009.03878v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.03878v1)
- **Published**: 2020-09-08 17:36:24+00:00
- **Updated**: 2020-09-08 17:36:24+00:00
- **Authors**: Sanidhya Mangal, Aanchal Chaurasia, Ayush Khajanchi
- **Comment**: 10 pages, 4 figures, 2 tables
- **Journal**: None
- **Summary**: Lung and Colon cancer are one of the leading causes of mortality and morbidity in adults. Histopathological diagnosis is one of the key components to discern cancer type. The aim of the present research is to propose a computer aided diagnosis system for diagnosing squamous cell carcinomas and adenocarcinomas of lung as well as adenocarcinomas of colon using convolutional neural networks by evaluating the digital pathology images for these cancers. Hereby, rendering artificial intelligence as useful technology in the near future. A total of 2500 digital images were acquired from LC25000 dataset containing 5000 images for each class. A shallow neural network architecture was used classify the histopathological slides into squamous cell carcinomas, adenocarcinomas and benign for the lung. Similar model was used to classify adenocarcinomas and benign for colon. The diagnostic accuracy of more than 97% and 96% was recorded for lung and colon respectively.



### Towards Unique and Informative Captioning of Images
- **Arxiv ID**: http://arxiv.org/abs/2009.03949v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.03949v1)
- **Published**: 2020-09-08 19:01:33+00:00
- **Updated**: 2020-09-08 19:01:33+00:00
- **Authors**: Zeyu Wang, Berthy Feng, Karthik Narasimhan, Olga Russakovsky
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: Despite considerable progress, state of the art image captioning models produce generic captions, leaving out important image details. Furthermore, these systems may even misrepresent the image in order to produce a simpler caption consisting of common concepts. In this paper, we first analyze both modern captioning systems and evaluation metrics through empirical experiments to quantify these phenomena. We find that modern captioning systems return higher likelihoods for incorrect distractor sentences compared to ground truth captions, and that evaluation metrics like SPICE can be 'topped' using simple captioning systems relying on object detectors. Inspired by these observations, we design a new metric (SPICE-U) by introducing a notion of uniqueness over the concepts generated in a caption. We show that SPICE-U is better correlated with human judgements compared to SPICE, and effectively captures notions of diversity and descriptiveness. Finally, we also demonstrate a general technique to improve any existing captioning model -- by using mutual information as a re-ranking objective during decoding. Empirically, this results in more unique and informative captions, and improves three different state-of-the-art models on SPICE-U as well as average score over existing metrics.



### Joint Pose and Shape Estimation of Vehicles from LiDAR Data
- **Arxiv ID**: http://arxiv.org/abs/2009.03964v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.03964v1)
- **Published**: 2020-09-08 19:22:23+00:00
- **Updated**: 2020-09-08 19:22:23+00:00
- **Authors**: Hunter Goforth, Xiaoyan Hu, Michael Happold, Simon Lucey
- **Comment**: None
- **Journal**: None
- **Summary**: We address the problem of estimating the pose and shape of vehicles from LiDAR scans, a common problem faced by the autonomous vehicle community. Recent work has tended to address pose and shape estimation separately in isolation, despite the inherent connection between the two. We investigate a method of jointly estimating shape and pose where a single encoding is learned from which shape and pose may be decoded in an efficient yet effective manner. We additionally introduce a novel joint pose and shape loss, and show that this joint training method produces better results than independently-trained pose and shape estimators. We evaluate our method on both synthetic data and real-world data, and show superior performance against a state-of-the-art baseline.



### Modeling Wildfire Perimeter Evolution using Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2009.03977v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.03977v1)
- **Published**: 2020-09-08 20:06:01+00:00
- **Updated**: 2020-09-08 20:06:01+00:00
- **Authors**: Maxfield E. Green, Karl Kaiser, Nat Shenton
- **Comment**: None
- **Journal**: None
- **Summary**: With the increased size and frequency of wildfire eventsworldwide, accurate real-time prediction of evolving wildfirefronts is a crucial component of firefighting efforts and for-est management practices. We propose a wildfire spreadingmodel that predicts the evolution of the wildfire perimeter in24 hour periods. The fire spreading simulation is based ona deep convolutional neural network (CNN) that is trainedon remotely sensed atmospheric and environmental time se-ries data. We show that the model is able to learn wildfirespreading dynamics from real historic data sets from a seriesof wildfires in the Western Sierra Nevada Mountains in Cal-ifornia. We validate the model on a previously unseen wild-fire and produce realistic results that significantly outperformhistoric alternatives with validation accuracies ranging from78% - 98%



### Fuzzy Unique Image Transformation: Defense Against Adversarial Attacks On Deep COVID-19 Models
- **Arxiv ID**: http://arxiv.org/abs/2009.04004v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.04004v1)
- **Published**: 2020-09-08 21:35:24+00:00
- **Updated**: 2020-09-08 21:35:24+00:00
- **Authors**: Achyut Mani Tripathi, Ashish Mishra
- **Comment**: None
- **Journal**: None
- **Summary**: Early identification of COVID-19 using a deep model trained on Chest X-Ray and CT images has gained considerable attention from researchers to speed up the process of identification of active COVID-19 cases. These deep models act as an aid to hospitals that suffer from the unavailability of specialists or radiologists, specifically in remote areas. Various deep models have been proposed to detect the COVID-19 cases, but few works have been performed to prevent the deep models against adversarial attacks capable of fooling the deep model by using a small perturbation in image pixels. This paper presents an evaluation of the performance of deep COVID-19 models against adversarial attacks. Also, it proposes an efficient yet effective Fuzzy Unique Image Transformation (FUIT) technique that downsamples the image pixels into an interval. The images obtained after the FUIT transformation are further utilized for training the secure deep model that preserves high accuracy of the diagnosis of COVID-19 cases and provides reliable defense against the adversarial attacks. The experiments and results show the proposed model prevents the deep model against the six adversarial attacks and maintains high accuracy to classify the COVID-19 cases from the Chest X-Ray image and CT image Datasets. The results also recommend that a careful inspection is required before practically applying the deep models to diagnose the COVID-19 cases.



### Learning joint segmentation of tissues and brain lesions from task-specific hetero-modal domain-shifted datasets
- **Arxiv ID**: http://arxiv.org/abs/2009.04009v1
- **DOI**: 10.1016/j.media.2020.101862
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.04009v1)
- **Published**: 2020-09-08 22:00:00+00:00
- **Updated**: 2020-09-08 22:00:00+00:00
- **Authors**: Reuben Dorent, Thomas Booth, Wenqi Li, Carole H. Sudre, Sina Kafiabadi, Jorge Cardoso, Sebastien Ourselin, Tom Vercauteren
- **Comment**: MIDL 2019 special issue - Medical Image Analysis
- **Journal**: None
- **Summary**: Brain tissue segmentation from multimodal MRI is a key building block of many neuroimaging analysis pipelines. Established tissue segmentation approaches have, however, not been developed to cope with large anatomical changes resulting from pathology, such as white matter lesions or tumours, and often fail in these cases. In the meantime, with the advent of deep neural networks (DNNs), segmentation of brain lesions has matured significantly. However, few existing approaches allow for the joint segmentation of normal tissue and brain lesions. Developing a DNN for such a joint task is currently hampered by the fact that annotated datasets typically address only one specific task and rely on task-specific imaging protocols including a task-specific set of imaging modalities. In this work, we propose a novel approach to build a joint tissue and lesion segmentation model from aggregated task-specific hetero-modal domain-shifted and partially-annotated datasets. Starting from a variational formulation of the joint problem, we show how the expected risk can be decomposed and optimised empirically. We exploit an upper bound of the risk to deal with heterogeneous imaging modalities across datasets. To deal with potential domain shift, we integrated and tested three conventional techniques based on data augmentation, adversarial learning and pseudo-healthy generation. For each individual task, our joint approach reaches comparable performance to task-specific and fully-supervised models. The proposed framework is assessed on two different types of brain lesions: White matter lesions and gliomas. In the latter case, lacking a joint ground-truth for quantitative assessment purposes, we propose and use a novel clinically-relevant qualitative assessment methodology.



