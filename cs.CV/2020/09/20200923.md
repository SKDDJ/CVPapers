# Arxiv Papers in cs.CV on 2020-09-23
### A Real-Time Predictive Pedestrian Collision Warning Service for Cooperative Intelligent Transportation Systems Using 3D Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2009.10868v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.10868v4)
- **Published**: 2020-09-23 00:55:12+00:00
- **Updated**: 2022-02-22 03:40:11+00:00
- **Authors**: Ue-Hwan Kim, Dongho Ka, Hwasoo Yeo, Jong-Hwan Kim
- **Comment**: 12 pages, 8 figures, 4 tables
- **Journal**: None
- **Summary**: Minimizing traffic accidents between vehicles and pedestrians is one of the primary research goals in intelligent transportation systems. To achieve the goal, pedestrian orientation recognition and prediction of pedestrian's crossing or not-crossing intention play a central role. Contemporary approaches do not guarantee satisfactory performance due to limited field-of-view, lack of generalization, and high computational complexity. To overcome these limitations, we propose a real-time predictive pedestrian collision warning service (P2CWS) for two tasks: pedestrian orientation recognition (100.53 FPS) and intention prediction (35.76 FPS). Our framework obtains satisfying generalization over multiple sites because of the proposed site-independent features. At the center of the feature extraction lies 3D pose estimation. The 3D pose analysis enables robust and accurate recognition of pedestrian orientations and prediction of intentions over multiple sites. The proposed vision framework realizes 89.3% accuracy in the behavior recognition task on the TUD dataset without any training process and 91.28% accuracy in intention prediction on our dataset achieving new state-of-the-art performance. To contribute to the corresponding research community, we make our source codes public which are available at https://github.com/Uehwan/VisionForPedestrian



### Hamming OCR: A Locality Sensitive Hashing Neural Network for Scene Text Recognition
- **Arxiv ID**: http://arxiv.org/abs/2009.10874v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.10874v1)
- **Published**: 2020-09-23 01:20:19+00:00
- **Updated**: 2020-09-23 01:20:19+00:00
- **Authors**: Bingcong Li, Xin Tang, Xianbiao Qi, Yihao Chen, Rong Xiao
- **Comment**: 9 Pages, 4 Figure
- **Journal**: None
- **Summary**: Recently, inspired by Transformer, self-attention-based scene text recognition approaches have achieved outstanding performance. However, we find that the size of model expands rapidly with the lexicon increasing. Specifically, the number of parameters for softmax classification layer and output embedding layer are proportional to the vocabulary size. It hinders the development of a lightweight text recognition model especially applied for Chinese and multiple languages. Thus, we propose a lightweight scene text recognition model named Hamming OCR. In this model, a novel Hamming classifier, which adopts locality sensitive hashing (LSH) algorithm to encode each character, is proposed to replace the softmax regression and the generated LSH code is directly employed to replace the output embedding. We also present a simplified transformer decoder to reduce the number of parameters by removing the feed-forward network and using cross-layer parameter sharing technique. Compared with traditional methods, the number of parameters in both classification and embedding layers is independent on the size of vocabulary, which significantly reduces the storage requirement without loss of accuracy. Experimental results on several datasets, including four public benchmaks and a Chinese text dataset synthesized by SynthText with more than 20,000 characters, shows that Hamming OCR achieves competitive results.



### Leveraging Local and Global Descriptors in Parallel to Search Correspondences for Visual Localization
- **Arxiv ID**: http://arxiv.org/abs/2009.10891v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.10891v1)
- **Published**: 2020-09-23 01:49:03+00:00
- **Updated**: 2020-09-23 01:49:03+00:00
- **Authors**: Pengju Zhang, Yihong Wu, Bingxi Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Visual localization to compute 6DoF camera pose from a given image has wide applications such as in robotics, virtual reality, augmented reality, etc. Two kinds of descriptors are important for the visual localization. One is global descriptors that extract the whole feature from each image. The other is local descriptors that extract the local feature from each image patch usually enclosing a key point. More and more methods of the visual localization have two stages: at first to perform image retrieval by global descriptors and then from the retrieval feedback to make 2D-3D point correspondences by local descriptors. The two stages are in serial for most of the methods. This simple combination has not achieved superiority of fusing local and global descriptors. The 3D points obtained from the retrieval feedback are as the nearest neighbor candidates of the 2D image points only by global descriptors. Each of the 2D image points is also called a query local feature when performing the 2D-3D point correspondences. In this paper, we propose a novel parallel search framework, which leverages advantages of both local and global descriptors to get nearest neighbor candidates of a query local feature. Specifically, besides using deep learning based global descriptors, we also utilize local descriptors to construct random tree structures for obtaining nearest neighbor candidates of the query local feature. We propose a new probabilistic model and a new deep learning based local descriptor when constructing the random trees. A weighted Hamming regularization term to keep discriminativeness after binarization is given in the loss function for the proposed local descriptor. The loss function co-trains both real and binary descriptors of which the results are integrated into the random trees.



### HiCOMEX: Facial Action Unit Recognition Based on Hierarchy Intensity Distribution and COMEX Relation Learning
- **Arxiv ID**: http://arxiv.org/abs/2009.10892v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.10892v3)
- **Published**: 2020-09-23 01:49:56+00:00
- **Updated**: 2021-04-29 02:04:23+00:00
- **Authors**: Ziqiang Shi, Liu Liu, Zhongling Liu, Rujie Liu, Xiaoyu Mi, and Kentaro Murase
- **Comment**: None
- **Journal**: None
- **Summary**: The detection of facial action units (AUs) has been studied as it has the competition due to the wide-ranging applications thereof. In this paper, we propose a novel framework for the AU detection from a single input image by grasping the \textbf{c}o-\textbf{o}ccurrence and \textbf{m}utual \textbf{ex}clusion (COMEX) as well as the intensity distribution among AUs. Our algorithm uses facial landmarks to detect the features of local AUs. The features are input to a bidirectional long short-term memory (BiLSTM) layer for learning the intensity distribution. Afterwards, the new AU feature continuously passed through a self-attention encoding layer and a continuous-state modern Hopfield layer for learning the COMEX relationships. Our experiments on the challenging BP4D and DISFA benchmarks without any external data or pre-trained models yield F1-scores of 63.7\% and 61.8\% respectively, which shows our proposed networks can lead to performance improvement in the AU detection task.



### Pruning Convolutional Filters using Batch Bridgeout
- **Arxiv ID**: http://arxiv.org/abs/2009.10893v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2009.10893v1)
- **Published**: 2020-09-23 01:51:47+00:00
- **Updated**: 2020-09-23 01:51:47+00:00
- **Authors**: Najeeb Khan, Ian Stavness
- **Comment**: None
- **Journal**: None
- **Summary**: State-of-the-art computer vision models are rapidly increasing in capacity, where the number of parameters far exceeds the number required to fit the training set. This results in better optimization and generalization performance. However, the huge size of contemporary models results in large inference costs and limits their use on resource-limited devices. In order to reduce inference costs, convolutional filters in trained neural networks could be pruned to reduce the run-time memory and computational requirements during inference. However, severe post-training pruning results in degraded performance if the training algorithm results in dense weight vectors. We propose the use of Batch Bridgeout, a sparsity inducing stochastic regularization scheme, to train neural networks so that they could be pruned efficiently with minimal degradation in performance. We evaluate the proposed method on common computer vision models VGGNet, ResNet, and Wide-ResNet on the CIFAR image classification task. For all the networks, experimental results show that Batch Bridgeout trained networks achieve higher accuracy across a wide range of pruning intensities compared to Dropout and weight decay regularization.



### CLASS: Cross-Level Attention and Supervision for Salient Objects Detection
- **Arxiv ID**: http://arxiv.org/abs/2009.10916v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.10916v2)
- **Published**: 2020-09-23 03:10:12+00:00
- **Updated**: 2020-09-24 08:12:12+00:00
- **Authors**: Lv Tang, Bo Li
- **Comment**: Full version of ACCV2020
- **Journal**: None
- **Summary**: Salient object detection (SOD) is a fundamental computer vision task. Recently, with the revival of deep neural networks, SOD has made great progresses. However, there still exist two thorny issues that cannot be well addressed by existing methods, indistinguishable regions and complex structures. To address these two issues, in this paper we propose a novel deep network for accurate SOD, named CLASS. First, in order to leverage the different advantages of low-level and high-level features, we propose a novel non-local cross-level attention (CLA), which can capture the long-range feature dependencies to enhance the distinction of complete salient object. Second, a novel cross-level supervision (CLS) is designed to learn complementary context for complex structures through pixel-level, region-level and object-level. Then the fine structures and boundaries of salient objects can be well restored. In experiments, with the proposed CLA and CLS, our CLASS net. consistently outperforms 13 state-of-the-art methods on five datasets.



### Using satellite imagery to understand and promote sustainable development
- **Arxiv ID**: http://arxiv.org/abs/2010.06988v1
- **DOI**: None
- **Categories**: **cs.CY**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2010.06988v1)
- **Published**: 2020-09-23 05:20:00+00:00
- **Updated**: 2020-09-23 05:20:00+00:00
- **Authors**: Marshall Burke, Anne Driscoll, David B. Lobell, Stefano Ermon
- **Comment**: None
- **Journal**: None
- **Summary**: Accurate and comprehensive measurements of a range of sustainable development outcomes are fundamental inputs into both research and policy. We synthesize the growing literature that uses satellite imagery to understand these outcomes, with a focus on approaches that combine imagery with machine learning. We quantify the paucity of ground data on key human-related outcomes and the growing abundance and resolution (spatial, temporal, and spectral) of satellite imagery. We then review recent machine learning approaches to model-building in the context of scarce and noisy training data, highlighting how this noise often leads to incorrect assessment of models' predictive performance. We quantify recent model performance across multiple sustainable development domains, discuss research and policy applications, explore constraints to future progress, and highlight key research directions for the field.



### Agent-based Simulation Model and Deep Learning Techniques to Evaluate and Predict Transportation Trends around COVID-19
- **Arxiv ID**: http://arxiv.org/abs/2010.09648v1
- **DOI**: None
- **Categories**: **cs.MA**, cs.CV, eess.IV, physics.soc-ph
- **Links**: [PDF](http://arxiv.org/pdf/2010.09648v1)
- **Published**: 2020-09-23 05:37:15+00:00
- **Updated**: 2020-09-23 05:37:15+00:00
- **Authors**: Ding Wang, Fan Zuo, Jingqin Gao, Yueshuai He, Zilin Bian, Suzana Duran Bernardes, Chaekuk Na, Jingxing Wang, John Petinos, Kaan Ozbay, Joseph Y. J. Chow, Shri Iyer, Hani Nassif, Xuegang Jeff Ban
- **Comment**: None
- **Journal**: None
- **Summary**: The COVID-19 pandemic has affected travel behaviors and transportation system operations, and cities are grappling with what policies can be effective for a phased reopening shaped by social distancing. This edition of the white paper updates travel trends and highlights an agent-based simulation model's results to predict the impact of proposed phased reopening strategies. It also introduces a real-time video processing method to measure social distancing through cameras on city streets.



### Scene Graph to Image Generation with Contextualized Object Layout Refinement
- **Arxiv ID**: http://arxiv.org/abs/2009.10939v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.10939v4)
- **Published**: 2020-09-23 06:27:54+00:00
- **Updated**: 2022-10-10 20:33:51+00:00
- **Authors**: Maor Ivgi, Yaniv Benny, Avichai Ben-David, Jonathan Berant, Lior Wolf
- **Comment**: Appeared at IEEE International Conference in Image Processing (ICIP)
  2021
- **Journal**: None
- **Summary**: Generating images from scene graphs is a challenging task that attracted substantial interest recently. Prior works have approached this task by generating an intermediate layout description of the target image. However, the representation of each object in the layout was generated independently, which resulted in high overlap, low coverage, and an overall blurry layout. We propose a novel method that alleviates these issues by generating the entire layout description gradually to improve inter-object dependency. We empirically show on the COCO-STUFF dataset that our approach improves the quality of both the intermediate layout and the final image. Our approach improves the layout coverage by almost 20 points and drops object overlap to negligible amounts.



### Exploring global diverse attention via pairwise temporal relation for video summarization
- **Arxiv ID**: http://arxiv.org/abs/2009.10942v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2009.10942v1)
- **Published**: 2020-09-23 06:29:09+00:00
- **Updated**: 2020-09-23 06:29:09+00:00
- **Authors**: Ping Li, Qinghao Ye, Luming Zhang, Li Yuan, Xianghua Xu, Ling Shao
- **Comment**: 12 pages, 8 figures
- **Journal**: Pattern Recognition, 2020
- **Summary**: Video summarization is an effective way to facilitate video searching and browsing. Most of existing systems employ encoder-decoder based recurrent neural networks, which fail to explicitly diversify the system-generated summary frames while requiring intensive computations. In this paper, we propose an efficient convolutional neural network architecture for video SUMmarization via Global Diverse Attention called SUM-GDA, which adapts attention mechanism in a global perspective to consider pairwise temporal relations of video frames. Particularly, the GDA module has two advantages: 1) it models the relations within paired frames as well as the relations among all pairs, thus capturing the global attention across all frames of one video; 2) it reflects the importance of each frame to the whole video, leading to diverse attention on these frames. Thus, SUM-GDA is beneficial for generating diverse frames to form satisfactory video summary. Extensive experiments on three data sets, i.e., SumMe, TVSum, and VTW, have demonstrated that SUM-GDA and its extension outperform other competing state-of-the-art methods with remarkable improvements. In addition, the proposed models can be run in parallel with significantly less computational costs, which helps the deployment in highly demanding applications.



### MAFF-Net: Filter False Positive for 3D Vehicle Detection with Multi-modal Adaptive Feature Fusion
- **Arxiv ID**: http://arxiv.org/abs/2009.10945v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.10945v1)
- **Published**: 2020-09-23 06:31:59+00:00
- **Updated**: 2020-09-23 06:31:59+00:00
- **Authors**: Zehan Zhang, Ming Zhang, Zhidong Liang, Xian Zhao, Ming Yang, Wenming Tan, ShiLiang Pu
- **Comment**: 8 pages, 6 figures
- **Journal**: None
- **Summary**: 3D vehicle detection based on multi-modal fusion is an important task of many applications such as autonomous driving. Although significant progress has been made, we still observe two aspects that need to be further improvement: First, the specific gain that camera images can bring to 3D detection is seldom explored by previous works. Second, many fusion algorithms run slowly, which is essential for applications with high real-time requirements(autonomous driving). To this end, we propose an end-to-end trainable single-stage multi-modal feature adaptive network in this paper, which uses image information to effectively reduce false positive of 3D detection and has a fast detection speed. A multi-modal adaptive feature fusion module based on channel attention mechanism is proposed to enable the network to adaptively use the feature of each modal. Based on the above mechanism, two fusion technologies are proposed to adapt to different usage scenarios: PointAttentionFusion is suitable for filtering simple false positive and faster; DenseAttentionFusion is suitable for filtering more difficult false positive and has better overall performance. Experimental results on the KITTI dataset demonstrate significant improvement in filtering false positive over the approach using only point cloud data. Furthermore, the proposed method can provide competitive results and has the fastest speed compared to the published state-of-the-art multi-modal methods in the KITTI benchmark.



### Demand Forecasting in Bike-sharing Systems Based on A Multiple Spatiotemporal Fusion Network
- **Arxiv ID**: http://arxiv.org/abs/2010.03027v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.03027v2)
- **Published**: 2020-09-23 06:51:23+00:00
- **Updated**: 2021-11-08 12:34:35+00:00
- **Authors**: Xiao Yan, Gang Kou, Feng Xiao, Dapeng Zhang, Xianghua Gan
- **Comment**: 12 pages, 15 figures
- **Journal**: None
- **Summary**: Bike-sharing systems (BSSs) have become increasingly popular around the globe and have attracted a wide range of research interests. In this paper, the demand forecasting problem in BSSs is studied. Spatial and temporal features are critical for demand forecasting in BSSs, but it is challenging to extract spatiotemporal dynamics. Another challenge is to capture the relations between spatiotemporal dynamics and external factors, such as weather, day-of-week, and time-of-day. To address these challenges, we propose a multiple spatiotemporal fusion network named MSTF-Net. MSTF-Net consists of multiple spatiotemporal blocks: 3D convolutional network (3D-CNN) blocks, eidetic 3D convolutional long short-term memory networks (E3D-LSTM) blocks, and fully-connected (FC) blocks. Specifically, 3D-CNN blocks highlight extracting short-term spatiotemporal dependence in each fragment (i.e., closeness, period, and trend); E3D-LSTM blocks further extract long-term spatiotemporal dependence over all fragments; FC blocks extract nonlinear correlations of external factors. Finally, the latent representations of E3D-LSTM and FC blocks are fused to obtain the final prediction. For two real-world datasets, it is shown that MSTF-Net outperforms seven state-of-the-art models.



### What is the Reward for Handwriting? -- Handwriting Generation by Imitation Learning
- **Arxiv ID**: http://arxiv.org/abs/2009.10962v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.10962v1)
- **Published**: 2020-09-23 07:04:08+00:00
- **Updated**: 2020-09-23 07:04:08+00:00
- **Authors**: Keisuke Kanda, Brian Kenji Iwana, Seiichi Uchida
- **Comment**: Accepted at ICFHR2020
- **Journal**: None
- **Summary**: Analyzing the handwriting generation process is an important issue and has been tackled by various generation models, such as kinematics based models and stochastic models. In this study, we use a reinforcement learning (RL) framework to realize handwriting generation with the careful future planning ability. In fact, the handwriting process of human beings is also supported by their future planning ability; for example, the ability is necessary to generate a closed trajectory like '0' because any shortsighted model, such as a Markovian model, cannot generate it. For the algorithm, we employ generative adversarial imitation learning (GAIL). Typical RL algorithms require the manual definition of the reward function, which is very crucial to control the generation process. In contrast, GAIL trains the reward function along with the other modules of the framework. In other words, through GAIL, we can understand the reward of the handwriting generation process from handwriting examples. Our experimental results qualitatively and quantitatively show that the learned reward catches the trends in handwriting generation and thus GAIL is well suited for the acquisition of handwriting behavior.



### Semantics-Preserving Adversarial Training
- **Arxiv ID**: http://arxiv.org/abs/2009.10978v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2009.10978v1)
- **Published**: 2020-09-23 07:42:14+00:00
- **Updated**: 2020-09-23 07:42:14+00:00
- **Authors**: Wonseok Lee, Hanbit Lee, Sang-goo Lee
- **Comment**: Preprint. Under Review
- **Journal**: None
- **Summary**: Adversarial training is a defense technique that improves adversarial robustness of a deep neural network (DNN) by including adversarial examples in the training data. In this paper, we identify an overlooked problem of adversarial training in that these adversarial examples often have different semantics than the original data, introducing unintended biases into the model. We hypothesize that such non-semantics-preserving (and resultingly ambiguous) adversarial data harm the robustness of the target models. To mitigate such unintended semantic changes of adversarial examples, we propose semantics-preserving adversarial training (SPAT) which encourages perturbation on the pixels that are shared among all classes when generating adversarial examples in the training stage. Experiment results show that SPAT improves adversarial robustness and achieves state-of-the-art results in CIFAR-10 and CIFAR-100.



### Learning Non-Unique Segmentation with Reward-Penalty Dice Loss
- **Arxiv ID**: http://arxiv.org/abs/2009.10987v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.10987v1)
- **Published**: 2020-09-23 07:59:49+00:00
- **Updated**: 2020-09-23 07:59:49+00:00
- **Authors**: Jiabo He, Sarah Erfani, Sudanthi Wijewickrema, Stephen O'Leary, Kotagiri Ramamohanarao
- **Comment**: None
- **Journal**: None
- **Summary**: Semantic segmentation is one of the key problems in the field of computer vision, as it enables computer image understanding. However, most research and applications of semantic segmentation focus on addressing unique segmentation problems, where there is only one gold standard segmentation result for every input image. This may not be true in some problems, e.g., medical applications. We may have non-unique segmentation annotations as different surgeons may perform successful surgeries for the same patient in slightly different ways. To comprehensively learn non-unique segmentation tasks, we propose the reward-penalty Dice loss (RPDL) function as the optimization objective for deep convolutional neural networks (DCNN). RPDL is capable of helping DCNN learn non-unique segmentation by enhancing common regions and penalizing outside ones. Experimental results show that RPDL improves the performance of DCNN models by up to 18.4% compared with other loss functions on our collected surgical dataset.



### An Attention Mechanism with Multiple Knowledge Sources for COVID-19 Detection from CT Images
- **Arxiv ID**: http://arxiv.org/abs/2009.11008v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2009.11008v4)
- **Published**: 2020-09-23 09:05:24+00:00
- **Updated**: 2020-12-01 15:16:06+00:00
- **Authors**: Duy M. H. Nguyen, Duy M. Nguyen, Huong Vu, Binh T. Nguyen, Fabrizio Nunnari, Daniel Sonntag
- **Comment**: In AAAI 2021 Workshop: Trustworthy AI for Healthcare
- **Journal**: None
- **Summary**: Until now, Coronavirus SARS-CoV-2 has caused more than 850,000 deaths and infected more than 27 million individuals in over 120 countries. Besides principal polymerase chain reaction (PCR) tests, automatically identifying positive samples based on computed tomography (CT) scans can present a promising option in the early diagnosis of COVID-19. Recently, there have been increasing efforts to utilize deep networks for COVID-19 diagnosis based on CT scans. While these approaches mostly focus on introducing novel architectures, transfer learning techniques, or construction large scale data, we propose a novel strategy to improve the performance of several baselines by leveraging multiple useful information sources relevant to doctors' judgments. Specifically, infected regions and heat maps extracted from learned networks are integrated with the global image via an attention mechanism during the learning process. This procedure not only makes our system more robust to noise but also guides the network focusing on local lesion areas. Extensive experiments illustrate the superior performance of our approach compared to recent baselines. Furthermore, our learned network guidance presents an explainable feature to doctors as we can understand the connection between input and output in a grey-box model.



### Automatic Breast Lesion Classification by Joint Neural Analysis of Mammography and Ultrasound
- **Arxiv ID**: http://arxiv.org/abs/2009.11009v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.11009v1)
- **Published**: 2020-09-23 09:08:24+00:00
- **Updated**: 2020-09-23 09:08:24+00:00
- **Authors**: Gavriel Habib, Nahum Kiryati, Miri Sklair-Levy, Anat Shalmon, Osnat Halshtok Neiman, Renata Faermann Weidenfeld, Yael Yagil, Eli Konen, Arnaldo Mayer
- **Comment**: 10 pages including references, 8 figures, 1 table. Accepted to MICCAI
  ML-CDS 2020 workshop (ML-CDS 2020/CLIP 2020, LNCS 12445 proceedings)
- **Journal**: None
- **Summary**: Mammography and ultrasound are extensively used by radiologists as complementary modalities to achieve better performance in breast cancer diagnosis. However, existing computer-aided diagnosis (CAD) systems for the breast are generally based on a single modality. In this work, we propose a deep-learning based method for classifying breast cancer lesions from their respective mammography and ultrasound images. We present various approaches and show a consistent improvement in performance when utilizing both modalities. The proposed approach is based on a GoogleNet architecture, fine-tuned for our data in two training steps. First, a distinct neural network is trained separately for each modality, generating high-level features. Then, the aggregated features originating from each modality are used to train a multimodal network to provide the final classification. In quantitative experiments, the proposed approach achieves an AUC of 0.94, outperforming state-of-the-art models trained over a single modality. Moreover, it performs similarly to an average radiologist, surpassing two out of four radiologists participating in a reader study. The promising results suggest that the proposed method may become a valuable decision support tool for breast radiologists.



### Generative Model without Prior Distribution Matching
- **Arxiv ID**: http://arxiv.org/abs/2009.11016v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.11016v1)
- **Published**: 2020-09-23 09:33:24+00:00
- **Updated**: 2020-09-23 09:33:24+00:00
- **Authors**: Cong Geng, Jia Wang, Li Chen, Zhiyong Gao
- **Comment**: 8 pages, 8 figures
- **Journal**: None
- **Summary**: Variational Autoencoder (VAE) and its variations are classic generative models by learning a low-dimensional latent representation to satisfy some prior distribution (e.g., Gaussian distribution). Their advantages over GAN are that they can simultaneously generate high dimensional data and learn latent representations to reconstruct the inputs. However, it has been observed that a trade-off exists between reconstruction and generation since matching prior distribution may destroy the geometric structure of data manifold. To mitigate this problem, we propose to let the prior match the embedding distribution rather than imposing the latent variables to fit the prior. The embedding distribution is trained using a simple regularized autoencoder architecture which preserves the geometric structure to the maximum. Then an adversarial strategy is employed to achieve a latent mapping. We provide both theoretical and experimental support for the effectiveness of our method, which alleviates the contradiction between topological properties' preserving of data manifold and distribution matching in latent space.



### Few-shot Font Generation with Localized Style Representations and Factorization
- **Arxiv ID**: http://arxiv.org/abs/2009.11042v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.11042v2)
- **Published**: 2020-09-23 10:33:01+00:00
- **Updated**: 2020-12-16 07:04:49+00:00
- **Authors**: Song Park, Sanghyuk Chun, Junbum Cha, Bado Lee, Hyunjung Shim
- **Comment**: Accepted at AAAI 2021, 12 pages, 11 figures, the first two authors
  contributed equally
- **Journal**: None
- **Summary**: Automatic few-shot font generation is a practical and widely studied problem because manual designs are expensive and sensitive to the expertise of designers. Existing few-shot font generation methods aim to learn to disentangle the style and content element from a few reference glyphs, and mainly focus on a universal style representation for each font style. However, such approach limits the model in representing diverse local styles, and thus makes it unsuitable to the most complicated letter system, e.g., Chinese, whose characters consist of a varying number of components (often called "radical") with a highly complex structure. In this paper, we propose a novel font generation method by learning localized styles, namely component-wise style representations, instead of universal styles. The proposed style representations enable us to synthesize complex local details in text designs. However, learning component-wise styles solely from reference glyphs is infeasible in the few-shot font generation scenario, when a target script has a large number of components, e.g., over 200 for Chinese. To reduce the number of reference glyphs, we simplify component-wise styles by a product of component factor and style factor, inspired by low-rank matrix factorization. Thanks to the combination of strong representation and a compact factorization strategy, our method shows remarkably better few-shot font generation results (with only 8 reference glyph images) than other state-of-the-arts, without utilizing strong locality supervision, e.g., location of each component, skeleton, or strokes. The source code is available at https://github.com/clovaai/lffont.



### Unsupervised Feature Learning for Event Data: Direct vs Inverse Problem Formulation
- **Arxiv ID**: http://arxiv.org/abs/2009.11044v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2009.11044v2)
- **Published**: 2020-09-23 10:40:03+00:00
- **Updated**: 2020-09-30 13:09:32+00:00
- **Authors**: Dimche Kostadinov, Davide Scaramuzza
- **Comment**: None
- **Journal**: IAPR IEEE/Computer Society International Conference on Pattern
  Recognition (ICPR), Milan, 2021
- **Summary**: Event-based cameras record an asynchronous stream of per-pixel brightness changes. As such, they have numerous advantages over the standard frame-based cameras, including high temporal resolution, high dynamic range, and no motion blur. Due to the asynchronous nature, efficient learning of compact representation for event data is challenging. While it remains not explored the extent to which the spatial and temporal event "information" is useful for pattern recognition tasks. In this paper, we focus on single-layer architectures. We analyze the performance of two general problem formulations: the direct and the inverse, for unsupervised feature learning from local event data (local volumes of events described in space-time). We identify and show the main advantages of each approach. Theoretically, we analyze guarantees for an optimal solution, possibility for asynchronous, parallel parameter update, and the computational complexity. We present numerical experiments for object recognition. We evaluate the solution under the direct and the inverse problem and give a comparison with the state-of-the-art methods. Our empirical results highlight the advantages of both approaches for representation learning from event data. We show improvements of up to 9 % in the recognition accuracy compared to the state-of-the-art methods from the same class of methods.



### Robust and efficient post-processing for video object detection
- **Arxiv ID**: http://arxiv.org/abs/2009.11050v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.11050v1)
- **Published**: 2020-09-23 10:47:24+00:00
- **Updated**: 2020-09-23 10:47:24+00:00
- **Authors**: Alberto Sabater, Luis Montesano, Ana C. Murillo
- **Comment**: Submitted to the International Conference on Intelligent Robots and
  Systems, IROS 2020
- **Journal**: None
- **Summary**: Object recognition in video is an important task for plenty of applications, including autonomous driving perception, surveillance tasks, wearable devices or IoT networks. Object recognition using video data is more challenging than using still images due to blur, occlusions or rare object poses. Specific video detectors with high computational cost or standard image detectors together with a fast post-processing algorithm achieve the current state-of-the-art. This work introduces a novel post-processing pipeline that overcomes some of the limitations of previous post-processing methods by introducing a learning-based similarity evaluation between detections across frames. Our method improves the results of state-of-the-art specific video detectors, specially regarding fast moving objects, and presents low resource requirements. And applied to efficient still image detectors, such as YOLO, provides comparable results to much more computationally intensive detectors.



### GSR-Net: Graph Super-Resolution Network for Predicting High-Resolution from Low-Resolution Functional Brain Connectomes
- **Arxiv ID**: http://arxiv.org/abs/2009.11080v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.11080v1)
- **Published**: 2020-09-23 12:02:55+00:00
- **Updated**: 2020-09-23 12:02:55+00:00
- **Authors**: Megi Isallari, Islem Rekik
- **Comment**: None
- **Journal**: None
- **Summary**: Catchy but rigorous deep learning architectures were tailored for image super-resolution (SR), however, these fail to generalize to non-Euclidean data such as brain connectomes. Specifically, building generative models for super-resolving a low-resolution (LR) brain connectome at a higher resolution (HR) (i.e., adding new graph nodes/edges) remains unexplored although this would circumvent the need for costly data collection and manual labelling of anatomical brain regions (i.e. parcellation). To fill this gap, we introduce GSR-Net (Graph Super-Resolution Network), the first super-resolution framework operating on graph-structured data that generates high-resolution brain graphs from low-resolution graphs. First, we adopt a U-Net like architecture based on graph convolution, pooling and unpooling operations specific to non-Euclidean data. However, unlike conventional U-Nets where graph nodes represent samples and node features are mapped to a low-dimensional space (encoding and decoding node attributes or sample features), our GSR-Net operates directly on a single connectome: a fully connected graph where conventionally, a node denotes a brain region, nodes have no features, and edge weights denote brain connectivity strength between two regions of interest (ROIs). In the absence of original node features, we initially assign identity feature vectors to each brain ROI (node) and then leverage the learned local receptive fields to learn node feature representations. Second, inspired by spectral theory, we break the symmetry of the U-Net architecture by topping it up with a graph super-resolution (GSR) layer and two graph convolutional network layers to predict a HR graph while preserving the characteristics of the LR input. Our proposed GSR-Net framework outperformed its variants for predicting high-resolution brain functional connectomes from low-resolution connectomes.



### Multiplexed Illumination for Classifying Visually Similar Objects
- **Arxiv ID**: http://arxiv.org/abs/2009.11084v1
- **DOI**: 10.1364/AO.414184
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.11084v1)
- **Published**: 2020-09-23 12:10:06+00:00
- **Updated**: 2020-09-23 12:10:06+00:00
- **Authors**: Taihua Wang, Donald G. Dansereau
- **Comment**: Submitted to Computer Vision and Image Understanding (CVIU)
- **Journal**: None
- **Summary**: Distinguishing visually similar objects like forged/authentic bills and healthy/unhealthy plants is beyond the capabilities of even the most sophisticated classifiers. We propose the use of multiplexed illumination to extend the range of objects that can be successfully classified. We construct a compact RGB-IR light stage that images samples under different combinations of illuminant position and colour. We then develop a methodology for selecting illumination patterns and training a classifier using the resulting imagery. We use the light stage to model and synthetically relight training samples, and propose a greedy pattern selection scheme that exploits this ability to train in simulation. We then apply the trained patterns to carry out fast classification of new objects. We demonstrate the approach on visually similar artificial and real fruit samples, showing a marked improvement compared with fixed-illuminant approaches as well as a more conventional code selection scheme. This work allows fast classification of previously indistinguishable objects, with potential applications in forgery detection, quality control in agriculture and manufacturing, and skin lesion classification.



### Robustification of Segmentation Models Against Adversarial Perturbations In Medical Imaging
- **Arxiv ID**: http://arxiv.org/abs/2009.11090v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.11090v1)
- **Published**: 2020-09-23 12:18:05+00:00
- **Updated**: 2020-09-23 12:18:05+00:00
- **Authors**: Hanwool Park, Amirhossein Bayat, Mohammad Sabokrou, Jan S. Kirschke, Bjoern H. Menze
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a novel yet efficient defense framework for segmentation models against adversarial attacks in medical imaging. In contrary to the defense methods against adversarial attacks for classification models which widely are investigated, such defense methods for segmentation models has been less explored. Our proposed method can be used for any deep learning models without revising the target deep learning models, as well as can be independent of adversarial attacks. Our framework consists of a frequency domain converter, a detector, and a reformer. The frequency domain converter helps the detector detects adversarial examples by using a frame domain of an image. The reformer helps target models to predict more precisely. We have experiments to empirically show that our proposed method has a better performance compared to the existing defense method.



### Eye Movement Feature Classification for Soccer Goalkeeper Expertise Identification in Virtual Reality
- **Arxiv ID**: http://arxiv.org/abs/2009.11676v2
- **DOI**: None
- **Categories**: **cs.HC**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.11676v2)
- **Published**: 2020-09-23 12:18:41+00:00
- **Updated**: 2021-01-06 17:22:41+00:00
- **Authors**: Benedikt Hosp, Florian Schultz, Oliver Höner, Enkelejda Kasneci
- **Comment**: None
- **Journal**: None
- **Summary**: The latest research in expertise assessment of soccer players has affirmed the importance of perceptual skills (especially for decision making) by focusing either on high experimental control or on a realistic presentation. To assess the perceptual skills of athletes in an optimized manner, we captured omnidirectional in-field scenes and showed these to 12 expert, 10 intermediate and 13 novice soccer goalkeepers on virtual reality glasses. All scenes were shown from the same natural goalkeeper perspective and ended after the return pass to the goalkeeper. Based on their gaze behavior we classified their expertise with common machine learning techniques. This pilot study shows promising results for objective classification of goalkeepers expertise based on their gaze behaviour and provided valuable insight to inform the design of training systems to enhance perceptual skills of athletes.



### Residual Embedding Similarity-Based Network Selection for Predicting Brain Network Evolution Trajectory from a Single Observation
- **Arxiv ID**: http://arxiv.org/abs/2009.11110v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.11110v1)
- **Published**: 2020-09-23 12:40:04+00:00
- **Updated**: 2020-09-23 12:40:04+00:00
- **Authors**: Ahmet Serkan Goktas, Alaa Bessadok, Islem Rekik
- **Comment**: None
- **Journal**: None
- **Summary**: While existing predictive frameworks are able to handle Euclidean structured data (i.e, brain images), they might fail to generalize to geometric non-Euclidean data such as brain networks. Besides, these are rooted the sample selection step in using Euclidean or learned similarity measure between vectorized training and testing brain networks. Such sample connectomic representation might include irrelevant and redundant features that could mislead the training sample selection step. Undoubtedly, this fails to exploit and preserve the topology of the brain connectome. To overcome this major drawback, we propose Residual Embedding Similarity-Based Network selection (RESNets) for predicting brain network evolution trajectory from a single timepoint. RESNets first learns a compact geometric embedding of each training and testing sample using adversarial connectome embedding network. This nicely reduces the high-dimensionality of brain networks while preserving their topological properties via graph convolutional networks. Next, to compute the similarity between subjects, we introduce the concept of a connectional brain template (CBT), a fixed network reference, where we further represent each training and testing network as a deviation from the reference CBT in the embedding space. As such, we select the most similar training subjects to the testing subject at baseline by comparing their learned residual embeddings with respect to the pre-defined CBT. Once the best training samples are selected at baseline, we simply average their corresponding brain networks at follow-up timepoints to predict the evolution trajectory of the testing network. Our experiments on both healthy and disordered brain networks demonstrate the success of our proposed method in comparison to RESNets ablated versions and traditional approaches.



### Multiple interaction learning with question-type prior knowledge for constraining answer search space in visual question answering
- **Arxiv ID**: http://arxiv.org/abs/2009.11118v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.11118v1)
- **Published**: 2020-09-23 12:54:34+00:00
- **Updated**: 2020-09-23 12:54:34+00:00
- **Authors**: Tuong Do, Binh X. Nguyen, Huy Tran, Erman Tjiputra, Quang D. Tran, Thanh-Toan Do
- **Comment**: Accepted in ECCV Workshop 2020
- **Journal**: None
- **Summary**: Different approaches have been proposed to Visual Question Answering (VQA). However, few works are aware of the behaviors of varying joint modality methods over question type prior knowledge extracted from data in constraining answer search space, of which information gives a reliable cue to reason about answers for questions asked in input images. In this paper, we propose a novel VQA model that utilizes the question-type prior information to improve VQA by leveraging the multiple interactions between different joint modality methods based on their behaviors in answering questions from different types. The solid experiments on two benchmark datasets, i.e., VQA 2.0 and TDIUC, indicate that the proposed method yields the best performance with the most competitive approaches.



### Anisotropic 3D Multi-Stream CNN for Accurate Prostate Segmentation from Multi-Planar MRI
- **Arxiv ID**: http://arxiv.org/abs/2009.11120v2
- **DOI**: 10.1016/j.cmpb.2020.105821
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.11120v2)
- **Published**: 2020-09-23 12:56:14+00:00
- **Updated**: 2020-12-02 13:01:03+00:00
- **Authors**: Anneke Meyer, Grzegorz Chlebus, Marko Rak, Daniel Schindele, Martin Schostak, Bram van Ginneken, Andrea Schenk, Hans Meine, Horst K. Hahn, Andreas Schreiber, Christian Hansen
- **Comment**: Accepted manuscript in Elsevier Computer Methods and Programs in
  Biomedicine. Anneke Meyer and Grzegorz Chlebus contributed equally to this
  work. Sourcecode available at
  https://github.com/AnnekeMeyer/AnisotropicMultiStreamCNN. Data available at
  https://doi.org/10.7937/TCIA.2019.DEG7ZG1U
- **Journal**: None
- **Summary**: Background and Objective: Accurate and reliable segmentation of the prostate gland in MR images can support the clinical assessment of prostate cancer, as well as the planning and monitoring of focal and loco-regional therapeutic interventions. Despite the availability of multi-planar MR scans due to standardized protocols, the majority of segmentation approaches presented in the literature consider the axial scans only. Methods: We propose an anisotropic 3D multi-stream CNN architecture, which processes additional scan directions to produce a higher-resolution isotropic prostate segmentation. We investigate two variants of our architecture, which work on two (dual-plane) and three (triple-plane) image orientations, respectively. We compare them with the standard baseline (single-plane) used in literature, i.e., plain axial segmentation. To realize a fair comparison, we employ a hyperparameter optimization strategy to select optimal configurations for the individual approaches. Results: Training and evaluation on two datasets spanning multiple sites obtain statistical significant improvement over the plain axial segmentation ($p<0.05$ on the Dice similarity coefficient). The improvement can be observed especially at the base ($0.898$ single-plane vs. $0.906$ triple-plane) and apex ($0.888$ single-plane vs. $0.901$ dual-plane). Conclusion: This study indicates that models employing two or three scan directions are superior to plain axial segmentation. The knowledge of precise boundaries of the prostate is crucial for the conservation of risk structures. Thus, the proposed models have the potential to improve the outcome of prostate cancer diagnosis and therapies.



### Information-Theoretic Visual Explanation for Black-Box Classifiers
- **Arxiv ID**: http://arxiv.org/abs/2009.11150v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.11150v2)
- **Published**: 2020-09-23 13:51:16+00:00
- **Updated**: 2021-07-16 07:40:24+00:00
- **Authors**: Jihun Yi, Eunji Kim, Siwon Kim, Sungroh Yoon
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we attempt to explain the prediction of any black-box classifier from an information-theoretic perspective. For each input feature, we compare the classifier outputs with and without that feature using two information-theoretic metrics. Accordingly, we obtain two attribution maps--an information gain (IG) map and a point-wise mutual information (PMI) map. IG map provides a class-independent answer to "How informative is each pixel?", and PMI map offers a class-specific explanation of "How much does each pixel support a specific class?" Compared to existing methods, our method improves the correctness of the attribution maps in terms of a quantitative metric. We also provide a detailed analysis of an ImageNet classifier using the proposed method, and the code is available online.



### 2D-3D Geometric Fusion Network using Multi-Neighbourhood Graph Convolution for RGB-D Indoor Scene Classification
- **Arxiv ID**: http://arxiv.org/abs/2009.11154v3
- **DOI**: 10.1016/j.inffus.2021.05.002
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2009.11154v3)
- **Published**: 2020-09-23 13:58:12+00:00
- **Updated**: 2021-05-27 10:06:33+00:00
- **Authors**: Albert Mosella-Montoro, Javier Ruiz-Hidalgo
- **Comment**: Information Fusion 2021
- **Journal**: None
- **Summary**: Multi-modal fusion has been proved to help enhance the performance of scene classification tasks. This paper presents a 2D-3D Fusion stage that combines 3D Geometric Features with 2D Texture Features obtained by 2D Convolutional Neural Networks. To get a robust 3D Geometric embedding, a network that uses two novel layers is proposed. The first layer, Multi-Neighbourhood Graph Convolution, aims to learn a more robust geometric descriptor of the scene combining two different neighbourhoods: one in the Euclidean space and the other in the Feature space. The second proposed layer, Nearest Voxel Pooling, improves the performance of the well-known Voxel Pooling. Experimental results, using NYU-Depth-V2 and SUN RGB-D datasets, show that the proposed method outperforms the current state-of-the-art in RGB-D indoor scene classification task.



### Label-Efficient Multi-Task Segmentation using Contrastive Learning
- **Arxiv ID**: http://arxiv.org/abs/2009.11160v1
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.TO
- **Links**: [PDF](http://arxiv.org/pdf/2009.11160v1)
- **Published**: 2020-09-23 14:12:17+00:00
- **Updated**: 2020-09-23 14:12:17+00:00
- **Authors**: Junichiro Iwasawa, Yuichiro Hirano, Yohei Sugawara
- **Comment**: Accepted to MICCAI BrainLes 2020 workshop
- **Journal**: None
- **Summary**: Obtaining annotations for 3D medical images is expensive and time-consuming, despite its importance for automating segmentation tasks. Although multi-task learning is considered an effective method for training segmentation models using small amounts of annotated data, a systematic understanding of various subtasks is still lacking. In this study, we propose a multi-task segmentation model with a contrastive learning based subtask and compare its performance with other multi-task models, varying the number of labeled data for training. We further extend our model so that it can utilize unlabeled data through the regularization branch in a semi-supervised manner. We experimentally show that our proposed method outperforms other multi-task methods including the state-of-the-art fully supervised model when the amount of annotated data is limited.



### Foreseeing Brain Graph Evolution Over Time Using Deep Adversarial Network Normalizer
- **Arxiv ID**: http://arxiv.org/abs/2009.11166v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.11166v1)
- **Published**: 2020-09-23 14:25:40+00:00
- **Updated**: 2020-09-23 14:25:40+00:00
- **Authors**: Zeynep Gurler, Ahmed Nebli, Islem Rekik
- **Comment**: None
- **Journal**: None
- **Summary**: Foreseeing the brain evolution as a complex highly inter-connected system, widely modeled as a graph, is crucial for mapping dynamic interactions between different anatomical regions of interest (ROIs) in health and disease. Interestingly, brain graph evolution models remain almost absent in the literature. Here we design an adversarial brain network normalizer for representing each brain network as a transformation of a fixed centered population-driven connectional template. Such graph normalization with respect to a fixed reference paves the way for reliably identifying the most similar training samples (i.e., brain graphs) to the testing sample at baseline timepoint. The testing evolution trajectory will be then spanned by the selected training graphs and their corresponding evolution trajectories. We base our prediction framework on geometric deep learning which naturally operates on graphs and nicely preserves their topological properties. Specifically, we propose the first graph-based Generative Adversarial Network (gGAN) that not only learns how to normalize brain graphs with respect to a fixed connectional brain template (CBT) (i.e., a brain template that selectively captures the most common features across a brain population) but also learns a high-order representation of the brain graphs also called embeddings. We use these embeddings to compute the similarity between training and testing subjects which allows us to pick the closest training subjects at baseline timepoint to predict the evolution of the testing brain graph over time. A series of benchmarks against several comparison methods showed that our proposed method achieved the lowest brain disease evolution prediction error using a single baseline timepoint. Our gGAN code is available at http://github.com/basiralab/gGAN.



### Whole Slide Images based Cancer Survival Prediction using Attention Guided Deep Multiple Instance Learning Networks
- **Arxiv ID**: http://arxiv.org/abs/2009.11169v1
- **DOI**: 10.1016/j.media.2020.101789
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.11169v1)
- **Published**: 2020-09-23 14:31:15+00:00
- **Updated**: 2020-09-23 14:31:15+00:00
- **Authors**: Jiawen Yao, Xinliang Zhu, Jitendra Jonnagaddala, Nicholas Hawkins, Junzhou Huang
- **Comment**: 22 pages, 13 figures, published in Medical Image Analysis 65, 101789
- **Journal**: Medical Image Analysis, Volume 65, October 2020, 101789
- **Summary**: Traditional image-based survival prediction models rely on discriminative patch labeling which make those methods not scalable to extend to large datasets. Recent studies have shown Multiple Instance Learning (MIL) framework is useful for histopathological images when no annotations are available in classification task. Different to the current image-based survival models that limit to key patches or clusters derived from Whole Slide Images (WSIs), we propose Deep Attention Multiple Instance Survival Learning (DeepAttnMISL) by introducing both siamese MI-FCN and attention-based MIL pooling to efficiently learn imaging features from the WSI and then aggregate WSI-level information to patient-level. Attention-based aggregation is more flexible and adaptive than aggregation techniques in recent survival models. We evaluated our methods on two large cancer whole slide images datasets and our results suggest that the proposed approach is more effective and suitable for large datasets and has better interpretability in locating important patterns and features that contribute to accurate cancer survival predictions. The proposed framework can also be used to assess individual patient's risk and thus assisting in delivering personalized medicine. Codes are available at https://github.com/uta-smile/DeepAttnMISL_MEDIA.



### Compressive spectral image classification using 3D coded convolutional neural network
- **Arxiv ID**: http://arxiv.org/abs/2009.11948v3
- **DOI**: 10.1364/OE.437717
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.11948v3)
- **Published**: 2020-09-23 15:05:57+00:00
- **Updated**: 2021-07-12 09:25:22+00:00
- **Authors**: Hao Zhang, Xu Ma, Xianhong Zhao, Gonzalo R. Arce
- **Comment**: None
- **Journal**: None
- **Summary**: Hyperspectral image classification (HIC) is an active research topic in remote sensing. Hyperspectral images typically generate large data cubes posing big challenges in data acquisition, storage, transmission and processing. To overcome these limitations, this paper develops a novel deep learning HIC approach based on compressive measurements of coded-aperture snapshot spectral imagers (CASSI), without reconstructing the complete hyperspectral data cube. A new kind of deep learning strategy, namely 3D coded convolutional neural network (3D-CCNN) is proposed to efficiently solve for the classification problem, where the hardware-based coded aperture is regarded as a pixel-wise connected network layer. An end-to-end training method is developed to jointly optimize the network parameters and the coded apertures with periodic structures. The accuracy of classification is effectively improved by exploiting the synergy between the deep learning network and coded apertures. The superiority of the proposed method is assessed over the state-of-the-art HIC methods on several hyperspectral datasets.



### Learning Visual Voice Activity Detection with an Automatically Annotated Dataset
- **Arxiv ID**: http://arxiv.org/abs/2009.11204v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.11204v2)
- **Published**: 2020-09-23 15:12:24+00:00
- **Updated**: 2020-10-16 15:08:12+00:00
- **Authors**: Sylvain Guy, Stéphane Lathuilière, Pablo Mesejo, Radu Horaud
- **Comment**: International Conference on Pattern Recognition, Milan, Italy,
  January 2021
- **Journal**: None
- **Summary**: Visual voice activity detection (V-VAD) uses visual features to predict whether a person is speaking or not. V-VAD is useful whenever audio VAD (A-VAD) is inefficient either because the acoustic signal is difficult to analyze or because it is simply missing. We propose two deep architectures for V-VAD, one based on facial landmarks and one based on optical flow. Moreover, available datasets, used for learning and for testing V-VAD, lack content variability. We introduce a novel methodology to automatically create and annotate very large datasets in-the-wild -- WildVVAD -- based on combining A-VAD with face detection and tracking. A thorough empirical evaluation shows the advantage of training the proposed deep V-VAD models with this dataset.



### A Simple Yet Effective Method for Video Temporal Grounding with Cross-Modality Attention
- **Arxiv ID**: http://arxiv.org/abs/2009.11232v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.11232v1)
- **Published**: 2020-09-23 16:03:00+00:00
- **Updated**: 2020-09-23 16:03:00+00:00
- **Authors**: Binjie Zhang, Yu Li, Chun Yuan, Dejing Xu, Pin Jiang, Ying Shan
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: The task of language-guided video temporal grounding is to localize the particular video clip corresponding to a query sentence in an untrimmed video. Though progress has been made continuously in this field, some issues still need to be resolved. First, most of the existing methods rely on the combination of multiple complicated modules to solve the task. Second, due to the semantic gaps between the two different modalities, aligning the information at different granularities (local and global) between the video and the language is significant, which is less addressed. Last, previous works do not consider the inevitable annotation bias due to the ambiguities of action boundaries. To address these limitations, we propose a simple two-branch Cross-Modality Attention (CMA) module with intuitive structure design, which alternatively modulates two modalities for better matching the information both locally and globally. In addition, we introduce a new task-specific regression loss function, which improves the temporal grounding accuracy by alleviating the impact of annotation bias. We conduct extensive experiments to validate our method, and the results show that just with this simple model, it can outperform the state of the arts on both Charades-STA and ActivityNet Captions datasets.



### Interactive Learning for Semantic Segmentation in Earth Observation
- **Arxiv ID**: http://arxiv.org/abs/2009.11250v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.11250v1)
- **Published**: 2020-09-23 16:57:28+00:00
- **Updated**: 2020-09-23 16:57:28+00:00
- **Authors**: Gaston Lenczner, Adrien Chan-Hon-Tong, Nicola Luminari, Bertrand Le Saux, Guy Le Besnerais
- **Comment**: 8 pages, 4 Figures, ECML-PKDD Workshop MACLEAN
- **Journal**: None
- **Summary**: Dense pixel-wise classification maps output by deep neural networks are of extreme importance for scene understanding. However, these maps are often partially inaccurate due to a variety of possible factors. Therefore, we propose to interactively refine them within a framework named DISCA (Deep Image Segmentation with Continual Adaptation). It consists of continually adapting a neural network to a target image using an interactive learning process with sparse user annotations as ground-truth. We show through experiments on three datasets using synthesized annotations the benefits of the approach, reaching an IoU improvement up to 4.7% for ten sampled clicks. Finally, we exhibit that our approach can be particularly rewarding when it is faced to additional issues such as domain adaptation.



### Fuzzy Simplicial Networks: A Topology-Inspired Model to Improve Task Generalization in Few-shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2009.11253v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, math.GN, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2009.11253v1)
- **Published**: 2020-09-23 17:01:09+00:00
- **Updated**: 2020-09-23 17:01:09+00:00
- **Authors**: Henry Kvinge, Zachary New, Nico Courts, Jung H. Lee, Lauren A. Phillips, Courtney D. Corley, Aaron Tuor, Andrew Avila, Nathan O. Hodas
- **Comment**: 17 pages
- **Journal**: None
- **Summary**: Deep learning has shown great success in settings with massive amounts of data but has struggled when data is limited. Few-shot learning algorithms, which seek to address this limitation, are designed to generalize well to new tasks with limited data. Typically, models are evaluated on unseen classes and datasets that are defined by the same fundamental task as they are trained for (e.g. category membership). One can also ask how well a model can generalize to fundamentally different tasks within a fixed dataset (for example: moving from category membership to tasks that involve detecting object orientation or quantity). To formalize this kind of shift we define a notion of "independence of tasks" and identify three new sets of labels for established computer vision datasets that test a model's ability to generalize to tasks which draw on orthogonal attributes in the data. We use these datasets to investigate the failure modes of metric-based few-shot models. Based on our findings, we introduce a new few-shot model called Fuzzy Simplicial Networks (FSN) which leverages a construction from topology to more flexibly represent each class from limited data. In particular, FSN models can not only form multiple representations for a given class but can also begin to capture the low-dimensional structure which characterizes class manifolds in the encoded space of deep networks. We show that FSN outperforms state-of-the-art models on the challenging tasks we introduce in this paper while remaining competitive on standard few-shot benchmarks.



### A Linear Transportation $\mathrm{L}^p$ Distance for Pattern Recognition
- **Arxiv ID**: http://arxiv.org/abs/2009.11262v1
- **DOI**: None
- **Categories**: **cs.CV**, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/2009.11262v1)
- **Published**: 2020-09-23 17:19:19+00:00
- **Updated**: 2020-09-23 17:19:19+00:00
- **Authors**: Oliver M. Crook, Mihai Cucuringu, Tim Hurst, Carola-Bibiane Schönlieb, Matthew Thorpe, Konstantinos C. Zygalakis
- **Comment**: None
- **Journal**: None
- **Summary**: The transportation $\mathrm{L}^p$ distance, denoted $\mathrm{TL}^p$, has been proposed as a generalisation of Wasserstein $\mathrm{W}^p$ distances motivated by the property that it can be applied directly to colour or multi-channelled images, as well as multivariate time-series without normalisation or mass constraints. These distances, as with $\mathrm{W}^p$, are powerful tools in modelling data with spatial or temporal perturbations. However, their computational cost can make them infeasible to apply to even moderate pattern recognition tasks. We propose linear versions of these distances and show that the linear $\mathrm{TL}^p$ distance significantly improves over the linear $\mathrm{W}^p$ distance on signal processing tasks, whilst being several orders of magnitude faster to compute than the $\mathrm{TL}^p$ distance.



### X-LXMERT: Paint, Caption and Answer Questions with Multi-Modal Transformers
- **Arxiv ID**: http://arxiv.org/abs/2009.11278v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.11278v1)
- **Published**: 2020-09-23 17:45:17+00:00
- **Updated**: 2020-09-23 17:45:17+00:00
- **Authors**: Jaemin Cho, Jiasen Lu, Dustin Schwenk, Hannaneh Hajishirzi, Aniruddha Kembhavi
- **Comment**: EMNLP 2020
- **Journal**: None
- **Summary**: Mirroring the success of masked language models, vision-and-language counterparts like ViLBERT, LXMERT and UNITER have achieved state of the art performance on a variety of multimodal discriminative tasks like visual question answering and visual grounding. Recent work has also successfully adapted such models towards the generative task of image captioning. This begs the question: Can these models go the other way and generate images from pieces of text? Our analysis of a popular representative from this model family - LXMERT - finds that it is unable to generate rich and semantically meaningful imagery with its current training setup. We introduce X-LXMERT, an extension to LXMERT with training refinements including: discretizing visual representations, using uniform masking with a large range of masking ratios and aligning the right pre-training datasets to the right objectives which enables it to paint. X-LXMERT's image generation capabilities rival state of the art generative models while its question answering and captioning abilities remains comparable to LXMERT. Finally, we demonstrate the generality of these training refinements by adding image generation capabilities into UNITER to produce X-UNITER.



### Augmented Convolutional LSTMs for Generation of High-Resolution Climate Change Projections
- **Arxiv ID**: http://arxiv.org/abs/2009.11279v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, physics.ao-ph
- **Links**: [PDF](http://arxiv.org/pdf/2009.11279v1)
- **Published**: 2020-09-23 17:52:09+00:00
- **Updated**: 2020-09-23 17:52:09+00:00
- **Authors**: Nidhin Harilal, Udit Bhatia, Mayank Singh
- **Comment**: None
- **Journal**: None
- **Summary**: Projection of changes in extreme indices of climate variables such as temperature and precipitation are critical to assess the potential impacts of climate change on human-made and natural systems, including critical infrastructures and ecosystems. While impact assessment and adaptation planning rely on high-resolution projections (typically in the order of a few kilometers), state-of-the-art Earth System Models (ESMs) are available at spatial resolutions of few hundreds of kilometers. Current solutions to obtain high-resolution projections of ESMs include downscaling approaches that consider the information at a coarse-scale to make predictions at local scales. Complex and non-linear interdependence among local climate variables (e.g., temperature and precipitation) and large-scale predictors (e.g., pressure fields) motivate the use of neural network-based super-resolution architectures. In this work, we present auxiliary variables informed spatio-temporal neural architecture for statistical downscaling. The current study performs daily downscaling of precipitation variable from an ESM output at 1.15 degrees (~115 km) to 0.25 degrees (25 km) over the world's most climatically diversified country, India. We showcase significant improvement gain against three popular state-of-the-art baselines with a better ability to predict extreme events. To facilitate reproducible research, we make available all the codes, processed datasets, and trained models in the public domain.



### Generative Modelling of 3D in-silico Spongiosa with Controllable Micro-Structural Parameters
- **Arxiv ID**: http://arxiv.org/abs/2009.11327v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.11327v1)
- **Published**: 2020-09-23 18:11:47+00:00
- **Updated**: 2020-09-23 18:11:47+00:00
- **Authors**: Emmanuel Iarussi, Felix Thomsen, Claudio Delrieux
- **Comment**: Accepted for publication in MICCAI 2020 conference
- **Journal**: None
- **Summary**: Research in vertebral bone micro-structure generally requires costly procedures to obtain physical scans of real bone with a specific pathology under study, since no methods are available yet to generate realistic bone structures in-silico. Here we propose to apply recent advances in generative adversarial networks (GANs) to develop such a method. We adapted style-transfer techniques, which have been largely used in other contexts, in order to transfer style between image pairs while preserving its informational content. In a first step, we trained a volumetric generative model in a progressive manner using a Wasserstein objective and gradient penalty (PWGAN-GP) to create patches of realistic bone structure in-silico. The training set contained 7660 purely spongeous bone samples from twelve human vertebrae (T12 or L1) with isotropic resolution of 164um and scanned with a high resolution peripheral quantitative CT (Scanco XCT). After training, we generated new samples with tailored micro-structure properties by optimizing a vector z in the learned latent space. To solve this optimization problem, we formulated a differentiable goal function that leads to valid samples while compromising the appearance (content) with target 3D properties (style). Properties of the learned latent space effectively matched the data distribution. Furthermore, we were able to simulate the resulting bone structure after deterioration or treatment effects of osteoporosis therapies based only on expected changes of micro-structural parameters. Our method allows to generate a virtually infinite number of patches of realistic bone micro-structure, and thereby likely serves for the development of bone-biomarkers and to simulate bone therapies in advance.



### Insights on Evaluation of Camera Re-localization Using Relative Pose Regression
- **Arxiv ID**: http://arxiv.org/abs/2009.11342v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2009.11342v1)
- **Published**: 2020-09-23 19:16:26+00:00
- **Updated**: 2020-09-23 19:16:26+00:00
- **Authors**: Amir Shalev, Omer Achrack, Brian Fulkerson, Ben-Zion Bobrovsky
- **Comment**: Accepted at ECCV 2020 joint workshop of UAVision and VisDrone
- **Journal**: None
- **Summary**: We consider the problem of relative pose regression in visual relocalization. Recently, several promising approaches have emerged in this area. We claim that even though they demonstrate on the same datasets using the same split to train and test, a faithful comparison between them was not available since on currently used evaluation metric, some approaches might perform favorably, while in reality performing worse. We reveal a tradeoff between accuracy and the 3D volume of the regressed subspace. We believe that unlike other relocalization approaches, in the case of relative pose regression, the regressed subspace 3D volume is less dependent on the scene and more affect by the method used to score the overlap, which determined how closely sampled viewpoints are. We propose three new metrics to remedy the issue mentioned above. The proposed metrics incorporate statistics about the regression subspace volume. We also propose a new pose regression network that serves as a new baseline for this task. We compare the performance of our trained model on Microsoft 7-Scenes and Cambridge Landmarks datasets both with the standard metrics and the newly proposed metrics and adjust the overlap score to reveal the tradeoff between the subspace and performance. The results show that the proposed metrics are more robust to different overlap threshold than the conventional approaches. Finally, we show that our network generalizes well, specifically, training on a single scene leads to little loss of performance on the other scenes.



### Dense Forecasting of Wildfire Smoke Particulate Matter Using Sparsity Invariant Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2009.11362v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.11362v1)
- **Published**: 2020-09-23 20:13:35+00:00
- **Updated**: 2020-09-23 20:13:35+00:00
- **Authors**: Renhao Wang, Ashutosh Bhudia, Brandon Dos Remedios, Minnie Teng, Raymond Ng
- **Comment**: Submitted to the 2020 NeurIPS Workshop on Machine learning in Public
  Health
- **Journal**: None
- **Summary**: Accurate forecasts of fine particulate matter (PM 2.5) from wildfire smoke are crucial to safeguarding cardiopulmonary public health. Existing forecasting systems are trained on sparse and inaccurate ground truths, and do not take sufficient advantage of important spatial inductive biases. In this work, we present a convolutional neural network which preserves sparsity invariance throughout, and leverages multitask learning to perform dense forecasts of PM 2.5values. We demonstrate that our model outperforms two existing smoke forecasting systems during the 2018 and 2019 wildfire season in British Columbia, Canada, predicting PM 2.5 at a grid resolution of 10 km, 24 hours in advance with high fidelity. Most interestingly, our model also generalizes to meaningful smoke dispersion patterns despite training with irregularly distributed ground truth PM 2.5 values available in only 0.5% of grid cells.



### Place Recognition in Forests with Urquhart Tessellations
- **Arxiv ID**: http://arxiv.org/abs/2010.03026v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2010.03026v2)
- **Published**: 2020-09-23 20:19:38+00:00
- **Updated**: 2020-11-16 14:13:38+00:00
- **Authors**: Guilherme V. Nardari, Avraham Cohen, Steven W. Chen, Xu Liu, Vaibhav Arcot, Roseli A. F. Romero, Vijay Kumar
- **Comment**: 9 pages, 6 Figures
- **Journal**: None
- **Summary**: In this letter, we present a novel descriptor based on Urquhart tessellations derived from the position of trees in a forest. We propose a framework that uses these descriptors to detect previously seen observations and landmark correspondences, even with partial overlap and noise. We run loop closure detection experiments in simulation and real-world data map-merging from different flights of an Unmanned Aerial Vehicle (UAV) in a pine tree forest and show that our method outperforms state-of-the-art approaches in accuracy and robustness.



### A deep learning pipeline for identification of motor units in musculoskeletal ultrasound
- **Arxiv ID**: http://arxiv.org/abs/2010.03028v1
- **DOI**: 10.1109/ACCESS.2020.3023495
- **Categories**: **cs.CV**, cs.LG, eess.IV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2010.03028v1)
- **Published**: 2020-09-23 20:44:29+00:00
- **Updated**: 2020-09-23 20:44:29+00:00
- **Authors**: Hazrat Ali, Johannes Umander, Robin Rohlén, Christer Grönlund
- **Comment**: None
- **Journal**: None
- **Summary**: Ultrasound imaging provides information from a large part of the muscle. It has recently been shown that ultrafast ultrasound imaging can be used to record and analyze the mechanical response of individual MUs using blind source separation. In this work, we present an alternative method - a deep learning pipeline - to identify active MUs in ultrasound image sequences, including segmentation of their territories and signal estimation of their mechanical responses (twitch train). We train and evaluate the model using simulated data mimicking the complex activation pattern of tens of activated MUs with overlapping territories and partially synchronized activation patterns. Using a slow fusion approach (based on 3D CNNs), we transform the spatiotemporal image sequence data to 2D representations and apply a deep neural network architecture for segmentation. Next, we employ a second deep neural network architecture for signal estimation. The results show that the proposed pipeline can effectively identify individual MUs, estimate their territories, and estimate their twitch train signal at low contraction forces. The framework can retain spatio-temporal consistencies and information of the mechanical response of MU activity even when the ultrasound image sequences are transformed into a 2D representation for compatibility with more traditional computer vision and image processing techniques. The proposed pipeline is potentially useful to identify simultaneously active MUs in whole muscles in ultrasound image sequences of voluntary skeletal muscle contractions at low force levels.



### Detection of Iterative Adversarial Attacks via Counter Attack
- **Arxiv ID**: http://arxiv.org/abs/2009.11397v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML, 68T45, 62-07
- **Links**: [PDF](http://arxiv.org/pdf/2009.11397v2)
- **Published**: 2020-09-23 21:54:36+00:00
- **Updated**: 2021-03-23 14:21:02+00:00
- **Authors**: Matthias Rottmann, Kira Maag, Mathis Peyron, Natasa Krejic, Hanno Gottschalk
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) have proven to be powerful tools for processing unstructured data. However for high-dimensional data, like images, they are inherently vulnerable to adversarial attacks. Small almost invisible perturbations added to the input can be used to fool DNNs. Various attacks, hardening methods and detection methods have been introduced in recent years. Notoriously, Carlini-Wagner (CW) type attacks computed by iterative minimization belong to those that are most difficult to detect. In this work we outline a mathematical proof that the CW attack can be used as a detector itself. That is, under certain assumptions and in the limit of attack iterations this detector provides asymptotically optimal separation of original and attacked images. In numerical experiments, we experimentally validate this statement and furthermore obtain AUROC values up to 99.73% on CIFAR10 and ImageNet. This is in the upper part of the spectrum of current state-of-the-art detection rates for CW attacks.



