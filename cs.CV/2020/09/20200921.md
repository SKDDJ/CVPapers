# Arxiv Papers in cs.CV on 2020-09-21
### ES Attack: Model Stealing against Deep Neural Networks without Data Hurdles
- **Arxiv ID**: http://arxiv.org/abs/2009.09560v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/2009.09560v2)
- **Published**: 2020-09-21 01:26:06+00:00
- **Updated**: 2022-01-25 20:36:12+00:00
- **Authors**: Xiaoyong Yuan, Leah Ding, Lan Zhang, Xiaolin Li, Dapeng Wu
- **Comment**: accepted to IEEE Transactions on Emerging Topics in Computational
  Intelligence (TETCI)
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) have become the essential components for various commercialized machine learning services, such as Machine Learning as a Service (MLaaS). Recent studies show that machine learning services face severe privacy threats - well-trained DNNs owned by MLaaS providers can be stolen through public APIs, namely model stealing attacks. However, most existing works undervalued the impact of such attacks, where a successful attack has to acquire confidential training data or auxiliary data regarding the victim DNN. In this paper, we propose ES Attack, a novel model stealing attack without any data hurdles. By using heuristically generated synthetic data, ES Attack iteratively trains a substitute model and eventually achieves a functionally equivalent copy of the victim DNN. The experimental results reveal the severity of ES Attack: i) ES Attack successfully steals the victim model without data hurdles, and ES Attack even outperforms most existing model stealing attacks using auxiliary data in terms of model accuracy; ii) most countermeasures are ineffective in defending ES Attack; iii) ES Attack facilitates further attacks relying on the stolen model.



### SSCR: Iterative Language-Based Image Editing via Self-Supervised Counterfactual Reasoning
- **Arxiv ID**: http://arxiv.org/abs/2009.09566v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.09566v2)
- **Published**: 2020-09-21 01:45:58+00:00
- **Updated**: 2020-09-29 00:24:25+00:00
- **Authors**: Tsu-Jui Fu, Xin Eric Wang, Scott Grafton, Miguel Eckstein, William Yang Wang
- **Comment**: EMNLP 2020
- **Journal**: None
- **Summary**: Iterative Language-Based Image Editing (IL-BIE) tasks follow iterative instructions to edit images step by step. Data scarcity is a significant issue for ILBIE as it is challenging to collect large-scale examples of images before and after instruction-based changes. However, humans still accomplish these editing tasks even when presented with an unfamiliar image-instruction pair. Such ability results from counterfactual thinking and the ability to think about alternatives to events that have happened already. In this paper, we introduce a Self-Supervised Counterfactual Reasoning (SSCR) framework that incorporates counterfactual thinking to overcome data scarcity. SSCR allows the model to consider out-of-distribution instructions paired with previous images. With the help of cross-task consistency (CTC), we train these counterfactual instructions in a self-supervised scenario. Extensive results show that SSCR improves the correctness of ILBIE in terms of both object identity and position, establishing a new state of the art (SOTA) on two IBLIE datasets (i-CLEVR and CoDraw). Even with only 50% of the training data, SSCR achieves a comparable result to using complete data.



### Semi-supervised Semantic Segmentation of Prostate and Organs-at-Risk on 3D Pelvic CT Images
- **Arxiv ID**: http://arxiv.org/abs/2009.09571v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2009.09571v4)
- **Published**: 2020-09-21 01:57:23+00:00
- **Updated**: 2021-09-11 19:38:46+00:00
- **Authors**: Zhuangzhuang Zhang, Tianyu Zhao, Hiram Gay, Baozhou Sun, Weixiong Zhang
- **Comment**: 16 pages, 4 figures
- **Journal**: None
- **Summary**: Automated segmentation can assist radiotherapy treatment planning by saving manual contouring efforts and reducing intra-observer and inter-observer variations. The recent development of deep learning approaches has revoluted medical data processing, including semantic segmentation, by dramatically improving performance. However, training effective deep learning models usually require a large amount of high-quality labeled data, which are often costly to collect. We developed a novel semi-supervised adversarial deep learning approach for 3D pelvic CT image semantic segmentation. Unlike supervised deep learning methods, the new approach can utilize both annotated and un-annotated data for training. It generates un-annotated synthetic data by a data augmentation scheme using generative adversarial networks (GANs). We applied the new approach to segmenting multiple organs in male pelvic CT images, where CT images without annotations and GAN-synthesized un-annotated images were used in semi-supervised learning. Experimental results, evaluated by three metrics (Dice similarity coefficient, average Hausdorff distance, and average surface Hausdorff distance), showed that the new method achieved either comparable performance with substantially fewer annotated images or better performance with the same amount of annotated data, outperforming the existing state-of-the-art methods.



### Reconstruct high-resolution multi-focal plane images from a single 2D wide field image
- **Arxiv ID**: http://arxiv.org/abs/2009.09574v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.09574v1)
- **Published**: 2020-09-21 02:09:36+00:00
- **Updated**: 2020-09-21 02:09:36+00:00
- **Authors**: Jiabo Ma, Sibo Liu, Shenghua Cheng, Xiuli Liu, Li Cheng, Shaoqun Zeng
- **Comment**: 9 pages, 4 figures,3 Tables
- **Journal**: None
- **Summary**: High-resolution 3D medical images are important for analysis and diagnosis, but axial scanning to acquire them is very time-consuming. In this paper, we propose a fast end-to-end multi-focal plane imaging network (MFPINet) to reconstruct high-resolution multi-focal plane images from a single 2D low-resolution wild filed image without relying on scanning. To acquire realistic MFP images fast, the proposed MFPINet adopts generative adversarial network framework and the strategies of post-sampling and refocusing all focal planes at one time. We conduct a series experiments on cytology microscopy images and demonstrate that MFPINet performs well on both axial refocusing and horizontal super resolution. Furthermore, MFPINet is approximately 24 times faster than current refocusing methods for reconstructing the same volume images. The proposed method has the potential to greatly increase the speed of high-resolution 3D imaging and expand the application of low-resolution wide-field images.



### Modeling Score Distributions and Continuous Covariates: A Bayesian Approach
- **Arxiv ID**: http://arxiv.org/abs/2009.09583v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2009.09583v1)
- **Published**: 2020-09-21 02:41:20+00:00
- **Updated**: 2020-09-21 02:41:20+00:00
- **Authors**: Mel McCurrie, Hamish Nicholson, Walter J. Scheirer, Samuel Anthony
- **Comment**: None
- **Journal**: None
- **Summary**: Computer Vision practitioners must thoroughly understand their model's performance, but conditional evaluation is complex and error-prone. In biometric verification, model performance over continuous covariates---real-number attributes of images that affect performance---is particularly challenging to study. We develop a generative model of the match and non-match score distributions over continuous covariates and perform inference with modern Bayesian methods. We use mixture models to capture arbitrary distributions and local basis functions to capture non-linear, multivariate trends. Three experiments demonstrate the accuracy and effectiveness of our approach. First, we study the relationship between age and face verification performance and find previous methods may overstate performance and confidence. Second, we study preprocessing for CNNs and find a highly non-linear, multivariate surface of model performance. Our method is accurate and data efficient when evaluated against previous synthetic methods. Third, we demonstrate the novel application of our method to pedestrian tracking and calculate variable thresholds and expected performance while controlling for multiple covariates.



### A Novel Transferability Attention Neural Network Model for EEG Emotion Recognition
- **Arxiv ID**: http://arxiv.org/abs/2009.09585v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2009.09585v1)
- **Published**: 2020-09-21 02:42:30+00:00
- **Updated**: 2020-09-21 02:42:30+00:00
- **Authors**: Yang Li, Boxun Fu, Fu Li, Guangming Shi, Wenming Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: The existed methods for electroencephalograph (EEG) emotion recognition always train the models based on all the EEG samples indistinguishably. However, some of the source (training) samples may lead to a negative influence because they are significant dissimilar with the target (test) samples. So it is necessary to give more attention to the EEG samples with strong transferability rather than forcefully training a classification model by all the samples. Furthermore, for an EEG sample, from the aspect of neuroscience, not all the brain regions of an EEG sample contains emotional information that can transferred to the test data effectively. Even some brain region data will make strong negative effect for learning the emotional classification model. Considering these two issues, in this paper, we propose a transferable attention neural network (TANN) for EEG emotion recognition, which learns the emotional discriminative information by highlighting the transferable EEG brain regions data and samples adaptively through local and global attention mechanism. This can be implemented by measuring the outputs of multiple brain-region-level discriminators and one single sample-level discriminator. We conduct the extensive experiments on three public EEG emotional datasets. The results validate that the proposed model achieves the state-of-the-art performance.



### The DongNiao International Birds 10000 Dataset
- **Arxiv ID**: http://arxiv.org/abs/2010.06454v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.06454v2)
- **Published**: 2020-09-21 04:19:27+00:00
- **Updated**: 2020-11-05 05:52:08+00:00
- **Authors**: Jian Mei, Hao Dong
- **Comment**: None
- **Journal**: None
- **Summary**: DongNiao International Birds 10000 (DIB-10K) is a challenging image dataset which has more than 10 thousand different types of birds. It was created to enable the study of machine learning and also ornithology research. DIB-10K does not own the copyright of these images. It only provides thumbnails of images, in a way similar to ImageNet.



### Improving Ensemble Robustness by Collaboratively Promoting and Demoting Adversarial Robustness
- **Arxiv ID**: http://arxiv.org/abs/2009.09612v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.09612v2)
- **Published**: 2020-09-21 04:54:38+00:00
- **Updated**: 2022-02-04 19:59:48+00:00
- **Authors**: Anh Bui, Trung Le, He Zhao, Paul Montague, Olivier deVel, Tamas Abraham, Dinh Phung
- **Comment**: None
- **Journal**: None
- **Summary**: Ensemble-based adversarial training is a principled approach to achieve robustness against adversarial attacks. An important technique of this approach is to control the transferability of adversarial examples among ensemble members. We propose in this work a simple yet effective strategy to collaborate among committee models of an ensemble model. This is achieved via the secure and insecure sets defined for each model member on a given sample, hence help us to quantify and regularize the transferability. Consequently, our proposed framework provides the flexibility to reduce the adversarial transferability as well as to promote the diversity of ensemble members, which are two crucial factors for better robustness in our ensemble approach. We conduct extensive and comprehensive experiments to demonstrate that our proposed method outperforms the state-of-the-art ensemble baselines, at the same time can detect a wide range of adversarial examples with a nearly perfect accuracy. Our code is available at: https://github.com/tuananhbui89/Crossing-Collaborative-Ensemble.



### 3D-FUTURE: 3D Furniture shape with TextURE
- **Arxiv ID**: http://arxiv.org/abs/2009.09633v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.09633v1)
- **Published**: 2020-09-21 06:26:39+00:00
- **Updated**: 2020-09-21 06:26:39+00:00
- **Authors**: Huan Fu, Rongfei Jia, Lin Gao, Mingming Gong, Binqiang Zhao, Steve Maybank, Dacheng Tao
- **Comment**: Project Page:
  https://tianchi.aliyun.com/specials/promotion/alibaba-3d-future
- **Journal**: None
- **Summary**: The 3D CAD shapes in current 3D benchmarks are mostly collected from online model repositories. Thus, they typically have insufficient geometric details and less informative textures, making them less attractive for comprehensive and subtle research in areas such as high-quality 3D mesh and texture recovery. This paper presents 3D Furniture shape with TextURE (3D-FUTURE): a richly-annotated and large-scale repository of 3D furniture shapes in the household scenario. At the time of this technical report, 3D-FUTURE contains 20,240 clean and realistic synthetic images of 5,000 different rooms. There are 9,992 unique detailed 3D instances of furniture with high-resolution textures. Experienced designers developed the room scenes, and the 3D CAD shapes in the scene are used for industrial production. Given the well-organized 3D-FUTURE, we provide baseline experiments on several widely studied tasks, such as joint 2D instance segmentation and 3D object pose estimation, image-based 3D shape retrieval, 3D object reconstruction from a single image, and texture recovery for 3D shapes, to facilitate related future researches on our database.



### Heuristics based Mosaic of Social-Sensor Services for Scene Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2009.11663v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.11663v1)
- **Published**: 2020-09-21 07:00:50+00:00
- **Updated**: 2020-09-21 07:00:50+00:00
- **Authors**: Tooba Aamir, Hai Dong, Athman Bouguettaya
- **Comment**: 13 pages, 8 figures and 1 table. This is an accepted paper and it is
  going to appear in the Proceedings of the 2020 21st International Conference
  on Web Information Systems Engineering (WISE 2020), Amsterdam and Leiden,
  Netherlands. The proceedings of WISE 2020 will be published by Springer in
  its Lecture Notes in Computer Science series
- **Journal**: None
- **Summary**: We propose a heuristics-based social-sensor cloud service selection and composition model to reconstruct mosaic scenes. The proposed approach leverages crowdsourced social media images to create an image mosaic to reconstruct a scene at a designated location and an interval of time. The novel approach relies on the set of features defined on the bases of the image metadata to determine the relevance and composability of services. Novel heuristics are developed to filter out non-relevant services. Multiple machine learning strategies are employed to produce smooth service composition resulting in a mosaic of relevant images indexed by geolocation and time. The preliminary analytical results prove the feasibility of the proposed composition model.



### Feature Flow: In-network Feature Flow Estimation for Video Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2009.09660v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.09660v2)
- **Published**: 2020-09-21 07:55:50+00:00
- **Updated**: 2021-11-10 06:58:57+00:00
- **Authors**: Ruibing Jin, Guosheng Lin, Changyun Wen, Jianliang Wang, Fayao Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Optical flow, which expresses pixel displacement, is widely used in many computer vision tasks to provide pixel-level motion information. However, with the remarkable progress of the convolutional neural network, recent state-of-the-art approaches are proposed to solve problems directly on feature-level. Since the displacement of feature vector is not consistent to the pixel displacement, a common approach is to:forward optical flow to a neural network and fine-tune this network on the task dataset. With this method,they expect the fine-tuned network to produce tensors encoding feature-level motion information. In this paper, we rethink this de facto paradigm and analyze its drawbacks in the video object detection task. To mitigate these issues, we propose a novel network (IFF-Net) with an \textbf{I}n-network \textbf{F}eature \textbf{F}low estimation module (IFF module) for video object detection. Without resorting pre-training on any additional dataset, our IFF module is able to directly produce \textbf{feature flow} which indicates the feature displacement. Our IFF module consists of a shallow module, which shares the features with the detection branches. This compact design enables our IFF-Net to accurately detect objects, while maintaining a fast inference speed. Furthermore, we propose a transformation residual loss (TRL) based on \textit{self-supervision}, which further improves the performance of our IFF-Net. Our IFF-Net outperforms existing methods and sets a state-of-the-art performance on ImageNet VID.



### Learning Spatio-Appearance Memory Network for High-Performance Visual Tracking
- **Arxiv ID**: http://arxiv.org/abs/2009.09669v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.09669v5)
- **Published**: 2020-09-21 08:12:02+00:00
- **Updated**: 2021-04-06 05:37:10+00:00
- **Authors**: Fei Xie, Wankou Yang, Bo Liu, Kaihua Zhang, Wanli Xue, Wangmeng Zuo
- **Comment**: 12 pages
- **Journal**: None
- **Summary**: Existing visual object tracking usually learns a bounding-box based template to match the targets across frames, which cannot accurately learn a pixel-wise representation, thereby being limited in handling severe appearance variations. To address these issues, much effort has been made on segmentation-based tracking, which learns a pixel-wise object-aware template and can achieve higher accuracy than bounding-box template based tracking. However, existing segmentation-based trackers are ineffective in learning the spatio-temporal correspondence across frames due to no use of the rich temporal information. To overcome this issue, this paper presents a novel segmentation-based tracking architecture, which is equipped with a spatio-appearance memory network to learn accurate spatio-temporal correspondence. Among it, an appearance memory network explores spatio-temporal non-local similarity to learn the dense correspondence between the segmentation mask and the current frame. Meanwhile, a spatial memory network is modeled as discriminative correlation filter to learn the mapping between feature map and spatial map. The appearance memory network helps to filter out the noisy samples in the spatial memory network while the latter provides the former with more accurate target geometrical center. This mutual promotion greatly boosts the tracking performance. Without bells and whistles, our simple-yet-effective tracking architecture sets new state-of-the-arts on the VOT2016, VOT2018, VOT2019, GOT-10K, TrackingNet, and VOT2020 benchmarks, respectively. Besides, our tracker outperforms the leading segmentation-based trackers SiamMask and D3S on two video object segmentation benchmarks DAVIS16 and DAVIS17 by a large margin. The source codes can be found at https://github.com/phiphiphi31/DMB.



### Feed-Forward On-Edge Fine-tuning Using Static Synthetic Gradient Modules
- **Arxiv ID**: http://arxiv.org/abs/2009.09675v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.09675v1)
- **Published**: 2020-09-21 08:27:01+00:00
- **Updated**: 2020-09-21 08:27:01+00:00
- **Authors**: Robby Neven, Marian Verhelst, Tinne Tuytelaars, Toon Goedemé
- **Comment**: None
- **Journal**: None
- **Summary**: Training deep learning models on embedded devices is typically avoided since this requires more memory, computation and power over inference. In this work, we focus on lowering the amount of memory needed for storing all activations, which are required during the backward pass to compute the gradients. Instead, during the forward pass, static Synthetic Gradient Modules (SGMs) predict gradients for each layer. This allows training the model in a feed-forward manner without having to store all activations. We tested our method on a robot grasping scenario where a robot needs to learn to grasp new objects given only a single demonstration. By first training the SGMs in a meta-learning manner on a set of common objects, during fine-tuning, the SGMs provided the model with accurate gradients to successfully learn to grasp new objects. We have shown that our method has comparable results to using standard backpropagation.



### Real-Time Resource Allocation for Tracking Systems
- **Arxiv ID**: http://arxiv.org/abs/2010.03024v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.03024v1)
- **Published**: 2020-09-21 08:29:05+00:00
- **Updated**: 2020-09-21 08:29:05+00:00
- **Authors**: Yash Satsangi, Shimon Whiteson, Frans A. Oliehoek, Henri Bouma
- **Comment**: http://auai.org/uai2017/proceedings/papers/130.pdf
- **Journal**: UAI 2017
- **Summary**: Automated tracking is key to many computer vision applications. However, many tracking systems struggle to perform in real-time due to the high computational cost of detecting people, especially in ultra high resolution images. We propose a new algorithm called \emph{PartiMax} that greatly reduces this cost by applying the person detector only to the relevant parts of the image. PartiMax exploits information in the particle filter to select $k$ of the $n$ candidate \emph{pixel boxes} in the image. We prove that PartiMax is guaranteed to make a near-optimal selection with error bounds that are independent of the problem size. Furthermore, empirical results on a real-life dataset show that our system runs in real-time by processing only 10\% of the pixel boxes in the image while still retaining 80\% of the original tracking performance achieved when processing all pixel boxes.



### A survey on Kornia: an Open Source Differentiable Computer Vision Library for PyTorch
- **Arxiv ID**: http://arxiv.org/abs/2009.10521v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.10521v1)
- **Published**: 2020-09-21 08:48:28+00:00
- **Updated**: 2020-09-21 08:48:28+00:00
- **Authors**: E. Riba, D. Mishkin, J. Shi, D. Ponsa, F. Moreno-Noguer, G. Bradski
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1910.02190
- **Journal**: None
- **Summary**: This work presents Kornia, an open source computer vision library built upon a set of differentiable routines and modules that aims to solve generic computer vision problems. The package uses PyTorch as its main backend, not only for efficiency but also to take advantage of the reverse auto-differentiation engine to define and compute the gradient of complex functions. Inspired by OpenCV, Kornia is composed of a set of modules containing operators that can be integrated into neural networks to train models to perform a wide range of operations including image transformations,camera calibration, epipolar geometry, and low level image processing techniques, such as filtering and edge detection that operate directly on high dimensional tensor representations on graphical processing units, generating faster systems. Examples of classical vision problems implemented using our framework are provided including a benchmark comparing to existing vision libraries.



### Contrastive Clustering
- **Arxiv ID**: http://arxiv.org/abs/2009.09687v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2009.09687v1)
- **Published**: 2020-09-21 08:54:40+00:00
- **Updated**: 2020-09-21 08:54:40+00:00
- **Authors**: Yunfan Li, Peng Hu, Zitao Liu, Dezhong Peng, Joey Tianyi Zhou, Xi Peng
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a one-stage online clustering method called Contrastive Clustering (CC) which explicitly performs the instance- and cluster-level contrastive learning. To be specific, for a given dataset, the positive and negative instance pairs are constructed through data augmentations and then projected into a feature space. Therein, the instance- and cluster-level contrastive learning are respectively conducted in the row and column space by maximizing the similarities of positive pairs while minimizing those of negative ones. Our key observation is that the rows of the feature matrix could be regarded as soft labels of instances, and accordingly the columns could be further regarded as cluster representations. By simultaneously optimizing the instance- and cluster-level contrastive loss, the model jointly learns representations and cluster assignments in an end-to-end manner. Extensive experimental results show that CC remarkably outperforms 17 competitive clustering methods on six challenging image benchmarks. In particular, CC achieves an NMI of 0.705 (0.431) on the CIFAR-10 (CIFAR-100) dataset, which is an up to 19\% (39\%) performance improvement compared with the best baseline.



### Batch Coherence-Driven Network for Part-aware Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2009.09692v2
- **DOI**: 10.1109/TIP.2021.3060909
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.09692v2)
- **Published**: 2020-09-21 09:04:13+00:00
- **Updated**: 2021-05-07 07:21:24+00:00
- **Authors**: Kan Wang, Pengfei Wang, Changxing Ding, Dacheng Tao
- **Comment**: Accepted Version to IEEE Transactions on Image Processing
- **Journal**: None
- **Summary**: Existing part-aware person re-identification methods typically employ two separate steps: namely, body part detection and part-level feature extraction. However, part detection introduces an additional computational cost and is inherently challenging for low-quality images. Accordingly, in this work, we propose a simple framework named Batch Coherence-Driven Network (BCD-Net) that bypasses body part detection during both the training and testing phases while still learning semantically aligned part features. Our key observation is that the statistics in a batch of images are stable, and therefore that batch-level constraints are robust. First, we introduce a batch coherence-guided channel attention (BCCA) module that highlights the relevant channels for each respective part from the output of a deep backbone model. We investigate channelpart correspondence using a batch of training images, then impose a novel batch-level supervision signal that helps BCCA to identify part-relevant channels. Second, the mean position of a body part is robust and consequently coherent between batches throughout the training process. Accordingly, we introduce a pair of regularization terms based on the semantic consistency between batches. The first term regularizes the high responses of BCD-Net for each part on one batch in order to constrain it within a predefined area, while the second encourages the aggregate of BCD-Nets responses for all parts covering the entire human body. The above constraints guide BCD-Net to learn diverse, complementary, and semantically aligned part-level features. Extensive experimental results demonstrate that BCDNet consistently achieves state-of-the-art performance on four large-scale ReID benchmarks.



### The High-Quality Wide Multi-Channel Attack (HQ-WMCA) database
- **Arxiv ID**: http://arxiv.org/abs/2009.09703v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.09703v1)
- **Published**: 2020-09-21 09:18:19+00:00
- **Updated**: 2020-09-21 09:18:19+00:00
- **Authors**: Zohreh Mostaani, Anjith George, Guillaume Heusch, David Geissbuhler, Sebastien Marcel
- **Comment**: None
- **Journal**: None
- **Summary**: The High-Quality Wide Multi-Channel Attack database (HQ-WMCA) database extends the previous Wide Multi-Channel Attack database(WMCA), with more channels including color, depth, thermal, infrared (spectra), and short-wave infrared (spectra), and also a wide variety of attacks.



### When Healthcare Meets Off-the-Shelf WiFi: A Non-Wearable and Low-Costs Approach for In-Home Monitoring
- **Arxiv ID**: http://arxiv.org/abs/2009.09715v1
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV, cs.LG, J.3
- **Links**: [PDF](http://arxiv.org/pdf/2009.09715v1)
- **Published**: 2020-09-21 09:35:13+00:00
- **Updated**: 2020-09-21 09:35:13+00:00
- **Authors**: Lingchao Guo, Zhaoming Lu, Shuang Zhou, Xiangming Wen, Zhihong He
- **Comment**: 41 pages, 14 figures
- **Journal**: None
- **Summary**: As elderly population grows, social and health care begin to face validation challenges, in-home monitoring is becoming a focus for professionals in the field. Governments urgently need to improve the quality of healthcare services at lower costs while ensuring the comfort and independence of the elderly. This work presents an in-home monitoring approach based on off-the-shelf WiFi, which is low-costs, non-wearable and makes all-round daily healthcare information available to caregivers. The proposed approach can capture fine-grained human pose figures even through a wall and track detailed respiration status simultaneously by off-the-shelf WiFi devices. Based on them, behavioral data, physiological data and the derived information (e.g., abnormal events and underlying diseases), of the elderly could be seen by caregivers directly. We design a series of signal processing methods and a neural network to capture human pose figures and extract respiration status curves from WiFi Channel State Information (CSI). Extensive experiments are conducted and according to the results, off-the-shelf WiFi devices are capable of capturing fine-grained human pose figures, similar to cameras, even through a wall and track accurate respiration status, thus demonstrating the effectiveness and feasibility of our approach for in-home monitoring.



### MFIF-GAN: A New Generative Adversarial Network for Multi-Focus Image Fusion
- **Arxiv ID**: http://arxiv.org/abs/2009.09718v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.09718v4)
- **Published**: 2020-09-21 09:36:34+00:00
- **Updated**: 2020-11-09 03:36:53+00:00
- **Authors**: Yicheng Wang, Shuang Xu, Junmin Liu, Zixiang Zhao, Chunxia Zhang, Jiangshe Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-Focus Image Fusion (MFIF) is a promising image enhancement technique to obtain all-in-focus images meeting visual needs and it is a precondition of other computer vision tasks. One of the research trends of MFIF is to avoid the defocus spread effect (DSE) around the focus/defocus boundary (FDB). In this paper,we propose a network termed MFIF-GAN to attenuate the DSE by generating focus maps in which the foreground region are correctly larger than the corresponding objects. The Squeeze and Excitation Residual module is employed in the network. By combining the prior knowledge of training condition, this network is trained on a synthetic dataset based on an {\alpha}-matte model. In addition, the reconstruction and gradient regularization terms are combined in the loss functions to enhance the boundary details and improve the quality of fused images. Extensive experiments demonstrate that the MFIF-GAN outperforms several state-of-the-art (SOTA) methods in visual perception, quantitative analysis as well as efficiency. Moreover, the edge diffusion and contraction module is firstly proposed to verify that focus maps generated by our method are accurate at the pixel level.



### Conditional Automated Channel Pruning for Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2009.09724v2
- **DOI**: 10.1109/LSP.2021.3088323
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.09724v2)
- **Published**: 2020-09-21 09:55:48+00:00
- **Updated**: 2020-09-27 03:29:23+00:00
- **Authors**: Yixin Liu, Yong Guo, Zichang Liu, Haohua Liu, Jingjie Zhang, Zejun Chen, Jing Liu, Jian Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Model compression aims to reduce the redundancy of deep networks to obtain compact models. Recently, channel pruning has become one of the predominant compression methods to deploy deep models on resource-constrained devices. Most channel pruning methods often use a fixed compression rate for all the layers of the model, which, however, may not be optimal. To address this issue, given a target compression rate for the whole model, one can search for the optimal compression rate for each layer. Nevertheless, these methods perform channel pruning for a specific target compression rate. When we consider multiple compression rates, they have to repeat the channel pruning process multiple times, which is very inefficient yet unnecessary. To address this issue, we propose a Conditional Automated Channel Pruning(CACP) method to obtain the compressed models with different compression rates through single channel pruning process. To this end, we develop a conditional model that takes an arbitrary compression rate as input and outputs the corresponding compressed model. In the experiments, the resultant models with different compression rates consistently outperform the models compressed by existing methods with a channel pruning process for each target compression rate.



### Improving Automated COVID-19 Grading with Convolutional Neural Networks in Computed Tomography Scans: An Ablation Study
- **Arxiv ID**: http://arxiv.org/abs/2009.09725v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.09725v1)
- **Published**: 2020-09-21 09:58:57+00:00
- **Updated**: 2020-09-21 09:58:57+00:00
- **Authors**: Coen de Vente, Luuk H. Boulogne, Kiran Vaidhya Venkadesh, Cheryl Sital, Nikolas Lessmann, Colin Jacobs, Clara I. Sánchez, Bram van Ginneken
- **Comment**: 9 pages, 6 figures
- **Journal**: None
- **Summary**: Amidst the ongoing pandemic, several studies have shown that COVID-19 classification and grading using computed tomography (CT) images can be automated with convolutional neural networks (CNNs). Many of these studies focused on reporting initial results of algorithms that were assembled from commonly used components. The choice of these components was often pragmatic rather than systematic. For instance, several studies used 2D CNNs even though these might not be optimal for handling 3D CT volumes. This paper identifies a variety of components that increase the performance of CNN-based algorithms for COVID-19 grading from CT images. We investigated the effectiveness of using a 3D CNN instead of a 2D CNN, of using transfer learning to initialize the network, of providing automatically computed lesion maps as additional network input, and of predicting a continuous instead of a categorical output. A 3D CNN with these components achieved an area under the ROC curve (AUC) of 0.934 on our test set of 105 CT scans and an AUC of 0.923 on a publicly available set of 742 CT scans, a substantial improvement in comparison with a previously published 2D CNN. An ablation study demonstrated that in addition to using a 3D CNN instead of a 2D CNN transfer learning contributed the most and continuous output contributed the least to improving the model performance.



### Generating Adversarial yet Inconspicuous Patches with a Single Image
- **Arxiv ID**: http://arxiv.org/abs/2009.09774v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2009.09774v2)
- **Published**: 2020-09-21 11:56:01+00:00
- **Updated**: 2021-04-21 12:05:48+00:00
- **Authors**: Jinqi Luo, Tao Bai, Jun Zhao
- **Comment**: Accepted by AAAI2021 Student Abstract and Poster Program. Full paper
  available as arXiv:2009.09774.v1
- **Journal**: None
- **Summary**: Deep neural networks have been shown vulnerable toadversarial patches, where exotic patterns can resultin models wrong prediction. Nevertheless, existing ap-proaches to adversarial patch generation hardly con-sider the contextual consistency between patches andthe image background, causing such patches to be eas-ily detected and adversarial attacks to fail. On the otherhand, these methods require a large amount of data fortraining, which is computationally expensive. To over-come these challenges, we propose an approach to gen-erate adversarial yet inconspicuous patches with onesingle image. In our approach, adversarial patches areproduced in a coarse-to-fine way with multiple scalesof generators and discriminators. Contextual informa-tion is encoded during the Min-Max training to makepatches consistent with surroundings. The selection ofpatch location is based on the perceptual sensitivity ofvictim models. Through extensive experiments, our ap-proach shows strong attacking ability in both the white-box and black-box setting. Experiments on saliency de-tection and user evaluation indicate that our adversar-ial patches can evade human observations, demonstratethe inconspicuousness of our approach. Lastly, we showthat our approach preserves the attack ability in thephysical world.



### Impact of lung segmentation on the diagnosis and explanation of COVID-19 in chest X-ray images
- **Arxiv ID**: http://arxiv.org/abs/2009.09780v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.09780v4)
- **Published**: 2020-09-21 12:03:54+00:00
- **Updated**: 2021-09-13 13:32:02+00:00
- **Authors**: Lucas O. Teixeira, Rodolfo M. Pereira, Diego Bertolini, Luiz S. Oliveira, Loris Nanni, George D. C. Cavalcanti, Yandre M. G. Costa
- **Comment**: Submitted to Sensors
- **Journal**: None
- **Summary**: COVID-19 frequently provokes pneumonia, which can be diagnosed using imaging exams. Chest X-ray (CXR) is often useful because it is cheap, fast, widespread, and uses less radiation. Here, we demonstrate the impact of lung segmentation in COVID-19 identification using CXR images and evaluate which contents of the image influenced the most. Semantic segmentation was performed using a U-Net CNN architecture, and the classification using three CNN architectures (VGG, ResNet, and Inception). Explainable Artificial Intelligence techniques were employed to estimate the impact of segmentation. A three-classes database was composed: lung opacity (pneumonia), COVID-19, and normal. We assessed the impact of creating a CXR image database from different sources, and the COVID-19 generalization from one source to another. The segmentation achieved a Jaccard distance of 0.034 and a Dice coefficient of 0.982. The classification using segmented images achieved an F1-Score of 0.88 for the multi-class setup, and 0.83 for COVID-19 identification. In the cross-dataset scenario, we obtained an F1-Score of 0.74 and an area under the ROC curve of 0.9 for COVID-19 identification using segmented images. Experiments support the conclusion that even after segmentation, there is a strong bias introduced by underlying factors from different sources.



### Multi-Modal Reasoning Graph for Scene-Text Based Fine-Grained Image Classification and Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2009.09809v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.09809v1)
- **Published**: 2020-09-21 12:31:42+00:00
- **Updated**: 2020-09-21 12:31:42+00:00
- **Authors**: Andres Mafla, Sounak Dey, Ali Furkan Biten, Lluis Gomez, Dimosthenis Karatzas
- **Comment**: None
- **Journal**: None
- **Summary**: Scene text instances found in natural images carry explicit semantic information that can provide important cues to solve a wide array of computer vision problems. In this paper, we focus on leveraging multi-modal content in the form of visual and textual cues to tackle the task of fine-grained image classification and retrieval. First, we obtain the text instances from images by employing a text reading system. Then, we combine textual features with salient image regions to exploit the complementary information carried by the two sources. Specifically, we employ a Graph Convolutional Network to perform multi-modal reasoning and obtain relationship-enhanced features by learning a common semantic space between salient objects and text found in an image. By obtaining an enhanced set of visual and textual features, the proposed model greatly outperforms the previous state-of-the-art in two different tasks, fine-grained classification and image retrieval in the Con-Text and Drink Bottle datasets.



### DeepActsNet: Spatial and Motion features from Face, Hands, and Body Combined with Convolutional and Graph Networks for Improved Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2009.09818v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.09818v3)
- **Published**: 2020-09-21 12:41:56+00:00
- **Updated**: 2021-06-04 04:09:54+00:00
- **Authors**: Umar Asif, Deval Mehta, Stefan von Cavallar, Jianbin Tang, Stefan Harrer
- **Comment**: None
- **Journal**: None
- **Summary**: Existing action recognition methods mainly focus on joint and bone information in human body skeleton data due to its robustness to complex backgrounds and dynamic characteristics of the environments. In this paper, we combine body skeleton data with spatial and motion features from face and two hands, and present "Deep Action Stamps (DeepActs)", a novel data representation to encode actions from video sequences. We also present "DeepActsNet", a deep learning based ensemble model which learns convolutional and structural features from Deep Action Stamps for highly accurate action recognition. Experiments on three challenging action recognition datasets (NTU60, NTU120, and SYSU) show that the proposed model trained using Deep Action Stamps produce considerable improvements in the action recognition accuracy with less computational cost compared to the state-of-the-art methods.



### Haar Wavelet based Block Autoregressive Flows for Trajectories
- **Arxiv ID**: http://arxiv.org/abs/2009.09878v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.09878v1)
- **Published**: 2020-09-21 13:57:10+00:00
- **Updated**: 2020-09-21 13:57:10+00:00
- **Authors**: Apratim Bhattacharyya, Christoph-Nikolas Straehle, Mario Fritz, Bernt Schiele
- **Comment**: German Conference on Pattern Recognition, 2020 (oral)
- **Journal**: None
- **Summary**: Prediction of trajectories such as that of pedestrians is crucial to the performance of autonomous agents. While previous works have leveraged conditional generative models like GANs and VAEs for learning the likely future trajectories, accurately modeling the dependency structure of these multimodal distributions, particularly over long time horizons remains challenging. Normalizing flow based generative models can model complex distributions admitting exact inference. These include variants with split coupling invertible transformations that are easier to parallelize compared to their autoregressive counterparts. To this end, we introduce a novel Haar wavelet based block autoregressive model leveraging split couplings, conditioned on coarse trajectories obtained from Haar wavelet based transformations at different levels of granularity. This yields an exact inference method that models trajectories at different spatio-temporal resolutions in a hierarchical manner. We illustrate the advantages of our approach for generating diverse and accurate trajectories on two real-world datasets - Stanford Drone and Intersection Drone.



### Beyond Identity: What Information Is Stored in Biometric Face Templates?
- **Arxiv ID**: http://arxiv.org/abs/2009.09918v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.09918v1)
- **Published**: 2020-09-21 14:41:18+00:00
- **Updated**: 2020-09-21 14:41:18+00:00
- **Authors**: Philipp Terhörst, Daniel Fährmann, Naser Damer, Florian Kirchbuchner, Arjan Kuijper
- **Comment**: To appear in IJCB 2020
- **Journal**: None
- **Summary**: Deeply-learned face representations enable the success of current face recognition systems. Despite the ability of these representations to encode the identity of an individual, recent works have shown that more information is stored within, such as demographics, image characteristics, and social traits. This threatens the user's privacy, since for many applications these templates are expected to be solely used for recognition purposes. Knowing the encoded information in face templates helps to develop bias-mitigating and privacy-preserving face recognition technologies. This work aims to support the development of these two branches by analysing face templates regarding 113 attributes. Experiments were conducted on two publicly available face embeddings. For evaluating the predictability of the attributes, we trained a massive attribute classifier that is additionally able to accurately state its prediction confidence. This allows us to make more sophisticated statements about the attribute predictability. The results demonstrate that up to 74 attributes can be accurately predicted from face templates. Especially non-permanent attributes, such as age, hairstyles, haircolors, beards, and various accessories, found to be easily-predictable. Since face recognition systems aim to be robust against these variations, future research might build on this work to develop more understandable privacy preserving solutions and build robust and fair face templates.



### PP-OCR: A Practical Ultra Lightweight OCR System
- **Arxiv ID**: http://arxiv.org/abs/2009.09941v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.09941v3)
- **Published**: 2020-09-21 14:57:18+00:00
- **Updated**: 2020-10-15 14:21:53+00:00
- **Authors**: Yuning Du, Chenxia Li, Ruoyu Guo, Xiaoting Yin, Weiwei Liu, Jun Zhou, Yifan Bai, Zilin Yu, Yehua Yang, Qingqing Dang, Haoshuang Wang
- **Comment**: None
- **Journal**: None
- **Summary**: The Optical Character Recognition (OCR) systems have been widely used in various of application scenarios, such as office automation (OA) systems, factory automations, online educations, map productions etc. However, OCR is still a challenging task due to the various of text appearances and the demand of computational efficiency. In this paper, we propose a practical ultra lightweight OCR system, i.e., PP-OCR. The overall model size of the PP-OCR is only 3.5M for recognizing 6622 Chinese characters and 2.8M for recognizing 63 alphanumeric symbols, respectively. We introduce a bag of strategies to either enhance the model ability or reduce the model size. The corresponding ablation experiments with the real data are also provided. Meanwhile, several pre-trained models for the Chinese and English recognition are released, including a text detector (97K images are used), a direction classifier (600K images are used) as well as a text recognizer (17.9M images are used). Besides, the proposed PP-OCR are also verified in several other language recognition tasks, including French, Korean, Japanese and German. All of the above mentioned models are open-sourced and the codes are available in the GitHub repository, i.e., https://github.com/PaddlePaddle/PaddleOCR.



### Towards Fast, Accurate and Stable 3D Dense Face Alignment
- **Arxiv ID**: http://arxiv.org/abs/2009.09960v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.09960v2)
- **Published**: 2020-09-21 15:37:37+00:00
- **Updated**: 2021-02-07 16:24:15+00:00
- **Authors**: Jianzhu Guo, Xiangyu Zhu, Yang Yang, Fan Yang, Zhen Lei, Stan Z. Li
- **Comment**: Accepted by ECCV 2020
- **Journal**: None
- **Summary**: Existing methods of 3D dense face alignment mainly concentrate on accuracy, thus limiting the scope of their practical applications. In this paper, we propose a novel regression framework named 3DDFA-V2 which makes a balance among speed, accuracy and stability. Firstly, on the basis of a lightweight backbone, we propose a meta-joint optimization strategy to dynamically regress a small set of 3DMM parameters, which greatly enhances speed and accuracy simultaneously. To further improve the stability on videos, we present a virtual synthesis method to transform one still image to a short-video which incorporates in-plane and out-of-plane face moving. On the premise of high accuracy and stability, 3DDFA-V2 runs at over 50fps on a single CPU core and outperforms other state-of-the-art heavy models simultaneously. Experiments on several challenging datasets validate the efficiency of our method. Pre-trained models and code are available at https://github.com/cleardusk/3DDFA_V2.



### Line Flow based SLAM
- **Arxiv ID**: http://arxiv.org/abs/2009.09972v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2009.09972v2)
- **Published**: 2020-09-21 15:55:45+00:00
- **Updated**: 2021-03-17 14:48:27+00:00
- **Authors**: Qiuyuan Wang, Zike Yan, Junqiu Wang, Fei Xue, Wei Ma, Hongbin Zha
- **Comment**: Accepted for publication in IEEE Transactions on Robotics
- **Journal**: None
- **Summary**: We propose a visual SLAM method by predicting and updating line flows that represent sequential 2D projections of 3D line segments. While feature-based SLAM methods have achieved excellent results, they still face problems in challenging scenes containing occlusions, blurred images, and repetitive textures. To address these problems, we leverage a line flow to encode the coherence of line segment observations of the same 3D line along the temporal dimension, which has been neglected in prior SLAM systems. Thanks to this line flow representation, line segments in a new frame can be predicted according to their corresponding 3D lines and their predecessors along the temporal dimension. We create, update, merge, and discard line flows on-the-fly. We model the proposed line flow based SLAM (LF-SLAM) using a Bayesian network. Extensive experimental results demonstrate that the proposed LF-SLAM method achieves state-of-the-art results due to the utilization of line flows. Specifically, LF-SLAM obtains good localization and mapping results in challenging scenes with occlusions, blurred images, and repetitive textures.



### Depth-Adapted CNN for RGB-D cameras
- **Arxiv ID**: http://arxiv.org/abs/2009.09976v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.09976v2)
- **Published**: 2020-09-21 15:58:32+00:00
- **Updated**: 2020-09-23 09:45:21+00:00
- **Authors**: Zongwei Wu, Guillaume Allibert, Christophe Stolz, Cedric Demonceaux
- **Comment**: Accepted manuscript in ACCV 2020 (Oral)
- **Journal**: None
- **Summary**: Conventional 2D Convolutional Neural Networks (CNN) extract features from an input image by applying linear filters. These filters compute the spatial coherence by weighting the photometric information on a fixed neighborhood without taking into account the geometric information. We tackle the problem of improving the classical RGB CNN methods by using the depth information provided by the RGB-D cameras. State-of-the-art approaches use depth as an additional channel or image (HHA) or pass from 2D CNN to 3D CNN. This paper proposes a novel and generic procedure to articulate both photometric and geometric information in CNN architecture. The depth data is represented as a 2D offset to adapt spatial sampling locations. The new model presented is invariant to scale and rotation around the X and the Y axis of the camera coordinate system. Moreover, when depth data is constant, our model is equivalent to a regular CNN. Experiments of benchmarks validate the effectiveness of our model.



### DR2S : Deep Regression with Region Selection for Camera Quality Evaluation
- **Arxiv ID**: http://arxiv.org/abs/2009.09981v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.09981v1)
- **Published**: 2020-09-21 16:05:15+00:00
- **Updated**: 2020-09-21 16:05:15+00:00
- **Authors**: Marcelin Tworski, Stéphane Lathuilière, Salim Belkarfa, Attilio Fiandrotti, Marco Cagnazzo
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we tackle the problem of estimating a camera capability to preserve fine texture details at a given lighting condition. Importantly, our texture preservation measurement should coincide with human perception. Consequently, we formulate our problem as a regression one and we introduce a deep convolutional network to estimate texture quality score. At training time, we use ground-truth quality scores provided by expert human annotators in order to obtain a subjective quality measure. In addition, we propose a region selection method to identify the image regions that are better suited at measuring perceptual quality. Finally, our experimental evaluation shows that our learning-based approach outperforms existing methods and that our region selection algorithm consistently improves the quality estimation.



### TRECVID 2019: An Evaluation Campaign to Benchmark Video Activity Detection, Video Captioning and Matching, and Video Search & Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2009.09984v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2009.09984v1)
- **Published**: 2020-09-21 16:08:47+00:00
- **Updated**: 2020-09-21 16:08:47+00:00
- **Authors**: George Awad, Asad A. Butt, Keith Curtis, Yooyoung Lee, Jonathan Fiscus, Afzal Godil, Andrew Delgado, Jesse Zhang, Eliot Godard, Lukas Diduch, Alan F. Smeaton, Yvette Graham, Wessel Kraaij, Georges Quenot
- **Comment**: TRECVID Workshop overview paper. 39 pages
- **Journal**: None
- **Summary**: The TREC Video Retrieval Evaluation (TRECVID) 2019 was a TREC-style video analysis and retrieval evaluation, the goal of which remains to promote progress in research and development of content-based exploitation and retrieval of information from digital video via open, metrics-based evaluation. Over the last nineteen years this effort has yielded a better understanding of how systems can effectively accomplish such processing and how one can reliably benchmark their performance. TRECVID has been funded by NIST (National Institute of Standards and Technology) and other US government agencies. In addition, many organizations and individuals worldwide contribute significant time and effort. TRECVID 2019 represented a continuation of four tasks from TRECVID 2018. In total, 27 teams from various research organizations worldwide completed one or more of the following four tasks: 1. Ad-hoc Video Search (AVS) 2. Instance Search (INS) 3. Activities in Extended Video (ActEV) 4. Video to Text Description (VTT) This paper is an introduction to the evaluation framework, tasks, data, and measures used in the workshop.



### Joint and Progressive Subspace Analysis (JPSA) with Spatial-Spectral Manifold Alignment for Semi-Supervised Hyperspectral Dimensionality Reduction
- **Arxiv ID**: http://arxiv.org/abs/2009.10003v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.10003v1)
- **Published**: 2020-09-21 16:29:59+00:00
- **Updated**: 2020-09-21 16:29:59+00:00
- **Authors**: Danfeng Hong, Naoto Yokoya, Jocelyn Chanussot, Jian Xu, Xiao Xiang Zhu
- **Comment**: None
- **Journal**: IEEE Transactions on Cybernetics, 2020
- **Summary**: Conventional nonlinear subspace learning techniques (e.g., manifold learning) usually introduce some drawbacks in explainability (explicit mapping) and cost-effectiveness (linearization), generalization capability (out-of-sample), and representability (spatial-spectral discrimination). To overcome these shortcomings, a novel linearized subspace analysis technique with spatial-spectral manifold alignment is developed for a semi-supervised hyperspectral dimensionality reduction (HDR), called joint and progressive subspace analysis (JPSA). The JPSA learns a high-level, semantically meaningful, joint spatial-spectral feature representation from hyperspectral data by 1) jointly learning latent subspaces and a linear classifier to find an effective projection direction favorable for classification; 2) progressively searching several intermediate states of subspaces to approach an optimal mapping from the original space to a potential more discriminative subspace; 3) spatially and spectrally aligning manifold structure in each learned latent subspace in order to preserve the same or similar topological property between the compressed data and the original data. A simple but effective classifier, i.e., nearest neighbor (NN), is explored as a potential application for validating the algorithm performance of different HDR approaches. Extensive experiments are conducted to demonstrate the superiority and effectiveness of the proposed JPSA on two widely-used hyperspectral datasets: Indian Pines (92.98\%) and the University of Houston (86.09\%) in comparison with previous state-of-the-art HDR methods. The demo of this basic work (i.e., ECCV2018) is openly available at https://github.com/danfenghong/ECCV2018_J-Play.



### Synthetic Training for Accurate 3D Human Pose and Shape Estimation in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2009.10013v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.10013v2)
- **Published**: 2020-09-21 16:39:04+00:00
- **Updated**: 2020-09-22 10:27:05+00:00
- **Authors**: Akash Sengupta, Ignas Budvytis, Roberto Cipolla
- **Comment**: 14 pages, 7 figures, BMVC 2020, Fixed abstract typos
- **Journal**: None
- **Summary**: This paper addresses the problem of monocular 3D human shape and pose estimation from an RGB image. Despite great progress in this field in terms of pose prediction accuracy, state-of-the-art methods often predict inaccurate body shapes. We suggest that this is primarily due to the scarcity of in-the-wild training data with diverse and accurate body shape labels. Thus, we propose STRAPS (Synthetic Training for Real Accurate Pose and Shape), a system that utilises proxy representations, such as silhouettes and 2D joints, as inputs to a shape and pose regression neural network, which is trained with synthetic training data (generated on-the-fly during training using the SMPL statistical body model) to overcome data scarcity. We bridge the gap between synthetic training inputs and noisy real inputs, which are predicted by keypoint detection and segmentation CNNs at test-time, by using data augmentation and corruption during training. In order to evaluate our approach, we curate and provide a challenging evaluation dataset for monocular human shape estimation, Sports Shape and Pose 3D (SSP-3D). It consists of RGB images of tightly-clothed sports-persons with a variety of body shapes and corresponding pseudo-ground-truth SMPL shape and pose parameters, obtained via multi-frame optimisation. We show that STRAPS outperforms other state-of-the-art methods on SSP-3D in terms of shape prediction accuracy, while remaining competitive with the state-of-the-art on pose-centric datasets and metrics.



### Visual-Semantic Embedding Model Informed by Structured Knowledge
- **Arxiv ID**: http://arxiv.org/abs/2009.10026v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.10026v1)
- **Published**: 2020-09-21 17:04:32+00:00
- **Updated**: 2020-09-21 17:04:32+00:00
- **Authors**: Mirantha Jayathilaka, Tingting Mu, Uli Sattler
- **Comment**: European Starting AI Researchers' Symposium 2020 (STAIRS 2020)
- **Journal**: None
- **Summary**: We propose a novel approach to improve a visual-semantic embedding model by incorporating concept representations captured from an external structured knowledge base. We investigate its performance on image classification under both standard and zero-shot settings. We propose two novel evaluation frameworks to analyse classification errors with respect to the class hierarchy indicated by the knowledge base. The approach is tested using the ILSVRC 2012 image dataset and a WordNet knowledge base. With respect to both standard and zero-shot image classification, our approach shows superior performance compared with the original approach, which uses word embeddings.



### Regularizing Attention Networks for Anomaly Detection in Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/2009.10054v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.10054v3)
- **Published**: 2020-09-21 17:47:49+00:00
- **Updated**: 2022-04-11 16:33:14+00:00
- **Authors**: Doyup Lee, Yeongjae Cheon, Wook-Shin Han
- **Comment**: 16 pages, 7 figures, Accepted by AAAI-21
- **Journal**: None
- **Summary**: For stability and reliability of real-world applications, the robustness of DNNs in unimodal tasks has been evaluated. However, few studies consider abnormal situations that a visual question answering (VQA) model might encounter at test time after deployment in the real-world. In this study, we evaluate the robustness of state-of-the-art VQA models to five different anomalies, including worst-case scenarios, the most frequent scenarios, and the current limitation of VQA models. Different from the results in unimodal tasks, the maximum confidence of answers in VQA models cannot detect anomalous inputs, and post-training of the outputs, such as outlier exposure, is ineffective for VQA models. Thus, we propose an attention-based method, which uses confidence of reasoning between input images and questions and shows much more promising results than the previous methods in unimodal tasks. In addition, we show that a maximum entropy regularization of attention networks can significantly improve the attention-based anomaly detection of the VQA models. Thanks to the simplicity, attention-based anomaly detection and the regularization are model-agnostic methods, which can be used for various cross-modal attentions in the state-of-the-art VQA models. The results imply that cross-modal attention in VQA is important to improve not only VQA accuracy, but also the robustness to various anomalies.



### Exploring Intensity Invariance in Deep Neural Networks for Brain Image Registration
- **Arxiv ID**: http://arxiv.org/abs/2009.10058v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.10058v1)
- **Published**: 2020-09-21 17:49:03+00:00
- **Updated**: 2020-09-21 17:49:03+00:00
- **Authors**: Hassan Mahmood, Asim Iqbal, Syed Mohammed Shamsul Islam
- **Comment**: None
- **Journal**: None
- **Summary**: Image registration is a widely-used technique in analysing large scale datasets that are captured through various imaging modalities and techniques in biomedical imaging such as MRI, X-Rays, etc. These datasets are typically collected from various sites and under different imaging protocols using a variety of scanners. Such heterogeneity in the data collection process causes inhomogeneity or variation in intensity (brightness) and noise distribution. These variations play a detrimental role in the performance of image registration, segmentation and detection algorithms. Classical image registration methods are computationally expensive but are able to handle these artifacts relatively better. However, deep learning-based techniques are shown to be computationally efficient for automated brain registration but are sensitive to the intensity variations. In this study, we investigate the effect of variation in intensity distribution among input image pairs for deep learning-based image registration methods. We find a performance degradation of these models when brain image pairs with different intensity distribution are presented even with similar structures. To overcome this limitation, we incorporate a structural similarity-based loss function in a deep neural network and test its performance on the validation split separated before training as well as on a completely unseen new dataset. We report that the deep learning models trained with structure similarity-based loss seems to perform better for both datasets. This investigation highlights a possible performance limiting factor in deep learning-based registration models and suggests a potential solution to incorporate the intensity distribution variation in the input image pairs. Our code and models are available at https://github.com/hassaanmahmood/DeepIntense.



### Improving Person Re-identification with Iterative Impression Aggregation
- **Arxiv ID**: http://arxiv.org/abs/2009.10066v1
- **DOI**: 10.1109/TIP.2020.3029415
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.10066v1)
- **Published**: 2020-09-21 17:59:04+00:00
- **Updated**: 2020-09-21 17:59:04+00:00
- **Authors**: Dengpan Fu, Bo Xin, Jingdong Wang, Dongdong Chen, Jianmin Bao, Gang Hua, Houqiang Li
- **Comment**: Accepted by Transactions on Image Processing
- **Journal**: None
- **Summary**: Our impression about one person often updates after we see more aspects of him/her and this process keeps iterating given more meetings. We formulate such an intuition into the problem of person re-identification (re-ID), where the representation of a query (probe) image is iteratively updated with new information from the candidates in the gallery. Specifically, we propose a simple attentional aggregation formulation to instantiate this idea and showcase that such a pipeline achieves competitive performance on standard benchmarks including CUHK03, Market-1501 and DukeMTMC. Not only does such a simple method improve the performance of the baseline models, it also achieves comparable performance with latest advanced re-ranking methods. Another advantage of this proposal is its flexibility to incorporate different representations and similarity metrics. By utilizing stronger representations and metrics, we further demonstrate state-of-the-art person re-ID performance, which also validates the general applicability of the proposed method.



### Extreme compression of grayscale images
- **Arxiv ID**: http://arxiv.org/abs/2009.10115v1
- **DOI**: 10.1016/j.cnsns.2020.105546
- **Categories**: **cs.CV**, 28A80, 68U10, 94A08
- **Links**: [PDF](http://arxiv.org/pdf/2009.10115v1)
- **Published**: 2020-09-21 18:19:23+00:00
- **Updated**: 2020-09-21 18:19:23+00:00
- **Authors**: Franklin Mendivil, Örjan Stenflo
- **Comment**: 16 pages, 12 figures
- **Journal**: None
- **Summary**: Given an grayscale digital image, and a positive integer $n$, how well can we store the image at a compression ratio of $n:1$?   In this paper we address the above question in extreme cases when $n>>50$ using "$\mathbf{V}$-variable image compression".



### A Sparse Sampling-based framework for Semantic Fast-Forward of First-Person Videos
- **Arxiv ID**: http://arxiv.org/abs/2009.11063v1
- **DOI**: 10.1109/TPAMI.2020.2983929
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.11063v1)
- **Published**: 2020-09-21 18:36:17+00:00
- **Updated**: 2020-09-21 18:36:17+00:00
- **Authors**: Michel Melo Silva, Washington Luis Souza Ramos, Mario Fernando Montenegro Campos, Erickson Rangel Nascimento
- **Comment**: Accepted at the IEEE Transactions on Pattern Analysis and Machine
  Intelligence (TPAMI) 2020. arXiv admin note: text overlap with
  arXiv:1802.08722
- **Journal**: None
- **Summary**: Technological advances in sensors have paved the way for digital cameras to become increasingly ubiquitous, which, in turn, led to the popularity of the self-recording culture. As a result, the amount of visual data on the Internet is moving in the opposite direction of the available time and patience of the users. Thus, most of the uploaded videos are doomed to be forgotten and unwatched stashed away in some computer folder or website. In this paper, we address the problem of creating smooth fast-forward videos without losing the relevant content. We present a new adaptive frame selection formulated as a weighted minimum reconstruction problem. Using a smoothing frame transition and filling visual gaps between segments, our approach accelerates first-person videos emphasizing the relevant segments and avoids visual discontinuities. Experiments conducted on controlled videos and also on an unconstrained dataset of First-Person Videos (FPVs) show that, when creating fast-forward videos, our method is able to retain as much relevant information and smoothness as the state-of-the-art techniques, but in less processing time.



### Deep Learning Applied to Chest X-Rays: Exploiting and Preventing Shortcuts
- **Arxiv ID**: http://arxiv.org/abs/2009.10132v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.10132v1)
- **Published**: 2020-09-21 18:52:43+00:00
- **Updated**: 2020-09-21 18:52:43+00:00
- **Authors**: Sarah Jabbour, David Fouhey, Ella Kazerooni, Michael W. Sjoding, Jenna Wiens
- **Comment**: 32 pages, 9 figures, 12 tables, MLHC 2020
- **Journal**: None
- **Summary**: While deep learning has shown promise in improving the automated diagnosis of disease based on chest X-rays, deep networks may exhibit undesirable behavior related to shortcuts. This paper studies the case of spurious class skew in which patients with a particular attribute are spuriously more likely to have the outcome of interest. For instance, clinical protocols might lead to a dataset in which patients with pacemakers are disproportionately likely to have congestive heart failure. This skew can lead to models that take shortcuts by heavily relying on the biased attribute. We explore this problem across a number of attributes in the context of diagnosing the cause of acute hypoxemic respiratory failure. Applied to chest X-rays, we show that i) deep nets can accurately identify many patient attributes including sex (AUROC = 0.96) and age (AUROC >= 0.90), ii) they tend to exploit correlations between such attributes and the outcome label when learning to predict a diagnosis, leading to poor performance when such correlations do not hold in the test population (e.g., everyone in the test set is male), and iii) a simple transfer learning approach is surprisingly effective at preventing the shortcut and promoting good generalization performance. On the task of diagnosing congestive heart failure based on a set of chest X-rays skewed towards older patients (age >= 63), the proposed approach improves generalization over standard training from 0.66 (95% CI: 0.54-0.77) to 0.84 (95% CI: 0.73-0.92) AUROC. While simple, the proposed approach has the potential to improve the performance of models across populations by encouraging reliance on clinically relevant manifestations of disease, i.e., those that a clinician would use to make a diagnosis.



### CCBlock: An Effective Use of Deep Learning for Automatic Diagnosis of COVID-19 Using X-Ray Images
- **Arxiv ID**: http://arxiv.org/abs/2009.10141v1
- **DOI**: 10.1007/s42600-020-00110-7
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.10141v1)
- **Published**: 2020-09-21 19:20:01+00:00
- **Updated**: 2020-09-21 19:20:01+00:00
- **Authors**: Ali Al-Bawi, Karrar Ali Al-Kaabi, Mohammed Jeryo, Ahmad Al-Fatlawi
- **Comment**: 12 pages, 5 figures
- **Journal**: None
- **Summary**: Propose: Troubling countries one after another, the COVID-19 pandemic has dramatically affected the health and well-being of the world's population. The disease may continue to persist more extensively due to the increasing number of new cases daily, the rapid spread of the virus, and delay in the PCR analysis results. Therefore, it is necessary to consider developing assistive methods for detecting and diagnosing the COVID-19 to eradicate the spread of the novel coronavirus among people. Based on convolutional neural networks (CNNs), automated detection systems have shown promising results of diagnosing patients with the COVID-19 through radiography; thus, they are introduced as a workable solution to the COVID-19 diagnosis. Materials and Methods: Based on the enhancement of the classical visual geometry group (VGG) network with the convolutional COVID block (CCBlock), an efficient screening model was proposed in this study to diagnose and distinguish patients with the COVID-19 from those with pneumonia and the healthy people through radiography. The model testing dataset included 1,828 x-ray images available on public platforms. 310 images were showing confirmed COVID-19 cases, 864 images indicating pneumonia cases, and 654 images showing healthy people. Results: According to the test results, enhancing the classical VGG network with radiography provided the highest diagnosis performance and overall accuracy of 98.52% for two classes as well as accuracy of 95.34% for three classes. Conclusions: According to the results, using the enhanced VGG deep neural network can help radiologists automatically diagnose the COVID-19 through radiography.



### Stereopagnosia: Fooling Stereo Networks with Adversarial Perturbations
- **Arxiv ID**: http://arxiv.org/abs/2009.10142v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.10142v3)
- **Published**: 2020-09-21 19:20:09+00:00
- **Updated**: 2021-03-26 10:53:55+00:00
- **Authors**: Alex Wong, Mukund Mundhra, Stefano Soatto
- **Comment**: None
- **Journal**: None
- **Summary**: We study the effect of adversarial perturbations of images on the estimates of disparity by deep learning models trained for stereo. We show that imperceptible additive perturbations can significantly alter the disparity map, and correspondingly the perceived geometry of the scene. These perturbations not only affect the specific model they are crafted for, but transfer to models with different architecture, trained with different loss functions. We show that, when used for adversarial data augmentation, our perturbations result in trained models that are more robust, without sacrificing overall accuracy of the model. This is unlike what has been observed in image classification, where adding the perturbed images to the training set makes the model less vulnerable to adversarial perturbations, but to the detriment of overall accuracy. We test our method using the most recent stereo networks and evaluate their performance on public benchmark datasets.



### Operator-valued formulas for Riemannian Gradient and Hessian and families of tractable metrics
- **Arxiv ID**: http://arxiv.org/abs/2009.10159v2
- **DOI**: None
- **Categories**: **math.OC**, cs.CV, cs.LG, math.DG, 65K10, 58C05, 49Q12, 53C25, 57Z20, 57Z25, 68T05
- **Links**: [PDF](http://arxiv.org/pdf/2009.10159v2)
- **Published**: 2020-09-21 20:15:57+00:00
- **Updated**: 2021-06-29 22:06:30+00:00
- **Authors**: Du Nguyen
- **Comment**: None
- **Journal**: None
- **Summary**: We provide an explicit formula for the Levi-Civita connection and Riemannian Hessian for a Riemannian manifold that is a quotient of a manifold embedded in an inner product space with a non-constant metric function. Together with a classical formula for projection, this allows us to evaluate Riemannian gradient and Hessian for several families of metrics on classical manifolds, including a family of metrics on Stiefel manifolds connecting both the constant and canonical ambient metrics with closed-form geodesics. Using these formulas, we derive Riemannian optimization frameworks on quotients of Stiefel manifolds, including flag manifolds, and a new family of complete quotient metrics on the manifold of positive-semidefinite matrices of fixed rank, considered as a quotient of a product of Stiefel and positive-definite matrix manifold with affine-invariant metrics. The method is procedural, and in many instances, the Riemannian gradient and Hessian formulas could be derived by symbolic calculus. The method extends the list of potential metrics that could be used in manifold optimization and machine learning.



### Segmentation and Defect Classification of the Power Line Insulators: A Deep Learning-based Approach
- **Arxiv ID**: http://arxiv.org/abs/2009.10163v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.10163v2)
- **Published**: 2020-09-21 20:25:51+00:00
- **Updated**: 2020-10-15 13:47:27+00:00
- **Authors**: Arman Alahyari, Anton Hinneck, Rahim Tariverdi, David Pozo
- **Comment**: None
- **Journal**: None
- **Summary**: Power transmission networks physically connect the power generators to the electric consumers. Such systems extend over hundreds of kilometers. There are many components in the transmission infrastructure that require a proper inspection to guarantee flawless performance and reliable delivery, which, if done manually, can be very costly and time consuming. One essential component is the insulator. Its failure can cause an interruption of the entire transmission line or a widespread power failure. Automated fault detection could significantly decrease inspection time and related costs. Recently, several works have been proposed based on convolutional neural networks, which address the issue mentioned above. However, existing studies focus on a specific type of insulator faults. Thus, in this study, we introduce a two-stage model that segments insulators from their background to then classify their states based on four different categories, namely: healthy, broken, burned/corroded and missing cap. The test results show that the proposed approach can realize the effective segmentation of insulators and achieve high accuracy in detecting several types of faults.



### Towards Image-based Automatic Meter Reading in Unconstrained Scenarios: A Robust and Efficient Approach
- **Arxiv ID**: http://arxiv.org/abs/2009.10181v5
- **DOI**: 10.1109/ACCESS.2021.3077415
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.10181v5)
- **Published**: 2020-09-21 21:21:23+00:00
- **Updated**: 2021-05-12 04:26:04+00:00
- **Authors**: Rayson Laroca, Alessandra B. Araujo, Luiz A. Zanlorensi, Eduardo C. de Almeida, David Menotti
- **Comment**: None
- **Journal**: IEEE Access, vol. 9, pp. 67569-67584, 2021
- **Summary**: Existing approaches for image-based Automatic Meter Reading (AMR) have been evaluated on images captured in well-controlled scenarios. However, real-world meter reading presents unconstrained scenarios that are way more challenging due to dirt, various lighting conditions, scale variations, in-plane and out-of-plane rotations, among other factors. In this work, we present an end-to-end approach for AMR focusing on unconstrained scenarios. Our main contribution is the insertion of a new stage in the AMR pipeline, called corner detection and counter classification, which enables the counter region to be rectified -- as well as the rejection of illegible/faulty meters -- prior to the recognition stage. We also introduce a publicly available dataset, called Copel-AMR, that contains 12,500 meter images acquired in the field by the service company's employees themselves, including 2,500 images of faulty meters or cases where the reading is illegible due to occlusions. Experimental evaluation demonstrates that the proposed system, which has three networks operating in a cascaded mode, outperforms all baselines in terms of recognition rate while still being quite efficient. Moreover, as very few reading errors are tolerated in real-world applications, we show that our AMR system achieves impressive recognition rates (i.e., > 99%) when rejecting readings made with lower confidence values.



### Federated Learning for Computational Pathology on Gigapixel Whole Slide Images
- **Arxiv ID**: http://arxiv.org/abs/2009.10190v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, q-bio.TO
- **Links**: [PDF](http://arxiv.org/pdf/2009.10190v2)
- **Published**: 2020-09-21 21:56:08+00:00
- **Updated**: 2020-09-23 00:11:40+00:00
- **Authors**: Ming Y. Lu, Dehan Kong, Jana Lipkova, Richard J. Chen, Rajendra Singh, Drew F. K. Williamson, Tiffany Y. Chen, Faisal Mahmood
- **Comment**: None
- **Journal**: None
- **Summary**: Deep Learning-based computational pathology algorithms have demonstrated profound ability to excel in a wide array of tasks that range from characterization of well known morphological phenotypes to predicting non-human-identifiable features from histology such as molecular alterations. However, the development of robust, adaptable, and accurate deep learning-based models often rely on the collection and time-costly curation large high-quality annotated training data that should ideally come from diverse sources and patient populations to cater for the heterogeneity that exists in such datasets. Multi-centric and collaborative integration of medical data across multiple institutions can naturally help overcome this challenge and boost the model performance but is limited by privacy concerns amongst other difficulties that may arise in the complex data sharing process as models scale towards using hundreds of thousands of gigapixel whole slide images. In this paper, we introduce privacy-preserving federated learning for gigapixel whole slide images in computational pathology using weakly-supervised attention multiple instance learning and differential privacy. We evaluated our approach on two different diagnostic problems using thousands of histology whole slide images with only slide-level labels. Additionally, we present a weakly-supervised learning framework for survival prediction and patient stratification from whole slide images and demonstrate its effectiveness in a federated setting. Our results show that using federated learning, we can effectively develop accurate weakly supervised deep learning models from distributed data silos without direct data sharing and its associated complexities, while also preserving differential privacy using randomized noise generation.



### Survey of explainable machine learning with visual and granular methods beyond quasi-explanations
- **Arxiv ID**: http://arxiv.org/abs/2009.10221v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.10221v1)
- **Published**: 2020-09-21 23:39:06+00:00
- **Updated**: 2020-09-21 23:39:06+00:00
- **Authors**: Boris Kovalerchuk, Muhammad Aurangzeb Ahmad, Ankur Teredesai
- **Comment**: 45 pages, 34 figures
- **Journal**: In: Interpretable Artificial Intelligence: A Perspective of
  Granular Computing (Eds. W. Pedrycz, S.M.Chen), Springer, 2021, pp. 217-267
- **Summary**: This paper surveys visual methods of explainability of Machine Learning (ML) with focus on moving from quasi-explanations that dominate in ML to domain-specific explanation supported by granular visuals. ML interpretation is fundamentally a human activity and visual methods are more readily interpretable. While efficient visual representations of high-dimensional data exist, the loss of interpretable information, occlusion, and clutter continue to be a challenge, which lead to quasi-explanations. We start with the motivation and the different definitions of explainability. The paper focuses on a clear distinction between quasi-explanations and domain specific explanations, and between explainable and an actually explained ML model that are critically important for the explainability domain. We discuss foundations of interpretability, overview visual interpretability and present several types of methods to visualize the ML models. Next, we present methods of visual discovery of ML models, with the focus on interpretable models, based on the recently introduced concept of General Line Coordinates (GLC). These methods take the critical step of creating visual explanations that are not merely quasi-explanations but are also domain specific visual explanations while these methods themselves are domain-agnostic. The paper includes results on theoretical limits to preserve n-D distances in lower dimensions, based on the Johnson-Lindenstrauss lemma, point-to-point and point-to-graph GLC approaches, and real-world case studies. The paper also covers traditional visual methods for understanding ML models, which include deep learning and time series models. We show that many of these methods are quasi-explanations and need further enhancement to become domain specific explanations. We conclude with outlining open problems and current research frontiers.



