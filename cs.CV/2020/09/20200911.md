# Arxiv Papers in cs.CV on 2020-09-11
### ARM: A Confidence-Based Adversarial Reweighting Module for Coarse Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2009.05205v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.05205v2)
- **Published**: 2020-09-11 02:31:46+00:00
- **Updated**: 2021-07-24 04:53:59+00:00
- **Authors**: Jingchao Liu, Ye Du, Zehua Fu, Qingjie Liu, Yunhong Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Coarsely-labeled semantic segmentation annotations are easy to obtain, but therefore bear the risk of losing edge details and introducing background pixels. Impeded by the inherent noise, existing coarse annotations are only taken as a bonus for model pre-training. In this paper, we try to exploit their potentials with a confidence-based reweighting strategy. To expand, loss-based reweighting strategies usually take the high loss value to identify two completely different types of pixels, namely, valuable pixels in noise-free annotations and mislabeled pixels in noisy annotations. This makes it impossible to perform two tasks of mining valuable pixels and suppressing mislabeled pixels at the same time. However, with the help of the prediction confidence, we successfully solve this dilemma and simultaneously perform two subtasks with a single reweighting strategy. Furthermore, we generalize this strategy into an Adversarial Reweighting Module (ARM) and prove its convergence strictly. Experiments on standard datasets shows our ARM can bring consistent improvements for both coarse annotations and fine annotations. Specifically, built on top of DeepLabv3+, ARM improves the mIoU on the coarsely-labeled Cityscapes by a considerable margin and increases the mIoU on the ADE20K dataset to 47.50.



### Adversarial Learning for Zero-shot Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2009.05214v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.05214v1)
- **Published**: 2020-09-11 03:41:32+00:00
- **Updated**: 2020-09-11 03:41:32+00:00
- **Authors**: Jinghua Wang, Jianmin Jiang
- **Comment**: None
- **Journal**: ECCV2020
- **Summary**: Zero-shot domain adaptation (ZSDA) is a category of domain adaptation problems where neither data sample nor label is available for parameter learning in the target domain. With the hypothesis that the shift between a given pair of domains is shared across tasks, we propose a new method for ZSDA by transferring domain shift from an irrelevant task (IrT) to the task of interest (ToI). Specifically, we first identify an IrT, where dual-domain samples are available, and capture the domain shift with a coupled generative adversarial networks (CoGAN) in this task. Then, we train a CoGAN for the ToI and restrict it to carry the same domain shift as the CoGAN for IrT does. In addition, we introduce a pair of co-training classifiers to regularize the training procedure of CoGAN in the ToI. The proposed method not only derives machine learning models for the non-available target-domain data, but also synthesizes the data themselves. We evaluate the proposed method on benchmark datasets and achieve the state-of-the-art performances.



### HAA500: Human-Centric Atomic Action Dataset with Curated Videos
- **Arxiv ID**: http://arxiv.org/abs/2009.05224v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2009.05224v2)
- **Published**: 2020-09-11 04:18:41+00:00
- **Updated**: 2021-08-16 16:59:58+00:00
- **Authors**: Jihoon Chung, Cheng-hsin Wuu, Hsuan-ru Yang, Yu-Wing Tai, Chi-Keung Tang
- **Comment**: None
- **Journal**: None
- **Summary**: We contribute HAA500, a manually annotated human-centric atomic action dataset for action recognition on 500 classes with over 591K labeled frames. To minimize ambiguities in action classification, HAA500 consists of highly diversified classes of fine-grained atomic actions, where only consistent actions fall under the same label, e.g., "Baseball Pitching" vs "Free Throw in Basketball". Thus HAA500 is different from existing atomic action datasets, where coarse-grained atomic actions were labeled with coarse action-verbs such as "Throw". HAA500 has been carefully curated to capture the precise movement of human figures with little class-irrelevant motions or spatio-temporal label noises. The advantages of HAA500 are fourfold: 1) human-centric actions with a high average of 69.7% detectable joints for the relevant human poses; 2) high scalability since adding a new class can be done under 20-60 minutes; 3) curated videos capturing essential elements of an atomic action without irrelevant frames; 4) fine-grained atomic action classes. Our extensive experiments including cross-data validation using datasets collected in the wild demonstrate the clear benefits of human-centric and atomic characteristics of HAA500, which enable training even a baseline deep learning model to improve prediction by attending to atomic human poses. We detail the HAA500 dataset statistics and collection methodology and compare quantitatively with existing action recognition datasets.



### Conditional Coupled Generative Adversarial Networks for Zero-Shot Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2009.05228v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.05228v1)
- **Published**: 2020-09-11 04:36:42+00:00
- **Updated**: 2020-09-11 04:36:42+00:00
- **Authors**: Jinghua Wang, Jianmin Jiang
- **Comment**: None
- **Journal**: ICCV2019
- **Summary**: Machine learning models trained in one domain perform poorly in the other domains due to the existence of domain shift. Domain adaptation techniques solve this problem by training transferable models from the label-rich source domain to the label-scarce target domain. Unfortunately, a majority of the existing domain adaptation techniques rely on the availability of target-domain data, and thus limit their applications to a small community across few computer vision problems. In this paper, we tackle the challenging zero-shot domain adaptation (ZSDA) problem, where target-domain data is non-available in the training stage. For this purpose, we propose conditional coupled generative adversarial networks (CoCoGAN) by extending the coupled generative adversarial networks (CoGAN) into a conditioning model. Compared with the existing state of the arts, our proposed CoCoGAN is able to capture the joint distribution of dual-domain samples in two different tasks, i.e. the relevant task (RT) and an irrelevant task (IRT). We train CoCoGAN with both source-domain samples in RT and dual-domain samples in IRT to complete the domain adaptation. While the former provide high-level concepts of the non-available target-domain data, the latter carry the sharing correlation between the two domains in RT and IRT. To train CoCoGAN in the absence of target-domain data for RT, we propose a new supervisory signal, i.e. the alignment between representations across tasks. Extensive experiments carried out demonstrate that our proposed CoCoGAN outperforms existing state of the arts in image classifications.



### An unsupervised deep learning framework via integrated optimization of representation learning and GMM-based modeling
- **Arxiv ID**: http://arxiv.org/abs/2009.05234v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.05234v1)
- **Published**: 2020-09-11 04:57:03+00:00
- **Updated**: 2020-09-11 04:57:03+00:00
- **Authors**: Jinghua Wang, Jianmin Jiang
- **Comment**: None
- **Journal**: ACCV2018
- **Summary**: While supervised deep learning has achieved great success in a range of applications, relatively little work has studied the discovery of knowledge from unlabeled data. In this paper, we propose an unsupervised deep learning framework to provide a potential solution for the problem that existing deep learning techniques require large labeled data sets for completing the training process. Our proposed introduces a new principle of joint learning on both deep representations and GMM (Gaussian Mixture Model)-based deep modeling, and thus an integrated objective function is proposed to facilitate the principle. In comparison with the existing work in similar areas, our objective function has two learning targets, which are created to be jointly optimized to achieve the best possible unsupervised learning and knowledge discovery from unlabeled data sets. While maximizing the first target enables the GMM to achieve the best possible modeling of the data representations and each Gaussian component corresponds to a compact cluster, maximizing the second term will enhance the separability of the Gaussian components and hence the inter-cluster distances. As a result, the compactness of clusters is significantly enhanced by reducing the intra-cluster distances, and the separability is improved by increasing the inter-cluster distances. Extensive experimental results show that the propose method can improve the clustering performance compared with benchmark methods.



### Spectral Analysis Network for Deep Representation Learning and Image Clustering
- **Arxiv ID**: http://arxiv.org/abs/2009.05235v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.05235v1)
- **Published**: 2020-09-11 05:07:15+00:00
- **Updated**: 2020-09-11 05:07:15+00:00
- **Authors**: Jinghua Wang, Adrian Hilton, Jianmin Jiang
- **Comment**: None
- **Journal**: ICME2019
- **Summary**: Deep representation learning is a crucial procedure in multimedia analysis and attracts increasing attention. Most of the popular techniques rely on convolutional neural network and require a large amount of labeled data in the training procedure. However, it is time consuming or even impossible to obtain the label information in some tasks due to cost limitation. Thus, it is necessary to develop unsupervised deep representation learning techniques. This paper proposes a new network structure for unsupervised deep representation learning based on spectral analysis, which is a popular technique with solid theory foundations. Compared with the existing spectral analysis methods, the proposed network structure has at least three advantages. Firstly, it can identify the local similarities among images in patch level and thus more robust against occlusion. Secondly, through multiple consecutive spectral analysis procedures, the proposed network can learn more clustering-friendly representations and is capable to reveal the deep correlations among data samples. Thirdly, it can elegantly integrate different spectral analysis procedures, so that each spectral analysis procedure can have their individual strengths in dealing with different data sample distributions. Extensive experimental results show the effectiveness of the proposed methods on various image clustering tasks.



### An Efficient Quantitative Approach for Optimizing Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2009.05236v4
- **DOI**: 10.1145/3459637.3482230
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2009.05236v4)
- **Published**: 2020-09-11 05:14:34+00:00
- **Updated**: 2021-09-15 16:59:45+00:00
- **Authors**: Yuke Wang, Boyuan Feng, Xueqiao Peng, Yufei Ding
- **Comment**: None
- **Journal**: None
- **Summary**: With the increasing popularity of deep learning, Convolutional Neural Networks (CNNs) have been widely applied in various domains, such as image classification and object detection, and achieve stunning success in terms of their high accuracy over the traditional statistical methods. To exploit the potential of CNN models, a huge amount of research and industry efforts have been devoted to optimizing CNNs. Among these endeavors, CNN architecture design has attracted tremendous attention because of its great potential of improving model accuracy or reducing model complexity. However, existing work either introduces repeated training overhead in the search process or lacks an interpretable metric to guide the design. To clear these hurdles, we propose 3D-Receptive Field (3DRF), an explainable and easy-to-compute metric, to estimate the quality of a CNN architecture and guide the search process of designs. To validate the effectiveness of 3DRF, we build a static optimizer to improve the CNN architectures at both the stage level and the kernel level. Our optimizer not only provides a clear and reproducible procedure but also mitigates unnecessary training efforts in the architecture search process. Extensive experiments and studies show that the models generated by our optimizer can achieve up to 5.47% accuracy improvement and up to 65.38% parameters deduction, compared with state-of-the-art CNN structures like MobileNet and ResNet.



### SA-Net: A deep spectral analysis network for image clustering
- **Arxiv ID**: http://arxiv.org/abs/2009.07026v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.07026v1)
- **Published**: 2020-09-11 05:27:23+00:00
- **Updated**: 2020-09-11 05:27:23+00:00
- **Authors**: Jinghua Wang, Jianmin Jiang
- **Comment**: arXiv admin note: text overlap with arXiv:2009.05235
- **Journal**: Neurocomputing 2020
- **Summary**: Although supervised deep representation learning has attracted enormous attentions across areas of pattern recognition and computer vision, little progress has been made towards unsupervised deep representation learning for image clustering. In this paper, we propose a deep spectral analysis network for unsupervised representation learning and image clustering. While spectral analysis is established with solid theoretical foundations and has been widely applied to unsupervised data mining, its essential weakness lies in the fact that it is difficult to construct a proper affinity matrix and determine the involving Laplacian matrix for a given dataset. In this paper, we propose a SA-Net to overcome these weaknesses and achieve improved image clustering by extending the spectral analysis procedure into a deep learning framework with multiple layers. The SA-Net has the capability to learn deep representations and reveal deep correlations among data samples. Compared with the existing spectral analysis, the SA-Net achieves two advantages: (i) Given the fact that one spectral analysis procedure can only deal with one subset of the given dataset, our proposed SA-Net elegantly integrates multiple parallel and consecutive spectral analysis procedures together to enable interactive learning across different units towards a coordinated clustering model; (ii) Our SA-Net can identify the local similarities among different images at patch level and hence achieves a higher level of robustness against occlusions. Extensive experiments on a number of popular datasets support that our proposed SA-Net outperforms 11 benchmarks across a number of image clustering applications.



### Defending Against Multiple and Unforeseen Adversarial Videos
- **Arxiv ID**: http://arxiv.org/abs/2009.05244v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2009.05244v3)
- **Published**: 2020-09-11 06:07:14+00:00
- **Updated**: 2021-12-14 06:38:07+00:00
- **Authors**: Shao-Yuan Lo, Vishal M. Patel
- **Comment**: Accepted in IEEE Transactions on Image Processing (TIP)
- **Journal**: None
- **Summary**: Adversarial robustness of deep neural networks has been actively investigated. However, most existing defense approaches are limited to a specific type of adversarial perturbations. Specifically, they often fail to offer resistance to multiple attack types simultaneously, i.e., they lack multi-perturbation robustness. Furthermore, compared to image recognition problems, the adversarial robustness of video recognition models is relatively unexplored. While several studies have proposed how to generate adversarial videos, only a handful of approaches about defense strategies have been published in the literature. In this paper, we propose one of the first defense strategies against multiple types of adversarial videos for video recognition. The proposed method, referred to as MultiBN, performs adversarial training on multiple adversarial video types using multiple independent batch normalization (BN) layers with a learning-based BN selection module. With a multiple BN structure, each BN brach is responsible for learning the distribution of a single perturbation type and thus provides more precise distribution estimations. This mechanism benefits dealing with multiple perturbation types. The BN selection module detects the attack type of an input video and sends it to the corresponding BN branch, making MultiBN fully automatic and allowing end-to-end training. Compared to present adversarial training approaches, the proposed MultiBN exhibits stronger multi-perturbation robustness against different and even unforeseen adversarial video types, ranging from Lp-bounded attacks and physically realizable attacks. This holds true on different datasets and target models. Moreover, we conduct an extensive analysis to study the properties of the multiple BN structure.



### Devil's in the Details: Aligning Visual Clues for Conditional Embedding in Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2009.05250v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.05250v2)
- **Published**: 2020-09-11 06:28:56+00:00
- **Updated**: 2020-12-07 11:07:51+00:00
- **Authors**: Fufu Yu, Xinyang Jiang, Yifei Gong, Shizhen Zhao, Xiaowei Guo, Wei-Shi Zheng, Feng Zheng, Xing Sun
- **Comment**: None
- **Journal**: None
- **Summary**: Although Person Re-Identification has made impressive progress, difficult cases like occlusion, change of view-pointand similar clothing still bring great challenges. Besides overall visual features, matching and comparing detailed information is also essential for tackling these challenges. This paper proposes two key recognition patterns to better utilize the detail information of pedestrian images, that most of the existing methods are unable to satisfy. Firstly, Visual Clue Alignment requires the model to select and align decisive regions pairs from two images for pair-wise comparison, while existing methods only align regions with predefined rules like high feature similarity or same semantic labels. Secondly, the Conditional Feature Embedding requires the overall feature of a query image to be dynamically adjusted based on the gallery image it matches, while most of the existing methods ignore the reference images. By introducing novel techniques including correspondence attention module and discrepancy-based GCN, we propose an end-to-end ReID method that integrates both patterns into a unified framework, called CACE-Net((C)lue(A)lignment and (C)onditional (E)mbedding). The experiments show that CACE-Net achieves state-of-the-art performance on three public datasets.



### Novel and Effective CNN-Based Binarization for Historically Degraded As-built Drawing Maps
- **Arxiv ID**: http://arxiv.org/abs/2009.05252v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.05252v1)
- **Published**: 2020-09-11 06:49:28+00:00
- **Updated**: 2020-09-11 06:49:28+00:00
- **Authors**: Kuo-Liang Chung, De-Wei Hsieh
- **Comment**: None
- **Journal**: None
- **Summary**: Binarizing historically degraded as-built drawing (HDAD) maps is a new challenging job, especially in terms of removing the three artifacts, namely noise, the yellowing areas, and the folded lines, while preserving the foreground components well. In this paper, we first propose a semi-automatic labeling method to create the HDAD-pair dataset of which each HDAD-pair consists of one HDAD map and its binarized HDAD map. Based on the created training HDAD-pair dataset, we propose a convolutional neural network-based (CNN-based) binarization method to produce high-quality binarized HDAD maps. Based on the testing HDAD maps, the thorough experimental data demonstrated that in terms of the accuracy, PSNR (peak-signal-to-noise-ratio), and the perceptual effect of the binarized HDAD maps, our method substantially outperforms the nine existing binarization methods. In addition, with similar accuracy, the experimental results demonstrated the significant execution-time reduction merit of our method relative to the retrained version of the state-of-the-art CNN-based binarization methods.



### Visually Analyzing and Steering Zero Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2009.05254v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.05254v1)
- **Published**: 2020-09-11 06:58:13+00:00
- **Updated**: 2020-09-11 06:58:13+00:00
- **Authors**: Saroj Sahoo, Matthew Berger
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a visual analytics system to help a user analyze and steer zero-shot learning models. Zero-shot learning has emerged as a viable scenario for categorizing data that consists of no labeled examples, and thus a promising approach to minimize data annotation from humans. However, it is challenging to understand where zero-shot learning fails, the cause of such failures, and how a user can modify the model to prevent such failures. Our visualization system is designed to help users diagnose and understand mispredictions in such models, so that they may gain insight on the behavior of a model when applied to data associated with categories not seen during training. Through usage scenarios, we highlight how our system can help a user improve performance in zero-shot learning.



### PiaNet: A pyramid input augmented convolutional neural network for GGO detection in 3D lung CT scans
- **Arxiv ID**: http://arxiv.org/abs/2009.05267v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.05267v2)
- **Published**: 2020-09-11 07:52:17+00:00
- **Updated**: 2020-10-10 05:23:46+00:00
- **Authors**: Weihua Liu, Xiabi Liua, Xiongbiao Luo, Murong Wang, Guanghui Han, Xinming Zhao, Zheng Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a new convolutional neural network with multiscale processing for detecting ground-glass opacity (GGO) nodules in 3D computed tomography (CT) images, which is referred to as PiaNet for short. PiaNet consists of a feature-extraction module and a prediction module. The former module is constructed by introducing pyramid multiscale source connections into a contracting-expanding structure. The latter module includes a bounding-box regressor and a classifier that are employed to simultaneously recognize GGO nodules and estimate bounding boxes at multiple scales. To train the proposed PiaNet, a two-stage transfer learning strategy is developed. In the first stage, the feature-extraction module is embedded into a classifier network that is trained on a large data set of GGO and non-GGO patches, which are generated by performing data augmentation from a small number of annotated CT scans. In the second stage, the pretrained feature-extraction module is loaded into PiaNet, and then PiaNet is fine-tuned using the annotated CT scans. We evaluate the proposed PiaNet on the LIDC-IDRI data set. The experimental results demonstrate that our method outperforms state-of-the-art counterparts, including the Subsolid CAD and Aidence systems and S4ND and GA-SSD methods. PiaNet achieves a sensitivity of 91.75% with only one false positive per scan



### Image Conditioned Keyframe-Based Video Summarization Using Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2009.05269v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.05269v1)
- **Published**: 2020-09-11 07:56:17+00:00
- **Updated**: 2020-09-11 07:56:17+00:00
- **Authors**: Neeraj Baghel, Suresh C. Raikwar, Charul Bhatnagar
- **Comment**: Paper is submitted in Pattern Analysis and Applications
  PAAA-D-19-00604
- **Journal**: None
- **Summary**: Video summarization plays an important role in selecting keyframe for understanding a video. Traditionally, it aims to find the most representative and diverse contents (or frames) in a video for short summaries. Recently, query-conditioned video summarization has been introduced, which considers user queries to learn more user-oriented summaries and its preference. However, there are obstacles in text queries for user subjectivity and finding similarity between the user query and input frames. In this work, (i) Image is introduced as a query for user preference (ii) a mathematical model is proposed to minimize redundancy based on the loss function & summary variance and (iii) the similarity score between the query image and input video to obtain the summarized video. Furthermore, the Object-based Query Image (OQI) dataset has been introduced, which contains the query images. The proposed method has been validated using UT Egocentric (UTE) dataset. The proposed model successfully resolved the issues of (i) user preference, (ii) recognize important frames and selecting that keyframe in daily life videos, with different illumination conditions. The proposed method achieved 57.06% average F1-Score for UTE dataset and outperforms the existing state-of-theart by 11.01%. The process time is 7.81 times faster than actual time of video Experiments on a recently proposed UTE dataset show the efficiency of the proposed method



### AFP-SRC: Identification of Antifreeze Proteins Using Sparse Representation Classifier
- **Arxiv ID**: http://arxiv.org/abs/2009.05277v3
- **DOI**: 10.1007/s00521-021-06558-7
- **Categories**: **cs.CV**, cs.LG, eess.SP, q-bio.BM
- **Links**: [PDF](http://arxiv.org/pdf/2009.05277v3)
- **Published**: 2020-09-11 08:24:50+00:00
- **Updated**: 2021-09-24 11:59:33+00:00
- **Authors**: Shujaat Khan, Muhammad Usman, Abdul Wahab
- **Comment**: This preprint has not undergone any post-submission improvements or
  corrections. The Version of Record of this article is published in Neural
  Computing and Applications, and is available online at
  https://doi.org/10.1007/s00521-021-06558-7
- **Journal**: None
- **Summary**: Species living in the extreme cold environment fight against the harsh conditions using antifreeze proteins (AFPs), that manipulates the freezing mechanism of water in more than one way. This amazing nature of AFP turns out to be extremely useful in several industrial and medical applications. The lack of similarity in their structure and sequence makes their prediction an arduous task and identifying them experimentally in the wet-lab is time-consuming and expensive. In this research, we propose a computational framework for the prediction of AFPs which is essentially based on a sample-specific classification method using the sparse reconstruction. A linear model and an over-complete dictionary matrix of known AFPs are used to predict a sparse class-label vector that provides a sample-association score. Delta-rule is applied for the reconstruction of two pseudo-samples using lower and upper parts of the sample-association vector and based on the minimum recovery score, class labels are assigned. We compare our approach with contemporary methods on a standard dataset and the proposed method is found to outperform in terms of Balanced accuracy and Youden's index. The MATLAB implementation of the proposed method is available at the author's GitHub page (\{https://github.com/Shujaat123/AFP-SRC}{https://github.com/Shujaat123/AFP-SRC}).



### Fair and accurate age prediction using distribution aware data curation and augmentation
- **Arxiv ID**: http://arxiv.org/abs/2009.05283v6
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.05283v6)
- **Published**: 2020-09-11 08:32:36+00:00
- **Updated**: 2021-11-16 10:25:52+00:00
- **Authors**: Yushi Cao, David Berend, Palina Tolmach, Guy Amit, Moshe Levy, Yang Liu, Asaf Shabtai, Yuval Elovici
- **Comment**: Preprint, accepted at WACV'22
- **Journal**: None
- **Summary**: Deep learning-based facial recognition systems have experienced increased media attention due to exhibiting unfair behavior. Large enterprises, such as IBM, shut down their facial recognition and age prediction systems as a consequence. Age prediction is an especially difficult application with the issue of fairness remaining an open research problem (e.g., predicting age for different ethnicity equally accurate). One of the main causes of unfair behavior in age prediction methods lies in the distribution and diversity of the training data. In this work, we present two novel approaches for dataset curation and data augmentation in order to increase fairness through balanced feature curation and increase diversity through distribution aware augmentation. To achieve this, we introduce out-of-distribution detection to the facial recognition domain which is used to select the data most relevant to the deep neural network's (DNN) task when balancing the data among age, ethnicity, and gender. Our approach shows promising results. Our best-trained DNN model outperformed all academic and industrial baselines in terms of fairness by up to 4.92 times and also enhanced the DNN's ability to generalize outperforming Amazon AWS and Microsoft Azure public cloud systems by 31.88% and 10.95%, respectively.



### Attribute-conditioned Layout GAN for Automatic Graphic Design
- **Arxiv ID**: http://arxiv.org/abs/2009.05284v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2009.05284v1)
- **Published**: 2020-09-11 08:34:17+00:00
- **Updated**: 2020-09-11 08:34:17+00:00
- **Authors**: Jianan Li, Jimei Yang, Jianming Zhang, Chang Liu, Christina Wang, Tingfa Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Modeling layout is an important first step for graphic design. Recently, methods for generating graphic layouts have progressed, particularly with Generative Adversarial Networks (GANs). However, the problem of specifying the locations and sizes of design elements usually involves constraints with respect to element attributes, such as area, aspect ratio and reading-order. Automating attribute conditional graphic layouts remains a complex and unsolved problem. In this paper, we introduce Attribute-conditioned Layout GAN to incorporate the attributes of design elements for graphic layout generation by forcing both the generator and the discriminator to meet attribute conditions. Due to the complexity of graphic designs, we further propose an element dropout method to make the discriminator look at partial lists of elements and learn their local patterns. In addition, we introduce various loss designs following different design principles for layout optimization. We demonstrate that the proposed method can synthesize graphic layouts conditioned on different element attributes. It can also adjust well-designed layouts to new sizes while retaining elements' original reading-orders. The effectiveness of our method is validated through a user study.



### Unsupervised Partial Point Set Registration via Joint Shape Completion and Registration
- **Arxiv ID**: http://arxiv.org/abs/2009.05290v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.05290v1)
- **Published**: 2020-09-11 08:50:53+00:00
- **Updated**: 2020-09-11 08:50:53+00:00
- **Authors**: Xiang Li, Lingjing Wang, Yi Fang
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: We propose a self-supervised method for partial point set registration. While recent proposed learning-based methods have achieved impressive registration performance on the full shape observations, these methods mostly suffer from performance degradation when dealing with partial shapes. To bridge the performance gaps between partial point set registration with full point set registration, we proposed to incorporate a shape completion network to benefit the registration process. To achieve this, we design a latent code for each pair of shapes, which can be regarded as a geometric encoding of the target shape. By doing so, our model does need an explicit feature embedding network to learn the feature encodings. More importantly, both our shape completion network and the point set registration network take the shared latent codes as input, which are optimized along with the parameters of two decoder networks in the training process. Therefore, the point set registration process can thus benefit from the joint optimization process of latent codes, which are enforced to represent the information of full shape instead of partial ones. In the inference stage, we fix the network parameter and optimize the latent codes to get the optimal shape completion and registration results. Our proposed method is pure unsupervised and does not need any ground truth supervision. Experiments on the ModelNet40 dataset demonstrate the effectiveness of our model for partial point set registration.



### Enabling Image Recognition on Constrained Devices Using Neural Network Pruning and a CycleGAN
- **Arxiv ID**: http://arxiv.org/abs/2009.05300v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.05300v1)
- **Published**: 2020-09-11 09:12:52+00:00
- **Updated**: 2020-09-11 09:12:52+00:00
- **Authors**: August Lidfelt, Daniel Isaksson, Ludwig Hedlund, Simon Åberg, Markus Borg, Erik Larsson
- **Comment**: Accepted for publication in the Proc. of the 1st international
  workshop on Internet of Things for Emergency Management
- **Journal**: None
- **Summary**: Smart cameras are increasingly used in surveillance solutions in public spaces. Contemporary computer vision applications can be used to recognize events that require intervention by emergency services. Smart cameras can be mounted in locations where citizens feel particularly unsafe, e.g., pathways and underpasses with a history of incidents. One promising approach for smart cameras is edge AI, i.e., deploying AI technology on IoT devices. However, implementing resource-demanding technology such as image recognition using deep neural networks (DNN) on constrained devices is a substantial challenge. In this paper, we explore two approaches to reduce the need for compute in contemporary image recognition in an underpass. First, we showcase successful neural network pruning, i.e., we retain comparable classification accuracy with only 1.1\% of the neurons remaining from the state-of-the-art DNN architecture. Second, we demonstrate how a CycleGAN can be used to transform out-of-distribution images to the operational design domain. We posit that both pruning and CycleGANs are promising enablers for efficient edge AI in smart cameras.



### A Density-Aware PointRCNN for 3D Object Detection in Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2009.05307v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.05307v2)
- **Published**: 2020-09-11 09:35:13+00:00
- **Updated**: 2021-01-08 04:33:52+00:00
- **Authors**: Jie Li, Yu Hu
- **Comment**: None
- **Journal**: None
- **Summary**: We present an improved version of PointRCNN for 3D object detection, in which a multi-branch backbone network is adopted to handle the non-uniform density of point clouds. An uncertainty-based sampling policy is proposed to deal with the distribution differences of different point clouds. The new model can achieve about 0.8 AP higher performance than the baseline PointRCNN on KITTI val set. In addition, a simplified model using a single scale grouping for each set-abstraction layer can achieve competitive performance with less computational cost.



### SoFAr: Shortcut-based Fractal Architectures for Binary Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2009.05317v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.05317v1)
- **Published**: 2020-09-11 10:00:47+00:00
- **Updated**: 2020-09-11 10:00:47+00:00
- **Authors**: Zhu Baozhou, Peter Hofstee, Jinho Lee, Zaid Al-Ars
- **Comment**: None
- **Journal**: None
- **Summary**: Binary Convolutional Neural Networks (BCNNs) can significantly improve the efficiency of Deep Convolutional Neural Networks (DCNNs) for their deployment on resource-constrained platforms, such as mobile and embedded systems. However, the accuracy degradation of BCNNs is still considerable compared with their full precision counterpart, impeding their practical deployment. Because of the inevitable binarization error in the forward propagation and gradient mismatch problem in the backward propagation, it is nontrivial to train BCNNs to achieve satisfactory accuracy. To ease the difficulty of training, the shortcut-based BCNNs, such as residual connection-based Bi-real ResNet and dense connection-based BinaryDenseNet, introduce additional shortcuts in addition to the shortcuts already present in their full precision counterparts. Furthermore, fractal architectures have been also been used to improve the training process of full-precision DCNNs since the fractal structure triggers effects akin to deep supervision and lateral student-teacher information flow. Inspired by the shortcuts and fractal architectures, we propose two Shortcut-based Fractal Architectures (SoFAr) specifically designed for BCNNs: 1. residual connection-based fractal architectures for binary ResNet, and 2. dense connection-based fractal architectures for binary DenseNet. Our proposed SoFAr combines the adoption of shortcuts and the fractal architectures in one unified model, which is helpful in the training of BCNNs. Results show that our proposed SoFAr achieves better accuracy compared with shortcut-based BCNNs. Specifically, the Top-1 accuracy of our proposed RF-c4d8 ResNet37(41) and DRF-c2d2 DenseNet51(53) on ImageNet outperforms Bi-real ResNet18(64) and BinaryDenseNet51(32) by 3.29% and 1.41%, respectively, with the same computational complexity overhead.



### The PREVENTION Challenge: How Good Are Humans Predicting Lane Changes?
- **Arxiv ID**: http://arxiv.org/abs/2009.05331v2
- **DOI**: 10.1109/IV47402.2020.9304640
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.05331v2)
- **Published**: 2020-09-11 10:47:07+00:00
- **Updated**: 2021-06-07 23:49:54+00:00
- **Authors**: A. Quintanar, R. Izquierdo, I. Parra, D. Fernández-Llorca, M. A. Sotelo
- **Comment**: This work was accepted and presented at IEEE Intelligent Vehicles
  Symposium 2020
- **Journal**: 2020 IEEE Intelligent Vehicles Symposium (IV), 2020, pp. 45-50
- **Summary**: While driving on highways, every driver tries to be aware of the behavior of surrounding vehicles, including possible emergency braking, evasive maneuvers trying to avoid obstacles, unexpected lane changes, or other emergencies that could lead to an accident. In this paper, human's ability to predict lane changes in highway scenarios is analyzed through the use of video sequences extracted from the PREVENTION dataset, a database focused on the development of research on vehicle intention and trajectory prediction. Thus, users had to indicate the moment at which they considered that a lane change maneuver was taking place in a target vehicle, subsequently indicating its direction: left or right. The results retrieved have been carefully analyzed and compared to ground truth labels, evaluating statistical models to understand whether humans can actually predict. The study has revealed that most participants are unable to anticipate lane-change maneuvers, detecting them after they have started. These results might serve as a baseline for AI's prediction ability evaluation, grading if those systems can outperform human skills by analyzing hidden cues that seem unnoticed, improving the detection time, and even anticipating maneuvers in some cases.



### Disentangling Neural Architectures and Weights: A Case Study in Supervised Classification
- **Arxiv ID**: http://arxiv.org/abs/2009.05346v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2009.05346v1)
- **Published**: 2020-09-11 11:22:22+00:00
- **Updated**: 2020-09-11 11:22:22+00:00
- **Authors**: Nicolo Colombo, Yang Gao
- **Comment**: 22 pages and 10 figures
- **Journal**: None
- **Summary**: The history of deep learning has shown that human-designed problem-specific networks can greatly improve the classification performance of general neural models. In most practical cases, however, choosing the optimal architecture for a given task remains a challenging problem. Recent architecture-search methods are able to automatically build neural models with strong performance but fail to fully appreciate the interaction between neural architecture and weights. This work investigates the problem of disentangling the role of the neural structure and its edge weights, by showing that well-trained architectures may not need any link-specific fine-tuning of the weights. We compare the performance of such weight-free networks (in our case these are binary networks with {0, 1}-valued weights) with random, weight-agnostic, pruned and standard fully connected networks. To find the optimal weight-agnostic network, we use a novel and computationally efficient method that translates the hard architecture-search problem into a feasible optimization problem.More specifically, we look at the optimal task-specific architectures as the optimal configuration of binary networks with {0, 1}-valued weights, which can be found through an approximate gradient descent strategy. Theoretical convergence guarantees of the proposed algorithm are obtained by bounding the error in the gradient approximation and its practical performance is evaluated on two real-world data sets. For measuring the structural similarities between different architectures, we use a novel spectral approach that allows us to underline the intrinsic differences between real-valued networks and weight-free architectures.



### Meta Learning for Few-Shot One-class Classification
- **Arxiv ID**: http://arxiv.org/abs/2009.05353v2
- **DOI**: 10.3390/ai2020012
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.05353v2)
- **Published**: 2020-09-11 11:35:28+00:00
- **Updated**: 2020-09-16 12:13:17+00:00
- **Authors**: Gabriel Dahia, Maurício Pamplona Segundo
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a method that can perform one-class classification given only a small number of examples from the target class and none from the others. We formulate the learning of meaningful features for one-class classification as a meta-learning problem in which the meta-training stage repeatedly simulates one-class classification, using the classification loss of the chosen algorithm to learn a feature representation. To learn these representations, we require only multiclass data from similar tasks. We show how the Support Vector Data Description method can be used with our method, and also propose a simpler variant based on Prototypical Networks that obtains comparable performance, indicating that learning feature representations directly from data may be more important than which one-class algorithm we choose. We validate our approach by adapting few-shot classification datasets to the few-shot one-class classification scenario, obtaining similar results to the state-of-the-art of traditional one-class classification, and that improves upon that of one-class classification baselines employed in the few-shot setting. Our code is available at https://github.com/gdahia/meta_occ



### Evaluation of the Robustness of Visual SLAM Methods in Different Environments
- **Arxiv ID**: http://arxiv.org/abs/2009.05427v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.05427v1)
- **Published**: 2020-09-11 13:21:34+00:00
- **Updated**: 2020-09-11 13:21:34+00:00
- **Authors**: Joonas Lomps, Artjom Lind, Amnir Hadachi
- **Comment**: None
- **Journal**: None
- **Summary**: Determining the position and orientation of a sensor vis-a-vis its surrounding, while simultaneously mapping the environment around that sensor or simultaneous localization and mapping is quickly becoming an important advancement in embedded vision with a large number of different possible applications. This paper presents a comprehensive comparison of the latest open-source SLAM algorithms with the main focus being their performance in different environmental surroundings. The chosen algorithms are evaluated on common publicly available datasets and the results reasoned with respect to the datasets' environment. This is the first stage of our main target of testing the methods in off-road scenarios.



### Embodied Visual Navigation with Automatic Curriculum Learning in Real Environments
- **Arxiv ID**: http://arxiv.org/abs/2009.05429v2
- **DOI**: 10.1109/LRA.2020.3048662
- **Categories**: **cs.RO**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.05429v2)
- **Published**: 2020-09-11 13:28:26+00:00
- **Updated**: 2021-01-06 18:29:42+00:00
- **Authors**: Steven D. Morad, Roberto Mecca, Rudra P. K. Poudel, Stephan Liwicki, Roberto Cipolla
- **Comment**: None
- **Journal**: None
- **Summary**: We present NavACL, a method of automatic curriculum learning tailored to the navigation task. NavACL is simple to train and efficiently selects relevant tasks using geometric features. In our experiments, deep reinforcement learning agents trained using NavACL significantly outperform state-of-the-art agents trained with uniform sampling -- the current standard. Furthermore, our agents can navigate through unknown cluttered indoor environments to semantically-specified targets using only RGB images. Obstacle-avoiding policies and frozen feature networks support transfer to unseen real-world environments, without any modification or retraining requirements. We evaluate our policies in simulation, and in the real world on a ground robot and a quadrotor drone. Videos of real-world results are available in the supplementary material.



### Heterogeneous Domain Generalization via Domain Mixup
- **Arxiv ID**: http://arxiv.org/abs/2009.05448v1
- **DOI**: 10.1109/ICASSP40776.2020.9053273
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.05448v1)
- **Published**: 2020-09-11 13:53:56+00:00
- **Updated**: 2020-09-11 13:53:56+00:00
- **Authors**: Yufei Wang, Haoliang Li, Alex C. Kot
- **Comment**: None
- **Journal**: None
- **Summary**: One of the main drawbacks of deep Convolutional Neural Networks (DCNN) is that they lack generalization capability. In this work, we focus on the problem of heterogeneous domain generalization which aims to improve the generalization capability across different tasks, which is, how to learn a DCNN model with multiple domain data such that the trained feature extractor can be generalized to supporting recognition of novel categories in a novel target domain. To solve this problem, we propose a novel heterogeneous domain generalization method by mixing up samples across multiple source domains with two different sampling strategies. Our experimental results based on the Visual Decathlon benchmark demonstrates the effectiveness of our proposed method. The code is released in \url{https://github.com/wyf0912/MIXALL}



### Object Recognition for Economic Development from Daytime Satellite Imagery
- **Arxiv ID**: http://arxiv.org/abs/2009.05455v1
- **DOI**: None
- **Categories**: **econ.GN**, cs.CV, eess.IV, q-fin.EC
- **Links**: [PDF](http://arxiv.org/pdf/2009.05455v1)
- **Published**: 2020-09-11 14:07:12+00:00
- **Updated**: 2020-09-11 14:07:12+00:00
- **Authors**: Klaus Ackermann, Alexey Chernikov, Nandini Anantharama, Miethy Zaman, Paul A Raschky
- **Comment**: 8 pages, 7 figures
- **Journal**: None
- **Summary**: Reliable data about the stock of physical capital and infrastructure in developing countries is typically very scarce. This is particular a problem for data at the subnational level where existing data is often outdated, not consistently measured or coverage is incomplete. Traditional data collection methods are time and labor-intensive costly, which often prohibits developing countries from collecting this type of data. This paper proposes a novel method to extract infrastructure features from high-resolution satellite images. We collected high-resolution satellite images for 5 million 1km $\times$ 1km grid cells covering 21 African countries. We contribute to the growing body of literature in this area by training our machine learning algorithm on ground-truth data. We show that our approach strongly improves the predictive accuracy. Our methodology can build the foundation to then predict subnational indicators of economic development for areas where this data is either missing or unreliable.



### Adversarial score matching and improved sampling for image generation
- **Arxiv ID**: http://arxiv.org/abs/2009.05475v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2009.05475v2)
- **Published**: 2020-09-11 14:49:53+00:00
- **Updated**: 2020-10-10 19:47:12+00:00
- **Authors**: Alexia Jolicoeur-Martineau, Rémi Piché-Taillefer, Rémi Tachet des Combes, Ioannis Mitliagkas
- **Comment**: Code at
  https://github.com/AlexiaJM/AdversarialConsistentScoreMatching
- **Journal**: None
- **Summary**: Denoising Score Matching with Annealed Langevin Sampling (DSM-ALS) has recently found success in generative modeling. The approach works by first training a neural network to estimate the score of a distribution, and then using Langevin dynamics to sample from the data distribution assumed by the score network. Despite the convincing visual quality of samples, this method appears to perform worse than Generative Adversarial Networks (GANs) under the Fr\'echet Inception Distance, a standard metric for generative models.   We show that this apparent gap vanishes when denoising the final Langevin samples using the score network. In addition, we propose two improvements to DSM-ALS: 1) Consistent Annealed Sampling as a more stable alternative to Annealed Langevin Sampling, and 2) a hybrid training formulation, composed of both Denoising Score Matching and adversarial objectives. By combining these two techniques and exploring different network architectures, we elevate score matching methods and obtain results competitive with state-of-the-art image generation on CIFAR-10.



### MRZ code extraction from visa and passport documents using convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/2009.05489v2
- **DOI**: 10.1007/s10032-021-00384-2
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.05489v2)
- **Published**: 2020-09-11 15:12:16+00:00
- **Updated**: 2021-07-20 19:09:32+00:00
- **Authors**: Yichuan Liu, Hailey James, Otkrist Gupta, Dan Raviv
- **Comment**: This is a preprint of https://doi.org/10.1007/s10032-021-00384-2
- **Journal**: None
- **Summary**: Detecting and extracting information from Machine-Readable Zone (MRZ) on passports and visas is becoming increasingly important for verifying document authenticity. However, computer vision methods for performing similar tasks, such as optical character recognition (OCR), fail to extract the MRZ given digital images of passports with reasonable accuracy. We present a specially designed model based on convolutional neural networks that is able to successfully extract MRZ information from digital images of passports of arbitrary orientation and size. Our model achieved 100% MRZ detection rate and 98.36% character recognition macro-f1 score on a passport and visa dataset.



### Dual Encoder Fusion U-Net (DEFU-Net) for Cross-manufacturer Chest X-ray Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2009.10608v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.10608v3)
- **Published**: 2020-09-11 15:57:44+00:00
- **Updated**: 2020-10-26 07:34:17+00:00
- **Authors**: Lipei Zhang, Aozhi Liu, Jing Xiao, Paul Taylor
- **Comment**: 6 pages, 6 figures, 3 tables, accepted by ICPR
- **Journal**: None
- **Summary**: A number of methods based on deep learning have been applied to medical image segmentation and have achieved state-of-the-art performance. Due to the importance of chest x-ray data in studying COVID-19, there is a demand for state-of-the-art models capable of precisely segmenting soft tissue on the chest x-rays. The dataset for exploring best segmentation model is from Montgomery and Shenzhen hospital which had opened in 2014. The most famous technique is U-Net which has been used to many medical datasets including the Chest X-rays. However, most variant U-Nets mainly focus on extraction of contextual information and skip connections. There is still a large space for improving extraction of spatial features. In this paper, we propose a dual encoder fusion U-Net framework for Chest X-rays based on Inception Convolutional Neural Network with dilation, Densely Connected Recurrent Convolutional Neural Network, which is named DEFU-Net. The densely connected recurrent path extends the network deeper for facilitating contextual feature extraction. In order to increase the width of network and enrich representation of features, the inception blocks with dilation are adopted. The inception blocks can capture globally and locally spatial information from various receptive fields. At the same time, the two paths are fused by summing features, thus preserving the contextual and spatial information for decoding part. This multi-learning-scale model is benefiting in Chest X-ray dataset from two different manufacturers (Montgomery and Shenzhen hospital). The DEFU-Net achieves the better performance than basic U-Net, residual U-Net, BCDU-Net, R2U-Net and attention R2U-Net. This model has proved the feasibility for mixed dataset and approaches state-of-the-art. The source code for this proposed framework is public https://github.com/uceclz0/DEFU-Net.



### TP-LSD: Tri-Points Based Line Segment Detector
- **Arxiv ID**: http://arxiv.org/abs/2009.05505v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.05505v1)
- **Published**: 2020-09-11 16:08:12+00:00
- **Updated**: 2020-09-11 16:08:12+00:00
- **Authors**: Siyu Huang, Fangbo Qin, Pengfei Xiong, Ning Ding, Yijia He, Xiao Liu
- **Comment**: Accepted by ECCV 2020
- **Journal**: Europeon Conference on Computer Vision (2020)
- **Summary**: This paper proposes a novel deep convolutional model, Tri-Points Based Line Segment Detector (TP-LSD), to detect line segments in an image at real-time speed. The previous related methods typically use the two-step strategy, relying on either heuristic post-process or extra classifier. To realize one-step detection with a faster and more compact model, we introduce the tri-points representation, converting the line segment detection to the end-to-end prediction of a root-point and two endpoints for each line segment. TP-LSD has two branches: tri-points extraction branch and line segmentation branch. The former predicts the heat map of root-points and the two displacement maps of endpoints. The latter segments the pixels on straight lines out from background. Moreover, the line segmentation map is reused in the first branch as structural prior. We propose an additional novel evaluation metric and evaluate our method on Wireframe and YorkUrban datasets, demonstrating not only the competitive accuracy compared to the most recent methods, but also the real-time run speed up to 78 FPS with the $320\times 320$ input.



### Monocular Depth Estimation Using Multi Scale Neural Network And Feature Fusion
- **Arxiv ID**: http://arxiv.org/abs/2009.09934v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.09934v1)
- **Published**: 2020-09-11 18:08:52+00:00
- **Updated**: 2020-09-11 18:08:52+00:00
- **Authors**: Abhinav Sagar
- **Comment**: 11 pages, 4 figures, Submitted to Neurips 2020
- **Journal**: None
- **Summary**: Depth estimation from monocular images is a challenging problem in computer vision. In this paper, we tackle this problem using a novel network architecture using multi scale feature fusion. Our network uses two different blocks, first which uses different filter sizes for convolution and merges all the individual feature maps. The second block uses dilated convolutions in place of fully connected layers thus reducing computations and increasing the receptive field. We present a new loss function for training the network which uses a depth regression term, SSIM loss term and a multinomial logistic loss term combined. We train and test our network on Make 3D dataset, NYU Depth V2 dataset and Kitti dataset using standard evaluation metrics for depth estimation comprised of RMSE loss and SILog loss. Our network outperforms previous state of the art methods with lesser parameters.



### 3D Reconstruction and Segmentation of Dissection Photographs for MRI-free Neuropathology
- **Arxiv ID**: http://arxiv.org/abs/2009.05596v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.05596v1)
- **Published**: 2020-09-11 18:21:00+00:00
- **Updated**: 2020-09-11 18:21:00+00:00
- **Authors**: Henry Tregidgo, Adria Casamitjana, Caitlin Latimer, Mitchell Kilgore, Eleanor Robinson, Emily Blackburn, Koen Van Leemput, Bruce Fischl, Adrian Dalca, Christine Mac Donald, Dirk Keene, Juan Eugenio Iglesias
- **Comment**: Accepted at MICCAI 2020
- **Journal**: None
- **Summary**: Neuroimaging to neuropathology correlation (NTNC) promises to enable the transfer of microscopic signatures of pathology to in vivo imaging with MRI, ultimately enhancing clinical care. NTNC traditionally requires a volumetric MRI scan, acquired either ex vivo or a short time prior to death. Unfortunately, ex vivo MRI is difficult and costly, and recent premortem scans of sufficient quality are seldom available. To bridge this gap, we present methodology to 3D reconstruct and segment full brain image volumes from brain dissection photographs, which are routinely acquired at many brain banks and neuropathology departments. The 3D reconstruction is achieved via a joint registration framework, which uses a reference volume other than MRI. This volume may represent either the sample at hand (e.g., a surface 3D scan) or the general population (a probabilistic atlas). In addition, we present a Bayesian method to segment the 3D reconstructed photographic volumes into 36 neuroanatomical structures, which is robust to nonuniform brightness within and across photographs. We evaluate our methods on a dataset with 24 brains, using Dice scores and volume correlations. The results show that dissection photography is a valid replacement for ex vivo MRI in many volumetric analyses, opening an avenue for MRI-free NTNC, including retrospective data. The code is available at https://github.com/htregidgo/DissectionPhotoVolumes.



### Deep Hiearchical Multi-Label Classification Applied to Chest X-Ray Abnormality Taxonomies
- **Arxiv ID**: http://arxiv.org/abs/2009.05609v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2009.05609v3)
- **Published**: 2020-09-11 18:50:23+00:00
- **Updated**: 2020-12-30 18:47:52+00:00
- **Authors**: Haomin Chen, Shun Miao, Daguang Xu, Gregory D. Hager, Adam P. Harrison
- **Comment**: None
- **Journal**: MEDIMA 101811, 5 September 2020
- **Summary**: CXRs are a crucial and extraordinarily common diagnostic tool, leading to heavy research for CAD solutions. However, both high classification accuracy and meaningful model predictions that respect and incorporate clinical taxonomies are crucial for CAD usability. To this end, we present a deep HMLC approach for CXR CAD. Different than other hierarchical systems, we show that first training the network to model conditional probability directly and then refining it with unconditional probabilities is key in boosting performance. In addition, we also formulate a numerically stable cross-entropy loss function for unconditional probabilities that provides concrete performance improvements. Finally, we demonstrate that HMLC can be an effective means to manage missing or incomplete labels. To the best of our knowledge, we are the first to apply HMLC to medical imaging CAD. We extensively evaluate our approach on detecting abnormality labels from the CXR arm of the PLCO dataset, which comprises over $198,000$ manually annotated CXRs. When using complete labels, we report a mean AUC of 0.887, the highest yet reported for this dataset. These results are supported by ancillary experiments on the PadChest dataset, where we also report significant improvements, 1.2% and 4.1% in AUC and AP, respectively over strong "flat" classifiers. Finally, we demonstrate that our HMLC approach can much better handle incompletely labelled data. These performance improvements, combined with the inherent usefulness of taxonomic predictions, indicate that our approach represents a useful step forward for CXR CAD.



### Automatic Target Recognition (ATR) from SAR Imaginary by Using Machine Learning Techniques
- **Arxiv ID**: http://arxiv.org/abs/2009.09939v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, F.2.2; I.2.7
- **Links**: [PDF](http://arxiv.org/pdf/2009.09939v1)
- **Published**: 2020-09-11 21:27:59+00:00
- **Updated**: 2020-09-11 21:27:59+00:00
- **Authors**: Umut Özkaya
- **Comment**: 5 page, 2 figure
- **Journal**: None
- **Summary**: Automatic Target Recognition (ATR) in Synthetic aperture radar (SAR) images becomes a very challenging problem owing to containing high level noise. In this study, a machine learning-based method is proposed to detect different moving and stationary targets using SAR images. First Order Statistical (FOS) features were obtained from Fast Fourier Transform (FFT), Discrete Cosine Transform (DCT) and Discrete Wavelet Transform (DWT) on gray level SAR images. Gray Level Co-occurrence Matrix (GLCM), Gray Level Run Length Matrix (GLRLM) and Gray Level Size Zone Matrix (GLSZM) algorithms are also used. These features are provided as input for the training and testing stage Support Vector Machine (SVM) model with Gaussian kernels. 4-fold cross-validations were implemented in performance evaluation. Obtained results showed that GLCM + SVM algorithm is the best model with 95.26% accuracy. This proposed method shows that moving and stationary targets in MSTAR database could be recognized with high performance.



### KSM: Fast Multiple Task Adaption via Kernel-wise Soft Mask Learning
- **Arxiv ID**: http://arxiv.org/abs/2009.05668v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.05668v1)
- **Published**: 2020-09-11 21:48:39+00:00
- **Updated**: 2020-09-11 21:48:39+00:00
- **Authors**: Li Yang, Zhezhi He, Junshan Zhang, Deliang Fan
- **Comment**: None
- **Journal**: None
- **Summary**: Deep Neural Networks (DNN) could forget the knowledge about earlier tasks when learning new tasks, and this is known as \textit{catastrophic forgetting}. While recent continual learning methods are capable of alleviating the catastrophic problem on toy-sized datasets, some issues still remain to be tackled when applying them in real-world problems. Recently, the fast mask-based learning method (e.g. piggyback \cite{mallya2018piggyback}) is proposed to address these issues by learning only a binary element-wise mask in a fast manner, while keeping the backbone model fixed. However, the binary mask has limited modeling capacity for new tasks. A more recent work \cite{hung2019compacting} proposes a compress-grow-based method (CPG) to achieve better accuracy for new tasks by partially training backbone model, but with order-higher training cost, which makes it infeasible to be deployed into popular state-of-the-art edge-/mobile-learning. The primary goal of this work is to simultaneously achieve fast and high-accuracy multi task adaption in continual learning setting. Thus motivated, we propose a new training method called \textit{kernel-wise Soft Mask} (KSM), which learns a kernel-wise hybrid binary and real-value soft mask for each task, while using the same backbone model. Such a soft mask can be viewed as a superposition of a binary mask and a properly scaled real-value tensor, which offers a richer representation capability without low-level kernel support to meet the objective of low hardware overhead. We validate KSM on multiple benchmark datasets against recent state-of-the-art methods (e.g. Piggyback, Packnet, CPG, etc.), which shows good improvement in both accuracy and training cost.



### Inverse mapping of face GANs
- **Arxiv ID**: http://arxiv.org/abs/2009.05671v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2009.05671v1)
- **Published**: 2020-09-11 22:06:56+00:00
- **Updated**: 2020-09-11 22:06:56+00:00
- **Authors**: Nicky Bayat, Vahid Reza Khazaie, Yalda Mohsenzadeh
- **Comment**: 8 pages, 6 figures
- **Journal**: None
- **Summary**: Generative adversarial networks (GANs) synthesize realistic images from a random latent vector. While many studies have explored various training configurations and architectures for GANs, the problem of inverting a generative model to extract latent vectors of given input images has been inadequately investigated. Although there is exactly one generated image per given random vector, the mapping from an image to its recovered latent vector can have more than one solution. We train a ResNet architecture to recover a latent vector for a given face that can be used to generate a face nearly identical to the target. We use a perceptual loss to embed face details in the recovered latent vector while maintaining visual quality using a pixel loss. The vast majority of studies on latent vector recovery perform well only on generated images, we argue that our method can be used to determine a mapping between real human faces and latent-space vectors that contain most of the important face style details. In addition, our proposed method projects generated faces to their latent-space with high fidelity and speed. At last, we demonstrate the performance of our approach on both real and generated faces.



### A Progressive Sub-Network Searching Framework for Dynamic Inference
- **Arxiv ID**: http://arxiv.org/abs/2009.05681v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.05681v1)
- **Published**: 2020-09-11 22:56:02+00:00
- **Updated**: 2020-09-11 22:56:02+00:00
- **Authors**: Li Yang, Zhezhi He, Yu Cao, Deliang Fan
- **Comment**: None
- **Journal**: None
- **Summary**: Many techniques have been developed, such as model compression, to make Deep Neural Networks (DNNs) inference more efficiently. Nevertheless, DNNs still lack excellent run-time dynamic inference capability to enable users trade-off accuracy and computation complexity (i.e., latency on target hardware) after model deployment, based on dynamic requirements and environments. Such research direction recently draws great attention, where one realization is to train the target DNN through a multiple-term objective function, which consists of cross-entropy terms from multiple sub-nets. Our investigation in this work show that the performance of dynamic inference highly relies on the quality of sub-net sampling. With objective to construct a dynamic DNN and search multiple high quality sub-nets with minimal searching cost, we propose a progressive sub-net searching framework, which is embedded with several effective techniques, including trainable noise ranking, channel group and fine-tuning threshold setting, sub-nets re-selection. The proposed framework empowers the target DNN with better dynamic inference capability, which outperforms prior works on both CIFAR-10 and ImageNet dataset via comprehensive experiments on different network structures. Taken ResNet18 as an example, our proposed method achieves much better dynamic inference accuracy compared with prior popular Universally-Slimmable-Network by 4.4%-maximally and 2.3%-averagely in ImageNet dataset with the same model size.



### AttnGrounder: Talking to Cars with Attention
- **Arxiv ID**: http://arxiv.org/abs/2009.05684v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.05684v2)
- **Published**: 2020-09-11 23:18:55+00:00
- **Updated**: 2020-12-11 10:00:22+00:00
- **Authors**: Vivek Mittal
- **Comment**: None
- **Journal**: None
- **Summary**: We propose Attention Grounder (AttnGrounder), a single-stage end-to-end trainable model for the task of visual grounding. Visual grounding aims to localize a specific object in an image based on a given natural language text query. Unlike previous methods that use the same text representation for every image region, we use a visual-text attention module that relates each word in the given query with every region in the corresponding image for constructing a region dependent text representation. Furthermore, for improving the localization ability of our model, we use our visual-text attention module to generate an attention mask around the referred object. The attention mask is trained as an auxiliary task using a rectangular mask generated with the provided ground-truth coordinates. We evaluate AttnGrounder on the Talk2Car dataset and show an improvement of 3.26% over the existing methods.



