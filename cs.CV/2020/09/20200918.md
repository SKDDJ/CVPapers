# Arxiv Papers in cs.CV on 2020-09-18
### SCREENet: A Multi-view Deep Convolutional Neural Network for Classification of High-resolution Synthetic Mammographic Screening Scans
- **Arxiv ID**: http://arxiv.org/abs/2009.08563v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.08563v3)
- **Published**: 2020-09-18 00:12:33+00:00
- **Updated**: 2020-09-25 19:36:05+00:00
- **Authors**: Saeed Seyyedi, Margaret J. Wong, Debra M. Ikeda, Curtis P. Langlotz
- **Comment**: None
- **Journal**: None
- **Summary**: Purpose: To develop and evaluate the accuracy of a multi-view deep learning approach to the analysis of high-resolution synthetic mammograms from digital breast tomosynthesis screening cases, and to assess the effect on accuracy of image resolution and training set size. Materials and Methods: In a retrospective study, 21,264 screening digital breast tomosynthesis (DBT) exams obtained at our institution were collected along with associated radiology reports. The 2D synthetic mammographic images from these exams, with varying resolutions and data set sizes, were used to train a multi-view deep convolutional neural network (MV-CNN) to classify screening images into BI-RADS classes (BI-RADS 0, 1 and 2) before evaluation on a held-out set of exams.   Results: Area under the receiver operating characteristic curve (AUC) for BI-RADS 0 vs non-BI-RADS 0 class was 0.912 for the MV-CNN trained on the full dataset. The model obtained accuracy of 84.8%, recall of 95.9% and precision of 95.0%. This AUC value decreased when the same model was trained with 50% and 25% of images (AUC = 0.877, P=0.010 and 0.834, P=0.009 respectively). Also, the performance dropped when the same model was trained using images that were under-sampled by 1/2 and 1/4 (AUC = 0.870, P=0.011 and 0.813, P=0.009 respectively).   Conclusion: This deep learning model classified high-resolution synthetic mammography scans into normal vs needing further workup using tens of thousands of high-resolution images. Smaller training data sets and lower resolution images both caused significant decrease in performance.



### MUTANT: A Training Paradigm for Out-of-Distribution Generalization in Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/2009.08566v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2009.08566v2)
- **Published**: 2020-09-18 00:22:54+00:00
- **Updated**: 2020-10-16 01:53:08+00:00
- **Authors**: Tejas Gokhale, Pratyay Banerjee, Chitta Baral, Yezhou Yang
- **Comment**: Accepted to EMNLP 2020, Long Papers
- **Journal**: None
- **Summary**: While progress has been made on the visual question answering leaderboards, models often utilize spurious correlations and priors in datasets under the i.i.d. setting. As such, evaluation on out-of-distribution (OOD) test samples has emerged as a proxy for generalization. In this paper, we present MUTANT, a training paradigm that exposes the model to perceptually similar, yet semantically distinct mutations of the input, to improve OOD generalization, such as the VQA-CP challenge. Under this paradigm, models utilize a consistency-constrained training objective to understand the effect of semantic changes in input (question-image pair) on the output (answer). Unlike existing methods on VQA-CP, MUTANT does not rely on the knowledge about the nature of train and test answer distributions. MUTANT establishes a new state-of-the-art accuracy on VQA-CP with a $10.57\%$ improvement. Our work opens up avenues for the use of semantic input mutations for OOD generalization in question answering.



### Pruning Neural Networks at Initialization: Why are We Missing the Mark?
- **Arxiv ID**: http://arxiv.org/abs/2009.08576v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2009.08576v2)
- **Published**: 2020-09-18 01:13:38+00:00
- **Updated**: 2021-03-21 21:38:32+00:00
- **Authors**: Jonathan Frankle, Gintare Karolina Dziugaite, Daniel M. Roy, Michael Carbin
- **Comment**: Published in ICLR 2021
- **Journal**: None
- **Summary**: Recent work has explored the possibility of pruning neural networks at initialization. We assess proposals for doing so: SNIP (Lee et al., 2019), GraSP (Wang et al., 2020), SynFlow (Tanaka et al., 2020), and magnitude pruning. Although these methods surpass the trivial baseline of random pruning, they remain below the accuracy of magnitude pruning after training, and we endeavor to understand why. We show that, unlike pruning after training, randomly shuffling the weights these methods prune within each layer or sampling new initial values preserves or improves accuracy. As such, the per-weight pruning decisions made by these methods can be replaced by a per-layer choice of the fraction of weights to prune. This property suggests broader challenges with the underlying pruning heuristics, the desire to prune at initialization, or both.



### Fast Search on Binary Codes by Weighted Hamming Distance
- **Arxiv ID**: http://arxiv.org/abs/2009.08591v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.08591v2)
- **Published**: 2020-09-18 02:24:44+00:00
- **Updated**: 2021-08-10 07:36:22+00:00
- **Authors**: Zhenyu Weng, Yuesheng Zhu, Ruixin Liu
- **Comment**: 12 pages
- **Journal**: None
- **Summary**: Weighted Hamming distance, as a similarity measure between binary codes and binary queries, provides superior accuracy in search tasks than Hamming distance. However, how to efficiently and accurately find $K$ binary codes that have the smallest weighted Hamming distance to the query remains an open issue. In this paper, a fast search algorithm is proposed to perform the non-exhaustive search for $K$ nearest binary codes by weighted Hamming distance. By using binary codes as direct bucket indices in a hash table, the search algorithm generates a sequence to probe the buckets based on the independence characteristic of the weights for each bit. Furthermore, a fast search framework based on the proposed search algorithm is designed to solve the problem of long binary codes. Specifically, long binary codes are split into substrings and multiple hash tables are built on them. Then, the search algorithm probes the buckets to obtain candidates according to the generated substring indices, and a merging algorithm is proposed to find the nearest binary codes by merging the candidates. Theoretical analysis and experimental results demonstrate that the search algorithm improves the search accuracy compared to other non-exhaustive algorithms and provides orders-of-magnitude faster search than the linear scan baseline.



### Consistency Regularization with High-dimensional Non-adversarial Source-guided Perturbation for Unsupervised Domain Adaptation in Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2009.08610v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.08610v1)
- **Published**: 2020-09-18 03:26:44+00:00
- **Updated**: 2020-09-18 03:26:44+00:00
- **Authors**: Kaihong Wang, Chenhongyi Yang, Margrit Betke
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised domain adaptation for semantic segmentation has been intensively studied due to the low cost of the pixel-level annotation for synthetic data. The most common approaches try to generate images or features mimicking the distribution in the target domain while preserving the semantic contents in the source domain so that a model can be trained with annotations from the latter. However, such methods highly rely on an image translator or feature extractor trained in an elaborated mechanism including adversarial training, which brings in extra complexity and instability in the adaptation process. Furthermore, these methods mainly focus on taking advantage of the labeled source dataset, leaving the unlabeled target dataset not fully utilized. In this paper, we propose a bidirectional style-induced domain adaptation method, called BiSIDA, that employs consistency regularization to efficiently exploit information from the unlabeled target domain dataset, requiring only a simple neural style transfer model. BiSIDA aligns domains by not only transferring source images into the style of target images but also transferring target images into the style of source images to perform high-dimensional perturbation on the unlabeled target images, which is crucial to the success in applying consistency regularization in segmentation tasks. Extensive experiments show that our BiSIDA achieves new state-of-the-art on two commonly-used synthetic-to-real domain adaptation benchmarks: GTA5-to-CityScapes and SYNTHIA-to-CityScapes.



### Reinforcement Learning for Weakly Supervised Temporal Grounding of Natural Language in Untrimmed Videos
- **Arxiv ID**: http://arxiv.org/abs/2009.08614v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.08614v1)
- **Published**: 2020-09-18 03:32:47+00:00
- **Updated**: 2020-09-18 03:32:47+00:00
- **Authors**: Jie Wu, Guanbin Li, Xiaoguang Han, Liang Lin
- **Comment**: Accepted by ACM MM 2020
- **Journal**: None
- **Summary**: Temporal grounding of natural language in untrimmed videos is a fundamental yet challenging multimedia task facilitating cross-media visual content retrieval. We focus on the weakly supervised setting of this task that merely accesses to coarse video-level language description annotation without temporal boundary, which is more consistent with reality as such weak labels are more readily available in practice. In this paper, we propose a \emph{Boundary Adaptive Refinement} (BAR) framework that resorts to reinforcement learning (RL) to guide the process of progressively refining the temporal boundary. To the best of our knowledge, we offer the first attempt to extend RL to temporal localization task with weak supervision. As it is non-trivial to obtain a straightforward reward function in the absence of pairwise granular boundary-query annotations, a cross-modal alignment evaluator is crafted to measure the alignment degree of segment-query pair to provide tailor-designed rewards. This refinement scheme completely abandons traditional sliding window based solution pattern and contributes to acquiring more efficient, boundary-flexible and content-aware grounding results. Extensive experiments on two public benchmarks Charades-STA and ActivityNet demonstrate that BAR outperforms the state-of-the-art weakly-supervised method and even beats some competitive fully-supervised ones.



### 6-DoF Grasp Planning using Fast 3D Reconstruction and Grasp Quality CNN
- **Arxiv ID**: http://arxiv.org/abs/2009.08618v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2009.08618v1)
- **Published**: 2020-09-18 03:53:18+00:00
- **Updated**: 2020-09-18 03:53:18+00:00
- **Authors**: Yahav Avigal, Samuel Paradis, Harry Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Recent consumer demand for home robots has accelerated performance of robotic grasping. However, a key component of the perception pipeline, the depth camera, is still expensive and inaccessible to most consumers. In addition, grasp planning has significantly improved recently, by leveraging large datasets and cloud robotics, and by limiting the state and action space to top-down grasps with 4 degrees of freedom (DoF). By leveraging multi-view geometry of the object using inexpensive equipment such as off-the-shelf RGB cameras and state-of-the-art algorithms such as Learn Stereo Machine (LSM\cite{kar2017learning}), the robot is able to generate more robust grasps from different angles with 6-DoF. In this paper, we present a modification of LSM to graspable objects, evaluate the grasps, and develop a 6-DoF grasp planner based on Grasp-Quality CNN (GQ-CNN\cite{mahler2017dex}) that exploits multiple camera views to plan a robust grasp, even in the absence of a possible top-down grasp.



### Identification of Abnormal States in Videos of Ants Undergoing Social Phase Change
- **Arxiv ID**: http://arxiv.org/abs/2009.08626v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2009.08626v2)
- **Published**: 2020-09-18 04:48:47+00:00
- **Updated**: 2021-08-14 21:18:06+00:00
- **Authors**: Taeyeong Choi, Benjamin Pyenson, Juergen Liebig, Theodore P. Pavlic
- **Comment**: Published in the proceedings of AAAI 2021
- **Journal**: Proceedings of the AAAI Conference on Artificial Intelligence,
  35(17), 15286-15292 (2021)
- **Summary**: Biology is both an important application area and a source of motivation for development of advanced machine learning techniques. Although much attention has been paid to large and complex data sets resulting from high-throughput sequencing, advances in high-quality video recording technology have begun to generate similarly rich data sets requiring sophisticated techniques from both computer vision and time-series analysis. Moreover, just as studying gene expression patterns in one organism can reveal general principles that apply to other organisms, the study of complex social interactions in an experimentally tractable model system, such as a laboratory ant colony, can provide general principles about the dynamics of other social groups. Here, we focus on one such example from the study of reproductive regulation in small laboratory colonies of more than 50 Harpegnathos ants. These ants can be artificially induced to begin a ~20 day process of hierarchy reformation. Although the conclusion of this process is conspicuous to a human observer, it remains unclear which behaviors during the transient period are contributing to the process. To address this issue, we explore the potential application of One-class Classification (OC) to the detection of abnormal states in ant colonies for which behavioral data is only available for the normal societal conditions during training. Specifically, we build upon the Deep Support Vector Data Description (DSVDD) and introduce the Inner-Outlier Generator (IO-GEN) that synthesizes fake "inner outlier" observations during training that are near the center of the DSVDD data description. We show that IO-GEN increases the reliability of the final OC classifier relative to other DSVDD baselines. This method can be used to screen video frames for which additional human observation is needed.



### Per-frame mAP Prediction for Continuous Performance Monitoring of Object Detection During Deployment
- **Arxiv ID**: http://arxiv.org/abs/2009.08650v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.08650v2)
- **Published**: 2020-09-18 06:37:52+00:00
- **Updated**: 2020-11-16 07:11:29+00:00
- **Authors**: Quazi Marufur Rahman, Niko Sünderhauf, Feras Dayoub
- **Comment**: None
- **Journal**: None
- **Summary**: Performance monitoring of object detection is crucial for safety-critical applications such as autonomous vehicles that operate under varying and complex environmental conditions. Currently, object detectors are evaluated using summary metrics based on a single dataset that is assumed to be representative of all future deployment conditions. In practice, this assumption does not hold, and the performance fluctuates as a function of the deployment conditions. To address this issue, we propose an introspection approach to performance monitoring during deployment without the need for ground truth data. We do so by predicting when the per-frame mean average precision drops below a critical threshold using the detector's internal features. We quantitatively evaluate and demonstrate our method's ability to reduce risk by trading off making an incorrect decision by raising the alarm and absenting from detection.



### Multi-species Seagrass Detection and Classification from Underwater Images
- **Arxiv ID**: http://arxiv.org/abs/2009.09924v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2009.09924v1)
- **Published**: 2020-09-18 07:20:44+00:00
- **Updated**: 2020-09-18 07:20:44+00:00
- **Authors**: Scarlett Raine, Ross Marchant, Peyman Moghadam, Frederic Maire, Brett Kettle, Brano Kusy
- **Comment**: Accepted to DICTA 2020. project page is at:
  https://github.com/csiro-robotics/deepseagrass
- **Journal**: None
- **Summary**: Underwater surveys conducted using divers or robots equipped with customized camera payloads can generate a large number of images. Manual review of these images to extract ecological data is prohibitive in terms of time and cost, thus providing strong incentive to automate this process using machine learning solutions. In this paper, we introduce a multi-species detector and classifier for seagrasses based on a deep convolutional neural network (achieved an overall accuracy of 92.4%). We also introduce a simple method to semi-automatically label image patches and therefore minimize manual labelling requirement. We describe and release publicly the dataset collected in this study as well as the code and pre-trained models to replicate our experiments at: https://github.com/csiro-robotics/deepseagrass



### An Analysis by Synthesis Method that Allows Accurate Spatial Modeling of Thickness of Cortical Bone from Clinical QCT
- **Arxiv ID**: http://arxiv.org/abs/2009.08664v1
- **DOI**: 10.1007/978-3-030-59725-2_62
- **Categories**: **eess.IV**, cs.CV, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2009.08664v1)
- **Published**: 2020-09-18 07:30:18+00:00
- **Updated**: 2020-09-18 07:30:18+00:00
- **Authors**: Stefan Reinhold, Timo Damm, Sebastian Büsse, Stanislav N. Gorb, Claus-C. Glüer, Reinhard Koch
- **Comment**: Accepted for publication in MICCAI 2020 conference
- **Journal**: None
- **Summary**: Osteoporosis is a skeletal disorder that leads to increased fracture risk due to decreased strength of cortical and trabecular bone. Even with state-of-the-art non-invasive assessment methods there is still a high underdiagnosis rate. Quantitative computed tomography (QCT) permits the selective analysis of cortical bone, however the low spatial resolution of clinical QCT leads to an overestimation of the thickness of cortical bone (Ct.Th) and bone strength.   We propose a novel, model based, fully automatic image analysis method that allows accurate spatial modeling of the thickness distribution of cortical bone from clinical QCT. In an analysis-by-synthesis (AbS) fashion a stochastic scan is synthesized from a probabilistic bone model, the optimal model parameters are estimated using a maximum a-posteriori approach. By exploiting the different characteristics of in-plane and out-of-plane point spread functions of CT scanners the proposed method is able assess the spatial distribution of cortical thickness.   The method was evaluated on eleven cadaveric human vertebrae, scanned by clinical QCT and analyzed using standard methods and AbS, both compared to high resolution peripheral QCT (HR-pQCT) as gold standard. While standard QCT based measurements overestimated Ct.Th. by 560% and did not show significant correlation with the gold standard ($r^2 = 0.20,\, p = 0.169$) the proposed method eliminated the overestimation and showed a significant tight correlation with the gold standard ($r^2 = 0.98,\, p < 0.0001$) a root mean square error below 10%.



### TopNet: Topology Preserving Metric Learning for Vessel Tree Reconstruction and Labelling
- **Arxiv ID**: http://arxiv.org/abs/2009.08674v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.08674v1)
- **Published**: 2020-09-18 07:55:58+00:00
- **Updated**: 2020-09-18 07:55:58+00:00
- **Authors**: Deepak Keshwani, Yoshiro Kitamura, Satoshi Ihara, Satoshi Iizuka, Edgar Simo-Serra
- **Comment**: Accepted in MICCAI 2020
- **Journal**: None
- **Summary**: Reconstructing Portal Vein and Hepatic Vein trees from contrast enhanced abdominal CT scans is a prerequisite for preoperative liver surgery simulation. Existing deep learning based methods treat vascular tree reconstruction as a semantic segmentation problem. However, vessels such as hepatic and portal vein look very similar locally and need to be traced to their source for robust label assignment. Therefore, semantic segmentation by looking at local 3D patch results in noisy misclassifications. To tackle this, we propose a novel multi-task deep learning architecture for vessel tree reconstruction. The network architecture simultaneously solves the task of detecting voxels on vascular centerlines (i.e. nodes) and estimates connectivity between center-voxels (edges) in the tree structure to be reconstructed. Further, we propose a novel connectivity metric which considers both inter-class distance and intra-class topological distance between center-voxel pairs. Vascular trees are reconstructed starting from the vessel source using the learned connectivity metric using the shortest path tree algorithm. A thorough evaluation on public IRCAD dataset shows that the proposed method considerably outperforms existing semantic segmentation based methods. To the best of our knowledge, this is the first deep learning based approach which learns multi-label tree structure connectivity from images.



### Face Sketch Synthesis with Style Transfer using Pyramid Column Feature
- **Arxiv ID**: http://arxiv.org/abs/2009.08679v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.08679v1)
- **Published**: 2020-09-18 08:15:55+00:00
- **Updated**: 2020-09-18 08:15:55+00:00
- **Authors**: Chaofeng Chen, Xiao Tan, Kwan-Yee K. Wong
- **Comment**: WACV2018
- **Journal**: None
- **Summary**: In this paper, we propose a novel framework based on deep neural networks for face sketch synthesis from a photo. Imitating the process of how artists draw sketches, our framework synthesizes face sketches in a cascaded manner. A content image is first generated that outlines the shape of the face and the key facial features. Textures and shadings are then added to enrich the details of the sketch. We utilize a fully convolutional neural network (FCNN) to create the content image, and propose a style transfer approach to introduce textures and shadings based on a newly proposed pyramid column feature. We demonstrate that our style transfer approach based on the pyramid column feature can not only preserve more sketch details than the common style transfer method, but also surpasses traditional patch based methods. Quantitative and qualitative evaluations suggest that our framework outperforms other state-of-the-arts methods, and can also generalize well to different test images. Codes are available at https://github.com/chaofengc/Face-Sketch



### Conditional Image Generation with One-Vs-All Classifier
- **Arxiv ID**: http://arxiv.org/abs/2009.08688v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.08688v1)
- **Published**: 2020-09-18 08:41:27+00:00
- **Updated**: 2020-09-18 08:41:27+00:00
- **Authors**: Xiangrui Xu, Yaqin Li, Cao Yuan
- **Comment**: None
- **Journal**: None
- **Summary**: This paper explores conditional image generation with a One-Vs-All classifier based on the Generative Adversarial Networks (GANs). Instead of the real/fake discriminator used in vanilla GANs, we propose to extend the discriminator to a One-Vs-All classifier (GAN-OVA) that can distinguish each input data to its category label. Specifically, we feed certain additional information as conditions to the generator and take the discriminator as a One-Vs-All classifier to identify each conditional category. Our model can be applied to different divergence or distances used to define the objective function, such as Jensen-Shannon divergence and Earth-Mover (or called Wasserstein-1) distance. We evaluate GAN-OVAs on MNIST and CelebA-HQ datasets, and the experimental results show that GAN-OVAs make progress toward stable training over regular conditional GANs. Furthermore, GAN-OVAs effectively accelerate the generation process of different classes and improves generation quality.



### DeepRemaster: Temporal Source-Reference Attention Networks for Comprehensive Video Enhancement
- **Arxiv ID**: http://arxiv.org/abs/2009.08692v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2009.08692v1)
- **Published**: 2020-09-18 08:55:11+00:00
- **Updated**: 2020-09-18 08:55:11+00:00
- **Authors**: Satoshi Iizuka, Edgar Simo-Serra
- **Comment**: Accepted to SIGGRAPH Asia 2019. Project page:
  http://iizuka.cs.tsukuba.ac.jp/projects/remastering/
- **Journal**: None
- **Summary**: The remastering of vintage film comprises of a diversity of sub-tasks including super-resolution, noise removal, and contrast enhancement which aim to restore the deteriorated film medium to its original state. Additionally, due to the technical limitations of the time, most vintage film is either recorded in black and white, or has low quality colors, for which colorization becomes necessary. In this work, we propose a single framework to tackle the entire remastering task semi-interactively. Our work is based on temporal convolutional neural networks with attention mechanisms trained on videos with data-driven deterioration simulation. Our proposed source-reference attention allows the model to handle an arbitrary number of reference color images to colorize long videos without the need for segmentation while maintaining temporal consistency. Quantitative analysis shows that our framework outperforms existing approaches, and that, in contrast to existing approaches, the performance of our framework increases with longer videos and more reference color images.



### Searching for Low-Bit Weights in Quantized Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2009.08695v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.08695v1)
- **Published**: 2020-09-18 09:13:26+00:00
- **Updated**: 2020-09-18 09:13:26+00:00
- **Authors**: Zhaohui Yang, Yunhe Wang, Kai Han, Chunjing Xu, Chao Xu, Dacheng Tao, Chang Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Quantized neural networks with low-bit weights and activations are attractive for developing AI accelerators. However, the quantization functions used in most conventional quantization methods are non-differentiable, which increases the optimization difficulty of quantized networks. Compared with full-precision parameters (i.e., 32-bit floating numbers), low-bit values are selected from a much smaller set. For example, there are only 16 possibilities in 4-bit space. Thus, we present to regard the discrete weights in an arbitrary quantized neural network as searchable variables, and utilize a differential method to search them accurately. In particular, each weight is represented as a probability distribution over the discrete value set. The probabilities are optimized during training and the values with the highest probability are selected to establish the desired quantized network. Experimental results on benchmarks demonstrate that the proposed method is able to produce quantized neural networks with higher performance over the state-of-the-art methods on both image classification and super-resolution tasks.



### Learning Emotional-Blinded Face Representations
- **Arxiv ID**: http://arxiv.org/abs/2009.08704v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.08704v1)
- **Published**: 2020-09-18 09:24:10+00:00
- **Updated**: 2020-09-18 09:24:10+00:00
- **Authors**: Alejandro Peña, Julian Fierrez, Agata Lapedriza, Aythami Morales
- **Comment**: IAPR Intl. Conf. on Pattern Recognition, 2020
- **Journal**: None
- **Summary**: We propose two face representations that are blind to facial expressions associated to emotional responses. This work is in part motivated by new international regulations for personal data protection, which enforce data controllers to protect any kind of sensitive information involved in automatic processes. The advances in Affective Computing have contributed to improve human-machine interfaces but, at the same time, the capacity to monitorize emotional responses triggers potential risks for humans, both in terms of fairness and privacy. We propose two different methods to learn these expression-blinded facial features. We show that it is possible to eliminate information related to emotion recognition tasks, while the performance of subject verification, gender recognition, and ethnicity classification are just slightly affected. We also present an application to train fairer classifiers in a case study of attractiveness classification with respect to a protected facial expression attribute. The results demonstrate that it is possible to reduce emotional information in the face representation while retaining competitive performance in other face-based artificial intelligence tasks.



### Progressive Semantic-Aware Style Transformation for Blind Face Restoration
- **Arxiv ID**: http://arxiv.org/abs/2009.08709v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.08709v2)
- **Published**: 2020-09-18 09:27:33+00:00
- **Updated**: 2021-03-21 09:35:05+00:00
- **Authors**: Chaofeng Chen, Xiaoming Li, Lingbo Yang, Xianhui Lin, Lei Zhang, Kwan-Yee K. Wong
- **Comment**: Accepted to CVPR2021, https://github.com/chaofengc/PSFRGAN
- **Journal**: None
- **Summary**: Face restoration is important in face image processing, and has been widely studied in recent years. However, previous works often fail to generate plausible high quality (HQ) results for real-world low quality (LQ) face images. In this paper, we propose a new progressive semantic-aware style transformation framework, named PSFR-GAN, for face restoration. Specifically, instead of using an encoder-decoder framework as previous methods, we formulate the restoration of LQ face images as a multi-scale progressive restoration procedure through semantic-aware style transformation. Given a pair of LQ face image and its corresponding parsing map, we first generate a multi-scale pyramid of the inputs, and then progressively modulate different scale features from coarse-to-fine in a semantic-aware style transfer way. Compared with previous networks, the proposed PSFR-GAN makes full use of the semantic (parsing maps) and pixel (LQ images) space information from different scales of input pairs. In addition, we further introduce a semantic aware style loss which calculates the feature style loss for each semantic region individually to improve the details of face textures. Finally, we pretrain a face parsing network which can generate decent parsing maps from real-world LQ face images. Experiment results show that our model trained with synthetic data can not only produce more realistic high-resolution results for synthetic LQ inputs and but also generalize better to natural LQ face images compared with state-of-the-art methods. Codes are available at https://github.com/chaofengc/PSFRGAN.



### Contextual Semantic Interpretability
- **Arxiv ID**: http://arxiv.org/abs/2009.08720v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.08720v1)
- **Published**: 2020-09-18 09:47:05+00:00
- **Updated**: 2020-09-18 09:47:05+00:00
- **Authors**: Diego Marcos, Ruth Fong, Sylvain Lobry, Remi Flamary, Nicolas Courty, Devis Tuia
- **Comment**: None
- **Journal**: ACCV 2020
- **Summary**: Convolutional neural networks (CNN) are known to learn an image representation that captures concepts relevant to the task, but do so in an implicit way that hampers model interpretability. However, one could argue that such a representation is hidden in the neurons and can be made explicit by teaching the model to recognize semantically interpretable attributes that are present in the scene. We call such an intermediate layer a \emph{semantic bottleneck}. Once the attributes are learned, they can be re-combined to reach the final decision and provide both an accurate prediction and an explicit reasoning behind the CNN decision. In this paper, we look into semantic bottlenecks that capture context: we want attributes to be in groups of a few meaningful elements and participate jointly to the final decision. We use a two-layer semantic bottleneck that gathers attributes into interpretable, sparse groups, allowing them contribute differently to the final output depending on the context. We test our contextual semantic interpretable bottleneck (CSIB) on the task of landscape scenicness estimation and train the semantic interpretable bottleneck using an auxiliary database (SUN Attributes). Our model yields in predictions as accurate as a non-interpretable baseline when applied to a real-world test set of Flickr images, all while providing clear and interpretable explanations for each prediction.



### Pose Correction Algorithm for Relative Frames between Keyframes in SLAM
- **Arxiv ID**: http://arxiv.org/abs/2009.08724v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.08724v1)
- **Published**: 2020-09-18 09:59:10+00:00
- **Updated**: 2020-09-18 09:59:10+00:00
- **Authors**: Youngseok Jang, Hojoon Shin, H. Jin Kim
- **Comment**: 15 pages
- **Journal**: None
- **Summary**: With the dominance of keyframe-based SLAM in the field of robotics, the relative frame poses between keyframes have typically been sacrificed for a faster algorithm to achieve online applications. However, those approaches can become insufficient for applications that may require refined poses of all frames, not just keyframes which are relatively sparse compared to all input frames. This paper proposes a novel algorithm to correct the relative frames between keyframes after the keyframes have been updated by a back-end optimization process. The correction model is derived using conservation of the measurement constraint between landmarks and the robot pose. The proposed algorithm is designed to be easily integrable to existing keyframe-based SLAM systems while exhibiting robust and accurate performance superior to existing interpolation methods. The algorithm also requires low computational resources and hence has a minimal burden on the whole SLAM pipeline. We provide the evaluation of the proposed pose correction algorithm in comparison to existing interpolation methods in various vector spaces, and our method has demonstrated excellent accuracy in both KITTI and EuRoC datasets.



### Moving object detection for visual odometry in a dynamic environment based on occlusion accumulation
- **Arxiv ID**: http://arxiv.org/abs/2009.08746v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2009.08746v1)
- **Published**: 2020-09-18 11:01:46+00:00
- **Updated**: 2020-09-18 11:01:46+00:00
- **Authors**: Haram Kim, Pyojin Kim, H. Jin Kim
- **Comment**: 7 pages
- **Journal**: ICRA 2020 published
- **Summary**: Detection of moving objects is an essential capability in dealing with dynamic environments. Most moving object detection algorithms have been designed for color images without depth. For robotic navigation where real-time RGB-D data is often readily available, utilization of the depth information would be beneficial for obstacle recognition.   Here, we propose a simple moving object detection algorithm that uses RGB-D images. The proposed algorithm does not require estimating a background model. Instead, it uses an occlusion model which enables us to estimate the camera pose on a background confused with moving objects that dominate the scene. The proposed algorithm allows to separate the moving object detection and visual odometry (VO) so that an arbitrary robust VO method can be employed in a dynamic situation with a combination of moving object detection, whereas other VO algorithms for a dynamic environment are inseparable. In this paper, we use dense visual odometry (DVO) as a VO method with a bi-square regression weight. Experimental results show the segmentation accuracy and the performance improvement of DVO in the situations. We validate our algorithm in public datasets and our dataset which also publicly accessible.



### DeltaGAN: Towards Diverse Few-shot Image Generation with Sample-Specific Delta
- **Arxiv ID**: http://arxiv.org/abs/2009.08753v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.08753v4)
- **Published**: 2020-09-18 11:25:05+00:00
- **Updated**: 2022-07-28 03:43:23+00:00
- **Authors**: Yan Hong, Li Niu, Jianfu Zhang, Jing Liang, Liqing Zhang
- **Comment**: This paper is accepted by ECCV 2022
- **Journal**: None
- **Summary**: Learning to generate new images for a novel category based on only a few images, named as few-shot image generation, has attracted increasing research interest. Several state-of-the-art works have yielded impressive results, but the diversity is still limited. In this work, we propose a novel Delta Generative Adversarial Network (DeltaGAN), which consists of a reconstruction subnetwork and a generation subnetwork. The reconstruction subnetwork captures intra-category transformation, i.e., "delta", between same-category pairs. The generation subnetwork generates sample-specific "delta" for an input image, which is combined with this input image to generate a new image within the same category. Besides, an adversarial delta matching loss is designed to link the above two subnetworks together. Extensive experiments on five few-shot image datasets demonstrate the effectiveness of our proposed method.



### Commands 4 Autonomous Vehicles (C4AV) Workshop Summary
- **Arxiv ID**: http://arxiv.org/abs/2009.08792v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2009.08792v1)
- **Published**: 2020-09-18 12:33:21+00:00
- **Updated**: 2020-09-18 12:33:21+00:00
- **Authors**: Thierry Deruyttere, Simon Vandenhende, Dusan Grujicic, Yu Liu, Luc Van Gool, Matthew Blaschko, Tinne Tuytelaars, Marie-Francine Moens
- **Comment**: None
- **Journal**: None
- **Summary**: The task of visual grounding requires locating the most relevant region or object in an image, given a natural language query. So far, progress on this task was mostly measured on curated datasets, which are not always representative of human spoken language. In this work, we deviate from recent, popular task settings and consider the problem under an autonomous vehicle scenario. In particular, we consider a situation where passengers can give free-form natural language commands to a vehicle which can be associated with an object in the street scene. To stimulate research on this topic, we have organized the \emph{Commands for Autonomous Vehicles} (C4AV) challenge based on the recent \emph{Talk2Car} dataset (URL: https://www.aicrowd.com/challenges/eccv-2020-commands-4-autonomous-vehicles). This paper presents the results of the challenge. First, we compare the used benchmark against existing datasets for visual grounding. Second, we identify the aspects that render top-performing models successful, and relate them to existing state-of-the-art models for visual grounding, in addition to detecting potential failure cases by evaluating on carefully selected subsets. Finally, we discuss several possibilities for future work.



### $σ^2$R Loss: a Weighted Loss by Multiplicative Factors using Sigmoidal Functions
- **Arxiv ID**: http://arxiv.org/abs/2009.08796v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.08796v1)
- **Published**: 2020-09-18 12:34:40+00:00
- **Updated**: 2020-09-18 12:34:40+00:00
- **Authors**: Riccardo La Grassa, Ignazio Gallo, Nicola Landro
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: In neural networks, the loss function represents the core of the learning process that leads the optimizer to an approximation of the optimal convergence error. Convolutional neural networks (CNN) use the loss function as a supervisory signal to train a deep model and contribute significantly to achieving the state of the art in some fields of artificial vision. Cross-entropy and Center loss functions are commonly used to increase the discriminating power of learned functions and increase the generalization performance of the model. Center loss minimizes the class intra-class variance and at the same time penalizes the long distance between the deep features inside each class. However, the total error of the center loss will be heavily influenced by the majority of the instances and can lead to a freezing state in terms of intra-class variance. To address this, we introduce a new loss function called sigma squared reduction loss ($\sigma^2$R loss), which is regulated by a sigmoid function to inflate/deflate the error per instance and then continue to reduce the intra-class variance. Our loss has clear intuition and geometric interpretation, furthermore, we demonstrate by experiments the effectiveness of our proposal on several benchmark datasets showing the intra-class variance reduction and overcoming the results obtained with center loss and soft nearest neighbour functions.



### Densely Guided Knowledge Distillation using Multiple Teacher Assistants
- **Arxiv ID**: http://arxiv.org/abs/2009.08825v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.08825v3)
- **Published**: 2020-09-18 13:12:52+00:00
- **Updated**: 2021-08-09 05:48:48+00:00
- **Authors**: Wonchul Son, Jaemin Na, Junyong Choi, Wonjun Hwang
- **Comment**: Accepted at ICCV 2021
- **Journal**: None
- **Summary**: With the success of deep neural networks, knowledge distillation which guides the learning of a small student network from a large teacher network is being actively studied for model compression and transfer learning. However, few studies have been performed to resolve the poor learning issue of the student network when the student and teacher model sizes significantly differ. In this paper, we propose a densely guided knowledge distillation using multiple teacher assistants that gradually decreases the model size to efficiently bridge the large gap between the teacher and student networks. To stimulate more efficient learning of the student network, we guide each teacher assistant to every other smaller teacher assistants iteratively. Specifically, when teaching a smaller teacher assistant at the next step, the existing larger teacher assistants from the previous step are used as well as the teacher network. Moreover, we design stochastic teaching where, for each mini-batch, a teacher or teacher assistants are randomly dropped. This acts as a regularizer to improve the efficiency of teaching of the student network. Thus, the student can always learn salient distilled knowledge from the multiple sources. We verified the effectiveness of the proposed method for a classification task using CIFAR-10, CIFAR-100, and ImageNet. We also achieved significant performance improvements with various backbone architectures such as ResNet, WideResNet, and VGG.



### Residual Spatial Attention Network for Retinal Vessel Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2009.08829v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.08829v1)
- **Published**: 2020-09-18 13:17:13+00:00
- **Updated**: 2020-09-18 13:17:13+00:00
- **Authors**: Changlu Guo, Márton Szemenyei, Yugen Yi, Wei Zhou, Haodong Bian
- **Comment**: ICONIP 2020
- **Journal**: None
- **Summary**: Reliable segmentation of retinal vessels can be employed as a way of monitoring and diagnosing certain diseases, such as diabetes and hypertension, as they affect the retinal vascular structure. In this work, we propose the Residual Spatial Attention Network (RSAN) for retinal vessel segmentation. RSAN employs a modified residual block structure that integrates DropBlock, which can not only be utilized to construct deep networks to extract more complex vascular features, but can also effectively alleviate the overfitting. Moreover, in order to further improve the representation capability of the network, based on this modified residual block, we introduce the spatial attention (SA) and propose the Residual Spatial Attention Block (RSAB) to build RSAN. We adopt the public DRIVE and CHASE DB1 color fundus image datasets to evaluate the proposed RSAN. Experiments show that the modified residual structure and the spatial attention are effective in this work, and our proposed RSAN achieves the state-of-the-art performance.



### Search and Rescue with Airborne Optical Sectioning
- **Arxiv ID**: http://arxiv.org/abs/2009.08835v1
- **DOI**: 10.1038/s42256-020-00261-3
- **Categories**: **cs.LG**, cs.CV, stat.ML, 68T07, 68T45, I.2.10; I.4.1
- **Links**: [PDF](http://arxiv.org/pdf/2009.08835v1)
- **Published**: 2020-09-18 13:40:19+00:00
- **Updated**: 2020-09-18 13:40:19+00:00
- **Authors**: David C. Schedl, Indrajit Kurmi, Oliver Bimber
- **Comment**: 11 pages, 5 figures, 3 tables, Nature Machine Intelligence (under
  review)
- **Journal**: None
- **Summary**: We show that automated person detection under occlusion conditions can be significantly improved by combining multi-perspective images before classification. Here, we employed image integration by Airborne Optical Sectioning (AOS)---a synthetic aperture imaging technique that uses camera drones to capture unstructured thermal light fields---to achieve this with a precision/recall of 96/93%. Finding lost or injured people in dense forests is not generally feasible with thermal recordings, but becomes practical with use of AOS integral images. Our findings lay the foundation for effective future search and rescue technologies that can be applied in combination with autonomous or manned aircraft. They can also be beneficial for other fields that currently suffer from inaccurate classification of partially occluded people, animals, or objects.



### IDA: Improved Data Augmentation Applied to Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2009.08845v1
- **DOI**: 10.1109/SIBGRAPI51738.2020.00036
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2009.08845v1)
- **Published**: 2020-09-18 14:03:27+00:00
- **Updated**: 2020-09-18 14:03:27+00:00
- **Authors**: Daniel V. Ruiz, Bruno A. Krinski, Eduardo Todt
- **Comment**: Accepted for presentation at SIBGRAPI 2020 - 33rd Conference on
  Graphics, Patterns and Images
- **Journal**: None
- **Summary**: In this paper, we present an Improved Data Augmentation (IDA) technique focused on Salient Object Detection (SOD). Standard data augmentation techniques proposed in the literature, such as image cropping, rotation, flipping, and resizing, only generate variations of the existing examples, providing a limited generalization. Our method combines image inpainting, affine transformations, and the linear combination of different generated background images with salient objects extracted from labeled data. Our proposed technique enables more precise control of the object's position and size while preserving background information. The background choice is based on an inter-image optimization, while object size follows a uniform random distribution within a specified interval, and the object position is intra-image optimal. We show that our method improves the segmentation quality when used for training state-of-the-art neural networks on several famous datasets of the SOD field. Combining our method with others surpasses traditional techniques such as horizontal-flip in 0.52% for F-measure and 1.19% for Precision. We also provide an evaluation in 7 different SOD datasets, with 9 distinct evaluation metrics and an average ranking of the evaluated methods.



### Synthetic Convolutional Features for Improved Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2009.08849v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.08849v1)
- **Published**: 2020-09-18 14:12:50+00:00
- **Updated**: 2020-09-18 14:12:50+00:00
- **Authors**: Yang He, Bernt Schiele, Mario Fritz
- **Comment**: ECCV 2020 Workshop on Assistive Computer Vision and Robotics
- **Journal**: None
- **Summary**: Recently, learning-based image synthesis has enabled to generate high-resolution images, either applying popular adversarial training or a powerful perceptual loss. However, it remains challenging to successfully leverage synthetic data for improving semantic segmentation with additional synthetic images. Therefore, we suggest to generate intermediate convolutional features and propose the first synthesis approach that is catered to such intermediate convolutional features. This allows us to generate new features from label masks and include them successfully into the training procedure in order to improve the performance of semantic segmentation. Experimental results and analysis on two challenging datasets Cityscapes and ADE20K show that our generated feature improves performance on segmentation tasks.



### PMVOS: Pixel-Level Matching-Based Video Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2009.08855v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.08855v1)
- **Published**: 2020-09-18 14:22:09+00:00
- **Updated**: 2020-09-18 14:22:09+00:00
- **Authors**: Suhwan Cho, Heansung Lee, Sungmin Woo, Sungjun Jang, Sangyoun Lee
- **Comment**: Code: https://github.com/suhwan-cho/PMVOS
- **Journal**: None
- **Summary**: Semi-supervised video object segmentation (VOS) aims to segment arbitrary target objects in video when the ground truth segmentation mask of the initial frame is provided. Due to this limitation of using prior knowledge about the target object, feature matching, which compares template features representing the target object with input features, is an essential step. Recently, pixel-level matching (PM), which matches every pixel in template features and input features, has been widely used for feature matching because of its high performance. However, despite its effectiveness, the information used to build the template features is limited to the initial and previous frames. We address this issue by proposing a novel method-PM-based video object segmentation (PMVOS)-that constructs strong template features containing the information of all past frames. Furthermore, we apply self-attention to the similarity maps generated from PM to capture global dependencies. On the DAVIS 2016 validation set, we achieve new state-of-the-art performance among real-time methods (> 30 fps), with a J&F score of 85.6%. Performance on the DAVIS 2017 and YouTube-VOS validation sets is also impressive, with J&F scores of 74.0% and 68.2%, respectively.



### Multi-modal Experts Network for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2009.08876v1
- **DOI**: 10.1109/ICRA40945.2020.9197459
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.08876v1)
- **Published**: 2020-09-18 14:54:54+00:00
- **Updated**: 2020-09-18 14:54:54+00:00
- **Authors**: Shihong Fang, Anna Choromanska
- **Comment**: Published at the International Conference on Robotics and Automation
  (ICRA), 2020
- **Journal**: 2020 IEEE International Conference on Robotics and Automation
  (ICRA), Paris, France, 2020, pp. 6439-6445
- **Summary**: End-to-end learning from sensory data has shown promising results in autonomous driving. While employing many sensors enhances world perception and should lead to more robust and reliable behavior of autonomous vehicles, it is challenging to train and deploy such network and at least two problems are encountered in the considered setting. The first one is the increase of computational complexity with the number of sensing devices. The other is the phenomena of network overfitting to the simplest and most informative input. We address both challenges with a novel, carefully tailored multi-modal experts network architecture and propose a multi-stage training procedure. The network contains a gating mechanism, which selects the most relevant input at each inference time step using a mixed discrete-continuous policy. We demonstrate the plausibility of the proposed approach on our 1/6 scale truck equipped with three cameras and one LiDAR.



### BNAS-v2: Memory-efficient and Performance-collapse-prevented Broad Neural Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/2009.08886v4
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2009.08886v4)
- **Published**: 2020-09-18 15:25:08+00:00
- **Updated**: 2021-01-25 09:05:02+00:00
- **Authors**: Zixiang Ding, Yaran Chen, Nannan Li, Dongbin Zhao
- **Comment**: 12 pages, 11 figures, 3 tables
- **Journal**: None
- **Summary**: In this paper, we propose BNAS-v2 to further improve the efficiency of NAS, embodying both superiorities of BCNN simultaneously. To mitigate the unfair training issue of BNAS, we employ continuous relaxation strategy to make each edge of cell in BCNN relevant to all candidate operations for over-parameterized BCNN construction. Moreover, the continuous relaxation strategy relaxes the choice of a candidate operation as a softmax over all predefined operations. Consequently, BNAS-v2 employs the gradient-based optimization algorithm to simultaneously update every possible path of over-parameterized BCNN, rather than the single sampled one as BNAS. However, continuous relaxation leads to another issue named performance collapse, in which those weight-free operations are prone to be selected by the search strategy. For this consequent issue, two solutions are given: 1) we propose Confident Learning Rate (CLR) that considers the confidence of gradient for architecture weights update, increasing with the training time of over-parameterized BCNN; 2) we introduce the combination of partial channel connections and edge normalization that also can improve the memory efficiency further. Moreover, we denote differentiable BNAS (i.e. BNAS with continuous relaxation) as BNAS-D, BNAS-D with CLR as BNAS-v2-CLR, and partial-connected BNAS-D as BNAS-v2-PC. Experimental results on CIFAR-10 and ImageNet show that 1) BNAS-v2 delivers state-of-the-art search efficiency on both CIFAR-10 (0.05 GPU days that is 4x faster than BNAS) and ImageNet (0.19 GPU days); and 2) the proposed CLR is effective to alleviate the performance collapse issue in both BNAS-D and vanilla differentiable NAS framework.



### AdderSR: Towards Energy Efficient Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2009.08891v7
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.08891v7)
- **Published**: 2020-09-18 15:29:13+00:00
- **Updated**: 2021-05-04 08:01:51+00:00
- **Authors**: Dehua Song, Yunhe Wang, Hanting Chen, Chang Xu, Chunjing Xu, Dacheng Tao
- **Comment**: None
- **Journal**: None
- **Summary**: This paper studies the single image super-resolution problem using adder neural networks (AdderNet). Compared with convolutional neural networks, AdderNet utilizing additions to calculate the output features thus avoid massive energy consumptions of conventional multiplications. However, it is very hard to directly inherit the existing success of AdderNet on large-scale image classification to the image super-resolution task due to the different calculation paradigm. Specifically, the adder operation cannot easily learn the identity mapping, which is essential for image processing tasks. In addition, the functionality of high-pass filters cannot be ensured by AdderNet. To this end, we thoroughly analyze the relationship between an adder operation and the identity mapping and insert shortcuts to enhance the performance of SR models using adder networks. Then, we develop a learnable power activation for adjusting the feature distribution and refining details. Experiments conducted on several benchmark models and datasets demonstrate that, our image super-resolution models using AdderNet can achieve comparable performance and visual quality to that of their CNN baselines with an about 2$\times$ reduction on the energy consumption.



### Image Captioning with Attention for Smart Local Tourism using EfficientNet
- **Arxiv ID**: http://arxiv.org/abs/2009.08899v1
- **DOI**: 10.1088/1757-899X/1077/1/012038
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2009.08899v1)
- **Published**: 2020-09-18 15:47:25+00:00
- **Updated**: 2020-09-18 15:47:25+00:00
- **Authors**: Dhomas Hatta Fudholi, Yurio Windiatmoko, Nurdi Afrianto, Prastyo Eko Susanto, Magfirah Suyuti, Ahmad Fathan Hidayatullah, Ridho Rahmadi
- **Comment**: 10 pages, 7 figures, still in review at ICITDA Conference
- **Journal**: None
- **Summary**: Smart systems have been massively developed to help humans in various tasks. Deep Learning technologies push even further in creating accurate assistant systems due to the explosion of data lakes. One of the smart system tasks is to disseminate users needed information. This is crucial in the tourism sector to promote local tourism destinations. In this research, we design a model of local tourism specific image captioning, which later will support the development of AI-powered systems that assist various users. The model is developed using a visual Attention mechanism and uses the state-of-the-art feature extractor architecture EfficientNet. A local tourism dataset is collected and is used in the research, along with two different kinds of captions. Captions that describe the image literally and captions that represent human logical responses when seeing the image. This is done to make the captioning model more humane when implemented in the assistance system. We compared the performance of two different models using EfficientNet architectures (B0 and B4) with other well known VGG16 and InceptionV3. The best BLEU scores we get are 73.39 and 24.51 for the training set and the validation set respectively, using EfficientNetB0. The captioning result using the developed model shows that the model can produce logical caption for local tourism-related images



### Learning Unseen Emotions from Gestures via Semantically-Conditioned Zero-Shot Perception with Adversarial Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/2009.08906v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.08906v2)
- **Published**: 2020-09-18 15:59:44+00:00
- **Updated**: 2021-12-02 08:16:02+00:00
- **Authors**: Abhishek Banerjee, Uttaran Bhattacharya, Aniket Bera
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel generalized zero-shot algorithm to recognize perceived emotions from gestures. Our task is to map gestures to novel emotion categories not encountered in training. We introduce an adversarial, autoencoder-based representation learning that correlates 3D motion-captured gesture sequence with the vectorized representation of the natural-language perceived emotion terms using word2vec embeddings. The language-semantic embedding provides a representation of the emotion label space, and we leverage this underlying distribution to map the gesture-sequences to the appropriate categorical emotion labels. We train our method using a combination of gestures annotated with known emotion terms and gestures not annotated with any emotions. We evaluate our method on the MPI Emotional Body Expressions Database (EBEDB) and obtain an accuracy of $58.43\%$. This improves the performance of current state-of-the-art algorithms for generalized zero-shot learning by $25$--$27\%$ on the absolute.



### Predicting molecular phenotypes from histopathology images: a transcriptome-wide expression-morphology analysis in breast cancer
- **Arxiv ID**: http://arxiv.org/abs/2009.08917v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2009.08917v1)
- **Published**: 2020-09-18 16:27:53+00:00
- **Updated**: 2020-09-18 16:27:53+00:00
- **Authors**: Yinxi Wang, Kimmo Kartasalo, Masi Valkonen, Christer Larsson, Pekka Ruusuvuori, Johan Hartman, Mattias Rantalainen
- **Comment**: 42 pages, 6 figures
- **Journal**: None
- **Summary**: Molecular phenotyping is central in cancer precision medicine, but remains costly and standard methods only provide a tumour average profile. Microscopic morphological patterns observable in histopathology sections from tumours are determined by the underlying molecular phenotype and associated with clinical factors. The relationship between morphology and molecular phenotype has a potential to be exploited for prediction of the molecular phenotype from the morphology visible in histopathology images.   We report the first transcriptome-wide Expression-MOrphology (EMO) analysis in breast cancer, where gene-specific models were optimised and validated for prediction of mRNA expression both as a tumour average and in spatially resolved manner. Individual deep convolutional neural networks (CNNs) were optimised to predict the expression of 17,695 genes from hematoxylin and eosin (HE) stained whole slide images (WSIs). Predictions for 9,334 (52.75%) genes were significantly associated with RNA-sequencing estimates (FDR adjusted p-value < 0.05). 1,011 of the genes were brought forward for validation, with 876 (87%) and 908 (90%) successfully replicated in internal and external test data, respectively. Predicted spatial intra-tumour variabilities in expression were validated in 76 genes, out of which 59 (77.6%) had a significant association (FDR adjusted p-value < 0.05) with spatial transcriptomics estimates. These results suggest that the proposed methodology can be applied to predict both tumour average gene expression and intra-tumour spatial expression directly from morphology, thus providing a scalable approach to characterise intra-tumour heterogeneity.



### Deep Learning for 3D Point Cloud Understanding: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2009.08920v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.08920v2)
- **Published**: 2020-09-18 16:34:12+00:00
- **Updated**: 2021-05-23 15:04:30+00:00
- **Authors**: Haoming Lu, Humphrey Shi
- **Comment**: None
- **Journal**: None
- **Summary**: The development of practical applications, such as autonomous driving and robotics, has brought increasing attention to 3D point cloud understanding. While deep learning has achieved remarkable success on image-based tasks, there are many unique challenges faced by deep neural networks in processing massive, unstructured and noisy 3D points. To demonstrate the latest progress of deep learning for 3D point cloud understanding, this paper summarizes recent remarkable research contributions in this area from several different directions (classification, segmentation, detection, tracking, flow estimation, registration, augmentation and completion), together with commonly used datasets, metrics and state-of-the-art performances. More information regarding this survey can be found at: https://github.com/SHI-Labs/3D-Point-Cloud-Learning.



### Multi-Resolution Graph Neural Network for Large-Scale Pointcloud Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2009.08924v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.08924v1)
- **Published**: 2020-09-18 16:42:02+00:00
- **Updated**: 2020-09-18 16:42:02+00:00
- **Authors**: Liuyue Xie, Tomotake Furuhata, Kenji Shimada
- **Comment**: None
- **Journal**: Conference on Robot Learning, 2020, 184
- **Summary**: In this paper, we propose a multi-resolution deep-learning architecture to semantically segment dense large-scale pointclouds. Dense pointcloud data require a computationally expensive feature encoding process before semantic segmentation. Previous work has used different approaches to drastically downsample from the original pointcloud so common computing hardware can be utilized. While these approaches can relieve the computation burden to some extent, they are still limited in their processing capability for multiple scans. We present MuGNet, a memory-efficient, end-to-end graph neural network framework to perform semantic segmentation on large-scale pointclouds. We reduce the computation demand by utilizing a graph neural network on the preformed pointcloud graphs and retain the precision of the segmentation with a bidirectional network that fuses feature embedding at different resolutions. Our framework has been validated on benchmark datasets including Stanford Large-Scale 3D Indoor Spaces Dataset(S3DIS) and Virtual KITTI Dataset. We demonstrate that our framework can process up to 45 room scans at once on a single 11 GB GPU while still surpassing other graph-based solutions for segmentation on S3DIS with an 88.5\% (+3\%) overall accuracy and 69.8\% (+7.7\%) mIOU accuracy.



### Light Direction and Color Estimation from Single Image with Deep Regression
- **Arxiv ID**: http://arxiv.org/abs/2009.08941v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2009.08941v1)
- **Published**: 2020-09-18 17:33:49+00:00
- **Updated**: 2020-09-18 17:33:49+00:00
- **Authors**: Hassan A. Sial, Ramon Baldrich, Maria Vanrell, Dimitris Samaras
- **Comment**: Conference: London Imaging Meeting 2020
- **Journal**: None
- **Summary**: We present a method to estimate the direction and color of the scene light source from a single image. Our method is based on two main ideas: (a) we use a new synthetic dataset with strong shadow effects with similar constraints to the SID dataset; (b) we define a deep architecture trained on the mentioned dataset to estimate the direction and color of the scene light source. Apart from showing good performance on synthetic images, we additionally propose a preliminary procedure to obtain light positions of the Multi-Illumination dataset, and, in this way, we also prove that our trained model achieves good performance when it is applied to real scenes.



### Encoding Robustness to Image Style via Adversarial Feature Perturbations
- **Arxiv ID**: http://arxiv.org/abs/2009.08965v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.08965v3)
- **Published**: 2020-09-18 17:52:34+00:00
- **Updated**: 2021-10-31 22:59:17+00:00
- **Authors**: Manli Shu, Zuxuan Wu, Micah Goldblum, Tom Goldstein
- **Comment**: NeurIPS 2021
- **Journal**: None
- **Summary**: Adversarial training is the industry standard for producing models that are robust to small adversarial perturbations. However, machine learning practitioners need models that are robust to other kinds of changes that occur naturally, such as changes in the style or illumination of input images. Such changes in input distribution have been effectively modeled as shifts in the mean and variance of deep image features. We adapt adversarial training by directly perturbing feature statistics, rather than image pixels, to produce models that are robust to various unseen distributional shifts. We explore the relationship between these perturbations and distributional shifts by visualizing adversarial features. Our proposed method, Adversarial Batch Normalization (AdvBN), is a single network layer that generates worst-case feature perturbations during training. By fine-tuning neural networks on adversarial feature distributions, we observe improved robustness of networks to various unseen distributional shifts, including style variations and image corruptions. In addition, we show that our proposed adversarial feature perturbation can be complementary to existing image space data augmentation methods, leading to improved performance. The source code and pre-trained models are released at \url{https://github.com/azshue/AdvBN}.



### Psoriasis Severity Assessment with a Similarity-Clustering Machine Learning Approach Reduces Intra- and Inter-observation variation
- **Arxiv ID**: http://arxiv.org/abs/2009.08997v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, 60G18, I.5.3; J.3
- **Links**: [PDF](http://arxiv.org/pdf/2009.08997v2)
- **Published**: 2020-09-18 18:04:32+00:00
- **Updated**: 2020-09-28 18:21:31+00:00
- **Authors**: Arman Garakani, Martin Malmstedt-Miller, Ionela Manole, Adrian Y. Rossler, John R. Zibert
- **Comment**: 11 pages, 3 figures
- **Journal**: None
- **Summary**: Psoriasis is a complex disease with many variations in genotype and phenotype. General advancements in medicine has further complicated both assessments and treatment for both physicians and dermatologist alike. Even with all of our technological progress we still primarily use the assessment tool Psoriasis Area and Severity Index (PASI) for severity assessments which was developed in the 1970s. In this study we evaluate a method involving digital images, a comparison web application and similarity clustering, developed to improve the assessment tool in terms of intra- and inter-observer variation. Images of patients was collected from a mobile device. Images were captured of the same lesion area taken approximately 1 week apart. Five dermatologists evaluated the severity of psoriasis by modified-PASI, absolute scoring and a relative pairwise PASI scoring using similarity-clustering and conducted using a web-program displaying two images at a time. mPASI scoring of single photos by the same or different dermatologist showed mPASI ratings of 50% to 80%, respectively. Repeated mPASI comparison using similarity clustering showed consistent mPASI ratings > 95%. Pearson correlation between absolute scoring and pairwise scoring progression was 0.72.



### Holistic Grid Fusion Based Stop Line Estimation
- **Arxiv ID**: http://arxiv.org/abs/2009.09093v1
- **DOI**: 10.1109/ICPR48806.2021.9413070
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2009.09093v1)
- **Published**: 2020-09-18 21:29:06+00:00
- **Updated**: 2020-09-18 21:29:06+00:00
- **Authors**: Runsheng Xu, Faezeh Tafazzoli, Li Zhang, Timo Rehfeld, Gunther Krehl, Arunava Seal
- **Comment**: Submitted to ICPR2020
- **Journal**: 2020 25th International Conference on Pattern Recognition (ICPR),
  Milan, Italy, 2021 pp. 8400-8407
- **Summary**: Intersection scenarios provide the most complex traffic situations in Autonomous Driving and Driving Assistance Systems. Knowing where to stop in advance in an intersection is an essential parameter in controlling the longitudinal velocity of the vehicle. Most of the existing methods in literature solely use cameras to detect stop lines, which is typically not sufficient in terms of detection range. To address this issue, we propose a method that takes advantage of fused multi-sensory data including stereo camera and lidar as input and utilizes a carefully designed convolutional neural network architecture to detect stop lines. Our experiments show that the proposed approach can improve detection range compared to camera data alone, works under heavy occlusion without observing the ground markings explicitly, is able to predict stop lines for all lanes and allows detection at a distance up to 50 meters.



### An Efficient Language-Independent Multi-Font OCR for Arabic Script
- **Arxiv ID**: http://arxiv.org/abs/2009.09115v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.09115v1)
- **Published**: 2020-09-18 22:57:03+00:00
- **Updated**: 2020-09-18 22:57:03+00:00
- **Authors**: Hussein Osman, Karim Zaghw, Mostafa Hazem, Seifeldin Elsehely
- **Comment**: 9 pages, 4 figures, 4 tables, 2 algorithms, accepted in the
  International Conference of Digital Image Processing and Pattern Recognition
  (DPPR), London, UK, November 2020
- **Journal**: None
- **Summary**: Optical Character Recognition (OCR) is the process of extracting digitized text from images of scanned documents. While OCR systems have already matured in many languages, they still have shortcomings in cursive languages with overlapping letters such as the Arabic language. This paper proposes a complete Arabic OCR system that takes a scanned image of Arabic Naskh script as an input and generates a corresponding digital document. Our Arabic OCR system consists of the following modules: Pre-processing, Word-level Feature Extraction, Character Segmentation, Character Recognition, and Post-processing. This paper also proposes an improved font-independent character segmentation algorithm that outperforms the state-of-the-art segmentation algorithms. Lastly, the paper proposes a neural network model for the character recognition task. The system has experimented on several open Arabic corpora datasets with an average character segmentation accuracy 98.06%, character recognition accuracy 99.89%, and overall system accuracy 97.94% achieving outstanding results compared to the state-of-the-art Arabic OCR systems.



