# Arxiv Papers in cs.CV on 2020-09-17
### Deep Collective Learning: Learning Optimal Inputs and Weights Jointly in Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2009.07988v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2009.07988v1)
- **Published**: 2020-09-17 00:33:04+00:00
- **Updated**: 2020-09-17 00:33:04+00:00
- **Authors**: Xiang Deng, Zhongfei, Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: It is well observed that in deep learning and computer vision literature, visual data are always represented in a manually designed coding scheme (eg., RGB images are represented as integers ranging from 0 to 255 for each channel) when they are input to an end-to-end deep neural network (DNN) for any learning task. We boldly question whether the manually designed inputs are good for DNN training for different tasks and study whether the input to a DNN can be optimally learned end-to-end together with learning the weights of the DNN. In this paper, we propose the paradigm of {\em deep collective learning} which aims to learn the weights of DNNs and the inputs to DNNs simultaneously for given tasks. We note that collective learning has been implicitly but widely used in natural language processing while it has almost never been studied in computer vision. Consequently, we propose the lookup vision networks (Lookup-VNets) as a solution to deep collective learning in computer vision. This is achieved by associating each color in each channel with a vector in lookup tables. As learning inputs in computer vision has almost never been studied in the existing literature, we explore several aspects of this question through varieties of experiments on image classification tasks. Experimental results on four benchmark datasets, i.e., CIFAR-10, CIFAR-100, Tiny ImageNet, and ImageNet (ILSVRC2012) have shown several surprising characteristics of Lookup-VNets and have demonstrated the advantages and promise of Lookup-VNets and deep collective learning.



### AAG: Self-Supervised Representation Learning by Auxiliary Augmentation with GNT-Xent Loss
- **Arxiv ID**: http://arxiv.org/abs/2009.07994v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.07994v2)
- **Published**: 2020-09-17 00:54:35+00:00
- **Updated**: 2020-10-20 08:15:20+00:00
- **Authors**: Yanlun Tu, Jianxing Feng, Yang Yang
- **Comment**: 8 pages,6 figures
- **Journal**: None
- **Summary**: Self-supervised representation learning is an emerging research topic for its powerful capacity in learning with unlabeled data. As a mainstream self-supervised learning method, augmentation-based contrastive learning has achieved great success in various computer vision tasks that lack manual annotations. Despite current progress, the existing methods are often limited by extra cost on memory or storage, and their performance still has large room for improvement. Here we present a self-supervised representation learning method, namely AAG, which is featured by an auxiliary augmentation strategy and GNT-Xent loss. The auxiliary augmentation is able to promote the performance of contrastive learning by increasing the diversity of images. The proposed GNT-Xent loss enables a steady and fast training process and yields competitive accuracy. Experiment results demonstrate the superiority of AAG to previous state-of-the-art methods on CIFAR10, CIFAR100, and SVHN. Especially, AAG achieves 94.5% top-1 accuracy on CIFAR10 with batch size 64, which is 0.5% higher than the best result of SimCLR with batch size 1024.



### MoPro: Webly Supervised Learning with Momentum Prototypes
- **Arxiv ID**: http://arxiv.org/abs/2009.07995v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.07995v1)
- **Published**: 2020-09-17 00:59:59+00:00
- **Updated**: 2020-09-17 00:59:59+00:00
- **Authors**: Junnan Li, Caiming Xiong, Steven C. H. Hoi
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a webly-supervised representation learning method that does not suffer from the annotation unscalability of supervised learning, nor the computation unscalability of self-supervised learning. Most existing works on webly-supervised representation learning adopt a vanilla supervised learning method without accounting for the prevalent noise in the training data, whereas most prior methods in learning with label noise are less effective for real-world large-scale noisy data. We propose momentum prototypes (MoPro), a simple contrastive learning method that achieves online label noise correction, out-of-distribution sample removal, and representation learning. MoPro achieves state-of-the-art performance on WebVision, a weakly-labeled noisy dataset. MoPro also shows superior performance when the pretrained model is transferred to down-stream image classification and detection tasks. It outperforms the ImageNet supervised pretrained model by +10.5 on 1-shot classification on VOC, and outperforms the best self-supervised pretrained model by +17.3 when finetuned on 1\% of ImageNet labeled samples. Furthermore, MoPro is more robust to distribution shifts. Code and pretrained models are available at https://github.com/salesforce/MoPro.



### Arbitrary Video Style Transfer via Multi-Channel Correlation
- **Arxiv ID**: http://arxiv.org/abs/2009.08003v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2009.08003v2)
- **Published**: 2020-09-17 01:30:46+00:00
- **Updated**: 2021-01-20 03:22:05+00:00
- **Authors**: Yingying Deng, Fan Tang, Weiming Dong, Haibin Huang, Chongyang Ma, Changsheng Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Video style transfer is getting more attention in AI community for its numerous applications such as augmented reality and animation productions. Compared with traditional image style transfer, performing this task on video presents new challenges: how to effectively generate satisfactory stylized results for any specified style, and maintain temporal coherence across frames at the same time. Towards this end, we propose Multi-Channel Correction network (MCCNet), which can be trained to fuse the exemplar style features and input content features for efficient style transfer while naturally maintaining the coherence of input videos. Specifically, MCCNet works directly on the feature space of style and content domain where it learns to rearrange and fuse style features based on their similarity with content features. The outputs generated by MCC are features containing the desired style patterns which can further be decoded into images with vivid style textures. Moreover, MCCNet is also designed to explicitly align the features to input which ensures the output maintains the content structures as well as the temporal continuity. To further improve the performance of MCCNet under complex light conditions, we also introduce the illumination loss during training. Qualitative and quantitative evaluations demonstrate that MCCNet performs well in both arbitrary video and image style transfer tasks.



### Model-based approach for analyzing prevalence of nuclear cataracts in elderly residents
- **Arxiv ID**: http://arxiv.org/abs/2009.08005v1
- **DOI**: 10.1016/j.compbiomed.2020.104009
- **Categories**: **physics.med-ph**, cs.CV, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/2009.08005v1)
- **Published**: 2020-09-17 01:35:58+00:00
- **Updated**: 2020-09-17 01:35:58+00:00
- **Authors**: Sachiko Kodera, Akimasa Hirata, Fumiaki Miura, Essam A. Rashed, Natsuko Hatsusaka, Naoki Yamamoto, Eri Kubo, Hiroshi Sasaki
- **Comment**: Submitted to Computers in Biology and Medicine
- **Journal**: Computers in Biology and Medicine, 2020
- **Summary**: Recent epidemiological studies have hypothesized that the prevalence of cortical cataracts is closely related to ultraviolet radiation. However, the prevalence of nuclear cataracts is higher in elderly people in tropical areas than in temperate areas. The dominant factors inducing nuclear cataracts have been widely debated. In this study, the temperature increase in the lens due to exposure to ambient conditions was computationally quantified in subjects of 50-60 years of age in tropical and temperate areas, accounting for differences in thermoregulation. A thermoregulatory response model was extended to consider elderly people in tropical areas. The time course of lens temperature for different weather conditions in five cities in Asia was computed. The temperature was higher around the mid and posterior part of the lens, which coincides with the position of the nuclear cataract. The duration of higher temperatures in the lens varied, although the daily maximum temperatures were comparable. A strong correlation (adjusted R2 > 0.85) was observed between the prevalence of nuclear cataract and the computed cumulative thermal dose in the lens. We propose the use of a cumulative thermal dose to assess the prevalence of nuclear cataracts. Cumulative wet-bulb globe temperature, a new metric computed from weather data, would be useful for practical assessment in different cities.



### Deep Momentum Uncertainty Hashing
- **Arxiv ID**: http://arxiv.org/abs/2009.08012v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.08012v3)
- **Published**: 2020-09-17 01:57:45+00:00
- **Updated**: 2021-07-13 07:25:50+00:00
- **Authors**: Chaoyou Fu, Guoli Wang, Xiang Wu, Qian Zhang, Ran He
- **Comment**: None
- **Journal**: None
- **Summary**: Combinatorial optimization (CO) has been a hot research topic because of its theoretic and practical importance. As a classic CO problem, deep hashing aims to find an optimal code for each data from finite discrete possibilities, while the discrete nature brings a big challenge to the optimization process. Previous methods usually mitigate this challenge by binary approximation, substituting binary codes for real-values via activation functions or regularizations. However, such approximation leads to uncertainty between real-values and binary ones, degrading retrieval performance. In this paper, we propose a novel Deep Momentum Uncertainty Hashing (DMUH). It explicitly estimates the uncertainty during training and leverages the uncertainty information to guide the approximation process. Specifically, we model bit-level uncertainty via measuring the discrepancy between the output of a hashing network and that of a momentum-updated network. The discrepancy of each bit indicates the uncertainty of the hashing network to the approximate output of that bit. Meanwhile, the mean discrepancy of all bits in a hashing code can be regarded as image-level uncertainty. It embodies the uncertainty of the hashing network to the corresponding input image. The hashing bit and image with higher uncertainty are paid more attention during optimization. To the best of our knowledge, this is the first work to study the uncertainty in hashing bits. Extensive experiments are conducted on four datasets to verify the superiority of our method, including CIFAR-10, NUS-WIDE, MS-COCO, and a million-scale dataset Clothing1M. Our method achieves the best performance on all of the datasets and surpasses existing state-of-the-art methods by a large margin.



### An Algorithm for Out-Of-Distribution Attack to Neural Network Encoder
- **Arxiv ID**: http://arxiv.org/abs/2009.08016v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2009.08016v4)
- **Published**: 2020-09-17 02:10:36+00:00
- **Updated**: 2021-01-27 17:58:34+00:00
- **Authors**: Liang Liang, Linhai Ma, Linchen Qian, Jiasong Chen
- **Comment**: 26 pages, 25 figures, 8 tables
- **Journal**: None
- **Summary**: Deep neural networks (DNNs), especially convolutional neural networks, have achieved superior performance on image classification tasks. However, such performance is only guaranteed if the input to a trained model is similar to the training samples, i.e., the input follows the probability distribution of the training set. Out-Of-Distribution (OOD) samples do not follow the distribution of training set, and therefore the predicted class labels on OOD samples become meaningless. Classification-based methods have been proposed for OOD detection; however, in this study we show that this type of method has no theoretical guarantee and is practically breakable by our OOD Attack algorithm because of dimensionality reduction in the DNN models. We also show that Glow likelihood-based OOD detection is breakable as well.



### LDNet: End-to-End Lane Marking Detection Approach Using a Dynamic Vision Sensor
- **Arxiv ID**: http://arxiv.org/abs/2009.08020v2
- **DOI**: 10.1109/TITS.2021.3102479
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.08020v2)
- **Published**: 2020-09-17 02:15:41+00:00
- **Updated**: 2021-11-30 10:08:40+00:00
- **Authors**: Farzeen Munir, Shoaib Azam, Moongu Jeon, Byung-Geun Lee, Witold Pedrycz
- **Comment**: None
- **Journal**: Munir, Farzeen, Shoaib Azam, Moongu Jeon, Byung-Geun Lee, and
  Witold Pedrycz. "LDNet: End-to-End Lane Marking Detection Approach Using a
  Dynamic Vision Sensor." IEEE Transactions on Intelligent Transportation
  Systems (2021)
- **Summary**: Modern vehicles are equipped with various driver-assistance systems, including automatic lane keeping, which prevents unintended lane departures. Traditional lane detection methods incorporate handcrafted or deep learning-based features followed by postprocessing techniques for lane extraction using frame-based RGB cameras. The utilization of frame-based RGB cameras for lane detection tasks is prone to illumination variations, sun glare, and motion blur, which limits the performance of lane detection methods. Incorporating an event camera for lane detection tasks in the perception stack of autonomous driving is one of the most promising solutions for mitigating challenges encountered by frame-based RGB cameras. The main contribution of this work is the design of the lane marking detection model, which employs the dynamic vision sensor. This paper explores the novel application of lane marking detection using an event camera by designing a convolutional encoder followed by the attention-guided decoder. The spatial resolution of the encoded features is retained by a dense atrous spatial pyramid pooling (ASPP) block. The additive attention mechanism in the decoder improves performance for high dimensional input encoded features that promote lane localization and relieve postprocessing computation. The efficacy of the proposed work is evaluated using the DVS dataset for lane extraction (DET). The experimental results show a significant improvement of $5.54\%$ and $5.03\%$ in $F1$ scores in multiclass and binary-class lane marking detection tasks. Additionally, the intersection over union ($IoU$) scores of the proposed method surpass those of the best-performing state-of-the-art method by $6.50\%$ and $9.37\%$ in multiclass and binary-class tasks, respectively.



### ShapeAssembly: Learning to Generate Programs for 3D Shape Structure Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2009.08026v1
- **DOI**: 10.1145/3414685.3417812
- **Categories**: **cs.GR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.08026v1)
- **Published**: 2020-09-17 02:26:45+00:00
- **Updated**: 2020-09-17 02:26:45+00:00
- **Authors**: R. Kenny Jones, Theresa Barton, Xianghao Xu, Kai Wang, Ellen Jiang, Paul Guerrero, Niloy J. Mitra, Daniel Ritchie
- **Comment**: Accepted to Siggraph Asia 2020; project page:
  https://rkjones4.github.io/shapeAssembly.html
- **Journal**: None
- **Summary**: Manually authoring 3D shapes is difficult and time consuming; generative models of 3D shapes offer compelling alternatives. Procedural representations are one such possibility: they offer high-quality and editable results but are difficult to author and often produce outputs with limited diversity. On the other extreme are deep generative models: given enough data, they can learn to generate any class of shape but their outputs have artifacts and the representation is not editable. In this paper, we take a step towards achieving the best of both worlds for novel 3D shape synthesis. We propose ShapeAssembly, a domain-specific "assembly-language" for 3D shape structures. ShapeAssembly programs construct shapes by declaring cuboid part proxies and attaching them to one another, in a hierarchical and symmetrical fashion. Its functions are parameterized with free variables, so that one program structure is able to capture a family of related shapes. We show how to extract ShapeAssembly programs from existing shape structures in the PartNet dataset. Then we train a deep generative model, a hierarchical sequence VAE, that learns to write novel ShapeAssembly programs. The program captures the subset of variability that is interpretable and editable. The deep model captures correlations across shape collections that are hard to express procedurally. We evaluate our approach by comparing shapes output by our generated programs to those from other recent shape structure synthesis models. We find that our generated shapes are more plausible and physically-valid than those of other methods. Additionally, we assess the latent spaces of these models, and find that ours is better structured and produces smoother interpolations. As an application, we use our generative model and differentiable program interpreter to infer and fit shape programs to unstructured geometry, such as point clouds.



### DanceIt: Music-inspired Dancing Video Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2009.08027v2
- **DOI**: 10.1109/TIP.2021.3086082
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.08027v2)
- **Published**: 2020-09-17 02:29:13+00:00
- **Updated**: 2021-08-07 09:14:40+00:00
- **Authors**: Xin Guo, Yifan Zhao, Jia Li
- **Comment**: 14 pages, 20 figures. [J]. IEEE Transactions on Image Processing,
  2021
- **Journal**: None
- **Summary**: Close your eyes and listen to music, one can easily imagine an actor dancing rhythmically along with the music. These dance movements are usually made up of dance movements you have seen before. In this paper, we propose to reproduce such an inherent capability of the human-being within a computer vision system. The proposed system consists of three modules. To explore the relationship between music and dance movements, we propose a cross-modal alignment module that focuses on dancing video clips, accompanied on pre-designed music, to learn a system that can judge the consistency between the visual features of pose sequences and the acoustic features of music. The learned model is then used in the imagination module to select a pose sequence for the given music. Such pose sequence selected from the music, however, is usually discontinuous. To solve this problem, in the spatial-temporal alignment module we develop a spatial alignment algorithm based on the tendency and periodicity of dance movements to predict dance movements between discontinuous fragments. In addition, the selected pose sequence is often misaligned with the music beat. To solve this problem, we further develop a temporal alignment algorithm to align the rhythm of music and dance. Finally, the processed pose sequence is used to synthesize realistic dancing videos in the imagination module. The generated dancing videos match the content and rhythm of the music. Experimental results and subjective evaluations show that the proposed approach can perform the function of generating promising dancing videos by inputting music.



### Cross-Modal Alignment with Mixture Experts Neural Network for Intral-City Retail Recommendation
- **Arxiv ID**: http://arxiv.org/abs/2009.09926v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.IR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.09926v1)
- **Published**: 2020-09-17 02:36:52+00:00
- **Updated**: 2020-09-17 02:36:52+00:00
- **Authors**: Po Li, Lei Li, Yan Fu, Jun Rong, Yu Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we introduce Cross-modal Alignment with mixture experts Neural Network (CameNN) recommendation model for intral-city retail industry, which aims to provide fresh foods and groceries retailing within 5 hours delivery service arising for the outbreak of Coronavirus disease (COVID-19) pandemic around the world. We propose CameNN, which is a multi-task model with three tasks including Image to Text Alignment (ITA) task, Text to Image Alignment (TIA) task and CVR prediction task. We use pre-trained BERT to generate the text embedding and pre-trained InceptionV4 to generate image patch embedding (each image is split into small patches with the same pixels and treat each patch as an image token). Softmax gating networks follow to learn the weight of each transformer expert output and choose only a subset of experts conditioned on the input. Then transformer encoder is applied as the share-bottom layer to learn all input features' shared interaction. Next, mixture of transformer experts (MoE) layer is implemented to model different aspects of tasks. At top of the MoE layer, we deploy a transformer layer for each task as task tower to learn task-specific information. On the real word intra-city dataset, experiments demonstrate CameNN outperform baselines and achieve significant improvements on the image and text representation. In practice, we applied CameNN on CVR prediction in our intra-city recommender system which is one of the leading intra-city platforms operated in China.



### Word Segmentation from Unconstrained Handwritten Bangla Document Images using Distance Transform
- **Arxiv ID**: http://arxiv.org/abs/2009.08037v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM, 68U10, 68U15
- **Links**: [PDF](http://arxiv.org/pdf/2009.08037v1)
- **Published**: 2020-09-17 03:14:27+00:00
- **Updated**: 2020-09-17 03:14:27+00:00
- **Authors**: Pawan Kumar Singh, Shubham Sinha, Sagnik Pal Chowdhury, Ram Sarkar, Mita Nasipuri
- **Comment**: 12 pages, 5 figures, conference
- **Journal**: 7th International Conference on Advances in Communication, Network
  and Computing (CNC),pp. 271-282, 2016
- **Summary**: Segmentation of handwritten document images into text lines and words is one of the most significant and challenging tasks in the development of a complete Optical Character Recognition (OCR) system. This paper addresses the automatic segmentation of text words directly from unconstrained Bangla handwritten document images. The popular Distance transform (DT) algorithm is applied for locating the outer boundary of the word images. This technique is free from generating the over-segmented words. A simple post-processing procedure is applied to isolate the under-segmented word images, if any. The proposed technique is tested on 50 random images taken from CMATERdb1.1.1 database. Satisfactory result is achieved with a segmentation accuracy of 91.88% which confirms the robustness of the proposed methodology.



### High-precision target positioning system for unmanned vehicles based on binocular vision
- **Arxiv ID**: http://arxiv.org/abs/2009.08040v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.08040v1)
- **Published**: 2020-09-17 03:33:05+00:00
- **Updated**: 2020-09-17 03:33:05+00:00
- **Authors**: Xianqi He, Zirui Li, Xufeng Yin, Jianwei Gong, Cheng Gong
- **Comment**: 15 pages, in Chinese
- **Journal**: None
- **Summary**: Unmanned vehicles often need to locate targets with high precision during work. In the unmanned material handling workshop, the unmanned vehicle needs to perform high-precision pose estimation of the workpiece to accurately grasp the workpiece. In this context, this paper proposes a high-precision unmanned vehicle target positioning system based on binocular vision. The system uses a region-based stereo matching algorithm to obtain a disparity map, and uses the RANSAC algorithm to extract position and posture features, which achives the estimation of the position and attitude of a six-degree-of-freedom cylindrical workpiece. In order to verify the effect of the system, this paper collects the accuracy and calculation time of the output results of the cylinder in different poses. The experimental data shows that the position accuracy of the system is 0.61~1.17mm and the angular accuracy is 1.95~5.13{\deg}, which can achieve better high-precision positioning effect.



### Image Retrieval for Structure-from-Motion via Graph Convolutional Network
- **Arxiv ID**: http://arxiv.org/abs/2009.08049v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.08049v1)
- **Published**: 2020-09-17 04:03:51+00:00
- **Updated**: 2020-09-17 04:03:51+00:00
- **Authors**: Shen Yan, Yang Pen, Shiming Lai, Yu Liu, Maojun Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Conventional image retrieval techniques for Structure-from-Motion (SfM) suffer from the limit of effectively recognizing repetitive patterns and cannot guarantee to create just enough match pairs with high precision and high recall. In this paper, we present a novel retrieval method based on Graph Convolutional Network (GCN) to generate accurate pairwise matches without costly redundancy. We formulate image retrieval task as a node binary classification problem in graph data: a node is marked as positive if it shares the scene overlaps with the query image. The key idea is that we find that the local context in feature space around a query image contains rich information about the matchable relation between this image and its neighbors. By constructing a subgraph surrounding the query image as input data, we adopt a learnable GCN to exploit whether nodes in the subgraph have overlapping regions with the query photograph. Experiments demonstrate that our method performs remarkably well on the challenging dataset of highly ambiguous and duplicated scenes. Besides, compared with state-of-the-art matchable retrieval methods, the proposed approach significantly reduces useless attempted matches without sacrificing the accuracy and completeness of reconstruction.



### MultAV: Multiplicative Adversarial Videos
- **Arxiv ID**: http://arxiv.org/abs/2009.08058v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2009.08058v2)
- **Published**: 2020-09-17 04:34:39+00:00
- **Updated**: 2021-10-10 04:26:18+00:00
- **Authors**: Shao-Yuan Lo, Vishal M. Patel
- **Comment**: Accepted at IEEE International Conference on Advanced Video and
  Signal-based Surveillance (AVSS) 2021
- **Journal**: None
- **Summary**: The majority of adversarial machine learning research focuses on additive attacks, which add adversarial perturbation to input data. On the other hand, unlike image recognition problems, only a handful of attack approaches have been explored in the video domain. In this paper, we propose a novel attack method against video recognition models, Multiplicative Adversarial Videos (MultAV), which imposes perturbation on video data by multiplication. MultAV has different noise distributions to the additive counterparts and thus challenges the defense methods tailored to resisting additive adversarial attacks. Moreover, it can be generalized to not only Lp-norm attacks with a new adversary constraint called ratio bound, but also different types of physically realizable attacks. Experimental results show that the model adversarially trained against additive attack is less robust to MultAV.



### Crossing You in Style: Cross-modal Style Transfer from Music to Visual Arts
- **Arxiv ID**: http://arxiv.org/abs/2009.08083v1
- **DOI**: 10.1145/3394171.3413624
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2009.08083v1)
- **Published**: 2020-09-17 05:58:13+00:00
- **Updated**: 2020-09-17 05:58:13+00:00
- **Authors**: Cheng-Che Lee, Wan-Yi Lin, Yen-Ting Shih, Pei-Yi Patricia Kuo, Li Su
- **Comment**: None
- **Journal**: None
- **Summary**: Music-to-visual style transfer is a challenging yet important cross-modal learning problem in the practice of creativity. Its major difference from the traditional image style transfer problem is that the style information is provided by music rather than images. Assuming that musical features can be properly mapped to visual contents through semantic links between the two domains, we solve the music-to-visual style transfer problem in two steps: music visualization and style transfer. The music visualization network utilizes an encoder-generator architecture with a conditional generative adversarial network to generate image-based music representations from music data. This network is integrated with an image style transfer method to accomplish the style transfer process. Experiments are conducted on WikiArt-IMSLP, a newly compiled dataset including Western music recordings and paintings listed by decades. By utilizing such a label to learn the semantic connection between paintings and music, we demonstrate that the proposed framework can generate diverse image style representations from a music piece, and these representations can unveil certain art forms of the same era. Subjective testing results also emphasize the role of the era label in improving the perceptual quality on the compatibility between music and visual content.



### Few-Shot Unsupervised Continual Learning through Meta-Examples
- **Arxiv ID**: http://arxiv.org/abs/2009.08107v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2009.08107v3)
- **Published**: 2020-09-17 07:02:07+00:00
- **Updated**: 2020-11-17 13:38:40+00:00
- **Authors**: Alessia Bertugli, Stefano Vincenzi, Simone Calderara, Andrea Passerini
- **Comment**: Accepted at 34th Conference on Neural Information Processing Systems
  (NeurIPS 2020), 4th Workshop on Meta-Learning, 16 pages, 3 figures
- **Journal**: None
- **Summary**: In real-world applications, data do not reflect the ones commonly used for neural networks training, since they are usually few, unlabeled and can be available as a stream. Hence many existing deep learning solutions suffer from a limited range of applications, in particular in the case of online streaming data that evolve over time. To narrow this gap, in this work we introduce a novel and complex setting involving unsupervised meta-continual learning with unbalanced tasks. These tasks are built through a clustering procedure applied to a fitted embedding space. We exploit a meta-learning scheme that simultaneously alleviates catastrophic forgetting and favors the generalization to new tasks. Moreover, to encourage feature reuse during the meta-optimization, we exploit a single inner loop taking advantage of an aggregated representation achieved through the use of a self-attention mechanism. Experimental results on few-shot learning benchmarks show competitive performance even compared to the supervised case. Additionally, we empirically observe that in an unsupervised scenario, the small tasks and the variability in the clusters pooling play a crucial role in the generalization capability of the network. Further, on complex datasets, the exploitation of more clusters than the true number of classes leads to higher results, even compared to the ones obtained with full supervision, suggesting that a predefined partitioning into classes can miss relevant structural information.



### Online Alternate Generator against Adversarial Attacks
- **Arxiv ID**: http://arxiv.org/abs/2009.08110v1
- **DOI**: 10.1109/TIP.2020.3025404
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.08110v1)
- **Published**: 2020-09-17 07:11:16+00:00
- **Updated**: 2020-09-17 07:11:16+00:00
- **Authors**: Haofeng Li, Yirui Zeng, Guanbin Li, Liang Lin, Yizhou Yu
- **Comment**: Accepted as a Regular paper in the IEEE Transactions on Image
  Processing
- **Journal**: None
- **Summary**: The field of computer vision has witnessed phenomenal progress in recent years partially due to the development of deep convolutional neural networks. However, deep learning models are notoriously sensitive to adversarial examples which are synthesized by adding quasi-perceptible noises on real images. Some existing defense methods require to re-train attacked target networks and augment the train set via known adversarial attacks, which is inefficient and might be unpromising with unknown attack types. To overcome the above issues, we propose a portable defense method, online alternate generator, which does not need to access or modify the parameters of the target networks. The proposed method works by online synthesizing another image from scratch for an input image, instead of removing or destroying adversarial noises. To avoid pretrained parameters exploited by attackers, we alternately update the generator and the synthesized image at the inference stage. Experimental results demonstrate that the proposed defensive scheme and method outperforms a series of state-of-the-art defending models against gray-box adversarial attacks.



### Collaborative Training between Region Proposal Localization and Classification for Domain Adaptive Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2009.08119v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.08119v2)
- **Published**: 2020-09-17 07:39:52+00:00
- **Updated**: 2020-09-18 03:34:05+00:00
- **Authors**: Ganlong Zhao, Guanbin Li, Ruijia Xu, Liang Lin
- **Comment**: Accepted by ECCV 2020
- **Journal**: None
- **Summary**: Object detectors are usually trained with large amount of labeled data, which is expensive and labor-intensive. Pre-trained detectors applied to unlabeled dataset always suffer from the difference of dataset distribution, also called domain shift. Domain adaptation for object detection tries to adapt the detector from labeled datasets to unlabeled ones for better performance. In this paper, we are the first to reveal that the region proposal network (RPN) and region proposal classifier~(RPC) in the endemic two-stage detectors (e.g., Faster RCNN) demonstrate significantly different transferability when facing large domain gap. The region classifier shows preferable performance but is limited without RPN's high-quality proposals while simple alignment in the backbone network is not effective enough for RPN adaptation. We delve into the consistency and the difference of RPN and RPC, treat them individually and leverage high-confidence output of one as mutual guidance to train the other. Moreover, the samples with low-confidence are used for discrepancy calculation between RPN and RPC and minimax optimization. Extensive experimental results on various scenarios have demonstrated the effectiveness of our proposed method in both domain-adaptive region proposal generation and object detection. Code is available at https://github.com/GanlongZhao/CST_DA_detection.



### DLBCL-Morph: Morphological features computed using deep learning for an annotated digital DLBCL image set
- **Arxiv ID**: http://arxiv.org/abs/2009.08123v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.08123v3)
- **Published**: 2020-09-17 07:43:42+00:00
- **Updated**: 2020-09-24 11:02:28+00:00
- **Authors**: Damir Vrabac, Akshay Smit, Rebecca Rojansky, Yasodha Natkunam, Ranjana H. Advani, Andrew Y. Ng, Sebastian Fernandez-Pol, Pranav Rajpurkar
- **Comment**: Corrections to folder structure figure
- **Journal**: None
- **Summary**: Diffuse Large B-Cell Lymphoma (DLBCL) is the most common non-Hodgkin lymphoma. Though histologically DLBCL shows varying morphologies, no morphologic features have been consistently demonstrated to correlate with prognosis. We present a morphologic analysis of histology sections from 209 DLBCL cases with associated clinical and cytogenetic data. Duplicate tissue core sections were arranged in tissue microarrays (TMAs), and replicate sections were stained with H&E and immunohistochemical stains for CD10, BCL6, MUM1, BCL2, and MYC. The TMAs are accompanied by pathologist-annotated regions-of-interest (ROIs) that identify areas of tissue representative of DLBCL. We used a deep learning model to segment all tumor nuclei in the ROIs, and computed several geometric features for each segmented nucleus. We fit a Cox proportional hazards model to demonstrate the utility of these geometric features in predicting survival outcome, and found that it achieved a C-index (95% CI) of 0.635 (0.574,0.691). Our finding suggests that geometric features computed from tumor nuclei are of prognostic importance, and should be validated in prospective studies.



### Multidimensional Scaling, Sammon Mapping, and Isomap: Tutorial and Survey
- **Arxiv ID**: http://arxiv.org/abs/2009.08136v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.08136v1)
- **Published**: 2020-09-17 08:12:25+00:00
- **Updated**: 2020-09-17 08:12:25+00:00
- **Authors**: Benyamin Ghojogh, Ali Ghodsi, Fakhri Karray, Mark Crowley
- **Comment**: To appear as a part of an upcoming academic book on dimensionality
  reduction and manifold learning
- **Journal**: None
- **Summary**: Multidimensional Scaling (MDS) is one of the first fundamental manifold learning methods. It can be categorized into several methods, i.e., classical MDS, kernel classical MDS, metric MDS, and non-metric MDS. Sammon mapping and Isomap can be considered as special cases of metric MDS and kernel classical MDS, respectively. In this tutorial and survey paper, we review the theory of MDS, Sammon mapping, and Isomap in detail. We explain all the mentioned categories of MDS. Then, Sammon mapping, Isomap, and kernel Isomap are explained. Out-of-sample embedding for MDS and Isomap using eigenfunctions and kernel mapping are introduced. Then, Nystrom approximation and its use in landmark MDS and landmark Isomap are introduced for big data embedding. We also provide some simulations for illustrating the embedding by these methods.



### POMP: Pomcp-based Online Motion Planning for active visual search in indoor environments
- **Arxiv ID**: http://arxiv.org/abs/2009.08140v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.08140v1)
- **Published**: 2020-09-17 08:23:50+00:00
- **Updated**: 2020-09-17 08:23:50+00:00
- **Authors**: Yiming Wang, Francesco Giuliari, Riccardo Berra, Alberto Castellini, Alessio Del Bue, Alessandro Farinelli, Marco Cristani, Francesco Setti
- **Comment**: Accepted at BMVC2020
- **Journal**: None
- **Summary**: In this paper we focus on the problem of learning an optimal policy for Active Visual Search (AVS) of objects in known indoor environments with an online setup. Our POMP method uses as input the current pose of an agent (e.g. a robot) and a RGB-D frame. The task is to plan the next move that brings the agent closer to the target object. We model this problem as a Partially Observable Markov Decision Process solved by a Monte-Carlo planning approach. This allows us to make decisions on the next moves by iterating over the known scenario at hand, exploring the environment and searching for the object at the same time. Differently from the current state of the art in Reinforcement Learning, POMP does not require extensive and expensive (in time and computation) labelled data so being very agile in solving AVS in small and medium real scenarios. We only require the information of the floormap of the environment, an information usually available or that can be easily extracted from an a priori single exploration run. We validate our method on the publicly available AVD benchmark, achieving an average success rate of 0.76 with an average path length of 17.1, performing close to the state of the art but without any training needed. Additionally, we show experimentally the robustness of our method when the quality of the object detection goes from ideal to faulty.



### Holistic Filter Pruning for Efficient Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2009.08169v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2009.08169v1)
- **Published**: 2020-09-17 09:23:36+00:00
- **Updated**: 2020-09-17 09:23:36+00:00
- **Authors**: Lukas Enderich, Fabian Timm, Wolfram Burgard
- **Comment**: preprint, accepted at WACV2021
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) are usually over-parameterized to increase the likelihood of getting adequate initial weights by random initialization. Consequently, trained DNNs have many redundancies which can be pruned from the model to reduce complexity and improve the ability to generalize. Structural sparsity, as achieved by filter pruning, directly reduces the tensor sizes of weights and activations and is thus particularly effective for reducing complexity. We propose "Holistic Filter Pruning" (HFP), a novel approach for common DNN training that is easy to implement and enables to specify accurate pruning rates for the number of both parameters and multiplications. After each forward pass, the current model complexity is calculated and compared to the desired target size. By gradient descent, a global solution can be found that allocates the pruning budget over the individual layers such that the desired target size is fulfilled. In various experiments, we give insights into the training and achieve state-of-the-art performance on CIFAR-10 and ImageNet (HFP prunes 60% of the multiplications of ResNet-50 on ImageNet with no significant loss in the accuracy). We believe our simple and powerful pruning approach to constitute a valuable contribution for users of DNNs in low-cost applications.



### Single Frame Deblurring with Laplacian Filters
- **Arxiv ID**: http://arxiv.org/abs/2009.08182v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.08182v1)
- **Published**: 2020-09-17 09:49:25+00:00
- **Updated**: 2020-09-17 09:49:25+00:00
- **Authors**: Baran Ataman, Esin Guldogan
- **Comment**: 6 pages, 3 figures, 1 tables
- **Journal**: None
- **Summary**: Blind single image deblurring has been a challenge over many decades due to the ill-posed nature of the problem. In this paper, we propose a single-frame blind deblurring solution with the aid of Laplacian filters. Utilized Residual Dense Network has proven its strengths in superresolution task, thus we selected it as a baseline architecture. We evaluated the proposed solution with state-of-art DNN methods on a benchmark dataset. The proposed method shows significant improvement in image quality measured objectively and subjectively.



### Deploying machine learning to assist digital humanitarians: making image annotation in OpenStreetMap more efficient
- **Arxiv ID**: http://arxiv.org/abs/2009.08188v1
- **DOI**: 10.1080/13658816.2020.1814303
- **Categories**: **cs.CV**, cs.HC, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2009.08188v1)
- **Published**: 2020-09-17 10:05:30+00:00
- **Updated**: 2020-09-17 10:05:30+00:00
- **Authors**: John E. Vargas-Muñoz, Devis Tuia, Alexandre X. Falcão
- **Comment**: None
- **Journal**: None
- **Summary**: Locating populations in rural areas of developing countries has attracted the attention of humanitarian mapping projects since it is important to plan actions that affect vulnerable areas. Recent efforts have tackled this problem as the detection of buildings in aerial images. However, the quality and the amount of rural building annotated data in open mapping services like OpenStreetMap (OSM) is not sufficient for training accurate models for such detection. Although these methods have the potential of aiding in the update of rural building information, they are not accurate enough to automatically update the rural building maps. In this paper, we explore a human-computer interaction approach and propose an interactive method to support and optimize the work of volunteers in OSM. The user is asked to verify/correct the annotation of selected tiles during several iterations and therefore improving the model with the new annotated data. The experimental results, with simulated and real user annotation corrections, show that the proposed method greatly reduces the amount of data that the volunteers of OSM need to verify/correct. The proposed methodology could benefit humanitarian mapping projects, not only by making more efficient the process of annotation but also by improving the engagement of volunteers.



### Vax-a-Net: Training-time Defence Against Adversarial Patch Attacks
- **Arxiv ID**: http://arxiv.org/abs/2009.08194v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.08194v1)
- **Published**: 2020-09-17 10:27:08+00:00
- **Updated**: 2020-09-17 10:27:08+00:00
- **Authors**: T. Gittings, S. Schneider, J. Collomosse
- **Comment**: 16 pages, 10 figures, ACCV 2020
- **Journal**: None
- **Summary**: We present Vax-a-Net; a technique for immunizing convolutional neural networks (CNNs) against adversarial patch attacks (APAs). APAs insert visually overt, local regions (patches) into an image to induce misclassification. We introduce a conditional Generative Adversarial Network (GAN) architecture that simultaneously learns to synthesise patches for use in APAs, whilst exploiting those attacks to adapt a pre-trained target CNN to reduce its susceptibility to them. This approach enables resilience against APAs to be conferred to pre-trained models, which would be impractical with conventional adversarial training due to the slow convergence of APA methods. We demonstrate transferability of this protection to defend against existing APAs, and show its efficacy across several contemporary CNN architectures.



### Deep Learning Approaches to Classification of Production Technology for 19th Century Books
- **Arxiv ID**: http://arxiv.org/abs/2009.08219v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.08219v1)
- **Published**: 2020-09-17 11:39:35+00:00
- **Updated**: 2020-09-17 11:39:35+00:00
- **Authors**: Chanjong Im, Junaid Ghauri, John Rothman, Thomas Mandl
- **Comment**: LWDA 2018: Mannheim, Germany
- **Journal**: Proceedings of the Conference "Lernen, Wissen, Daten, Analysen",
  {LWDA} 2018, Mannheim, Germany, August 22-24, 2018
- **Summary**: Cultural research is dedicated to understanding the processes of knowledge dissemination and the social and technological practices in the book industry. Research on children books in the 19th century can be supported by computer systems. Specifically, the advances in digital image processing seem to offer great opportunities for analyzing and quantifying the visual components in the books. The production technology for illustrations in books in the 19th century was characterized by a shift from wood or copper engraving to lithography. We report classification experiments which intend to classify images based on the production technology. For a classification task that is also difficult for humans, the classification quality reaches only around 70%. We analyze some further error sources and identify reasons for the low performance.



### Label Smoothing and Adversarial Robustness
- **Arxiv ID**: http://arxiv.org/abs/2009.08233v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.08233v1)
- **Published**: 2020-09-17 12:36:35+00:00
- **Updated**: 2020-09-17 12:36:35+00:00
- **Authors**: Chaohao Fu, Hongbin Chen, Na Ruan, Weijia Jia
- **Comment**: None
- **Journal**: None
- **Summary**: Recent studies indicate that current adversarial attack methods are flawed and easy to fail when encountering some deliberately designed defense. Sometimes even a slight modification in the model details will invalidate the attack. We find that training model with label smoothing can easily achieve striking accuracy under most gradient-based attacks. For instance, the robust accuracy of a WideResNet model trained with label smoothing on CIFAR-10 achieves 75% at most under PGD attack. To understand the reason underlying the subtle robustness, we investigate the relationship between label smoothing and adversarial robustness. Through theoretical analysis about the characteristics of the network trained with label smoothing and experiment verification of its performance under various attacks. We demonstrate that the robustness produced by label smoothing is incomplete based on the fact that its defense effect is volatile, and it cannot defend attacks transferred from a naturally trained model. Our study enlightens the research community to rethink how to evaluate the model's robustness appropriately.



### Learning a Deep Part-based Representation by Preserving Data Distribution
- **Arxiv ID**: http://arxiv.org/abs/2009.08246v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.08246v1)
- **Published**: 2020-09-17 12:49:36+00:00
- **Updated**: 2020-09-17 12:49:36+00:00
- **Authors**: Anyong Qin, Zhaowei Shang, Zhuolin Tan, Taiping Zhang, Yuan Yan Tang
- **Comment**: 11 pages, 8 figures
- **Journal**: None
- **Summary**: Unsupervised dimensionality reduction is one of the commonly used techniques in the field of high dimensional data recognition problems. The deep autoencoder network which constrains the weights to be non-negative, can learn a low dimensional part-based representation of data. On the other hand, the inherent structure of the each data cluster can be described by the distribution of the intraclass samples. Then one hopes to learn a new low dimensional representation which can preserve the intrinsic structure embedded in the original high dimensional data space perfectly. In this paper, by preserving the data distribution, a deep part-based representation can be learned, and the novel algorithm is called Distribution Preserving Network Embedding (DPNE). In DPNE, we first need to estimate the distribution of the original high dimensional data using the $k$-nearest neighbor kernel density estimation, and then we seek a part-based representation which respects the above distribution. The experimental results on the real-world data sets show that the proposed algorithm has good performance in terms of cluster accuracy and AMI. It turns out that the manifold structure in the raw data can be well preserved in the low dimensional feature space.



### Dynamic Edge Weights in Graph Neural Networks for 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2009.08253v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, I.2.10; I.4; I.5
- **Links**: [PDF](http://arxiv.org/pdf/2009.08253v1)
- **Published**: 2020-09-17 12:56:17+00:00
- **Updated**: 2020-09-17 12:56:17+00:00
- **Authors**: Sumesh Thakur, Jiju Peethambaran
- **Comment**: 11 pages; 7 figures
- **Journal**: None
- **Summary**: A robust and accurate 3D detection system is an integral part of autonomous vehicles. Traditionally, a majority of 3D object detection algorithms focus on processing 3D point clouds using voxel grids or bird's eye view (BEV). Recent works, however, demonstrate the utilization of the graph neural network (GNN) as a promising approach to 3D object detection. In this work, we propose an attention based feature aggregation technique in GNN for detecting objects in LiDAR scan. We first employ a distance-aware down-sampling scheme that not only enhances the algorithmic performance but also retains maximum geometric features of objects even if they lie far from the sensor. In each layer of the GNN, apart from the linear transformation which maps the per node input features to the corresponding higher level features, a per node masked attention by specifying different weights to different nodes in its first ring neighborhood is also performed. The masked attention implicitly accounts for the underlying neighborhood graph structure of every node and also eliminates the need of costly matrix operations thereby improving the detection accuracy without compromising the performance. The experiments on KITTI dataset show that our method yields comparable results for 3D object detection.



### Adversarial Image Composition with Auxiliary Illumination
- **Arxiv ID**: http://arxiv.org/abs/2009.08255v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.08255v2)
- **Published**: 2020-09-17 12:58:16+00:00
- **Updated**: 2021-01-09 15:05:42+00:00
- **Authors**: Fangneng Zhan, Shijian Lu, Changgong Zhang, Feiying Ma, Xuansong Xie
- **Comment**: Accepted to ACCV 2020 (Oral)
- **Journal**: None
- **Summary**: Dealing with the inconsistency between a foreground object and a background image is a challenging task in high-fidelity image composition. State-of-the-art methods strive to harmonize the composed image by adapting the style of foreground objects to be compatible with the background image, whereas the potential shadow of foreground objects within the composed image which is critical to the composition realism is largely neglected. In this paper, we propose an Adversarial Image Composition Net (AIC-Net) that achieves realistic image composition by considering potential shadows that the foreground object projects in the composed image. A novel branched generation mechanism is proposed, which disentangles the generation of shadows and the transfer of foreground styles for optimal accomplishment of the two tasks simultaneously. A differentiable spatial transformation module is designed which bridges the local harmonization and the global harmonization to achieve their joint optimization effectively. Extensive experiments on pedestrian and car composition tasks show that the proposed AIC-Net achieves superior composition performance qualitatively and quantitatively.



### Evaluating and Mitigating Bias in Image Classifiers: A Causal Perspective Using Counterfactuals
- **Arxiv ID**: http://arxiv.org/abs/2009.08270v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.08270v4)
- **Published**: 2020-09-17 13:19:31+00:00
- **Updated**: 2022-01-06 12:40:39+00:00
- **Authors**: Saloni Dash, Vineeth N Balasubramanian, Amit Sharma
- **Comment**: Accepted for Publication at WACV 2022
- **Journal**: None
- **Summary**: Counterfactual examples for an input -- perturbations that change specific features but not others -- have been shown to be useful for evaluating bias of machine learning models, e.g., against specific demographic groups. However, generating counterfactual examples for images is non-trivial due to the underlying causal structure on the various features of an image. To be meaningful, generated perturbations need to satisfy constraints implied by the causal model. We present a method for generating counterfactuals by incorporating a structural causal model (SCM) in an improved variant of Adversarially Learned Inference (ALI), that generates counterfactuals in accordance with the causal relationships between attributes of an image. Based on the generated counterfactuals, we show how to explain a pre-trained machine learning classifier, evaluate its bias, and mitigate the bias using a counterfactual regularizer. On the Morpho-MNIST dataset, our method generates counterfactuals comparable in quality to prior work on SCM-based counterfactuals (DeepSCM), while on the more complex CelebA dataset our method outperforms DeepSCM in generating high-quality valid counterfactuals. Moreover, generated counterfactuals are indistinguishable from reconstructed images in a human evaluation experiment and we subsequently use them to evaluate the fairness of a standard classifier trained on CelebA data. We show that the classifier is biased w.r.t. skin and hair color, and how counterfactual regularization can remove those biases.



### Video based real-time positional tracker
- **Arxiv ID**: http://arxiv.org/abs/2009.08276v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.08276v3)
- **Published**: 2020-09-17 13:24:39+00:00
- **Updated**: 2020-10-29 08:57:00+00:00
- **Authors**: David Albarracín, Jesús Hormigo, José David Fernández
- **Comment**: 8 pages, 5 figures
- **Journal**: None
- **Summary**: We propose a system that uses video as the input to track the position of objects relative to their surrounding environment in real-time. The neural network employed is trained on a 100% synthetic dataset coming from our own automated generator. The positional tracker relies on a range of 1 to n video cameras placed around an arena of choice.   The system returns the positions of the tracked objects relative to the broader world by understanding the overlapping matrices formed by the cameras and therefore these can be extrapolated into real world coordinates.   In most cases, we achieve a higher update rate and positioning precision than any of the existing GPS-based systems, in particular for indoor objects or those occluded from clear sky.



### A Linked Aggregate Code for Processing Faces (Revised Version)
- **Arxiv ID**: http://arxiv.org/abs/2009.08281v1
- **DOI**: 10.5281/zenodo.4034544
- **Categories**: **cs.CV**, cs.AI, cs.CY, 68T01, I.2.0; K.4.0
- **Links**: [PDF](http://arxiv.org/pdf/2009.08281v1)
- **Published**: 2020-09-17 13:29:25+00:00
- **Updated**: 2020-09-17 13:29:25+00:00
- **Authors**: Michael Lyons, Kazunori Morikawa
- **Comment**: 18 pages, 3 figures, 1 table
- **Journal**: None
- **Summary**: A model of face representation, inspired by the biology of the visual system, is compared to experimental data on the perception of facial similarity. The face representation model uses aggregate primary visual cortex (V1) cell responses topographically linked to a grid covering the face, allowing comparison of shape and texture at corresponding points in two facial images. When a set of relatively similar faces was used as stimuli, this Linked Aggregate Code (LAC) predicted human performance in similarity judgment experiments. When faces of perceivable categories were used, dimensions such as apparent sex and race emerged from the LAC model without training. The dimensional structure of the LAC similarity measure for the mixed category task displayed some psychologically plausible features but also highlighted differences between the model and the human similarity judgements. The human judgements exhibited a racial perceptual bias that was not shared by the LAC model. The results suggest that the LAC based similarity measure may offer a fertile starting point for further modelling studies of face representation in higher visual areas, including studies of the development of biases in face perception.



### Back to Event Basics: Self-Supervised Learning of Image Reconstruction for Event Cameras via Photometric Constancy
- **Arxiv ID**: http://arxiv.org/abs/2009.08283v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.08283v2)
- **Published**: 2020-09-17 13:30:05+00:00
- **Updated**: 2021-04-12 15:19:15+00:00
- **Authors**: F. Paredes-Vallés, G. C. H. E. de Croon
- **Comment**: 9 pages, 6 figures, 5 tables
- **Journal**: None
- **Summary**: Event cameras are novel vision sensors that sample, in an asynchronous fashion, brightness increments with low latency and high temporal resolution. The resulting streams of events are of high value by themselves, especially for high speed motion estimation. However, a growing body of work has also focused on the reconstruction of intensity frames from the events, as this allows bridging the gap with the existing literature on appearance- and frame-based computer vision. Recent work has mostly approached this problem using neural networks trained with synthetic, ground-truth data. In this work we approach, for the first time, the intensity reconstruction problem from a self-supervised learning perspective. Our method, which leverages the knowledge of the inner workings of event cameras, combines estimated optical flow and the event-based photometric constancy to train neural networks without the need for any ground-truth or synthetic data. Results across multiple datasets show that the performance of the proposed self-supervised approach is in line with the state-of-the-art. Additionally, we propose a novel, lightweight neural network for optical flow estimation that achieves high speed inference with only a minor drop in performance.



### Learning to Identify Physical Parameters from Video Using Differentiable Physics
- **Arxiv ID**: http://arxiv.org/abs/2009.08292v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2009.08292v1)
- **Published**: 2020-09-17 13:36:57+00:00
- **Updated**: 2020-09-17 13:36:57+00:00
- **Authors**: Rama Krishna Kandukuri, Jan Achterhold, Michael Möller, Jörg Stückler
- **Comment**: Accepted for 42nd German Conference on Pattern Recognition (DAGM-GCPR
  2020), T\"ubingen, Germany
- **Journal**: None
- **Summary**: Video representation learning has recently attracted attention in computer vision due to its applications for activity and scene forecasting or vision-based planning and control. Video prediction models often learn a latent representation of video which is encoded from input frames and decoded back into images. Even when conditioned on actions, purely deep learning based architectures typically lack a physically interpretable latent space. In this study, we use a differentiable physics engine within an action-conditional video representation network to learn a physical latent representation. We propose supervised and self-supervised learning methods to train our network and identify physical properties. The latter uses spatial transformers to decode physical states back into images. The simulation scenarios in our experiments comprise pushing, sliding and colliding objects, for which we also analyze the observability of the physical properties. In experiments we demonstrate that our network can learn to encode images and identify physical properties like mass and friction from videos and action sequences in the simulated scenarios. We evaluate the accuracy of our supervised and self-supervised methods and compare it with a system identification baseline which directly learns from state trajectories. We also demonstrate the ability of our method to predict future video frames from input images and actions.



### Low-Rank Matrix Recovery from Noise via an MDL Framework-based Atomic Norm
- **Arxiv ID**: http://arxiv.org/abs/2009.08297v2
- **DOI**: 10.3390/s20216111
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.08297v2)
- **Published**: 2020-09-17 13:45:18+00:00
- **Updated**: 2020-10-28 01:24:19+00:00
- **Authors**: Anyong Qin, Lina Xian, Yongliang Yang, Taiping Zhang, Yuan Yan Tang
- **Comment**: 14 pages, 13 figures
- **Journal**: sensors-2020
- **Summary**: The recovery of the underlying low-rank structure of clean data corrupted with sparse noise/outliers is attracting increasing interest. However, in many low-level vision problems, the exact target rank of the underlying structure and the particular locations and values of the sparse outliers are not known. Thus, the conventional methods cannot separate the low-rank and sparse components completely, especially in the case of gross outliers or deficient observations. Therefore, in this study, we employ the minimum description length (MDL) principle and atomic norm for low-rank matrix recovery to overcome these limitations. First, we employ the atomic norm to find all the candidate atoms of low-rank and sparse terms, and then we minimize the description length of the model in order to select the appropriate atoms of low-rank and the sparse matrices, respectively. Our experimental analyses show that the proposed approach can obtain a higher success rate than the state-of-the-art methods, even when the number of observations is limited or the corruption ratio is high. Experimental results utilizing synthetic data and real sensing applications (high dynamic range imaging, background modeling, removing noise and shadows) demonstrate the effectiveness, robustness and efficiency of the proposed method.



### Novel View Synthesis from Single Images via Point Cloud Transformation
- **Arxiv ID**: http://arxiv.org/abs/2009.08321v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.08321v2)
- **Published**: 2020-09-17 14:13:19+00:00
- **Updated**: 2020-09-18 14:54:03+00:00
- **Authors**: Hoang-An Le, Thomas Mensink, Partha Das, Theo Gevers
- **Comment**: Accepted at British Machine Vision Conference 2020
- **Journal**: None
- **Summary**: In this paper the argument is made that for true novel view synthesis of objects, where the object can be synthesized from any viewpoint, an explicit 3D shape representation isdesired. Our method estimates point clouds to capture the geometry of the object, which can be freely rotated into the desired view and then projected into a new image. This image, however, is sparse by nature and hence this coarse view is used as the input of an image completion network to obtain the dense target view. The point cloud is obtained using the predicted pixel-wise depth map, estimated from a single RGB input image,combined with the camera intrinsics. By using forward warping and backward warpingbetween the input view and the target view, the network can be trained end-to-end without supervision on depth. The benefit of using point clouds as an explicit 3D shape for novel view synthesis is experimentally validated on the 3D ShapeNet benchmark. Source code and data will be available at https://lhoangan.github.io/pc4novis/.



### Noisy Concurrent Training for Efficient Learning under Label Noise
- **Arxiv ID**: http://arxiv.org/abs/2009.08325v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.08325v1)
- **Published**: 2020-09-17 14:22:17+00:00
- **Updated**: 2020-09-17 14:22:17+00:00
- **Authors**: Fahad Sarfraz, Elahe Arani, Bahram Zonooz
- **Comment**: Accepted at IEEE Winter Conference on Applications of Computer Vision
  (WACV, 2021)
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) fail to learn effectively under label noise and have been shown to memorize random labels which affect their generalization performance. We consider learning in isolation, using one-hot encoded labels as the sole source of supervision, and a lack of regularization to discourage memorization as the major shortcomings of the standard training procedure. Thus, we propose Noisy Concurrent Training (NCT) which leverages collaborative learning to use the consensus between two models as an additional source of supervision. Furthermore, inspired by trial-to-trial variability in the brain, we propose a counter-intuitive regularization technique, target variability, which entails randomly changing the labels of a percentage of training samples in each batch as a deterrent to memorization and over-generalization in DNNs. Target variability is applied independently to each model to keep them diverged and avoid the confirmation bias. As DNNs tend to prioritize learning simple patterns first before memorizing the noisy labels, we employ a dynamic learning scheme whereby as the training progresses, the two models increasingly rely more on their consensus. NCT also progressively increases the target variability to avoid memorization in later stages. We demonstrate the effectiveness of our approach on both synthetic and real-world noisy benchmark datasets.



### Review: Deep Learning in Electron Microscopy
- **Arxiv ID**: http://arxiv.org/abs/2009.08328v7
- **DOI**: 10.1088/2632-2153/abd614
- **Categories**: **eess.IV**, cond-mat.mtrl-sci, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.08328v7)
- **Published**: 2020-09-17 14:23:55+00:00
- **Updated**: 2021-03-08 10:12:04+00:00
- **Authors**: Jeffrey M. Ede
- **Comment**: 33 pages, 16 figures + 2 tables + 65 pages of references
- **Journal**: None
- **Summary**: Deep learning is transforming most areas of science and technology, including electron microscopy. This review paper offers a practical perspective aimed at developers with limited familiarity. For context, we review popular applications of deep learning in electron microscopy. Afterwards, we discuss hardware and software needed to get started with deep learning and interface with electron microscopes. We then review neural network components, popular architectures, and their optimization. Finally, we discuss future directions of deep learning in electron microscopy.



### S2SD: Simultaneous Similarity-based Self-Distillation for Deep Metric Learning
- **Arxiv ID**: http://arxiv.org/abs/2009.08348v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.08348v3)
- **Published**: 2020-09-17 14:54:24+00:00
- **Updated**: 2021-06-04 23:07:38+00:00
- **Authors**: Karsten Roth, Timo Milbich, Björn Ommer, Joseph Paul Cohen, Marzyeh Ghassemi
- **Comment**: Accepted to ICML2021
- **Journal**: None
- **Summary**: Deep Metric Learning (DML) provides a crucial tool for visual similarity and zero-shot applications by learning generalizing embedding spaces, although recent work in DML has shown strong performance saturation across training objectives. However, generalization capacity is known to scale with the embedding space dimensionality. Unfortunately, high dimensional embeddings also create higher retrieval cost for downstream applications. To remedy this, we propose \emph{Simultaneous Similarity-based Self-distillation (S2SD). S2SD extends DML with knowledge distillation from auxiliary, high-dimensional embedding and feature spaces to leverage complementary context during training while retaining test-time cost and with negligible changes to the training time. Experiments and ablations across different objectives and standard benchmarks show S2SD offers notable improvements of up to 7% in Recall@1, while also setting a new state-of-the-art. Code available at https://github.com/MLforHealth/S2SD.



### Face Mask Detection using Transfer Learning of InceptionV3
- **Arxiv ID**: http://arxiv.org/abs/2009.08369v2
- **DOI**: 10.1007/978-3-030-66665-1_6
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2009.08369v2)
- **Published**: 2020-09-17 15:34:06+00:00
- **Updated**: 2020-10-20 19:04:26+00:00
- **Authors**: G. Jignesh Chowdary, Narinder Singh Punn, Sanjay Kumar Sonbhadra, Sonali Agarwal
- **Comment**: None
- **Journal**: None
- **Summary**: The world is facing a huge health crisis due to the rapid transmission of coronavirus (COVID-19). Several guidelines were issued by the World Health Organization (WHO) for protection against the spread of coronavirus. According to WHO, the most effective preventive measure against COVID-19 is wearing a mask in public places and crowded areas. It is very difficult to monitor people manually in these areas. In this paper, a transfer learning model is proposed to automate the process of identifying the people who are not wearing mask. The proposed model is built by fine-tuning the pre-trained state-of-the-art deep learning model, InceptionV3. The proposed model is trained and tested on the Simulated Masked Face Dataset (SMFD). Image augmentation technique is adopted to address the limited availability of data for better training and testing of the model. The model outperformed the other recently proposed approaches by achieving an accuracy of 99.9% during training and 100% during testing.



### Microtubule Tracking in Electron Microscopy Volumes
- **Arxiv ID**: http://arxiv.org/abs/2009.08371v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2009.08371v1)
- **Published**: 2020-09-17 15:37:30+00:00
- **Updated**: 2020-09-17 15:37:30+00:00
- **Authors**: Nils Eckstein, Julia Buhmann, Matthew Cook, Jan Funke
- **Comment**: Accepted at MICCAI 2020
- **Journal**: None
- **Summary**: We present a method for microtubule tracking in electron microscopy volumes. Our method first identifies a sparse set of voxels that likely belong to microtubules. Similar to prior work, we then enumerate potential edges between these voxels, which we represent in a candidate graph. Tracks of microtubules are found by selecting nodes and edges in the candidate graph by solving a constrained optimization problem incorporating biological priors on microtubule structure. For this, we present a novel integer linear programming formulation, which results in speed-ups of three orders of magnitude and an increase of 53% in accuracy compared to prior art (evaluated on three 1.2 x 4 x 4$\mu$m volumes of Drosophila neural tissue). We also propose a scheme to solve the optimization problem in a block-wise fashion, which allows distributed tracking and is necessary to process very large electron microscopy volumes. Finally, we release a benchmark dataset for microtubule tracking, here used for training, testing and validation, consisting of eight 30 x 1000 x 1000 voxel blocks (1.2 x 4 x 4$\mu$m) of densely annotated microtubules in the CREMI data set (https://github.com/nilsec/micron).



### Modeling human visual search: A combined Bayesian searcher and saliency map approach for eye movement guidance in natural scenes
- **Arxiv ID**: http://arxiv.org/abs/2009.08373v2
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/2009.08373v2)
- **Published**: 2020-09-17 15:38:23+00:00
- **Updated**: 2020-12-08 04:02:44+00:00
- **Authors**: M. Sclar, G. Bujia, S. Vita, G. Solovey, J. E. Kamienkowski
- **Comment**: None
- **Journal**: None
- **Summary**: Finding objects is essential for almost any daily-life visual task. Saliency models have been useful to predict fixation locations in natural images, but are static, i.e., they provide no information about the time-sequence of fixations. Nowadays, one of the biggest challenges in the field is to go beyond saliency maps to predict a sequence of fixations related to a visual task, such as searching for a given target. Bayesian observer models have been proposed for this task, as they represent visual search as an active sampling process. Nevertheless, they were mostly evaluated on artificial images, and how they adapt to natural images remains largely unexplored.   Here, we propose a unified Bayesian model for visual search guided by saliency maps as prior information. We validated our model with a visual search experiment in natural scenes recording eye movements. We show that, although state-of-the-art saliency models perform well in predicting the first two fixations in a visual search task, their performance degrades to chance afterward. This suggests that saliency maps alone are good to model bottom-up first impressions, but are not enough to explain the scanpaths when top-down task information is critical. Thus, we propose to use them as priors of Bayesian searchers. This approach leads to a behavior very similar to humans for the whole scanpath, both in the percentage of target found as a function of the fixation rank and the scanpath similarity, reproducing the entire sequence of eye movements.



### A Multimodal Memes Classification: A Survey and Open Research Issues
- **Arxiv ID**: http://arxiv.org/abs/2009.08395v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2009.08395v1)
- **Published**: 2020-09-17 16:13:21+00:00
- **Updated**: 2020-09-17 16:13:21+00:00
- **Authors**: Tariq Habib Afridi, Aftab Alam, Muhammad Numan Khan, Jawad Khan, Young-Koo Lee
- **Comment**: This is a survey paper on recent state of the art VL models that can
  be used for memes classification. it has 15 pages and 2 figures
- **Journal**: None
- **Summary**: Memes are graphics and text overlapped so that together they present concepts that become dubious if one of them is absent. It is spread mostly on social media platforms, in the form of jokes, sarcasm, motivating, etc. After the success of BERT in Natural Language Processing (NLP), researchers inclined to Visual-Linguistic (VL) multimodal problems like memes classification, image captioning, Visual Question Answering (VQA), and many more. Unfortunately, many memes get uploaded each day on social media platforms that need automatic censoring to curb misinformation and hate. Recently, this issue has attracted the attention of researchers and practitioners. State-of-the-art methods that performed significantly on other VL dataset, tends to fail on memes classification. In this context, this work aims to conduct a comprehensive study on memes classification, generally on the VL multimodal problems and cutting edge solutions. We propose a generalized framework for VL problems. We cover the early and next-generation works on VL problems. Finally, we identify and articulate several open research issues and challenges. This is the first study that presents the generalized view of the advanced classification techniques concerning memes classification to the best of our knowledge. We believe this study presents a clear road-map for the Machine Learning (ML) research community to implement and enhance memes classification techniques.



### Population Mapping in Informal Settlements with High-Resolution Satellite Imagery and Equitable Ground-Truth
- **Arxiv ID**: http://arxiv.org/abs/2009.08410v1
- **DOI**: None
- **Categories**: **cs.CY**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.08410v1)
- **Published**: 2020-09-17 16:37:32+00:00
- **Updated**: 2020-09-17 16:37:32+00:00
- **Authors**: Konstantin Klemmer, Godwin Yeboah, João Porto de Albuquerque, Stephen A Jarvis
- **Comment**: ML-IRL workshop at ICLR 2020
- **Journal**: None
- **Summary**: We propose a generalizable framework for the population estimation of dense, informal settlements in low-income urban areas--so called 'slums'--using high-resolution satellite imagery. Precise population estimates are a crucial factor for efficient resource allocations by government authorities and NGO's, for instance in medical emergencies. We utilize equitable ground-truth data, which is gathered in collaboration with local communities: Through training and community mapping, the local population contributes their unique domain knowledge, while also maintaining agency over their data. This practice allows us to avoid carrying forward potential biases into the modeling pipeline, which might arise from a less rigorous ground-truthing approach. We contextualize our approach in respect to the ongoing discussion within the machine learning community, aiming to make real-world machine learning applications more inclusive, fair and accountable. Because of the resource intensive ground-truth generation process, our training data is limited. We propose a gridded population estimation model, enabling flexible and customizable spatial resolutions. We test our pipeline on three experimental site in Nigeria, utilizing pre-trained and fine-tune vision networks to overcome data sparsity. Our findings highlight the difficulties of transferring common benchmark models to real-world tasks. We discuss this and propose steps forward.



### Discovering Dynamic Salient Regions for Spatio-Temporal Graph Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2009.08427v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.08427v3)
- **Published**: 2020-09-17 17:23:38+00:00
- **Updated**: 2021-12-07 12:41:45+00:00
- **Authors**: Iulia Duta, Andrei Nicolicioiu, Marius Leordeanu
- **Comment**: Accepted at Neural Information Processing Systems (NeurIPS 2021)
- **Journal**: None
- **Summary**: Graph Neural Networks are perfectly suited to capture latent interactions between various entities in the spatio-temporal domain (e.g. videos). However, when an explicit structure is not available, it is not obvious what atomic elements should be represented as nodes. Current works generally use pre-trained object detectors or fixed, predefined regions to extract graph nodes. Improving upon this, our proposed model learns nodes that dynamically attach to well-delimited salient regions, which are relevant for a higher-level task, without using any object-level supervision. Constructing these localized, adaptive nodes gives our model inductive bias towards object-centric representations and we show that it discovers regions that are well correlated with objects in the video. In extensive ablation studies and experiments on two challenging datasets, we show superior performance to previous graph neural networks models for video classification.



### Radar-Camera Sensor Fusion for Joint Object Detection and Distance Estimation in Autonomous Vehicles
- **Arxiv ID**: http://arxiv.org/abs/2009.08428v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.08428v1)
- **Published**: 2020-09-17 17:23:40+00:00
- **Updated**: 2020-09-17 17:23:40+00:00
- **Authors**: Ramin Nabati, Hairong Qi
- **Comment**: 12th Workshop on Planning, Perception and Navigation for Intelligent
  Vehicles, IROS 2020
- **Journal**: None
- **Summary**: In this paper we present a novel radar-camera sensor fusion framework for accurate object detection and distance estimation in autonomous driving scenarios. The proposed architecture uses a middle-fusion approach to fuse the radar point clouds and RGB images. Our radar object proposal network uses radar point clouds to generate 3D proposals from a set of 3D prior boxes. These proposals are mapped to the image and fed into a Radar Proposal Refinement (RPR) network for objectness score prediction and box refinement. The RPR network utilizes both radar information and image feature maps to generate accurate object proposals and distance estimations. The radar-based proposals are combined with image-based proposals generated by a modified Region Proposal Network (RPN). The RPN has a distance regression layer for estimating distance for every generated proposal. The radar-based and image-based proposals are merged and used in the next stage for object classification. Experiments on the challenging nuScenes dataset show our method outperforms other existing radar-camera fusion methods in the 2D object detection task while at the same time accurately estimates objects' distances.



### Large Norms of CNN Layers Do Not Hurt Adversarial Robustness
- **Arxiv ID**: http://arxiv.org/abs/2009.08435v6
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2009.08435v6)
- **Published**: 2020-09-17 17:33:50+00:00
- **Updated**: 2021-08-15 10:31:17+00:00
- **Authors**: Youwei Liang, Dong Huang
- **Comment**: AAAI 2021, including Appendix, 15 pages, 4 figures
- **Journal**: None
- **Summary**: Since the Lipschitz properties of convolutional neural networks (CNNs) are widely considered to be related to adversarial robustness, we theoretically characterize the $\ell_1$ norm and $\ell_\infty$ norm of 2D multi-channel convolutional layers and provide efficient methods to compute the exact $\ell_1$ norm and $\ell_\infty$ norm. Based on our theorem, we propose a novel regularization method termed norm decay, which can effectively reduce the norms of convolutional layers and fully-connected layers. Experiments show that norm-regularization methods, including norm decay, weight decay, and singular value clipping, can improve generalization of CNNs. However, they can slightly hurt adversarial robustness. Observing this unexpected phenomenon, we compute the norms of layers in the CNNs trained with three different adversarial training frameworks and surprisingly find that adversarially robust CNNs have comparable or even larger layer norms than their non-adversarially robust counterparts. Furthermore, we prove that under a mild assumption, adversarially robust classifiers can be achieved using neural networks, and an adversarially robust neural network can have an arbitrarily large Lipschitz constant. For this reason, enforcing small norms on CNN layers may be neither necessary nor effective in achieving adversarial robustness. The code is available at https://github.com/youweiliang/norm_robustness.



### Tropical time series, iterated-sums signatures and quasisymmetric functions
- **Arxiv ID**: http://arxiv.org/abs/2009.08443v3
- **DOI**: 10.1137/20M138004
- **Categories**: **math.RA**, cs.CV, cs.LG, 60L10, 60L70, 16Y60, 93C55
- **Links**: [PDF](http://arxiv.org/pdf/2009.08443v3)
- **Published**: 2020-09-17 17:51:43+00:00
- **Updated**: 2022-04-02 14:53:19+00:00
- **Authors**: Joscha Diehl, Kurusch Ebrahimi-Fard, Nikolas Tapia
- **Comment**: fix notational errors, clarify certain proofs
- **Journal**: SIAM Journal on Applied Algebra and Geometry, Vol 6, Issue 4,
  (2022), 563-599
- **Summary**: Aiming for a systematic feature-extraction from time series, we introduce the iterated-sums signature over arbitrary commutative semirings. The case of the tropical semiring is a central, and our motivating example. It leads to features of (real-valued) time series that are not easily available using existing signature-type objects. We demonstrate how the signature extracts chronological aspects of a time series, and that its calculation is possible in linear time. We identify quasisymmetric expressions over semirings as the appropriate framework for iterated-sums signatures over semiring-valued time series.



### MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks
- **Arxiv ID**: http://arxiv.org/abs/2009.08453v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2009.08453v2)
- **Published**: 2020-09-17 17:59:33+00:00
- **Updated**: 2021-03-19 17:40:19+00:00
- **Authors**: Zhiqiang Shen, Marios Savvides
- **Comment**: 12 pages. Code and trained models are available at:
  https://github.com/szq0214/MEAL-V2
- **Journal**: None
- **Summary**: We introduce a simple yet effective distillation framework that is able to boost the vanilla ResNet-50 to 80%+ Top-1 accuracy on ImageNet without tricks. We construct such a framework through analyzing the problems in the existing classification system and simplify the base method ensemble knowledge distillation via discriminators by: (1) adopting the similarity loss and discriminator only on the final outputs and (2) using the average of softmax probabilities from all teacher ensembles as the stronger supervision. Intriguingly, three novel perspectives are presented for distillation: (1) weight decay can be weakened or even completely removed since the soft label also has a regularization effect; (2) using a good initialization for students is critical; and (3) one-hot/hard label is not necessary in the distillation process if the weights are well initialized. We show that such a straight-forward framework can achieve state-of-the-art results without involving any commonly-used techniques, such as architecture modification; outside training data beyond ImageNet; autoaug/randaug; cosine learning rate; mixup/cutmix training; label smoothing; etc. Our method obtains 80.67% top-1 accuracy on ImageNet using a single crop-size of 224x224 with vanilla ResNet-50, outperforming the previous state-of-the-arts by a significant margin under the same network structure. Our result can be regarded as a strong baseline using knowledge distillation, and to our best knowledge, this is also the first method that is able to boost vanilla ResNet-50 to surpass 80% on ImageNet without architecture modification or additional training data. On smaller ResNet-18, our distillation framework consistently improves from 69.76% to 73.19%, which shows tremendous practical values in real-world applications. Our code and models are available at: https://github.com/szq0214/MEAL-V2.



### The Next Big Thing(s) in Unsupervised Machine Learning: Five Lessons from Infant Learning
- **Arxiv ID**: http://arxiv.org/abs/2009.08497v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2009.08497v1)
- **Published**: 2020-09-17 18:47:06+00:00
- **Updated**: 2020-09-17 18:47:06+00:00
- **Authors**: Lorijn Zaadnoordijk, Tarek R. Besold, Rhodri Cusack
- **Comment**: None
- **Journal**: None
- **Summary**: After a surge in popularity of supervised Deep Learning, the desire to reduce the dependence on curated, labelled data sets and to leverage the vast quantities of unlabelled data available recently triggered renewed interest in unsupervised learning algorithms. Despite a significantly improved performance due to approaches such as the identification of disentangled latent representations, contrastive learning, and clustering optimisations, the performance of unsupervised machine learning still falls short of its hypothesised potential. Machine learning has previously taken inspiration from neuroscience and cognitive science with great success. However, this has mostly been based on adult learners with access to labels and a vast amount of prior knowledge. In order to push unsupervised machine learning forward, we argue that developmental science of infant cognition might hold the key to unlocking the next generation of unsupervised learning approaches. Conceptually, human infant learning is the closest biological parallel to artificial unsupervised learning, as infants too must learn useful representations from unlabelled data. In contrast to machine learning, these new representations are learned rapidly and from relatively few examples. Moreover, infants learn robust representations that can be used flexibly and efficiently in a number of different tasks and contexts. We identify five crucial factors enabling infants' quality and speed of learning, assess the extent to which these have already been exploited in machine learning, and propose how further adoption of these factors can give rise to previously unseen performance levels in unsupervised learning.



### Smartphone Camera De-identification while Preserving Biometric Utility
- **Arxiv ID**: http://arxiv.org/abs/2009.08511v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2009.08511v1)
- **Published**: 2020-09-17 19:48:43+00:00
- **Updated**: 2020-09-17 19:48:43+00:00
- **Authors**: Sudipta Banerjee, Arun Ross
- **Comment**: None
- **Journal**: Proc. of 10th IEEE International Conference on Biometrics: Theory,
  Applications and Systems (BTAS), (Tampa, USA), September 2019
- **Summary**: The principle of Photo Response Non Uniformity (PRNU) is often exploited to deduce the identity of the smartphone device whose camera or sensor was used to acquire a certain image. In this work, we design an algorithm that perturbs a face image acquired using a smartphone camera such that (a) sensor-specific details pertaining to the smartphone camera are suppressed (sensor anonymization); (b) the sensor pattern of a different device is incorporated (sensor spoofing); and (c) biometric matching using the perturbed image is not affected (biometric utility). We employ a simple approach utilizing Discrete Cosine Transform to achieve the aforementioned objectives. Experiments conducted on the MICHE-I and OULU-NPU datasets, which contain periocular and facial data acquired using 12 smartphone cameras, demonstrate the efficacy of the proposed de-identification algorithm on three different PRNU-based sensor identification schemes. This work has application in sensor forensics and personal privacy.



### Objective, Probabilistic, and Generalized Noise Level Dependent Classifications of sets of more or less 2D Periodic Images into Plane Symmetry Groups
- **Arxiv ID**: http://arxiv.org/abs/2009.08539v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2009.08539v2)
- **Published**: 2020-09-17 21:28:49+00:00
- **Updated**: 2020-12-15 02:54:51+00:00
- **Authors**: Andrew Dempsey, Peter Moeck
- **Comment**: 74 pages, 12 figures, 56 tables
- **Journal**: None
- **Summary**: Crystallographic symmetry classifications from real-world images with periodicities in two dimensions (2D) are of interest to crystallographers and practitioners of computer vision studies alike. Currently, these classifications are typically made by both communities in a subjective manner that relies on arbitrary thresholds for judgments, and are reported under the pretense of being definitive, which is impossible. Moreover, the computer vision community tends to use direct space methods to make such classifications instead of more powerful and computationally efficient Fourier space methods. This is because the proper functioning of those methods requires more periodic repeats of a unit cell motif than are commonly present in images analyzed by the computer vision community. We demonstrate a novel approach to plane symmetry group classifications that is enabled by Kenichi Kanatani's Geometric Akaike Information Criterion and associated Geometric Akaike weights. Our approach leverages the advantages of working in Fourier space, is well suited for handling the hierarchic nature of crystallographic symmetries, and yields probabilistic results that are generalized noise level dependent. The latter feature means crystallographic symmetry classifications can be updated when less noisy image data and more accurate processing algorithms become available. We demonstrate the ability of our approach to objectively estimate the plane symmetry and pseudosymmetries of sets of synthetic 2D-periodic images with varying amounts of red-green-blue and spread noise. Additionally, we suggest a simple solution to the problem of too few periodic repeats in an input image for practical application of Fourier space methods. In doing so, we effectively solve the decades-old and heretofore intractable problem from computer vision of symmetry detection and classification from images in the presence of noise.



### On the Effectiveness of Weight-Encoded Neural Implicit 3D Shapes
- **Arxiv ID**: http://arxiv.org/abs/2009.09808v3
- **DOI**: None
- **Categories**: **cs.GR**, cs.CG, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2009.09808v3)
- **Published**: 2020-09-17 23:10:19+00:00
- **Updated**: 2021-01-17 21:27:01+00:00
- **Authors**: Thomas Davies, Derek Nowrouzezahrai, Alec Jacobson
- **Comment**: None
- **Journal**: None
- **Summary**: A neural implicit outputs a number indicating whether the given query point in space is inside, outside, or on a surface. Many prior works have focused on _latent-encoded_ neural implicits, where a latent vector encoding of a specific shape is also fed as input. While affording latent-space interpolation, this comes at the cost of reconstruction accuracy for any _single_ shape. Training a specific network for each 3D shape, a _weight-encoded_ neural implicit may forgo the latent vector and focus reconstruction accuracy on the details of a single shape. While previously considered as an intermediary representation for 3D scanning tasks or as a toy-problem leading up to latent-encoding tasks, weight-encoded neural implicits have not yet been taken seriously as a 3D shape representation. In this paper, we establish that weight-encoded neural implicits meet the criteria of a first-class 3D shape representation. We introduce a suite of technical contributions to improve reconstruction accuracy, convergence, and robustness when learning the signed distance field induced by a polygonal mesh -- the _de facto_ standard representation. Viewed as a lossy compression, our conversion outperforms standard techniques from geometry processing. Compared to previous latent- and weight-encoded neural implicits we demonstrate superior robustness, scalability, and performance.



