# Arxiv Papers in cs.CV on 2020-11-29
### Artificial Intelligence applied to chest X-Ray images for the automatic detection of COVID-19. A thoughtful evaluation approach
- **Arxiv ID**: http://arxiv.org/abs/2011.14259v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.14259v1)
- **Published**: 2020-11-29 02:48:39+00:00
- **Updated**: 2020-11-29 02:48:39+00:00
- **Authors**: Julian D. Arias-Londoño, Jorge A. Gomez-Garcia, Laureano Moro-Velazquez, Juan I. Godino-Llorente
- **Comment**: None
- **Journal**: None
- **Summary**: Current standard protocols used in the clinic for diagnosing COVID-19 include molecular or antigen tests, generally complemented by a plain chest X-Ray. The combined analysis aims to reduce the significant number of false negatives of these tests, but also to provide complementary evidence about the presence and severity of the disease. However, the procedure is not free of errors, and the interpretation of the chest X-Ray is only restricted to radiologists due to its complexity. With the long term goal to provide new evidence for the diagnosis, this paper presents an evaluation of different methods based on a deep neural network. These are the first steps to develop an automatic COVID-19 diagnosis tool using chest X-Ray images, that would additionally differentiate between controls, pneumonia or COVID-19 groups. The paper describes the process followed to train a Convolutional Neural Network with a dataset of more than 79,500 X-Ray images compiled from different sources, including more than 8,500 COVID-19 examples. For the sake of evaluation and comparison of the models developed, three different experiments were carried out following three preprocessing schemes. The aim is to evaluate how preprocessing the data affects the results and improves its explainability. Likewise, a critical analysis is carried out about different variability issues that might compromise the system and the effects on the performance. With the employed methodology, a 91.5% classification accuracy is obtained, with a 87.4% average recall for the worst but most explainable experiment, which requires a previous automatic segmentation of the lungs region.



### Fully Quantized Image Super-Resolution Networks
- **Arxiv ID**: http://arxiv.org/abs/2011.14265v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.14265v2)
- **Published**: 2020-11-29 03:53:49+00:00
- **Updated**: 2021-04-19 03:38:50+00:00
- **Authors**: Hu Wang, Peng Chen, Bohan Zhuang, Chunhua Shen
- **Comment**: Results updated
- **Journal**: None
- **Summary**: With the rising popularity of intelligent mobile devices, it is of great practical significance to develop accurate, realtime and energy-efficient image Super-Resolution (SR) inference methods. A prevailing method for improving the inference efficiency is model quantization, which allows for replacing the expensive floating-point operations with efficient fixed-point or bitwise arithmetic. To date, it is still challenging for quantized SR frameworks to deliver feasible accuracy-efficiency trade-off. Here, we propose a Fully Quantized image Super-Resolution framework (FQSR) to jointly optimize efficiency and accuracy. In particular, we target on obtaining end-to-end quantized models for all layers, especially including skip connections, which was rarely addressed in the literature. We further identify training obstacles faced by low-bit SR networks and propose two novel methods accordingly. The two difficulites are caused by 1) activation and weight distributions being vastly distinctive in different layers; 2) the inaccurate approximation of the quantization. We apply our quantization scheme on multiple mainstream super-resolution architectures, including SRResNet, SRGAN and EDSR. Experimental results show that our FQSR using low bits quantization can achieve on par performance compared with the full-precision counterparts on five benchmark datasets and surpass state-of-the-art quantized SR methods with significantly reduced computational cost and memory consumption.



### Multi-task GANs for Semantic Segmentation and Depth Completion with Cycle Consistency
- **Arxiv ID**: http://arxiv.org/abs/2011.14272v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.14272v1)
- **Published**: 2020-11-29 04:12:16+00:00
- **Updated**: 2020-11-29 04:12:16+00:00
- **Authors**: Chongzhen Zhang, Yang Tang, Chaoqiang Zhao, Qiyu Sun, Zhencheng Ye, Jürgen Kurths
- **Comment**: None
- **Journal**: None
- **Summary**: Semantic segmentation and depth completion are two challenging tasks in scene understanding, and they are widely used in robotics and autonomous driving. Although several works are proposed to jointly train these two tasks using some small modifications, like changing the last layer, the result of one task is not utilized to improve the performance of the other one despite that there are some similarities between these two tasks. In this paper, we propose multi-task generative adversarial networks (Multi-task GANs), which are not only competent in semantic segmentation and depth completion, but also improve the accuracy of depth completion through generated semantic images. In addition, we improve the details of generated semantic images based on CycleGAN by introducing multi-scale spatial pooling blocks and the structural similarity reconstruction loss. Furthermore, considering the inner consistency between semantic and geometric structures, we develop a semantic-guided smoothness loss to improve depth completion results. Extensive experiments on Cityscapes dataset and KITTI depth completion benchmark show that the Multi-task GANs are capable of achieving competitive performance for both semantic segmentation and depth completion tasks.



### UVid-Net: Enhanced Semantic Segmentation of UAV Aerial Videos by Embedding Temporal Information
- **Arxiv ID**: http://arxiv.org/abs/2011.14284v2
- **DOI**: 10.1109/JSTARS.2021.3069909
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.14284v2)
- **Published**: 2020-11-29 05:01:39+00:00
- **Updated**: 2021-05-27 13:04:56+00:00
- **Authors**: Girisha S, Ujjwal Verma, Manohara Pai M M, Radhika Pai
- **Comment**: Includes additional discussions/results and comparison with SOTA
  methods. Published in IEEE JSTARS
- **Journal**: Published in IEEE Journal of Selected Topics in Applied Earth
  Observations and Remote Sensing, vol. 14, pp. 4115-4127, 2021
- **Summary**: Semantic segmentation of aerial videos has been extensively used for decision making in monitoring environmental changes, urban planning, and disaster management. The reliability of these decision support systems is dependent on the accuracy of the video semantic segmentation algorithms. The existing CNN based video semantic segmentation methods have enhanced the image semantic segmentation methods by incorporating an additional module such as LSTM or optical flow for computing temporal dynamics of the video which is a computational overhead. The proposed research work modifies the CNN architecture by incorporating temporal information to improve the efficiency of video semantic segmentation.   In this work, an enhanced encoder-decoder based CNN architecture (UVid-Net) is proposed for UAV video semantic segmentation. The encoder of the proposed architecture embeds temporal information for temporally consistent labelling. The decoder is enhanced by introducing the feature-refiner module, which aids in accurate localization of the class labels. The proposed UVid-Net architecture for UAV video semantic segmentation is quantitatively evaluated on extended ManipalUAVid dataset. The performance metric mIoU of 0.79 has been observed which is significantly greater than the other state-of-the-art algorithms. Further, the proposed work produced promising results even for the pre-trained model of UVid-Net on urban street scene with fine tuning the final layer on UAV aerial videos.



### Deeper or Wider Networks of Point Clouds with Self-attention?
- **Arxiv ID**: http://arxiv.org/abs/2011.14285v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2011.14285v2)
- **Published**: 2020-11-29 05:03:06+00:00
- **Updated**: 2021-08-14 12:20:06+00:00
- **Authors**: Haoxi Ran, Li Lu
- **Comment**: The experiments is incompleted
- **Journal**: None
- **Summary**: Prevalence of deeper networks driven by self-attention is in stark contrast to underexplored point-based methods. In this paper, we propose groupwise self-attention as the basic block to construct our network: SepNet. Our proposed module can effectively capture both local and global dependencies. This module computes the features of a group based on the summation of the weighted features of any point within the group. For convenience, we generalize groupwise operations to assemble this module. To further facilitate our networks, we deepen and widen SepNet on the tasks of segmentation and classification respectively, and verify its practicality. Specifically, SepNet achieves state-of-the-art for the tasks of classification and segmentation on most of the datasets. We show empirical evidence that SepNet can obtain extra accuracy in classification or segmentation from increased width or depth, respectively.



### Learning Affinity-Aware Upsampling for Deep Image Matting
- **Arxiv ID**: http://arxiv.org/abs/2011.14288v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.14288v1)
- **Published**: 2020-11-29 05:09:43+00:00
- **Updated**: 2020-11-29 05:09:43+00:00
- **Authors**: Yutong Dai, Hao Lu, Chunhua Shen
- **Comment**: None
- **Journal**: None
- **Summary**: We show that learning affinity in upsampling provides an effective and efficient approach to exploit pairwise interactions in deep networks. Second-order features are commonly used in dense prediction to build adjacent relations with a learnable module after upsampling such as non-local blocks. Since upsampling is essential, learning affinity in upsampling can avoid additional propagation layers, offering the potential for building compact models. By looking at existing upsampling operators from a unified mathematical perspective, we generalize them into a second-order form and introduce Affinity-Aware Upsampling (A2U) where upsampling kernels are generated using a light-weight lowrank bilinear model and are conditioned on second-order features. Our upsampling operator can also be extended to downsampling. We discuss alternative implementations of A2U and verify their effectiveness on two detail-sensitive tasks: image reconstruction on a toy dataset; and a largescale image matting task where affinity-based ideas constitute mainstream matting approaches. In particular, results on the Composition-1k matting dataset show that A2U achieves a 14% relative improvement in the SAD metric against a strong baseline with negligible increase of parameters (<0.5%). Compared with the state-of-the-art matting network, we achieve 8% higher performance with only 40% model complexity.



### Learning geometry-image representation for 3D point cloud generation
- **Arxiv ID**: http://arxiv.org/abs/2011.14289v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.14289v1)
- **Published**: 2020-11-29 05:21:10+00:00
- **Updated**: 2020-11-29 05:21:10+00:00
- **Authors**: Lei Wang, Yuchun Huang, Pengjie Tao, Yaolin Hou, Yuxuan Liu
- **Comment**: None
- **Journal**: None
- **Summary**: We study the problem of generating point clouds of 3D objects. Instead of discretizing the object into 3D voxels with huge computational cost and resolution limitations, we propose a novel geometry image based generator (GIG) to convert the 3D point cloud generation problem to a 2D geometry image generation problem. Since the geometry image is a completely regular 2D array that contains the surface points of the 3D object, it leverages both the regularity of the 2D array and the geodesic neighborhood of the 3D surface. Thus, one significant benefit of our GIG is that it allows us to directly generate the 3D point clouds using efficient 2D image generation networks. Experiments on both rigid and non-rigid 3D object datasets have demonstrated the promising performance of our method to not only create plausible and novel 3D objects, but also learn a probabilistic latent space that well supports the shape editing like interpolation and arithmetic.



### A method for large diffeomorphic registration via broken geodesics
- **Arxiv ID**: http://arxiv.org/abs/2011.14298v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.14298v2)
- **Published**: 2020-11-29 06:14:53+00:00
- **Updated**: 2021-01-03 05:49:37+00:00
- **Authors**: Alphin J. Thottupattu, Jayanthi Sivaswamy, Venkateswaran P. Krishnan
- **Comment**: 18 pages and 9 figures
- **Journal**: None
- **Summary**: Anatomical variabilities seen in longitudinal data or inter-subject data is usually described by the underlying deformation, captured by non-rigid registration of these images. Stationary Velocity Field (SVF) based non-rigid registration algorithms are widely used for registration. SVF based methods form a metric-free framework which captures a finite dimensional submanifold of deformations embedded in the infinite dimensional smooth manifold of diffeomorphisms. However, these methods cover only a limited degree of deformations. In this paper, we address this limitation and define an approximate metric space for the manifold of diffeomorphisms $\mathcal{G}$. We propose a method to break down the large deformation into finite compositions of small deformations. This results in a broken geodesic path on $\mathcal{G}$ and its length now forms an approximate registration metric. We illustrate the method using a simple, intensity-based, log-demon implementation. Validation results of the proposed method show that it can capture large and complex deformations while producing qualitatively better results than the state-of-the-art methods. The results also demonstrate that the proposed registration metric is a good indicator of the degree of deformation.



### Automated Prostate Cancer Diagnosis Based on Gleason Grading Using Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2011.14301v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2011.14301v1)
- **Published**: 2020-11-29 06:42:08+00:00
- **Updated**: 2020-11-29 06:42:08+00:00
- **Authors**: Haotian Xie, Yong Zhang, Jun Wang, Jingjing Zhang, Yifan Ma, Zhaogang Yang
- **Comment**: This article has been removed by arXiv administrators because the
  submitter did not have the authority to grant the license applied at the time
  of submission
- **Journal**: None
- **Summary**: The Gleason grading system using histological images is the most powerful diagnostic and prognostic predictor of prostate cancer. The current standard inspection is evaluating Gleason H&E-stained histopathology images by pathologists. However, it is complicated, time-consuming, and subject to observers. Deep learning (DL) based-methods that automatically learn image features and achieve higher generalization ability have attracted significant attention. However, challenges remain especially using DL to train the whole slide image (WSI), a predominant clinical source in the current diagnostic setting, containing billions of pixels, morphological heterogeneity, and artifacts. Hence, we proposed a convolutional neural network (CNN)-based automatic classification method for accurate grading of PCa using whole slide histopathology images. In this paper, a data augmentation method named Patch-Based Image Reconstruction (PBIR) was proposed to reduce the high resolution and increase the diversity of WSIs. In addition, a distribution correction (DC) module was developed to enhance the adaption of pretrained model to the target dataset by adjusting the data distribution. Besides, a Quadratic Weighted Mean Square Error (QWMSE) function was presented to reduce the misdiagnosis caused by equal Euclidean distances. Our experiments indicated the combination of PBIR, DC, and QWMSE function was necessary for achieving superior expert-level performance, leading to the best results (0.8885 quadratic-weighted kappa coefficient).



### Multi-stage Attention ResU-Net for Semantic Segmentation of Fine-Resolution Remote Sensing Images
- **Arxiv ID**: http://arxiv.org/abs/2011.14302v2
- **DOI**: 10.1109/LGRS.2021.3063381
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.14302v2)
- **Published**: 2020-11-29 07:24:21+00:00
- **Updated**: 2020-12-01 06:25:01+00:00
- **Authors**: Rui Li, Shunyi Zheng, Chenxi Duan, Jianlin Su, Ce Zhang
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2007.14902,
  arXiv:2009.02130
- **Journal**: None
- **Summary**: The attention mechanism can refine the extracted feature maps and boost the classification performance of the deep network, which has become an essential technique in computer vision and natural language processing. However, the memory and computational costs of the dot-product attention mechanism increase quadratically with the spatio-temporal size of the input. Such growth hinders the usage of attention mechanisms considerably in application scenarios with large-scale inputs. In this Letter, we propose a Linear Attention Mechanism (LAM) to address this issue, which is approximately equivalent to dot-product attention with computational efficiency. Such a design makes the incorporation between attention mechanisms and deep networks much more flexible and versatile. Based on the proposed LAM, we re-factor the skip connections in the raw U-Net and design a Multi-stage Attention ResU-Net (MAResU-Net) for semantic segmentation from fine-resolution remote sensing images. Experiments conducted on the Vaihingen dataset demonstrated the effectiveness and efficiency of our MAResU-Net. Open-source code is available at https://github.com/lironui/Multistage-Attention-ResU-Net.



### Image-based Plant Disease Diagnosis with Unsupervised Anomaly Detection Based on Reconstructability of Colors
- **Arxiv ID**: http://arxiv.org/abs/2011.14306v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.14306v5)
- **Published**: 2020-11-29 07:44:05+00:00
- **Updated**: 2021-09-08 14:45:38+00:00
- **Authors**: Ryoya Katafuchi, Terumasa Tokunaga
- **Comment**: This paper is accepted by IMPROVE 2021. 14 pages, 7 figures, 2 tables
- **Journal**: None
- **Summary**: This paper proposes an unsupervised anomaly detection technique for image-based plant disease diagnosis. The construction of large and publicly available datasets containing labeled images of healthy and diseased crop plants led to growing interest in computer vision techniques for automatic plant disease diagnosis. Although supervised image classifiers based on deep learning can be a powerful tool for plant disease diagnosis, they require a huge amount of labeled data. The data mining technique of anomaly detection includes unsupervised approaches that do not require rare samples for training classifiers. We propose an unsupervised anomaly detection technique for image-based plant disease diagnosis that is based on the reconstructability of colors; a deep encoder-decoder network trained to reconstruct the colors of \textit{healthy} plant images should fail to reconstruct colors of symptomatic regions. Our proposed method includes a new image-based framework for plant disease detection that utilizes a conditional adversarial network called pix2pix and a new anomaly score based on CIEDE2000 color difference. Experiments with PlantVillage dataset demonstrated the superiority of our proposed method compared to an existing anomaly detector at identifying diseased crop images in terms of accuracy, interpretability and computational efficiency.



### BSNet: Bi-Similarity Network for Few-shot Fine-grained Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2011.14311v1
- **DOI**: 10.1109/TIP.2020.3043128
- **Categories**: **cs.CV**, 68U10, I.4
- **Links**: [PDF](http://arxiv.org/pdf/2011.14311v1)
- **Published**: 2020-11-29 08:38:17+00:00
- **Updated**: 2020-11-29 08:38:17+00:00
- **Authors**: Xiaoxu Li, Jijie Wu, Zhuo Sun, Zhanyu Ma, Jie Cao, Jing-Hao Xue
- **Comment**: IEEE TIP 2020
- **Journal**: None
- **Summary**: Few-shot learning for fine-grained image classification has gained recent attention in computer vision. Among the approaches for few-shot learning, due to the simplicity and effectiveness, metric-based methods are favorably state-of-the-art on many tasks. Most of the metric-based methods assume a single similarity measure and thus obtain a single feature space. However, if samples can simultaneously be well classified via two distinct similarity measures, the samples within a class can distribute more compactly in a smaller feature space, producing more discriminative feature maps. Motivated by this, we propose a so-called \textit{Bi-Similarity Network} (\textit{BSNet}) that consists of a single embedding module and a bi-similarity module of two similarity measures. After the support images and the query images pass through the convolution-based embedding module, the bi-similarity module learns feature maps according to two similarity measures of diverse characteristics. In this way, the model is enabled to learn more discriminative and less similarity-biased features from few shots of fine-grained images, such that the model generalization ability can be significantly improved. Through extensive experiments by slightly modifying established metric/similarity based networks, we show that the proposed approach produces a substantial improvement on several fine-grained image benchmark datasets. Codes are available at: https://github.com/spraise/BSNet



### Dank or Not? -- Analyzing and Predicting the Popularity of Memes on Reddit
- **Arxiv ID**: http://arxiv.org/abs/2011.14326v2
- **DOI**: 10.1007/s41109-021-00358-7
- **Categories**: **cs.SI**, cs.CL, cs.CV, cs.CY, physics.soc-ph, J.4; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2011.14326v2)
- **Published**: 2020-11-29 09:57:17+00:00
- **Updated**: 2021-01-22 08:31:42+00:00
- **Authors**: Kate Barnes, Tiernon Riesenmy, Minh Duc Trinh, Eli Lleshi, Nóra Balogh, Roland Molontay
- **Comment**: 23 pages, 12 figures
- **Journal**: None
- **Summary**: Internet memes have become an increasingly pervasive form of contemporary social communication that attracted a lot of research interest recently. In this paper, we analyze the data of 129,326 memes collected from Reddit in the middle of March, 2020, when the most serious coronavirus restrictions were being introduced around the world. This article not only provides a looking glass into the thoughts of Internet users during the COVID-19 pandemic but we also perform a content-based predictive analysis of what makes a meme go viral. Using machine learning methods, we also study what incremental predictive power image related attributes have over textual attributes on meme popularity. We find that the success of a meme can be predicted based on its content alone moderately well, our best performing machine learning model predicts viral memes with AUC=0.68. We also find that both image related and textual attributes have significant incremental predictive power over each other.



### Malaria Detection and Classificaiton
- **Arxiv ID**: http://arxiv.org/abs/2011.14329v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.14329v1)
- **Published**: 2020-11-29 10:04:01+00:00
- **Updated**: 2020-11-29 10:04:01+00:00
- **Authors**: Ruskin Raj Manku, Ayush Sharma, Anand Panchbhai
- **Comment**: None
- **Journal**: None
- **Summary**: Malaria is a disease of global concern according to the World Health Organization. Billions of people in the world are at risk of Malaria today. Microscopy is considered the gold standard for Malaria diagnosis. Microscopic assessment of blood samples requires the need of trained professionals who at times are not available in rural areas where Malaria is a problem. Full automation of Malaria diagnosis is a challenging task. In this work, we put forward a framework for diagnosis of malaria. We adopt a two layer approach, where we detect infected cells using a Faster-RCNN in the first layer, crop them out, and feed the cropped cells to a seperate neural network for classification. The proposed methodology was tested on an openly available dataset, this will serve as a baseline for the future methods as currently there is no common dataset on which results are reported for Malaria Diagnosis.



### Audio-visual Speech Separation with Adversarially Disentangled Visual Representation
- **Arxiv ID**: http://arxiv.org/abs/2011.14334v1
- **DOI**: None
- **Categories**: **cs.SD**, cs.CV, eess.AS, I.2.m
- **Links**: [PDF](http://arxiv.org/pdf/2011.14334v1)
- **Published**: 2020-11-29 10:48:42+00:00
- **Updated**: 2020-11-29 10:48:42+00:00
- **Authors**: Peng Zhang, Jiaming Xu, Jing shi, Yunzhe Hao, Bo Xu
- **Comment**: 5 pages, 3 figures
- **Journal**: None
- **Summary**: Speech separation aims to separate individual voice from an audio mixture of multiple simultaneous talkers. Although audio-only approaches achieve satisfactory performance, they build on a strategy to handle the predefined conditions, limiting their application in the complex auditory scene. Towards the cocktail party problem, we propose a novel audio-visual speech separation model. In our model, we use the face detector to detect the number of speakers in the scene and use visual information to avoid the permutation problem. To improve our model's generalization ability to unknown speakers, we extract speech-related visual features from visual inputs explicitly by the adversarially disentangled method, and use this feature to assist speech separation. Besides, the time-domain approach is adopted, which could avoid the phase reconstruction problem existing in the time-frequency domain models. To compare our model's performance with other models, we create two benchmark datasets of 2-speaker mixture from GRID and TCDTIMIT audio-visual datasets. Through a series of experiments, our proposed model is shown to outperform the state-of-the-art audio-only model and three audio-visual models.



### Digital rock reconstruction with user-defined properties using conditional generative adversarial networks
- **Arxiv ID**: http://arxiv.org/abs/2012.07719v2
- **DOI**: 10.1007/s11242-021-01728-6
- **Categories**: **cs.CV**, cs.AI, cs.LG, physics.data-an
- **Links**: [PDF](http://arxiv.org/pdf/2012.07719v2)
- **Published**: 2020-11-29 10:55:58+00:00
- **Updated**: 2021-06-01 06:32:43+00:00
- **Authors**: Qiang Zheng, Dongxiao Zhang
- **Comment**: 36 pages, 20 figures
- **Journal**: Transport in Porous Media, 2022
- **Summary**: Uncertainty is ubiquitous with flow in subsurface rocks because of their inherent heterogeneity and lack of in-situ measurements. To complete uncertainty analysis in a multi-scale manner, it is a prerequisite to provide sufficient rock samples. Even though the advent of digital rock technology offers opportunities to reproduce rocks, it still cannot be utilized to provide massive samples due to its high cost, thus leading to the development of diversified mathematical methods. Among them, two-point statistics (TPS) and multi-point statistics (MPS) are commonly utilized, which feature incorporating low-order and high-order statistical information, respectively. Recently, generative adversarial networks (GANs) are becoming increasingly popular since they can reproduce training images with excellent visual and consequent geologic realism. However, standard GANs can only incorporate information from data, while leaving no interface for user-defined properties, and thus may limit the representativeness of reconstructed samples. In this study, we propose conditional GANs for digital rock reconstruction, aiming to reproduce samples not only similar to the real training data, but also satisfying user-specified properties. In fact, the proposed framework can realize the targets of MPS and TPS simultaneously by incorporating high-order information directly from rock images with the GANs scheme, while preserving low-order counterparts through conditioning. We conduct three reconstruction experiments, and the results demonstrate that rock type, rock porosity, and correlation length can be successfully conditioned to affect the reconstructed rock images. Furthermore, in contrast to existing GANs, the proposed conditioning enables learning of multiple rock types simultaneously, and thus invisibly saves computational cost.



### ProtoPShare: Prototype Sharing for Interpretable Image Classification and Similarity Discovery
- **Arxiv ID**: http://arxiv.org/abs/2011.14340v1
- **DOI**: 10.1145/3447548.3467245
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.14340v1)
- **Published**: 2020-11-29 11:23:05+00:00
- **Updated**: 2020-11-29 11:23:05+00:00
- **Authors**: Dawid Rymarczyk, Łukasz Struski, Jacek Tabor, Bartosz Zieliński
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we introduce ProtoPShare, a self-explained method that incorporates the paradigm of prototypical parts to explain its predictions. The main novelty of the ProtoPShare is its ability to efficiently share prototypical parts between the classes thanks to our data-dependent merge-pruning. Moreover, the prototypes are more consistent and the model is more robust to image perturbations than the state of the art method ProtoPNet. We verify our findings on two datasets, the CUB-200-2011 and the Stanford Cars.



### Semi-Supervised Learning of Mutually Accelerated MRI Synthesis without Fully-Sampled Ground Truths
- **Arxiv ID**: http://arxiv.org/abs/2011.14347v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.14347v2)
- **Published**: 2020-11-29 11:56:37+00:00
- **Updated**: 2021-05-12 10:51:11+00:00
- **Authors**: Mahmut Yurt, Salman Ul Hassan Dar, Muzaffer Özbey, Berk Tınaz, Kader Karlı Oğuz, Tolga Çukur
- **Comment**: None
- **Journal**: None
- **Summary**: Learning-based synthetic multi-contrast MRI commonly involves deep models trained using high-quality images of source and target contrasts, regardless of whether source and target domain samples are paired or unpaired. This results in undesirable reliance on fully-sampled acquisitions of all MRI contrasts, which might prove impractical due to limitations on scan costs and time. Here, we propose a novel semi-supervised deep generative model that instead learns to recover high-quality target images directly from accelerated acquisitions of source and target contrasts. To achieve this, the proposed model introduces novel multi-coil tensor losses in image, k-space and adversarial domains. These selective losses are based only on acquired k-space samples, and randomized sampling masks are used across subjects to capture relationships among acquired and non-acquired k-space regions. Comprehensive experiments on multi-contrast neuroimaging datasets demonstrate that our semi-supervised approach yields equivalent performance to gold-standard fully-supervised models, while outperforming a cascaded approach that learns to synthesize based on reconstructions of undersampled data. Therefore, the proposed approach holds great promise to improve the feasibility and utility of accelerated MRI acquisitions mutually undersampled across both contrast sets and k-space.



### Layer Pruning via Fusible Residual Convolutional Block for Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2011.14356v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.14356v1)
- **Published**: 2020-11-29 12:51:16+00:00
- **Updated**: 2020-11-29 12:51:16+00:00
- **Authors**: Pengtao Xu, Jian Cao, Fanhua Shang, Wenyu Sun, Pu Li
- **Comment**: None
- **Journal**: None
- **Summary**: In order to deploy deep convolutional neural networks (CNNs) on resource-limited devices, many model pruning methods for filters and weights have been developed, while only a few to layer pruning. However, compared with filter pruning and weight pruning, the compact model obtained by layer pruning has less inference time and run-time memory usage when the same FLOPs and number of parameters are pruned because of less data moving in memory. In this paper, we propose a simple layer pruning method using fusible residual convolutional block (ResConv), which is implemented by inserting shortcut connection with a trainable information control parameter into a single convolutional layer. Using ResConv structures in training can improve network accuracy and train deep plain networks, and adds no additional computation during inference process because ResConv is fused to be an ordinary convolutional layer after training. For layer pruning, we convert convolutional layers of network into ResConv with a layer scaling factor. In the training process, the L1 regularization is adopted to make the scaling factors sparse, so that unimportant layers are automatically identified and then removed, resulting in a model of layer reduction. Our pruning method achieves excellent performance of compression and acceleration over the state-of-the-arts on different datasets, and needs no retraining in the case of low pruning rate. For example, with ResNet-110, we achieve a 65.5%-FLOPs reduction by removing 55.5% of the parameters, with only a small loss of 0.13% in top-1 accuracy on CIFAR-10.



### Exploring Deep 3D Spatial Encodings for Large-Scale 3D Scene Understanding
- **Arxiv ID**: http://arxiv.org/abs/2011.14358v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.14358v1)
- **Published**: 2020-11-29 12:56:19+00:00
- **Updated**: 2020-11-29 12:56:19+00:00
- **Authors**: Saqib Ali Khan, Yilei Shi, Muhammad Shahzad, Xiao Xiang Zhu
- **Comment**: Submitted to IEEE Geoscience and Remote Sensing Letters (GRSL)
  Journal
- **Journal**: None
- **Summary**: Semantic segmentation of raw 3D point clouds is an essential component in 3D scene analysis, but it poses several challenges, primarily due to the non-Euclidean nature of 3D point clouds. Although, several deep learning based approaches have been proposed to address this task, but almost all of them emphasized on using the latent (global) feature representations from traditional convolutional neural networks (CNN), resulting in severe loss of spatial information, thus failing to model the geometry of the underlying 3D objects, that plays an important role in remote sensing 3D scenes. In this letter, we have proposed an alternative approach to overcome the limitations of CNN based approaches by encoding the spatial features of raw 3D point clouds into undirected symmetrical graph models. These encodings are then combined with a high-dimensional feature vector extracted from a traditional CNN into a localized graph convolution operator that outputs the required 3D segmentation map. We have performed experiments on two standard benchmark datasets (including an outdoor aerial remote sensing dataset and an indoor synthetic dataset). The proposed method achieves on par state-of-the-art accuracy with improved training time and model stability thus indicating strong potential for further research towards a generalized state-of-the-art method for 3D scene understanding.



### A smartphone based multi input workflow for non-invasive estimation of haemoglobin levels using machine learning techniques
- **Arxiv ID**: http://arxiv.org/abs/2011.14370v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.HC, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2011.14370v1)
- **Published**: 2020-11-29 13:57:09+00:00
- **Updated**: 2020-11-29 13:57:09+00:00
- **Authors**: Sarah, S. Sidhartha Narayan, Irfaan Arif, Hrithwik Shalu, Juned Kadiwala
- **Comment**: None
- **Journal**: None
- **Summary**: We suggest a low cost, non invasive healthcare system that measures haemoglobin levels in patients and can be used as a preliminary diagnostic test for anaemia. A combination of image processing, machine learning and deep learning techniques are employed to develop predictive models to measure haemoglobin levels. This is achieved through the color analysis of the fingernail beds, palpebral conjunctiva and tongue of the patients. This predictive model is then encapsulated in a healthcare application. This application expedites data collection and facilitates active learning of the model. It also incorporates personalized calibration of the model for each patient, assisting in the continual monitoring of the haemoglobin levels of the patient. Upon validating this framework using data, it can serve as a highly accurate preliminary diagnostic test for anaemia.



### CNN-based Lung CT Registration with Multiple Anatomical Constraints
- **Arxiv ID**: http://arxiv.org/abs/2011.14372v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.14372v2)
- **Published**: 2020-11-29 14:09:31+00:00
- **Updated**: 2021-06-14 10:43:23+00:00
- **Authors**: Alessa Hering, Stephanie Häger, Jan Moltz, Nikolas Lessmann, Stefan Heldmann, Bram van Ginneken
- **Comment**: None
- **Journal**: None
- **Summary**: Deep-learning-based registration methods emerged as a fast alternative to conventional registration methods. However, these methods often still cannot achieve the same performance as conventional registration methods because they are either limited to small deformation or they fail to handle a superposition of large and small deformations without producing implausible deformation fields with foldings inside.   In this paper, we identify important strategies of conventional registration methods for lung registration and successfully developed the deep-learning counterpart. We employ a Gaussian-pyramid-based multilevel framework that can solve the image registration optimization in a coarse-to-fine fashion. Furthermore, we prevent foldings of the deformation field and restrict the determinant of the Jacobian to physiologically meaningful values by combining a volume change penalty with a curvature regularizer in the loss function. Keypoint correspondences are integrated to focus on the alignment of smaller structures.   We perform an extensive evaluation to assess the accuracy, the robustness, the plausibility of the estimated deformation fields, and the transferability of our registration approach. We show that it achieves state-of-the-art results on the COPDGene dataset compared to conventional registration method with much shorter execution time. In our experiments on the DIRLab exhale to inhale lung registration, we demonstrate substantial improvements (TRE below $1.2$ mm) over other deep learning methods. Our algorithm is publicly available at https://grand-challenge.org/algorithms/deep-learning-based-ct-lung-registration/.



### Single Image Super-resolution with a Switch Guided Hybrid Network for Satellite Images
- **Arxiv ID**: http://arxiv.org/abs/2011.14380v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.14380v1)
- **Published**: 2020-11-29 14:47:23+00:00
- **Updated**: 2020-11-29 14:47:23+00:00
- **Authors**: Shreya Roy, Anirban Chakraborty
- **Comment**: None
- **Journal**: None
- **Summary**: The major drawbacks with Satellite Images are low resolution, Low resolution makes it difficult to identify the objects present in Satellite images. We have experimented with several deep models available for Single Image Superresolution on the SpaceNet dataset and have evaluated the performance of each of them on the satellite image data. We will dive into the recent evolution of the deep models in the context of SISR over the past few years and will present a comparative study between these models. The entire Satellite image of an area is divided into equal-sized patches. Each patch will be used independently for training. These patches will differ in nature. Say, for example, the patches over urban areas have non-homogeneous backgrounds because of different types of objects like vehicles, buildings, roads, etc. On the other hand, patches over jungles will be more homogeneous in nature. Hence, different deep models will fit on different kinds of patches. In this study, we will try to explore this further with the help of a Switching Convolution Network. The idea is to train a switch classifier that will automatically classify a patch into one category of models best suited for it.



### Overcoming Measurement Inconsistency in Deep Learning for Linear Inverse Problems: Applications in Medical Imaging
- **Arxiv ID**: http://arxiv.org/abs/2011.14387v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, eess.SP, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/2011.14387v2)
- **Published**: 2020-11-29 15:19:41+00:00
- **Updated**: 2021-05-31 10:28:45+00:00
- **Authors**: Marija Vella, João F. C. Mota
- **Comment**: Accepted for publication at ICASSP 2021
- **Journal**: None
- **Summary**: The remarkable performance of deep neural networks (DNNs) currently makes them the method of choice for solving linear inverse problems. They have been applied to super-resolve and restore images, as well as to reconstruct MR and CT images. In these applications, DNNs invert a forward operator by finding, via training data, a map between the measurements and the input images. It is then expected that the map is still valid for the test data. This framework, however, introduces measurement inconsistency during testing. We show that such inconsistency, which can be critical in domains like medical imaging or defense, is intimately related to the generalization error. We then propose a framework that post-processes the output of DNNs with an optimization algorithm that enforces measurement consistency. Experiments on MR images show that enforcing measurement consistency via our method can lead to large gains in reconstruction performance.



### Overcoming Barriers to Data Sharing with Medical Image Generation: A Comprehensive Evaluation
- **Arxiv ID**: http://arxiv.org/abs/2012.03769v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2012.03769v3)
- **Published**: 2020-11-29 15:41:46+00:00
- **Updated**: 2021-08-16 18:46:55+00:00
- **Authors**: August DuMont Schütte, Jürgen Hetzel, Sergios Gatidis, Tobias Hepp, Benedikt Dietz, Stefan Bauer, Patrick Schwab
- **Comment**: None
- **Journal**: None
- **Summary**: Privacy concerns around sharing personally identifiable information are a major practical barrier to data sharing in medical research. However, in many cases, researchers have no interest in a particular individual's information but rather aim to derive insights at the level of cohorts. Here, we utilize Generative Adversarial Networks (GANs) to create derived medical imaging datasets consisting entirely of synthetic patient data. The synthetic images ideally have, in aggregate, similar statistical properties to those of a source dataset but do not contain sensitive personal information. We assess the quality of synthetic data generated by two GAN models for chest radiographs with 14 different radiology findings and brain computed tomography (CT) scans with six types of intracranial hemorrhages. We measure the synthetic image quality by the performance difference of predictive models trained on either the synthetic or the real dataset. We find that synthetic data performance disproportionately benefits from a reduced number of unique label combinations. Our open-source benchmark also indicates that at low number of samples per class, label overfitting effects start to dominate GAN training. We additionally conducted a reader study in which trained radiologists do not perform better than random on discriminating between synthetic and real medical images for intermediate levels of resolutions. In accordance with our benchmark results, the classification accuracy of radiologists increases at higher spatial resolution levels. Our study offers valuable guidelines and outlines practical conditions under which insights derived from synthetic medical images are similar to those that would have been derived from real imaging data. Our results indicate that synthetic data sharing may be an attractive and privacy-preserving alternative to sharing real patient-level data in the right settings.



### There and Back Again: Learning to Simulate Radar Data for Real-World Applications
- **Arxiv ID**: http://arxiv.org/abs/2011.14389v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2011.14389v1)
- **Published**: 2020-11-29 15:49:23+00:00
- **Updated**: 2020-11-29 15:49:23+00:00
- **Authors**: Rob Weston, Oiwi Parker Jones, Ingmar Posner
- **Comment**: 6 pages + 2 references
- **Journal**: None
- **Summary**: Simulating realistic radar data has the potential to significantly accelerate the development of data-driven approaches to radar processing. However, it is fraught with difficulty due to the notoriously complex image formation process. Here we propose to learn a radar sensor model capable of synthesising faithful radar observations based on simulated elevation maps. In particular, we adopt an adversarial approach to learning a forward sensor model from unaligned radar examples. In addition, modelling the backward model encourages the output to remain aligned to the world state through a cyclical consistency criterion. The backward model is further constrained to predict elevation maps from real radar data that are grounded by partial measurements obtained from corresponding lidar scans. Both models are trained in a joint optimisation. We demonstrate the efficacy of our approach by evaluating a down-stream segmentation model trained purely on simulated data in a real-world deployment. This achieves performance within four percentage points of the same model trained entirely on real data.



### RGBD-Net: Predicting color and depth images for novel views synthesis
- **Arxiv ID**: http://arxiv.org/abs/2011.14398v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2011.14398v2)
- **Published**: 2020-11-29 16:42:53+00:00
- **Updated**: 2021-07-09 15:06:24+00:00
- **Authors**: Phong Nguyen-Ha, Animesh Karnewar, Lam Huynh, Esa Rahtu, Jiri Matas, Janne Heikkila
- **Comment**: 19 pages, 15 figures. Code will be available at:
  https://github.com/phongnhhn92/RGBDNet
- **Journal**: None
- **Summary**: We propose a new cascaded architecture for novel view synthesis, called RGBD-Net, which consists of two core components: a hierarchical depth regression network and a depth-aware generator network. The former one predicts depth maps of the target views by using adaptive depth scaling, while the latter one leverages the predicted depths and renders spatially and temporally consistent target images. In the experimental evaluation on standard datasets, RGBD-Net not only outperforms the state-of-the-art by a clear margin, but it also generalizes well to new scenes without per-scene optimization. Moreover, we show that RGBD-Net can be optionally trained without depth supervision while still retaining high-quality rendering. Thanks to the depth regression network, RGBD-Net can be also used for creating dense 3D point clouds that are more accurate than those produced by some state-of-the-art multi-view stereo methods.



### Reconfigurable Cyber-Physical System for Critical Infrastructure Protection in Smart Cities via Smart Video-Surveillance
- **Arxiv ID**: http://arxiv.org/abs/2011.14416v1
- **DOI**: 10.1016/j.patrec.2020.11.004
- **Categories**: **cs.CV**, cs.DC
- **Links**: [PDF](http://arxiv.org/pdf/2011.14416v1)
- **Published**: 2020-11-29 18:43:25+00:00
- **Updated**: 2020-11-29 18:43:25+00:00
- **Authors**: Juan Isern, Francisco Barranco, Daniel Deniz, Juho Lesonen, Jari Hannuksela, Richard R. Carrillo
- **Comment**: 13 pages, 8 figures and 5 tables
- **Journal**: Pattern Recognition Letters Volume 140, December 2020, Pages
  303-309
- **Summary**: Automated surveillance is essential for the protection of Critical Infrastructures (CIs) in future Smart Cities. The dynamic environments and bandwidth requirements demand systems that adapt themselves to react when events of interest occur. We present a reconfigurable Cyber Physical System for the protection of CIs using distributed cloud-edge smart video surveillance. Our local edge nodes perform people detection via Deep Learning. Processing is embedded in high performance SoCs (System-on-Chip) achieving real-time performance ($\approx$ 100 fps - frames per second) which enables efficiently managing video streams of more cameras source at lower frame rate. Cloud server gathers results from nodes to carry out biometric facial identification, tracking, and perimeter monitoring. A Quality and Resource Management module monitors data bandwidth and triggers reconfiguration adapting the transmitted video resolution. This also enables a flexible use of the network by multiple cameras while maintaining the accuracy of biometric identification. A real-world example shows a reduction of $\approx$ 75\% bandwidth use with respect to the no-reconfiguration scenario.



### LABNet: Local Graph Aggregation Network with Class Balanced Loss for Vehicle Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2011.14417v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.14417v2)
- **Published**: 2020-11-29 18:43:30+00:00
- **Updated**: 2021-01-31 04:47:03+00:00
- **Authors**: Abu Md Niamul Taufique, Andreas Savakis
- **Comment**: 12 pages, 4 figures
- **Journal**: None
- **Summary**: Vehicle re-identification is an important computer vision task where the objective is to identify a specific vehicle among a set of vehicles seen at various viewpoints. Recent methods based on deep learning utilize a global average pooling layer after the backbone feature extractor, however, this ignores any spatial reasoning on the feature map. In this paper, we propose local graph aggregation on the backbone feature map, to learn associations of local information and hence improve feature learning as well as reduce the effects of partial occlusion and background clutter. Our local graph aggregation network considers spatial regions of the feature map as nodes and builds a local neighborhood graph that performs local feature aggregation before the global average pooling layer. We further utilize a batch normalization layer to improve the system effectiveness. Additionally, we introduce a class balanced loss to compensate for the imbalance in the sample distributions found in the most widely used vehicle re-identification datasets. Finally, we evaluate our method in three popular benchmarks and show that our approach outperforms many state-of-the-art methods.



### Architectural Adversarial Robustness: The Case for Deep Pursuit
- **Arxiv ID**: http://arxiv.org/abs/2011.14427v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2011.14427v1)
- **Published**: 2020-11-29 19:39:23+00:00
- **Updated**: 2020-11-29 19:39:23+00:00
- **Authors**: George Cazenavette, Calvin Murdock, Simon Lucey
- **Comment**: 11 pages, 14 figures
- **Journal**: None
- **Summary**: Despite their unmatched performance, deep neural networks remain susceptible to targeted attacks by nearly imperceptible levels of adversarial noise. While the underlying cause of this sensitivity is not well understood, theoretical analyses can be simplified by reframing each layer of a feed-forward network as an approximate solution to a sparse coding problem. Iterative solutions using basis pursuit are theoretically more stable and have improved adversarial robustness. However, cascading layer-wise pursuit implementations suffer from error accumulation in deeper networks. In contrast, our new method of deep pursuit approximates the activations of all layers as a single global optimization problem, allowing us to consider deeper, real-world architectures with skip connections such as residual networks. Experimentally, our approach demonstrates improved robustness to adversarial noise.



### Intrinsic Decomposition of Document Images In-the-Wild
- **Arxiv ID**: http://arxiv.org/abs/2011.14447v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.14447v1)
- **Published**: 2020-11-29 21:39:58+00:00
- **Updated**: 2020-11-29 21:39:58+00:00
- **Authors**: Sagnik Das, Hassan Ahmed Sial, Ke Ma, Ramon Baldrich, Maria Vanrell, Dimitris Samaras
- **Comment**: This a modified version of the BMVC 2020 accepted manuscript
- **Journal**: None
- **Summary**: Automatic document content processing is affected by artifacts caused by the shape of the paper, non-uniform and diverse color of lighting conditions. Fully-supervised methods on real data are impossible due to the large amount of data needed. Hence, the current state of the art deep learning models are trained on fully or partially synthetic images. However, document shadow or shading removal results still suffer because: (a) prior methods rely on uniformity of local color statistics, which limit their application on real-scenarios with complex document shapes and textures and; (b) synthetic or hybrid datasets with non-realistic, simulated lighting conditions are used to train the models. In this paper we tackle these problems with our two main contributions. First, a physically constrained learning-based method that directly estimates document reflectance based on intrinsic image formation which generalizes to challenging illumination conditions. Second, a new dataset that clearly improves previous synthetic ones, by adding a large range of realistic shading and diverse multi-illuminant conditions, uniquely customized to deal with documents in-the-wild. The proposed architecture works in a self-supervised manner where only the synthetic texture is used as a weak training signal (obviating the need for very costly ground truth with disentangled versions of shading and reflectance). The proposed approach leads to a significant generalization of document reflectance estimation in real scenes with challenging illumination. We extensively evaluate on the real benchmark datasets available for intrinsic image decomposition and document shadow removal tasks. Our reflectance estimation scheme, when used as a pre-processing step of an OCR pipeline, shows a 26% improvement of character error rate (CER), thus, proving the practical applicability.



### Improved Handling of Motion Blur in Online Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2011.14448v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.14448v2)
- **Published**: 2020-11-29 21:58:26+00:00
- **Updated**: 2021-03-30 14:34:38+00:00
- **Authors**: Mohamed Sayed, Gabriel Brostow
- **Comment**: Mirroring accepted CVPR paper. Added results for other real-world
  blur datasets. Main paper: 8 pages + 3 references. Supplemental: 9 pages
- **Journal**: None
- **Summary**: We wish to detect specific categories of objects, for online vision systems that will run in the real world. Object detection is already very challenging. It is even harder when the images are blurred, from the camera being in a car or a hand-held phone. Most existing efforts either focused on sharp images, with easy to label ground truth, or they have treated motion blur as one of many generic corruptions.   Instead, we focus especially on the details of egomotion induced blur. We explore five classes of remedies, where each targets different potential causes for the performance gap between sharp and blurred images. For example, first deblurring an image changes its human interpretability, but at present, only partly improves object detection. The other four classes of remedies address multi-scale texture, out-of-distribution testing, label generation, and conditioning by blur-type. Surprisingly, we discover that custom label generation aimed at resolving spatial ambiguity, ahead of all others, markedly improves object detection. Also, in contrast to findings from classification, we see a noteworthy boost by conditioning our model on bespoke categories of motion blur.   We validate and cross-breed the different remedies experimentally on blurred COCO images and real-world blur datasets, producing an easy and practical favorite model with superior detection rates.



### Conditional Link Prediction of Category-Implicit Keypoint Detection
- **Arxiv ID**: http://arxiv.org/abs/2011.14462v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.14462v1)
- **Published**: 2020-11-29 23:00:37+00:00
- **Updated**: 2020-11-29 23:00:37+00:00
- **Authors**: Ellen Yi-Ge, Rui Fan, Zechun Liu, Zhiqiang Shen
- **Comment**: WACV 2021 paper
- **Journal**: None
- **Summary**: Keypoints of objects reflect their concise abstractions, while the corresponding connection links (CL) build the skeleton by detecting the intrinsic relations between keypoints. Existing approaches are typically computationally-intensive, inapplicable for instances belonging to multiple classes, and/or infeasible to simultaneously encode connection information. To address the aforementioned issues, we propose an end-to-end category-implicit Keypoint and Link Prediction Network (KLPNet), which is the first approach for simultaneous semantic keypoint detection (for multi-class instances) and CL rejuvenation. In our KLPNet, a novel Conditional Link Prediction Graph is proposed for link prediction among keypoints that are contingent on a predefined category. Furthermore, a Cross-stage Keypoint Localization Module (CKLM) is introduced to explore feature aggregation for coarse-to-fine keypoint localization. Comprehensive experiments conducted on three publicly available benchmarks demonstrate that our KLPNet consistently outperforms all other state-of-the-art approaches. Furthermore, the experimental results of CL prediction also show the effectiveness of our KLPNet with respect to occlusion problems.



