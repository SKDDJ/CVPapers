# Arxiv Papers in cs.CV on 2020-11-26
### A Recurrent Vision-and-Language BERT for Navigation
- **Arxiv ID**: http://arxiv.org/abs/2011.13922v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.13922v2)
- **Published**: 2020-11-26 00:23:00+00:00
- **Updated**: 2021-03-28 11:45:58+00:00
- **Authors**: Yicong Hong, Qi Wu, Yuankai Qi, Cristian Rodriguez-Opazo, Stephen Gould
- **Comment**: None
- **Journal**: None
- **Summary**: Accuracy of many visiolinguistic tasks has benefited significantly from the application of vision-and-language(V&L) BERT. However, its application for the task of vision-and-language navigation (VLN) remains limited. One reason for this is the difficulty adapting the BERT architecture to the partially observable Markov decision process present in VLN, requiring history-dependent attention and decision making. In this paper we propose a recurrent BERT model that is time-aware for use in VLN. Specifically, we equip the BERT model with a recurrent function that maintains cross-modal state information for the agent. Through extensive experiments on R2R and REVERIE we demonstrate that our model can replace more complex encoder-decoder models to achieve state-of-the-art results. Moreover, our approach can be generalised to other transformer-based architectures, supports pre-training, and is capable of solving navigation and referring expression tasks simultaneously.



### Omni-GAN: On the Secrets of cGANs and Beyond
- **Arxiv ID**: http://arxiv.org/abs/2011.13074v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.13074v3)
- **Published**: 2020-11-26 00:30:20+00:00
- **Updated**: 2021-03-29 03:00:44+00:00
- **Authors**: Peng Zhou, Lingxi Xie, Bingbing Ni, Cong Geng, Qi Tian
- **Comment**: Introducing Omni-INR-GAN, which can extrapolate low-resolution images
  to arbitrary resolution
- **Journal**: None
- **Summary**: The conditional generative adversarial network (cGAN) is a powerful tool of generating high-quality images, but existing approaches mostly suffer unsatisfying performance or the risk of mode collapse. This paper presents Omni-GAN, a variant of cGAN that reveals the devil in designing a proper discriminator for training the model. The key is to ensure that the discriminator receives strong supervision to perceive the concepts and moderate regularization to avoid collapse. Omni-GAN is easily implemented and freely integrated with off-the-shelf encoding methods (e.g., implicit neural representation, INR). Experiments validate the superior performance of Omni-GAN and Omni-INR-GAN in a wide range of image generation and restoration tasks. In particular, Omni-INR-GAN sets new records on the ImageNet dataset with impressive Inception scores of 262.85 and 343.22 for the image sizes of 128 and 256, respectively, surpassing the previous records by 100+ points. Moreover, leveraging the generator prior, Omni-INR-GAN can extrapolate low-resolution images to arbitrary resolution, even up to x60+ higher resolution. Code is available.



### Energy Drain of the Object Detection Processing Pipeline for Mobile Devices: Analysis and Implications
- **Arxiv ID**: http://arxiv.org/abs/2011.13075v1
- **DOI**: None
- **Categories**: **cs.PF**, cs.CV, cs.MM, cs.NI
- **Links**: [PDF](http://arxiv.org/pdf/2011.13075v1)
- **Published**: 2020-11-26 00:32:07+00:00
- **Updated**: 2020-11-26 00:32:07+00:00
- **Authors**: Haoxin Wang, BaekGyu Kim, Jiang Xie, Zhu Han
- **Comment**: This is a personal copy of the authors. Not for redistribution. The
  final version of this paper was accepted by IEEE Transactions on Green
  Communications and Networking
- **Journal**: None
- **Summary**: Applying deep learning to object detection provides the capability to accurately detect and classify complex objects in the real world. However, currently, few mobile applications use deep learning because such technology is computation-intensive and energy-consuming. This paper, to the best of our knowledge, presents the first detailed experimental study of a mobile augmented reality (AR) client's energy consumption and the detection latency of executing Convolutional Neural Networks (CNN) based object detection, either locally on the smartphone or remotely on an edge server. In order to accurately measure the energy consumption on the smartphone and obtain the breakdown of energy consumed by each phase of the object detection processing pipeline, we propose a new measurement strategy. Our detailed measurements refine the energy analysis of mobile AR clients and reveal several interesting perspectives regarding the energy consumption of executing CNN-based object detection. Furthermore, several insights and research opportunities are proposed based on our experimental results. These findings from our experimental study will guide the design of energy-efficient processing pipeline of CNN-based object detection.



### Non-Rigid Puzzles
- **Arxiv ID**: http://arxiv.org/abs/2011.13076v1
- **DOI**: 10.1111/cgf.12970
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2011.13076v1)
- **Published**: 2020-11-26 00:32:30+00:00
- **Updated**: 2020-11-26 00:32:30+00:00
- **Authors**: Or Litany, Emanuele Rodol√†, Alex Bronstein, Michael Bronstein, Daniel Cremers
- **Comment**: None
- **Journal**: Computer Graphics Forum, Volume 35, Issue 5, August 2016
- **Summary**: Shape correspondence is a fundamental problem in computer graphics and vision, with applications in various problems including animation, texture mapping, robotic vision, medical imaging, archaeology and many more. In settings where the shapes are allowed to undergo non-rigid deformations and only partial views are available, the problem becomes very challenging. To this end, we present a non-rigid multi-part shape matching algorithm. We assume to be given a reference shape and its multiple parts undergoing a non-rigid deformation. Each of these query parts can be additionally contaminated by clutter, may overlap with other parts, and there might be missing parts or redundant ones. Our method simultaneously solves for the segmentation of the reference model, and for a dense correspondence to (subsets of) the parts. Experimental results on synthetic as well as real scans demonstrate the effectiveness of our method in dealing with this challenging matching scenario.



### Photoacoustic Reconstruction Using Sparsity in Curvelet Frame: Image versus Data Domain
- **Arxiv ID**: http://arxiv.org/abs/2011.13080v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.NA, math.NA
- **Links**: [PDF](http://arxiv.org/pdf/2011.13080v2)
- **Published**: 2020-11-26 00:51:39+00:00
- **Updated**: 2021-08-06 10:10:24+00:00
- **Authors**: Bolin Pan, Simon R. Arridge, Felix Lucka, Ben T. Cox, Nam Huynh, Paul C. Beard, Edward Z. Zhang, Marta M. Betcke
- **Comment**: 06 August 2021 (Accepted Version)
- **Journal**: None
- **Summary**: Curvelet frame is of special significance for photoacoustic tomography (PAT) due to its sparsifying and microlocalisation properties. We derive a one-to-one map between wavefront directions in image and data spaces in PAT which suggests near equivalence between the recovery of the initial pressure and PAT data from compressed/subsampled measurements when assuming sparsity in Curvelet frame. As the latter is computationally more tractable, investigation to which extent this equivalence holds conducted in this paper is of immediate practical significance. To this end we formulate and compare DR, a two step approach based on the recovery of the complete volume of the photoacoustic data from the subsampled data followed by the acoustic inversion, and p0R, a one step approach where the photoacoustic image (the initial pressure, p0) is directly recovered from the subsampled data. Effective representation of the photoacoustic data requires basis defined on the range of the photoacoustic forward operator. To this end we propose a novel wedge-restriction of Curvelet transform which enables us to construct such basis. Both recovery problems are formulated in a variational framework. As the Curvelet frame is heavily overdetermined, we use reweighted l1 norm penalties to enhance the sparsity of the solution. The data reconstruction problem DR is a standard compressed sensing recovery problem, which we solve using an ADMMtype algorithm, SALSA. Subsequently, the initial pressure is recovered using time reversal as implemented in the k-Wave Toolbox. The p0 reconstruction problem, p0R, aims to recover the photoacoustic image directly via FISTA, or ADMM when in addition including a non-negativity constraint. We compare and discuss the relative merits of the two approaches and illustrate them on 2D simulated and 3D real data in a fair and rigorous manner.



### Neural Scene Flow Fields for Space-Time View Synthesis of Dynamic Scenes
- **Arxiv ID**: http://arxiv.org/abs/2011.13084v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.13084v3)
- **Published**: 2020-11-26 01:23:44+00:00
- **Updated**: 2021-04-21 02:11:44+00:00
- **Authors**: Zhengqi Li, Simon Niklaus, Noah Snavely, Oliver Wang
- **Comment**: CVPR 2021, Project Website: http://www.cs.cornell.edu/~zl548/NSFF/
- **Journal**: None
- **Summary**: We present a method to perform novel view and time synthesis of dynamic scenes, requiring only a monocular video with known camera poses as input. To do this, we introduce Neural Scene Flow Fields, a new representation that models the dynamic scene as a time-variant continuous function of appearance, geometry, and 3D scene motion. Our representation is optimized through a neural network to fit the observed input views. We show that our representation can be used for complex dynamic scenes, including thin structures, view-dependent effects, and natural degrees of motion. We conduct a number of experiments that demonstrate our approach significantly outperforms recent monocular view synthesis methods, and show qualitative results of space-time view synthesis on a variety of real-world videos.



### Automatic Detection of Cardiac Chambers Using an Attention-based YOLOv4 Framework from Four-chamber View of Fetal Echocardiography
- **Arxiv ID**: http://arxiv.org/abs/2011.13096v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2011.13096v2)
- **Published**: 2020-11-26 02:28:24+00:00
- **Updated**: 2020-12-13 07:45:12+00:00
- **Authors**: Sibo Qiao, Shanchen Pang, Gang Luo, Silin Pan, Xun Wang, Min Wang, Xue Zhai, Taotao Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Echocardiography is a powerful prenatal examination tool for early diagnosis of fetal congenital heart diseases (CHDs). The four-chamber (FC) view is a crucial and easily accessible ultrasound (US) image among echocardiography images. Automatic analysis of FC views contributes significantly to the early diagnosis of CHDs. The first step to automatically analyze fetal FC views is locating the fetal four crucial chambers of heart in a US image. However, it is a greatly challenging task due to several key factors, such as numerous speckles in US images, the fetal cardiac chambers with small size and unfixed positions, and category indistinction caused by the similarity of cardiac chambers. These factors hinder the process of capturing robust and discriminative features, hence destroying fetal cardiac anatomical chambers precise localization. Therefore, we first propose a multistage residual hybrid attention module (MRHAM) to improve the feature learning. Then, we present an improved YOLOv4 detection model, namely MRHAM-YOLOv4-Slim. Specially, the residual identity mapping is replaced with the MRHAM in the backbone of MRHAM-YOLOv4-Slim, accurately locating the four important chambers in fetal FC views. Extensive experiments demonstrate that our proposed method outperforms current state-of-the-art, including the precision of 0.919, the recall of 0.971, the F1 score of 0.944, the mAP of 0.953, and the frames per second (FPS) of 43.



### Deep Metric Learning-based Image Retrieval System for Chest Radiograph and its Clinical Applications in COVID-19
- **Arxiv ID**: http://arxiv.org/abs/2012.03663v1
- **DOI**: 10.1016/j.media.2021.101993
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2012.03663v1)
- **Published**: 2020-11-26 03:16:48+00:00
- **Updated**: 2020-11-26 03:16:48+00:00
- **Authors**: Aoxiao Zhong, Xiang Li, Dufan Wu, Hui Ren, Kyungsang Kim, Younggon Kim, Varun Buch, Nir Neumark, Bernardo Bizzo, Won Young Tak, Soo Young Park, Yu Rim Lee, Min Kyu Kang, Jung Gil Park, Byung Seok Kim, Woo Jin Chung, Ning Guo, Ittai Dayan, Mannudeep K. Kalra, Quanzheng Li
- **Comment**: Aoxiao Zhong and Xiang Li contribute equally to this work
- **Journal**: Medical Image Analysis. 70 (2021) 101993
- **Summary**: In recent years, deep learning-based image analysis methods have been widely applied in computer-aided detection, diagnosis and prognosis, and has shown its value during the public health crisis of the novel coronavirus disease 2019 (COVID-19) pandemic. Chest radiograph (CXR) has been playing a crucial role in COVID-19 patient triaging, diagnosing and monitoring, particularly in the United States. Considering the mixed and unspecific signals in CXR, an image retrieval model of CXR that provides both similar images and associated clinical information can be more clinically meaningful than a direct image diagnostic model. In this work we develop a novel CXR image retrieval model based on deep metric learning. Unlike traditional diagnostic models which aims at learning the direct mapping from images to labels, the proposed model aims at learning the optimized embedding space of images, where images with the same labels and similar contents are pulled together. It utilizes multi-similarity loss with hard-mining sampling strategy and attention mechanism to learn the optimized embedding space, and provides similar images to the query image. The model is trained and validated on an international multi-site COVID-19 dataset collected from 3 different sources. Experimental results of COVID-19 image retrieval and diagnosis tasks show that the proposed model can serve as a robust solution for CXR analysis and patient management for COVID-19. The model is also tested on its transferability on a different clinical decision support task, where the pre-trained model is applied to extract image features from a new dataset without any further training. These results demonstrate our deep metric learning based image retrieval model is highly efficient in the CXR retrieval, diagnosis and prognosis, and thus has great clinical value for the treatment and management of COVID-19 patients.



### Polka Lines: Learning Structured Illumination and Reconstruction for Active Stereo
- **Arxiv ID**: http://arxiv.org/abs/2011.13117v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.13117v2)
- **Published**: 2020-11-26 04:02:43+00:00
- **Updated**: 2021-05-26 00:09:24+00:00
- **Authors**: Seung-Hwan Baek, Felix Heide
- **Comment**: None
- **Journal**: None
- **Summary**: Active stereo cameras that recover depth from structured light captures have become a cornerstone sensor modality for 3D scene reconstruction and understanding tasks across application domains. Existing active stereo cameras project a pseudo-random dot pattern on object surfaces to extract disparity independently of object texture. Such hand-crafted patterns are designed in isolation from the scene statistics, ambient illumination conditions, and the reconstruction method. In this work, we propose the first method to jointly learn structured illumination and reconstruction, parameterized by a diffractive optical element and a neural network, in an end-to-end fashion. To this end, we introduce a novel differentiable image formation model for active stereo, relying on both wave and geometric optics, and a novel trinocular reconstruction network. The jointly optimized pattern, which we dub "Polka Lines," together with the reconstruction network, achieve state-of-the-art active-stereo depth estimates across imaging conditions. We validate the proposed method in simulation and on a hardware prototype, and show that our method outperforms existing active stereo systems.



### Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
- **Arxiv ID**: http://arxiv.org/abs/2011.13118v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.13118v3)
- **Published**: 2020-11-26 04:04:21+00:00
- **Updated**: 2021-07-12 16:02:54+00:00
- **Authors**: Xiaoxiao Long, Lingjie Liu, Wei Li, Christian Theobalt, Wenping Wang
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel method for multi-view depth estimation from a single video, which is a critical task in various applications, such as perception, reconstruction and robot navigation. Although previous learning-based methods have demonstrated compelling results, most works estimate depth maps of individual video frames independently, without taking into consideration the strong geometric and temporal coherence among the frames. Moreover, current state-of-the-art (SOTA) models mostly adopt a fully 3D convolution network for cost regularization and therefore require high computational cost, thus limiting their deployment in real-world applications. Our method achieves temporally coherent depth estimation results by using a novel Epipolar Spatio-Temporal (EST) transformer to explicitly associate geometric and temporal correlation with multiple estimated depth maps. Furthermore, to reduce the computational cost, inspired by recent Mixture-of-Experts models, we design a compact hybrid network consisting of a 2D context-aware network and a 3D matching network which learn 2D context information and 3D disparity cues separately. Extensive experiments demonstrate that our method achieves higher accuracy in depth estimation and significant speedup than the SOTA methods.



### Lifting 2D StyleGAN for 3D-Aware Face Generation
- **Arxiv ID**: http://arxiv.org/abs/2011.13126v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.13126v2)
- **Published**: 2020-11-26 05:02:09+00:00
- **Updated**: 2021-04-18 09:43:49+00:00
- **Authors**: Yichun Shi, Divyansh Aggarwal, Anil K. Jain
- **Comment**: in CVPR 2021
- **Journal**: None
- **Summary**: We propose a framework, called LiftedGAN, that disentangles and lifts a pre-trained StyleGAN2 for 3D-aware face generation. Our model is "3D-aware" in the sense that it is able to (1) disentangle the latent space of StyleGAN2 into texture, shape, viewpoint, lighting and (2) generate 3D components for rendering synthetic images. Unlike most previous methods, our method is completely self-supervised, i.e. it neither requires any manual annotation nor 3DMM model for training. Instead, it learns to generate images as well as their 3D components by distilling the prior knowledge in StyleGAN2 with a differentiable renderer. The proposed model is able to output both the 3D shape and texture, allowing explicit pose and lighting control over generated images. Qualitative and quantitative results show the superiority of our approach over existing methods on 3D-controllable GANs in content controllability while generating realistic high quality images.



### A Fast Point Cloud Ground Segmentation Approach Based on Coarse-To-Fine Markov Random Field
- **Arxiv ID**: http://arxiv.org/abs/2011.13140v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.13140v2)
- **Published**: 2020-11-26 06:07:24+00:00
- **Updated**: 2021-01-29 07:32:04+00:00
- **Authors**: Weixin Huang, Huawei Liang, Linglong Lin, Zhiling Wang, Shaobo Wang, Biao Yu, Runxin Niu
- **Comment**: 16 page,22 figures
- **Journal**: None
- **Summary**: Ground segmentation is an important preprocessing task for autonomous vehicles (AVs) with 3D LiDARs. To solve the problem of existing ground segmentation methods being very difficult to balance accuracy and computational complexity, a fast point cloud ground segmentation approach based on a coarse-to-fine Markov random field (MRF) method is proposed. The method uses an improved elevation map for ground coarse segmentation, and then uses spatiotemporal adjacent points to optimize the segmentation results. The processed point cloud is classified into high-confidence obstacle points, ground points, and unknown classification points to initialize an MRF model. The graph cut method is then used to solve the model to achieve fine segmentation. Experiments on datasets showed that our method improves on other algorithms in terms of ground segmentation accuracy and is faster than other graph-based algorithms, which require only a single core of an I7-3770 CPU to process a frame of Velodyne HDL-64E data (in 39.77 ms, on average). Field tests were also conducted to demonstrate the effectiveness of the proposed method.



### Dense Attention Fluid Network for Salient Object Detection in Optical Remote Sensing Images
- **Arxiv ID**: http://arxiv.org/abs/2011.13144v1
- **DOI**: 10.1109/TIP.2020.3042084
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.13144v1)
- **Published**: 2020-11-26 06:14:10+00:00
- **Updated**: 2020-11-26 06:14:10+00:00
- **Authors**: Qijian Zhang, Runmin Cong, Chongyi Li, Ming-Ming Cheng, Yuming Fang, Xiaochun Cao, Yao Zhao, Sam Kwong
- **Comment**: Accepted by IEEE Transactions on Image Processing, EORSSD dataset:
  https://github.com/rmcong/EORSSD-dataset
- **Journal**: None
- **Summary**: Despite the remarkable advances in visual saliency analysis for natural scene images (NSIs), salient object detection (SOD) for optical remote sensing images (RSIs) still remains an open and challenging problem. In this paper, we propose an end-to-end Dense Attention Fluid Network (DAFNet) for SOD in optical RSIs. A Global Context-aware Attention (GCA) module is proposed to adaptively capture long-range semantic context relationships, and is further embedded in a Dense Attention Fluid (DAF) structure that enables shallow attention cues flow into deep layers to guide the generation of high-level feature attention maps. Specifically, the GCA module is composed of two key components, where the global feature aggregation module achieves mutual reinforcement of salient feature embeddings from any two spatial locations, and the cascaded pyramid attention module tackles the scale variation issue by building up a cascaded pyramid framework to progressively refine the attention map in a coarse-to-fine manner. In addition, we construct a new and challenging optical RSI dataset for SOD that contains 2,000 images with pixel-wise saliency annotations, which is currently the largest publicly available benchmark. Extensive experiments demonstrate that our proposed DAFNet significantly outperforms the existing state-of-the-art SOD competitors. https://github.com/rmcong/DAFNet_TIP20



### Continuous Conversion of CT Kernel using Switchable CycleGAN with AdaIN
- **Arxiv ID**: http://arxiv.org/abs/2011.13150v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.13150v2)
- **Published**: 2020-11-26 06:35:57+00:00
- **Updated**: 2021-04-26 00:51:33+00:00
- **Authors**: Serin Yang, Eung Yeop Kim, Jong Chul Ye
- **Comment**: None
- **Journal**: None
- **Summary**: X-ray computed tomography (CT) uses different filter kernels to highlight different structures. Since the raw sinogram data is usually removed after the reconstruction, in case there are additional need for other types of kernel images that were not previously generated, the patient may need to be scanned again. Accordingly, there exists increasing demand for post-hoc image domain conversion from one kernel to another without sacrificing the image quality. In this paper, we propose a novel unsupervised continuous kernel conversion method using cycle-consistent generative adversarial network (cycleGAN) with adaptive instance normalization (AdaIN). Even without paired training data, not only can our network translate the images between two different kernels, but it can also convert images along the interpolation path between the two kernel domains. We also show that the quality of generated images can be further improved if intermediate kernel domain images are available. Experimental results confirm that our method not only enables accurate kernel conversion that is comparable to supervised learning methods, but also generates intermediate kernel images in the unseen domain that are useful for hypopharyngeal cancer diagnosis.



### Transformation Driven Visual Reasoning
- **Arxiv ID**: http://arxiv.org/abs/2011.13160v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.13160v2)
- **Published**: 2020-11-26 07:11:31+00:00
- **Updated**: 2021-04-02 06:25:46+00:00
- **Authors**: Xin Hong, Yanyan Lan, Liang Pang, Jiafeng Guo, Xueqi Cheng
- **Comment**: Accepted to CVPR 2021. Resources including the TRANCE dataset and the
  code can be found at our homepage https://hongxin2019.github.io/TVR
- **Journal**: None
- **Summary**: This paper defines a new visual reasoning paradigm by introducing an important factor, i.e.~transformation. The motivation comes from the fact that most existing visual reasoning tasks, such as CLEVR in VQA, are solely defined to test how well the machine understands the concepts and relations within static settings, like one image. We argue that this kind of \textbf{state driven visual reasoning} approach has limitations in reflecting whether the machine has the ability to infer the dynamics between different states, which has been shown as important as state-level reasoning for human cognition in Piaget's theory. To tackle this problem, we propose a novel \textbf{transformation driven visual reasoning} task. Given both the initial and final states, the target is to infer the corresponding single-step or multi-step transformation, represented as a triplet (object, attribute, value) or a sequence of triplets, respectively. Following this definition, a new dataset namely TRANCE is constructed on the basis of CLEVR, including three levels of settings, i.e.~Basic (single-step transformation), Event (multi-step transformation), and View (multi-step transformation with variant views). Experimental results show that the state-of-the-art visual reasoning models perform well on Basic, but are still far from human-level intelligence on Event and View. We believe the proposed new paradigm will boost the development of machine visual reasoning. More advanced methods and real data need to be investigated in this direction. The resource of TVR is available at https://hongxin2019.github.io/TVR.



### Saliency-based segmentation of dermoscopic images using color information
- **Arxiv ID**: http://arxiv.org/abs/2011.13179v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.13179v3)
- **Published**: 2020-11-26 08:47:10+00:00
- **Updated**: 2021-11-05 15:45:26+00:00
- **Authors**: Giuliana Ramella
- **Comment**: Preprint
- **Journal**: None
- **Summary**: Skin lesion segmentation is one of the crucial steps for an efficient non-invasive computer-aided early diagnosis of melanoma. This paper investigates how color information, besides saliency, can be used to determine the pigmented lesion region automatically. Unlike most existing segmentation methods using only the saliency in order to discriminate against the skin lesion from the surrounding regions, we propose a novel method employing a binarization process coupled with new perceptual criteria, inspired by the human visual perception, related to the properties of saliency and color of the input image data distribution. As a means of refining the accuracy of the proposed method, the segmentation step is preceded by a pre-processing aimed at reducing the computation burden, removing artifacts, and improving contrast. We have assessed the method on two public databases, including 1497 dermoscopic images. We have also compared its performance with classical and recent saliency-based methods designed explicitly for dermoscopic images. The qualitative and quantitative evaluation indicates that the proposed method is promising since it produces an accurate skin lesion segmentation and performs satisfactorily compared to other existing saliency-based segmentation methods.



### Regularization with Latent Space Virtual Adversarial Training
- **Arxiv ID**: http://arxiv.org/abs/2011.13181v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.13181v2)
- **Published**: 2020-11-26 08:51:38+00:00
- **Updated**: 2022-12-24 10:54:02+00:00
- **Authors**: Genki Osada, Budrul Ahsan, Revoti Prasad Bora, Takashi Nishide
- **Comment**: Accepted at ECCV 2020 (Oral)
- **Journal**: None
- **Summary**: Virtual Adversarial Training (VAT) has shown impressive results among recently developed regularization methods called consistency regularization. VAT utilizes adversarial samples, generated by injecting perturbation in the input space, for training and thereby enhances the generalization ability of a classifier. However, such adversarial samples can be generated only within a very small area around the input data point, which limits the adversarial effectiveness of such samples. To address this problem we propose LVAT (Latent space VAT), which injects perturbation in the latent space instead of the input space. LVAT can generate adversarial samples flexibly, resulting in more adverse effects and thus more effective regularization. The latent space is built by a generative model, and in this paper, we examine two different type of models: variational auto-encoder and normalizing flow, specifically Glow. We evaluated the performance of our method in both supervised and semi-supervised learning scenarios for an image classification task using SVHN and CIFAR-10 datasets. In our evaluation, we found that our method outperforms VAT and other state-of-the-art methods.



### TinaFace: Strong but Simple Baseline for Face Detection
- **Arxiv ID**: http://arxiv.org/abs/2011.13183v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.13183v3)
- **Published**: 2020-11-26 08:54:19+00:00
- **Updated**: 2021-01-22 08:05:39+00:00
- **Authors**: Yanjia Zhu, Hongxiang Cai, Shuhan Zhang, Chenhao Wang, Yichao Xiong
- **Comment**: None
- **Journal**: None
- **Summary**: Face detection has received intensive attention in recent years. Many works present lots of special methods for face detection from different perspectives like model architecture, data augmentation, label assignment and etc., which make the overall algorithm and system become more and more complex. In this paper, we point out that \textbf{there is no gap between face detection and generic object detection}. Then we provide a strong but simple baseline method to deal with face detection named TinaFace. We use ResNet-50 \cite{he2016deep} as backbone, and all modules and techniques in TinaFace are constructed on existing modules, easily implemented and based on generic object detection. On the hard test set of the most popular and challenging face detection benchmark WIDER FACE \cite{yang2016wider}, with single-model and single-scale, our TinaFace achieves 92.1\% average precision (AP), which exceeds most of the recent face detectors with larger backbone. And after using test time augmentation (TTA), our TinaFace outperforms the current state-of-the-art method and achieves 92.4\% AP. The code will be available at \url{https://github.com/Media-Smart/vedadet}.



### t-EVA: Time-Efficient t-SNE Video Annotation
- **Arxiv ID**: http://arxiv.org/abs/2011.13202v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2011.13202v1)
- **Published**: 2020-11-26 09:56:54+00:00
- **Updated**: 2020-11-26 09:56:54+00:00
- **Authors**: Soroosh Poorgholi, Osman Semih Kayhan, Jan C. van Gemert
- **Comment**: ICPR 2020 (HCAU)
- **Journal**: None
- **Summary**: Video understanding has received more attention in the past few years due to the availability of several large-scale video datasets. However, annotating large-scale video datasets are cost-intensive. In this work, we propose a time-efficient video annotation method using spatio-temporal feature similarity and t-SNE dimensionality reduction to speed up the annotation process massively. Placing the same actions from different videos near each other in the two-dimensional space based on feature similarity helps the annotator to group-label video clips. We evaluate our method on two subsets of the ActivityNet (v1.3) and a subset of the Sports-1M dataset. We show that t-EVA can outperform other video annotation tools while maintaining test accuracy on video classification.



### Handling Object Symmetries in CNN-based Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2011.13209v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2011.13209v2)
- **Published**: 2020-11-26 10:10:25+00:00
- **Updated**: 2021-04-27 07:24:24+00:00
- **Authors**: Jesse Richter-Klug, Udo Frese
- **Comment**: This work has been accepte at ICRA 2021. Copyright may be transferred
  without notice, after which this version may no longer be accessible
- **Journal**: None
- **Summary**: In this paper, we investigate the problems that Convolutional Neural Networks (CNN)-based pose estimators have with symmetric objects. We considered the value of the CNN's output representation when continuously rotating the object and found that it has to form a closed loop after each step of symmetry. Otherwise, the CNN (which is itself a continuous function) has to replicate an uncontinuous function. On a 1-DOF toy example we show that commonly used representations do not fulfill this demand and analyze the problems caused thereby. In particular, we find that the popular min-over-symmetries approach for creating a symmetry-aware loss tends not to work well with gradient-based optimization, i.e. deep learning.   We propose a representation called "closed symmetry loop" (csl) from these insights, where the angle of relevant vectors is multiplied by the symmetry order and then generalize it to 6-DOF. The representation extends our algorithm from [Richter-Klug, ICVS, 2019] including a method to disambiguate symmetric equivalents during the final pose estimation. The algorithm handles continuous rotational symmetry (e.g. a bottle) and discrete rotational symmetry (e.g. a 4-fold symmetric box). It is evaluated on the T-LESS dataset, where it reaches state-of-the-art for unrefining RGB-based methods.



### Depth-Enhanced Feature Pyramid Network for Occlusion-Aware Verification of Buildings from Oblique Images
- **Arxiv ID**: http://arxiv.org/abs/2011.13226v2
- **DOI**: 10.1016/j.isprsjprs.2021.01.025
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.13226v2)
- **Published**: 2020-11-26 10:51:36+00:00
- **Updated**: 2021-01-23 08:46:50+00:00
- **Authors**: Qing Zhu, Shengzhi Huang, Han Hu, Haifeng Li, Min Chen, Ruofei Zhong
- **Comment**: None
- **Journal**: None
- **Summary**: Detecting the changes of buildings in urban environments is essential. Existing methods that use only nadir images suffer from severe problems of ambiguous features and occlusions between buildings and other regions. Furthermore, buildings in urban environments vary significantly in scale, which leads to performance issues when using single-scale features. To solve these issues, this paper proposes a fused feature pyramid network, which utilizes both color and depth data for the 3D verification of existing buildings 2D footprints from oblique images. First, the color data of oblique images are enriched with the depth information rendered from 3D mesh models. Second, multiscale features are fused in the feature pyramid network to convolve both the color and depth data. Finally, multi-view information from both the nadir and oblique images is used in a robust voting procedure to label changes in existing buildings. Experimental evaluations using both the ISPRS benchmark datasets and Shenzhen datasets reveal that the proposed method outperforms the ResNet and EfficientNet networks by 5\% and 2\%, respectively, in terms of recall rate and precision. We demonstrate that the proposed method can successfully detect all changed buildings; therefore, only those marked as changed need to be manually checked during the pipeline updating procedure; this significantly reduces the manual quality control requirements. Moreover, ablation studies indicate that using depth data, feature pyramid modules, and multi-view voting strategies can lead to clear and progressive improvements.



### MultiStar: Instance Segmentation of Overlapping Objects with Star-Convex Polygons
- **Arxiv ID**: http://arxiv.org/abs/2011.13228v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2011.13228v2)
- **Published**: 2020-11-26 10:52:33+00:00
- **Updated**: 2021-01-14 10:19:15+00:00
- **Authors**: Florin C. Walter, Sebastian Damrich, Fred A. Hamprecht
- **Comment**: Accepted for ISBI 2021
- **Journal**: None
- **Summary**: Instance segmentation of overlapping objects in biomedical images remains a largely unsolved problem. We take up this challenge and present MultiStar, an extension to the popular instance segmentation method StarDist. The key novelty of our method is that we identify pixels at which objects overlap and use this information to improve proposal sampling and to avoid suppressing proposals of truly overlapping objects. This allows us to apply the ideas of StarDist to images with overlapping objects, while incurring only a small overhead compared to the established method. MultiStar shows promising results on two datasets and has the advantage of using a simple and easy to train network architecture.



### The Devil is in the Boundary: Exploiting Boundary Representation for Basis-based Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2011.13241v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.13241v1)
- **Published**: 2020-11-26 11:26:06+00:00
- **Updated**: 2020-11-26 11:26:06+00:00
- **Authors**: Myungchul Kim, Sanghyun Woo, Dahun Kim, In So Kweon
- **Comment**: None
- **Journal**: None
- **Summary**: Pursuing a more coherent scene understanding towards real-time vision applications, single-stage instance segmentation has recently gained popularity, achieving a simpler and more efficient design than its two-stage counterparts. Besides, its global mask representation often leads to superior accuracy to the two-stage Mask R-CNN which has been dominant thus far. Despite the promising advances in single-stage methods, finer delineation of instance boundaries still remains unexcavated. Indeed, boundary information provides a strong shape representation that can operate in synergy with the fully-convolutional mask features of the single-stage segmenter. In this work, we propose Boundary Basis based Instance Segmentation(B2Inst) to learn a global boundary representation that can complement existing global-mask-based methods that are often lacking high-frequency details. Besides, we devise a unified quality measure of both mask and boundary and introduce a network block that learns to score the per-instance predictions of itself. When applied to the strongest baselines in single-stage instance segmentation, our B2Inst leads to consistent improvements and accurately parse out the instance boundaries in a scene. Regardless of being single-stage or two-stage frameworks, we outperform the existing state-of-the-art methods on the COCO dataset with the same ResNet-50 and ResNet-101 backbones.



### MVTN: Multi-View Transformation Network for 3D Shape Recognition
- **Arxiv ID**: http://arxiv.org/abs/2011.13244v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, 68T45
- **Links**: [PDF](http://arxiv.org/pdf/2011.13244v3)
- **Published**: 2020-11-26 11:33:53+00:00
- **Updated**: 2021-08-17 15:10:28+00:00
- **Authors**: Abdullah Hamdi, Silvio Giancola, Bernard Ghanem
- **Comment**: Published at ICCV 2021
- **Journal**: None
- **Summary**: Multi-view projection methods have demonstrated their ability to reach state-of-the-art performance on 3D shape recognition. Those methods learn different ways to aggregate information from multiple views. However, the camera view-points for those views tend to be heuristically set and fixed for all shapes. To circumvent the lack of dynamism of current multi-view methods, we propose to learn those view-points. In particular, we introduce the Multi-View Transformation Network (MVTN) that regresses optimal view-points for 3D shape recognition, building upon advances in differentiable rendering. As a result, MVTN can be trained end-to-end along with any multi-view network for 3D shape classification. We integrate MVTN in a novel adaptive multi-view pipeline that can render either 3D meshes or point clouds. MVTN exhibits clear performance gains in the tasks of 3D shape classification and 3D shape retrieval without the need for extra training supervision. In these tasks, MVTN achieves state-of-the-art performance on ModelNet40, ShapeNet Core55, and the most recent and realistic ScanObjectNN dataset (up to 6% improvement). Interestingly, we also show that MVTN can provide network robustness against rotation and occlusion in the 3D domain. The code is available at https://github.com/ajhamdi/MVTN .



### IFSS-Net: Interactive Few-Shot Siamese Network for Faster Muscle Segmentation and Propagation in Volumetric Ultrasound
- **Arxiv ID**: http://arxiv.org/abs/2011.13246v2
- **DOI**: 10.1109/TMI.2021.3058303
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.13246v2)
- **Published**: 2020-11-26 11:37:25+00:00
- **Updated**: 2021-01-30 11:40:48+00:00
- **Authors**: Dawood Al Chanti, Vanessa Gonzalez Duque, Marion Crouzier, Antoine Nordez, Lilian Lacourpaille, Diana Mateus
- **Comment**: 14 pages, 18 figures, 10 Tables
- **Journal**: None
- **Summary**: We present an accurate, fast and efficient method for segmentation and muscle mask propagation in 3D freehand ultrasound data, towards accurate volume quantification. A deep Siamese 3D Encoder-Decoder network that captures the evolution of the muscle appearance and shape for contiguous slices is deployed. We uses it to propagate a reference mask annotated by a clinical expert. To handle longer changes of the muscle shape over the entire volume and to provide an accurate propagation, we devise a Bidirectional Long Short Term Memory module. Also, to train our model with a minimal amount of training samples, we propose a strategy combining learning from few annotated 2D ultrasound slices with sequential pseudo-labeling of the unannotated slices. We introduce a decremental update of the objective function to guide the model convergence in the absence of large amounts of annotated data. After training with a small number of volumes, the decremental update transitions from a weakly-supervised training to a few-shot setting. Finally, to handle the class-imbalance between foreground and background muscle pixels, we propose a parametric Tversky loss function that learns to adaptively penalize false positives and false negatives. We validate our approach for the segmentation, label propagation, and volume computation of the three low-limb muscles on a dataset of 61600 images from 44 subjects. We achieve a Dice score coefficient of over $95~\%$ and a volumetric error \textcolor{black}{of} $1.6035 \pm 0.587~\%$.



### Channel-wise Knowledge Distillation for Dense Prediction
- **Arxiv ID**: http://arxiv.org/abs/2011.13256v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.13256v4)
- **Published**: 2020-11-26 12:00:38+00:00
- **Updated**: 2021-08-27 03:05:25+00:00
- **Authors**: Changyong Shu, Yifan Liu, Jianfei Gao, Zheng Yan, Chunhua Shen
- **Comment**: Accepted to Proc. Int. Conf. Computer Vision (ICCV) 2021. Code is
  available at: https://git.io/Distiller
- **Journal**: None
- **Summary**: Knowledge distillation (KD) has been proven to be a simple and effective tool for training compact models. Almost all KD variants for dense prediction tasks align the student and teacher networks' feature maps in the spatial domain, typically by minimizing point-wise and/or pair-wise discrepancy. Observing that in semantic segmentation, some layers' feature activations of each channel tend to encode saliency of scene categories (analogue to class activation mapping), we propose to align features channel-wise between the student and teacher networks. To this end, we first transform the feature map of each channel into a probabilty map using softmax normalization, and then minimize the Kullback-Leibler (KL) divergence of the corresponding channels of the two networks. By doing so, our method focuses on mimicking the soft distributions of channels between networks. In particular, the KL divergence enables learning to pay more attention to the most salient regions of the channel-wise maps, presumably corresponding to the most useful signals for semantic segmentation. Experiments demonstrate that our channel-wise distillation outperforms almost all existing spatial distillation methods for semantic segmentation considerably, and requires less computational cost during training. We consistently achieve superior performance on three benchmarks with various network structures. Code is available at: https://git.io/Distiller



### CYPUR-NN: Crop Yield Prediction Using Regression and Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2011.13265v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.13265v1)
- **Published**: 2020-11-26 12:50:58+00:00
- **Updated**: 2020-11-26 12:50:58+00:00
- **Authors**: Sandesh Ramesh, Anirudh Hebbar, Varun Yadav, Thulasiram Gunta, A Balachandra
- **Comment**: Advances in Intelligent Systems and Computing
- **Journal**: None
- **Summary**: Our recent study using historic data of paddy yield and associated conditions include humidity, luminescence, and temperature. By incorporating regression models and neural networks (NN), one can produce highly satisfactory forecasting of paddy yield. Simulations indicate that our model can predict paddy yield with high accuracy while concurrently detecting diseases that may exist and are oblivious to the human eye. Crop Yield Prediction Using Regression and Neural Networks (CYPUR-NN) is developed here as a system that will facilitate agriculturists and farmers to predict yield from a picture or by entering values via a web interface. CYPUR-NN has been tested on stock images and the experimental results are promising.



### Group-Skeleton-Based Human Action Recognition in Complex Events
- **Arxiv ID**: http://arxiv.org/abs/2011.13273v2
- **DOI**: 10.1145/3394171.3416280
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.13273v2)
- **Published**: 2020-11-26 13:19:14+00:00
- **Updated**: 2021-02-25 03:42:32+00:00
- **Authors**: Tingtian Li, Zixun Sun, Xiao Chen
- **Comment**: accpeted by ACM MM 2020
- **Journal**: None
- **Summary**: Human action recognition as an important application of computer vision has been studied for decades. Among various approaches, skeleton-based methods recently attract increasing attention due to their robust and superior performance. However, existing skeleton-based methods ignore the potential action relationships between different persons, while the action of a person is highly likely to be impacted by another person especially in complex events. In this paper, we propose a novel group-skeleton-based human action recognition method in complex events. This method first utilizes multi-scale spatial-temporal graph convolutional networks (MS-G3Ds) to extract skeleton features from multiple persons. In addition to the traditional key point coordinates, we also input the key point speed values to the networks for better performance. Then we use multilayer perceptrons (MLPs) to embed the distance values between the reference person and other persons into the extracted features. Lastly, all the features are fed into another MS-G3D for feature fusion and classification. For avoiding class imbalance problems, the networks are trained with a focal loss. The proposed algorithm is also our solution for the Large-scale Human-centric Video Analysis in Complex Events Challenge. Results on the HiEve dataset show that our method can give superior performance compared to other state-of-the-art methods.



### Polygon-free: Unconstrained Scene Text Detection with Box Annotations
- **Arxiv ID**: http://arxiv.org/abs/2011.13307v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.13307v3)
- **Published**: 2020-11-26 14:19:33+00:00
- **Updated**: 2022-05-26 10:47:26+00:00
- **Authors**: Weijia Wu, Enze Xie, Ruimao Zhang, Wenhai Wang, Hong Zhou, Ping Luo
- **Comment**: None
- **Journal**: None
- **Summary**: Although a polygon is a more accurate representation than an upright bounding box for text detection, the annotations of polygons are extremely expensive and challenging. Unlike existing works that employ fully-supervised training with polygon annotations, this study proposes an unconstrained text detection system termed Polygon-free (PF), in which most existing polygon-based text detectors (e.g., PSENet [33],DB [16]) are trained with only upright bounding box annotations. Our core idea is to transfer knowledge from synthetic data to real data to enhance the supervision information of upright bounding boxes. This is made possible with a simple segmentation network, namely Skeleton Attention Segmentation Network (SASN), that includes three vital components (i.e., channel attention, spatial attention and skeleton attention map) and one soft cross-entropy loss. Experiments demonstrate that the proposed Polygonfree system can combine general detectors (e.g., EAST, PSENet, DB) to yield surprisingly high-quality pixel-level results with only upright bounding box annotations on a variety of datasets (e.g., ICDAR2019-Art, TotalText, ICDAR2015). For example, without using polygon annotations, PSENet achieves an 80.5% F-score on TotalText [3] (vs. 80.9% of fully supervised counterpart), 31.1% better than training directly with upright bounding box annotations, and saves 80%+ labeling costs. We hope that PF can provide a new perspective for text detection to reduce the labeling costs. The code can be found at https://github.com/weijiawu/Unconstrained-Text-Detection-with-Box-Supervisionand-Dynamic-Self-Training.



### Polarization-driven Semantic Segmentation via Efficient Attention-bridged Fusion
- **Arxiv ID**: http://arxiv.org/abs/2011.13313v2
- **DOI**: 10.1364/OE.416130
- **Categories**: **cs.CV**, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2011.13313v2)
- **Published**: 2020-11-26 14:32:42+00:00
- **Updated**: 2021-01-22 22:06:59+00:00
- **Authors**: Kaite Xiang, Kailun Yang, Kaiwei Wang
- **Comment**: Accepted by Optics Express. 18 pages, 16 figures, 3 tables, 9
  equations. Code will be made publicly available at
  https://github.com/Katexiang/EAFNet
- **Journal**: None
- **Summary**: Semantic Segmentation (SS) is promising for outdoor scene perception in safety-critical applications like autonomous vehicles, assisted navigation and so on. However, traditional SS is primarily based on RGB images, which limits the reliability of SS in complex outdoor scenes, where RGB images lack necessary information dimensions to fully perceive unconstrained environments. As preliminary investigation, we examine SS in an unexpected obstacle detection scenario, which demonstrates the necessity of multimodal fusion. Thereby, in this work, we present EAFNet, an Efficient Attention-bridged Fusion Network to exploit complementary information coming from different optical sensors. Specifically, we incorporate polarization sensing to obtain supplementary information, considering its optical characteristics for robust representation of diverse materials. By using a single-shot polarization sensor, we build the first RGB-P dataset which consists of 394 annotated pixel-aligned RGB-Polarization images. A comprehensive variety of experiments shows the effectiveness of EAFNet to fuse polarization and RGB information, as well as the flexibility to be adapted to other sensor combination scenarios.



### Adaptive Multiplane Image Generation from a Single Internet Picture
- **Arxiv ID**: http://arxiv.org/abs/2011.13317v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.13317v1)
- **Published**: 2020-11-26 14:35:05+00:00
- **Updated**: 2020-11-26 14:35:05+00:00
- **Authors**: Diogo C. Luvizon, Gustavo Sutter P. Carvalho, Andreza A. dos Santos, Jhonatas S. Conceicao, Jose L. Flores-Campana, Luis G. L. Decker, Marcos R. Souza, Helio Pedrini, Antonio Joia, Otavio A. B. Penatti
- **Comment**: None
- **Journal**: None
- **Summary**: In the last few years, several works have tackled the problem of novel view synthesis from stereo images or even from a single picture. However, previous methods are computationally expensive, specially for high-resolution images. In this paper, we address the problem of generating a multiplane image (MPI) from a single high-resolution picture. We present the adaptive-MPI representation, which allows rendering novel views with low computational requirements. To this end, we propose an adaptive slicing algorithm that produces an MPI with a variable number of image planes. We present a new lightweight CNN for depth estimation, which is learned by knowledge distillation from a larger network. Occluded regions in the adaptive-MPI are inpainted also by a lightweight CNN. We show that our method is capable of producing high-quality predictions with one order of magnitude less parameters compared to previous approaches. The robustness of our method is evidenced on challenging pictures from the Internet.



### Spatio-Temporal Inception Graph Convolutional Networks for Skeleton-Based Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2011.13322v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2011.13322v2)
- **Published**: 2020-11-26 14:43:04+00:00
- **Updated**: 2021-08-20 02:14:37+00:00
- **Authors**: Zhen Huang, Xu Shen, Xinmei Tian, Houqiang Li, Jianqiang Huang, Xian-Sheng Hua
- **Comment**: ACM MM 2020
- **Journal**: None
- **Summary**: Skeleton-based human action recognition has attracted much attention with the prevalence of accessible depth sensors. Recently, graph convolutional networks (GCNs) have been widely used for this task due to their powerful capability to model graph data. The topology of the adjacency graph is a key factor for modeling the correlations of the input skeletons. Thus, previous methods mainly focus on the design/learning of the graph topology. But once the topology is learned, only a single-scale feature and one transformation exist in each layer of the networks. Many insights, such as multi-scale information and multiple sets of transformations, that have been proven to be very effective in convolutional neural networks (CNNs), have not been investigated in GCNs. The reason is that, due to the gap between graph-structured skeleton data and conventional image/video data, it is very challenging to embed these insights into GCNs. To overcome this gap, we reinvent the split-transform-merge strategy in GCNs for skeleton sequence processing. Specifically, we design a simple and highly modularized graph convolutional network architecture for skeleton-based action recognition. Our network is constructed by repeating a building block that aggregates multi-granularity information from both the spatial and temporal paths. Extensive experiments demonstrate that our network outperforms state-of-the-art methods by a significant margin with only 1/5 of the parameters and 1/10 of the FLOPs. Code is available at https://github.com/yellowtownhz/STIGCN.



### Analysing Social Media Network Data with R: Semi-Automated Screening of Users, Comments and Communication Patterns
- **Arxiv ID**: http://arxiv.org/abs/2011.13327v1
- **DOI**: None
- **Categories**: **cs.SI**, cs.CV, stat.AP, D.1.5; D.3.0; J.4; K.4.1
- **Links**: [PDF](http://arxiv.org/pdf/2011.13327v1)
- **Published**: 2020-11-26 14:52:01+00:00
- **Updated**: 2020-11-26 14:52:01+00:00
- **Authors**: Dennis Klinkhammer
- **Comment**: 14 pages, 2 figures
- **Journal**: None
- **Summary**: Communication on social media platforms is not only culturally and politically relevant, it is also increasingly widespread across societies. Users not only communicate via social media platforms, but also search specifically for information, disseminate it or post information themselves. However, fake news, hate speech and even radicalizing elements are part of this modern form of communication: Sometimes with far-reaching effects on individuals and societies. A basic understanding of these mechanisms and communication patterns could help to counteract negative forms of communication, e.g. bullying among children or extreme political points of view. To this end, a method will be presented in order to break down the underlying communication patterns, to trace individual users and to inspect their comments and range on social media platforms; Or to contrast them later on via qualitative research. This approeach can identify particularly active users with an accuracy of 100 percent, if the framing social networks as well as the topics are taken into account. However, methodological as well as counteracting approaches must be even more dynamic and flexible to ensure sensitivity and specifity regarding users who spread hate speech, fake news and radicalizing elements.



### DyCo3D: Robust Instance Segmentation of 3D Point Clouds through Dynamic Convolution
- **Arxiv ID**: http://arxiv.org/abs/2011.13328v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.13328v2)
- **Published**: 2020-11-26 14:56:57+00:00
- **Updated**: 2021-03-06 00:41:22+00:00
- **Authors**: Tong He, Chunhua Shen, Anton van den Hengel
- **Comment**: Appearing in IEEE Conf. Computer Vision and Pattern Recognition
  (CVPR), 2021
- **Journal**: None
- **Summary**: Previous top-performing approaches for point cloud instance segmentation involve a bottom-up strategy, which often includes inefficient operations or complex pipelines, such as grouping over-segmented components, introducing additional steps for refining, or designing complicated loss functions. The inevitable variation in the instance scales can lead bottom-up methods to become particularly sensitive to hyper-parameter values. To this end, we propose instead a dynamic, proposal-free, data-driven approach that generates the appropriate convolution kernels to apply in response to the nature of the instances. To make the kernels discriminative, we explore a large context by gathering homogeneous points that share identical semantic categories and have close votes for the geometric centroids. Instances are then decoded by several simple convolutional layers. Due to the limited receptive field introduced by the sparse convolution, a small light-weight transformer is also devised to capture the long-range dependencies and high-level interactions among point samples. The proposed method achieves promising results on both ScanetNetV2 and S3DIS, and this performance is robust to the particular hyper-parameter values chosen. It also improves inference speed by more than 25% over the current state-of-the-art. Code is available at: https://git.io/DyCo3D



### 4D Human Body Capture from Egocentric Video via 3D Scene Grounding
- **Arxiv ID**: http://arxiv.org/abs/2011.13341v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.13341v2)
- **Published**: 2020-11-26 15:17:16+00:00
- **Updated**: 2021-10-15 23:03:13+00:00
- **Authors**: Miao Liu, Dexin Yang, Yan Zhang, Zhaopeng Cui, James M. Rehg, Siyu Tang
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a novel task of reconstructing a time series of second-person 3D human body meshes from monocular egocentric videos. The unique viewpoint and rapid embodied camera motion of egocentric videos raise additional technical barriers for human body capture. To address those challenges, we propose a simple yet effective optimization-based approach that leverages 2D observations of the entire video sequence and human-scene interaction constraint to estimate second-person human poses, shapes, and global motion that are grounded on the 3D environment captured from the egocentric view. We conduct detailed ablation studies to validate our design choice. Moreover, we compare our method with the previous state-of-the-art method on human motion capture from monocular video, and show that our method estimates more accurate human-body poses and shapes under the challenging egocentric setting. In addition, we demonstrate that our approach produces more realistic human-scene interaction.



### A Unified Mixture-View Framework for Unsupervised Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2011.13356v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.13356v2)
- **Published**: 2020-11-26 15:43:27+00:00
- **Updated**: 2022-10-10 08:29:35+00:00
- **Authors**: Xiangxiang Chu, Xiaohang Zhan, Bo Zhang
- **Comment**: BMVC 2022
- **Journal**: None
- **Summary**: Recent unsupervised contrastive representation learning follows a Single Instance Multi-view (SIM) paradigm where positive pairs are usually constructed with intra-image data augmentation. In this paper, we propose an effective approach called Beyond Single Instance Multi-view (BSIM). Specifically, we impose more accurate instance discrimination capability by measuring the joint similarity between two randomly sampled instances and their mixture, namely spurious-positive pairs. We believe that learning joint similarity helps to improve the performance when encoded features are distributed more evenly in the latent space. We apply it as an orthogonal improvement for unsupervised contrastive representation learning, including current outstanding methods SimCLR, MoCo, and BYOL. We evaluate our learned representations on many downstream benchmarks like linear classification on ImageNet-1k and PASCAL VOC 2007, object detection on MS COCO 2017 and VOC, etc. We obtain substantial gains with a large margin almost on all these tasks compared with prior arts.



### ClusterFace: Joint Clustering and Classification for Set-Based Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2011.13360v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, I.4.0; I.5.0
- **Links**: [PDF](http://arxiv.org/pdf/2011.13360v1)
- **Published**: 2020-11-26 15:55:27+00:00
- **Updated**: 2020-11-26 15:55:27+00:00
- **Authors**: S. W. Arachchilage, E. Izquierdo
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning technology has enabled successful modeling of complex facial features when high quality images are available. Nonetheless, accurate modeling and recognition of human faces in real world scenarios `on the wild' or under adverse conditions remains an open problem. When unconstrained faces are mapped into deep features, variations such as illumination, pose, occlusion, etc., can create inconsistencies in the resultant feature space. Hence, deriving conclusions based on direct associations could lead to degraded performance. This rises the requirement for a basic feature space analysis prior to face recognition. This paper devises a joint clustering and classification scheme which learns deep face associations in an easy-to-hard way. Our method is based on hierarchical clustering where the early iterations tend to preserve high reliability. The rationale of our method is that a reliable clustering result can provide insights on the distribution of the feature space, that can guide the classification that follows. Experimental evaluations on three tasks, face verification, face identification and rank-order search, demonstrates better or competitive performance compared to the state-of-the-art, on all three experiments.



### SSDL: Self-Supervised Domain Learning for Improved Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2011.13361v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, I.5.0; I.4.0
- **Links**: [PDF](http://arxiv.org/pdf/2011.13361v1)
- **Published**: 2020-11-26 15:55:59+00:00
- **Updated**: 2020-11-26 15:55:59+00:00
- **Authors**: S. W. Arachchilage, E. Izquierdo
- **Comment**: None
- **Journal**: None
- **Summary**: Face recognition in unconstrained environments is challenging due to variations in illumination, quality of sensing, motion blur and etc. An individual's face appearance can vary drastically under different conditions creating a gap between train (source) and varying test (target) data. The domain gap could cause decreased performance levels in direct knowledge transfer from source to target. Despite fine-tuning with domain specific data could be an effective solution, collecting and annotating data for all domains is extremely expensive. To this end, we propose a self-supervised domain learning (SSDL) scheme that trains on triplets mined from unlabelled data. A key factor in effective discriminative learning, is selecting informative triplets. Building on most confident predictions, we follow an "easy-to-hard" scheme of alternate triplet mining and self-learning. Comprehensive experiments on four different benchmarks show that SSDL generalizes well on different domains.



### SoccerNet-v2: A Dataset and Benchmarks for Holistic Understanding of Broadcast Soccer Videos
- **Arxiv ID**: http://arxiv.org/abs/2011.13367v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.13367v3)
- **Published**: 2020-11-26 16:10:16+00:00
- **Updated**: 2021-04-19 15:03:26+00:00
- **Authors**: Adrien Deli√®ge, Anthony Cioppa, Silvio Giancola, Meisam J. Seikavandi, Jacob V. Dueholm, Kamal Nasrollahi, Bernard Ghanem, Thomas B. Moeslund, Marc Van Droogenbroeck
- **Comment**: Paper accepted for the CVsports workshop at CVPR2021. This document
  contains 8 pages + references + supplementary material
- **Journal**: None
- **Summary**: Understanding broadcast videos is a challenging task in computer vision, as it requires generic reasoning capabilities to appreciate the content offered by the video editing. In this work, we propose SoccerNet-v2, a novel large-scale corpus of manual annotations for the SoccerNet video dataset, along with open challenges to encourage more research in soccer understanding and broadcast production. Specifically, we release around 300k annotations within SoccerNet's 500 untrimmed broadcast soccer videos. We extend current tasks in the realm of soccer to include action spotting, camera shot segmentation with boundary detection, and we define a novel replay grounding task. For each task, we provide and discuss benchmark results, reproducible with our open-source adapted implementations of the most relevant works in the field. SoccerNet-v2 is presented to the broader research community to help push computer vision closer to automatic solutions for more general video understanding and production purposes.



### A Deep Learning Bidirectional Temporal Tracking Algorithm for Automated Blood Cell Counting from Non-invasive Capillaroscopy Videos
- **Arxiv ID**: http://arxiv.org/abs/2011.13371v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.13371v3)
- **Published**: 2020-11-26 16:27:13+00:00
- **Updated**: 2021-04-12 12:47:35+00:00
- **Authors**: Luojie Huang, Gregory N. McKay, Nicholas J. Durr
- **Comment**: 10 pages, 6 figures
- **Journal**: None
- **Summary**: Oblique back-illumination capillaroscopy has recently been introduced as a method for high-quality, non-invasive blood cell imaging in human capillaries. To make this technique practical for clinical blood cell counting, solutions for automatic processing of acquired videos are needed. Here, we take the first step towards this goal, by introducing a deep learning multi-cell tracking model, named CycleTrack, which achieves accurate blood cell counting from capillaroscopic videos. CycleTrack combines two simple online tracking models, SORT and CenterTrack, and is tailored to features of capillary blood cell flow. Blood cells are tracked by displacement vectors in two opposing temporal directions (forward- and backward-tracking) between consecutive frames. This approach yields accurate tracking despite rapidly moving and deforming blood cells. The proposed model outperforms other baseline trackers, achieving 65.57% Multiple Object Tracking Accuracy and 73.95% ID F1 score on test videos. Compared to manual blood cell counting, CycleTrack achieves 96.58 $\pm$ 2.43% cell counting accuracy among 8 test videos with 1000 frames each compared to 93.45% and 77.02% accuracy for independent CenterTrack and SORT almost without additional time expense. It takes 800s to track and count approximately 8000 blood cells from 9,600 frames captured in a typical one-minute video. Moreover, the blood cell velocity measured by CycleTrack demonstrates a consistent, pulsatile pattern within the physiological range of heart rate. Lastly, we discuss future improvements for the CycleTrack framework, which would enable clinical translation of the oblique back-illumination microscope towards a real-time and non-invasive point-of-care blood cell counting and analyzing technology.



### Invisible Perturbations: Physical Adversarial Examples Exploiting the Rolling Shutter Effect
- **Arxiv ID**: http://arxiv.org/abs/2011.13375v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.13375v3)
- **Published**: 2020-11-26 16:34:47+00:00
- **Updated**: 2021-04-18 16:21:42+00:00
- **Authors**: Athena Sayles, Ashish Hooda, Mohit Gupta, Rahul Chatterjee, Earlence Fernandes
- **Comment**: None
- **Journal**: None
- **Summary**: Physical adversarial examples for camera-based computer vision have so far been achieved through visible artifacts -- a sticker on a Stop sign, colorful borders around eyeglasses or a 3D printed object with a colorful texture. An implicit assumption here is that the perturbations must be visible so that a camera can sense them. By contrast, we contribute a procedure to generate, for the first time, physical adversarial examples that are invisible to human eyes. Rather than modifying the victim object with visible artifacts, we modify light that illuminates the object. We demonstrate how an attacker can craft a modulated light signal that adversarially illuminates a scene and causes targeted misclassifications on a state-of-the-art ImageNet deep learning model. Concretely, we exploit the radiometric rolling shutter effect in commodity cameras to create precise striping patterns that appear on images. To human eyes, it appears like the object is illuminated, but the camera creates an image with stripes that will cause ML models to output the attacker-desired classification. We conduct a range of simulation and physical experiments with LEDs, demonstrating targeted attack rates up to 84%.



### How Well Do Self-Supervised Models Transfer?
- **Arxiv ID**: http://arxiv.org/abs/2011.13377v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.13377v2)
- **Published**: 2020-11-26 16:38:39+00:00
- **Updated**: 2021-03-29 13:20:03+00:00
- **Authors**: Linus Ericsson, Henry Gouk, Timothy M. Hospedales
- **Comment**: CVPR 2021. Code available at
  https://github.com/linusericsson/ssl-transfer
- **Journal**: None
- **Summary**: Self-supervised visual representation learning has seen huge progress recently, but no large scale evaluation has compared the many models now available. We evaluate the transfer performance of 13 top self-supervised models on 40 downstream tasks, including many-shot and few-shot recognition, object detection, and dense prediction. We compare their performance to a supervised baseline and show that on most tasks the best self-supervised models outperform supervision, confirming the recently observed trend in the literature. We find ImageNet Top-1 accuracy to be highly correlated with transfer to many-shot recognition, but increasingly less so for few-shot, object detection and dense prediction. No single self-supervised method dominates overall, suggesting that universal pre-training is still unsolved. Our analysis of features suggests that top self-supervised learners fail to preserve colour information as well as supervised alternatives, but tend to induce better classifier calibration, and less attentive overfitting than supervised learners.



### 3DSNet: Unsupervised Shape-to-Shape 3D Style Transfer
- **Arxiv ID**: http://arxiv.org/abs/2011.13388v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.13388v4)
- **Published**: 2020-11-26 16:59:12+00:00
- **Updated**: 2021-05-18 09:17:13+00:00
- **Authors**: Mattia Segu, Margarita Grinvald, Roland Siegwart, Federico Tombari
- **Comment**: None
- **Journal**: None
- **Summary**: Transferring the style from one image onto another is a popular and widely studied task in computer vision. Yet, style transfer in the 3D setting remains a largely unexplored problem. To our knowledge, we propose the first learning-based approach for style transfer between 3D objects based on disentangled content and style representations. The proposed method can synthesize new 3D shapes both in the form of point clouds and meshes, combining the content and style of a source and target 3D model to generate a novel shape that resembles in style the target while retaining the source content. Furthermore, we extend our technique to implicitly learn the multimodal style distribution of the chosen domains. By sampling style codes from the learned distributions, we increase the variety of styles that our model can confer to an input shape. Experimental results validate the effectiveness of the proposed 3D style transfer method on a number of benchmarks. The implementation of our framework will be released upon acceptance.



### Joint Reconstruction and Calibration using Regularization by Denoising
- **Arxiv ID**: http://arxiv.org/abs/2011.13391v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.13391v1)
- **Published**: 2020-11-26 17:05:56+00:00
- **Updated**: 2020-11-26 17:05:56+00:00
- **Authors**: Mingyang Xie, Yu Sun, Jiaming Liu, Brendt Wohlberg, Ulugbek S. Kamilov
- **Comment**: None
- **Journal**: None
- **Summary**: Regularization by denoising (RED) is a broadly applicable framework for solving inverse problems by using priors specified as denoisers. While RED has been shown to provide state-of-the-art performance in a number of applications, existing RED algorithms require exact knowledge of the measurement operator characterizing the imaging system, limiting their applicability in problems where the measurement operator has parametric uncertainties. We propose a new method, called Calibrated RED (Cal-RED), that enables joint calibration of the measurement operator along with reconstruction of the unknown image. Cal-RED extends the traditional RED methodology to imaging problems that require the calibration of the measurement operator. We validate Cal-RED on the problem of image reconstruction in computerized tomography (CT) under perturbed projection angles. Our results corroborate the effectiveness of Cal-RED for joint calibration and reconstruction using pre-trained deep denoisers as image priors.



### Exposing the Robustness and Vulnerability of Hybrid 8T-6T SRAM Memory Architectures to Adversarial Attacks in Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2011.13392v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.13392v1)
- **Published**: 2020-11-26 17:08:06+00:00
- **Updated**: 2020-11-26 17:08:06+00:00
- **Authors**: Abhishek Moitra, Priyadarshini Panda
- **Comment**: 11 pages, 13 figures
- **Journal**: None
- **Summary**: Deep Learning is able to solve a plethora of once impossible problems. However, they are vulnerable to input adversarial attacks preventing them from being autonomously deployed in critical applications. Several algorithm-centered works have discussed methods to cause adversarial attacks and improve adversarial robustness of a Deep Neural Network (DNN). In this work, we elicit the advantages and vulnerabilities of hybrid 6T-8T memories to improve the adversarial robustness and cause adversarial attacks on DNNs. We show that bit-error noise in hybrid memories due to erroneous 6T-SRAM cells have deterministic behaviour based on the hybrid memory configurations (V_DD, 8T-6T ratio). This controlled noise (surgical noise) can be strategically introduced into specific DNN layers to improve the adversarial accuracy of DNNs. At the same time, surgical noise can be carefully injected into the DNN parameters stored in hybrid memory to cause adversarial attacks. To improve the adversarial robustness of DNNs using surgical noise, we propose a methodology to select appropriate DNN layers and their corresponding hybrid memory configurations to introduce the required surgical noise. Using this, we achieve 2-8% higher adversarial accuracy without re-training against white-box attacks like FGSM, than the baseline models (with no surgical noise introduced). To demonstrate adversarial attacks using surgical noise, we design a novel, white-box attack on DNN parameters stored in hybrid memory banks that causes the DNN inference accuracy to drop by more than 60% with over 90% confidence value. We support our claims with experiments, performed using benchmark datasets-CIFAR10 and CIFAR100 on VGG19 and ResNet18 networks.



### Depth-Aware Action Recognition: Pose-Motion Encoding through Temporal Heatmaps
- **Arxiv ID**: http://arxiv.org/abs/2011.13399v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.13399v1)
- **Published**: 2020-11-26 17:26:42+00:00
- **Updated**: 2020-11-26 17:26:42+00:00
- **Authors**: Mattia Segu, Federico Pirovano, Gianmario Fumagalli, Amedeo Fabris
- **Comment**: None
- **Journal**: None
- **Summary**: Most state-of-the-art methods for action recognition rely only on 2D spatial features encoding appearance, motion or pose. However, 2D data lacks the depth information, which is crucial for recognizing fine-grained actions. In this paper, we propose a depth-aware volumetric descriptor that encodes pose and motion information in a unified representation for action classification in-the-wild. Our framework is robust to many challenges inherent to action recognition, e.g. variation in viewpoint, scene, clothing and body shape. The key component of our method is the Depth-Aware Pose Motion representation (DA-PoTion), a new video descriptor that encodes the 3D movement of semantic keypoints of the human body. Given a video, we produce human joint heatmaps for each frame using a state-of-the-art 3D human pose regressor and we give each of them a unique color code according to the relative time in the clip. Then, we aggregate such 3D time-encoded heatmaps for all human joints to obtain a fixed-size descriptor (DA-PoTion), which is suitable for classifying actions using a shallow 3D convolutional neural network (CNN). The DA-PoTion alone defines a new state-of-the-art on the Penn Action Dataset. Moreover, we leverage the intrinsic complementarity of our pose motion descriptor with appearance based approaches by combining it with Inflated 3D ConvNet (I3D) to define a new state-of-the-art on the JHMDB Dataset.



### Learning from Lexical Perturbations for Consistent Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/2011.13406v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.13406v2)
- **Published**: 2020-11-26 17:38:03+00:00
- **Updated**: 2020-12-23 00:29:27+00:00
- **Authors**: Spencer Whitehead, Hui Wu, Yi Ren Fung, Heng Ji, Rogerio Feris, Kate Saenko
- **Comment**: 14 pages, 8 figures
- **Journal**: None
- **Summary**: Existing Visual Question Answering (VQA) models are often fragile and sensitive to input variations. In this paper, we propose a novel approach to address this issue based on modular networks, which creates two questions related by linguistic perturbations and regularizes the visual reasoning process between them to be consistent during training. We show that our framework markedly improves consistency and generalization ability, demonstrating the value of controlled linguistic perturbations as a useful and currently underutilized training and regularization tool for VQA models. We also present VQA Perturbed Pairings (VQA P2), a new, low-cost benchmark and augmentation pipeline to create controllable linguistic variations of VQA questions. Our benchmark uniquely draws from large-scale linguistic resources, avoiding human annotation effort while maintaining data quality compared to generative approaches. We benchmark existing VQA models using VQA P2 and provide robustness analysis on each type of linguistic variation.



### Generative Layout Modeling using Constraint Graphs
- **Arxiv ID**: http://arxiv.org/abs/2011.13417v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2011.13417v1)
- **Published**: 2020-11-26 18:18:37+00:00
- **Updated**: 2020-11-26 18:18:37+00:00
- **Authors**: Wamiq Para, Paul Guerrero, Tom Kelly, Leonidas Guibas, Peter Wonka
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a new generative model for layout generation. We generate layouts in three steps. First, we generate the layout elements as nodes in a layout graph. Second, we compute constraints between layout elements as edges in the layout graph. Third, we solve for the final layout using constrained optimization. For the first two steps, we build on recent transformer architectures. The layout optimization implements the constraints efficiently. We show three practical contributions compared to the state of the art: our work requires no user input, produces higher quality layouts, and enables many novel capabilities for conditional layout generation.



### Multi-view Human Pose and Shape Estimation Using Learnable Volumetric Aggregation
- **Arxiv ID**: http://arxiv.org/abs/2011.13427v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.13427v1)
- **Published**: 2020-11-26 18:33:35+00:00
- **Updated**: 2020-11-26 18:33:35+00:00
- **Authors**: Soyong Shin, Eni Halilaj
- **Comment**: code will be released soon
- **Journal**: None
- **Summary**: Human pose and shape estimation from RGB images is a highly sought after alternative to marker-based motion capture, which is laborious, requires expensive equipment, and constrains capture to laboratory environments. Monocular vision-based algorithms, however, still suffer from rotational ambiguities and are not ready for translation in healthcare applications, where high accuracy is paramount. While fusion of data from multiple viewpoints could overcome these challenges, current algorithms require further improvement to obtain clinically acceptable accuracies. In this paper, we propose a learnable volumetric aggregation approach to reconstruct 3D human body pose and shape from calibrated multi-view images. We use a parametric representation of the human body, which makes our approach directly applicable to medical applications. Compared to previous approaches, our framework shows higher accuracy and greater promise for real-time prediction, given its cost efficiency.



### Explaining Deep Learning Models for Structured Data using Layer-Wise Relevance Propagation
- **Arxiv ID**: http://arxiv.org/abs/2011.13429v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.13429v1)
- **Published**: 2020-11-26 18:34:21+00:00
- **Updated**: 2020-11-26 18:34:21+00:00
- **Authors**: hsan Ullah, Andre Rios, Vaibhav Gala, Susan Mckeever
- **Comment**: 13 pages, 5 figures, 6 tables
- **Journal**: None
- **Summary**: Trust and credibility in machine learning models is bolstered by the ability of a model to explain itsdecisions. While explainability of deep learning models is a well-known challenge, a further chal-lenge is clarity of the explanation itself, which must be interpreted by downstream users. Layer-wiseRelevance Propagation (LRP), an established explainability technique developed for deep models incomputer vision, provides intuitive human-readable heat maps of input images. We present the novelapplication of LRP for the first time with structured datasets using a deep neural network (1D-CNN),for Credit Card Fraud detection and Telecom Customer Churn prediction datasets. We show how LRPis more effective than traditional explainability concepts of Local Interpretable Model-agnostic Ex-planations (LIME) and Shapley Additive Explanations (SHAP) for explainability. This effectivenessis both local to a sample level and holistic over the whole testing set. We also discuss the significantcomputational time advantage of LRP (1-2s) over LIME (22s) and SHAP (108s), and thus its poten-tial for real time application scenarios. In addition, our validation of LRP has highlighted features forenhancing model performance, thus opening up a new area of research of using XAI as an approachfor feature subset selection



### Fine-Grained Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2011.13475v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.13475v2)
- **Published**: 2020-11-26 21:04:17+00:00
- **Updated**: 2021-03-22 17:06:19+00:00
- **Authors**: Priyank Pathak
- **Comment**: None
- **Journal**: None
- **Summary**: Research into the task of re-identification (ReID) is picking up momentum in computer vision for its many use cases and zero-shot learning nature. This paper proposes a computationally efficient fine-grained ReID model, FGReID, which is among the first models to unify image and video ReID while keeping the number of training parameters minimal. FGReID takes advantage of video-based pre-training and spatial feature attention to improve performance on both video and image ReID tasks. FGReID achieves state-of-the-art (SOTA) on MARS, iLIDS-VID, and PRID-2011 video person ReID benchmarks. Eliminating temporal pooling yields an image ReID model that surpasses SOTA on CUHK01 and Market1501 image person ReID benchmarks. The FGReID achieves near SOTA performance on the vehicle ReID dataset VeRi as well, demonstrating its ability to generalize. Additionally we do an ablation study analyzing the key features influencing model performance on ReID tasks. Finally, we discuss the moral dilemmas related to ReID tasks, including the potential for misuse. Code for this work is publicly available at https: //github.com/ppriyank/Fine-grained-ReIdentification.



### Modelling brain lesion volume in patches with CNN-based Poisson Regression
- **Arxiv ID**: http://arxiv.org/abs/2011.13927v1
- **DOI**: 10.5220/0009102701720176
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.13927v1)
- **Published**: 2020-11-26 21:11:15+00:00
- **Updated**: 2020-11-26 21:11:15+00:00
- **Authors**: Kevin Raina
- **Comment**: None
- **Journal**: In BIOIMAGING (pp. 172-176) 2020
- **Summary**: Monitoring the progression of lesions is important for clinical response. Summary statistics such as lesion volume are objective and easy to interpret, which can help clinicians assess lesion growth or decay. CNNs are commonly used in medical image segmentation for their ability to produce useful features within large contexts and their associated efficient iterative patch-based training. Many CNN architectures require hundreds of thousands parameters to yield a good segmentation. In this work, an efficient, computationally inexpensive CNN is implemented to estimate the number of lesion voxels in a predefined patch size from magnetic resonance (MR) images. The output of the CNN is interpreted as the conditional Poisson parameter over the patch, allowing standard mini-batch gradient descent to be employed. The ISLES2015 (SISS) data is used to train and evaluate the model, which by estimating lesion volume from raw features, accurately identified the lesion image with the larger lesion volume for 86% of paired sample patches. An argument for the development and use of estimating lesion volumes to also aid in model selection for segmentation is made.



### Bidirectional Modeling and Analysis of Brain Aging with Normalizing Flows
- **Arxiv ID**: http://arxiv.org/abs/2011.13484v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.13484v1)
- **Published**: 2020-11-26 22:23:48+00:00
- **Updated**: 2020-11-26 22:23:48+00:00
- **Authors**: Matthias Wilms, Jordan J. Bannister, Pauline Mouches, M. Ethan MacDonald, Deepthi Rajashekar, S√∂nke Langner, Nils D. Forkert
- **Comment**: Presented at 3rd International Workshop on Machine Learning in
  Clinical Neuroimaging - in conjunction with MICCAI 2020
- **Journal**: None
- **Summary**: Brain aging is a widely studied longitudinal process throughout which the brain undergoes considerable morphological changes and various machine learning approaches have been proposed to analyze it. Within this context, brain age prediction from structural MR images and age-specific brain morphology template generation are two problems that have attracted much attention. While most approaches tackle these tasks independently, we assume that they are inverse directions of the same functional bidirectional relationship between a brain's morphology and an age variable. In this paper, we propose to model this relationship with a single conditional normalizing flow, which unifies brain age prediction and age-conditioned generative modeling in a novel way. In an initial evaluation of this idea, we show that our normalizing flow brain aging model can accurately predict brain age while also being able to generate age-specific brain morphology templates that realistically represent the typical aging trend in a healthy population. This work is a step towards unified modeling of functional relationships between 3D brain morphology and clinical variables of interest with powerful normalizing flows.



### Neural-Pull: Learning Signed Distance Functions from Point Clouds by Learning to Pull Space onto Surfaces
- **Arxiv ID**: http://arxiv.org/abs/2011.13495v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.13495v2)
- **Published**: 2020-11-26 23:18:10+00:00
- **Updated**: 2021-05-23 17:54:34+00:00
- **Authors**: Baorui Ma, Zhizhong Han, Yu-Shen Liu, Matthias Zwicker
- **Comment**: To appear at ICML2021. Code and data are available at
  https://github.com/mabaorui/NeuralPull
- **Journal**: None
- **Summary**: Reconstructing continuous surfaces from 3D point clouds is a fundamental operation in 3D geometry processing. Several recent state-of-the-art methods address this problem using neural networks to learn signed distance functions (SDFs). In this paper, we introduce \textit{Neural-Pull}, a new approach that is simple and leads to high quality SDFs. Specifically, we train a neural network to pull query 3D locations to their closest points on the surface using the predicted signed distance values and the gradient at the query locations, both of which are computed by the network itself. The pulling operation moves each query location with a stride given by the distance predicted by the network. Based on the sign of the distance, this may move the query location along or against the direction of the gradient of the SDF. This is a differentiable operation that allows us to update the signed distance value and the gradient simultaneously during training. Our outperforming results under widely used benchmarks demonstrate that we can learn SDFs more accurately and flexibly for surface reconstruction and single image reconstruction than the state-of-the-art methods.



### Image Denoising for Strong Gaussian Noises With Specialized CNNs for Different Frequency Components
- **Arxiv ID**: http://arxiv.org/abs/2011.14908v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.14908v1)
- **Published**: 2020-11-26 23:20:25+00:00
- **Updated**: 2020-11-26 23:20:25+00:00
- **Authors**: Seyed Mohsen Hosseini
- **Comment**: None
- **Journal**: None
- **Summary**: In machine learning approach to image denoising a network is trained to recover a clean image from a noisy one. In this paper a novel structure is proposed based on training multiple specialized networks as opposed to existing structures that are base on a single network. The proposed model is an alternative for training a very deep network to avoid issues like vanishing or exploding gradient. By dividing a very deep network into two smaller networks the same number of learnable parameters will be available, but two smaller networks should be trained which are easier to train. Over smoothing and waxy artifacts are major problems with existing methods; because the network tries to keep the Mean Square Error (MSE) low for general structures and details, which leads to overlooking of details. This problem is more severe in the presence of strong noise. To reduce this problem, in the proposed structure, the image is decomposed into its low and high frequency components and each component is used to train a separate denoising convolutional neural network. One network is specialized to reconstruct the general structure of the image and the other one is specialized to reconstruct the details. Results of the proposed method show higher peak signal to noise ratio (PSNR), and structural similarity index (SSIM) compared to a popular state of the art denoising method in the presence of strong noises.



