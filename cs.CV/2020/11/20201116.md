# Arxiv Papers in cs.CV on 2020-11-16
### A Large-Scale Database for Graph Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2011.07682v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CR, cs.CV, cs.SI
- **Links**: [PDF](http://arxiv.org/pdf/2011.07682v3)
- **Published**: 2020-11-16 01:50:21+00:00
- **Updated**: 2021-11-07 01:48:30+00:00
- **Authors**: Scott Freitas, Yuxiao Dong, Joshua Neil, Duen Horng Chau
- **Comment**: Published in NeurIPS Datasets and Benchmarks Track, 2021
- **Journal**: None
- **Summary**: With the rapid emergence of graph representation learning, the construction of new large-scale datasets is necessary to distinguish model capabilities and accurately assess the strengths and weaknesses of each technique. By carefully analyzing existing graph databases, we identify 3 critical components important for advancing the field of graph representation learning: (1) large graphs, (2) many graphs, and (3) class diversity. To date, no single graph database offers all these desired properties. We introduce MalNet, the largest public graph database ever constructed, representing a large-scale ontology of malicious software function call graphs. MalNet contains over 1.2 million graphs, averaging over 15k nodes and 35k edges per graph, across a hierarchy of 47 types and 696 families. Compared to the popular REDDIT-12K database, MalNet offers 105x more graphs, 39x larger graphs on average, and 63x more classes. We provide a detailed analysis of MalNet, discussing its properties and provenance, along with the evaluation of state-of-the-art machine learning and graph neural network techniques. The unprecedented scale and diversity of MalNet offers exciting opportunities to advance the frontiers of graph representation learning--enabling new discoveries and research into imbalanced classification, explainability and the impact of class hardness. The database is publicly available at www.mal-net.org.



### Drone LAMS: A Drone-based Face Detection Dataset with Large Angles and Many Scenarios
- **Arxiv ID**: http://arxiv.org/abs/2011.07689v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, 68T45 (Primary), 68T07 (Secondary), I.4; I.5
- **Links**: [PDF](http://arxiv.org/pdf/2011.07689v2)
- **Published**: 2020-11-16 02:26:05+00:00
- **Updated**: 2021-10-10 08:12:14+00:00
- **Authors**: Yi Luo, Siyi Chen, X. -G. Ma
- **Comment**: 10 pages, 6 figures,conference or other essential info
- **Journal**: None
- **Summary**: This work presented a new drone-based face detection dataset Drone LAMS in order to solve issues of low performance of drone-based face detection in scenarios such as large angles which was a predominant working condition when a drone flies high. The proposed dataset captured images from 261 videos with over 43k annotations and 4.0k images with pitch or yaw angle in the range of -90{\deg} to 90{\deg}. Drone LAMS showed significant improvement over currently available drone-based face detection datasets in terms of detection performance, especially with large pitch and yaw angle. Detailed analysis of how key factors, such as duplication rate, annotation method, etc., impact dataset performance was also provided to facilitate further usage of a drone on face detection.



### Ensemble of Models Trained by Key-based Transformed Images for Adversarially Robust Defense Against Black-box Attacks
- **Arxiv ID**: http://arxiv.org/abs/2011.07697v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.07697v1)
- **Published**: 2020-11-16 02:48:37+00:00
- **Updated**: 2020-11-16 02:48:37+00:00
- **Authors**: MaungMaung AprilPyone, Hitoshi Kiya
- **Comment**: Under review
- **Journal**: None
- **Summary**: We propose a voting ensemble of models trained by using block-wise transformed images with secret keys for an adversarially robust defense. Key-based adversarial defenses were demonstrated to outperform state-of-the-art defenses against gradient-based (white-box) attacks. However, the key-based defenses are not effective enough against gradient-free (black-box) attacks without requiring any secret keys. Accordingly, we aim to enhance robustness against black-box attacks by using a voting ensemble of models. In the proposed ensemble, a number of models are trained by using images transformed with different keys and block sizes, and then a voting ensemble is applied to the models. In image classification experiments, the proposed defense is demonstrated to defend state-of-the-art attacks. The proposed defense achieves a clean accuracy of 95.56 % and an attack success rate of less than 9 % under attacks with a noise distance of 8/255 on the CIFAR-10 dataset.



### Multi-view Sensor Fusion by Integrating Model-based Estimation and Graph Learning for Collaborative Object Localization
- **Arxiv ID**: http://arxiv.org/abs/2011.07704v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2011.07704v2)
- **Published**: 2020-11-16 03:33:28+00:00
- **Updated**: 2021-03-07 01:31:36+00:00
- **Authors**: Peng Gao, Rui Guo, Hongsheng Lu, Hao Zhang
- **Comment**: Revise several typos and change the Fig2 to be more illustrative
- **Journal**: None
- **Summary**: Collaborative object localization aims to collaboratively estimate locations of objects observed from multiple views or perspectives, which is a critical ability for multi-agent systems such as connected vehicles. To enable collaborative localization, several model-based state estimation and learning-based localization methods have been developed. Given their encouraging performance, model-based state estimation often lacks the ability to model the complex relationships among multiple objects, while learning-based methods are typically not able to fuse the observations from an arbitrary number of views and cannot well model uncertainty. In this paper, we introduce a novel spatiotemporal graph filter approach that integrates graph learning and model-based estimation to perform multi-view sensor fusion for collaborative object localization. Our approach models complex object relationships using a new spatiotemporal graph representation and fuses multi-view observations in a Bayesian fashion to improve location estimation under uncertainty. We evaluate our approach in the applications of connected autonomous driving and multiple pedestrian localization. Experimental results show that our approach outperforms previous techniques and achieves the state-of-the-art performance on collaboration localization.



### Mode Penalty Generative Adversarial Network with adapted Auto-encoder
- **Arxiv ID**: http://arxiv.org/abs/2011.07706v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2011.07706v1)
- **Published**: 2020-11-16 03:39:53+00:00
- **Updated**: 2020-11-16 03:39:53+00:00
- **Authors**: Gahye Lee, Seungkyu Lee
- **Comment**: None
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GAN) are trained to generate sample images of interest distribution. To this end, generator network of GAN learns implicit distribution of real data set from the classification with candidate generated samples. Recently, various GANs have suggested novel ideas for stable optimizing of its networks. However, in real implementation, sometimes they still represent a only narrow part of true distribution or fail to converge. We assume this ill posed problem comes from poor gradient from objective function of discriminator, which easily trap the generator in a bad situation. To address this problem, we propose a mode penalty GAN combined with pre-trained auto encoder for explicit representation of generated and real data samples in the encoded space. In this space, we make a generator manifold to follow a real manifold by finding entire modes of target distribution. In addition, penalty for uncovered modes of target distribution is given to the generator which encourages it to find overall target distribution. We demonstrate that applying the proposed method to GANs helps generator's optimization becoming more stable and having faster convergence through experimental evaluations.



### DARE: AI-based Diver Action Recognition System using Multi-Channel CNNs for AUV Supervision
- **Arxiv ID**: http://arxiv.org/abs/2011.07713v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.HC, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2011.07713v1)
- **Published**: 2020-11-16 04:05:32+00:00
- **Updated**: 2020-11-16 04:05:32+00:00
- **Authors**: Jing Yang, James P. Wilson, Shalabh Gupta
- **Comment**: None
- **Journal**: None
- **Summary**: With the growth of sensing, control and robotic technologies, autonomous underwater vehicles (AUVs) have become useful assistants to human divers for performing various underwater operations. In the current practice, the divers are required to carry expensive, bulky, and waterproof keyboards or joystick-based controllers for supervision and control of AUVs. Therefore, diver action-based supervision is becoming increasingly popular because it is convenient, easier to use, faster, and cost effective. However, the various environmental, diver and sensing uncertainties present underwater makes it challenging to train a robust and reliable diver action recognition system. In this regard, this paper presents DARE, a diver action recognition system, that is trained based on Cognitive Autonomous Driving Buddy (CADDY) dataset, which is a rich set of data containing images of different diver gestures and poses in several different and realistic underwater environments. DARE is based on fusion of stereo-pairs of camera images using a multi-channel convolutional neural network supported with a systematically trained tree-topological deep neural network classifier to enhance the classification performance. DARE is fast and requires only a few milliseconds to classify one stereo-pair, thus making it suitable for real-time underwater implementation. DARE is comparatively evaluated against several existing classifier architectures and the results show that DARE supersedes the performance of all classifiers for diver action recognition in terms of overall as well as individual class accuracies and F1-scores.



### Gram Regularization for Multi-view 3D Shape Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2011.07733v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.07733v1)
- **Published**: 2020-11-16 05:37:24+00:00
- **Updated**: 2020-11-16 05:37:24+00:00
- **Authors**: Zhaoqun Li
- **Comment**: None
- **Journal**: None
- **Summary**: How to obtain the desirable representation of a 3D shape is a key challenge in 3D shape retrieval task. Most existing 3D shape retrieval methods focus on capturing shape representation with different neural network architectures, while the learning ability of each layer in the network is neglected. A common and tough issue that limits the capacity of the network is overfitting. To tackle this, L2 regularization is applied widely in existing deep learning frameworks. However,the effect on the generalization ability with L2 regularization is limited as it only controls large value in parameters. To make up the gap, in this paper, we propose a novel regularization term called Gram regularization which reinforces the learning ability of the network by encouraging the weight kernels to extract different information on the corresponding feature map. By forcing the variance between weight kernels to be large, the regularizer can help to extract discriminative features. The proposed Gram regularization is data independent and can converge stably and quickly without bells and whistles. Moreover, it can be easily plugged into existing off-the-shelf architectures. Extensive experimental results on the popular 3D object retrieval benchmark ModelNet demonstrate the effectiveness of our method.



### iPerceive: Applying Common-Sense Reasoning to Multi-Modal Dense Video Captioning and Video Question Answering
- **Arxiv ID**: http://arxiv.org/abs/2011.07735v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.MM, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2011.07735v1)
- **Published**: 2020-11-16 05:44:45+00:00
- **Updated**: 2020-11-16 05:44:45+00:00
- **Authors**: Aman Chadha, Gurneet Arora, Navpreet Kaloty
- **Comment**: 13 pages, 6 figures, 4 tables, Project Page:
  https://iperceive.amanchadha.com
- **Journal**: IEEE Winter Conference on Applications of Computer Vision (WACV)
  2021
- **Summary**: Most prior art in visual understanding relies solely on analyzing the "what" (e.g., event recognition) and "where" (e.g., event localization), which in some cases, fails to describe correct contextual relationships between events or leads to incorrect underlying visual attention. Part of what defines us as human and fundamentally different from machines is our instinct to seek causality behind any association, say an event Y that happened as a direct result of event X. To this end, we propose iPerceive, a framework capable of understanding the "why" between events in a video by building a common-sense knowledge base using contextual cues to infer causal relationships between objects in the video. We demonstrate the effectiveness of our technique using the dense video captioning (DVC) and video question answering (VideoQA) tasks. Furthermore, while most prior work in DVC and VideoQA relies solely on visual information, other modalities such as audio and speech are vital for a human observer's perception of an environment. We formulate DVC and VideoQA tasks as machine translation problems that utilize multiple modalities. By evaluating the performance of iPerceive DVC and iPerceive VideoQA on the ActivityNet Captions and TVQA datasets respectively, we show that our approach furthers the state-of-the-art. Code and samples are available at: iperceive.amanchadha.com.



### Application of Computer Vision Techniques for Segregation of PlasticWaste based on Resin Identification Code
- **Arxiv ID**: http://arxiv.org/abs/2011.07747v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.07747v1)
- **Published**: 2020-11-16 06:50:32+00:00
- **Updated**: 2020-11-16 06:50:32+00:00
- **Authors**: Shivaank Agarwal, Ravindra Gudi, Paresh Saxena
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents methods to identify the plastic waste based on its resin identification code to provide an efficient recycling of post-consumer plastic waste. We propose the design, training and testing of different machine learning techniques to (i) identify a plastic waste that belongs to the known categories of plastic waste when the system is trained and (ii) identify a new plastic waste that do not belong the any known categories of plastic waste while the system is trained. For the first case,we propose the use of one-shot learning techniques using Siamese and Triplet loss networks. Our proposed approach does not require any augmentation to increase the size of the database and achieved a high accuracy of 99.74%. For the second case, we propose the use of supervised and unsupervised dimensionality reduction techniques and achieved an accuracy of 95% to correctly identify a new plastic waste.



### Fast Uncertainty Quantification for Deep Object Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2011.07748v3
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.07748v3)
- **Published**: 2020-11-16 06:51:55+00:00
- **Updated**: 2021-03-26 05:13:32+00:00
- **Authors**: Guanya Shi, Yifeng Zhu, Jonathan Tremblay, Stan Birchfield, Fabio Ramos, Animashree Anandkumar, Yuke Zhu
- **Comment**: Video and code are available at https://sites.google.com/view/fastuq
- **Journal**: International Conferenceon Robotics and Automation (ICRA), 2021
- **Summary**: Deep learning-based object pose estimators are often unreliable and overconfident especially when the input image is outside the training domain, for instance, with sim2real transfer. Efficient and robust uncertainty quantification (UQ) in pose estimators is critically needed in many robotic tasks. In this work, we propose a simple, efficient, and plug-and-play UQ method for 6-DoF object pose estimation. We ensemble 2-3 pre-trained models with different neural network architectures and/or training data sources, and compute their average pairwise disagreement against one another to obtain the uncertainty quantification. We propose four disagreement metrics, including a learned metric, and show that the average distance (ADD) is the best learning-free metric and it is only slightly worse than the learned metric, which requires labeled target data. Our method has several advantages compared to the prior art: 1) our method does not require any modification of the training process or the model inputs; and 2) it needs only one forward pass for each model. We evaluate the proposed UQ method on three tasks where our uncertainty quantification yields much stronger correlations with pose estimation errors than the baselines. Moreover, in a real robot grasping task, our method increases the grasping success rate from 35% to 90%.



### Online Monitoring of Object Detection Performance During Deployment
- **Arxiv ID**: http://arxiv.org/abs/2011.07750v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.07750v2)
- **Published**: 2020-11-16 07:01:43+00:00
- **Updated**: 2021-03-09 06:20:05+00:00
- **Authors**: Quazi Marufur Rahman, Niko Sünderhauf, Feras Dayoub
- **Comment**: V2 with more experimental results and improved clarity of
  presentation
- **Journal**: None
- **Summary**: During deployment, an object detector is expected to operate at a similar performance level reported on its testing dataset. However, when deployed onboard mobile robots that operate under varying and complex environmental conditions, the detector's performance can fluctuate and occasionally degrade severely without warning. Undetected, this can lead the robot to take unsafe and risky actions based on low-quality and unreliable object detections. We address this problem and introduce a cascaded neural network that monitors the performance of the object detector by predicting the quality of its mean average precision (mAP) on a sliding window of the input frames. The proposed cascaded network exploits the internal features from the deep neural network of the object detector. We evaluate our proposed approach using different combinations of autonomous driving datasets and object detectors.



### Zero Cost Improvements for General Object Detection Network
- **Arxiv ID**: http://arxiv.org/abs/2011.07756v2
- **DOI**: 10.1109/CCDC52312.2021.9602729
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.07756v2)
- **Published**: 2020-11-16 07:21:57+00:00
- **Updated**: 2021-11-15 17:24:01+00:00
- **Authors**: Shaohua Wang, Yaping Dai
- **Comment**: None
- **Journal**: 2021 33rd Chinese Control and Decision Conference (CCDC)
- **Summary**: Modern object detection networks pursuit higher precision on general object detection datasets, at the same time the computation burden is also increasing along with the improvement of precision. Nevertheless, the inference time and precision are both critical to object detection system which needs to be real-time. It is necessary to research precision improvement without extra computation cost. In this work, two modules are proposed to improve detection precision with zero cost, which are focus on FPN and detection head improvement for general object detection networks. We employ the scale attention mechanism to efficiently fuse multi-level feature maps with less parameters, which is called SA-FPN module. Considering the correlation of classification head and regression head, we use sequential head to take the place of widely-used parallel head, which is called Seq-HEAD module. To evaluate the effectiveness, we apply the two modules to some modern state-of-art object detection networks, including anchor-based and anchor-free. Experiment results on coco dataset show that the networks with the two modules can surpass original networks by 1.1 AP and 0.8 AP with zero cost for anchor-based and anchor-free networks, respectively. Code will be available at https://git.io/JTFGl.



### DSIC: Dynamic Sample-Individualized Connector for Multi-Scale Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2011.07774v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.07774v2)
- **Published**: 2020-11-16 08:13:58+00:00
- **Updated**: 2021-03-25 02:14:16+00:00
- **Authors**: Zekun Li, Yufan Liu, Bing Li, Weiming Hu
- **Comment**: None
- **Journal**: None
- **Summary**: Although object detection has reached a milestone thanks to the great success of deep learning, the scale variation is still the key challenge. Integrating multi-level features is presented to alleviate the problems, like the classic Feature Pyramid Network (FPN) and its improvements. However, the specifically designed feature integration modules of these methods may not have the optimal architecture for feature fusion. Moreover, these models have fixed architectures and data flow paths, when fed with various samples. They cannot adjust and be compatible with each kind of data. To overcome the above limitations, we propose a Dynamic Sample-Individualized Connector (DSIC) for multi-scale object detection. It dynamically adjusts network connections to fit different samples. In particular, DSIC consists of two components: Intra-scale Selection Gate (ISG) and Cross-scale Selection Gate (CSG). ISG adaptively extracts multi-level features from backbone as the input of feature integration. CSG automatically activate informative data flow paths based on the multi-level features. Furthermore, these two components are both plug-and-play and can be embedded in any backbone. Experimental results demonstrate that the proposed method outperforms the state-of-the-arts.



### Robust Facial Landmark Detection by Cross-order Cross-semantic Deep Network
- **Arxiv ID**: http://arxiv.org/abs/2011.07777v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.07777v1)
- **Published**: 2020-11-16 08:19:26+00:00
- **Updated**: 2020-11-16 08:19:26+00:00
- **Authors**: Jun Wan, Zhihui Lai, Linlin Shen, Jie Zhou, Can Gao, Gang Xiao, Xianxu Hou
- **Comment**: This paper has been accepted by Neural Networks, November 2020
- **Journal**: None
- **Summary**: Recently, convolutional neural networks (CNNs)-based facial landmark detection methods have achieved great success. However, most of existing CNN-based facial landmark detection methods have not attempted to activate multiple correlated facial parts and learn different semantic features from them that they can not accurately model the relationships among the local details and can not fully explore more discriminative and fine semantic features, thus they suffer from partial occlusions and large pose variations. To address these problems, we propose a cross-order cross-semantic deep network (CCDN) to boost the semantic features learning for robust facial landmark detection. Specifically, a cross-order two-squeeze multi-excitation (CTM) module is proposed to introduce the cross-order channel correlations for more discriminative representations learning and multiple attention-specific part activation. Moreover, a novel cross-order cross-semantic (COCS) regularizer is designed to drive the network to learn cross-order cross-semantic features from different activation for facial landmark detection. It is interesting to show that by integrating the CTM module and COCS regularizer, the proposed CCDN can effectively activate and learn more fine and complementary cross-order cross-semantic features to improve the accuracy of facial landmark detection under extremely challenging scenarios. Experimental results on challenging benchmark datasets demonstrate the superiority of our CCDN over state-of-the-art facial landmark detection methods.



### Manual-Label Free 3D Detection via An Open-Source Simulator
- **Arxiv ID**: http://arxiv.org/abs/2011.07784v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.07784v1)
- **Published**: 2020-11-16 08:29:01+00:00
- **Updated**: 2020-11-16 08:29:01+00:00
- **Authors**: Zhen Yang, Chi Zhang, Huiming Guo, Zhaoxiang Zhang
- **Comment**: 8 pages, 4 figures, ICPR2020
- **Journal**: None
- **Summary**: LiDAR based 3D object detectors typically need a large amount of detailed-labeled point cloud data for training, but these detailed labels are commonly expensive to acquire. In this paper, we propose a manual-label free 3D detection algorithm that leverages the CARLA simulator to generate a large amount of self-labeled training samples and introduces a novel Domain Adaptive VoxelNet (DA-VoxelNet) that can cross the distribution gap from the synthetic data to the real scenario. The self-labeled training samples are generated by a set of high quality 3D models embedded in a CARLA simulator and a proposed LiDAR-guided sampling algorithm. Then a DA-VoxelNet that integrates both a sample-level DA module and an anchor-level DA module is proposed to enable the detector trained by the synthetic data to adapt to real scenario. Experimental results show that the proposed unsupervised DA 3D detector on KITTI evaluation set can achieve 76.66% and 56.64% mAP on BEV mode and 3D mode respectively. The results reveal a promising perspective of training a LIDAR-based 3D detector without any hand-tagged label.



### JOLO-GCN: Mining Joint-Centered Light-Weight Information for Skeleton-Based Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2011.07787v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.07787v1)
- **Published**: 2020-11-16 08:39:22+00:00
- **Updated**: 2020-11-16 08:39:22+00:00
- **Authors**: Jinmiao Cai, Nianjuan Jiang, Xiaoguang Han, Kui Jia, Jiangbo Lu
- **Comment**: Accepted at IEEE WACV 2021
- **Journal**: None
- **Summary**: Skeleton-based action recognition has attracted research attentions in recent years. One common drawback in currently popular skeleton-based human action recognition methods is that the sparse skeleton information alone is not sufficient to fully characterize human motion. This limitation makes several existing methods incapable of correctly classifying action categories which exhibit only subtle motion differences. In this paper, we propose a novel framework for employing human pose skeleton and joint-centered light-weight information jointly in a two-stream graph convolutional network, namely, JOLO-GCN. Specifically, we use Joint-aligned optical Flow Patches (JFP) to capture the local subtle motion around each joint as the pivotal joint-centered visual information. Compared to the pure skeleton-based baseline, this hybrid scheme effectively boosts performance, while keeping the computational and memory overheads low. Experiments on the NTU RGB+D, NTU RGB+D 120, and the Kinetics-Skeleton dataset demonstrate clear accuracy improvements attained by the proposed method over the state-of-the-art skeleton-based methods.



### Training Strategies and Data Augmentations in CNN-based DeepFake Video Detection
- **Arxiv ID**: http://arxiv.org/abs/2011.07792v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2011.07792v1)
- **Published**: 2020-11-16 08:50:56+00:00
- **Updated**: 2020-11-16 08:50:56+00:00
- **Authors**: Luca Bondi, Edoardo Daniele Cannas, Paolo Bestagini, Stefano Tubaro
- **Comment**: None
- **Journal**: None
- **Summary**: The fast and continuous growth in number and quality of deepfake videos calls for the development of reliable detection systems capable of automatically warning users on social media and on the Internet about the potential untruthfulness of such contents. While algorithms, software, and smartphone apps are getting better every day in generating manipulated videos and swapping faces, the accuracy of automated systems for face forgery detection in videos is still quite limited and generally biased toward the dataset used to design and train a specific detection system. In this paper we analyze how different training strategies and data augmentation techniques affect CNN-based deepfake detectors when training and testing on the same dataset or across different datasets.



### Deep learning in magnetic resonance prostate segmentation: A review and a new perspective
- **Arxiv ID**: http://arxiv.org/abs/2011.07795v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.07795v1)
- **Published**: 2020-11-16 08:58:38+00:00
- **Updated**: 2020-11-16 08:58:38+00:00
- **Authors**: David Gillespie, Connah Kendrick, Ian Boon, Cheng Boon, Tim Rattay, Moi Hoon Yap
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Prostate radiotherapy is a well established curative oncology modality, which in future will use Magnetic Resonance Imaging (MRI)-based radiotherapy for daily adaptive radiotherapy target definition. However the time needed to delineate the prostate from MRI data accurately is a time consuming process. Deep learning has been identified as a potential new technology for the delivery of precision radiotherapy in prostate cancer, where accurate prostate segmentation helps in cancer detection and therapy. However, the trained models can be limited in their application to clinical setting due to different acquisition protocols, limited publicly available datasets, where the size of the datasets are relatively small. Therefore, to explore the field of prostate segmentation and to discover a generalisable solution, we review the state-of-the-art deep learning algorithms in MR prostate segmentation; provide insights to the field by discussing their limitations and strengths; and propose an optimised 2D U-Net for MR prostate segmentation. We evaluate the performance on four publicly available datasets using Dice Similarity Coefficient (DSC) as performance metric. Our experiments include within dataset evaluation and cross-dataset evaluation. The best result is achieved by composite evaluation (DSC of 0.9427 on Decathlon test set) and the poorest result is achieved by cross-dataset evaluation (DSC of 0.5892, Prostate X training set, Promise 12 testing set). We outline the challenges and provide recommendations for future work. Our research provides a new perspective to MR prostate segmentation and more importantly, we provide standardised experiment settings for researchers to evaluate their algorithms. Our code is available at https://github.com/AIEMMU/MRI\_Prostate.



### An End-to-end Method for Producing Scanning-robust Stylized QR Codes
- **Arxiv ID**: http://arxiv.org/abs/2011.07815v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2011.07815v1)
- **Published**: 2020-11-16 09:38:27+00:00
- **Updated**: 2020-11-16 09:38:27+00:00
- **Authors**: Hao Su, Jianwei Niu, Xuefeng Liu, Qingfeng Li, Ji Wan, Mingliang Xu, Tao Ren
- **Comment**: 11 pages, 16 figures
- **Journal**: None
- **Summary**: Quick Response (QR) code is one of the most worldwide used two-dimensional codes.~Traditional QR codes appear as random collections of black-and-white modules that lack visual semantics and aesthetic elements, which inspires the recent works to beautify the appearances of QR codes. However, these works adopt fixed generation algorithms and therefore can only generate QR codes with a pre-defined style. In this paper, combining the Neural Style Transfer technique, we propose a novel end-to-end method, named ArtCoder, to generate the stylized QR codes that are personalized, diverse, attractive, and scanning-robust.~To guarantee that the generated stylized QR codes are still scanning-robust, we propose a Sampling-Simulation layer, a module-based code loss, and a competition mechanism. The experimental results show that our stylized QR codes have high-quality in both the visual effect and the scanning-robustness, and they are able to support the real-world application.



### Multi-Decoder Networks with Multi-Denoising Inputs for Tumor Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2012.03684v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2012.03684v1)
- **Published**: 2020-11-16 12:58:03+00:00
- **Updated**: 2020-11-16 12:58:03+00:00
- **Authors**: Minh H. Vu, Tufve Nyholm, Tommy Löfstedt
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic segmentation of brain glioma from multimodal MRI scans plays a key role in clinical trials and practice. Unfortunately, manual segmentation is very challenging, time-consuming, costly, and often inaccurate despite human expertise due to the high variance and high uncertainty in the human annotations. In the present work, we develop an end-to-end deep-learning-based segmentation method using a multi-decoder architecture by jointly learning three separate sub-problems using a partly shared encoder. We also propose to apply smoothing methods to the input images to generate denoised versions as additional inputs to the network. The validation performance indicate an improvement when using the proposed method. The proposed method was ranked 2nd in the task of Quantification of Uncertainty in Segmentation in the Brain Tumors in Multimodal Magnetic Resonance Imaging Challenge 2020.



### LAP-Net: Adaptive Features Sampling via Learning Action Progression for Online Action Detection
- **Arxiv ID**: http://arxiv.org/abs/2011.07915v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.07915v1)
- **Published**: 2020-11-16 13:08:47+00:00
- **Updated**: 2020-11-16 13:08:47+00:00
- **Authors**: Sanqing Qu, Guang Chen, Dan Xu, Jinhu Dong, Fan Lu, Alois Knoll
- **Comment**: None
- **Journal**: None
- **Summary**: Online action detection is a task with the aim of identifying ongoing actions from streaming videos without any side information or access to future frames. Recent methods proposed to aggregate fixed temporal ranges of invisible but anticipated future frames representations as supplementary features and achieved promising performance. They are based on the observation that human beings often detect ongoing actions by contemplating the future vision simultaneously. However, we observed that at different action progressions, the optimal supplementary features should be obtained from distinct temporal ranges instead of simply fixed future temporal ranges. To this end, we introduce an adaptive features sampling strategy to overcome the mentioned variable-ranges of optimal supplementary features. Specifically, in this paper, we propose a novel Learning Action Progression Network termed LAP-Net, which integrates an adaptive features sampling strategy. At each time step, this sampling strategy first estimates current action progression and then decide what temporal ranges should be used to aggregate the optimal supplementary features. We evaluated our LAP-Net on three benchmark datasets, TVSeries, THUMOS-14 and HDD. The extensive experiments demonstrate that with our adaptive feature sampling strategy, the proposed LAP-Net can significantly outperform current state-of-the-art methods with a large margin.



### Deep Learning -- A first Meta-Survey of selected Reviews across Scientific Disciplines, their Commonalities, Challenges and Research Impact
- **Arxiv ID**: http://arxiv.org/abs/2011.08184v2
- **DOI**: 10.7717/peerj-cs.773
- **Categories**: **cs.DL**, cs.CV, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2011.08184v2)
- **Published**: 2020-11-16 13:14:18+00:00
- **Updated**: 2021-11-17 12:40:20+00:00
- **Authors**: Jan Egger, Antonio Pepe, Christina Gsaxner, Yuan Jin, Jianning Li, Roman Kern
- **Comment**: 83 pages, 22 figures, 9 tables, 100 references
- **Journal**: PeerJ Computer Science 7:e773, 2021
- **Summary**: Deep learning belongs to the field of artificial intelligence, where machines perform tasks that typically require some kind of human intelligence. Similar to the basic structure of a brain, a deep learning algorithm consists of an artificial neural network, which resembles the biological brain structure. Mimicking the learning process of humans with their senses, deep learning networks are fed with (sensory) data, like texts, images, videos or sounds. These networks outperform the state-of-the-art methods in different tasks and, because of this, the whole field saw an exponential growth during the last years. This growth resulted in way over 10,000 publications per year in the last years. For example, the search engine PubMed alone, which covers only a sub-set of all publications in the medical field, provides already over 11,000 results in Q3 2020 for the search term 'deep learning', and around 90% of these results are from the last three years. Consequently, a complete overview over the field of deep learning is already impossible to obtain and, in the near future, it will potentially become difficult to obtain an overview over a subfield. However, there are several review articles about deep learning, which are focused on specific scientific fields or applications, for example deep learning advances in computer vision or in specific tasks like object detection. With these surveys as a foundation, the aim of this contribution is to provide a first high-level, categorized meta-survey of selected reviews on deep learning across different scientific disciplines. The categories (computer vision, language processing, medical informatics and additional works) have been chosen according to the underlying data sources (image, language, medical, mixed). In addition, we review the common architectures, methods, pros, cons, evaluations, challenges and future directions for every sub-category.



### Hierarchical Complementary Learning for Weakly Supervised Object Localization
- **Arxiv ID**: http://arxiv.org/abs/2011.08014v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.08014v1)
- **Published**: 2020-11-16 14:58:51+00:00
- **Updated**: 2020-11-16 14:58:51+00:00
- **Authors**: Sabrina Narimene Benassou, Wuzhen Shi, Feng Jiang, Abdallah Benzine
- **Comment**: None
- **Journal**: None
- **Summary**: Weakly supervised object localization (WSOL) is a challenging problem which aims to localize objects with only image-level labels. Due to the lack of ground truth bounding boxes, class labels are mainly employed to train the model. This model generates a class activation map (CAM) which activates the most discriminate features. However, the main drawback of CAM is the ability to detect just a part of the object. To solve this problem, some researchers have removed parts from the detected object \cite{b1, b2, b4}, or the image \cite{b3}. The aim of removing parts from image or detected parts of the object is to force the model to detect the other features. However, these methods require one or many hyper-parameters to erase the appropriate pixels on the image, which could involve a loss of information. In contrast, this paper proposes a Hierarchical Complementary Learning Network method (HCLNet) that helps the CNN to perform better classification and localization of objects on the images. HCLNet uses a complementary map to force the network to detect the other parts of the object. Unlike previous works, this method does not need any extras hyper-parameters to generate different CAMs, as well as does not introduce a big loss of information. In order to fuse these different maps, two different fusion strategies known as the addition strategy and the l1-norm strategy have been used. These strategies allowed to detect the whole object while excluding the background. Extensive experiments show that HCLNet obtains better performance than state-of-the-art methods.



### High-level Prior-based Loss Functions for Medical Image Segmentation: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2011.08018v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.08018v2)
- **Published**: 2020-11-16 15:12:05+00:00
- **Updated**: 2020-11-22 22:16:52+00:00
- **Authors**: Rosana El Jurdi, Caroline Petitjean, Paul Honeine, Veronika Cheplygina, Fahed Abdallah
- **Comment**: None
- **Journal**: None
- **Summary**: Today, deep convolutional neural networks (CNNs) have demonstrated state of the art performance for supervised medical image segmentation, across various imaging modalities and tasks. Despite early success, segmentation networks may still generate anatomically aberrant segmentations, with holes or inaccuracies near the object boundaries. To mitigate this effect, recent research works have focused on incorporating spatial information or prior knowledge to enforce anatomically plausible segmentation. If the integration of prior knowledge in image segmentation is not a new topic in classical optimization approaches, it is today an increasing trend in CNN based image segmentation, as shown by the growing literature on the topic. In this survey, we focus on high level prior, embedded at the loss function level. We categorize the articles according to the nature of the prior: the object shape, size, topology, and the inter-regions constraints. We highlight strengths and limitations of current approaches, discuss the challenge related to the design and the integration of prior-based losses, and the optimization strategies, and draw future research directions.



### On the Effectiveness of Vision Transformers for Zero-shot Face Anti-Spoofing
- **Arxiv ID**: http://arxiv.org/abs/2011.08019v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.08019v2)
- **Published**: 2020-11-16 15:14:59+00:00
- **Updated**: 2021-06-02 10:37:30+00:00
- **Authors**: Anjith George, Sebastien Marcel
- **Comment**: 8 pages, 3 figures, Accepted for Publication in IJCB2021
- **Journal**: None
- **Summary**: The vulnerability of face recognition systems to presentation attacks has limited their application in security-critical scenarios. Automatic methods of detecting such malicious attempts are essential for the safe use of facial recognition technology. Although various methods have been suggested for detecting such attacks, most of them over-fit the training set and fail in generalizing to unseen attacks and environments. In this work, we use transfer learning from the vision transformer model for the zero-shot anti-spoofing task. The effectiveness of the proposed approach is demonstrated through experiments in publicly available datasets. The proposed approach outperforms the state-of-the-art methods in the zero-shot protocols in the HQ-WMCA and SiW-M datasets by a large margin. Besides, the model achieves a significant boost in cross-database performance as well.



### Multi-Modal Hybrid Architecture for Pedestrian Action Prediction
- **Arxiv ID**: http://arxiv.org/abs/2012.00514v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2012.00514v1)
- **Published**: 2020-11-16 15:17:58+00:00
- **Updated**: 2020-11-16 15:17:58+00:00
- **Authors**: Amir Rasouli, Tiffany Yau, Mohsen Rohani, Jun Luo
- **Comment**: 7 pages, 4 Figures, 3 tables, submitted to ICRA 2021
- **Journal**: None
- **Summary**: Pedestrian behavior prediction is one of the major challenges for intelligent driving systems in urban environments. Pedestrians often exhibit a wide range of behaviors and adequate interpretations of those depend on various sources of information such as pedestrian appearance, states of other road users, the environment layout, etc. To address this problem, we propose a novel multi-modal prediction algorithm that incorporates different sources of information captured from the environment to predict future crossing actions of pedestrians. The proposed model benefits from a hybrid learning architecture consisting of feedforward and recurrent networks for analyzing visual features of the environment and dynamics of the scene. Using the existing 2D pedestrian behavior benchmarks and a newly annotated 3D driving dataset, we show that our proposed model achieves state-of-the-art performance in pedestrian crossing prediction.



### Cycle-Consistent Generative Rendering for 2D-3D Modality Translation
- **Arxiv ID**: http://arxiv.org/abs/2011.08026v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, I.2.10; I.2.6
- **Links**: [PDF](http://arxiv.org/pdf/2011.08026v1)
- **Published**: 2020-11-16 15:23:03+00:00
- **Updated**: 2020-11-16 15:23:03+00:00
- **Authors**: Tristan Aumentado-Armstrong, Alex Levinshtein, Stavros Tsogkas, Konstantinos G. Derpanis, Allan D. Jepson
- **Comment**: 3DV 2020 (oral). Project page: https://ttaa9.github.io/genren/
- **Journal**: None
- **Summary**: For humans, visual understanding is inherently generative: given a 3D shape, we can postulate how it would look in the world; given a 2D image, we can infer the 3D structure that likely gave rise to it. We can thus translate between the 2D visual and 3D structural modalities of a given object. In the context of computer vision, this corresponds to a learnable module that serves two purposes: (i) generate a realistic rendering of a 3D object (shape-to-image translation) and (ii) infer a realistic 3D shape from an image (image-to-shape translation). In this paper, we learn such a module while being conscious of the difficulties in obtaining large paired 2D-3D datasets. By leveraging generative domain translation methods, we are able to define a learning algorithm that requires only weak supervision, with unpaired data. The resulting model is not only able to perform 3D shape, pose, and texture inference from 2D images, but can also generate novel textured 3D shapes and renders, similar to a graphics pipeline. More specifically, our method (i) infers an explicit 3D mesh representation, (ii) utilizes example shapes to regularize inference, (iii) requires only an image mask (no keypoints or camera extrinsics), and (iv) has generative capabilities. While prior work explores subsets of these properties, their combination is novel. We demonstrate the utility of our learned representation, as well as its performance on image generation and unpaired 3D shape inference tasks.



### Scaled-YOLOv4: Scaling Cross Stage Partial Network
- **Arxiv ID**: http://arxiv.org/abs/2011.08036v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.08036v2)
- **Published**: 2020-11-16 15:42:00+00:00
- **Updated**: 2021-02-22 01:32:18+00:00
- **Authors**: Chien-Yao Wang, Alexey Bochkovskiy, Hong-Yuan Mark Liao
- **Comment**: Added references. Corrected typos. The new results are slightly
  better
- **Journal**: None
- **Summary**: We show that the YOLOv4 object detection neural network based on the CSP approach, scales both up and down and is applicable to small and large networks while maintaining optimal speed and accuracy. We propose a network scaling approach that modifies not only the depth, width, resolution, but also structure of the network. YOLOv4-large model achieves state-of-the-art results: 55.5% AP (73.4% AP50) for the MS COCO dataset at a speed of ~16 FPS on Tesla V100, while with the test time augmentation, YOLOv4-large achieves 56.0% AP (73.3 AP50). To the best of our knowledge, this is currently the highest accuracy on the COCO dataset among any published work. The YOLOv4-tiny model achieves 22.0% AP (42.0% AP50) at a speed of 443 FPS on RTX 2080Ti, while by using TensorRT, batch size = 4 and FP16-precision the YOLOv4-tiny achieves 1774 FPS.



### Assessing Wireless Sensing Potential with Large Intelligent Surfaces
- **Arxiv ID**: http://arxiv.org/abs/2011.08465v3
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.08465v3)
- **Published**: 2020-11-16 15:50:22+00:00
- **Updated**: 2021-04-12 19:49:36+00:00
- **Authors**: Cristian J. Vaca-Rubio, Pablo Ramirez-Espinosa, Kimmo Kansanen, Zheng-Hua Tan, Elisabeth de Carvalho, Petar Popovski
- **Comment**: arXiv admin note: text overlap with arXiv:2006.06563
- **Journal**: None
- **Summary**: Sensing capability is one of the most highlighted new feature of future 6G wireless networks. This paper addresses the sensing potential of Large Intelligent Surfaces (LIS) in an exemplary Industry 4.0 scenario. Besides the attention received by LIS in terms of communication aspects, it can offer a high-resolution rendering of the propagation environment. This is because, in an indoor setting, it can be placed in proximity to the sensed phenomena, while the high resolution is offered by densely spaced tiny antennas deployed over a large area. By treating an LIS as a radio image of the environment relying on the received signal power, we develop techniques to sense the environment, by leveraging the tools of image processing and machine learning. Once a holographic image is obtained, a Denoising Autoencoder (DAE) network can be used for constructing a super-resolution image leading to sensing advantages not available in traditional sensing systems. Also, we derive a statistical test based on the Generalized Likelihood Ratio (GLRT) as a benchmark for the machine learning solution. We test these methods for a scenario where we need to detect whether an industrial robot deviates from a predefined route. The results show that the LIS-based sensing offers high precision and has a high application potential in indoor industrial environments.



### FRDet: Balanced and Lightweight Object Detector based on Fire-Residual Modules for Embedded Processor of Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2011.08061v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2011.08061v1)
- **Published**: 2020-11-16 16:15:43+00:00
- **Updated**: 2020-11-16 16:15:43+00:00
- **Authors**: Seontaek Oh, Ji-Hwan You, Young-Keun Kim
- **Comment**: None
- **Journal**: None
- **Summary**: For deployment on an embedded processor for autonomous driving, the object detection network should satisfy all of the accuracy, real-time inference, and light model size requirements. Conventional deep CNN-based detectors aim for high accuracy, making their model size heavy for an embedded system with limited memory space. In contrast, lightweight object detectors are greatly compressed but at a significant sacrifice of accuracy. Therefore, we propose FRDet, a lightweight one-stage object detector that is balanced to satisfy all the constraints of accuracy, model size, and real-time processing on an embedded GPU processor for autonomous driving applications. Our network aims to maximize the compression of the model while achieving or surpassing YOLOv3 level of accuracy. This paper proposes the Fire-Residual (FR) module to design a lightweight network with low accuracy loss by adapting fire modules with residual skip connections. In addition, the Gaussian uncertainty modeling of the bounding box is applied to further enhance the localization accuracy. Experiments on the KITTI dataset showed that FRDet reduced the memory size by 50.8% but achieved higher accuracy by 1.12% mAP compared to YOLOv3. Moreover, the real-time detection speed reached 31.3 FPS on an embedded GPU board(NVIDIA Xavier). The proposed network achieved higher compression with comparable accuracy compared to other deep CNN object detectors while showing improved accuracy than the lightweight detector baselines. Therefore, the proposed FRDet is a well-balanced and efficient object detector for practical application in autonomous driving that can satisfies all the criteria of accuracy, real-time inference, and light model size.



### Multiclass Yeast Segmentation in Microstructured Environments with Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2011.08062v2
- **DOI**: 10.1109/CIBCB48159.2020.9277693
- **Categories**: **q-bio.QM**, cs.CV, eess.IV, physics.ins-det
- **Links**: [PDF](http://arxiv.org/pdf/2011.08062v2)
- **Published**: 2020-11-16 16:16:13+00:00
- **Updated**: 2020-11-19 14:28:20+00:00
- **Authors**: Tim Prangemeier, Christian Wildner, André O. Françani, Christoph Reich, Heinz Koeppl
- **Comment**: IEEE CIBCB 2020 (accepted)
- **Journal**: None
- **Summary**: Cell segmentation is a major bottleneck in extracting quantitative single-cell information from microscopy data. The challenge is exasperated in the setting of microstructured environments. While deep learning approaches have proven useful for general cell segmentation tasks, existing segmentation tools for the yeast-microstructure setting rely on traditional machine learning approaches. Here we present convolutional neural networks trained for multiclass segmenting of individual yeast cells and discerning these from cell-similar microstructures. We give an overview of the datasets recorded for training, validating and testing the networks, as well as a typical use-case. We showcase the method's contribution to segmenting yeast in microstructured environments with a typical synthetic biology application in mind. The models achieve robust segmentation results, outperforming the previous state-of-the-art in both accuracy and speed. The combination of fast and accurate segmentation is not only beneficial for a posteriori data processing, it also makes online monitoring of thousands of trapped cells or closed-loop optimal experimental design feasible from an image processing perspective.



### Combining GANs and AutoEncoders for Efficient Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/2011.08102v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2011.08102v2)
- **Published**: 2020-11-16 17:07:55+00:00
- **Updated**: 2020-11-26 16:09:50+00:00
- **Authors**: Fabio Carrara, Giuseppe Amato, Luca Brombin, Fabrizio Falchi, Claudio Gennaro
- **Comment**: 8 pages, 5 figures, 3 tables, pre-print, to be published in the
  proceedings of the 25th International Conference on Pattern Recognition
  (ICPR2020)
- **Journal**: None
- **Summary**: In this work, we propose CBiGAN -- a novel method for anomaly detection in images, where a consistency constraint is introduced as a regularization term in both the encoder and decoder of a BiGAN. Our model exhibits fairly good modeling power and reconstruction consistency capability. We evaluate the proposed method on MVTec AD -- a real-world benchmark for unsupervised anomaly detection on high-resolution images -- and compare against standard baselines and state-of-the-art approaches. Experiments show that the proposed method improves the performance of BiGAN formulations by a large margin and performs comparably to expensive state-of-the-art iterative methods while reducing the computational cost. We also observe that our model is particularly effective in texture-type anomaly detection, as it sets a new state of the art in this category. Our code is available at https://github.com/fabiocarrara/cbigan-ad/.



### Recovering and Simulating Pedestrians in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2011.08106v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2011.08106v1)
- **Published**: 2020-11-16 17:16:32+00:00
- **Updated**: 2020-11-16 17:16:32+00:00
- **Authors**: Ze Yang, Siva Manivasagam, Ming Liang, Bin Yang, Wei-Chiu Ma, Raquel Urtasun
- **Comment**: CoRL 2020
- **Journal**: None
- **Summary**: Sensor simulation is a key component for testing the performance of self-driving vehicles and for data augmentation to better train perception systems. Typical approaches rely on artists to create both 3D assets and their animations to generate a new scenario. This, however, does not scale. In contrast, we propose to recover the shape and motion of pedestrians from sensor readings captured in the wild by a self-driving car driving around. Towards this goal, we formulate the problem as energy minimization in a deep structured model that exploits human shape priors, reprojection consistency with 2D poses extracted from images, and a ray-caster that encourages the reconstructed mesh to agree with the LiDAR readings. Importantly, we do not require any ground-truth 3D scans or 3D pose annotations. We then incorporate the reconstructed pedestrian assets bank in a realistic LiDAR simulation system by performing motion retargeting, and show that the simulated LiDAR data can be used to significantly reduce the amount of annotated real-world data required for visual perception tasks.



### Stylized Neural Painting
- **Arxiv ID**: http://arxiv.org/abs/2011.08114v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.08114v1)
- **Published**: 2020-11-16 17:24:21+00:00
- **Updated**: 2020-11-16 17:24:21+00:00
- **Authors**: Zhengxia Zou, Tianyang Shi, Shuang Qiu, Yi Yuan, Zhenwei Shi
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes an image-to-painting translation method that generates vivid and realistic painting artworks with controllable styles. Different from previous image-to-image translation methods that formulate the translation as pixel-wise prediction, we deal with such an artistic creation process in a vectorized environment and produce a sequence of physically meaningful stroke parameters that can be further used for rendering. Since a typical vector render is not differentiable, we design a novel neural renderer which imitates the behavior of the vector renderer and then frame the stroke prediction as a parameter searching process that maximizes the similarity between the input and the rendering output. We explored the zero-gradient problem on parameter searching and propose to solve this problem from an optimal transportation perspective. We also show that previous neural renderers have a parameter coupling problem and we re-design the rendering network with a rasterization network and a shading network that better handles the disentanglement of shape and color. Experiments show that the paintings generated by our method have a high degree of fidelity in both global appearance and local textures. Our method can be also jointly optimized with neural style transfer that further transfers visual style from other images. Our code and animated results are available at \url{https://jiupinjia.github.io/neuralpainter/}.



### Cinematic-L1 Video Stabilization with a Log-Homography Model
- **Arxiv ID**: http://arxiv.org/abs/2011.08144v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.08144v2)
- **Published**: 2020-11-16 18:10:57+00:00
- **Updated**: 2020-11-20 19:14:09+00:00
- **Authors**: Arwen Bradley, Jason Klivington, Joseph Triscari, Rudolph van der Merwe
- **Comment**: 8 pages, 11 figures
- **Journal**: None
- **Summary**: We present a method for stabilizing handheld video that simulates the camera motions cinematographers achieve with equipment like tripods, dollies, and Steadicams. We formulate a constrained convex optimization problem minimizing the $\ell_1$-norm of the first three derivatives of the stabilized motion. Our approach extends the work of Grundmann et al. [9] by solving with full homographies (rather than affinities) in order to correct perspective, preserving linearity by working in log-homography space. We also construct crop constraints that preserve field-of-view; model the problem as a quadratic (rather than linear) program to allow for an $\ell_2$ term encouraging fidelity to the original trajectory; and add constraints and objectives to reduce distortion. Furthermore, we propose new methods for handling salient objects via both inclusion constraints and centering objectives. Finally, we describe a windowing strategy to approximate the solution in linear time and bounded memory. Our method is computationally efficient, running at 300fps on an iPhone XS, and yields high-quality results, as we demonstrate with a collection of stabilized videos, quantitative and qualitative comparisons to [9] and other methods, and an ablation study.



### Combining Self-Supervised and Supervised Learning with Noisy Labels
- **Arxiv ID**: http://arxiv.org/abs/2011.08145v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.08145v2)
- **Published**: 2020-11-16 18:13:41+00:00
- **Updated**: 2023-06-25 14:19:38+00:00
- **Authors**: Yongqi Zhang, Hui Zhang, Quanming Yao, Jun Wan
- **Comment**: None
- **Journal**: None
- **Summary**: Since convolutional neural networks (CNNs) can easily overfit noisy labels, which are ubiquitous in visual classification tasks, it has been a great challenge to train CNNs against them robustly. Various methods have been proposed for this challenge. However, none of them pay attention to the difference between representation and classifier learning of CNNs. Thus, inspired by the observation that classifier is more robust to noisy labels while representation is much more fragile, and by the recent advances of self-supervised representation learning (SSRL) technologies, we design a new method, i.e., CS$^3$NL, to obtain representation by SSRL without labels and train the classifier directly with noisy labels. Extensive experiments are performed on both synthetic and real benchmark datasets. Results demonstrate that the proposed method can beat the state-of-the-art ones by a large margin, especially under a high noisy level.



### A New Dataset and Proposed Convolutional Neural Network Architecture for Classification of American Sign Language Digits
- **Arxiv ID**: http://arxiv.org/abs/2011.08927v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2011.08927v2)
- **Published**: 2020-11-16 18:32:22+00:00
- **Updated**: 2021-02-23 14:28:09+00:00
- **Authors**: Arda Mavi
- **Comment**: 5 pages, 1 figure
- **Journal**: None
- **Summary**: According to interviews with people who work with speech impaired persons, speech impaired people have difficulties in communicating with other people around them who do not know the sign language, and this situation may cause them to isolate themselves from society and lose their sense of independence. With this paper, to increase the quality of life of individuals with facilitating communication between individuals who use sign language and who do not know this language, a new American Sign Language (ASL) digits dataset that can help to create machine learning algorithms which need to large and varied data to be successful created and published as Sign Language Digits Dataset on Kaggle Datasets web page, a proposal Convolutional Neural Network (CNN) architecture that can get 98% test accuracy on our dataset presented, and compared with the existing popular CNN models.



### Denoising Score-Matching for Uncertainty Quantification in Inverse Problems
- **Arxiv ID**: http://arxiv.org/abs/2011.08698v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG, eess.SP, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2011.08698v1)
- **Published**: 2020-11-16 18:33:06+00:00
- **Updated**: 2020-11-16 18:33:06+00:00
- **Authors**: Zaccharie Ramzi, Benjamin Remy, Francois Lanusse, Jean-Luc Starck, Philippe Ciuciu
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks have proven extremely efficient at solving a wide rangeof inverse problems, but most often the uncertainty on the solution they provideis hard to quantify. In this work, we propose a generic Bayesian framework forsolving inverse problems, in which we limit the use of deep neural networks tolearning a prior distribution on the signals to recover. We adopt recent denoisingscore matching techniques to learn this prior from data, and subsequently use it aspart of an annealed Hamiltonian Monte-Carlo scheme to sample the full posteriorof image inverse problems. We apply this framework to Magnetic ResonanceImage (MRI) reconstruction and illustrate how this approach not only yields highquality reconstructions but can also be used to assess the uncertainty on particularfeatures of a reconstructed image.



### Fast and Robust Cascade Model for Multiple Degradation Single Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2011.07068v1
- **DOI**: 10.1109/TIP.2021.3074821
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2011.07068v1)
- **Published**: 2020-11-16 18:59:49+00:00
- **Updated**: 2020-11-16 18:59:49+00:00
- **Authors**: Santiago López-Tapia, Nicolás Pérez de la Blanca
- **Comment**: 12 pages and 11 figures (8 figures and 3 tables)
- **Journal**: None
- **Summary**: Single Image Super-Resolution (SISR) is one of the low-level computer vision problems that has received increased attention in the last few years. Current approaches are primarily based on harnessing the power of deep learning models and optimization techniques to reverse the degradation model. Owing to its hardness, isotropic blurring or Gaussians with small anisotropic deformations have been mainly considered. Here, we widen this scenario by including large non-Gaussian blurs that arise in real camera movements. Our approach leverages the degradation model and proposes a new formulation of the Convolutional Neural Network (CNN) cascade model, where each network sub-module is constrained to solve a specific degradation: deblurring or upsampling. A new densely connected CNN-architecture is proposed where the output of each sub-module is restricted using some external knowledge to focus it on its specific task. As far we know this use of domain-knowledge to module-level is a novelty in SISR. To fit the finest model, a final sub-module takes care of the residual errors propagated by the previous sub-modules. We check our model with three state of the art (SOTA) datasets in SISR and compare the results with the SOTA models. The results show that our model is the only one able to manage our wider set of deformations. Furthermore, our model overcomes all current SOTA methods for a standard set of deformations. In terms of computational load, our model also improves on the two closest competitors in terms of efficiency. Although the approach is non-blind and requires an estimation of the blur kernel, it shows robustness to blur kernel estimation errors, making it a good alternative to blind models.



### Large-scale kernelized GRANGER causality to infer topology of directed graphs with applications to brain networks
- **Arxiv ID**: http://arxiv.org/abs/2011.08261v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.08261v1)
- **Published**: 2020-11-16 20:30:19+00:00
- **Updated**: 2020-11-16 20:30:19+00:00
- **Authors**: M. Ali Vosoughi, Axel Wismuller
- **Comment**: 5 pages, 3 figures, conference paper
- **Journal**: None
- **Summary**: Graph topology inference of network processes with co-evolving and interacting time-series is crucial for network studies. Vector autoregressive models (VAR) are popular approaches for topology inference of directed graphs; however, in large networks with short time-series, topology estimation becomes ill-posed. The present paper proposes a novel nonlinearity-preserving topology inference method for directed networks with co-evolving nodal processes that solves the ill-posedness problem. The proposed method, large-scale kernelized Granger causality (lsKGC), uses kernel functions to transform data into a low-dimensional feature space and solves the autoregressive problem in the feature space, then finds the pre-images in the input space to infer the topology. Extensive simulations on synthetic datasets with nonlinear and linear dependencies and known ground-truth demonstrate significant improvement in the Area Under the receiver operating characteristic Curve ( AUC ) of the receiver operating characteristic for network recovery compared to existing methods. Furthermore, tests on real datasets from a functional magnetic resonance imaging (fMRI) study demonstrate 96.3 percent accuracy in diagnosis tasks of schizophrenia patients, which is the highest in the literature with only brain time-series information.



### Where Are You? Localization from Embodied Dialog
- **Arxiv ID**: http://arxiv.org/abs/2011.08277v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2011.08277v2)
- **Published**: 2020-11-16 21:09:43+00:00
- **Updated**: 2021-09-03 13:06:58+00:00
- **Authors**: Meera Hahn, Jacob Krantz, Dhruv Batra, Devi Parikh, James M. Rehg, Stefan Lee, Peter Anderson
- **Comment**: None
- **Journal**: EMNLP 2020
- **Summary**: We present Where Are You? (WAY), a dataset of ~6k dialogs in which two humans -- an Observer and a Locator -- complete a cooperative localization task. The Observer is spawned at random in a 3D environment and can navigate from first-person views while answering questions from the Locator. The Locator must localize the Observer in a detailed top-down map by asking questions and giving instructions. Based on this dataset, we define three challenging tasks: Localization from Embodied Dialog or LED (localizing the Observer from dialog history), Embodied Visual Dialog (modeling the Observer), and Cooperative Localization (modeling both agents). In this paper, we focus on the LED task -- providing a strong baseline model with detailed ablations characterizing both dataset biases and the importance of various modeling choices. Our best model achieves 32.7% success at identifying the Observer's location within 3m in unseen buildings, vs. 70.4% for human Locators.



### A Transfer Learning Based Active Learning Framework for Brain Tumor Classification
- **Arxiv ID**: http://arxiv.org/abs/2011.09265v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2011.09265v1)
- **Published**: 2020-11-16 21:11:40+00:00
- **Updated**: 2020-11-16 21:11:40+00:00
- **Authors**: Ruqian Hao, Khashayar Namdar, Lin Liu, Farzad Khalvati
- **Comment**: None
- **Journal**: None
- **Summary**: Brain tumor is one of the leading causes of cancer-related death globally among children and adults. Precise classification of brain tumor grade (low-grade and high-grade glioma) at early stage plays a key role in successful prognosis and treatment planning. With recent advances in deep learning, Artificial Intelligence-enabled brain tumor grading systems can assist radiologists in the interpretation of medical images within seconds. The performance of deep learning techniques is, however, highly depended on the size of the annotated dataset. It is extremely challenging to label a large quantity of medical images given the complexity and volume of medical data. In this work, we propose a novel transfer learning based active learning framework to reduce the annotation cost while maintaining stability and robustness of the model performance for brain tumor classification. We employed a 2D slice-based approach to train and finetune our model on the Magnetic Resonance Imaging (MRI) training dataset of 203 patients and a validation dataset of 66 patients which was used as the baseline. With our proposed method, the model achieved Area Under Receiver Operating Characteristic (ROC) Curve (AUC) of 82.89% on a separate test dataset of 66 patients, which was 2.92% higher than the baseline AUC while saving at least 40% of labeling cost. In order to further examine the robustness of our method, we created a balanced dataset, which underwent the same procedure. The model achieved AUC of 82% compared with AUC of 78.48% for the baseline, which reassures the robustness and stability of our proposed transfer learning augmented with active learning framework while significantly reducing the size of training data.



### Overcomplete Deep Subspace Clustering Networks
- **Arxiv ID**: http://arxiv.org/abs/2011.08306v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.08306v1)
- **Published**: 2020-11-16 22:07:18+00:00
- **Updated**: 2020-11-16 22:07:18+00:00
- **Authors**: Jeya Maria Jose Valanarasu, Vishal M. Patel
- **Comment**: WACV 2021
- **Journal**: None
- **Summary**: Deep Subspace Clustering Networks (DSC) provide an efficient solution to the problem of unsupervised subspace clustering by using an undercomplete deep auto-encoder with a fully-connected layer to exploit the self expressiveness property. This method uses undercomplete representations of the input data which makes it not so robust and more dependent on pre-training. To overcome this, we propose a simple yet efficient alternative method - Overcomplete Deep Subspace Clustering Networks (ODSC) where we use overcomplete representations for subspace clustering. In our proposed method, we fuse the features from both undercomplete and overcomplete auto-encoder networks before passing them through the self-expressive layer thus enabling us to extract a more meaningful and robust representation of the input data for clustering. Experimental results on four benchmark datasets show the effectiveness of the proposed method over DSC and other clustering methods in terms of clustering error. Our method is also not as dependent as DSC is on where pre-training should be stopped to get the best performance and is also more robust to noise. Code - \href{https://github.com/jeya-maria-jose/Overcomplete-Deep-Subspace-Clustering}{https://github.com/jeya-maria-jose/Overcomplete-Deep-Subspace-Clustering



### Feature Sharing and Integration for Cooperative Cognition and Perception with Volumetric Sensors
- **Arxiv ID**: http://arxiv.org/abs/2011.08317v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.08317v3)
- **Published**: 2020-11-16 22:43:44+00:00
- **Updated**: 2020-12-04 17:58:14+00:00
- **Authors**: Ehsan Emad Marvasti, Arash Raftari, Amir Emad Marvasti, Yaser P. Fallah, Rui Guo, Hongsheng Lu
- **Comment**: 12 pages, 12 figures, 1 table
- **Journal**: None
- **Summary**: The recent advancement in computational and communication systems has led to the introduction of high-performing neural networks and high-speed wireless vehicular communication networks. As a result, new technologies such as cooperative perception and cognition have emerged, addressing the inherent limitations of sensory devices by providing solutions for the detection of partially occluded targets and expanding the sensing range. However, designing a reliable cooperative cognition or perception system requires addressing the challenges caused by limited network resources and discrepancies between the data shared by different sources. In this paper, we examine the requirements, limitations, and performance of different cooperative perception techniques, and present an in-depth analysis of the notion of Deep Feature Sharing (DFS). We explore different cooperative object detection designs and evaluate their performance in terms of average precision. We use the Volony dataset for our experimental study. The results confirm that the DFS methods are significantly less sensitive to the localization error caused by GPS noise. Furthermore, the results attest that detection gain of DFS methods caused by adding more cooperative participants in the scenes is comparable to raw information sharing technique while DFS enables flexibility in design toward satisfying communication requirements.



### A New Similarity Space Tailored for Supervised Deep Metric Learning
- **Arxiv ID**: http://arxiv.org/abs/2011.08325v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.08325v2)
- **Published**: 2020-11-16 22:58:06+00:00
- **Updated**: 2020-11-18 21:24:02+00:00
- **Authors**: Pedro H. Barros, Fabiane Queiroz, Flavio Figueredo, Jefersson A. dos Santos, Heitor S. Ramos
- **Comment**: 47 pages, 11 figures
- **Journal**: None
- **Summary**: We propose a novel deep metric learning method. Differently from many works on this area, we defined a novel latent space obtained through an autoencoder. The new space, namely S-space, is divided into different regions that describe the positions where pairs of objects are similar/dissimilar. We locate makers to identify these regions. We estimate the similarities between objects through a kernel-based t-student distribution to measure the markers' distance and the new data representation. In our approach, we simultaneously estimate the markers' position in the S-space and represent the objects in the same space. Moreover, we propose a new regularization function to avoid similar markers to collapse altogether. We present evidences that our proposal can represent complex spaces, for instance, when groups of similar objects are located in disjoint regions. We compare our proposal to 9 different distance metric learning approaches (four of them are based on deep-learning) on 28 real-world heterogeneous datasets. According to the four quantitative metrics used, our method overcomes all the nine strategies from the literature.



### EffiScene: Efficient Per-Pixel Rigidity Inference for Unsupervised Joint Learning of Optical Flow, Depth, Camera Pose and Motion Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2011.08332v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.08332v3)
- **Published**: 2020-11-16 23:28:22+00:00
- **Updated**: 2021-05-15 03:45:42+00:00
- **Authors**: Yang Jiao, Trac D. Tran, Guangming Shi
- **Comment**: Accpeted by IEEE Conf. on Computer Vision and Pattern Recognition
  (CVPR) 2021; v1 - original submit v2 - camera ready version v3 - correct
  small bugs in Equation(2)
- **Journal**: None
- **Summary**: This paper addresses the challenging unsupervised scene flow estimation problem by jointly learning four low-level vision sub-tasks: optical flow $\textbf{F}$, stereo-depth $\textbf{D}$, camera pose $\textbf{P}$ and motion segmentation $\textbf{S}$. Our key insight is that the rigidity of the scene shares the same inherent geometrical structure with object movements and scene depth. Hence, rigidity from $\textbf{S}$ can be inferred by jointly coupling $\textbf{F}$, $\textbf{D}$ and $\textbf{P}$ to achieve more robust estimation. To this end, we propose a novel scene flow framework named EffiScene with efficient joint rigidity learning, going beyond the existing pipeline with independent auxiliary structures. In EffiScene, we first estimate optical flow and depth at the coarse level and then compute camera pose by Perspective-$n$-Points method. To jointly learn local rigidity, we design a novel Rigidity From Motion (RfM) layer with three principal components: \emph{}{(i)} correlation extraction; \emph{}{(ii)} boundary learning; and \emph{}{(iii)} outlier exclusion. Final outputs are fused based on the rigid map $M_R$ from RfM at finer levels. To efficiently train EffiScene, two new losses $\mathcal{L}_{bnd}$ and $\mathcal{L}_{unc}$ are designed to prevent trivial solutions and to regularize the flow boundary discontinuity. Extensive experiments on scene flow benchmark KITTI show that our method is effective and significantly improves the state-of-the-art approaches for all sub-tasks, i.e. optical flow ($5.19 \rightarrow 4.20$), depth estimation ($3.78 \rightarrow 3.46$), visual odometry ($0.012 \rightarrow 0.011$) and motion segmentation ($0.57 \rightarrow 0.62$).



### 2D+3D Facial Expression Recognition via Discriminative Dynamic Range Enhancement and Multi-Scale Learning
- **Arxiv ID**: http://arxiv.org/abs/2011.08333v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.08333v1)
- **Published**: 2020-11-16 23:29:50+00:00
- **Updated**: 2020-11-16 23:29:50+00:00
- **Authors**: Yang Jiao, Yi Niu, Trac D. Tran, Guangming Shi
- **Comment**: None
- **Journal**: None
- **Summary**: In 2D+3D facial expression recognition (FER), existing methods generate multi-view geometry maps to enhance the depth feature representation. However, this may introduce false estimations due to local plane fitting from incomplete point clouds. In this paper, we propose a novel Map Generation technique from the viewpoint of information theory, to boost the slight 3D expression differences from strong personality variations. First, we examine the HDR depth data to extract the discriminative dynamic range $r_{dis}$, and maximize the entropy of $r_{dis}$ to a global optimum. Then, to prevent the large deformation caused by over-enhancement, we introduce a depth distortion constraint and reduce the complexity from $O(KN^2)$ to $O(KN\tau)$. Furthermore, the constrained optimization is modeled as a $K$-edges maximum weight path problem in a directed acyclic graph, and we solve it efficiently via dynamic programming. Finally, we also design an efficient Facial Attention structure to automatically locate subtle discriminative facial parts for multi-scale learning, and train it with a proposed loss function $\mathcal{L}_{FA}$ without any facial landmarks. Experimental results on different datasets show that the proposed method is effective and outperforms the state-of-the-art 2D+3D FER methods in both FER accuracy and the output entropy of the generated maps.



### Robust Deep Learning with Active Noise Cancellation for Spatial Computing
- **Arxiv ID**: http://arxiv.org/abs/2011.08341v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.08341v1)
- **Published**: 2020-11-16 23:56:14+00:00
- **Updated**: 2020-11-16 23:56:14+00:00
- **Authors**: Li Chen, David Yang, Purvi Goel, Ilknur Kabul
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes CANC, a Co-teaching Active Noise Cancellation method, applied in spatial computing to address deep learning trained with extreme noisy labels. Deep learning algorithms have been successful in spatial computing for land or building footprint recognition. However a lot of noise exists in ground truth labels due to how labels are collected in spatial computing and satellite imagery. Existing methods to deal with extreme label noise conduct clean sample selection and do not utilize the remaining samples. Such techniques can be wasteful due to the cost of data retrieval. Our proposed CANC algorithm not only conserves high-cost training samples but also provides active label correction to better improve robust deep learning with extreme noisy labels. We demonstrate the effectiveness of CANC for building footprint recognition for spatial computing.



