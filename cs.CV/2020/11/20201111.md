# Arxiv Papers in cs.CV on 2020-11-11
### ForestNet: Classifying Drivers of Deforestation in Indonesia using Deep Learning on Satellite Imagery
- **Arxiv ID**: http://arxiv.org/abs/2011.05479v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2011.05479v1)
- **Published**: 2020-11-11 00:28:40+00:00
- **Updated**: 2020-11-11 00:28:40+00:00
- **Authors**: Jeremy Irvin, Hao Sheng, Neel Ramachandran, Sonja Johnson-Yu, Sharon Zhou, Kyle Story, Rose Rustowicz, Cooper Elsworth, Kemen Austin, Andrew Y. Ng
- **Comment**: Tackling Climate Change with Machine Learning at NeurIPS 2020
- **Journal**: None
- **Summary**: Characterizing the processes leading to deforestation is critical to the development and implementation of targeted forest conservation and management policies. In this work, we develop a deep learning model called ForestNet to classify the drivers of primary forest loss in Indonesia, a country with one of the highest deforestation rates in the world. Using satellite imagery, ForestNet identifies the direct drivers of deforestation in forest loss patches of any size. We curate a dataset of Landsat 8 satellite images of known forest loss events paired with driver annotations from expert interpreters. We use the dataset to train and validate the models and demonstrate that ForestNet substantially outperforms other standard driver classification approaches. In order to support future research on automated approaches to deforestation driver classification, the dataset curated in this study is publicly available at https://stanfordmlgroup.github.io/projects/forestnet .



### Personality-Driven Gaze Animation with Conditional Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2012.02224v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, I.3.0; J.4; I.2.6
- **Links**: [PDF](http://arxiv.org/pdf/2012.02224v1)
- **Published**: 2020-11-11 00:31:45+00:00
- **Updated**: 2020-11-11 00:31:45+00:00
- **Authors**: Funda Durupinar
- **Comment**: 7 pages, 5 figures
- **Journal**: None
- **Summary**: We present a generative adversarial learning approach to synthesize gaze behavior of a given personality. We train the model using an existing data set that comprises eye-tracking data and personality traits of 42 participants performing an everyday task. Given the values of Big-Five personality traits (openness, conscientiousness, extroversion, agreeableness, and neuroticism), our model generates time series data consisting of gaze target, blinking times, and pupil dimensions. We use the generated data to synthesize the gaze motion of virtual agents on a game engine.



### Dense U-net for super-resolution with shuffle pooling layer
- **Arxiv ID**: http://arxiv.org/abs/2011.05490v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.05490v2)
- **Published**: 2020-11-11 00:59:43+00:00
- **Updated**: 2021-01-09 05:58:08+00:00
- **Authors**: Zhengyang Lu, Ying Chen
- **Comment**: 17 pages, 6 figures
- **Journal**: None
- **Summary**: Recent researches have achieved great progress on single image super-resolution(SISR) due to the development of deep learning in the field of computer vision. In these method, the high resolution input image is down-scaled to low resolution space using a single filter, commonly max-pooling, before feature extraction. This means that the feature extraction is performed in biased filtered feature space. We demonstrate that this is sub-optimal and causes information loss. In this work, we proposed a state-of-the-art convolutional neural network method called Dense U-net with shuffle pooling. To achieve this, a modified U-net with dense blocks, called dense U-net, is proposed for SISR. Then, a new pooling strategy called shuffle pooling is designed, which is aimed to replace the dense U-Net for down-scale operation. By doing so, we effectively replace the handcrafted filter in the SISR pipeline with more lossy down-sampling filters specifically trained for each feature map, whilst also reducing the information loss of the overall SISR operation. In addition, a mix loss function, which combined with Mean Square Error(MSE), Structural Similarity Index(SSIM) and Mean Gradient Error (MGE), comes up to reduce the perception loss and high-level information loss. Our proposed method achieves superior accuracy over previous state-of-the-art on the three benchmark datasets: SET14, BSD300, ICDAR2003. Code is available online.



### Unsupervised Learning of Dense Visual Representations
- **Arxiv ID**: http://arxiv.org/abs/2011.05499v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.05499v2)
- **Published**: 2020-11-11 01:28:11+00:00
- **Updated**: 2020-12-07 20:16:40+00:00
- **Authors**: Pedro O. Pinheiro, Amjad Almahairi, Ryan Y. Benmalek, Florian Golemo, Aaron Courville
- **Comment**: None
- **Journal**: None
- **Summary**: Contrastive self-supervised learning has emerged as a promising approach to unsupervised visual representation learning. In general, these methods learn global (image-level) representations that are invariant to different views (i.e., compositions of data augmentation) of the same image. However, many visual understanding tasks require dense (pixel-level) representations. In this paper, we propose View-Agnostic Dense Representation (VADeR) for unsupervised learning of dense representations. VADeR learns pixelwise representations by forcing local features to remain constant over different viewing conditions. Specifically, this is achieved through pixel-level contrastive learning: matching features (that is, features that describes the same location of the scene on different views) should be close in an embedding space, while non-matching features should be apart. VADeR provides a natural representation for dense prediction tasks and transfers well to downstream tasks. Our method outperforms ImageNet supervised pretraining (and strong unsupervised baselines) in multiple dense prediction tasks.



### Automatic Open-World Reliability Assessment
- **Arxiv ID**: http://arxiv.org/abs/2011.05506v2
- **DOI**: 10.1109/WACV48630.2021.00203
- **Categories**: **cs.CV**, cs.LG, eess.IV, eess.SP, 68T45, I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/2011.05506v2)
- **Published**: 2020-11-11 01:56:23+00:00
- **Updated**: 2020-12-14 01:35:18+00:00
- **Authors**: Mohsen Jafarzadeh, Touqeer Ahmad, Akshay Raj Dhamija, Chunchun Li, Steve Cruz, Terrance E. Boult
- **Comment**: 2021 IEEE Winter Conference on Applications of Computer Vision (WACV)
- **Journal**: 2021 IEEE Winter Conference on Applications of Computer Vision
  (WACV)
- **Summary**: Image classification in the open-world must handle out-of-distribution (OOD) images. Systems should ideally reject OOD images, or they will map atop of known classes and reduce reliability. Using open-set classifiers that can reject OOD inputs can help. However, optimal accuracy of open-set classifiers depend on the frequency of OOD data. Thus, for either standard or open-set classifiers, it is important to be able to determine when the world changes and increasing OOD inputs will result in reduced system reliability. However, during operations, we cannot directly assess accuracy as there are no labels. Thus, the reliability assessment of these classifiers must be done by human operators, made more complex because networks are not 100% accurate, so some failures are to be expected. To automate this process, herein, we formalize the open-world recognition reliability problem and propose multiple automatic reliability assessment policies to address this new problem using only the distribution of reported scores/probability data. The distributional algorithms can be applied to both classic classifiers with SoftMax as well as the open-world Extreme Value Machine (EVM) to provide automated reliability assessment. We show that all of the new algorithms significantly outperform detection using the mean of SoftMax.



### Optimized Loss Functions for Object detection: A Case Study on Nighttime Vehicle Detection
- **Arxiv ID**: http://arxiv.org/abs/2011.05523v2
- **DOI**: 10.1177/09544070211036366
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.05523v2)
- **Published**: 2020-11-11 03:00:49+00:00
- **Updated**: 2020-11-18 10:34:48+00:00
- **Authors**: Shang Jiang, Haoran Qin, Bingli Zhang, Jieyu Zheng
- **Comment**: Part D: Journal of Automobile Engineering, 2021
- **Journal**: None
- **Summary**: Loss functions is a crucial factor that affecting the detection precision in object detection task. In this paper, we optimize both two loss functions for classification and localization simultaneously. Firstly, by multiplying an IoU-based coefficient by the standard cross entropy loss in classification loss function, the correlation between localization and classification is established. Compared to the existing studies, in which the correlation is only applied to improve the localization accuracy for positive samples, this paper utilizes the correlation to obtain the really hard negative samples and aims to decrease the misclassified rate for negative samples. Besides, a novel localization loss named MIoU is proposed by incorporating a Mahalanobis distance between predicted box and target box, which eliminate the gradients inconsistency problem in the DIoU loss, further improving the localization accuracy. Finally, sufficient experiments for nighttime vehicle detection have been done on two datasets. Our results show than when train with the proposed loss functions, the detection performance can be outstandingly improved. The source code and trained models are available at https://github.com/therebellll/NegIoU-PosIoU-Miou.



### A Unified Framework for Compressive Video Recovery from Coded Exposure Techniques
- **Arxiv ID**: http://arxiv.org/abs/2011.05532v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.05532v1)
- **Published**: 2020-11-11 03:45:31+00:00
- **Updated**: 2020-11-11 03:45:31+00:00
- **Authors**: Prasan Shedligeri, Anupama S, Kaushik Mitra
- **Comment**: 10 pages, 6 figures, 4 tables. Accepted to be published at WACV 2021
- **Journal**: None
- **Summary**: Several coded exposure techniques have been proposed for acquiring high frame rate videos at low bandwidth. Most recently, a Coded-2-Bucket camera has been proposed that can acquire two compressed measurements in a single exposure, unlike previously proposed coded exposure techniques, which can acquire only a single measurement. Although two measurements are better than one for an effective video recovery, we are yet unaware of the clear advantage of two measurements, either quantitatively or qualitatively. Here, we propose a unified learning-based framework to make such a qualitative and quantitative comparison between those which capture only a single coded image (Flutter Shutter, Pixel-wise coded exposure) and those that capture two measurements per exposure (C2B). Our learning-based framework consists of a shift-variant convolutional layer followed by a fully convolutional deep neural network. Our proposed unified framework achieves the state of the art reconstructions in all three sensing techniques. Further analysis shows that when most scene points are static, the C2B sensor has a significant advantage over acquiring a single pixel-wise coded measurement. However, when most scene points undergo motion, the C2B sensor has only a marginal benefit over the single pixel-wise coded exposure measurement.



### An ensemble-based approach by fine-tuning the deep transfer learning models to classify pneumonia from chest X-ray images
- **Arxiv ID**: http://arxiv.org/abs/2011.05543v1
- **DOI**: 10.5220/0010377403900401
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.05543v1)
- **Published**: 2020-11-11 04:50:06+00:00
- **Updated**: 2020-11-11 04:50:06+00:00
- **Authors**: Sagar Kora Venu
- **Comment**: None
- **Journal**: SciTePress 2021
- **Summary**: Pneumonia is caused by viruses, bacteria, or fungi that infect the lungs, which, if not diagnosed, can be fatal and lead to respiratory failure. More than 250,000 individuals in the United States, mainly adults, are diagnosed with pneumonia each year, and 50,000 die from the disease. Chest Radiography (X-ray) is widely used by radiologists to detect pneumonia. It is not uncommon to overlook pneumonia detection for a well-trained radiologist, which triggers the need for improvement in the diagnosis's accuracy. In this work, we propose using transfer learning, which can reduce the neural network's training time and minimize the generalization error. We trained, fine-tuned the state-of-the-art deep learning models such as InceptionResNet, MobileNetV2, Xception, DenseNet201, and ResNet152V2 to classify pneumonia accurately. Later, we created a weighted average ensemble of these models and achieved a test accuracy of 98.46%, precision of 98.38%, recall of 99.53%, and f1 score of 98.96%. These performance metrics of accuracy, precision, and f1 score are at their highest levels ever reported in the literature, which can be considered a benchmark for the accurate pneumonia classification.



### End-to-End Chinese Landscape Painting Creation Using Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2011.05552v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2011.05552v1)
- **Published**: 2020-11-11 05:20:42+00:00
- **Updated**: 2020-11-11 05:20:42+00:00
- **Authors**: Alice Xue
- **Comment**: This research is an extension of Alice Xue's senior thesis at
  Princeton University. The paper will be published in the proceedings of IEEE
  Winter Conference on Applications of Computer Vision (WACV) 2021 and
  presented at the conference
- **Journal**: None
- **Summary**: Current GAN-based art generation methods produce unoriginal artwork due to their dependence on conditional input. Here, we propose Sketch-And-Paint GAN (SAPGAN), the first model which generates Chinese landscape paintings from end to end, without conditional input. SAPGAN is composed of two GANs: SketchGAN for generation of edge maps, and PaintGAN for subsequent edge-to-painting translation. Our model is trained on a new dataset of traditional Chinese landscape paintings never before used for generative research. A 242-person Visual Turing Test study reveals that SAPGAN paintings are mistaken as human artwork with 55% frequency, significantly outperforming paintings from baseline GANs. Our work lays a groundwork for truly machine-original art generation.



### Intentonomy: a Dataset and Study towards Human Intent Understanding
- **Arxiv ID**: http://arxiv.org/abs/2011.05558v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.SI
- **Links**: [PDF](http://arxiv.org/pdf/2011.05558v2)
- **Published**: 2020-11-11 05:39:00+00:00
- **Updated**: 2021-03-28 02:24:46+00:00
- **Authors**: Menglin Jia, Zuxuan Wu, Austin Reiter, Claire Cardie, Serge Belongie, Ser-Nam Lim
- **Comment**: CVPR2021
- **Journal**: None
- **Summary**: An image is worth a thousand words, conveying information that goes beyond the physical visual content therein. In this paper, we study the intent behind social media images with an aim to analyze how visual information can help the recognition of human intent. Towards this goal, we introduce an intent dataset, Intentonomy, comprising 14K images covering a wide range of everyday scenes. These images are manually annotated with 28 intent categories that are derived from a social psychology taxonomy. We then systematically study whether, and to what extent, commonly used visual information, i.e., object and context, contribute to human motive understanding. Based on our findings, we conduct further study to quantify the effect of attending to object and context classes as well as textual information in the form of hashtags when training an intent classifier. Our results quantitatively and qualitatively shed light on how visual and textual information can produce observable effects when predicting intent.



### Strict Enforcement of Conservation Laws and Invertibility in CNN-Based Super Resolution for Scientific Datasets
- **Arxiv ID**: http://arxiv.org/abs/2011.05586v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, 68T07, 94A08, I.4.3; I.4.4
- **Links**: [PDF](http://arxiv.org/pdf/2011.05586v2)
- **Published**: 2020-11-11 06:18:54+00:00
- **Updated**: 2021-10-26 21:41:29+00:00
- **Authors**: Andrew Geiss, Joseph C. Hardin
- **Comment**: 16 pages, 7 figures
- **Journal**: None
- **Summary**: Recently, deep Convolutional Neural Networks (CNNs) have revolutionized image super-resolution (SR), dramatically outperforming past methods for enhancing image resolution. They could be a boon for the many scientific fields that involve image or gridded datasets: satellite remote sensing, radar meteorology, medical imaging, numerical modeling etc. Unfortunately, while SR-CNNs produce visually compelling outputs, they may break physical conservation laws when applied to scientific datasets. Here, a method for ``Downsampling Enforcement" in SR-CNNs is proposed. A differentiable operator is derived that, when applied as the final transfer function of a CNN, ensures the high resolution outputs exactly reproduce the low resolution inputs under 2D-average downsampling while improving performance of the SR schemes. The method is demonstrated across seven modern CNN-based SR schemes on several benchmark image datasets, and applications to weather radar, satellite imager, and climate model data are also shown. The approach improves training time and performance while ensuring physical consistency between the super-resolved and low resolution data.



### Scribble-Supervised Semantic Segmentation by Random Walk on Neural Representation and Self-Supervision on Neural Eigenspace
- **Arxiv ID**: http://arxiv.org/abs/2011.05621v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.05621v2)
- **Published**: 2020-11-11 08:22:25+00:00
- **Updated**: 2020-11-12 05:53:26+00:00
- **Authors**: Zhiyi Pan, Peng Jiang, Changhe Tu
- **Comment**: None
- **Journal**: None
- **Summary**: Scribble-supervised semantic segmentation has gained much attention recently for its promising performance without high-quality annotations. Many approaches have been proposed. Typically, they handle this problem to either introduce a well-labeled dataset from another related task, turn to iterative refinement and post-processing with the graphical model, or manipulate the scribble label. This work aims to achieve semantic segmentation supervised by scribble label directly without auxiliary information and other intermediate manipulation. Specifically, we impose diffusion on neural representation by random walk and consistency on neural eigenspace by self-supervision, which forces the neural network to produce dense and consistent predictions over the whole dataset. The random walk embedded in the network will compute a probabilistic transition matrix, with which the neural representation diffused to be uniform. Moreover, given the probabilistic transition matrix, we apply the self-supervision on its eigenspace for consistency in the image's main parts. In addition to comparing the common scribble dataset, we also conduct experiments on the modified datasets that randomly shrink and even drop the scribbles on image objects. The results demonstrate the superiority of the proposed method and are even comparable to some full-label supervised ones. The code and datasets are available at https://github.com/panzhiyi/RW-SS.



### Fooling the primate brain with minimal, targeted image manipulation
- **Arxiv ID**: http://arxiv.org/abs/2011.05623v3
- **DOI**: None
- **Categories**: **q-bio.NC**, cs.CV, cs.NE, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2011.05623v3)
- **Published**: 2020-11-11 08:30:54+00:00
- **Updated**: 2022-03-30 05:36:53+00:00
- **Authors**: Li Yuan, Will Xiao, Giorgia Dellaferrera, Gabriel Kreiman, Francis E. H. Tay, Jiashi Feng, Margaret S. Livingstone
- **Comment**: None
- **Journal**: None
- **Summary**: Artificial neural networks (ANNs) are considered the current best models of biological vision. ANNs are the best predictors of neural activity in the ventral stream; moreover, recent work has demonstrated that ANN models fitted to neuronal activity can guide the synthesis of images that drive pre-specified response patterns in small neuronal populations. Despite the success in predicting and steering firing activity, these results have not been connected with perceptual or behavioral changes. Here we propose an array of methods for creating minimal, targeted image perturbations that lead to changes in both neuronal activity and perception as reflected in behavior. We generated 'deceptive images' of human faces, monkey faces, and noise patterns so that they are perceived as a different, pre-specified target category, and measured both monkey neuronal responses and human behavior to these images. We found several effective methods for changing primate visual categorization that required much smaller image change compared to untargeted noise. Our work shares the same goal with adversarial attack, namely the manipulation of images with minimal, targeted noise that leads ANN models to misclassify the images. Our results represent a valuable step in quantifying and characterizing the differences in perturbation robustness of biological and artificial vision.



### Self-supervised Segmentation via Background Inpainting
- **Arxiv ID**: http://arxiv.org/abs/2011.05626v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.05626v1)
- **Published**: 2020-11-11 08:34:40+00:00
- **Updated**: 2020-11-11 08:34:40+00:00
- **Authors**: Isinsu Katircioglu, Helge Rhodin, Victor Constantin, Jörg Spörri, Mathieu Salzmann, Pascal Fua
- **Comment**: arXiv admin note: text overlap with arXiv:1907.08051
- **Journal**: None
- **Summary**: While supervised object detection and segmentation methods achieve impressive accuracy, they generalize poorly to images whose appearance significantly differs from the data they have been trained on. To address this when annotating data is prohibitively expensive, we introduce a self-supervised detection and segmentation approach that can work with single images captured by a potentially moving camera. At the heart of our approach lies the observation that object segmentation and background reconstruction are linked tasks, and that, for structured scenes, background regions can be re-synthesized from their surroundings, whereas regions depicting the moving object cannot. We encode this intuition into a self-supervised loss function that we exploit to train a proposal-based segmentation network. To account for the discrete nature of the proposals, we develop a Monte Carlo-based training strategy that allows the algorithm to explore the large space of object proposals. We apply our method to human detection and segmentation in images that visually depart from those of standard benchmarks and outperform existing self-supervised methods.



### Skin disease diagnosis with deep learning: a review
- **Arxiv ID**: http://arxiv.org/abs/2011.05627v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.05627v2)
- **Published**: 2020-11-11 08:35:21+00:00
- **Updated**: 2020-12-06 14:16:58+00:00
- **Authors**: Hongfeng Li, Yini Pan, Jie Zhao, Li Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Skin cancer is one of the most threatening diseases worldwide. However, diagnosing skin cancer correctly is challenging. Recently, deep learning algorithms have emerged to achieve excellent performance on various tasks. Particularly, they have been applied to the skin disease diagnosis tasks. In this paper, we present a review on deep learning methods and their applications in skin disease diagnosis. We first present a brief introduction to skin diseases and image acquisition methods in dermatology, and list several publicly available skin datasets for training and testing algorithms. Then, we introduce the conception of deep learning and review popular deep learning architectures. Thereafter, popular deep learning frameworks facilitating the implementation of deep learning algorithms and performance evaluation metrics are presented. As an important part of this article, we then review the literature involving deep learning methods for skin disease diagnosis from several aspects according to the specific tasks. Additionally, we discuss the challenges faced in the area and suggest possible future research directions. The major purpose of this article is to provide a conceptual and systematically review of the recent works on skin disease diagnosis with deep learning. Given the popularity of deep learning, there remains great challenges in the area, as well as opportunities that we can explore in the future.



### Semi-supervised Sparse Representation with Graph Regularization for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2011.05648v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/2011.05648v1)
- **Published**: 2020-11-11 09:16:48+00:00
- **Updated**: 2020-11-11 09:16:48+00:00
- **Authors**: Hongfeng Li
- **Comment**: None
- **Journal**: None
- **Summary**: Image classification is a challenging problem for computer in reality. Large numbers of methods can achieve satisfying performances with sufficient labeled images. However, labeled images are still highly limited for certain image classification tasks. Instead, lots of unlabeled images are available and easy to be obtained. Therefore, making full use of the available unlabeled data can be a potential way to further improve the performance of current image classification methods. In this paper, we propose a discriminative semi-supervised sparse representation algorithm for image classification. In the algorithm, the classification process is combined with the sparse coding to learn a data-driven linear classifier. To obtain discriminative predictions, the predicted labels are regularized with three graphs, i.e., the global manifold structure graph, the within-class graph and the between-classes graph. The constructed graphs are able to extract structure information included in both the labeled and unlabeled data. Moreover, the proposed method is extended to a kernel version for dealing with data that cannot be linearly classified. Accordingly, efficient algorithms are developed to solve the corresponding optimization problems. Experimental results on several challenging databases demonstrate that the proposed algorithm achieves excellent performances compared with related popular methods.



### Skeleton-based Relational Reasoning for Group Activity Analysis
- **Arxiv ID**: http://arxiv.org/abs/2011.05653v3
- **DOI**: 10.1016/j.patcog.2021.108360
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.05653v3)
- **Published**: 2020-11-11 09:25:53+00:00
- **Updated**: 2021-10-08 02:05:35+00:00
- **Authors**: Mauricio Perez, Jun Liu, Alex C. Kot
- **Comment**: 26 pages, 5 figures, accepted manuscript in Elsevier Pattern
  Recognition, minor writing revisions and new references
- **Journal**: None
- **Summary**: Research on group activity recognition mostly leans on the standard two-stream approach (RGB and Optical Flow) as their input features. Few have explored explicit pose information, with none using it directly to reason about the persons interactions. In this paper, we leverage the skeleton information to learn the interactions between the individuals straight from it. With our proposed method GIRN, multiple relationship types are inferred from independent modules, that describe the relations between the body joints pair-by-pair. Additionally to the joints relations, we also experiment with the previously unexplored relationship between individuals and relevant objects (e.g. volleyball). The individuals distinct relations are then merged through an attention mechanism, that gives more importance to those individuals more relevant for distinguishing the group activity. We evaluate our method in the Volleyball dataset, obtaining competitive results to the state-of-the-art. Our experiments demonstrate the potential of skeleton-based approaches for modeling multi-person interactions.



### Progressive Spatio-Temporal Graph Convolutional Network for Skeleton-Based Human Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2011.05668v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.05668v2)
- **Published**: 2020-11-11 09:57:49+00:00
- **Updated**: 2021-04-26 20:43:10+00:00
- **Authors**: Negar Heidari, Alexandros Iosifidis
- **Comment**: Accepted by the 2021 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP 2021)
- **Journal**: None
- **Summary**: Graph convolutional networks (GCNs) have been very successful in skeleton-based human action recognition where the sequence of skeletons is modeled as a graph. However, most of the GCN-based methods in this area train a deep feed-forward network with a fixed topology that leads to high computational complexity and restricts their application in low computation scenarios. In this paper, we propose a method to automatically find a compact and problem-specific topology for spatio-temporal graph convolutional networks in a progressive manner. Experimental results on two widely used datasets for skeleton-based human action recognition indicate that the proposed method has competitive or even better classification performance compared to the state-of-the-art methods with much lower computational complexity.



### A Hybrid Approach for 6DoF Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2011.05669v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.05669v1)
- **Published**: 2020-11-11 09:58:23+00:00
- **Updated**: 2020-11-11 09:58:23+00:00
- **Authors**: Rebecca König, Bertram Drost
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a method for 6DoF pose estimation of rigid objects that uses a state-of-the-art deep learning based instance detector to segment object instances in an RGB image, followed by a point-pair based voting method to recover the object's pose. We additionally use an automatic method selection that chooses the instance detector and the training set as that with the highest performance on the validation set. This hybrid approach leverages the best of learning and classic approaches, using CNNs to filter highly unstructured data and cut through the clutter, and a local geometric approach with proven convergence for robust pose estimation. The method is evaluated on the BOP core datasets where it significantly exceeds the baseline method and is the best fast method in the BOP 2020 Challenge.



### FPGA: Fast Patch-Free Global Learning Framework for Fully End-to-End Hyperspectral Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2011.05670v1
- **DOI**: 10.1109/TGRS.2020.2967821
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2011.05670v1)
- **Published**: 2020-11-11 09:59:48+00:00
- **Updated**: 2020-11-11 09:59:48+00:00
- **Authors**: Zhuo Zheng, Yanfei Zhong, Ailong Ma, Liangpei Zhang
- **Comment**: 16 pages, 15 figures, IEEE Transactions on Geoscience and Remote
  Sensing, 2020
- **Journal**: None
- **Summary**: Deep learning techniques have provided significant improvements in hyperspectral image (HSI) classification. The current deep learning based HSI classifiers follow a patch-based learning framework by dividing the image into overlapping patches. As such, these methods are local learning methods, which have a high computational cost. In this paper, a fast patch-free global learning (FPGA) framework is proposed for HSI classification. In FPGA, an encoder-decoder based FCN is utilized to consider the global spatial information by processing the whole image, which results in fast inference. However, it is difficult to directly utilize the encoder-decoder based FCN for HSI classification as it always fails to converge due to the insufficiently diverse gradients caused by the limited training samples. To solve the divergence problem and maintain the abilities of FCN of fast inference and global spatial information mining, a global stochastic stratified sampling strategy is first proposed by transforming all the training samples into a stochastic sequence of stratified samples. This strategy can obtain diverse gradients to guarantee the convergence of the FCN in the FPGA framework. For a better design of FCN architecture, FreeNet, which is a fully end-to-end network for HSI classification, is proposed to maximize the exploitation of the global spatial information and boost the performance via a spectral attention based encoder and a lightweight decoder. A lateral connection module is also designed to connect the encoder and decoder, fusing the spatial details in the encoder and the semantic features in the decoder. The experimental results obtained using three public benchmark datasets suggest that the FPGA framework is superior to the patch-based framework in both speed and accuracy for HSI classification. Code has been made available at: https://github.com/Z-Zheng/FreeNet.



### Zero-Pair Image to Image Translation using Domain Conditional Normalization
- **Arxiv ID**: http://arxiv.org/abs/2011.05680v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.05680v1)
- **Published**: 2020-11-11 10:20:47+00:00
- **Updated**: 2020-11-11 10:20:47+00:00
- **Authors**: Samarth Shukla, Andrés Romero, Luc Van Gool, Radu Timofte
- **Comment**: Paper accepted for publication at WACV 2021
- **Journal**: None
- **Summary**: In this paper, we propose an approach based on domain conditional normalization (DCN) for zero-pair image-to-image translation, i.e., translating between two domains which have no paired training data available but each have paired training data with a third domain. We employ a single generator which has an encoder-decoder structure and analyze different implementations of domain conditional normalization to obtain the desired target domain output. The validation benchmark uses RGB-depth pairs and RGB-semantic pairs for training and compares performance for the depth-semantic translation task. The proposed approaches improve in qualitative and quantitative terms over the compared methods, while using much fewer parameters. Code available at https://github.com/samarthshukla/dcn



### Noise Conscious Training of Non Local Neural Network powered by Self Attentive Spectral Normalized Markovian Patch GAN for Low Dose CT Denoising
- **Arxiv ID**: http://arxiv.org/abs/2011.05684v1
- **DOI**: 10.1109/TMI.2021.3094525
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2011.05684v1)
- **Published**: 2020-11-11 10:44:52+00:00
- **Updated**: 2020-11-11 10:44:52+00:00
- **Authors**: Sutanu Bera, Prabir Kumar Biswas
- **Comment**: None
- **Journal**: IEEE Transactions on Medical Imaging 2021
- **Summary**: The explosive rise of the use of Computer tomography (CT) imaging in medical practice has heightened public concern over the patient's associated radiation dose. However, reducing the radiation dose leads to increased noise and artifacts, which adversely degrades the scan's interpretability. Consequently, an advanced image reconstruction algorithm to improve the diagnostic performance of low dose ct arose as the primary concern among the researchers, which is challenging due to the ill-posedness of the problem. In recent times, the deep learning-based technique has emerged as a dominant method for low dose CT(LDCT) denoising. However, some common bottleneck still exists, which hinders deep learning-based techniques from furnishing the best performance. In this study, we attempted to mitigate these problems with three novel accretions. First, we propose a novel convolutional module as the first attempt to utilize neighborhood similarity of CT images for denoising tasks. Our proposed module assisted in boosting the denoising by a significant margin. Next, we moved towards the problem of non-stationarity of CT noise and introduced a new noise aware mean square error loss for LDCT denoising. Moreover, the loss mentioned above also assisted to alleviate the laborious effort required while training CT denoising network using image patches. Lastly, we propose a novel discriminator function for CT denoising tasks. The conventional vanilla discriminator tends to overlook the fine structural details and focus on the global agreement. Our proposed discriminator leverage self-attention and pixel-wise GANs for restoring the diagnostic quality of LDCT images. Our method validated on a publicly available dataset of the 2016 NIH-AAPM-Mayo Clinic Low Dose CT Grand Challenge performed remarkably better than the existing state of the art method.



### Invariant Deep Compressible Covariance Pooling for Aerial Scene Categorization
- **Arxiv ID**: http://arxiv.org/abs/2011.05702v1
- **DOI**: 10.1109/TGRS.2020.3026221
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.05702v1)
- **Published**: 2020-11-11 11:13:07+00:00
- **Updated**: 2020-11-11 11:13:07+00:00
- **Authors**: Shidong Wang, Yi Ren, Gerard Parr, Yu Guan, Ling Shao
- **Comment**: This article has been accepted for inclusion in a future issue of
  IEEE Transactions on Geoscience and Remote Sensing. Content is final as
  presented, with the exception of pagination
- **Journal**: None
- **Summary**: Learning discriminative and invariant feature representation is the key to visual image categorization. In this article, we propose a novel invariant deep compressible covariance pooling (IDCCP) to solve nuisance variations in aerial scene categorization. We consider transforming the input image according to a finite transformation group that consists of multiple confounding orthogonal matrices, such as the D4 group. Then, we adopt a Siamese-style network to transfer the group structure to the representation space, where we can derive a trivial representation that is invariant under the group action. The linear classifier trained with trivial representation will also be possessed with invariance. To further improve the discriminative power of representation, we extend the representation to the tensor space while imposing orthogonal constraints on the transformation matrix to effectively reduce feature dimensions. We conduct extensive experiments on the publicly released aerial scene image data sets and demonstrate the superiority of this method compared with state-of-the-art methods. In particular, with using ResNet architecture, our IDCCP model can reduce the dimension of the tensor representation by about 98% without sacrificing accuracy (i.e., <0.5%).



### EvidentialMix: Learning with Combined Open-set and Closed-set Noisy Labels
- **Arxiv ID**: http://arxiv.org/abs/2011.05704v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.05704v1)
- **Published**: 2020-11-11 11:15:32+00:00
- **Updated**: 2020-11-11 11:15:32+00:00
- **Authors**: Ragav Sachdeva, Filipe R. Cordeiro, Vasileios Belagiannis, Ian Reid, Gustavo Carneiro
- **Comment**: Paper accepted at WACV'21: Winter Conference on Applications of
  Computer Vision
- **Journal**: None
- **Summary**: The efficacy of deep learning depends on large-scale data sets that have been carefully curated with reliable data acquisition and annotation processes. However, acquiring such large-scale data sets with precise annotations is very expensive and time-consuming, and the cheap alternatives often yield data sets that have noisy labels. The field has addressed this problem by focusing on training models under two types of label noise: 1) closed-set noise, where some training samples are incorrectly annotated to a training label other than their known true class; and 2) open-set noise, where the training set includes samples that possess a true class that is (strictly) not contained in the set of known training labels. In this work, we study a new variant of the noisy label problem that combines the open-set and closed-set noisy labels, and introduce a benchmark evaluation to assess the performance of training algorithms under this setup. We argue that such problem is more general and better reflects the noisy label scenarios in practice. Furthermore, we propose a novel algorithm, called EvidentialMix, that addresses this problem and compare its performance with the state-of-the-art methods for both closed-set and open-set noise on the proposed benchmark. Our results show that our method produces superior classification results and better feature representations than previous state-of-the-art methods. The code is available at https://github.com/ragavsachdeva/EvidentialMix.



### Multiscale Attention Guided Network for COVID-19 Diagnosis Using Chest X-ray Images
- **Arxiv ID**: http://arxiv.org/abs/2012.02278v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2012.02278v2)
- **Published**: 2020-11-11 11:20:10+00:00
- **Updated**: 2021-01-08 07:10:15+00:00
- **Authors**: Jingxiong Li, Yaqi Wang, Shuai Wang, Jun Wang, Jun Liu, Qun Jin, Lingling Sun
- **Comment**: None
- **Journal**: None
- **Summary**: Coronavirus disease 2019 (COVID-19) is one of the most destructive pandemic after millennium, forcing the world to tackle a health crisis. Automated lung infections classification using chest X-ray (CXR) images could strengthen diagnostic capability when handling COVID-19. However, classifying COVID-19 from pneumonia cases using CXR image is a difficult task because of shared spatial characteristics, high feature variation and contrast diversity between cases. Moreover, massive data collection is impractical for a newly emerged disease, which limited the performance of data thirsty deep learning models. To address these challenges, Multiscale Attention Guided deep network with Soft Distance regularization (MAG-SD) is proposed to automatically classify COVID-19 from pneumonia CXR images. In MAG-SD, MA-Net is used to produce prediction vector and attention from multiscale feature maps. To improve the robustness of trained model and relieve the shortage of training data, attention guided augmentations along with a soft distance regularization are posed, which aims at generating meaningful augmentations and reduce noise. Our multiscale attention model achieves better classification performance on our pneumonia CXR image dataset. Plentiful experiments are proposed for MAG-SD which demonstrates its unique advantage in pneumonia classification over cutting-edge models. The code is available at https://github.com/JasonLeeGHub/MAG-SD.



### Learning from THEODORE: A Synthetic Omnidirectional Top-View Indoor Dataset for Deep Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/2011.05719v1
- **DOI**: 10.1109/WACV45572.2020.9093563
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.05719v1)
- **Published**: 2020-11-11 11:46:33+00:00
- **Updated**: 2020-11-11 11:46:33+00:00
- **Authors**: Tobias Scheck, Roman Seidel, Gangolf Hirtz
- **Comment**: Paper accepted in WACV 2020
- **Journal**: None
- **Summary**: Recent work about synthetic indoor datasets from perspective views has shown significant improvements of object detection results with Convolutional Neural Networks(CNNs). In this paper, we introduce THEODORE: a novel, large-scale indoor dataset containing 100,000 high-resolution diversified fisheye images with 14 classes. To this end, we create 3D virtual environments of living rooms, different human characters and interior textures. Beside capturing fisheye images from virtual environments we create annotations for semantic segmentation, instance masks and bounding boxes for object detection tasks. We compare our synthetic dataset to state of the art real-world datasets for omnidirectional images. Based on MS COCO weights, we show that our dataset is well suited for fine-tuning CNNs for object detection. Through a high generalization of our models by means of image synthesis and domain randomization, we reach an AP up to 0.84 for class person on High-Definition Analytics dataset.



### Generic Semi-Supervised Adversarial Subject Translation for Sensor-Based Human Activity Recognition
- **Arxiv ID**: http://arxiv.org/abs/2012.03682v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2012.03682v1)
- **Published**: 2020-11-11 12:16:23+00:00
- **Updated**: 2020-11-11 12:16:23+00:00
- **Authors**: Elnaz Soleimani, Ghazaleh Khodabandelou, Abdelghani Chibani, Yacine Amirat
- **Comment**: None
- **Journal**: None
- **Summary**: The performance of Human Activity Recognition (HAR) models, particularly deep neural networks, is highly contingent upon the availability of the massive amount of annotated training data which should be sufficiently labeled. Though, data acquisition and manual annotation in the HAR domain are prohibitively expensive due to skilled human resource requirements in both steps. Hence, domain adaptation techniques have been proposed to adapt the knowledge from the existing source of data. More recently, adversarial transfer learning methods have shown very promising results in image classification, yet limited for sensor-based HAR problems, which are still prone to the unfavorable effects of the imbalanced distribution of samples. This paper presents a novel generic and robust approach for semi-supervised domain adaptation in HAR, which capitalizes on the advantages of the adversarial framework to tackle the shortcomings, by leveraging knowledge from annotated samples exclusively from the source subject and unlabeled ones of the target subject. Extensive subject translation experiments are conducted on three large, middle, and small-size datasets with different levels of imbalance to assess the robustness and effectiveness of the proposed model to the scale as well as imbalance in the data. The results demonstrate the effectiveness of our proposed algorithms over state-of-the-art methods, which led in up to 13%, 4%, and 13% improvement of our high-level activities recognition metrics for Opportunity, LISSI, and PAMAP2 datasets, respectively. The LISSI dataset is the most challenging one owing to its less populated and imbalanced distribution. Compared to the SA-GAN adversarial domain adaptation method, the proposed approach enhances the final classification performance with an average of 7.5% for the three datasets, which emphasizes the effectiveness of micro-mini-batch training.



### A CNN-based Feature Space for Semi-supervised Incremental Learning in Assisted Living Applications
- **Arxiv ID**: http://arxiv.org/abs/2011.05734v1
- **DOI**: 10.5220/0008871302170224
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.05734v1)
- **Published**: 2020-11-11 12:31:48+00:00
- **Updated**: 2020-11-11 12:31:48+00:00
- **Authors**: Tobias Scheck, Ana Perez Grassi, Gangolf Hirtz
- **Comment**: Accepted in VISAPP 2020
- **Journal**: None
- **Summary**: A Convolutional Neural Network (CNN) is sometimes confronted with objects of changing appearance ( new instances) that exceed its generalization capability. This requires the CNN to incorporate new knowledge, i.e., to learn incrementally. In this paper, we are concerned with this problem in the context of assisted living. We propose using the feature space that results from the training dataset to automatically label problematic images that could not be properly recognized by the CNN. The idea is to exploit the extra information in the feature space for a semi-supervised labeling and to employ problematic images to improve the CNN's classification model. Among other benefits, the resulting semi-supervised incremental learning process allows improving the classification accuracy of new instances by 40% as illustrated by extensive experiments.



### DeepSim: Semantic similarity metrics for learned image registration
- **Arxiv ID**: http://arxiv.org/abs/2011.05735v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.05735v1)
- **Published**: 2020-11-11 12:35:07+00:00
- **Updated**: 2020-11-11 12:35:07+00:00
- **Authors**: Steffen Czolbe, Oswin Krause, Aasa Feragen
- **Comment**: Talk given at Medical Imaging Meets NeurIPS, NeurIPS 2020 workshop.
  Extended Abstract
- **Journal**: None
- **Summary**: We propose a semantic similarity metric for image registration. Existing metrics like euclidean distance or normalized cross-correlation focus on aligning intensity values, giving difficulties with low intensity contrast or noise. Our semantic approach learns dataset-specific features that drive the optimization of a learning-based registration model. Comparing to existing unsupervised and supervised methods across multiple image modalities and applications, we achieve consistently high registration accuracy and faster convergence than state of the art, and the learned invariance to noise gives smoother transformations on low-quality images.



### Survey on 3D face reconstruction from uncalibrated images
- **Arxiv ID**: http://arxiv.org/abs/2011.05740v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.05740v2)
- **Published**: 2020-11-11 12:48:11+00:00
- **Updated**: 2021-02-26 08:32:32+00:00
- **Authors**: Araceli Morales, Gemma Piella, Federico M. Sukno
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, a lot of attention has been focused on the incorporation of 3D data into face analysis and its applications. Despite providing a more accurate representation of the face, 3D facial images are more complex to acquire than 2D pictures. As a consequence, great effort has been invested in developing systems that reconstruct 3D faces from an uncalibrated 2D image. However, the 3D-from-2D face reconstruction problem is ill-posed, thus prior knowledge is needed to restrict the solutions space. In this work, we review 3D face reconstruction methods proposed in the last decade, focusing on those that only use 2D pictures captured under uncontrolled conditions. We present a classification of the proposed methods based on the technique used to add prior knowledge, considering three main strategies, namely, statistical model fitting, photometry, and deep learning, and reviewing each of them separately. In addition, given the relevance of statistical 3D facial models as prior knowledge, we explain the construction procedure and provide a list of the most popular publicly available 3D facial models. After the exhaustive study of 3D-from-2D face reconstruction approaches, we observe that the deep learning strategy is rapidly growing since the last few years, becoming the standard choice in replacement of the widespread statistical model fitting. Unlike the other two strategies, photometry-based methods have decreased in number due to the need for strong underlying assumptions that limit the quality of their reconstructions compared to statistical model fitting and deep learning methods. The review also identifies current challenges and suggests avenues for future research.



### Classification of COVID-19 in Chest CT Images using Convolutional Support Vector Machines
- **Arxiv ID**: http://arxiv.org/abs/2011.05746v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.05746v1)
- **Published**: 2020-11-11 13:04:38+00:00
- **Updated**: 2020-11-11 13:04:38+00:00
- **Authors**: Umut Özkaya, Şaban Öztürk, Serkan Budak, Farid Melgani, Kemal Polat
- **Comment**: 20 pages, 11 figures
- **Journal**: None
- **Summary**: Purpose: Coronavirus 2019 (COVID-19), which emerged in Wuhan, China and affected the whole world, has cost the lives of thousands of people. Manual diagnosis is inefficient due to the rapid spread of this virus. For this reason, automatic COVID-19 detection studies are carried out with the support of artificial intelligence algorithms. Methods: In this study, a deep learning model that detects COVID-19 cases with high performance is presented. The proposed method is defined as Convolutional Support Vector Machine (CSVM) and can automatically classify Computed Tomography (CT) images. Unlike the pre-trained Convolutional Neural Networks (CNN) trained with the transfer learning method, the CSVM model is trained as a scratch. To evaluate the performance of the CSVM method, the dataset is divided into two parts as training (%75) and testing (%25). The CSVM model consists of blocks containing three different numbers of SVM kernels. Results: When the performance of pre-trained CNN networks and CSVM models is assessed, CSVM (7x7, 3x3, 1x1) model shows the highest performance with 94.03% ACC, 96.09% SEN, 92.01% SPE, 92.19% PRE, 94.10% F1-Score, 88.15% MCC and 88.07% Kappa metric values. Conclusion: The proposed method is more effective than other methods. It has proven in experiments performed to be an inspiration for combating COVID and for future studies.



### Finding Relevant Flood Images on Twitter using Content-based Filters
- **Arxiv ID**: http://arxiv.org/abs/2011.05756v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.05756v1)
- **Published**: 2020-11-11 13:16:54+00:00
- **Updated**: 2020-11-11 13:16:54+00:00
- **Authors**: Björn Barz, Kai Schröter, Ann-Christin Kra, Joachim Denzler
- **Comment**: ICPR 2020 Workshop on Machine Learning Advances Environmental Science
  (MAES)
- **Journal**: None
- **Summary**: The analysis of natural disasters such as floods in a timely manner often suffers from limited data due to coarsely distributed sensors or sensor failures. At the same time, a plethora of information is buried in an abundance of images of the event posted on social media platforms such as Twitter. These images could be used to document and rapidly assess the situation and derive proxy-data not available from sensors, e.g., the degree of water pollution. However, not all images posted online are suitable or informative enough for this purpose. Therefore, we propose an automatic filtering approach using machine learning techniques for finding Twitter images that are relevant for one of the following information objectives: assessing the flooded area, the inundation depth, and the degree of water pollution. Instead of relying on textual information present in the tweet, the filter analyzes the image contents directly. We evaluate the performance of two different approaches and various features on a case-study of two major flooding events. Our image-based filter is able to enhance the quality of the results substantially compared with a keyword-based filter, improving the mean average precision from 23% to 53% on average.



### Generative and Discriminative Learning for Distorted Image Restoration
- **Arxiv ID**: http://arxiv.org/abs/2011.05784v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.05784v3)
- **Published**: 2020-11-11 14:01:29+00:00
- **Updated**: 2020-11-27 06:09:41+00:00
- **Authors**: Yi Gu, Yuting Gao, Jie Li, Chentao Wu, Weijia Jia
- **Comment**: None
- **Journal**: None
- **Summary**: Liquify is a common technique for image editing, which can be used for image distortion. Due to the uncertainty in the distortion variation, restoring distorted images caused by liquify filter is a challenging task. To edit images in an efficient way, distorted images are expected to be restored automatically. This paper aims at the distorted image restoration, which is characterized by seeking the appropriate warping and completion of a distorted image. Existing methods focus on the hardware assistance or the geometric principle to solve the specific regular deformation caused by natural phenomena, but they cannot handle the irregularity and uncertainty of artificial distortion in this task. To address this issue, we propose a novel generative and discriminative learning method based on deep neural networks, which can learn various reconstruction mappings and represent complex and high-dimensional data. This method decomposes the task into a rectification stage and a refinement stage. The first stage generative network predicts the mapping from the distorted images to the rectified ones. The second stage generative network then further optimizes the perceptual quality. Since there is no available dataset or benchmark to explore this task, we create a Distorted Face Dataset (DFD) by forward distortion mapping based on CelebA dataset. Extensive experimental evaluation on the proposed benchmark and the application demonstrates that our method is an effective way for distorted image restoration.



### Learned Equivariant Rendering without Transformation Supervision
- **Arxiv ID**: http://arxiv.org/abs/2011.05787v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.05787v1)
- **Published**: 2020-11-11 14:05:05+00:00
- **Updated**: 2020-11-11 14:05:05+00:00
- **Authors**: Cinjon Resnick, Or Litany, Hugo Larochelle, Joan Bruna, Kyunghyun Cho
- **Comment**: Workshop on Differentiable Vision, Graphics, and Physics in Machine
  Learning at NeurIPS 2020
- **Journal**: None
- **Summary**: We propose a self-supervised framework to learn scene representations from video that are automatically delineated into objects and background. Our method relies on moving objects being equivariant with respect to their transformation across frames and the background being constant. After training, we can manipulate and render the scenes in real time to create unseen combinations of objects, transformations, and backgrounds. We show results on moving MNIST with backgrounds.



### Dynamic Plane Convolutional Occupancy Networks
- **Arxiv ID**: http://arxiv.org/abs/2011.05813v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.05813v1)
- **Published**: 2020-11-11 14:24:52+00:00
- **Updated**: 2020-11-11 14:24:52+00:00
- **Authors**: Stefan Lionar, Daniil Emtsev, Dusan Svilarkovic, Songyou Peng
- **Comment**: To be presented at WACV 2021. Equal contribution between the first
  three authors
- **Journal**: None
- **Summary**: Learning-based 3D reconstruction using implicit neural representations has shown promising progress not only at the object level but also in more complicated scenes. In this paper, we propose Dynamic Plane Convolutional Occupancy Networks, a novel implicit representation pushing further the quality of 3D surface reconstruction. The input noisy point clouds are encoded into per-point features that are projected onto multiple 2D dynamic planes. A fully-connected network learns to predict plane parameters that best describe the shapes of objects or scenes. To further exploit translational equivariance, convolutional neural networks are applied to process the plane features. Our method shows superior performance in surface reconstruction from unoriented point clouds in ShapeNet as well as an indoor scene dataset. Moreover, we also provide interesting observations on the distribution of learned dynamic planes.



### Where to drive: free space detection with one fisheye camera
- **Arxiv ID**: http://arxiv.org/abs/2011.05822v1
- **DOI**: 10.1117/12.2556380
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.05822v1)
- **Published**: 2020-11-11 14:36:45+00:00
- **Updated**: 2020-11-11 14:36:45+00:00
- **Authors**: Tobias Scheck, Adarsh Mallandur, Christian Wiede, Gangolf Hirtz
- **Comment**: Accepted at International Conference on Machine Vision 2019 (ICMV
  2019)
- **Journal**: Proceedings Volume 11433, Twelfth International Conference on
  Machine Vision (ICMV 2019); 114332V
- **Summary**: The development in the field of autonomous driving goes hand in hand with ever new developments in the field of image processing and machine learning methods. In order to fully exploit the advantages of deep learning, it is necessary to have sufficient labeled training data available. This is especially not the case for omnidirectional fisheye cameras. As a solution, we propose in this paper to use synthetic training data based on Unity3D. A five-pass algorithm is used to create a virtual fisheye camera. This synthetic training data is evaluated for the application of free space detection for different deep learning network architectures. The results indicate that synthetic fisheye images can be used in deep learning context.



### DeepI2I: Enabling Deep Hierarchical Image-to-Image Translation by Transferring from GANs
- **Arxiv ID**: http://arxiv.org/abs/2011.05867v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.05867v1)
- **Published**: 2020-11-11 16:03:03+00:00
- **Updated**: 2020-11-11 16:03:03+00:00
- **Authors**: Yaxing Wang, Lu Yu, Joost van de Weijer
- **Comment**: NeurIPS2020
- **Journal**: None
- **Summary**: Image-to-image translation has recently achieved remarkable results. But despite current success, it suffers from inferior performance when translations between classes require large shape changes. We attribute this to the high-resolution bottlenecks which are used by current state-of-the-art image-to-image methods. Therefore, in this work, we propose a novel deep hierarchical Image-to-Image Translation method, called DeepI2I. We learn a model by leveraging hierarchical features: (a) structural information contained in the shallow layers and (b) semantic information extracted from the deep layers. To enable the training of deep I2I models on small datasets, we propose a novel transfer learning method, that transfers knowledge from pre-trained GANs. Specifically, we leverage the discriminator of a pre-trained GANs (i.e. BigGAN or StyleGAN) to initialize both the encoder and the discriminator and the pre-trained generator to initialize the generator of our model. Applying knowledge transfer leads to an alignment problem between the encoder and generator. We introduce an adaptor network to address this. On many-class image-to-image translation on three datasets (Animal faces, Birds, and Foods) we decrease mFID by at least 35% when compared to the state-of-the-art. Furthermore, we qualitatively and quantitatively demonstrate that transfer learning significantly improves the performance of I2I systems, especially for small datasets. Finally, we are the first to perform I2I translations for domains with over 100 classes.



### FAT: Training Neural Networks for Reliable Inference Under Hardware Faults
- **Arxiv ID**: http://arxiv.org/abs/2011.05873v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.05873v1)
- **Published**: 2020-11-11 16:09:39+00:00
- **Updated**: 2020-11-11 16:09:39+00:00
- **Authors**: Ussama Zahid, Giulio Gambardella, Nicholas J. Fraser, Michaela Blott, Kees Vissers
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) are state-of-the-art algorithms for multiple applications, spanning from image classification to speech recognition. While providing excellent accuracy, they often have enormous compute and memory requirements. As a result of this, quantized neural networks (QNNs) are increasingly being adopted and deployed especially on embedded devices, thanks to their high accuracy, but also since they have significantly lower compute and memory requirements compared to their floating point equivalents. QNN deployment is also being evaluated for safety-critical applications, such as automotive, avionics, medical or industrial. These systems require functional safety, guaranteeing failure-free behaviour even in the presence of hardware faults. In general fault tolerance can be achieved by adding redundancy to the system, which further exacerbates the overall computational demands and makes it difficult to meet the power and performance requirements. In order to decrease the hardware cost for achieving functional safety, it is vital to explore domain-specific solutions which can exploit the inherent features of DNNs. In this work we present a novel methodology called fault-aware training (FAT), which includes error modeling during neural network (NN) training, to make QNNs resilient to specific fault models on the device. Our experiments show that by injecting faults in the convolutional layers during training, highly accurate convolutional neural networks (CNNs) can be trained which exhibits much better error tolerance compared to the original. Furthermore, we show that redundant systems which are built from QNNs trained with FAT achieve higher worse-case accuracy at lower hardware cost. This has been validated for numerous classification tasks including CIFAR10, GTSRB, SVHN and ImageNet.



### Transferred Fusion Learning using Skipped Networks
- **Arxiv ID**: http://arxiv.org/abs/2011.05895v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.05895v1)
- **Published**: 2020-11-11 16:41:55+00:00
- **Updated**: 2020-11-11 16:41:55+00:00
- **Authors**: Vinayaka R Kamath, Vishal S, Varun M
- **Comment**: 9 Pages, 7 figures, Conference
- **Journal**: None
- **Summary**: Identification of an entity that is of interest is prominent in any intelligent system. The visual intelligence of the model is enhanced when the capability of recognition is added. Several methods such as transfer learning and zero shot learning help to reuse the existing models or augment the existing model to achieve improved performance at the task of object recognition. Transferred fusion learning is one such mechanism that intends to use the best of both worlds and build a model that is capable of outperforming the models involved in the system. We propose a novel mechanism to amplify the process of transfer learning by introducing a student architecture where the networks learn from each other.



### Age Gap Reducer-GAN for Recognizing Age-Separated Faces
- **Arxiv ID**: http://arxiv.org/abs/2011.05897v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.05897v1)
- **Published**: 2020-11-11 16:43:32+00:00
- **Updated**: 2020-11-11 16:43:32+00:00
- **Authors**: Daksha Yadav, Naman Kohli, Mayank Vatsa, Richa Singh, Afzel Noore
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a novel algorithm for matching faces with temporal variations caused due to age progression. The proposed generative adversarial network algorithm is a unified framework that combines facial age estimation and age-separated face verification. The key idea of this approach is to learn the age variations across time by conditioning the input image on the subject's gender and the target age group to which the face needs to be progressed. The loss function accounts for reducing the age gap between the original image and generated face image as well as preserving the identity. Both visual fidelity and quantitative evaluations demonstrate the efficacy of the proposed architecture on different facial age databases for age-separated face recognition.



### LittleYOLO-SPP: A Delicate Real-Time Vehicle Detection Algorithm
- **Arxiv ID**: http://arxiv.org/abs/2011.05940v1
- **DOI**: 10.1016/j.ijleo.2020.165818
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2011.05940v1)
- **Published**: 2020-11-11 17:57:49+00:00
- **Updated**: 2020-11-11 17:57:49+00:00
- **Authors**: Sri Jamiya S, Esther Rani P
- **Comment**: 18 pages, 8 Figures, 7 Tables
- **Journal**: Optik - International Journal for Light and Electron optics Volume
  225, 2021, 165818, ISSN 0030-4026
- **Summary**: Vehicle detection in real-time is a challenging and important task. The existing real-time vehicle detection lacks accuracy and speed. Real-time systems must detect and locate vehicles during criminal activities like theft of vehicle and road traffic violations with high accuracy. Detection of vehicles in complex scenes with occlusion is also extremely difficult. In this study, a lightweight model of deep neural network LittleYOLO-SPP based on the YOLOv3-tiny network is proposed to detect vehicles effectively in real-time. The YOLOv3-tiny object detection network is improved by modifying its feature extraction network to increase the speed and accuracy of vehicle detection. The proposed network incorporated Spatial pyramid pooling into the network, which consists of different scales of pooling layers for concatenation of features to enhance network learning capability. The Mean square error (MSE) and Generalized IoU (GIoU) loss function for bounding box regression is used to increase the performance of the network. The network training includes vehicle-based classes from PASCAL VOC 2007,2012 and MS COCO 2014 datasets such as car, bus, and truck. LittleYOLO-SPP network detects the vehicle in real-time with high accuracy regardless of video frame and weather conditions. The improved network achieves a higher mAP of 77.44% on PASCAL VOC and 52.95% mAP on MS COCO datasets.



### Transformers for One-Shot Visual Imitation
- **Arxiv ID**: http://arxiv.org/abs/2011.05970v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2011.05970v1)
- **Published**: 2020-11-11 18:41:07+00:00
- **Updated**: 2020-11-11 18:41:07+00:00
- **Authors**: Sudeep Dasari, Abhinav Gupta
- **Comment**: For code and project video please check our website:
  https://oneshotfeatures.github.io/
- **Journal**: None
- **Summary**: Humans are able to seamlessly visually imitate others, by inferring their intentions and using past experience to achieve the same end goal. In other words, we can parse complex semantic knowledge from raw video and efficiently translate that into concrete motor control. Is it possible to give a robot this same capability? Prior research in robot imitation learning has created agents which can acquire diverse skills from expert human operators. However, expanding these techniques to work with a single positive example during test time is still an open challenge. Apart from control, the difficulty stems from mismatches between the demonstrator and robot domains. For example, objects may be placed in different locations (e.g. kitchen layouts are different in every house). Additionally, the demonstration may come from an agent with different morphology and physical appearance (e.g. human), so one-to-one action correspondences are not available. This paper investigates techniques which allow robots to partially bridge these domain gaps, using their past experience. A neural network is trained to mimic ground truth robot actions given context video from another agent, and must generalize to unseen task instances when prompted with new videos during test time. We hypothesize that our policy representations must be both context driven and dynamics aware in order to perform these tasks. These assumptions are baked into the neural network using the Transformers attention mechanism and a self-supervised inverse dynamics loss. Finally, we experimentally determine that our method accomplishes a $\sim 2$x improvement in terms of task success rate over prior baselines in a suite of one-shot manipulation tasks.



### GRCNN: Graph Recognition Convolutional Neural Network for Synthesizing Programs from Flow Charts
- **Arxiv ID**: http://arxiv.org/abs/2011.05980v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.05980v1)
- **Published**: 2020-11-11 18:52:25+00:00
- **Updated**: 2020-11-11 18:52:25+00:00
- **Authors**: Lin Cheng, Zijiang Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Program synthesis is the task to automatically generate programs based on user specification. In this paper, we present a framework that synthesizes programs from flow charts that serve as accurate and intuitive specifications. In order doing so, we propose a deep neural network called GRCNN that recognizes graph structure from its image. GRCNN is trained end-to-end, which can predict edge and node information of the flow chart simultaneously. Experiments show that the accuracy rate to synthesize a program is 66.4%, and the accuracy rates to recognize edge and nodes are 94.1% and 67.9%, respectively. On average, it takes about 60 milliseconds to synthesize a program.



### FastPathology: An open-source platform for deep learning-based research and decision support in digital pathology
- **Arxiv ID**: http://arxiv.org/abs/2011.06033v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV, J.6; I.4.9; I.5.4; I.5.5; J.3
- **Links**: [PDF](http://arxiv.org/pdf/2011.06033v1)
- **Published**: 2020-11-11 19:35:31+00:00
- **Updated**: 2020-11-11 19:35:31+00:00
- **Authors**: André Pedersen, Marit Valla, Anna M. Bofin, Javier Pérez de Frutos, Ingerid Reinertsen, Erik Smistad
- **Comment**: 12 pages, 4 figures, submitted to IEEE Access
- **Journal**: None
- **Summary**: Deep convolutional neural networks (CNNs) are the current state-of-the-art for digital analysis of histopathological images. The large size of whole-slide microscopy images (WSIs) requires advanced memory handling to read, display and process these images. There are several open-source platforms for working with WSIs, but few support deployment of CNN models. These applications use third-party solutions for inference, making them less user-friendly and unsuitable for high-performance image analysis. To make deployment of CNNs user-friendly and feasible on low-end machines, we have developed a new platform, FastPathology, using the FAST framework and C++. It minimizes memory usage for reading and processing WSIs, deployment of CNN models, and real-time interactive visualization of results. Runtime experiments were conducted on four different use cases, using different architectures, inference engines, hardware configurations and operating systems. Memory usage for reading, visualizing, zooming and panning a WSI were measured, using FastPathology and three existing platforms. FastPathology performed similarly in terms of memory to the other C++ based application, while using considerably less than the two Java-based platforms. The choice of neural network model, inference engine, hardware and processors influenced runtime considerably. Thus, FastPathology includes all steps needed for efficient visualization and processing of WSIs in a single application, including inference of CNNs with real-time display of the results. Source code, binary releases and test data can be found online on GitHub at https://github.com/SINTEFMedtek/FAST-Pathology/.



### Unsupervised Video Representation Learning by Bidirectional Feature Prediction
- **Arxiv ID**: http://arxiv.org/abs/2011.06037v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.06037v1)
- **Published**: 2020-11-11 19:42:31+00:00
- **Updated**: 2020-11-11 19:42:31+00:00
- **Authors**: Nadine Behrmann, Juergen Gall, Mehdi Noroozi
- **Comment**: Accepted at WACV 2021
- **Journal**: None
- **Summary**: This paper introduces a novel method for self-supervised video representation learning via feature prediction. In contrast to the previous methods that focus on future feature prediction, we argue that a supervisory signal arising from unobserved past frames is complementary to one that originates from the future frames. The rationale behind our method is to encourage the network to explore the temporal structure of videos by distinguishing between future and past given present observations. We train our model in a contrastive learning framework, where joint encoding of future and past provides us with a comprehensive set of temporal hard negatives via swapping. We empirically show that utilizing both signals enriches the learned representations for the downstream task of action recognition. It outperforms independent prediction of future and past.



### A comparative study of semi- and self-supervised semantic segmentation of biomedical microscopy data
- **Arxiv ID**: http://arxiv.org/abs/2011.08076v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, stat.AP
- **Links**: [PDF](http://arxiv.org/pdf/2011.08076v2)
- **Published**: 2020-11-11 20:57:10+00:00
- **Updated**: 2020-11-23 13:03:10+00:00
- **Authors**: Nastassya Horlava, Alisa Mironenko, Sebastian Niehaus, Sebastian Wagner, Ingo Roeder, Nico Scherf
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, Convolutional Neural Networks (CNNs) have become the state-of-the-art method for biomedical image analysis. However, these networks are usually trained in a supervised manner, requiring large amounts of labelled training data. These labelled data sets are often difficult to acquire in the biomedical domain. In this work, we validate alternative ways to train CNNs with fewer labels for biomedical image segmentation using. We adapt two semi- and self-supervised image classification methods and analyse their performance for semantic segmentation of biomedical microscopy images.



### Continuous Perception for Classifying Shapes and Weights of Garmentsfor Robotic Vision Applications
- **Arxiv ID**: http://arxiv.org/abs/2011.06089v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.06089v2)
- **Published**: 2020-11-11 21:55:58+00:00
- **Updated**: 2021-11-19 10:07:32+00:00
- **Authors**: Li Duan, Gerardo Aragon-Camarasa
- **Comment**: Accepted by the 17th International Conference on Computer Vision
  Theory and Applications
- **Journal**: None
- **Summary**: We present an approach to continuous perception for robotic laundry tasks. Our assumption is that the visual prediction of a garment's shapes and weights is possible via a neural network that learns the dynamic changes of garments from video sequences. Continuous perception is leveraged during training by inputting consecutive frames, of which the network learns how a garment deforms. To evaluate our hypothesis, we captured a dataset of 40K RGB and 40K depth video sequences while a garment is being manipulated. We also conducted ablation studies to understand whether the neural network learns the physical and dynamic properties of garments. Our findings suggest that a modified AlexNet-LSTM architecture has the best classification performance for the garment's shape and weights. To further provide evidence that continuous perception facilitates the prediction of the garment's shapes and weights, we evaluated our network on unseen video sequences and computed the 'Moving Average' over a sequence of predictions. We found that our network has a classification accuracy of 48% and 60% for shapes and weights of garments, respectively.



### Monitoring and Diagnosability of Perception Systems
- **Arxiv ID**: http://arxiv.org/abs/2011.07010v5
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.07010v5)
- **Published**: 2020-11-11 23:03:14+00:00
- **Updated**: 2021-10-16 15:31:24+00:00
- **Authors**: Pasquale Antonante, David I. Spivak, Luca Carlone
- **Comment**: Updated version of arXiv:2005.11816
- **Journal**: None
- **Summary**: Perception is a critical component of high-integrity applications of robotics and autonomous systems, such as self-driving vehicles. In these applications, failure of perception systems may put human life at risk, and a broad adoption of these technologies requires the development of methodologies to guarantee and monitor safe operation. Despite the paramount importance of perception systems, currently there is no formal approach for system-level monitoring. In this work, we propose a mathematical model for runtime monitoring and fault detection and identification in perception systems. Towards this goal, we draw connections with the literature on diagnosability in multiprocessor systems, and generalize it to account for modules with heterogeneous outputs that interact over time. The resulting temporal diagnostic graphs (i) provide a framework to reason over the consistency of perception outputs -- across modules and over time -- thus enabling fault detection, (ii) allow us to establish formal guarantees on the maximum number of faults that can be uniquely identified in a given perception system, and (iii) enable the design of efficient algorithms for fault identification. We demonstrate our monitoring system, dubbed PerSyS, in realistic simulations using the LGSVL self-driving simulator and the Apollo Auto autonomy software stack, and show that PerSyS is able to detect failures in challenging scenarios (including scenarios that have caused self-driving car accidents in recent years), and is able to correctly identify faults while entailing a minimal computation overhead (< 5 ms on a single-core CPU).



### Deep learning and hand-crafted features for virus image classification
- **Arxiv ID**: http://arxiv.org/abs/2011.06123v2
- **DOI**: 10.3390/jimaging6120143
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2011.06123v2)
- **Published**: 2020-11-11 23:46:16+00:00
- **Updated**: 2020-12-13 00:29:22+00:00
- **Authors**: Loris Nanni, Eugenio De Luca, Marco Ludovico Facin, Gianluca Maguolo
- **Comment**: None
- **Journal**: Journal of Imaging 2020, 6(12), 143
- **Summary**: In this work, we present an ensemble of descriptors for the classification of transmission electron microscopy images of viruses. We propose to combine handcrafted and deep learning approaches for virus image classification. The set of handcrafted is mainly based on Local Binary Pattern variants, for each descriptor a different Support Vector Machine is trained, then the set of classifiers is combined by sum rule. The deep learning approach is a densenet201 pretrained on ImageNet and then tuned in the virus dataset, the net is used as features extractor for feeding another Support Vector Machine, in particular the last average pooling layer is used as feature extractor. Finally, classifiers trained on handcrafted features and classifier trained on deep learning features are combined by sum rule. The proposed fusion strongly boosts the performance obtained by each stand-alone approach, obtaining state of the art performance.



