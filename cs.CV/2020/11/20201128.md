# Arxiv Papers in cs.CV on 2020-11-28
### Voting based ensemble improves robustness of defensive models
- **Arxiv ID**: http://arxiv.org/abs/2011.14031v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2011.14031v1)
- **Published**: 2020-11-28 00:08:45+00:00
- **Updated**: 2020-11-28 00:08:45+00:00
- **Authors**: Devvrit, Minhao Cheng, Cho-Jui Hsieh, Inderjit Dhillon
- **Comment**: None
- **Journal**: None
- **Summary**: Developing robust models against adversarial perturbations has been an active area of research and many algorithms have been proposed to train individual robust models. Taking these pretrained robust models, we aim to study whether it is possible to create an ensemble to further improve robustness. Several previous attempts tackled this problem by ensembling the soft-label prediction and have been proved vulnerable based on the latest attack methods. In this paper, we show that if the robust training loss is diverse enough, a simple hard-label based voting ensemble can boost the robust error over each individual model. Furthermore, given a pool of robust models, we develop a principled way to select which models to ensemble. Finally, to verify the improved robustness, we conduct extensive experiments to study how to attack a voting-based ensemble and develop several new white-box attacks. On CIFAR-10 dataset, by ensembling several state-of-the-art pre-trained defense models, our method can achieve a 59.8% robust accuracy, outperforming all the existing defensive models without using additional data.



### cMinMax: A Fast Algorithm to Find the Corners of an N-dimensional Convex Polytope
- **Arxiv ID**: http://arxiv.org/abs/2011.14035v3
- **DOI**: 10.5220/0010259002290236
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2011.14035v3)
- **Published**: 2020-11-28 00:32:11+00:00
- **Updated**: 2022-05-13 19:33:33+00:00
- **Authors**: Dimitrios Chamzas, Constantinos Chamzas, Konstantinos Moustakas
- **Comment**: Accepted in GRAPP 2021, Code available at
  https://github.com/jimas95/CMinMax and video presentation at
  https://www.youtube.com/watch?v=Ug313Nf-S-A
- **Journal**: None
- **Summary**: During the last years, the emerging field of Augmented & Virtual Reality (AR-VR) has seen tremendousgrowth. At the same time there is a trend to develop low cost high-quality AR systems where computing poweris in demand. Feature points are extensively used in these real-time frame-rate and 3D applications, thereforeefficient high-speed feature detectors are necessary. Corners are such special features and often are used as thefirst step in the marker alignment in Augmented Reality (AR). Corners are also used in image registration andrecognition, tracking, SLAM, robot path finding and 2D or 3D object detection and retrieval. Therefore thereis a large number of corner detection algorithms but most of them are too computationally intensive for use inreal-time applications of any complexity. Many times the border of the image is a convex polygon. For thisspecial, but quite common case, we have developed a specific algorithm, cMinMax. The proposed algorithmis faster, approximately by a factor of 5 compared to the widely used Harris Corner Detection algorithm. Inaddition is highly parallelizable. The algorithm is suitable for the fast registration of markers in augmentedreality systems and in applications where a computationally efficient real time feature detector is necessary.The algorithm can also be extended to N-dimensional polyhedrons.



### Differences between human and machine perception in medical diagnosis
- **Arxiv ID**: http://arxiv.org/abs/2011.14036v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.CY, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.14036v1)
- **Published**: 2020-11-28 00:32:17+00:00
- **Updated**: 2020-11-28 00:32:17+00:00
- **Authors**: Taro Makino, Stanislaw Jastrzebski, Witold Oleszkiewicz, Celin Chacko, Robin Ehrenpreis, Naziya Samreen, Chloe Chhor, Eric Kim, Jiyon Lee, Kristine Pysarenko, Beatriu Reig, Hildegard Toth, Divya Awal, Linda Du, Alice Kim, James Park, Daniel K. Sodickson, Laura Heacock, Linda Moy, Kyunghyun Cho, Krzysztof J. Geras
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) show promise in image-based medical diagnosis, but cannot be fully trusted since their performance can be severely degraded by dataset shifts to which human perception remains invariant. If we can better understand the differences between human and machine perception, we can potentially characterize and mitigate this effect. We therefore propose a framework for comparing human and machine perception in medical diagnosis. The two are compared with respect to their sensitivity to the removal of clinically meaningful information, and to the regions of an image deemed most suspicious. Drawing inspiration from the natural image domain, we frame both comparisons in terms of perturbation robustness. The novelty of our framework is that separate analyses are performed for subgroups with clinically meaningful differences. We argue that this is necessary in order to avert Simpson's paradox and draw correct conclusions. We demonstrate our framework with a case study in breast cancer screening, and reveal significant differences between radiologists and DNNs. We compare the two with respect to their robustness to Gaussian low-pass filtering, performing a subgroup analysis on microcalcifications and soft tissue lesions. For microcalcifications, DNNs use a separate set of high frequency components than radiologists, some of which lie outside the image regions considered most suspicious by radiologists. These features run the risk of being spurious, but if not, could represent potential new biomarkers. For soft tissue lesions, the divergence between radiologists and DNNs is even starker, with DNNs relying heavily on spurious high frequency components ignored by radiologists. Importantly, this deviation in soft tissue lesions was only observable through subgroup analysis, which highlights the importance of incorporating medical domain knowledge into our comparison framework.



### Uncertainty-Aware Physically-Guided Proxy Tasks for Unseen Domain Face Anti-spoofing
- **Arxiv ID**: http://arxiv.org/abs/2011.14054v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.14054v1)
- **Published**: 2020-11-28 03:22:26+00:00
- **Updated**: 2020-11-28 03:22:26+00:00
- **Authors**: Junru Wu, Xiang Yu, Buyu Liu, Zhangyang Wang, Manmohan Chandraker
- **Comment**: None
- **Journal**: None
- **Summary**: Face anti-spoofing (FAS) seeks to discriminate genuine faces from fake ones arising from any type of spoofing attack. Due to the wide varieties of attacks, it is implausible to obtain training data that spans all attack types. We propose to leverage physical cues to attain better generalization on unseen domains. As a specific demonstration, we use physically guided proxy cues such as depth, reflection, and material to complement our main anti-spoofing (a.k.a liveness detection) task, with the intuition that genuine faces across domains have consistent face-like geometry, minimal reflection, and skin material. We introduce a novel uncertainty-aware attention scheme that independently learns to weigh the relative contributions of the main and proxy tasks, preventing the over-confident issue with traditional attention modules. Further, we propose attribute-assisted hard negative mining to disentangle liveness-irrelevant features with liveness features during learning. We evaluate extensively on public benchmarks with intra-dataset and inter-dataset protocols. Our method achieves the superior performance especially in unseen domain generalization for FAS.



### Efficient Attention Network: Accelerate Attention by Searching Where to Plug
- **Arxiv ID**: http://arxiv.org/abs/2011.14058v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.14058v2)
- **Published**: 2020-11-28 03:31:08+00:00
- **Updated**: 2021-07-11 12:44:58+00:00
- **Authors**: Zhongzhan Huang, Senwei Liang, Mingfu Liang, Wei He, Haizhao Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, many plug-and-play self-attention modules are proposed to enhance the model generalization by exploiting the internal information of deep convolutional neural networks (CNNs). Previous works lay an emphasis on the design of attention module for specific functionality, e.g., light-weighted or task-oriented attention. However, they ignore the importance of where to plug in the attention module since they connect the modules individually with each block of the entire CNN backbone for granted, leading to incremental computational cost and number of parameters with the growth of network depth. Thus, we propose a framework called Efficient Attention Network (EAN) to improve the efficiency for the existing attention modules. In EAN, we leverage the sharing mechanism (Huang et al. 2020) to share the attention module within the backbone and search where to connect the shared attention module via reinforcement learning. Finally, we obtain the attention network with sparse connections between the backbone and modules, while (1) maintaining accuracy (2) reducing extra parameter increment and (3) accelerating inference. Extensive experiments on widely-used benchmarks and popular attention networks show the effectiveness of EAN. Furthermore, we empirically illustrate that our EAN has the capacity of transferring to other tasks and capturing the informative features. The code is available at https://github.com/gbup-group/EAN-efficient-attention-network.



### Movement Tracks for the Automatic Detection of Fish Behavior in Videos
- **Arxiv ID**: http://arxiv.org/abs/2011.14070v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.14070v1)
- **Published**: 2020-11-28 05:51:19+00:00
- **Updated**: 2020-11-28 05:51:19+00:00
- **Authors**: Declan McIntosh, Tunai Porto Marques, Alexandra Branzan Albu, Rodney Rountree, Fabio De Leo
- **Comment**: 8 pages, To be published in NeurIPS 2020 Workshop Tackling Climate
  Change with Machine Learning
- **Journal**: None
- **Summary**: Global warming is predicted to profoundly impact ocean ecosystems. Fish behavior is an important indicator of changes in such marine environments. Thus, the automatic identification of key fish behavior in videos represents a much needed tool for marine researchers, enabling them to study climate change-related phenomena. We offer a dataset of sablefish (Anoplopoma fimbria) startle behaviors in underwater videos, and investigate the use of deep learning (DL) methods for behavior detection on it. Our proposed detection system identifies fish instances using DL-based frameworks, determines trajectory tracks, derives novel behavior-specific features, and employs Long Short-Term Memory (LSTM) networks to identify startle behavior in sablefish. Its performance is studied by comparing it with a state-of-the-art DL-based video event detector.



### OpenKBP: The open-access knowledge-based planning grand challenge
- **Arxiv ID**: http://arxiv.org/abs/2011.14076v2
- **DOI**: 10.1002/mp.14845
- **Categories**: **physics.med-ph**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.14076v2)
- **Published**: 2020-11-28 06:45:06+00:00
- **Updated**: 2021-01-13 19:46:18+00:00
- **Authors**: Aaron Babier, Binghao Zhang, Rafid Mahmood, Kevin L. Moore, Thomas G. Purdie, Andrea L. McNiven, Timothy C. Y. Chan
- **Comment**: 26 pages, 6 figures, 5 tables
- **Journal**: None
- **Summary**: The purpose of this work is to advance fair and consistent comparisons of dose prediction methods for knowledge-based planning (KBP) in radiation therapy research. We hosted OpenKBP, a 2020 AAPM Grand Challenge, and challenged participants to develop the best method for predicting the dose of contoured CT images. The models were evaluated according to two separate scores: (1) dose score, which evaluates the full 3D dose distributions, and (2) dose-volume histogram (DVH) score, which evaluates a set DVH metrics. Participants were given the data of 340 patients who were treated for head-and-neck cancer with radiation therapy. The data was partitioned into training (n=200), validation (n=40), and testing (n=100) datasets. All participants performed training and validation with the corresponding datasets during the validation phase of the Challenge, and we ranked the models in the testing phase based on out-of-sample performance. The Challenge attracted 195 participants from 28 countries, and 73 of those participants formed 44 teams in the validation phase, which received a total of 1750 submissions. The testing phase garnered submissions from 28 teams. On average, over the course of the validation phase, participants improved the dose and DVH scores of their models by a factor of 2.7 and 5.7, respectively. In the testing phase one model achieved significantly better dose and DVH score than the runner-up models. Lastly, many of the top performing teams reported using generalizable techniques (e.g., ensembles) to achieve higher performance than their competition. This is the first competition for knowledge-based planning research, and it helped launch the first platform for comparing KBP prediction methods fairly and consistently. The OpenKBP datasets are available publicly to help benchmark future KBP research, which has also democratized KBP research by making it accessible to everyone.



### FreezeNet: Full Performance by Reduced Storage Costs
- **Arxiv ID**: http://arxiv.org/abs/2011.14087v1
- **DOI**: 10.1007/978-3-030-69544-6_41
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.14087v1)
- **Published**: 2020-11-28 08:32:44+00:00
- **Updated**: 2020-11-28 08:32:44+00:00
- **Authors**: Paul Wimmer, Jens Mehnert, Alexandru Condurache
- **Comment**: Conference Paper of the Asian Conference on Computer Vision (ACCV)
  2020
- **Journal**: ACCV (6) 2020: 685-701
- **Summary**: Pruning generates sparse networks by setting parameters to zero. In this work we improve one-shot pruning methods, applied before training, without adding any additional storage costs while preserving the sparse gradient computations. The main difference to pruning is that we do not sparsify the network's weights but learn just a few key parameters and keep the other ones fixed at their random initialized value. This mechanism is called freezing the parameters. Those frozen weights can be stored efficiently with a single 32bit random seed number. The parameters to be frozen are determined one-shot by a single for- and backward pass applied before training starts. We call the introduced method FreezeNet. In our experiments we show that FreezeNets achieve good results, especially for extreme freezing rates. Freezing weights preserves the gradient flow throughout the network and consequently, FreezeNets train better and have an increased capacity compared to their pruned counterparts. On the classification tasks MNIST and CIFAR-10/100 we outperform SNIP, in this setting the best reported one-shot pruning method, applied before training. On MNIST, FreezeNet achieves 99.2% performance of the baseline LeNet-5-Caffe architecture, while compressing the number of trained and stored parameters by a factor of x 157.



### Time Series Change Point Detection with Self-Supervised Contrastive Predictive Coding
- **Arxiv ID**: http://arxiv.org/abs/2011.14097v5
- **DOI**: 10.1145/3442381.3449903
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.14097v5)
- **Published**: 2020-11-28 09:36:18+00:00
- **Updated**: 2021-03-05 00:24:56+00:00
- **Authors**: Shohreh Deldari, Daniel V. Smith, Hao Xue, Flora D. Salim
- **Comment**: Accepted at The WEB Conference 2021 (WWW'21)
- **Journal**: None
- **Summary**: Change Point Detection (CPD) methods identify the times associated with changes in the trends and properties of time series data in order to describe the underlying behaviour of the system. For instance, detecting the changes and anomalies associated with web service usage, application usage or human behaviour can provide valuable insights for downstream modelling tasks. We propose a novel approach for self-supervised Time Series Change Point detection method based onContrastivePredictive coding (TS-CP^2). TS-CP^2 is the first approach to employ a contrastive learning strategy for CPD by learning an embedded representation that separates pairs of embeddings of time adjacent intervals from pairs of interval embeddings separated across time. Through extensive experiments on three diverse, widely used time series datasets, we demonstrate that our method outperforms five state-of-the-art CPD methods, which include unsupervised and semi-supervisedapproaches. TS-CP^2 is shown to improve the performance of methods that use either handcrafted statistical or temporal features by 79.4% and deep learning-based methods by 17.0% with respect to the F1-score averaged across the three datasets.



### Semi-Supervised Learning for Sparsely-Labeled Sequential Data: Application to Healthcare Video Processing
- **Arxiv ID**: http://arxiv.org/abs/2011.14101v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.14101v5)
- **Published**: 2020-11-28 09:54:44+00:00
- **Updated**: 2022-10-01 18:25:20+00:00
- **Authors**: Florian Dubost, Erin Hong, Nandita Bhaskhar, Siyi Tang, Daniel Rubin, Christopher Lee-Messer
- **Comment**: None
- **Journal**: In Proceedings of the IEEE/CVF Winter Conference on Applications
  of Computer Vision, 2023
- **Summary**: Labeled data is a critical resource for training and evaluating machine learning models. However, many real-life datasets are only partially labeled. We propose a semi-supervised machine learning training strategy to improve event detection performance on sequential data, such as video recordings, when only sparse labels are available, such as event start times without their corresponding end times. Our method uses noisy guesses of the events' end times to train event detection models. Depending on how conservative these guesses are, mislabeled samples may be introduced into the training set. We further propose a mathematical model for explaining and estimating the evolution of the classification performance for increasingly noisier end time estimates. We show that neural networks can improve their detection performance by leveraging more training data with less conservative approximations despite the higher proportion of incorrect labels. We adapt sequential versions of CIFAR-10 and MNIST, and use the Berkeley MHAD and HMBD51 video datasets to empirically evaluate our method, and find that our risk-tolerant strategy outperforms conservative estimates by 3.5 points of mean average precision for CIFAR, 30 points for MNIST, 3 points for MHAD, and 14 points for HMBD51. Then, we leverage the proposed training strategy to tackle a real-life application: processing continuous video recordings of epilepsy patients, and show that our method outperforms baseline labeling methods by 17 points of average precision, and reaches a classification performance similar to that of fully supervised models. We share part of the code for this article.



### Hijack-GAN: Unintended-Use of Pretrained, Black-Box GANs
- **Arxiv ID**: http://arxiv.org/abs/2011.14107v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/2011.14107v2)
- **Published**: 2020-11-28 11:07:36+00:00
- **Updated**: 2021-03-29 13:20:00+00:00
- **Authors**: Hui-Po Wang, Ning Yu, Mario Fritz
- **Comment**: Accepted by CVPR 2021
- **Journal**: None
- **Summary**: While Generative Adversarial Networks (GANs) show increasing performance and the level of realism is becoming indistinguishable from natural images, this also comes with high demands on data and computation. We show that state-of-the-art GAN models -- such as they are being publicly released by researchers and industry -- can be used for a range of applications beyond unconditional image generation. We achieve this by an iterative scheme that also allows gaining control over the image generation process despite the highly non-linear latent spaces of the latest GAN models. We demonstrate that this opens up the possibility to re-use state-of-the-art, difficult to train, pre-trained GANs with a high level of control even if only black-box access is granted. Our work also raises concerns and awareness that the use cases of a published GAN model may well reach beyond the creators' intention, which needs to be taken into account before a full public release. Code is available at https://github.com/a514514772/hijackgan.



### Robotic grasp detection using a novel two-stage approach
- **Arxiv ID**: http://arxiv.org/abs/2011.14123v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.14123v1)
- **Published**: 2020-11-28 12:26:35+00:00
- **Updated**: 2020-11-28 12:26:35+00:00
- **Authors**: Zhe Chu, Mengkai Hu, Xiangyu Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, deep learning has been successfully applied to robotic grasp detection. Based on convolutional neural networks (CNNs), there have been lots of end-to-end detection approaches. But end-to-end approaches have strict requirements for the dataset used for training the neural network models and it's hard to achieve in practical use. Therefore, we proposed a two-stage approach using particle swarm optimizer (PSO) candidate estimator and CNN to detect the most likely grasp. Our approach achieved an accuracy of 92.8% on the Cornell Grasp Dataset, which leaped into the front ranks of the existing approaches and is able to run at real-time speeds. After a small change of the approach, we can predict multiple grasps per object in the meantime so that an object can be grasped in a variety of ways.



### MIINet: An Image Quality Improvement Framework for Supporting Medical Diagnosis
- **Arxiv ID**: http://arxiv.org/abs/2011.14132v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.14132v1)
- **Published**: 2020-11-28 13:44:42+00:00
- **Updated**: 2020-11-28 13:44:42+00:00
- **Authors**: Quan Huu Cap, Hitoshi Iyatomi, Atsushi Fukuda
- **Comment**: Accepted at the ICPR2020 Workshops
- **Journal**: None
- **Summary**: Medical images have been indispensable and useful tools for supporting medical experts in making diagnostic decisions. However, taken medical images especially throat and endoscopy images are normally hazy, lack of focus, or uneven illumination. Thus, these could difficult the diagnosis process for doctors. In this paper, we propose MIINet, a novel image-to-image translation network for improving quality of medical images by unsupervised translating low-quality images to the high-quality clean version. Our MIINet is not only capable of generating high-resolution clean images, but also preserving the attributes of original images, making the diagnostic more favorable for doctors. Experiments on dehazing 100 practical throat images show that our MIINet largely improves the mean doctor opinion score (MDOS), which assesses the quality and the reproducibility of the images from the baseline of 2.36 to 4.11, while dehazed images by CycleGAN got lower score of 3.83. The MIINet is confirmed by three physicians to be satisfying in supporting throat disease diagnostic from original low-quality images.



### Towards Fast and Light-Weight Restoration of Dark Images
- **Arxiv ID**: http://arxiv.org/abs/2011.14133v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2011.14133v1)
- **Published**: 2020-11-28 13:53:50+00:00
- **Updated**: 2020-11-28 13:53:50+00:00
- **Authors**: Mohit Lamba, Atul Balaji, Kaushik Mitra
- **Comment**: None
- **Journal**: None
- **Summary**: The ability to capture good quality images in the dark and near-zero lux conditions has been a long-standing pursuit of the computer vision community. The seminal work by Chen et al. [5] has especially caused renewed interest in this area, resulting in methods that build on top of their work in a bid to improve the reconstruction. However, for practical utility and deployment of low-light enhancement algorithms on edge devices such as embedded systems, surveillance cameras, autonomous robots and smartphones, the solution must respect additional constraints such as limited GPU memory and processing power. With this in mind, we propose a deep neural network architecture that aims to strike a balance between the network latency, memory utilization, model parameters, and reconstruction quality. The key idea is to forbid computations in the High-Resolution (HR) space and limit them to a Low-Resolution (LR) space. However, doing the bulk of computations in the LR space causes artifacts in the restored image. We thus propose Pack and UnPack operations, which allow us to effectively transit between the HR and LR spaces without incurring much artifacts in the restored image. We show that we can enhance a full resolution, 2848 x 4256, extremely dark single-image in the ballpark of 3 seconds even on a CPU. We achieve this with 2 - 7x fewer model parameters, 2 - 3x lower memory utilization, 5 - 20x speed up and yet maintain a competitive image reconstruction quality compared to the state-of-the-art algorithms.



### Retrospective Motion Correction of MR Images using Prior-Assisted Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2011.14134v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.14134v1)
- **Published**: 2020-11-28 14:03:59+00:00
- **Updated**: 2020-11-28 14:03:59+00:00
- **Authors**: Soumick Chatterjee, Alessandro Sciarra, Max Dünnwald, Steffen Oeltze-Jafra, Andreas Nürnberger, Oliver Speck
- **Comment**: None
- **Journal**: Medical Imaging Meets NeurIPS 2020
- **Summary**: In MRI, motion artefacts are among the most common types of artefacts. They can degrade images and render them unusable for accurate diagnosis. Traditional methods, such as prospective or retrospective motion correction, have been proposed to avoid or alleviate motion artefacts. Recently, several other methods based on deep learning approaches have been proposed to solve this problem. This work proposes to enhance the performance of existing deep learning models by the inclusion of additional information present as image priors. The proposed approach has shown promising results and will be further investigated for clinical validity.



### AdaBins: Depth Estimation using Adaptive Bins
- **Arxiv ID**: http://arxiv.org/abs/2011.14141v1
- **DOI**: 10.1109/CVPR46437.2021.00400
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.14141v1)
- **Published**: 2020-11-28 14:40:45+00:00
- **Updated**: 2020-11-28 14:40:45+00:00
- **Authors**: Shariq Farooq Bhat, Ibraheem Alhashim, Peter Wonka
- **Comment**: 13 pages
- **Journal**: None
- **Summary**: We address the problem of estimating a high quality dense depth map from a single RGB input image. We start out with a baseline encoder-decoder convolutional neural network architecture and pose the question of how the global processing of information can help improve overall depth estimation. To this end, we propose a transformer-based architecture block that divides the depth range into bins whose center value is estimated adaptively per image. The final depth values are estimated as linear combinations of the bin centers. We call our new building block AdaBins. Our results show a decisive improvement over the state-of-the-art on several popular depth datasets across all metrics. We also validate the effectiveness of the proposed block with an ablation study and provide the code and corresponding pre-trained weights of the new state-of-the-art model.



### i3DMM: Deep Implicit 3D Morphable Model of Human Heads
- **Arxiv ID**: http://arxiv.org/abs/2011.14143v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.14143v1)
- **Published**: 2020-11-28 15:01:53+00:00
- **Updated**: 2020-11-28 15:01:53+00:00
- **Authors**: Tarun Yenamandra, Ayush Tewari, Florian Bernard, Hans-Peter Seidel, Mohamed Elgharib, Daniel Cremers, Christian Theobalt
- **Comment**: Project page: http://gvv.mpi-inf.mpg.de/projects/i3DMM/
- **Journal**: None
- **Summary**: We present the first deep implicit 3D morphable model (i3DMM) of full heads. Unlike earlier morphable face models it not only captures identity-specific geometry, texture, and expressions of the frontal face, but also models the entire head, including hair. We collect a new dataset consisting of 64 people with different expressions and hairstyles to train i3DMM. Our approach has the following favorable properties: (i) It is the first full head morphable model that includes hair. (ii) In contrast to mesh-based models it can be trained on merely rigidly aligned scans, without requiring difficult non-rigid registration. (iii) We design a novel architecture to decouple the shape model into an implicit reference shape and a deformation of this reference shape. With that, dense correspondences between shapes can be learned implicitly. (iv) This architecture allows us to semantically disentangle the geometry and color components, as color is learned in the reference space. Geometry is further disentangled as identity, expressions, and hairstyle, while color is disentangled as identity and hairstyle components. We show the merits of i3DMM using ablation studies, comparisons to state-of-the-art models, and applications such as semantic head editing and texture transfer. We will make our model publicly available.



### Batch Normalization with Enhanced Linear Transformation
- **Arxiv ID**: http://arxiv.org/abs/2011.14150v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.14150v1)
- **Published**: 2020-11-28 15:42:36+00:00
- **Updated**: 2020-11-28 15:42:36+00:00
- **Authors**: Yuhui Xu, Lingxi Xie, Cihang Xie, Jieru Mei, Siyuan Qiao, Wei Shen, Hongkai Xiong, Alan Yuille
- **Comment**: 12 pages. The code is available at
  https://github.com/yuhuixu1993/BNET
- **Journal**: None
- **Summary**: Batch normalization (BN) is a fundamental unit in modern deep networks, in which a linear transformation module was designed for improving BN's flexibility of fitting complex data distributions. In this paper, we demonstrate properly enhancing this linear transformation module can effectively improve the ability of BN. Specifically, rather than using a single neuron, we propose to additionally consider each neuron's neighborhood for calculating the outputs of the linear transformation. Our method, named BNET, can be implemented with 2-3 lines of code in most deep learning libraries. Despite the simplicity, BNET brings consistent performance gains over a wide range of backbones and visual benchmarks. Moreover, we verify that BNET accelerates the convergence of network training and enhances spatial information by assigning the important neurons with larger weights accordingly. The code is available at https://github.com/yuhuixu1993/BNET.



### Inter-slice Context Residual Learning for 3D Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2011.14155v1
- **DOI**: 10.1109/TMI.2020.3034995
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.14155v1)
- **Published**: 2020-11-28 16:03:39+00:00
- **Updated**: 2020-11-28 16:03:39+00:00
- **Authors**: Jianpeng Zhang, Yutong Xie, Yan Wang, Yong Xia
- **Comment**: Accpeted by IEEE-TMI
- **Journal**: IEEE Trans. Med. Imaging (2020)
- **Summary**: Automated and accurate 3D medical image segmentation plays an essential role in assisting medical professionals to evaluate disease progresses and make fast therapeutic schedules. Although deep convolutional neural networks (DCNNs) have widely applied to this task, the accuracy of these models still need to be further improved mainly due to their limited ability to 3D context perception. In this paper, we propose the 3D context residual network (ConResNet) for the accurate segmentation of 3D medical images. This model consists of an encoder, a segmentation decoder, and a context residual decoder. We design the context residual module and use it to bridge both decoders at each scale. Each context residual module contains both context residual mapping and context attention mapping, the formal aims to explicitly learn the inter-slice context information and the latter uses such context as a kind of attention to boost the segmentation accuracy. We evaluated this model on the MICCAI 2018 Brain Tumor Segmentation (BraTS) dataset and NIH Pancreas Segmentation (Pancreas-CT) dataset. Our results not only demonstrate the effectiveness of the proposed 3D context residual learning scheme but also indicate that the proposed ConResNet is more accurate than six top-ranking methods in brain tumor segmentation and seven top-ranking methods in pancreas segmentation. Code is available at https://git.io/ConResNet



### Towards Robust Partially Supervised Multi-Structure Medical Image Segmentation on Small-Scale Data
- **Arxiv ID**: http://arxiv.org/abs/2011.14164v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2011.14164v2)
- **Published**: 2020-11-28 16:31:00+00:00
- **Updated**: 2021-10-26 10:47:18+00:00
- **Authors**: Nanqing Dong, Michael Kampffmeyer, Xiaodan Liang, Min Xu, Irina Voiculescu, Eric P. Xing
- **Comment**: Accepted by Applied Soft Computing
- **Journal**: None
- **Summary**: The data-driven nature of deep learning (DL) models for semantic segmentation requires a large number of pixel-level annotations. However, large-scale and fully labeled medical datasets are often unavailable for practical tasks. Recently, partially supervised methods have been proposed to utilize images with incomplete labels in the medical domain. To bridge the methodological gaps in partially supervised learning (PSL) under data scarcity, we propose Vicinal Labels Under Uncertainty (VLUU), a simple yet efficient framework utilizing the human structure similarity for partially supervised medical image segmentation. Motivated by multi-task learning and vicinal risk minimization, VLUU transforms the partially supervised problem into a fully supervised problem by generating vicinal labels. We systematically evaluate VLUU under the challenges of small-scale data, dataset shift, and class imbalance on two commonly used segmentation datasets for the tasks of chest organ segmentation and optic disc-and-cup segmentation. The experimental results show that VLUU can consistently outperform previous partially supervised models in these settings. Our research suggests a new research direction in label-efficient deep learning with partial supervision.



### Lattice Fusion Networks for Image Denoising
- **Arxiv ID**: http://arxiv.org/abs/2011.14196v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.14196v3)
- **Published**: 2020-11-28 18:57:54+00:00
- **Updated**: 2021-07-01 17:27:18+00:00
- **Authors**: Seyed Mohsen Hosseini
- **Comment**: None
- **Journal**: None
- **Summary**: A novel method for feature fusion in convolutional neural networks is proposed in this paper. Different feature fusion techniques are suggested to facilitate the flow of information and improve the training of deep neural networks. Some of these techniques as well as the proposed network can be considered a type of Directed Acyclic Graph (DAG) Network, where a layer can receive inputs from other layers and have outputs to other layers. In the proposed general framework of Lattice Fusion Network (LFNet), feature maps of each convolutional layer are passed to other layers based on a lattice graph structure, where nodes are convolutional layers. To evaluate the performance of the proposed architecture, different designs based on the general framework of LFNet are implemented for the task of image denoising. This task is used as an example where training deep convolutional networks is needed. Results are compared with state of the art methods. The proposed network is able to achieve better results with far fewer learnable parameters, which shows the effectiveness of LFNets for training of deep neural networks.



### E-Pro: Euler Angle and Probabilistic Model for Face Detection and Recognition
- **Arxiv ID**: http://arxiv.org/abs/2011.14200v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.14200v1)
- **Published**: 2020-11-28 19:12:39+00:00
- **Updated**: 2020-11-28 19:12:39+00:00
- **Authors**: Sandesh Ramesh, Manoj Kumar M V, Sanjay H A
- **Comment**: 4th International Conference on Inventive Systems and Control
  (ICISC), 2020
- **Journal**: None
- **Summary**: It is human nature to give prime importance to facial appearances. Often, to look good is to feel good. Also, facial features are unique to every individual on this planet, which means it is a source of vital information. This work proposes a framework named E-Pro for the detection and recognition of faces by taking facial images as inputs. E-Pro has its potential application in various domains, namely attendance, surveillance, crowd monitoring, biometric-based authentication etc. E-Pro is developed here as a mobile application that aims to aid lecturers to mark attendance in a classroom by detecting and recognizing the faces of students from a picture clicked through the app. E-Pro has been developed using Google Firebase Face Recognition APIs, which uses Euler Angles, and Probabilistic Model. E-Pro has been tested on stock images and the experimental results are promising.



### Class-agnostic Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2011.14204v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2011.14204v1)
- **Published**: 2020-11-28 19:22:38+00:00
- **Updated**: 2020-11-28 19:22:38+00:00
- **Authors**: Ayush Jaiswal, Yue Wu, Pradeep Natarajan, Premkumar Natarajan
- **Comment**: To appear in Proceedings of WACV 2021
- **Journal**: None
- **Summary**: Object detection models perform well at localizing and classifying objects that they are shown during training. However, due to the difficulty and cost associated with creating and annotating detection datasets, trained models detect a limited number of object types with unknown objects treated as background content. This hinders the adoption of conventional detectors in real-world applications like large-scale object matching, visual grounding, visual relation prediction, obstacle detection (where it is more important to determine the presence and location of objects than to find specific types), etc. We propose class-agnostic object detection as a new problem that focuses on detecting objects irrespective of their object-classes. Specifically, the goal is to predict bounding boxes for all objects in an image but not their object-classes. The predicted boxes can then be consumed by another system to perform application-specific classification, retrieval, etc. We propose training and evaluation protocols for benchmarking class-agnostic detectors to advance future research in this domain. Finally, we propose (1) baseline methods and (2) a new adversarial learning framework for class-agnostic detection that forces the model to exclude class-specific information from features used for predictions. Experimental results show that adversarial learning improves class-agnostic detection efficacy.



### Curvature Regularization to Prevent Distortion in Graph Embedding
- **Arxiv ID**: http://arxiv.org/abs/2011.14211v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2011.14211v1)
- **Published**: 2020-11-28 20:16:24+00:00
- **Updated**: 2020-11-28 20:16:24+00:00
- **Authors**: Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Chunxu Zhang, Bo Yang
- **Comment**: Published as a conference paper at NeurIPS 2020
- **Journal**: None
- **Summary**: Recent research on graph embedding has achieved success in various applications. Most graph embedding methods preserve the proximity in a graph into a manifold in an embedding space. We argue an important but neglected problem about this proximity-preserving strategy: Graph topology patterns, while preserved well into an embedding manifold by preserving proximity, may distort in the ambient embedding Euclidean space, and hence to detect them becomes difficult for machine learning models. To address the problem, we propose curvature regularization, to enforce flatness for embedding manifolds, thereby preventing the distortion. We present a novel angle-based sectional curvature, termed ABS curvature, and accordingly three kinds of curvature regularization to induce flat embedding manifolds during graph embedding. We integrate curvature regularization into five popular proximity-preserving embedding methods, and empirical results in two applications show significant improvements on a wide range of open graph datasets.



### Truly shift-invariant convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/2011.14214v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.14214v4)
- **Published**: 2020-11-28 20:57:35+00:00
- **Updated**: 2021-03-30 19:47:57+00:00
- **Authors**: Anadi Chaman, Ivan Dokmanić
- **Comment**: None
- **Journal**: None
- **Summary**: Thanks to the use of convolution and pooling layers, convolutional neural networks were for a long time thought to be shift-invariant. However, recent works have shown that the output of a CNN can change significantly with small shifts in input: a problem caused by the presence of downsampling (stride) layers. The existing solutions rely either on data augmentation or on anti-aliasing, both of which have limitations and neither of which enables perfect shift invariance. Additionally, the gains obtained from these methods do not extend to image patterns not seen during training. To address these challenges, we propose adaptive polyphase sampling (APS), a simple sub-sampling scheme that allows convolutional neural networks to achieve 100% consistency in classification performance under shifts, without any loss in accuracy. With APS, the networks exhibit perfect consistency to shifts even before training, making it the first approach that makes convolutional neural networks truly shift-invariant.



### FaceGuard: A Self-Supervised Defense Against Adversarial Face Images
- **Arxiv ID**: http://arxiv.org/abs/2011.14218v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.14218v2)
- **Published**: 2020-11-28 21:18:46+00:00
- **Updated**: 2021-04-05 20:37:56+00:00
- **Authors**: Debayan Deb, Xiaoming Liu, Anil K. Jain
- **Comment**: None
- **Journal**: None
- **Summary**: Prevailing defense mechanisms against adversarial face images tend to overfit to the adversarial perturbations in the training set and fail to generalize to unseen adversarial attacks. We propose a new self-supervised adversarial defense framework, namely FaceGuard, that can automatically detect, localize, and purify a wide variety of adversarial faces without utilizing pre-computed adversarial training samples. During training, FaceGuard automatically synthesizes challenging and diverse adversarial attacks, enabling a classifier to learn to distinguish them from real faces and a purifier attempts to remove the adversarial perturbations in the image space. Experimental results on LFW dataset show that FaceGuard can achieve 99.81% detection accuracy on six unseen adversarial attack types. In addition, the proposed method can enhance the face recognition performance of ArcFace from 34.27% TAR @ 0.1% FAR under no defense to 77.46% TAR @ 0.1% FAR.



### Deep Learning for Regularization Prediction in Diffeomorphic Image Registration
- **Arxiv ID**: http://arxiv.org/abs/2011.14229v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.14229v3)
- **Published**: 2020-11-28 22:56:44+00:00
- **Updated**: 2022-02-04 21:42:56+00:00
- **Authors**: Jian Wang, Miaomiao Zhang
- **Comment**: 20 pages, 8 figures
- **Journal**: Machine Learning for Biomedical Imaging (MELBA) 2022
- **Summary**: This paper presents a predictive model for estimating regularization parameters of diffeomorphic image registration. We introduce a novel framework that automatically determines the parameters controlling the smoothness of diffeomorphic transformations. Our method significantly reduces the effort of parameter tuning, which is time and labor-consuming. To achieve the goal, we develop a predictive model based on deep convolutional neural networks (CNN) that learns the mapping between pairwise images and the regularization parameter of image registration. In contrast to previous methods that estimate such parameters in a high-dimensional image space, our model is built in an efficient bandlimited space with much lower dimensions. We demonstrate the effectiveness of our model on both 2D synthetic data and 3D real brain images. Experimental results show that our model not only predicts appropriate regularization parameters for image registration, but also improving the network training in terms of time and memory efficiency.



