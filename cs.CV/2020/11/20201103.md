# Arxiv Papers in cs.CV on 2020-11-03
### Parameter Efficient Deep Neural Networks with Bilinear Projections
- **Arxiv ID**: http://arxiv.org/abs/2011.01391v1
- **DOI**: 10.1109/TNNLS.2020.3016688
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.01391v1)
- **Published**: 2020-11-03 00:17:24+00:00
- **Updated**: 2020-11-03 00:17:24+00:00
- **Authors**: Litao Yu, Yongsheng Gao, Jun Zhou, Jian Zhang
- **Comment**: None
- **Journal**: IEEE Transactions on Neural Networks and Learning Systems 2020
- **Summary**: Recent research on deep neural networks (DNNs) has primarily focused on improving the model accuracy. Given a proper deep learning framework, it is generally possible to increase the depth or layer width to achieve a higher level of accuracy. However, the huge number of model parameters imposes more computational and memory usage overhead and leads to the parameter redundancy. In this paper, we address the parameter redundancy problem in DNNs by replacing conventional full projections with bilinear projections. For a fully-connected layer with $D$ input nodes and $D$ output nodes, applying bilinear projection can reduce the model space complexity from $\mathcal{O}(D^2)$ to $\mathcal{O}(2D)$, achieving a deep model with a sub-linear layer size. However, structured projection has a lower freedom of degree compared to the full projection, causing the under-fitting problem. So we simply scale up the mapping size by increasing the number of output channels, which can keep and even boosts the model accuracy. This makes it very parameter-efficient and handy to deploy such deep models on mobile systems with memory limitations. Experiments on four benchmark datasets show that applying the proposed bilinear projection to deep neural networks can achieve even higher accuracies than conventional full DNNs, while significantly reduces the model size.



### Faraway-Frustum: Dealing with Lidar Sparsity for 3D Object Detection using Fusion
- **Arxiv ID**: http://arxiv.org/abs/2011.01404v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.01404v3)
- **Published**: 2020-11-03 01:14:51+00:00
- **Updated**: 2021-03-27 15:31:29+00:00
- **Authors**: Haolin Zhang, Dongfang Yang, Ekim Yurtsever, Keith A. Redmill, Ümit Özgüner
- **Comment**: None
- **Journal**: None
- **Summary**: Learned pointcloud representations do not generalize well with an increase in distance to the sensor. For example, at a range greater than 60 meters, the sparsity of lidar pointclouds reaches to a point where even humans cannot discern object shapes from each other. However, this distance should not be considered very far for fast-moving vehicles: A vehicle can traverse 60 meters under two seconds while moving at 70 mph. For safe and robust driving automation, acute 3D object detection at these ranges is indispensable. Against this backdrop, we introduce faraway-frustum: a novel fusion strategy for detecting faraway objects. The main strategy is to depend solely on the 2D vision for recognizing object class, as object shape does not change drastically with an increase in depth, and use pointcloud data for object localization in the 3D space for faraway objects. For closer objects, we use learned pointcloud representations instead, following state-of-the-art. This strategy alleviates the main shortcoming of object detection with learned pointcloud representations. Experiments on the KITTI dataset demonstrate that our method outperforms state-of-the-art by a considerable margin for faraway object detection in bird's-eye-view and 3D. Our code is open-source and publicly available: https://github.com/dongfang-steven-yang/faraway-frustum.



### BIGPrior: Towards Decoupling Learned Prior Hallucination and Data Fidelity in Image Restoration
- **Arxiv ID**: http://arxiv.org/abs/2011.01406v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.01406v3)
- **Published**: 2020-11-03 01:16:41+00:00
- **Updated**: 2022-01-08 11:47:43+00:00
- **Authors**: Majed El Helou, Sabine Süsstrunk
- **Comment**: IEEE TIP 2022. Code available on
  https://github.com/majedelhelou/BIGPrior. Main change relative to v1: added
  Table VI and computation times
- **Journal**: None
- **Summary**: Classic image-restoration algorithms use a variety of priors, either implicitly or explicitly. Their priors are hand-designed and their corresponding weights are heuristically assigned. Hence, deep learning methods often produce superior image restoration quality. Deep networks are, however, capable of inducing strong and hardly predictable hallucinations. Networks implicitly learn to be jointly faithful to the observed data while learning an image prior; and the separation of original data and hallucinated data downstream is then not possible. This limits their wide-spread adoption in image restoration. Furthermore, it is often the hallucinated part that is victim to degradation-model overfitting.   We present an approach with decoupled network-prior based hallucination and data fidelity terms. We refer to our framework as the Bayesian Integration of a Generative Prior (BIGPrior). Our method is rooted in a Bayesian framework and tightly connected to classic restoration methods. In fact, it can be viewed as a generalization of a large family of classic restoration algorithms. We use network inversion to extract image prior information from a generative network. We show that, on image colorization, inpainting and denoising, our framework consistently improves the inversion results. Our method, though partly reliant on the quality of the generative network inversion, is competitive with state-of-the-art supervised and task-specific restoration methods. It also provides an additional metric that sets forth the degree of prior reliance per pixel relative to data fidelity.



### Out-of-Distribution Detection for Automotive Perception
- **Arxiv ID**: http://arxiv.org/abs/2011.01413v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO, I.2.10; I.2.9
- **Links**: [PDF](http://arxiv.org/pdf/2011.01413v2)
- **Published**: 2020-11-03 01:46:35+00:00
- **Updated**: 2021-09-06 03:43:53+00:00
- **Authors**: Julia Nitsch, Masha Itkina, Ransalu Senanayake, Juan Nieto, Max Schmidt, Roland Siegwart, Mykel J. Kochenderfer, Cesar Cadena
- **Comment**: 6 pages, 4 figures, paper accepted at Intelligent Transportation
  Systems Conference (ITSC) 2021
- **Journal**: None
- **Summary**: Neural networks (NNs) are widely used for object classification in autonomous driving. However, NNs can fail on input data not well represented by the training dataset, known as out-of-distribution (OOD) data. A mechanism to detect OOD samples is important for safety-critical applications, such as automotive perception, to trigger a safe fallback mode. NNs often rely on softmax normalization for confidence estimation, which can lead to high confidences being assigned to OOD samples, thus hindering the detection of failures. This paper presents a method for determining whether inputs are OOD, which does not require OOD data during training and does not increase the computational cost of inference. The latter property is especially important in automotive applications with limited computational resources and real-time constraints. Our proposed approach outperforms state-of-the-art methods on real-world automotive datasets.



### Content-based Analysis of the Cultural Differences between TikTok and Douyin
- **Arxiv ID**: http://arxiv.org/abs/2011.01414v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM, cs.SI
- **Links**: [PDF](http://arxiv.org/pdf/2011.01414v1)
- **Published**: 2020-11-03 01:47:49+00:00
- **Updated**: 2020-11-03 01:47:49+00:00
- **Authors**: Li Sun, Haoqi Zhang, Songyang Zhang, Jiebo Luo
- **Comment**: Accepted by IEEE Big Data 2020
- **Journal**: None
- **Summary**: Short-form video social media shifts away from the traditional media paradigm by telling the audience a dynamic story to attract their attention. In particular, different combinations of everyday objects can be employed to represent a unique scene that is both interesting and understandable. Offered by the same company, TikTok and Douyin are popular examples of such new media that has become popular in recent years, while being tailored for different markets (e.g. the United States and China). The hypothesis that they express cultural differences together with media fashion and social idiosyncrasy is the primary target of our research. To that end, we first employ the Faster Regional Convolutional Neural Network (Faster R-CNN) pre-trained with the Microsoft Common Objects in COntext (MS-COCO) dataset to perform object detection. Based on a suite of objects detected from videos, we perform statistical analysis including label statistics, label similarity, and label-person distribution. We further use the Two-Stream Inflated 3D ConvNet (I3D) pre-trained with the Kinetics dataset to categorize and analyze human actions. By comparing the distributional results of TikTok and Douyin, we uncover a wealth of similarity and contrast between the two closely related video social media platforms along the content dimensions of object quantity, object categories, and human action categories.



### Distilling Knowledge by Mimicking Features
- **Arxiv ID**: http://arxiv.org/abs/2011.01424v2
- **DOI**: 10.1109/TPAMI.2021.3103973
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.01424v2)
- **Published**: 2020-11-03 02:15:14+00:00
- **Updated**: 2021-08-14 01:38:50+00:00
- **Authors**: Guo-Hua Wang, Yifan Ge, Jianxin Wu
- **Comment**: To appear in IEEE Trans. PAMI
- **Journal**: None
- **Summary**: Knowledge distillation (KD) is a popular method to train efficient networks ("student") with the help of high-capacity networks ("teacher"). Traditional methods use the teacher's soft logits as extra supervision to train the student network. In this paper, we argue that it is more advantageous to make the student mimic the teacher's features in the penultimate layer. Not only the student can directly learn more effective information from the teacher feature, feature mimicking can also be applied for teachers trained without a softmax layer. Experiments show that it can achieve higher accuracy than traditional KD. To further facilitate feature mimicking, we decompose a feature vector into the magnitude and the direction. We argue that the teacher should give more freedom to the student feature's magnitude, and let the student pay more attention on mimicking the feature direction. To meet this requirement, we propose a loss term based on locality-sensitive hashing (LSH). With the help of this new loss, our method indeed mimics feature directions more accurately, relaxes constraints on feature magnitudes, and achieves state-of-the-art distillation accuracy. We provide theoretical analyses of how LSH facilitates feature direction mimicking, and further extend feature mimicking to multi-label recognition and object detection.



### Self-semi-supervised Learning to Learn from NoisyLabeled Data
- **Arxiv ID**: http://arxiv.org/abs/2011.01429v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.01429v1)
- **Published**: 2020-11-03 02:31:29+00:00
- **Updated**: 2020-11-03 02:31:29+00:00
- **Authors**: Jiacheng Wang, Yue Ma, Shuang Gao
- **Comment**: None
- **Journal**: None
- **Summary**: The remarkable success of today's deep neural networks highly depends on a massive number of correctly labeled data. However, it is rather costly to obtain high-quality human-labeled data, leading to the active research area of training models robust to noisy labels. To achieve this goal, on the one hand, many papers have been dedicated to differentiating noisy labels from clean ones to increase the generalization of DNN. On the other hand, the increasingly prevalent methods of self-semi-supervised learning have been proven to benefit the tasks when labels are incomplete. By 'semi' we regard the wrongly labeled data detected as un-labeled data; by 'self' we choose a self-supervised technique to conduct semi-supervised learning. In this project, we designed methods to more accurately differentiate clean and noisy labels and borrowed the wisdom of self-semi-supervised learning to train noisy labeled data.



### "You eat with your eyes first": Optimizing Yelp Image Advertising
- **Arxiv ID**: http://arxiv.org/abs/2011.01434v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.01434v1)
- **Published**: 2020-11-03 02:49:40+00:00
- **Updated**: 2020-11-03 02:49:40+00:00
- **Authors**: Gaurab Banerjee, Samuel Spinner, Yasmine Mitchell
- **Comment**: 9 pages, 8 figures, 3 tables
- **Journal**: None
- **Summary**: A business's online, photographic representation can play a crucial role in its success or failure. We use Yelp's image dataset and star-based review system as a measurement of an image's effectiveness in promoting a business. After preprocessing the Yelp dataset, we use transfer learning to train a classifier which accepts Yelp images and predicts star-ratings. Additionally, we then train a GAN to qualitatively investigate the common properties of highly effective images. We achieve 90-98% accuracy in classifying simplified star ratings for various image categories and observe that images containing blue skies, open surroundings, and many windows are correlated with higher Yelp reviews.



### Developing High Quality Training Samples for Deep Learning Based Local Climate Zone Classification in Korea
- **Arxiv ID**: http://arxiv.org/abs/2011.01436v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2011.01436v2)
- **Published**: 2020-11-03 02:52:37+00:00
- **Updated**: 2020-12-10 09:20:52+00:00
- **Authors**: Minho Kim, Doyoung Jeong, Hyoungwoo Choi, Yongil Kim
- **Comment**: 7 pages, 7 figures; AI for Earth Workshop at NeurIPS2020
- **Journal**: None
- **Summary**: Two out of three people will be living in urban areas by 2050, as projected by the United Nations, emphasizing the need for sustainable urban development and monitoring. Common urban footprint data provide high-resolution city extents but lack essential information on the distribution, pattern, and characteristics. The Local Climate Zone (LCZ) offers an efficient and standardized framework that can delineate the internal structure and characteristics of urban areas. Global-scale LCZ mapping has been explored, but are limited by low accuracy, variable labeling quality, or domain adaptation challenges. Instead, this study developed a custom LCZ data to map key Korean cities using a multi-scale convolutional neural network. Results demonstrated that using a novel, custom LCZ data with deep learning can generate more accurate LCZ map results compared to conventional community-based LCZ mapping with machine learning as well as transfer learning of the global So2Sat dataset.



### Learning Deformable Tetrahedral Meshes for 3D Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2011.01437v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.01437v2)
- **Published**: 2020-11-03 02:57:01+00:00
- **Updated**: 2020-11-23 19:52:13+00:00
- **Authors**: Jun Gao, Wenzheng Chen, Tommy Xiang, Clement Fuji Tsang, Alec Jacobson, Morgan McGuire, Sanja Fidler
- **Comment**: Accepted to NeurIPS 2020. Webpage: https://nv-tlabs.github.io/DefTet/
- **Journal**: None
- **Summary**: 3D shape representations that accommodate learning-based 3D reconstruction are an open problem in machine learning and computer graphics. Previous work on neural 3D reconstruction demonstrated benefits, but also limitations, of point cloud, voxel, surface mesh, and implicit function representations. We introduce Deformable Tetrahedral Meshes (DefTet) as a particular parameterization that utilizes volumetric tetrahedral meshes for the reconstruction problem. Unlike existing volumetric approaches, DefTet optimizes for both vertex placement and occupancy, and is differentiable with respect to standard 3D reconstruction loss functions. It is thus simultaneously high-precision, volumetric, and amenable to learning-based neural architectures. We show that it can represent arbitrary, complex topology, is both memory and computationally efficient, and can produce high-fidelity reconstructions with a significantly smaller grid size than alternative volumetric approaches. The predicted surfaces are also inherently defined as tetrahedral meshes, thus do not require post-processing. We demonstrate that DefTet matches or exceeds both the quality of the previous best approaches and the performance of the fastest ones. Our approach obtains high-quality tetrahedral meshes computed directly from noisy point clouds, and is the first to showcase high-quality 3D tet-mesh results using only a single image as input. Our project webpage: https://nv-tlabs.github.io/DefTet/



### Domain Adaptive Knowledge Distillation for Driving Scene Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2011.08007v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.08007v2)
- **Published**: 2020-11-03 03:01:09+00:00
- **Updated**: 2020-11-26 13:02:27+00:00
- **Authors**: Divya Kothandaraman, Athira Nambiar, Anurag Mittal
- **Comment**: Accepted for publication at Autonomous Vehicle Vision Workshop at
  WACV 2021, 10 pages (including references)
- **Journal**: None
- **Summary**: Practical autonomous driving systems face two crucial challenges: memory constraints and domain gap issues. In this paper, we present a novel approach to learn domain adaptive knowledge in models with limited memory, thus bestowing the model with the ability to deal with these issues in a comprehensive manner. We term this as "Domain Adaptive Knowledge Distillation" and address the same in the context of unsupervised domain-adaptive semantic segmentation by proposing a multi-level distillation strategy to effectively distil knowledge at different levels. Further, we introduce a novel cross entropy loss that leverages pseudo labels from the teacher. These pseudo teacher labels play a multifaceted role towards: (i) knowledge distillation from the teacher network to the student network & (ii) serving as a proxy for the ground truth for target domain images, where the problem is completely unsupervised. We introduce four paradigms for distilling domain adaptive knowledge and carry out extensive experiments and ablation studies on real-to-real as well as synthetic-to-real scenarios. Our experiments demonstrate the profound success of our proposed method.



### Gait Recognition via Effective Global-Local Feature Representation and Local Temporal Aggregation
- **Arxiv ID**: http://arxiv.org/abs/2011.01461v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.01461v2)
- **Published**: 2020-11-03 04:07:13+00:00
- **Updated**: 2021-08-14 07:53:05+00:00
- **Authors**: Beibei Lin, Shunli Zhang, Xin Yu
- **Comment**: Accepted by ICCV 2021
- **Journal**: IEEE International Conference on Computer Vision (ICCV) 2021
- **Summary**: Gait recognition is one of the most important biometric technologies and has been applied in many fields. Recent gait recognition frameworks represent each gait frame by descriptors extracted from either global appearances or local regions of humans. However, the representations based on global information often neglect the details of the gait frame, while local region based descriptors cannot capture the relations among neighboring regions, thus reducing their discriminativeness. In this paper, we propose a novel feature extraction and fusion framework to achieve discriminative feature representations for gait recognition. Towards this goal, we take advantage of both global visual information and local region details and develop a Global and Local Feature Extractor (GLFE). Specifically, our GLFE module is composed of our newly designed multiple global and local convolutional layers (GLConv) to ensemble global and local features in a principle manner. Furthermore, we present a novel operation, namely Local Temporal Aggregation (LTA), to further preserve the spatial information by reducing the temporal resolution to obtain higher spatial resolution. With the help of our GLFE and LTA, our method significantly improves the discriminativeness of our visual features, thus improving the gait recognition performance. Extensive experiments demonstrate that our proposed method outperforms state-of-the-art gait recognition methods on two popular datasets.



### Distribution-aware Margin Calibration for Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2011.01462v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.01462v1)
- **Published**: 2020-11-03 04:07:47+00:00
- **Updated**: 2020-11-03 04:07:47+00:00
- **Authors**: Zhibin Li, Litao Yu, Jian Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: The Jaccard index, also known as Intersection-over-Union (IoU score), is one of the most critical evaluation metrics in medical image segmentation. However, directly optimizing the mean IoU (mIoU) score over multiple objective classes is an open problem. Although some algorithms have been proposed to optimize its surrogates, there is no guarantee provided for their generalization ability. In this paper, we present a novel data-distribution-aware margin calibration method for a better generalization of the mIoU over the whole data-distribution, underpinned by a rigid lower bound. This scheme ensures a better segmentation performance in terms of IoU scores in practice. We evaluate the effectiveness of the proposed margin calibration method on two medical image segmentation datasets, showing substantial improvements of IoU scores over other learning schemes using deep segmentation models.



### MACE: Model Agnostic Concept Extractor for Explaining Image Classification Networks
- **Arxiv ID**: http://arxiv.org/abs/2011.01472v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.01472v1)
- **Published**: 2020-11-03 04:40:49+00:00
- **Updated**: 2020-11-03 04:40:49+00:00
- **Authors**: Ashish Kumar, Karan Sehgal, Prerna Garg, Vidhya Kamakshi, Narayanan C Krishnan
- **Comment**: None
- **Journal**: None
- **Summary**: Deep convolutional networks have been quite successful at various image classification tasks. The current methods to explain the predictions of a pre-trained model rely on gradient information, often resulting in saliency maps that focus on the foreground object as a whole. However, humans typically reason by dissecting an image and pointing out the presence of smaller concepts. The final output is often an aggregation of the presence or absence of these smaller concepts. In this work, we propose MACE: a Model Agnostic Concept Extractor, which can explain the working of a convolutional network through smaller concepts. The MACE framework dissects the feature maps generated by a convolution network for an image to extract concept based prototypical explanations. Further, it estimates the relevance of the extracted concepts to the pre-trained model's predictions, a critical aspect required for explaining the individual class predictions, missing in existing approaches. We validate our framework using VGG16 and ResNet50 CNN architectures, and on datasets like Animals With Attributes 2 (AWA2) and Places365. Our experiments demonstrate that the concepts extracted by the MACE framework increase the human interpretability of the explanations, and are faithful to the underlying pre-trained black-box model.



### Kernel Two-Dimensional Ridge Regression for Subspace Clustering
- **Arxiv ID**: http://arxiv.org/abs/2011.01477v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2011.01477v1)
- **Published**: 2020-11-03 04:52:46+00:00
- **Updated**: 2020-11-03 04:52:46+00:00
- **Authors**: Chong Peng, Qian Zhang, Zhao Kang, Chenglizhao Chen, Qiang Cheng
- **Comment**: accepted to Pattern Recognition
- **Journal**: None
- **Summary**: Subspace clustering methods have been widely studied recently. When the inputs are 2-dimensional (2D) data, existing subspace clustering methods usually convert them into vectors, which severely damages inherent structures and relationships from original data. In this paper, we propose a novel subspace clustering method for 2D data. It directly uses 2D data as inputs such that the learning of representations benefits from inherent structures and relationships of the data. It simultaneously seeks image projection and representation coefficients such that they mutually enhance each other and lead to powerful data representations. An efficient algorithm is developed to solve the proposed objective function with provable decreasing and convergence property. Extensive experimental results verify the effectiveness of the new method.



### Wheat Crop Yield Prediction Using Deep LSTM Model
- **Arxiv ID**: http://arxiv.org/abs/2011.01498v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.01498v1)
- **Published**: 2020-11-03 06:11:31+00:00
- **Updated**: 2020-11-03 06:11:31+00:00
- **Authors**: Sagarika Sharma, Sujit Rai, Narayanan C. Krishnan
- **Comment**: None
- **Journal**: None
- **Summary**: An in-season early crop yield forecast before harvest can benefit the farmers to improve the production and enable various agencies to devise plans accordingly. We introduce a reliable and inexpensive method to predict crop yields from publicly available satellite imagery. The proposed method works directly on raw satellite imagery without the need to extract any hand-crafted features or perform dimensionality reduction on the images. The approach implicitly models the relevance of the different steps in the growing season and the various bands in the satellite imagery. We evaluate the proposed approach on tehsil (block) level wheat predictions across several states in India and demonstrate that it outperforms existing methods by over 50\%. We also show that incorporating additional contextual information such as the location of farmlands, water bodies, and urban areas helps in improving the yield estimates.



### VEGA: Towards an End-to-End Configurable AutoML Pipeline
- **Arxiv ID**: http://arxiv.org/abs/2011.01507v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.01507v4)
- **Published**: 2020-11-03 06:53:53+00:00
- **Updated**: 2020-11-26 05:28:50+00:00
- **Authors**: Bochao Wang, Hang Xu, Jiajin Zhang, Chen Chen, Xiaozhi Fang, Yixing Xu, Ning Kang, Lanqing Hong, Chenhan Jiang, Xinyue Cai, Jiawei Li, Fengwei Zhou, Yong Li, Zhicheng Liu, Xinghao Chen, Kai Han, Han Shu, Dehua Song, Yunhe Wang, Wei Zhang, Chunjing Xu, Zhenguo Li, Wenzhi Liu, Tong Zhang
- **Comment**: AutoML pipeline. Code is open-sourced at
  https://github.com/huawei-noah/vega
- **Journal**: None
- **Summary**: Automated Machine Learning (AutoML) is an important industrial solution for automatic discovery and deployment of the machine learning models. However, designing an integrated AutoML system faces four great challenges of configurability, scalability, integrability, and platform diversity. In this work, we present VEGA, an efficient and comprehensive AutoML framework that is compatible and optimized for multiple hardware platforms. a) The VEGA pipeline integrates various modules of AutoML, including Neural Architecture Search (NAS), Hyperparameter Optimization (HPO), Auto Data Augmentation, Model Compression, and Fully Train. b) To support a variety of search algorithms and tasks, we design a novel fine-grained search space and its description language to enable easy adaptation to different search algorithms and tasks. c) We abstract the common components of deep learning frameworks into a unified interface. VEGA can be executed with multiple back-ends and hardwares. Extensive benchmark experiments on multiple tasks demonstrate that VEGA can improve the existing AutoML algorithms and discover new high-performance models against SOTA methods, e.g. the searched DNet model zoo for Ascend 10x faster than EfficientNet-B5 and 9.2x faster than RegNetX-32GF on ImageNet. VEGA is open-sourced at https://github.com/huawei-noah/vega.



### Recent Advances in Understanding Adversarial Robustness of Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2011.01539v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.01539v1)
- **Published**: 2020-11-03 07:42:53+00:00
- **Updated**: 2020-11-03 07:42:53+00:00
- **Authors**: Tao Bai, Jinqi Luo, Jun Zhao
- **Comment**: Initial Draft
- **Journal**: None
- **Summary**: Adversarial examples are inevitable on the road of pervasive applications of deep neural networks (DNN). Imperceptible perturbations applied on natural samples can lead DNN-based classifiers to output wrong prediction with fair confidence score. It is increasingly important to obtain models with high robustness that are resistant to adversarial examples. In this paper, we survey recent advances in how to understand such intriguing property, i.e. adversarial robustness, from different perspectives. We give preliminary definitions on what adversarial attacks and robustness are. After that, we study frequently-used benchmarks and mention theoretically-proved bounds for adversarial robustness. We then provide an overview on analyzing correlations among adversarial robustness and other critical indicators of DNN models. Lastly, we introduce recent arguments on potential costs of adversarial training which have attracted wide attention from the research community.



### CooGAN: A Memory-Efficient Framework for High-Resolution Facial Attribute Editing
- **Arxiv ID**: http://arxiv.org/abs/2011.01563v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.01563v1)
- **Published**: 2020-11-03 08:40:00+00:00
- **Updated**: 2020-11-03 08:40:00+00:00
- **Authors**: Xuanhong Chen, Bingbing Ni, Naiyuan Liu, Ziang Liu, Yiliu Jiang, Loc Truong, Qi Tian
- **Comment**: None
- **Journal**: European Conference on Computer Vision(ECCV) 2020
- **Summary**: In contrast to great success of memory-consuming face editing methods at a low resolution, to manipulate high-resolution (HR) facial images, i.e., typically larger than 7682 pixels, with very limited memory is still challenging. This is due to the reasons of 1) intractable huge demand of memory; 2) inefficient multi-scale features fusion. To address these issues, we propose a NOVEL pixel translation framework called Cooperative GAN(CooGAN) for HR facial image editing. This framework features a local path for fine-grained local facial patch generation (i.e., patch-level HR, LOW memory) and a global path for global lowresolution (LR) facial structure monitoring (i.e., image-level LR, LOW memory), which largely reduce memory requirements. Both paths work in a cooperative manner under a local-to-global consistency objective (i.e., for smooth stitching). In addition, we propose a lighter selective transfer unit for more efficient multi-scale features fusion, yielding higher fidelity facial attributes manipulation. Extensive experiments on CelebAHQ well demonstrate the memory efficiency as well as the high image generation quality of the proposed framework.



### Cross-Media Keyphrase Prediction: A Unified Framework with Multi-Modality Multi-Head Attention and Image Wordings
- **Arxiv ID**: http://arxiv.org/abs/2011.01565v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2011.01565v1)
- **Published**: 2020-11-03 08:44:18+00:00
- **Updated**: 2020-11-03 08:44:18+00:00
- **Authors**: Yue Wang, Jing Li, Michael R. Lyu, Irwin King
- **Comment**: EMNLP 2020 (14 pages)
- **Journal**: None
- **Summary**: Social media produces large amounts of contents every day. To help users quickly capture what they need, keyphrase prediction is receiving a growing attention. Nevertheless, most prior efforts focus on text modeling, largely ignoring the rich features embedded in the matching images. In this work, we explore the joint effects of texts and images in predicting the keyphrases for a multimedia post. To better align social media style texts and images, we propose: (1) a novel Multi-Modality Multi-Head Attention (M3H-Att) to capture the intricate cross-media interactions; (2) image wordings, in forms of optical characters and image attributes, to bridge the two modalities. Moreover, we design a unified framework to leverage the outputs of keyphrase classification and generation and couple their advantages. Extensive experiments on a large-scale dataset newly collected from Twitter show that our model significantly outperforms the previous state of the art based on traditional attention networks. Further analyses show that our multi-head attention is able to attend information from various aspects and boost classification or generation in diverse scenarios.



### A Deep Temporal Fusion Framework for Scene Flow Using a Learnable Motion Model and Occlusions
- **Arxiv ID**: http://arxiv.org/abs/2011.01603v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.01603v2)
- **Published**: 2020-11-03 10:14:11+00:00
- **Updated**: 2020-11-04 09:13:32+00:00
- **Authors**: René Schuster, Christian Unger, Didier Stricker
- **Comment**: Accepted to WACV21
- **Journal**: None
- **Summary**: Motion estimation is one of the core challenges in computer vision. With traditional dual-frame approaches, occlusions and out-of-view motions are a limiting factor, especially in the context of environmental perception for vehicles due to the large (ego-) motion of objects. Our work proposes a novel data-driven approach for temporal fusion of scene flow estimates in a multi-frame setup to overcome the issue of occlusion. Contrary to most previous methods, we do not rely on a constant motion model, but instead learn a generic temporal relation of motion from data. In a second step, a neural network combines bi-directional scene flow estimates from a common reference frame, yielding a refined estimate and a natural byproduct of occlusion masks. This way, our approach provides a fast multi-frame extension for a variety of scene flow estimators, which outperforms the underlying dual-frame approaches.



### Generalized Wasserstein Dice Score, Distributionally Robust Deep Learning, and Ranger for brain tumor segmentation: BraTS 2020 challenge
- **Arxiv ID**: http://arxiv.org/abs/2011.01614v2
- **DOI**: 10.1007/978-3-030-72087-2_18
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.01614v2)
- **Published**: 2020-11-03 10:50:48+00:00
- **Updated**: 2021-01-25 11:41:03+00:00
- **Authors**: Lucas Fidon, Sebastien Ourselin, Tom Vercauteren
- **Comment**: MICCAI 2020 BrainLes Workshop. Our method ranked fourth out of the
  693 registered teams for the segmentation task of the BraTS 2020 challenge.
  v2: Added some clarifications following reviewers' feedback (camera-ready
  version)
- **Journal**: None
- **Summary**: Training a deep neural network is an optimization problem with four main ingredients: the design of the deep neural network, the per-sample loss function, the population loss function, and the optimizer. However, methods developed to compete in recent BraTS challenges tend to focus only on the design of deep neural network architectures, while paying less attention to the three other aspects. In this paper, we experimented with adopting the opposite approach. We stuck to a generic and state-of-the-art 3D U-Net architecture and experimented with a non-standard per-sample loss function, the generalized Wasserstein Dice loss, a non-standard population loss function, corresponding to distributionally robust optimization, and a non-standard optimizer, Ranger. Those variations were selected specifically for the problem of multi-class brain tumor segmentation. The generalized Wasserstein Dice loss is a per-sample loss function that allows taking advantage of the hierarchical structure of the tumor regions labeled in BraTS. Distributionally robust optimization is a generalization of empirical risk minimization that accounts for the presence of underrepresented subdomains in the training dataset. Ranger is a generalization of the widely used Adam optimizer that is more stable with small batch size and noisy labels. We found that each of those variations of the optimization of deep neural networks for brain tumor segmentation leads to improvements in terms of Dice scores and Hausdorff distances. With an ensemble of three deep neural networks trained with various optimization procedures, we achieved promising results on the validation dataset of the BraTS 2020 challenge. Our ensemble ranked fourth out of the 693 registered teams for the segmentation task of the BraTS 2020 challenge.



### Relational Graph Learning on Visual and Kinematics Embeddings for Accurate Gesture Recognition in Robotic Surgery
- **Arxiv ID**: http://arxiv.org/abs/2011.01619v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2011.01619v2)
- **Published**: 2020-11-03 11:00:10+00:00
- **Updated**: 2021-06-29 05:52:38+00:00
- **Authors**: Yonghao Long, Jie Ying Wu, Bo Lu, Yueming Jin, Mathias Unberath, Yun-Hui Liu, Pheng Ann Heng, Qi Dou
- **Comment**: Accepted for ICRA 2021
- **Journal**: None
- **Summary**: Automatic surgical gesture recognition is fundamentally important to enable intelligent cognitive assistance in robotic surgery. With recent advancement in robot-assisted minimally invasive surgery, rich information including surgical videos and robotic kinematics can be recorded, which provide complementary knowledge for understanding surgical gestures. However, existing methods either solely adopt uni-modal data or directly concatenate multi-modal representations, which can not sufficiently exploit the informative correlations inherent in visual and kinematics data to boost gesture recognition accuracies. In this regard, we propose a novel online approach of multi-modal relational graph network (i.e., MRG-Net) to dynamically integrate visual and kinematics information through interactive message propagation in the latent feature space. In specific, we first extract embeddings from video and kinematics sequences with temporal convolutional networks and LSTM units. Next, we identify multi-relations in these multi-modal embeddings and leverage them through a hierarchical relational graph learning module. The effectiveness of our method is demonstrated with state-of-the-art results on the public JIGSAWS dataset, outperforming current uni-modal and multi-modal methods on both suturing and knot typing tasks. Furthermore, we validated our method on in-house visual-kinematics datasets collected with da Vinci Research Kit (dVRK) platforms in two centers, with consistent promising performance achieved.



### Towards Map-Based Validation of Semantic Segmentation Masks
- **Arxiv ID**: http://arxiv.org/abs/2011.08008v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2011.08008v2)
- **Published**: 2020-11-03 11:07:22+00:00
- **Updated**: 2020-11-26 15:28:58+00:00
- **Authors**: Laura von Rueden, Tim Wirtz, Fabian Hueger, Jan David Schneider, Christian Bauckhage
- **Comment**: None
- **Journal**: None
- **Summary**: Artificial intelligence for autonomous driving must meet strict requirements on safety and robustness. We propose to validate machine learning models for self-driving vehicles not only with given ground truth labels, but also with additional a-priori knowledge. In particular, we suggest to validate the drivable area in semantic segmentation masks using given street map data. We present first results, which indicate that prediction errors can be uncovered by map-based validation.



### The Aleatoric Uncertainty Estimation Using a Separate Formulation with Virtual Residuals
- **Arxiv ID**: http://arxiv.org/abs/2011.01655v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.01655v1)
- **Published**: 2020-11-03 12:11:27+00:00
- **Updated**: 2020-11-03 12:11:27+00:00
- **Authors**: Takumi Kawashima, Qing Yu, Akari Asai, Daiki Ikami, Kiyoharu Aizawa
- **Comment**: None
- **Journal**: ICPR2020
- **Summary**: We propose a new optimization framework for aleatoric uncertainty estimation in regression problems. Existing methods can quantify the error in the target estimation, but they tend to underestimate it. To obtain the predictive uncertainty inherent in an observation, we propose a new separable formulation for the estimation of a signal and of its uncertainty, avoiding the effect of overfitting. By decoupling target estimation and uncertainty estimation, we also control the balance between signal estimation and uncertainty estimation. We conduct three types of experiments: regression with simulation data, age estimation, and depth estimation. We demonstrate that the proposed method outperforms a state-of-the-art technique for signal and uncertainty estimation.



### A spatial hue similarity measure for assessment of colourisation
- **Arxiv ID**: http://arxiv.org/abs/2011.01700v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.01700v1)
- **Published**: 2020-11-03 13:43:36+00:00
- **Updated**: 2020-11-03 13:43:36+00:00
- **Authors**: Seán Mullery, Paul F. Whelan
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic colourisation of grey-scale images is an ill-posed multi-modal problem. Where full-reference images exist, objective performance measures rely on pixel-difference techniques such as MSE and PSNR. These measures penalise any plausible modes other than the reference ground-truth; They often fail to adequately penalise implausible modes if they are close in pixel distance to the ground-truth; As these are pixel-difference methods they cannot assess spatial coherency. We use the polar form of the a*b* channels from the CIEL*a*b* colour space to separate the multi-modal problems, which we confine to the hue channel, and the common-mode which applies to the chroma channel. We apply SSIM to the chroma channel but reformulate SSIM for the hue channel to a measure we call the Spatial Hue Similarity Measure (SHSM). This reformulation allows spatially-coherent hue channels to achieve a high score while penalising spatially-incoherent modes. This method allows qualitative and quantitative performance comparison of SOTA colourisation methods and reduces reliance on subjective human visual inspection.



### Exploring DeshuffleGANs in Self-Supervised Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2011.01730v2
- **DOI**: 10.1016/j.patcog.2021.108244
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.01730v2)
- **Published**: 2020-11-03 14:22:54+00:00
- **Updated**: 2021-09-01 14:00:57+00:00
- **Authors**: Gulcin Baykal, Furkan Ozcelik, Gozde Unal
- **Comment**: Accepted manuscript
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) have become the most used networks towards solving the problem of image generation. Self-supervised GANs are later proposed to avoid the catastrophic forgetting of the discriminator and to improve the image generation quality without needing the class labels. However, the generalizability of the self-supervision tasks on different GAN architectures is not studied before. To that end, we extensively analyze the contribution of a previously proposed self-supervision task, deshuffling of the DeshuffleGANs in the generalizability context. We assign the deshuffling task to two different GAN discriminators and study the effects of the task on both architectures. We extend the evaluations compared to the previously proposed DeshuffleGANs on various datasets. We show that the DeshuffleGAN obtains the best FID results for several datasets compared to the other self-supervised GANs. Furthermore, we compare the deshuffling with the rotation prediction that is firstly deployed to the GAN training and demonstrate that its contribution exceeds the rotation prediction. We design the conditional DeshuffleGAN called cDeshuffleGAN to evaluate the quality of the learnt representations. Lastly, we show the contribution of the self-supervision tasks to the GAN training on the loss landscape and present that the effects of these tasks may not be cooperative to the adversarial training in some settings. Our code can be found at https://github.com/gulcinbaykal/DeshuffleGAN.



### Learning a Generative Motion Model from Image Sequences based on a Latent Motion Matrix
- **Arxiv ID**: http://arxiv.org/abs/2011.01741v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.01741v2)
- **Published**: 2020-11-03 14:44:09+00:00
- **Updated**: 2021-01-31 13:26:54+00:00
- **Authors**: Julian Krebs, Hervé Delingette, Nicholas Ayache, Tommaso Mansi
- **Comment**: accepted at IEEE TMI
- **Journal**: None
- **Summary**: We propose to learn a probabilistic motion model from a sequence of images for spatio-temporal registration. Our model encodes motion in a low-dimensional probabilistic space - the motion matrix - which enables various motion analysis tasks such as simulation and interpolation of realistic motion patterns allowing for faster data acquisition and data augmentation. More precisely, the motion matrix allows to transport the recovered motion from one subject to another simulating for example a pathological motion in a healthy subject without the need for inter-subject registration. The method is based on a conditional latent variable model that is trained using amortized variational inference. This unsupervised generative model follows a novel multivariate Gaussian process prior and is applied within a temporal convolutional network which leads to a diffeomorphic motion model. Temporal consistency and generalizability is further improved by applying a temporal dropout training scheme. Applied to cardiac cine-MRI sequences, we show improved registration accuracy and spatio-temporally smoother deformations compared to three state-of-the-art registration algorithms. Besides, we demonstrate the model's applicability for motion analysis, simulation and super-resolution by an improved motion reconstruction from sequences with missing frames compared to linear and cubic interpolation.



### Solving Inverse Problems with Hybrid Deep Image Priors: the challenge of preventing overfitting
- **Arxiv ID**: http://arxiv.org/abs/2011.01748v3
- **DOI**: 10.1109/ICASSP39728.2021.9414879
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.01748v3)
- **Published**: 2020-11-03 14:50:53+00:00
- **Updated**: 2023-02-17 16:55:51+00:00
- **Authors**: Zhaodong Sun, Thomas Sanchez, Fabian Latorre, Volkan Cevher
- **Comment**: Part of the work has been published on ICASSP 2021 with the paper
  title "a plug-and-play deep image prior"
- **Journal**: None
- **Summary**: We mainly analyze and solve the overfitting problem of deep image prior (DIP). Deep image prior can solve inverse problems such as super-resolution, inpainting and denoising. The main advantage of DIP over other deep learning approaches is that it does not need access to a large dataset. However, due to the large number of parameters of the neural network and noisy data, DIP overfits to the noise in the image as the number of iterations grows. In the thesis, we use hybrid deep image priors to avoid overfitting. The hybrid priors are to combine DIP with an explicit prior such as total variation or with an implicit prior such as a denoising algorithm. We use the alternating direction method-of-multipliers (ADMM) to incorporate the new prior and try different forms of ADMM to avoid extra computation caused by the inner loop of ADMM steps. We also study the relation between the dynamics of gradient descent, and the overfitting phenomenon. The numerical results show the hybrid priors play an important role in preventing overfitting. Besides, we try to fit the image along some directions and find this method can reduce overfitting when the noise level is large. When the noise level is small, it does not considerably reduce the overfitting problem.



### Attention Beam: An Image Captioning Approach
- **Arxiv ID**: http://arxiv.org/abs/2011.01753v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.01753v2)
- **Published**: 2020-11-03 14:57:42+00:00
- **Updated**: 2020-11-11 15:17:56+00:00
- **Authors**: Anubhav Shrimal, Tanmoy Chakraborty
- **Comment**: 5 pages, 6 figures, 1 table, in Proceedings of the 35th AAAI
  Conference on Artificial Intelligence (AAAI-21) Student Abstract
- **Journal**: None
- **Summary**: The aim of image captioning is to generate textual description of a given image. Though seemingly an easy task for humans, it is challenging for machines as it requires the ability to comprehend the image (computer vision) and consequently generate a human-like description for the image (natural language understanding). In recent times, encoder-decoder based architectures have achieved state-of-the-art results for image captioning. Here, we present a heuristic of beam search on top of the encoder-decoder based architecture that gives better quality captions on three benchmark datasets: Flickr8k, Flickr30k and MS COCO.



### Learning Representations from Audio-Visual Spatial Alignment
- **Arxiv ID**: http://arxiv.org/abs/2011.01819v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.01819v1)
- **Published**: 2020-11-03 16:20:04+00:00
- **Updated**: 2020-11-03 16:20:04+00:00
- **Authors**: Pedro Morgado, Yi Li, Nuno Vasconcelos
- **Comment**: To appear at Advances in Neural Information Processing Systems
  (NeurIPS), 2020
- **Journal**: None
- **Summary**: We introduce a novel self-supervised pretext task for learning representations from audio-visual content. Prior work on audio-visual representation learning leverages correspondences at the video level. Approaches based on audio-visual correspondence (AVC) predict whether audio and video clips originate from the same or different video instances. Audio-visual temporal synchronization (AVTS) further discriminates negative pairs originated from the same video instance but at different moments in time. While these approaches learn high-quality representations for downstream tasks such as action recognition, their training objectives disregard spatial cues naturally occurring in audio and visual signals. To learn from these spatial cues, we tasked a network to perform contrastive audio-visual spatial alignment of 360{\deg} video and spatial audio. The ability to perform spatial alignment is enhanced by reasoning over the full spatial content of the 360{\deg} video using a transformer architecture to combine representations from multiple viewpoints. The advantages of the proposed pretext task are demonstrated on a variety of audio and visual downstream tasks, including audio-visual correspondence, spatial alignment, action recognition, and video semantic segmentation.



### A Comprehensive Study of Class Incremental Learning Algorithms for Visual Tasks
- **Arxiv ID**: http://arxiv.org/abs/2011.01844v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.01844v4)
- **Published**: 2020-11-03 16:59:21+00:00
- **Updated**: 2020-12-15 16:40:55+00:00
- **Authors**: Eden Belouadah, Adrian Popescu, Ioannis Kanellos
- **Comment**: Accepted for publication in the Elsevier's Neural Networks journal
- **Journal**: None
- **Summary**: The ability of artificial agents to increment their capabilities when confronted with new data is an open challenge in artificial intelligence. The main challenge faced in such cases is catastrophic forgetting, i.e., the tendency of neural networks to underfit past data when new ones are ingested. A first group of approaches tackles forgetting by increasing deep model capacity to accommodate new knowledge. A second type of approaches fix the deep model size and introduce a mechanism whose objective is to ensure a good compromise between stability and plasticity of the model. While the first type of algorithms were compared thoroughly, this is not the case for methods which exploit a fixed size model. Here, we focus on the latter, place them in a common conceptual and experimental framework and propose the following contributions: (1) define six desirable properties of incremental learning algorithms and analyze them according to these properties, (2) introduce a unified formalization of the class-incremental learning problem, (3) propose a common evaluation framework which is more thorough than existing ones in terms of number of datasets, size of datasets, size of bounded memory and number of incremental states, (4) investigate the usefulness of herding for past exemplars selection, (5) provide experimental evidence that it is possible to obtain competitive performance without the use of knowledge distillation to tackle catastrophic forgetting and (6) facilitate reproducibility by integrating all tested methods in a common open-source repository. The main experimental finding is that none of the existing algorithms achieves the best results in all evaluated settings. Important differences arise notably if a bounded memory of past classes is allowed or not.



### Similarity-Based Clustering for Enhancing Image Classification Architectures
- **Arxiv ID**: http://arxiv.org/abs/2011.04728v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2011.04728v3)
- **Published**: 2020-11-03 17:03:28+00:00
- **Updated**: 2021-10-06 04:48:18+00:00
- **Authors**: Dishant Parikh
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional networks are at the center of best-in-class computer vision applications for a wide assortment of undertakings. Since 2014, a profound amount of work began to make better convolutional architectures, yielding generous additions in different benchmarks. Albeit expanded model size and computational cost will, in general, mean prompt quality increases for most undertakings but, the architectures now need to have some additional information to increase the performance. I show evidence that with the amalgamation of content-based image similarity and deep learning models, we can provide the flow of information which can be used in making clustered learning possible. The paper shows how training of sub-dataset clusters not only reduces the cost of computation but also increases the speed of evaluating and tuning a model on the given dataset.



### Semi-supervised Facial Action Unit Intensity Estimation with Contrastive Learning
- **Arxiv ID**: http://arxiv.org/abs/2011.01864v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.01864v2)
- **Published**: 2020-11-03 17:35:57+00:00
- **Updated**: 2020-11-04 09:40:36+00:00
- **Authors**: Enrique Sanchez, Adrian Bulat, Anestis Zaganidis, Georgios Tzimiropoulos
- **Comment**: ACCV 2020
- **Journal**: None
- **Summary**: This paper tackles the challenging problem of estimating the intensity of Facial Action Units with few labeled images. Contrary to previous works, our method does not require to manually select key frames, and produces state-of-the-art results with as little as $2\%$ of annotated frames, which are \textit{randomly chosen}. To this end, we propose a semi-supervised learning approach where a spatio-temporal model combining a feature extractor and a temporal module are learned in two stages. The first stage uses datasets of unlabeled videos to learn a strong spatio-temporal representation of facial behavior dynamics based on contrastive learning. To our knowledge we are the first to build upon this framework for modeling facial behavior in an unsupervised manner. The second stage uses another dataset of randomly chosen labeled frames to train a regressor on top of our spatio-temporal model for estimating the AU intensity. We show that although backpropagation through time is applied only with respect to the output of the network for extremely sparse and randomly chosen labeled frames, our model can be effectively trained to estimate AU intensity accurately, thanks to the unsupervised pre-training of the first stage. We experimentally validate that our method outperforms existing methods when working with as little as $2\%$ of randomly chosen data for both DISFA and BP4D datasets, without a careful choice of labeled frames, a time-consuming task still required in previous approaches.



### Learning unbiased group-wise registration (LUGR) and joint segmentation: evaluation on longitudinal diffusion MRI
- **Arxiv ID**: http://arxiv.org/abs/2011.01869v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2011.01869v2)
- **Published**: 2020-11-03 17:44:15+00:00
- **Updated**: 2021-02-24 09:23:00+00:00
- **Authors**: Bo Li, Wiro J. Niessen, Stefan Klein, M. Arfan Ikram, Meike W. Vernooij, Esther E. Bron
- **Comment**: SPIE Medical Imaging 2021 (oral)
- **Journal**: None
- **Summary**: Analysis of longitudinal changes in imaging studies often involves both segmentation of structures of interest and registration of multiple timeframes. The accuracy of such analysis could benefit from a tailored framework that jointly optimizes both tasks to fully exploit the information available in the longitudinal data. Most learning-based registration algorithms, including joint optimization approaches, currently suffer from bias due to selection of a fixed reference frame and only support pairwise transformations. We here propose an analytical framework based on an unbiased learning strategy for group-wise registration that simultaneously registers images to the mean space of a group to obtain consistent segmentations. We evaluate the proposed method on longitudinal analysis of a white matter tract in a brain MRI dataset with 2-3 time-points for 3249 individuals, i.e., 8045 images in total. The reproducibility of the method is evaluated on test-retest data from 97 individuals. The results confirm that the implicit reference image is an average of the input image. In addition, the proposed framework leads to consistent segmentations and significantly lower processing bias than that of a pair-wise fixed-reference approach. This processing bias is even smaller than those obtained when translating segmentations by only one voxel, which can be attributed to subtle numerical instabilities and interpolation. Therefore, we postulate that the proposed mean-space learning strategy could be widely applied to learning-based registration tasks. In addition, this group-wise framework introduces a novel way for learning-based longitudinal studies by direct construction of an unbiased within-subject template and allowing reliable and efficient analysis of spatio-temporal imaging biomarkers.



### Unsupervised Attention Based Instance Discriminative Learning for Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2011.01888v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.01888v1)
- **Published**: 2020-11-03 18:08:31+00:00
- **Updated**: 2020-11-03 18:08:31+00:00
- **Authors**: Kshitij Nikhal, Benjamin S. Riggan
- **Comment**: WACV 2021
- **Journal**: None
- **Summary**: Recent advances in person re-identification have demonstrated enhanced discriminability, especially with supervised learning or transfer learning. However, since the data requirements---including the degree of data curations---are becoming increasingly complex and laborious, there is a critical need for unsupervised methods that are robust to large intra-class variations, such as changes in perspective, illumination, articulated motion, resolution, etc. Therefore, we propose an unsupervised framework for person re-identification which is trained in an end-to-end manner without any pre-training. Our proposed framework leverages a new attention mechanism that combines group convolutions to (1) enhance spatial attention at multiple scales and (2) reduce the number of trainable parameters by 59.6%. Additionally, our framework jointly optimizes the network with agglomerative clustering and instance learning to tackle hard samples. We perform extensive analysis using the Market1501 and DukeMTMC-reID datasets to demonstrate that our method consistently outperforms the state-of-the-art methods (with and without pre-trained weights).



### RealHePoNet: a robust single-stage ConvNet for head pose estimation in the wild
- **Arxiv ID**: http://arxiv.org/abs/2011.01890v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2011.01890v1)
- **Published**: 2020-11-03 18:09:05+00:00
- **Updated**: 2020-11-03 18:09:05+00:00
- **Authors**: Rafael Berral-Soler, Francisco J. Madrid-Cuevas, Rafael Muñoz-Salinas, Manuel J. Marín-Jiménez
- **Comment**: Accepted for publication at Neural Computing and Applications
- **Journal**: None
- **Summary**: Human head pose estimation in images has applications in many fields such as human-computer interaction or video surveillance tasks. In this work, we address this problem, defined here as the estimation of both vertical (tilt/pitch) and horizontal (pan/yaw) angles, through the use of a single Convolutional Neural Network (ConvNet) model, trying to balance precision and inference speed in order to maximize its usability in real-world applications. Our model is trained over the combination of two datasets: 'Pointing'04' (aiming at covering a wide range of poses) and 'Annotated Facial Landmarks in the Wild' (in order to improve robustness of our model for its use on real-world images). Three different partitions of the combined dataset are defined and used for training, validation and testing purposes. As a result of this work, we have obtained a trained ConvNet model, coined RealHePoNet, that given a low-resolution grayscale input image, and without the need of using facial landmarks, is able to estimate with low error both tilt and pan angles (~4.4{\deg} average error on the test partition). Also, given its low inference time (~6 ms per head), we consider our model usable even when paired with medium-spec hardware (i.e. GTX 1060 GPU). * Code available at: https://github.com/rafabs97/headpose_final * Demo video at: https://www.youtube.com/watch?v=2UeuXh5DjAE



### Learning Visual Representations for Transfer Learning by Suppressing Texture
- **Arxiv ID**: http://arxiv.org/abs/2011.01901v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.01901v3)
- **Published**: 2020-11-03 18:27:03+00:00
- **Updated**: 2023-01-27 04:46:02+00:00
- **Authors**: Shlok Mishra, Anshul Shah, Ankan Bansal, Janit Anjaria, Jonghyun Choi, Abhinav Shrivastava, Abhishek Sharma, David Jacobs
- **Comment**: None
- **Journal**: BMVC 2022
- **Summary**: Recent literature has shown that features obtained from supervised training of CNNs may over-emphasize texture rather than encoding high-level information. In self-supervised learning in particular, texture as a low-level cue may provide shortcuts that prevent the network from learning higher level representations. To address these problems we propose to use classic methods based on anisotropic diffusion to augment training using images with suppressed texture. This simple method helps retain important edge information and suppress texture at the same time. We empirically show that our method achieves state-of-the-art results on object detection and image classification with eight diverse datasets in either supervised or self-supervised learning tasks such as MoCoV2 and Jigsaw. Our method is particularly effective for transfer learning tasks and we observed improved performance on five standard transfer learning datasets. The large improvements (up to 11.49\%) on the Sketch-ImageNet dataset, DTD dataset and additional visual analyses with saliency maps suggest that our approach helps in learning better representations that better transfer.



### Deep Joint Transmission-Recognition for Multi-View Cameras
- **Arxiv ID**: http://arxiv.org/abs/2011.01902v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.IT, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/2011.01902v1)
- **Published**: 2020-11-03 18:27:49+00:00
- **Updated**: 2020-11-03 18:27:49+00:00
- **Authors**: Ezgi Ozyilkan, Mikolaj Jankowski
- **Comment**: 9 pages, 12 figures
- **Journal**: None
- **Summary**: We propose joint transmission-recognition schemes for efficient inference at the wireless edge. Motivated by the surveillance applications with wireless cameras, we consider the person classification task over a wireless channel carried out by multi-view cameras operating as edge devices. We introduce deep neural network (DNN) based compression schemes which incorporate digital (separate) transmission and joint source-channel coding (JSCC) methods. We evaluate the proposed device-edge communication schemes under different channel SNRs, bandwidth and power constraints. We show that the JSCC schemes not only improve the end-to-end accuracy but also simplify the encoding process and provide graceful degradation with channel quality.



### Classifier Pool Generation based on a Two-level Diversity Approach
- **Arxiv ID**: http://arxiv.org/abs/2011.01908v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.01908v1)
- **Published**: 2020-11-03 18:41:53+00:00
- **Updated**: 2020-11-03 18:41:53+00:00
- **Authors**: Marcos Monteiro, Alceu S. Britto Jr, Jean P. Barddal, Luiz S. Oliveira, Robert Sabourin
- **Comment**: None
- **Journal**: None
- **Summary**: This paper describes a classifier pool generation method guided by the diversity estimated on the data complexity and classifier decisions. First, the behavior of complexity measures is assessed by considering several subsamples of the dataset. The complexity measures with high variability across the subsamples are selected for posterior pool adaptation, where an evolutionary algorithm optimizes diversity in both complexity and decision spaces. A robust experimental protocol with 28 datasets and 20 replications is used to evaluate the proposed method. Results show significant accuracy improvements in 69.4% of the experiments when Dynamic Classifier Selection and Dynamic Ensemble Selection methods are applied.



### Generating Unobserved Alternatives
- **Arxiv ID**: http://arxiv.org/abs/2011.01926v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2011.01926v4)
- **Published**: 2020-11-03 18:57:57+00:00
- **Updated**: 2020-12-01 10:05:52+00:00
- **Authors**: Shichong Peng, Ke Li
- **Comment**: Videos in the article are also available as ancillary files in the
  previous version (arXiv:2011.01926v3). Website:
  https://niopeng.github.io/HyperRIM/
- **Journal**: None
- **Summary**: We consider problems where multiple predictions can be considered correct, but only one of them is given as supervision. This setting differs from both the regression and class-conditional generative modelling settings: in the former, there is a unique observed output for each input, which is provided as supervision; in the latter, there are many observed outputs for each input, and many are provided as supervision. Applying either regression methods and conditional generative models to the present setting often results in a model that can only make a single prediction for each input. We explore several problems that have this property and develop an approach that can generate multiple high-quality predictions given the same input. As a result, it can be used to generate high-quality outputs that are different from the observed output.



### Learning 3D Dynamic Scene Representations for Robot Manipulation
- **Arxiv ID**: http://arxiv.org/abs/2011.01968v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.01968v2)
- **Published**: 2020-11-03 19:23:06+00:00
- **Updated**: 2020-12-10 16:53:29+00:00
- **Authors**: Zhenjia Xu, Zhanpeng He, Jiajun Wu, Shuran Song
- **Comment**: CoRL 2020. The first two authors contributed equally to this paper.
  Project page: https://dsr-net.cs.columbia.edu/
- **Journal**: None
- **Summary**: 3D scene representation for robot manipulation should capture three key object properties: permanency -- objects that become occluded over time continue to exist; amodal completeness -- objects have 3D occupancy, even if only partial observations are available; spatiotemporal continuity -- the movement of each object is continuous over space and time. In this paper, we introduce 3D Dynamic Scene Representation (DSR), a 3D volumetric scene representation that simultaneously discovers, tracks, reconstructs objects, and predicts their dynamics while capturing all three properties. We further propose DSR-Net, which learns to aggregate visual observations over multiple interactions to gradually build and refine DSR. Our model achieves state-of-the-art performance in modeling 3D scene dynamics with DSR on both simulated and real data. Combined with model predictive control, DSR-Net enables accurate planning in downstream robotic manipulation tasks such as planar pushing. Video is available at https://youtu.be/GQjYG3nQJ80.



### Multi Projection Fusion for Real-time Semantic Segmentation of 3D LiDAR Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2011.01974v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.01974v2)
- **Published**: 2020-11-03 19:40:43+00:00
- **Updated**: 2020-11-06 17:00:05+00:00
- **Authors**: Yara Ali Alnaggar, Mohamed Afifi, Karim Amer, Mohamed Elhelw
- **Comment**: Accepted at the 2021 Winter Conference on Applications of Computer
  Vision (WACV 2021)
- **Journal**: None
- **Summary**: Semantic segmentation of 3D point cloud data is essential for enhanced high-level perception in autonomous platforms. Furthermore, given the increasing deployment of LiDAR sensors onboard of cars and drones, a special emphasis is also placed on non-computationally intensive algorithms that operate on mobile GPUs. Previous efficient state-of-the-art methods relied on 2D spherical projection of point clouds as input for 2D fully convolutional neural networks to balance the accuracy-speed trade-off. This paper introduces a novel approach for 3D point cloud semantic segmentation that exploits multiple projections of the point cloud to mitigate the loss of information inherent in single projection methods. Our Multi-Projection Fusion (MPF) framework analyzes spherical and bird's-eye view projections using two separate highly-efficient 2D fully convolutional models then combines the segmentation results of both views. The proposed framework is validated on the SemanticKITTI dataset where it achieved a mIoU of 55.5 which is higher than state-of-the-art projection-based methods RangeNet++ and PolarNet while being 1.6x faster than the former and 3.1x faster than the latter.



### Rearrangement: A Challenge for Embodied AI
- **Arxiv ID**: http://arxiv.org/abs/2011.01975v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2011.01975v1)
- **Published**: 2020-11-03 19:42:32+00:00
- **Updated**: 2020-11-03 19:42:32+00:00
- **Authors**: Dhruv Batra, Angel X. Chang, Sonia Chernova, Andrew J. Davison, Jia Deng, Vladlen Koltun, Sergey Levine, Jitendra Malik, Igor Mordatch, Roozbeh Mottaghi, Manolis Savva, Hao Su
- **Comment**: Authors are listed in alphabetical order
- **Journal**: None
- **Summary**: We describe a framework for research and evaluation in Embodied AI. Our proposal is based on a canonical task: Rearrangement. A standard task can focus the development of new techniques and serve as a source of trained models that can be transferred to other settings. In the rearrangement task, the goal is to bring a given physical environment into a specified state. The goal state can be specified by object poses, by images, by a description in language, or by letting the agent experience the environment in the goal state. We characterize rearrangement scenarios along different axes and describe metrics for benchmarking rearrangement performance. To facilitate research and exploration, we present experimental testbeds of rearrangement scenarios in four different simulation environments. We anticipate that other datasets will be released and new simulation platforms will be built to support training of rearrangement agents and their deployment on physical systems.



### A Tunable Robust Pruning Framework Through Dynamic Network Rewiring of DNNs
- **Arxiv ID**: http://arxiv.org/abs/2011.03083v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.03083v2)
- **Published**: 2020-11-03 19:49:00+00:00
- **Updated**: 2020-11-24 17:40:52+00:00
- **Authors**: Souvik Kundu, Mahdi Nazemi, Peter A. Beerel, Massoud Pedram
- **Comment**: 8 pages, 4 figures, conference paper
- **Journal**: None
- **Summary**: This paper presents a dynamic network rewiring (DNR) method to generate pruned deep neural network (DNN) models that are robust against adversarial attacks yet maintain high accuracy on clean images. In particular, the disclosed DNR method is based on a unified constrained optimization formulation using a hybrid loss function that merges ultra-high model compression with robust adversarial training. This training strategy dynamically adjusts inter-layer connectivity based on per-layer normalized momentum computed from the hybrid loss function. In contrast to existing robust pruning frameworks that require multiple training iterations, the proposed learning strategy achieves an overall target pruning ratio with only a single training iteration and can be tuned to support both irregular and structured channel pruning. To evaluate the merits of DNR, experiments were performed with two widely accepted models, namely VGG16 and ResNet-18, on CIFAR-10, CIFAR-100 as well as with VGG16 on Tiny-ImageNet. Compared to the baseline uncompressed models, DNR provides over20x compression on all the datasets with no significant drop in either clean or adversarial classification accuracy. Moreover, our experiments show that DNR consistently finds compressed models with better clean and adversarial image classification performance than what is achievable through state-of-the-art alternatives.



### Single Image Human Proxemics Estimation for Visual Social Distancing
- **Arxiv ID**: http://arxiv.org/abs/2011.02018v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.02018v2)
- **Published**: 2020-11-03 21:49:13+00:00
- **Updated**: 2020-11-05 14:13:24+00:00
- **Authors**: Maya Aghaei, Matteo Bustreo, Yiming Wang, Gianluca Bailo, Pietro Morerio, Alessio Del Bue
- **Comment**: Paper accepted at WACV 2021 conference
- **Journal**: None
- **Summary**: In this work, we address the problem of estimating the so-called "Social Distancing" given a single uncalibrated image in unconstrained scenarios. Our approach proposes a semi-automatic solution to approximate the homography matrix between the scene ground and image plane. With the estimated homography, we then leverage an off-the-shelf pose detector to detect body poses on the image and to reason upon their inter-personal distances using the length of their body-parts. Inter-personal distances are further locally inspected to detect possible violations of the social distancing rules. We validate our proposed method quantitatively and qualitatively against baselines on public domain datasets for which we provided groundtruth on inter-personal distances. Besides, we demonstrate the application of our method deployed in a real testing scenario where statistics on the inter-personal distances are currently used to improve the safety in a critical environment.



### Face Morphing Attack Generation & Detection: A Comprehensive Survey
- **Arxiv ID**: http://arxiv.org/abs/2011.02045v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/2011.02045v1)
- **Published**: 2020-11-03 22:36:27+00:00
- **Updated**: 2020-11-03 22:36:27+00:00
- **Authors**: Sushma Venkatesh, Raghavendra Ramachandra, Kiran Raja, Christoph Busch
- **Comment**: None
- **Journal**: None
- **Summary**: The vulnerability of Face Recognition System (FRS) to various kind of attacks (both direct and in-direct attacks) and face morphing attacks has received a great interest from the biometric community. The goal of a morphing attack is to subvert the FRS at Automatic Border Control (ABC) gates by presenting the Electronic Machine Readable Travel Document (eMRTD) or e-passport that is obtained based on the morphed face image. Since the application process for the e-passport in the majority countries requires a passport photo to be presented by the applicant, a malicious actor and the accomplice can generate the morphed face image and to obtain the e-passport. An e-passport with a morphed face images can be used by both the malicious actor and the accomplice to cross the border as the morphed face image can be verified against both of them. This can result in a significant threat as a malicious actor can cross the border without revealing the track of his/her criminal background while the details of accomplice are recorded in the log of the access control system. This survey aims to present a systematic overview of the progress made in the area of face morphing in terms of both morph generation and morph detection. In this paper, we describe and illustrate various aspects of face morphing attacks, including different techniques for generating morphed face images but also the state-of-the-art regarding Morph Attack Detection (MAD) algorithms based on a stringent taxonomy and finally the availability of public databases, which allow to benchmark new MAD algorithms in a reproducible manner. The outcomes of competitions/benchmarking, vulnerability assessments and performance evaluation metrics are also provided in a comprehensive manner. Furthermore, we discuss the open challenges and potential future works that need to be addressed in this evolving field of biometrics.



### Self-Adaptively Learning to Demoire from Focused and Defocused Image Pairs
- **Arxiv ID**: http://arxiv.org/abs/2011.02055v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.02055v2)
- **Published**: 2020-11-03 23:09:02+00:00
- **Updated**: 2020-11-05 10:19:40+00:00
- **Authors**: Lin Liu, Shanxin Yuan, Jianzhuang Liu, Liping Bao, Gregory Slabaugh, Qi Tian
- **Comment**: Accepted to NeurIPS 2020. Project page:
  "http://home.ustc.edu.cn/~ll0825/project_FDNet.html"
- **Journal**: None
- **Summary**: Moire artifacts are common in digital photography, resulting from the interference between high-frequency scene content and the color filter array of the camera. Existing deep learning-based demoireing methods trained on large scale datasets are limited in handling various complex moire patterns, and mainly focus on demoireing of photos taken of digital displays. Moreover, obtaining moire-free ground-truth in natural scenes is difficult but needed for training. In this paper, we propose a self-adaptive learning method for demoireing a high-frequency image, with the help of an additional defocused moire-free blur image. Given an image degraded with moire artifacts and a moire-free blur image, our network predicts a moire-free clean image and a blur kernel with a self-adaptive strategy that does not require an explicit training stage, instead performing test-time adaptation. Our model has two sub-networks and works iteratively. During each iteration, one sub-network takes the moire image as input, removing moire patterns and restoring image details, and the other sub-network estimates the blur kernel from the blur image. The two sub-networks are jointly optimized. Extensive experiments demonstrate that our method outperforms state-of-the-art methods and can produce high-quality demoired results. It can generalize well to the task of removing moire artifacts caused by display screens. In addition, we build a new moire dataset, including images with screen and texture moire artifacts. As far as we know, this is the first dataset with real texture moire patterns.



### NAS-FAS: Static-Dynamic Central Difference Network Search for Face Anti-Spoofing
- **Arxiv ID**: http://arxiv.org/abs/2011.02062v1
- **DOI**: 10.1109/TPAMI.2020.3036338
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.02062v1)
- **Published**: 2020-11-03 23:34:40+00:00
- **Updated**: 2020-11-03 23:34:40+00:00
- **Authors**: Zitong Yu, Jun Wan, Yunxiao Qin, Xiaobai Li, Stan Z. Li, Guoying Zhao
- **Comment**: Accepted by IEEE Transactions on Pattern Analysis and Machine
  Intelligence (TPAMI); the CASIA-SURF 3DMask dataset is available at
  http://www.cbsr.ia.ac.cn/users/jwan/database/3DMask.pdf
- **Journal**: None
- **Summary**: Face anti-spoofing (FAS) plays a vital role in securing face recognition systems. Existing methods heavily rely on the expert-designed networks, which may lead to a sub-optimal solution for FAS task. Here we propose the first FAS method based on neural architecture search (NAS), called NAS-FAS, to discover the well-suited task-aware networks. Unlike previous NAS works mainly focus on developing efficient search strategies in generic object classification, we pay more attention to study the search spaces for FAS task. The challenges of utilizing NAS for FAS are in two folds: the networks searched on 1) a specific acquisition condition might perform poorly in unseen conditions, and 2) particular spoofing attacks might generalize badly for unseen attacks. To overcome these two issues, we develop a novel search space consisting of central difference convolution and pooling operators. Moreover, an efficient static-dynamic representation is exploited for fully mining the FAS-aware spatio-temporal discrepancy. Besides, we propose Domain/Type-aware Meta-NAS, which leverages cross-domain/type knowledge for robust searching. Finally, in order to evaluate the NAS transferability for cross datasets and unknown attack types, we release a large-scale 3D mask dataset, namely CASIA-SURF 3DMask, for supporting the new 'cross-dataset cross-type' testing protocol. Experiments demonstrate that the proposed NAS-FAS achieves state-of-the-art performance on nine FAS benchmark datasets with four testing protocols.



### Uncertainty Estimation in Medical Image Localization: Towards Robust Anterior Thalamus Targeting for Deep Brain Stimulation
- **Arxiv ID**: http://arxiv.org/abs/2011.02067v1
- **DOI**: 10.1007/978-3-030-61166-8_14
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.02067v1)
- **Published**: 2020-11-03 23:43:52+00:00
- **Updated**: 2020-11-03 23:43:52+00:00
- **Authors**: Han Liu, Can Cui, Dario J. Englot, Benoit M. Dawant
- **Comment**: 8 pages, 3 figures
- **Journal**: None
- **Summary**: Atlas-based methods are the standard approaches for automatic targeting of the Anterior Nucleus of the Thalamus (ANT) for Deep Brain Stimulation (DBS), but these are known to lack robustness when anatomic differences between atlases and subjects are large. To improve the localization robustness, we propose a novel two-stage deep learning (DL) framework, where the first stage identifies and crops the thalamus regions from the whole brain MRI and the second stage performs per-voxel regression on the cropped volume to localize the targets at the finest resolution scale. To address the issue of data scarcity, we train the models with the pseudo labels which are created based on the available labeled data using multi-atlas registration. To assess the performance of the proposed framework, we validate two sampling-based uncertainty estimation techniques namely Monte Carlo Dropout (MCDO) and Test-Time Augmentation (TTA) on the second-stage localization network. Moreover, we propose a novel uncertainty estimation metric called maximum activation dispersion (MAD) to estimate the image-wise uncertainty for localization tasks. Our results show that the proposed method achieved more robust localization performance than the traditional multi-atlas method and TTA could further improve the robustness. Moreover, the epistemic and hybrid uncertainty estimated by MAD could be used to detect the unreliable localizations and the magnitude of the uncertainty estimated by MAD could reflect the degree of unreliability for the rejected predictions.



