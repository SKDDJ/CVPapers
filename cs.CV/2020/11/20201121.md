# Arxiv Papers in cs.CV on 2020-11-21
### HDR Environment Map Estimation for Real-Time Augmented Reality
- **Arxiv ID**: http://arxiv.org/abs/2011.10687v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR, cs.LG, I.2.10; I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/2011.10687v5)
- **Published**: 2020-11-21 01:01:53+00:00
- **Updated**: 2021-07-27 20:48:22+00:00
- **Authors**: Gowri Somanath, Daniel Kurz
- **Comment**: Supplementary video at
  https://docs-assets.developer.apple.com/ml-research/papers/hdr-environment-map.mp4
  Code at https://github.com/apple/ml-envmapnet Accepted to CVPR 2021
- **Journal**: None
- **Summary**: We present a method to estimate an HDR environment map from a narrow field-of-view LDR camera image in real-time. This enables perceptually appealing reflections and shading on virtual objects of any material finish, from mirror to diffuse, rendered into a real physical environment using augmented reality. Our method is based on our efficient convolutional neural network architecture, EnvMapNet, trained end-to-end with two novel losses, ProjectionLoss for the generated image, and ClusterLoss for adversarial training. Through qualitative and quantitative comparison to state-of-the-art methods, we demonstrate that our algorithm reduces the directional error of estimated light sources by more than 50%, and achieves 3.7 times lower Frechet Inception Distance (FID). We further showcase a mobile application that is able to run our neural network model in under 9 ms on an iPhone XS, and render in real-time, visually coherent virtual objects in previously unseen real-world environments.



### Iterative Text-based Editing of Talking-heads Using Neural Retargeting
- **Arxiv ID**: http://arxiv.org/abs/2011.10688v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2011.10688v1)
- **Published**: 2020-11-21 01:05:55+00:00
- **Updated**: 2020-11-21 01:05:55+00:00
- **Authors**: Xinwei Yao, Ohad Fried, Kayvon Fatahalian, Maneesh Agrawala
- **Comment**: Project Website is https://davidyao.me/projects/text2vid
- **Journal**: None
- **Summary**: We present a text-based tool for editing talking-head video that enables an iterative editing workflow. On each iteration users can edit the wording of the speech, further refine mouth motions if necessary to reduce artifacts and manipulate non-verbal aspects of the performance by inserting mouth gestures (e.g. a smile) or changing the overall performance style (e.g. energetic, mumble). Our tool requires only 2-3 minutes of the target actor video and it synthesizes the video for each iteration in about 40 seconds, allowing users to quickly explore many editing possibilities as they iterate. Our approach is based on two key ideas. (1) We develop a fast phoneme search algorithm that can quickly identify phoneme-level subsequences of the source repository video that best match a desired edit. This enables our fast iteration loop. (2) We leverage a large repository of video of a source actor and develop a new self-supervised neural retargeting technique for transferring the mouth motions of the source actor to the target actor. This allows us to work with relatively short target actor videos, making our approach applicable in many real-world editing scenarios. Finally, our refinement and performance controls give users the ability to further fine-tune the synthesized results.



### Aerial Height Prediction and Refinement Neural Networks with Semantic and Geometric Guidance
- **Arxiv ID**: http://arxiv.org/abs/2011.10697v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.10697v4)
- **Published**: 2020-11-21 01:39:37+00:00
- **Updated**: 2021-11-12 16:54:23+00:00
- **Authors**: Elhousni Mahdi, Zhang Ziming, Huang Xinming
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning provides a powerful new approach to many computer vision tasks. Height prediction from aerial images is one of those tasks that benefited greatly from the deployment of deep learning which replaced old multi-view geometry techniques. This letter proposes a two-stage approach, where first a multi-task neural network is used to predict the height map resulting from a single RGB aerial input image. We also include a second refinement step, where a denoising autoencoder is used to produce higher quality height maps. Experiments on two publicly available datasets show that our method is capable of producing state-of-the-art results. Code is available at https://github.com/melhousni/DSMNet.



### Backdoor Attacks on the DNN Interpretation System
- **Arxiv ID**: http://arxiv.org/abs/2011.10698v3
- **DOI**: None
- **Categories**: **cs.CR**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.10698v3)
- **Published**: 2020-11-21 01:54:45+00:00
- **Updated**: 2022-07-19 21:42:22+00:00
- **Authors**: Shihong Fang, Anna Choromanska
- **Comment**: Published at the 2022 AAAI Conference on Artificial Intelligence
  (AAAI), 2022
- **Journal**: None
- **Summary**: Interpretability is crucial to understand the inner workings of deep neural networks (DNNs) and many interpretation methods generate saliency maps that highlight parts of the input image that contribute the most to the prediction made by the DNN. In this paper we design a backdoor attack that alters the saliency map produced by the network for an input image only with injected trigger that is invisible to the naked eye while maintaining the prediction accuracy. The attack relies on injecting poisoned data with a trigger into the training data set. The saliency maps are incorporated in the penalty term of the objective function that is used to train a deep model and its influence on model training is conditioned upon the presence of a trigger. We design two types of attacks: targeted attack that enforces a specific modification of the saliency map and untargeted attack when the importance scores of the top pixels from the original saliency map are significantly reduced. We perform empirical evaluation of the proposed backdoor attacks on gradient-based and gradient-free interpretation methods for a variety of deep learning architectures. We show that our attacks constitute a serious security threat when deploying deep learning models developed by untrusty sources. Finally, in the Supplement we demonstrate that the proposed methodology can be used in an inverted setting, where the correct saliency map can be obtained only in the presence of a trigger (key), effectively making the interpretation system available only to selected users.



### CancerNet-SCa: Tailored Deep Neural Network Designs for Detection of Skin Cancer from Dermoscopy Images
- **Arxiv ID**: http://arxiv.org/abs/2011.10702v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.10702v1)
- **Published**: 2020-11-21 02:17:59+00:00
- **Updated**: 2020-11-21 02:17:59+00:00
- **Authors**: James Ren Hou Lee, Maya Pavlova, Mahmoud Famouri, Alexander Wong
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: Skin cancer continues to be the most frequently diagnosed form of cancer in the U.S., with not only significant effects on health and well-being but also significant economic costs associated with treatment. A crucial step to the treatment and management of skin cancer is effective skin cancer detection due to strong prognosis when treated at an early stage, with one of the key screening approaches being dermoscopy examination. Motivated by the advances of deep learning and inspired by the open source initiatives in the research community, in this study we introduce CancerNet-SCa, a suite of deep neural network designs tailored for the detection of skin cancer from dermoscopy images that is open source and available to the general public as part of the Cancer-Net initiative. To the best of the authors' knowledge, CancerNet-SCa comprises of the first machine-designed deep neural network architecture designs tailored specifically for skin cancer detection, one of which possessing a self-attention architecture design with attention condensers. Furthermore, we investigate and audit the behaviour of CancerNet-SCa in a responsible and transparent manner via explainability-driven model auditing. While CancerNet-SCa is not a production-ready screening solution, the hope is that the release of CancerNet-SCa in open source, open access form will encourage researchers, clinicians, and citizen data scientists alike to leverage and build upon them.



### Neural Group Testing to Accelerate Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2011.10704v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.10704v2)
- **Published**: 2020-11-21 02:23:54+00:00
- **Updated**: 2021-05-09 23:03:47+00:00
- **Authors**: Weixin Liang, James Zou
- **Comment**: ISIT 2021. Code & data available at
  https://github.com/Weixin-Liang/NeuralGroupTesting
- **Journal**: None
- **Summary**: Recent advances in deep learning have made the use of large, deep neural networks with tens of millions of parameters. The sheer size of these networks imposes a challenging computational burden during inference. Existing work focuses primarily on accelerating each forward pass of a neural network. Inspired by the group testing strategy for efficient disease testing, we propose neural group testing, which accelerates by testing a group of samples in one forward pass. Groups of samples that test negative are ruled out. If a group tests positive, samples in that group are then retested adaptively. A key challenge of neural group testing is to modify a deep neural network so that it could test multiple samples in one forward pass. We propose three designs to achieve this without introducing any new parameters and evaluate their performances. We applied neural group testing in an image moderation task to detect rare but inappropriate images. We found that neural group testing can group up to 16 images in one forward pass and reduce the overall computation cost by over 73% while improving detection performance.



### Densely connected multidilated convolutional networks for dense prediction tasks
- **Arxiv ID**: http://arxiv.org/abs/2011.11844v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.11844v2)
- **Published**: 2020-11-21 05:15:12+00:00
- **Updated**: 2021-06-09 00:31:49+00:00
- **Authors**: Naoya Takahashi, Yuki Mitsufuji
- **Comment**: Accepted to CVPR 2021. arXiv admin note: text overlap with
  arXiv:2010.01733
- **Journal**: None
- **Summary**: Tasks that involve high-resolution dense prediction require a modeling of both local and global patterns in a large input field. Although the local and global structures often depend on each other and their simultaneous modeling is important, many convolutional neural network (CNN)-based approaches interchange representations in different resolutions only a few times. In this paper, we claim the importance of a dense simultaneous modeling of multiresolution representation and propose a novel CNN architecture called densely connected multidilated DenseNet (D3Net). D3Net involves a novel multidilated convolution that has different dilation factors in a single layer to model different resolutions simultaneously. By combining the multidilated convolution with the DenseNet architecture, D3Net incorporates multiresolution learning with an exponentially growing receptive field in almost all layers, while avoiding the aliasing problem that occurs when we naively incorporate the dilated convolution in DenseNet. Experiments on the image semantic segmentation task using Cityscapes and the audio source separation task using MUSDB18 show that the proposed method has superior performance over state-of-the-art methods.



### Object Rearrangement Using Learned Implicit Collision Functions
- **Arxiv ID**: http://arxiv.org/abs/2011.10726v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.10726v2)
- **Published**: 2020-11-21 05:36:06+00:00
- **Updated**: 2021-03-26 07:38:35+00:00
- **Authors**: Michael Danielczuk, Arsalan Mousavian, Clemens Eppner, Dieter Fox
- **Comment**: First two authors contributed equally. 2021 IEEE International
  Conference on Robotics and Automation. 8 pages, 4 figures, 3 tables
- **Journal**: None
- **Summary**: Robotic object rearrangement combines the skills of picking and placing objects. When object models are unavailable, typical collision-checking models may be unable to predict collisions in partial point clouds with occlusions, making generation of collision-free grasping or placement trajectories challenging. We propose a learned collision model that accepts scene and query object point clouds and predicts collisions for 6DOF object poses within the scene. We train the model on a synthetic set of 1 million scene/object point cloud pairs and 2 billion collision queries. We leverage the learned collision model as part of a model predictive path integral (MPPI) policy in a tabletop rearrangement task and show that the policy can plan collision-free grasps and placements for objects unseen in training in both simulated and physical cluttered scenes with a Franka Panda robot. The learned model outperforms both traditional pipelines and learned ablations by 9.8% in accuracy on a dataset of simulated collision queries and is 75x faster than the best-performing baseline. Videos and supplementary material are available at https://research.nvidia.com/publication/2021-03_Object-Rearrangement-Using.



### Stochastic Talking Face Generation Using Latent Distribution Matching
- **Arxiv ID**: http://arxiv.org/abs/2011.10727v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.10727v1)
- **Published**: 2020-11-21 06:05:24+00:00
- **Updated**: 2020-11-21 06:05:24+00:00
- **Authors**: Ravindra Yadav, Ashish Sardana, Vinay P Namboodiri, Rajesh M Hegde
- **Comment**: InterSpeech 2020
- **Journal**: None
- **Summary**: The ability to envisage the visual of a talking face based just on hearing a voice is a unique human capability. There have been a number of works that have solved for this ability recently. We differ from these approaches by enabling a variety of talking face generations based on single audio input. Indeed, just having the ability to generate a single talking face would make a system almost robotic in nature. In contrast, our unsupervised stochastic audio-to-video generation model allows for diverse generations from a single audio input. Particularly, we present an unsupervised stochastic audio-to-video generation model that can capture multiple modes of the video distribution. We ensure that all the diverse generations are plausible. We do so through a principled multi-modal variational autoencoder framework. We demonstrate its efficacy on the challenging LRW and GRID datasets and demonstrate performance better than the baseline, while having the ability to generate multiple diverse lip synchronized videos.



### LRTA: A Transparent Neural-Symbolic Reasoning Framework with Modular Supervision for Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/2011.10731v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.10731v1)
- **Published**: 2020-11-21 06:39:42+00:00
- **Updated**: 2020-11-21 06:39:42+00:00
- **Authors**: Weixin Liang, Feiyang Niu, Aishwarya Reganti, Govind Thattai, Gokhan Tur
- **Comment**: NeurIPS KR2ML 2020
- **Journal**: None
- **Summary**: The predominant approach to visual question answering (VQA) relies on encoding the image and question with a "black-box" neural encoder and decoding a single token as the answer like "yes" or "no". Despite this approach's strong quantitative results, it struggles to come up with intuitive, human-readable forms of justification for the prediction process. To address this insufficiency, we reformulate VQA as a full answer generation task, which requires the model to justify its predictions in natural language. We propose LRTA [Look, Read, Think, Answer], a transparent neural-symbolic reasoning framework for visual question answering that solves the problem step-by-step like humans and provides human-readable form of justification at each step. Specifically, LRTA learns to first convert an image into a scene graph and parse a question into multiple reasoning instructions. It then executes the reasoning instructions one at a time by traversing the scene graph using a recurrent neural-symbolic execution module. Finally, it generates a full answer to the given question with natural language justifications. Our experiments on GQA dataset show that LRTA outperforms the state-of-the-art model by a large margin (43.1% v.s. 28.0%) on the full answer generation task. We also create a perturbed GQA test set by removing linguistic cues (attributes and relations) in the questions for analyzing whether a model is having a smart guess with superficial data correlations. We show that LRTA makes a step towards truly understanding the question while the state-of-the-art model tends to learn superficial correlations from the training data.



### SuperOCR: A Conversion from Optical Character Recognition to Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/2012.02033v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2012.02033v1)
- **Published**: 2020-11-21 06:40:04+00:00
- **Updated**: 2020-11-21 06:40:04+00:00
- **Authors**: Baohua Sun, Michael Lin, Hao Sha, Lin Yang
- **Comment**: 8 pages, 2 figures, 2 tables
- **Journal**: None
- **Summary**: Optical Character Recognition (OCR) has many real world applications. The existing methods normally detect where the characters are, and then recognize the character for each detected location. Thus the accuracy of characters recognition is impacted by the performance of characters detection. In this paper, we propose a method for recognizing characters without detecting the location of each character. This is done by converting the OCR task into an image captioning task. One advantage of the proposed method is that the labeled bounding boxes for the characters are not needed during training. The experimental results show the proposed method outperforms the existing methods on both the license plate recognition and the watermeter character recognition tasks. The proposed method is also deployed into a low-power (300mW) CNN accelerator chip connected to a Raspberry Pi 3 for on-device applications.



### MRI-Guided High Intensity Focused Ultrasound of Liver and Kidney
- **Arxiv ID**: http://arxiv.org/abs/2011.10752v1
- **DOI**: 10.1007/174_2011_394
- **Categories**: **physics.med-ph**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.10752v1)
- **Published**: 2020-11-21 09:37:06+00:00
- **Updated**: 2020-11-21 09:37:06+00:00
- **Authors**: Baudouin Denis de Senneville, Mario Ries, Wilbert Bartels, Chrit Moonen
- **Comment**: 27 pages, 5 figures
- **Journal**: None
- **Summary**: High Intensity Focused Ultrasound (HIFU) can be used to achieve a local temperature increase deep inside the human body in a non-invasive way. MRI guidance of the procedure allows in situ target definition. In addition, MRI can be used to provide continuous temperature mapping during HIFU for spatial and temporal control of the heating procedure and prediction of the final lesion based on the received thermal dose. Temperature mapping of mobile organs as kidney and liver is challenging, as well as real-time processing methods for feedback control of the HIFU procedure. In this paper, recent technological advances are reviewed in MR temperature mapping of these organs, in motion compensation of the HIFU beam, in intercostal HIFU sonication, and in volumetric ablation and feedback control strategies. Recent pre-clinical studies have demonstrated the feasibility of each of these novel methods. The perspectives to translate those advances into the clinic are addressed. It can be concluded that MR guided HIFU for ablation in liver and kidney appears feasible but requires further work on integration of technologically advanced methods.



### Visual Recognition of Great Ape Behaviours in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2011.10759v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.10759v1)
- **Published**: 2020-11-21 10:27:21+00:00
- **Updated**: 2020-11-21 10:27:21+00:00
- **Authors**: Faizaan Sakib, Tilo Burghardt
- **Comment**: 4 pages, 4 figures, to be published in the proceedings of ICPR 2020
  at the Visual observation and analysis of Vertebrate And Insect Behaviour
  (VAIB) workshop
- **Journal**: None
- **Summary**: We propose a first great ape-specific visual behaviour recognition system utilising deep learning that is capable of detecting nine core ape behaviours.



### One Metric to Measure them All: Localisation Recall Precision (LRP) for Evaluating Visual Detection Tasks
- **Arxiv ID**: http://arxiv.org/abs/2011.10772v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.10772v3)
- **Published**: 2020-11-21 11:20:42+00:00
- **Updated**: 2021-11-21 17:26:03+00:00
- **Authors**: Kemal Oksuz, Baris Can Cam, Sinan Kalkan, Emre Akbas
- **Comment**: Accepted to TPAMI
- **Journal**: None
- **Summary**: Despite being widely used as a performance measure for visual detection tasks, Average Precision (AP) is limited in (i) reflecting localisation quality, (ii) interpretability and (iii) robustness to the design choices regarding its computation, and its applicability to outputs without confidence scores. Panoptic Quality (PQ), a measure proposed for evaluating panoptic segmentation (Kirillov et al., 2019), does not suffer from these limitations but is limited to panoptic segmentation. In this paper, we propose Localisation Recall Precision (LRP) Error as the average matching error of a visual detector computed based on both its localisation and classification qualities for a given confidence score threshold. LRP Error, initially proposed only for object detection by Oksuz et al. (2018), does not suffer from the aforementioned limitations and is applicable to all visual detection tasks. We also introduce Optimal LRP (oLRP) Error as the minimum LRP Error obtained over confidence scores to evaluate visual detectors and obtain optimal thresholds for deployment. We provide a detailed comparative analysis of LRP Error with AP and PQ, and use nearly 100 state-of-the-art visual detectors from seven visual detection tasks (i.e. object detection, keypoint detection, instance segmentation, panoptic segmentation, visual relationship detection, zero-shot detection and generalised zero-shot detection) using ten datasets to empirically show that LRP Error provides richer and more discriminative information than its counterparts. Code available at: https://github.com/kemaloksuz/LRP-Error



### DmifNet:3D Shape Reconstruction Based on Dynamic Multi-Branch Information Fusion
- **Arxiv ID**: http://arxiv.org/abs/2011.10776v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.10776v1)
- **Published**: 2020-11-21 11:31:27+00:00
- **Updated**: 2020-11-21 11:31:27+00:00
- **Authors**: Lei Li, Suping Wu
- **Comment**: ICPR 2020 (Oral)
- **Journal**: None
- **Summary**: 3D object reconstruction from a single-view image is a long-standing challenging problem. Previous work was difficult to accurately reconstruct 3D shapes with a complex topology which has rich details at the edges and corners. Moreover, previous works used synthetic data to train their network, but domain adaptation problems occurred when tested on real data. In this paper, we propose a Dynamic Multi-branch Information Fusion Network (DmifNet) which can recover a high-fidelity 3D shape of arbitrary topology from a 2D image. Specifically, we design several side branches from the intermediate layers to make the network produce more diverse representations to improve the generalization ability of network. In addition, we utilize DoG (Difference of Gaussians) to extract edge geometry and corners information from input images. Then, we use a separate side branch network to process the extracted data to better capture edge geometry and corners feature information. Finally, we dynamically fuse the information of all branches to gain final predicted probability. Extensive qualitative and quantitative experiments on a large-scale publicly available dataset demonstrate the validity and efficiency of our method. Code and models are publicly available at https://github.com/leilimaster/DmifNet.



### MoNet: Motion-based Point Cloud Prediction Network
- **Arxiv ID**: http://arxiv.org/abs/2011.10812v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.10812v1)
- **Published**: 2020-11-21 15:43:31+00:00
- **Updated**: 2020-11-21 15:43:31+00:00
- **Authors**: Fan Lu, Guang Chen, Yinlong Liu, Zhijun Li, Sanqing Qu, Tianpei Zou
- **Comment**: None
- **Journal**: None
- **Summary**: Predicting the future can significantly improve the safety of intelligent vehicles, which is a key component in autonomous driving. 3D point clouds accurately model 3D information of surrounding environment and are crucial for intelligent vehicles to perceive the scene. Therefore, prediction of 3D point clouds has great significance for intelligent vehicles, which can be utilized for numerous further applications. However, due to point clouds are unordered and unstructured, point cloud prediction is challenging and has not been deeply explored in current literature. In this paper, we propose a novel motion-based neural network named MoNet. The key idea of the proposed MoNet is to integrate motion features between two consecutive point clouds into the prediction pipeline. The introduction of motion features enables the model to more accurately capture the variations of motion information across frames and thus make better predictions for future motion. In addition, content features are introduced to model the spatial content of individual point clouds. A recurrent neural network named MotionRNN is proposed to capture the temporal correlations of both features. Besides, we propose an attention-based motion align module to address the problem of missing motion features in the inference pipeline. Extensive experiments on two large scale outdoor LiDAR datasets demonstrate the performance of the proposed MoNet. Moreover, we perform experiments on applications using the predicted point clouds and the results indicate the great application potential of the proposed method.



### A System for Automatic Rice Disease Detection from Rice Paddy Images Serviced via a Chatbot
- **Arxiv ID**: http://arxiv.org/abs/2011.10823v2
- **DOI**: 10.1016/j.compag.2021.106156
- **Categories**: **eess.SY**, cs.CV, cs.SY
- **Links**: [PDF](http://arxiv.org/pdf/2011.10823v2)
- **Published**: 2020-11-21 16:45:02+00:00
- **Updated**: 2021-06-23 08:49:24+00:00
- **Authors**: Pitchayagan Temniranrat, Kantip Kiratiratanapruk, Apichon Kitvimonrat, Wasin Sinthupinyo, Sujin Patarapuwadol
- **Comment**: 19 pages, 6 figures
- **Journal**: Computers and Electronics in Agriculture, Volume 185, June 2021,
  106156
- **Summary**: A LINE Bot System to diagnose rice diseases from actual paddy field images was developed and presented in this paper. It was easy-to-use and automatic system designed to help rice farmers improve the rice yield and quality. The targeted images were taken from the actual paddy environment without special sample preparation. We used a deep learning neural networks technique to detect rice diseases from the images. We developed an object detection model training and refinement process to improve the performance of our previous research on rice leave diseases detection. The process was based on analyzing the model's predictive results and could be repeatedly used to improve the quality of the database in the next training of the model. The deployment model for our LINE Bot system was created from the selected best performance technique in our previous paper, YOLOv3, trained by refined training data set. The performance of the deployment model was measured on 5 target classes and found that the Average True Positive Point improved from 91.1% in the previous paper to 95.6% in this study. Therefore, we used this deployment model for Rice Disease LINE Bot system. Our system worked automatically real-time to suggest primary diagnosis results to the users in the LINE group, which included rice farmers and rice disease specialists. They could communicate freely via chat. In the real LINE Bot deployment, the model's performance was measured by our own defined measurement Average True Positive Point and was found to be an average of 78.86%. The system was fast and took only 2-3 s for detection process in our system server.



### Boundary-sensitive Pre-training for Temporal Localization in Videos
- **Arxiv ID**: http://arxiv.org/abs/2011.10830v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.10830v3)
- **Published**: 2020-11-21 17:46:24+00:00
- **Updated**: 2021-03-26 11:01:35+00:00
- **Authors**: Mengmeng Xu, Juan-Manuel Perez-Rua, Victor Escorcia, Brais Martinez, Xiatian Zhu, Li Zhang, Bernard Ghanem, Tao Xiang
- **Comment**: 11 pages, 4 figures
- **Journal**: None
- **Summary**: Many video analysis tasks require temporal localization thus detection of content changes. However, most existing models developed for these tasks are pre-trained on general video action classification tasks. This is because large scale annotation of temporal boundaries in untrimmed videos is expensive. Therefore no suitable datasets exist for temporal boundary-sensitive pre-training. In this paper for the first time, we investigate model pre-training for temporal localization by introducing a novel boundary-sensitive pretext (BSP) task. Instead of relying on costly manual annotations of temporal boundaries, we propose to synthesize temporal boundaries in existing video action classification datasets. With the synthesized boundaries, BSP can be simply conducted via classifying the boundary types. This enables the learning of video representations that are much more transferable to downstream temporal localization tasks. Extensive experiments show that the proposed BSP is superior and complementary to the existing action classification based pre-training counterpart, and achieves new state-of-the-art performance on several temporal localization tasks.



### Deep Learning-Based Computer Vision for Real Time Intravenous Drip Infusion Monitoring
- **Arxiv ID**: http://arxiv.org/abs/2011.10839v1
- **DOI**: 10.1109/JSEN.2020.3039009
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.10839v1)
- **Published**: 2020-11-21 18:26:44+00:00
- **Updated**: 2020-11-21 18:26:44+00:00
- **Authors**: Nicola Giaquinto, Marco Scarpetta, Maurizio Spadavecchia, Gregorio Andria
- **Comment**: 7 pages, 9 figures
- **Journal**: None
- **Summary**: This paper explores the use of deep learning-based computer vision for real-time monitoring of the flow in intravenous (IV) infusions. IV infusions are among the most common therapies in hospitalized patients and, given that both over-infusion and under-infusion can cause severe damages, monitoring the flow rate of the fluid being administered to patients is very important for their safety. The proposed system uses a camera to film the IV drip infusion kit and a deep learning-based algorithm to classify acquired frames into two different states: frames with a drop that has just begun to take shape and frames with a well-formed drop. The alternation of these two states is used to count drops and derive a measurement of the flow rate of the drip. The usage of a camera as sensing element makes the proposed system safe in medical environments and easier to be integrated into current health facilities. Experimental results are reported in the paper that confirm the accuracy of the system and its capability to produce real-time estimates. The proposed method can be therefore effectively adopted to implement IV infusion monitoring and control systems.



### Robust Data Hiding Using Inverse Gradient Attention
- **Arxiv ID**: http://arxiv.org/abs/2011.10850v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/2011.10850v5)
- **Published**: 2020-11-21 19:08:23+00:00
- **Updated**: 2022-10-17 14:36:53+00:00
- **Authors**: Honglei Zhang, Hu Wang, Yuanzhouhan Cao, Chunhua Shen, Yidong Li
- **Comment**: 9 pages, 6 figures
- **Journal**: None
- **Summary**: Data hiding is the procedure of encoding desired information into a certain types of cover media (e.g. images) to resist potential noises for data recovery, while ensuring the embedded image has few perceptual perturbations. Recently, with the tremendous successes gained by deep neural networks in various fields, the research on data hiding with deep learning models has attracted an increasing amount of attentions. In deep data hiding models, to maximize the encoding capacity, each pixel of the cover image ought to be treated differently since they have different sensitivities w.r.t. visual quality. The neglecting to consider the sensitivity of each pixel inevitably affects the model's robustness for information hiding. In this paper, we propose a novel deep data hiding scheme with Inverse Gradient Attention (IGA), combining the idea of attention mechanism to endow different attention weights for different pixels. Equipped with the proposed modules, the model can spotlight pixels with more robustness for data hiding. Extensive experiments demonstrate that the proposed model outperforms the mainstream deep learning based data hiding methods on two prevalent datasets under multiple evaluation metrics. Besides, we further identify and discuss the connections between the proposed inverse gradient attention and high-frequency regions within images, which can serve as an informative reference to the deep data hiding research community. The codes are available at: https://github.com/hongleizhang/IGA.



### Contextual Interference Reduction by Selective Fine-Tuning of Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2011.10857v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.10857v1)
- **Published**: 2020-11-21 20:11:12+00:00
- **Updated**: 2020-11-21 20:11:12+00:00
- **Authors**: Mahdi Biparva, John Tsotsos
- **Comment**: None
- **Journal**: None
- **Summary**: Feature disentanglement of the foreground target objects and the background surrounding context has not been yet fully accomplished. The lack of network interpretability prevents advancing for feature disentanglement and better generalization robustness. We study the role of the context on interfering with a disentangled foreground target object representation in this work. We hypothesize that the representation of the surrounding context is heavily tied with the foreground object due to the dense hierarchical parametrization of convolutional networks with under-constrained learning algorithms. Working on a framework that benefits from the bottom-up and top-down processing paradigms, we investigate a systematic approach to shift learned representations in feedforward networks from the emphasis on the irrelevant context to the foreground objects. The top-down processing provides importance maps as the means of the network internal self-interpretation that will guide the learning algorithm to focus on the relevant foreground regions towards achieving a more robust representations. We define an experimental evaluation setup with the role of context emphasized using the MNIST dataset. The experimental results reveal not only that the label prediction accuracy is improved but also a higher degree of robustness to the background perturbation using various noise generation methods is obtained.



### Transparent Object Tracking Benchmark
- **Arxiv ID**: http://arxiv.org/abs/2011.10875v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.10875v2)
- **Published**: 2020-11-21 21:39:43+00:00
- **Updated**: 2021-08-01 21:14:37+00:00
- **Authors**: Heng Fan, Halady Akhilesha Miththanthaya, Harshit, Siranjiv Ramana Rajan, Xiaoqiong Liu, Zhilin Zou, Yuewei Lin, Haibin Ling
- **Comment**: Tech. Report
- **Journal**: None
- **Summary**: Visual tracking has achieved considerable progress in recent years. However, current research in the field mainly focuses on tracking of opaque objects, while little attention is paid to transparent object tracking. In this paper, we make the first attempt in exploring this problem by proposing a Transparent Object Tracking Benchmark (TOTB). Specifically, TOTB consists of 225 videos (86K frames) from 15 diverse transparent object categories. Each sequence is manually labeled with axis-aligned bounding boxes. To the best of our knowledge, TOTB is the first benchmark dedicated to transparent object tracking. In order to understand how existing trackers perform and to provide comparison for future research on TOTB, we extensively evaluate 25 state-of-the-art tracking algorithms. The evaluation results exhibit that more efforts are needed to improve transparent object tracking. Besides, we observe some nontrivial findings from the evaluation that are discrepant with some common beliefs in opaque object tracking. For example, we find that deeper features are not always good for improvements. Moreover, to encourage future research, we introduce a novel tracker, named TransATOM, which leverages transparency features for tracking and surpasses all 25 evaluated approaches by a large margin. By releasing TOTB, we expect to facilitate future research and application of transparent object tracking in both the academia and industry. The TOTB and evaluation results as well as TransATOM are available at https://hengfan2010.github.io/projects/TOTB.



### Rethinking Transformer-based Set Prediction for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2011.10881v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.10881v2)
- **Published**: 2020-11-21 21:59:42+00:00
- **Updated**: 2021-10-12 06:09:03+00:00
- **Authors**: Zhiqing Sun, Shengcao Cao, Yiming Yang, Kris Kitani
- **Comment**: Accepted to ICCV 2021
- **Journal**: None
- **Summary**: DETR is a recently proposed Transformer-based method which views object detection as a set prediction problem and achieves state-of-the-art performance but demands extra-long training time to converge. In this paper, we investigate the causes of the optimization difficulty in the training of DETR. Our examinations reveal several factors contributing to the slow convergence of DETR, primarily the issues with the Hungarian loss and the Transformer cross-attention mechanism. To overcome these issues we propose two solutions, namely, TSP-FCOS (Transformer-based Set Prediction with FCOS) and TSP-RCNN (Transformer-based Set Prediction with RCNN). Experimental results show that the proposed methods not only converge much faster than the original DETR, but also significantly outperform DETR and other baselines in terms of detection accuracy.



### Deep learning for video game genre classification
- **Arxiv ID**: http://arxiv.org/abs/2011.12143v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, I.4.9; I.2.6
- **Links**: [PDF](http://arxiv.org/pdf/2011.12143v1)
- **Published**: 2020-11-21 22:31:43+00:00
- **Updated**: 2020-11-21 22:31:43+00:00
- **Authors**: Yuhang Jiang, Lukun Zheng
- **Comment**: 21 pages, 6 figures, 3 tables. arXiv admin note: substantial text
  overlap with arXiv:2011.07658
- **Journal**: None
- **Summary**: Video game genre classification based on its cover and textual description would be utterly beneficial to many modern identification, collocation, and retrieval systems. At the same time, it is also an extremely challenging task due to the following reasons: First, there exists a wide variety of video game genres, many of which are not concretely defined. Second, video game covers vary in many different ways such as colors, styles, textual information, etc, even for games of the same genre. Third, cover designs and textual descriptions may vary due to many external factors such as country, culture, target reader populations, etc. With the growing competitiveness in the video game industry, the cover designers and typographers push the cover designs to its limit in the hope of attracting sales. The computer-based automatic video game genre classification systems become a particularly exciting research topic in recent years. In this paper, we propose a multi-modal deep learning framework to solve this problem. The contribution of this paper is four-fold. First, we compiles a large dataset consisting of 50,000 video games from 21 genres made of cover images, description text, and title text and the genre information. Second, image-based and text-based, state-of-the-art models are evaluated thoroughly for the task of genre classification for video games. Third, we developed an efficient and salable multi-modal framework based on both images and texts. Fourth, a thorough analysis of the experimental results is given and future works to improve the performance is suggested. The results show that the multi-modal framework outperforms the current state-of-the-art image-based or text-based models. Several challenges are outlined for this task. More efforts and resources are needed for this classification task in order to reach a satisfactory level.



### Zero-Shot Learning with Knowledge Enhanced Visual Semantic Embeddings
- **Arxiv ID**: http://arxiv.org/abs/2011.10889v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.10889v1)
- **Published**: 2020-11-21 22:58:38+00:00
- **Updated**: 2020-11-21 22:58:38+00:00
- **Authors**: Karan Sikka, Jihua Huang, Andrew Silberfarb, Prateeth Nayak, Luke Rohrer, Pritish Sahu, John Byrnes, Ajay Divakaran, Richard Rohwer
- **Comment**: None
- **Journal**: None
- **Summary**: We improve zero-shot learning (ZSL) by incorporating common-sense knowledge in DNNs. We propose Common-Sense based Neuro-Symbolic Loss (CSNL) that formulates prior knowledge as novel neuro-symbolic loss functions that regularize visual-semantic embedding. CSNL forces visual features in the VSE to obey common-sense rules relating to hypernyms and attributes. We introduce two key novelties for improved learning: (1) enforcement of rules for a group instead of a single concept to take into account class-wise relationships, and (2) confidence margins inside logical operators that enable implicit curriculum learning and prevent premature overfitting. We evaluate the advantages of incorporating each knowledge source and show consistent gains over prior state-of-art methods in both conventional and generalized ZSL e.g. 11.5%, 5.5%, and 11.6% improvements on AWA2, CUB, and Kinetics respectively.



### Rank-smoothed Pairwise Learning In Perceptual Quality Assessment
- **Arxiv ID**: http://arxiv.org/abs/2011.10893v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.10893v1)
- **Published**: 2020-11-21 23:33:14+00:00
- **Updated**: 2020-11-21 23:33:14+00:00
- **Authors**: Hossein Talebi, Ehsan Amid, Peyman Milanfar, Manfred K. Warmuth
- **Comment**: None
- **Journal**: IEEE International Conference on Image Processing (ICIP) 2020
- **Summary**: Conducting pairwise comparisons is a widely used approach in curating human perceptual preference data. Typically raters are instructed to make their choices according to a specific set of rules that address certain dimensions of image quality and aesthetics. The outcome of this process is a dataset of sampled image pairs with their associated empirical preference probabilities. Training a model on these pairwise preferences is a common deep learning approach. However, optimizing by gradient descent through mini-batch learning means that the "global" ranking of the images is not explicitly taken into account. In other words, each step of the gradient descent relies only on a limited number of pairwise comparisons. In this work, we demonstrate that regularizing the pairwise empirical probabilities with aggregated rankwise probabilities leads to a more reliable training loss. We show that training a deep image quality assessment model with our rank-smoothed loss consistently improves the accuracy of predicting human preferences.



