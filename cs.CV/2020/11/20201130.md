# Arxiv Papers in cs.CV on 2020-11-30
### What Can Style Transfer and Paintings Do For Model Robustness?
- **Arxiv ID**: http://arxiv.org/abs/2011.14477v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.14477v2)
- **Published**: 2020-11-30 00:25:04+00:00
- **Updated**: 2021-05-27 11:31:36+00:00
- **Authors**: Hubert Lin, Mitchell van Zuijlen, Sylvia C. Pont, Maarten W. A. Wijntjes, Kavita Bala
- **Comment**: CVPR 2021
- **Journal**: None
- **Summary**: A common strategy for improving model robustness is through data augmentations. Data augmentations encourage models to learn desired invariances, such as invariance to horizontal flipping or small changes in color. Recent work has shown that arbitrary style transfer can be used as a form of data augmentation to encourage invariance to textures by creating painting-like images from photographs. However, a stylized photograph is not quite the same as an artist-created painting. Artists depict perceptually meaningful cues in paintings so that humans can recognize salient components in scenes, an emphasis which is not enforced in style transfer. Therefore, we study how style transfer and paintings differ in their impact on model robustness. First, we investigate the role of paintings as style images for stylization-based data augmentation. We find that style transfer functions well even without paintings as style images. Second, we show that learning from paintings as a form of perceptual data augmentation can improve model robustness. Finally, we investigate the invariances learned from stylization and from paintings, and show that models learn different invariances from these differing forms of data. Our results provide insights into how stylization improves model robustness, and provide evidence that artist-created paintings can be a valuable source of data for model robustness.



### Annotation-Efficient Untrimmed Video Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2011.14478v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.14478v3)
- **Published**: 2020-11-30 00:26:58+00:00
- **Updated**: 2021-11-01 03:27:54+00:00
- **Authors**: Yixiong Zou, Shanghang Zhang, Guangyao Chen, Yonghong Tian, Kurt Keutzer, Jos√© M. F. Moura
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning has achieved great success in recognizing video actions, but the collection and annotation of training data are still quite laborious, which mainly lies in two aspects: (1) the amount of required annotated data is large; (2) temporally annotating the location of each action is time-consuming. Works such as few-shot learning or untrimmed video recognition have been proposed to handle either one aspect or the other. However, very few existing works can handle both issues simultaneously. In this paper, we target a new problem, Annotation-Efficient Video Recognition, to reduce the requirement of annotations for both large amount of samples and the action location. Such problem is challenging due to two aspects: (1) the untrimmed videos only have weak supervision; (2) video segments not relevant to current actions of interests (background, BG) could contain actions of interests (foreground, FG) in novel classes, which is a widely existing phenomenon but has rarely been studied in few-shot untrimmed video recognition. To achieve this goal, by analyzing the property of BG, we categorize BG into informative BG (IBG) and non-informative BG (NBG), and we propose (1) an open-set detection based method to find the NBG and FG, (2) a contrastive learning method to learn IBG and distinguish NBG in a self-supervised way, and (3) a self-weighting mechanism for the better distinguishing of IBG and FG. Extensive experiments on ActivityNet v1.2 and ActivityNet v1.3 verify the rationale and effectiveness of the proposed methods.



### Multi-scale Adaptive Task Attention Network for Few-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2011.14479v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.14479v1)
- **Published**: 2020-11-30 00:36:01+00:00
- **Updated**: 2020-11-30 00:36:01+00:00
- **Authors**: Haoxing Chen, Huaxiong Li, Yaohui Li, Chunlin Chen
- **Comment**: None
- **Journal**: None
- **Summary**: The goal of few-shot learning is to classify unseen categories with few labeled samples. Recently, the low-level information metric-learning based methods have achieved satisfying performance, since local representations (LRs) are more consistent between seen and unseen classes. However, most of these methods deal with each category in the support set independently, which is not sufficient to measure the relation between features, especially in a certain task. Moreover, the low-level information-based metric learning method suffers when dominant objects of different scales exist in a complex background. To address these issues, this paper proposes a novel Multi-scale Adaptive Task Attention Network (MATANet) for few-shot learning. Specifically, we first use a multi-scale feature generator to generate multiple features at different scales. Then, an adaptive task attention module is proposed to select the most important LRs among the entire task. Afterwards, a similarity-to-class module and a fusion layer are utilized to calculate a joint multi-scale similarity between the query image and the support set. Extensive experiments on popular benchmarks clearly show the effectiveness of the proposed MATANet compared with state-of-the-art methods.



### Self-Supervised Real-to-Sim Scene Generation
- **Arxiv ID**: http://arxiv.org/abs/2011.14488v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.14488v2)
- **Published**: 2020-11-30 01:21:55+00:00
- **Updated**: 2021-08-18 18:06:41+00:00
- **Authors**: Aayush Prakash, Shoubhik Debnath, Jean-Francois Lafleche, Eric Cameracci, Gavriel State, Stan Birchfield, Marc T. Law
- **Comment**: accepted at ICCV 2021. Project page:
  https://research.nvidia.com/publication/2021-08_Sim2SG
- **Journal**: None
- **Summary**: Synthetic data is emerging as a promising solution to the scalability issue of supervised deep learning, especially when real data are difficult to acquire or hard to annotate. Synthetic data generation, however, can itself be prohibitively expensive when domain experts have to manually and painstakingly oversee the process. Moreover, neural networks trained on synthetic data often do not perform well on real data because of the domain gap. To solve these challenges, we propose Sim2SG, a self-supervised automatic scene generation technique for matching the distribution of real data. Importantly, Sim2SG does not require supervision from the real-world dataset, thus making it applicable in situations for which such annotations are difficult to obtain. Sim2SG is designed to bridge both the content and appearance gaps, by matching the content of real data, and by matching the features in the source and target domains. We select scene graph (SG) generation as the downstream task, due to the limited availability of labeled datasets. Experiments demonstrate significant improvements over leading baselines in reducing the domain gap both qualitatively and quantitatively, on several synthetic datasets as well as the real-world KITTI dataset.



### End-to-End Video Instance Segmentation with Transformers
- **Arxiv ID**: http://arxiv.org/abs/2011.14503v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.14503v5)
- **Published**: 2020-11-30 02:03:50+00:00
- **Updated**: 2021-10-08 07:23:21+00:00
- **Authors**: Yuqing Wang, Zhaoliang Xu, Xinlong Wang, Chunhua Shen, Baoshan Cheng, Hao Shen, Huaxia Xia
- **Comment**: CVPR2021 Oral
- **Journal**: None
- **Summary**: Video instance segmentation (VIS) is the task that requires simultaneously classifying, segmenting and tracking object instances of interest in video. Recent methods typically develop sophisticated pipelines to tackle this task. Here, we propose a new video instance segmentation framework built upon Transformers, termed VisTR, which views the VIS task as a direct end-to-end parallel sequence decoding/prediction problem. Given a video clip consisting of multiple image frames as input, VisTR outputs the sequence of masks for each instance in the video in order directly. At the core is a new, effective instance sequence matching and segmentation strategy, which supervises and segments instances at the sequence level as a whole. VisTR frames the instance segmentation and tracking in the same perspective of similarity learning, thus considerably simplifying the overall pipeline and is significantly different from existing approaches. Without bells and whistles, VisTR achieves the highest speed among all existing VIS models, and achieves the best result among methods using single model on the YouTube-VIS dataset. For the first time, we demonstrate a much simpler and faster video instance segmentation framework built upon Transformers, achieving competitive accuracy. We hope that VisTR can motivate future research for more video understanding tasks.



### Training and Inference for Integer-Based Semantic Segmentation Network
- **Arxiv ID**: http://arxiv.org/abs/2011.14504v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2011.14504v1)
- **Published**: 2020-11-30 02:07:07+00:00
- **Updated**: 2020-11-30 02:07:07+00:00
- **Authors**: Jiayi Yang, Lei Deng, Yukuan Yang, Yuan Xie, Guoqi Li
- **Comment**: 17 page, 12 figures, submitted to Neurocomputing
- **Journal**: None
- **Summary**: Semantic segmentation has been a major topic in research and industry in recent years. However, due to the computation complexity of pixel-wise prediction and backpropagation algorithm, semantic segmentation has been demanding in computation resources, resulting in slow training and inference speed and large storage space to store models. Existing schemes that speed up segmentation network change the network structure and come with noticeable accuracy degradation. However, neural network quantization can be used to reduce computation load while maintaining comparable accuracy and original network structure. Semantic segmentation networks are different from traditional deep convolutional neural networks (DCNNs) in many ways, and this topic has not been thoroughly explored in existing works. In this paper, we propose a new quantization framework for training and inference of segmentation networks, where parameters and operations are constrained to 8-bit integer-based values for the first time. Full quantization of the data flow and the removal of square and root operations in batch normalization give our framework the ability to perform inference on fixed-point devices. Our proposed framework is evaluated on mainstream semantic segmentation networks like FCN-VGG16 and DeepLabv3-ResNet50, achieving comparable accuracy against floating-point framework on ADE20K dataset and PASCAL VOC 2012 dataset.



### Adaptive noise imitation for image denoising
- **Arxiv ID**: http://arxiv.org/abs/2011.14512v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.14512v1)
- **Published**: 2020-11-30 02:49:36+00:00
- **Updated**: 2020-11-30 02:49:36+00:00
- **Authors**: Huangxing Lin, Yihong Zhuang, Yue Huang, Xinghao Ding, Yizhou Yu, Xiaoqing Liu, John Paisley
- **Comment**: None
- **Journal**: None
- **Summary**: The effectiveness of existing denoising algorithms typically relies on accurate pre-defined noise statistics or plenty of paired data, which limits their practicality. In this work, we focus on denoising in the more common case where noise statistics and paired data are unavailable. Considering that denoising CNNs require supervision, we develop a new \textbf{adaptive noise imitation (ADANI)} algorithm that can synthesize noisy data from naturally noisy images. To produce realistic noise, a noise generator takes unpaired noisy/clean images as input, where the noisy image is a guide for noise generation. By imposing explicit constraints on the type, level and gradient of noise, the output noise of ADANI will be similar to the guided noise, while keeping the original clean background of the image. Coupling the noisy data output from ADANI with the corresponding ground-truth, a denoising CNN is then trained in a fully-supervised manner. Experiments show that the noisy data produced by ADANI are visually and statistically similar to real ones so that the denoising CNN in our method is competitive to other networks trained with external paired data.



### Inter-layer Transition in Neural Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/2011.14525v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.14525v1)
- **Published**: 2020-11-30 03:33:52+00:00
- **Updated**: 2020-11-30 03:33:52+00:00
- **Authors**: Benteng Ma, Jing Zhang, Yong Xia, Dacheng Tao
- **Comment**: None
- **Journal**: None
- **Summary**: Differential Neural Architecture Search (NAS) methods represent the network architecture as a repetitive proxy directed acyclic graph (DAG) and optimize the network weights and architecture weights alternatively in a differential manner. However, existing methods model the architecture weights on each edge (i.e., a layer in the network) as statistically independent variables, ignoring the dependency between edges in DAG induced by their directed topological connections. In this paper, we make the first attempt to investigate such dependency by proposing a novel Inter-layer Transition NAS method. It casts the architecture optimization into a sequential decision process where the dependency between the architecture weights of connected edges is explicitly modeled. Specifically, edges are divided into inner and outer groups according to whether or not their predecessor edges are in the same cell. While the architecture weights of outer edges are optimized independently, those of inner edges are derived sequentially based on the architecture weights of their predecessor edges and the learnable transition matrices in an attentive probability transition manner. Experiments on five benchmarks confirm the value of modeling inter-layer dependency and demonstrate the proposed method outperforms state-of-the-art methods.



### Heuristic Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2011.14540v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.14540v1)
- **Published**: 2020-11-30 04:21:35+00:00
- **Updated**: 2020-11-30 04:21:35+00:00
- **Authors**: Shuhao Cui, Xuan Jin, Shuhui Wang, Yuan He, Qingming Huang
- **Comment**: None
- **Journal**: NeurIPS 2020
- **Summary**: In visual domain adaptation (DA), separating the domain-specific characteristics from the domain-invariant representations is an ill-posed problem. Existing methods apply different kinds of priors or directly minimize the domain discrepancy to address this problem, which lack flexibility in handling real-world situations. Another research pipeline expresses the domain-specific information as a gradual transferring process, which tends to be suboptimal in accurately removing the domain-specific properties. In this paper, we address the modeling of domain-invariant and domain-specific information from the heuristic search perspective. We identify the characteristics in the existing representations that lead to larger domain discrepancy as the heuristic representations. With the guidance of heuristic representations, we formulate a principled framework of Heuristic Domain Adaptation (HDA) with well-founded theoretical guarantees. To perform HDA, the cosine similarity scores and independence measurements between domain-invariant and domain-specific representations are cast into the constraints at the initial and final states during the learning procedure. Similar to the final condition of heuristic search, we further derive a constraint enforcing the final range of heuristic network output to be small. Accordingly, we propose Heuristic Domain Adaptation Network (HDAN), which explicitly learns the domain-invariant and domain-specific representations with the above mentioned constraints. Extensive experiments show that HDAN has exceeded state-of-the-art on unsupervised DA, multi-source DA and semi-supervised DA. The code is available at https://github.com/cuishuhao/HDA.



### Learnable Motion Coherence for Correspondence Pruning
- **Arxiv ID**: http://arxiv.org/abs/2011.14563v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.14563v1)
- **Published**: 2020-11-30 05:57:25+00:00
- **Updated**: 2020-11-30 05:57:25+00:00
- **Authors**: Yuan Liu, Lingjie Liu, Cheng Lin, Zhen Dong, Wenping Wang
- **Comment**: 10 pages, 7 figures, project page:
  https://liuyuan-pal.github.io/LMCNet/
- **Journal**: None
- **Summary**: Motion coherence is an important clue for distinguishing true correspondences from false ones. Modeling motion coherence on sparse putative correspondences is challenging due to their sparsity and uneven distributions. Existing works on motion coherence are sensitive to parameter settings and have difficulty in dealing with complex motion patterns. In this paper, we introduce a network called Laplacian Motion Coherence Network (LMCNet) to learn motion coherence property for correspondence pruning. We propose a novel formulation of fitting coherent motions with a smooth function on a graph of correspondences and show that this formulation allows a closed-form solution by graph Laplacian. This closed-form solution enables us to design a differentiable layer in a learning framework to capture global motion coherence from putative correspondences. The global motion coherence is further combined with local coherence extracted by another local layer to robustly detect inlier correspondences. Experiments demonstrate that LMCNet has superior performances to the state of the art in relative camera pose estimation and correspondences pruning of dynamic scenes.



### Deep Implicit Templates for 3D Shape Representation
- **Arxiv ID**: http://arxiv.org/abs/2011.14565v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.14565v2)
- **Published**: 2020-11-30 06:01:49+00:00
- **Updated**: 2021-05-13 09:22:32+00:00
- **Authors**: Zerong Zheng, Tao Yu, Qionghai Dai, Yebin Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Deep implicit functions (DIFs), as a kind of 3D shape representation, are becoming more and more popular in the 3D vision community due to their compactness and strong representation power. However, unlike polygon mesh-based templates, it remains a challenge to reason dense correspondences or other semantic relationships across shapes represented by DIFs, which limits its applications in texture transfer, shape analysis and so on. To overcome this limitation and also make DIFs more interpretable, we propose Deep Implicit Templates, a new 3D shape representation that supports explicit correspondence reasoning in deep implicit representations. Our key idea is to formulate DIFs as conditional deformations of a template implicit function. To this end, we propose Spatial Warping LSTM, which decomposes the conditional spatial transformation into multiple affine transformations and guarantees generalization capability. Moreover, the training loss is carefully designed in order to achieve high reconstruction accuracy while learning a plausible template with accurate correspondences in an unsupervised manner. Experiments show that our method can not only learn a common implicit template for a collection of shapes, but also establish dense correspondences across all the shapes simultaneously without any supervision.



### DUT: Learning Video Stabilization by Simply Watching Unstable Videos
- **Arxiv ID**: http://arxiv.org/abs/2011.14574v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.14574v3)
- **Published**: 2020-11-30 06:48:20+00:00
- **Updated**: 2022-06-09 08:30:07+00:00
- **Authors**: Yufei Xu, Jing Zhang, Stephen J. Maybank, Dacheng Tao
- **Comment**: None
- **Journal**: None
- **Summary**: Previous deep learning-based video stabilizers require a large scale of paired unstable and stable videos for training, which are difficult to collect. Traditional trajectory-based stabilizers, on the other hand, divide the task into several sub-tasks and tackle them subsequently, which are fragile in textureless and occluded regions regarding the usage of hand-crafted features. In this paper, we attempt to tackle the video stabilization problem in a deep unsupervised learning manner, which borrows the divide-and-conquer idea from traditional stabilizers while leveraging the representation power of DNNs to handle the challenges in real-world scenarios. Technically, DUT is composed of a trajectory estimation stage and a trajectory smoothing stage. In the trajectory estimation stage, we first estimate the motion of keypoints, initialize and refine the motion of grids via a novel multi-homography estimation strategy and a motion refinement network, respectively, and get the grid-based trajectories via temporal association. In the trajectory smoothing stage, we devise a novel network to predict dynamic smoothing kernels for trajectory smoothing, which can well adapt to trajectories with different dynamic patterns. We exploit the spatial and temporal coherence of keypoints and grid vertices to formulate the training objectives, resulting in an unsupervised training scheme. Experiment results on public benchmarks show that DUT outperforms state-of-the-art methods both qualitatively and quantitatively. The source code is available at https://github.com/Annbless/DUTCode.



### Where Should We Begin? A Low-Level Exploration of Weight Initialization Impact on Quantized Behaviour of Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2011.14578v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.14578v1)
- **Published**: 2020-11-30 06:54:28+00:00
- **Updated**: 2020-11-30 06:54:28+00:00
- **Authors**: Stone Yun, Alexander Wong
- **Comment**: Accepted for publication at the 6th Annual Conference on Computer
  Vision and Intelligent Systems (CVIS2020)
- **Journal**: None
- **Summary**: With the proliferation of deep convolutional neural network (CNN) algorithms for mobile processing, limited precision quantization has become an essential tool for CNN efficiency. Consequently, various works have sought to design fixed precision quantization algorithms and quantization-focused optimization techniques that minimize quantization induced performance degradation. However, there is little concrete understanding of how various CNN design decisions/best practices affect quantized inference behaviour. Weight initialization strategies are often associated with solving issues such as vanishing/exploding gradients but an often-overlooked aspect is their impact on the final trained distributions of each layer. We present an in-depth, fine-grained ablation study of the effect of different weights initializations on the final distributions of weights and activations of different CNN architectures. The fine-grained, layerwise analysis enables us to gain deep insights on how initial weights distributions will affect final accuracy and quantized behaviour. To our best knowledge, we are the first to perform such a low-level, in-depth quantitative analysis of weights initialization and its effect on quantized behaviour.



### End-to-End 3D Point Cloud Learning for Registration Task Using Virtual Correspondences
- **Arxiv ID**: http://arxiv.org/abs/2011.14579v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2011.14579v2)
- **Published**: 2020-11-30 06:55:05+00:00
- **Updated**: 2021-06-18 02:06:59+00:00
- **Authors**: Zhijian Qiao, Huanshu Wei, Zhe Liu, Chuanzhe Suo, Hesheng Wang
- **Comment**: Accepted to IROS 2020
- **Journal**: None
- **Summary**: 3D Point cloud registration is still a very challenging topic due to the difficulty in finding the rigid transformation between two point clouds with partial correspondences, and it's even harder in the absence of any initial estimation information. In this paper, we present an end-to-end deep-learning based approach to resolve the point cloud registration problem. Firstly, the revised LPD-Net is introduced to extract features and aggregate them with the graph network. Secondly, the self-attention mechanism is utilized to enhance the structure information in the point cloud and the cross-attention mechanism is designed to enhance the corresponding information between the two input point clouds. Based on which, the virtual corresponding points can be generated by a soft pointer based method, and finally, the point cloud registration problem can be solved by implementing the SVD method. Comparison results in ModelNet40 dataset validate that the proposed approach reaches the state-of-the-art in point cloud registration tasks and experiment resutls in KITTI dataset validate the effectiveness of the proposed approach in real applications.Our source code is available at \url{https://github.com/qiaozhijian/VCR-Net.git}



### ScaleNAS: One-Shot Learning of Scale-Aware Representations for Visual Recognition
- **Arxiv ID**: http://arxiv.org/abs/2011.14584v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2011.14584v1)
- **Published**: 2020-11-30 07:11:11+00:00
- **Updated**: 2020-11-30 07:11:11+00:00
- **Authors**: Hsin-Pai Cheng, Feng Liang, Meng Li, Bowen Cheng, Feng Yan, Hai Li, Vikas Chandra, Yiran Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Scale variance among different sizes of body parts and objects is a challenging problem for visual recognition tasks. Existing works usually design dedicated backbone or apply Neural architecture Search(NAS) for each task to tackle this challenge. However, existing works impose significant limitations on the design or search space. To solve these problems, we present ScaleNAS, a one-shot learning method for exploring scale-aware representations. ScaleNAS solves multiple tasks at a time by searching multi-scale feature aggregation. ScaleNAS adopts a flexible search space that allows an arbitrary number of blocks and cross-scale feature fusions. To cope with the high search cost incurred by the flexible space, ScaleNAS employs one-shot learning for multi-scale supernet driven by grouped sampling and evolutionary search. Without further retraining, ScaleNet can be directly deployed for different visual recognition tasks with superior performance. We use ScaleNAS to create high-resolution models for two different tasks, ScaleNet-P for human pose estimation and ScaleNet-S for semantic segmentation. ScaleNet-P and ScaleNet-S outperform existing manually crafted and NAS-based methods in both tasks. When applying ScaleNet-P to bottom-up human pose estimation, it surpasses the state-of-the-art HigherHRNet. In particular, ScaleNet-P4 achieves 71.6% AP on COCO test-dev, achieving new state-of-the-art result.



### Just One Moment: Structural Vulnerability of Deep Action Recognition against One Frame Attack
- **Arxiv ID**: http://arxiv.org/abs/2011.14585v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.14585v2)
- **Published**: 2020-11-30 07:11:56+00:00
- **Updated**: 2021-08-17 10:48:04+00:00
- **Authors**: Jaehui Hwang, Jun-Hyuk Kim, Jun-Ho Choi, Jong-Seok Lee
- **Comment**: Accepted by ICCV 2021
- **Journal**: None
- **Summary**: The video-based action recognition task has been extensively studied in recent years. In this paper, we study the structural vulnerability of deep learning-based action recognition models against the adversarial attack using the one frame attack that adds an inconspicuous perturbation to only a single frame of a given video clip. Our analysis shows that the models are highly vulnerable against the one frame attack due to their structural properties. Experiments demonstrate high fooling rates and inconspicuous characteristics of the attack. Furthermore, we show that strong universal one frame perturbations can be obtained under various scenarios. Our work raises the serious issue of adversarial vulnerability of the state-of-the-art action recognition models in various perspectives.



### FactorizeNet: Progressive Depth Factorization for Efficient Network Architecture Exploration Under Quantization Constraints
- **Arxiv ID**: http://arxiv.org/abs/2011.14586v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.14586v1)
- **Published**: 2020-11-30 07:12:26+00:00
- **Updated**: 2020-11-30 07:12:26+00:00
- **Authors**: Stone Yun, Alexander Wong
- **Comment**: Accepted for Publication at the 2020 Workshop on Energy Efficient
  Machine Learning and Cognitive Computing (EMC2 2020)
- **Journal**: None
- **Summary**: Depth factorization and quantization have emerged as two of the principal strategies for designing efficient deep convolutional neural network (CNN) architectures tailored for low-power inference on the edge. However, there is still little detailed understanding of how different depth factorization choices affect the final, trained distributions of each layer in a CNN, particularly in the situation of quantized weights and activations. In this study, we introduce a progressive depth factorization strategy for efficient CNN architecture exploration under quantization constraints. By algorithmically increasing the granularity of depth factorization in a progressive manner, the proposed strategy enables a fine-grained, low-level analysis of layer-wise distributions. Thus enabling the gain of in-depth, layer-level insights on efficiency-accuracy tradeoffs under fixed-precision quantization. Such a progressive depth factorization strategy also enables efficient identification of the optimal depth-factorized macroarchitecture design (which we will refer to here as FactorizeNet) based on the desired efficiency-accuracy requirements.



### Monocular 3D Object Detection with Sequential Feature Association and Depth Hint Augmentation
- **Arxiv ID**: http://arxiv.org/abs/2011.14589v4
- **DOI**: 10.1109/TIV.2022.3143954
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.14589v4)
- **Published**: 2020-11-30 07:19:14+00:00
- **Updated**: 2022-01-23 03:09:25+00:00
- **Authors**: Tianze Gao, Huihui Pan, Huijun Gao
- **Comment**: 11 pages, 11 figures. This paper has been accepted by IEEE
  Transactions of Intelligent Vehicles. Digital Object Identifier:
  10.1109/TIV.2022.3143954
- **Journal**: None
- **Summary**: Monocular 3D object detection, with the aim of predicting the geometric properties of on-road objects, is a promising research topic for the intelligent perception systems of autonomous driving. Most state-of-the-art methods follow a keypoint-based paradigm, where the keypoints of objects are predicted and employed as the basis for regressing the other geometric properties. In this work, a unified network named as FADNet is presented to address the task of monocular 3D object detection. In contrast to previous keypoint-based methods, we propose to divide the output modalities into different groups according to the estimation difficulty of object properties. Different groups are treated differently and sequentially associated by a convolutional Gated Recurrent Unit. Another contribution of this work is the strategy of depth hint augmentation. To provide characterized depth patterns as hints for depth estimation, a dedicated depth hint module is designed to generate row-wise features named as depth hints, which are explicitly supervised in a bin-wise manner. The contributions of this work are validated by conducting experiments and ablation study on the KITTI benchmark. Without utilizing depth priors, post optimization, or other refinement modules, our network performs competitively against state-of-the-art methods while maintaining a decent running speed.



### Incremental Learning via Rate Reduction
- **Arxiv ID**: http://arxiv.org/abs/2011.14593v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.14593v1)
- **Published**: 2020-11-30 07:23:55+00:00
- **Updated**: 2020-11-30 07:23:55+00:00
- **Authors**: Ziyang Wu, Christina Baek, Chong You, Yi Ma
- **Comment**: None
- **Journal**: None
- **Summary**: Current deep learning architectures suffer from catastrophic forgetting, a failure to retain knowledge of previously learned classes when incrementally trained on new classes. The fundamental roadblock faced by deep learning methods is that deep learning models are optimized as "black boxes," making it difficult to properly adjust the model parameters to preserve knowledge about previously seen data. To overcome the problem of catastrophic forgetting, we propose utilizing an alternative "white box" architecture derived from the principle of rate reduction, where each layer of the network is explicitly computed without back propagation. Under this paradigm, we demonstrate that, given a pre-trained network and new data classes, our approach can provably construct a new network that emulates joint training with all past and new classes. Finally, our experiments show that our proposed learning algorithm observes significantly less decay in classification performance, outperforming state of the art methods on MNIST and CIFAR-10 by a large margin and justifying the use of "white box" algorithms for incremental learning even for sufficiently complex image data.



### A CRF-based Framework for Tracklet Inactivation in Online Multi-Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/2011.14594v2
- **DOI**: 10.1109/TMM.2021.3062489
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.14594v2)
- **Published**: 2020-11-30 07:28:52+00:00
- **Updated**: 2021-04-23 07:49:35+00:00
- **Authors**: Tianze Gao, Huihui Pan, Zidong Wang, Huijun Gao
- **Comment**: 13 pages, 8 figures. This work has been accepted by IEEE Transactions
  on Multimedia. (DOI: 10.1109/TMM.2021.3062489)
- **Journal**: None
- **Summary**: Online multi-object tracking (MOT) is an active research topic in the domain of computer vision. Although many previously proposed algorithms have exhibited decent results, the issue of tracklet inactivation has not been sufficiently studied. Simple strategies such as using a fixed threshold on classification scores are adopted, yielding undesirable tracking mistakes and limiting the overall performance. In this paper, a conditional random field (CRF) based framework is put forward to tackle the tracklet inactivation issue in online MOT problems. A discrete CRF which exploits the intra-frame relationship between tracking hypotheses is developed to improve the robustness of tracklet inactivation. Separate sets of feature functions are designed for the unary and binary terms in the CRF, which take into account various tracking challenges in practical scenarios. To handle the problem of varying CRF nodes in the MOT context, two strategies named as hypothesis filtering and dummy nodes are employed. In the proposed framework, the inference stage is conducted by using the loopy belief propagation algorithm, and the CRF parameters are determined by utilizing the maximum likelihood estimation method followed by slight manual adjustment. Experimental results show that the tracker combined with the CRF-based framework outperforms the baseline on the MOT16 and MOT17 benchmarks. The extensibility of the proposed framework is further validated by an extensive experiment.



### Video Self-Stitching Graph Network for Temporal Action Localization
- **Arxiv ID**: http://arxiv.org/abs/2011.14598v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.14598v3)
- **Published**: 2020-11-30 07:44:52+00:00
- **Updated**: 2021-03-30 05:08:55+00:00
- **Authors**: Chen Zhao, Ali Thabet, Bernard Ghanem
- **Comment**: None
- **Journal**: None
- **Summary**: Temporal action localization (TAL) in videos is a challenging task, especially due to the large variation in action temporal scales. Short actions usually occupy the major proportion in the data, but have the lowest performance with all current methods. In this paper, we confront the challenge of short actions and propose a multi-level cross-scale solution dubbed as video self-stitching graph network (VSGN). We have two key components in VSGN: video self-stitching (VSS) and cross-scale graph pyramid network (xGPN). In VSS, we focus on a short period of a video and magnify it along the temporal dimension to obtain a larger scale. We stitch the original clip and its magnified counterpart in one input sequence to take advantage of the complementary properties of both scales. The xGPN component further exploits the cross-scale correlations by a pyramid of cross-scale graph networks, each containing a hybrid module to aggregate features from across scales as well as within the same scale. Our VSGN not only enhances the feature representations, but also generates more positive anchors for short actions and more short training samples. Experiments demonstrate that VSGN obviously improves the localization performance of short actions as well as achieving the state-of-the-art overall performance on THUMOS-14 and ActivityNet-v1.3.



### REaL: Real-time Face Detection and Recognition Using Euclidean Space and Likelihood Estimation
- **Arxiv ID**: http://arxiv.org/abs/2011.14603v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.14603v1)
- **Published**: 2020-11-30 08:03:04+00:00
- **Updated**: 2020-11-30 08:03:04+00:00
- **Authors**: Sandesh Ramesh, Manoj Kumar M V, K Aditya Shastry
- **Comment**: International Journal of System Assurance Engineering and Management
- **Journal**: None
- **Summary**: Detecting and recognizing faces accurately has always been a challenge. Differentiating facial features, training images, and producing quick results require a lot of computation. The REaL system we have proposed in this paper discusses its functioning and ways in which computations can be carried out in a short period. REaL experiments are carried out on live images and the recognition rates are promising. The system is also successful in removing non-human objects from its calculations. The system uses a local database to store captured images and feeds the neural network frequently. The captured images are cropped automatically to remove unwanted noise. The system calculates the Euler angles and the probability of whether the face is smiling, has its left eye, and right eyes open or not.



### Zero-Shot Calibration of Fisheye Cameras
- **Arxiv ID**: http://arxiv.org/abs/2011.14607v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2011.14607v1)
- **Published**: 2020-11-30 08:10:24+00:00
- **Updated**: 2020-11-30 08:10:24+00:00
- **Authors**: Jae-Yeong Lee
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present a novel zero-shot camera calibration method that estimates camera parameters with no calibration image. It is common sense that we need at least one or more pattern images for camera calibration. However, the proposed method estimates camera parameters from the horizontal and vertical field of view information of the camera without any image acquisition. The proposed method is particularly useful for wide-angle or fisheye cameras that have large image distortion. Image distortion is modeled in the way fisheye lenses are designed and estimated based on the square pixel assumption of the image sensors. The calibration accuracy of the proposed method is evaluated on eight different commercial cameras qualitatively and quantitatively, and compared with conventional calibration methods. The experimental results show that the calibration accuracy of the zero-shot method is comparable to conventional full calibration results. The method can be used as a practical alternative in real applications where individual calibration is difficult or impractical, and in most field applications where calibration accuracy is less critical. Moreover, the estimated camera parameters by the method can also be used to provide proper initialization of any existing calibration methods, making them to converge more stably and avoid local minima.



### SIR: Self-supervised Image Rectification via Seeing the Same Scene from Multiple Different Lenses
- **Arxiv ID**: http://arxiv.org/abs/2011.14611v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2011.14611v2)
- **Published**: 2020-11-30 08:23:25+00:00
- **Updated**: 2021-06-18 07:26:29+00:00
- **Authors**: Jinlong Fan, Jing Zhang, Dacheng Tao
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning has demonstrated its power in image rectification by leveraging the representation capacity of deep neural networks via supervised training based on a large-scale synthetic dataset. However, the model may overfit the synthetic images and generalize not well on real-world fisheye images due to the limited universality of a specific distortion model and the lack of explicitly modeling the distortion and rectification process. In this paper, we propose a novel self-supervised image rectification (SIR) method based on an important insight that the rectified results of distorted images of a same scene from different lens should be the same. Specifically, we devise a new network architecture with a shared encoder and several prediction heads, each of which predicts the distortion parameter of a specific distortion model. We further leverage a differentiable warping module to generate the rectified images and re-distorted images from the distortion parameters and exploit the intra- and inter-model consistency between them during training, thereby leading to a self-supervised learning scheme without the need for ground-truth distortion parameters or normal images. Experiments on synthetic dataset and real-world fisheye images demonstrate that our method achieves comparable or even better performance than the supervised baseline method and representative state-of-the-art methods. Self-supervised learning also improves the universality of distortion models while keeping their self-consistency.



### DeepCloth: Neural Garment Representation for Shape and Style Editing
- **Arxiv ID**: http://arxiv.org/abs/2011.14619v2
- **DOI**: 10.1109/TPAMI.2022.3168569
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.14619v2)
- **Published**: 2020-11-30 08:42:38+00:00
- **Updated**: 2022-05-03 14:13:57+00:00
- **Authors**: Zhaoqi Su, Tao Yu, Yangang Wang, Yebin Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Garment representation, editing and animation are challenging topics in the area of computer vision and graphics. It remains difficult for existing garment representations to achieve smooth and plausible transitions between different shapes and topologies. In this work, we introduce, DeepCloth, a unified framework for garment representation, reconstruction, animation and editing. Our unified framework contains 3 components: First, we represent the garment geometry with a "topology-aware UV-position map", which allows for the unified description of various garments with different shapes and topologies by introducing an additional topology-aware UV-mask for the UV-position map. Second, to further enable garment reconstruction and editing, we contribute a method to embed the UV-based representations into a continuous feature space, which enables garment shape reconstruction and editing by optimization and control in the latent space, respectively. Finally, we propose a garment animation method by unifying our neural garment representation with body shape and pose, which achieves plausible garment animation results leveraging the dynamic information encoded by our shape and style representation, even under drastic garment editing operations. To conclude, with DeepCloth, we move a step forward in establishing a more flexible and general 3D garment digitization framework. Experiments demonstrate that our method can achieve state-of-the-art garment representation performance compared with previous methods.



### SAR Image Despeckling Based on Convolutional Denoising Autoencoder
- **Arxiv ID**: http://arxiv.org/abs/2011.14627v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.14627v1)
- **Published**: 2020-11-30 09:02:25+00:00
- **Updated**: 2020-11-30 09:02:25+00:00
- **Authors**: Qianqian Zhang, Ruizhi Sun
- **Comment**: None
- **Journal**: None
- **Summary**: In Synthetic Aperture Radar (SAR) imaging, despeckling is very important for image analysis,whereas speckle is known as a kind of multiplicative noise caused by the coherent imaging system. During the past three decades, various algorithms have been proposed to denoise the SAR image. Generally, the BM3D is considered as the state of art technique to despeckle the speckle noise with excellent performance. More recently, deep learning make a success in image denoising and achieved a improvement over conventional method where large train dataset is required. Unlike most of the images SAR image despeckling approach, the proposed approach learns the speckle from corrupted images directly. In this paper, the limited scale of dataset make a efficient exploration by using convolutioal denoising autoencoder (C-DAE) to reconstruct the speckle-free SAR images. Batch normalization strategy is integrated with C- DAE to speed up the train time. Moreover, we compute image quality in standard metrics, PSNR and SSIM. It is revealed that our approach perform well than some others.



### Cross-MPI: Cross-scale Stereo for Image Super-Resolution using Multiplane Images
- **Arxiv ID**: http://arxiv.org/abs/2011.14631v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.14631v2)
- **Published**: 2020-11-30 09:14:07+00:00
- **Updated**: 2021-03-29 13:58:19+00:00
- **Authors**: Yuemei Zhou, Gaochang Wu, Ying Fu, Kun Li, Yebin Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Various combinations of cameras enrich computational photography, among which reference-based superresolution (RefSR) plays a critical role in multiscale imaging systems. However, existing RefSR approaches fail to accomplish high-fidelity super-resolution under a large resolution gap, e.g., 8x upscaling, due to the lower consideration of the underlying scene structure. In this paper, we aim to solve the RefSR problem in actual multiscale camera systems inspired by multiplane image (MPI) representation. Specifically, we propose Cross-MPI, an end-to-end RefSR network composed of a novel plane-aware attention-based MPI mechanism, a multiscale guided upsampling module as well as a super-resolution (SR) synthesis and fusion module. Instead of using a direct and exhaustive matching between the cross-scale stereo, the proposed plane-aware attention mechanism fully utilizes the concealed scene structure for efficient attention-based correspondence searching. Further combined with a gentle coarse-to-fine guided upsampling strategy, the proposed Cross-MPI can achieve a robust and accurate detail transmission. Experimental results on both digitally synthesized and optical zoom cross-scale data show that the Cross-MPI framework can achieve superior performance against the existing RefSR methods and is a real fit for actual multiscale camera systems even with large-scale differences.



### Vehicle Reconstruction and Texture Estimation Using Deep Implicit Semantic Template Mapping
- **Arxiv ID**: http://arxiv.org/abs/2011.14642v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.14642v2)
- **Published**: 2020-11-30 09:27:10+00:00
- **Updated**: 2021-03-29 05:32:08+00:00
- **Authors**: Xiaochen Zhao, Zerong Zheng, Chaonan Ji, Zhenyi Liu, Siyou Lin, Tao Yu, Jinli Suo, Yebin Liu
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce VERTEX, an effective solution to recover 3D shape and intrinsic texture of vehicles from uncalibrated monocular input in real-world street environments. To fully utilize the template prior of vehicles, we propose a novel geometry and texture joint representation, based on implicit semantic template mapping. Compared to existing representations which infer 3D texture distribution, our method explicitly constrains the texture distribution on the 2D surface of the template as well as avoids limitations of fixed resolution and topology. Moreover, by fusing the global and local features together, our approach is capable to generate consistent and detailed texture in both visible and invisible areas. We also contribute a new synthetic dataset containing 830 elaborate textured car models labeled with sparse key points and rendered using Physically Based Rendering (PBRT) system with measured HDRI skymaps to obtain highly realistic images. Experiments demonstrate the superior performance of our approach on both testing dataset and in-the-wild images. Furthermore, the presented technique enables additional applications such as 3D vehicle texture transfer and material identification.



### Towards Better Accuracy-efficiency Trade-offs: Divide and Co-training
- **Arxiv ID**: http://arxiv.org/abs/2011.14660v4
- **DOI**: 10.1109/TIP.2022.3201602
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.14660v4)
- **Published**: 2020-11-30 10:03:34+00:00
- **Updated**: 2022-09-06 03:15:23+00:00
- **Authors**: Shuai Zhao, Liguang Zhou, Wenxiao Wang, Deng Cai, Tin Lun Lam, Yangsheng Xu
- **Comment**: accepted by T-IP 2022, code is at
  https://github.com/FreeformRobotics/Divide-and-Co-training
- **Journal**: None
- **Summary**: The width of a neural network matters since increasing the width will necessarily increase the model capacity. However, the performance of a network does not improve linearly with the width and soon gets saturated. In this case, we argue that increasing the number of networks (ensemble) can achieve better accuracy-efficiency trade-offs than purely increasing the width. To prove it, one large network is divided into several small ones regarding its parameters and regularization components. Each of these small networks has a fraction of the original one's parameters. We then train these small networks together and make them see various views of the same data to increase their diversity. During this co-training process, networks can also learn from each other. As a result, small networks can achieve better ensemble performance than the large one with few or no extra parameters or FLOPs, \ie, achieving better accuracy-efficiency trade-offs. Small networks can also achieve faster inference speed than the large one by concurrent running. All of the above shows that the number of networks is a new dimension of model scaling. We validate our argument with 8 different neural architectures on common benchmarks through extensive experiments. The code is available at \url{https://github.com/FreeformRobotics/Divide-and-Co-training}.



### TransMIA: Membership Inference Attacks Using Transfer Shadow Training
- **Arxiv ID**: http://arxiv.org/abs/2011.14661v3
- **DOI**: 10.1109/IJCNN52387.2021.9534207
- **Categories**: **cs.CR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.14661v3)
- **Published**: 2020-11-30 10:03:43+00:00
- **Updated**: 2021-04-23 14:50:44+00:00
- **Authors**: Seira Hidano, Takao Murakami, Yusuke Kawamoto
- **Comment**: IJCNN 2021 conference paper
- **Journal**: Proceedings of the 2021 International Joint Conference on Neural
  Networks (IJCNN 2021), pp.1-10, 2021
- **Summary**: Transfer learning has been widely studied and gained increasing popularity to improve the accuracy of machine learning models by transferring some knowledge acquired in different training. However, no prior work has pointed out that transfer learning can strengthen privacy attacks on machine learning models. In this paper, we propose TransMIA (Transfer learning-based Membership Inference Attacks), which use transfer learning to perform membership inference attacks on the source model when the adversary is able to access the parameters of the transferred model. In particular, we propose a transfer shadow training technique, where an adversary employs the parameters of the transferred model to construct shadow models, to significantly improve the performance of membership inference when a limited amount of shadow training data is available to the adversary. We evaluate our attacks using two real datasets, and show that our attacks outperform the state-of-the-art that does not use our transfer shadow training technique. We also compare four combinations of the learning-based/entropy-based approach and the fine-tuning/freezing approach, all of which employ our transfer shadow training technique. Then we examine the performance of these four approaches based on the distributions of confidence values, and discuss possible countermeasures against our attacks.



### Revisiting Unsupervised Meta-Learning via the Characteristics of Few-Shot Tasks
- **Arxiv ID**: http://arxiv.org/abs/2011.14663v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.14663v3)
- **Published**: 2020-11-30 10:08:35+00:00
- **Updated**: 2022-06-09 13:42:03+00:00
- **Authors**: Han-Jia Ye, Lu Han, De-Chuan Zhan
- **Comment**: Accepted to IEEE TPAMI. Code is available at
  https://github.com/hanlu-nju/revisiting-UML
- **Journal**: None
- **Summary**: Meta-learning has become a practical approach towards few-shot image classification, where "a strategy to learn a classifier" is meta-learned on labeled base classes and can be applied to tasks with novel classes. We remove the requirement of base class labels and learn generalizable embeddings via Unsupervised Meta-Learning (UML). Specifically, episodes of tasks are constructed with data augmentations from unlabeled base classes during meta-training, and we apply embedding-based classifiers to novel tasks with labeled few-shot examples during meta-test. We observe two elements play important roles in UML, i.e., the way to sample tasks and measure similarities between instances. Thus we obtain a strong baseline with two simple modifications -- a sufficient sampling strategy constructing multiple tasks per episode efficiently together with a semi-normalized similarity. We then take advantage of the characteristics of tasks from two directions to get further improvements. First, synthesized confusing instances are incorporated to help extract more discriminative embeddings. Second, we utilize an additional task-specific embedding transformation as an auxiliary component during meta-training to promote the generalization ability of the pre-adapted embeddings. Experiments on few-shot learning benchmarks verify that our approaches outperform previous UML methods and achieve comparable or even better performance than its supervised variants.



### Why Convolutional Networks Learn Oriented Bandpass Filters: Theory and Empirical Support
- **Arxiv ID**: http://arxiv.org/abs/2011.14665v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.14665v1)
- **Published**: 2020-11-30 10:10:44+00:00
- **Updated**: 2020-11-30 10:10:44+00:00
- **Authors**: Isma Hadji, Richard P. Wildes
- **Comment**: None
- **Journal**: None
- **Summary**: It has been repeatedly observed that convolutional architectures when applied to image understanding tasks learn oriented bandpass filters. A standard explanation of this result is that these filters reflect the structure of the images that they have been exposed to during training: Natural images typically are locally composed of oriented contours at various scales and oriented bandpass filters are matched to such structure. We offer an alternative explanation based not on the structure of images, but rather on the structure of convolutional architectures. In particular, complex exponentials are the eigenfunctions of convolution. These eigenfunctions are defined globally; however, convolutional architectures operate locally. To enforce locality, one can apply a windowing function to the eigenfunctions, which leads to oriented bandpass filters as the natural operators to be learned with convolutional architectures. From a representational point of view, these filters allow for a local systematic way to characterize and operate on an image or other signal. We offer empirical support for the hypothesis that convolutional networks learn such filters at all of their convolutional layers. While previous research has shown evidence of filters having oriented bandpass characteristics at early layers, ours appears to be the first study to document the predominance of such filter characteristics at all layers. Previous studies have missed this observation because they have concentrated on the cumulative compositional effects of filtering across layers, while we examine the filter characteristics that are present at each layer.



### AFD-Net: Adaptive Fully-Dual Network for Few-Shot Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2011.14667v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.14667v2)
- **Published**: 2020-11-30 10:21:32+00:00
- **Updated**: 2021-04-19 07:29:07+00:00
- **Authors**: Longyao Liu, Bo Ma, Yulin Zhang, Xin Yi, Haozhi Li
- **Comment**: 9 pages, 8 figures
- **Journal**: None
- **Summary**: Few-shot object detection (FSOD) aims at learning a detector that can fast adapt to previously unseen objects with scarce annotated examples, which is challenging and demanding. Existing methods solve this problem by performing subtasks of classification and localization utilizing a shared component (e.g., RoI head) in the detector, yet few of them take the distinct preferences of two subtasks towards feature embedding into consideration. In this paper, we carefully analyze the characteristics of FSOD, and present that a general few-shot detector should consider the explicit decomposition of two subtasks, as well as leveraging information from both of them to enhance feature representations. To the end, we propose a simple yet effective Adaptive Fully-Dual Network (AFD-Net). Specifically, we extend Faster R-CNN by introducing Dual Query Encoder and Dual Attention Generator for separate feature extraction, and Dual Aggregator for separate model reweighting. Spontaneously, separate state estimation is achieved by the R-CNN detector. Besides, for the acquisition of enhanced feature representations, we further introduce Adaptive Fusion Mechanism to adaptively perform feature fusion in different subtasks. Extensive experiments on PASCAL VOC and MS COCO in various settings show that, our method achieves new state-of-the-art performance by a large margin, demonstrating its effectiveness and generalization ability.



### Where to Explore Next? ExHistCNN for History-aware Autonomous 3D Exploration
- **Arxiv ID**: http://arxiv.org/abs/2011.14669v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2011.14669v1)
- **Published**: 2020-11-30 10:29:29+00:00
- **Updated**: 2020-11-30 10:29:29+00:00
- **Authors**: Yiming Wang, Alessio Del Bue
- **Comment**: published on European Conference on Computer Vision, 2020
- **Journal**: None
- **Summary**: In this work we address the problem of autonomous 3D exploration of an unknown indoor environment using a depth camera. We cast the problem as the estimation of the Next Best View (NBV) that maximises the coverage of the unknown area. We do this by re-formulating NBV estimation as a classification problem and we propose a novel learning-based metric that encodes both, the current 3D observation (a depth frame) and the history of the ongoing reconstruction. One of the major contributions of this work is about introducing a new representation for the 3D reconstruction history as an auxiliary utility map which is efficiently coupled with the current depth observation. With both pieces of information, we train a light-weight CNN, named ExHistCNN, that estimates the NBV as a set of directions towards which the depth sensor finds most unexplored areas. We perform extensive evaluation on both synthetic and real room scans demonstrating that the proposed ExHistCNN is able to approach the exploration performance of an oracle using the complete knowledge of the 3D environment.



### Meta Batch-Instance Normalization for Generalizable Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2011.14670v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.14670v2)
- **Published**: 2020-11-30 10:31:03+00:00
- **Updated**: 2021-03-29 17:38:27+00:00
- **Authors**: Seokeon Choi, Taekyung Kim, Minki Jeong, Hyoungseob Park, Changick Kim
- **Comment**: CVPR 2021
- **Journal**: None
- **Summary**: Although supervised person re-identification (Re-ID) methods have shown impressive performance, they suffer from a poor generalization capability on unseen domains. Therefore, generalizable Re-ID has recently attracted growing attention. Many existing methods have employed an instance normalization technique to reduce style variations, but the loss of discriminative information could not be avoided. In this paper, we propose a novel generalizable Re-ID framework, named Meta Batch-Instance Normalization (MetaBIN). Our main idea is to generalize normalization layers by simulating unsuccessful generalization scenarios beforehand in the meta-learning pipeline. To this end, we combine learnable batch-instance normalization layers with meta-learning and investigate the challenging cases caused by both batch and instance normalization layers. Moreover, we diversify the virtual simulations via our meta-train loss accompanied by a cyclic inner-updating manner to boost generalization capability. After all, the MetaBIN framework prevents our model from overfitting to the given source styles and improves the generalization capability to unseen domains without additional data augmentation or complicated network design. Extensive experimental results show that our model outperforms the state-of-the-art methods on the large-scale domain generalization Re-ID benchmark and the cross-domain Re-ID problem. The source code is available at: https://github.com/bismex/MetaBIN.



### HybrIK: A Hybrid Analytical-Neural Inverse Kinematics Solution for 3D Human Pose and Shape Estimation
- **Arxiv ID**: http://arxiv.org/abs/2011.14672v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.14672v4)
- **Published**: 2020-11-30 10:32:30+00:00
- **Updated**: 2022-04-27 04:49:00+00:00
- **Authors**: Jiefeng Li, Chao Xu, Zhicun Chen, Siyuan Bian, Lixin Yang, Cewu Lu
- **Comment**: CVPR 2021
- **Journal**: None
- **Summary**: Model-based 3D pose and shape estimation methods reconstruct a full 3D mesh for the human body by estimating several parameters. However, learning the abstract parameters is a highly non-linear process and suffers from image-model misalignment, leading to mediocre model performance. In contrast, 3D keypoint estimation methods combine deep CNN network with the volumetric representation to achieve pixel-level localization accuracy but may predict unrealistic body structure. In this paper, we address the above issues by bridging the gap between body mesh estimation and 3D keypoint estimation. We propose a novel hybrid inverse kinematics solution (HybrIK). HybrIK directly transforms accurate 3D joints to relative body-part rotations for 3D body mesh reconstruction, via the twist-and-swing decomposition. The swing rotation is analytically solved with 3D joints, and the twist rotation is derived from the visual cues through the neural network. We show that HybrIK preserves both the accuracy of 3D pose and the realistic body structure of the parametric human model, leading to a pixel-aligned 3D body mesh and a more accurate 3D pose than the pure 3D keypoint estimation methods. Without bells and whistles, the proposed method surpasses the state-of-the-art methods by a large margin on various 3D human pose and shape benchmarks. As an illustrative example, HybrIK outperforms all the previous methods by 13.2 mm MPJPE and 21.9 mm PVE on 3DPW dataset. Our code is available at https://github.com/Jeff-sjtu/HybrIK.



### CanonPose: Self-Supervised Monocular 3D Human Pose Estimation in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2011.14679v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.14679v1)
- **Published**: 2020-11-30 10:42:27+00:00
- **Updated**: 2020-11-30 10:42:27+00:00
- **Authors**: Bastian Wandt, Marco Rudolph, Petrissa Zell, Helge Rhodin, Bodo Rosenhahn
- **Comment**: None
- **Journal**: None
- **Summary**: Human pose estimation from single images is a challenging problem in computer vision that requires large amounts of labeled training data to be solved accurately. Unfortunately, for many human activities (\eg outdoor sports) such training data does not exist and is hard or even impossible to acquire with traditional motion capture systems. We propose a self-supervised approach that learns a single image 3D pose estimator from unlabeled multi-view data. To this end, we exploit multi-view consistency constraints to disentangle the observed 2D pose into the underlying 3D pose and camera rotation. In contrast to most existing methods, we do not require calibrated cameras and can therefore learn from moving cameras. Nevertheless, in the case of a static camera setup, we present an optional extension to include constant relative camera rotations over multiple views into our framework. Key to the success are new, unbiased reconstruction objectives that mix information across views and training samples. The proposed approach is evaluated on two benchmark datasets (Human3.6M and MPII-INF-3DHP) and on the in-the-wild SkiPose dataset.



### Adaptive Compact Attention For Few-shot Video-to-video Translation
- **Arxiv ID**: http://arxiv.org/abs/2011.14695v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.14695v1)
- **Published**: 2020-11-30 11:19:12+00:00
- **Updated**: 2020-11-30 11:19:12+00:00
- **Authors**: Risheng Huang, Li Shen, Xuan Wang, Cheng Lin, Hao-Zhi Huang
- **Comment**: Video available: https://youtu.be/1OCFbUrypKQ
- **Journal**: None
- **Summary**: This paper proposes an adaptive compact attention model for few-shot video-to-video translation. Existing works in this domain only use features from pixel-wise attention without considering the correlations among multiple reference images, which leads to heavy computation but limited performance. Therefore, we introduce a novel adaptive compact attention mechanism to efficiently extract contextual features jointly from multiple reference images, of which encoded view-dependent and motion-dependent information can significantly benefit the synthesis of realistic videos. Our core idea is to extract compact basis sets from all the reference images as higher-level representations. To further improve the reliability, in the inference phase, we also propose a novel method based on the Delaunay Triangulation algorithm to automatically select the resourceful references according to the input label. We extensively evaluate our method on a large-scale talking-head video dataset and a human dancing dataset; the experimental results show the superior performance of our method for producing photorealistic and temporally consistent videos, and considerable improvements over the state-of-the-art method.



### On Initial Pools for Deep Active Learning
- **Arxiv ID**: http://arxiv.org/abs/2011.14696v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.14696v2)
- **Published**: 2020-11-30 11:22:31+00:00
- **Updated**: 2021-07-14 11:14:09+00:00
- **Authors**: Akshay L Chandra, Sai Vikas Desai, Chaitanya Devaguptapu, Vineeth N Balasubramanian
- **Comment**: Accepted at NeurIPS 2020 Preregistration Workshop and included in
  PMLR v148. 19 pages, 9 figures
- **Journal**: Proceedings of Machine Learning Research. 148 (2021) 14-32
- **Summary**: Active Learning (AL) techniques aim to minimize the training data required to train a model for a given task. Pool-based AL techniques start with a small initial labeled pool and then iteratively pick batches of the most informative samples for labeling. Generally, the initial pool is sampled randomly and labeled to seed the AL iterations. While recent studies have focused on evaluating the robustness of various query functions in AL, little to no attention has been given to the design of the initial labeled pool for deep active learning. Given the recent successes of learning representations in self-supervised/unsupervised ways, we study if an intelligently sampled initial labeled pool can improve deep AL performance. We investigate the effect of intelligently sampled initial labeled pools, including the use of self-supervised and unsupervised strategies, on deep AL methods. The setup, hypotheses, methodology, and implementation details were evaluated by peer review before experiments were conducted. Experimental results could not conclusively prove that intelligently sampled initial pools are better for AL than random initial pools in the long run, although a Variational Autoencoder-based initial pool sampling strategy showed interesting trends that merit deeper investigation.



### Learning-based lossless compression of 3D point cloud geometry
- **Arxiv ID**: http://arxiv.org/abs/2011.14700v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.14700v2)
- **Published**: 2020-11-30 11:27:16+00:00
- **Updated**: 2021-04-20 09:29:28+00:00
- **Authors**: Dat Thanh Nguyen, Maurice Quach, Giuseppe Valenzise, Pierre Duhamel
- **Comment**: 5 pages, accepted paper at ICASSP 2021
- **Journal**: None
- **Summary**: This paper presents a learning-based, lossless compression method for static point cloud geometry, based on context-adaptive arithmetic coding. Unlike most existing methods working in the octree domain, our encoder operates in a hybrid mode, mixing octree and voxel-based coding. We adaptively partition the point cloud into multi-resolution voxel blocks according to the point cloud structure, and use octree to signal the partitioning. On the one hand, octree representation can eliminate the sparsity in the point cloud. On the other hand, in the voxel domain, convolutions can be naturally expressed, and geometric information (i.e., planes, surfaces, etc.) is explicitly processed by a neural network. Our context model benefits from these properties and learns a probability distribution of the voxels using a deep convolutional neural network with masked filters, called VoxelDNN. Experiments show that our method outperforms the state-of-the-art MPEG G-PCC standard with average rate savings of 28% on a diverse set of point clouds from the Microsoft Voxelized Upper Bodies (MVUB) and MPEG. The implementation is available at https://github.com/Weafre/VoxelDNN.



### CM-Net: Concentric Mask based Arbitrary-Shaped Text Detection
- **Arxiv ID**: http://arxiv.org/abs/2011.14714v9
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.14714v9)
- **Published**: 2020-11-30 11:54:14+00:00
- **Updated**: 2022-01-22 03:53:15+00:00
- **Authors**: Chuang Yang, Mulin Chen, Zhitong Xiong, Yuan Yuan, Qi Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Recently fast arbitrary-shaped text detection has become an attractive research topic. However, most existing methods are non-real-time, which may fall short in intelligent systems. Although a few real-time text methods are proposed, the detection accuracy is far behind non-real-time methods. To improve the detection accuracy and speed simultaneously, we propose a novel fast and accurate text detection framework, namely CM-Net, which is constructed based on a new text representation method and a multi-perspective feature (MPF) module. The former can fit arbitrary-shaped text contours by concentric mask (CM) in an efficient and robust way. The latter encourages the network to learn more CM-related discriminative features from multiple perspectives and brings no extra computational cost. Benefiting the advantages of CM and MPF, the proposed CM-Net only needs to predict one CM of the text instance to rebuild the text contour and achieves the best balance between detection accuracy and speed compared with previous works. Moreover, to ensure that multi-perspective features are effectively learned, the multi-factor constraints loss is proposed. Extensive experiments demonstrate the proposed CM is efficient and robust to fit arbitrary-shaped text instances, and also validate the effectiveness of MPF and constraints loss for discriminative text features recognition. Furthermore, experimental results show that the proposed CM-Net is superior to existing state-of-the-art (SOTA) real-time text detection methods in both detection speed and accuracy on MSRA-TD500, CTW1500, Total-Text, and ICDAR2015 datasets.



### Dual Geometric Graph Network (DG2N) -- Iterative network for deformable shape alignment
- **Arxiv ID**: http://arxiv.org/abs/2011.14723v2
- **DOI**: 10.1109/3DV53792.2021.00141
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.14723v2)
- **Published**: 2020-11-30 12:03:28+00:00
- **Updated**: 2021-03-27 06:23:32+00:00
- **Authors**: Dvir Ginzburg, Dan Raviv
- **Comment**: None
- **Journal**: None
- **Summary**: We provide a novel new approach for aligning geometric models using a dual graph structure where local features are mapping probabilities. Alignment of non-rigid structures is one of the most challenging computer vision tasks due to the high number of unknowns needed to model the correspondence. We have seen a leap forward using DNN models in template alignment and functional maps, but those methods fail for inter-class alignment where nonisometric deformations exist. Here we propose to rethink this task and use unrolling concepts on a dual graph structure - one for a forward map and one for a backward map, where the features are pulled back matching probabilities from the target into the source. We report state of the art results on stretchable domains alignment in a rapid and stable solution for meshes and cloud of points.



### Exploration of Whether Skylight Polarization Patterns Contain Three-dimensional Attitude Information
- **Arxiv ID**: http://arxiv.org/abs/2012.09154v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/2012.09154v1)
- **Published**: 2020-11-30 12:10:29+00:00
- **Updated**: 2020-11-30 12:10:29+00:00
- **Authors**: Huaju Liang, Hongyang Bai, Tong Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: Our previous work has demonstrated that Rayleigh model, which is widely used in polarized skylight navigation to describe skylight polarization patterns, does not contain three-dimensional (3D) attitude information [1]. However, it is still necessary to further explore whether the skylight polarization patterns contain 3D attitude information. So, in this paper, a social spider optimization (SSO) method is proposed to estimate three Euler angles, which considers the difference of each pixel among polarization images based on template matching (TM) to make full use of the captured polarization information. In addition, to explore this problem, we not only use angle of polarization (AOP) and degree of polarization (DOP) information, but also the light intensity (LI) information. So, a sky model is established, which combines Berry model and Hosek model to fully describe AOP, DOP, and LI information in the sky, and considers the influence of four neutral points, ground albedo, atmospheric turbidity, and wavelength. The results of simulation show that the SSO algorithm can estimate 3D attitude and the established sky model contains 3D attitude information. However, when there are measurement noise or model error, the accuracy of 3D attitude estimation drops significantly. Especially in field experiment, it is very difficult to estimate 3D attitude. Finally, the results are discussed in detail.



### SelectScale: Mining More Patterns from Images via Selective and Soft Dropout
- **Arxiv ID**: http://arxiv.org/abs/2012.15766v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.15766v1)
- **Published**: 2020-11-30 12:15:08+00:00
- **Updated**: 2020-11-30 12:15:08+00:00
- **Authors**: Zhengsu Chen, Jianwei Niu, Xuefeng Liu, Shaojie Tang
- **Comment**: arXiv admin note: text overlap with arXiv:1810.09849 by other authors
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) have achieved remarkable success in image recognition. Although the internal patterns of the input images are effectively learned by the CNNs, these patterns only constitute a small proportion of useful patterns contained in the input images. This can be attributed to the fact that the CNNs will stop learning if the learned patterns are enough to make a correct classification. Network regularization methods like dropout and SpatialDropout can ease this problem. During training, they randomly drop the features. These dropout methods, in essence, change the patterns learned by the networks, and in turn, forces the networks to learn other patterns to make the correct classification. However, the above methods have an important drawback. Randomly dropping features is generally inefficient and can introduce unnecessary noise. To tackle this problem, we propose SelectScale. Instead of randomly dropping units, SelectScale selects the important features in networks and adjusts them during training. Using SelectScale, we improve the performance of CNNs on CIFAR and ImageNet.



### DRDr II: Detecting the Severity Level of Diabetic Retinopathy Using Mask RCNN and Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/2011.14733v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.14733v1)
- **Published**: 2020-11-30 12:23:22+00:00
- **Updated**: 2020-11-30 12:23:22+00:00
- **Authors**: Farzan Shenavarmasouleh, Farid Ghareh Mohammadi, M. Hadi Amini, Hamid R. Arabnia
- **Comment**: The 2020 International Conference on Computational Science and
  Computational Intelligence (CSCI'2020)
- **Journal**: None
- **Summary**: DRDr II is a hybrid of machine learning and deep learning worlds. It builds on the successes of its antecedent, namely, DRDr, that was trained to detect, locate, and create segmentation masks for two types of lesions (exudates and microaneurysms) that can be found in the eyes of the Diabetic Retinopathy (DR) patients; and uses the entire model as a solid feature extractor in the core of its pipeline to detect the severity level of the DR cases. We employ a big dataset with over 35 thousand fundus images collected from around the globe and after 2 phases of preprocessing alongside feature extraction, we succeed in predicting the correct severity levels with over 92% accuracy.



### RfD-Net: Point Scene Understanding by Semantic Instance Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2011.14744v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.14744v1)
- **Published**: 2020-11-30 12:58:05+00:00
- **Updated**: 2020-11-30 12:58:05+00:00
- **Authors**: Yinyu Nie, Ji Hou, Xiaoguang Han, Matthias Nie√üner
- **Comment**: None
- **Journal**: None
- **Summary**: Semantic scene understanding from point clouds is particularly challenging as the points reflect only a sparse set of the underlying 3D geometry. Previous works often convert point cloud into regular grids (e.g. voxels or bird-eye view images), and resort to grid-based convolutions for scene understanding. In this work, we introduce RfD-Net that jointly detects and reconstructs dense object surfaces directly from raw point clouds. Instead of representing scenes with regular grids, our method leverages the sparsity of point cloud data and focuses on predicting shapes that are recognized with high objectness. With this design, we decouple the instance reconstruction into global object localization and local shape prediction. It not only eases the difficulty of learning 2-D manifold surfaces from sparse 3D space, the point clouds in each object proposal convey shape details that support implicit function learning to reconstruct any high-resolution surfaces. Our experiments indicate that instance detection and reconstruction present complementary effects, where the shape prediction head shows consistent effects on improving object detection with modern 3D proposal network backbones. The qualitative and quantitative evaluations further demonstrate that our approach consistently outperforms the state-of-the-arts and improves over 11 of mesh IoU in object reconstruction.



### A Comprehensive Review on Recent Methods and Challenges of Video Description
- **Arxiv ID**: http://arxiv.org/abs/2011.14752v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2011.14752v1)
- **Published**: 2020-11-30 13:08:45+00:00
- **Updated**: 2020-11-30 13:08:45+00:00
- **Authors**: Alok Singh, Thoudam Doren Singh, Sivaji Bandyopadhyay
- **Comment**: Paper of 35 pages submitted to ACM Computing Surveys
- **Journal**: None
- **Summary**: Video description involves the generation of the natural language description of actions, events, and objects in the video. There are various applications of video description by filling the gap between languages and vision for visually impaired people, generating automatic title suggestion based on content, browsing of the video based on the content and video-guided machine translation [86] etc.In the past decade, several works had been done in this field in terms of approaches/methods for video description, evaluation metrics,and datasets. For analyzing the progress in the video description task, a comprehensive survey is needed that covers all the phases of video description approaches with a special focus on recent deep learning approaches. In this work, we report a comprehensive survey on the phases of video description approaches, the dataset for video description, evaluation metrics, open competitions for motivating the research on the video description, open challenges in this field, and future research directions. In this survey, we cover the state-of-the-art approaches proposed for each and every dataset with their pros and cons. For the growth of this research domain,the availability of numerous benchmark dataset is a basic need. Further, we categorize all the dataset into two classes: open domain dataset and domain-specific dataset. From our survey, we observe that the work in this field is in fast-paced development since the task of video description falls in the intersection of computer vision and natural language processing. But still, the work in the video description is far from saturation stage due to various challenges like the redundancy due to similar frames which affect the quality of visual features, the availability of dataset containing more diverse content and availability of an effective evaluation metric.



### Scale-covariant and scale-invariant Gaussian derivative networks
- **Arxiv ID**: http://arxiv.org/abs/2011.14759v10
- **DOI**: 10.1007/s10851-021-01057-9
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.14759v10)
- **Published**: 2020-11-30 13:15:10+00:00
- **Updated**: 2021-09-26 16:46:57+00:00
- **Authors**: Tony Lindeberg
- **Comment**: 21 pages, 10 figures
- **Journal**: Journal of Mathematical Imaging and Vision 64(3): 223-242, 2022.
  Earlier and shorter version in SSVM 2021: Scale Space and Variational Methods
  in Computer Vision, Springer LNCS 12679: 3-14, 2021
- **Summary**: This paper presents a hybrid approach between scale-space theory and deep learning, where a deep learning architecture is constructed by coupling parameterized scale-space operations in cascade. By sharing the learnt parameters between multiple scale channels, and by using the transformation properties of the scale-space primitives under scaling transformations, the resulting network becomes provably scale covariant. By in addition performing max pooling over the multiple scale channels, a resulting network architecture for image classification also becomes provably scale invariant. We investigate the performance of such networks on the MNISTLargeScale dataset, which contains rescaled images from original MNIST over a factor of 4 concerning training data and over a factor of 16 concerning testing data. It is demonstrated that the resulting approach allows for scale generalization, enabling good performance for classifying patterns at scales not present in the training data.



### How Good MVSNets Are at Depth Fusion
- **Arxiv ID**: http://arxiv.org/abs/2011.14761v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.14761v1)
- **Published**: 2020-11-30 13:15:51+00:00
- **Updated**: 2020-11-30 13:15:51+00:00
- **Authors**: Oleg Voynov, Aleksandr Safin, Savva Ignatyev, Evgeny Burnaev
- **Comment**: 7 pages, 6 figures, 1 table. Accepted to ICMV 2020
- **Journal**: None
- **Summary**: We study the effects of the additional input to deep multi-view stereo methods in the form of low-quality sensor depth. We modify two state-of-the-art deep multi-view stereo methods for using with the input depth. We show that the additional input depth may improve the quality of deep multi-view stereo.



### Deep learning approach to left ventricular non-compaction measurement
- **Arxiv ID**: http://arxiv.org/abs/2011.14773v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.14773v1)
- **Published**: 2020-11-30 13:32:44+00:00
- **Updated**: 2020-11-30 13:32:44+00:00
- **Authors**: Jes√∫s M. Rodr√≠guez-de-Vera, Josefa Gonz√°lez-Carrillo, Jos√© M. Garc√≠a, Gregorio Bernab√©
- **Comment**: None
- **Journal**: None
- **Summary**: Left ventricular non-compaction (LVNC) is a rare cardiomyopathy characterized by abnormal trabeculations in the left ventricle cavity. Although traditional computer vision approaches exist for LVNC diagnosis, deep learning-based tools could not be found in the literature. In this paper, a first approach using convolutional neural networks (CNNs) is presented. Four CNNs are trained to automatically segment the compacted and trabecular areas of the left ventricle for a population of patients diagnosed with Hypertrophic cardiomyopathy. Inference results confirm that deep learning-based approaches can achieve excellent results in the diagnosis and measurement of LVNC. The two best CNNs (U-Net and Efficient U-Net B1) perform image segmentation in less than 0.2 s on a CPU and in less than 0.01 s on a GPU. Additionally, a subjective evaluation of the output images with the identified zones is performed by expert cardiologists, with a perfect visual agreement for all the slices, outperforming already existing automatic tools.



### S2FGAN: Semantically Aware Interactive Sketch-to-Face Translation
- **Arxiv ID**: http://arxiv.org/abs/2011.14785v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.14785v3)
- **Published**: 2020-11-30 13:42:39+00:00
- **Updated**: 2021-10-11 06:58:13+00:00
- **Authors**: Yan Yang, Md Zakir Hossain, Tom Gedeon, Shafin Rahman
- **Comment**: None
- **Journal**: None
- **Summary**: Interactive facial image manipulation attempts to edit single and multiple face attributes using a photo-realistic face and/or semantic mask as input. In the absence of the photo-realistic image (only sketch/mask available), previous methods only retrieve the original face but ignore the potential of aiding model controllability and diversity in the translation process. This paper proposes a sketch-to-image generation framework called S2FGAN, aiming to improve users' ability to interpret and flexibility of face attribute editing from a simple sketch. The proposed framework modifies the constrained latent space semantics trained on Generative Adversarial Networks (GANs). We employ two latent spaces to control the face appearance and adjust the desired attributes of the generated face. Instead of constraining the translation process by using a reference image, the users can command the model to retouch the generated images by involving the semantic information in the generation process. In this way, our method can manipulate single or multiple face attributes by only specifying attributes to be changed. Extensive experimental results on CelebAMask-HQ dataset empirically shows our superior performance and effectiveness on this task. Our method successfully outperforms state-of-the-art methods on attribute manipulation by exploiting greater control of attribute intensity.



### Unsupervised Path Regression Networks
- **Arxiv ID**: http://arxiv.org/abs/2011.14787v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.14787v2)
- **Published**: 2020-11-30 13:45:55+00:00
- **Updated**: 2021-03-09 11:38:55+00:00
- **Authors**: Michal P√°ndy, Daniel Lenton, Ronald Clark
- **Comment**: None
- **Journal**: None
- **Summary**: We demonstrate that challenging shortest path problems can be solved via direct spline regression from a neural network, trained in an unsupervised manner (i.e. without requiring ground truth optimal paths for training). To achieve this, we derive a geometry-dependent optimal cost function whose minima guarantees collision-free solutions. Our method beats state-of-the-art supervised learning baselines for shortest path planning, with a much more scalable training pipeline, and a significant speedup in inference time.



### NeuralFusion: Online Depth Fusion in Latent Space
- **Arxiv ID**: http://arxiv.org/abs/2011.14791v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.14791v2)
- **Published**: 2020-11-30 13:50:59+00:00
- **Updated**: 2021-06-08 12:28:27+00:00
- **Authors**: Silvan Weder, Johannes L. Sch√∂nberger, Marc Pollefeys, Martin R. Oswald
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel online depth map fusion approach that learns depth map aggregation in a latent feature space. While previous fusion methods use an explicit scene representation like signed distance functions (SDFs), we propose a learned feature representation for the fusion. The key idea is a separation between the scene representation used for the fusion and the output scene representation, via an additional translator network. Our neural network architecture consists of two main parts: a depth and feature fusion sub-network, which is followed by a translator sub-network to produce the final surface representation (e.g. TSDF) for visualization or other tasks. Our approach is an online process, handles high noise levels, and is particularly able to deal with gross outliers common for photometric stereo-based depth maps. Experiments on real and synthetic data demonstrate improved results compared to the state of the art, especially in challenging scenarios with large amounts of noise and outliers.



### Cost Function Unrolling in Unsupervised Optical Flow
- **Arxiv ID**: http://arxiv.org/abs/2011.14814v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.14814v2)
- **Published**: 2020-11-30 14:10:03+00:00
- **Updated**: 2021-11-24 10:09:18+00:00
- **Authors**: Gal Lifshitz, Dan Raviv
- **Comment**: None
- **Journal**: None
- **Summary**: Steepest descent algorithms, which are commonly used in deep learning, use the gradient as the descent direction, either as-is or after a direction shift using preconditioning. In many scenarios calculating the gradient is numerically hard due to complex or non-differentiable cost functions, specifically next to singular points. In this work we focus on the derivation of the Total Variation semi-norm commonly used in unsupervised cost functions. Specifically, we derive a differentiable proxy to the hard L1 smoothness constraint in a novel iterative scheme which we refer to as Cost Unrolling. Producing more accurate gradients during training, our method enables finer predictions of a given DNN model through improved convergence, without modifying its architecture or increasing computational complexity. We demonstrate our method in the unsupervised optical flow task. Replacing the L1 smoothness constraint with our unrolled cost during the training of a well known baseline, we report improved results on both MPI Sintel and KITTI 2015 unsupervised optical flow benchmarks. Particularly, we report EPE reduced by up to 15.82% on occluded pixels, where the smoothness constraint is dominant, enabling the detection of much sharper motion edges.



### Sparse-View Spectral CT Reconstruction Using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2011.14842v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.14842v2)
- **Published**: 2020-11-30 14:36:23+00:00
- **Updated**: 2021-03-26 23:49:03+00:00
- **Authors**: Wail Mustafa, Christian Kehl, Ulrik Lund Olsen, S√∏ren Kimmer Schou Gregersen, David Malmgren-Hansen, Jan Kehres, Anders Bjorholm Dahl
- **Comment**: 13 pages, 9 figures, submitted to The IEEE Transactions on
  Computational Imaging
- **Journal**: None
- **Summary**: Spectral computed tomography (CT) is an emerging technology capable of providing high chemical specificity, which is crucial for many applications such as detecting threats in luggage. This type of application requires both fast and high-quality image reconstruction and is often based on sparse-view (few) projections. The conventional filtered back projection (FBP) method is fast but it produces low-quality images dominated by noise and artifacts in sparse-view CT. Iterative methods with, e.g., total variation regularizers can circumvent that but they are computationally expensive, as the computational load proportionally increases with the number of spectral channels. Instead, we propose an approach for fast reconstruction of sparse-view spectral CT data using a U-Net convolutional neural network architecture with multi-channel input and output. The network is trained to output high-quality CT images from FBP input image reconstructions. Our method is fast at run-time and because the internal convolutions are shared between the channels, the computational load increases only at the first and last layers, making it an efficient approach to process spectral data with a large number of channels. We have validated our approach using real CT scans. Our results show qualitatively and quantitatively that our approach outperforms the state-of-the-art iterative methods. Furthermore, the results indicate that the network can exploit the coupling of the channels to enhance the overall quality and robustness.



### A Tiny CNN Architecture for Medical Face Mask Detection for Resource-Constrained Endpoints
- **Arxiv ID**: http://arxiv.org/abs/2011.14858v3
- **DOI**: 10.1007/978-981-16-0749-3_52
- **Categories**: **cs.CV**, cs.CY, cs.LG, eess.IV, 68T45, 68T10, 68T07, 68U10, C.3; I.2.6; I.2.10; I.4.9; I.5.1; I.5.2; I.5.4; I.5.5; K.4.1; K.4.3
- **Links**: [PDF](http://arxiv.org/pdf/2011.14858v3)
- **Published**: 2020-11-30 14:56:23+00:00
- **Updated**: 2021-06-03 12:55:21+00:00
- **Authors**: Puranjay Mohan, Aditya Jyoti Paul, Abhay Chirania
- **Comment**: 11 pages, Published in Springer LNEE at
  http://link.springer.com/chapter/10.1007%2F978-981-16-0749-3_52
- **Journal**: Innovations in Electrical and Electronic Engineering. Lecture
  Notes in Electrical Engineering, vol 756, pp 657-670, Springer, Singapore,
  2021
- **Summary**: The world is going through one of the most dangerous pandemics of all time with the rapid spread of the novel coronavirus (COVID-19). According to the World Health Organisation, the most effective way to thwart the transmission of coronavirus is to wear medical face masks. Monitoring the use of face masks in public places has been a challenge because manual monitoring could be unsafe. This paper proposes an architecture for detecting medical face masks for deployment on resource-constrained endpoints having extremely low memory footprints. A small development board with an ARM Cortex-M7 microcontroller clocked at 480 Mhz and having just 496 KB of framebuffer RAM, has been used for the deployment of the model. Using the TensorFlow Lite framework, the model is quantized to further reduce its size. The proposed model is 138 KB post quantization and runs at the inference speed of 30 FPS.



### Doubly Stochastic Subspace Clustering
- **Arxiv ID**: http://arxiv.org/abs/2011.14859v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.14859v2)
- **Published**: 2020-11-30 14:56:54+00:00
- **Updated**: 2021-04-19 23:50:41+00:00
- **Authors**: Derek Lim, Ren√© Vidal, Benjamin D. Haeffele
- **Comment**: None
- **Journal**: None
- **Summary**: Many state-of-the-art subspace clustering methods follow a two-step process by first constructing an affinity matrix between data points and then applying spectral clustering to this affinity. Most of the research into these methods focuses on the first step of generating the affinity, which often exploits the self-expressive property of linear subspaces, with little consideration typically given to the spectral clustering step that produces the final clustering. Moreover, existing methods often obtain the final affinity that is used in the spectral clustering step by applying ad-hoc or arbitrarily chosen postprocessing steps to the affinity generated by a self-expressive clustering formulation, which can have a significant impact on the overall clustering performance. In this work, we unify these two steps by learning both a self-expressive representation of the data and an affinity matrix that is well-normalized for spectral clustering. In our proposed models, we constrain the affinity matrix to be doubly stochastic, which results in a principled method for affinity matrix normalization while also exploiting known benefits of doubly stochastic normalization in spectral clustering. We develop a general framework and derive two models: one that jointly learns the self-expressive representation along with the doubly stochastic affinity, and one that sequentially solves for one then the other. Furthermore, we leverage sparsity in the problem to develop a fast active-set method for the sequential solver that enables efficient computation on large datasets. Experiments show that our method achieves state-of-the-art subspace clustering performance on many common datasets in computer vision.



### Prior Flow Variational Autoencoder: A density estimation model for Non-Intrusive Load Monitoring
- **Arxiv ID**: http://arxiv.org/abs/2011.14870v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.14870v2)
- **Published**: 2020-11-30 15:05:59+00:00
- **Updated**: 2021-06-27 22:51:43+00:00
- **Authors**: Luis Felipe M. O. Henriques, Eduardo Morgan, Sergio Colcher, Ruy Luiz Milidi√∫
- **Comment**: None
- **Journal**: None
- **Summary**: Non-Intrusive Load Monitoring (NILM) is a computational technique to estimate the power loads' appliance-by-appliance from the whole consumption measured by a single meter. In this paper, we propose a conditional density estimation model, based on deep neural networks, that joins a Conditional Variational Autoencoder with a Conditional Invertible Normalizing Flow model to estimate the individual appliance's power demand. The resulting model is called Prior Flow Variational Autoencoder or, for simplicity PFVAE. Thus, instead of having one model per appliance, the resulting model is responsible for estimating the power demand, appliance-by-appliance, at once. We train and evaluate our proposed model in a publicly available dataset composed of power demand measures from a poultry feed factory located in Brazil. The proposed model's quality is evaluated by comparing the obtained normalized disaggregation error (NDE) and signal aggregated error (SAE) with the previous work values on the same dataset. Our proposal achieves highly competitive results, and for six of the eight machines belonging to the dataset, we observe consistent improvements that go from 28% up to 81% in NDE and from 27% up to 86% in SAE.



### ViDi: Descriptive Visual Data Clustering as Radiologist Assistant in COVID-19 Streamline Diagnostic
- **Arxiv ID**: http://arxiv.org/abs/2011.14871v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.14871v1)
- **Published**: 2020-11-30 15:06:08+00:00
- **Updated**: 2020-11-30 15:06:08+00:00
- **Authors**: Sahithya Ravi, Samaneh Khoshrou, Mykola Pechenizkiy
- **Comment**: None
- **Journal**: None
- **Summary**: In the light of the COVID-19 pandemic, deep learning methods have been widely investigated in detecting COVID-19 from chest X-rays. However, a more pragmatic approach to applying AI methods to a medical diagnosis is designing a framework that facilitates human-machine interaction and expert decision making. Studies have shown that categorization can play an essential rule in accelerating real-world decision making. Inspired by descriptive document clustering, we propose a domain-independent explanatory clustering framework to group contextually related instances and support radiologists' decision making. While most descriptive clustering approaches employ domain-specific characteristics to form meaningful clusters, we focus on model-level explanation as a more general-purpose element of every learning process to achieve cluster homogeneity. We employ DeepSHAP to generate homogeneous clusters in terms of disease severity and describe the clusters using favorable and unfavorable saliency maps, which visualize the class discriminating regions of an image. These human-interpretable maps complement radiologist knowledge to investigate the whole cluster at once. Besides, as part of this study, we evaluate a model based on VGG-19, which can identify COVID and pneumonia cases with a positive predictive value of 95% and 97%, respectively, comparable to the recent explainable approaches for COVID diagnosis.



### Deep Interactive Denoiser (DID) for X-Ray Computed Tomography
- **Arxiv ID**: http://arxiv.org/abs/2011.14873v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.14873v2)
- **Published**: 2020-11-30 15:08:32+00:00
- **Updated**: 2020-12-06 17:53:20+00:00
- **Authors**: Ti Bai, Biling Wang, Dan Nguyen, Bao Wang, Bin Dong, Wenxiang Cong, Mannudeep K. Kalra, Steve Jiang
- **Comment**: under review
- **Journal**: None
- **Summary**: Low dose computed tomography (LDCT) is desirable for both diagnostic imaging and image guided interventions. Denoisers are openly used to improve the quality of LDCT. Deep learning (DL)-based denoisers have shown state-of-the-art performance and are becoming one of the mainstream methods. However, there exists two challenges regarding the DL-based denoisers: 1) a trained model typically does not generate different image candidates with different noise-resolution tradeoffs which sometimes are needed for different clinical tasks; 2) the model generalizability might be an issue when the noise level in the testing images is different from that in the training dataset. To address these two challenges, in this work, we introduce a lightweight optimization process at the testing phase on top of any existing DL-based denoisers to generate multiple image candidates with different noise-resolution tradeoffs suitable for different clinical tasks in real-time. Consequently, our method allows the users to interact with the denoiser to efficiently review various image candidates and quickly pick up the desired one, and thereby was termed as deep interactive denoiser (DID). Experimental results demonstrated that DID can deliver multiple image candidates with different noise-resolution tradeoffs, and shows great generalizability regarding various network architectures, as well as training and testing datasets with various noise levels.



### Occlusion Guided Scene Flow Estimation on 3D Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2011.14880v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.14880v2)
- **Published**: 2020-11-30 15:22:03+00:00
- **Updated**: 2021-04-19 16:49:36+00:00
- **Authors**: Bojun Ouyang, Dan Raviv
- **Comment**: Aaccepted at CVPR 2021 Workshop on Autonomous Driving
- **Journal**: None
- **Summary**: 3D scene flow estimation is a vital tool in perceiving our environment given depth or range sensors. Unlike optical flow, the data is usually sparse and in most cases partially occluded in between two temporal samplings. Here we propose a new scene flow architecture called OGSF-Net which tightly couples the learning for both flow and occlusions between frames. Their coupled symbiosis results in a more accurate prediction of flow in space. Unlike a traditional multi-action network, our unified approach is fused throughout the network, boosting performances for both occlusion detection and flow estimation. Our architecture is the first to gauge the occlusion in 3D scene flow estimation on point clouds. In key datasets such as Flyingthings3D and KITTI, we achieve the state-of-the-art results.



### Language-Driven Region Pointer Advancement for Controllable Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/2011.14901v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.LG, cs.NE, 68T07, 68T45, 68T50, I.2.7; I.2.10; I.5.1
- **Links**: [PDF](http://arxiv.org/pdf/2011.14901v1)
- **Published**: 2020-11-30 15:34:59+00:00
- **Updated**: 2020-11-30 15:34:59+00:00
- **Authors**: Annika Lindh, Robert J. Ross, John D. Kelleher
- **Comment**: Accepted to COLING 2020
- **Journal**: None
- **Summary**: Controllable Image Captioning is a recent sub-field in the multi-modal task of Image Captioning wherein constraints are placed on which regions in an image should be described in the generated natural language caption. This puts a stronger focus on producing more detailed descriptions, and opens the door for more end-user control over results. A vital component of the Controllable Image Captioning architecture is the mechanism that decides the timing of attending to each region through the advancement of a region pointer. In this paper, we propose a novel method for predicting the timing of region pointer advancement by treating the advancement step as a natural part of the language structure via a NEXT-token, motivated by a strong correlation to the sentence structure in the training data. We find that our timing agrees with the ground-truth timing in the Flickr30k Entities test data with a precision of 86.55% and a recall of 97.92%. Our model implementing this technique improves the state-of-the-art on standard captioning metrics while additionally demonstrating a considerably larger effective vocabulary size.



### Person Perception Biases Exposed: Revisiting the First Impressions Dataset
- **Arxiv ID**: http://arxiv.org/abs/2011.14906v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.DB
- **Links**: [PDF](http://arxiv.org/pdf/2011.14906v1)
- **Published**: 2020-11-30 15:41:27+00:00
- **Updated**: 2020-11-30 15:41:27+00:00
- **Authors**: Julio C. S. Jacques Junior, Agata Lapedriza, Cristina Palmero, Xavier Bar√≥, Sergio Escalera
- **Comment**: accepted on 11th International Workshop on Human Behavior
  Understanding (HBU), organized as part of WACV 2021
- **Journal**: None
- **Summary**: This work revisits the ChaLearn First Impressions database, annotated for personality perception using pairwise comparisons via crowdsourcing. We analyse for the first time the original pairwise annotations, and reveal existing person perception biases associated to perceived attributes like gender, ethnicity, age and face attractiveness. We show how person perception bias can influence data labelling of a subjective task, which has received little attention from the computer vision and machine learning communities by now. We further show that the mechanism used to convert pairwise annotations to continuous values may magnify the biases if no special treatment is considered. The findings of this study are relevant for the computer vision community that is still creating new datasets on subjective tasks, and using them for practical applications, ignoring these perceptual biases.



### Trajformer: Trajectory Prediction with Local Self-Attentive Contexts for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2011.14910v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.14910v1)
- **Published**: 2020-11-30 15:42:15+00:00
- **Updated**: 2020-11-30 15:42:15+00:00
- **Authors**: Manoj Bhat, Jonathan Francis, Jean Oh
- **Comment**: Accepted: Machine Learning for Autonomous Driving @ NeurIPS 2020
- **Journal**: None
- **Summary**: Effective feature-extraction is critical to models' contextual understanding, particularly for applications to robotics and autonomous driving, such as multimodal trajectory prediction. However, state-of-the-art generative methods face limitations in representing the scene context, leading to predictions of inadmissible futures. We alleviate these limitations through the use of self-attention, which enables better control over representing the agent's social context; we propose a local feature-extraction pipeline that produces more salient information downstream, with improved parameter efficiency. We show improvements on standard metrics (minADE, minFDE, DAO, DAC) over various baselines on the Argoverse dataset. We release our code at: https://github.com/Manojbhat09/Trajformer



### Driver Behavior Extraction from Videos in Naturalistic Driving Datasets with 3D ConvNets
- **Arxiv ID**: http://arxiv.org/abs/2011.14922v1
- **DOI**: 10.1007/s42421-022-00053-8
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2011.14922v1)
- **Published**: 2020-11-30 15:53:15+00:00
- **Updated**: 2020-11-30 15:53:15+00:00
- **Authors**: Hanwen Miao, Shengan Zhang, Carol Flannagan
- **Comment**: None
- **Journal**: None
- **Summary**: Naturalistic driving data (NDD) is an important source of information to understand crash causation and human factors and to further develop crash avoidance countermeasures. Videos recorded while driving are often included in such datasets. While there is often a large amount of video data in NDD, only a small portion of them can be annotated by human coders and used for research, which underuses all video data. In this paper, we explored a computer vision method to automatically extract the information we need from videos. More specifically, we developed a 3D ConvNet algorithm to automatically extract cell-phone-related behaviors from videos. The experiments show that our method can extract chunks from videos, most of which (~79%) contain the automatically labeled cell phone behaviors. In conjunction with human review of the extracted chunks, this approach can find cell-phone-related driver behaviors much more efficiently than simply viewing video.



### Rethinking and Designing a High-performing Automatic License Plate Recognition Approach
- **Arxiv ID**: http://arxiv.org/abs/2011.14936v2
- **DOI**: 10.1109/TITS.2021.3087158
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.14936v2)
- **Published**: 2020-11-30 16:03:57+00:00
- **Updated**: 2021-06-17 07:05:00+00:00
- **Authors**: Yi Wang, Zhen-Peng Bian, Yunhao Zhou, Lap-Pui Chau
- **Comment**: 13 pages. Accepted for Publication at IEEE Transactions on
  Intelligent Transportation Systems
- **Journal**: None
- **Summary**: In this paper, we propose a real-time and accurate automatic license plate recognition (ALPR) approach. Our study illustrates the outstanding design of ALPR with four insights: (1) the resampling-based cascaded framework is beneficial to both speed and accuracy; (2) the highly efficient license plate recognition should abundant additional character segmentation and recurrent neural network (RNN), but adopt a plain convolutional neural network (CNN); (3) in the case of CNN, taking advantage of vertex information on license plates improves the recognition performance; and (4) the weight-sharing character classifier addresses the lack of training images in small-scale datasets. Based on these insights, we propose a novel ALPR approach, termed VSNet. Specifically, VSNet includes two CNNs, i.e., VertexNet for license plate detection and SCR-Net for license plate recognition, integrated in a resampling-based cascaded manner. In VertexNet, we propose an efficient integration block to extract the spatial features of license plates. With vertex supervisory information, we propose a vertex-estimation branch in VertexNet such that license plates can be rectified as the input images of SCR-Net. In SCR-Net, we introduce a horizontal encoding technique for left-to-right feature extraction and propose a weight-sharing classifier for character recognition. Experimental results show that the proposed VSNet outperforms state-of-the-art methods by more than 50% relative improvement on error rate, achieving > 99% recognition accuracy on CCPD and AOLP datasets with 149 FPS inference speed. Moreover, our method illustrates an outstanding generalization capability when evaluated on the unseen PKUData and CLPD datasets.



### Floods Detection in Twitter Text and Images
- **Arxiv ID**: http://arxiv.org/abs/2011.14943v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.14943v1)
- **Published**: 2020-11-30 16:08:19+00:00
- **Updated**: 2020-11-30 16:08:19+00:00
- **Authors**: Naina Said, Kashif Ahmad, Asma Gul, Nasir Ahmad, Ala Al-Fuqaha
- **Comment**: 3 pages
- **Journal**: None
- **Summary**: In this paper, we present our methods for the MediaEval 2020 Flood Related Multimedia task, which aims to analyze and combine textual and visual content from social media for the detection of real-world flooding events. The task mainly focuses on identifying floods related tweets relevant to a specific area. We propose several schemes to address the challenge. For text-based flood events detection, we use three different methods, relying on Bog of Words (BOW) and an Italian Version of Bert individually and in combination, achieving an F1-score of 0.77%, 0.68%, and 0.70% on the development set, respectively. For the visual analysis, we rely on features extracted via multiple state-of-the-art deep models pre-trained on ImageNet. The extracted features are then used to train multiple individual classifiers whose scores are then combined in a late fusion manner achieving an F1-score of 0.75%. For our mandatory multi-modal run, we combine the classification scores obtained with the best textual and visual schemes in a late fusion manner. Overall, better results are obtained with the multimodal scheme achieving an F1-score of 0.80% on the development set.



### Flood Detection via Twitter Streams using Textual and Visual Features
- **Arxiv ID**: http://arxiv.org/abs/2011.14944v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.14944v1)
- **Published**: 2020-11-30 16:09:11+00:00
- **Updated**: 2020-11-30 16:09:11+00:00
- **Authors**: Firoj Alam, Zohaib Hassan, Kashif Ahmad, Asma Gul, Michael Reiglar, Nicola Conci, Ala AL-Fuqaha
- **Comment**: 3 pages
- **Journal**: None
- **Summary**: The paper presents our proposed solutions for the MediaEval 2020 Flood-Related Multimedia Task, which aims to analyze and detect flooding events in multimedia content shared over Twitter. In total, we proposed four different solutions including a multi-modal solution combining textual and visual information for the mandatory run, and three single modal image and text-based solutions as optional runs. In the multimodal method, we rely on a supervised multimodal bitransformer model that combines textual and visual features in an early fusion, achieving a micro F1-score of .859 on the development data set. For the text-based flood events detection, we use a transformer network (i.e., pretrained Italian BERT model) achieving an F1-score of .853. For image-based solutions, we employed multiple deep models, pre-trained on both, the ImageNet and places data sets, individually and combined in an early fusion achieving F1-scores of .816 and .805 on the development set, respectively.



### Deep Dose Plugin Towards Real-time Monte Carlo Dose Calculation Through a Deep Learning based Denoising Algorithm
- **Arxiv ID**: http://arxiv.org/abs/2011.14959v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.14959v2)
- **Published**: 2020-11-30 16:33:51+00:00
- **Updated**: 2020-12-23 16:57:15+00:00
- **Authors**: Ti Bai, Biling Wang, Dan Nguyen, Steve Jiang
- **Comment**: under review
- **Journal**: None
- **Summary**: Monte Carlo (MC) simulation is considered the gold standard method for radiotherapy dose calculation. However, achieving high precision requires a large number of simulation histories, which is time consuming. The use of computer graphics processing units (GPUs) has greatly accelerated MC simulation and allows dose calculation within a few minutes for a typical radiotherapy treatment plan. However, some clinical applications demand real time efficiency for MC dose calculation. To tackle this problem, we have developed a real time, deep learning based dose denoiser that can be plugged into a current GPU based MC dose engine to enable real time MC dose calculation. We used two different acceleration strategies to achieve this goal: 1) we applied voxel unshuffle and voxel shuffle operators to decrease the input and output sizes without any information loss, and 2) we decoupled the 3D volumetric convolution into a 2D axial convolution and a 1D slice convolution. In addition, we used a weakly supervised learning framework to train the network, which greatly reduces the size of the required training dataset and thus enables fast fine tuning based adaptation of the trained model to different radiation beams. Experimental results show that the proposed denoiser can run in as little as 39 ms, which is around 11.6 times faster than the baseline model. As a result, the whole MC dose calculation pipeline can be finished within 0.15 seconds, including both GPU MC dose calculation and deep learning based denoising, achieving the real time efficiency needed for some radiotherapy applications, such as online adaptive radiotherapy.



### Guided Adversarial Attack for Evaluating and Enhancing Adversarial Defenses
- **Arxiv ID**: http://arxiv.org/abs/2011.14969v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.14969v1)
- **Published**: 2020-11-30 16:39:39+00:00
- **Updated**: 2020-11-30 16:39:39+00:00
- **Authors**: Gaurang Sriramanan, Sravanti Addepalli, Arya Baburaj, R. Venkatesh Babu
- **Comment**: NeurIPS 2020 (Spotlight)
- **Journal**: None
- **Summary**: Advances in the development of adversarial attacks have been fundamental to the progress of adversarial defense research. Efficient and effective attacks are crucial for reliable evaluation of defenses, and also for developing robust models. Adversarial attacks are often generated by maximizing standard losses such as the cross-entropy loss or maximum-margin loss within a constraint set using Projected Gradient Descent (PGD). In this work, we introduce a relaxation term to the standard loss, that finds more suitable gradient-directions, increases attack efficacy and leads to more efficient adversarial training. We propose Guided Adversarial Margin Attack (GAMA), which utilizes function mapping of the clean image to guide the generation of adversaries, thereby resulting in stronger attacks. We evaluate our attack against multiple defenses and show improved performance when compared to existing attacks. Further, we propose Guided Adversarial Training (GAT), which achieves state-of-the-art performance amongst single-step defenses by utilizing the proposed relaxation term for both attack generation and training.



### MAVIDH Score: A COVID-19 Severity Scoring using Chest X-Ray Pathology Features
- **Arxiv ID**: http://arxiv.org/abs/2011.14983v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.14983v3)
- **Published**: 2020-11-30 16:55:28+00:00
- **Updated**: 2021-02-02 13:01:09+00:00
- **Authors**: Douglas P. S. Gomes, Michael J. Horry, Anwaar Ulhaq, Manoranjan Paul, Subrata Chakraborty, Manash Saha, Tanmoy Debnath, D. M. Motiur Rahaman
- **Comment**: None
- **Journal**: None
- **Summary**: The application of computer vision for COVID-19 diagnosis is complex and challenging, given the risks associated with patient misclassifications. Arguably, the primary value of medical imaging for COVID-19 lies rather on patient prognosis. Radiological images can guide physicians assessing the severity of the disease, and a series of images from the same patient at different stages can help to gauge disease progression. Hence, a simple method based on lung-pathology interpretable features for scoring disease severity from Chest X-rays is proposed here. As the primary contribution, this method correlates well to patient severity in different stages of disease progression with competitive results compared to other existing, more complex methods. An original data selection approach is also proposed, allowing the simple model to learn the severity-related features. It is hypothesized that the resulting competitive performance presented here is related to the method being feature-based rather than reliant on lung involvement or opacity as others in the literature. A second contribution comes from the validation of the results, conceptualized as the scoring of patients groups from different stages of the disease. Besides performing such validation on an independent data set, the results were also compared with other proposed scoring methods in the literature. The results show that there is a significant correlation between the scoring system (MAVIDH) and patient outcome, which could potentially help physicians rating and following disease progression in COVID-19 patients.



### Fast, Self Supervised, Fully Convolutional Color Normalization of H&E Stained Images
- **Arxiv ID**: http://arxiv.org/abs/2011.15000v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2011.15000v1)
- **Published**: 2020-11-30 17:05:58+00:00
- **Updated**: 2020-11-30 17:05:58+00:00
- **Authors**: Abhijeet Patil, Mohd. Talha, Aniket Bhatia, Nikhil Cherian Kurian, Sammed Mangale, Sunil Patel, Amit Sethi
- **Comment**: --
- **Journal**: None
- **Summary**: Performance of deep learning algorithms decreases drastically if the data distributions of the training and testing sets are different. Due to variations in staining protocols, reagent brands, and habits of technicians, color variation in digital histopathology images is quite common. Color variation causes problems for the deployment of deep learning-based solutions for automatic diagnosis system in histopathology. Previously proposed color normalization methods consider a small patch as a reference for normalization, which creates artifacts on out-of-distribution source images. These methods are also slow as most of the computation is performed on CPUs instead of the GPUs. We propose a color normalization technique, which is fast during its self-supervised training as well as inference. Our method is based on a lightweight fully-convolutional neural network and can be easily attached to a deep learning-based pipeline as a pre-processing block. For classification and segmentation tasks on CAMELYON17 and MoNuSeg datasets respectively, the proposed method is faster and gives a greater increase in accuracy than the state of the art methods.



### Image Quality Assessment for Perceptual Image Restoration: A New Dataset, Benchmark and Metric
- **Arxiv ID**: http://arxiv.org/abs/2011.15002v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.15002v1)
- **Published**: 2020-11-30 17:06:46+00:00
- **Updated**: 2020-11-30 17:06:46+00:00
- **Authors**: Jinjin Gu, Haoming Cai, Haoyu Chen, Xiaoxing Ye, Jimmy Ren, Chao Dong
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2007.12142
- **Journal**: None
- **Summary**: Image quality assessment (IQA) is the key factor for the fast development of image restoration (IR) algorithms. The most recent perceptual IR algorithms based on generative adversarial networks (GANs) have brought in significant improvement on visual performance, but also pose great challenges for quantitative evaluation. Notably, we observe an increasing inconsistency between perceptual quality and the evaluation results. We present two questions: Can existing IQA methods objectively evaluate recent IR algorithms? With the focus on beating current benchmarks, are we getting better IR algorithms? To answer the questions and promote the development of IQA methods, we contribute a large-scale IQA dataset, called Perceptual Image Processing ALgorithms (PIPAL) dataset. Especially, this dataset includes the results of GAN-based IR algorithms, which are missing in previous datasets. We collect more than 1.13 million human judgments to assign subjective scores for PIPAL images using the more reliable Elo system. Based on PIPAL, we present new benchmarks for both IQA and SR methods. Our results indicate that existing IQA methods cannot fairly evaluate GAN-based IR algorithms. While using appropriate evaluation methods is important, IQA methods should also be updated along with the development of IR algorithms. At last, we shed light on how to improve the IQA performance on GAN-based distortion. Inspired by the find that the existing IQA methods have an unsatisfactory performance on the GAN-based distortion partially because of their low tolerance to spatial misalignment, we propose to improve the performance of an IQA network on GAN-based distortion by explicitly considering this misalignment. We propose the Space Warping Difference Network, which includes the novel l_2 pooling layers and Space Warping Difference layers. Experiments demonstrate the effectiveness of the proposed method.



### Unsupervised Deep Video Denoising
- **Arxiv ID**: http://arxiv.org/abs/2011.15045v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2011.15045v3)
- **Published**: 2020-11-30 17:45:08+00:00
- **Updated**: 2021-08-19 19:41:54+00:00
- **Authors**: Dev Yashpal Sheth, Sreyas Mohan, Joshua L. Vincent, Ramon Manzorro, Peter A. Crozier, Mitesh M. Khapra, Eero P. Simoncelli, Carlos Fernandez-Granda
- **Comment**: Dev and Sreyas contributed equally. To appear at 2021 IEEE/CVF
  International Conference on Computer Vision (ICCV). See
  https://sreyas-mohan.github.io/udvd/ for code and more results
- **Journal**: None
- **Summary**: Deep convolutional neural networks (CNNs) for video denoising are typically trained with supervision, assuming the availability of clean videos. However, in many applications, such as microscopy, noiseless videos are not available. To address this, we propose an Unsupervised Deep Video Denoiser (UDVD), a CNN architecture designed to be trained exclusively with noisy data. The performance of UDVD is comparable to the supervised state-of-the-art, even when trained only on a single short noisy video. We demonstrate the promise of our approach in real-world imaging applications by denoising raw video, fluorescence-microscopy and electron-microscopy data. In contrast to many current approaches to video denoising, UDVD does not require explicit motion compensation. This is advantageous because motion compensation is computationally expensive, and can be unreliable when the input data are noisy. A gradient-based analysis reveals that UDVD automatically adapts to local motion in the input noisy videos. Thus, the network learns to perform implicit motion compensation, even though it is only trained for denoising.



### Long-range medical image registration through generalized mutual information (GMI): toward a fully automatic volumetric alignment
- **Arxiv ID**: http://arxiv.org/abs/2011.15049v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.IT, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/2011.15049v1)
- **Published**: 2020-11-30 17:48:28+00:00
- **Updated**: 2020-11-30 17:48:28+00:00
- **Authors**: Vinicius Pavanelli Vianna, Luiz Otavio Murta Jr
- **Comment**: 13 pages, 8 figures, 3 tables
- **Journal**: None
- **Summary**: Image registration is a key operation in medical image processing, allowing a plethora of applications. Mutual information (MI) is consolidated as a robust similarity metric often used for medical image registration. Although MI provides a robust medical image registration, it usually fails when the needed image transform is too big due to MI local maxima traps. In this paper, we propose and evaluate a generalized parametric MI as an affine registration cost function. We assessed the generalized MI (GMI) functions for separable affine transforms and exhaustively evaluated the GMI mathematical image seeking the maximum registration range through a gradient descent simulation. We also employed Monte Carlo simulation essays for testing translation registering of randomized T1 versus T2 images. GMI functions showed to have smooth isosurfaces driving the algorithm to the global maxima. Results show significantly prolonged registration ranges, avoiding the traps of local maxima. We evaluated a range of [-150mm,150mm] for translations, [-180{\deg},180{\deg}] for rotations, [0.5,2] for scales, and [-1,1] for skew with a success rate of 99.99%, 97.58%, 99.99%, and 99.99% respectively for the transforms in the simulated gradient descent. We also obtained 99.75% success in Monte Carlo simulation from 2,000 randomized translations trials with 1,113 subjects T1 and T2 MRI images. The findings point towards the reliability of GMI for long-range registration with enhanced speed performance



### Forecasting Characteristic 3D Poses of Human Actions
- **Arxiv ID**: http://arxiv.org/abs/2011.15079v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.15079v3)
- **Published**: 2020-11-30 18:20:17+00:00
- **Updated**: 2022-03-08 18:59:31+00:00
- **Authors**: Christian Diller, Thomas Funkhouser, Angela Dai
- **Comment**: CVPR 2022; Project Page: https://charposes.christian-diller.de/;
  Paper Video: https://youtu.be/kVhn8OWMgME
- **Journal**: None
- **Summary**: We propose the task of forecasting characteristic 3d poses: from a short sequence observation of a person, predict a future 3d pose of that person in a likely action-defining, characteristic pose -- for instance, from observing a person picking up an apple, predict the pose of the person eating the apple. Prior work on human motion prediction estimates future poses at fixed time intervals. Although easy to define, this frame-by-frame formulation confounds temporal and intentional aspects of human action. Instead, we define a semantically meaningful pose prediction task that decouples the predicted pose from time, taking inspiration from goal-directed behavior. To predict characteristic poses, we propose a probabilistic approach that models the possible multi-modality in the distribution of likely characteristic poses. We then sample future pose hypotheses from the predicted distribution in an autoregressive fashion to model dependencies between joints. To evaluate our method, we construct a dataset of manually annotated characteristic 3d poses. Our experiments with this dataset suggest that our proposed probabilistic approach outperforms state-of-the-art methods by 26% on average.



### DEF: Deep Estimation of Sharp Geometric Features in 3D Shapes
- **Arxiv ID**: http://arxiv.org/abs/2011.15081v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.CG
- **Links**: [PDF](http://arxiv.org/pdf/2011.15081v4)
- **Published**: 2020-11-30 18:21:00+00:00
- **Updated**: 2022-05-26 12:27:19+00:00
- **Authors**: Albert Matveev, Ruslan Rakhimov, Alexey Artemov, Gleb Bobrovskikh, Vage Egiazarian, Emil Bogomolov, Daniele Panozzo, Denis Zorin, Evgeny Burnaev
- **Comment**: None
- **Journal**: None
- **Summary**: We propose Deep Estimators of Features (DEFs), a learning-based framework for predicting sharp geometric features in sampled 3D shapes. Differently from existing data-driven methods, which reduce this problem to feature classification, we propose to regress a scalar field representing the distance from point samples to the closest feature line on local patches. Our approach is the first that scales to massive point clouds by fusing distance-to-feature estimates obtained on individual patches. We extensively evaluate our approach against related state-of-the-art methods on newly proposed synthetic and real-world 3D CAD model benchmarks. Our approach not only outperforms these (with improvements in Recall and False Positives Rates), but generalizes to real-world scans after training our model on synthetic data and fine-tuning it on a small dataset of scanned data. We demonstrate a downstream application, where we reconstruct an explicit representation of straight and curved sharp feature lines from range scan data.



### Likelihood-Based Diverse Sampling for Trajectory Forecasting
- **Arxiv ID**: http://arxiv.org/abs/2011.15084v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.15084v2)
- **Published**: 2020-11-30 18:23:29+00:00
- **Updated**: 2021-09-14 23:29:16+00:00
- **Authors**: Yecheng Jason Ma, Jeevana Priya Inala, Dinesh Jayaraman, Osbert Bastani
- **Comment**: ICCV 2021
- **Journal**: None
- **Summary**: Forecasting complex vehicle and pedestrian multi-modal distributions requires powerful probabilistic approaches. Normalizing flows (NF) have recently emerged as an attractive tool to model such distributions. However, a key drawback is that independent samples drawn from a flow model often do not adequately capture all the modes in the underlying distribution. We propose Likelihood-Based Diverse Sampling (LDS), a method for improving the quality and the diversity of trajectory samples from a pre-trained flow model. Rather than producing individual samples, LDS produces a set of trajectories in one shot. Given a pre-trained forecasting flow model, we train LDS using gradients from the model, to optimize an objective function that rewards high likelihood for individual trajectories in the predicted set, together with high spatial separation among trajectories. LDS outperforms state-of-art post-hoc neural diverse forecasting methods for various pre-trained flow models as well as conditional variational autoencoder (CVAE) models. Crucially, it can also be used for transductive trajectory forecasting, where the diverse forecasts are trained on-the-fly on unlabeled test examples. LDS is easy to implement, and we show that it offers a simple plug-in improvement over baselines on two challenging benchmarks. Code is at: https://github.com/JasonMa2016/LDS



### Reducing Textural Bias Improves Robustness of Deep Segmentation Models
- **Arxiv ID**: http://arxiv.org/abs/2011.15093v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.15093v3)
- **Published**: 2020-11-30 18:29:53+00:00
- **Updated**: 2021-06-27 21:15:40+00:00
- **Authors**: Seoin Chai, Daniel Rueckert, Ahmed E. Fetit
- **Comment**: To appear in MIUA 2021 (accepted version)
- **Journal**: None
- **Summary**: Despite advances in deep learning, robustness under domain shift remains a major bottleneck in medical imaging settings. Findings on natural images suggest that deep neural models can show a strong textural bias when carrying out image classification tasks. In this thorough empirical study, we draw inspiration from findings on natural images and investigate ways in which addressing the textural bias phenomenon could bring up the robustness of deep segmentation models when applied to three-dimensional (3D) medical data. To achieve this, publicly available MRI scans from the Developing Human Connectome Project are used to study ways in which simulating textural noise can help train robust models in a complex semantic segmentation task. We contribute an extensive empirical investigation consisting of 176 experiments and illustrate how applying specific types of simulated textural noise prior to training can lead to texture invariant models, resulting in improved robustness when segmenting scans corrupted by previously unseen noise types and levels.



### Learning by Passing Tests, with Application to Neural Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/2011.15102v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.15102v2)
- **Published**: 2020-11-30 18:33:34+00:00
- **Updated**: 2021-03-12 03:43:01+00:00
- **Authors**: Xuefeng Du, Haochen Zhang, Pengtao Xie
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2012.04863,
  arXiv:2012.12502, arXiv:2012.12899
- **Journal**: None
- **Summary**: Learning through tests is a broadly used methodology in human learning and shows great effectiveness in improving learning outcome: a sequence of tests are made with increasing levels of difficulty; the learner takes these tests to identify his/her weak points in learning and continuously addresses these weak points to successfully pass these tests. We are interested in investigating whether this powerful learning technique can be borrowed from humans to improve the learning abilities of machines. We propose a novel learning approach called learning by passing tests (LPT). In our approach, a tester model creates increasingly more-difficult tests to evaluate a learner model. The learner tries to continuously improve its learning ability so that it can successfully pass however difficult tests created by the tester. We propose a multi-level optimization framework to formulate LPT, where the tester learns to create difficult and meaningful tests and the learner learns to pass these tests. We develop an efficient algorithm to solve the LPT problem. Our method is applied for neural architecture search and achieves significant improvement over state-of-the-art baselines on CIFAR-100, CIFAR-10, and ImageNet.



### Automating Artifact Detection in Video Games
- **Arxiv ID**: http://arxiv.org/abs/2011.15103v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.15103v1)
- **Published**: 2020-11-30 18:34:40+00:00
- **Updated**: 2020-11-30 18:34:40+00:00
- **Authors**: Parmida Davarmanesh, Kuanhao Jiang, Tingting Ou, Artem Vysogorets, Stanislav Ivashkevich, Max Kiehn, Shantanu H. Joshi, Nicholas Malaya
- **Comment**: None
- **Journal**: None
- **Summary**: In spite of advances in gaming hardware and software, gameplay is often tainted with graphics errors, glitches, and screen artifacts. This proof of concept study presents a machine learning approach for automated detection of graphics corruptions in video games. Based on a sample of representative screen corruption examples, the model was able to identify 10 of the most commonly occurring screen artifacts with reasonable accuracy. Feature representation of the data included discrete Fourier transforms, histograms of oriented gradients, and graph Laplacians. Various combinations of these features were used to train machine learning models that identify individual classes of graphics corruptions and that later were assembled into a single mixed experts "ensemble" classifier. The ensemble classifier was tested on heldout test sets, and produced an accuracy of 84% on the games it had seen before, and 69% on games it had never seen before.



### UniCon: Universal Neural Controller For Physics-based Character Motion
- **Arxiv ID**: http://arxiv.org/abs/2011.15119v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2011.15119v1)
- **Published**: 2020-11-30 18:51:16+00:00
- **Updated**: 2020-11-30 18:51:16+00:00
- **Authors**: Tingwu Wang, Yunrong Guo, Maria Shugrina, Sanja Fidler
- **Comment**: 15 pages, 15 figures
- **Journal**: None
- **Summary**: The field of physics-based animation is gaining importance due to the increasing demand for realism in video games and films, and has recently seen wide adoption of data-driven techniques, such as deep reinforcement learning (RL), which learn control from (human) demonstrations. While RL has shown impressive results at reproducing individual motions and interactive locomotion, existing methods are limited in their ability to generalize to new motions and their ability to compose a complex motion sequence interactively. In this paper, we propose a physics-based universal neural controller (UniCon) that learns to master thousands of motions with different styles by learning on large-scale motion datasets. UniCon is a two-level framework that consists of a high-level motion scheduler and an RL-powered low-level motion executor, which is our key innovation. By systematically analyzing existing multi-motion RL frameworks, we introduce a novel objective function and training techniques which make a significant leap in performance. Once trained, our motion executor can be combined with different high-level schedulers without the need for retraining, enabling a variety of real-time interactive applications. We show that UniCon can support keyboard-driven control, compose motion sequences drawn from a large pool of locomotion and acrobatics skills and teleport a person captured on video to a physics-based virtual avatar. Numerical and qualitative results demonstrate a significant improvement in efficiency, robustness and generalizability of UniCon over prior state-of-the-art, showcasing transferability to unseen motions, unseen humanoid models and unseen perturbation.



### Multimodal Pretraining Unmasked: A Meta-Analysis and a Unified Framework of Vision-and-Language BERTs
- **Arxiv ID**: http://arxiv.org/abs/2011.15124v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.15124v2)
- **Published**: 2020-11-30 18:55:24+00:00
- **Updated**: 2021-05-30 23:37:58+00:00
- **Authors**: Emanuele Bugliarello, Ryan Cotterell, Naoaki Okazaki, Desmond Elliott
- **Comment**: To appear in TACL 2021
- **Journal**: None
- **Summary**: Large-scale pretraining and task-specific fine-tuning is now the standard methodology for many tasks in computer vision and natural language processing. Recently, a multitude of methods have been proposed for pretraining vision and language BERTs to tackle challenges at the intersection of these two key areas of AI. These models can be categorised into either single-stream or dual-stream encoders. We study the differences between these two categories, and show how they can be unified under a single theoretical framework. We then conduct controlled experiments to discern the empirical differences between five V&L BERTs. Our experiments show that training data and hyperparameters are responsible for most of the differences between the reported results, but they also reveal that the embedding layer plays a crucial role in these massive models.



### One-Shot Free-View Neural Talking-Head Synthesis for Video Conferencing
- **Arxiv ID**: http://arxiv.org/abs/2011.15126v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.15126v3)
- **Published**: 2020-11-30 18:56:35+00:00
- **Updated**: 2021-04-02 23:37:06+00:00
- **Authors**: Ting-Chun Wang, Arun Mallya, Ming-Yu Liu
- **Comment**: CVPR 2021 camera ready (oral). Our project page can be found at
  https://nvlabs.github.io/face-vid2vid
- **Journal**: None
- **Summary**: We propose a neural talking-head video synthesis model and demonstrate its application to video conferencing. Our model learns to synthesize a talking-head video using a source image containing the target person's appearance and a driving video that dictates the motion in the output. Our motion is encoded based on a novel keypoint representation, where the identity-specific and motion-related information is decomposed unsupervisedly. Extensive experimental validation shows that our model outperforms competing methods on benchmark datasets. Moreover, our compact keypoint representation enables a video conferencing system that achieves the same visual quality as the commercial H.264 standard while only using one-tenth of the bandwidth. Besides, we show our keypoint representation allows the user to rotate the head during synthesis, which is useful for simulating face-to-face video conferencing experiences.



### Animating Pictures with Eulerian Motion Fields
- **Arxiv ID**: http://arxiv.org/abs/2011.15128v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2011.15128v1)
- **Published**: 2020-11-30 18:59:06+00:00
- **Updated**: 2020-11-30 18:59:06+00:00
- **Authors**: Aleksander Holynski, Brian Curless, Steven M. Seitz, Richard Szeliski
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we demonstrate a fully automatic method for converting a still image into a realistic animated looping video. We target scenes with continuous fluid motion, such as flowing water and billowing smoke. Our method relies on the observation that this type of natural motion can be convincingly reproduced from a static Eulerian motion description, i.e. a single, temporally constant flow field that defines the immediate motion of a particle at a given 2D location. We use an image-to-image translation network to encode motion priors of natural scenes collected from online videos, so that for a new photo, we can synthesize a corresponding motion field. The image is then animated using the generated motion through a deep warping technique: pixels are encoded as deep features, those features are warped via Eulerian motion, and the resulting warped feature maps are decoded as images. In order to produce continuous, seamlessly looping video textures, we propose a novel video looping technique that flows features both forward and backward in time and then blends the results. We demonstrate the effectiveness and robustness of our method by applying it to a large collection of examples including beaches, waterfalls, and flowing rivers.



### Move to See Better: Self-Improving Embodied Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2012.00057v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2012.00057v2)
- **Published**: 2020-11-30 19:16:51+00:00
- **Updated**: 2021-03-29 08:09:11+00:00
- **Authors**: Zhaoyuan Fang, Ayush Jain, Gabriel Sarch, Adam W. Harley, Katerina Fragkiadaki
- **Comment**: First three authors contributed equally. Project Page:
  https://ayushjain1144.github.io/SeeingByMoving/
- **Journal**: None
- **Summary**: Passive methods for object detection and segmentation treat images of the same scene as individual samples and do not exploit object permanence across multiple views. Generalization to novel or difficult viewpoints thus requires additional training with lots of annotations. In contrast, humans often recognize objects by simply moving around, to get more informative viewpoints. In this paper, we propose a method for improving object detection in testing environments, assuming nothing but an embodied agent with a pre-trained 2D object detector. Our agent collects multi-view data, generates 2D and 3D pseudo-labels, and fine-tunes its detector in a self-supervised manner. Experiments on both indoor and outdoor datasets show that (1) our method obtains high-quality 2D and 3D pseudo-labels from multi-view RGB-D data; (2) fine-tuning with these pseudo-labels improves the 2D detector significantly in the test environment; (3) training a 3D detector with our pseudo-labels outperforms a prior self-supervised method by a large margin; (4) given weak supervision, our method can generate better pseudo-labels for novel objects.



### Locally Linear Attributes of ReLU Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2012.01940v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2012.01940v1)
- **Published**: 2020-11-30 19:31:23+00:00
- **Updated**: 2020-11-30 19:31:23+00:00
- **Authors**: Ben Sattelberg, Renzo Cavalieri, Michael Kirby, Chris Peterson, Ross Beveridge
- **Comment**: 18 pages, 12 figures, 2 tables, submitted to SIMODS
- **Journal**: None
- **Summary**: A ReLU neural network determines/is a continuous piecewise linear map from an input space to an output space. The weights in the neural network determine a decomposition of the input space into convex polytopes and on each of these polytopes the network can be described by a single affine mapping. The structure of the decomposition, together with the affine map attached to each polytope, can be analyzed to investigate the behavior of the associated neural network.



### Ultrasound Diagnosis of COVID-19: Robustness and Explainability
- **Arxiv ID**: http://arxiv.org/abs/2012.01145v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2012.01145v1)
- **Published**: 2020-11-30 20:26:39+00:00
- **Updated**: 2020-11-30 20:26:39+00:00
- **Authors**: Jay Roberts, Theodoros Tsiligkaridis
- **Comment**: None
- **Journal**: None
- **Summary**: Diagnosis of COVID-19 at point of care is vital to the containment of the global pandemic. Point of care ultrasound (POCUS) provides rapid imagery of lungs to detect COVID-19 in patients in a repeatable and cost effective way. Previous work has used public datasets of POCUS videos to train an AI model for diagnosis that obtains high sensitivity. Due to the high stakes application we propose the use of robust and explainable techniques. We demonstrate experimentally that robust models have more stable predictions and offer improved interpretability. A framework of contrastive explanations based on adversarial perturbations is used to explain model predictions that aligns with human visual perception.



### Prototype-based Incremental Few-Shot Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2012.01415v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.01415v2)
- **Published**: 2020-11-30 20:45:56+00:00
- **Updated**: 2021-10-18 16:51:31+00:00
- **Authors**: Fabio Cermelli, Massimiliano Mancini, Yongqin Xian, Zeynep Akata, Barbara Caputo
- **Comment**: Accepted at BMVC 2021 (Poster)
- **Journal**: None
- **Summary**: Semantic segmentation models have two fundamental weaknesses: i) they require large training sets with costly pixel-level annotations, and ii) they have a static output space, constrained to the classes of the training set. Toward addressing both problems, we introduce a new task, Incremental Few-Shot Segmentation (iFSS). The goal of iFSS is to extend a pretrained segmentation model with new classes from few annotated images and without access to old training data. To overcome the limitations of existing models iniFSS, we propose Prototype-based Incremental Few-Shot Segmentation (PIFS) that couples prototype learning and knowledge distillation. PIFS exploits prototypes to initialize the classifiers of new classes, fine-tuning the network to refine its features representation. We design a prototype-based distillation loss on the scores of both old and new class prototypes to avoid overfitting and forgetting, and batch-renormalization to cope with non-i.i.d.few-shot data. We create an extensive benchmark for iFSS showing that PIFS outperforms several few-shot and incremental learning methods in all scenarios.



### Nothing But Geometric Constraints: A Model-Free Method for Articulated Object Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2012.00088v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2012.00088v1)
- **Published**: 2020-11-30 20:46:48+00:00
- **Updated**: 2020-11-30 20:46:48+00:00
- **Authors**: Qihao Liu, Weichao Qiu, Weiyao Wang, Gregory D. Hager, Alan L. Yuille
- **Comment**: 10 pages, 3 figures
- **Journal**: None
- **Summary**: We propose an unsupervised vision-based system to estimate the joint configurations of the robot arm from a sequence of RGB or RGB-D images without knowing the model a priori, and then adapt it to the task of category-independent articulated object pose estimation. We combine a classical geometric formulation with deep learning and extend the use of epipolar constraint to multi-rigid-body systems to solve this task. Given a video sequence, the optical flow is estimated to get the pixel-wise dense correspondences. After that, the 6D pose is computed by a modified PnP algorithm. The key idea is to leverage the geometric constraints and the constraint between multiple frames. Furthermore, we build a synthetic dataset with different kinds of robots and multi-joint articulated objects for the research of vision-based robot control and robotic vision. We demonstrate the effectiveness of our method on three benchmark datasets and show that our method achieves higher accuracy than the state-of-the-art supervised methods in estimating joint angles of robot arms and articulated objects.



### Dynamic Image for 3D MRI Image Alzheimer's Disease Classification
- **Arxiv ID**: http://arxiv.org/abs/2012.00119v1
- **DOI**: 10.1007/978-3-030-66415-2_23
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2012.00119v1)
- **Published**: 2020-11-30 21:39:32+00:00
- **Updated**: 2020-11-30 21:39:32+00:00
- **Authors**: Xin Xing, Gongbo Liang, Hunter Blanton, Muhammad Usman Rafique, Chris Wang, Ai-Ling Lin, Nathan Jacobs
- **Comment**: Accepted to ECCV2020 Workshop on BioImage Computing
- **Journal**: None
- **Summary**: We propose to apply a 2D CNN architecture to 3D MRI image Alzheimer's disease classification. Training a 3D convolutional neural network (CNN) is time-consuming and computationally expensive. We make use of approximate rank pooling to transform the 3D MRI image volume into a 2D image to use as input to a 2D CNN. We show our proposed CNN model achieves $9.5\%$ better Alzheimer's disease classification accuracy than the baseline 3D models. We also show that our method allows for efficient training, requiring only 20% of the training time compared to 3D CNN models. The code is available online: https://github.com/UkyVision/alzheimer-project.



### Model Adaptation for Inverse Problems in Imaging
- **Arxiv ID**: http://arxiv.org/abs/2012.00139v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2012.00139v2)
- **Published**: 2020-11-30 22:19:59+00:00
- **Updated**: 2021-04-12 20:38:44+00:00
- **Authors**: Davis Gilton, Gregory Ongie, Rebecca Willett
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks have been applied successfully to a wide variety of inverse problems arising in computational imaging. These networks are typically trained using a forward model that describes the measurement process to be inverted, which is often incorporated directly into the network itself. However, these approaches are sensitive to changes in the forward model: if at test time the forward model varies (even slightly) from the one the network was trained for, the reconstruction performance can degrade substantially. Given a network trained to solve an initial inverse problem with a known forward model, we propose two novel procedures that adapt the network to a change in the forward model, even without full knowledge of the change. Our approaches do not require access to more labeled data (i.e., ground truth images). We show these simple model adaptation approaches achieve empirical success in a variety of inverse problems, including deblurring, super-resolution, and undersampled image reconstruction in magnetic resonance imaging.



### Improved Diagnosis of Tibiofemoral Cartilage Defects on MRI Images Using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2012.00144v1
- **DOI**: 10.1016/j.jcjp.2021.100009
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2012.00144v1)
- **Published**: 2020-11-30 22:50:37+00:00
- **Updated**: 2020-11-30 22:50:37+00:00
- **Authors**: Gergo Merkely, Alireza Borjali, Molly Zgoda, Evan M. Farina, Simon Gortz, Orhun Muratoglu, Christian Lattermann, Kartik M. Varadarajan
- **Comment**: None
- **Journal**: https://doi.org/10.1016/j.jcjp.2021.100009
- **Summary**: Background: MRI is the modality of choice for cartilage imaging; however, its diagnostic performance is variable and significantly lower than the gold standard diagnostic knee arthroscopy. In recent years, deep learning has been used to automatically interpret medical images to improve diagnostic accuracy and speed. Purpose: The primary purpose of this study was to evaluate whether deep learning applied to the interpretation of knee MRI images can be utilized to identify cartilage defects accurately. Methods: We analyzed data from patients who underwent knee MRI evaluation and consequently had arthroscopic knee surgery (207 with cartilage defect, 90 without cartilage defect). Patients' arthroscopic findings were compared to preoperative MRI images to verify the presence or absence of isolated tibiofemoral cartilage defects. We developed three convolutional neural networks (CNNs) to analyze the MRI images and implemented image-specific saliency maps to visualize the CNNs' decision-making process. To compare the CNNs' performance against human interpretation, the same test dataset images were provided to an experienced orthopaedic surgeon and an orthopaedic resident. Results: Saliency maps demonstrated that the CNNs learned to focus on the clinically relevant areas of the tibiofemoral articular cartilage on MRI images during the decision-making processes. One CNN achieved higher performance than the orthopaedic surgeon, with two more accurate diagnoses made by the CNN. All the CNNs outperformed the orthopaedic resident. Conclusion: CNN can be used to enhance the diagnostic performance of MRI in identifying isolated tibiofemoral cartilage defects and may replace diagnostic knee arthroscopy in certain cases in the future.



### MUSCLE: Strengthening Semi-Supervised Learning Via Concurrent Unsupervised Learning Using Mutual Information Maximization
- **Arxiv ID**: http://arxiv.org/abs/2012.00150v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2012.00150v1)
- **Published**: 2020-11-30 23:01:04+00:00
- **Updated**: 2020-11-30 23:01:04+00:00
- **Authors**: Hanchen Xie, Mohamed E. Hussein, Aram Galstyan, Wael Abd-Almageed
- **Comment**: 10 pages, 3 figures, Accepted to WACV2021
- **Journal**: None
- **Summary**: Deep neural networks are powerful, massively parameterized machine learning models that have been shown to perform well in supervised learning tasks. However, very large amounts of labeled data are usually needed to train deep neural networks. Several semi-supervised learning approaches have been proposed to train neural networks using smaller amounts of labeled data with a large amount of unlabeled data. The performance of these semi-supervised methods significantly degrades as the size of labeled data decreases. We introduce Mutual-information-based Unsupervised & Semi-supervised Concurrent LEarning (MUSCLE), a hybrid learning approach that uses mutual information to combine both unsupervised and semi-supervised learning. MUSCLE can be used as a stand-alone training scheme for neural networks, and can also be incorporated into other learning approaches. We show that the proposed hybrid model outperforms state of the art on several standard benchmarks, including CIFAR-10, CIFAR-100, and Mini-Imagenet. Furthermore, the performance gain consistently increases with the reduction in the amount of labeled data, as well as in the presence of bias. We also show that MUSCLE has the potential to boost the classification performance when used in the fine-tuning phase for a model pre-trained only on unlabeled data.



### Deconstructing the Structure of Sparse Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2012.00172v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, I.2.6; I.5.1
- **Links**: [PDF](http://arxiv.org/pdf/2012.00172v1)
- **Published**: 2020-11-30 23:51:33+00:00
- **Updated**: 2020-11-30 23:51:33+00:00
- **Authors**: Maxwell Van Gelder, Mitchell Wortsman, Kiana Ehsani
- **Comment**: 6 pages, 4 figures, Accepted to ML-Retrospectives, Surveys &
  Meta-Analyses @ NeurIPS 2020 Workshop
- **Journal**: None
- **Summary**: Although sparse neural networks have been studied extensively, the focus has been primarily on accuracy. In this work, we focus instead on network structure, and analyze three popular algorithms. We first measure performance when structure persists and weights are reset to a different random initialization, thereby extending experiments in Deconstructing Lottery Tickets (Zhou et al., 2019). This experiment reveals that accuracy can be derived from structure alone. Second, to measure structural robustness we investigate the sensitivity of sparse neural networks to further pruning after training, finding a stark contrast between algorithms. Finally, for a recent dynamic sparsity algorithm we investigate how early in training the structure emerges. We find that even after one epoch the structure is mostly determined, allowing us to propose a more efficient algorithm which does not require dense gradients throughout training. In looking back at algorithms for sparse neural networks and analyzing their performance from a different lens, we uncover several interesting properties and promising directions for future research.



