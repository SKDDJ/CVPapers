# Arxiv Papers in cs.CV on 2020-11-17
### CG-Net: Conditional GIS-aware Network for Individual Building Segmentation in VHR SAR Images
- **Arxiv ID**: http://arxiv.org/abs/2011.08362v1
- **DOI**: 10.1109/TGRS.2020.3043089
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.08362v1)
- **Published**: 2020-11-17 01:52:22+00:00
- **Updated**: 2020-11-17 01:52:22+00:00
- **Authors**: Yao Sun, Yuansheng Hua, Lichao Mou, Xiao Xiang Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: Object retrieval and reconstruction from very high resolution (VHR) synthetic aperture radar (SAR) images are of great importance for urban SAR applications, yet highly challenging owing to the complexity of SAR data. This paper addresses the issue of individual building segmentation from a single VHR SAR image in large-scale urban areas. To achieve this, we introduce building footprints from GIS data as complementary information and propose a novel conditional GIS-aware network (CG-Net). The proposed model learns multi-level visual features and employs building footprints to normalize the features for predicting building masks in the SAR image. We validate our method using a high resolution spotlight TerraSAR-X image collected over Berlin. Experimental results show that the proposed CG-Net effectively brings improvements with variant backbones. We further compare two representations of building footprints, namely complete building footprints and sensor-visible footprint segments, for our task, and conclude that the use of the former leads to better segmentation results. Moreover, we investigate the impact of inaccurate GIS data on our CG-Net, and this study shows that CG-Net is robust against positioning errors in GIS data. In addition, we propose an approach of ground truth generation of buildings from an accurate digital elevation model (DEM), which can be used to generate large-scale SAR image datasets. The segmentation results can be applied to reconstruct 3D building models at level-of-detail (LoD) 1, which is demonstrated in our experiments.



### Vis-CRF, A Classical Receptive Field Model for VISION
- **Arxiv ID**: http://arxiv.org/abs/2011.08363v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.08363v1)
- **Published**: 2020-11-17 01:52:33+00:00
- **Updated**: 2020-11-17 01:52:33+00:00
- **Authors**: Nasim Nematzadeh, David MW Powers, Trent Lewis
- **Comment**: This study reflects the core of the lead author's PhD research
  (Nematzadeh, 2018) which focused on illusions in which lines of tiles or
  squares were perceived to bend or tilt (such as Caf\'e Wall and checkerboard
  illusions)
- **Journal**: None
- **Summary**: Over the last decade, a variety of new neurophysiological experiments have led to new insights as to how, when and where retinal processing takes place, and the nature of the retinal representation encoding sent to the cortex for further processing. Based on these neurobiological discoveries, in our previous work, we provided computer simulation evidence to suggest that Geometrical illusions are explained in part, by the interaction of multiscale visual processing performed in the retina. The output of our retinal stage model, named Vis-CRF, is presented here for a sample of natural image and for several types of Tilt Illusion, in which the final tilt percept arises from multiple scale processing of Difference of Gaussians (DoG) and the perceptual interaction of foreground and background elements (Nematzadeh and Powers, 2019; Nematzadeh, 2018; Nematzadeh, Powers and Lewis, 2017; Nematzadeh, Lewis and Powers, 2015).



### Extreme Value Preserving Networks
- **Arxiv ID**: http://arxiv.org/abs/2011.08367v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.08367v1)
- **Published**: 2020-11-17 02:06:52+00:00
- **Updated**: 2020-11-17 02:06:52+00:00
- **Authors**: Mingjie Sun, Jianguo Li, Changshui Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Recent evidence shows that convolutional neural networks (CNNs) are biased towards textures so that CNNs are non-robust to adversarial perturbations over textures, while traditional robust visual features like SIFT (scale-invariant feature transforms) are designed to be robust across a substantial range of affine distortion, addition of noise, etc with the mimic of human perception nature. This paper aims to leverage good properties of SIFT to renovate CNN architectures towards better accuracy and robustness. We borrow the scale-space extreme value idea from SIFT, and propose extreme value preserving networks (EVPNets). Experiments demonstrate that EVPNets can achieve similar or better accuracy than conventional CNNs, while achieving much better robustness on a set of adversarial attacks (FGSM,PGD,etc) even without adversarial training.



### Learning Efficient GANs for Image Translation via Differentiable Masks and co-Attention Distillation
- **Arxiv ID**: http://arxiv.org/abs/2011.08382v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.08382v4)
- **Published**: 2020-11-17 02:39:19+00:00
- **Updated**: 2022-03-02 09:17:14+00:00
- **Authors**: Shaojie Li, Mingbao Lin, Yan Wang, Fei Chao, Ling Shao, Rongrong Ji
- **Comment**: Accepted by IEEE Transactions on Multimedia (IEEE TMM)
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) have been widely-used in image translation, but their high computation and storage costs impede the deployment on mobile devices. Prevalent methods for CNN compression cannot be directly applied to GANs due to the peculiarties of GAN tasks and the unstable adversarial training. To solve these, in this paper, we introduce a novel GAN compression method, termed DMAD, by proposing a Differentiable Mask and a co-Attention Distillation. The former searches for a light-weight generator architecture in a training-adaptive manner. To overcome channel inconsistency when pruning the residual connections, an adaptive cross-block group sparsity is further incorporated. The latter simultaneously distills informative attention maps from both the generator and discriminator of a pre-trained model to the searched generator, effectively stabilizing the adversarial training of our light-weight model. Experiments show that DMAD can reduce the Multiply Accumulate Operations (MACs) of CycleGAN by 13x and that of Pix2Pix by 4x while retaining a comparable performance against the full model. Our code can be available at https://github.com/SJLeo/DMAD.



### Domain Adaptation based Technique for Image Emotion Recognition using Pre-trained Facial Expression Recognition Models
- **Arxiv ID**: http://arxiv.org/abs/2011.08388v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.08388v1)
- **Published**: 2020-11-17 02:55:16+00:00
- **Updated**: 2020-11-17 02:55:16+00:00
- **Authors**: Puneet Kumar, Balasubramanian Raman
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, a domain adaptation based technique for recognizing the emotions in images containing facial, non-facial, and non-human components has been proposed. We have also proposed a novel technique to explain the proposed system's predictions in terms of Intersection Score. Image emotion recognition is useful for graphics, gaming, animation, entertainment, and cinematography. However, well-labeled large scale datasets and pre-trained models are not available for image emotion recognition. To overcome this challenge, we have proposed a deep learning approach based on an attentional convolutional network that adapts pre-trained facial expression recognition models. It detects the visual features of an image and performs emotion classification based on them. The experiments have been performed on the Flickr image dataset, and the images have been classified in 'angry,' 'happy,' 'sad,' and 'neutral' emotion classes. The proposed system has demonstrated better performance than the benchmark results with an accuracy of 63.87% for image emotion recognition. We have also analyzed the embedding plots for various emotion classes to explain the proposed system's predictions.



### Sub-clusters of Normal Data for Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/2011.08408v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.08408v1)
- **Published**: 2020-11-17 03:53:31+00:00
- **Updated**: 2020-11-17 03:53:31+00:00
- **Authors**: Gahye Lee, Seungkyu Lee
- **Comment**: None
- **Journal**: None
- **Summary**: Anomaly detection in data analysis is an interesting but still challenging research topic in real world applications. As the complexity of data dimension increases, it requires to understand the semantic contexts in its description for effective anomaly characterization. However, existing anomaly detection methods show limited performances with high dimensional data such as ImageNet. Existing studies have evaluated their performance on low dimensional, clean and well separated data set such as MNIST and CIFAR-10. In this paper, we study anomaly detection with high dimensional and complex normal data. Our observation is that, in general, anomaly data is defined by semantically explainable features which are able to be used in defining semantic sub-clusters of normal data as well. We hypothesize that if there exists reasonably good feature space semantically separating sub-clusters of given normal data, unseen anomaly also can be well distinguished in the space from the normal data. We propose to perform semantic clustering on given normal data and train a classifier to learn the discriminative feature space where anomaly detection is finally performed. Based on our careful and extensive experimental evaluations with MNIST, CIFAR-10, and ImageNet with various combinations of normal and anomaly data, we show that our anomaly detection scheme outperforms state of the art methods especially with high dimensional real world images.



### Semi-Supervised Few-Shot Atomic Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2011.08410v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.08410v1)
- **Published**: 2020-11-17 03:59:05+00:00
- **Updated**: 2020-11-17 03:59:05+00:00
- **Authors**: Xiaoyuan Ni, Sizhe Song, Yu-Wing Tai, Chi-Keung Tang
- **Comment**: 7 pages, 3 figures, 2 tables
- **Journal**: None
- **Summary**: Despite excellent progress has been made, the performance on action recognition still heavily relies on specific datasets, which are difficult to extend new action classes due to labor-intensive labeling. Moreover, the high diversity in Spatio-temporal appearance requires robust and representative action feature aggregation and attention. To address the above issues, we focus on atomic actions and propose a novel model for semi-supervised few-shot atomic action recognition. Our model features unsupervised and contrastive video embedding, loose action alignment, multi-head feature comparison, and attention-based aggregation, together of which enables action recognition with only a few training examples through extracting more representative features and allowing flexibility in spatial and temporal alignment and variations in the action. Experiments show that our model can attain high accuracy on representative atomic action datasets outperforming their respective state-of-the-art classification accuracy in full supervision setting.



### Quantifying Sources of Uncertainty in Deep Learning-Based Image Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2011.08413v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.08413v2)
- **Published**: 2020-11-17 04:12:52+00:00
- **Updated**: 2020-11-30 03:47:56+00:00
- **Authors**: Riccardo Barbano, Željko Kereta, Chen Zhang, Andreas Hauptmann, Simon Arridge, Bangti Jin
- **Comment**: None
- **Journal**: NeurIPS 2020 Workshop on Deep Learning and Inverse Problems
- **Summary**: Image reconstruction methods based on deep neural networks have shown outstanding performance, equalling or exceeding the state-of-the-art results of conventional approaches, but often do not provide uncertainty information about the reconstruction. In this work we propose a scalable and efficient framework to simultaneously quantify aleatoric and epistemic uncertainties in learned iterative image reconstruction. We build on a Bayesian deep gradient descent method for quantifying epistemic uncertainty, and incorporate the heteroscedastic variance of the noise to account for the aleatoric uncertainty. We show that our method exhibits competitive performance against conventional benchmarks for computed tomography with both sparse view and limited angle data. The estimated uncertainty captures the variability in the reconstructions, caused by the restricted measurement model, and by missing information, due to the limited angle geometry.



### Transducer Adaptive Ultrasound Volume Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2011.08419v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.08419v1)
- **Published**: 2020-11-17 04:46:57+00:00
- **Updated**: 2020-11-17 04:46:57+00:00
- **Authors**: Hengtao Guo, Sheng Xu, Bradford J. Wood, Pingkun Yan
- **Comment**: In submission to ISBI 2021
- **Journal**: None
- **Summary**: Reconstructed 3D ultrasound volume provides more context information compared to a sequence of 2D scanning frames, which is desirable for various clinical applications such as ultrasound-guided prostate biopsy. Nevertheless, 3D volume reconstruction from freehand 2D scans is a very challenging problem, especially without the use of external tracking devices. Recent deep learning based methods demonstrate the potential of directly estimating inter-frame motion between consecutive ultrasound frames. However, such algorithms are specific to particular transducers and scanning trajectories associated with the training data, which may not be generalized to other image acquisition settings. In this paper, we tackle the data acquisition difference as a domain shift problem and propose a novel domain adaptation strategy to adapt deep learning algorithms to data acquired with different transducers. Specifically, feature extractors that generate transducer-invariant features from different datasets are trained by minimizing the discrepancy between deep features of paired samples in a latent space. Our results show that the proposed domain adaptation method can successfully align different feature distributions while preserving the transducer-specific information for universal freehand ultrasound volume reconstruction.



### Assistive Diagnostic Tool for Brain Tumor Detection using Computer Vision
- **Arxiv ID**: http://arxiv.org/abs/2011.08185v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2011.08185v1)
- **Published**: 2020-11-17 04:58:33+00:00
- **Updated**: 2020-11-17 04:58:33+00:00
- **Authors**: Sahithi Ankireddy
- **Comment**: Accepted and presented at 2020 IEEE MIT URTC
- **Journal**: None
- **Summary**: Today, over 700,000 people are living with brain tumors in the United States. Brain tumors can spread very quickly to other parts of the brain and the spinal cord unless necessary preventive action is taken. Thus, the survival rate for this disease is less than 40% for both men and women. A conclusive and early diagnosis of a brain tumor could be the difference between life and death for some. However, brain tumor detection and segmentation are tedious and time-consuming processes as it can only be done by radiologists and clinical experts. The use of computer vision techniques, such as Mask R Convolutional Neural Network (Mask R CNN), to detect and segment brain tumors can mitigate the possibility of human error while increasing prediction accuracy rates. The goal of this project is to create an assistive diagnostics tool for brain tumor detection and segmentation. Transfer learning was used with the Mask R CNN, and necessary parameters were accordingly altered, as a starting point. The model was trained with 20 epochs and later tested. The prediction segmentation matched 90% with the ground truth. This suggests that the model was able to perform at a high level. Once the model was finalized, the application running on Flask was created. The application will serve as a tool for medical professionals. It allows doctors to upload patient brain tumor MRI images in order to receive immediate results on the diagnosis and segmentation for each patient.



### AdCo: Adversarial Contrast for Efficient Learning of Unsupervised Representations from Self-Trained Negative Adversaries
- **Arxiv ID**: http://arxiv.org/abs/2011.08435v5
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.08435v5)
- **Published**: 2020-11-17 05:45:46+00:00
- **Updated**: 2021-03-05 07:01:19+00:00
- **Authors**: Qianjiang Hu, Xiao Wang, Wei Hu, Guo-Jun Qi
- **Comment**: Appendices with more results on symmetric loss, different numbers of
  negative samples, computing costs is presented. We also discuss "whether we
  still need negative examples" in Appendix C, a question emerging from the
  comparison with the BYOL. The source code is also available at
  https://github.com/maple-research-lab/AdCo/
- **Journal**: IEEE/CVF Conference on Computer Vision and Pattern Recognition
  (CVPR), June 19th - June 25th, 2021
- **Summary**: Contrastive learning relies on constructing a collection of negative examples that are sufficiently hard to discriminate against positive queries when their representations are self-trained. Existing contrastive learning methods either maintain a queue of negative samples over minibatches while only a small portion of them are updated in an iteration, or only use the other examples from the current minibatch as negatives. They could not closely track the change of the learned representation over iterations by updating the entire queue as a whole, or discard the useful information from the past minibatches. Alternatively, we present to directly learn a set of negative adversaries playing against the self-trained representation. Two players, the representation network and negative adversaries, are alternately updated to obtain the most challenging negative examples against which the representation of positive queries will be trained to discriminate. We further show that the negative adversaries are updated towards a weighted combination of positive queries by maximizing the adversarial contrastive loss, thereby allowing them to closely track the change of representations over time. Experiment results demonstrate the proposed Adversarial Contrastive (AdCo) model not only achieves superior performances (a top-1 accuracy of 73.2\% over 200 epochs and 75.7\% over 800 epochs with linear evaluation on ImageNet), but also can be pre-trained more efficiently with fewer epochs.



### EvoPose2D: Pushing the Boundaries of 2D Human Pose Estimation using Accelerated Neuroevolution with Weight Transfer
- **Arxiv ID**: http://arxiv.org/abs/2011.08446v2
- **DOI**: 10.1109/ACCESS.2021.3118207
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2011.08446v2)
- **Published**: 2020-11-17 05:56:16+00:00
- **Updated**: 2021-10-04 19:42:10+00:00
- **Authors**: William McNally, Kanav Vats, Alexander Wong, John McPhee
- **Comment**: None
- **Journal**: None
- **Summary**: Neural architecture search has proven to be highly effective in the design of efficient convolutional neural networks that are better suited for mobile deployment than hand-designed networks. Hypothesizing that neural architecture search holds great potential for human pose estimation, we explore the application of neuroevolution, a form of neural architecture search inspired by biological evolution, in the design of 2D human pose networks for the first time. Additionally, we propose a new weight transfer scheme that enables us to accelerate neuroevolution in a flexible manner. Our method produces network designs that are more efficient and more accurate than state-of-the-art hand-designed networks. In fact, the generated networks process images at higher resolutions using less computation than previous hand-designed networks at lower resolutions, allowing us to push the boundaries of 2D human pose estimation. Our base network designed via neuroevolution, which we refer to as EvoPose2D-S, achieves comparable accuracy to SimpleBaseline while being 50% faster and 12.7x smaller in terms of file size. Our largest network, EvoPose2D-L, achieves new state-of-the-art accuracy on the Microsoft COCO Keypoints benchmark, is 4.3x smaller than its nearest competitor, and has similar inference speed. The code is publicly available at https://github.com/wmcnally/evopose2d.



### SRF-GAN: Super-Resolved Feature GAN for Multi-Scale Representation
- **Arxiv ID**: http://arxiv.org/abs/2011.08459v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.08459v1)
- **Published**: 2020-11-17 06:27:32+00:00
- **Updated**: 2020-11-17 06:27:32+00:00
- **Authors**: Seong-Ho Lee, Seung-Hwan Bae
- **Comment**: 10 pages, 2figures
- **Journal**: None
- **Summary**: Recent convolutional object detectors exploit multi-scale feature representations added with top-down pathway in order to detect objects at different scales and learn stronger semantic feature responses. In general, during the top-down feature propagation, the coarser feature maps are upsampled to be combined with the features forwarded from bottom-up pathway, and the combined stronger semantic features are inputs of detector's headers. However, simple interpolation methods (e.g. nearest neighbor and bilinear) are still used for increasing feature resolutions although they cause noisy and blurred features. In this paper, we propose a novel generator for super-resolving features of the convolutional object detectors. To achieve this, we first design super-resolved feature GAN (SRF-GAN) consisting of a detection-based generator and a feature patch discriminator. In addition, we present SRF-GAN losses for generating the high quality of super-resolved features and improving detection accuracy together. Our SRF generator can substitute for the traditional interpolation methods, and easily fine-tuned combined with other conventional detectors. To prove this, we have implemented our SRF-GAN by using the several recent one-stage and two-stage detectors, and improved detection accuracy over those detectors. Code is available at https://github.com/SHLee-cv/SRF-GAN.



### Exploring intermediate representation for monocular vehicle pose estimation
- **Arxiv ID**: http://arxiv.org/abs/2011.08464v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2011.08464v5)
- **Published**: 2020-11-17 06:30:51+00:00
- **Updated**: 2021-07-12 12:09:45+00:00
- **Authors**: Shichao Li, Zengqiang Yan, Hongyang Li, Kwang-Ting Cheng
- **Comment**: CVPR 2021 with supplementary materials
- **Journal**: None
- **Summary**: We present a new learning-based framework to recover vehicle pose in SO(3) from a single RGB image. In contrast to previous works that map from local appearance to observation angles, we explore a progressive approach by extracting meaningful Intermediate Geometrical Representations (IGRs) to estimate egocentric vehicle orientation. This approach features a deep model that transforms perceived intensities to IGRs, which are mapped to a 3D representation encoding object orientation in the camera coordinate system. Core problems are what IGRs to use and how to learn them more effectively. We answer the former question by designing IGRs based on an interpolated cuboid that derives from primitive 3D annotation readily. The latter question motivates us to incorporate geometry knowledge with a new loss function based on a projective invariant. This loss function allows unlabeled data to be used in the training stage to improve representation learning. Without additional labels, our system outperforms previous monocular RGB-based methods for joint vehicle detection and pose estimation on the KITTI benchmark, achieving performance even comparable to stereo methods. Code and pre-trained models are available at this https URL.



### Unsupervised BatchNorm Adaptation (UBNA): A Domain Adaptation Method for Semantic Segmentation Without Using Source Domain Representations
- **Arxiv ID**: http://arxiv.org/abs/2011.08502v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.08502v2)
- **Published**: 2020-11-17 08:37:40+00:00
- **Updated**: 2021-11-11 14:38:12+00:00
- **Authors**: Marvin Klingner, Jan-Aike Termöhlen, Jacob Ritterbach, Tim Fingscheidt
- **Comment**: Accepted to WACV DNOW Workshop
- **Journal**: None
- **Summary**: In this paper we present a solution to the task of "unsupervised domain adaptation (UDA) of a given pre-trained semantic segmentation model without relying on any source domain representations". Previous UDA approaches for semantic segmentation either employed simultaneous training of the model in the source and target domains, or they relied on an additional network, replaying source domain knowledge to the model during adaptation. In contrast, we present our novel Unsupervised BatchNorm Adaptation (UBNA) method, which adapts a given pre-trained model to an unseen target domain without using -- beyond the existing model parameters from pre-training -- any source domain representations (neither data, nor networks) and which can also be applied in an online setting or using just a few unlabeled images from the target domain in a few-shot manner. Specifically, we partially adapt the normalization layer statistics to the target domain using an exponentially decaying momentum factor, thereby mixing the statistics from both domains. By evaluation on standard UDA benchmarks for semantic segmentation we show that this is superior to a model without adaptation and to baseline approaches using statistics from the target domain only. Compared to standard UDA approaches we report a trade-off between performance and usage of source domain representations.



### Digging Deeper into CRNN Model in Chinese Text Images Recognition
- **Arxiv ID**: http://arxiv.org/abs/2011.08505v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.08505v1)
- **Published**: 2020-11-17 08:46:37+00:00
- **Updated**: 2020-11-17 08:46:37+00:00
- **Authors**: Kunhong Yu, Yuze Zhang
- **Comment**: 16 pages, 10 figures
- **Journal**: None
- **Summary**: Automatic text image recognition is a prevalent application in computer vision field. One efficient way is use Convolutional Recurrent Neural Network(CRNN) to accomplish task in an end-to-end(End2End) fashion. However, CRNN notoriously fails to detect multi-row images and excel-like images. In this paper, we present one alternative to first recognize single-row images, then extend the same architecture to recognize multi-row images with proposed multiple methods. To recognize excel-like images containing box lines, we propose Line-Deep Denoising Convolutional AutoEncoder(Line-DDeCAE) to recover box lines. Finally, we present one Knowledge Distillation(KD) method to compress original CRNN model without loss of generality. To carry out experiments, we first generate artificial samples from one Chinese novel book, then conduct various experiments to verify our methods.



### Generalized Continual Zero-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2011.08508v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.08508v3)
- **Published**: 2020-11-17 08:47:54+00:00
- **Updated**: 2021-02-01 02:28:33+00:00
- **Authors**: Chandan Gautam, Sethupathy Parameswaran, Ashish Mishra, Suresh Sundaram
- **Comment**: Zero-shot Learning, Continual Learning, Incremental Learning
- **Journal**: None
- **Summary**: Recently, zero-shot learning (ZSL) emerged as an exciting topic and attracted a lot of attention. ZSL aims to classify unseen classes by transferring the knowledge from seen classes to unseen classes based on the class description. Despite showing promising performance, ZSL approaches assume that the training samples from all seen classes are available during the training, which is practically not feasible. To address this issue, we propose a more generalized and practical setup for ZSL, i.e., continual ZSL (CZSL), where classes arrive sequentially in the form of a task and it actively learns from the changing environment by leveraging the past experience. Further, to enhance the reliability, we develop CZSL for a single head continual learning setting where task identity is revealed during the training process but not during the testing. To avoid catastrophic forgetting and intransigence, we use knowledge distillation and storing and replay the few samples from previous tasks using a small episodic memory. We develop baselines and evaluate generalized CZSL on five ZSL benchmark datasets for two different settings of continual learning: with and without class incremental. Moreover, CZSL is developed for two types of variational autoencoders, which generates two types of features for classification: (i) generated features at output space and (ii) generated discriminative features at the latent space. The experimental results clearly indicate the single head CZSL is more generalizable and suitable for practical applications.



### A Digital Image Processing Approach for Hepatic Diseases Staging based on the Glisson's Capsule
- **Arxiv ID**: http://arxiv.org/abs/2011.08513v1
- **DOI**: 10.1109/ICECIE50279.2020.9309633
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2011.08513v1)
- **Published**: 2020-11-17 09:05:54+00:00
- **Updated**: 2020-11-17 09:05:54+00:00
- **Authors**: Marco Trombini, Paolo Borro, Sebastiano Ziola, Silvana Dellepiane
- **Comment**: Paper accepted with content unaltered to publish with IEEE Xplore
  Digital Library - 2020 IEEE Malaysian International Biomedical Conference
  (MIBEC)
- **Journal**: None
- **Summary**: Due to the need for quick and effective treatments for liver diseases, which are among the most common health problems in the world, staging fibrosis through non-invasive and economic methods has become of great importance. Taking inspiration from diagnostic laparoscopy, used in the past for hepatic diseases, in this paper ultrasound images of the liver are studied, focusing on a specific region of the organ where the Glisson's capsule is visible. In ultrasound images, the Glisson's capsule appears in the shape of a line which can be extracted via classical methods in literature. By making use of a combination of standard image processing techniques and Convolutional Neural Network approaches, the scope of this work is to give evidence to the idea that a great informative potential relies on smoothness of the Glisson's capsule surface. To this purpose, several classifiers are taken into consideration, which deal with different type of data, namely ultrasound images, binary images depicting the Glisson's line, and features vector extracted from the original image. This is a preliminary study that has been retrospectively conducted, based on the results of the elastosonography examination.



### ACSC: Automatic Calibration for Non-repetitive Scanning Solid-State LiDAR and Camera Systems
- **Arxiv ID**: http://arxiv.org/abs/2011.08516v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.08516v1)
- **Published**: 2020-11-17 09:11:28+00:00
- **Updated**: 2020-11-17 09:11:28+00:00
- **Authors**: Jiahe Cui, Jianwei Niu, Zhenchao Ouyang, Yunxiang He, Dian Liu
- **Comment**: conference
- **Journal**: None
- **Summary**: Recently, the rapid development of Solid-State LiDAR (SSL) enables low-cost and efficient obtainment of 3D point clouds from the environment, which has inspired a large quantity of studies and applications. However, the non-uniformity of its scanning pattern, and the inconsistency of the ranging error distribution bring challenges to its calibration task. In this paper, we proposed a fully automatic calibration method for the non-repetitive scanning SSL and camera systems. First, a temporal-spatial-based geometric feature refinement method is presented, to extract effective features from SSL point clouds; then, the 3D corners of the calibration target (a printed checkerboard) are estimated with the reflectance distribution of points. Based on the above, a target-based extrinsic calibration method is finally proposed. We evaluate the proposed method on different types of LiDAR and camera sensor combinations in real conditions, and achieve accuracy and robustness calibration results. The code is available at https://github.com/HViktorTsoi/ACSC.git .



### Bridging the Reality Gap for Pose Estimation Networks using Sensor-Based Domain Randomization
- **Arxiv ID**: http://arxiv.org/abs/2011.08517v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.08517v3)
- **Published**: 2020-11-17 09:12:11+00:00
- **Updated**: 2021-08-17 09:50:01+00:00
- **Authors**: Frederik Hagelskjaer, Anders Glent Buch
- **Comment**: 10 pages, 5 figures, 7 tables
- **Journal**: None
- **Summary**: Since the introduction of modern deep learning methods for object pose estimation, test accuracy and efficiency has increased significantly. For training, however, large amounts of annotated training data are required for good performance. While the use of synthetic training data prevents the need for manual annotation, there is currently a large performance gap between methods trained on real and synthetic data. This paper introduces a new method, which bridges this gap.   Most methods trained on synthetic data use 2D images, as domain randomization in 2D is more developed. To obtain precise poses, many of these methods perform a final refinement using 3D data. Our method integrates the 3D data into the network to increase the accuracy of the pose estimation. To allow for domain randomization in 3D, a sensor-based data augmentation has been developed. Additionally, we introduce the SparseEdge feature, which uses a wider search space during point cloud propagation to avoid relying on specific features without increasing run-time.   Experiments on three large pose estimation benchmarks show that the presented method outperforms previous methods trained on synthetic data and achieves comparable results to existing methods trained on real data.



### DeepSeqSLAM: A Trainable CNN+RNN for Joint Global Description and Sequence-based Place Recognition
- **Arxiv ID**: http://arxiv.org/abs/2011.08518v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2011.08518v1)
- **Published**: 2020-11-17 09:14:02+00:00
- **Updated**: 2020-11-17 09:14:02+00:00
- **Authors**: Marvin Chancán, Michael Milford
- **Comment**: 9 pages, 6 figures, 2 tables
- **Journal**: NeurIPS 2020 Workshop on Machine Learning for Autonomous Driving
  (ML4AD)
- **Summary**: Sequence-based place recognition methods for all-weather navigation are well-known for producing state-of-the-art results under challenging day-night or summer-winter transitions. These systems, however, rely on complex handcrafted heuristics for sequential matching - which are applied on top of a pre-computed pairwise similarity matrix between reference and query image sequences of a single route - to further reduce false-positive rates compared to single-frame retrieval methods. As a result, performing multi-frame place recognition can be extremely slow for deployment on autonomous vehicles or evaluation on large datasets, and fail when using relatively short parameter values such as a sequence length of 2 frames. In this paper, we propose DeepSeqSLAM: a trainable CNN+RNN architecture for jointly learning visual and positional representations from a single monocular image sequence of a route. We demonstrate our approach on two large benchmark datasets, Nordland and Oxford RobotCar - recorded over 728 km and 10 km routes, respectively, each during 1 year with multiple seasons, weather, and lighting conditions. On Nordland, we compare our method to two state-of-the-art sequence-based methods across the entire route under summer-winter changes using a sequence length of 2 and show that our approach can get over 72% AUC compared to 27% AUC for Delta Descriptors and 2% AUC for SeqSLAM; while drastically reducing the deployment time from around 1 hour to 1 minute against both. The framework code and video are available at https://mchancan.github.io/deepseqslam



### Building Movie Map -- A Tool for Exploring Areas in a City -- and its Evaluation
- **Arxiv ID**: http://arxiv.org/abs/2011.08525v1
- **DOI**: 10.1145/3394171.3413881
- **Categories**: **cs.MM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.08525v1)
- **Published**: 2020-11-17 09:24:05+00:00
- **Updated**: 2020-11-17 09:24:05+00:00
- **Authors**: Naoki Sugimoto, Yoshihito Ebine, Kiyoharu Aizawa
- **Comment**: None
- **Journal**: ACM Multimedia 2020
- **Summary**: We propose a new Movie Map system, with an interface for exploring cities. The system consists of four stages; acquisition, analysis, management, and interaction. In the acquisition stage, omnidirectional videos are taken along streets in target areas. Frames of the video are localized on the map, intersections are detected, and videos are segmented. Turning views at intersections are subsequently generated. By connecting the video segments following the specified movement in an area, we can view the streets better. The interface allows for easy exploration of a target area, and it can show virtual billboards of stores in the view. We conducted user studies to compare our system to the GSV in a scenario where users could freely move and explore to find a landmark. The experiment showed that our system had a better user experience than GSV.



### Decision and Feature Level Fusion of Deep Features Extracted from Public COVID-19 Data-sets
- **Arxiv ID**: http://arxiv.org/abs/2011.08528v1
- **DOI**: 10.1007/s10489-021-02945-8
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.08528v1)
- **Published**: 2020-11-17 09:36:21+00:00
- **Updated**: 2020-11-17 09:36:21+00:00
- **Authors**: Hamza Osman Ilhan, Gorkem Serbes, Nizamettin Aydin
- **Comment**: 20 Pages, 9 Figures, 4 Tables and submitted a journal
- **Journal**: None
- **Summary**: The Coronavirus (COVID-19), which is an infectious pulmonary disorder, has affected millions of people and has been declared as a global pandemic by the WHO. Due to highly contagious nature of COVID-19 and its high possibility of causing severe conditions in the patients, the development of rapid and accurate diagnostic tools have gained importance. The real-time reverse transcription-polymerize chain reaction (RT-PCR) is used to detect the presence of Coronavirus RNA by using the mucus and saliva mixture samples. But, RT-PCR suffers from having low-sensitivity especially in the early stage. Therefore, the usage of chest radiography has been increasing in the early diagnosis of COVID-19 due to its fast imaging speed, significantly low cost and low dosage exposure of radiation. In our study, a computer-aided diagnosis system for X-ray images based on convolutional neural networks (CNNs), which can be used by radiologists as a supporting tool in COVID-19 detection, has been proposed. Deep feature sets extracted by using CNNs were concatenated for feature level fusion and fed to multiple classifiers in terms of decision level fusion idea with the aim of discriminating COVID-19, pneumonia and no-finding classes. In the decision level fusion idea, a majority voting scheme was applied to the resultant decisions of classifiers. The obtained accuracy values and confusion matrix based evaluation criteria were presented for three progressively created data-sets. The aspects of the proposed method that are superior to existing COVID-19 detection studies have been discussed and the fusion performance of proposed approach was validated visually by using Class Activation Mapping technique. The experimental results show that the proposed approach has attained high COVID-19 detection performance that was proven by its comparable accuracy and superior precision/recall values with the existing studies.



### Slender Object Detection: Diagnoses and Improvements
- **Arxiv ID**: http://arxiv.org/abs/2011.08529v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.08529v4)
- **Published**: 2020-11-17 09:39:42+00:00
- **Updated**: 2021-04-07 02:35:15+00:00
- **Authors**: Zhaoyi Wan, Yimin Chen, Sutao Deng, Kunpeng Chen, Cong Yao, Jiebo Luo
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we are concerned with the detection of a particular type of objects with extreme aspect ratios, namely \textbf{slender objects}. In real-world scenarios, slender objects are actually very common and crucial to the objective of a detection system. However, this type of objects has been largely overlooked by previous object detection algorithms. Upon our investigation, for a classical object detection method, a drastic drop of $18.9\%$ mAP on COCO is observed, if solely evaluated on slender objects. Therefore, we systematically study the problem of slender object detection in this work. Accordingly, an analytical framework with carefully designed benchmark and evaluation protocols is established, in which different algorithms and modules can be inspected and compared. \New Our study reveals that effective slender object detection can be achieved ~\textbf{with none of} (1) anchor-based localization; (2) specially designed box representations. Instead, \textbf{the critical aspect of improving slender object detection is feature adaptation}. It identifies and extends the insights of existing methods that are previously underexploited. Furthermore, we propose a feature adaption strategy that achieves clear and consistent improvements over current representative object detection methods.



### Wavelet-based clustering for time-series trend detection
- **Arxiv ID**: http://arxiv.org/abs/2011.12111v1
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV, cs.LG, 65T60, 62H30, 62H25, I.5.3; I.5.4; I.6.5; G.1.0
- **Links**: [PDF](http://arxiv.org/pdf/2011.12111v1)
- **Published**: 2020-11-17 09:41:49+00:00
- **Updated**: 2020-11-17 09:41:49+00:00
- **Authors**: Vincent Talbo, Mehdi Haddab, Derek Aubert, Redha Moulla
- **Comment**: 10 pages, 11 figures
- **Journal**: None
- **Summary**: In this paper, we introduce a method performing clustering of time-series on the basis of their trend (increasing, stagnating/decreasing, and seasonal behavior). The clustering is performed using $k$-means method on a selection of coefficients obtained by discrete wavelet transform, reducing drastically the dimensionality. The method is applied on an use case for the clustering of a 864 daily sales revenue time-series for 61 retail shops. The results are presented for different mother wavelets. The importance of each wavelet coefficient and its level is discussed thanks to a principal component analysis along with a reconstruction of the signal from the selected wavelet coefficients.



### A Divide et Impera Approach for 3D Shape Reconstruction from Multiple Views
- **Arxiv ID**: http://arxiv.org/abs/2011.08534v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.08534v2)
- **Published**: 2020-11-17 09:59:32+00:00
- **Updated**: 2020-11-18 09:16:53+00:00
- **Authors**: Riccardo Spezialetti, David Joseph Tan, Alessio Tonioni, Keisuke Tateno, Federico Tombari
- **Comment**: Accepted to 3DV 2020 as oral
- **Journal**: None
- **Summary**: Estimating the 3D shape of an object from a single or multiple images has gained popularity thanks to the recent breakthroughs powered by deep learning. Most approaches regress the full object shape in a canonical pose, possibly extrapolating the occluded parts based on the learned priors. However, their viewpoint invariant technique often discards the unique structures visible from the input images. In contrast, this paper proposes to rely on viewpoint variant reconstructions by merging the visible information from the given views. Our approach is divided into three steps. Starting from the sparse views of the object, we first align them into a common coordinate system by estimating the relative pose between all the pairs. Then, inspired by the traditional voxel carving, we generate an occupancy grid of the object taken from the silhouette on the images and their relative poses. Finally, we refine the initial reconstruction to build a clean 3D model which preserves the details from each viewpoint. To validate the proposed method, we perform a comprehensive evaluation on the ShapeNet reference benchmark in terms of relative pose estimation and 3D shape reconstruction.



### Structural and Functional Decomposition for Personality Image Captioning in a Communication Game
- **Arxiv ID**: http://arxiv.org/abs/2011.08543v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.08543v1)
- **Published**: 2020-11-17 10:19:27+00:00
- **Updated**: 2020-11-17 10:19:27+00:00
- **Authors**: Thu Nguyen, Duy Phung, Minh Hoai, Thien Huu Nguyen
- **Comment**: 10 pages, EMNLP-Findings 2020
- **Journal**: EMNLP-Findings 2020
- **Summary**: Personality image captioning (PIC) aims to describe an image with a natural language caption given a personality trait. In this work, we introduce a novel formulation for PIC based on a communication game between a speaker and a listener. The speaker attempts to generate natural language captions while the listener encourages the generated captions to contain discriminative information about the input images and personality traits. In this way, we expect that the generated captions can be improved to naturally represent the images and express the traits. In addition, we propose to adapt the language model GPT2 to perform caption generation for PIC. This enables the speaker and listener to benefit from the language encoding capacity of GPT2. Our experiments show that the proposed model achieves the state-of-the-art performance for PIC.



### Deep Learning Based HPV Status Prediction for Oropharyngeal Cancer Patients
- **Arxiv ID**: http://arxiv.org/abs/2011.08555v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.08555v1)
- **Published**: 2020-11-17 10:40:44+00:00
- **Updated**: 2020-11-17 10:40:44+00:00
- **Authors**: Daniel M. Lang, Jan C. Peeken, Stephanie E. Combs, Jan J. Wilkens, Stefan Bartzsch
- **Comment**: 15 pages, 2 figures
- **Journal**: None
- **Summary**: We investigated the ability of deep learning models for imaging based HPV status detection. To overcome the problem of small medical datasets we used a transfer learning approach. A 3D convolutional network pre-trained on sports video clips was fine tuned such that full 3D information in the CT images could be exploited. The video pre-trained model was able to differentiate HPV-positive from HPV-negative cases with an area under the receiver operating characteristic curve (AUC) of 0.81 for an external test set. In comparison to a 3D convolutional neural network (CNN) trained from scratch and a 2D architecture pre-trained on ImageNet the video pre-trained model performed best.



### Normalized Weighting Schemes for Image Interpolation Algorithms
- **Arxiv ID**: http://arxiv.org/abs/2011.08559v6
- **DOI**: 10.3390/app13031741
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.08559v6)
- **Published**: 2020-11-17 10:47:50+00:00
- **Updated**: 2023-01-31 14:31:46+00:00
- **Authors**: Olivier Rukundo
- **Comment**: 17 pages, 15 figures, 2 Tables
- **Journal**: Applied Sciences, 13(3), 2023, 1-17
- **Summary**: Image interpolation algorithms pervade many modern image processing and analysis applications. However, when their weighting schemes inefficiently generate very unrealistic estimates, they may negatively affect the performance of the end user applications. Therefore, in this work, the author introduced four weighting schemes based on some geometric shapes for digital image interpolation operations. And, the quantity used to express the extent of each shape weight was the normalized area, especially when the sums of areas exceeded a unit square size. The introduced four weighting schemes are based on the minimum side based diameter (MD) of a regular tetragon, hypotenuse based radius (HR), the virtual pixel length based height for the area of the triangle (AT), and the virtual pixel length for hypotenuse based radius for the area of the circle (AC). At the smaller scaling ratio, the image interpolation algorithm based on the HR scheme scored the highest at 66.6 % among non traditional image interpolation algorithms presented. But, at the higher scaling ratio, the AC scheme based image interpolation algorithm scored the highest at 66.6 % among non traditional algorithms presented and, here, its image interpolation quality was generally superior or comparable to the quality of images interpolated by both non traditional and traditional algorithms.



### MGIC: Multigrid-in-Channels Neural Network Architectures
- **Arxiv ID**: http://arxiv.org/abs/2011.09128v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2011.09128v4)
- **Published**: 2020-11-17 11:29:10+00:00
- **Updated**: 2022-09-26 11:21:16+00:00
- **Authors**: Moshe Eliasof, Jonathan Ephrath, Lars Ruthotto, Eran Treister
- **Comment**: This paper supersedes arXiv:2006.06799 Accepted to SISC
- **Journal**: None
- **Summary**: We present a multigrid-in-channels (MGIC) approach that tackles the quadratic growth of the number of parameters with respect to the number of channels in standard convolutional neural networks (CNNs). Thereby our approach addresses the redundancy in CNNs that is also exposed by the recent success of lightweight CNNs. Lightweight CNNs can achieve comparable accuracy to standard CNNs with fewer parameters; however, the number of weights still scales quadratically with the CNN's width. Our MGIC architectures replace each CNN block with an MGIC counterpart that utilizes a hierarchy of nested grouped convolutions of small group size to address this.   Hence, our proposed architectures scale linearly with respect to the network's width while retaining full coupling of the channels as in standard CNNs.   Our extensive experiments on image classification, segmentation, and point cloud classification show that applying this strategy to different architectures like ResNet and MobileNetV3 reduces the number of parameters while obtaining similar or better accuracy.



### Multi Receptive Field Network for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2011.08577v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.08577v2)
- **Published**: 2020-11-17 11:52:23+00:00
- **Updated**: 2022-09-07 14:01:42+00:00
- **Authors**: Jianlong Yuan, Zelu Deng, Shu Wang, Zhenbo Luo
- **Comment**: Accept by WACV 2020
- **Journal**: None
- **Summary**: Semantic segmentation is one of the key tasks in computer vision, which is to assign a category label to each pixel in an image. Despite significant progress achieved recently, most existing methods still suffer from two challenging issues: 1) the size of objects and stuff in an image can be very diverse, demanding for incorporating multi-scale features into the fully convolutional networks (FCNs); 2) the pixels close to or at the boundaries of object/stuff are hard to classify due to the intrinsic weakness of convolutional networks. To address the first issue, we propose a new Multi-Receptive Field Module (MRFM), explicitly taking multi-scale features into account. For the second issue, we design an edge-aware loss which is effective in distinguishing the boundaries of object/stuff. With these two designs, our Multi Receptive Field Network achieves new state-of-the-art results on two widely-used semantic segmentation benchmark datasets. Specifically, we achieve a mean IoU of 83.0 on the Cityscapes dataset and 88.4 mean IoU on the Pascal VOC2012 dataset.



### Empowering Things with Intelligence: A Survey of the Progress, Challenges, and Opportunities in Artificial Intelligence of Things
- **Arxiv ID**: http://arxiv.org/abs/2011.08612v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2011.08612v1)
- **Published**: 2020-11-17 13:14:28+00:00
- **Updated**: 2020-11-17 13:14:28+00:00
- **Authors**: Jing Zhang, Dacheng Tao
- **Comment**: Accepted by IEEE Internet of Things Journal
- **Journal**: None
- **Summary**: In the Internet of Things (IoT) era, billions of sensors and devices collect and process data from the environment, transmit them to cloud centers, and receive feedback via the internet for connectivity and perception. However, transmitting massive amounts of heterogeneous data, perceiving complex environments from these data, and then making smart decisions in a timely manner are difficult. Artificial intelligence (AI), especially deep learning, is now a proven success in various areas including computer vision, speech recognition, and natural language processing. AI introduced into the IoT heralds the era of artificial intelligence of things (AIoT). This paper presents a comprehensive survey on AIoT to show how AI can empower the IoT to make it faster, smarter, greener, and safer. Specifically, we briefly present the AIoT architecture in the context of cloud computing, fog computing, and edge computing. Then, we present progress in AI research for IoT from four perspectives: perceiving, learning, reasoning, and behaving. Next, we summarize some promising applications of AIoT that are likely to profoundly reshape our world. Finally, we highlight the challenges facing AIoT and some potential research opportunities.



### Mutual Information Based Method for Unsupervised Disentanglement of Video Representation
- **Arxiv ID**: http://arxiv.org/abs/2011.08614v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.08614v1)
- **Published**: 2020-11-17 13:16:07+00:00
- **Updated**: 2020-11-17 13:16:07+00:00
- **Authors**: P Aditya Sreekar, Ujjwal Tiwari, Anoop Namboodiri
- **Comment**: None
- **Journal**: None
- **Summary**: Video Prediction is an interesting and challenging task of predicting future frames from a given set context frames that belong to a video sequence. Video prediction models have found prospective applications in Maneuver Planning, Health care, Autonomous Navigation and Simulation. One of the major challenges in future frame generation is due to the high dimensional nature of visual data. In this work, we propose Mutual Information Predictive Auto-Encoder (MIPAE) framework, that reduces the task of predicting high dimensional video frames by factorising video representations into content and low dimensional pose latent variables that are easy to predict. A standard LSTM network is used to predict these low dimensional pose representations. Content and the predicted pose representations are decoded to generate future frames. Our approach leverages the temporal structure of the latent generative factors of a video and a novel mutual information loss to learn disentangled video representations. We also propose a metric based on mutual information gap (MIG) to quantitatively access the effectiveness of disentanglement on DSprites and MPI3D-real datasets. MIG scores corroborate with the visual superiority of frames predicted by MIPAE. We also compare our method quantitatively on evaluation metrics LPIPS, SSIM and PSNR.



### Can Semantic Labels Assist Self-Supervised Visual Representation Learning?
- **Arxiv ID**: http://arxiv.org/abs/2011.08621v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.08621v1)
- **Published**: 2020-11-17 13:25:00+00:00
- **Updated**: 2020-11-17 13:25:00+00:00
- **Authors**: Longhui Wei, Lingxi Xie, Jianzhong He, Jianlong Chang, Xiaopeng Zhang, Wengang Zhou, Houqiang Li, Qi Tian
- **Comment**: 10 pages, 4 figures, 8 tables
- **Journal**: None
- **Summary**: Recently, contrastive learning has largely advanced the progress of unsupervised visual representation learning. Pre-trained on ImageNet, some self-supervised algorithms reported higher transfer learning performance compared to fully-supervised methods, seeming to deliver the message that human labels hardly contribute to learning transferrable visual features. In this paper, we defend the usefulness of semantic labels but point out that fully-supervised and self-supervised methods are pursuing different kinds of features. To alleviate this issue, we present a new algorithm named Supervised Contrastive Adjustment in Neighborhood (SCAN) that maximally prevents the semantic guidance from damaging the appearance feature embedding. In a series of downstream tasks, SCAN achieves superior performance compared to previous fully-supervised and self-supervised methods, and sometimes the gain is significant. More importantly, our study reveals that semantic labels are useful in assisting self-supervised methods, opening a new direction for the community.



### Beyond Static Features for Temporally Consistent 3D Human Pose and Shape from a Video
- **Arxiv ID**: http://arxiv.org/abs/2011.08627v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.08627v4)
- **Published**: 2020-11-17 13:41:34+00:00
- **Updated**: 2021-04-27 06:54:19+00:00
- **Authors**: Hongsuk Choi, Gyeongsik Moon, Ju Yong Chang, Kyoung Mu Lee
- **Comment**: Accepted to CVPR 2021, 10 pages
- **Journal**: None
- **Summary**: Despite the recent success of single image-based 3D human pose and shape estimation methods, recovering temporally consistent and smooth 3D human motion from a video is still challenging. Several video-based methods have been proposed; however, they fail to resolve the single image-based methods' temporal inconsistency issue due to a strong dependency on a static feature of the current frame. In this regard, we present a temporally consistent mesh recovery system (TCMR). It effectively focuses on the past and future frames' temporal information without being dominated by the current static feature. Our TCMR significantly outperforms previous video-based methods in temporal consistency with better per-frame 3D pose and shape accuracy. We also release the codes. For the demo video, see https://youtu.be/WB3nTnSQDII. For the codes, see https://github.com/hongsukchoi/TCMR_RELEASE.



### Exploring Self-Attention for Visual Odometry
- **Arxiv ID**: http://arxiv.org/abs/2011.08634v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2011.08634v1)
- **Published**: 2020-11-17 13:53:26+00:00
- **Updated**: 2020-11-17 13:53:26+00:00
- **Authors**: Hamed Damirchi, Rooholla Khorrambakht, Hamid D. Taghirad
- **Comment**: 8 pages, 7 figures, 1 table
- **Journal**: None
- **Summary**: Visual odometry networks commonly use pretrained optical flow networks in order to derive the ego-motion between consecutive frames. The features extracted by these networks represent the motion of all the pixels between frames. However, due to the existence of dynamic objects and texture-less surfaces in the scene, the motion information for every image region might not be reliable for inferring odometry due to the ineffectiveness of dynamic objects in derivation of the incremental changes in position. Recent works in this area lack attention mechanisms in their structures to facilitate dynamic reweighing of the feature maps for extracting more refined egomotion information. In this paper, we explore the effectiveness of self-attention in visual odometry. We report qualitative and quantitative results against the SOTA methods. Furthermore, saliency-based studies alongside specially designed experiments are utilized to investigate the effect of self-attention on VO. Our experiments show that using self-attention allows for the extraction of better features while achieving a better odometry performance compared to networks that lack such structures.



### A Review of Generalized Zero-Shot Learning Methods
- **Arxiv ID**: http://arxiv.org/abs/2011.08641v5
- **DOI**: 10.1109/TPAMI.2022.3191696
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.08641v5)
- **Published**: 2020-11-17 14:00:30+00:00
- **Updated**: 2022-07-13 00:21:46+00:00
- **Authors**: Farhad Pourpanah, Moloud Abdar, Yuxuan Luo, Xinlei Zhou, Ran Wang, Chee Peng Lim, Xi-Zhao Wang, Q. M. Jonathan Wu
- **Comment**: 26 pages, 12 figures
- **Journal**: None
- **Summary**: Generalized zero-shot learning (GZSL) aims to train a model for classifying data samples under the condition that some output classes are unknown during supervised learning. To address this challenging task, GZSL leverages semantic information of the seen (source) and unseen (target) classes to bridge the gap between both seen and unseen classes. Since its introduction, many GZSL models have been formulated. In this review paper, we present a comprehensive review on GZSL. Firstly, we provide an overview of GZSL including the problems and challenges. Then, we introduce a hierarchical categorization for the GZSL methods and discuss the representative methods in each category. In addition, we discuss the available benchmark data sets and applications of GZSL, along with a discussion on the research gaps and directions for future investigations.



### 3D CNNs with Adaptive Temporal Feature Resolutions
- **Arxiv ID**: http://arxiv.org/abs/2011.08652v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.08652v4)
- **Published**: 2020-11-17 14:34:05+00:00
- **Updated**: 2021-08-11 09:14:20+00:00
- **Authors**: Mohsen Fayyaz, Emad Bahrami, Ali Diba, Mehdi Noroozi, Ehsan Adeli, Luc Van Gool, Juergen Gall
- **Comment**: CVPR 2021
- **Journal**: None
- **Summary**: While state-of-the-art 3D Convolutional Neural Networks (CNN) achieve very good results on action recognition datasets, they are computationally very expensive and require many GFLOPs. While the GFLOPs of a 3D CNN can be decreased by reducing the temporal feature resolution within the network, there is no setting that is optimal for all input clips. In this work, we therefore introduce a differentiable Similarity Guided Sampling (SGS) module, which can be plugged into any existing 3D CNN architecture. SGS empowers 3D CNNs by learning the similarity of temporal features and grouping similar features together. As a result, the temporal feature resolution is not anymore static but it varies for each input video clip. By integrating SGS as an additional layer within current 3D CNNs, we can convert them into much more efficient 3D CNNs with adaptive temporal feature resolutions (ATFR). Our evaluations show that the proposed module improves the state-of-the-art by reducing the computational cost (GFLOPs) by half while preserving or even improving the accuracy. We evaluate our module by adding it to multiple state-of-the-art 3D CNNs on various datasets such as Kinetics-600, Kinetics-400, mini-Kinetics, Something-Something V2, UCF101, and HMDB51.



### Non-Local Robust Quaternion Matrix Completion for Color Images and Videos Inpainting
- **Arxiv ID**: http://arxiv.org/abs/2011.08675v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.NA, math.NA, 65F55, G.1.3
- **Links**: [PDF](http://arxiv.org/pdf/2011.08675v3)
- **Published**: 2020-11-17 14:50:40+00:00
- **Updated**: 2022-05-13 14:27:27+00:00
- **Authors**: Zhigang Jia, Qiyu Jin, Michael K. Ng, Xile Zhao
- **Comment**: 34 pages, 17 figures
- **Journal**: IEEE Transactions on Image Processing, 2022
- **Summary**: The image nonlocal self-similarity (NSS) prior refers to the fact that a local patch often has many nonlocal similar patches to it across the image and has been widely applied in many recently proposed machining learning algorithms for image processing. However, there is no theoretical analysis on its working principle in the literature. In this paper, we discover a potential causality between NSS and low-rank property of color images, which is also available to grey images. A new patch group based NSS prior scheme is proposed to learn explicit NSS models of natural color images. The numerical low-rank property of patched matrices is also rigorously proved. The NSS-based QMC algorithm computes an optimal low-rank approximation to the high-rank color image, resulting in high PSNR and SSIM measures and particularly the better visual quality. A new tensor NSS-based QMC method is also presented to solve the color video inpainting problem based on quaternion tensor representation. The numerical experiments on color images and videos indicate the advantages of NSS-based QMC over the state-of-the-art methods.



### SeekNet: Improved Human Instance Segmentation and Tracking via Reinforcement Learning Based Optimized Robot Relocation
- **Arxiv ID**: http://arxiv.org/abs/2011.08682v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2011.08682v2)
- **Published**: 2020-11-17 15:03:30+00:00
- **Updated**: 2023-03-08 15:45:20+00:00
- **Authors**: Venkatraman Narayanan, Bala Murali Manoghar, Rama Prashanth RV, Phu Pham, Aniket Bera
- **Comment**: None
- **Journal**: None
- **Summary**: Amodal recognition is the ability of the system to detect occluded objects. Most SOTA Visual Recognition systems lack the ability to perform amodal recognition. Few studies have achieved amodal recognition through passive prediction or embodied recognition approaches. However, these approaches suffer from challenges in real-world applications, such as dynamic obstacles. We propose SeekNet, an improved optimization method for amodal recognition through embodied visual recognition. Additionally, we implement SeekNet for social robots, where there are multiple interactions with crowded pedestrians. We also demonstrate the benefits of our algorithm on occluded human detection and tracking over other baselines. Additionally, we set up a multi-robot environment with SeekNet to identify and track visual disease markers for airborne disease in crowded areas. We conduct our experiments in a simulated indoor environment and show that our method enhances the overall accuracy of the amodal recognition task and achieves the largest improvement in detection accuracy over time in comparison to the baseline approaches.



### Developing an Effective and Automated Patient Engagement Estimator for Telehealth: A Machine Learning Approach
- **Arxiv ID**: http://arxiv.org/abs/2011.08690v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2011.08690v4)
- **Published**: 2020-11-17 15:18:38+00:00
- **Updated**: 2023-02-13 02:55:00+00:00
- **Authors**: Pooja Guhan, Naman Awasthi, and Kathryn McDonald, Kristin Bussell, Dinesh Manocha, Gloria Reeves, Aniket Bera
- **Comment**: None
- **Journal**: None
- **Summary**: We discuss MET, a learning-based algorithm proposed for perceiving a patient's level of engagement during telehealth sessions. We leverage latent vectors corresponding to Affective and Cognitive features frequently used in psychology literature to understand a person's level of engagement in a semi-supervised GAN-based framework. We showcase the efficacy of this method from the perspective of mental health and more specifically how this can be leveraged for a better understanding of patient engagement during telemental health sessions. To further the development of similar technologies that can be useful for telehealth, we also plan to release a dataset MEDICA containing 1299 video clips, each 3 seconds long and show experiments on the same. Our framework reports a 40% improvement in RMSE (Root Mean Squared Error) over state-of-the-art methods for engagement estimation. In our real-world tests, we also observed positive correlations between the working alliance inventory scores reported by psychotherapists. This indicates the potential of the proposed model to present patient engagement estimations that aligns well with the engagement measures used by psychotherapists.



### Pyramid Point: A Multi-Level Focusing Network for Revisiting Feature Layers
- **Arxiv ID**: http://arxiv.org/abs/2011.08692v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.08692v2)
- **Published**: 2020-11-17 15:23:27+00:00
- **Updated**: 2020-11-23 16:35:43+00:00
- **Authors**: Nina Varney, Vijayan K. Asari, Quinn Graehling
- **Comment**: 10 pages, 10 figures
- **Journal**: None
- **Summary**: We present a method to learn a diverse group of object categories from an unordered point set. We propose our Pyramid Point network, which uses a dense pyramid structure instead of the traditional 'U' shape, typically seen in semantic segmentation networks. This pyramid structure gives a second look, allowing the network to revisit different layers simultaneously, increasing the contextual information by creating additional layers with less noise. We introduce a Focused Kernel Point convolution (FKP Conv), which expands on the traditional point convolutions by adding an attention mechanism to the kernel outputs. This FKP Conv increases our feature quality and allows us to weigh the kernel outputs dynamically. These FKP Convs are the central part of our Recurrent FKP Bottleneck block, which makes up the backbone of our encoder. With this distinct network, we demonstrate competitive performance on three benchmark data sets. We also perform an ablation study to show the positive effects of each element in our FKP Conv.



### A Simple Framework to Quantify Different Types of Uncertainty in Deep Neural Networks for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2011.08712v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2011.08712v5)
- **Published**: 2020-11-17 15:36:42+00:00
- **Updated**: 2021-05-28 15:33:37+00:00
- **Authors**: Aria Khoshsirat
- **Comment**: None
- **Journal**: None
- **Summary**: Quantifying uncertainty in a model's predictions is important as it enables the safety of an AI system to be increased by acting on the model's output in an informed manner. This is crucial for applications where the cost of an error is high, such as in autonomous vehicle control, medical image analysis, financial estimations or legal fields. Deep Neural Networks are powerful predictors that have recently achieved state-of-the-art performance on a wide spectrum of tasks. Quantifying predictive uncertainty in DNNs is a challenging and yet on-going problem. In this paper we propose a complete framework to capture and quantify three known types of uncertainty in DNNs for the task of image classification. This framework includes an ensemble of CNNs for model uncertainty, a supervised reconstruction auto-encoder to capture distributional uncertainty and using the output of activation functions in the last layer of the network, to capture data uncertainty. Finally we demonstrate the efficiency of our method on popular image datasets for classification.



### RAIST: Learning Risk Aware Traffic Interactions via Spatio-Temporal Graph Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/2011.08722v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2011.08722v3)
- **Published**: 2020-11-17 15:49:22+00:00
- **Updated**: 2023-08-06 17:14:33+00:00
- **Authors**: Videsh Suman, Phu Pham, Aniket Bera
- **Comment**: To appear in 2023 IEEE/RSJ International Conference on Intelligent
  Robots and Systems (IROS 2023)
- **Journal**: None
- **Summary**: A key aspect of driving a road vehicle is to interact with other road users, assess their intentions and make risk-aware tactical decisions. An intuitive approach to enabling an intelligent automated driving system would be incorporating some aspects of human driving behavior. To this end, we propose a novel driving framework for egocentric views based on spatio-temporal traffic graphs. The traffic graphs model not only the spatial interactions amongst the road users but also their individual intentions through temporally associated message passing. We leverage a spatio-temporal graph convolutional network (ST-GCN) to train the graph edges. These edges are formulated using parameterized functions of 3D positions and scene-aware appearance features of road agents. Along with tactical behavior prediction, it is crucial to evaluate the risk-assessing ability of the proposed framework. We claim that our framework learns risk-aware representations by improving on the task of risk object identification, especially in identifying objects with vulnerable interactions like pedestrians and cyclists.



### Modality-Buffet for Real-Time Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2011.08726v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2011.08726v1)
- **Published**: 2020-11-17 15:57:06+00:00
- **Updated**: 2020-11-17 15:57:06+00:00
- **Authors**: Nicolai Dorka, Johannes Meyer, Wolfram Burgard
- **Comment**: Accepted at the 2020 IEEE/RSJ International Conference on Intelligent
  Robots and Systems (IROS)
- **Journal**: None
- **Summary**: Real-time object detection in videos using lightweight hardware is a crucial component of many robotic tasks. Detectors using different modalities and with varying computational complexities offer different trade-offs. One option is to have a very lightweight model that can predict from all modalities at once for each frame. However, in some situations (e.g., in static scenes) it might be better to have a more complex but more accurate model and to extrapolate from previous predictions for the frames coming in at processing time. We formulate this task as a sequential decision making problem and use reinforcement learning (RL) to generate a policy that decides from the RGB input which detector out of a portfolio of different object detectors to take for the next prediction. The objective of the RL agent is to maximize the accuracy of the predictions per image. We evaluate the approach on the Waymo Open Dataset and show that it exceeds the performance of each single detector.



### Noise-Resilient Automatic Interpretation of Holter ECG Recordings
- **Arxiv ID**: http://arxiv.org/abs/2011.09303v1
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.09303v1)
- **Published**: 2020-11-17 16:15:49+00:00
- **Updated**: 2020-11-17 16:15:49+00:00
- **Authors**: Konstantin Egorov, Elena Sokolova, Manvel Avetisian, Alexander Tuzhilin
- **Comment**: Accepted for publication on BIOSIGNALS 2021
- **Journal**: None
- **Summary**: Holter monitoring, a long-term ECG recording (24-hours and more), contains a large amount of valuable diagnostic information about the patient. Its interpretation becomes a difficult and time-consuming task for the doctor who analyzes them because every heartbeat needs to be classified, thus requiring highly accurate methods for automatic interpretation. In this paper, we present a three-stage process for analysing Holter recordings with robustness to noisy signal. First stage is a segmentation neural network (NN) with encoderdecoder architecture which detects positions of heartbeats. Second stage is a classification NN which will classify heartbeats as wide or narrow. Third stage in gradient boosting decision trees (GBDT) on top of NN features that incorporates patient-wise features and further increases performance of our approach. As a part of this work we acquired 5095 Holter recordings of patients annotated by an experienced cardiologist. A committee of three cardiologists served as a ground truth annotators for the 291 examples in the test set. We show that the proposed method outperforms the selected baselines, including two commercial-grade software packages and some methods previously published in the literature.



### Global Road Damage Detection: State-of-the-art Solutions
- **Arxiv ID**: http://arxiv.org/abs/2011.08740v1
- **DOI**: 10.1109/BigData50022.2020.9377790
- **Categories**: **cs.CV**, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/2011.08740v1)
- **Published**: 2020-11-17 16:19:02+00:00
- **Updated**: 2020-11-17 16:19:02+00:00
- **Authors**: Deeksha Arya, Hiroya Maeda, Sanjay Kumar Ghosh, Durga Toshniwal, Hiroshi Omata, Takehiro Kashiyama, Yoshihide Sekimoto
- **Comment**: 11 Pages, 2 Figures, 3 Tables
- **Journal**: None
- **Summary**: This paper summarizes the Global Road Damage Detection Challenge (GRDDC), a Big Data Cup organized as a part of the IEEE International Conference on Big Data'2020. The Big Data Cup challenges involve a released dataset and a well-defined problem with clear evaluation metrics. The challenges run on a data competition platform that maintains a leaderboard for the participants. In the presented case, the data constitute 26336 road images collected from India, Japan, and the Czech Republic to propose methods for automatically detecting road damages in these countries. In total, 121 teams from several countries registered for this competition. The submitted solutions were evaluated using two datasets test1 and test2, comprising 2,631 and 2,664 images. This paper encapsulates the top 12 solutions proposed by these teams. The best performing model utilizes YOLO-based ensemble learning to yield an F1 score of 0.67 on test1 and 0.66 on test2. The paper concludes with a review of the facets that worked well for the presented challenge and those that could be improved in future challenges.



### Multi-frame Feature Aggregation for Real-time Instrument Segmentation in Endoscopic Video
- **Arxiv ID**: http://arxiv.org/abs/2011.08752v2
- **DOI**: 10.1109/LRA.2021.3096156
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.08752v2)
- **Published**: 2020-11-17 16:27:27+00:00
- **Updated**: 2021-07-26 00:39:27+00:00
- **Authors**: Shan Lin, Fangbo Qin, Haonan Peng, Randall A. Bly, Kris S. Moe, Blake Hannaford
- **Comment**: Published in IEEE Robotics and Automation Letters (Early Access)
- **Journal**: None
- **Summary**: Deep learning-based methods have achieved promising results on surgical instrument segmentation. However, the high computation cost may limit the application of deep models to time-sensitive tasks such as online surgical video analysis for robotic-assisted surgery. Moreover, current methods may still suffer from challenging conditions in surgical images such as various lighting conditions and the presence of blood. We propose a novel Multi-frame Feature Aggregation (MFFA) module to aggregate video frame features temporally and spatially in a recurrent mode. By distributing the computation load of deep feature extraction over sequential frames, we can use a lightweight encoder to reduce the computation costs at each time step. Moreover, public surgical videos usually are not labeled frame by frame, so we develop a method that can randomly synthesize a surgical frame sequence from a single labeled frame to assist network training. We demonstrate that our approach achieves superior performance to corresponding deeper segmentation models on two public surgery datasets.



### Recognition and standardization of cardiac MRI orientation via multi-tasking learning and deep neural networks
- **Arxiv ID**: http://arxiv.org/abs/2011.08761v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, I.4.6
- **Links**: [PDF](http://arxiv.org/pdf/2011.08761v1)
- **Published**: 2020-11-17 16:41:31+00:00
- **Updated**: 2020-11-17 16:41:31+00:00
- **Authors**: Ke Zhang, Xiahai Zhuang
- **Comment**: 10 pages, 2 figures, to be published in STACOM 2020 (MICCAI Workshop)
- **Journal**: None
- **Summary**: In this paper, we study the problem of imaging orientation in cardiac MRI, and propose a framework to categorize the orientation for recognition and standardization via deep neural networks. The method uses a new multi-tasking strategy, where both the tasks of cardiac segmentation and orientation recognition are simultaneously achieved. For multiple sequences and modalities of MRI, we propose a transfer learning strategy, which adapts our proposed model from a single modality to multiple modalities. We embed the orientation recognition network in a Cardiac MRI Orientation Adjust Tool, i.e., CMRadjustNet. We implemented two versions of CMRadjustNet, including a user-interface (UI) software, and a command-line tool. The former version supports MRI image visualization, orientation prediction, adjustment, and storage operations; and the latter version enables the batch operations. The source code, neural network models and tools have been released and open via https://zmiclab.github.io/projects.html.



### Anatomy Prior Based U-net for Pathology Segmentation with Attention
- **Arxiv ID**: http://arxiv.org/abs/2011.08769v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, I.4.6
- **Links**: [PDF](http://arxiv.org/pdf/2011.08769v1)
- **Published**: 2020-11-17 16:52:29+00:00
- **Updated**: 2020-11-17 16:52:29+00:00
- **Authors**: Yuncheng Zhou, Ke Zhang, Xinzhe Luo, Sihan Wang, Xiahai Zhuang
- **Comment**: 8 pages, 3 figures, to be published in STACOM 2020 (MICCAI Workshop)
- **Journal**: None
- **Summary**: Pathological area segmentation in cardiac magnetic resonance (MR) images plays a vital role in the clinical diagnosis of cardiovascular diseases. Because of the irregular shape and small area, pathological segmentation has always been a challenging task. We propose an anatomy prior based framework, which combines the U-net segmentation network with the attention technique. Leveraging the fact that the pathology is inclusive, we propose a neighborhood penalty strategy to gauge the inclusion relationship between the myocardium and the myocardial infarction and no-reflow areas. This neighborhood penalty strategy can be applied to any two labels with inclusive relationships (such as the whole infarction and myocardium, etc.) to form a neighboring loss. The proposed framework is evaluated on the EMIDEC dataset. Results show that our framework is effective in pathological area segmentation.



### A Method to Generate High Precision Mesh Model and RGB-D Datasetfor 6D Pose Estimation Task
- **Arxiv ID**: http://arxiv.org/abs/2011.08771v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.08771v1)
- **Published**: 2020-11-17 16:56:57+00:00
- **Updated**: 2020-11-17 16:56:57+00:00
- **Authors**: Minglei Lu, Yu Guo, Fei Wang, Zheng Dang
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, 3D version has been improved greatly due to the development of deep neural networks. A high quality dataset is important to the deep learning method. Existing datasets for 3D vision has been constructed, such as Bigbird and YCB. However, the depth sensors used to make these datasets are out of date, which made the resolution and accuracy of the datasets cannot full fill the higher standards of demand. Although the equipment and technology got better, but no one was trying to collect new and better dataset. Here we are trying to fill that gap. To this end, we propose a new method for object reconstruction, which takes into account the speed, accuracy and robustness. Our method could be used to produce large dataset with better and more accurate annotation. More importantly, our data is more close to the rendering data, which shrinking the gap between the real data and synthetic data further.



### PaDiM: a Patch Distribution Modeling Framework for Anomaly Detection and Localization
- **Arxiv ID**: http://arxiv.org/abs/2011.08785v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.08785v1)
- **Published**: 2020-11-17 17:29:18+00:00
- **Updated**: 2020-11-17 17:29:18+00:00
- **Authors**: Thomas Defard, Aleksandr Setkov, Angelique Loesch, Romaric Audigier
- **Comment**: 7 pages, 2 figures, 8 tables, accepted at the 1st International
  Workshop on Industrial Machine Learning, ICPR 2020
- **Journal**: None
- **Summary**: We present a new framework for Patch Distribution Modeling, PaDiM, to concurrently detect and localize anomalies in images in a one-class learning setting. PaDiM makes use of a pretrained convolutional neural network (CNN) for patch embedding, and of multivariate Gaussian distributions to get a probabilistic representation of the normal class. It also exploits correlations between the different semantic levels of CNN to better localize anomalies. PaDiM outperforms current state-of-the-art approaches for both anomaly detection and localization on the MVTec AD and STC datasets. To match real-world visual industrial inspection, we extend the evaluation protocol to assess performance of anomaly localization algorithms on non-aligned dataset. The state-of-the-art performance and low complexity of PaDiM make it a good candidate for many industrial applications.



### P1AC: Revisiting Absolute Pose From a Single Affine Correspondence
- **Arxiv ID**: http://arxiv.org/abs/2011.08790v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.08790v4)
- **Published**: 2020-11-17 17:36:16+00:00
- **Updated**: 2023-08-23 19:19:14+00:00
- **Authors**: Jonathan Ventura, Zuzana Kukelova, Torsten Sattler, Dániel Baráth
- **Comment**: ICCV 2023
- **Journal**: None
- **Summary**: Affine correspondences have traditionally been used to improve feature matching over wide baselines. While recent work has successfully used affine correspondences to solve various relative camera pose estimation problems, less attention has been given to their use in absolute pose estimation. We introduce the first general solution to the problem of estimating the pose of a calibrated camera given a single observation of an oriented point and an affine correspondence. The advantage of our approach (P1AC) is that it requires only a single correspondence, in comparison to the traditional point-based approach (P3P), significantly reducing the combinatorics in robust estimation. P1AC provides a general solution that removes restrictive assumptions made in prior work and is applicable to large-scale image-based localization. We propose a minimal solution to the P1AC problem and evaluate our novel solver on synthetic data, showing its numerical stability and performance under various types of noise. On standard image-based localization benchmarks we show that P1AC achieves more accurate results than the widely used P3P algorithm. Code for our method is available at https://github.com/jonathanventura/P1AC/ .



### Facial Expressions as a Vulnerability in Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2011.08809v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.08809v2)
- **Published**: 2020-11-17 18:12:41+00:00
- **Updated**: 2021-06-18 08:27:24+00:00
- **Authors**: Alejandro Peña, Ignacio Serna, Aythami Morales, Julian Fierrez, Agata Lapedriza
- **Comment**: Proc. of IEEE Int. Conf. on Image Processing (ICIP)
- **Journal**: None
- **Summary**: This work explores facial expression bias as a security vulnerability of face recognition systems. Despite the great performance achieved by state-of-the-art face recognition systems, the algorithms are still sensitive to a large range of covariates. We present a comprehensive analysis of how facial expression bias impacts the performance of face recognition technologies. Our study analyzes: i) facial expression biases in the most popular face recognition databases; and ii) the impact of facial expression in face recognition performances. Our experimental framework includes two face detectors, three face recognition models, and three different databases. Our results demonstrate a huge facial expression bias in the most widely used databases, as well as a related impact of face expression in the performance of state-of-the-art algorithms. This work opens the door to new research lines focused on mitigating the observed vulnerability.



### RELLIS-3D Dataset: Data, Benchmarks and Analysis
- **Arxiv ID**: http://arxiv.org/abs/2011.12954v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO, I.2.9; I.2.10; I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/2011.12954v4)
- **Published**: 2020-11-17 18:28:01+00:00
- **Updated**: 2022-05-25 15:11:10+00:00
- **Authors**: Peng Jiang, Philip Osteen, Maggie Wigness, Srikanth Saripalli
- **Comment**: 7 pages, 7 figures
- **Journal**: None
- **Summary**: Semantic scene understanding is crucial for robust and safe autonomous navigation, particularly so in off-road environments. Recent deep learning advances for 3D semantic segmentation rely heavily on large sets of training data, however existing autonomy datasets either represent urban environments or lack multimodal off-road data. We fill this gap with RELLIS-3D, a multimodal dataset collected in an off-road environment, which contains annotations for 13,556 LiDAR scans and 6,235 images. The data was collected on the Rellis Campus of Texas A\&M University and presents challenges to existing algorithms related to class imbalance and environmental topography. Additionally, we evaluate the current state-of-the-art deep learning semantic segmentation models on this dataset. Experimental results show that RELLIS-3D presents challenges for algorithms designed for segmentation in urban environments. This novel dataset provides the resources needed by researchers to continue to develop more advanced algorithms and investigate new research directions to enhance autonomous navigation in off-road environments. RELLIS-3D is available at https://github.com/unmannedlab/RELLIS-3D



### Spatio-Temporal Analysis of Facial Actions using Lifecycle-Aware Capsule Networks
- **Arxiv ID**: http://arxiv.org/abs/2011.08819v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.08819v2)
- **Published**: 2020-11-17 18:36:38+00:00
- **Updated**: 2021-03-04 02:41:43+00:00
- **Authors**: Nikhil Churamani, Sinan Kalkan, Hatice Gunes
- **Comment**: Updated Figure 6 and the Acknowledgements. Corrected typos. 11 pages,
  6 figures, 3 tables
- **Journal**: None
- **Summary**: Most state-of-the-art approaches for Facial Action Unit (AU) detection rely upon evaluating facial expressions from static frames, encoding a snapshot of heightened facial activity. In real-world interactions, however, facial expressions are usually more subtle and evolve in a temporal manner requiring AU detection models to learn spatial as well as temporal information. In this paper, we focus on both spatial and spatio-temporal features encoding the temporal evolution of facial AU activation. For this purpose, we propose the Action Unit Lifecycle-Aware Capsule Network (AULA-Caps) that performs AU detection using both frame and sequence-level features. While at the frame-level the capsule layers of AULA-Caps learn spatial feature primitives to determine AU activations, at the sequence-level, it learns temporal dependencies between contiguous frames by focusing on relevant spatio-temporal segments in the sequence. The learnt feature capsules are routed together such that the model learns to selectively focus more on spatial or spatio-temporal information depending upon the AU lifecycle. The proposed model is evaluated on the commonly used BP4D and GFT benchmark datasets obtaining state-of-the-art results on both the datasets.



### Learning Canonical Transformations
- **Arxiv ID**: http://arxiv.org/abs/2011.08822v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2011.08822v1)
- **Published**: 2020-11-17 18:41:07+00:00
- **Updated**: 2020-11-17 18:41:07+00:00
- **Authors**: Zachary Dulberg, Jonathan Cohen
- **Comment**: NeurIPS 2020 Workshop on BabyMind
- **Journal**: None
- **Summary**: Humans understand a set of canonical geometric transformations (such as translation and rotation) that support generalization by being untethered to any specific object. We explore inductive biases that help a neural network model learn these transformations in pixel space in a way that can generalize out-of-domain. Specifically, we find that high training set diversity is sufficient for the extrapolation of translation to unseen shapes and scales, and that an iterative training scheme achieves significant extrapolation of rotation in time.



### Deep Active Surface Models
- **Arxiv ID**: http://arxiv.org/abs/2011.08826v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.08826v5)
- **Published**: 2020-11-17 18:48:28+00:00
- **Updated**: 2021-06-07 22:19:00+00:00
- **Authors**: Udaranga Wickramasinghe, Graham Knott, Pascal Fua
- **Comment**: 11 pages, 7 figures, 6 tables
- **Journal**: None
- **Summary**: Active Surface Models have a long history of being useful to model complex 3D surfaces but only Active Contours have been used in conjunction with deep networks, and then only to produce the data term as well as meta-parameter maps controlling them. In this paper, we advocate a much tighter integration. We introduce layers that implement them that can be integrated seamlessly into Graph Convolutional Networks to enforce sophisticated smoothness priors at an acceptable computational cost. We will show that the resulting Deep Active Surface Models outperform equivalent architectures that use traditional regularization loss terms to impose smoothness priors for 3D surface reconstruction from 2D images and for 3D volume segmentation.



### Towards Improved and Interpretable Deep Metric Learning via Attentive Grouping
- **Arxiv ID**: http://arxiv.org/abs/2011.08877v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.08877v3)
- **Published**: 2020-11-17 19:08:24+00:00
- **Updated**: 2021-08-25 05:42:12+00:00
- **Authors**: Xinyi Xu, Zhengyang Wang, Cheng Deng, Hao Yuan, Shuiwang Ji
- **Comment**: None
- **Journal**: None
- **Summary**: Grouping has been commonly used in deep metric learning for computing diverse features. However, current methods are prone to overfitting and lack interpretability. In this work, we propose an improved and interpretable grouping method to be integrated flexibly with any metric learning framework. Our method is based on the attention mechanism with a learnable query for each group. The query is fully trainable and can capture group-specific information when combined with the diversity loss. An appealing property of our method is that it naturally lends itself interpretability. The attention scores between the learnable query and each spatial position can be interpreted as the importance of that position. We formally show that our proposed grouping method is invariant to spatial permutations of features. When used as a module in convolutional neural networks, our method leads to translational invariance. We conduct comprehensive experiments to evaluate our method. Our quantitative results indicate that the proposed method outperforms prior methods consistently and significantly across different datasets, evaluation metrics, base models, and loss functions. For the first time to the best of our knowledge, our interpretation results clearly demonstrate that the proposed method enables the learning of distinct and diverse features across groups. The code is available on https://github.com/XinyiXuXD/DGML-master.



### Generating Natural Questions from Images for Multimodal Assistants
- **Arxiv ID**: http://arxiv.org/abs/2012.03678v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2012.03678v1)
- **Published**: 2020-11-17 19:12:23+00:00
- **Updated**: 2020-11-17 19:12:23+00:00
- **Authors**: Alkesh Patel, Akanksha Bindal, Hadas Kotek, Christopher Klein, Jason Williams
- **Comment**: 4 pages, 1 reference page, 5 figures, 4 tables
- **Journal**: None
- **Summary**: Generating natural, diverse, and meaningful questions from images is an essential task for multimodal assistants as it confirms whether they have understood the object and scene in the images properly. The research in visual question answering (VQA) and visual question generation (VQG) is a great step. However, this research does not capture questions that a visually-abled person would ask multimodal assistants. Recently published datasets such as KB-VQA, FVQA, and OK-VQA try to collect questions that look for external knowledge which makes them appropriate for multimodal assistants. However, they still contain many obvious and common-sense questions that humans would not usually ask a digital assistant. In this paper, we provide a new benchmark dataset that contains questions generated by human annotators keeping in mind what they would ask multimodal digital assistants. Large scale annotations for several hundred thousand images are expensive and time-consuming, so we also present an effective way of automatically generating questions from unseen images. In this paper, we present an approach for generating diverse and meaningful questions that consider image content and metadata of image (e.g., location, associated keyword). We evaluate our approach using standard evaluation metrics such as BLEU, METEOR, ROUGE, and CIDEr to show the relevance of generated questions with human-provided questions. We also measure the diversity of generated questions using generative strength and inventiveness metrics. We report new state-of-the-art results on the public and our datasets.



### Use HiResCAM instead of Grad-CAM for faithful explanations of convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/2011.08891v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.08891v4)
- **Published**: 2020-11-17 19:26:14+00:00
- **Updated**: 2021-11-21 01:35:57+00:00
- **Authors**: Rachel Lea Draelos, Lawrence Carin
- **Comment**: 20 pages, 8 figures, 2 tables
- **Journal**: None
- **Summary**: Explanation methods facilitate the development of models that learn meaningful concepts and avoid exploiting spurious correlations. We illustrate a previously unrecognized limitation of the popular neural network explanation method Grad-CAM: as a side effect of the gradient averaging step, Grad-CAM sometimes highlights locations the model did not actually use. To solve this problem, we propose HiResCAM, a novel class-specific explanation method that is guaranteed to highlight only the locations the model used to make each prediction. We prove that HiResCAM is a generalization of CAM and explore the relationships between HiResCAM and other gradient-based explanation methods. Experiments on PASCAL VOC 2012, including crowd-sourced evaluations, illustrate that while HiResCAM's explanations faithfully reflect the model, Grad-CAM often expands the attention to create bigger and smoother visualizations. Overall, this work advances convolutional neural network explanation approaches and may aid in the development of trustworthy models for sensitive applications.



### Contrastive Registration for Unsupervised Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2011.08894v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.08894v3)
- **Published**: 2020-11-17 19:29:08+00:00
- **Updated**: 2022-07-20 23:39:17+00:00
- **Authors**: Lihao Liu, Angelica I Aviles-Rivero, Carola-Bibiane Schönlieb
- **Comment**: 12 pages, 8 figures, 4 tables
- **Journal**: None
- **Summary**: Medical image segmentation is a relevant task as it serves as the first step for several diagnosis processes, thus it is indispensable in clinical usage. Whilst major success has been reported using supervised techniques, they assume a large and well-representative labelled set. This is a strong assumption in the medical domain where annotations are expensive, time-consuming, and inherent to human bias. To address this problem, unsupervised techniques have been proposed in the literature yet it is still an open problem due to the difficulty of learning any transformation pattern. In this work, we present a novel optimisation model framed into a new CNN-based contrastive registration architecture for unsupervised medical image segmentation. The core of our approach is to exploit image-level registration and feature-level from a contrastive learning mechanism, to perform registration-based segmentation. Firstly, we propose an architecture to capture the image-to-image transformation pattern via registration for unsupervised medical image segmentation. Secondly, we embed a contrastive learning mechanism into the registration architecture to enhance the discriminating capacity of the network in the feature-level. We show that our proposed technique mitigates the major drawbacks of existing unsupervised techniques. We demonstrate, through numerical and visual experiments, that our technique substantially outperforms the current state-of-the-art unsupervised segmentation methods on two major medical image datasets.



### Probing Fairness of Mobile Ocular Biometrics Methods Across Gender on VISOB 2.0 Dataset
- **Arxiv ID**: http://arxiv.org/abs/2011.08898v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.08898v1)
- **Published**: 2020-11-17 19:32:56+00:00
- **Updated**: 2020-11-17 19:32:56+00:00
- **Authors**: Anoop Krishnan, Ali Almadan, Ajita Rattani
- **Comment**: None
- **Journal**: 25th International Conference on Pattern Recognition (ICPR) 2020|
  Milan, Italy
- **Summary**: Recent research has questioned the fairness of face-based recognition and attribute classification methods (such as gender and race) for dark-skinned people and women. Ocular biometrics in the visible spectrum is an alternate solution over face biometrics, thanks to its accuracy, security, robustness against facial expression, and ease of use in mobile devices. With the recent COVID-19 crisis, ocular biometrics has a further advantage over face biometrics in the presence of a mask. However, fairness of ocular biometrics has not been studied till now. This first study aims to explore the fairness of ocular-based authentication and gender classification methods across males and females. To this aim, VISOB $2.0$ dataset, along with its gender annotations, is used for the fairness analysis of ocular biometrics methods based on ResNet-50, MobileNet-V2 and lightCNN-29 models. Experimental results suggest the equivalent performance of males and females for ocular-based mobile user-authentication in terms of genuine match rate (GMR) at lower false match rates (FMRs) and an overall Area Under Curve (AUC). For instance, an AUC of 0.96 for females and 0.95 for males was obtained for lightCNN-29 on an average. However, males significantly outperformed females in deep learning based gender classification models based on ocular-region.



### Multimodal Prototypical Networks for Few-shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2011.08899v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.08899v1)
- **Published**: 2020-11-17 19:32:59+00:00
- **Updated**: 2020-11-17 19:32:59+00:00
- **Authors**: Frederik Pahde, Mihai Puscas, Tassilo Klein, Moin Nabi
- **Comment**: To appear at WACV 2021
- **Journal**: None
- **Summary**: Although providing exceptional results for many computer vision tasks, state-of-the-art deep learning algorithms catastrophically struggle in low data scenarios. However, if data in additional modalities exist (e.g. text) this can compensate for the lack of data and improve the classification results. To overcome this data scarcity, we design a cross-modal feature generation framework capable of enriching the low populated embedding space in few-shot scenarios, leveraging data from the auxiliary modality. Specifically, we train a generative model that maps text data into the visual feature space to obtain more reliable prototypes. This allows to exploit data from additional modalities (e.g. text) during training while the ultimate task at test time remains classification with exclusively visual data. We show that in such cases nearest neighbor classification is a viable approach and outperform state-of-the-art single-modal and multimodal few-shot learning methods on the CUB-200 and Oxford-102 datasets.



### Whose hand is this? Person Identification from Egocentric Hand Gestures
- **Arxiv ID**: http://arxiv.org/abs/2011.08900v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.08900v1)
- **Published**: 2020-11-17 19:35:27+00:00
- **Updated**: 2020-11-17 19:35:27+00:00
- **Authors**: Satoshi Tsutsui, Yanwei Fu, David Crandall
- **Comment**: Accepted to IEEE Winter Conference on Applications of Computer Vision
  (WACV) 2021 (First round acceptance)
- **Journal**: None
- **Summary**: Recognizing people by faces and other biometrics has been extensively studied in computer vision. But these techniques do not work for identifying the wearer of an egocentric (first-person) camera because that person rarely (if ever) appears in their own first-person view. But while one's own face is not frequently visible, their hands are: in fact, hands are among the most common objects in one's own field of view. It is thus natural to ask whether the appearance and motion patterns of people's hands are distinctive enough to recognize them. In this paper, we systematically study the possibility of Egocentric Hand Identification (EHI) with unconstrained egocentric hand gestures. We explore several different visual cues, including color, shape, skin texture, and depth maps to identify users' hands. Extensive ablation experiments are conducted to analyze the properties of hands that are most distinctive. Finally, we show that EHI can improve generalization of other tasks, such as gesture recognition, by training adversarially to encourage these models to ignore differences between users.



### Deep Learning Benchmarks and Datasets for Social Media Image Classification for Disaster Response
- **Arxiv ID**: http://arxiv.org/abs/2011.08916v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2011.08916v1)
- **Published**: 2020-11-17 20:15:49+00:00
- **Updated**: 2020-11-17 20:15:49+00:00
- **Authors**: Firoj Alam, Ferda Ofli, Muhammad Imran, Tanvirul Alam, Umair Qazi
- **Comment**: None
- **Journal**: None
- **Summary**: During a disaster event, images shared on social media helps crisis managers gain situational awareness and assess incurred damages, among other response tasks. Recent advances in computer vision and deep neural networks have enabled the development of models for real-time image classification for a number of tasks, including detecting crisis incidents, filtering irrelevant images, classifying images into specific humanitarian categories, and assessing the severity of damage. Despite several efforts, past works mainly suffer from limited resources (i.e., labeled images) available to train more robust deep learning models. In this study, we propose new datasets for disaster type detection, and informativeness classification, and damage severity assessment. Moreover, we relabel existing publicly available datasets for new tasks. We identify exact- and near-duplicates to form non-overlapping data splits, and finally consolidate them to create larger datasets. In our extensive experiments, we benchmark several state-of-the-art deep learning models and achieve promising results. We release our datasets and models publicly, aiming to provide proper baselines as well as to spur further research in the crisis informatics community.



### Modeling Fashion Influence from Photos
- **Arxiv ID**: http://arxiv.org/abs/2011.09663v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2011.09663v1)
- **Published**: 2020-11-17 20:24:03+00:00
- **Updated**: 2020-11-17 20:24:03+00:00
- **Authors**: Ziad Al-Halah, Kristen Grauman
- **Comment**: To appear in the IEEE Transactions on Multimedia, 2020. Project page:
  https://www.cs.utexas.edu/~ziad/influence_from_photos.html. arXiv admin note:
  substantial text overlap with arXiv:2004.01316
- **Journal**: None
- **Summary**: The evolution of clothing styles and their migration across the world is intriguing, yet difficult to describe quantitatively. We propose to discover and quantify fashion influences from catalog and social media photos. We explore fashion influence along two channels: geolocation and fashion brands. We introduce an approach that detects which of these entities influence which other entities in terms of propagating their styles. We then leverage the discovered influence patterns to inform a novel forecasting model that predicts the future popularity of any given style within any given city or brand. To demonstrate our idea, we leverage public large-scale datasets of 7.7M Instagram photos from 44 major world cities (where styles are worn with variable frequency) as well as 41K Amazon product photos (where styles are purchased with variable frequency). Our model learns directly from the image data how styles move between locations and how certain brands affect each other's designs in a predictable way. The discovered influence relationships reveal how both cities and brands exert and receive fashion influence for an array of visual styles inferred from the images. Furthermore, the proposed forecasting model achieves state-of-the-art results for challenging style forecasting tasks. Our results indicate the advantage of grounding visual style evolution both spatially and temporally, and for the first time, they quantify the propagation of inter-brand and inter-city influences.



### Analyzing and Mitigating JPEG Compression Defects in Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2011.08932v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.08932v2)
- **Published**: 2020-11-17 20:32:57+00:00
- **Updated**: 2021-09-20 12:28:30+00:00
- **Authors**: Max Ehrlich, Larry Davis, Ser-Nam Lim, Abhinav Shrivastava
- **Comment**: Accepted to the ICCV MELEX Workshop
- **Journal**: None
- **Summary**: With the proliferation of deep learning methods, many computer vision problems which were considered academic are now viable in the consumer setting. One drawback of consumer applications is lossy compression, which is necessary from an engineering standpoint to efficiently and cheaply store and transmit user images. Despite this, there has been little study of the effect of compression on deep neural networks and benchmark datasets are often losslessly compressed or compressed at high quality. Here we present a unified study of the effects of JPEG compression on a range of common tasks and datasets. We show that there is a significant penalty on common performance metrics for high compression. We test several methods for mitigating this penalty, including a novel method based on artifact correction which requires no labels to train.



### Dual-stream Multiple Instance Learning Network for Whole Slide Image Classification with Self-supervised Contrastive Learning
- **Arxiv ID**: http://arxiv.org/abs/2011.08939v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.08939v3)
- **Published**: 2020-11-17 20:51:15+00:00
- **Updated**: 2021-04-02 16:48:03+00:00
- **Authors**: Bin Li, Yin Li, Kevin W. Eliceiri
- **Comment**: CVPR 2021, accepted for oral presentation
- **Journal**: None
- **Summary**: We address the challenging problem of whole slide image (WSI) classification. WSIs have very high resolutions and usually lack localized annotations. WSI classification can be cast as a multiple instance learning (MIL) problem when only slide-level labels are available. We propose a MIL-based method for WSI classification and tumor detection that does not require localized annotations. Our method has three major components. First, we introduce a novel MIL aggregator that models the relations of the instances in a dual-stream architecture with trainable distance measurement. Second, since WSIs can produce large or unbalanced bags that hinder the training of MIL models, we propose to use self-supervised contrastive learning to extract good representations for MIL and alleviate the issue of prohibitive memory cost for large bags. Third, we adopt a pyramidal fusion mechanism for multiscale WSI features, and further improve the accuracy of classification and localization. Our model is evaluated on two representative WSI datasets. The classification accuracy of our model compares favorably to fully-supervised methods, with less than 2% accuracy gap across datasets. Our results also outperform all previous MIL-based methods. Additional benchmark results on standard MIL datasets further demonstrate the superior performance of our MIL aggregator on general MIL problems. GitHub repository: https://github.com/binli123/dsmil-wsi



### Reactive Human-to-Robot Handovers of Arbitrary Objects
- **Arxiv ID**: http://arxiv.org/abs/2011.08961v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.08961v2)
- **Published**: 2020-11-17 21:52:22+00:00
- **Updated**: 2021-06-03 20:48:44+00:00
- **Authors**: Wei Yang, Chris Paxton, Arsalan Mousavian, Yu-Wei Chao, Maya Cakmak, Dieter Fox
- **Comment**: Accepted to the International Conference on Robotics and Automation
  (ICRA) 2021
- **Journal**: None
- **Summary**: Human-robot object handovers have been an actively studied area of robotics over the past decade; however, very few techniques and systems have addressed the challenge of handing over diverse objects with arbitrary appearance, size, shape, and rigidity. In this paper, we present a vision-based system that enables reactive human-to-robot handovers of unknown objects. Our approach combines closed-loop motion planning with real-time, temporally-consistent grasp generation to ensure reactivity and motion smoothness. Our system is robust to different object positions and orientations, and can grasp both rigid and non-rigid objects. We demonstrate the generalizability, usability, and robustness of our approach on a novel benchmark set of 26 diverse household objects, a user study with naive users (N=6) handing over a subset of 15 objects, and a systematic evaluation examining different ways of handing objects. More results and videos can be found at https://sites.google.com/nvidia.com/handovers-of-arbitrary-objects.



### Interpretable Survival Prediction for Colorectal Cancer using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2011.08965v1
- **DOI**: 10.1038/s41746-021-00427-2
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.08965v1)
- **Published**: 2020-11-17 21:57:16+00:00
- **Updated**: 2020-11-17 21:57:16+00:00
- **Authors**: Ellery Wulczyn, David F. Steiner, Melissa Moran, Markus Plass, Robert Reihs, Fraser Tan, Isabelle Flament-Auvigne, Trissia Brown, Peter Regitnig, Po-Hsuan Cameron Chen, Narayan Hegde, Apaar Sadhwani, Robert MacDonald, Benny Ayalew, Greg S. Corrado, Lily H. Peng, Daniel Tse, Heimo Müller, Zhaoyang Xu, Yun Liu, Martin C. Stumpe, Kurt Zatloukal, Craig H. Mermel
- **Comment**: None
- **Journal**: Nature Partner Journal Digital Medicine (2021)
- **Summary**: Deriving interpretable prognostic features from deep-learning-based prognostic histopathology models remains a challenge. In this study, we developed a deep learning system (DLS) for predicting disease specific survival for stage II and III colorectal cancer using 3,652 cases (27,300 slides). When evaluated on two validation datasets containing 1,239 cases (9,340 slides) and 738 cases (7,140 slides) respectively, the DLS achieved a 5-year disease-specific survival AUC of 0.70 (95%CI 0.66-0.73) and 0.69 (95%CI 0.64-0.72), and added significant predictive value to a set of 9 clinicopathologic features. To interpret the DLS, we explored the ability of different human-interpretable features to explain the variance in DLS scores. We observed that clinicopathologic features such as T-category, N-category, and grade explained a small fraction of the variance in DLS scores (R2=18% in both validation sets). Next, we generated human-interpretable histologic features by clustering embeddings from a deep-learning based image-similarity model and showed that they explain the majority of the variance (R2 of 73% to 80%). Furthermore, the clustering-derived feature most strongly associated with high DLS scores was also highly prognostic in isolation. With a distinct visual appearance (poorly differentiated tumor cell clusters adjacent to adipose tissue), this feature was identified by annotators with 87.0-95.5% accuracy. Our approach can be used to explain predictions from a prognostic deep learning model and uncover potentially-novel prognostic features that can be reliably identified by people for future validation studies.



### Minimal Solvers for Single-View Lens-Distorted Camera Auto-Calibration
- **Arxiv ID**: http://arxiv.org/abs/2011.08988v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.08988v1)
- **Published**: 2020-11-17 22:32:17+00:00
- **Updated**: 2020-11-17 22:32:17+00:00
- **Authors**: Yaroslava Lochman, Oles Dobosevych, Rostyslav Hryniv, James Pritts
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes minimal solvers that use combinations of imaged translational symmetries and parallel scene lines to jointly estimate lens undistortion with either affine rectification or focal length and absolute orientation. We use constraints provided by orthogonal scene planes to recover the focal length. We show that solvers using feature combinations can recover more accurate calibrations than solvers using only one feature type on scenes that have a balance of lines and texture. We also show that the proposed solvers are complementary and can be used together in a RANSAC-based estimator to improve auto-calibration accuracy. State-of-the-art performance is demonstrated on a standard dataset of lens-distorted urban images. The code is available at https://github.com/ylochman/single-view-autocalib.



### Towards Spatial Variability Aware Deep Neural Networks (SVANN): A Summary of Results
- **Arxiv ID**: http://arxiv.org/abs/2011.08992v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.08992v1)
- **Published**: 2020-11-17 22:52:40+00:00
- **Updated**: 2020-11-17 22:52:40+00:00
- **Authors**: Jayant Gupta, Yiqun Xie, Shashi Shekhar
- **Comment**: Accepted in 1st ACM SIGKDD Workshop on Deep Learning for
  Spatiotemporal Data, Applications, and Systems (Deepspatial 2020), San Diego,
  CA, August 24, 2020
- **Journal**: None
- **Summary**: Spatial variability has been observed in many geo-phenomena including climatic zones, USDA plant hardiness zones, and terrestrial habitat types (e.g., forest, grasslands, wetlands, and deserts). However, current deep learning methods follow a spatial-one-size-fits-all(OSFA) approach to train single deep neural network models that do not account for spatial variability. In this work, we propose and investigate a spatial-variability aware deep neural network(SVANN) approach, where distinct deep neural network models are built for each geographic area. We evaluate this approach using aerial imagery from two geographic areas for the task of mapping urban gardens. The experimental results show that SVANN provides better performance than OSFA in terms of precision, recall,and F1-score to identify urban gardens.



