# Arxiv Papers in cs.CV on 2020-11-13
### Robust Policies via Mid-Level Visual Representations: An Experimental Study in Manipulation and Navigation
- **Arxiv ID**: http://arxiv.org/abs/2011.06698v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.06698v1)
- **Published**: 2020-11-13 00:16:05+00:00
- **Updated**: 2020-11-13 00:16:05+00:00
- **Authors**: Bryan Chen, Alexander Sax, Gene Lewis, Iro Armeni, Silvio Savarese, Amir Zamir, Jitendra Malik, Lerrel Pinto
- **Comment**: Extended version of CoRL 2020 camera ready. Supplementary released
  separately
- **Journal**: None
- **Summary**: Vision-based robotics often separates the control loop into one module for perception and a separate module for control. It is possible to train the whole system end-to-end (e.g. with deep RL), but doing it "from scratch" comes with a high sample complexity cost and the final result is often brittle, failing unexpectedly if the test environment differs from that of training.   We study the effects of using mid-level visual representations (features learned asynchronously for traditional computer vision objectives), as a generic and easy-to-decode perceptual state in an end-to-end RL framework. Mid-level representations encode invariances about the world, and we show that they aid generalization, improve sample complexity, and lead to a higher final performance. Compared to other approaches for incorporating invariances, such as domain randomization, asynchronously trained mid-level representations scale better: both to harder problems and to larger domain shifts. In practice, this means that mid-level representations could be used to successfully train policies for tasks where domain randomization and learning-from-scratch failed. We report results on both manipulation and navigation tasks, and for navigation include zero-shot sim-to-real experiments on real robots.



### Diffusion models for Handwriting Generation
- **Arxiv ID**: http://arxiv.org/abs/2011.06704v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.06704v1)
- **Published**: 2020-11-13 00:31:22+00:00
- **Updated**: 2020-11-13 00:31:22+00:00
- **Authors**: Troy Luhman, Eric Luhman
- **Comment**: 17 figures, 17 pages
- **Journal**: None
- **Summary**: In this paper, we propose a diffusion probabilistic model for handwriting generation. Diffusion models are a class of generative models where samples start from Gaussian noise and are gradually denoised to produce output. Our method of handwriting generation does not require using any text-recognition based, writer-style based, or adversarial loss functions, nor does it require training of auxiliary networks. Our model is able to incorporate writer stylistic features directly from image data, eliminating the need for user interaction during sampling. Experiments reveal that our model is able to generate realistic , high quality images of handwritten text in a similar style to a given writer. Our implementation can be found at https://github.com/tcl9876/Diffusion-Handwriting-Generation



### Local Anomaly Detection in Videos using Object-Centric Adversarial Learning
- **Arxiv ID**: http://arxiv.org/abs/2011.06722v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.06722v1)
- **Published**: 2020-11-13 02:02:37+00:00
- **Updated**: 2020-11-13 02:02:37+00:00
- **Authors**: Pankaj Raj Roy, Guillaume-Alexandre Bilodeau, Lama Seoud
- **Comment**: Accepted for The First International Workshop on Deep Learning for
  Human-Centric Activity Understanding (ICPR2020 workshop)
- **Journal**: None
- **Summary**: We propose a novel unsupervised approach based on a two-stage object-centric adversarial framework that only needs object regions for detecting frame-level local anomalies in videos. The first stage consists in learning the correspondence between the current appearance and past gradient images of objects in scenes deemed normal, allowing us to either generate the past gradient from current appearance or the reverse. The second stage extracts the partial reconstruction errors between real and generated images (appearance and past gradient) with normal object behaviour, and trains a discriminator in an adversarial fashion. In inference mode, we employ the trained image generators with the adversarially learned binary classifier for outputting region-level anomaly detection scores. We tested our method on four public benchmarks, UMN, UCSD, Avenue and ShanghaiTech and our proposed object-centric adversarial approach yields competitive or even superior results compared to state-of-the-art methods.



### One Explanation is Not Enough: Structured Attention Graphs for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2011.06733v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.06733v4)
- **Published**: 2020-11-13 02:51:54+00:00
- **Updated**: 2021-11-07 09:58:28+00:00
- **Authors**: Vivswan Shitole, Li Fuxin, Minsuk Kahng, Prasad Tadepalli, Alan Fern
- **Comment**: 26 pages, 25 figures
- **Journal**: NeuRIPS 2021
- **Summary**: Attention maps are a popular way of explaining the decisions of convolutional networks for image classification. Typically, for each image of interest, a single attention map is produced, which assigns weights to pixels based on their importance to the classification. A single attention map, however, provides an incomplete understanding since there are often many other maps that explain a classification equally well. In this paper, we introduce structured attention graphs (SAGs), which compactly represent sets of attention maps for an image by capturing how different combinations of image regions impact a classifier's confidence. We propose an approach to compute SAGs and a visualization for SAGs so that deeper insight can be gained into a classifier's decisions. We conduct a user study comparing the use of SAGs to traditional attention maps for answering counterfactual questions about image classifications. Our results show that the users are more correct when answering comparative counterfactual questions based on SAGs compared to the baselines.



### Filter Pre-Pruning for Improved Fine-tuning of Quantized Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2011.06751v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.06751v2)
- **Published**: 2020-11-13 04:12:54+00:00
- **Updated**: 2020-11-25 05:22:16+00:00
- **Authors**: Jun Nishikawa, Ryoji Ikegaya
- **Comment**: updated for ICLR2021 OpenReview rebuttal
- **Journal**: None
- **Summary**: Deep Neural Networks(DNNs) have many parameters and activation data, and these both are expensive to implement. One method to reduce the size of the DNN is to quantize the pre-trained model by using a low-bit expression for weights and activations, using fine-tuning to recover the drop in accuracy. However, it is generally difficult to train neural networks which use low-bit expressions. One reason is that the weights in the middle layer of the DNN have a wide dynamic range and so when quantizing the wide dynamic range into a few bits, the step size becomes large, which leads to a large quantization error and finally a large degradation in accuracy. To solve this problem, this paper makes the following three contributions without using any additional learning parameters and hyper-parameters. First, we analyze how batch normalization, which causes the aforementioned problem, disturbs the fine-tuning of the quantized DNN. Second, based on these results, we propose a new pruning method called Pruning for Quantization (PfQ) which removes the filters that disturb the fine-tuning of the DNN while not affecting the inferred result as far as possible. Third, we propose a workflow of fine-tuning for quantized DNNs using the proposed pruning method(PfQ). Experiments using well-known models and datasets confirmed that the proposed method achieves higher performance with a similar model size than conventional quantization methods including fine-tuning.



### Lightweight Single-Image Super-Resolution Network with Attentive Auxiliary Feature Learning
- **Arxiv ID**: http://arxiv.org/abs/2011.06773v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.06773v1)
- **Published**: 2020-11-13 06:01:46+00:00
- **Updated**: 2020-11-13 06:01:46+00:00
- **Authors**: Xuehui Wang, Qing Wang, Yuzhi Zhao, Junchi Yan, Lei Fan, Long Chen
- **Comment**: Accepted by ACCV 2020
- **Journal**: None
- **Summary**: Despite convolutional network-based methods have boosted the performance of single image super-resolution (SISR), the huge computation costs restrict their practical applicability. In this paper, we develop a computation efficient yet accurate network based on the proposed attentive auxiliary features (A$^2$F) for SISR. Firstly, to explore the features from the bottom layers, the auxiliary feature from all the previous layers are projected into a common space. Then, to better utilize these projected auxiliary features and filter the redundant information, the channel attention is employed to select the most important common feature based on current layer feature. We incorporate these two modules into a block and implement it with a lightweight network. Experimental results on large-scale dataset demonstrate the effectiveness of the proposed model against the state-of-the-art (SOTA) SR methods. Notably, when parameters are less than 320k, A$^2$F outperforms SOTA methods for all scales, which proves its ability to better utilize the auxiliary features. Codes are available at https://github.com/wxxxxxxh/A2F-SR.



### Fast and Scalable Earth Texture Synthesis using Spatially Assembled Generative Adversarial Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2011.06776v1
- **DOI**: 10.1016/j.jconhyd.2021.103867
- **Categories**: **cs.CV**, cs.LG, eess.IV, physics.flu-dyn
- **Links**: [PDF](http://arxiv.org/pdf/2011.06776v1)
- **Published**: 2020-11-13 06:18:09+00:00
- **Updated**: 2020-11-13 06:18:09+00:00
- **Authors**: Sung Eun Kim, Hongkyu Yoon, Jonghyun Lee
- **Comment**: 17 pages, 11 figures, 2 tables, and a table in Appendix
- **Journal**: None
- **Summary**: The earth texture with complex morphological geometry and compositions such as shale and carbonate rocks, is typically characterized with sparse field samples because of an expensive and time-consuming characterization process. Accordingly, generating arbitrary large size of the geological texture with similar topological structures at a low computation cost has become one of the key tasks for realistic geomaterial reconstruction. Recently, generative adversarial neural networks (GANs) have demonstrated a potential of synthesizing input textural images and creating equiprobable geomaterial images. However, the texture synthesis with the GANs framework is often limited by the computational cost and scalability of the output texture size. In this study, we proposed a spatially assembled GANs (SAGANs) that can generate output images of an arbitrary large size regardless of the size of training images with computational efficiency. The performance of the SAGANs was evaluated with two and three dimensional (2D and 3D) rock image samples widely used in geostatistical reconstruction of the earth texture. We demonstrate SAGANs can generate the arbitrary large size of statistical realizations with connectivity and structural properties similar to training images, and also can generate a variety of realizations even on a single training image. In addition, the computational time was significantly improved compared to standard GANs frameworks.



### Adaptive Future Frame Prediction with Ensemble Network
- **Arxiv ID**: http://arxiv.org/abs/2011.06788v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.06788v2)
- **Published**: 2020-11-13 07:08:06+00:00
- **Updated**: 2020-11-16 01:43:53+00:00
- **Authors**: Wonjik Kim, Masayuki Tanaka, Masatoshi Okutomi, Yoko Sasaki
- **Comment**: Accepted at 25th International Conference on Pattern Recognition
  Workshop (ICPRW 2020)
- **Journal**: None
- **Summary**: Future frame prediction in videos is a challenging problem because videos include complicated movements and large appearance changes. Learning-based future frame prediction approaches have been proposed in kinds of literature. A common limitation of the existing learning-based approaches is a mismatch of training data and test data. In the future frame prediction task, we can obtain the ground truth data by just waiting for a few frames. It means we can update the prediction model online in the test phase. Then, we propose an adaptive update framework for the future frame prediction task. The proposed adaptive updating framework consists of a pre-trained prediction network, a continuous-updating prediction network, and a weight estimation network. We also show that our pre-trained prediction model achieves comparable performance to the existing state-of-the-art approaches. We demonstrate that our approach outperforms existing methods especially for dynamically changing scenes.



### Deep Template Matching for Pedestrian Attribute Recognition with the Auxiliary Supervision of Attribute-wise Keypoints
- **Arxiv ID**: http://arxiv.org/abs/2011.06798v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.06798v1)
- **Published**: 2020-11-13 07:52:26+00:00
- **Updated**: 2020-11-13 07:52:26+00:00
- **Authors**: Jiajun Zhang, Pengyuan Ren, Jianmin Li
- **Comment**: 8 pages, 5 figures
- **Journal**: None
- **Summary**: Pedestrian Attribute Recognition (PAR) has aroused extensive attention due to its important role in video surveillance scenarios. In most cases, the existence of a particular attribute is strongly related to a partial region. Recent works design complicated modules, e.g., attention mechanism and proposal of body parts to localize the attribute corresponding region. These works further prove that localization of attribute specific regions precisely will help in improving performance. However, these part-information-based methods are still not accurate as well as increasing model complexity which makes it hard to deploy on realistic applications. In this paper, we propose a Deep Template Matching based method to capture body parts features with less computation. Further, we also proposed an auxiliary supervision method that use human pose keypoints to guide the learning toward discriminative local cues. Extensive experiments show that the proposed method outperforms and has lower computational complexity, compared with the state-of-the-art approaches on large-scale pedestrian attribute datasets, including PETA, PA-100K, RAP, and RAPv2 zs.



### Learning Object Manipulation Skills via Approximate State Estimation from Real Videos
- **Arxiv ID**: http://arxiv.org/abs/2011.06813v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.06813v1)
- **Published**: 2020-11-13 08:53:47+00:00
- **Updated**: 2020-11-13 08:53:47+00:00
- **Authors**: Vladimír Petrík, Makarand Tapaswi, Ivan Laptev, Josef Sivic
- **Comment**: CoRL 2020, code at
  https://github.com/makarandtapaswi/Real2Sim_CoRL2020, project page at
  https://data.ciirc.cvut.cz/public/projects/2020Real2Sim/
- **Journal**: None
- **Summary**: Humans are adept at learning new tasks by watching a few instructional videos. On the other hand, robots that learn new actions either require a lot of effort through trial and error, or use expert demonstrations that are challenging to obtain. In this paper, we explore a method that facilitates learning object manipulation skills directly from videos. Leveraging recent advances in 2D visual recognition and differentiable rendering, we develop an optimization based method to estimate a coarse 3D state representation for the hand and the manipulated object(s) without requiring any supervision. We use these trajectories as dense rewards for an agent that learns to mimic them through reinforcement learning. We evaluate our method on simple single- and two-object actions from the Something-Something dataset. Our approach allows an agent to learn actions from single videos, while watching multiple demonstrations makes the policy more robust. We show that policies learned in a simulated environment can be easily transferred to a real robot.



### SHAD3S: A model to Sketch, Shade and Shadow
- **Arxiv ID**: http://arxiv.org/abs/2011.06822v3
- **DOI**: 10.1109/WACV48630.2021.00366
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.06822v3)
- **Published**: 2020-11-13 09:25:46+00:00
- **Updated**: 2021-09-05 02:16:43+00:00
- **Authors**: Raghav B. Venkataramaiyer, Abhishek Joshi, Saisha Narang, Vinay P. Namboodiri
- **Comment**: 10 pages, 11 figures, 2 tables Accepted to WACV 2021. Project Page:
  https://bvraghav.com/shad3s/
- **Journal**: 2021 IEEE Winter Conference on Applications of Computer Vision
  (WACV), 2021, pp. 3615-3624
- **Summary**: Hatching is a common method used by artists to accentuate the third dimension of a sketch, and to illuminate the scene. Our system SHAD3S attempts to compete with a human at hatching generic three-dimensional (3D) shapes, and also tries to assist her in a form exploration exercise. The novelty of our approach lies in the fact that we make no assumptions about the input other than that it represents a 3D shape, and yet, given a contextual information of illumination and texture, we synthesise an accurate hatch pattern over the sketch, without access to 3D or pseudo 3D. In the process, we contribute towards a) a cheap yet effective method to synthesise a sufficiently large high fidelity dataset, pertinent to task; b) creating a pipeline with conditional generative adversarial network (CGAN); and c) creating an interactive utility with GIMP, that is a tool for artists to engage with automated hatching or a form-exploration exercise. User evaluation of the tool suggests that the model performance does generalise satisfactorily over diverse input, both in terms of style as well as shape. A simple comparison of inception scores suggest that the generated distribution is as diverse as the ground truth.



### LULC classification by semantic segmentation of satellite images using FastFCN
- **Arxiv ID**: http://arxiv.org/abs/2011.06825v2
- **DOI**: 10.1109/ICAICT51780.2020.9333522
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.06825v2)
- **Published**: 2020-11-13 09:33:03+00:00
- **Updated**: 2020-12-03 19:50:31+00:00
- **Authors**: Md. Saif Hassan Onim, Aiman Rafeed Ehtesham, Amreen Anbar, A. K. M. Nazrul Islam, A. K. M. Mahbubur Rahman
- **Comment**: None
- **Journal**: None
- **Summary**: This paper analyses how well a Fast Fully Convolutional Network (FastFCN) semantically segments satellite images and thus classifies Land Use/Land Cover(LULC) classes. Fast-FCN was used on Gaofen-2 Image Dataset (GID-2) to segment them in five different classes: BuiltUp, Meadow, Farmland, Water and Forest. The results showed better accuracy (0.93), precision (0.99), recall (0.98) and mean Intersection over Union (mIoU)(0.97) than other approaches like using FCN-8 or eCognition, a readily available software. We presented a comparison between the results. We propose FastFCN to be both faster and more accurate automated method than other existing methods for LULC classification.



### FastTrack: an open-source software for tracking varying numbers of deformable objects
- **Arxiv ID**: http://arxiv.org/abs/2011.06837v1
- **DOI**: 10.1371/journal.pcbi.1008697
- **Categories**: **q-bio.QM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.06837v1)
- **Published**: 2020-11-13 09:52:58+00:00
- **Updated**: 2020-11-13 09:52:58+00:00
- **Authors**: Benjamin Gallois, Raphaël Candelier
- **Comment**: None
- **Journal**: None
- **Summary**: Analyzing the dynamical properties of mobile objects requires to extract trajectories from recordings, which is often done by tracking movies. We compiled a database of two-dimensional movies for very different biological and physical systems spanning a wide range of length scales and developed a general-purpose, optimized, open-source, cross-platform, easy to install and use, self-updating software called FastTrack. It can handle a changing number of deformable objects in a region of interest, and is particularly suitable for animal and cell tracking in two-dimensions. Furthermore, we introduce the probability of incursions as a new measure of a movie's trackability that doesn't require the knowledge of ground truth trajectories, since it is resilient to small amounts of errors and can be computed on the basis of an ad hoc tracking. We also leveraged the versatility and speed of FastTrack to implement an iterative algorithm determining a set of nearly-optimized tracking parameters -- yet further reducing the amount of human intervention -- and demonstrate that FastTrack can be used to explore the space of tracking parameters to optimize the number of swaps for a batch of similar movies. A benchmark shows that FastTrack is orders of magnitude faster than state-of-the-art tracking algorithms, with a comparable tracking accuracy. The source code is available under the GNU GPLv3 at https://github.com/FastTrackOrg/FastTrack and pre-compiled binaries for Windows, Mac and Linux are available at http://www.fasttrack.sh.



### Unified Multi-Modal Landmark Tracking for Tightly Coupled Lidar-Visual-Inertial Odometry
- **Arxiv ID**: http://arxiv.org/abs/2011.06838v3
- **DOI**: 10.1109/LRA.2021.3056380
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.06838v3)
- **Published**: 2020-11-13 09:54:03+00:00
- **Updated**: 2021-02-17 11:42:25+00:00
- **Authors**: David Wisth, Marco Camurri, Sandipan Das, Maurice Fallon
- **Comment**: Video: https://youtu.be/MjXYAHurWe8
- **Journal**: None
- **Summary**: We present an efficient multi-sensor odometry system for mobile platforms that jointly optimizes visual, lidar, and inertial information within a single integrated factor graph. This runs in real-time at full framerate using fixed lag smoothing. To perform such tight integration, a new method to extract 3D line and planar primitives from lidar point clouds is presented. This approach overcomes the suboptimality of typical frame-to-frame tracking methods by treating the primitives as landmarks and tracking them over multiple scans. True integration of lidar features with standard visual features and IMU is made possible using a subtle passive synchronization of lidar and camera frames. The lightweight formulation of the 3D features allows for real-time execution on a single CPU. Our proposed system has been tested on a variety of platforms and scenarios, including underground exploration with a legged robot and outdoor scanning with a dynamically moving handheld device, for a total duration of 96 min and 2.4 km traveled distance. In these test sequences, using only one exteroceptive sensor leads to failure due to either underconstrained geometry (affecting lidar) or textureless areas caused by aggressive lighting changes (affecting vision). In these conditions, our factor graph naturally uses the best information available from each sensor modality without any hard switches.



### Transductive Zero-Shot Learning using Cross-Modal CycleGAN
- **Arxiv ID**: http://arxiv.org/abs/2011.06850v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2011.06850v1)
- **Published**: 2020-11-13 10:37:29+00:00
- **Updated**: 2020-11-13 10:37:29+00:00
- **Authors**: Patrick Bordes, Eloi Zablocki, Benjamin Piwowarski, Patrick Gallinari
- **Comment**: None
- **Journal**: None
- **Summary**: In Computer Vision, Zero-Shot Learning (ZSL) aims at classifying unseen classes -- classes for which no matching training image exists. Most of ZSL works learn a cross-modal mapping between images and class labels for seen classes. However, the data distribution of seen and unseen classes might differ, causing a domain shift problem. Following this observation, transductive ZSL (T-ZSL) assumes that unseen classes and their associated images are known during training, but not their correspondence. As current T-ZSL approaches do not scale efficiently when the number of seen classes is high, we tackle this problem with a new model for T-ZSL based upon CycleGAN. Our model jointly (i) projects images on their seen class labels with a supervised objective and (ii) aligns unseen class labels and visual exemplars with adversarial and cycle-consistency objectives. We show the efficiency of our Cross-Modal CycleGAN model (CM-GAN) on the ImageNet T-ZSL task where we obtain state-of-the-art results. We further validate CM-GAN on a language grounding task, and on a new task that we propose: zero-shot sentence-to-image matching on MS COCO.



### Discriminative Feature Representation with Spatio-temporal Cues for Vehicle Re-identification
- **Arxiv ID**: http://arxiv.org/abs/2011.06852v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.06852v1)
- **Published**: 2020-11-13 10:50:21+00:00
- **Updated**: 2020-11-13 10:50:21+00:00
- **Authors**: J. Tu, C. Chen, X. Huang, J. He, X. Guan
- **Comment**: 12 pages, 9 figures
- **Journal**: None
- **Summary**: Vehicle re-identification (re-ID) aims to discover and match the target vehicles from a gallery image set taken by different cameras on a wide range of road networks. It is crucial for lots of applications such as security surveillance and traffic management. The remarkably similar appearances of distinct vehicles and the significant changes of viewpoints and illumination conditions take grand challenges to vehicle re-ID. Conventional solutions focus on designing global visual appearances without sufficient consideration of vehicles' spatiotamporal relationships in different images. In this paper, we propose a novel discriminative feature representation with spatiotemporal clues (DFR-ST) for vehicle re-ID. It is capable of building robust features in the embedding space by involving appearance and spatio-temporal information. Based on this multi-modal information, the proposed DFR-ST constructs an appearance model for a multi-grained visual representation by a two-stream architecture and a spatio-temporal metric to provide complementary information. Experimental results on two public datasets demonstrate DFR-ST outperforms the state-of-the-art methods, which validate the effectiveness of the proposed method.



### REPAC: Reliable estimation of phase-amplitude coupling in brain networks
- **Arxiv ID**: http://arxiv.org/abs/2011.06878v1
- **DOI**: 10.1109/ICASSP39728.2021.9414749
- **Categories**: **eess.SP**, cs.CV, cs.LG, q-bio.NC, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2011.06878v1)
- **Published**: 2020-11-13 12:26:54+00:00
- **Updated**: 2020-11-13 12:26:54+00:00
- **Authors**: Giulia Cisotto
- **Comment**: None
- **Journal**: 2021 IEEE International Conference on Acoustics, Speech and Signal
  Processing (ICASSP)
- **Summary**: Recent evidence has revealed cross-frequency coupling and, particularly, phase-amplitude coupling (PAC) as an important strategy for the brain to accomplish a variety of high-level cognitive and sensory functions. However, decoding PAC is still challenging. This contribution presents REPAC, a reliable and robust algorithm for modeling and detecting PAC events in EEG signals. First, we explain the synthesis of PAC-like EEG signals, with special attention to the most critical parameters that characterize PAC, i.e., SNR, modulation index, duration of coupling. Second, REPAC is introduced in detail. We use computer simulations to generate a set of random PAC-like EEG signals and test the performance of REPAC with regard to a baseline method. REPAC is shown to outperform the baseline method even with realistic values of SNR, e.g., -10 dB. They both reach accuracy levels around 99%, but REPAC leads to a significant improvement of sensitivity, from 20.11% to 65.21%, with comparable specificity (around 99%). REPAC is also applied to a real EEG signal showing preliminary encouraging results.



### Image Animation with Perturbed Masks
- **Arxiv ID**: http://arxiv.org/abs/2011.06922v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.06922v3)
- **Published**: 2020-11-13 14:17:17+00:00
- **Updated**: 2022-03-29 09:30:26+00:00
- **Authors**: Yoav Shalev, Lior Wolf
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel approach for image-animation of a source image by a driving video, both depicting the same type of object. We do not assume the existence of pose models and our method is able to animate arbitrary objects without the knowledge of the object's structure. Furthermore, both, the driving video and the source image are only seen during test-time. Our method is based on a shared mask generator, which separates the foreground object from its background, and captures the object's general pose and shape. To control the source of the identity of the output frame, we employ perturbations to interrupt the unwanted identity information on the driver's mask. A mask-refinement module then replaces the identity of the driver with the identity of the source. Conditioned on the source image, the transformed mask is then decoded by a multi-scale generator that renders a realistic image, in which the content of the source frame is animated by the pose in the driving video. Due to the lack of fully supervised data, we train on the task of reconstructing frames from the same video the source image is taken from. Our method is shown to greatly outperform the state-of-the-art methods on multiple benchmarks. Our code and samples are available at https://github.com/itsyoavshalev/Image-Animation-with-Perturbed-Masks.



### LEAN: graph-based pruning for convolutional neural networks by extracting longest chains
- **Arxiv ID**: http://arxiv.org/abs/2011.06923v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2011.06923v3)
- **Published**: 2020-11-13 14:17:51+00:00
- **Updated**: 2022-06-23 09:15:26+00:00
- **Authors**: Richard Schoonhoven, Allard A. Hendriksen, Daniël M. Pelt, K. Joost Batenburg
- **Comment**: 10 pages + 2 pages references. Code is publicly available at:
  https://github.com/schoonhovenrichard/LEAN_CNN_pruning
- **Journal**: None
- **Summary**: Neural network pruning techniques can substantially reduce the computational cost of applying convolutional neural networks (CNNs). Common pruning methods determine which convolutional filters to remove by ranking the filters individually, i.e., without taking into account their interdependence. In this paper, we advocate the viewpoint that pruning should consider the interdependence between series of consecutive operators. We propose the LongEst-chAiN (LEAN) method that prunes CNNs by using graph-based algorithms to select relevant chains of convolutions. A CNN is interpreted as a graph, with the operator norm of each operator as distance metric for the edges. LEAN pruning iteratively extracts the highest value path from the graph to keep. In our experiments, we test LEAN pruning on several image-to-image tasks, including the well-known CamVid dataset, and a real-world X-ray CT dataset. Results indicate that LEAN pruning can result in networks with similar accuracy, while using 1.7-12x fewer convolutional filters than existing approaches.



### SALAD: Self-Assessment Learning for Action Detection
- **Arxiv ID**: http://arxiv.org/abs/2011.06958v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.06958v1)
- **Published**: 2020-11-13 15:10:40+00:00
- **Updated**: 2020-11-13 15:10:40+00:00
- **Authors**: Guillaume Vaudaux-Ruth, Adrien Chan-Hon-Tong, Catherine Achard
- **Comment**: None
- **Journal**: None
- **Summary**: Literature on self-assessment in machine learning mainly focuses on the production of well-calibrated algorithms through consensus frameworks i.e. calibration is seen as a problem. Yet, we observe that learning to be properly confident could behave like a powerful regularization and thus, could be an opportunity to improve performance.Precisely, we show that used within a framework of action detection, the learning of a self-assessment score is able to improve the whole action localization process.Experimental results show that our approach outperforms the state-of-the-art on two action detection benchmarks. On THUMOS14 dataset, the mAP at tIoU@0.5 is improved from 42.8\% to 44.6\%, and from 50.4\% to 51.7\% on ActivityNet1.3 dataset. For lower tIoU values, we achieve even more significant improvements on both datasets.



### Efficient RGB-D Semantic Segmentation for Indoor Scene Analysis
- **Arxiv ID**: http://arxiv.org/abs/2011.06961v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2011.06961v3)
- **Published**: 2020-11-13 15:17:31+00:00
- **Updated**: 2021-04-07 14:41:24+00:00
- **Authors**: Daniel Seichter, Mona Köhler, Benjamin Lewandowski, Tim Wengefeld, Horst-Michael Gross
- **Comment**: To be published in IEEE International Conference on Robotics and
  Automation (ICRA) 2021; fixed reference in Fig. 4
- **Journal**: None
- **Summary**: Analyzing scenes thoroughly is crucial for mobile robots acting in different environments. Semantic segmentation can enhance various subsequent tasks, such as (semantically assisted) person perception, (semantic) free space detection, (semantic) mapping, and (semantic) navigation. In this paper, we propose an efficient and robust RGB-D segmentation approach that can be optimized to a high degree using NVIDIA TensorRT and, thus, is well suited as a common initial processing step in a complex system for scene analysis on mobile robots. We show that RGB-D segmentation is superior to processing RGB images solely and that it can still be performed in real time if the network architecture is carefully designed. We evaluate our proposed Efficient Scene Analysis Network (ESANet) on the common indoor datasets NYUv2 and SUNRGB-D and show that we reach state-of-the-art performance while enabling faster inference. Furthermore, our evaluation on the outdoor dataset Cityscapes shows that our approach is suitable for other areas of application as well. Finally, instead of presenting benchmark results only, we also show qualitative results in one of our indoor application scenarios.



### Deep-LIBRA: Artificial intelligence method for robust quantification of breast density with independent validation in breast cancer risk assessment
- **Arxiv ID**: http://arxiv.org/abs/2011.08001v3
- **DOI**: 10.1016/j.media.2021.102138
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.08001v3)
- **Published**: 2020-11-13 15:21:17+00:00
- **Updated**: 2021-10-19 02:04:38+00:00
- **Authors**: Omid Haji Maghsoudi, Aimilia Gastounioti, Christopher Scott, Lauren Pantalone, Fang-Fang Wu, Eric A. Cohen, Stacey Winham, Emily F. Conant, Celine Vachon, Despina Kontos
- **Comment**: None
- **Journal**: None
- **Summary**: Breast density is an important risk factor for breast cancer that also affects the specificity and sensitivity of screening mammography. Current federal legislation mandates reporting of breast density for all women undergoing breast screening. Clinically, breast density is assessed visually using the American College of Radiology Breast Imaging Reporting And Data System (BI-RADS) scale. Here, we introduce an artificial intelligence (AI) method to estimate breast percentage density (PD) from digital mammograms. Our method leverages deep learning (DL) using two convolutional neural network architectures to accurately segment the breast area. A machine-learning algorithm combining superpixel generation, texture feature analysis, and support vector machine is then applied to differentiate dense from non-dense tissue regions, from which PD is estimated. Our method has been trained and validated on a multi-ethnic, multi-institutional dataset of 15,661 images (4,437 women), and then tested on an independent dataset of 6,368 digital mammograms (1,702 women; cases=414) for both PD estimation and discrimination of breast cancer. On the independent dataset, PD estimates from Deep-LIBRA and an expert reader were strongly correlated (Spearman correlation coefficient = 0.90). Moreover, Deep-LIBRA yielded a higher breast cancer discrimination performance (area under the ROC curve, AUC = 0.611 [95% confidence interval (CI): 0.583, 0.639]) compared to four other widely-used research and commercial PD assessment methods (AUCs = 0.528 to 0.588). Our results suggest a strong agreement of PD estimates between Deep-LIBRA and gold-standard assessment by an expert reader, as well as improved performance in breast cancer risk assessment over state-of-the-art open-source and commercial methods.



### Transformer-Encoder Detector Module: Using Context to Improve Robustness to Adversarial Attacks on Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2011.06978v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.06978v1)
- **Published**: 2020-11-13 15:52:53+00:00
- **Updated**: 2020-11-13 15:52:53+00:00
- **Authors**: Faisal Alamri, Sinan Kalkan, Nicolas Pugeault
- **Comment**: Accepted for the 25th International Conference on Pattern Recognition
  (ICPR'2020)
- **Journal**: None
- **Summary**: Deep neural network approaches have demonstrated high performance in object recognition (CNN) and detection (Faster-RCNN) tasks, but experiments have shown that such architectures are vulnerable to adversarial attacks (FFF, UAP): low amplitude perturbations, barely perceptible by the human eye, can lead to a drastic reduction in labeling performance. This article proposes a new context module, called \textit{Transformer-Encoder Detector Module}, that can be applied to an object detector to (i) improve the labeling of object instances; and (ii) improve the detector's robustness to adversarial attacks. The proposed model achieves higher mAP, F1 scores and AUC average score of up to 13\% compared to the baseline Faster-RCNN detector, and an mAP score 8 points higher on images subjected to FFF or UAP attacks due to the inclusion of both contextual and visual features extracted from scene and encoded into the model. The result demonstrates that a simple ad-hoc context module can improve the reliability of object detectors significantly.



### Multi-layered tensor networks for image classification
- **Arxiv ID**: http://arxiv.org/abs/2011.06982v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2011.06982v2)
- **Published**: 2020-11-13 16:01:26+00:00
- **Updated**: 2021-03-19 11:37:15+00:00
- **Authors**: Raghavendra Selvan, Silas Ørting, Erik B Dam
- **Comment**: Updated version with exact computation costs. 6 pages. Accepted to
  the First Workshop on Quantum Tensor Networks in Machine Learning. In
  conjunction with 34th NeurIPS, 2020. Source code at
  https://github.com/raghavian/mltn
- **Journal**: None
- **Summary**: The recently introduced locally orderless tensor network (LoTeNet) for supervised image classification uses matrix product state (MPS) operations on grids of transformed image patches. The resulting patch representations are combined back together into the image space and aggregated hierarchically using multiple MPS blocks per layer to obtain the final decision rules. In this work, we propose a non-patch based modification to LoTeNet that performs one MPS operation per layer, instead of several patch-level operations. The spatial information in the input images to MPS blocks at each layer is squeezed into the feature dimension, similar to LoTeNet, to maximise retained spatial correlation between pixels when images are flattened into 1D vectors. The proposed multi-layered tensor network (MLTN) is capable of learning linear decision boundaries in high dimensional spaces in a multi-layered setting, which results in a reduction in the computation cost compared to LoTeNet without any degradation in performance.



### Metastatic Cancer Image Classification Based On Deep Learning Method
- **Arxiv ID**: http://arxiv.org/abs/2011.06984v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.06984v1)
- **Published**: 2020-11-13 16:04:39+00:00
- **Updated**: 2020-11-13 16:04:39+00:00
- **Authors**: Guanwen Qiu, Xiaobing Yu, Baolin Sun, Yunpeng Wang, Lipei Zhang
- **Comment**: 4 pages, 3 figures, 1 table, accepted by ICCECE
- **Journal**: None
- **Summary**: Using histopathological images to automatically classify cancer is a difficult task for accurately detecting cancer, especially to identify metastatic cancer in small image patches obtained from larger digital pathology scans. Computer diagnosis technology has attracted wide attention from researchers. In this paper, we propose a noval method which combines the deep learning algorithm in image classification, the DenseNet169 framework and Rectified Adam optimization algorithm. The connectivity pattern of DenseNet is direct connections from any layer to all consecutive layers, which can effectively improve the information flow between different layers. With the fact that RAdam is not easy to fall into a local optimal solution, and it can converge quickly in model training. The experimental results shows that our model achieves superior performance over the other classical convolutional neural networks approaches, such as Vgg19, Resnet34, Resnet50. In particular, the Auc-Roc score of our DenseNet169 model is 1.77% higher than Vgg19 model, and the Accuracy score is 1.50% higher. Moreover, we also study the relationship between loss value and batches processed during the training stage and validation stage, and obtain some important and interesting findings.



### Relative Drone-Ground Vehicle Localization using LiDAR and Fisheye Cameras through Direct and Indirect Observations
- **Arxiv ID**: http://arxiv.org/abs/2011.07008v3
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.07008v3)
- **Published**: 2020-11-13 16:41:55+00:00
- **Updated**: 2020-11-17 10:37:49+00:00
- **Authors**: Jan Hausberg, Ryoichi Ishikawa, Menandro Roxas, Takeshi Oishi
- **Comment**: None
- **Journal**: None
- **Summary**: Estimating the pose of an unmanned aerial vehicle (UAV) or drone is a challenging task. It is useful for many applications such as navigation, surveillance, tracking objects on the ground, and 3D reconstruction. In this work, we present a LiDAR-camera-based relative pose estimation method between a drone and a ground vehicle, using a LiDAR sensor and a fisheye camera on the vehicle's roof and another fisheye camera mounted under the drone. The LiDAR sensor directly observes the drone and measures its position, and the two cameras estimate the relative orientation using indirect observation of the surrounding objects. We propose a dynamically adaptive kernel-based method for drone detection and tracking using the LiDAR. We detect vanishing points in both cameras and find their correspondences to estimate the relative orientation. Additionally, we propose a rotation correction technique by relying on the observed motion of the drone through the LiDAR. In our experiments, we were able to achieve very fast initial detection and real-time tracking of the drone. Our method is fully automatic.



### NightVision: Generating Nighttime Satellite Imagery from Infra-Red Observations
- **Arxiv ID**: http://arxiv.org/abs/2011.07017v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2011.07017v2)
- **Published**: 2020-11-13 16:55:46+00:00
- **Updated**: 2020-12-08 15:43:52+00:00
- **Authors**: Paula Harder, William Jones, Redouane Lguensat, Shahine Bouabid, James Fulton, Dánell Quesada-Chacón, Aris Marcolongo, Sofija Stefanović, Yuhan Rao, Peter Manshausen, Duncan Watson-Parris
- **Comment**: None
- **Journal**: None
- **Summary**: The recent explosion in applications of machine learning to satellite imagery often rely on visible images and therefore suffer from a lack of data during the night. The gap can be filled by employing available infra-red observations to generate visible images. This work presents how deep learning can be applied successfully to create those images by using U-Net based architectures. The proposed methods show promising results, achieving a structural similarity index (SSIM) up to 86\% on an independent test set and providing visually convincing output images, generated from infra-red observations.



### A Study of Domain Generalization on Ultrasound-based Multi-Class Segmentation of Arteries, Veins, Ligaments, and Nerves Using Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/2011.07019v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2011.07019v1)
- **Published**: 2020-11-13 16:59:20+00:00
- **Updated**: 2020-11-13 16:59:20+00:00
- **Authors**: Edward Chen, Tejas Sudharshan Mathai, Vinit Sarode, Howie Choset, John Galeotti
- **Comment**: Machine Learning for Health (ML4H) at NeurIPS 2020 - Extended
  Abstract
- **Journal**: None
- **Summary**: Identifying landmarks in the femoral area is crucial for ultrasound (US) -based robot-guided catheter insertion, and their presentation varies when imaged with different scanners. As such, the performance of past deep learning-based approaches is also narrowly limited to the training data distribution; this can be circumvented by fine-tuning all or part of the model, yet the effects of fine-tuning are seldom discussed. In this work, we study the US-based segmentation of multiple classes through transfer learning by fine-tuning different contiguous blocks within the model, and evaluating on a gamut of US data from different scanners and settings. We propose a simple method for predicting generalization on unseen datasets and observe statistically significant differences between the fine-tuning methods while working towards domain generalization.



### Automatic segmentation with detection of local segmentation failures in cardiac MRI
- **Arxiv ID**: http://arxiv.org/abs/2011.07025v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.07025v1)
- **Published**: 2020-11-13 17:19:05+00:00
- **Updated**: 2020-11-13 17:19:05+00:00
- **Authors**: Jörg Sander, Bob D. de Vos, Ivana Išgum
- **Comment**: None
- **Journal**: currently under press - Scientific Reports (2020)
- **Summary**: Segmentation of cardiac anatomical structures in cardiac magnetic resonance images (CMRI) is a prerequisite for automatic diagnosis and prognosis of cardiovascular diseases. To increase robustness and performance of segmentation methods this study combines automatic segmentation and assessment of segmentation uncertainty in CMRI to detect image regions containing local segmentation failures. Three state-of-the-art convolutional neural networks (CNN) were trained to automatically segment cardiac anatomical structures and obtain two measures of predictive uncertainty: entropy and a measure derived by MC-dropout. Thereafter, using the uncertainties another CNN was trained to detect local segmentation failures that potentially need correction by an expert. Finally, manual correction of the detected regions was simulated. Using publicly available CMR scans from the MICCAI 2017 ACDC challenge, the impact of CNN architecture and loss function for segmentation, and the uncertainty measure was investigated. Performance was evaluated using the Dice coefficient and 3D Hausdorff distance between manual and automatic segmentation. The experiments reveal that combining automatic segmentation with simulated manual correction of detected segmentation failures leads to statistically significant performance increase.



### Detection of masses and architectural distortions in digital breast tomosynthesis: a publicly available dataset of 5,060 patients and a deep learning model
- **Arxiv ID**: http://arxiv.org/abs/2011.07995v4
- **DOI**: 10.1001/jamanetworkopen.2021.19100
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.07995v4)
- **Published**: 2020-11-13 18:33:31+00:00
- **Updated**: 2022-11-20 17:34:21+00:00
- **Authors**: Mateusz Buda, Ashirbani Saha, Ruth Walsh, Sujata Ghate, Nianyi Li, Albert Święcicki, Joseph Y. Lo, Maciej A. Mazurowski
- **Comment**: None
- **Journal**: JAMA Netw Open. 2021;4(8):e2119100
- **Summary**: Breast cancer screening is one of the most common radiological tasks with over 39 million exams performed each year. While breast cancer screening has been one of the most studied medical imaging applications of artificial intelligence, the development and evaluation of the algorithms are hindered due to the lack of well-annotated large-scale publicly available datasets. This is particularly an issue for digital breast tomosynthesis (DBT) which is a relatively new breast cancer screening modality. We have curated and made publicly available a large-scale dataset of digital breast tomosynthesis images. It contains 22,032 reconstructed DBT volumes belonging to 5,610 studies from 5,060 patients. This included four groups: (1) 5,129 normal studies, (2) 280 studies where additional imaging was needed but no biopsy was performed, (3) 112 benign biopsied studies, and (4) 89 studies with cancer. Our dataset included masses and architectural distortions which were annotated by two experienced radiologists. Additionally, we developed a single-phase deep learning detection model and tested it using our dataset to serve as a baseline for future research. Our model reached a sensitivity of 65% at 2 false positives per breast. Our large, diverse, and highly-curated dataset will facilitate development and evaluation of AI algorithms for breast cancer screening through providing data for training as well as common set of cases for model validation. The performance of the model developed in our study shows that the task remains challenging and will serve as a baseline for future model development.



### Using Graph Neural Networks to Reconstruct Ancient Documents
- **Arxiv ID**: http://arxiv.org/abs/2011.07048v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.07048v1)
- **Published**: 2020-11-13 18:36:36+00:00
- **Updated**: 2020-11-13 18:36:36+00:00
- **Authors**: Cecilia Ostertag, Marie Beurton-Aimar
- **Comment**: Accepted at Pattern Recognition for Cultural Heritage (PatReCH) -
  Workshop in conjunction with ICPR 2020
- **Journal**: None
- **Summary**: In recent years, machine learning and deep learning approaches such as artificial neural networks have gained in popularity for the resolution of automatic puzzle resolution problems. Indeed, these methods are able to extract high-level representations from images, and then can be trained to separate matching image pieces from non-matching ones. These applications have many similarities to the problem of ancient document reconstruction from partially recovered fragments. In this work we present a solution based on a Graph Neural Network, using pairwise patch information to assign labels to edges representing the spatial relationships between pairs. This network classifies the relationship between a source and a target patch as being one of Up, Down, Left, Right or None. By doing so for all edges, our model outputs a new graph representing a reconstruction proposal. Finally, we show that our model is not only able to provide correct classifications at the edge-level, but also to generate partial or full reconstruction graphs from a set of patches.



### RAMP-CNN: A Novel Neural Network for Enhanced Automotive Radar Object Recognition
- **Arxiv ID**: http://arxiv.org/abs/2011.08981v2
- **DOI**: 10.1109/JSEN.2020.3036047
- **Categories**: **eess.SP**, cs.AI, cs.CV, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2011.08981v2)
- **Published**: 2020-11-13 19:12:12+00:00
- **Updated**: 2022-04-28 19:42:52+00:00
- **Authors**: Xiangyu Gao, Guanbin Xing, Sumit Roy, Hui Liu
- **Comment**: 15 pages
- **Journal**: IEEE Sensor Journal, 2020
- **Summary**: Millimeter-wave radars are being increasingly integrated into commercial vehicles to support new advanced driver-assistance systems by enabling robust and high-performance object detection, localization, as well as recognition - a key component of new environmental perception. In this paper, we propose a novel radar multiple-perspectives convolutional neural network (RAMP-CNN) that extracts the location and class of objects based on further processing of the range-velocity-angle (RVA) heatmap sequences. To bypass the complexity of 4D convolutional neural networks (NN), we propose to combine several lower-dimension NN models within our RAMP-CNN model that nonetheless approaches the performance upper-bound with lower complexity. The extensive experiments show that the proposed RAMP-CNN model achieves better average recall and average precision than prior works in all testing scenarios. Besides, the RAMP-CNN model is validated to work robustly under nighttime, which enables low-cost radars as a potential substitute for pure optical sensing under severe conditions.



### Reducing Inference Latency with Concurrent Architectures for Image Recognition
- **Arxiv ID**: http://arxiv.org/abs/2011.07092v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.07092v1)
- **Published**: 2020-11-13 19:15:02+00:00
- **Updated**: 2020-11-13 19:15:02+00:00
- **Authors**: Ramyad Hadidi, Jiashen Cao, Michael S. Ryoo, Hyesoon Kim
- **Comment**: None
- **Journal**: None
- **Summary**: Satisfying the high computation demand of modern deep learning architectures is challenging for achieving low inference latency. The current approaches in decreasing latency only increase parallelism within a layer. This is because architectures typically capture a single-chain dependency pattern that prevents efficient distribution with a higher concurrency (i.e., simultaneous execution of one inference among devices). Such single-chain dependencies are so widespread that even implicitly biases recent neural architecture search (NAS) studies. In this visionary paper, we draw attention to an entirely new space of NAS that relaxes the single-chain dependency to provide higher concurrency and distribution opportunities. To quantitatively compare these architectures, we propose a score that encapsulates crucial metrics such as communication, concurrency, and load balancing. Additionally, we propose a new generator and transformation block that consistently deliver superior architectures compared to current state-of-the-art methods. Finally, our preliminary results show that these new architectures reduce the inference latency and deserve more attention.



### Benchmarking Domain Randomisation for Visual Sim-to-Real Transfer
- **Arxiv ID**: http://arxiv.org/abs/2011.07112v3
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.07112v3)
- **Published**: 2020-11-13 20:13:18+00:00
- **Updated**: 2021-05-21 12:22:11+00:00
- **Authors**: Raghad Alghonaim, Edward Johns
- **Comment**: Published at ICRA 2021. For project page, please visit:
  https://www.robot-learning.uk/benchmarking-domain-randomisation
- **Journal**: None
- **Summary**: Domain randomisation is a very popular method for visual sim-to-real transfer in robotics, due to its simplicity and ability to achieve transfer without any real-world images at all. Nonetheless, a number of design choices must be made to achieve optimal transfer. In this paper, we perform a comprehensive benchmarking study on these different choices, with two key experiments evaluated on a real-world object pose estimation task. First, we study the rendering quality, and find that a small number of high-quality images is superior to a large number of low-quality images. Second, we study the type of randomisation, and find that both distractors and textures are important for generalisation to novel environments.



### Deep Multi-view Image Fusion for Soybean Yield Estimation in Breeding Applications Deep Multi-view Image Fusion for Soybean Yield Estimation in Breeding Applications
- **Arxiv ID**: http://arxiv.org/abs/2011.07118v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.07118v1)
- **Published**: 2020-11-13 20:37:04+00:00
- **Updated**: 2020-11-13 20:37:04+00:00
- **Authors**: Luis G Riera, Matthew E. Carroll, Zhisheng Zhang, Johnathon M. Shook, Sambuddha Ghosal, Tianshuang Gao, Arti Singh, Sourabh Bhattacharya, Baskar Ganapathysubramanian, Asheesh K. Singh, Soumik Sarkar
- **Comment**: 18 pages, 8 figures, and 3 Tables
- **Journal**: None
- **Summary**: Reliable seed yield estimation is an indispensable step in plant breeding programs geared towards cultivar development in major row crops. The objective of this study is to develop a machine learning (ML) approach adept at soybean [\textit{Glycine max} L. (Merr.)] pod counting to enable genotype seed yield rank prediction from in-field video data collected by a ground robot. To meet this goal, we developed a multi-view image-based yield estimation framework utilizing deep learning architectures. Plant images captured from different angles were fused to estimate the yield and subsequently to rank soybean genotypes for application in breeding decisions. We used data from controlled imaging environment in field, as well as from plant breeding test plots in field to demonstrate the efficacy of our framework via comparing performance with manual pod counting and yield estimation.   Our results demonstrate the promise of ML models in making breeding decisions with significant reduction of time and human effort, and opening new breeding methods avenues to develop cultivars.



### Sparse Representations of Positive Functions via First and Second-Order Pseudo-Mirror Descent
- **Arxiv ID**: http://arxiv.org/abs/2011.07142v4
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/2011.07142v4)
- **Published**: 2020-11-13 21:54:28+00:00
- **Updated**: 2022-05-03 20:42:27+00:00
- **Authors**: Abhishek Chakraborty, Ketan Rajawat, Alec Koppel
- **Comment**: None
- **Journal**: None
- **Summary**: We consider expected risk minimization problems when the range of the estimator is required to be nonnegative, motivated by the settings of maximum likelihood estimation (MLE) and trajectory optimization. To facilitate nonlinear interpolation, we hypothesize that the search space is a Reproducing Kernel Hilbert Space (RKHS). We develop first and second-order variants of stochastic mirror descent employing (i) \emph{pseudo-gradients} and (ii) complexity-reducing projections. Compressive projection in the first-order scheme is executed via kernel orthogonal matching pursuit (KOMP), which overcomes the fact that the vanilla RKHS parameterization grows unbounded with the iteration index in the stochastic setting. Moreover, pseudo-gradients are needed when gradient estimates for cost are only computable up to some numerical error, which arise in, e.g., integral approximations. Under constant step-size and compression budget, we establish tradeoffs between the radius of convergence of the expected sub-optimality and the projection budget parameter, as well as non-asymptotic bounds on the model complexity. To refine the solution's precision, we develop a second-order extension which employs recursively averaged pseudo-gradient outer-products to approximate the Hessian inverse, whose convergence in mean is established under an additional eigenvalue decay condition on the Hessian of the optimal RKHS element, which is unique to this work. Experiments demonstrate favorable performance on inhomogeneous Poisson Process intensity estimation in practice.



