# Arxiv Papers in cs.CV on 2020-11-19
### Deep Multi-view Depth Estimation with Predicted Uncertainty
- **Arxiv ID**: http://arxiv.org/abs/2011.09594v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2011.09594v2)
- **Published**: 2020-11-19 00:22:09+00:00
- **Updated**: 2021-03-27 14:33:40+00:00
- **Authors**: Tong Ke, Tien Do, Khiem Vuong, Kourosh Sartipi, Stergios I. Roumeliotis
- **Comment**: IEEE International Conference on Robotics and Automation (ICRA 2021)
- **Journal**: None
- **Summary**: In this paper, we address the problem of estimating dense depth from a sequence of images using deep neural networks. Specifically, we employ a dense-optical-flow network to compute correspondences and then triangulate the point cloud to obtain an initial depth map.Parts of the point cloud, however, may be less accurate than others due to lack of common observations or small parallax. To further increase the triangulation accuracy, we introduce a depth-refinement network (DRN) that optimizes the initial depth map based on the image's contextual cues. In particular, the DRN contains an iterative refinement module (IRM) that improves the depth accuracy over iterations by refining the deep features. Lastly, the DRN also predicts the uncertainty in the refined depths, which is desirable in applications such as measurement selection for scene reconstruction. We show experimentally that our algorithm outperforms state-of-the-art approaches in terms of depth accuracy, and verify that our predicted uncertainty is highly correlated to the actual depth error.



### Bidirectional RNN-based Few Shot Learning for 3D Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2011.09608v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2011.09608v1)
- **Published**: 2020-11-19 01:44:55+00:00
- **Updated**: 2020-11-19 01:44:55+00:00
- **Authors**: Soopil Kim, Sion An, Philip Chikontwe, Sang Hyun Park
- **Comment**: Submitted to AAAI21
- **Journal**: None
- **Summary**: Segmentation of organs of interest in 3D medical images is necessary for accurate diagnosis and longitudinal studies. Though recent advances using deep learning have shown success for many segmentation tasks, large datasets are required for high performance and the annotation process is both time consuming and labor intensive. In this paper, we propose a 3D few shot segmentation framework for accurate organ segmentation using limited training samples of the target organ annotation. To achieve this, a U-Net like network is designed to predict segmentation by learning the relationship between 2D slices of support data and a query image, including a bidirectional gated recurrent unit (GRU) that learns consistency of encoded features between adjacent slices. Also, we introduce a transfer learning method to adapt the characteristics of the target image and organ by updating the model before testing with arbitrary support and query data sampled from the support data. We evaluate our proposed model using three 3D CT datasets with annotations of different organs. Our model yielded significantly improved performance over state-of-the-art few shot segmentation models and was comparable to a fully supervised model trained with more target training data.



### Abnormal Event Detection in Urban Surveillance Videos Using GAN and Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/2011.09619v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.09619v1)
- **Published**: 2020-11-19 02:39:35+00:00
- **Updated**: 2020-11-19 02:39:35+00:00
- **Authors**: Ali Atghaei, Soroush Ziaeinejad, Mohammad Rahmati
- **Comment**: 7 pages, 9 figures, 3 tables
- **Journal**: None
- **Summary**: Abnormal event detection (AED) in urban surveillance videos has multiple challenges. Unlike other computer vision problems, the AED is not solely dependent on the content of frames. It also depends on the appearance of the objects and their movements in the scene. Various methods have been proposed to address the AED problem. Among those, deep learning based methods show the best results. This paper is based on deep learning methods and provides an effective way to detect and locate abnormal events in videos by handling spatio temporal data. This paper uses generative adversarial networks (GANs) and performs transfer learning algorithms on pre trained convolutional neural network (CNN) which result in an accurate and efficient model. The efficiency of the model is further improved by processing the optical flow information of the video. This paper runs experiments on two benchmark datasets for AED problem (UCSD Peds1 and UCSD Peds2) and compares the results with other previous methods. The comparisons are based on various criteria such as area under curve (AUC) and true positive rate (TPR). Experimental results show that the proposed method can effectively detect and locate abnormal events in crowd scenes.



### Watch and Learn: Mapping Language and Noisy Real-world Videos with Self-supervision
- **Arxiv ID**: http://arxiv.org/abs/2011.09634v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.09634v2)
- **Published**: 2020-11-19 03:43:56+00:00
- **Updated**: 2021-01-11 09:52:47+00:00
- **Authors**: Yujie Zhong, Linhai Xie, Sen Wang, Lucia Specia, Yishu Miao
- **Comment**: NeurIPS 2020 Self-Supervised Learning Workshop
- **Journal**: None
- **Summary**: In this paper, we teach machines to understand visuals and natural language by learning the mapping between sentences and noisy video snippets without explicit annotations. Firstly, we define a self-supervised learning framework that captures the cross-modal information. A novel adversarial learning module is then introduced to explicitly handle the noises in the natural videos, where the subtitle sentences are not guaranteed to be strongly corresponded to the video snippets. For training and evaluation, we contribute a new dataset `ApartmenTour' that contains a large number of online videos and subtitles. We carry out experiments on the bidirectional retrieval tasks between sentences and videos, and the results demonstrate that our proposed model achieves the state-of-the-art performance on both retrieval tasks and exceeds several strong baselines. The dataset can be downloaded at https://github.com/zyj-13/WAL.



### HMFlow: Hybrid Matching Optical Flow Network for Small and Fast-Moving Objects
- **Arxiv ID**: http://arxiv.org/abs/2011.09654v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.09654v1)
- **Published**: 2020-11-19 04:58:13+00:00
- **Updated**: 2020-11-19 04:58:13+00:00
- **Authors**: Suihanjin Yu, Youmin Zhang, Chen Wang, Xiao Bai, Liang Zhang, Edwin R. Hancock
- **Comment**: 8 pages, 10 figures
- **Journal**: None
- **Summary**: In optical flow estimation task, coarse-to-fine (C2F) warping strategy is widely used to deal with the large displacement problem and provides efficiency and speed. However, limited by the small search range between the first images and warped second images, current coarse-to-fine optical flow networks fail to capture small and fast-moving objects which disappear at coarse resolution levels. To address this problem, we introduce a lightweight but effective Global Matching Component (GMC) to grab global matching features. We propose a new Hybrid Matching Optical Flow Network (HMFlow) by integrating GMC into existing coarse-to-fine networks seamlessly. Besides keeping in high accuracy and small model size, our proposed HMFlow can apply global matching features to guide the network to discover the small and fast-moving objects mismatched by local matching features. We also build a new dataset, named Small and Fast-Moving Chairs (SFChairs), for evaluation. The experimental results show that our proposed network achieves considerable performance, especially at regions with small and fast-moving objects.



### Dense Label Encoding for Boundary Discontinuity Free Rotation Detection
- **Arxiv ID**: http://arxiv.org/abs/2011.09670v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2011.09670v4)
- **Published**: 2020-11-19 05:42:02+00:00
- **Updated**: 2021-05-25 08:54:16+00:00
- **Authors**: Xue Yang, Liping Hou, Yue Zhou, Wentao Wang, Junchi Yan
- **Comment**: 12 pages, 6 figures, 9 tables, accepted by CVPR21
- **Journal**: None
- **Summary**: Rotation detection serves as a fundamental building block in many visual applications involving aerial image, scene text, and face etc. Differing from the dominant regression-based approaches for orientation estimation, this paper explores a relatively less-studied methodology based on classification. The hope is to inherently dismiss the boundary discontinuity issue as encountered by the regression-based detectors. We propose new techniques to push its frontier in two aspects: i) new encoding mechanism: the design of two Densely Coded Labels (DCL) for angle classification, to replace the Sparsely Coded Label (SCL) in existing classification-based detectors, leading to three times training speed increase as empirically observed across benchmarks, further with notable improvement in detection accuracy; ii) loss re-weighting: we propose Angle Distance and Aspect Ratio Sensitive Weighting (ADARSW), which improves the detection accuracy especially for square-like objects, by making DCL-based detectors sensitive to angular distance and object's aspect ratio. Extensive experiments and visual analysis on large-scale public datasets for aerial images i.e. DOTA, UCAS-AOD, HRSC2016, as well as scene text dataset ICDAR2015 and MLT, show the effectiveness of our approach. The source code is available at https://github.com/Thinklab-SJTU/DCL_RetinaNet_Tensorflow and is also integrated in our open source rotation detection benchmark: https://github.com/yangxue0827/RotationDetection.



### Defocus Blur Detection via Salient Region Detection Prior
- **Arxiv ID**: http://arxiv.org/abs/2011.09677v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.09677v1)
- **Published**: 2020-11-19 05:56:11+00:00
- **Updated**: 2020-11-19 05:56:11+00:00
- **Authors**: Ming Qian, Min Xia, Chunyi Sun, Zhiwei Wang, Liguo Weng
- **Comment**: None
- **Journal**: None
- **Summary**: Defocus blur always occurred in photos when people take photos by Digital Single Lens Reflex Camera(DSLR), giving salient region and aesthetic pleasure. Defocus blur Detection aims to separate the out-of-focus and depth-of-field areas in photos, which is an important work in computer vision. Current works for defocus blur detection mainly focus on the designing of networks, the optimizing of the loss function, and the application of multi-stream strategy, meanwhile, these works do not pay attention to the shortage of training data. In this work, to address the above data-shortage problem, we turn to rethink the relationship between two tasks: defocus blur detection and salient region detection. In an image with bokeh effect, it is obvious that the salient region and the depth-of-field area overlap in most cases. So we first train our network on the salient region detection tasks, then transfer the pre-trained model to the defocus blur detection tasks. Besides, we propose a novel network for defocus blur detection. Experiments show that our transfer strategy works well on many current models, and demonstrate the superiority of our network.



### Deep LF-Net: Semantic Lung Segmentation from Indian Chest Radiographs Including Severely Unhealthy Images
- **Arxiv ID**: http://arxiv.org/abs/2011.09695v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.09695v1)
- **Published**: 2020-11-19 07:21:02+00:00
- **Updated**: 2020-11-19 07:21:02+00:00
- **Authors**: Anushikha Singh, Brejesh Lall, B. K. Panigrahi, Anjali Agrawal, Anurag Agrawal, DJ Christopher, Balamugesh Thangakunam
- **Comment**: None
- **Journal**: None
- **Summary**: A chest radiograph, commonly called chest x-ray (CxR), plays a vital role in the diagnosis of various lung diseases, such as lung cancer, tuberculosis, pneumonia, and many more. Automated segmentation of the lungs is an important step to design a computer-aided diagnostic tool for examination of a CxR. Precise lung segmentation is considered extremely challenging because of variance in the shape of the lung caused by health issues, age, and gender. The proposed work investigates the use of an efficient deep convolutional neural network for accurate segmentation of lungs from CxR. We attempt an end to end DeepLabv3+ network which integrates DeepLab architecture, encoder-decoder, and dilated convolution for semantic lung segmentation with fast training and high accuracy. We experimented with the different pre-trained base networks: Resnet18 and Mobilenetv2, associated with the Deeplabv3+ model for performance analysis. The proposed approach does not require any pre-processing technique on chest x-ray images before being fed to a neural network. Morphological operations were used to remove false positives that occurred during semantic segmentation. We construct a CxR dataset of the Indian population that contain healthy and unhealthy CxRs of clinically confirmed patients of tuberculosis, chronic obstructive pulmonary disease, interstitial lung disease, pleural effusion, and lung cancer. The proposed method is tested on 688 images of our Indian CxR dataset including images with severe abnormal findings to validate its robustness. We also experimented on commonly used benchmark datasets such as Japanese Society of Radiological Technology; Montgomery County, USA; and Shenzhen, China for state-of-the-art comparison. The performance of our method is tested against techniques described in the literature and achieved the highest accuracy for lung segmentation on Indian and public datasets.



### Deep Motion Blind Video Stabilization
- **Arxiv ID**: http://arxiv.org/abs/2011.09697v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.09697v2)
- **Published**: 2020-11-19 07:26:06+00:00
- **Updated**: 2021-10-22 08:28:08+00:00
- **Authors**: Muhammad Kashif Ali, Sangjoon Yu, Tae Hyun Kim
- **Comment**: None
- **Journal**: None
- **Summary**: Despite the advances in the field of generative models in computer vision, video stabilization still lacks a pure regressive deep-learning-based formulation. Deep video stabilization is generally formulated with the help of explicit motion estimation modules due to the lack of a dataset containing pairs of videos with similar perspective but different motion. Therefore, the deep learning approaches for this task have difficulties in the pixel-level synthesis of latent stabilized frames, and resort to motion estimation modules for indirect transformations of the unstable frames to stabilized frames, leading to the loss of visual content near the frame boundaries. In this work, we aim to declutter this over-complicated formulation of video stabilization with the help of a novel dataset that contains pairs of training videos with similar perspective but different motion, and verify its effectiveness by successfully learning motion blind full-frame video stabilization through employing strictly conventional generative techniques and further improve the stability through a curriculum-learning inspired adversarial training strategy. Through extensive experimentation, we show the quantitative and qualitative advantages of the proposed approach to the state-of-the-art video stabilization approaches. Moreover, our method achieves $\sim3\times$ speed-up over the currently available fastest video stabilization methods.



### Style Intervention: How to Achieve Spatial Disentanglement with Style-based Generators?
- **Arxiv ID**: http://arxiv.org/abs/2011.09699v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.09699v1)
- **Published**: 2020-11-19 07:37:31+00:00
- **Updated**: 2020-11-19 07:37:31+00:00
- **Authors**: Yunfan Liu, Qi Li, Zhenan Sun, Tieniu Tan
- **Comment**: None
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) with style-based generators (e.g. StyleGAN) successfully enable semantic control over image synthesis, and recent studies have also revealed that interpretable image translations could be obtained by modifying the latent code. However, in terms of the low-level image content, traveling in the latent space would lead to `spatially entangled changes' in corresponding images, which is undesirable in many real-world applications where local editing is required. To solve this problem, we analyze properties of the 'style space' and explore the possibility of controlling the local translation with pre-trained style-based generators. Concretely, we propose 'Style Intervention', a lightweight optimization-based algorithm which could adapt to arbitrary input images and render natural translation effects under flexible objectives. We verify the performance of the proposed framework in facial attribute editing on high-resolution images, where both photo-realism and consistency are required. Extensive qualitative results demonstrate the effectiveness of our method, and quantitative measurements also show that the proposed algorithm outperforms state-of-the-art benchmarks in various aspects.



### Spectral Response Function Guided Deep Optimization-driven Network for Spectral Super-resolution
- **Arxiv ID**: http://arxiv.org/abs/2011.09701v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.09701v2)
- **Published**: 2020-11-19 07:52:45+00:00
- **Updated**: 2020-12-08 13:38:53+00:00
- **Authors**: Jiang He, Jie Li, Qiangqiang Yuan, Huanfeng Shen, Liangpei Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Hyperspectral images are crucial for many research works. Spectral super-resolution (SSR) is a method used to obtain high spatial resolution (HR) hyperspectral images from HR multispectral images. Traditional SSR methods include model-driven algorithms and deep learning. By unfolding a variational method, this paper proposes an optimization-driven convolutional neural network (CNN) with a deep spatial-spectral prior, resulting in physically interpretable networks. Unlike the fully data-driven CNN, auxiliary spectral response function (SRF) is utilized to guide CNNs to group the bands with spectral relevance. In addition, the channel attention module (CAM) and reformulated spectral angle mapper loss function are applied to achieve an effective reconstruction model. Finally, experiments on two types of datasets, including natural and remote sensing images, demonstrate the spectral enhancement effect of the proposed method. And the classification results on the remote sensing dataset also verified the validity of the information enhanced by the proposed method.



### Causal Contextual Prediction for Learned Image Compression
- **Arxiv ID**: http://arxiv.org/abs/2011.09704v5
- **DOI**: 10.1109/TCSVT.2021.3089491
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2011.09704v5)
- **Published**: 2020-11-19 08:15:10+00:00
- **Updated**: 2021-10-31 05:06:18+00:00
- **Authors**: Zongyu Guo, Zhizheng Zhang, Runsen Feng, Zhibo Chen
- **Comment**: We add some descriptions for the improved quantization in the latest
  arxiv version
- **Journal**: None
- **Summary**: Over the past several years, we have witnessed impressive progress in the field of learned image compression. Recent learned image codecs are commonly based on autoencoders, that first encode an image into low-dimensional latent representations and then decode them for reconstruction purposes. To capture spatial dependencies in the latent space, prior works exploit hyperprior and spatial context model to build an entropy model, which estimates the bit-rate for end-to-end rate-distortion optimization. However, such an entropy model is suboptimal from two aspects: (1) It fails to capture spatially global correlations among the latents. (2) Cross-channel relationships of the latents are still underexplored. In this paper, we propose the concept of separate entropy coding to leverage a serial decoding process for causal contextual entropy prediction in the latent space. A causal context model is proposed that separates the latents across channels and makes use of cross-channel relationships to generate highly informative contexts. Furthermore, we propose a causal global prediction model, which is able to find global reference points for accurate predictions of unknown points. Both these two models facilitate entropy estimation without the transmission of overhead. In addition, we further adopt a new separate attention module to build more powerful transform networks. Experimental results demonstrate that our full image compression model outperforms standard VVC/H.266 codec on Kodak dataset in terms of both PSNR and MS-SSIM, yielding the state-of-the-art rate-distortion performance.



### Face Forgery Detection by 3D Decomposition
- **Arxiv ID**: http://arxiv.org/abs/2011.09737v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.09737v1)
- **Published**: 2020-11-19 09:25:44+00:00
- **Updated**: 2020-11-19 09:25:44+00:00
- **Authors**: Xiangyu Zhu, Hao Wang, Hongyan Fei, Zhen Lei, Stan Z. Li
- **Comment**: None
- **Journal**: None
- **Summary**: Detecting digital face manipulation has attracted extensive attention due to fake media's potential harms to the public. However, recent advances have been able to reduce the forgery signals to a low magnitude. Decomposition, which reversibly decomposes an image into several constituent elements, is a promising way to highlight the hidden forgery details. In this paper, we consider a face image as the production of the intervention of the underlying 3D geometry and the lighting environment, and decompose it in a computer graphics view. Specifically, by disentangling the face image into 3D shape, common texture, identity texture, ambient light, and direct light, we find the devil lies in the direct light and the identity texture. Based on this observation, we propose to utilize facial detail, which is the combination of direct light and identity texture, as the clue to detect the subtle forgery patterns. Besides, we highlight the manipulated region with a supervised attention mechanism and introduce a two-stream structure to exploit both face image and facial detail together as a multi-modality task. Extensive experiments indicate the effectiveness of the extra features extracted from the facial detail, and our method achieves the state-of-the-art performance.



### Attention-Based Transformers for Instance Segmentation of Cells in Microstructures
- **Arxiv ID**: http://arxiv.org/abs/2011.09763v2
- **DOI**: 10.1109/BIBM49941.2020.9313305
- **Categories**: **cs.CV**, eess.SP, physics.ins-det, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2011.09763v2)
- **Published**: 2020-11-19 10:49:56+00:00
- **Updated**: 2020-11-20 08:04:27+00:00
- **Authors**: Tim Prangemeier, Christoph Reich, Heinz Koeppl
- **Comment**: IEEE BIBM 2020 (accepted)
- **Journal**: None
- **Summary**: Detecting and segmenting object instances is a common task in biomedical applications. Examples range from detecting lesions on functional magnetic resonance images, to the detection of tumours in histopathological images and extracting quantitative single-cell information from microscopy imagery, where cell segmentation is a major bottleneck. Attention-based transformers are state-of-the-art in a range of deep learning fields. They have recently been proposed for segmentation tasks where they are beginning to outperforming other methods. We present a novel attention-based cell detection transformer (Cell-DETR) for direct end-to-end instance segmentation. While the segmentation performance is on par with a state-of-the-art instance segmentation method, Cell-DETR is simpler and faster. We showcase the method's contribution in a the typical use case of segmenting yeast in microstructured environments, commonly employed in systems or synthetic biology. For the specific use case, the proposed method surpasses the state-of-the-art tools for semantic segmentation and additionally predicts the individual object instances. The fast and accurate instance segmentation performance increases the experimental information yield for a posteriori data processing and makes online monitoring of experiments and closed-loop optimal experimental design feasible.



### Foreground-Aware Relation Network for Geospatial Object Segmentation in High Spatial Resolution Remote Sensing Imagery
- **Arxiv ID**: http://arxiv.org/abs/2011.09766v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2011.09766v1)
- **Published**: 2020-11-19 10:57:43+00:00
- **Updated**: 2020-11-19 10:57:43+00:00
- **Authors**: Zhuo Zheng, Yanfei Zhong, Junjue Wang, Ailong Ma
- **Comment**: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
  Recognition(CVPR). 2020
- **Journal**: None
- **Summary**: Geospatial object segmentation, as a particular semantic segmentation task, always faces with larger-scale variation, larger intra-class variance of background, and foreground-background imbalance in the high spatial resolution (HSR) remote sensing imagery. However, general semantic segmentation methods mainly focus on scale variation in the natural scene, with inadequate consideration of the other two problems that usually happen in the large area earth observation scene. In this paper, we argue that the problems lie on the lack of foreground modeling and propose a foreground-aware relation network (FarSeg) from the perspectives of relation-based and optimization-based foreground modeling, to alleviate the above two problems. From perspective of relation, FarSeg enhances the discrimination of foreground features via foreground-correlated contexts associated by learning foreground-scene relation. Meanwhile, from perspective of optimization, a foreground-aware optimization is proposed to focus on foreground examples and hard examples of background during training for a balanced optimization. The experimental results obtained using a large scale dataset suggest that the proposed method is superior to the state-of-the-art general semantic segmentation methods and achieves a better trade-off between speed and accuracy. Code has been made available at: \url{https://github.com/Z-Zheng/FarSeg}.



### Scene text removal via cascaded text stroke detection and erasing
- **Arxiv ID**: http://arxiv.org/abs/2011.09768v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.09768v1)
- **Published**: 2020-11-19 11:05:13+00:00
- **Updated**: 2020-11-19 11:05:13+00:00
- **Authors**: Xuewei Bian, Chaoqun Wang, Weize Quan, Juntao Ye, Xiaopeng Zhang, Dong-Ming Yan
- **Comment**: 14 pages, 9 figures
- **Journal**: None
- **Summary**: Recent learning-based approaches show promising performance improvement for scene text removal task. However, these methods usually leave some remnants of text and obtain visually unpleasant results. In this work, we propose a novel "end-to-end" framework based on accurate text stroke detection. Specifically, we decouple the text removal problem into text stroke detection and stroke removal. We design a text stroke detection network and a text removal generation network to solve these two sub-problems separately. Then, we combine these two networks as a processing unit, and cascade this unit to obtain the final model for text removal. Experimental results demonstrate that the proposed method significantly outperforms the state-of-the-art approaches for locating and erasing scene text. Since current publicly available datasets are all synthetic and cannot properly measure the performance of different methods, we therefore construct a new real-world dataset, which will be released to facilitate the relevant research.



### Deep Learning for Automated Screening of Tuberculosis from Indian Chest X-rays: Analysis and Update
- **Arxiv ID**: http://arxiv.org/abs/2011.09778v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.09778v1)
- **Published**: 2020-11-19 11:34:27+00:00
- **Updated**: 2020-11-19 11:34:27+00:00
- **Authors**: Anushikha Singh, Brejesh Lall, B. K. Panigrahi, Anjali Agrawal, Anurag Agrawal, Balamugesh Thangakunam, DJ Christopher
- **Comment**: None
- **Journal**: None
- **Summary**: Background and Objective: Tuberculosis (TB) is a significant public health issue and a leading cause of death worldwide. Millions of deaths can be averted by early diagnosis and successful treatment of TB patients. Automated diagnosis of TB holds vast potential to assist medical experts in expediting and improving its diagnosis, especially in developing countries like India, where there is a shortage of trained medical experts and radiologists. To date, several deep learning based methods for automated detection of TB from chest radiographs have been proposed. However, the performance of a few of these methods on the Indian chest radiograph data set has been suboptimal, possibly due to different texture of the lungs on chest radiographs of Indian subjects compared to other countries. Thus deep learning for accurate and automated diagnosis of TB on Indian datasets remains an important subject of research. Methods: The proposed work explores the performance of convolutional neural networks (CNNs) for the diagnosis of TB in Indian chest x-ray images. Three different pre-trained neural network models, AlexNet, GoogLenet, and ResNet are used to classify chest x-ray images into healthy or TB infected. The proposed approach does not require any pre-processing technique. Also, other works use pre-trained NNs as a tool for crafting features and then apply standard classification techniques. However, we attempt an end to end NN model based diagnosis of TB from chest x-rays. The proposed visualization tool can also be used by radiologists in the screening of large datasets. Results: The proposed method achieved 93.40% accuracy with 98.60% sensitivity to diagnose TB for the Indian population. Conclusions: The performance of the proposed method is also tested against techniques described in the literature. The proposed method outperforms the state of art on Indian and Shenzhen datasets.



### Towards Spatio-Temporal Video Scene Text Detection via Temporal Clustering
- **Arxiv ID**: http://arxiv.org/abs/2011.09781v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.09781v1)
- **Published**: 2020-11-19 11:40:35+00:00
- **Updated**: 2020-11-19 11:40:35+00:00
- **Authors**: Yuanqiang Cai, Chang Liu, Weiqiang Wang, Qixiang Ye
- **Comment**: None
- **Journal**: None
- **Summary**: With only bounding-box annotations in the spatial domain, existing video scene text detection (VSTD) benchmarks lack temporal relation of text instances among video frames, which hinders the development of video text-related applications. In this paper, we systematically introduce a new large-scale benchmark, named as STVText4, a well-designed spatial-temporal detection metric (STDM), and a novel clustering-based baseline method, referred to as Temporal Clustering (TC). STVText4 opens a challenging yet promising direction of VSTD, termed as ST-VSTD, which targets at simultaneously detecting video scene texts in both spatial and temporal domains. STVText4 contains more than 1.4 million text instances from 161,347 video frames of 106 videos, where each instance is annotated with not only spatial bounding box and temporal range but also four intrinsic attributes, including legibility, density, scale, and lifecycle, to facilitate the community. With continuous propagation of identical texts in the video sequence, TC can accurately output the spatial quadrilateral and temporal range of the texts, which sets a strong baseline for ST-VSTD. Experiments demonstrate the efficacy of our method and the great academic and practical value of the STVText4. The dataset and code will be available soon.



### DeepMorph: A System for Hiding Bitstrings in Morphable Vector Drawings
- **Arxiv ID**: http://arxiv.org/abs/2011.09783v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.09783v1)
- **Published**: 2020-11-19 11:55:39+00:00
- **Updated**: 2020-11-19 11:55:39+00:00
- **Authors**: Søren Rasmussen, Karsten Østergaard Noe, Oliver Gyldenberg Hjermitslev, Henrik Pedersen
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce DeepMorph, an information embedding technique for vector drawings. Provided a vector drawing, such as a Scalable Vector Graphics (SVG) file, our method embeds bitstrings in the image by perturbing the drawing primitives (lines, circles, etc.). This results in a morphed image that can be decoded to recover the original bitstring. The use-case is similar to that of the well-known QR code, but our solution provides creatives with artistic freedom to transfer digital information via drawings of their own design. The method comprises two neural networks, which are trained jointly: an encoder network that transforms a bitstring into a perturbation of the drawing primitives, and a decoder network that recovers the bitstring from an image of the morphed drawing. To enable end-to-end training via back propagation, we introduce a soft rasterizer, which is differentiable with respect to perturbations of the drawing primitives. In order to add robustness towards real-world image capture conditions, image corruptions are injected between the soft rasterizer and the decoder. Further, the addition of an object detection and camera pose estimation system enables decoding of drawings in complex scenes as well as use of the drawings as markers for use in augmented reality applications. We demonstrate that our method reliably recovers bitstrings from real-world photos of printed drawings, thereby providing a novel solution for creatives to transfer digital information via artistic imagery.



### An Experimental Study of Semantic Continuity for Deep Learning Models
- **Arxiv ID**: http://arxiv.org/abs/2011.09789v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.09789v1)
- **Published**: 2020-11-19 12:23:28+00:00
- **Updated**: 2020-11-19 12:23:28+00:00
- **Authors**: Shangxi Wu, Jitao Sang, Xian Zhao, Lizhang Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning models suffer from the problem of semantic discontinuity: small perturbations in the input space tend to cause semantic-level interference to the model output. We argue that the semantic discontinuity results from these inappropriate training targets and contributes to notorious issues such as adversarial robustness, interpretability, etc. We first conduct data analysis to provide evidence of semantic discontinuity in existing deep learning models, and then design a simple semantic continuity constraint which theoretically enables models to obtain smooth gradients and learn semantic-oriented features. Qualitative and quantitative experiments prove that semantically continuous models successfully reduce the use of non-semantic information, which further contributes to the improvement in adversarial robustness, interpretability, model transfer, and machine bias.



### Unifying Instance and Panoptic Segmentation with Dynamic Rank-1 Convolutions
- **Arxiv ID**: http://arxiv.org/abs/2011.09796v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.09796v1)
- **Published**: 2020-11-19 12:42:10+00:00
- **Updated**: 2020-11-19 12:42:10+00:00
- **Authors**: Hao Chen, Chunhua Shen, Zhi Tian
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, fully-convolutional one-stage networks have shown superior performance comparing to two-stage frameworks for instance segmentation as typically they can generate higher-quality mask predictions with less computation. In addition, their simple design opens up new opportunities for joint multi-task learning. In this paper, we demonstrate that adding a single classification layer for semantic segmentation, fully-convolutional instance segmentation networks can achieve state-of-the-art panoptic segmentation quality. This is made possible by our novel dynamic rank-1 convolution (DR1Conv), a novel dynamic module that can efficiently merge high-level context information with low-level detailed features which is beneficial for both semantic and instance segmentation. Importantly, the proposed new method, termed DR1Mask, can perform panoptic segmentation by adding a single layer. To our knowledge, DR1Mask is the first panoptic segmentation framework that exploits a shared feature map for both instance and semantic segmentation by considering both efficacy and efficiency. Not only our framework is much more efficient -- twice as fast as previous best two-branch approaches, but also the unified framework opens up opportunities for using the same context module to improve the performance for both tasks. As a byproduct, when performing instance segmentation alone, DR1Mask is 10% faster and 1 point in mAP more accurate than previous state-of-the-art instance segmentation network BlendMask. Code is available at: https://git.io/AdelaiDet



### TaL: a synchronised multi-speaker corpus of ultrasound tongue imaging, audio, and lip videos
- **Arxiv ID**: http://arxiv.org/abs/2011.09804v1
- **DOI**: None
- **Categories**: **eess.AS**, cs.CL, cs.CV, cs.SD, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2011.09804v1)
- **Published**: 2020-11-19 13:11:46+00:00
- **Updated**: 2020-11-19 13:11:46+00:00
- **Authors**: Manuel Sam Ribeiro, Jennifer Sanger, Jing-Xuan Zhang, Aciel Eshky, Alan Wrench, Korin Richmond, Steve Renals
- **Comment**: 8 pages, 4 figures, Accepted to SLT2021, IEEE Spoken Language
  Technology Workshop
- **Journal**: None
- **Summary**: We present the Tongue and Lips corpus (TaL), a multi-speaker corpus of audio, ultrasound tongue imaging, and lip videos. TaL consists of two parts: TaL1 is a set of six recording sessions of one professional voice talent, a male native speaker of English; TaL80 is a set of recording sessions of 81 native speakers of English without voice talent experience. Overall, the corpus contains 24 hours of parallel ultrasound, video, and audio data, of which approximately 13.5 hours are speech. This paper describes the corpus and presents benchmark results for the tasks of speech recognition, speech synthesis (articulatory-to-acoustic mapping), and automatic synchronisation of ultrasound to audio. The TaL corpus is publicly available under the CC BY-NC 4.0 license.



### Unmixing Convolutional Features for Crisp Edge Detection
- **Arxiv ID**: http://arxiv.org/abs/2011.09808v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.09808v2)
- **Published**: 2020-11-19 13:30:15+00:00
- **Updated**: 2021-06-29 13:01:08+00:00
- **Authors**: Linxi Huan, Nan Xue, Xianwei Zheng, Wei He, Jianya Gong, Gui-Song Xia
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a context-aware tracing strategy (CATS) for crisp edge detection with deep edge detectors, based on an observation that the localization ambiguity of deep edge detectors is mainly caused by the mixing phenomenon of convolutional neural networks: feature mixing in edge classification and side mixing during fusing side predictions. The CATS consists of two modules: a novel tracing loss that performs feature unmixing by tracing boundaries for better side edge learning, and a context-aware fusion block that tackles the side mixing by aggregating the complementary merits of learned side edges. Experiments demonstrate that the proposed CATS can be integrated into modern deep edge detectors to improve localization accuracy. With the vanilla VGG16 backbone, in terms of BSDS500 dataset, our CATS improves the F-measure (ODS) of the RCF and BDCN deep edge detectors by 12% and 6% respectively when evaluating without using the morphological non-maximal suppression scheme for edge detection.



### Interval-valued aggregation functions based on moderate deviations applied to Motor-Imagery-Based Brain Computer Interface
- **Arxiv ID**: http://arxiv.org/abs/2011.09831v2
- **DOI**: 10.1109/TFUZZ.2021.3092824
- **Categories**: **cs.HC**, cs.CV, cs.NA, math.NA
- **Links**: [PDF](http://arxiv.org/pdf/2011.09831v2)
- **Published**: 2020-11-19 14:10:29+00:00
- **Updated**: 2021-07-01 07:09:17+00:00
- **Authors**: Javier Fumanal-Idocin, Zdenko Takáč, Javier Fernández Jose Antonio Sanz, Harkaitz Goyena, Ching-Teng Lin, Yu-Kai Wang, Humberto Bustince
- **Comment**: None
- **Journal**: None
- **Summary**: In this work we study the use of moderate deviation functions to measure similarity and dissimilarity among a set of given interval-valued data. To do so, we introduce the notion of interval-valued moderate deviation function and we study in particular those interval-valued moderate deviation functions which preserve the width of the input intervals. Then, we study how to apply these functions to construct interval-valued aggregation functions. We have applied them in the decision making phase of two Motor-Imagery Brain Computer Interface frameworks, obtaining better results than those obtained using other numerical and intervalar aggregations.



### Differentiable Data Augmentation with Kornia
- **Arxiv ID**: http://arxiv.org/abs/2011.09832v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.09832v1)
- **Published**: 2020-11-19 14:15:21+00:00
- **Updated**: 2020-11-19 14:15:21+00:00
- **Authors**: Jian Shi, Edgar Riba, Dmytro Mishkin, Francesc Moreno, Anguelos Nicolaou
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we present a review of the Kornia differentiable data augmentation (DDA) module for both for spatial (2D) and volumetric (3D) tensors. This module leverages differentiable computer vision solutions from Kornia, with an aim of integrating data augmentation (DA) pipelines and strategies to existing PyTorch components (e.g. autograd for differentiability, optim for optimization). In addition, we provide a benchmark comparing different DA frameworks and a short review for a number of approaches that make use of Kornia DDA.



### Everybody Sign Now: Translating Spoken Language to Photo Realistic Sign Language Video
- **Arxiv ID**: http://arxiv.org/abs/2011.09846v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.09846v4)
- **Published**: 2020-11-19 14:31:06+00:00
- **Updated**: 2020-11-26 19:00:34+00:00
- **Authors**: Ben Saunders, Necati Cihan Camgoz, Richard Bowden
- **Comment**: None
- **Journal**: None
- **Summary**: To be truly understandable and accepted by Deaf communities, an automatic Sign Language Production (SLP) system must generate a photo-realistic signer. Prior approaches based on graphical avatars have proven unpopular, whereas recent neural SLP works that produce skeleton pose sequences have been shown to be not understandable to Deaf viewers.   In this paper, we propose SignGAN, the first SLP model to produce photo-realistic continuous sign language videos directly from spoken language. We employ a transformer architecture with a Mixture Density Network (MDN) formulation to handle the translation from spoken language to skeletal pose. A pose-conditioned human synthesis model is then introduced to generate a photo-realistic sign language video from the skeletal pose sequence. This allows the photo-realistic production of sign videos directly translated from written text.   We further propose a novel keypoint-based loss function, which significantly improves the quality of synthesized hand images, operating in the keypoint space to avoid issues caused by motion blur. In addition, we introduce a method for controllable video generation, enabling training on large, diverse sign language datasets and providing the ability to control the signer appearance at inference.   Using a dataset of eight different sign language interpreters extracted from broadcast footage, we show that SignGAN significantly outperforms all baseline methods for quantitative metrics and human perceptual studies.



### Recursive Deep Prior Video: a Super Resolution algorithm for Time-Lapse Microscopy of organ-on-chip experiments
- **Arxiv ID**: http://arxiv.org/abs/2011.09855v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, 97P80, 68U10, 92C55, I.4.4; I.4.9; I.2.1
- **Links**: [PDF](http://arxiv.org/pdf/2011.09855v1)
- **Published**: 2020-11-19 14:36:33+00:00
- **Updated**: 2020-11-19 14:36:33+00:00
- **Authors**: Pasquale Cascarano, Maria Colomba Comes, Arianna Mencattini, Maria Carla Parrini, Elena Loli Piccolomini, Eugenio Martinelli
- **Comment**: Paper submitted to a peer-reviewed journal
- **Journal**: None
- **Summary**: Biological experiments based on organ-on-chips (OOCs) exploit light Time-Lapse Microscopy (TLM) for a direct observation of cell movement that is an observable signature of underlying biological processes. A high spatial resolution is essential to capture cell dynamics and interactions from recorded experiments by TLM. Unfortunately, due to physical and cost limitations, acquiring high resolution videos is not always possible. To overcome the problem, we present here a new deep learning-based algorithm that extends the well known Deep Image Prior (DIP) to TLM Video Super Resolution (SR) without requiring any training. The proposed Recursive Deep Prior Video (RDPV) method introduces some novelties. The weights of the DIP network architecture are initialized for each of the frames according to a new recursive updating rule combined with an efficient early stopping criterion. Moreover, the DIP loss function is penalized by two different Total Variation (TV) based terms. The method has been validated on synthetic, i.e., artificially generated, as well as real videos from OOC experiments related to tumor-immune interaction. Achieved results are compared with several state-of-the-art trained deep learning SR algorithms showing outstanding performances.



### DCT-Mask: Discrete Cosine Transform Mask Representation for Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2011.09876v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.09876v3)
- **Published**: 2020-11-19 15:00:21+00:00
- **Updated**: 2021-04-27 13:53:02+00:00
- **Authors**: Xing Shen, Jirui Yang, Chunbo Wei, Bing Deng, Jianqiang Huang, Xiansheng Hua, Xiaoliang Cheng, Kewei Liang
- **Comment**: None
- **Journal**: None
- **Summary**: Binary grid mask representation is broadly used in instance segmentation. A representative instantiation is Mask R-CNN which predicts masks on a $28\times 28$ binary grid. Generally, a low-resolution grid is not sufficient to capture the details, while a high-resolution grid dramatically increases the training complexity. In this paper, we propose a new mask representation by applying the discrete cosine transform(DCT) to encode the high-resolution binary grid mask into a compact vector. Our method, termed DCT-Mask, could be easily integrated into most pixel-based instance segmentation methods. Without any bells and whistles, DCT-Mask yields significant gains on different frameworks, backbones, datasets, and training schedules. It does not require any pre-processing or pre-training, and almost no harm to the running speed. Especially, for higher-quality annotations and more complex backbones, our method has a greater improvement. Moreover, we analyze the performance of our method from the perspective of the quality of mask representation. The main reason why DCT-Mask works well is that it obtains a high-quality mask representation with low complexity. Code is available at https://github.com/aliyun/DCT-Mask.git.



### DeepRepair: Style-Guided Repairing for DNNs in the Real-world Operational Environment
- **Arxiv ID**: http://arxiv.org/abs/2011.09884v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.SE
- **Links**: [PDF](http://arxiv.org/pdf/2011.09884v1)
- **Published**: 2020-11-19 15:09:44+00:00
- **Updated**: 2020-11-19 15:09:44+00:00
- **Authors**: Bing Yu, Hua Qi, Qing Guo, Felix Juefei-Xu, Xiaofei Xie, Lei Ma, Jianjun Zhao
- **Comment**: 14 pages; 5 figures
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) are being widely applied for various real-world applications across domains due to their high performance (e.g., high accuracy on image classification). Nevertheless, a well-trained DNN after deployment could oftentimes raise errors during practical use in the operational environment due to the mismatching between distributions of the training dataset and the potential unknown noise factors in the operational environment, e.g., weather, blur, noise etc. Hence, it poses a rather important problem for the DNNs' real-world applications: how to repair the deployed DNNs for correcting the failure samples (i.e., incorrect prediction) under the deployed operational environment while not harming their capability of handling normal or clean data. The number of failure samples we can collect in practice, caused by the noise factors in the operational environment, is often limited. Therefore, It is rather challenging how to repair more similar failures based on the limited failure samples we can collect.   In this paper, we propose a style-guided data augmentation for repairing DNN in the operational environment. We propose a style transfer method to learn and introduce the unknown failure patterns within the failure data into the training data via data augmentation. Moreover, we further propose the clustering-based failure data generation for much more effective style-guided data augmentation. We conduct a large-scale evaluation with fifteen degradation factors that may happen in the real world and compare with four state-of-the-art data augmentation methods and two DNN repairing methods, demonstrating that our method can significantly enhance the deployed DNNs on the corrupted data in the operational environment, and with even better accuracy on clean datasets.



### MixMix: All You Need for Data-Free Compression Are Feature and Data Mixing
- **Arxiv ID**: http://arxiv.org/abs/2011.09899v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.09899v3)
- **Published**: 2020-11-19 15:33:43+00:00
- **Updated**: 2022-01-26 22:25:41+00:00
- **Authors**: Yuhang Li, Feng Zhu, Ruihao Gong, Mingzhu Shen, Xin Dong, Fengwei Yu, Shaoqing Lu, Shi Gu
- **Comment**: None
- **Journal**: None
- **Summary**: User data confidentiality protection is becoming a rising challenge in the present deep learning research. Without access to data, conventional data-driven model compression faces a higher risk of performance degradation. Recently, some works propose to generate images from a specific pretrained model to serve as training data. However, the inversion process only utilizes biased feature statistics stored in one model and is from low-dimension to high-dimension. As a consequence, it inevitably encounters the difficulties of generalizability and inexact inversion, which leads to unsatisfactory performance. To address these problems, we propose MixMix based on two simple yet effective techniques: (1) Feature Mixing: utilizes various models to construct a universal feature space for generalized inversion; (2) Data Mixing: mixes the synthesized images and labels to generate exact label information. We prove the effectiveness of MixMix from both theoretical and empirical perspectives. Extensive experiments show that MixMix outperforms existing methods on the mainstream compression tasks, including quantization, knowledge distillation, and pruning. Specifically, MixMix achieves up to 4% and 20% accuracy uplift on quantization and pruning, respectively, compared to existing data-free compression work.



### All-in-Focus Iris Camera With a Great Capture Volume
- **Arxiv ID**: http://arxiv.org/abs/2011.09908v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.09908v1)
- **Published**: 2020-11-19 15:39:45+00:00
- **Updated**: 2020-11-19 15:39:45+00:00
- **Authors**: Kunbo Zhang, Zhenteng Shen, Yunlong Wang, Zhenan Sun
- **Comment**: to be published in International Joint Conference on Biometrics 2020
- **Journal**: None
- **Summary**: Imaging volume of an iris recognition system has been restricting the throughput and cooperation convenience in biometric applications. Numerous improvement trials are still impractical to supersede the dominant fixed-focus lens in stand-off iris recognition due to incremental performance increase and complicated optical design. In this study, we develop a novel all-in-focus iris imaging system using a focus-tunable lens and a 2D steering mirror to greatly extend capture volume by spatiotemporal multiplexing method. Our iris imaging depth of field extension system requires no mechanical motion and is capable to adjust the focal plane at extremely high speed. In addition, the motorized reflection mirror adaptively steers the light beam to extend the horizontal and vertical field of views in an active manner. The proposed all-in-focus iris camera increases the depth of field up to 3.9 m which is a factor of 37.5 compared with conventional long focal lens. We also experimentally demonstrate the capability of this 3D light beam steering imaging system in real-time multi-person iris refocusing using dynamic focal stacks and the potential of continuous iris recognition for moving participants.



### Using Text to Teach Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2011.09928v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.09928v1)
- **Published**: 2020-11-19 16:09:14+00:00
- **Updated**: 2020-11-19 16:09:14+00:00
- **Authors**: Haoyu Dong, Ze Wang, Qiang Qiu, Guillermo Sapiro
- **Comment**: None
- **Journal**: None
- **Summary**: Image retrieval relies heavily on the quality of the data modeling and the distance measurement in the feature space. Building on the concept of image manifold, we first propose to represent the feature space of images, learned via neural networks, as a graph. Neighborhoods in the feature space are now defined by the geodesic distance between images, represented as graph vertices or manifold samples. When limited images are available, this manifold is sparsely sampled, making the geodesic computation and the corresponding retrieval harder. To address this, we augment the manifold samples with geometrically aligned text, thereby using a plethora of sentences to teach us about images. In addition to extensive results on standard datasets illustrating the power of text to help in image retrieval, a new public dataset based on CLEVR is introduced to quantify the semantic similarity between visual data and text data. The experimental results show that the joint embedding manifold is a robust representation, allowing it to be a better basis to perform image retrieval given only an image and a textual instruction on the desired modifications over the image



### Heterogeneous Contrastive Learning: Encoding Spatial Information for Compact Visual Representations
- **Arxiv ID**: http://arxiv.org/abs/2011.09941v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.09941v1)
- **Published**: 2020-11-19 16:26:25+00:00
- **Updated**: 2020-11-19 16:26:25+00:00
- **Authors**: Xinyue Huo, Lingxi Xie, Longhui Wei, Xiaopeng Zhang, Hao Li, Zijie Yang, Wengang Zhou, Houqiang Li, Qi Tian
- **Comment**: 10 pages, 4 figures, 6 tables
- **Journal**: None
- **Summary**: Contrastive learning has achieved great success in self-supervised visual representation learning, but existing approaches mostly ignored spatial information which is often crucial for visual representation. This paper presents heterogeneous contrastive learning (HCL), an effective approach that adds spatial information to the encoding stage to alleviate the learning inconsistency between the contrastive objective and strong data augmentation operations. We demonstrate the effectiveness of HCL by showing that (i) it achieves higher accuracy in instance discrimination and (ii) it surpasses existing pre-training methods in a series of downstream tasks while shrinking the pre-training costs by half. More importantly, we show that our approach achieves higher efficiency in visual representations, and thus delivers a key message to inspire the future research of self-supervised visual representation learning.



### A Preliminary Comparison Between Compressive Sampling and Anisotropic Mesh-based Image Representation
- **Arxiv ID**: http://arxiv.org/abs/2011.09944v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.09944v2)
- **Published**: 2020-11-19 16:38:02+00:00
- **Updated**: 2020-12-14 03:14:04+00:00
- **Authors**: Xianping Li, Teresa Wu
- **Comment**: 9 pages, 4 figures, 2 tables
- **Journal**: None
- **Summary**: Compressed sensing (CS) has become a popular field in the last two decades to represent and reconstruct a sparse signal with much fewer samples than the signal itself. Although regular images are not sparse on their own, many can be sparsely represented in wavelet transform domain. Therefore, CS has also been widely applied to represent digital images. However, an alternative approach, adaptive sampling such as mesh-based image representation (MbIR), has not attracted as much attention. MbIR works directly on image pixels and represents the image with fewer points using a triangular mesh. In this paper, we perform a preliminary comparison between the CS and a recently developed MbIR method, AMA representation. The results demonstrate that, at the same sample density, AMA representation can provide better reconstruction quality than CS based on the tested algorithms. Further investigation with recent algorithms is needed to perform a thorough comparison.



### Adversarial Threats to DeepFake Detection: A Practical Perspective
- **Arxiv ID**: http://arxiv.org/abs/2011.09957v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.09957v1)
- **Published**: 2020-11-19 16:53:38+00:00
- **Updated**: 2020-11-19 16:53:38+00:00
- **Authors**: Paarth Neekhara, Brian Dolhansky, Joanna Bitton, Cristian Canton Ferrer
- **Comment**: None
- **Journal**: None
- **Summary**: Facially manipulated images and videos or DeepFakes can be used maliciously to fuel misinformation or defame individuals. Therefore, detecting DeepFakes is crucial to increase the credibility of social media platforms and other media sharing web sites. State-of-the art DeepFake detection techniques rely on neural network based classification models which are known to be vulnerable to adversarial examples. In this work, we study the vulnerabilities of state-of-the-art DeepFake detection methods from a practical stand point. We perform adversarial attacks on DeepFake detectors in a black box setting where the adversary does not have complete knowledge of the classification models. We study the extent to which adversarial perturbations transfer across different models and propose techniques to improve the transferability of adversarial examples. We also create more accessible attacks using Universal Adversarial Perturbations which pose a very feasible attack scenario since they can be easily shared amongst attackers. We perform our evaluations on the winning entries of the DeepFake Detection Challenge (DFDC) and demonstrate that they can be easily bypassed in a practical attack scenario by designing transferable and accessible adversarial attacks.



### Learning to Predict the 3D Layout of a Scene
- **Arxiv ID**: http://arxiv.org/abs/2011.09977v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.09977v1)
- **Published**: 2020-11-19 17:23:30+00:00
- **Updated**: 2020-11-19 17:23:30+00:00
- **Authors**: Jihao Andreas Lin, Jakob Brünker, Daniel Fährmann
- **Comment**: None
- **Journal**: None
- **Summary**: While 2D object detection has improved significantly over the past, real world applications of computer vision often require an understanding of the 3D layout of a scene. Many recent approaches to 3D detection use LiDAR point clouds for prediction. We propose a method that only uses a single RGB image, thus enabling applications in devices or vehicles that do not have LiDAR sensors. By using an RGB image, we can leverage the maturity and success of recent 2D object detectors, by extending a 2D detector with a 3D detection head. In this paper we discuss different approaches and experiments, including both regression and classification methods, for designing this 3D detection head. Furthermore, we evaluate how subproblems and implementation details impact the overall prediction result. We use the KITTI dataset for training, which consists of street traffic scenes with class labels, 2D bounding boxes and 3D annotations with seven degrees of freedom. Our final architecture is based on Faster R-CNN. The outputs of the convolutional backbone are fixed sized feature maps for every region of interest. Fully connected layers within the network head then propose an object class and perform 2D bounding box regression. We extend the network head by a 3D detection head, which predicts every degree of freedom of a 3D bounding box via classification. We achieve a mean average precision of 47.3% for moderately difficult data, measured at a 3D intersection over union threshold of 70%, as required by the official KITTI benchmark; outperforming previous state-of-the-art single RGB only methods by a large margin.



### Geography-Aware Self-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2011.09980v7
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.09980v7)
- **Published**: 2020-11-19 17:29:13+00:00
- **Updated**: 2022-03-08 05:44:33+00:00
- **Authors**: Kumar Ayush, Burak Uzkent, Chenlin Meng, Kumar Tanmay, Marshall Burke, David Lobell, Stefano Ermon
- **Comment**: Accepted at ICCV 2021
- **Journal**: None
- **Summary**: Contrastive learning methods have significantly narrowed the gap between supervised and unsupervised learning on computer vision tasks. In this paper, we explore their application to geo-located datasets, e.g. remote sensing, where unlabeled data is often abundant but labeled data is scarce. We first show that due to their different characteristics, a non-trivial gap persists between contrastive and supervised learning on standard benchmarks. To close the gap, we propose novel training methods that exploit the spatio-temporal structure of remote sensing data. We leverage spatially aligned images over time to construct temporal positive pairs in contrastive learning and geo-location to design pre-text tasks. Our experiments show that our proposed method closes the gap between contrastive and supervised learning on image classification, object detection and semantic segmentation for remote sensing. Moreover, we demonstrate that the proposed method can also be applied to geo-tagged ImageNet images, improving downstream performance on various tasks. Project Webpage can be found at this link geography-aware-ssl.github.io.



### Multi-Plane Program Induction with 3D Box Priors
- **Arxiv ID**: http://arxiv.org/abs/2011.10007v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2011.10007v2)
- **Published**: 2020-11-19 18:07:46+00:00
- **Updated**: 2020-11-22 19:13:03+00:00
- **Authors**: Yikai Li, Jiayuan Mao, Xiuming Zhang, William T. Freeman, Joshua B. Tenenbaum, Noah Snavely, Jiajun Wu
- **Comment**: NeurIPS 2020. First two authors contributed equally. Project page:
  http://bpi.csail.mit.edu
- **Journal**: None
- **Summary**: We consider two important aspects in understanding and editing images: modeling regular, program-like texture or patterns in 2D planes, and 3D posing of these planes in the scene. Unlike prior work on image-based program synthesis, which assumes the image contains a single visible 2D plane, we present Box Program Induction (BPI), which infers a program-like scene representation that simultaneously models repeated structure on multiple 2D planes, the 3D position and orientation of the planes, and camera parameters, all from a single image. Our model assumes a box prior, i.e., that the image captures either an inner view or an outer view of a box in 3D. It uses neural networks to infer visual cues such as vanishing points, wireframe lines to guide a search-based algorithm to find the program that best explains the image. Such a holistic, structured scene representation enables 3D-aware interactive image editing operations such as inpainting missing pixels, changing camera parameters, and extrapolate the image contents.



### The Cube++ Illumination Estimation Dataset
- **Arxiv ID**: http://arxiv.org/abs/2011.10028v1
- **DOI**: 10.1109/ACCESS.2020.3045066
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.10028v1)
- **Published**: 2020-11-19 18:50:08+00:00
- **Updated**: 2020-11-19 18:50:08+00:00
- **Authors**: Egor Ershov, Alex Savchik, Illya Semenkov, Nikola Banić, Alexander Belokopytov, Daria Senshina, Karlo Koscević, Marko Subašić, Sven Lončarić
- **Comment**: None
- **Journal**: None
- **Summary**: Computational color constancy has the important task of reducing the influence of the scene illumination on the object colors. As such, it is an essential part of the image processing pipelines of most digital cameras. One of the important parts of the computational color constancy is illumination estimation, i.e. estimating the illumination color. When an illumination estimation method is proposed, its accuracy is usually reported by providing the values of error metrics obtained on the images of publicly available datasets. However, over time it has been shown that many of these datasets have problems such as too few images, inappropriate image quality, lack of scene diversity, absence of version tracking, violation of various assumptions, GDPR regulation violation, lack of additional shooting procedure info, etc. In this paper, a new illumination estimation dataset is proposed that aims to alleviate many of the mentioned problems and to help the illumination estimation research. It consists of 4890 images with known illumination colors as well as with additional semantic data that can further make the learning process more accurate. Due to the usage of the SpyderCube color target, for every image there are two ground-truth illumination records covering different directions. Because of that, the dataset can be used for training and testing of methods that perform single or two-illuminant estimation. This makes it superior to many similar existing datasets. The datasets, it's smaller version SimpleCube++, and the accompanying code are available at https://github.com/Visillect/CubePlusPlus/.



### Cylindrical and Asymmetrical 3D Convolution Networks for LiDAR Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2011.10033v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.10033v1)
- **Published**: 2020-11-19 18:53:11+00:00
- **Updated**: 2020-11-19 18:53:11+00:00
- **Authors**: Xinge Zhu, Hui Zhou, Tai Wang, Fangzhou Hong, Yuexin Ma, Wei Li, Hongsheng Li, Dahua Lin
- **Comment**: This work achieves the 1st place in the leaderboard of SemanticKITTI
  (until CVPR DDL) and based on this work, we also achieve the 1st place in the
  leaderboard of SemanticKITTI panoptic segmentation; Code at
  https://github.com/xinge008/Cylinder3D
- **Journal**: None
- **Summary**: State-of-the-art methods for large-scale driving-scene LiDAR segmentation often project the point clouds to 2D space and then process them via 2D convolution. Although this corporation shows the competitiveness in the point cloud, it inevitably alters and abandons the 3D topology and geometric relations. A natural remedy is to utilize the3D voxelization and 3D convolution network. However, we found that in the outdoor point cloud, the improvement obtained in this way is quite limited. An important reason is the property of the outdoor point cloud, namely sparsity and varying density. Motivated by this investigation, we propose a new framework for the outdoor LiDAR segmentation, where cylindrical partition and asymmetrical 3D convolution networks are designed to explore the 3D geometric pat-tern while maintaining these inherent properties. Moreover, a point-wise refinement module is introduced to alleviate the interference of lossy voxel-based label encoding. We evaluate the proposed model on two large-scale datasets, i.e., SemanticKITTI and nuScenes. Our method achieves the 1st place in the leaderboard of SemanticKITTI and outperforms existing methods on nuScenes with a noticeable margin, about 4%. Furthermore, the proposed 3D framework also generalizes well to LiDAR panoptic segmentation and LiDAR 3D detection.



### Creative Sketch Generation
- **Arxiv ID**: http://arxiv.org/abs/2011.10039v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2011.10039v2)
- **Published**: 2020-11-19 18:57:00+00:00
- **Updated**: 2021-03-03 20:01:54+00:00
- **Authors**: Songwei Ge, Vedanuj Goswami, C. Lawrence Zitnick, Devi Parikh
- **Comment**: Published as a conference paper at ICLR 2021
- **Journal**: None
- **Summary**: Sketching or doodling is a popular creative activity that people engage in. However, most existing work in automatic sketch understanding or generation has focused on sketches that are quite mundane. In this work, we introduce two datasets of creative sketches -- Creative Birds and Creative Creatures -- containing 10k sketches each along with part annotations. We propose DoodlerGAN -- a part-based Generative Adversarial Network (GAN) -- to generate unseen compositions of novel part appearances. Quantitative evaluations as well as human studies demonstrate that sketches generated by our approach are more creative and of higher quality than existing approaches. In fact, in Creative Birds, subjects prefer sketches generated by DoodlerGAN over those drawn by humans! Our code can be found at https://github.com/facebookresearch/DoodlerGAN and a demo can be found at http://doodlergan.cloudcv.org.



### Propagate Yourself: Exploring Pixel-Level Consistency for Unsupervised Visual Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2011.10043v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.10043v2)
- **Published**: 2020-11-19 18:59:45+00:00
- **Updated**: 2021-03-09 14:29:39+00:00
- **Authors**: Zhenda Xie, Yutong Lin, Zheng Zhang, Yue Cao, Stephen Lin, Han Hu
- **Comment**: Accepted in CVPR2021
- **Journal**: None
- **Summary**: Contrastive learning methods for unsupervised visual representation learning have reached remarkable levels of transfer performance. We argue that the power of contrastive learning has yet to be fully unleashed, as current methods are trained only on instance-level pretext tasks, leading to representations that may be sub-optimal for downstream tasks requiring dense pixel predictions. In this paper, we introduce pixel-level pretext tasks for learning dense feature representations. The first task directly applies contrastive learning at the pixel level. We additionally propose a pixel-to-propagation consistency task that produces better results, even surpassing the state-of-the-art approaches by a large margin. Specifically, it achieves 60.2 AP, 41.4 / 40.5 mAP and 77.2 mIoU when transferred to Pascal VOC object detection (C4), COCO object detection (FPN / C4) and Cityscapes semantic segmentation using a ResNet-50 backbone network, which are 2.6 AP, 0.8 / 1.0 mAP and 1.0 mIoU better than the previous best methods built on instance-level contrastive learning. Moreover, the pixel-level pretext tasks are found to be effective for pre-training not only regular backbone networks but also head networks used for dense downstream tasks, and are complementary to instance-level contrastive methods. These results demonstrate the strong potential of defining pretext tasks at the pixel level, and suggest a new path forward in unsupervised visual representation learning. Code is available at \url{https://github.com/zdaxie/PixPro}.



### Dual Contradistinctive Generative Autoencoder
- **Arxiv ID**: http://arxiv.org/abs/2011.10063v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.10063v1)
- **Published**: 2020-11-19 19:00:53+00:00
- **Updated**: 2020-11-19 19:00:53+00:00
- **Authors**: Gaurav Parmar, Dacheng Li, Kwonjoon Lee, Zhuowen Tu
- **Comment**: None
- **Journal**: None
- **Summary**: We present a new generative autoencoder model with dual contradistinctive losses to improve generative autoencoder that performs simultaneous inference (reconstruction) and synthesis (sampling). Our model, named dual contradistinctive generative autoencoder (DC-VAE), integrates an instance-level discriminative loss (maintaining the instance-level fidelity for the reconstruction/synthesis) with a set-level adversarial loss (encouraging the set-level fidelity for there construction/synthesis), both being contradistinctive. Extensive experimental results by DC-VAE across different resolutions including 32x32, 64x64, 128x128, and 512x512 are reported. The two contradistinctive losses in VAE work harmoniously in DC-VAE leading to a significant qualitative and quantitative performance enhancement over the baseline VAEs without architectural changes. State-of-the-art or competitive results among generative autoencoders for image reconstruction, image synthesis, image interpolation, and representation learning are observed. DC-VAE is a general-purpose VAE model, applicable to a wide variety of downstream tasks in computer vision and machine learning.



### Error-Bounded Correction of Noisy Labels
- **Arxiv ID**: http://arxiv.org/abs/2011.10077v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.10077v1)
- **Published**: 2020-11-19 19:23:23+00:00
- **Updated**: 2020-11-19 19:23:23+00:00
- **Authors**: Songzhu Zheng, Pengxiang Wu, Aman Goswami, Mayank Goswami, Dimitris Metaxas, Chao Chen
- **Comment**: None
- **Journal**: None
- **Summary**: To collect large scale annotated data, it is inevitable to introduce label noise, i.e., incorrect class labels. To be robust against label noise, many successful methods rely on the noisy classifiers (i.e., models trained on the noisy training data) to determine whether a label is trustworthy. However, it remains unknown why this heuristic works well in practice. In this paper, we provide the first theoretical explanation for these methods. We prove that the prediction of a noisy classifier can indeed be a good indicator of whether the label of a training data is clean. Based on the theoretical result, we propose a novel algorithm that corrects the labels based on the noisy classifier prediction. The corrected labels are consistent with the true Bayesian optimal classifier with high probability. We incorporate our label correction algorithm into the training of deep neural networks and train models that achieve superior testing performance on multiple public datasets.



### Hybrid Consistency Training with Prototype Adaptation for Few-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2011.10082v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.10082v1)
- **Published**: 2020-11-19 19:51:33+00:00
- **Updated**: 2020-11-19 19:51:33+00:00
- **Authors**: Meng Ye, Xiao Lin, Giedrius Burachas, Ajay Divakaran, Yi Yao
- **Comment**: None
- **Journal**: None
- **Summary**: Few-Shot Learning (FSL) aims to improve a model's generalization capability in low data regimes. Recent FSL works have made steady progress via metric learning, meta learning, representation learning, etc. However, FSL remains challenging due to the following longstanding difficulties. 1) The seen and unseen classes are disjoint, resulting in a distribution shift between training and testing. 2) During testing, labeled data of previously unseen classes is sparse, making it difficult to reliably extrapolate from labeled support examples to unlabeled query examples. To tackle the first challenge, we introduce Hybrid Consistency Training to jointly leverage interpolation consistency, including interpolating hidden features, that imposes linear behavior locally and data augmentation consistency that learns robust embeddings against sample variations. As for the second challenge, we use unlabeled examples to iteratively normalize features and adapt prototypes, as opposed to commonly used one-time update, for more reliable prototype-based transductive inference. We show that our method generates a 2% to 5% improvement over the state-of-the-art methods with similar backbones on five FSL datasets and, more notably, a 7% to 8% improvement for more challenging cross-domain FSL.



### Classification by Attention: Scene Graph Classification with Prior Knowledge
- **Arxiv ID**: http://arxiv.org/abs/2011.10084v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.10084v2)
- **Published**: 2020-11-19 19:54:04+00:00
- **Updated**: 2020-12-17 11:52:05+00:00
- **Authors**: Sahand Sharifzadeh, Sina Moayed Baharlou, Volker Tresp
- **Comment**: Thirty-Fifth AAAI Conference on Artificial Intelligence (AAAI-2021)
- **Journal**: None
- **Summary**: A major challenge in scene graph classification is that the appearance of objects and relations can be significantly different from one image to another. Previous works have addressed this by relational reasoning over all objects in an image or incorporating prior knowledge into classification. Unlike previous works, we do not consider separate models for perception and prior knowledge. Instead, we take a multi-task learning approach, where we implement the classification as an attention layer. This allows for the prior knowledge to emerge and propagate within the perception model. By enforcing the model also to represent the prior, we achieve a strong inductive bias. We show that our model can accurately generate commonsense knowledge and that the iterative injection of this knowledge to scene representations leads to significantly higher classification performance. Additionally, our model can be fine-tuned on external knowledge given as triples. When combined with self-supervised learning and with 1% of annotated images only, this gives more than 3% improvement in object classification, 26% in scene graph classification, and 36% in predicate prediction accuracy.



### Logically Consistent Loss for Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/2011.10094v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2011.10094v1)
- **Published**: 2020-11-19 20:31:05+00:00
- **Updated**: 2020-11-19 20:31:05+00:00
- **Authors**: Anh-Cat Le-Ngo, Truyen Tran, Santu Rana, Sunil Gupta, Svetha Venkatesh
- **Comment**: 10 pages, 6 figure, 9 tables
- **Journal**: None
- **Summary**: Given an image, a back-ground knowledge, and a set of questions about an object, human learners answer the questions very consistently regardless of question forms and semantic tasks. The current advancement in neural-network based Visual Question Answering (VQA), despite their impressive performance, cannot ensure such consistency due to identically distribution (i.i.d.) assumption. We propose a new model-agnostic logic constraint to tackle this issue by formulating a logically consistent loss in the multi-task learning framework as well as a data organisation called family-batch and hybrid-batch. To demonstrate usefulness of this proposal, we train and evaluate MAC-net based VQA machines with and without the proposed logically consistent loss and the proposed data organization. The experiments confirm that the proposed loss formulae and introduction of hybrid-batch leads to more consistency as well as better performance. Though the proposed approach is tested with MAC-net, it can be utilised in any other QA methods whenever the logical consistency between answers exist.



### Online Multi-Object Tracking with delta-GLMB Filter based on Occlusion and Identity Switch Handling
- **Arxiv ID**: http://arxiv.org/abs/2011.10111v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2011.10111v2)
- **Published**: 2020-11-19 21:38:40+00:00
- **Updated**: 2021-04-26 08:31:57+00:00
- **Authors**: Mohammadjavad Abbaspour, Mohammad Ali Masnadi-Shirazi
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose an online multi-object tracking (MOT) method in a delta Generalized Labeled Multi-Bernoulli (delta-GLMB) filter framework to address occlusion and miss-detection issues, reduce false alarms, and recover identity switch (ID switch). To handle occlusion and miss-detection issues, we propose a measurement-to-disappeared track association method based on one-step delta-GLMB filter, so it is possible to manage these difficulties by jointly processing occluded or miss-detected objects. This part of proposed method is based on a proposed similarity metric which is responsible for defining the weight of hypothesized reappeared tracks. We also extend the delta-GLMB filter to efficiently recover switched IDs using the cardinality density, size and color features of the hypothesized tracks. We also propose a novel birth model to achieve more effective clutter removal performance. In both occlusion/miss-detection handler and newly-birthed object detector sections of the proposed method, unassigned measurements play a significant role, since they are used as the candidates for reappeared or birth objects. In addition, we perform an ablation study which confirms the effectiveness of our contributions in comparison with the baseline method. We evaluate the proposed method on well-known and publicly available MOT15 and MOT17 test datasets which are focused on pedestrian tracking. Experimental results show that the proposed tracker performs better or at least at the same level of the state-of-the-art online and offline MOT methods. It effectively handles the occlusion and ID switch issues and reduces false alarms as well.



### Robust Detection of Non-overlapping Ellipses from Points with Applications to Circular Target Extraction in Images and Cylinder Detection in Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2011.13849v3
- **DOI**: 10.1016/j.isprsjprs.2021.04.010
- **Categories**: **cs.CV**, cs.CG, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.13849v3)
- **Published**: 2020-11-19 21:56:02+00:00
- **Updated**: 2021-03-30 17:56:30+00:00
- **Authors**: Reza Maalek, Derek Lichti
- **Comment**: None
- **Journal**: None
- **Summary**: This manuscript provides a collection of new methods for the automated detection of non-overlapping ellipses from edge points. The methods introduce new developments in: (i) robust Monte Carlo-based ellipse fitting to 2-dimensional (2D) points in the presence of outliers; (ii) detection of non-overlapping ellipse from 2D edge points; and (iii) extraction of cylinder from 3D point clouds. The proposed methods were thoroughly compared with established state-of-the-art methods, using simulated and real-world datasets, through the design of four sets of original experiments. It was found that the proposed robust ellipse detection was superior to four reliable robust methods, including the popular least median of squares, in both simulated and real-world datasets. The proposed process for detecting non-overlapping ellipses achieved F-measure of 99.3% on real images, compared to F-measures of 42.4%, 65.6%, and 59.2%, obtained using the methods of Fornaciari, Patraucean, and Panagiotakis, respectively. The proposed cylinder extraction method identified all detectable mechanical pipes in two real-world point clouds, obtained under laboratory, and industrial construction site conditions. The results of this investigation show promise for the application of the proposed methods for automatic extraction of circular targets from images and pipes from point clouds.



### Batteries, camera, action! Learning a semantic control space for expressive robot cinematography
- **Arxiv ID**: http://arxiv.org/abs/2011.10118v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR, cs.HC, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2011.10118v2)
- **Published**: 2020-11-19 21:56:53+00:00
- **Updated**: 2021-03-31 21:15:21+00:00
- **Authors**: Rogerio Bonatti, Arthur Bucker, Sebastian Scherer, Mustafa Mukadam, Jessica Hodgins
- **Comment**: None
- **Journal**: None
- **Summary**: Aerial vehicles are revolutionizing the way film-makers can capture shots of actors by composing novel aerial and dynamic viewpoints. However, despite great advancements in autonomous flight technology, generating expressive camera behaviors is still a challenge and requires non-technical users to edit a large number of unintuitive control parameters. In this work, we develop a data-driven framework that enables editing of these complex camera positioning parameters in a semantic space (e.g. calm, enjoyable, establishing). First, we generate a database of video clips with a diverse range of shots in a photo-realistic simulator, and use hundreds of participants in a crowd-sourcing framework to obtain scores for a set of semantic descriptors for each clip. Next, we analyze correlations between descriptors and build a semantic control space based on cinematography guidelines and human perception studies. Finally, we learn a generative model that can map a set of desired semantic video descriptors into low-level camera trajectory parameters. We evaluate our system by demonstrating that our model successfully generates shots that are rated by participants as having the expected degrees of expression for each descriptor. We also show that our models generalize to different scenes in both simulation and real-world experiments. Data and video found at: https://sites.google.com/view/robotcam.



### VLG-Net: Video-Language Graph Matching Network for Video Grounding
- **Arxiv ID**: http://arxiv.org/abs/2011.10132v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2011.10132v2)
- **Published**: 2020-11-19 22:32:03+00:00
- **Updated**: 2021-08-16 14:53:59+00:00
- **Authors**: Mattia Soldan, Mengmeng Xu, Sisi Qu, Jesper Tegner, Bernard Ghanem
- **Comment**: 14 pages, 7 figures, In proceeding of the ICCV21 workshop: AI for
  Creative Video Editing and Understanding 2021
- **Journal**: None
- **Summary**: Grounding language queries in videos aims at identifying the time interval (or moment) semantically relevant to a language query. The solution to this challenging task demands understanding videos' and queries' semantic content and the fine-grained reasoning about their multi-modal interactions. Our key idea is to recast this challenge into an algorithmic graph matching problem. Fueled by recent advances in Graph Neural Networks, we propose to leverage Graph Convolutional Networks to model video and textual information as well as their semantic alignment. To enable the mutual exchange of information across the modalities, we design a novel Video-Language Graph Matching Network (VLG-Net) to match video and query graphs. Core ingredients include representation graphs built atop video snippets and query tokens separately and used to model intra-modality relationships. A Graph Matching layer is adopted for cross-modal context modeling and multi-modal fusion. Finally, moment candidates are created using masked moment attention pooling by fusing the moment's enriched snippet features. We demonstrate superior performance over state-of-the-art grounding methods on three widely used datasets for temporal localization of moments in videos with language queries: ActivityNet-Captions, TACoS, and DiDeMo.



### Cooperating RPN's Improve Few-Shot Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2011.10142v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.10142v1)
- **Published**: 2020-11-19 23:03:22+00:00
- **Updated**: 2020-11-19 23:03:22+00:00
- **Authors**: Weilin Zhang, Yu-Xiong Wang, David A. Forsyth
- **Comment**: None
- **Journal**: None
- **Summary**: Learning to detect an object in an image from very few training examples - few-shot object detection - is challenging, because the classifier that sees proposal boxes has very little training data. A particularly challenging training regime occurs when there are one or two training examples. In this case, if the region proposal network (RPN) misses even one high intersection-over-union (IOU) training box, the classifier's model of how object appearance varies can be severely impacted. We use multiple distinct yet cooperating RPN's. Our RPN's are trained to be different, but not too different; doing so yields significant performance improvements over state of the art for COCO and PASCAL VOC in the very few-shot setting. This effect appears to be independent of the choice of classifier or dataset.



### FlowStep3D: Model Unrolling for Self-Supervised Scene Flow Estimation
- **Arxiv ID**: http://arxiv.org/abs/2011.10147v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.10147v2)
- **Published**: 2020-11-19 23:23:48+00:00
- **Updated**: 2021-04-04 14:19:35+00:00
- **Authors**: Yair Kittenplon, Yonina C. Eldar, Dan Raviv
- **Comment**: CVPR 2021
- **Journal**: None
- **Summary**: Estimating the 3D motion of points in a scene, known as scene flow, is a core problem in computer vision. Traditional learning-based methods designed to learn end-to-end 3D flow often suffer from poor generalization. Here we present a recurrent architecture that learns a single step of an unrolled iterative alignment procedure for refining scene flow predictions. Inspired by classical algorithms, we demonstrate iterative convergence toward the solution using strong regularization. The proposed method can handle sizeable temporal deformations and suggests a slimmer architecture than competitive all-to-all correlation approaches. Trained on FlyingThings3D synthetic data only, our network successfully generalizes to real scans, outperforming all existing methods by a large margin on the KITTI self-supervised benchmark.



