# Arxiv Papers in cs.CV on 2020-11-22
### Evolving Search Space for Neural Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/2011.10904v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.10904v2)
- **Published**: 2020-11-22 01:11:19+00:00
- **Updated**: 2021-08-18 13:35:00+00:00
- **Authors**: Yuanzheng Ci, Chen Lin, Ming Sun, Boyu Chen, Hongwen Zhang, Wanli Ouyang
- **Comment**: Accepted for publication at the 2021 International Conference on
  Computer Vision (ICCV 2021)
- **Journal**: None
- **Summary**: The automation of neural architecture design has been a coveted alternative to human experts. Recent works have small search space, which is easier to optimize but has a limited upper bound of the optimal solution. Extra human design is needed for those methods to propose a more suitable space with respect to the specific task and algorithm capacity. To further enhance the degree of automation for neural architecture search, we present a Neural Search-space Evolution (NSE) scheme that iteratively amplifies the results from the previous effort by maintaining an optimized search space subset. This design minimizes the necessity of a well-designed search space. We further extend the flexibility of obtainable architectures by introducing a learnable multi-branch setting. By employing the proposed method, a consistent performance gain is achieved during a progressive search over upcoming search spaces. We achieve 77.3% top-1 retrain accuracy on ImageNet with 333M FLOPs, which yielded a state-of-the-art performance among previous auto-generated architectures that do not involve knowledge distillation or weight pruning. When the latency constraint is adopted, our result also performs better than the previous best-performing mobile models with a 77.9% Top-1 retrain accuracy.



### Video SemNet: Memory-Augmented Video Semantic Network
- **Arxiv ID**: http://arxiv.org/abs/2011.10909v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2011.10909v1)
- **Published**: 2020-11-22 01:36:37+00:00
- **Updated**: 2020-11-22 01:36:37+00:00
- **Authors**: Prashanth Vijayaraghavan, Deb Roy
- **Comment**: 6 pages, NIPS 2017 Workshop Visually-Grounded Interaction and
  Language (ViGIL)
- **Journal**: None
- **Summary**: Stories are a very compelling medium to convey ideas, experiences, social and cultural values. Narrative is a specific manifestation of the story that turns it into knowledge for the audience. In this paper, we propose a machine learning approach to capture the narrative elements in movies by bridging the gap between the low-level data representations and semantic aspects of the visual medium. We present a Memory-Augmented Video Semantic Network, called Video SemNet, to encode the semantic descriptors and learn an embedding for the video. The model employs two main components: (i) a neural semantic learner that learns latent embeddings of semantic descriptors and (ii) a memory module that retains and memorizes specific semantic patterns from the video. We evaluate the video representations obtained from variants of our model on two tasks: (a) genre prediction and (b) IMDB Rating prediction. We demonstrate that our model is able to predict genres and IMDB ratings with a weighted F-1 score of 0.72 and 0.63 respectively. The results are indicative of the representational power of our model and the ability of such representations to measure audience engagement.



### Hierachical Delta-Attention Method for Multimodal Fusion
- **Arxiv ID**: http://arxiv.org/abs/2011.10916v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.10916v2)
- **Published**: 2020-11-22 02:45:52+00:00
- **Updated**: 2022-11-28 13:08:47+00:00
- **Authors**: Kunjal Panchal
- **Comment**: Need to update the results
- **Journal**: None
- **Summary**: In vision and linguistics; the main input modalities are facial expressions, speech patterns, and the words uttered. The issue with analysis of any one mode of expression (Visual, Verbal or Vocal) is that lot of contextual information can get lost. This asks researchers to inspect multiple modalities to get a thorough understanding of the cross-modal dependencies and temporal context of the situation to analyze the expression. This work attempts at preserving the long-range dependencies within and across different modalities, which would be bottle-necked by the use of recurrent networks and adds the concept of delta-attention to focus on local differences per modality to capture the idiosyncrasy of different people. We explore a cross-attention fusion technique to get the global view of the emotion expressed through these delta-self-attended modalities, in order to fuse all the local nuances and global context together. The addition of attention is new to the multi-modal fusion field and currently being scrutinized for on what stage the attention mechanism should be used, this work achieves competitive accuracy for overall and per-class classification which is close to the current state-of-the-art with almost half number of parameters.



### Locally Linear Embedding and its Variants: Tutorial and Survey
- **Arxiv ID**: http://arxiv.org/abs/2011.10925v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.10925v1)
- **Published**: 2020-11-22 03:44:45+00:00
- **Updated**: 2020-11-22 03:44:45+00:00
- **Authors**: Benyamin Ghojogh, Ali Ghodsi, Fakhri Karray, Mark Crowley
- **Comment**: To appear as a part of an upcoming textbook on dimensionality
  reduction and manifold learning
- **Journal**: None
- **Summary**: This is a tutorial and survey paper for Locally Linear Embedding (LLE) and its variants. The idea of LLE is fitting the local structure of manifold in the embedding space. In this paper, we first cover LLE, kernel LLE, inverse LLE, and feature fusion with LLE. Then, we cover out-of-sample embedding using linear reconstruction, eigenfunctions, and kernel mapping. Incremental LLE is explained for embedding streaming data. Landmark LLE methods using the Nystrom approximation and locally linear landmarks are explained for big data embedding. We introduce the methods for parameter selection of number of neighbors using residual variance, Procrustes statistics, preservation neighborhood error, and local neighborhood selection. Afterwards, Supervised LLE (SLLE), enhanced SLLE, SLLE projection, probabilistic SLLE, supervised guided LLE (using Hilbert-Schmidt independence criterion), and semi-supervised LLE are explained for supervised and semi-supervised embedding. Robust LLE methods using least squares problem and penalty functions are also introduced for embedding in the presence of outliers and noise. Then, we introduce fusion of LLE with other manifold learning methods including Isomap (i.e., ISOLLE), principal component analysis, Fisher discriminant analysis, discriminant LLE, and Isotop. Finally, we explain weighted LLE in which the distances, reconstruction weights, or the embeddings are adjusted for better embedding; we cover weighted LLE for deformed distributed data, weighted LLE using probability of occurrence, SLLE by adjusting weights, modified LLE, and iterative LLE.



### We don't Need Thousand Proposals$\colon$ Single Shot Actor-Action Detection in Videos
- **Arxiv ID**: http://arxiv.org/abs/2011.10927v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.10927v1)
- **Published**: 2020-11-22 03:53:40+00:00
- **Updated**: 2020-11-22 03:53:40+00:00
- **Authors**: Aayush J Rana, Yogesh S Rawat
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: We propose SSA2D, a simple yet effective end-to-end deep network for actor-action detection in videos. The existing methods take a top-down approach based on region-proposals (RPN), where the action is estimated based on the detected proposals followed by post-processing such as non-maximal suppression. While effective in terms of performance, these methods pose limitations in scalability for dense video scenes with a high memory requirement for thousands of proposals. We propose to solve this problem from a different perspective where we don't need any proposals. SSA2D is a unified network, which performs pixel level joint actor-action detection in a single-shot, where every pixel of the detected actor is assigned an action label. SSA2D has two main advantages: 1) It is a fully convolutional network which does not require any proposals and post-processing making it memory as well as time efficient, 2) It is easily scalable to dense video scenes as its memory requirement is independent of the number of actors present in the scene. We evaluate the proposed method on the Actor-Action dataset (A2D) and Video Object Relation (VidOR) dataset, demonstrating its effectiveness in multiple actors and action detection in a video. SSA2D is 11x faster during inference with comparable (sometimes better) performance and fewer network parameters when compared with the prior works.



### Agglomerative Clustering of Handwritten Numerals to Determine Similarity of Different Languages
- **Arxiv ID**: http://arxiv.org/abs/2012.07599v1
- **DOI**: 10.1109/ICCIT48885.2019.9038550
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2012.07599v1)
- **Published**: 2020-11-22 04:36:25+00:00
- **Updated**: 2020-11-22 04:36:25+00:00
- **Authors**: Md. Rahat-uz-Zaman, Shadmaan Hye
- **Comment**: Submitted to the 22nd International Conference on Computer and
  Information Technology (ICCIT), 18-20 December, 2019, 6 pages, 5 figures and
  1 table
- **Journal**: None
- **Summary**: Handwritten numerals of different languages have various characteristics. Similarities and dissimilarities of the languages can be measured by analyzing the extracted features of the numerals. Handwritten numeral datasets are available and accessible for many renowned languages of different regions. In this paper, several handwritten numeral datasets of different languages are collected. Then they are used to find the similarity among those written languages through determining and comparing the similitude of each handwritten numerals. This will help to find which languages have the same or adjacent parent language. Firstly, a similarity measure of two numeral images is constructed with a Siamese network. Secondly, the similarity of the numeral datasets is determined with the help of the Siamese network and a new random sample with replacement similarity averaging technique. Finally, an agglomerative clustering is done based on the similarities of each dataset. This clustering technique shows some very interesting properties of the datasets. The property focused in this paper is the regional resemblance of the datasets. By analyzing the clusters, it becomes easy to identify which languages are originated from similar regions.



### CORAL: Colored structural representation for bi-modal place recognition
- **Arxiv ID**: http://arxiv.org/abs/2011.10934v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2011.10934v2)
- **Published**: 2020-11-22 04:51:40+00:00
- **Updated**: 2021-07-19 11:56:04+00:00
- **Authors**: Yiyuan Pan, Xuecheng Xu, Weijie Li, Yunxiang Cui, Yue Wang, Rong Xiong
- **Comment**: None
- **Journal**: None
- **Summary**: Place recognition is indispensable for a drift-free localization system. Due to the variations of the environment, place recognition using single-modality has limitations. In this paper, we propose a bi-modal place recognition method, which can extract a compound global descriptor from the two modalities, vision and LiDAR. Specifically, we first build the elevation image generated from 3D points as a structural representation. Then, we derive the correspondences between 3D points and image pixels that are further used in merging the pixel-wise visual features into the elevation map grids. In this way, we fuse the structural features and visual features in the consistent bird-eye view frame, yielding a semantic representation, namely CORAL. And the whole network is called CORAL-VLAD. Comparisons on the Oxford RobotCar show that CORAL-VLAD has superior performance against other state-of-the-art methods. We also demonstrate that our network can be generalized to other scenes and sensor configurations on cross-city datasets.



### Run Away From your Teacher: Understanding BYOL by a Novel Self-Supervised Approach
- **Arxiv ID**: http://arxiv.org/abs/2011.10944v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.10944v1)
- **Published**: 2020-11-22 05:49:50+00:00
- **Updated**: 2020-11-22 05:49:50+00:00
- **Authors**: Haizhou Shi, Dongliang Luo, Siliang Tang, Jian Wang, Yueting Zhuang
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, a newly proposed self-supervised framework Bootstrap Your Own Latent (BYOL) seriously challenges the necessity of negative samples in contrastive learning frameworks. BYOL works like a charm despite the fact that it discards the negative samples completely and there is no measure to prevent collapse in its training objective. In this paper, we suggest understanding BYOL from the view of our proposed interpretable self-supervised learning framework, Run Away From your Teacher (RAFT). RAFT optimizes two objectives at the same time: (i) aligning two views of the same data to similar representations and (ii) running away from the model's Mean Teacher (MT, the exponential moving average of the history models) instead of BYOL's running towards it. The second term of RAFT explicitly prevents the representation collapse and thus makes RAFT a more conceptually reliable framework. We provide basic benchmarks of RAFT on CIFAR10 to validate the effectiveness of our method. Furthermore, we prove that BYOL is equivalent to RAFT under certain conditions, providing solid reasoning for BYOL's counter-intuitive success.



### FP-NAS: Fast Probabilistic Neural Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/2011.10949v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.10949v3)
- **Published**: 2020-11-22 06:10:05+00:00
- **Updated**: 2021-03-31 17:21:49+00:00
- **Authors**: Zhicheng Yan, Xiaoliang Dai, Peizhao Zhang, Yuandong Tian, Bichen Wu, Matt Feiszli
- **Comment**: CVPR 2021 camera-ready version
- **Journal**: None
- **Summary**: Differential Neural Architecture Search (NAS) requires all layer choices to be held in memory simultaneously; this limits the size of both search space and final architecture. In contrast, Probabilistic NAS, such as PARSEC, learns a distribution over high-performing architectures, and uses only as much memory as needed to train a single model. Nevertheless, it needs to sample many architectures, making it computationally expensive for searching in an extensive space. To solve these problems, we propose a sampling method adaptive to the distribution entropy, drawing more samples to encourage explorations at the beginning, and reducing samples as learning proceeds. Furthermore, to search fast in the multi-variate space, we propose a coarse-to-fine strategy by using a factorized distribution at the beginning which can reduce the number of architecture parameters by over an order of magnitude. We call this method Fast Probabilistic NAS (FP-NAS). Compared with PARSEC, it can sample 64% fewer architectures and search 2.1x faster. Compared with FBNetV2, FP-NAS is 1.9x - 3.5x faster, and the searched models outperform FBNetV2 models on ImageNet. FP-NAS allows us to expand the giant FBNetV2 space to be wider (i.e. larger channel choices) and deeper (i.e. more blocks), while adding Split-Attention block and enabling the search over the number of splits. When searching a model of size 0.4G FLOPS, FP-NAS is 132x faster than EfficientNet, and the searched FP-NAS-L0 model outperforms EfficientNet-B0 by 0.7% accuracy. Without using any architecture surrogate or scaling tricks, we directly search large models up to 1.0G FLOPS. Our FP-NAS-L2 model with simple distillation outperforms BigNAS-XL with advanced in-place distillation by 0.7% accuracy using similar FLOPS.



### Language-guided Navigation via Cross-Modal Grounding and Alternate Adversarial Learning
- **Arxiv ID**: http://arxiv.org/abs/2011.10972v1
- **DOI**: 10.1109/TCSVT.2020.3039522
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2011.10972v1)
- **Published**: 2020-11-22 09:13:46+00:00
- **Updated**: 2020-11-22 09:13:46+00:00
- **Authors**: Weixia Zhang, Chao Ma, Qi Wu, Xiaokang Yang
- **Comment**: Accepted to IEEE TCSVT
- **Journal**: None
- **Summary**: The emerging vision-and-language navigation (VLN) problem aims at learning to navigate an agent to the target location in unseen photo-realistic environments according to the given language instruction. The main challenges of VLN arise mainly from two aspects: first, the agent needs to attend to the meaningful paragraphs of the language instruction corresponding to the dynamically-varying visual environments; second, during the training process, the agent usually imitate the shortest-path to the target location. Due to the discrepancy of action selection between training and inference, the agent solely on the basis of imitation learning does not perform well. Sampling the next action from its predicted probability distribution during the training process allows the agent to explore diverse routes from the environments, yielding higher success rates. Nevertheless, without being presented with the shortest navigation paths during the training process, the agent may arrive at the target location through an unexpected longer route. To overcome these challenges, we design a cross-modal grounding module, which is composed of two complementary attention mechanisms, to equip the agent with a better ability to track the correspondence between the textual and visual modalities. We then propose to recursively alternate the learning schemes of imitation and exploration to narrow the discrepancy between training and inference. We further exploit the advantages of both these two learning schemes via adversarial learning. Extensive experimental results on the Room-to-Room (R2R) benchmark dataset demonstrate that the proposed learning scheme is generalized and complementary to prior arts. Our method performs well against state-of-the-art approaches in terms of effectiveness and efficiency.



### Learnable Sampling 3D Convolution for Video Enhancement and Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2011.10974v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.10974v1)
- **Published**: 2020-11-22 09:20:49+00:00
- **Updated**: 2020-11-22 09:20:49+00:00
- **Authors**: Shuyang Gu, Jianmin Bao, Dong Chen
- **Comment**: None
- **Journal**: None
- **Summary**: A key challenge in video enhancement and action recognition is to fuse useful information from neighboring frames. Recent works suggest establishing accurate correspondences between neighboring frames before fusing temporal information. However, the generated results heavily depend on the quality of correspondence estimation. In this paper, we propose a more robust solution: \emph{sampling and fusing multi-level features} across neighborhood frames to generate the results. Based on this idea, we introduce a new module to improve the capability of 3D convolution, namely, learnable sampling 3D convolution (\emph{LS3D-Conv}). We add learnable 2D offsets to 3D convolution which aims to sample locations on spatial feature maps across frames. The offsets can be learned for specific tasks. The \emph{LS3D-Conv} can flexibly replace 3D convolution layers in existing 3D networks and get new architectures, which learns the sampling at multiple feature levels. The experiments on video interpolation, video super-resolution, video denoising, and action recognition demonstrate the effectiveness of our approach.



### Robust Unsupervised Small Area Change Detection from SAR Imagery Using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2011.11005v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2011.11005v1)
- **Published**: 2020-11-22 12:50:08+00:00
- **Updated**: 2020-11-22 12:50:08+00:00
- **Authors**: Xinzheng Zhang, Hang Su, Ce Zhang, Xiaowei Gu, Xiaoheng Tan, Peter M. Atkinson
- **Comment**: None
- **Journal**: None
- **Summary**: Small area change detection from synthetic aperture radar (SAR) is a highly challenging task. In this paper, a robust unsupervised approach is proposed for small area change detection from multi-temporal SAR images using deep learning. First, a multi-scale superpixel reconstruction method is developed to generate a difference image (DI), which can suppress the speckle noise effectively and enhance edges by exploiting local, spatially homogeneous information. Second, a two-stage centre-constrained fuzzy c-means clustering algorithm is proposed to divide the pixels of the DI into changed, unchanged and intermediate classes with a parallel clustering strategy. Image patches belonging to the first two classes are then constructed as pseudo-label training samples, and image patches of the intermediate class are treated as testing samples. Finally, a convolutional wavelet neural network (CWNN) is designed and trained to classify testing samples into changed or unchanged classes, coupled with a deep convolutional generative adversarial network (DCGAN) to increase the number of changed class within the pseudo-label training samples. Numerical experiments on four real SAR datasets demonstrate the validity and robustness of the proposed approach, achieving up to 99.61% accuracy for small area change detection.



### SAMA-VTOL: A new unmanned aircraft system for remotely sensed data collection
- **Arxiv ID**: http://arxiv.org/abs/2011.11007v1
- **DOI**: 10.1117/12.2580533
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.11007v1)
- **Published**: 2020-11-22 12:55:16+00:00
- **Updated**: 2020-11-22 12:55:16+00:00
- **Authors**: Mohammad R. Bayanlou, Mehdi Khoshboresh-Masouleh
- **Comment**: 12 pages, 6 figures
- **Journal**: Proceedings Volume 11525, SPIE Future Sensing Technologies;
  115250V (2020)
- **Summary**: In recent years, unmanned aircraft systems (UASs) are frequently used in many different applications of photogrammetry such as building damage monitoring, archaeological mapping and vegetation monitoring. In this paper, a new state-of-the-art vertical take-off and landing fixed-wing UAS is proposed to robust photogrammetry missions, called SAMA-VTOL. In this study, the capability of SAMA-VTOL is investigated for generating orthophoto. The major stages are including designing, building and experimental scenario. First, a brief description of design and build is introduced. Next, an experiment was done to generate accurate orthophoto with minimum ground control points requirements. The processing step, which includes automatic aerial triangulation with camera calibration and model generation. In this regard, the Pix4Dmapper software was used to orientate the images, produce point clouds, creating digital surface model and generating orthophoto mosaic. Experimental results based on the test area covering 26.3 hectares indicate that our SAMA-VTOL performs well in the orthophoto mosaic task.



### Enriching ImageNet with Human Similarity Judgments and Psychological Embeddings
- **Arxiv ID**: http://arxiv.org/abs/2011.11015v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.11015v1)
- **Published**: 2020-11-22 13:41:54+00:00
- **Updated**: 2020-11-22 13:41:54+00:00
- **Authors**: Brett D. Roads, Bradley C. Love
- **Comment**: None
- **Journal**: None
- **Summary**: Advances in object recognition flourished in part because of the availability of high-quality datasets and associated benchmarks. However, these benchmarks---such as ILSVRC---are relatively task-specific, focusing predominately on predicting class labels. We introduce a publicly-available dataset that embodies the task-general capabilities of human perception and reasoning. The Human Similarity Judgments extension to ImageNet (ImageNet-HSJ) is composed of human similarity judgments that supplement the ILSVRC validation set. The new dataset supports a range of task and performance metrics, including the evaluation of unsupervised learning algorithms. We demonstrate two methods of assessment: using the similarity judgments directly and using a psychological embedding trained on the similarity judgments. This embedding space contains an order of magnitude more points (i.e., images) than previous efforts based on human judgments. Scaling to the full 50,000 image set was made possible through a selective sampling process that used variational Bayesian inference and model ensembles to sample aspects of the embedding space that were most uncertain. This methodological innovation not only enables scaling, but should also improve the quality of solutions by focusing sampling where it is needed. To demonstrate the utility of ImageNet-HSJ, we used the similarity ratings and the embedding space to evaluate how well several popular models conform to human similarity judgments. One finding is that more complex models that perform better on task-specific benchmarks do not better conform to human semantic judgments. In addition to the human similarity judgments, pre-trained psychological embeddings and code for inferring variational embeddings are made publicly available. Collectively, ImageNet-HSJ assets support the appraisal of internal representations and the development of more human-like models.



### Cryo-ZSSR: multiple-image super-resolution based on deep internal learning
- **Arxiv ID**: http://arxiv.org/abs/2011.11020v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, q-bio.BM
- **Links**: [PDF](http://arxiv.org/pdf/2011.11020v1)
- **Published**: 2020-11-22 14:04:54+00:00
- **Updated**: 2020-11-22 14:04:54+00:00
- **Authors**: Qinwen Huang, Ye Zhou, Xiaochen Du, Reed Chen, Jianyou Wang, Cynthia Rudin, Alberto Bartesaghi
- **Comment**: 11 pages, 4 figures
- **Journal**: None
- **Summary**: Single-particle cryo-electron microscopy (cryo-EM) is an emerging imaging modality capable of visualizing proteins and macro-molecular complexes at near-atomic resolution. The low electron-doses used to prevent sample radiation damage, result in images where the power of the noise is 100 times greater than the power of the signal. To overcome the low-SNRs, hundreds of thousands of particle projections acquired over several days of data collection are averaged in 3D to determine the structure of interest. Meanwhile, recent image super-resolution (SR) techniques based on neural networks have shown state of the art performance on natural images. Building on these advances, we present a multiple-image SR algorithm based on deep internal learning designed specifically to work under low-SNR conditions. Our approach leverages the internal image statistics of cryo-EM movies and does not require training on ground-truth data. When applied to a single-particle dataset of apoferritin, we show that the resolution of 3D structures obtained from SR micrographs can surpass the limits imposed by the imaging system. Our results indicate that the combination of low magnification imaging with image SR has the potential to accelerate cryo-EM data collection without sacrificing resolution.



### Interpreting Super-Resolution Networks with Local Attribution Maps
- **Arxiv ID**: http://arxiv.org/abs/2011.11036v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.11036v2)
- **Published**: 2020-11-22 15:11:00+00:00
- **Updated**: 2021-08-22 14:52:10+00:00
- **Authors**: Jinjin Gu, Chao Dong
- **Comment**: Accepted by CVPR 2021
- **Journal**: None
- **Summary**: Image super-resolution (SR) techniques have been developing rapidly, benefiting from the invention of deep networks and its successive breakthroughs. However, it is acknowledged that deep learning and deep neural networks are difficult to interpret. SR networks inherit this mysterious nature and little works make attempt to understand them. In this paper, we perform attribution analysis of SR networks, which aims at finding the input pixels that strongly influence the SR results. We propose a novel attribution approach called local attribution map (LAM), which inherits the integral gradient method yet with two unique features. One is to use the blurred image as the baseline input, and the other is to adopt the progressive blurring function as the path function. Based on LAM, we show that: (1) SR networks with a wider range of involved input pixels could achieve better performance. (2) Attention networks and non-local networks extract features from a wider range of input pixels. (3) Comparing with the range that actually contributes, the receptive field is large enough for most deep networks. (4) For SR networks, textures with regular stripes or grids are more likely to be noticed, while complex semantics are difficult to utilize. Our work opens new directions for designing SR networks and interpreting low-level vision deep models.



### PS-DeVCEM: Pathology-sensitive deep learning model for video capsule endoscopy based on weakly labeled data
- **Arxiv ID**: http://arxiv.org/abs/2011.12957v1
- **DOI**: 10.1016/j.cviu.2020.103062
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.12957v1)
- **Published**: 2020-11-22 15:33:37+00:00
- **Updated**: 2020-11-22 15:33:37+00:00
- **Authors**: A. Mohammed, I. Farup, M. Pedersen, S. Yildirim, Ø Hovde
- **Comment**: None
- **Journal**: Computer Vision and Image Understanding 201 (2020): 103062
- **Summary**: We propose a novel pathology-sensitive deep learning model (PS-DeVCEM) for frame-level anomaly detection and multi-label classification of different colon diseases in video capsule endoscopy (VCE) data. Our proposed model is capable of coping with the key challenge of colon apparent heterogeneity caused by several types of diseases. Our model is driven by attention-based deep multiple instance learning and is trained end-to-end on weakly labeled data using video labels instead of detailed frame-by-frame annotation. The spatial and temporal features are obtained through ResNet50 and residual Long short-term memory (residual LSTM) blocks, respectively. Additionally, the learned temporal attention module provides the importance of each frame to the final label prediction. Moreover, we developed a self-supervision method to maximize the distance between classes of pathologies. We demonstrate through qualitative and quantitative experiments that our proposed weakly supervised learning model gives superior precision and F1-score reaching, 61.6% and 55.1%, as compared to three state-of-the-art video analysis methods respectively. We also show our model's ability to temporally localize frames with pathologies, without frame annotation information during training. Furthermore, we collected and annotated the first and largest VCE dataset with only video labels. The dataset contains 455 short video segments with 28,304 frames and 14 classes of colorectal diseases and artifacts. Dataset and code supporting this publication will be made available on our home page.



### Efficient embedding network for 3D brain tumor segmentation
- **Arxiv ID**: http://arxiv.org/abs/2011.11052v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.11052v1)
- **Published**: 2020-11-22 16:17:29+00:00
- **Updated**: 2020-11-22 16:17:29+00:00
- **Authors**: Hicham Messaoudi, Ahror Belaid, Mohamed Lamine Allaoui, Ahcene Zetout, Mohand Said Allili, Souhil Tliba, Douraied Ben Salem, Pierre-Henri Conze
- **Comment**: Multimodal Brain Tumor Segmentation Challenge 2020
- **Journal**: Multimodal Brain Tumor Segmentation Challenge 2020 (BRATS)
  BrainLes 2020
- **Summary**: 3D medical image processing with deep learning greatly suffers from a lack of data. Thus, studies carried out in this field are limited compared to works related to 2D natural image analysis, where very large datasets exist. As a result, powerful and efficient 2D convolutional neural networks have been developed and trained. In this paper, we investigate a way to transfer the performance of a two-dimensional classiffication network for the purpose of three-dimensional semantic segmentation of brain tumors. We propose an asymmetric U-Net network by incorporating the EfficientNet model as part of the encoding branch. As the input data is in 3D, the first layers of the encoder are devoted to the reduction of the third dimension in order to fit the input of the EfficientNet network. Experimental results on validation and test data from the BraTS 2020 challenge demonstrate that the proposed method achieve promising performance.



### Investigating Emotion-Color Association in Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2011.11058v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.11058v1)
- **Published**: 2020-11-22 16:48:02+00:00
- **Updated**: 2020-11-22 16:48:02+00:00
- **Authors**: Shivi Gupta, Shashi Kant Gupta
- **Comment**: 8 pages, 4 figures
- **Journal**: None
- **Summary**: It has been found that representations learned by Deep Neural Networks (DNNs) correlate very well to neural responses measured in primates' brains and psychological representations exhibited by human similarity judgment. On another hand, past studies have shown that particular colors can be associated with specific emotion arousal in humans. Do deep neural networks also learn this behavior? In this study, we investigate if DNNs can learn implicit associations in stimuli, particularly, an emotion-color association between image stimuli. Our study was conducted in two parts. First, we collected human responses on a forced-choice decision task in which subjects were asked to select a color for a specified emotion-inducing image. Next, we modeled this decision task on neural networks using the similarity between deep representation (extracted using DNNs trained on object classification tasks) of the images and images of colors used in the task. We found that our model showed a fuzzy linear relationship between the two decision probabilities. This results in two interesting findings, 1. The representations learned by deep neural networks can indeed show an emotion-color association 2. The emotion-color association is not just random but involves some cognitive phenomena. Finally, we also show that this method can help us in the emotion classification task, specifically when there are very few examples to train the model. This analysis can be relevant to psychologists studying emotion-color associations and artificial intelligence researchers modeling emotional intelligence in machines or studying representations learned by deep neural networks.



### Registration of serial sections: An evaluation method based on distortions of the ground truths
- **Arxiv ID**: http://arxiv.org/abs/2011.11060v2
- **DOI**: 10.1109/ACCESS.2021.3124341
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.11060v2)
- **Published**: 2020-11-22 16:50:52+00:00
- **Updated**: 2021-06-06 18:01:02+00:00
- **Authors**: Oleg Lobachev, Takuya Funatomi, Alexander Pfaffenroth, Reinhold Förster, Lars Knudsen, Christoph Wrede, Michael Guthe, David Haberthür, Ruslan Hlushchuk, Thomas Salaets, Jaan Toelen, Simone Gaffling, Christian Mühlfeld, Roman Grothausmann
- **Comment**: Supplemental data available under https://zenodo.org/record/4282448
- **Journal**: None
- **Summary**: Registration of histological serial sections is a challenging task. Serial sections exhibit distortions and damage from sectioning. Missing information on how the tissue looked before cutting makes a realistic validation of 2D registrations extremely difficult.   This work proposes methods for ground-truth-based evaluation of registrations. Firstly, we present a methodology to generate test data for registrations. We distort an innately registered image stack in the manner similar to the cutting distortion of serial sections. Test cases are generated from existing 3D data sets, thus the ground truth is known. Secondly, our test case generation premises evaluation of the registrations with known ground truths. Our methodology for such an evaluation technique distinguishes this work from other approaches. Both under- and over-registration become evident in our evaluations. We also survey existing validation efforts.   We present a full-series evaluation across six different registration methods applied to our distorted 3D data sets of animal lungs. Our distorted and ground truth data sets are made publicly available.



### RNNP: A Robust Few-Shot Learning Approach
- **Arxiv ID**: http://arxiv.org/abs/2011.11067v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.11067v1)
- **Published**: 2020-11-22 17:23:08+00:00
- **Updated**: 2020-11-22 17:23:08+00:00
- **Authors**: Pratik Mazumder, Pravendra Singh, Vinay P. Namboodiri
- **Comment**: Accepted in WACV 2021
- **Journal**: None
- **Summary**: Learning from a few examples is an important practical aspect of training classifiers. Various works have examined this aspect quite well. However, all existing approaches assume that the few examples provided are always correctly labeled. This is a strong assumption, especially if one considers the current techniques for labeling using crowd-based labeling services. We address this issue by proposing a novel robust few-shot learning approach. Our method relies on generating robust prototypes from a set of few examples. Specifically, our method refines the class prototypes by producing hybrid features from the support examples of each class. The refined prototypes help to classify the query images better. Our method can replace the evaluation phase of any few-shot learning method that uses a nearest neighbor prototype-based evaluation procedure to make them robust. We evaluate our method on standard mini-ImageNet and tiered-ImageNet datasets. We perform experiments with various label corruption rates in the support examples of the few-shot classes. We obtain significant improvement over widely used few-shot learning methods that suffer significant performance degeneration in the presence of label noise. We finally provide extensive ablation experiments to validate our method.



### QuerYD: A video dataset with high-quality text and audio narrations
- **Arxiv ID**: http://arxiv.org/abs/2011.11071v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.11071v2)
- **Published**: 2020-11-22 17:33:44+00:00
- **Updated**: 2021-02-17 13:38:19+00:00
- **Authors**: Andreea-Maria Oncescu, João F. Henriques, Yang Liu, Andrew Zisserman, Samuel Albanie
- **Comment**: 5 pages, 4 figures, accepted at ICASSP 2021
- **Journal**: None
- **Summary**: We introduce QuerYD, a new large-scale dataset for retrieval and event localisation in video. A unique feature of our dataset is the availability of two audio tracks for each video: the original audio, and a high-quality spoken description of the visual content. The dataset is based on YouDescribe, a volunteer project that assists visually-impaired people by attaching voiced narrations to existing YouTube videos. This ever-growing collection of videos contains highly detailed, temporally aligned audio and text annotations. The content descriptions are more relevant than dialogue, and more detailed than previous description attempts, which can be observed to contain many superficial or uninformative descriptions. To demonstrate the utility of the QuerYD dataset, we show that it can be used to train and benchmark strong models for retrieval and event localisation. Data, code and models are made publicly available, and we hope that QuerYD inspires further research on video understanding with written and spoken natural language.



### Generative Adversarial Stacked Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/2011.12236v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.12236v1)
- **Published**: 2020-11-22 17:51:59+00:00
- **Updated**: 2020-11-22 17:51:59+00:00
- **Authors**: Ariel Ruiz-Garcia, Ibrahim Almakky, Vasile Palade, Luke Hicks
- **Comment**: arXiv admin note: text overlap with arXiv:2007.09790
- **Journal**: Proceedings of the LatinX in AI Research Workshop at at the 34th
  Conference on Neural Information Processing Systems (NeurIPS 2020),
  Vancouver, Canada
- **Summary**: Generative Adversarial Networks (GANs) have become predominant in image generation tasks. Their success is attributed to the training regime which employs two models: a generator G and discriminator D that compete in a minimax zero sum game. Nonetheless, GANs are difficult to train due to their sensitivity to hyperparameter and parameter initialisation, which often leads to vanishing gradients, non-convergence, or mode collapse, where the generator is unable to create samples with different variations. In this work, we propose a novel Generative Adversarial Stacked Convolutional Autoencoder(GASCA) model and a generative adversarial gradual greedy layer-wise learning algorithm de-signed to train Adversarial Autoencoders in an efficient and incremental manner. Our training approach produces images with significantly lower reconstruction error than vanilla joint training.



### End-to-End Differentiable 6DoF Object Pose Estimation with Local and Global Constraints
- **Arxiv ID**: http://arxiv.org/abs/2011.11078v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.11078v1)
- **Published**: 2020-11-22 18:02:25+00:00
- **Updated**: 2020-11-22 18:02:25+00:00
- **Authors**: Anshul Gupta, Joydeep Medhi, Aratrik Chattopadhyay, Vikram Gupta
- **Comment**: Accepted at the Workshop on Differentiable vision, graphics, and
  physics applied to machine learning at Neurips 2020
- **Journal**: None
- **Summary**: Inferring the 6DoF pose of an object from a single RGB image is an important but challenging task, especially under heavy occlusion. While recent approaches improve upon the two stage approaches by training an end-to-end pipeline, they do not leverage local and global constraints. In this paper, we propose pairwise feature extraction to integrate local constraints, and triplet regularization to integrate global constraints for improved 6DoF object pose estimation. Coupled with better augmentation, our approach achieves state of the art results on the challenging Occlusion Linemod dataset, with a 9% improvement over the previous state of the art, and achieves competitive results on the Linemod dataset.



### Nudge Attacks on Point-Cloud DNNs
- **Arxiv ID**: http://arxiv.org/abs/2011.11637v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.11637v1)
- **Published**: 2020-11-22 18:04:02+00:00
- **Updated**: 2020-11-22 18:04:02+00:00
- **Authors**: Yiren Zhao, Ilia Shumailov, Robert Mullins, Ross Anderson
- **Comment**: None
- **Journal**: None
- **Summary**: The wide adaption of 3D point-cloud data in safety-critical applications such as autonomous driving makes adversarial samples a real threat. Existing adversarial attacks on point clouds achieve high success rates but modify a large number of points, which is usually difficult to do in real-life scenarios. In this paper, we explore a family of attacks that only perturb a few points of an input point cloud, and name them nudge attacks. We demonstrate that nudge attacks can successfully flip the results of modern point-cloud DNNs. We present two variants, gradient-based and decision-based, showing their effectiveness in white-box and grey-box scenarios. Our extensive experiments show nudge attacks are effective at generating both targeted and untargeted adversarial point clouds, by changing a few points or even a single point from the entire point-cloud input. We find that with a single point we can reliably thwart predictions in 12--80% of cases, whereas 10 points allow us to further increase this to 37--95%. Finally, we discuss the possible defenses against such attacks, and explore their limitations.



### Deep learning model trained on mobile phone-acquired frozen section images effectively detects basal cell carcinoma
- **Arxiv ID**: http://arxiv.org/abs/2011.11081v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.11081v1)
- **Published**: 2020-11-22 18:30:23+00:00
- **Updated**: 2020-11-22 18:30:23+00:00
- **Authors**: Junli Cao, B. S., Junyan Wu, M. S., Jing W. Zhang, M. D., Ph. D., Jay J. Ye, M. D., Ph. D., Limin Yu, M. D., M. S
- **Comment**: None
- **Journal**: None
- **Summary**: Background: Margin assessment of basal cell carcinoma using the frozen section is a common task of pathology intraoperative consultation. Although frequently straight-forward, the determination of the presence or absence of basal cell carcinoma on the tissue sections can sometimes be challenging. We explore if a deep learning model trained on mobile phone-acquired frozen section images can have adequate performance for future deployment. Materials and Methods: One thousand two hundred and forty-one (1241) images of frozen sections performed for basal cell carcinoma margin status were acquired using mobile phones. The photos were taken at 100x magnification (10x objective). The images were downscaled from a 4032 x 3024 pixel resolution to 576 x 432 pixel resolution. Semantic segmentation algorithm Deeplab V3 with Xception backbone was used for model training. Results: The model uses an image as input and produces a 2-dimensional black and white output of prediction of the same dimension; the areas determined to be basal cell carcinoma were displayed with white color, in a black background. Any output with the number of white pixels exceeding 0.5% of the total number of pixels is deemed positive for basal cell carcinoma. On the test set, the model achieves area under curve of 0.99 for receiver operator curve and 0.97 for precision-recall curve at the pixel level. The accuracy of classification at the slide level is 96%. Conclusions: The deep learning model trained with mobile phone images shows satisfactory performance characteristics, and thus demonstrates the potential for deploying as a mobile phone app to assist in frozen section interpretation in real time.



### Dense open-set recognition with synthetic outliers generated by Real NVP
- **Arxiv ID**: http://arxiv.org/abs/2011.11094v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.11094v1)
- **Published**: 2020-11-22 19:40:26+00:00
- **Updated**: 2020-11-22 19:40:26+00:00
- **Authors**: Matej Grcić, Petra Bevandić, Siniša Šegvić
- **Comment**: Accepted to VISAPP 2021 conference
- **Journal**: None
- **Summary**: Today's deep models are often unable to detect inputs which do not belong to the training distribution. This gives rise to confident incorrect predictions which could lead to devastating consequences in many important application fields such as healthcare and autonomous driving. Interestingly, both discriminative and generative models appear to be equally affected. Consequently, this vulnerability represents an important research challenge. We consider an outlier detection approach based on discriminative training with jointly learned synthetic outliers. We obtain the synthetic outliers by sampling an RNVP model which is jointly trained to generate datapoints at the border of the training distribution. We show that this approach can be adapted for simultaneous semantic segmentation and dense outlier detection. We present image classification experiments on CIFAR-10, as well as semantic segmentation experiments on three existing datasets (StreetHazards, WD-Pascal, Fishyscapes Lost & Found), and one contributed dataset. Our models perform competitively with respect to the state of the art despite producing predictions with only one forward pass.



### Multiresolution Knowledge Distillation for Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/2011.11108v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.11108v1)
- **Published**: 2020-11-22 21:16:35+00:00
- **Updated**: 2020-11-22 21:16:35+00:00
- **Authors**: Mohammadreza Salehi, Niousha Sadjadi, Soroosh Baselizadeh, Mohammad Hossein Rohban, Hamid R. Rabiee
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised representation learning has proved to be a critical component of anomaly detection/localization in images. The challenges to learn such a representation are two-fold. Firstly, the sample size is not often large enough to learn a rich generalizable representation through conventional techniques. Secondly, while only normal samples are available at training, the learned features should be discriminative of normal and anomalous samples. Here, we propose to use the "distillation" of features at various layers of an expert network, pre-trained on ImageNet, into a simpler cloner network to tackle both issues. We detect and localize anomalies using the discrepancy between the expert and cloner networks' intermediate activation values given the input data. We show that considering multiple intermediate hints in distillation leads to better exploiting the expert's knowledge and more distinctive discrepancy compared to solely utilizing the last layer activation values. Notably, previous methods either fail in precise anomaly localization or need expensive region-based training. In contrast, with no need for any special or intensive training procedure, we incorporate interpretability algorithms in our novel framework for the localization of anomalous regions. Despite the striking contrast between some test datasets and ImageNet, we achieve competitive or significantly superior results compared to the SOTA methods on MNIST, F-MNIST, CIFAR-10, MVTecAD, Retinal-OCT, and two Medical datasets on both anomaly detection and localization.



