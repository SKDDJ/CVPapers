# Arxiv Papers in cs.CV on 2020-11-07
### Strawberry Detection Using a Heterogeneous Multi-Processor Platform
- **Arxiv ID**: http://arxiv.org/abs/2011.03651v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.03651v1)
- **Published**: 2020-11-07 01:08:21+00:00
- **Updated**: 2020-11-07 01:08:21+00:00
- **Authors**: Samuel Brandenburg, Pedro Machado, Nikesh Lama, T. M. McGinnity
- **Comment**: None
- **Journal**: None
- **Summary**: Over the last few years, the number of precision farming projects has increased specifically in harvesting robots and many of which have made continued progress from identifying crops to grasping the desired fruit or vegetable. One of the most common issues found in precision farming projects is that successful application is heavily dependent not just on identifying the fruit but also on ensuring that localisation allows for accurate navigation. These issues become significant factors when the robot is not operating in a prearranged environment, or when vegetation becomes too thick, thus covering crop. Moreover, running a state-of-the-art deep learning algorithm on an embedded platform is also very challenging, resulting most of the times in low frame rates. This paper proposes using the You Only Look Once version 3 (YOLOv3) Convolutional Neural Network (CNN) in combination with utilising image processing techniques for the application of precision farming robots targeting strawberry detection, accelerated on a heterogeneous multiprocessor platform. The results show a performance acceleration by five times when implemented on a Field-Programmable Gate Array (FPGA) when compared with the same algorithm running on the processor side with an accuracy of 78.3\% over the test set comprised of 146 images.



### ROBIN: a Graph-Theoretic Approach to Reject Outliers in Robust Estimation using Invariants
- **Arxiv ID**: http://arxiv.org/abs/2011.03659v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2011.03659v2)
- **Published**: 2020-11-07 02:09:33+00:00
- **Updated**: 2021-03-23 20:02:00+00:00
- **Authors**: Jingnan Shi, Heng Yang, Luca Carlone
- **Comment**: None
- **Journal**: None
- **Summary**: Many estimation problems in robotics, computer vision, and learning require estimating unknown quantities in the face of outliers. Outliers are typically the result of incorrect data association or feature matching, and it is common to have problems where more than 90% of the measurements used for estimation are outliers. While current approaches for robust estimation are able to deal with moderate amounts of outliers, they fail to produce accurate estimates in the presence of many outliers. This paper develops an approach to prune outliers. First, we develop a theory of invariance that allows us to quickly check if a subset of measurements are mutually compatible without explicitly solving the estimation problem. Second, we develop a graph-theoretic framework, where measurements are modeled as vertices and mutual compatibility is captured by edges. We generalize existing results showing that the inliers form a clique in this graph and typically belong to the maximum clique. We also show that in practice the maximum k-core of the compatibility graph provides an approximation of the maximum clique, while being faster to compute in large problems. These two contributions leads to ROBIN, our approach to Reject Outliers Based on INvariants, which allows us to quickly prune outliers in generic estimation problems. We demonstrate ROBIN in four geometric perception problems and show it boosts robustness of existing solvers while running in milliseconds in large problems.



### Identifying Mislabeled Images in Supervised Learning Utilizing Autoencoder
- **Arxiv ID**: http://arxiv.org/abs/2011.03667v2
- **DOI**: 10.1007/978-3-030-89880-9_21
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.03667v2)
- **Published**: 2020-11-07 03:09:34+00:00
- **Updated**: 2021-01-18 22:59:44+00:00
- **Authors**: Yunhao Yang, Andrew Whinston
- **Comment**: UTCS Tech Report: Honors Thesis. 12 pages, 11 figures
- **Journal**: Lecture Notes in Networks and Systems vol 359 (2021) 266-282
- **Summary**: Supervised learning is based on the assumption that the ground truth in the training data is accurate. However, this may not be guaranteed in real-world settings. Inaccurate training data will result in some unexpected predictions. In image classification, incorrect labels may cause the classification model to be inaccurate as well. In this paper, I am going to apply unsupervised techniques to the training data before training the classification network. A convolutional autoencoder is applied to encode and reconstruct images. The encoder will project the image data on to latent space. In the latent space, image features are preserved in a lower dimension. The assumption is that data samples with similar features are likely to have the same label. Noised samples can be classified in the latent space by the Density-Base Scan (DBSCAN) clustering algorithm. These incorrectly labeled data are visualized as outliers in the latent space. Therefore, the outliers identified by the DBSCAN algorithm can be classified as incorrectly labeled samples. After the outliers are detected, all the outliers are treated as mislabeled data samples and removed from the dataset. Thus the training data can be directly used in training the supervised learning network. The algorithm can detect and remove above 67\% of mislabeled data in the experimental dataset.



### Domain-Aware Unsupervised Hyperspectral Reconstruction for Aerial Image Dehazing
- **Arxiv ID**: http://arxiv.org/abs/2011.03677v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.03677v1)
- **Published**: 2020-11-07 03:30:52+00:00
- **Updated**: 2020-11-07 03:30:52+00:00
- **Authors**: Aditya Mehta, Harsh Sinha, Murari Mandal, Pratik Narang
- **Comment**: WACV-2021
- **Journal**: None
- **Summary**: Haze removal in aerial images is a challenging problem due to considerable variation in spatial details and varying contrast. Changes in particulate matter density often lead to degradation in visibility. Therefore, several approaches utilize multi-spectral data as auxiliary information for haze removal. In this paper, we propose SkyGAN for haze removal in aerial images. SkyGAN consists of 1) a domain-aware hazy-to-hyperspectral (H2H) module, and 2) a conditional GAN (cGAN) based multi-cue image-to-image translation module (I2I) for dehazing. The proposed H2H module reconstructs several visual bands from RGB images in an unsupervised manner, which overcomes the lack of hazy hyperspectral aerial image datasets. The module utilizes task supervision and domain adaptation in order to create a "hyperspectral catalyst" for image dehazing. The I2I module uses the hyperspectral catalyst along with a 12-channel multi-cue input and performs effective image dehazing by utilizing the entire visual spectrum. In addition, this work introduces a new dataset, called Hazy Aerial-Image (HAI) dataset, that contains more than 65,000 pairs of hazy and ground truth aerial images with realistic, non-homogeneous haze of varying density. The performance of SkyGAN is evaluated on the recent SateHaze1k dataset as well as the HAI dataset. We also present a comprehensive evaluation of HAI dataset with a representative set of state-of-the-art techniques in terms of PSNR and SSIM.



### Deeply-Supervised Density Regression for Automatic Cell Counting in Microscopy Images
- **Arxiv ID**: http://arxiv.org/abs/2011.03683v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.03683v2)
- **Published**: 2020-11-07 04:02:47+00:00
- **Updated**: 2020-11-10 01:57:30+00:00
- **Authors**: Shenghua He, Kyaw Thu Minn, Lilianna Solnica-Krezel, Mark A. Anastasio, Hua Li
- **Comment**: Medical Image Analysis 2020
- **Journal**: None
- **Summary**: Accurately counting the number of cells in microscopy images is required in many medical diagnosis and biological studies. This task is tedious, time-consuming, and prone to subjective errors. However, designing automatic counting methods remains challenging due to low image contrast, complex background, large variance in cell shapes and counts, and significant cell occlusions in two-dimensional microscopy images. In this study, we proposed a new density regression-based method for automatically counting cells in microscopy images. The proposed method processes two innovations compared to other state-of-the-art density regression-based methods. First, the density regression model (DRM) is designed as a concatenated fully convolutional regression network (C-FCRN) to employ multi-scale image features for the estimation of cell density maps from given images. Second, auxiliary convolutional neural networks (AuxCNNs) are employed to assist in the training of intermediate layers of the designed C-FCRN to improve the DRM performance on unseen datasets. Experimental studies evaluated on four datasets demonstrate the superior performance of the proposed method.



### Data--driven Image Restoration with Option--driven Learning for Big and Small Astronomical Image Datasets
- **Arxiv ID**: http://arxiv.org/abs/2011.03696v1
- **DOI**: 10.1093/mnras/staa3535
- **Categories**: **astro-ph.IM**, astro-ph.GA, astro-ph.SR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.03696v1)
- **Published**: 2020-11-07 05:05:55+00:00
- **Updated**: 2020-11-07 05:05:55+00:00
- **Authors**: Peng Jia, Ruiyu Ning, Ruiqi Sun, Xiaoshan Yang, Dongmei Cai
- **Comment**: 11 pages. Submitted to MNRAS with minor revision
- **Journal**: None
- **Summary**: Image restoration methods are commonly used to improve the quality of astronomical images. In recent years, developments of deep neural networks and increments of the number of astronomical images have evoked a lot of data--driven image restoration methods. However, most of these methods belong to supervised learning algorithms, which require paired images either from real observations or simulated data as training set. For some applications, it is hard to get enough paired images from real observations and simulated images are quite different from real observed ones. In this paper, we propose a new data--driven image restoration method based on generative adversarial networks with option--driven learning. Our method uses several high resolution images as references and applies different learning strategies when the number of reference images is different. For sky surveys with variable observation conditions, our method can obtain very stable image restoration results, regardless of the number of reference images.



### Depthwise Multiception Convolution for Reducing Network Parameters without Sacrificing Accuracy
- **Arxiv ID**: http://arxiv.org/abs/2011.03701v1
- **DOI**: 10.1109/ICARCV50220.2020.9305369
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2011.03701v1)
- **Published**: 2020-11-07 05:33:54+00:00
- **Updated**: 2020-11-07 05:33:54+00:00
- **Authors**: Guoqing Bao, Manuel B. Graeber, Xiuying Wang
- **Comment**: This paper was accepted by ICARCV 2020
- **Journal**: None
- **Summary**: Deep convolutional neural networks have been proven successful in multiple benchmark challenges in recent years. However, the performance improvements are heavily reliant on increasingly complex network architecture and a high number of parameters, which require ever increasing amounts of storage and memory capacity. Depthwise separable convolution (DSConv) can effectively reduce the number of required parameters through decoupling standard convolution into spatial and cross-channel convolution steps. However, the method causes a degradation of accuracy. To address this problem, we present depthwise multiception convolution, termed Multiception, which introduces layer-wise multiscale kernels to learn multiscale representations of all individual input channels simultaneously. We have carried out the experiment on four benchmark datasets, i.e. Cifar-10, Cifar-100, STL-10 and ImageNet32x32, using five popular CNN models, Multiception achieved accuracy promotion in all models and demonstrated higher accuracy performance compared to related works. Meanwhile, Multiception significantly reduces the number of parameters of standard convolution-based models by 32.48% on average while still preserving accuracy.



### TB-Net: A Three-Stream Boundary-Aware Network for Fine-Grained Pavement Disease Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2011.03703v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.03703v1)
- **Published**: 2020-11-07 05:39:31+00:00
- **Updated**: 2020-11-07 05:39:31+00:00
- **Authors**: Yujia Zhang, Qianzhong Li, Xiaoguang Zhao, Min Tan
- **Comment**: WACV 2021
- **Journal**: None
- **Summary**: Regular pavement inspection plays a significant role in road maintenance for safety assurance. Existing methods mainly address the tasks of crack detection and segmentation that are only tailored for long-thin crack disease. However, there are many other types of diseases with a wider variety of sizes and patterns that are also essential to segment in practice, bringing more challenges towards fine-grained pavement inspection. In this paper, our goal is not only to automatically segment cracks, but also to segment other complex pavement diseases as well as typical landmarks (markings, runway lights, etc.) and commonly seen water/oil stains in a single model. To this end, we propose a three-stream boundary-aware network (TB-Net). It consists of three streams fusing the low-level spatial and the high-level contextual representations as well as the detailed boundary information. Specifically, the spatial stream captures rich spatial features. The context stream, where an attention mechanism is utilized, models the contextual relationships over local features. The boundary stream learns detailed boundaries using a global-gated convolution to further refine the segmentation outputs. The network is trained using a dual-task loss in an end-to-end manner, and experiments on a newly collected fine-grained pavement disease dataset show the effectiveness of our TB-Net.



### Blind Motion Deblurring through SinGAN Architecture
- **Arxiv ID**: http://arxiv.org/abs/2011.03705v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.03705v1)
- **Published**: 2020-11-07 06:09:16+00:00
- **Updated**: 2020-11-07 06:09:16+00:00
- **Authors**: Harshil Jain, Rohit Patil, Indra Deep Mastan, Shanmuganathan Raman
- **Comment**: Deep Internal Learning: Training with no prior examples. ECCV'2020
  Workshop
- **Journal**: None
- **Summary**: Blind motion deblurring involves reconstructing a sharp image from an observation that is blurry. It is a problem that is ill-posed and lies in the categories of image restoration problems. The training data-based methods for image deblurring mostly involve training models that take a lot of time. These models are data-hungry i.e., they require a lot of training data to generate satisfactory results. Recently, there are various image feature learning methods developed which relieve us of the need for training data and perform image restoration and image synthesis, e.g., DIP, InGAN, and SinGAN. SinGAN is a generative model that is unconditional and could be learned from a single natural image. This model primarily captures the internal distribution of the patches which are present in the image and is capable of generating samples of varied diversity while preserving the visual content of the image. Images generated from the model are very much like real natural images. In this paper, we focus on blind motion deblurring through SinGAN architecture.



### DeepCFL: Deep Contextual Features Learning from a Single Image
- **Arxiv ID**: http://arxiv.org/abs/2011.03712v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.03712v1)
- **Published**: 2020-11-07 06:54:59+00:00
- **Updated**: 2020-11-07 06:54:59+00:00
- **Authors**: Indra Deep Mastan, Shanmuganathan Raman
- **Comment**: IEEE Winter Conference on Applications of Computer Vision (WACV
  2021), Waikoloa, US, Jan. 5-9, 2021
- **Journal**: None
- **Summary**: Recently, there is a vast interest in developing image feature learning methods that are independent of the training data, such as deep image prior, InGAN, SinGAN, and DCIL. These methods are unsupervised and are used to perform low-level vision tasks such as image restoration, image editing, and image synthesis. In this work, we proposed a new training data-independent framework, called Deep Contextual Features Learning (DeepCFL), to perform image synthesis and image restoration based on the semantics of the input image. The contextual features are simply the high dimensional vectors representing the semantics of the given image. DeepCFL is a single image GAN framework that learns the distribution of the context vectors from the input image. We show the performance of contextual learning in various challenging scenarios: outpainting, inpainting, and restoration of randomly removed pixels. DeepCFL is applicable when the input source image and the generated target image are not aligned. We illustrate image synthesis using DeepCFL for the task of image resizing.



### Coarse- and Fine-grained Attention Network with Background-aware Loss for Crowd Density Map Estimation
- **Arxiv ID**: http://arxiv.org/abs/2011.03721v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2011.03721v1)
- **Published**: 2020-11-07 08:05:54+00:00
- **Updated**: 2020-11-07 08:05:54+00:00
- **Authors**: Liangzi Rong, Chunping Li
- **Comment**: Accepted by WACV 2020
- **Journal**: None
- **Summary**: In this paper, we present a novel method Coarse- and Fine-grained Attention Network (CFANet) for generating high-quality crowd density maps and people count estimation by incorporating attention maps to better focus on the crowd area. We devise a from-coarse-to-fine progressive attention mechanism by integrating Crowd Region Recognizer (CRR) and Density Level Estimator (DLE) branch, which can suppress the influence of irrelevant background and assign attention weights according to the crowd density levels, because generating accurate fine-grained attention maps directly is normally difficult. We also employ a multi-level supervision mechanism to assist the backpropagation of gradient and reduce overfitting. Besides, we propose a Background-aware Structural Loss (BSL) to reduce the false recognition ratio while improving the structural similarity to groundtruth. Extensive experiments on commonly used datasets show that our method can not only outperform previous state-of-the-art methods in terms of count accuracy but also improve the image quality of density maps as well as reduce the false recognition ratio.



### A Strong Baseline for Crowd Counting and Unsupervised People Localization
- **Arxiv ID**: http://arxiv.org/abs/2011.03725v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.03725v1)
- **Published**: 2020-11-07 08:29:03+00:00
- **Updated**: 2020-11-07 08:29:03+00:00
- **Authors**: Liangzi Rong, Chunping Li
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we explore a strong baseline for crowd counting and an unsupervised people localization algorithm based on estimated density maps. Firstly, existing methods achieve state-of-the-art performance based on different backbones and kinds of training tricks. We collect different backbones and training tricks and evaluate the impact of changing them and develop an efficient pipeline for crowd counting, which decreases MAE and RMSE significantly on multiple datasets. We also propose a clustering algorithm named isolated KMeans to locate the heads in density maps. This method can divide the density maps into subregions and find the centers under local count constraints without training any parameter and can be integrated with existing methods easily.



### Bridging the Performance Gap between FGSM and PGD Adversarial Training
- **Arxiv ID**: http://arxiv.org/abs/2011.05157v2
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.05157v2)
- **Published**: 2020-11-07 09:08:54+00:00
- **Updated**: 2022-10-03 17:55:19+00:00
- **Authors**: Tianjin Huang, Vlado Menkovski, Yulong Pei, Mykola Pechenizkiy
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning achieves state-of-the-art performance in many tasks but exposes to the underlying vulnerability against adversarial examples. Across existing defense techniques, adversarial training with the projected gradient decent attack (adv.PGD) is considered as one of the most effective ways to achieve moderate adversarial robustness. However, adv.PGD requires too much training time since the projected gradient attack (PGD) takes multiple iterations to generate perturbations. On the other hand, adversarial training with the fast gradient sign method (adv.FGSM) takes much less training time since the fast gradient sign method (FGSM) takes one step to generate perturbations but fails to increase adversarial robustness. In this work, we extend adv.FGSM to make it achieve the adversarial robustness of adv.PGD. We demonstrate that the large curvature along FGSM perturbed direction leads to a large difference in performance of adversarial robustness between adv.FGSM and adv.PGD, and therefore propose combining adv.FGSM with a curvature regularization (adv.FGSMR) in order to bridge the performance gap between adv.FGSM and adv.PGD. The experiments show that adv.FGSMR has higher training efficiency than adv.PGD. In addition, it achieves comparable performance of adversarial robustness on MNIST dataset under white-box attack, and it achieves better performance than adv.PGD under white-box attack and effectively defends the transferable adversarial attack on CIFAR-10 dataset.



### Interventional Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2011.03737v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2011.03737v1)
- **Published**: 2020-11-07 09:53:13+00:00
- **Updated**: 2020-11-07 09:53:13+00:00
- **Authors**: Jun Wen, Changjian Shui, Kun Kuang, Junsong Yuan, Zenan Huang, Zhefeng Gong, Nenggan Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: Domain adaptation (DA) aims to transfer discriminative features learned from source domain to target domain. Most of DA methods focus on enhancing feature transferability through domain-invariance learning. However, source-learned discriminability itself might be tailored to be biased and unsafely transferable by spurious correlations, \emph{i.e.}, part of source-specific features are correlated with category labels. We find that standard domain-invariance learning suffers from such correlations and incorrectly transfers the source-specifics. To address this issue, we intervene in the learning of feature discriminability using unlabeled target data to guide it to get rid of the domain-specific part and be safely transferable. Concretely, we generate counterfactual features that distinguish the domain-specifics from domain-sharable part through a novel feature intervention strategy. To prevent the residence of domain-specifics, the feature discriminability is trained to be invariant to the mutations in the domain-specifics of counterfactual features. Experimenting on typical \emph{one-to-one} unsupervised domain adaptation and challenging domain-agnostic adaptation tasks, the consistent performance improvements of our method over state-of-the-art approaches validate that the learned discriminative features are more safely transferable and generalize well to novel domains.



### Robustness and Diversity Seeking Data-Free Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/2011.03749v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.SY, eess.SY
- **Links**: [PDF](http://arxiv.org/pdf/2011.03749v3)
- **Published**: 2020-11-07 10:57:53+00:00
- **Updated**: 2021-02-10 09:47:13+00:00
- **Authors**: Pengchao Han, Jihong Park, Shiqiang Wang, Yejun Liu
- **Comment**: Accepted in IEEE ICASSP 2021
- **Journal**: None
- **Summary**: Knowledge distillation (KD) has enabled remarkable progress in model compression and knowledge transfer. However, KD requires a large volume of original data or their representation statistics that are not usually available in practice. Data-free KD has recently been proposed to resolve this problem, wherein teacher and student models are fed by a synthetic sample generator trained from the teacher. Nonetheless, existing data-free KD methods rely on fine-tuning of weights to balance multiple losses, and ignore the diversity of generated samples, resulting in limited accuracy and robustness. To overcome this challenge, we propose robustness and diversity seeking data-free KD (RDSKD) in this paper. The generator loss function is crafted to produce samples with high authenticity, class diversity, and inter-sample diversity. Without real data, the objectives of seeking high sample authenticity and class diversity often conflict with each other, causing frequent loss fluctuations. We mitigate this by exponentially penalizing loss increments. With MNIST, CIFAR-10, and SVHN datasets, our experiments show that RDSKD achieves higher accuracy with more robustness over different hyperparameter settings, compared to other data-free KD methods such as DAFL, MSKD, ZSKD, and DeepInversion.



### A Multi-stream Convolutional Neural Network for Micro-expression Recognition Using Optical Flow and EVM
- **Arxiv ID**: http://arxiv.org/abs/2011.03756v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2011.03756v2)
- **Published**: 2020-11-07 11:28:53+00:00
- **Updated**: 2020-11-10 10:34:27+00:00
- **Authors**: Jinming Liu, Ke Li, Baolin Song, Li Zhao
- **Comment**: ICEIC 2020, Barcelona, Spain, January 2020
- **Journal**: None
- **Summary**: Micro-expression (ME) recognition plays a crucial role in a wide range of applications, particularly in public security and psychotherapy. Recently, traditional methods rely excessively on machine learning design and the recognition rate is not high enough for its practical application because of its short duration and low intensity. On the other hand, some methods based on deep learning also cannot get high accuracy due to problems such as the imbalance of databases. To address these problems, we design a multi-stream convolutional neural network (MSCNN) for ME recognition in this paper. Specifically, we employ EVM and optical flow to magnify and visualize subtle movement changes in MEs and extract the masks from the optical flow images. And then, we add the masks, optical flow images, and grayscale images into the MSCNN. After that, in order to overcome the imbalance of databases, we added a random over-sampler after the Dense Layer of the neural network. Finally, extensive experiments are conducted on two public ME databases: CASME II and SAMM. Compared with many recent state-of-the-art approaches, our method achieves more promising recognition results.



### Automated Grading System of Retinal Arterio-venous Crossing Patterns: A Deep Learning Approach Replicating Ophthalmologist's Diagnostic Process of Arteriolosclerosis
- **Arxiv ID**: http://arxiv.org/abs/2011.03772v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.03772v2)
- **Published**: 2020-11-07 13:15:17+00:00
- **Updated**: 2022-12-01 14:04:37+00:00
- **Authors**: Liangzhi Li, Manisha Verma, Bowen Wang, Yuta Nakashima, Hajime Nagahara, Ryo Kawasaki
- **Comment**: Accepted in PLOS Digital Health
- **Journal**: None
- **Summary**: The status of retinal arteriovenous crossing is of great significance for clinical evaluation of arteriolosclerosis and systemic hypertension. As an ophthalmology diagnostic criteria, Scheie's classification has been used to grade the severity of arteriolosclerosis. In this paper, we propose a deep learning approach to support the diagnosis process, which, to the best of our knowledge, is one of the earliest attempts in medical imaging. The proposed pipeline is three-fold. First, we adopt segmentation and classification models to automatically obtain vessels in a retinal image with the corresponding artery/vein labels and find candidate arteriovenous crossing points. Second, we use a classification model to validate the true crossing point. At last, the grade of severity for the vessel crossings is classified. To better address the problem of label ambiguity and imbalanced label distribution, we propose a new model, named multi-diagnosis team network (MDTNet), in which the sub-models with different structures or different loss functions provide different decisions. MDTNet unifies these diverse theories to give the final decision with high accuracy. Our severity grading method was able to validate crossing points with precision and recall of 96.3% and 96.3%, respectively. Among correctly detected crossing points, the kappa value for the agreement between the grading by a retina specialist and the estimated score was 0.85, with an accuracy of 0.92. The numerical results demonstrate that our method can achieve a good performance in both arteriovenous crossing validation and severity grading tasks. By the proposed models, we could build a pipeline reproducing retina specialist's subjective grading without feature extractions. The code is available for reproducibility.



### Text-to-Image Generation Grounded by Fine-Grained User Attention
- **Arxiv ID**: http://arxiv.org/abs/2011.03775v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2011.03775v2)
- **Published**: 2020-11-07 13:23:31+00:00
- **Updated**: 2021-03-30 19:52:34+00:00
- **Authors**: Jing Yu Koh, Jason Baldridge, Honglak Lee, Yinfei Yang
- **Comment**: To appear in WACV 2021
- **Journal**: None
- **Summary**: Localized Narratives is a dataset with detailed natural language descriptions of images paired with mouse traces that provide a sparse, fine-grained visual grounding for phrases. We propose TReCS, a sequential model that exploits this grounding to generate images. TReCS uses descriptions to retrieve segmentation masks and predict object labels aligned with mouse traces. These alignments are used to select and position masks to generate a fully covered segmentation canvas; the final image is produced by a segmentation-to-image generator using this canvas. This multi-step, retrieval-based approach outperforms existing direct text-to-image generation models on both automatic metrics and human evaluations: overall, its generated images are more photo-realistic and better match descriptions.



### Multi-regime analysis for computer vision-based traffic surveillance using a change-point detection algorithm
- **Arxiv ID**: http://arxiv.org/abs/2011.11758v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.SY, eess.SY
- **Links**: [PDF](http://arxiv.org/pdf/2011.11758v1)
- **Published**: 2020-11-07 13:54:36+00:00
- **Updated**: 2020-11-07 13:54:36+00:00
- **Authors**: Seungyun Jeong, Keemin Sohn
- **Comment**: None
- **Journal**: None
- **Summary**: As a result of significant advances in deep learning, computer vision technology has been widely adopted in the field of traffic surveillance. Nonetheless, it is difficult to find a universal model that can measure traffic parameters irrespective of ambient conditions such as times of the day, weather, or shadows. These conditions vary recurrently, but the exact points of change are inconsistent and unpredictable. Thus, the application of a multi-regime method would be problematic, even when separate sets of model parameters are prepared in advance. In the present study we devised a robust approach that facilitates multi-regime analysis. This approach employs an online parametric algorithm to determine the change-points for ambient conditions. An autoencoder was used to reduce the dimensions of input images, and reduced feature vectors were used to implement the online change-point algorithm. Seven separate periods were tagged with typical times in a given day. Multi-regime analysis was then performed so that the traffic density could be separately measured for each period. To train and test models for vehicle counting, 1,100 video images were randomly chosen for each period and labeled with traffic counts. The measurement accuracy of multi-regime analysis was much higher than that of an integrated model trained on all data.



### Rapid Pose Label Generation through Sparse Representation of Unknown Objects
- **Arxiv ID**: http://arxiv.org/abs/2011.03790v1
- **DOI**: 10.1109/ICRA48506.2021.9561277
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.03790v1)
- **Published**: 2020-11-07 15:14:03+00:00
- **Updated**: 2020-11-07 15:14:03+00:00
- **Authors**: Rohan Pratap Singh, Mehdi Benallegue, Yusuke Yoshiyasu, Fumio Kanehiro
- **Comment**: None
- **Journal**: 2021 IEEE International Conference on Robotics and Automation
  (ICRA)
- **Summary**: Deep Convolutional Neural Networks (CNNs) have been successfully deployed on robots for 6-DoF object pose estimation through visual perception. However, obtaining labeled data on a scale required for the supervised training of CNNs is a difficult task - exacerbated if the object is novel and a 3D model is unavailable. To this end, this work presents an approach for rapidly generating real-world, pose-annotated RGB-D data for unknown objects. Our method not only circumvents the need for a prior 3D object model (textured or otherwise) but also bypasses complicated setups of fiducial markers, turntables, and sensors. With the help of a human user, we first source minimalistic labelings of an ordered set of arbitrarily chosen keypoints over a set of RGB-D videos. Then, by solving an optimization problem, we combine these labels under a world frame to recover a sparse, keypoint-based representation of the object. The sparse representation leads to the development of a dense model and the pose labels for each image frame in the set of scenes. We show that the sparse model can also be efficiently used for scaling to a large number of new scenes. We demonstrate the practicality of the generated labeled dataset by training a pipeline for 6-DoF object pose estimation and a pixel-wise segmentation network.



### Deep Learning Analysis and Age Prediction from Shoeprints
- **Arxiv ID**: http://arxiv.org/abs/2011.03794v2
- **DOI**: 10.1016/j.forsciint.2021.110987
- **Categories**: **cs.CV**, 68T07, I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2011.03794v2)
- **Published**: 2020-11-07 15:36:11+00:00
- **Updated**: 2020-11-23 16:37:22+00:00
- **Authors**: Muhammad Hassan, Yan Wang, Di Wang, Daixi Li, Yanchun Liang, You Zhou, Dong Xu
- **Comment**: 24 pages, 20 Figures
- **Journal**: None
- **Summary**: Human walking and gaits involve several complex body parts and are influenced by personality, mood, social and cultural traits, and aging. These factors are reflected in shoeprints, which in turn can be used to predict age, a problem not systematically addressed using any computational approach. We collected 100,000 shoeprints of subjects ranging from 7 to 80 years old and used the data to develop a deep learning end-to-end model ShoeNet to analyze age-related patterns and predict age. The model integrates various convolutional neural network models together using a skip mechanism to extract age-related features, especially in pressure and abrasion regions from pair-wise shoeprints. The results show that 40.23% of the subjects had prediction errors within 5-years of age and the prediction accuracy for gender classification reached 86.07%. Interestingly, the age-related features mostly reside in the asymmetric differences between left and right shoeprints. The analysis also reveals interesting age-related and gender-related patterns in the pressure distributions on shoeprints; in particular, the pressure forces spread from the middle of the toe toward outside regions over age with gender-specific variations on heel regions. Such statistics provide insight into new methods for forensic investigations, medical studies of gait-pattern disorders, biometrics, and sport studies.



### Multiscale Point Cloud Geometry Compression
- **Arxiv ID**: http://arxiv.org/abs/2011.03799v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.03799v1)
- **Published**: 2020-11-07 16:11:16+00:00
- **Updated**: 2020-11-07 16:11:16+00:00
- **Authors**: Jianqiang Wang, Dandan Ding, Zhu Li, Zhan Ma
- **Comment**: None
- **Journal**: None
- **Summary**: Recent years have witnessed the growth of point cloud based applications because of its realistic and fine-grained representation of 3D objects and scenes. However, it is a challenging problem to compress sparse, unstructured, and high-precision 3D points for efficient communication. In this paper, leveraging the sparsity nature of point cloud, we propose a multiscale end-to-end learning framework which hierarchically reconstructs the 3D Point Cloud Geometry (PCG) via progressive re-sampling. The framework is developed on top of a sparse convolution based autoencoder for point cloud compression and reconstruction. For the input PCG which has only the binary occupancy attribute, our framework translates it to a downscaled point cloud at the bottleneck layer which possesses both geometry and associated feature attributes. Then, the geometric occupancy is losslessly compressed using an octree codec and the feature attributes are lossy compressed using a learned probabilistic context model.Compared to state-of-the-art Video-based Point Cloud Compression (V-PCC) and Geometry-based PCC (G-PCC) schemes standardized by the Moving Picture Experts Group (MPEG), our method achieves more than 40% and 70% BD-Rate (Bjontegaard Delta Rate) reduction, respectively. Its encoding runtime is comparable to that of G-PCC, which is only 1.5% of V-PCC.



### Symmetric Parallax Attention for Stereo Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2011.03802v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.03802v2)
- **Published**: 2020-11-07 16:28:35+00:00
- **Updated**: 2021-04-20 07:37:52+00:00
- **Authors**: Yingqian Wang, Xinyi Ying, Longguang Wang, Jungang Yang, Wei An, Yulan Guo
- **Comment**: Accepted to NTIRE workshop at CVPR 2021. The first two authors
  contribute equally to this work
- **Journal**: None
- **Summary**: Although recent years have witnessed the great advances in stereo image super-resolution (SR), the beneficial information provided by binocular systems has not been fully used. Since stereo images are highly symmetric under epipolar constraint, in this paper, we improve the performance of stereo image SR by exploiting symmetry cues in stereo image pairs. Specifically, we propose a symmetric bi-directional parallax attention module (biPAM) and an inline occlusion handling scheme to effectively interact cross-view information. Then, we design a Siamese network equipped with a biPAM to super-resolve both sides of views in a highly symmetric manner. Finally, we design several illuminance-robust losses to enhance stereo consistency. Experiments on four public datasets demonstrate the superior performance of our method. Source code is available at https://github.com/YingqianWang/iPASSR.



### Sim-to-Real Transfer for Vision-and-Language Navigation
- **Arxiv ID**: http://arxiv.org/abs/2011.03807v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2011.03807v1)
- **Published**: 2020-11-07 16:49:04+00:00
- **Updated**: 2020-11-07 16:49:04+00:00
- **Authors**: Peter Anderson, Ayush Shrivastava, Joanne Truong, Arjun Majumdar, Devi Parikh, Dhruv Batra, Stefan Lee
- **Comment**: CoRL 2020
- **Journal**: None
- **Summary**: We study the challenging problem of releasing a robot in a previously unseen environment, and having it follow unconstrained natural language navigation instructions. Recent work on the task of Vision-and-Language Navigation (VLN) has achieved significant progress in simulation. To assess the implications of this work for robotics, we transfer a VLN agent trained in simulation to a physical robot. To bridge the gap between the high-level discrete action space learned by the VLN agent, and the robot's low-level continuous action space, we propose a subgoal model to identify nearby waypoints, and use domain randomization to mitigate visual domain differences. For accurate sim and real comparisons in parallel environments, we annotate a 325m2 office space with 1.3km of navigation instructions, and create a digitized replica in simulation. We find that sim-to-real transfer to an environment not seen in training is successful if an occupancy map and navigation graph can be collected and annotated in advance (success rate of 46.8% vs. 55.9% in sim), but much more challenging in the hardest setting with no prior mapping at all (success rate of 22.5%).



### Towards Resolving the Challenge of Long-tail Distribution in UAV Images for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2011.03822v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2011.03822v1)
- **Published**: 2020-11-07 17:53:12+00:00
- **Updated**: 2020-11-07 17:53:12+00:00
- **Authors**: Weiping Yu, Taojiannan Yang, Chen Chen
- **Comment**: WACV 2021
- **Journal**: None
- **Summary**: Existing methods for object detection in UAV images ignored an important challenge - imbalanced class distribution in UAV images - which leads to poor performance on tail classes. We systematically investigate existing solutions to long-tail problems and unveil that re-balancing methods that are effective on natural image datasets cannot be trivially applied to UAV datasets. To this end, we rethink long-tailed object detection in UAV images and propose the Dual Sampler and Head detection Network (DSHNet), which is the first work that aims to resolve long-tail distribution in UAV images. The key components in DSHNet include Class-Biased Samplers (CBS) and Bilateral Box Heads (BBH), which are developed to cope with tail classes and head classes in a dual-path manner. Without bells and whistles, DSHNet significantly boosts the performance of tail classes on different detection frameworks. Moreover, DSHNet significantly outperforms base detectors and generic approaches for long-tail problems on VisDrone and UAVDT datasets. It achieves new state-of-the-art performance when combining with image cropping methods. Code is available at https://github.com/we1pingyu/DSHNet



### On the spatial attention in Spatio-Temporal Graph Convolutional Networks for skeleton-based human action recognition
- **Arxiv ID**: http://arxiv.org/abs/2011.03833v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.03833v2)
- **Published**: 2020-11-07 19:03:04+00:00
- **Updated**: 2021-04-22 17:53:15+00:00
- **Authors**: Negar Heidari, Alexandros Iosifidis
- **Comment**: Accepted by the 2021 International Joint Conference on Neural
  Networks (IJCNN 2021)
- **Journal**: None
- **Summary**: Graph convolutional networks (GCNs) achieved promising performance in skeleton-based human action recognition by modeling a sequence of skeletons as a spatio-temporal graph. Most of the recently proposed GCN-based methods improve the performance by learning the graph structure at each layer of the network using a spatial attention applied on a predefined graph Adjacency matrix that is optimized jointly with model's parameters in an end-to-end manner. In this paper, we analyze the spatial attention used in spatio-temporal GCN layers and propose a symmetric spatial attention for better reflecting the symmetric property of the relative positions of the human body joints when executing actions. We also highlight the connection of spatio-temporal GCN layers employing additive spatial attention to bilinear layers, and we propose the spatio-temporal bilinear network (ST-BLN) which does not require the use of predefined Adjacency matrices and allows for more flexible design of the model. Experimental results show that the three models lead to effectively the same performance. Moreover, by exploiting the flexibility provided by the proposed ST-BLN, one can increase the efficiency of the model.



### Autonomous Intruder Detection Using a ROS-Based Multi-Robot System Equipped with 2D-LiDAR Sensors
- **Arxiv ID**: http://arxiv.org/abs/2011.03838v1
- **DOI**: 10.1109/SSRR50563.2020.9292639
- **Categories**: **cs.RO**, cs.CV, cs.MA, 68T45, 65D19, I.2.9; I.2.10; I.2.11
- **Links**: [PDF](http://arxiv.org/pdf/2011.03838v1)
- **Published**: 2020-11-07 19:49:07+00:00
- **Updated**: 2020-11-07 19:49:07+00:00
- **Authors**: Mashnoon Islam, Touhid Ahmed, Abu Tammam Bin Nuruddin, Mashuda Islam, Shahnewaz Siddique
- **Comment**: Accepted Version, 2020 IEEE International Symposium on Safety,
  Security, and Rescue Robotics (SSRR) November 4-6, Abu Dhabi, UAE
- **Journal**: None
- **Summary**: The application of autonomous mobile robots in robotic security platforms is becoming a promising field of innovation due to their adaptive capability of responding to potential disturbances perceived through a wide range of sensors. Researchers have proposed systems that either focus on utilizing a single mobile robot or a system of cooperative multiple robots. However, very few of the proposed works, particularly in the field of multi-robot systems, are completely dependent on LiDAR sensors for achieving various tasks. This is essential when other sensors on a robot fail to provide peak performance in particular conditions, such as a camera operating in the absence of light. This paper proposes a multi-robot system that is developed using ROS (Robot Operating System) for intruder detection in a single-range-sensor-per-robot scenario with centralized processing of detections from all robots by our central bot MIDNet (Multiple Intruder Detection Network). This work is aimed at providing an autonomous multi-robot security solution for a warehouse in the absence of human personnel.



### Deep traffic light detection by overlaying synthetic context on arbitrary natural images
- **Arxiv ID**: http://arxiv.org/abs/2011.03841v3
- **DOI**: 10.1016/j.cag.2020.09.012
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.03841v3)
- **Published**: 2020-11-07 19:57:22+00:00
- **Updated**: 2020-12-10 22:44:41+00:00
- **Authors**: Jean Pablo Vieira de Mello, Lucas Tabelini, Rodrigo F. Berriel, Thiago M. Paixão, Alberto F. de Souza, Claudine Badue, Nicu Sebe, Thiago Oliveira-Santos
- **Comment**: None
- **Journal**: Computers & Graphics (2020)
- **Summary**: Deep neural networks come as an effective solution to many problems associated with autonomous driving. By providing real image samples with traffic context to the network, the model learns to detect and classify elements of interest, such as pedestrians, traffic signs, and traffic lights. However, acquiring and annotating real data can be extremely costly in terms of time and effort. In this context, we propose a method to generate artificial traffic-related training data for deep traffic light detectors. This data is generated using basic non-realistic computer graphics to blend fake traffic scenes on top of arbitrary image backgrounds that are not related to the traffic domain. Thus, a large amount of training data can be generated without annotation efforts. Furthermore, it also tackles the intrinsic data imbalance problem in traffic light datasets, caused mainly by the low amount of samples of the yellow state. Experiments show that it is possible to achieve results comparable to those obtained with real training data from the problem domain, yielding an average mAP and an average F1-score which are each nearly 4 p.p. higher than the respective metrics obtained with a real-world reference model.



### Learning to Model and Ignore Dataset Bias with Mixed Capacity Ensembles
- **Arxiv ID**: http://arxiv.org/abs/2011.03856v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.03856v1)
- **Published**: 2020-11-07 22:20:03+00:00
- **Updated**: 2020-11-07 22:20:03+00:00
- **Authors**: Christopher Clark, Mark Yatskar, Luke Zettlemoyer
- **Comment**: In EMNLP Findings
- **Journal**: None
- **Summary**: Many datasets have been shown to contain incidental correlations created by idiosyncrasies in the data collection process. For example, sentence entailment datasets can have spurious word-class correlations if nearly all contradiction sentences contain the word "not", and image recognition datasets can have tell-tale object-background correlations if dogs are always indoors. In this paper, we propose a method that can automatically detect and ignore these kinds of dataset-specific patterns, which we call dataset biases. Our method trains a lower capacity model in an ensemble with a higher capacity model. During training, the lower capacity model learns to capture relatively shallow correlations, which we hypothesize are likely to reflect dataset bias. This frees the higher capacity model to focus on patterns that should generalize better. We ensure the models learn non-overlapping approaches by introducing a novel method to make them conditionally independent. Importantly, our approach does not require the bias to be known in advance. We evaluate performance on synthetic datasets, and four datasets built to penalize models that exploit known biases on textual entailment, visual question answering, and image recognition tasks. We show improvement in all settings, including a 10 point gain on the visual question answering dataset.



### Latent Neural Differential Equations for Video Generation
- **Arxiv ID**: http://arxiv.org/abs/2011.03864v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.03864v3)
- **Published**: 2020-11-07 23:08:29+00:00
- **Updated**: 2021-05-11 05:31:13+00:00
- **Authors**: Cade Gordon, Natalie Parde
- **Comment**: None
- **Journal**: None
- **Summary**: Generative Adversarial Networks have recently shown promise for video generation, building off of the success of image generation while also addressing a new challenge: time. Although time was analyzed in some early work, the literature has not adequately grown with temporal modeling developments. We study the effects of Neural Differential Equations to model the temporal dynamics of video generation. The paradigm of Neural Differential Equations presents many theoretical strengths including the first continuous representation of time within video generation. In order to address the effects of Neural Differential Equations, we investigate how changes in temporal models affect generated video quality. Our results give support to the usage of Neural Differential Equations as a simple replacement for older temporal generators. While keeping run times similar and decreasing parameter count, we produce a new state-of-the-art model in 64$\times$64 pixel unconditional video generation, with an Inception Score of 15.20.



