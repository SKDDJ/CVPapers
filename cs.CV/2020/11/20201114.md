# Arxiv Papers in cs.CV on 2020-11-14
### Texture image classification based on a pseudo-parabolic diffusion model
- **Arxiv ID**: http://arxiv.org/abs/2011.07173v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.NA, math.NA
- **Links**: [PDF](http://arxiv.org/pdf/2011.07173v2)
- **Published**: 2020-11-14 00:04:07+00:00
- **Updated**: 2021-01-24 00:39:00+00:00
- **Authors**: Jardel Vieira, Eduardo Abreu, Joao B. Florindo
- **Comment**: None
- **Journal**: None
- **Summary**: This work proposes a novel method based on a pseudo-parabolic diffusion process to be employed for texture recognition. The proposed operator is applied over a range of time scales giving rise to a family of images transformed by nonlinear filters. Therefore each of those images are encoded by a local descriptor (we use local binary patterns for that purpose) and they are summarized by a simple histogram, yielding in this way the image feature vector. The proposed approach is tested on the classification of well established benchmark texture databases and on a practical task of plant species recognition. In both cases, it is compared with several state-of-the-art methodologies employed for texture recognition. Our proposal outperforms those methods in terms of classification accuracy, confirming its competitiveness. The good performance can be justified to a large extent by the ability of the pseudo-parabolic operator to smooth possibly noisy details inside homogeneous regions of the image at the same time that it preserves discontinuities that convey critical information for the object description. Such results also confirm that model-based approaches like the proposed one can still be competitive with the omnipresent learning-based approaches, especially when the user does not have access to a powerful computational structure and a large amount of labeled data for training.



### A needle-based deep-neural-network camera
- **Arxiv ID**: http://arxiv.org/abs/2011.07184v1
- **DOI**: 10.1364/AO.415059
- **Categories**: **eess.IV**, cs.CV, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/2011.07184v1)
- **Published**: 2020-11-14 01:39:14+00:00
- **Updated**: 2020-11-14 01:39:14+00:00
- **Authors**: Ruipeng Guo, Soren Nelson, Rajesh Menon
- **Comment**: None
- **Journal**: None
- **Summary**: We experimentally demonstrate a camera whose primary optic is a cannula (diameter=0.22mm and length=12.5mm) that acts a lightpipe transporting light intensity from an object plane (35cm away) to its opposite end. Deep neural networks (DNNs) are used to reconstruct color and grayscale images with field of view of 180 and angular resolution of ~0.40. When trained on images with depth information, the DNN can create depth maps. Finally, we show DNN-based classification of the EMNIST dataset without and with image reconstructions. The former could be useful for imaging with enhanced privacy.



### Duality-Gated Mutual Condition Network for RGBT Tracking
- **Arxiv ID**: http://arxiv.org/abs/2011.07188v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.07188v3)
- **Published**: 2020-11-14 01:48:36+00:00
- **Updated**: 2022-04-29 14:32:42+00:00
- **Authors**: Andong Lu, Cun Qian, Chenglong Li, Jin Tang, Liang Wang
- **Comment**: Accepted at TNNLS-2022
- **Journal**: None
- **Summary**: Low-quality modalities contain not only a lot of noisy information but also some discriminative features in RGBT tracking. However, the potentials of low-quality modalities are not well explored in existing RGBT tracking algorithms. In this work, we propose a novel duality-gated mutual condition network to fully exploit the discriminative information of all modalities while suppressing the effects of data noise. In specific, we design a mutual condition module, which takes the discriminative information of a modality as the condition to guide feature learning of target appearance in another modality. Such module can effectively enhance target representations of all modalities even in the presence of low-quality modalities. To improve the quality of conditions and further reduce data noise, we propose a duality-gated mechanism and integrate it into the mutual condition module. To deal with the tracking failure caused by sudden camera motion, which often occurs in RGBT tracking, we design a resampling strategy based on optical flow algorithms. It does not increase much computational cost since we perform optical flow calculation only when the model prediction is unreliable and then execute resampling when the sudden camera motion is detected. Extensive experiments on four RGBT tracking benchmark datasets show that our method performs favorably against the state-of-the-art tracking algorithms



### RGBT Tracking via Multi-Adapter Network with Hierarchical Divergence Loss
- **Arxiv ID**: http://arxiv.org/abs/2011.07189v3
- **DOI**: 10.1109/TIP.2021.3087341
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.07189v3)
- **Published**: 2020-11-14 01:50:46+00:00
- **Updated**: 2021-06-04 06:53:08+00:00
- **Authors**: Andong Lu, Chenglong Li, Yuqing Yan, Jin Tang, Bin Luo
- **Comment**: Accepted by IEEE TIP
- **Journal**: None
- **Summary**: RGBT tracking has attracted increasing attention since RGB and thermal infrared data have strong complementary advantages, which could make trackers all-day and all-weather work. However, how to effectively represent RGBT data for visual tracking remains unstudied well. Existing works usually focus on extracting modality-shared or modality-specific information, but the potentials of these two cues are not well explored and exploited in RGBT tracking. In this paper, we propose a novel multi-adapter network to jointly perform modality-shared, modality-specific and instance-aware target representation learning for RGBT tracking. To this end, we design three kinds of adapters within an end-to-end deep learning framework. In specific, we use the modified VGG-M as the generality adapter to extract the modality-shared target representations.To extract the modality-specific features while reducing the computational complexity, we design a modality adapter, which adds a small block to the generality adapter in each layer and each modality in a parallel manner. Such a design could learn multilevel modality-specific representations with a modest number of parameters as the vast majority of parameters are shared with the generality adapter. We also design instance adapter to capture the appearance properties and temporal variations of a certain target. Moreover, to enhance the shared and specific features, we employ the loss of multiple kernel maximum mean discrepancy to measure the distribution divergence of different modal features and integrate it into each layer for more robust representation learning. Extensive experiments on two RGBT tracking benchmark datasets demonstrate the outstanding performance of the proposed tracker against the state-of-the-art methods.



### Existence of Two View Chiral Reconstructions
- **Arxiv ID**: http://arxiv.org/abs/2011.07197v3
- **DOI**: None
- **Categories**: **cs.CV**, math.AG
- **Links**: [PDF](http://arxiv.org/pdf/2011.07197v3)
- **Published**: 2020-11-14 02:27:20+00:00
- **Updated**: 2021-12-04 01:57:20+00:00
- **Authors**: Andrew Pryhuber, Rainer Sinn, Rekha R. Thomas
- **Comment**: None
- **Journal**: None
- **Summary**: A fundamental question in computer vision is whether a set of point pairs is the image of a scene that lies in front of two cameras. Such a scene and the cameras together are known as a chiral reconstruction of the point pairs. In this paper we provide a complete classification of k point pairs for which a chiral reconstruction exists. The existence of chiral reconstructions is equivalent to the non-emptiness of certain semialgebraic sets. For up to three point pairs, we prove that a chiral reconstruction always exists while the set of five or more point pairs that do not have a chiral reconstruction is Zariski-dense. We show that for five generic point pairs, the chiral region is bounded by line segments in a Schl\"afli double six on a cubic surface with 27 real lines. Four point pairs have a chiral reconstruction unless they belong to two non-generic combinatorial types, in which case they may or may not.



### Bi-Dimensional Feature Alignment for Cross-Domain Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2011.07205v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.07205v1)
- **Published**: 2020-11-14 03:03:11+00:00
- **Updated**: 2020-11-14 03:03:11+00:00
- **Authors**: Zhen Zhao, Yuhong Guo, Jieping Ye
- **Comment**: ECCV20 TASK-CV Workshop
- **Journal**: None
- **Summary**: Recently the problem of cross-domain object detection has started drawing attention in the computer vision community. In this paper, we propose a novel unsupervised cross-domain detection model that exploits the annotated data in a source domain to train an object detector for a different target domain. The proposed model mitigates the cross-domain representation divergence for object detection by performing cross-domain feature alignment in two dimensions, the depth dimension and the spatial dimension. In the depth dimension of channel layers, it uses inter-channel information to bridge the domain divergence with respect to image style alignment. In the dimension of spatial layers, it deploys spatial attention modules to enhance detection relevant regions and suppress irrelevant regions with respect to cross-domain feature alignment. Experiments are conducted on a number of benchmark cross-domain detection datasets. The empirical results show the proposed method outperforms the state-of-the-art comparison methods.



### Lung Segmentation in Chest X-rays with Res-CR-Net
- **Arxiv ID**: http://arxiv.org/abs/2011.08655v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, 68Txx (Primary), 68T07 (Secondary), 92B20, 68T45, I.2; I.4; J.3
- **Links**: [PDF](http://arxiv.org/pdf/2011.08655v1)
- **Published**: 2020-11-14 04:06:28+00:00
- **Updated**: 2020-11-14 04:06:28+00:00
- **Authors**: Haikal Abdulah, Benjamin Huber, Sinan Lal, Hassan Abdallah, Hamid Soltanian-Zadeh, Domenico L. Gatti
- **Comment**: 8 pages, 5 figures
- **Journal**: None
- **Summary**: Deep Neural Networks (DNN) are widely used to carry out segmentation tasks in biomedical images. Most DNNs developed for this purpose are based on some variation of the encoder-decoder U-Net architecture. Here we show that Res-CR-Net, a new type of fully convolutional neural network, which was originally developed for the semantic segmentation of microscopy images, and which does not adopt a U-Net architecture, is very effective at segmenting the lung fields in chest X-rays from either healthy patients or patients with a variety of lung pathologies.



### G-RCN: Optimizing the Gap between Classification and Localization Tasks for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2012.03677v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.03677v1)
- **Published**: 2020-11-14 04:14:01+00:00
- **Updated**: 2020-11-14 04:14:01+00:00
- **Authors**: Yufan Luo, Li Xiao
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-task learning is widely used in computer vision. Currently, object detection models utilize shared feature map to complete classification and localization tasks simultaneously. By comparing the performance between the original Faster R-CNN and that with partially separated feature maps, we show that: (1) Sharing high-level features for the classification and localization tasks is sub-optimal; (2) Large stride is beneficial for classification but harmful for localization; (3) Global context information could improve the performance of classification. Based on these findings, we proposed a paradigm called Gap-optimized region based convolutional network (G-RCN), which aims to separating these two tasks and optimizing the gap between them. The paradigm was firstly applied to correct the current ResNet protocol by simply reducing the stride and moving the Conv5 block from the head to the feature extraction network, which brings 3.6 improvement of AP70 on the PASCAL VOC dataset and 1.5 improvement of AP on the COCO dataset for ResNet50. Next, the new method is applied on the Faster R-CNN with backbone of VGG16,ResNet50 and ResNet101, which brings above 2.0 improvement of AP70 on the PASCAL VOC dataset and above 1.9 improvement of AP on the COCO dataset. Noticeably, the implementation of G-RCN only involves a few structural modifications, with no extra module added.



### Deep Interpretable Classification and Weakly-Supervised Segmentation of Histology Images via Max-Min Uncertainty
- **Arxiv ID**: http://arxiv.org/abs/2011.07221v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.07221v3)
- **Published**: 2020-11-14 04:45:07+00:00
- **Updated**: 2021-10-09 00:59:44+00:00
- **Authors**: Soufiane Belharbi, Jérôme Rony, Jose Dolz, Ismail Ben Ayed, Luke McCaffrey, Eric Granger
- **Comment**: 16 pages, 15 figures, under review
- **Journal**: None
- **Summary**: Weakly-supervised learning (WSL) has recently triggered substantial interest as it mitigates the lack of pixel-wise annotations. Given global image labels, WSL methods yield pixel-level predictions (segmentations), which enable to interpret class predictions. Despite their recent success, mostly with natural images, such methods can face important challenges when the foreground and background regions have similar visual cues, yielding high false-positive rates in segmentations, as is the case in challenging histology images. WSL training is commonly driven by standard classification losses, which implicitly maximize model confidence, and locate the discriminative regions linked to classification decisions. Therefore, they lack mechanisms for modeling explicitly non-discriminative regions and reducing false-positive rates. We propose novel regularization terms, which enable the model to seek both non-discriminative and discriminative regions, while discouraging unbalanced segmentations. We introduce high uncertainty as a criterion to localize non-discriminative regions that do not affect classifier decision, and describe it with original Kullback-Leibler (KL) divergence losses evaluating the deviation of posterior predictions from the uniform distribution. Our KL terms encourage high uncertainty of the model when the latter inputs the latent non-discriminative regions. Our loss integrates: (i) a cross-entropy seeking a foreground, where model confidence about class prediction is high; (ii) a KL regularizer seeking a background, where model uncertainty is high; and (iii) log-barrier terms discouraging unbalanced segmentations. Comprehensive experiments and ablation studies over the public GlaS colon cancer data and a Camelyon16 patch-based benchmark for breast cancer show substantial improvements over state-of-the-art WSL methods, and confirm the effect of our new regularizers.



### OGNet: Towards a Global Oil and Gas Infrastructure Database using Deep Learning on Remotely Sensed Imagery
- **Arxiv ID**: http://arxiv.org/abs/2011.07227v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.07227v1)
- **Published**: 2020-11-14 06:20:21+00:00
- **Updated**: 2020-11-14 06:20:21+00:00
- **Authors**: Hao Sheng, Jeremy Irvin, Sasankh Munukutla, Shawn Zhang, Christopher Cross, Kyle Story, Rose Rustowicz, Cooper Elsworth, Zutao Yang, Mark Omara, Ritesh Gautam, Robert B. Jackson, Andrew Y. Ng
- **Comment**: Tackling Climate Change with Machine Learning at NeurIPS 2020
  (Spotlight talk)
- **Journal**: None
- **Summary**: At least a quarter of the warming that the Earth is experiencing today is due to anthropogenic methane emissions. There are multiple satellites in orbit and planned for launch in the next few years which can detect and quantify these emissions; however, to attribute methane emissions to their sources on the ground, a comprehensive database of the locations and characteristics of emission sources worldwide is essential. In this work, we develop deep learning algorithms that leverage freely available high-resolution aerial imagery to automatically detect oil and gas infrastructure, one of the largest contributors to global methane emissions. We use the best algorithm, which we call OGNet, together with expert review to identify the locations of oil refineries and petroleum terminals in the U.S. We show that OGNet detects many facilities which are not present in four standard public datasets of oil and gas infrastructure. All detected facilities are associated with characteristics known to contribute to methane emissions, including the infrastructure type and the number of storage tanks. The data curated and produced in this study is freely available at http://stanfordmlgroup.github.io/projects/ognet .



### TDAsweep: A Novel Dimensionality Reduction Method for Image Classification Tasks
- **Arxiv ID**: http://arxiv.org/abs/2011.07230v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.07230v1)
- **Published**: 2020-11-14 06:55:31+00:00
- **Updated**: 2020-11-14 06:55:31+00:00
- **Authors**: Yu-Shih Chen, Melissa Goh, Norm Matloff
- **Comment**: None
- **Journal**: None
- **Summary**: One of the most celebrated achievements of modern machine learning technology is automatic classification of images. However, success is typically achieved only with major computational costs. Here we introduce TDAsweep, a machine learning tool aimed at improving the efficiency of automatic classification of images.



### ActBERT: Learning Global-Local Video-Text Representations
- **Arxiv ID**: http://arxiv.org/abs/2011.07231v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.07231v1)
- **Published**: 2020-11-14 07:14:08+00:00
- **Updated**: 2020-11-14 07:14:08+00:00
- **Authors**: Linchao Zhu, Yi Yang
- **Comment**: A few new results are included
- **Journal**: None
- **Summary**: In this paper, we introduce ActBERT for self-supervised learning of joint video-text representations from unlabeled data. First, we leverage global action information to catalyze the mutual interactions between linguistic texts and local regional objects. It uncovers global and local visual clues from paired video sequences and text descriptions for detailed visual and text relation modeling. Second, we introduce an ENtangled Transformer block (ENT) to encode three sources of information, i.e., global actions, local regional objects, and linguistic descriptions. Global-local correspondences are discovered via judicious clues extraction from contextual information. It enforces the joint videotext representation to be aware of fine-grained objects as well as global human intention. We validate the generalization capability of ActBERT on downstream video-and language tasks, i.e., text-video clip retrieval, video captioning, video question answering, action segmentation, and action step localization. ActBERT significantly outperforms the state-of-the-arts, demonstrating its superiority in video-text representation learning.



### Stable View Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2011.07233v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.07233v2)
- **Published**: 2020-11-14 07:24:43+00:00
- **Updated**: 2021-05-02 12:20:20+00:00
- **Authors**: Gernot Riegler, Vladlen Koltun
- **Comment**: Published at CVPR 2021, https://youtu.be/gqgXIY09htI
- **Journal**: None
- **Summary**: We present Stable View Synthesis (SVS). Given a set of source images depicting a scene from freely distributed viewpoints, SVS synthesizes new views of the scene. The method operates on a geometric scaffold computed via structure-from-motion and multi-view stereo. Each point on this 3D scaffold is associated with view rays and corresponding feature vectors that encode the appearance of this point in the input images. The core of SVS is view-dependent on-surface feature aggregation, in which directional feature vectors at each 3D point are processed to produce a new feature vector for a ray that maps this point into the new target view. The target view is then rendered by a convolutional network from a tensor of features synthesized in this way for all pixels. The method is composed of differentiable modules and is trained end-to-end. It supports spatially-varying view-dependent importance weighting and feature transformation of source images at each point; spatial and temporal stability due to the smooth dependence of on-surface feature aggregation on the target view; and synthesis of view-dependent effects such as specular reflection. Experimental results demonstrate that SVS outperforms state-of-the-art view synthesis methods both quantitatively and qualitatively on three diverse real-world datasets, achieving unprecedented levels of realism in free-viewpoint video of challenging large-scale scenes. Code is available at https://github.com/intel-isl/StableViewSynthesis



### Prototypical Contrast and Reverse Prediction: Unsupervised Skeleton Based Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2011.07236v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.07236v1)
- **Published**: 2020-11-14 08:04:23+00:00
- **Updated**: 2020-11-14 08:04:23+00:00
- **Authors**: Shihao Xu, Haocong Rao, Xiping Hu, Bin Hu
- **Comment**: Codes are available at https://github.com/Mikexu007/PCRP
- **Journal**: None
- **Summary**: In this paper, we focus on unsupervised representation learning for skeleton-based action recognition. Existing approaches usually learn action representations by sequential prediction but they suffer from the inability to fully learn semantic information. To address this limitation, we propose a novel framework named Prototypical Contrast and Reverse Prediction (PCRP), which not only creates reverse sequential prediction to learn low-level information (e.g., body posture at every frame) and high-level pattern (e.g., motion order), but also devises action prototypes to implicitly encode semantic similarity shared among sequences. In general, we regard action prototypes as latent variables and formulate PCRP as an expectation-maximization task. Specifically, PCRP iteratively runs (1) E-step as determining the distribution of prototypes by clustering action encoding from the encoder, and (2) M-step as optimizing the encoder by minimizing the proposed ProtoMAE loss, which helps simultaneously pull the action encoding closer to its assigned prototype and perform reverse prediction task. Extensive experiments on N-UCLA, NTU 60, and NTU 120 dataset present that PCRP outperforms state-of-the-art unsupervised methods and even achieves superior performance over some of supervised methods. Codes are available at https://github.com/Mikexu007/PCRP.



### Ego2Hands: A Dataset for Egocentric Two-hand Segmentation and Detection
- **Arxiv ID**: http://arxiv.org/abs/2011.07252v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.07252v4)
- **Published**: 2020-11-14 10:12:35+00:00
- **Updated**: 2021-12-20 10:37:48+00:00
- **Authors**: Fanqing Lin, Brian Price, Tony Martinez
- **Comment**: None
- **Journal**: None
- **Summary**: Hand segmentation and detection in truly unconstrained RGB-based settings is important for many applications. However, existing datasets are far from sufficient in terms of size and variety due to the infeasibility of manual annotation of large amounts of segmentation and detection data. As a result, current methods are limited by many underlying assumptions such as constrained environment, consistent skin color and lighting. In this work, we present Ego2Hands, a large-scale RGB-based egocentric hand segmentation/detection dataset that is semi-automatically annotated and a color-invariant compositing-based data generation technique capable of creating training data with large quantity and variety. For quantitative analysis, we manually annotated an evaluation set that significantly exceeds existing benchmarks in quantity, diversity and annotation accuracy. We provide cross-dataset evaluation as well as thorough analysis on the performance of state-of-the-art models on Ego2Hands to show that our dataset and data generation technique can produce models that generalize to unseen environments without domain adaptation.



### Factorized Gaussian Process Variational Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/2011.07255v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.07255v1)
- **Published**: 2020-11-14 10:24:10+00:00
- **Updated**: 2020-11-14 10:24:10+00:00
- **Authors**: Metod Jazbec, Michael Pearce, Vincent Fortuin
- **Comment**: None
- **Journal**: None
- **Summary**: Variational autoencoders often assume isotropic Gaussian priors and mean-field posteriors, hence do not exploit structure in scenarios where we may expect similarity or consistency across latent variables. Gaussian process variational autoencoders alleviate this problem through the use of a latent Gaussian process, but lead to a cubic inference time complexity. We propose a more scalable extension of these models by leveraging the independence of the auxiliary features, which is present in many datasets. Our model factorizes the latent kernel across these features in different dimensions, leading to a significant speed-up (in theory and practice), while empirically performing comparably to existing non-scalable approaches. Moreover, our approach allows for additional modeling of global latent information and for more general extrapolation to unseen input combinations.



### Towards Zero-Shot Learning with Fewer Seen Class Examples
- **Arxiv ID**: http://arxiv.org/abs/2011.07279v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.07279v1)
- **Published**: 2020-11-14 11:58:35+00:00
- **Updated**: 2020-11-14 11:58:35+00:00
- **Authors**: Vinay Kumar Verma, Ashish Mishra, Anubha Pandey, Hema A. Murthy, Piyush Rai
- **Comment**: Accepted in WACV 2021
- **Journal**: None
- **Summary**: We present a meta-learning based generative model for zero-shot learning (ZSL) towards a challenging setting when the number of training examples from each \emph{seen} class is very few. This setup contrasts with the conventional ZSL approaches, where training typically assumes the availability of a sufficiently large number of training examples from each of the seen classes. The proposed approach leverages meta-learning to train a deep generative model that integrates variational autoencoder and generative adversarial networks. We propose a novel task distribution where meta-train and meta-validation classes are disjoint to simulate the ZSL behaviour in training. Once trained, the model can generate synthetic examples from seen and unseen classes. Synthesize samples can then be used to train the ZSL framework in a supervised manner. The meta-learner enables our model to generates high-fidelity samples using only a small number of training examples from seen classes. We conduct extensive experiments and ablation studies on four benchmark datasets of ZSL and observe that the proposed model outperforms state-of-the-art approaches by a significant margin when the number of examples per seen class is very small.



### Pose-dependent weights and Domain Randomization for fully automatic X-ray to CT Registration
- **Arxiv ID**: http://arxiv.org/abs/2011.07294v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.07294v2)
- **Published**: 2020-11-14 12:50:32+00:00
- **Updated**: 2021-04-15 09:54:28+00:00
- **Authors**: Matthias Grimm, Javier Esteban, Mathias Unberath, Nassir Navab
- **Comment**: None
- **Journal**: None
- **Summary**: Fully automatic X-ray to CT registration requires a solid initialization to provide an initial alignment within the capture range of existing intensity-based registrations. This work adresses that need by providing a novel automatic initialization, which enables end to end registration. First, a neural network is trained once to detect a set of anatomical landmarks on simulated X-rays. A domain randomization scheme is proposed to enable the network to overcome the challenge of being trained purely on simulated data and run inference on real Xrays. Then, for each patient CT, a patient-specific landmark extraction scheme is used. It is based on backprojecting and clustering the previously trained networks predictions on a set of simulated X-rays. Next, the network is retrained to detect the new landmarks. Finally the combination of network and 3D landmark locations is used to compute the initialization using a perspective-n-point algorithm. During the computation of the pose, a weighting scheme is introduced to incorporate the confidence of the network in detecting the landmarks. The algorithm is evaluated on the pelvis using both real and simulated x-rays. The mean (+-standard deviation) target registration error in millimetres is 4.1 +- 4.3 for simulated X-rays with a success rate of 92% and 4.2 +- 3.9 for real X-rays with a success rate of 86.8%, where a success is defined as a translation error of less than 30mm.



### Speech Prediction in Silent Videos using Variational Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/2011.07340v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.07340v1)
- **Published**: 2020-11-14 17:09:03+00:00
- **Updated**: 2020-11-14 17:09:03+00:00
- **Authors**: Ravindra Yadav, Ashish Sardana, Vinay P Namboodiri, Rajesh M Hegde
- **Comment**: None
- **Journal**: None
- **Summary**: Understanding the relationship between the auditory and visual signals is crucial for many different applications ranging from computer-generated imagery (CGI) and video editing automation to assisting people with hearing or visual impairments. However, this is challenging since the distribution of both audio and visual modality is inherently multimodal. Therefore, most of the existing methods ignore the multimodal aspect and assume that there only exists a deterministic one-to-one mapping between the two modalities. It can lead to low-quality predictions as the model collapses to optimizing the average behavior rather than learning the full data distributions. In this paper, we present a stochastic model for generating speech in a silent video. The proposed model combines recurrent neural networks and variational deep generative models to learn the auditory signal's conditional distribution given the visual signal. We demonstrate the performance of our model on the GRID dataset based on standard benchmarks.



### Pneumothorax and chest tube classification on chest x-rays for detection of missed pneumothorax
- **Arxiv ID**: http://arxiv.org/abs/2011.07353v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.07353v1)
- **Published**: 2020-11-14 18:06:06+00:00
- **Updated**: 2020-11-14 18:06:06+00:00
- **Authors**: Benedikt Graf, Arkadiusz Sitek, Amin Katouzian, Yen-Fu Lu, Arun Krishnan, Justin Rafael, Kirstin Small, Yiting Xie
- **Comment**: None
- **Journal**: None
- **Summary**: Chest x-ray imaging is widely used for the diagnosis of pneumothorax and there has been significant interest in developing automated methods to assist in image interpretation. We present an image classification pipeline which detects pneumothorax as well as the various types of chest tubes that are commonly used to treat pneumothorax. Our multi-stage algorithm is based on lung segmentation followed by pneumothorax classification, including classification of patches that are most likely to contain pneumothorax. This algorithm achieves state of the art performance for pneumothorax classification on an open-source benchmark dataset. Unlike previous work, this algorithm shows comparable performance on data with and without chest tubes and thus has an improved clinical utility. To evaluate these algorithms in a realistic clinical scenario, we demonstrate the ability to identify real cases of missed pneumothorax in a large dataset of chest x-ray studies.



### Counting Cows: Tracking Illegal Cattle Ranching From High-Resolution Satellite Imagery
- **Arxiv ID**: http://arxiv.org/abs/2011.07369v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.07369v1)
- **Published**: 2020-11-14 19:07:39+00:00
- **Updated**: 2020-11-14 19:07:39+00:00
- **Authors**: Issam Laradji, Pau Rodriguez, Freddie Kalaitzis, David Vazquez, Ross Young, Ed Davey, Alexandre Lacoste
- **Comment**: None
- **Journal**: None
- **Summary**: Cattle farming is responsible for 8.8\% of greenhouse gas emissions worldwide. In addition to the methane emitted due to their digestive process, the growing need for grazing areas is an important driver of deforestation. While some regulations are in place for preserving the Amazon against deforestation, these are being flouted in various ways, hence the need to scale and automate the monitoring of cattle ranching activities. Through a partnership with \textit{Global Witness}, we explore the feasibility of tracking and counting cattle at the continental scale from satellite imagery. With a license from Maxar Technologies, we obtained satellite imagery of the Amazon at 40cm resolution, and compiled a dataset of 903 images containing a total of 28498 cattle. Our experiments show promising results and highlight important directions for the next steps on both counting algorithms and the data collection process for solving such challenges. The code is available at \url{https://github.com/IssamLaradji/cownter_strike}.



### An Autonomous Approach to Measure Social Distances and Hygienic Practices during COVID-19 Pandemic in Public Open Spaces
- **Arxiv ID**: http://arxiv.org/abs/2011.07375v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.07375v1)
- **Published**: 2020-11-14 19:35:09+00:00
- **Updated**: 2020-11-14 19:35:09+00:00
- **Authors**: Peng Sun, Gabriel Draughon, Jerome Lynch
- **Comment**: 11 pages, 9 figures
- **Journal**: None
- **Summary**: Coronavirus has been spreading around the world since the end of 2019. The virus can cause acute respiratory syndrome, which can be lethal, and is easily transmitted between hosts. Most states have issued state-at-home executive orders, however, parks and other public open spaces have largely remained open and are seeing sharp increases in public use. Therefore, in order to ensure public safety, it is imperative for patrons of public open spaces to practice safe hygiene and take preventative measures. This work provides a scalable sensing approach to detect physical activities within public open spaces and monitor adherence to social distancing guidelines suggested by the US Centers for Disease Control and Prevention (CDC). A deep learning-based computer vision sensing framework is designed to investigate the careful and proper utilization of parks and park facilities with hard surfaces (e.g. benches, fence poles, and trash cans) using video feeds from a pre-installed surveillance camera network. The sensing framework consists of a CNN-based object detector, a multi-target tracker, a mapping module, and a group reasoning module. The experiments are carried out during the COVID-19 pandemic between March 2020 and May 2020 across several key locations at the Detroit Riverfront Parks in Detroit, Michigan. The sensing framework is validated by comparing automatic sensing results with manually labeled ground-truth results. The proposed approach significantly improves the efficiency of providing spatial and temporal statistics of users in public open spaces by creating straightforward data visualizations for federal and state agencies. The results can also provide on-time triggering information for an alarming or actuator system which can later be added to intervene inappropriate behavior during this pandemic.



### Few-shot Object Grounding and Mapping for Natural Language Robot Instruction Following
- **Arxiv ID**: http://arxiv.org/abs/2011.07384v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CL, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.07384v1)
- **Published**: 2020-11-14 20:35:20+00:00
- **Updated**: 2020-11-14 20:35:20+00:00
- **Authors**: Valts Blukis, Ross A. Knepper, Yoav Artzi
- **Comment**: 4th Conference on Robot Learning (CoRL 2020), Cambridge MA, USA
- **Journal**: None
- **Summary**: We study the problem of learning a robot policy to follow natural language instructions that can be easily extended to reason about new objects. We introduce a few-shot language-conditioned object grounding method trained from augmented reality data that uses exemplars to identify objects and align them to their mentions in instructions. We present a learned map representation that encodes object locations and their instructed use, and construct it from our few-shot grounding output. We integrate this mapping approach into an instruction-following policy, thereby allowing it to reason about previously unseen objects at test-time by simply adding exemplars. We evaluate on the task of learning to map raw observations and instructions to continuous control of a physical quadcopter. Our approach significantly outperforms the prior state of the art in the presence of new objects, even when the prior approach observes all objects during training.



### Privacy-Preserving Pose Estimation for Human-Robot Interaction
- **Arxiv ID**: http://arxiv.org/abs/2011.07387v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.07387v1)
- **Published**: 2020-11-14 21:09:53+00:00
- **Updated**: 2020-11-14 21:09:53+00:00
- **Authors**: Youya Xia, Yifan Tang, Yuhan Hu, Guy Hoffman
- **Comment**: Submitted for review at ICRA2021
- **Journal**: None
- **Summary**: Pose estimation is an important technique for nonverbal human-robot interaction. That said, the presence of a camera in a person's space raises privacy concerns and could lead to distrust of the robot. In this paper, we propose a privacy-preserving camera-based pose estimation method. The proposed system consists of a user-controlled translucent filter that covers the camera and an image enhancement module designed to facilitate pose estimation from the filtered (shadow) images, while never capturing clear images of the user. We evaluate the system's performance on a new filtered image dataset, considering the effects of distance from the camera, background clutter, and film thickness. Based on our findings, we conclude that our system can protect humans' privacy while detecting humans' pose information effectively.



### Automatic classification of multiple catheters in neonatal radiographs with deep learning
- **Arxiv ID**: http://arxiv.org/abs/2011.07394v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.07394v1)
- **Published**: 2020-11-14 21:27:21+00:00
- **Updated**: 2020-11-14 21:27:21+00:00
- **Authors**: Robert D. E. Henderson, Xin Yi, Scott J. Adams, Paul Babyn
- **Comment**: 10 pages, 5 figures (+1 suppl.), 2 tables (+2 suppl.). Submitted to
  Journal of Digital Imaging
- **Journal**: None
- **Summary**: We develop and evaluate a deep learning algorithm to classify multiple catheters on neonatal chest and abdominal radiographs. A convolutional neural network (CNN) was trained using a dataset of 777 neonatal chest and abdominal radiographs, with a split of 81%-9%-10% for training-validation-testing, respectively. We employed ResNet-50 (a CNN), pre-trained on ImageNet. Ground truth labelling was limited to tagging each image to indicate the presence or absence of endotracheal tubes (ETTs), nasogastric tubes (NGTs), and umbilical arterial and venous catheters (UACs, UVCs). The data set included 561 images containing 2 or more catheters, 167 images with only one, and 49 with none. Performance was measured with average precision (AP), calculated from the area under the precision-recall curve. On our test data, the algorithm achieved an overall AP (95% confidence interval) of 0.977 (0.679-0.999) for NGTs, 0.989 (0.751-1.000) for ETTs, 0.979 (0.873-0.997) for UACs, and 0.937 (0.785-0.984) for UVCs. Performance was similar for the set of 58 test images consisting of 2 or more catheters, with an AP of 0.975 (0.255-1.000) for NGTs, 0.997 (0.009-1.000) for ETTs, 0.981 (0.797-0.998) for UACs, and 0.937 (0.689-0.990) for UVCs. Our network thus achieves strong performance in the simultaneous detection of these four catheter types. Radiologists may use such an algorithm as a time-saving mechanism to automate reporting of catheters on radiographs.



