# Arxiv Papers in cs.CV on 2020-11-24
### The Interpretable Dictionary in Sparse Coding
- **Arxiv ID**: http://arxiv.org/abs/2011.11805v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.11805v1)
- **Published**: 2020-11-24 00:26:40+00:00
- **Updated**: 2020-11-24 00:26:40+00:00
- **Authors**: Edward Kim, Connor Onweller, Andrew O'Brien, Kathleen McCoy
- **Comment**: None
- **Journal**: None
- **Summary**: Artificial neural networks (ANNs), specifically deep learning networks, have often been labeled as black boxes due to the fact that the internal representation of the data is not easily interpretable. In our work, we illustrate that an ANN, trained using sparse coding under specific sparsity constraints, yields a more interpretable model than the standard deep learning model. The dictionary learned by sparse coding can be more easily understood and the activations of these elements creates a selective feature output. We compare and contrast our sparse coding model with an equivalent feed forward convolutional autoencoder trained on the same data. Our results show both qualitative and quantitative benefits in the interpretation of the learned sparse coding dictionary as well as the internal activation representations.



### MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
- **Arxiv ID**: http://arxiv.org/abs/2011.11814v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.11814v3)
- **Published**: 2020-11-24 00:40:36+00:00
- **Updated**: 2021-05-06 09:04:06+00:00
- **Authors**: Felix Wimbauer, Nan Yang, Lukas von Stumberg, Niclas Zeller, Daniel Cremers
- **Comment**: CVPR 2021, Project page with video can be found under
  https://vision.in.tum.de/research/monorec. 14 pages, 10 figures, 5 tables
- **Journal**: Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition (CVPR), 2021, pp. 6112-6122
- **Summary**: In this paper, we propose MonoRec, a semi-supervised monocular dense reconstruction architecture that predicts depth maps from a single moving camera in dynamic environments. MonoRec is based on a multi-view stereo setting which encodes the information of multiple consecutive images in a cost volume. To deal with dynamic objects in the scene, we introduce a MaskModule that predicts moving object masks by leveraging the photometric inconsistencies encoded in the cost volumes. Unlike other multi-view stereo methods, MonoRec is able to reconstruct both static and moving objects by leveraging the predicted masks. Furthermore, we present a novel multi-stage training scheme with a semi-supervised loss formulation that does not require LiDAR depth values. We carefully evaluate MonoRec on the KITTI dataset and show that it achieves state-of-the-art performance compared to both multi-view and single-view methods. With the model trained on KITTI, we further demonstrate that MonoRec is able to generalize well to both the Oxford RobotCar dataset and the more challenging TUM-Mono dataset recorded by a handheld camera. Code and related materials will be available at https://vision.in.tum.de/research/monorec.



### Dissecting Image Crops
- **Arxiv ID**: http://arxiv.org/abs/2011.11831v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.11831v4)
- **Published**: 2020-11-24 01:33:47+00:00
- **Updated**: 2021-09-05 23:39:12+00:00
- **Authors**: Basile Van Hoorick, Carl Vondrick
- **Comment**: Updated smartphone datasets & table; some rewording
- **Journal**: None
- **Summary**: The elementary operation of cropping underpins nearly every computer vision system, ranging from data augmentation and translation invariance to computational photography and representation learning. This paper investigates the subtle traces introduced by this operation. For example, despite refinements to camera optics, lenses will leave behind certain clues, notably chromatic aberration and vignetting. Photographers also leave behind other clues relating to image aesthetics and scene composition. We study how to detect these traces, and investigate the impact that cropping has on the image distribution. While our aim is to dissect the fundamental impact of spatial crops, there are also a number of practical implications to our work, such as revealing faulty photojournalism and equipping neural network researchers with a better understanding of shortcut learning. Code is available at https://github.com/basilevh/dissecting-image-crops.



### Comparisons among different stochastic selection of activation layers for convolutional neural networks for healthcare
- **Arxiv ID**: http://arxiv.org/abs/2011.11834v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2011.11834v1)
- **Published**: 2020-11-24 01:53:39+00:00
- **Updated**: 2020-11-24 01:53:39+00:00
- **Authors**: Loris Nanni, Alessandra Lumini, Stefano Ghidoni, Gianluca Maguolo
- **Comment**: None
- **Journal**: None
- **Summary**: Classification of biological images is an important task with crucial application in many fields, such as cell phenotypes recognition, detection of cell organelles and histopathological classification, and it might help in early medical diagnosis, allowing automatic disease classification without the need of a human expert. In this paper we classify biomedical images using ensembles of neural networks. We create this ensemble using a ResNet50 architecture and modifying its activation layers by substituting ReLUs with other functions. We select our activations among the following ones: ReLU, leaky ReLU, Parametric ReLU, ELU, Adaptive Piecewice Linear Unit, S-Shaped ReLU, Swish , Mish, Mexican Linear Unit, Gaussian Linear Unit, Parametric Deformable Linear Unit, Soft Root Sign (SRS) and others.   As a baseline, we used an ensemble of neural networks that only use ReLU activations. We tested our networks on several small and medium sized biomedical image datasets. Our results prove that our best ensemble obtains a better performance than the ones of the naive approaches. In order to encourage the reproducibility of this work, the MATLAB code of all the experiments will be shared at https://github.com/LorisNanni.



### Benchmarking Inference Performance of Deep Learning Models on Analog Devices
- **Arxiv ID**: http://arxiv.org/abs/2011.11840v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.AR, cs.CV, cs.PF
- **Links**: [PDF](http://arxiv.org/pdf/2011.11840v2)
- **Published**: 2020-11-24 02:14:39+00:00
- **Updated**: 2020-12-16 22:04:52+00:00
- **Authors**: Omobayode Fagbohungbe, Lijun Qian
- **Comment**: None
- **Journal**: None
- **Summary**: Analog hardware implemented deep learning models are promising for computation and energy constrained systems such as edge computing devices. However, the analog nature of the device and the associated many noise sources will cause changes to the value of the weights in the trained deep learning models deployed on such devices. In this study, systematic evaluation of the inference performance of trained popular deep learning models for image classification deployed on analog devices has been carried out, where additive white Gaussian noise has been added to the weights of the trained models during inference. It is observed that deeper models and models with more redundancy in design such as VGG are more robust to the noise in general. However, the performance is also affected by the design philosophy of the model, the detailed structure of the model, the exact machine learning task, as well as the datasets.



### Unsupervised Discovery of Disentangled Manifolds in GANs
- **Arxiv ID**: http://arxiv.org/abs/2011.11842v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.11842v2)
- **Published**: 2020-11-24 02:18:08+00:00
- **Updated**: 2020-11-29 18:56:50+00:00
- **Authors**: Yu-Ding Lu, Hsin-Ying Lee, Hung-Yu Tseng, Ming-Hsuan Yang
- **Comment**: None
- **Journal**: None
- **Summary**: As recent generative models can generate photo-realistic images, people seek to understand the mechanism behind the generation process. Interpretable generation process is beneficial to various image editing applications. In this work, we propose a framework to discover interpretable directions in the latent space given arbitrary pre-trained generative adversarial networks. We propose to learn the transformation from prior one-hot vectors representing different attributes to the latent space used by pre-trained models. Furthermore, we apply a centroid loss function to improve consistency and smoothness while traversing through different directions. We demonstrate the efficacy of the proposed framework on a wide range of datasets. The discovered direction vectors are shown to be visually corresponding to various distinct attributes and thus enable attribute editing.



### Augmented Lagrangian Adversarial Attacks
- **Arxiv ID**: http://arxiv.org/abs/2011.11857v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.11857v2)
- **Published**: 2020-11-24 02:51:08+00:00
- **Updated**: 2021-08-19 15:26:06+00:00
- **Authors**: Jérôme Rony, Eric Granger, Marco Pedersoli, Ismail Ben Ayed
- **Comment**: ICCV 2021 (Poster). Code available at:
  https://github.com/jeromerony/augmented_lagrangian_adversarial_attacks
- **Journal**: None
- **Summary**: Adversarial attack algorithms are dominated by penalty methods, which are slow in practice, or more efficient distance-customized methods, which are heavily tailored to the properties of the distance considered. We propose a white-box attack algorithm to generate minimally perturbed adversarial examples based on Augmented Lagrangian principles. We bring several algorithmic modifications, which have a crucial effect on performance. Our attack enjoys the generality of penalty methods and the computational efficiency of distance-customized algorithms, and can be readily used for a wide set of distances. We compare our attack to state-of-the-art methods on three datasets and several models, and consistently obtain competitive performances with similar or lower computational complexity.



### GMOT-40: A Benchmark for Generic Multiple Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/2011.11858v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.11858v3)
- **Published**: 2020-11-24 02:51:46+00:00
- **Updated**: 2021-04-07 19:13:00+00:00
- **Authors**: Hexin Bai, Wensheng Cheng, Peng Chu, Juehuan Liu, Kai Zhang, Haibin Ling
- **Comment**: None
- **Journal**: None
- **Summary**: Multiple Object Tracking (MOT) has witnessed remarkable advances in recent years. However, existing studies dominantly request prior knowledge of the tracking target, and hence may not generalize well to unseen categories. In contrast, Generic Multiple Object Tracking (GMOT), which requires little prior information about the target, is largely under-explored. In this paper, we make contributions to boost the study of GMOT in three aspects. First, we construct the first public GMOT dataset, dubbed GMOT-40, which contains 40 carefully annotated sequences evenly distributed among 10 object categories. In addition, two tracking protocols are adopted to evaluate different characteristics of tracking algorithms. Second, by noting the lack of devoted tracking algorithms, we have designed a series of baseline GMOT algorithms. Third, we perform a thorough evaluation on GMOT-40, involving popular MOT algorithms (with necessary modifications) and the proposed baselines. We will release the GMOT-40 benchmark, the evaluation results, as well as the baseline algorithm to the public upon the publication of the paper.



### Multi-Scale Progressive Fusion Learning for Depth Map Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2011.11865v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.11865v1)
- **Published**: 2020-11-24 03:03:07+00:00
- **Updated**: 2020-11-24 03:03:07+00:00
- **Authors**: Chuhua Xian, Kun Qian, Zitian Zhang, Charlie C. L. Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Limited by the cost and technology, the resolution of depth map collected by depth camera is often lower than that of its associated RGB camera. Although there have been many researches on RGB image super-resolution (SR), a major problem with depth map super-resolution is that there will be obvious jagged edges and excessive loss of details. To tackle these difficulties, in this work, we propose a multi-scale progressive fusion network for depth map SR, which possess an asymptotic structure to integrate hierarchical features in different domains. Given a low-resolution (LR) depth map and its associated high-resolution (HR) color image, We utilize two different branches to achieve multi-scale feature learning. Next, we propose a step-wise fusion strategy to restore the HR depth map. Finally, a multi-dimensional loss is introduced to constrain clear boundaries and details. Extensive experiments show that our proposed method produces improved results against state-of-the-art methods both qualitatively and quantitatively.



### Mixture-based Feature Space Learning for Few-shot Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2011.11872v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.11872v2)
- **Published**: 2020-11-24 03:16:27+00:00
- **Updated**: 2021-08-17 17:44:41+00:00
- **Authors**: Arman Afrasiyabi, Jean-François Lalonde, Christian Gagné
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce Mixture-based Feature Space Learning (MixtFSL) for obtaining a rich and robust feature representation in the context of few-shot image classification. Previous works have proposed to model each base class either with a single point or with a mixture model by relying on offline clustering algorithms. In contrast, we propose to model base classes with mixture models by simultaneously training the feature extractor and learning the mixture model parameters in an online manner. This results in a richer and more discriminative feature space which can be employed to classify novel examples from very few samples. Two main stages are proposed to train the MixtFSL model. First, the multimodal mixtures for each base class and the feature extractor parameters are learned using a combination of two loss functions. Second, the resulting network and mixture models are progressively refined through a leader-follower learning procedure, which uses the current estimate as a "target" network. This target network is used to make a consistent assignment of instances to mixture components, which increases performance and stabilizes training. The effectiveness of our end-to-end feature space learning approach is demonstrated with extensive experiments on four standard datasets and four backbones. Notably, we demonstrate that when we combine our robust representation with recent alignment-based approaches, we achieve new state-of-the-art results in the inductive setting, with an absolute accuracy for 5-shot classification of 82.45 on miniImageNet, 88.20 with tieredImageNet, and 60.70 in FC100 using the ResNet-12 backbone.



### Utilizing UNet for the future traffic map prediction task Traffic4cast challenge 2020
- **Arxiv ID**: http://arxiv.org/abs/2012.00125v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, 68T07
- **Links**: [PDF](http://arxiv.org/pdf/2012.00125v1)
- **Published**: 2020-11-24 03:21:32+00:00
- **Updated**: 2020-11-24 03:21:32+00:00
- **Authors**: Sungbin Choi
- **Comment**: 8 pages, 4 figures, NeurIPS 2020 Traffic4cast workshop paper
- **Journal**: None
- **Summary**: This paper describes our UNet based experiments on the Traffic4cast challenge 2020. Similar to the Traffic4cast challenge 2019, the task is to predict traffic flow volume, direction and speed on a high resolution map of three large cities worldwide. We mainly experimented with UNet based deep convolutional networks with various compositions of densely connected convolution layers, average pooling layers and max pooling layers. Three base UNet model types are tried and predictions are combined by averaging prediction scores or taking median value. Our method achieved best performance in this years newly built challenge dataset.



### Blind deblurring for microscopic pathology images using deep learning networks
- **Arxiv ID**: http://arxiv.org/abs/2011.11879v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.11879v1)
- **Published**: 2020-11-24 03:52:45+00:00
- **Updated**: 2020-11-24 03:52:45+00:00
- **Authors**: Cheng Jiang, Jun Liao, Pei Dong, Zhaoxuan Ma, De Cai, Guoan Zheng, Yueping Liu, Hong Bu, Jianhua Yao
- **Comment**: None
- **Journal**: None
- **Summary**: Artificial Intelligence (AI)-powered pathology is a revolutionary step in the world of digital pathology and shows great promise to increase both diagnosis accuracy and efficiency. However, defocus and motion blur can obscure tissue or cell characteristics hence compromising AI algorithms'accuracy and robustness in analyzing the images. In this paper, we demonstrate a deep-learning-based approach that can alleviate the defocus and motion blur of a microscopic image and output a sharper and cleaner image with retrieved fine details without prior knowledge of the blur type, blur extent and pathological stain. In this approach, a deep learning classifier is first trained to identify the image blur type. Then, two encoder-decoder networks are trained and used alone or in combination to deblur the input image. It is an end-to-end approach and introduces no corrugated artifacts as traditional blind deconvolution methods do. We test our approach on different types of pathology specimens and demonstrate great performance on image blur correction and the subsequent improvement on the diagnosis outcome of AI algorithms.



### Cross-Camera Convolutional Color Constancy
- **Arxiv ID**: http://arxiv.org/abs/2011.11890v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.11890v6)
- **Published**: 2020-11-24 04:37:22+00:00
- **Updated**: 2022-02-11 03:45:43+00:00
- **Authors**: Mahmoud Afifi, Jonathan T. Barron, Chloe LeGendre, Yun-Ta Tsai, Francois Bleibel
- **Comment**: None
- **Journal**: ICCV 2021
- **Summary**: We present "Cross-Camera Convolutional Color Constancy" (C5), a learning-based method, trained on images from multiple cameras, that accurately estimates a scene's illuminant color from raw images captured by a new camera previously unseen during training. C5 is a hypernetwork-like extension of the convolutional color constancy (CCC) approach: C5 learns to generate the weights of a CCC model that is then evaluated on the input image, with the CCC weights dynamically adapted to different input content. Unlike prior cross-camera color constancy models, which are usually designed to be agnostic to the spectral properties of test-set images from unobserved cameras, C5 approaches this problem through the lens of transductive inference: additional unlabeled images are provided as input to the model at test time, which allows the model to calibrate itself to the spectral properties of the test-set camera during inference. C5 achieves state-of-the-art accuracy for cross-camera color constancy on several datasets, is fast to evaluate (~7 and ~90 ms per image on a GPU or CPU, respectively), and requires little memory (~2 MB), and thus is a practical solution to the problem of calibration-free automatic white balance for mobile photography.



### Temporal Action Detection with Multi-level Supervision
- **Arxiv ID**: http://arxiv.org/abs/2011.11893v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.11893v2)
- **Published**: 2020-11-24 04:45:17+00:00
- **Updated**: 2021-02-18 10:23:05+00:00
- **Authors**: Baifeng Shi, Qi Dai, Judy Hoffman, Kate Saenko, Trevor Darrell, Huijuan Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Training temporal action detection in videos requires large amounts of labeled data, yet such annotation is expensive to collect. Incorporating unlabeled or weakly-labeled data to train action detection model could help reduce annotation cost. In this work, we first introduce the Semi-supervised Action Detection (SSAD) task with a mixture of labeled and unlabeled data and analyze different types of errors in the proposed SSAD baselines which are directly adapted from the semi-supervised classification task. To alleviate the main error of action incompleteness (i.e., missing parts of actions) in SSAD baselines, we further design an unsupervised foreground attention (UFA) module utilizing the "independence" between foreground and background motion. Then we incorporate weakly-labeled data into SSAD and propose Omni-supervised Action Detection (OSAD) with three levels of supervision. An information bottleneck (IB) suppressing the scene information in non-action frames while preserving the action information is designed to help overcome the accompanying action-context confusion problem in OSAD baselines. We extensively benchmark against the baselines for SSAD and OSAD on our created data splits in THUMOS14 and ActivityNet1.2, and demonstrate the effectiveness of the proposed UFA and IB methods. Lastly, the benefit of our full OSAD-IB model under limited annotation budgets is shown by exploring the optimal annotation strategy for labeled, unlabeled and weakly-labeled data.



### CAFE-GAN: Arbitrary Face Attribute Editing with Complementary Attention Feature
- **Arxiv ID**: http://arxiv.org/abs/2011.11900v1
- **DOI**: 10.1007/978-3-030-58568-6_31
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.11900v1)
- **Published**: 2020-11-24 05:21:03+00:00
- **Updated**: 2020-11-24 05:21:03+00:00
- **Authors**: Jeong-gi Kwak, David K. Han, Hanseok Ko
- **Comment**: None
- **Journal**: European Conference on Computer Vision (ECCV), 2020
- **Summary**: The goal of face attribute editing is altering a facial image according to given target attributes such as hair color, mustache, gender, etc. It belongs to the image-to-image domain transfer problem with a set of attributes considered as a distinctive domain. There have been some works in multi-domain transfer problem focusing on facial attribute editing employing Generative Adversarial Network (GAN). These methods have reported some successes but they also result in unintended changes in facial regions - meaning the generator alters regions unrelated to the specified attributes. To address this unintended altering problem, we propose a novel GAN model which is designed to edit only the parts of a face pertinent to the target attributes by the concept of Complementary Attention Feature (CAFE). CAFE identifies the facial regions to be transformed by considering both target attributes as well as complementary attributes, which we define as those attributes absent in the input facial image. In addition, we introduce a complementary feature matching to help in training the generator for utilizing the spatial information of attributes. Effectiveness of the proposed method is demonstrated by analysis and comparison study with state-of-the-art methods.



### Spatiotemporal Imaging with Diffeomorphic Optimal Transportation
- **Arxiv ID**: http://arxiv.org/abs/2011.11906v1
- **DOI**: 10.1088/1361-6420/ac2a91
- **Categories**: **math.OC**, cs.CV, physics.med-ph, 65F22, 65R32, 65R30, 65D18, 65J22, 65J20, 65L09, 68U10, 94A12,
  94A08, 92C55, 54C56, 57N25, 47A52
- **Links**: [PDF](http://arxiv.org/pdf/2011.11906v1)
- **Published**: 2020-11-24 05:55:25+00:00
- **Updated**: 2020-11-24 05:55:25+00:00
- **Authors**: Chong Chen
- **Comment**: 39 pages, 7 figures, 4 tables
- **Journal**: Inverse Problems, 2021
- **Summary**: We propose a variational model with diffeomorphic optimal transportation for joint image reconstruction and motion estimation. The proposed model is a production of assembling the Wasserstein distance with the Benamou--Brenier formula in optimal transportation and the flow of diffeomorphisms involved in large deformation diffeomorphic metric mapping, which is suitable for the scenario of spatiotemporal imaging with large diffeomorphic and mass-preserving deformations. Specifically, we first use the Benamou--Brenier formula to characterize the optimal transport cost among the flow of mass-preserving images, and restrict the velocity field into the admissible Hilbert space to guarantee the generated deformation flow being diffeomorphic. We then gain the ODE-constrained equivalent formulation for Benamou--Brenier formula. We finally obtain the proposed model with ODE constraint following the framework that presented in our previous work. We further get the equivalent PDE-constrained optimal control formulation. The proposed model is compared against several existing alternatives theoretically. The alternating minimization algorithm is presented for solving the time-discretized version of the proposed model with ODE constraint. Several important issues on the proposed model and associated algorithms are also discussed. Particularly, we present several potential models based on the proposed diffeomorphic optimal transportation. Under appropriate conditions, the proposed algorithm also provides a new scheme to solve the models using quadratic Wasserstein distance. The performance is finally evaluated by several numerical experiments in space-time tomography, where the data is measured from the concerned sequential images with sparse views and/or various noise levels.



### Variational Monocular Depth Estimation for Reliability Prediction
- **Arxiv ID**: http://arxiv.org/abs/2011.11912v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.11912v1)
- **Published**: 2020-11-24 06:23:51+00:00
- **Updated**: 2020-11-24 06:23:51+00:00
- **Authors**: Noriaki Hirose, Shun Taguchi, Keisuke Kawano, Satoshi Koide
- **Comment**: 17 pages, 11 figures, 7 tables
- **Journal**: None
- **Summary**: Self-supervised learning for monocular depth estimation is widely investigated as an alternative to supervised learning approach, that requires a lot of ground truths. Previous works have successfully improved the accuracy of depth estimation by modifying the model structure, adding objectives, and masking dynamic objects and occluded area. However, when using such estimated depth image in applications, such as autonomous vehicles, and robots, we have to uniformly believe the estimated depth at each pixel position. This could lead to fatal errors in performing the tasks, because estimated depth at some pixels may make a bigger mistake. In this paper, we theoretically formulate a variational model for the monocular depth estimation to predict the reliability of the estimated depth image. Based on the results, we can exclude the estimated depths with low reliability or refine them for actual use. The effectiveness of the proposed method is quantitatively and qualitatively demonstrated using the KITTI benchmark and Make3D dataset.



### Benchmarking Image Retrieval for Visual Localization
- **Arxiv ID**: http://arxiv.org/abs/2011.11946v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.11946v2)
- **Published**: 2020-11-24 07:59:52+00:00
- **Updated**: 2020-12-01 07:19:03+00:00
- **Authors**: Noé Pion, Martin Humenberger, Gabriela Csurka, Yohann Cabon, Torsten Sattler
- **Comment**: International Conference on 3D Vision, 2020
- **Journal**: None
- **Summary**: Visual localization, i.e., camera pose estimation in a known scene, is a core component of technologies such as autonomous driving and augmented reality. State-of-the-art localization approaches often rely on image retrieval techniques for one of two tasks: (1) provide an approximate pose estimate or (2) determine which parts of the scene are potentially visible in a given query image. It is common practice to use state-of-the-art image retrieval algorithms for these tasks. These algorithms are often trained for the goal of retrieving the same landmark under a large range of viewpoint changes. However, robustness to viewpoint changes is not necessarily desirable in the context of visual localization. This paper focuses on understanding the role of image retrieval for multiple visual localization tasks. We introduce a benchmark setup and compare state-of-the-art retrieval representations on multiple datasets. We show that retrieval performance on classical landmark retrieval/recognition tasks correlates only for some but not all tasks to localization performance. This indicates a need for retrieval approaches specifically designed for localization tasks. Our benchmark and evaluation protocols are available at https://github.com/naver/kapture-localization.



### Alleviating Class-wise Gradient Imbalance for Pulmonary Airway Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2011.11952v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.11952v2)
- **Published**: 2020-11-24 08:14:38+00:00
- **Updated**: 2021-04-29 10:15:37+00:00
- **Authors**: Hao Zheng, Yulei Qin, Yun Gu, Fangfang Xie, Jie Yang, Jiayuan Sun, Guang-zhong Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Automated airway segmentation is a prerequisite for pre-operative diagnosis and intra-operative navigation for pulmonary intervention. Due to the small size and scattered spatial distribution of peripheral bronchi, this is hampered by severe class imbalance between foreground and background regions, which makes it challenging for CNN-based methods to parse distal small airways. In this paper, we demonstrate that this problem is arisen by gradient erosion and dilation of the neighborhood voxels. During back-propagation, if the ratio of the foreground gradient to background gradient is small while the class imbalance is local, the foreground gradients can be eroded by their neighborhoods. This process cumulatively increases the noise information included in the gradient flow from top layers to the bottom ones, limiting the learning of small structures in CNNs. To alleviate this problem, we use group supervision and the corresponding WingsNet to provide complementary gradient flows to enhance the training of shallow layers. To further address the intra-class imbalance between large and small airways, we design a General Union loss function which obviates the impact of airway size by distance-based weights and adaptively tunes the gradient ratio based on the learning process. Extensive experiments on public datasets demonstrate that the proposed method can predict the airway structures with higher accuracy and better morphological completeness than the baselines.



### DomainMix: Learning Generalizable Person Re-Identification Without Human Annotations
- **Arxiv ID**: http://arxiv.org/abs/2011.11953v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.11953v3)
- **Published**: 2020-11-24 08:15:53+00:00
- **Updated**: 2021-10-22 16:02:31+00:00
- **Authors**: Wenhao Wang, Shengcai Liao, Fang Zhao, Cuicui Kang, Ling Shao
- **Comment**: Accepted to BMVC 2021
- **Journal**: None
- **Summary**: Existing person re-identification models often have low generalizability, which is mostly due to limited availability of large-scale labeled data in training. However, labeling large-scale training data is very expensive and time-consuming, while large-scale synthetic dataset shows promising value in learning generalizable person re-identification models. Therefore, in this paper a novel and practical person re-identification task is proposed,i.e. how to use labeled synthetic dataset and unlabeled real-world dataset to train a universal model. In this way, human annotations are no longer required, and it is scalable to large and diverse real-world datasets. To address the task, we introduce a framework with high generalizability, namely DomainMix. Specifically, the proposed method firstly clusters the unlabeled real-world images and selects the reliable clusters. During training, to address the large domain gap between two domains, a domain-invariant feature learning method is proposed, which introduces a new loss,i.e. domain balance loss, to conduct an adversarial learning between domain-invariant feature learning and domain discrimination, and meanwhile learns a discriminative feature for person re-identification. This way, the domain gap between synthetic and real-world data is much reduced, and the learned feature is generalizable thanks to the large-scale and diverse training data. Experimental results show that the proposed annotation-free method is more or less comparable to the counterpart trained with full human annotations, which is quite promising. In addition, it achieves the current state of the art on several person re-identification datasets under direct cross-dataset evaluation.



### SimTreeLS: Simulating aerial and terrestrial laser scans of trees
- **Arxiv ID**: http://arxiv.org/abs/2011.11954v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.11954v1)
- **Published**: 2020-11-24 08:25:42+00:00
- **Updated**: 2020-11-24 08:25:42+00:00
- **Authors**: Fredrik Westling, Mitch Bryson, James Underwood
- **Comment**: None
- **Journal**: None
- **Summary**: There are numerous emerging applications for digitizing trees using terrestrial and aerial laser scanning, particularly in the fields of agriculture and forestry. Interpretation of LiDAR point clouds is increasingly relying on data-driven methods (such as supervised machine learning) that rely on large quantities of hand-labelled data. As this data is potentially expensive to capture, and difficult to clearly visualise and label manually, a means of supplementing real LiDAR scans with simulated data is becoming a necessary step in realising the potential of these methods. We present an open source tool, SimTreeLS (Simulated Tree Laser Scans), for generating point clouds which simulate scanning with user-defined sensor, trajectory, tree shape and layout parameters. Upon simulation, material classification is kept in a pointwise fashion so leaf and woody matter are perfectly known, and unique identifiers separate individual trees, foregoing post-simulation labelling. This allows for an endless supply of procedurally generated data with similar characteristics to real LiDAR captures, which can then be used for development of data processing techniques or training of machine learning algorithms. To validate our method, we compare the characteristics of a simulated scan with a real scan using similar trees and the same sensor and trajectory parameters. Results suggest the simulated data is significantly more similar to real data than a sample-based control. We also demonstrate application of SimTreeLS on contexts beyond the real data available, simulating scans of new tree shapes, new trajectories and new layouts, with results presenting well. SimTreeLS is available as an open source resource built on publicly available libraries.



### Ultrasound Confidence Maps of Intensity and Structure Based on Directed Acyclic Graphs and Artifact Models
- **Arxiv ID**: http://arxiv.org/abs/2011.11956v4
- **DOI**: 10.1109/ISBI48211.2021.9433862
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.11956v4)
- **Published**: 2020-11-24 08:32:56+00:00
- **Updated**: 2021-04-27 18:52:10+00:00
- **Authors**: Alex Ling Yu Hung, Wanwen Chen, John Galeotti
- **Comment**: 5 pages, conference
- **Journal**: None
- **Summary**: Ultrasound imaging has been improving, but continues to suffer from inherent artifacts that are challenging to model, such as attenuation, shadowing, diffraction, speckle, etc. These artifacts can potentially confuse image analysis algorithms unless an attempt is made to assess the certainty of individual pixel values. Our novel confidence algorithms analyze pixel values using a directed acyclic graph based on acoustic physical properties of ultrasound imaging. We demonstrate unique capabilities of our approach and compare it against previous confidence-measurement algorithms for shadow-detection and image-compounding tasks.



### Towards Imperceptible Universal Attacks on Texture Recognition
- **Arxiv ID**: http://arxiv.org/abs/2011.11957v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.11957v1)
- **Published**: 2020-11-24 08:33:59+00:00
- **Updated**: 2020-11-24 08:33:59+00:00
- **Authors**: Yingpeng Deng, Lina J. Karam
- **Comment**: None
- **Journal**: None
- **Summary**: Although deep neural networks (DNNs) have been shown to be susceptible to image-agnostic adversarial attacks on natural image classification problems, the effects of such attacks on DNN-based texture recognition have yet to be explored. As part of our work, we find that limiting the perturbation's $l_p$ norm in the spatial domain may not be a suitable way to restrict the perceptibility of universal adversarial perturbations for texture images. Based on the fact that human perception is affected by local visual frequency characteristics, we propose a frequency-tuned universal attack method to compute universal perturbations in the frequency domain. Our experiments indicate that our proposed method can produce less perceptible perturbations yet with a similar or higher white-box fooling rates on various DNN texture classifiers and texture datasets as compared to existing universal attack techniques. We also demonstrate that our approach can improve the attack robustness against defended models as well as the cross-dataset transferability for texture recognition problems.



### Weakly- and Semi-Supervised Probabilistic Segmentation and Quantification of Ultrasound Needle-Reverberation Artifacts to Allow Better AI Understanding of Tissue Beneath Needles
- **Arxiv ID**: http://arxiv.org/abs/2011.11958v2
- **DOI**: 10.34133/2022/9837076
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.11958v2)
- **Published**: 2020-11-24 08:34:38+00:00
- **Updated**: 2021-06-03 18:57:04+00:00
- **Authors**: Alex Ling Yu Hung, Edward Chen, John Galeotti
- **Comment**: None
- **Journal**: None
- **Summary**: Ultrasound image quality has continually been improving. However, when needles or other metallic objects are operating inside the tissue, the resulting reverberation artifacts can severely corrupt the surrounding image quality. Such effects are challenging for existing computer vision algorithms for medical image analysis. Needle reverberation artifacts can be hard to identify at times and affect various pixel values to different degrees. The boundaries of such artifacts are ambiguous, leading to disagreement among human experts labeling the artifacts. We propose a weakly- and semi-supervised, probabilistic needle-and-reverberation-artifact segmentation algorithm to separate the desired tissue-based pixel values from the superimposed artifacts. Our method models the intensity decay of artifact intensities and is designed to minimize the human labeling error. We demonstrate the applicability of the approach and compare it against other segmentation algorithms. Our method is capable of differentiating between the reverberations from artifact-free patches as well as of modeling the intensity fall-off in the artifacts. Our method matches state-of-the-art artifact segmentation performance and sets a new standard in estimating the per-pixel contributions of artifact vs underlying anatomy, especially in the immediately adjacent regions between reverberation lines. Our algorithm is also able to improve the performance downstream image analysis algorithms.



### MODNet: Real-Time Trimap-Free Portrait Matting via Objective Decomposition
- **Arxiv ID**: http://arxiv.org/abs/2011.11961v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.11961v4)
- **Published**: 2020-11-24 08:38:36+00:00
- **Updated**: 2022-03-18 04:49:53+00:00
- **Authors**: Zhanghan Ke, Jiayu Sun, Kaican Li, Qiong Yan, Rynson W. H. Lau
- **Comment**: None
- **Journal**: None
- **Summary**: Existing portrait matting methods either require auxiliary inputs that are costly to obtain or involve multiple stages that are computationally expensive, making them less suitable for real-time applications. In this work, we present a light-weight matting objective decomposition network (MODNet) for portrait matting in real-time with a single input image. The key idea behind our efficient design is by optimizing a series of sub-objectives simultaneously via explicit constraints. In addition, MODNet includes two novel techniques for improving model efficiency and robustness. First, an Efficient Atrous Spatial Pyramid Pooling (e-ASPP) module is introduced to fuse multi-scale features for semantic estimation. Second, a self-supervised sub-objectives consistency (SOC) strategy is proposed to adapt MODNet to real-world data to address the domain shift problem common to trimap-free methods. MODNet is easy to be trained in an end-to-end manner. It is much faster than contemporaneous methods and runs at 67 frames per second on a 1080Ti GPU. Experiments show that MODNet outperforms prior trimap-free methods by a large margin on both Adobe Matting Dataset and a carefully designed photographic portrait matting (PPM-100) benchmark proposed by us. Further, MODNet achieves remarkable results on daily photos and videos. Our code and models are available at https://github.com/ZHKKKe/MODNet, and the PPM-100 benchmark is released at https://github.com/ZHKKKe/PPM.



### Good and Bad Boundaries in Ultrasound Compounding: Preserving Anatomic Boundaries While Suppressing Artifacts
- **Arxiv ID**: http://arxiv.org/abs/2011.11962v3
- **DOI**: 10.1007/s11548-021-02464-4
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.11962v3)
- **Published**: 2020-11-24 08:41:51+00:00
- **Updated**: 2021-08-10 15:55:49+00:00
- **Authors**: Alex Ling Yu Hung, John Galeotti
- **Comment**: Int J CARS (2021)
- **Journal**: None
- **Summary**: Ultrasound 3D compounding is important for volumetric reconstruction, but as of yet there is no consensus on best practices for compounding. Ultrasound images depend on probe direction and the path sound waves pass through, so when multiple intersecting B-scans of the same spot from different perspectives yield different pixel values, there is not a single, ideal representation for compounding (i.e. combining) the overlapping pixel values. Current popular methods inevitably suppress or altogether leave out bright or dark regions that are useful, and potentially introduce new artifacts. In this work, we establish a new algorithm to compound the overlapping pixels from different view points in ultrasound. We uniquely leverage Laplacian and Gaussian Pyramids to preserve the maximum boundary contrast without overemphasizing noise and speckle. We evaluate our algorithm by comparing ours with previous algorithms, and we show that our approach not only preserves both light and dark details, but also somewhat suppresses artifacts, rather than amplifying them.



### LiDAR-based Panoptic Segmentation via Dynamic Shifting Network
- **Arxiv ID**: http://arxiv.org/abs/2011.11964v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.11964v2)
- **Published**: 2020-11-24 08:44:46+00:00
- **Updated**: 2020-12-01 05:49:08+00:00
- **Authors**: Fangzhou Hong, Hui Zhou, Xinge Zhu, Hongsheng Li, Ziwei Liu
- **Comment**: Rank 1st place in the leaderboard of SemanticKITTI Panoptic
  Segmentation (accessed at 2020-11-16); Codes at
  https://github.com/hongfz16/DS-Net
- **Journal**: None
- **Summary**: With the rapid advances of autonomous driving, it becomes critical to equip its sensing system with more holistic 3D perception. However, existing works focus on parsing either the objects (e.g. cars and pedestrians) or scenes (e.g. trees and buildings) from the LiDAR sensor. In this work, we address the task of LiDAR-based panoptic segmentation, which aims to parse both objects and scenes in a unified manner. As one of the first endeavors towards this new challenging task, we propose the Dynamic Shifting Network (DS-Net), which serves as an effective panoptic segmentation framework in the point cloud realm. In particular, DS-Net has three appealing properties: 1) strong backbone design. DS-Net adopts the cylinder convolution that is specifically designed for LiDAR point clouds. The extracted features are shared by the semantic branch and the instance branch which operates in a bottom-up clustering style. 2) Dynamic Shifting for complex point distributions. We observe that commonly-used clustering algorithms like BFS or DBSCAN are incapable of handling complex autonomous driving scenes with non-uniform point cloud distributions and varying instance sizes. Thus, we present an efficient learnable clustering module, dynamic shifting, which adapts kernel functions on-the-fly for different instances. 3) Consensus-driven Fusion. Finally, consensus-driven fusion is used to deal with the disagreement between semantic and instance predictions. To comprehensively evaluate the performance of LiDAR-based panoptic segmentation, we construct and curate benchmarks from two large-scale autonomous driving LiDAR datasets, SemanticKITTI and nuScenes. Extensive experiments demonstrate that our proposed DS-Net achieves superior accuracies over current state-of-the-art methods. Notably, we achieve 1st place on the public leaderboard of SemanticKITTI, outperforming 2nd place by 2.6% in terms of the PQ metric.



### UKPGAN: A General Self-Supervised Keypoint Detector
- **Arxiv ID**: http://arxiv.org/abs/2011.11974v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.11974v3)
- **Published**: 2020-11-24 09:08:21+00:00
- **Updated**: 2022-03-09 05:27:26+00:00
- **Authors**: Yang You, Wenhai Liu, Yanjie Ze, Yong-Lu Li, Weiming Wang, Cewu Lu
- **Comment**: Accepted to CVPR2022
- **Journal**: None
- **Summary**: Keypoint detection is an essential component for the object registration and alignment. In this work, we reckon keypoint detection as information compression, and force the model to distill out irrelevant points of an object. Based on this, we propose UKPGAN, a general self-supervised 3D keypoint detector where keypoints are detected so that they could reconstruct the original object shape. Two modules: GAN-based keypoint sparsity control and salient information distillation modules are proposed to locate those important keypoints. Extensive experiments show that our keypoints align well with human annotated keypoint labels, and can be applied to SMPL human bodies under various non-rigid deformations. Furthermore, our keypoint detector trained on clean object collections generalizes well to real-world scenarios, thus further improves geometric registration when combined with off-the-shelf point descriptors. Repeatability experiments show that our model is stable under both rigid and non-rigid transformations, with local reference frame estimation. Our code is available on https://github.com/qq456cvb/UKPGAN.



### Efficient Initial Pose-graph Generation for Global SfM
- **Arxiv ID**: http://arxiv.org/abs/2011.11986v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.11986v2)
- **Published**: 2020-11-24 09:32:03+00:00
- **Updated**: 2020-11-26 16:11:55+00:00
- **Authors**: Daniel Barath, Dmytro Mishkin, Ivan Eichhardt, Ilia Shipachev, Jiri Matas
- **Comment**: Added supplementary material
- **Journal**: None
- **Summary**: We propose ways to speed up the initial pose-graph generation for global Structure-from-Motion algorithms. To avoid forming tentative point correspondences by FLANN and geometric verification by RANSAC, which are the most time-consuming steps of the pose-graph creation, we propose two new methods - built on the fact that image pairs usually are matched consecutively. Thus, candidate relative poses can be recovered from paths in the partly-built pose-graph. We propose a heuristic for the A* traversal, considering global similarity of images and the quality of the pose-graph edges. Given a relative pose from a path, descriptor-based feature matching is made "light-weight" by exploiting the known epipolar geometry. To speed up PROSAC-based sampling when RANSAC is applied, we propose a third method to order the correspondences by their inlier probabilities from previous estimations. The algorithms are tested on 402130 image pairs from the 1DSfM dataset and they speed up the feature matching 17 times and pose estimation 5 times.



### Canonical Voting: Towards Robust Oriented Bounding Box Detection in 3D Scenes
- **Arxiv ID**: http://arxiv.org/abs/2011.12001v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.12001v3)
- **Published**: 2020-11-24 10:03:24+00:00
- **Updated**: 2022-03-09 08:18:00+00:00
- **Authors**: Yang You, Zelin Ye, Yujing Lou, Chengkun Li, Yong-Lu Li, Lizhuang Ma, Weiming Wang, Cewu Lu
- **Comment**: Accepted to CVPR2022
- **Journal**: None
- **Summary**: 3D object detection has attracted much attention thanks to the advances in sensors and deep learning methods for point clouds. Current state-of-the-art methods like VoteNet regress direct offset towards object centers and box orientations with an additional Multi-Layer-Perceptron network. Both their offset and orientation predictions are not accurate due to the fundamental difficulty in rotation classification. In the work, we disentangle the direct offset into Local Canonical Coordinates (LCC), box scales and box orientations. Only LCC and box scales are regressed, while box orientations are generated by a canonical voting scheme. Finally, an LCC-aware back-projection checking algorithm iteratively cuts out bounding boxes from the generated vote maps, with the elimination of false positives. Our model achieves state-of-the-art performance on three standard real-world benchmarks: ScanNet, SceneNN and SUN RGB-D. Our code is available on https://github.com/qq456cvb/CanonicalVoting.



### KShapeNet: Riemannian network on Kendall shape space for Skeleton based Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2011.12004v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.12004v1)
- **Published**: 2020-11-24 10:14:07+00:00
- **Updated**: 2020-11-24 10:14:07+00:00
- **Authors**: Racha Friji, Hassen Drira, Faten Chaieb, Sebastian Kurtek, Hamza Kchok
- **Comment**: None
- **Journal**: None
- **Summary**: Deep Learning architectures, albeit successful in most computer vision tasks, were designed for data with an underlying Euclidean structure, which is not usually fulfilled since pre-processed data may lie on a non-linear space. In this paper, we propose a geometry aware deep learning approach for skeleton-based action recognition. Skeleton sequences are first modeled as trajectories on Kendall's shape space and then mapped to the linear tangent space. The resulting structured data are then fed to a deep learning architecture, which includes a layer that optimizes over rigid and non rigid transformations of the 3D skeletons, followed by a CNN-LSTM network. The assessment on two large scale skeleton datasets, namely NTU-RGB+D and NTU-RGB+D 120, has proven that proposed approach outperforms existing geometric deep learning methods and is competitive with respect to recently published approaches.



### RIN: Textured Human Model Recovery and Imitation with a Single Image
- **Arxiv ID**: http://arxiv.org/abs/2011.12024v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2011.12024v4)
- **Published**: 2020-11-24 11:04:35+00:00
- **Updated**: 2021-08-14 12:19:46+00:00
- **Authors**: Haoxi Ran, Guangfu Wang, Li Lu
- **Comment**: The experiments is incompleted
- **Journal**: None
- **Summary**: Human imitation has become topical recently, driven by GAN's ability to disentangle human pose and body content. However, the latest methods hardly focus on 3D information, and to avoid self-occlusion, a massive amount of input images are needed. In this paper, we propose RIN, a novel volume-based framework for reconstructing a textured 3D model from a single picture and imitating a subject with the generated model. Specifically, to estimate most of the human texture, we propose a U-Net-like front-to-back translation network. With both front and back images input, the textured volume recovery module allows us to color a volumetric human. A sequence of 3D poses then guides the colored volume via Flowable Disentangle Networks as a volume-to-volume translation task. To project volumes to a 2D plane during training, we design a differentiable depth-aware renderer. Our experiments demonstrate that our volume-based model is adequate for human imitation, and the back view can be estimated reliably using our network. While prior works based on either 2D pose or semantic map often fail for the unstable appearance of a human, our framework can still produce concrete results, which are competitive to those imagined from multi-view input.



### SegBlocks: Block-Based Dynamic Resolution Networks for Real-Time Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2011.12025v2
- **DOI**: 10.1109/TPAMI.2022.3162528
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.12025v2)
- **Published**: 2020-11-24 11:05:07+00:00
- **Updated**: 2022-08-05 15:46:02+00:00
- **Authors**: Thomas Verelst, Tinne Tuytelaars
- **Comment**: long version, 12 pages. Accepted to IEEE Transactions on Pattern
  Analysis and Machine Intelligence
- **Journal**: None
- **Summary**: SegBlocks reduces the computational cost of existing neural networks, by dynamically adjusting the processing resolution of image regions based on their complexity. Our method splits an image into blocks and downsamples blocks of low complexity, reducing the number of operations and memory consumption. A lightweight policy network, selecting the complex regions, is trained using reinforcement learning. In addition, we introduce several modules implemented in CUDA to process images in blocks. Most important, our novel BlockPad module prevents the feature discontinuities at block borders of which existing methods suffer, while keeping memory consumption under control. Our experiments on Cityscapes, Camvid and Mapillary Vistas datasets for semantic segmentation show that dynamically processing images offers a better accuracy versus complexity trade-off compared to static baselines of similar complexity. For instance, our method reduces the number of floating-point operations of SwiftNet-RN18 by 60% and increases the inference speed by 50%, with only 0.3% decrease in mIoU accuracy on Cityscapes.



### Adversarial Generation of Continuous Images
- **Arxiv ID**: http://arxiv.org/abs/2011.12026v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.12026v2)
- **Published**: 2020-11-24 11:06:40+00:00
- **Updated**: 2021-06-28 09:00:05+00:00
- **Authors**: Ivan Skorokhodov, Savva Ignatyev, Mohamed Elhoseiny
- **Comment**: 19 pages, 17 figures
- **Journal**: None
- **Summary**: In most existing learning systems, images are typically viewed as 2D pixel arrays. However, in another paradigm gaining popularity, a 2D image is represented as an implicit neural representation (INR) - an MLP that predicts an RGB pixel value given its (x,y) coordinate. In this paper, we propose two novel architectural techniques for building INR-based image decoders: factorized multiplicative modulation and multi-scale INRs, and use them to build a state-of-the-art continuous image GAN. Previous attempts to adapt INRs for image generation were limited to MNIST-like datasets and do not scale to complex real-world data. Our proposed INR-GAN architecture improves the performance of continuous image generators by several times, greatly reducing the gap between continuous image GANs and pixel-based ones. Apart from that, we explore several exciting properties of the INR-based decoders, like out-of-the-box superresolution, meaningful image-space interpolation, accelerated inference of low-resolution images, an ability to extrapolate outside of image boundaries, and strong geometric prior. The project page is located at https://universome.github.io/inr-gan.



### Revisiting Pixel-Wise Supervision for Face Anti-Spoofing
- **Arxiv ID**: http://arxiv.org/abs/2011.12032v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.12032v1)
- **Published**: 2020-11-24 11:25:58+00:00
- **Updated**: 2020-11-24 11:25:58+00:00
- **Authors**: Zitong Yu, Xiaobai Li, Jingang Shi, Zhaoqiang Xia, Guoying Zhao
- **Comment**: submitted to IEEE Transactions on Biometrics, Behavior and Identity
  Science
- **Journal**: None
- **Summary**: Face anti-spoofing (FAS) plays a vital role in securing face recognition systems from the presentation attacks (PAs). As more and more realistic PAs with novel types spring up, it is necessary to develop robust algorithms for detecting unknown attacks even in unseen scenarios. However, deep models supervised by traditional binary loss (e.g., `0' for bonafide vs. `1' for PAs) are weak in describing intrinsic and discriminative spoofing patterns. Recently, pixel-wise supervision has been proposed for the FAS task, intending to provide more fine-grained pixel/patch-level cues. In this paper, we firstly give a comprehensive review and analysis about the existing pixel-wise supervision methods for FAS. Then we propose a novel pyramid supervision, which guides deep models to learn both local details and global semantics from multi-scale spatial context. Extensive experiments are performed on five FAS benchmark datasets to show that, without bells and whistles, the proposed pyramid supervision could not only improve the performance beyond existing pixel-wise supervision frameworks, but also enhance the model's interpretability (i.e., locating the patch-level positions of PAs more reasonably). Furthermore, elaborate studies are conducted for exploring the efficacy of different architecture configurations with two kinds of pixel-wise supervisions (binary mask and depth map supervisions), which provides inspirable insights for future architecture/supervision design.



### Infrared small target detection based on isotropic constraint under complex background
- **Arxiv ID**: http://arxiv.org/abs/2011.12059v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2011.12059v1)
- **Published**: 2020-11-24 12:25:05+00:00
- **Updated**: 2020-11-24 12:25:05+00:00
- **Authors**: Fan Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Infrared search and tracking (IRST) system has been widely concerned and applied in the area of national defence. Small target detection under complex background is a very challenging task in the development of system algorithm. Low signal-to-clutter ratio (SCR) of target and the interference caused by irregular background clutter make it difficult to get an accurate result. In this paper, small targets are considered to have two characteristics of high contrast and isotropy, and we propose a multilayer gray difference (MGD) method constrained by isotropy. Firstly, the suspected regions are obtained through MGD, and then the eigenvalues of the original image's Hessian matrix are calculated to obtain the isotropy parameter of each region. Finally, those regions do not meet the isotropic constraint condition are suppressed. Experiments show that the proposed method is effective and superior to several common methods in terms of signal-to-clutter ratio gain (SCRG) and receiver operating characteristic (ROC) curve.



### CLAWS: Clustering Assisted Weakly Supervised Learning with Normalcy Suppression for Anomalous Event Detection
- **Arxiv ID**: http://arxiv.org/abs/2011.12077v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2011.12077v4)
- **Published**: 2020-11-24 13:27:40+00:00
- **Updated**: 2021-08-04 08:14:06+00:00
- **Authors**: Muhammad Zaigham Zaheer, Arif Mahmood, Marcella Astrid, Seung-Ik Lee
- **Comment**: Presented in the European Conference on Computer Vision ECCV 2020.
  (Changes from actual paper: 1) Recently published methods have been added in
  ShanghaiTech and UCF Crime comparison tabs. 2) Due to some error in arxiv
  compilation, few references are exceeding the paragraph. Also, word
  'normalcy' in the title is misspelling despite being correct in the code.
  (Contents are intact)
- **Journal**: None
- **Summary**: Learning to detect real-world anomalous events through video-level labels is a challenging task due to the rare occurrence of anomalies as well as noise in the labels. In this work, we propose a weakly supervised anomaly detection method which has manifold contributions including1) a random batch based training procedure to reduce inter-batch correlation, 2) a normalcy suppression mechanism to minimize anomaly scores of the normal regions of a video by taking into account the overall information available in one training batch, and 3) a clustering distance based loss to contribute towards mitigating the label noise and to produce better anomaly representations by encouraging our model to generate distinct normal and anomalous clusters. The proposed method obtains83.03% and 89.67% frame-level AUC performance on the UCF Crime and ShanghaiTech datasets respectively, demonstrating its superiority over the existing state-of-the-art algorithms.



### Multi-Features Guidance Network for partial-to-partial point cloud registration
- **Arxiv ID**: http://arxiv.org/abs/2011.12079v2
- **DOI**: 10.1007/s00521-021-06464-y
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.12079v2)
- **Published**: 2020-11-24 13:31:58+00:00
- **Updated**: 2021-09-10 03:29:28+00:00
- **Authors**: Hongyuan Wang, Xiang Liu, Wen Kang, Zhiqiang Yan, Bingwen Wang, Qianhao Ning
- **Comment**: preprint version, the final version is accepted by Neural Computing
  and Applications and is available online at
  https://doi.org/10.1007/s00521-021-06464-y
- **Journal**: None
- **Summary**: To eliminate the problems of large dimensional differences, big semantic gap, and mutual interference caused by hybrid features, in this paper, we propose a novel Multi-Features Guidance Network for partial-to-partial point cloud registration(MFG). The proposed network mainly includes four parts: keypoints' feature extraction, correspondences searching, correspondences credibility computation, and SVD, among which correspondences searching and correspondence credibility computation are the cores of the network. Unlike the previous work, we utilize the shape features and the spatial coordinates to guide correspondences search independently and fusing the matching results to obtain the final matching matrix. In the correspondences credibility computation module, based on the conflicted relationship between the features matching matrix and the coordinates matching matrix, we score the reliability for each correspondence, which can reduce the impact of mismatched or non-matched points. Experimental results show that our network outperforms the current state-of-the-art while maintaining computational efficiency.



### Computational efficient deep neural network with difference attention maps for facial action unit detection
- **Arxiv ID**: http://arxiv.org/abs/2011.12082v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.12082v2)
- **Published**: 2020-11-24 13:34:58+00:00
- **Updated**: 2020-11-27 03:23:50+00:00
- **Authors**: Jing Chen, Chenhui Wang, Kejun Wang, Meichen Liu
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a computational efficient end-to-end training deep neural network (CEDNN) model and spatial attention maps based on difference images. Firstly, the difference image is generated by image processing. Then five binary images of difference images are obtained using different thresholds, which are used as spatial attention maps. We use group convolution to reduce model complexity. Skip connection and $\text{1}\times \text{1}$ convolution are used to ensure good performance even if the network model is not deep. As an input, spatial attention map can be selectively fed into the input of each block. The feature maps tend to focus on the parts that are related to the target task better. In addition, we only need to adjust the parameters of classifier to train different numbers of AU. It can be easily extended to varying datasets without increasing too much computation. A large number of experimental results show that the proposed CEDNN is obviously better than the traditional deep learning method on DISFA+ and CK+ datasets. After adding spatial attention maps, the result is better than the most advanced AU detection method. At the same time, the scale of the network is small, the running speed is fast, and the requirement for experimental equipment is low.



### SEA: Sentence Encoder Assembly for Video Retrieval by Textual Queries
- **Arxiv ID**: http://arxiv.org/abs/2011.12091v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2011.12091v1)
- **Published**: 2020-11-24 13:54:28+00:00
- **Updated**: 2020-11-24 13:54:28+00:00
- **Authors**: Xirong Li, Fangming Zhou, Chaoxi Xu, Jiaqi Ji, Gang Yang
- **Comment**: accepted for publication as a REGULAR paper in the IEEE Transactions
  on Multimedia
- **Journal**: None
- **Summary**: Retrieving unlabeled videos by textual queries, known as Ad-hoc Video Search (AVS), is a core theme in multimedia data management and retrieval. The success of AVS counts on cross-modal representation learning that encodes both query sentences and videos into common spaces for semantic similarity computation. Inspired by the initial success of previously few works in combining multiple sentence encoders, this paper takes a step forward by developing a new and general method for effectively exploiting diverse sentence encoders. The novelty of the proposed method, which we term Sentence Encoder Assembly (SEA), is two-fold. First, different from prior art that use only a single common space, SEA supports text-video matching in multiple encoder-specific common spaces. Such a property prevents the matching from being dominated by a specific encoder that produces an encoding vector much longer than other encoders. Second, in order to explore complementarities among the individual common spaces, we propose multi-space multi-loss learning. As extensive experiments on four benchmarks (MSR-VTT, TRECVID AVS 2016-2019, TGIF and MSVD) show, SEA surpasses the state-of-the-art. In addition, SEA is extremely ease to implement. All this makes SEA an appealing solution for AVS and promising for continuously advancing the task by harvesting new sentence encoders.



### Learning to Sample the Most Useful Training Patches from Images
- **Arxiv ID**: http://arxiv.org/abs/2011.12097v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2011.12097v1)
- **Published**: 2020-11-24 14:06:50+00:00
- **Updated**: 2020-11-24 14:06:50+00:00
- **Authors**: Shuyang Sun, Liang Chen, Gregory Slabaugh, Philip Torr
- **Comment**: None
- **Journal**: None
- **Summary**: Some image restoration tasks like demosaicing require difficult training samples to learn effective models. Existing methods attempt to address this data training problem by manually collecting a new training dataset that contains adequate hard samples, however, there are still hard and simple areas even within one single image. In this paper, we present a data-driven approach called PatchNet that learns to select the most useful patches from an image to construct a new training set instead of manual or random selection. We show that our simple idea automatically selects informative samples out from a large-scale dataset, leading to a surprising 2.35dB generalisation gain in terms of PSNR. In addition to its remarkable effectiveness, PatchNet is also resource-friendly as it is applied only during training and therefore does not require any additional computational cost during inference.



### GIRAFFE: Representing Scenes as Compositional Generative Neural Feature Fields
- **Arxiv ID**: http://arxiv.org/abs/2011.12100v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.12100v2)
- **Published**: 2020-11-24 14:14:15+00:00
- **Updated**: 2021-04-29 14:46:36+00:00
- **Authors**: Michael Niemeyer, Andreas Geiger
- **Comment**: Accepted to CVPR 2021 (oral). Project page:
  http://bit.ly/giraffe-project
- **Journal**: None
- **Summary**: Deep generative models allow for photorealistic image synthesis at high resolutions. But for many applications, this is not enough: content creation also needs to be controllable. While several recent works investigate how to disentangle underlying factors of variation in the data, most of them operate in 2D and hence ignore that our world is three-dimensional. Further, only few works consider the compositional nature of scenes. Our key hypothesis is that incorporating a compositional 3D scene representation into the generative model leads to more controllable image synthesis. Representing scenes as compositional generative neural feature fields allows us to disentangle one or multiple objects from the background as well as individual objects' shapes and appearances while learning from unstructured and unposed image collections without any additional supervision. Combining this scene representation with a neural rendering pipeline yields a fast and realistic image synthesis model. As evidenced by our experiments, our model is able to disentangle individual objects and allows for translating and rotating them in the scene as well as changing the camera pose.



### Do You Live a Healthy Life? Analyzing Lifestyle by Visual Life Logging
- **Arxiv ID**: http://arxiv.org/abs/2011.12102v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.12102v1)
- **Published**: 2020-11-24 14:20:12+00:00
- **Updated**: 2020-11-24 14:20:12+00:00
- **Authors**: Qing Gao, Mingtao Pei, Hongyu Shen
- **Comment**: None
- **Journal**: None
- **Summary**: A healthy lifestyle is the key to better health and happiness and has a considerable effect on quality of life and disease prevention. Current lifelogging/egocentric datasets are not suitable for lifestyle analysis; consequently, there is no research on lifestyle analysis in the field of computer vision. In this work, we investigate the problem of lifestyle analysis and build a visual lifelogging dataset for lifestyle analysis (VLDLA). The VLDLA contains images captured by a wearable camera every 3 seconds from 8:00 am to 6:00 pm for seven days. In contrast to current lifelogging/egocentric datasets, our dataset is suitable for lifestyle analysis as images are taken with short intervals to capture activities of short duration; moreover, images are taken continuously from morning to evening to record all the activities performed by a user. Based on our dataset, we classify the user activities in each frame and use three latent fluents of the user, which change over time and are associated with activities, to measure the healthy degree of the user's lifestyle. The scores for the three latent fluents are computed based on recognized activities, and the healthy degree of the lifestyle for the day is determined based on the scores for the latent fluents. Experimental results show that our method can be used to analyze the healthiness of users' lifestyles.



### Recurrent Multi-view Alignment Network for Unsupervised Surface Registration
- **Arxiv ID**: http://arxiv.org/abs/2011.12104v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2011.12104v2)
- **Published**: 2020-11-24 14:22:42+00:00
- **Updated**: 2021-04-13 08:07:17+00:00
- **Authors**: Wanquan Feng, Juyong Zhang, Hongrui Cai, Haofei Xu, Junhui Hou, Hujun Bao
- **Comment**: Accepted to CVPR2021. The source codes are available at
  https://github.com/WanquanF/RMA-Net
- **Journal**: None
- **Summary**: Learning non-rigid registration in an end-to-end manner is challenging due to the inherent high degrees of freedom and the lack of labeled training data. In this paper, we resolve these two challenges simultaneously. First, we propose to represent the non-rigid transformation with a point-wise combination of several rigid transformations. This representation not only makes the solution space well-constrained but also enables our method to be solved iteratively with a recurrent framework, which greatly reduces the difficulty of learning. Second, we introduce a differentiable loss function that measures the 3D shape similarity on the projected multi-view 2D depth images so that our full framework can be trained end-to-end without ground truth supervision. Extensive experiments on several different datasets demonstrate that our proposed method outperforms the previous state-of-the-art by a large margin. The source codes are available at https://github.com/WanquanF/RMA-Net.



### Learning of Long-Horizon Sparse-Reward Robotic Manipulator Tasks with Base Controllers
- **Arxiv ID**: http://arxiv.org/abs/2011.12105v3
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.12105v3)
- **Published**: 2020-11-24 14:23:57+00:00
- **Updated**: 2021-12-04 05:01:24+00:00
- **Authors**: Guangming Wang, Minjian Xin, Wenhua Wu, Zhe Liu, Hesheng Wang
- **Comment**: 10 pages, 6 figures, under review
- **Journal**: None
- **Summary**: Deep Reinforcement Learning (DRL) enables robots to perform some intelligent tasks end-to-end. However, there are still many challenges for long-horizon sparse-reward robotic manipulator tasks. On the one hand, a sparse-reward setting causes exploration inefficient. On the other hand, exploration using physical robots is of high cost and unsafe. In this paper, we propose a method of learning long-horizon sparse-reward tasks utilizing one or more existing traditional controllers named base controllers in this paper. Built upon Deep Deterministic Policy Gradients (DDPG), our algorithm incorporates the existing base controllers into stages of exploration, value learning, and policy update. Furthermore, we present a straightforward way of synthesizing different base controllers to integrate their strengths. Through experiments ranging from stacking blocks to cups, it is demonstrated that the learned state-based or image-based policies steadily outperform base controllers. Compared to previous works of learning from demonstrations, our method improves sample efficiency by orders of magnitude and improves the performance. Overall, our method bears the potential of leveraging existing industrial robot manipulation systems to build more flexible and intelligent controllers.



### SpinNet: Learning a General Surface Descriptor for 3D Point Cloud Registration
- **Arxiv ID**: http://arxiv.org/abs/2011.12149v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2011.12149v2)
- **Published**: 2020-11-24 15:00:56+00:00
- **Updated**: 2021-04-09 16:42:19+00:00
- **Authors**: Sheng Ao, Qingyong Hu, Bo Yang, Andrew Markham, Yulan Guo
- **Comment**: None
- **Journal**: None
- **Summary**: Extracting robust and general 3D local features is key to downstream tasks such as point cloud registration and reconstruction. Existing learning-based local descriptors are either sensitive to rotation transformations, or rely on classical handcrafted features which are neither general nor representative. In this paper, we introduce a new, yet conceptually simple, neural architecture, termed SpinNet, to extract local features which are rotationally invariant whilst sufficiently informative to enable accurate registration. A Spatial Point Transformer is first introduced to map the input local surface into a carefully designed cylindrical space, enabling end-to-end optimization with SO(2) equivariant representation. A Neural Feature Extractor which leverages the powerful point-based and 3D cylindrical convolutional neural layers is then utilized to derive a compact and representative descriptor for matching. Extensive experiments on both indoor and outdoor datasets demonstrate that SpinNet outperforms existing state-of-the-art techniques by a large margin. More critically, it has the best generalization ability across unseen scenarios with different sensor modalities. The code is available at https://github.com/QingyongHu/SpinNet.



### VIGOR: Cross-View Image Geo-localization beyond One-to-one Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2011.12172v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2011.12172v2)
- **Published**: 2020-11-24 15:50:54+00:00
- **Updated**: 2021-03-22 04:01:54+00:00
- **Authors**: Sijie Zhu, Taojiannan Yang, Chen Chen
- **Comment**: CVPR 2021
- **Journal**: None
- **Summary**: Cross-view image geo-localization aims to determine the locations of street-view query images by matching with GPS-tagged reference images from aerial view. Recent works have achieved surprisingly high retrieval accuracy on city-scale datasets. However, these results rely on the assumption that there exists a reference image exactly centered at the location of any query image, which is not applicable for practical scenarios. In this paper, we redefine this problem with a more realistic assumption that the query image can be arbitrary in the area of interest and the reference images are captured before the queries emerge. This assumption breaks the one-to-one retrieval setting of existing datasets as the queries and reference images are not perfectly aligned pairs, and there may be multiple reference images covering one query location. To bridge the gap between this realistic setting and existing datasets, we propose a new large-scale benchmark -- VIGOR -- for cross-View Image Geo-localization beyond One-to-one Retrieval. We benchmark existing state-of-the-art methods and propose a novel end-to-end framework to localize the query in a coarse-to-fine manner. Apart from the image-level retrieval accuracy, we also evaluate the localization accuracy in terms of the actual distance (meters) using the raw GPS data. Extensive experiments are conducted under different application scenarios to validate the effectiveness of the proposed method. The results indicate that cross-view geo-localization in this realistic setting is still challenging, fostering new research in this direction. Our dataset and code will be released at \url{https://github.com/Jeff-Zilence/VIGOR}



### Discovering Hidden Physics Behind Transport Dynamics
- **Arxiv ID**: http://arxiv.org/abs/2011.12222v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.12222v2)
- **Published**: 2020-11-24 17:19:25+00:00
- **Updated**: 2021-03-29 14:21:08+00:00
- **Authors**: Peirong Liu, Lin Tian, Yubo Zhang, Stephen R. Aylward, Yueh Z. Lee, Marc Niethammer
- **Comment**: Accepted as ORAL at CVPR 2021 (20 pages, 13 figures)
- **Journal**: None
- **Summary**: Transport processes are ubiquitous. They are, for example, at the heart of optical flow approaches; or of perfusion imaging, where blood transport is assessed, most commonly by injecting a tracer. An advection-diffusion equation is widely used to describe these transport phenomena. Our goal is estimating the underlying physics of advection-diffusion equations, expressed as velocity and diffusion tensor fields. We propose a learning framework (YETI) building on an auto-encoder structure between 2D and 3D image time-series, which incorporates the advection-diffusion model. To help with identifiability, we develop an advection-diffusion simulator which allows pre-training of our model by supervised learning using the velocity and diffusion tensor fields. Instead of directly learning these velocity and diffusion tensor fields, we introduce representations that assure incompressible flow and symmetric positive semi-definite diffusion fields and demonstrate the additional benefits of these representations on improving estimation accuracy. We further use transfer learning to apply YETI on a public brain magnetic resonance (MR) perfusion dataset of stroke patients and show its ability to successfully distinguish stroke lesions from normal brain regions via the estimated velocity and diffusion tensor fields.



### Multi-Stage CNN-Based Monocular 3D Vehicle Localization and Orientation Estimation
- **Arxiv ID**: http://arxiv.org/abs/2011.12256v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2011.12256v1)
- **Published**: 2020-11-24 18:01:57+00:00
- **Updated**: 2020-11-24 18:01:57+00:00
- **Authors**: Ali Babolhavaeji, Mohammad Fanaei
- **Comment**: None
- **Journal**: None
- **Summary**: This paper aims to design a 3D object detection model from 2D images taken by monocular cameras by combining the estimated bird's-eye view elevation map and the deep representation of object features. The proposed model has a pre-trained ResNet-50 network as its backend network and three more branches. The model first builds a bird's-eye view elevation map to estimate the depth of the object in the scene and by using that estimates the object's 3D bounding boxes. We have trained and evaluate it on two major datasets: a syntactic dataset and the KIITI dataset.



### Is First Person Vision Challenging for Object Tracking?
- **Arxiv ID**: http://arxiv.org/abs/2011.12263v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.12263v2)
- **Published**: 2020-11-24 18:18:15+00:00
- **Updated**: 2021-09-24 15:14:19+00:00
- **Authors**: Matteo Dunnhofer, Antonino Furnari, Giovanni Maria Farinella, Christian Micheloni
- **Comment**: Extended Abstract accepted by the EPIC workshop at ICCV 2021. The
  full version of this paper is available at arXiv:2108.13665
- **Journal**: None
- **Summary**: Understanding human-object interactions is fundamental in First Person Vision (FPV). Tracking algorithms which follow the objects manipulated by the camera wearer can provide useful cues to effectively model such interactions. Despite a few previous attempts to exploit trackers in FPV applications, a methodical analysis of the performance of state-of-the-art visual trackers in this domain is still missing. In this short paper, we provide a recap of the first systematic study of object tracking in FPV. Our work extensively analyses the performance of recent and baseline FPV trackers with respect to different aspects. This is achieved through TREK-150, a novel benchmark dataset composed of 150 densely annotated video sequences. The results suggest that more research efforts should be devoted to this problem so that tracking could benefit FPV tasks. The full version of this paper is available at arXiv:2108.13665.



### A Framework for Fluid Motion Estimation using a Constraint-Based Refinement Approach
- **Arxiv ID**: http://arxiv.org/abs/2011.12267v3
- **DOI**: None
- **Categories**: **cs.CV**, math.AP, 35J50, 35Q68
- **Links**: [PDF](http://arxiv.org/pdf/2011.12267v3)
- **Published**: 2020-11-24 18:23:39+00:00
- **Updated**: 2023-06-21 01:04:15+00:00
- **Authors**: Hirak Doshi, N. Uday Kiran
- **Comment**: None
- **Journal**: None
- **Summary**: Physics-based optical flow models have been successful in capturing the deformities in fluid motion arising from digital imagery. However, a common theoretical framework analyzing several physics-based models is missing. In this regard, we formulate a general framework for fluid motion estimation using a constraint-based refinement approach. We demonstrate that for a particular choice of constraint, our results closely approximate the classical continuity equation-based method for fluid flow. This closeness is theoretically justified by augmented Lagrangian method in a novel way. The convergence of Uzawa iterates is shown using a modified bounded constraint algorithm. The mathematical well-posedness is studied in a Hilbert space setting. Further, we observe a surprising connection to the Cauchy-Riemann operator that diagonalizes the system leading to a diffusive phenomenon involving the divergence and the curl of the flow. Several numerical experiments are performed and the results are shown on different datasets. Additionally, we demonstrate that a flow-driven refinement process involving the curl of the flow outperforms the classical physics-based optical flow method without any additional assumptions on the image data.



### Insights From A Large-Scale Database of Material Depictions In Paintings
- **Arxiv ID**: http://arxiv.org/abs/2011.12276v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.12276v1)
- **Published**: 2020-11-24 18:42:58+00:00
- **Updated**: 2020-11-24 18:42:58+00:00
- **Authors**: Hubert Lin, Mitchell Van Zuijlen, Maarten W. A. Wijntjes, Sylvia C. Pont, Kavita Bala
- **Comment**: International Workshop on Fine Art Pattern Extraction and
  Recognition, ICPR 2020
- **Journal**: None
- **Summary**: Deep learning has paved the way for strong recognition systems which are often both trained on and applied to natural images. In this paper, we examine the give-and-take relationship between such visual recognition systems and the rich information available in the fine arts. First, we find that visual recognition systems designed for natural images can work surprisingly well on paintings. In particular, we find that interactive segmentation tools can be used to cleanly annotate polygonal segments within paintings, a task which is time consuming to undertake by hand. We also find that FasterRCNN, a model which has been designed for object recognition in natural scenes, can be quickly repurposed for detection of materials in paintings. Second, we show that learning from paintings can be beneficial for neural networks that are intended to be used on natural images. We find that training on paintings instead of natural images can improve the quality of learned features and we further find that a large number of paintings can be a valuable source of test data for evaluating domain adaptation algorithms. Our experiments are based on a novel large-scale annotated database of material depictions in paintings which we detail in a separate manuscript.



### MicroNet: Towards Image Recognition with Extremely Low FLOPs
- **Arxiv ID**: http://arxiv.org/abs/2011.12289v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.12289v1)
- **Published**: 2020-11-24 18:59:39+00:00
- **Updated**: 2020-11-24 18:59:39+00:00
- **Authors**: Yunsheng Li, Yinpeng Chen, Xiyang Dai, Dongdong Chen, Mengchen Liu, Lu Yuan, Zicheng Liu, Lei Zhang, Nuno Vasconcelos
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present MicroNet, which is an efficient convolutional neural network using extremely low computational cost (e.g. 6 MFLOPs on ImageNet classification). Such a low cost network is highly desired on edge devices, yet usually suffers from a significant performance degradation. We handle the extremely low FLOPs based upon two design principles: (a) avoiding the reduction of network width by lowering the node connectivity, and (b) compensating for the reduction of network depth by introducing more complex non-linearity per layer. Firstly, we propose Micro-Factorized convolution to factorize both pointwise and depthwise convolutions into low rank matrices for a good tradeoff between the number of channels and input/output connectivity. Secondly, we propose a new activation function, named Dynamic Shift-Max, to improve the non-linearity via maxing out multiple dynamic fusions between an input feature map and its circular channel shift. The fusions are dynamic as their parameters are adapted to the input. Building upon Micro-Factorized convolution and dynamic Shift-Max, a family of MicroNets achieve a significant performance gain over the state-of-the-art in the low FLOP regime. For instance, MicroNet-M1 achieves 61.1% top-1 accuracy on ImageNet classification with 12 MFLOPs, outperforming MobileNetV3 by 11.3%.



### Play Fair: Frame Attributions in Video Models
- **Arxiv ID**: http://arxiv.org/abs/2011.12372v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.12372v1)
- **Published**: 2020-11-24 20:45:29+00:00
- **Updated**: 2020-11-24 20:45:29+00:00
- **Authors**: Will Price, Dima Damen
- **Comment**: Code available at: https://github.com/willprice/play-fair/ and
  supporting website at: https://play-fair.willprice.dev/
- **Journal**: None
- **Summary**: In this paper, we introduce an attribution method for explaining action recognition models. Such models fuse information from multiple frames within a video, through score aggregation or relational reasoning. We break down a model's class score into the sum of contributions from each frame, fairly. Our method adapts an axiomatic solution to fair reward distribution in cooperative games, known as the Shapley value, for elements in a variable-length sequence, which we call the Element Shapley Value (ESV). Critically, we propose a tractable approximation of ESV that scales linearly with the number of frames in the sequence. We employ ESV to explain two action recognition models (TRN and TSN) on the fine-grained dataset Something-Something. We offer detailed analysis of supporting/distracting frames, and the relationships of ESVs to the frame's position, class prediction, and sequence length. We compare ESV to naive baselines and two commonly used feature attribution methods: Grad-CAM and Integrated-Gradients.



### A3D: Adaptive 3D Networks for Video Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2011.12384v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2011.12384v1)
- **Published**: 2020-11-24 21:01:11+00:00
- **Updated**: 2020-11-24 21:01:11+00:00
- **Authors**: Sijie Zhu, Taojiannan Yang, Matias Mendieta, Chen Chen
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents A3D, an adaptive 3D network that can infer at a wide range of computational constraints with one-time training. Instead of training multiple models in a grid-search manner, it generates good configurations by trading off between network width and spatio-temporal resolution. Furthermore, the computation cost can be adapted after the model is deployed to meet variable constraints, for example, on edge devices. Even under the same computational constraints, the performance of our adaptive networks can be significantly boosted over the baseline counterparts by the mutual training along three dimensions. When a multiple pathway framework, e.g. SlowFast, is adopted, our adaptive method encourages a better trade-off between pathways than manual designs. Extensive experiments on the Kinetics dataset show the effectiveness of the proposed framework. The performance gain is also verified to transfer well between datasets and tasks. Code will be made available.



### Distribution Conditional Denoising: A Flexible Discriminative Image Denoiser
- **Arxiv ID**: http://arxiv.org/abs/2011.12398v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.12398v1)
- **Published**: 2020-11-24 21:27:18+00:00
- **Updated**: 2020-11-24 21:27:18+00:00
- **Authors**: Anthony Kelly
- **Comment**: 10 pages, 8 figures, 4 tables
- **Journal**: None
- **Summary**: A flexible discriminative image denoiser is introduced in which multi-task learning methods are applied to a densoising FCN based on U-Net. The activations of the U-Net model are modified by affine transforms that are a learned function of conditioning inputs. The learning procedure for multiple noise types and levels involves applying a distribution of noise parameters during training to the conditioning inputs, with the same noise parameters applied to a noise generating layer at the input (similar to the approach taken in a denoising autoencoder). It is shown that this flexible denoising model achieves state of the art performance on images corrupted with Gaussian and Poisson noise. It has also been shown that this conditional training method can generalise a fixed noise level U-Net denoiser to a variety of noise levels.



### Online Domain Adaptation for Continuous Cross-Subject Liver Viability Evaluation Based on Irregular Thermal Data
- **Arxiv ID**: http://arxiv.org/abs/2011.12408v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.12408v1)
- **Published**: 2020-11-24 21:42:19+00:00
- **Updated**: 2020-11-24 21:42:19+00:00
- **Authors**: Sahand Hajifar, Hongyue Sun
- **Comment**: None
- **Journal**: None
- **Summary**: Accurate evaluation of liver viability during its procurement is a challenging issue and has traditionally been addressed by taking invasive biopsy on liver. Recently, people have started to investigate on the non-invasive evaluation of liver viability during its procurement using the liver surface thermal images. However, existing works include the background noise in the thermal images and do not consider the cross-subject heterogeneity of livers, thus the viability evaluation accuracy can be affected. In this paper, we propose to use the irregular thermal data of the pure liver region, and the cross-subject liver evaluation information (i.e., the available viability label information in cross-subject livers), for the real-time evaluation of a new liver's viability. To achieve this objective, we extract features of irregular thermal data based on tools from graph signal processing (GSP), and propose an online domain adaptation (DA) and classification framework using the GSP features of cross-subject livers. A multiconvex block coordinate descent based algorithm is designed to jointly learn the domain-invariant features during online DA and learn the classifier. Our proposed framework is applied to the liver procurement data, and classifies the liver viability accurately.



### Stochastic sparse adversarial attacks
- **Arxiv ID**: http://arxiv.org/abs/2011.12423v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.12423v4)
- **Published**: 2020-11-24 22:07:51+00:00
- **Updated**: 2022-02-19 16:37:56+00:00
- **Authors**: Manon Césaire, Lucas Schott, Hatem Hajri, Sylvain Lamprier, Patrick Gallinari
- **Comment**: Final version published at the ICTAI 2021 conference with a best
  student paper award. Codes are available through the link:
  https://github.com/hhajri/stochastic-sparse-adv-attacks
- **Journal**: None
- **Summary**: This paper introduces stochastic sparse adversarial attacks (SSAA), standing as simple, fast and purely noise-based targeted and untargeted attacks of neural network classifiers (NNC). SSAA offer new examples of sparse (or $L_0$) attacks for which only few methods have been proposed previously. These attacks are devised by exploiting a small-time expansion idea widely used for Markov processes. Experiments on small and large datasets (CIFAR-10 and ImageNet) illustrate several advantages of SSAA in comparison with the-state-of-the-art methods. For instance, in the untargeted case, our method called Voting Folded Gaussian Attack (VFGA) scales efficiently to ImageNet and achieves a significantly lower $L_0$ score than SparseFool (up to $\frac{2}{5}$) while being faster. Moreover, VFGA achieves better $L_0$ scores on ImageNet than Sparse-RS when both attacks are fully successful on a large number of samples.



### Towards Zero-shot Cross-lingual Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2012.05107v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2012.05107v1)
- **Published**: 2020-11-24 22:13:21+00:00
- **Updated**: 2020-11-24 22:13:21+00:00
- **Authors**: Pranav Aggarwal, Ajinkya Kale
- **Comment**: None
- **Journal**: None
- **Summary**: There has been a recent spike in interest in multi-modal Language and Vision problems. On the language side, most of these models primarily focus on English since most multi-modal datasets are monolingual. We try to bridge this gap with a zero-shot approach for learning multi-modal representations using cross-lingual pre-training on the text side. We present a simple yet practical approach for building a cross-lingual image retrieval model which trains on a monolingual training dataset but can be used in a zero-shot cross-lingual fashion during inference. We also introduce a new objective function which tightens the text embedding clusters by pushing dissimilar texts from each other. Finally, we introduce a new 1K multi-lingual MSCOCO2014 caption test dataset (XTD10) in 7 languages that we collected using a crowdsourcing platform. We use this as the test set for evaluating zero-shot model performance across languages. XTD10 dataset is made publicly available here: https://github.com/adobe-research/Cross-lingual-Test-Dataset-XTD10



### A New Periocular Dataset Collected by Mobile Devices in Unconstrained Scenarios
- **Arxiv ID**: http://arxiv.org/abs/2011.12427v2
- **DOI**: 10.1038/s41598-022-22811-y
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.12427v2)
- **Published**: 2020-11-24 22:20:37+00:00
- **Updated**: 2022-11-14 22:34:16+00:00
- **Authors**: Luiz A. Zanlorensi, Rayson Laroca, Diego R. Lucio, Lucas R. Santos, Alceu S. Britto Jr., David Menotti
- **Comment**: None
- **Journal**: Scientific Reports, vol. 12, p. 17989, 2022
- **Summary**: Recently, ocular biometrics in unconstrained environments using images obtained at visible wavelength have gained the researchers' attention, especially with images captured by mobile devices. Periocular recognition has been demonstrated to be an alternative when the iris trait is not available due to occlusions or low image resolution. However, the periocular trait does not have the high uniqueness presented in the iris trait. Thus, the use of datasets containing many subjects is essential to assess biometric systems' capacity to extract discriminating information from the periocular region. Also, to address the within-class variability caused by lighting and attributes in the periocular region, it is of paramount importance to use datasets with images of the same subject captured in distinct sessions. As the datasets available in the literature do not present all these factors, in this work, we present a new periocular dataset containing samples from 1,122 subjects, acquired in 3 sessions by 196 different mobile devices. The images were captured under unconstrained environments with just a single instruction to the participants: to place their eyes on a region of interest. We also performed an extensive benchmark with several Convolutional Neural Network (CNN) architectures and models that have been employed in state-of-the-art approaches based on Multi-class Classification, Multitask Learning, Pairwise Filters Network, and Siamese Network. The results achieved in the closed- and open-world protocol, considering the identification and verification tasks, show that this area still needs research and development.



### Impact of Power Supply Noise on Image Sensor Performance in Automotive Applications
- **Arxiv ID**: http://arxiv.org/abs/2012.03666v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.AR, cs.CV, cs.RO, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2012.03666v1)
- **Published**: 2020-11-24 22:25:30+00:00
- **Updated**: 2020-11-24 22:25:30+00:00
- **Authors**: Shane Gilroy
- **Comment**: None
- **Journal**: Waterford Institute of Technology 2016
- **Summary**: Vision Systems are quickly becoming a large component of Active Automotive Safety Systems. In order to be effective in critical safety applications these systems must produce high quality images in both daytime and night-time scenarios in order to provide the large informational content required for software analysis in applications such as lane departure, pedestrian detection and collision detection. The challenge in capturing high quality images in low light scenarios is that the signal to noise ratio is greatly reduced, which can result in noise becoming the dominant factor in a captured image, thereby making these safety systems less effective at night. Research has been undertaken to develop a systematic method of characterising image sensor performance in response to electrical noise in order to improve the design and performance of automotive cameras in low light scenarios. The root cause of image row noise has been established and a mathematical algorithm for determining the magnitude of row noise in an image has been devised. An automated characterisation method has been developed to allow performance characterisation in response to a large frequency spectrum of electrical noise on the image sensor power supply. Various strategies of improving image sensor performance for low light applications have also been proposed from the research outcomes.



### Assessing Post-Disaster Damage from Satellite Imagery using Semi-Supervised Learning Techniques
- **Arxiv ID**: http://arxiv.org/abs/2011.14004v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, I.2.10; I.2.1; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/2011.14004v1)
- **Published**: 2020-11-24 22:26:14+00:00
- **Updated**: 2020-11-24 22:26:14+00:00
- **Authors**: Jihyeon Lee, Joseph Z. Xu, Kihyuk Sohn, Wenhan Lu, David Berthelot, Izzeddin Gur, Pranav Khaitan, Ke-Wei, Huang, Kyriacos Koupparis, Bernhard Kowatsch
- **Comment**: NeurIPS 2020 Artificial Intelligence for Humanitarian Assistance and
  Disaster Response Workshop
- **Journal**: None
- **Summary**: To respond to disasters such as earthquakes, wildfires, and armed conflicts, humanitarian organizations require accurate and timely data in the form of damage assessments, which indicate what buildings and population centers have been most affected. Recent research combines machine learning with remote sensing to automatically extract such information from satellite imagery, reducing manual labor and turn-around time. A major impediment to using machine learning methods in real disaster response scenarios is the difficulty of obtaining a sufficient amount of labeled data to train a model for an unfolding disaster. This paper shows a novel application of semi-supervised learning (SSL) to train models for damage assessment with a minimal amount of labeled data and large amount of unlabeled data. We compare the performance of state-of-the-art SSL methods, including MixMatch and FixMatch, to a supervised baseline for the 2010 Haiti earthquake, 2017 Santa Rosa wildfire, and 2016 armed conflict in Syria. We show how models trained with SSL methods can reach fully supervised performance despite using only a fraction of labeled data and identify areas for further improvements.



### Fully Automated Mitral Inflow Doppler Analysis Using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2011.12429v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.12429v1)
- **Published**: 2020-11-24 22:27:14+00:00
- **Updated**: 2020-11-24 22:27:14+00:00
- **Authors**: Mohamed Y. Elwazir, Zeynettin Akkus, Didem Oguz, Jae K. Oh
- **Comment**: None
- **Journal**: IEEE BIBE 2020 Proceedings
- **Summary**: Echocardiography (echo) is an indispensable tool in a cardiologist's diagnostic armamentarium. To date, almost all echocardiographic parameters require time-consuming manual labeling and measurements by an experienced echocardiographer and exhibit significant variability, owing to the noisy and artifact-laden nature of echo images. For example, mitral inflow (MI) Doppler is used to assess left ventricular (LV) diastolic function, which is of paramount clinical importance to distinguish between different cardiac diseases. In the current work we present a fully automated workflow which leverages deep learning to a) label MI Doppler images acquired in an echo study, b) detect the envelope of MI Doppler signal, c) extract early and late filing (E and A wave) flow velocities and E-wave deceleration time from the envelope. We trained a variety of convolutional neural networks (CNN) models on 5544 images of 140 patients for predicting 24 image classes including MI Doppler images and obtained overall accuracy of 0.97 on 1737 images of 40 patients. Automated E and A wave velocity showed excellent correlation (Pearson R 0.99 and 0.98 respectively) and Bland Altman agreement (mean difference 0.06 and 0.05 m/s respectively and SD 0.03 for both) with the operator measurements. Deceleration time also showed good but lower correlation (Pearson R 0.82) and Bland-Altman agreement (mean difference: 34.1ms, SD: 30.9ms). These results demonstrate feasibility of Doppler echocardiography measurement automation and the promise of a fully automated echocardiography measurement package.



### SOE-Net: A Self-Attention and Orientation Encoding Network for Point Cloud based Place Recognition
- **Arxiv ID**: http://arxiv.org/abs/2011.12430v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.12430v2)
- **Published**: 2020-11-24 22:28:25+00:00
- **Updated**: 2021-05-23 11:37:49+00:00
- **Authors**: Yan Xia, Yusheng Xu, Shuang Li, Rui Wang, Juan Du, Daniel Cremers, Uwe Stilla
- **Comment**: Accepted by CVPR2021 (Oral)
- **Journal**: None
- **Summary**: We tackle the problem of place recognition from point cloud data and introduce a self-attention and orientation encoding network (SOE-Net) that fully explores the relationship between points and incorporates long-range context into point-wise local descriptors. Local information of each point from eight orientations is captured in a PointOE module, whereas long-range feature dependencies among local descriptors are captured with a self-attention unit. Moreover, we propose a novel loss function called Hard Positive Hard Negative quadruplet loss (HPHN quadruplet), that achieves better performance than the commonly used metric learning loss. Experiments on various benchmark datasets demonstrate superior performance of the proposed network over the current state-of-the-art approaches. Our code is released publicly at https://github.com/Yan-Xia/SOE-Net.



### Characterisation of CMOS Image Sensor Performance in Low Light Automotive Applications
- **Arxiv ID**: http://arxiv.org/abs/2011.12436v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.12436v1)
- **Published**: 2020-11-24 22:49:54+00:00
- **Updated**: 2020-11-24 22:49:54+00:00
- **Authors**: Shane Gilroy, John O'Dwyer, Lucas Bortoleto
- **Comment**: None
- **Journal**: Irish Machine Vision and Image Processing Conference Proceedings
  2017
- **Summary**: The applications of automotive cameras in Advanced Driver-Assistance Systems (ADAS) are growing rapidly as automotive manufacturers strive to provide 360 degree protection for their customers. Vision systems must capture high quality images in both daytime and night-time scenarios in order to produce the large informational content required for software analysis in applications such as lane departure, pedestrian detection and collision detection. The challenge in producing high quality images in low light scenarios is that the signal to noise ratio is greatly reduced. This can result in noise becoming the dominant factor in a captured image thereby making these safety systems less effective at night. This paper outlines a systematic method for characterisation of state of the art image sensor performance in response to noise, so as to improve the design and performance of automotive cameras in low light scenarios. The experiment outlined in this paper demonstrates how this method can be used to characterise the performance of CMOS image sensors in response to electrical noise on the power supply lines.



### DeepShadows: Separating Low Surface Brightness Galaxies from Artifacts using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2011.12437v1
- **DOI**: None
- **Categories**: **astro-ph.GA**, astro-ph.CO, astro-ph.IM, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.12437v1)
- **Published**: 2020-11-24 22:51:08+00:00
- **Updated**: 2020-11-24 22:51:08+00:00
- **Authors**: Dimitrios Tanoglidis, Aleksandra Ćiprijanović, Alex Drlica-Wagner
- **Comment**: 22 pages, 11 figures. Code and data related to this work can be found
  at: https://github.com/dtanoglidis/DeepShadows
- **Journal**: None
- **Summary**: Searches for low-surface-brightness galaxies (LSBGs) in galaxy surveys are plagued by the presence of a large number of artifacts (e.g., objects blended in the diffuse light from stars and galaxies, Galactic cirrus, star-forming regions in the arms of spiral galaxies, etc.) that have to be rejected through time consuming visual inspection. In future surveys, which are expected to collect hundreds of petabytes of data and detect billions of objects, such an approach will not be feasible. We investigate the use of convolutional neural networks (CNNs) for the problem of separating LSBGs from artifacts in survey images. We take advantage of the fact that, for the first time, we have available a large number of labeled LSBGs and artifacts from the Dark Energy Survey, that we use to train, validate, and test a CNN model. That model, which we call DeepShadows, achieves a test accuracy of $92.0 \%$, a significant improvement relative to feature-based machine learning models. We also study the ability to use transfer learning to adapt this model to classify objects from the deeper Hyper-Suprime-Cam survey, and we show that after the model is retrained on a very small sample from the new survey, it can reach an accuracy of $87.6\%$. These results demonstrate that CNNs offer a very promising path in the quest to study the low-surface-brightness universe.



### Continuous Surface Embeddings
- **Arxiv ID**: http://arxiv.org/abs/2011.12438v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.12438v1)
- **Published**: 2020-11-24 22:52:15+00:00
- **Updated**: 2020-11-24 22:52:15+00:00
- **Authors**: Natalia Neverova, David Novotny, Vasil Khalidov, Marc Szafraniec, Patrick Labatut, Andrea Vedaldi
- **Comment**: NeurIPS, 2020
- **Journal**: None
- **Summary**: In this work, we focus on the task of learning and representing dense correspondences in deformable object categories. While this problem has been considered before, solutions so far have been rather ad-hoc for specific object types (i.e., humans), often with significant manual work involved. However, scaling the geometry understanding to all objects in nature requires more automated approaches that can also express correspondences between related, but geometrically different objects. To this end, we propose a new, learnable image-based representation of dense correspondences. Our model predicts, for each pixel in a 2D image, an embedding vector of the corresponding vertex in the object mesh, therefore establishing dense correspondences between image pixels and 3D object geometry. We demonstrate that the proposed approach performs on par or better than the state-of-the-art methods for dense pose estimation for humans, while being conceptually simpler. We also collect a new in-the-wild dataset of dense correspondences for animal classes and demonstrate that our framework scales naturally to the new deformable object categories.



### Building 3D Morphable Models from a Single Scan
- **Arxiv ID**: http://arxiv.org/abs/2011.12440v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2011.12440v2)
- **Published**: 2020-11-24 23:08:14+00:00
- **Updated**: 2021-09-30 18:16:32+00:00
- **Authors**: Skylar Sutherland, Bernhard Egger, Joshua Tenenbaum
- **Comment**: ICCV Workshops: 1st Workshop on Traditional Computer Vision in the
  Age of Deep Learning (TradiCV)
- **Journal**: None
- **Summary**: We propose a method for constructing generative models of 3D objects from a single 3D mesh. Our method produces a 3D morphable model that represents shape and albedo in terms of Gaussian processes. We define the shape deformations in physical (3D) space and the albedo deformations as a combination of physical-space and color-space deformations. Whereas previous approaches have typically built 3D morphable models from multiple high-quality 3D scans through principal component analysis, we build 3D morphable models from a single scan or template. As we demonstrate in the face domain, these models can be used to infer 3D reconstructions from 2D data (inverse graphics) or 3D data (registration). Specifically, we show that our approach can be used to perform face recognition using only a single 3D scan (one scan total, not one per person), and further demonstrate how multiple scans can be incorporated to improve performance without requiring dense correspondence. Our approach enables the synthesis of 3D morphable models for 3D object categories where dense correspondence between multiple scans is unavailable. We demonstrate this by constructing additional 3D morphable models for fish and birds and use them to perform simple inverse rendering tasks. We share the code used to generate these models and to perform our inverse rendering and registration experiments.



### Independent Sign Language Recognition with 3D Body, Hands, and Face Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2012.05698v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.05698v1)
- **Published**: 2020-11-24 23:50:26+00:00
- **Updated**: 2020-11-24 23:50:26+00:00
- **Authors**: Agelos Kratimenos, Georgios Pavlakos, Petros Maragos
- **Comment**: Submitted to ICASSP 2021
- **Journal**: None
- **Summary**: Independent Sign Language Recognition is a complex visual recognition problem that combines several challenging tasks of Computer Vision due to the necessity to exploit and fuse information from hand gestures, body features and facial expressions. While many state-of-the-art works have managed to deeply elaborate on these features independently, to the best of our knowledge, no work has adequately combined all three information channels to efficiently recognize Sign Language. In this work, we employ SMPL-X, a contemporary parametric model that enables joint extraction of 3D body shape, face and hands information from a single image. We use this holistic 3D reconstruction for SLR, demonstrating that it leads to higher accuracy than recognition from raw RGB images and their optical flow fed into the state-of-the-art I3D-type network for 3D action recognition and from 2D Openpose skeletons fed into a Recurrent Neural Network. Finally, a set of experiments on the body, face and hand features showed that neglecting any of these, significantly reduces the classification accuracy, proving the importance of jointly modeling body shape, facial expression and hand pose for Sign Language Recognition.



