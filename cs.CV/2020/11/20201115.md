# Arxiv Papers in cs.CV on 2020-11-15
### Accounting for Affect in Pain Level Recognition
- **Arxiv ID**: http://arxiv.org/abs/2011.07421v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.07421v1)
- **Published**: 2020-11-15 00:23:31+00:00
- **Updated**: 2020-11-15 00:23:31+00:00
- **Authors**: Md Taufeeq Uddin, Shaun Canavan, Ghada Zamzmi
- **Comment**: Machine Learning for Health (ML4H) at NeurIPS 2020 - Extended
  Abstract
- **Journal**: None
- **Summary**: In this work, we address the importance of affect in automated pain assessment and the implications in real-world settings. To achieve this, we curate a new physiological dataset by merging the publicly available bioVid pain and emotion datasets. We then investigate pain level recognition on this dataset simulating participants' naturalistic affective behaviors. Our findings demonstrate that acknowledging affect in pain assessment is essential. We observe degradation in recognition performance when simulating the existence of affect to validate pain assessment models that do not account for it. Conversely, we observe a performance boost in recognition when we account for affect.



### Pollen Grain Microscopic Image Classification Using an Ensemble of Fine-Tuned Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2011.07428v1
- **DOI**: 10.1007/978-3-030-68763-2_26
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.07428v1)
- **Published**: 2020-11-15 01:25:46+00:00
- **Updated**: 2020-11-15 01:25:46+00:00
- **Authors**: Amirreza Mahbod, Gerald Schaefer, Rupert Ecker, Isabella Ellinger
- **Comment**: Accepted for the Artificial Intelligence for Healthcare Applications
  workshop at the 25th International Conference on Pattern Recognition (ICPR
  2020)
- **Journal**: None
- **Summary**: Pollen grain micrograph classification has multiple applications in medicine and biology. Automatic pollen grain image classification can alleviate the problems of manual categorisation such as subjectivity and time constraints. While a number of computer-based methods have been introduced in the literature to perform this task, classification performance needs to be improved for these methods to be useful in practice.   In this paper, we present an ensemble approach for pollen grain microscopic image classification into four categories: Corylus Avellana well-developed pollen grain, Corylus Avellana anomalous pollen grain, Alnus well-developed pollen grain, and non-pollen (debris) instances. In our approach, we develop a classification strategy that is based on fusion of four state-of-the-art fine-tuned convolutional neural networks, namely EfficientNetB0, EfficientNetB1, EfficientNetB2 and SeResNeXt-50 deep models. These models are trained with images of three fixed sizes (224x224, 240x240, and 260x260 pixels) and their prediction probability vectors are then fused in an ensemble method to form a final classification vector for a given pollen grain image.   Our proposed method is shown to yield excellent classification performance, obtaining an accuracy of of 94.48% and a weighted F1-score of 94.54% on the ICPR 2020 Pollen Grain Classification Challenge training dataset based on five-fold cross-validation. Evaluated on the test set of the challenge, our approach achieved a very competitive performance in comparison to the top ranked approaches with an accuracy and a weighted F1-score of 96.28% and 96.30%, respectively.



### Audio-Visual Event Recognition through the lens of Adversary
- **Arxiv ID**: http://arxiv.org/abs/2011.07430v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2011.07430v1)
- **Published**: 2020-11-15 01:36:09+00:00
- **Updated**: 2020-11-15 01:36:09+00:00
- **Authors**: Juncheng B Li, Kaixin Ma, Shuhui Qu, Po-Yao Huang, Florian Metze
- **Comment**: 4 pages
- **Journal**: None
- **Summary**: As audio/visual classification models are widely deployed for sensitive tasks like content filtering at scale, it is critical to understand their robustness along with improving the accuracy. This work aims to study several key questions related to multimodal learning through the lens of adversarial noises: 1) The trade-off between early/middle/late fusion affecting its robustness and accuracy 2) How do different frequency/time domain features contribute to the robustness? 3) How do different neural modules contribute to the adversarial noise? In our experiment, we construct adversarial examples to attack state-of-the-art neural models trained on Google AudioSet. We compare how much attack potency in terms of adversarial perturbation of size $\epsilon$ using different $L_p$ norms we would need to "deactivate" the victim model. Using adversarial noise to ablate multimodal models, we are able to provide insights into what is the best potential fusion strategy to balance the model parameters/accuracy and robustness trade-off and distinguish the robust features versus the non-robust features that various neural networks model tend to learn.



### Binary Segmentation of Seismic Facies Using Encoder-Decoder Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2012.03675v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2012.03675v1)
- **Published**: 2020-11-15 01:36:52+00:00
- **Updated**: 2020-11-15 01:36:52+00:00
- **Authors**: Gefersom Lima, Gabriel Ramos, Sandro Rigo, Felipe Zeiser, Ariane da Silveira
- **Comment**: None
- **Journal**: None
- **Summary**: The interpretation of seismic data is vital for characterizing sediments' shape in areas of geological study. In seismic interpretation, deep learning becomes useful for reducing the dependence on handcrafted facies segmentation geometry and the time required to study geological areas. This work presents a Deep Neural Network for Facies Segmentation (DNFS) to obtain state-of-the-art results for seismic facies segmentation. DNFS is trained using a combination of cross-entropy and Jaccard loss functions. Our results show that DNFS obtains highly detailed predictions for seismic facies segmentation using fewer parameters than StNet and U-Net.



### Enhance Gender and Identity Preservation in Face Aging Simulation for Infants and Toddlers
- **Arxiv ID**: http://arxiv.org/abs/2011.07431v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.07431v1)
- **Published**: 2020-11-15 01:40:36+00:00
- **Updated**: 2020-11-15 01:40:36+00:00
- **Authors**: Yao Xiao, Yijun Zhao
- **Comment**: 8 pages, 2 figures
- **Journal**: None
- **Summary**: Realistic age-progressed photos provide invaluable biometric information in a wide range of applications. In recent years, deep learning-based approaches have made remarkable progress in modeling the aging process of the human face. Nevertheless, it remains a challenging task to generate accurate age-progressed faces from infant or toddler photos. In particular, the lack of visually detectable gender characteristics and the drastic appearance changes in early life contribute to the difficulty of the task. We propose a new deep learning method inspired by the successful Conditional Adversarial Autoencoder (CAAE, 2017) model. In our approach, we extend the CAAE architecture to 1) incorporate gender information, and 2) augment the model's overall architecture with an identity-preserving component based on facial features. We trained our model using the publicly available UTKFace dataset and evaluated our model by simulating up to 100 years of aging on 1,156 male and 1,207 female infant and toddler face photos. Compared to the CAAE approach, our new model demonstrates noticeable visual improvements. Quantitatively, our model exhibits an overall gain of 77.0% (male) and 13.8% (female) in gender fidelity measured by a gender classifier for the simulated photos across the age spectrum. Our model also demonstrates a 22.4% gain in identity preservation measured by a facial recognition neural network.



### Online Ensemble Model Compression using Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/2011.07449v1
- **DOI**: 10.1007/978-3-030-58529-7_2
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.07449v1)
- **Published**: 2020-11-15 04:46:29+00:00
- **Updated**: 2020-11-15 04:46:29+00:00
- **Authors**: Devesh Walawalkar, Zhiqiang Shen, Marios Savvides
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a novel knowledge distillation based model compression framework consisting of a student ensemble. It enables distillation of simultaneously learnt ensemble knowledge onto each of the compressed student models. Each model learns unique representations from the data distribution due to its distinct architecture. This helps the ensemble generalize better by combining every model's knowledge. The distilled students and ensemble teacher are trained simultaneously without requiring any pretrained weights. Moreover, our proposed method can deliver multi-compressed students with single training, which is efficient and flexible for different scenarios. We provide comprehensive experiments using state-of-the-art classification models to validate our framework's effectiveness. Notably, using our framework a 97% compressed ResNet110 student model managed to produce a 10.64% relative accuracy gain over its individual baseline training on CIFAR100 dataset. Similarly a 95% compressed DenseNet-BC(k=12) model managed a 8.17% relative accuracy gain.



### Debiasing Convolutional Neural Networks via Meta Orthogonalization
- **Arxiv ID**: http://arxiv.org/abs/2011.07453v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/2011.07453v1)
- **Published**: 2020-11-15 05:13:22+00:00
- **Updated**: 2020-11-15 05:13:22+00:00
- **Authors**: Kurtis Evan David, Qiang Liu, Ruth Fong
- **Comment**: Accepted to NeuRIPS 2020 Workshop on Algorithmic Fairness through the
  Lens of Causality and Interpretability (AFCI). Supplemental materials
  provided at:
  https://drive.google.com/drive/folders/1klIAqZDgg3sCVmzFjLw5Y_T-GTc2E3oh?usp=sharing
- **Journal**: None
- **Summary**: While deep learning models often achieve strong task performance, their successes are hampered by their inability to disentangle spurious correlations from causative factors, such as when they use protected attributes (e.g., race, gender, etc.) to make decisions. In this work, we tackle the problem of debiasing convolutional neural networks (CNNs) in such instances. Building off of existing work on debiasing word embeddings and model interpretability, our Meta Orthogonalization method encourages the CNN representations of different concepts (e.g., gender and class labels) to be orthogonal to one another in activation space while maintaining strong downstream task performance. Through a variety of experiments, we systematically test our method and demonstrate that it significantly mitigates model bias and is competitive against current adversarial debiasing methods.



### Direct Classification of Emotional Intensity
- **Arxiv ID**: http://arxiv.org/abs/2011.07460v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, I.4.8; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2011.07460v1)
- **Published**: 2020-11-15 06:32:48+00:00
- **Updated**: 2020-11-15 06:32:48+00:00
- **Authors**: Jacob Ouyang, Isaac R Galatzer-Levy, Vidya Koesmahargyo, Li Zhang
- **Comment**: 7 pages, 6 figures
- **Journal**: None
- **Summary**: In this paper, we present a model that can directly predict emotion intensity score from video inputs, instead of deriving from action units. Using a 3d DNN incorporated with dynamic emotion information, we train a model using videos of different people smiling that outputs an intensity score from 0-10. Each video is labeled framewise using a normalized action-unit based intensity score. Our model then employs an adaptive learning technique to improve performance when dealing with new subjects. Compared to other models, our model excels in generalization between different people as well as provides a new framework to directly classify emotional intensity.



### Shared Cross-Modal Trajectory Prediction for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2011.08436v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2011.08436v2)
- **Published**: 2020-11-15 07:18:50+00:00
- **Updated**: 2021-03-27 09:05:58+00:00
- **Authors**: Chiho Choi, Joon Hee Choi, Jiachen Li, Srikanth Malla
- **Comment**: CVPR 2021 [Oral]
- **Journal**: None
- **Summary**: Predicting future trajectories of traffic agents in highly interactive environments is an essential and challenging problem for the safe operation of autonomous driving systems. On the basis of the fact that self-driving vehicles are equipped with various types of sensors (e.g., LiDAR scanner, RGB camera, radar, etc.), we propose a Cross-Modal Embedding framework that aims to benefit from the use of multiple input modalities. At training time, our model learns to embed a set of complementary features in a shared latent space by jointly optimizing the objective functions across different types of input data. At test time, a single input modality (e.g., LiDAR data) is required to generate predictions from the input perspective (i.e., in the LiDAR space), while taking advantages from the model trained with multiple sensor modalities. An extensive evaluation is conducted to show the efficacy of the proposed framework using two benchmark driving datasets.



### Continuous Conditional Generative Adversarial Networks: Novel Empirical Losses and Label Input Mechanisms
- **Arxiv ID**: http://arxiv.org/abs/2011.07466v8
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2011.07466v8)
- **Published**: 2020-11-15 07:29:41+00:00
- **Updated**: 2022-12-11 04:18:58+00:00
- **Authors**: Xin Ding, Yongwei Wang, Zuheng Xu, William J. Welch, Z. Jane Wang
- **Comment**: Accepted by IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE
  INTELLIGENCE
- **Journal**: None
- **Summary**: This work proposes the continuous conditional generative adversarial network (CcGAN), the first generative model for image generation conditional on continuous, scalar conditions (termed regression labels). Existing conditional GANs (cGANs) are mainly designed for categorical conditions (eg, class labels); conditioning on regression labels is mathematically distinct and raises two fundamental problems:(P1) Since there may be very few (even zero) real images for some regression labels, minimizing existing empirical versions of cGAN losses (aka empirical cGAN losses) often fails in practice;(P2) Since regression labels are scalar and infinitely many, conventional label input methods are not applicable. The proposed CcGAN solves the above problems, respectively, by (S1) reformulating existing empirical cGAN losses to be appropriate for the continuous scenario; and (S2) proposing a naive label input (NLI) method and an improved label input (ILI) method to incorporate regression labels into the generator and the discriminator. The reformulation in (S1) leads to two novel empirical discriminator losses, termed the hard vicinal discriminator loss (HVDL) and the soft vicinal discriminator loss (SVDL) respectively, and a novel empirical generator loss. The error bounds of a discriminator trained with HVDL and SVDL are derived under mild assumptions in this work. Two new benchmark datasets (RC-49 and Cell-200) and a novel evaluation metric (Sliding Fr\'echet Inception Distance) are also proposed for this continuous scenario. Our experiments on the Circular 2-D Gaussians, RC-49, UTKFace, Cell-200, and Steering Angle datasets show that CcGAN is able to generate diverse, high-quality samples from the image distribution conditional on a given regression label. Moreover, in these experiments, CcGAN substantially outperforms cGAN both visually and quantitatively.



### Towards Trainable Saliency Maps in Medical Imaging
- **Arxiv ID**: http://arxiv.org/abs/2011.07482v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2011.07482v1)
- **Published**: 2020-11-15 09:01:55+00:00
- **Updated**: 2020-11-15 09:01:55+00:00
- **Authors**: Mehak Aggarwal, Nishanth Arun, Sharut Gupta, Ashwin Vaswani, Bryan Chen, Matthew Li, Ken Chang, Jay Patel, Katherine Hoebel, Mishka Gidwani, Jayashree Kalpathy-Cramer, Praveer Singh
- **Comment**: Machine Learning for Health (ML4H) at NeurIPS 2020 - Extended
  Abstract
- **Journal**: None
- **Summary**: While success of Deep Learning (DL) in automated diagnosis can be transformative to the medicinal practice especially for people with little or no access to doctors, its widespread acceptability is severely limited by inherent black-box decision making and unsafe failure modes. While saliency methods attempt to tackle this problem in non-medical contexts, their apriori explanations do not transfer well to medical usecases. With this study we validate a model design element agnostic to both architecture complexity and model task, and show how introducing this element gives an inherently self-explanatory model. We compare our results with state of the art non-trainable saliency maps on RSNA Pneumonia Dataset and demonstrate a much higher localization efficacy using our adopted technique. We also compare, with a fully supervised baseline and provide a reasonable alternative to it's high data labelling overhead. We further investigate the validity of our claims through qualitative evaluation from an expert reader.



### Anomaly Detection in Video via Self-Supervised and Multi-Task Learning
- **Arxiv ID**: http://arxiv.org/abs/2011.07491v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2011.07491v3)
- **Published**: 2020-11-15 10:21:28+00:00
- **Updated**: 2021-09-10 18:05:19+00:00
- **Authors**: Mariana-Iuliana Georgescu, Antonio Barbalau, Radu Tudor Ionescu, Fahad Shahbaz Khan, Marius Popescu, Mubarak Shah
- **Comment**: Accepted at CVPR 2021. Main paper and supplementary are both included
- **Journal**: None
- **Summary**: Anomaly detection in video is a challenging computer vision problem. Due to the lack of anomalous events at training time, anomaly detection requires the design of learning methods without full supervision. In this paper, we approach anomalous event detection in video through self-supervised and multi-task learning at the object level. We first utilize a pre-trained detector to detect objects. Then, we train a 3D convolutional neural network to produce discriminative anomaly-specific information by jointly learning multiple proxy tasks: three self-supervised and one based on knowledge distillation. The self-supervised tasks are: (i) discrimination of forward/backward moving objects (arrow of time), (ii) discrimination of objects in consecutive/intermittent frames (motion irregularity) and (iii) reconstruction of object-specific appearance information. The knowledge distillation task takes into account both classification and detection information, generating large prediction discrepancies between teacher and student models when anomalies occur. To the best of our knowledge, we are the first to approach anomalous event detection in video as a multi-task learning problem, integrating multiple self-supervised and knowledge distillation proxy tasks in a single architecture. Our lightweight architecture outperforms the state-of-the-art methods on three benchmarks: Avenue, ShanghaiTech and UCSD Ped2. Additionally, we perform an ablation study demonstrating the importance of integrating self-supervised learning and normality-specific distillation in a multi-task learning setting.



### BanglaWriting: A multi-purpose offline Bangla handwriting dataset
- **Arxiv ID**: http://arxiv.org/abs/2011.07499v3
- **DOI**: 10.1016/j.dib.2020.106633
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.07499v3)
- **Published**: 2020-11-15 11:08:53+00:00
- **Updated**: 2022-08-19 14:06:08+00:00
- **Authors**: M. F. Mridha, Abu Quwsar Ohi, M. Ameer Ali, Mazedul Islam Emon, Muhammad Mohsin Kabir
- **Comment**: Accepted in journal Data in Brief. The dataset is available on
  https://data.mendeley.com/datasets/r43wkvdk4w/
- **Journal**: None
- **Summary**: This article presents a Bangla handwriting dataset named BanglaWriting that contains single-page handwritings of 260 individuals of different personalities and ages. Each page includes bounding-boxes that bounds each word, along with the unicode representation of the writing. This dataset contains 21,234 words and 32,787 characters in total. Moreover, this dataset includes 5,470 unique words of Bangla vocabulary. Apart from the usual words, the dataset comprises 261 comprehensible overwriting and 450 handwritten strikes and mistakes. All of the bounding-boxes and word labels are manually-generated. The dataset can be used for complex optical character/word recognition, writer identification, handwritten word segmentation, and word generation. Furthermore, this dataset is suitable for extracting age-based and gender-based variation of handwriting.



### AmphibianDetector: adaptive computation for moving objects detection
- **Arxiv ID**: http://arxiv.org/abs/2011.07513v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.07513v2)
- **Published**: 2020-11-15 12:37:44+00:00
- **Updated**: 2020-12-25 09:09:54+00:00
- **Authors**: David Svitov, Sergey Alyamkin
- **Comment**: 12 pages, 6 figures, 1 table
- **Journal**: None
- **Summary**: Convolutional neural networks (CNN) allow achieving the highest accuracy for the task of object detection in images. Major challenges in further development of object detectors are false-positive detections and high demand of processing power. In this paper, we propose an approach to object detection which makes it possible to reduce the number of false-positive detections by processing only moving objects and reduce the required processing power for algorithm inference. The proposed approach is a modification of CNN already trained for object detection task. This method can be used to improve the accuracy of an existing system by applying minor changes to the algorithm. The efficiency of the proposed approach was demonstrated on the open dataset "CDNet2014 pedestrian". The implementation of the method proposed in the article is available on the GitHub: https://github.com/david-svitov/AmphibianDetector



### Data-efficient Alignment of Multimodal Sequences by Aligning Gradient Updates and Internal Feature Distributions
- **Arxiv ID**: http://arxiv.org/abs/2011.07517v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.07517v1)
- **Published**: 2020-11-15 13:04:25+00:00
- **Updated**: 2020-11-15 13:04:25+00:00
- **Authors**: Jianan Wang, Boyang Li, Xiangyu Fan, Jing Lin, Yanwei Fu
- **Comment**: This paper is accepted to WACV2021
- **Journal**: None
- **Summary**: The task of video and text sequence alignment is a prerequisite step toward joint understanding of movie videos and screenplays. However, supervised methods face the obstacle of limited realistic training data. With this paper, we attempt to enhance data efficiency of the end-to-end alignment network NeuMATCH [15]. Recent research [56] suggests that network components dealing with different modalities may overfit and generalize at different speeds, creating difficulties for training. We propose to employ (1) layer-wise adaptive rate scaling (LARS) to align the magnitudes of gradient updates in different layers and balance the pace of learning and (2) sequence-wise batch normalization (SBN) to align the internal feature distributions from different modalities. Finally, we leverage random projection to reduce the dimensionality of input features. On the YouTube Movie Summary dataset, the combined use of these technique closes the performance gap when the pretraining on the LSMDC dataset is omitted and achieves the state-of-the-art result. Extensive empirical comparisons and analysis reveal that these techniques improve optimization and regularize the network more effectively than two different setups of layer normalization.



### Domain Adaptation Gaze Estimation by Embedding with Prediction Consistency
- **Arxiv ID**: http://arxiv.org/abs/2011.07526v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.07526v1)
- **Published**: 2020-11-15 13:33:43+00:00
- **Updated**: 2020-11-15 13:33:43+00:00
- **Authors**: Zidong Guo, Zejian Yuan, Chong Zhang, Wanchao Chi, Yonggen Ling, Shenghao Zhang
- **Comment**: 16 pages, 6 figures, ACCV 2020 (oral)
- **Journal**: None
- **Summary**: Gaze is the essential manifestation of human attention. In recent years, a series of work has achieved high accuracy in gaze estimation. However, the inter-personal difference limits the reduction of the subject-independent gaze estimation error. This paper proposes an unsupervised method for domain adaptation gaze estimation to eliminate the impact of inter-personal diversity. In domain adaption, we design an embedding representation with prediction consistency to ensure that the linear relationship between gaze directions in different domains remains consistent on gaze space and embedding space. Specifically, we employ source gaze to form a locally linear representation in the gaze space for each target domain prediction. Then the same linear combinations are applied in the embedding space to generate hypothesis embedding for the target domain sample, remaining prediction consistency. The deviation between the target and source domain is reduced by approximating the predicted and hypothesis embedding for the target domain sample. Guided by the proposed strategy, we design Domain Adaptation Gaze Estimation Network(DAGEN), which learns embedding with prediction consistency and achieves state-of-the-art results on both the MPIIGaze and the EYEDIAP datasets.



### Efficient Medical Image Segmentation with Intermediate Supervision Mechanism
- **Arxiv ID**: http://arxiv.org/abs/2012.03673v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2012.03673v1)
- **Published**: 2020-11-15 13:46:00+00:00
- **Updated**: 2020-11-15 13:46:00+00:00
- **Authors**: Di Yuan, Junyang Chen, Zhenghua Xu, Thomas Lukasiewicz, Zhigang Fu, Guizhi Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Because the expansion path of U-Net may ignore the characteristics of small targets, intermediate supervision mechanism is proposed. The original mask is also entered into the network as a label for intermediate output. However, U-Net is mainly engaged in segmentation, and the extracted features are also targeted at segmentation location information, and the input and output are different. The label we need is that the input and output are both original masks, which is more similar to the refactoring process, so we propose another intermediate supervision mechanism. However, the features extracted by the contraction path of this intermediate monitoring mechanism are not necessarily consistent. For example, U-Net's contraction path extracts transverse features, while auto-encoder extracts longitudinal features, which may cause the output of the expansion path to be inconsistent with the label. Therefore, we put forward the intermediate supervision mechanism of shared-weight decoder module. Although the intermediate supervision mechanism improves the segmentation accuracy, the training time is too long due to the extra input and multiple loss functions. For one of these problems, we have introduced tied-weight decoder. To reduce the redundancy of the model, we combine shared-weight decoder module with tied-weight decoder module.



### w-Net: Dual Supervised Medical Image Segmentation Model with Multi-Dimensional Attention and Cascade Multi-Scale Convolution
- **Arxiv ID**: http://arxiv.org/abs/2012.03674v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.03674v1)
- **Published**: 2020-11-15 13:54:22+00:00
- **Updated**: 2020-11-15 13:54:22+00:00
- **Authors**: Bo Wang, Lei Wang, Junyang Chen, Zhenghua Xu, Thomas Lukasiewicz, Zhigang Fu
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning-based medical image segmentation technology aims at automatic recognizing and annotating objects on the medical image. Non-local attention and feature learning by multi-scale methods are widely used to model network, which drives progress in medical image segmentation. However, those attention mechanism methods have weakly non-local receptive fields' strengthened connection for small objects in medical images. Then, the features of important small objects in abstract or coarse feature maps may be deserted, which leads to unsatisfactory performance. Moreover, the existing multi-scale methods only simply focus on different sizes of view, whose sparse multi-scale features collected are not abundant enough for small objects segmentation. In this work, a multi-dimensional attention segmentation model with cascade multi-scale convolution is proposed to predict accurate segmentation for small objects in medical images. As the weight function, multi-dimensional attention modules provide coefficient modification for significant/informative small objects features. Furthermore, The cascade multi-scale convolution modules in each skip-connection path are exploited to capture multi-scale features in different semantic depth. The proposed method is evaluated on three datasets: KiTS19, Pancreas CT of Decathlon-10, and MICCAI 2018 LiTS Challenge, demonstrating better segmentation performances than the state-of-the-art baselines.



### SAG-GAN: Semi-Supervised Attention-Guided GANs for Data Augmentation on Medical Images
- **Arxiv ID**: http://arxiv.org/abs/2011.07534v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.07534v1)
- **Published**: 2020-11-15 14:01:24+00:00
- **Updated**: 2020-11-15 14:01:24+00:00
- **Authors**: Chang Qi, Junyang Chen, Guizhi Xu, Zhenghua Xu, Thomas Lukasiewicz, Yang Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Recently deep learning methods, in particular, convolutional neural networks (CNNs), have led to a massive breakthrough in the range of computer vision. Also, the large-scale annotated dataset is the essential key to a successful training procedure. However, it is a huge challenge to get such datasets in the medical domain. Towards this, we present a data augmentation method for generating synthetic medical images using cycle-consistency Generative Adversarial Networks (GANs). We add semi-supervised attention modules to generate images with convincing details. We treat tumor images and normal images as two domains. The proposed GANs-based model can generate a tumor image from a normal image, and in turn, it can also generate a normal image from a tumor image. Furthermore, we show that generated medical images can be used for improving the performance of ResNet18 for medical image classification. Our model is applied to three limited datasets of tumor MRI images. We first generate MRI images on limited datasets, then we trained three popular classification models to get the best model for tumor classification. Finally, we train the classification model using real images with classic data augmentation methods and classification models using synthetic images. The classification results between those trained models showed that the proposed SAG-GAN data augmentation method can boost Accuracy and AUC compare with classic data augmentation methods. We believe the proposed data augmentation method can apply to other medical image domains, and improve the accuracy of computer-assisted diagnosis.



### Learn an Effective Lip Reading Model without Pains
- **Arxiv ID**: http://arxiv.org/abs/2011.07557v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.07557v1)
- **Published**: 2020-11-15 15:29:19+00:00
- **Updated**: 2020-11-15 15:29:19+00:00
- **Authors**: Dalu Feng, Shuang Yang, Shiguang Shan, Xilin Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Lip reading, also known as visual speech recognition, aims to recognize the speech content from videos by analyzing the lip dynamics. There have been several appealing progress in recent years, benefiting much from the rapidly developed deep learning techniques and the recent large-scale lip-reading datasets. Most existing methods obtained high performance by constructing a complex neural network, together with several customized training strategies which were always given in a very brief description or even shown only in the source code. We find that making proper use of these strategies could always bring exciting improvements without changing much of the model. Considering the non-negligible effects of these strategies and the existing tough status to train an effective lip reading model, we perform a comprehensive quantitative study and comparative analysis, for the first time, to show the effects of several different choices for lip reading. By only introducing some easy-to-get refinements to the baseline pipeline, we obtain an obvious improvement of the performance from 83.7% to 88.4% and from 38.2% to 55.7% on two largest public available lip reading datasets, LRW and LRW-1000, respectively. They are comparable and even surpass the existing state-of-the-art results.



### Pix2Streams: Dynamic Hydrology Maps from Satellite-LiDAR Fusion
- **Arxiv ID**: http://arxiv.org/abs/2011.07584v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.AP
- **Links**: [PDF](http://arxiv.org/pdf/2011.07584v1)
- **Published**: 2020-11-15 17:14:28+00:00
- **Updated**: 2020-11-15 17:14:28+00:00
- **Authors**: Dolores Garcia, Gonzalo Mateo-Garcia, Hannes Bernhardt, Ron Hagensieker, Ignacio G. Lopez Francos, Jonathan Stock, Guy Schumann, Kevin Dobbs, Freddie Kalaitzis
- **Comment**: Work completed during the 2020 Frontier Development Lab research
  accelerator, a private-public partnership with NASA in the US, and ESA in
  Europe. Accepted as a spotlight/long oral talk at AI for Earth Sciences
  Workshop at NeurIPS 2020
- **Journal**: None
- **Summary**: Where are the Earth's streams flowing right now? Inland surface waters expand with floods and contract with droughts, so there is no one map of our streams. Current satellite approaches are limited to monthly observations that map only the widest streams. These are fed by smaller tributaries that make up much of the dendritic surface network but whose flow is unobserved. A complete map of our daily waters can give us an early warning for where droughts are born: the receding tips of the flowing network. Mapping them over years can give us a map of impermanence of our waters, showing where to expect water, and where not to. To that end, we feed the latest high-res sensor data to multiple deep learning models in order to map these flowing networks every day, stacking the times series maps over many years. Specifically, i) we enhance water segmentation to $50$ cm/pixel resolution, a 60$\times$ improvement over previous state-of-the-art results. Our U-Net trained on 30-40cm WorldView3 images can detect streams as narrow as 1-3m (30-60$\times$ over SOTA). Our multi-sensor, multi-res variant, WasserNetz, fuses a multi-day window of 3m PlanetScope imagery with 1m LiDAR data, to detect streams 5-7m wide. Both U-Nets produce a water probability map at the pixel-level. ii) We integrate this water map over a DEM-derived synthetic valley network map to produce a snapshot of flow at the stream level. iii) We apply this pipeline, which we call Pix2Streams, to a 2-year daily PlanetScope time-series of three watersheds in the US to produce the first high-fidelity dynamic map of stream flow frequency. The end result is a new map that, if applied at the national scale, could fundamentally improve how we manage our water resources around the world.



### Learning normal appearance for fetal anomaly screening: Application to the unsupervised detection of Hypoplastic Left Heart Syndrome
- **Arxiv ID**: http://arxiv.org/abs/2012.03679v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2012.03679v2)
- **Published**: 2020-11-15 17:18:37+00:00
- **Updated**: 2021-09-09 21:12:25+00:00
- **Authors**: Elisa Chotzoglou, Thomas Day, Jeremy Tan, Jacqueline Matthew, David Lloyd, Reza Razavi, John Simpson, Bernhard Kainz
- **Comment**: None
- **Journal**: None
- **Summary**: Congenital heart disease is considered as one the most common groups of congenital malformations which affects $6-11$ per $1000$ newborns. In this work, an automated framework for detection of cardiac anomalies during ultrasound screening is proposed and evaluated on the example of Hypoplastic Left Heart Syndrome (HLHS), a sub-category of congenital heart disease. We propose an unsupervised approach that learns healthy anatomy exclusively from clinically confirmed normal control patients. We evaluate a number of known anomaly detection frameworks together with a model architecture based on the $\alpha$-GAN network and find evidence that the proposed model performs significantly better than the state-of-the-art in image-based anomaly detection, yielding average $0.81$ AUC \emph{and} a better robustness towards initialisation compared to previous works.



### DIRL: Domain-Invariant Representation Learning for Sim-to-Real Transfer
- **Arxiv ID**: http://arxiv.org/abs/2011.07589v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2011.07589v3)
- **Published**: 2020-11-15 17:39:01+00:00
- **Updated**: 2021-01-07 06:08:20+00:00
- **Authors**: Ajay Kumar Tanwani
- **Comment**: 4th Conference on Robot Learning (CoRL), 2020 [plenary talk, Best
  System Paper Award Finalist]
- **Journal**: None
- **Summary**: Generating large-scale synthetic data in simulation is a feasible alternative to collecting/labelling real data for training vision-based deep learning models, albeit the modelling inaccuracies do not generalize to the physical world. In this paper, we present a domain-invariant representation learning (DIRL) algorithm to adapt deep models to the physical environment with a small amount of real data. Existing approaches that only mitigate the covariate shift by aligning the marginal distributions across the domains and assume the conditional distributions to be domain-invariant can lead to ambiguous transfer in real scenarios. We propose to jointly align the marginal (input domains) and the conditional (output labels) distributions to mitigate the covariate and the conditional shift across the domains with adversarial learning, and combine it with a triplet distribution loss to make the conditional distributions disjoint in the shared feature space. Experiments on digit domains yield state-of-the-art performance on challenging benchmarks, while sim-to-real transfer of object recognition for vision-based decluttering with a mobile robot improves from 26.8 % to 91.0 %, resulting in 86.5 % grasping accuracy of a wide variety of objects. Code and supplementary details are available at https://sites.google.com/view/dirl



### MuSCLE: Multi Sweep Compression of LiDAR using Deep Entropy Models
- **Arxiv ID**: http://arxiv.org/abs/2011.07590v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.IT, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/2011.07590v2)
- **Published**: 2020-11-15 17:41:14+00:00
- **Updated**: 2021-01-08 21:58:57+00:00
- **Authors**: Sourav Biswas, Jerry Liu, Kelvin Wong, Shenlong Wang, Raquel Urtasun
- **Comment**: NeurIPS 2020
- **Journal**: None
- **Summary**: We present a novel compression algorithm for reducing the storage of LiDAR sensor data streams. Our model exploits spatio-temporal relationships across multiple LiDAR sweeps to reduce the bitrate of both geometry and intensity values. Towards this goal, we propose a novel conditional entropy model that models the probabilities of the octree symbols by considering both coarse level geometry and previous sweeps' geometric and intensity information. We then use the learned probability to encode the full data stream into a compact one. Our experiments demonstrate that our method significantly reduces the joint geometry and intensity bitrate over prior state-of-the-art LiDAR compression methods, with a reduction of 7-17% and 15-35% on the UrbanCity and SemanticKITTI datasets respectively.



### Studying Robustness of Semantic Segmentation under Domain Shift in cardiac MRI
- **Arxiv ID**: http://arxiv.org/abs/2011.07592v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.07592v1)
- **Published**: 2020-11-15 17:50:23+00:00
- **Updated**: 2020-11-15 17:50:23+00:00
- **Authors**: Peter M. Full, Fabian Isensee, Paul F. Jäger, Klaus Maier-Hein
- **Comment**: 12 pages, 3 Figures, Contribution to the STACOM Workshop at MICCAI
  2020
- **Journal**: None
- **Summary**: Cardiac magnetic resonance imaging (cMRI) is an integral part of diagnosis in many heart related diseases. Recently, deep neural networks have demonstrated successful automatic segmentation, thus alleviating the burden of time-consuming manual contouring of cardiac structures. Moreover, frameworks such as nnU-Net provide entirely automatic model configuration to unseen datasets enabling out-of-the-box application even by non-experts. However, current studies commonly neglect the clinically realistic scenario, in which a trained network is applied to data from a different domain such as deviating scanners or imaging protocols. This potentially leads to unexpected performance drops of deep learning models in real life applications. In this work, we systematically study challenges and opportunities of domain transfer across images from multiple clinical centres and scanner vendors. In order to maintain out-of-the-box usability, we build upon a fixed U-Net architecture configured by the nnU-net framework to investigate various data augmentation techniques and batch normalization layers as an easy-to-customize pipeline component and provide general guidelines on how to improve domain generalizability abilities in existing deep learning methods. Our proposed method ranked first at the Multi-Centre, Multi-Vendor & Multi-Disease Cardiac Image Segmentation Challenge (M&Ms).



### BirdSLAM: Monocular Multibody SLAM in Bird's-Eye View
- **Arxiv ID**: http://arxiv.org/abs/2011.07613v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.07613v1)
- **Published**: 2020-11-15 19:37:24+00:00
- **Updated**: 2020-11-15 19:37:24+00:00
- **Authors**: Swapnil Daga, Gokul B. Nair, Anirudha Ramesh, Rahul Sajnani, Junaid Ahmed Ansari, K. Madhava Krishna
- **Comment**: Accepted in VISIGRAPP (VISAPP) 2021
- **Journal**: None
- **Summary**: In this paper, we present BirdSLAM, a novel simultaneous localization and mapping (SLAM) system for the challenging scenario of autonomous driving platforms equipped with only a monocular camera. BirdSLAM tackles challenges faced by other monocular SLAM systems (such as scale ambiguity in monocular reconstruction, dynamic object localization, and uncertainty in feature representation) by using an orthographic (bird's-eye) view as the configuration space in which localization and mapping are performed. By assuming only the height of the ego-camera above the ground, BirdSLAM leverages single-view metrology cues to accurately localize the ego-vehicle and all other traffic participants in bird's-eye view. We demonstrate that our system outperforms prior work that uses strictly greater information, and highlight the relevance of each design decision via an ablation analysis.



### Real-Time Polyp Detection, Localization and Segmentation in Colonoscopy Using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2011.07631v2
- **DOI**: 10.1109/ACCESS.2021.3063716
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.07631v2)
- **Published**: 2020-11-15 21:14:50+00:00
- **Updated**: 2021-03-31 20:21:06+00:00
- **Authors**: Debesh Jha, Sharib Ali, Nikhil Kumar Tomar, Håvard D. Johansen, Dag D. Johansen, Jens Rittscher, Michael A. Riegler, Pål Halvorsen
- **Comment**: None
- **Journal**: Published in: IEEE Access, Page(s): 40496 - 40510, Date of
  Publication: 04 March 2021, Electronic ISSN: 2169-3536, PubMed ID: 33747684
  Publisher: IEEE
- **Summary**: Computer-aided detection, localisation, and segmentation methods can help improve colonoscopy procedures. Even though many methods have been built to tackle automatic detection and segmentation of polyps, benchmarking of state-of-the-art methods still remains an open problem. This is due to the increasing number of researched computer vision methods that can be applied to polyp datasets. Benchmarking of novel methods can provide a direction to the development of automated polyp detection and segmentation tasks. Furthermore, it ensures that the produced results in the community are reproducible and provide a fair comparison of developed methods. In this paper, we benchmark several recent state-of-the-art methods using Kvasir-SEG, an open-access dataset of colonoscopy images for polyp detection, localisation, and segmentation evaluating both method accuracy and speed. Whilst, most methods in literature have competitive performance over accuracy, we show that the proposed ColonSegNet achieved a better trade-off between an average precision of 0.8000 and mean IoU of 0.8100, and the fastest speed of 180 frames per second for the detection and localisation task. Likewise, the proposed ColonSegNet achieved a competitive dice coefficient of 0.8206 and the best average speed of 182.38 frames per second for the segmentation task. Our comprehensive comparison with various state-of-the-art methods reveals the importance of benchmarking the deep learning methods for automated real-time polyp identification and delineations that can potentially transform current clinical practices and minimise miss-detection rates.



### Deep multi-modal networks for book genre classification based on its cover
- **Arxiv ID**: http://arxiv.org/abs/2011.07658v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2011.07658v1)
- **Published**: 2020-11-15 23:27:43+00:00
- **Updated**: 2020-11-15 23:27:43+00:00
- **Authors**: Chandra Kundu, Lukun Zheng
- **Comment**: 23 pages, 8 figures
- **Journal**: None
- **Summary**: Book covers are usually the very first impression to its readers and they often convey important information about the content of the book. Book genre classification based on its cover would be utterly beneficial to many modern retrieval systems, considering that the complete digitization of books is an extremely expensive task. At the same time, it is also an extremely challenging task due to the following reasons: First, there exists a wide variety of book genres, many of which are not concretely defined. Second, book covers, as graphic designs, vary in many different ways such as colors, styles, textual information, etc, even for books of the same genre. Third, book cover designs may vary due to many external factors such as country, culture, target reader populations, etc. With the growing competitiveness in the book industry, the book cover designers and typographers push the cover designs to its limit in the hope of attracting sales. The cover-based book classification systems become a particularly exciting research topic in recent years. In this paper, we propose a multi-modal deep learning framework to solve this problem. The contribution of this paper is four-fold. First, our method adds an extra modality by extracting texts automatically from the book covers. Second, image-based and text-based, state-of-the-art models are evaluated thoroughly for the task of book cover classification. Third, we develop an efficient and salable multi-modal framework based on the images and texts shown on the covers only. Fourth, a thorough analysis of the experimental results is given and future works to improve the performance is suggested. The results show that the multi-modal framework significantly outperforms the current state-of-the-art image-based models. However, more efforts and resources are needed for this classification task in order to reach a satisfactory level.



### ArraMon: A Joint Navigation-Assembly Instruction Interpretation Task in Dynamic Environments
- **Arxiv ID**: http://arxiv.org/abs/2011.07660v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2011.07660v1)
- **Published**: 2020-11-15 23:30:36+00:00
- **Updated**: 2020-11-15 23:30:36+00:00
- **Authors**: Hyounghun Kim, Abhay Zala, Graham Burri, Hao Tan, Mohit Bansal
- **Comment**: EMNLP Findings 2020 (18 pages; extended to Hindi)
- **Journal**: None
- **Summary**: For embodied agents, navigation is an important ability but not an isolated goal. Agents are also expected to perform specific tasks after reaching the target location, such as picking up objects and assembling them into a particular arrangement. We combine Vision-and-Language Navigation, assembling of collected objects, and object referring expression comprehension, to create a novel joint navigation-and-assembly task, named ArraMon. During this task, the agent (similar to a PokeMON GO player) is asked to find and collect different target objects one-by-one by navigating based on natural language instructions in a complex, realistic outdoor environment, but then also ARRAnge the collected objects part-by-part in an egocentric grid-layout environment. To support this task, we implement a 3D dynamic environment simulator and collect a dataset (in English; and also extended to Hindi) with human-written navigation and assembling instructions, and the corresponding ground truth trajectories. We also filter the collected instructions via a verification stage, leading to a total of 7.7K task instances (30.8K instructions and paths). We present results for several baseline models (integrated and biased) and metrics (nDTW, CTC, rPOD, and PTC), and the large model-human performance gap demonstrates that our task is challenging and presents a wide scope for future work. Our dataset, simulator, and code are publicly available at: https://arramonunc.github.io



### hyper-sinh: An Accurate and Reliable Function from Shallow to Deep Learning in TensorFlow and Keras
- **Arxiv ID**: http://arxiv.org/abs/2011.07661v1
- **DOI**: 10.1016/j.mlwa.2021.100112
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.LG, cs.NE, 68T07, 68T10, 68T45, 68T50, 68U35, I.2.1; I.2.7; I.2.10; I.4.9; I.5.1; I.5.4; I.5.5
- **Links**: [PDF](http://arxiv.org/pdf/2011.07661v1)
- **Published**: 2020-11-15 23:38:59+00:00
- **Updated**: 2020-11-15 23:38:59+00:00
- **Authors**: Luca Parisi, Renfei Ma, Narrendar RaviChandran, Matteo Lanzillotta
- **Comment**: 19 pages, 6 listings/Python code snippets, 4 figures, 5 tables
- **Journal**: None
- **Summary**: This paper presents the 'hyper-sinh', a variation of the m-arcsinh activation function suitable for Deep Learning (DL)-based algorithms for supervised learning, such as Convolutional Neural Networks (CNN). hyper-sinh, developed in the open source Python libraries TensorFlow and Keras, is thus described and validated as an accurate and reliable activation function for both shallow and deep neural networks. Improvements in accuracy and reliability in image and text classification tasks on five (N = 5) benchmark data sets available from Keras are discussed. Experimental results demonstrate the overall competitive classification performance of both shallow and deep neural networks, obtained via this novel function. This function is evaluated with respect to gold standard activation functions, demonstrating its overall competitive accuracy and reliability for both image and text classification.



