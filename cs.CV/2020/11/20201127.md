# Arxiv Papers in cs.CV on 2020-11-27
### Tractable loss function and color image generation of multinary restricted Boltzmann machine
- **Arxiv ID**: http://arxiv.org/abs/2011.13509v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.13509v1)
- **Published**: 2020-11-27 00:50:59+00:00
- **Updated**: 2020-11-27 00:50:59+00:00
- **Authors**: Juno Hwang, Wonseok Hwang, Junghyo Jo
- **Comment**: NueRIPS 2020 DiffCVGP workshop paper
- **Journal**: None
- **Summary**: The restricted Boltzmann machine (RBM) is a representative generative model based on the concept of statistical mechanics. In spite of the strong merit of interpretability, unavailability of backpropagation makes it less competitive than other generative models. Here we derive differentiable loss functions for both binary and multinary RBMs. Then we demonstrate their learnability and performance by generating colored face images.



### Robust Attacks on Deep Learning Face Recognition in the Physical World
- **Arxiv ID**: http://arxiv.org/abs/2011.13526v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.13526v1)
- **Published**: 2020-11-27 02:24:43+00:00
- **Updated**: 2020-11-27 02:24:43+00:00
- **Authors**: Meng Shen, Hao Yu, Liehuang Zhu, Ke Xu, Qi Li, Xiaojiang Du
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) have been increasingly used in face recognition (FR) systems. Recent studies, however, show that DNNs are vulnerable to adversarial examples, which can potentially mislead the FR systems using DNNs in the physical world. Existing attacks on these systems either generate perturbations working merely in the digital world, or rely on customized equipments to generate perturbations and are not robust in varying physical environments. In this paper, we propose FaceAdv, a physical-world attack that crafts adversarial stickers to deceive FR systems. It mainly consists of a sticker generator and a transformer, where the former can craft several stickers with different shapes and the latter transformer aims to digitally attach stickers to human faces and provide feedbacks to the generator to improve the effectiveness of stickers. We conduct extensive experiments to evaluate the effectiveness of FaceAdv on attacking 3 typical FR systems (i.e., ArcFace, CosFace and FaceNet). The results show that compared with a state-of-the-art attack, FaceAdv can significantly improve success rate of both dodging and impersonating attacks. We also conduct comprehensive evaluations to demonstrate the robustness of FaceAdv.



### The NEOLIX Open Dataset for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2011.13528v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.13528v2)
- **Published**: 2020-11-27 02:27:39+00:00
- **Updated**: 2021-01-28 06:41:15+00:00
- **Authors**: Lichao Wang, Lanxin Lei, Hongli Song, Weibao Wang
- **Comment**: 7 pages, 6 figures
- **Journal**: None
- **Summary**: With the gradual maturity of 5G technology,autonomous driving technology has attracted moreand more attention among the research commu-nity. Autonomous driving vehicles rely on the co-operation of artificial intelligence, visual comput-ing, radar, monitoring equipment and GPS, whichenables computers to operate motor vehicles auto-matically and safely without human interference.However, the large-scale dataset for training andsystem evaluation is still a hot potato in the devel-opment of robust perception models. In this paper,we present the NEOLIX dataset and its applica-tions in the autonomous driving area. Our datasetincludes about 30,000 frames with point cloud la-bels, and more than 600k 3D bounding boxes withannotations. The data collection covers multipleregions, and various driving conditions, includingday, night, dawn, dusk and sunny day. In orderto label this complete dataset, we developed vari-ous tools and algorithms specified for each task tospeed up the labelling process. It is expected thatour dataset and related algorithms can support andmotivate researchers for the further developmentof autonomous driving in the field of computer vi-sion.



### They are Not Completely Useless: Towards Recycling Transferable Unlabeled Data for Class-Mismatched Semi-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2011.13529v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.13529v4)
- **Published**: 2020-11-27 02:29:35+00:00
- **Updated**: 2022-04-12 05:56:04+00:00
- **Authors**: Zhuo Huang, Ying Tai, Chengjie Wang, Jian Yang, Chen Gong
- **Comment**: None
- **Journal**: None
- **Summary**: Semi-Supervised Learning (SSL) with mismatched classes deals with the problem that the classes-of-interests in the limited labeled data is only a subset of the classes in massive unlabeled data. As a result, the classes only possessed by the unlabeled data may mislead the classifier training and thus hindering the realistic landing of various SSL methods. To solve this problem, existing methods usually divide unlabeled data to in-distribution (ID) data and out-of-distribution (OOD) data, and directly discard or weaken the OOD data to avoid their adverse impact. In other words, they treat OOD data as completely useless and thus the potential valuable information for classification contained by them is totally ignored. To remedy this defect, this paper proposes a "Transferable OOD data Recycling" (TOOR) method which properly utilizes ID data as well as the "recyclable" OOD data to enrich the information for conducting class-mismatched SSL. Specifically, TOOR firstly attributes all unlabeled data to ID data or OOD data, among which the ID data are directly used for training. Then we treat the OOD data that have a close relationship with ID data and labeled data as recyclable, and employ adversarial domain adaptation to project them to the space of ID data and labeled data. In other words, the recyclability of an OOD datum is evaluated by its transferability, and the recyclable OOD data are transferred so that they are compatible with the distribution of known classes-of-interests. Consequently, our TOOR method extracts more information from unlabeled data than existing approaches, so it can achieve the improved performance which is demonstrated by the experiments on typical benchmark datasets.



### A Survey of Deep Learning Approaches for OCR and Document Understanding
- **Arxiv ID**: http://arxiv.org/abs/2011.13534v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.IR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.13534v2)
- **Published**: 2020-11-27 03:05:59+00:00
- **Updated**: 2021-02-04 23:48:39+00:00
- **Authors**: Nishant Subramani, Alexandre Matton, Malcolm Greaves, Adrian Lam
- **Comment**: Accepted to the ML-RSA Workshop at NeurIPS2020. 15 pages (10 +
  References)
- **Journal**: None
- **Summary**: Documents are a core part of many businesses in many fields such as law, finance, and technology among others. Automatic understanding of documents such as invoices, contracts, and resumes is lucrative, opening up many new avenues of business. The fields of natural language processing and computer vision have seen tremendous progress through the development of deep learning such that these methods have started to become infused in contemporary document understanding systems. In this survey paper, we review different techniques for document understanding for documents written in English and consolidate methodologies present in literature to act as a jumping-off point for researchers exploring this area.



### Patch-VQ: 'Patching Up' the Video Quality Problem
- **Arxiv ID**: http://arxiv.org/abs/2011.13544v2
- **DOI**: 10.1109/CVPR46437.2021.01380
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.13544v2)
- **Published**: 2020-11-27 03:46:44+00:00
- **Updated**: 2022-02-25 05:57:21+00:00
- **Authors**: Zhenqiang Ying, Maniratnam Mandal, Deepti Ghadiyaram, Alan Bovik
- **Comment**: None
- **Journal**: None
- **Summary**: No-reference (NR) perceptual video quality assessment (VQA) is a complex, unsolved, and important problem to social and streaming media applications. Efficient and accurate video quality predictors are needed to monitor and guide the processing of billions of shared, often imperfect, user-generated content (UGC). Unfortunately, current NR models are limited in their prediction capabilities on real-world, "in-the-wild" UGC video data. To advance progress on this problem, we created the largest (by far) subjective video quality dataset, containing 39, 000 realworld distorted videos and 117, 000 space-time localized video patches ('v-patches'), and 5.5M human perceptual quality annotations. Using this, we created two unique NR-VQA models: (a) a local-to-global region-based NR VQA architecture (called PVQ) that learns to predict global video quality and achieves state-of-the-art performance on 3 UGC datasets, and (b) a first-of-a-kind space-time video quality mapping engine (called PVQ Mapper) that helps localize and visualize perceptual distortions in space and time. We will make the new database and prediction models available immediately following the review process.



### Association: Remind Your GAN not to Forget
- **Arxiv ID**: http://arxiv.org/abs/2011.13553v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.13553v2)
- **Published**: 2020-11-27 04:43:15+00:00
- **Updated**: 2021-03-25 09:35:00+00:00
- **Authors**: Yi Gu, Jie Li, Yuting Gao, Ruoxin Chen, Chentao Wu, Feiyang Cai, Chao Wang, Zirui Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Neural networks are susceptible to catastrophic forgetting. They fail to preserve previously acquired knowledge when adapting to new tasks. Inspired by human associative memory system, we propose a brain-like approach that imitates the associative learning process to achieve continual learning. We design a heuristics mechanism to potentiatively stimulate the model, which guides the model to recall the historical episodes based on the current circumstance and obtained association experience. Besides, a distillation measure is added to depressively alter the efficacy of synaptic transmission, which dampens the feature reconstruction learning for new task. The framework is mediated by potentiation and depression stimulation that play opposing roles in directing synaptic and behavioral plasticity. It requires no access to the original data and is more similar to human cognitive process. Experiments demonstrate the effectiveness of our method in alleviating catastrophic forgetting on image-to-image translation tasks.



### SocialGuard: An Adversarial Example Based Privacy-Preserving Technique for Social Images
- **Arxiv ID**: http://arxiv.org/abs/2011.13560v1
- **DOI**: 10.1016/j.jisa.2021.102993
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.13560v1)
- **Published**: 2020-11-27 05:12:47+00:00
- **Updated**: 2020-11-27 05:12:47+00:00
- **Authors**: Mingfu Xue, Shichang Sun, Zhiyu Wu, Can He, Jian Wang, Weiqiang Liu
- **Comment**: None
- **Journal**: Journal of Information Security and Applications, Vol. 63, 2021,
  102993
- **Summary**: The popularity of various social platforms has prompted more people to share their routine photos online. However, undesirable privacy leakages occur due to such online photo sharing behaviors. Advanced deep neural network (DNN) based object detectors can easily steal users' personal information exposed in shared photos. In this paper, we propose a novel adversarial example based privacy-preserving technique for social images against object detectors based privacy stealing. Specifically, we develop an Object Disappearance Algorithm to craft two kinds of adversarial social images. One can hide all objects in the social images from being detected by an object detector, and the other can make the customized sensitive objects be incorrectly classified by the object detector. The Object Disappearance Algorithm constructs perturbation on a clean social image. After being injected with the perturbation, the social image can easily fool the object detector, while its visual quality will not be degraded. We use two metrics, privacy-preserving success rate and privacy leakage rate, to evaluate the effectiveness of the proposed method. Experimental results show that, the proposed method can effectively protect the privacy of social images. The privacy-preserving success rates of the proposed method on MS-COCO and PASCAL VOC 2007 datasets are high up to 96.1% and 99.3%, respectively, and the privacy leakage rates on these two datasets are as low as 0.57% and 0.07%, respectively. In addition, compared with existing image processing methods (low brightness, noise, blur, mosaic and JPEG compression), the proposed method can achieve much better performance in privacy protection and image visual quality maintenance.



### A Sheaf and Topology Approach to Generating Local Branch Numbers in Digital Images
- **Arxiv ID**: http://arxiv.org/abs/2011.13580v2
- **DOI**: None
- **Categories**: **cs.CV**, math.AT
- **Links**: [PDF](http://arxiv.org/pdf/2011.13580v2)
- **Published**: 2020-11-27 06:50:59+00:00
- **Updated**: 2020-12-03 02:43:38+00:00
- **Authors**: Chuan-Shen Hu, Yu-Min Chung
- **Comment**: None
- **Journal**: None
- **Summary**: This paper concerns a theoretical approach that combines topological data analysis (TDA) and sheaf theory. Topological data analysis, a rising field in mathematics and computer science, concerns the shape of the data and has been proven effective in many scientific disciplines. Sheaf theory, a mathematics subject in algebraic geometry, provides a framework for describing the local consistency in geometric objects. Persistent homology (PH) is one of the main driving forces in TDA, and the idea is to track changes of geometric objects at different scales. The persistence diagram (PD) summarizes the information of PH in the form of a multi-set. While PD provides useful information about the underlying objects, it lacks fine relations about the local consistency of specific pairs of generators in PD, such as the merging relation between two connected components in the PH. The sheaf structure provides a novel point of view for describing the merging relation of local objects in PH. It is the goal of this paper to establish a theoretic framework that utilizes the sheaf theory to uncover finer information from the PH. We also show that the proposed theory can be applied to identify the branch numbers of local objects in digital images.



### An Ethical Highlighter for People-Centric Dataset Creation
- **Arxiv ID**: http://arxiv.org/abs/2011.13583v1
- **DOI**: None
- **Categories**: **cs.CY**, cs.CV, cs.DB
- **Links**: [PDF](http://arxiv.org/pdf/2011.13583v1)
- **Published**: 2020-11-27 07:18:44+00:00
- **Updated**: 2020-11-27 07:18:44+00:00
- **Authors**: Margot Hanley, Apoorv Khandelwal, Hadar Averbuch-Elor, Noah Snavely, Helen Nissenbaum
- **Comment**: Part of the Navigating the Broader Impacts of AI Research Workshop at
  NeurIPS 2020
- **Journal**: None
- **Summary**: Important ethical concerns arising from computer vision datasets of people have been receiving significant attention, and a number of datasets have been withdrawn as a result. To meet the academic need for people-centric datasets, we propose an analytical framework to guide ethical evaluation of existing datasets and to serve future dataset creators in avoiding missteps. Our work is informed by a review and analysis of prior works and highlights where such ethical challenges arise.



### Road Scene Graph: A Semantic Graph-Based Scene Representation Dataset for Intelligent Vehicles
- **Arxiv ID**: http://arxiv.org/abs/2011.13588v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.13588v1)
- **Published**: 2020-11-27 07:33:11+00:00
- **Updated**: 2020-11-27 07:33:11+00:00
- **Authors**: Yafu Tian, Alexander Carballo, Ruifeng Li, Kazuya Takeda
- **Comment**: 8 pages, 8 figures, under review ICRA 2021
- **Journal**: None
- **Summary**: Rich semantic information extraction plays a vital role on next-generation intelligent vehicles. Currently there is great amount of research focusing on fundamental applications such as 6D pose detection, road scene semantic segmentation, etc. And this provides us a great opportunity to think about how shall these data be organized and exploited.   In this paper we propose road scene graph,a special scene-graph for intelligent vehicles. Different to classical data representation, this graph provides not only object proposals but also their pair-wise relationships. By organizing them in a topological graph, these data are explainable, fully-connected, and could be easily processed by GCNs (Graph Convolutional Networks). Here we apply scene graph on roads using our Road Scene Graph dataset, including the basic graph prediction model. This work also includes experimental evaluations using the proposed model.



### Multi-objective Neural Architecture Search with Almost No Training
- **Arxiv ID**: http://arxiv.org/abs/2011.13591v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2011.13591v1)
- **Published**: 2020-11-27 07:39:17+00:00
- **Updated**: 2020-11-27 07:39:17+00:00
- **Authors**: Shengran Hu, Ran Cheng, Cheng He, Zhichao Lu
- **Comment**: EMO 2021
- **Journal**: None
- **Summary**: In the recent past, neural architecture search (NAS) has attracted increasing attention from both academia and industries. Despite the steady stream of impressive empirical results, most existing NAS algorithms are computationally prohibitive to execute due to the costly iterations of stochastic gradient descent (SGD) training. In this work, we propose an effective alternative, dubbed Random-Weight Evaluation (RWE), to rapidly estimate the performance of network architectures. By just training the last linear classification layer, RWE reduces the computational cost of evaluating an architecture from hours to seconds. When integrated within an evolutionary multi-objective algorithm, RWE obtains a set of efficient architectures with state-of-the-art performance on CIFAR-10 with less than two hours' searching on a single GPU card. Ablation studies on rank-order correlations and transfer learning experiments to ImageNet have further validated the effectiveness of RWE.



### An anatomically-informed 3D CNN for brain aneurysm classification with weak labels
- **Arxiv ID**: http://arxiv.org/abs/2012.08645v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2012.08645v1)
- **Published**: 2020-11-27 08:00:54+00:00
- **Updated**: 2020-11-27 08:00:54+00:00
- **Authors**: Tommaso Di Noto, Guillaume Marie, Sébastien Tourbier, Yasser Alemán-Gómez, Guillaume Saliou, Meritxell Bach Cuadra, Patric Hagmann, Jonas Richiardi
- **Comment**: 11 pages, 2 figures, Paper accepted at the "Machine Learning in
  Clinical Neuroimaging" (MLCN) MICCAI workshop 2020 (https://mlcnws.com/)
- **Journal**: None
- **Summary**: A commonly adopted approach to carry out detection tasks in medical imaging is to rely on an initial segmentation. However, this approach strongly depends on voxel-wise annotations which are repetitive and time-consuming to draw for medical experts. An interesting alternative to voxel-wise masks are so-called "weak" labels: these can either be coarse or oversized annotations that are less precise, but noticeably faster to create. In this work, we address the task of brain aneurysm detection as a patch-wise binary classification with weak labels, in contrast to related studies that rather use supervised segmentation methods and voxel-wise delineations. Our approach comes with the non-trivial challenge of the data set creation: as for most focal diseases, anomalous patches (with aneurysm) are outnumbered by those showing no anomaly, and the two classes usually have different spatial distributions. To tackle this frequent scenario of inherently imbalanced, spatially skewed data sets, we propose a novel, anatomically-driven approach by using a multi-scale and multi-input 3D Convolutional Neural Network (CNN). We apply our model to 214 subjects (83 patients, 131 controls) who underwent Time-Of-Flight Magnetic Resonance Angiography (TOF-MRA) and presented a total of 111 unruptured cerebral aneurysms. We compare two strategies for negative patch sampling that have an increasing level of difficulty for the network and we show how this choice can strongly affect the results. To assess whether the added spatial information helps improving performances, we compare our anatomically-informed CNN with a baseline, spatially-agnostic CNN. When considering the more realistic and challenging scenario including vessel-like negative patches, the former model attains the highest classification results (accuracy$\simeq$95\%, AUROC$\simeq$0.95, AUPR$\simeq$0.71), thus outperforming the baseline.



### PCLs: Geometry-aware Neural Reconstruction of 3D Pose with Perspective Crop Layers
- **Arxiv ID**: http://arxiv.org/abs/2011.13607v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.13607v2)
- **Published**: 2020-11-27 08:48:43+00:00
- **Updated**: 2021-04-15 17:39:07+00:00
- **Authors**: Frank Yu, Mathieu Salzmann, Pascal Fua, Helge Rhodin
- **Comment**: CVPR 2021
- **Journal**: None
- **Summary**: Local processing is an essential feature of CNNs and other neural network architectures - it is one of the reasons why they work so well on images where relevant information is, to a large extent, local. However, perspective effects stemming from the projection in a conventional camera vary for different global positions in the image. We introduce Perspective Crop Layers (PCLs) - a form of perspective crop of the region of interest based on the camera geometry - and show that accounting for the perspective consistently improves the accuracy of state-of-the-art 3D pose reconstruction methods. PCLs are modular neural network layers, which, when inserted into existing CNN and MLP architectures, deterministically remove the location-dependent perspective effects while leaving end-to-end training and the number of parameters of the underlying neural network unchanged. We demonstrate that PCL leads to improved 3D human pose reconstruction accuracy for CNN architectures that use cropping operations, such as spatial transformer networks (STN), and, somewhat surprisingly, MLPs used for 2D-to-3D keypoint lifting. Our conclusion is that it is important to utilize camera calibration information when available, for classical and deep-learning-based computer vision alike. PCL offers an easy way to improve the accuracy of existing 3D reconstruction networks by making them geometry aware. Our code is publicly available at github.com/yu-frank/PerspectiveCropLayers.



### Frequency Domain Image Translation: More Photo-realistic, Better Identity-preserving
- **Arxiv ID**: http://arxiv.org/abs/2011.13611v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.13611v3)
- **Published**: 2020-11-27 08:58:56+00:00
- **Updated**: 2021-08-05 03:33:15+00:00
- **Authors**: Mu Cai, Hong Zhang, Huijuan Huang, Qichuan Geng, Yixuan Li, Gao Huang
- **Comment**: 15 pages
- **Journal**: None
- **Summary**: Image-to-image translation has been revolutionized with GAN-based methods. However, existing methods lack the ability to preserve the identity of the source domain. As a result, synthesized images can often over-adapt to the reference domain, losing important structural characteristics and suffering from suboptimal visual quality. To solve these challenges, we propose a novel frequency domain image translation (FDIT) framework, exploiting frequency information for enhancing the image generation process. Our key idea is to decompose the image into low-frequency and high-frequency components, where the high-frequency feature captures object structure akin to the identity. Our training objective facilitates the preservation of frequency information in both pixel space and Fourier spectral space. We broadly evaluate FDIT across five large-scale datasets and multiple tasks including image translation and GAN inversion. Extensive experiments and ablations show that FDIT effectively preserves the identity of the source image, and produces photo-realistic images. FDIT establishes state-of-the-art performance, reducing the average FID score by 5.6% compared to the previous best method.



### Multi-task MR Imaging with Iterative Teacher Forcing and Re-weighted Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2011.13614v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2011.13614v1)
- **Published**: 2020-11-27 09:08:05+00:00
- **Updated**: 2020-11-27 09:08:05+00:00
- **Authors**: Kehan Qi, Yu Gong, Xinfeng Liu, Xin Liu, Hairong Zheng, Shanshan Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Noises, artifacts, and loss of information caused by the magnetic resonance (MR) reconstruction may compromise the final performance of the downstream applications. In this paper, we develop a re-weighted multi-task deep learning method to learn prior knowledge from the existing big dataset and then utilize them to assist simultaneous MR reconstruction and segmentation from the under-sampled k-space data. The multi-task deep learning framework is equipped with two network sub-modules, which are integrated and trained by our designed iterative teacher forcing scheme (ITFS) under the dynamic re-weighted loss constraint (DRLC). The ITFS is designed to avoid error accumulation by injecting the fully-sampled data into the training process. The DRLC is proposed to dynamically balance the contributions from the reconstruction and segmentation sub-modules so as to co-prompt the multi-task accuracy. The proposed method has been evaluated on two open datasets and one in vivo in-house dataset and compared to six state-of-the-art methods. Results show that the proposed method possesses encouraging capabilities for simultaneous and accurate MR reconstruction and segmentation.



### Manipulating Medical Image Translation with Manifold Disentanglement
- **Arxiv ID**: http://arxiv.org/abs/2011.13615v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2011.13615v1)
- **Published**: 2020-11-27 09:11:52+00:00
- **Updated**: 2020-11-27 09:11:52+00:00
- **Authors**: Siyu Liu, Jason A. Dowling, Craig Engstrom, Peter B. Greer, Stuart Crozier, Shekhar S. Chandra
- **Comment**: None
- **Journal**: None
- **Summary**: Medical image translation (e.g. CT to MR) is a challenging task as it requires I) faithful translation of domain-invariant features (e.g. shape information of anatomical structures) and II) realistic synthesis of target-domain features (e.g. tissue appearance in MR). In this work, we propose Manifold Disentanglement Generative Adversarial Network (MDGAN), a novel image translation framework that explicitly models these two types of features. It employs a fully convolutional generator to model domain-invariant features, and it uses style codes to separately model target-domain features as a manifold. This design aims to explicitly disentangle domain-invariant features and domain-specific features while gaining individual control of both. The image translation process is formulated as a stylisation task, where the input is "stylised" (translated) into diverse target-domain images based on style codes sampled from the learnt manifold. We test MDGAN for multi-modal medical image translation, where we create two domain-specific manifold clusters on the manifold to translate segmentation maps into pseudo-CT and pseudo-MR images, respectively. We show that by traversing a path across the MR manifold cluster, the target output can be manipulated while still retaining the shape information from the input.



### SS-SFDA : Self-Supervised Source-Free Domain Adaptation for Road Segmentation in Hazardous Environments
- **Arxiv ID**: http://arxiv.org/abs/2012.08939v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.08939v2)
- **Published**: 2020-11-27 09:19:03+00:00
- **Updated**: 2021-03-25 09:33:13+00:00
- **Authors**: Divya Kothandaraman, Rohan Chandra, Dinesh Manocha
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel approach for unsupervised road segmentation in adverse weather conditions such as rain or fog. This includes a new algorithm for source-free domain adaptation (SFDA) using self-supervised learning. Moreover, our approach uses several techniques to address various challenges in SFDA and improve performance, including online generation of pseudo-labels and self-attention as well as use of curriculum learning, entropy minimization and model distillation. We have evaluated the performance on $6$ datasets corresponding to real and synthetic adverse weather conditions. Our method outperforms all prior works on unsupervised road segmentation and SFDA by at least 10.26%, and improves the training time by 18-180x. Moreover, our self-supervised algorithm exhibits similar accuracy performance in terms of mIOU score as compared to prior supervised methods.



### Temporal-Channel Transformer for 3D Lidar-Based Video Object Detection in Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2011.13628v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.13628v1)
- **Published**: 2020-11-27 09:35:39+00:00
- **Updated**: 2020-11-27 09:35:39+00:00
- **Authors**: Zhenxun Yuan, Xiao Song, Lei Bai, Wengang Zhou, Zhe Wang, Wanli Ouyang
- **Comment**: None
- **Journal**: None
- **Summary**: The strong demand of autonomous driving in the industry has lead to strong interest in 3D object detection and resulted in many excellent 3D object detection algorithms. However, the vast majority of algorithms only model single-frame data, ignoring the temporal information of the sequence of data. In this work, we propose a new transformer, called Temporal-Channel Transformer, to model the spatial-temporal domain and channel domain relationships for video object detecting from Lidar data. As a special design of this transformer, the information encoded in the encoder is different from that in the decoder, i.e. the encoder encodes temporal-channel information of multiple frames while the decoder decodes the spatial-channel information for the current frame in a voxel-wise manner. Specifically, the temporal-channel encoder of the transformer is designed to encode the information of different channels and frames by utilizing the correlation among features from different channels and frames. On the other hand, the spatial decoder of the transformer will decode the information for each location of the current frame. Before conducting the object detection with detection head, the gate mechanism is deployed for re-calibrating the features of current frame, which filters out the object irrelevant information by repetitively refine the representation of target frame along with the up-sampling process. Experimental results show that we achieve the state-of-the-art performance in grid voxel-based 3D object detection on the nuScenes benchmark.



### Descriptor-Free Multi-View Region Matching for Instance-Wise 3D Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2011.13649v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.13649v1)
- **Published**: 2020-11-27 10:45:18+00:00
- **Updated**: 2020-11-27 10:45:18+00:00
- **Authors**: Takuma Doi, Fumio Okura, Toshiki Nagahara, Yasuyuki Matsushita, Yasushi Yagi
- **Comment**: ACCV2020 Oral
- **Journal**: None
- **Summary**: This paper proposes a multi-view extension of instance segmentation without relying on texture or shape descriptor matching. Multi-view instance segmentation becomes challenging for scenes with repetitive textures and shapes, e.g., plant leaves, due to the difficulty of multi-view matching using texture or shape descriptors. To this end, we propose a multi-view region matching method based on epipolar geometry, which does not rely on any feature descriptors. We further show that the epipolar region matching can be easily integrated into instance segmentation and effective for instance-wise 3D reconstruction. Experiments demonstrate the improved accuracy of multi-view instance matching and the 3D reconstruction compared to the baseline methods.



### Deformed Implicit Field: Modeling 3D Shapes with Learned Dense Correspondence
- **Arxiv ID**: http://arxiv.org/abs/2011.13650v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.13650v3)
- **Published**: 2020-11-27 10:45:26+00:00
- **Updated**: 2021-03-29 10:46:14+00:00
- **Authors**: Yu Deng, Jiaolong Yang, Xin Tong
- **Comment**: CVPR21 camera ready version
- **Journal**: None
- **Summary**: We propose a novel Deformed Implicit Field (DIF) representation for modeling 3D shapes of a category and generating dense correspondences among shapes. With DIF, a 3D shape is represented by a template implicit field shared across the category, together with a 3D deformation field and a correction field dedicated for each shape instance. Shape correspondences can be easily established using their deformation fields. Our neural network, dubbed DIF-Net, jointly learns a shape latent space and these fields for 3D objects belonging to a category without using any correspondence or part label. The learned DIF-Net can also provides reliable correspondence uncertainty measurement reflecting shape structure discrepancy. Experiments show that DIF-Net not only produces high-fidelity 3D shapes but also builds high-quality dense correspondences across different shapes. We also demonstrate several applications such as texture transfer and shape editing, where our method achieves compelling results that cannot be achieved by previous methods.



### Towards real-time object recognition and pose estimation in point clouds
- **Arxiv ID**: http://arxiv.org/abs/2011.13669v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.13669v1)
- **Published**: 2020-11-27 11:10:46+00:00
- **Updated**: 2020-11-27 11:10:46+00:00
- **Authors**: Marlon Marcon, Olga Regina Pereira Bellon, Luciano Silva
- **Comment**: Accepted as Full paper at VISAPP2021
- **Journal**: None
- **Summary**: Object recognition and 6DoF pose estimation are quite challenging tasks in computer vision applications. Despite efficiency in such tasks, standard methods deliver far from real-time processing rates. This paper presents a novel pipeline to estimate a fine 6DoF pose of objects, applied to realistic scenarios in real-time. We split our proposal into three main parts. Firstly, a Color feature classification leverages the use of pre-trained CNN color features trained on the ImageNet for object detection. A Feature-based registration module conducts a coarse pose estimation, and finally, a Fine-adjustment step performs an ICP-based dense registration. Our proposal achieves, in the best case, an accuracy performance of almost 83\% on the RGB-D Scenes dataset. Regarding processing time, the object detection task is done at a frame processing rate up to 90 FPS, and the pose estimation at almost 14 FPS in a full execution strategy. We discuss that due to the proposal's modularity, we could let the full execution occurs only when necessary and perform a scheduled execution that unlocks real-time processing, even for multitask situations.



### Rethinking deinterlacing for early interlaced videos
- **Arxiv ID**: http://arxiv.org/abs/2011.13675v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2011.13675v2)
- **Published**: 2020-11-27 11:24:36+00:00
- **Updated**: 2021-09-12 13:15:27+00:00
- **Authors**: Yang Zhao, Wei Jia, Ronggang Wang
- **Comment**: 6 pages, 5 figures
- **Journal**: None
- **Summary**: With the rapid development of image restoration techniques, high-definition reconstruction of early videos has achieved impressive results. However, there are few studies about the interlacing artifacts that often appear in early videos and significantly affect visual perception. Traditional deinterlacing approaches are mainly focused on early interlacing scanning systems and thus cannot handle the complex and complicated artifacts in real-world early interlaced videos. Hence, this paper proposes a specific deinterlacing network (DIN), which is motivated by the traditional deinterlacing strategy. The proposed DIN consists of two stages, i.e., a cooperative vertical interpolation stage for split fields, and a merging stage that is applied to perceive movements and remove ghost artifacts. Experimental results demonstrate that the proposed method can effectively remove complex artifacts in early interlaced videos.



### Self-EMD: Self-Supervised Object Detection without ImageNet
- **Arxiv ID**: http://arxiv.org/abs/2011.13677v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.13677v3)
- **Published**: 2020-11-27 11:27:19+00:00
- **Updated**: 2021-03-22 09:41:15+00:00
- **Authors**: Songtao Liu, Zeming Li, Jian Sun
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a novel self-supervised representation learning method, Self-EMD, for object detection. Our method directly trained on unlabeled non-iconic image dataset like COCO, instead of commonly used iconic-object image dataset like ImageNet. We keep the convolutional feature maps as the image embedding to preserve spatial structures and adopt Earth Mover's Distance (EMD) to compute the similarity between two embeddings. Our Faster R-CNN (ResNet50-FPN) baseline achieves 39.8% mAP on COCO, which is on par with the state of the art self-supervised methods pre-trained on ImageNet. More importantly, it can be further improved to 40.4% mAP with more unlabeled images, showing its great potential for leveraging more easily obtained unlabeled data. Code will be made available.



### Point and Ask: Incorporating Pointing into Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/2011.13681v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.13681v4)
- **Published**: 2020-11-27 11:43:45+00:00
- **Updated**: 2022-02-18 05:50:50+00:00
- **Authors**: Arjun Mani, Nobline Yoo, Will Hinthorn, Olga Russakovsky
- **Comment**: None
- **Journal**: None
- **Summary**: Visual Question Answering (VQA) has become one of the key benchmarks of visual recognition progress. Multiple VQA extensions have been explored to better simulate real-world settings: different question formulations, changing training and test distributions, conversational consistency in dialogues, and explanation-based answering. In this work, we further expand this space by considering visual questions that include a spatial point of reference. Pointing is a nearly universal gesture among humans, and real-world VQA is likely to involve a gesture towards the target region.   Concretely, we (1) introduce and motivate point-input questions as an extension of VQA, (2) define three novel classes of questions within this space, and (3) for each class, introduce both a benchmark dataset and a series of baseline models to handle its unique challenges. There are two key distinctions from prior work. First, we explicitly design the benchmarks to require the point input, i.e., we ensure that the visual question cannot be answered accurately without the spatial reference. Second, we explicitly explore the more realistic point spatial input rather than the standard but unnatural bounding box input. Through our exploration we uncover and address several visual recognition challenges, including the ability to infer human intent, reason both locally and globally about the image, and effectively combine visual, language and spatial inputs. Code is available at: https://github.com/princetonvisualai/pointingqa .



### MEBOW: Monocular Estimation of Body Orientation In the Wild
- **Arxiv ID**: http://arxiv.org/abs/2011.13688v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.13688v1)
- **Published**: 2020-11-27 11:56:13+00:00
- **Updated**: 2020-11-27 11:56:13+00:00
- **Authors**: Chenyan Wu, Yukun Chen, Jiajia Luo, Che-Chun Su, Anuja Dawane, Bikramjot Hanzra, Zhuo Deng, Bilan Liu, James Wang, Cheng-Hao Kuo
- **Comment**: Accepted by CVPR2020
- **Journal**: None
- **Summary**: Body orientation estimation provides crucial visual cues in many applications, including robotics and autonomous driving. It is particularly desirable when 3-D pose estimation is difficult to infer due to poor image resolution, occlusion or indistinguishable body parts. We present COCO-MEBOW (Monocular Estimation of Body Orientation in the Wild), a new large-scale dataset for orientation estimation from a single in-the-wild image. The body-orientation labels for around 130K human bodies within 55K images from the COCO dataset have been collected using an efficient and high-precision annotation pipeline. We also validated the benefits of the dataset. First, we show that our dataset can substantially improve the performance and the robustness of a human body orientation estimation model, the development of which was previously limited by the scale and diversity of the available training data. Additionally, we present a novel triple-source solution for 3-D human pose estimation, where 3-D pose labels, 2-D pose labels, and our body-orientation labels are all used in joint training. Our model significantly outperforms state-of-the-art dual-source solutions for monocular 3-D human pose estimation, where training only uses 3-D pose labels and 2-D pose labels. This substantiates an important advantage of MEBOW for 3-D human pose estimation, which is particularly appealing because the per-instance labeling cost for body orientations is far less than that for 3-D poses. The work demonstrates high potential of MEBOW in addressing real-world challenges involving understanding human behaviors. Further information of this work is available at https://chenyanwu.github.io/MEBOW/.



### NaturalAE: Natural and Robust Physical Adversarial Examples for Object Detectors
- **Arxiv ID**: http://arxiv.org/abs/2011.13692v2
- **DOI**: 10.1016/j.jisa.2020.102694
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.13692v2)
- **Published**: 2020-11-27 12:03:53+00:00
- **Updated**: 2021-03-17 08:47:26+00:00
- **Authors**: Mingfu Xue, Chengxiang Yuan, Can He, Jian Wang, Weiqiang Liu
- **Comment**: None
- **Journal**: Journal of Information Security and Applications 57 (2021) 102694,
  1-12
- **Summary**: In this paper, we propose a natural and robust physical adversarial example attack method targeting object detectors under real-world conditions. The generated adversarial examples are robust to various physical constraints and visually look similar to the original images, thus these adversarial examples are natural to humans and will not cause any suspicions. First, to ensure the robustness of the adversarial examples in real-world conditions, the proposed method exploits different image transformation functions, to simulate various physical changes during the iterative optimization of the adversarial examples generation. Second, to construct natural adversarial examples, the proposed method uses an adaptive mask to constrain the area and intensities of the added perturbations, and utilizes the real-world perturbation score (RPS) to make the perturbations be similar to those real noises in physical world. Compared with existing studies, our generated adversarial examples can achieve a high success rate with less conspicuous perturbations. Experimental results demonstrate that, the generated adversarial examples are robust under various indoor and outdoor physical conditions, including different distances, angles, illuminations, and photographing. Specifically, the attack success rate of generated adversarial examples indoors and outdoors is high up to 73.33% and 82.22%, respectively. Meanwhile, the proposed method ensures the naturalness of the generated adversarial example, and the size of added perturbations is much smaller than the perturbations in the existing works. Further, the proposed physical adversarial attack method can be transferred from the white-box models to other object detection models.



### Lightweight U-Net for High-Resolution Breast Imaging
- **Arxiv ID**: http://arxiv.org/abs/2011.13698v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.13698v1)
- **Published**: 2020-11-27 12:25:24+00:00
- **Updated**: 2020-11-27 12:25:24+00:00
- **Authors**: Mickael Tardy, Diana Mateus
- **Comment**: in Proceedings of iTWIST'20, Paper-ID: 30, Nantes, France, December,
  2-4, 2020
- **Journal**: None
- **Summary**: We study the fully convolutional neural networks in the context of malignancy detection for breast cancer screening. We work on a supervised segmentation task looking for an acceptable compromise between the precision of the network and the computational complexity.



### MRI Images Analysis Method for Early Stage Alzheimer's Disease Detection
- **Arxiv ID**: http://arxiv.org/abs/2012.00830v1
- **DOI**: 10.22937/IJCSNS.2020.20.09.26
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2012.00830v1)
- **Published**: 2020-11-27 12:36:36+00:00
- **Updated**: 2020-11-27 12:36:36+00:00
- **Authors**: Achraf Ben Miled, Taoufik Yeferny, Amira ben Rabeh
- **Comment**: None
- **Journal**: IJCSNS International Journal of Computer Science and Network
  Security, VOL.20 No.9, September 2020
- **Summary**: Alzheimer's disease is a neurogenerative disease that alters memories, cognitive functions leading to death. Early diagnosis of the disease, by detection of the preliminary stage, called Mild Cognitive Impairment (MCI), remains a challenging issue. In this respect, we introduce, in this paper, a powerful classification architecture that implements the pre-trained network AlexNet to automatically extract the most prominent features from Magnetic Resonance Imaging (MRI) images in order to detect the Alzheimer's disease at the MCI stage. The proposed method is evaluated using a big database from OASIS Database Brain. Various sections of the brain: frontal, sagittal and axial were used. The proposed method achieved 96.83% accuracy by using 420 subjects: 210 Normal and 210 MRI



### 3D Invisible Cloak
- **Arxiv ID**: http://arxiv.org/abs/2011.13705v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.13705v1)
- **Published**: 2020-11-27 12:43:04+00:00
- **Updated**: 2020-11-27 12:43:04+00:00
- **Authors**: Mingfu Xue, Can He, Zhiyu Wu, Jian Wang, Zhe Liu, Weiqiang Liu
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a novel physical stealth attack against the person detectors in real world. The proposed method generates an adversarial patch, and prints it on real clothes to make a three dimensional (3D) invisible cloak. Anyone wearing the cloak can evade the detection of person detectors and achieve stealth. We consider the impacts of those 3D physical constraints (i.e., radian, wrinkle, occlusion, angle, etc.) on person stealth attacks, and propose 3D transformations to generate 3D invisible cloak. We launch the person stealth attacks in 3D physical space instead of 2D plane by printing the adversarial patches on real clothes under challenging and complex 3D physical scenarios. The conventional and 3D transformations are performed on the patch during its optimization process. Further, we study how to generate the optimal 3D invisible cloak. Specifically, we explore how to choose input images with specific shapes and colors to generate the optimal 3D invisible cloak. Besides, after successfully making the object detector misjudge the person as other objects, we explore how to make a person completely disappeared, i.e., the person will not be detected as any objects. Finally, we present a systematic evaluation framework to methodically evaluate the performance of the proposed attack in digital domain and physical world. Experimental results in various indoor and outdoor physical scenarios show that, the proposed person stealth attack method is robust and effective even under those complex and challenging physical conditions, such as the cloak is wrinkled, obscured, curved, and from different angles. The attack success rate in digital domain (Inria data set) is 86.56%, while the static and dynamic stealth attack performance in physical world is 100% and 77%, respectively, which are significantly better than existing works.



### Detection of Malaria Vector Breeding Habitats using Topographic Models
- **Arxiv ID**: http://arxiv.org/abs/2011.13714v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2011.13714v1)
- **Published**: 2020-11-27 12:59:55+00:00
- **Updated**: 2020-11-27 12:59:55+00:00
- **Authors**: Aishwarya Jadhav
- **Comment**: Presented at NeurIPS 2020 Workshop on Machine Learning for the
  Developing World
- **Journal**: None
- **Summary**: Treatment of stagnant water bodies that act as a breeding site for malarial vectors is a fundamental step in most malaria elimination campaigns. However, identification of such water bodies over large areas is expensive, labour-intensive and time-consuming and hence, challenging in countries with limited resources. Practical models that can efficiently locate water bodies can target the limited resources by greatly reducing the area that needs to be scanned by the field workers. To this end, we propose a practical topographic model based on easily available, global, high-resolution DEM data to predict locations of potential vector-breeding water sites. We surveyed the Obuasi region of Ghana to assess the impact of various topographic features on different types of water bodies and uncover the features that significantly influence the formation of aquatic habitats. We further evaluate the effectiveness of multiple models. Our best model significantly outperforms earlier attempts that employ topographic variables for detection of small water sites, even the ones that utilize additional satellite imagery data and demonstrates robustness across different settings.



### A study of traits that affect learnability in GANs
- **Arxiv ID**: http://arxiv.org/abs/2011.13728v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.13728v1)
- **Published**: 2020-11-27 13:31:37+00:00
- **Updated**: 2020-11-27 13:31:37+00:00
- **Authors**: Niladri Shekhar Dutt, Sunil Patel
- **Comment**: None
- **Journal**: None
- **Summary**: Generative Adversarial Networks GANs are algorithmic architectures that use two neural networks, pitting one against the opposite so as to come up with new, synthetic instances of data that can pass for real data. Training a GAN is a challenging problem which requires us to apply advanced techniques like hyperparameter tuning, architecture engineering etc. Many different losses, regularization and normalization schemes, network architectures have been proposed to solve this challenging problem for different types of datasets. It becomes necessary to understand the experimental observations and deduce a simple theory for it. In this paper, we perform empirical experiments using parameterized synthetic datasets to probe what traits affect learnability.



### Rethinking Generalization in American Sign Language Prediction for Edge Devices with Extremely Low Memory Footprint
- **Arxiv ID**: http://arxiv.org/abs/2011.13741v2
- **DOI**: 10.1109/RAICS51191.2020.9332480
- **Categories**: **cs.LG**, cs.AI, cs.CL, cs.CV, cs.HC, 68T45, 68T10, 68T07, 68U10, I.2.10; I.4.8; I.5.1; J.3; I.4.1; K.4.2
- **Links**: [PDF](http://arxiv.org/pdf/2011.13741v2)
- **Published**: 2020-11-27 14:05:42+00:00
- **Updated**: 2021-02-13 10:24:01+00:00
- **Authors**: Aditya Jyoti Paul, Puranjay Mohan, Stuti Sehgal
- **Comment**: 6 pages, Published in IEEE RAICS 2020, see https://raics.in
- **Journal**: 2020 IEEE Recent Advances in Intelligent Computational Systems
  (RAICS), 2020, pp. 147-152
- **Summary**: Due to the boom in technical compute in the last few years, the world has seen massive advances in artificially intelligent systems solving diverse real-world problems. But a major roadblock in the ubiquitous acceptance of these models is their enormous computational complexity and memory footprint. Hence efficient architectures and training techniques are required for deployment on extremely low resource inference endpoints. This paper proposes an architecture for detection of alphabets in American Sign Language on an ARM Cortex-M7 microcontroller having just 496 KB of framebuffer RAM. Leveraging parameter quantization is a common technique that might cause varying drops in test accuracy. This paper proposes using interpolation as augmentation amongst other techniques as an efficient method of reducing this drop, which also helps the model generalize well to previously unseen noisy data. The proposed model is about 185 KB post-quantization and inference speed is 20 frames per second.



### Uncertainty-driven ensembles of deep architectures for multiclass classification. Application to COVID-19 diagnosis in chest X-ray images
- **Arxiv ID**: http://arxiv.org/abs/2011.14894v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2011.14894v1)
- **Published**: 2020-11-27 14:06:25+00:00
- **Updated**: 2020-11-27 14:06:25+00:00
- **Authors**: Juan E. Arco, A. Ortiz, J. Ramirez, F. J. Martinez-Murcia, Yu-Dong Zhang, Juan M. Gorriz
- **Comment**: 1 Table, 7 Figures
- **Journal**: None
- **Summary**: Respiratory diseases kill million of people each year. Diagnosis of these pathologies is a manual, time-consuming process that has inter and intra-observer variability, delaying diagnosis and treatment. The recent COVID-19 pandemic has demonstrated the need of developing systems to automatize the diagnosis of pneumonia, whilst Convolutional Neural Network (CNNs) have proved to be an excellent option for the automatic classification of medical images. However, given the need of providing a confidence classification in this context it is crucial to quantify the reliability of the model's predictions. In this work, we propose a multi-level ensemble classification system based on a Bayesian Deep Learning approach in order to maximize performance while quantifying the uncertainty of each classification decision. This tool combines the information extracted from different architectures by weighting their results according to the uncertainty of their predictions. Performance of the Bayesian network is evaluated in a real scenario where simultaneously differentiating between four different pathologies: control vs bacterial pneumonia vs viral pneumonia vs COVID-19 pneumonia. A three-level decision tree is employed to divide the 4-class classification into three binary classifications, yielding an accuracy of 98.06% and overcoming the results obtained by recent literature. The reduced preprocessing needed for obtaining this high performance, in addition to the information provided about the reliability of the predictions evidence the applicability of the system to be used as an aid for clinicians.



### Robust Autonomous Landing of UAV in Non-Cooperative Environments based on Dynamic Time Camera-LiDAR Fusion
- **Arxiv ID**: http://arxiv.org/abs/2011.13761v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.13761v1)
- **Published**: 2020-11-27 14:47:02+00:00
- **Updated**: 2020-11-27 14:47:02+00:00
- **Authors**: Lyujie Chen, Xiaming Yuan, Yao Xiao, Yiding Zhang, Jihong Zhu
- **Comment**: Submitted to ICRA 2021
- **Journal**: None
- **Summary**: Selecting safe landing sites in non-cooperative environments is a key step towards the full autonomy of UAVs. However, the existing methods have the common problems of poor generalization ability and robustness. Their performance in unknown environments is significantly degraded and the error cannot be self-detected and corrected. In this paper, we construct a UAV system equipped with low-cost LiDAR and binocular cameras to realize autonomous landing in non-cooperative environments by detecting the flat and safe ground area. Taking advantage of the non-repetitive scanning and high FOV coverage characteristics of LiDAR, we come up with a dynamic time depth completion algorithm. In conjunction with the proposed self-evaluation method of the depth map, our model can dynamically select the LiDAR accumulation time at the inference phase to ensure an accurate prediction result. Based on the depth map, the high-level terrain information such as slope, roughness, and the size of the safe area are derived. We have conducted extensive autonomous landing experiments in a variety of familiar or completely unknown environments, verifying that our model can adaptively balance the accuracy and speed, and the UAV can robustly select a safe landing site.



### Image Generators with Conditionally-Independent Pixel Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2011.13775v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.13775v1)
- **Published**: 2020-11-27 15:16:11+00:00
- **Updated**: 2020-11-27 15:16:11+00:00
- **Authors**: Ivan Anokhin, Kirill Demochkin, Taras Khakhulin, Gleb Sterkin, Victor Lempitsky, Denis Korzhenkov
- **Comment**: None
- **Journal**: None
- **Summary**: Existing image generator networks rely heavily on spatial convolutions and, optionally, self-attention blocks in order to gradually synthesize images in a coarse-to-fine manner. Here, we present a new architecture for image generators, where the color value at each pixel is computed independently given the value of a random latent vector and the coordinate of that pixel. No spatial convolutions or similar operations that propagate information across pixels are involved during the synthesis. We analyze the modeling capabilities of such generators when trained in an adversarial fashion, and observe the new generators to achieve similar generation quality to state-of-the-art convolutional generators. We also investigate several interesting properties unique to the new architecture.



### Enhancing Diversity in Teacher-Student Networks via Asymmetric branches for Unsupervised Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/2011.13776v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.13776v1)
- **Published**: 2020-11-27 15:17:10+00:00
- **Updated**: 2020-11-27 15:17:10+00:00
- **Authors**: Hao Chen, Benoit Lagadec, Francois Bremond
- **Comment**: WACV 2021
- **Journal**: None
- **Summary**: The objective of unsupervised person re-identification (Re-ID) is to learn discriminative features without labor-intensive identity annotations. State-of-the-art unsupervised Re-ID methods assign pseudo labels to unlabeled images in the target domain and learn from these noisy pseudo labels. Recently introduced Mean Teacher Model is a promising way to mitigate the label noise. However, during the training, self-ensembled teacher-student networks quickly converge to a consensus which leads to a local minimum. We explore the possibility of using an asymmetric structure inside neural network to address this problem. First, asymmetric branches are proposed to extract features in different manners, which enhances the feature diversity in appearance signatures. Then, our proposed cross-branch supervision allows one branch to get supervision from the other branch, which transfers distinct knowledge and enhances the weight diversity between teacher and student networks. Extensive experiments show that our proposed method can significantly surpass the performance of previous work on both unsupervised domain adaptation and fully unsupervised Re-ID tasks.



### Spherical Interpolated Convolutional Network with Distance-Feature Density for 3D Semantic Segmentation of Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2011.13784v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.13784v1)
- **Published**: 2020-11-27 15:35:12+00:00
- **Updated**: 2020-11-27 15:35:12+00:00
- **Authors**: Guangming Wang, Yehui Yang, Huixin Zhang, Zhe Liu, Hesheng Wang
- **Comment**: 10 pages, 10 figures, under review
- **Journal**: None
- **Summary**: The semantic segmentation of point clouds is an important part of the environment perception for robots. However, it is difficult to directly adopt the traditional 3D convolution kernel to extract features from raw 3D point clouds because of the unstructured property of point clouds. In this paper, a spherical interpolated convolution operator is proposed to replace the traditional grid-shaped 3D convolution operator. This newly proposed feature extraction operator improves the accuracy of the network and reduces the parameters of the network. In addition, this paper analyzes the defect of point cloud interpolation methods based on the distance as the interpolation weight and proposes the self-learned distance-feature density by combining the distance and the feature correlation. The proposed method makes the feature extraction of spherical interpolated convolution network more rational and effective. The effectiveness of the proposed network is demonstrated on the 3D semantic segmentation task of point clouds. Experiments show that the proposed method achieves good performance on the ScanNet dataset and Paris-Lille-3D dataset.



### Navigating the GAN Parameter Space for Semantic Image Editing
- **Arxiv ID**: http://arxiv.org/abs/2011.13786v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.13786v3)
- **Published**: 2020-11-27 15:38:56+00:00
- **Updated**: 2021-04-21 12:45:11+00:00
- **Authors**: Anton Cherepkov, Andrey Voynov, Artem Babenko
- **Comment**: Supplementary code: https://github.com/yandex-research/navigan
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) are currently an indispensable tool for visual editing, being a standard component of image-to-image translation and image restoration pipelines. Furthermore, GANs are especially useful for controllable generation since their latent spaces contain a wide range of interpretable directions, well suited for semantic editing operations. By gradually changing latent codes along these directions, one can produce impressive visual effects, unattainable without GANs.   In this paper, we significantly expand the range of visual effects achievable with the state-of-the-art models, like StyleGAN2. In contrast to existing works, which mostly operate by latent codes, we discover interpretable directions in the space of the generator parameters. By several simple methods, we explore this space and demonstrate that it also contains a plethora of interpretable directions, which are an excellent source of non-trivial semantic manipulations. The discovered manipulations cannot be achieved by transforming the latent codes and can be used to edit both synthetic and real images. We release our code and models and hope they will serve as a handy tool for further efforts on GAN-based image editing.



### Leveraging Regular Fundus Images for Training UWF Fundus Diagnosis Models via Adversarial Learning and Pseudo-Labeling
- **Arxiv ID**: http://arxiv.org/abs/2011.13816v2
- **DOI**: 10.1109/TMI.2021.3056395
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.13816v2)
- **Published**: 2020-11-27 16:25:30+00:00
- **Updated**: 2021-02-02 03:38:41+00:00
- **Authors**: Lie Ju, Xin Wang, Xin Zhao, Paul Bonnington, Tom Drummond, Zongyuan Ge
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, ultra-widefield (UWF) 200\degree~fundus imaging by Optos cameras has gradually been introduced because of its broader insights for detecting more information on the fundus than regular 30 degree - 60 degree fundus cameras. Compared with UWF fundus images, regular fundus images contain a large amount of high-quality and well-annotated data. Due to the domain gap, models trained by regular fundus images to recognize UWF fundus images perform poorly. Hence, given that annotating medical data is labor intensive and time consuming, in this paper, we explore how to leverage regular fundus images to improve the limited UWF fundus data and annotations for more efficient training. We propose the use of a modified cycle generative adversarial network (CycleGAN) model to bridge the gap between regular and UWF fundus and generate additional UWF fundus images for training. A consistency regularization term is proposed in the loss of the GAN to improve and regulate the quality of the generated data. Our method does not require that images from the two domains be paired or even that the semantic labels be the same, which provides great convenience for data collection. Furthermore, we show that our method is robust to noise and errors introduced by the generated unlabeled data with the pseudo-labeling technique. We evaluated the effectiveness of our methods on several common fundus diseases and tasks, such as diabetic retinopathy (DR) classification, lesion detection and tessellated fundus segmentation. The experimental results demonstrate that our proposed method simultaneously achieves superior generalizability of the learned representations and performance improvements in multiple tasks.



### Generalized Pose-and-Scale Estimation using 4-Point Congruence Constraints
- **Arxiv ID**: http://arxiv.org/abs/2011.13817v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.13817v1)
- **Published**: 2020-11-27 16:30:19+00:00
- **Updated**: 2020-11-27 16:30:19+00:00
- **Authors**: Victor Fragoso, Sudipta Sinha
- **Comment**: None
- **Journal**: None
- **Summary**: We present gP4Pc, a new method for computing the absolute pose of a generalized camera with unknown internal scale from four corresponding 3D point-and-ray pairs. Unlike most pose-and-scale methods, gP4Pc is based on constraints arising from the congruence of shapes defined by two sets of four points related by an unknown similarity transformation. By choosing a novel parametrization for the problem, we derive a system of four quadratic equations in four scalar variables. The variables represent the distances of 3D points along the rays from the camera centers. After solving this system via Groebner basis-based automatic polynomial solvers, we compute the similarity transformation using an efficient 3D point-point alignment method. We also propose a specialized variant of our solver for the case of coplanar points, which is computationally very efficient and about 3x faster than the fastest existing solver. Our experiments on real and synthetic datasets, demonstrate that gP4Pc is among the fastest methods in terms of total running time when used within a RANSAC framework, while achieving competitive numerical stability, accuracy, and robustness to noise.



### SFTrack++: A Fast Learnable Spectral Segmentation Approach for Space-Time Consistent Tracking
- **Arxiv ID**: http://arxiv.org/abs/2011.13843v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.13843v3)
- **Published**: 2020-11-27 17:15:20+00:00
- **Updated**: 2021-11-04 14:04:45+00:00
- **Authors**: Elena Burceanu
- **Comment**: Accepted at Neural Information Processing Systems (NeurIPS) 2020 -
  Pre-registration Workshop and at The International Conference on Computer
  Vision (ICCV) 2021 - Structured Representations for Video Understanding
  Workshop
- **Journal**: None
- **Summary**: We propose an object tracking method, SFTrack++, that smoothly learns to preserve the tracked object consistency over space and time dimensions by taking a spectral clustering approach over the graph of pixels from the video, using a fast 3D filtering formulation for finding the principal eigenvector of this graph's adjacency matrix. To better capture complex aspects of the tracked object, we enrich our formulation to multi-channel inputs, which permit different points of view for the same input. The channel inputs are in our experiments, the output of multiple tracking methods. After combining them, instead of relying only on hidden layers representations to predict a good tracking bounding box, we explicitly learn an intermediate, more refined one, namely the segmentation map of the tracked object. This prevents the rough common bounding box approach to introduce noise and distractors in the learning process. We test our method, SFTrack++, on five tracking benchmarks: OTB, UAV, NFS, GOT-10k, and TrackingNet, using five top trackers as input. Our experimental results validate the pre-registered hypothesis. We obtain consistent and robust results, competitive on the three traditional benchmarks (OTB, UAV, NFS) and significantly on top of others (by over $1.1\%$ on accuracy) on GOT-10k and TrackingNet, which are newer, larger, and more varied datasets.



### Real-time Active Vision for a Humanoid Soccer Robot Using Deep Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/2011.13851v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2011.13851v1)
- **Published**: 2020-11-27 17:29:48+00:00
- **Updated**: 2020-11-27 17:29:48+00:00
- **Authors**: Soheil Khatibi, Meisam Teimouri, Mahdi Rezaei
- **Comment**: The paper has been accepted in ICAART 2021
- **Journal**: None
- **Summary**: In this paper, we present an active vision method using a deep reinforcement learning approach for a humanoid soccer-playing robot. The proposed method adaptively optimises the viewpoint of the robot to acquire the most useful landmarks for self-localisation while keeping the ball into its viewpoint. Active vision is critical for humanoid decision-maker robots with a limited field of view. To deal with an active vision problem, several probabilistic entropy-based approaches have previously been proposed which are highly dependent on the accuracy of the self-localisation model. However, in this research, we formulate the problem as an episodic reinforcement learning problem and employ a Deep Q-learning method to solve it. The proposed network only requires the raw images of the camera to move the robot's head toward the best viewpoint. The model shows a very competitive rate of 80% success rate in achieving the best viewpoint. We implemented the proposed method on a humanoid robot simulated in Webots simulator. Our evaluations and experimental results show that the proposed method outperforms the entropy-based methods in the RoboCup context, in cases with high self-localisation errors.



### Field of Junctions: Extracting Boundary Structure at Low SNR
- **Arxiv ID**: http://arxiv.org/abs/2011.13866v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.13866v3)
- **Published**: 2020-11-27 17:46:08+00:00
- **Updated**: 2021-11-11 16:13:54+00:00
- **Authors**: Dor Verbin, Todd Zickler
- **Comment**: ICCV 2021. Project page with demo, video, and code:
  https://vision.seas.harvard.edu/foj/
- **Journal**: None
- **Summary**: We introduce a bottom-up model for simultaneously finding many boundary elements in an image, including contours, corners and junctions. The model explains boundary shape in each small patch using a 'generalized M-junction' comprising M angles and a freely-moving vertex. Images are analyzed using non-convex optimization to cooperatively find M+2 junction values at every location, with spatial consistency being enforced by a novel regularizer that reduces curvature while preserving corners and junctions. The resulting 'field of junctions' is simultaneously a contour detector, corner/junction detector, and boundary-aware smoothing of regional appearance. Notably, its unified analysis of contours, corners, junctions and uniform regions allows it to succeed at high noise levels, where other methods for segmentation and boundary detection fail.



### Efficient Scene Compression for Visual-based Localization
- **Arxiv ID**: http://arxiv.org/abs/2011.13894v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.13894v1)
- **Published**: 2020-11-27 18:36:06+00:00
- **Updated**: 2020-11-27 18:36:06+00:00
- **Authors**: Marcela Mera-Trujillo, Benjamin Smith, Victor Fragoso
- **Comment**: None
- **Journal**: None
- **Summary**: Estimating the pose of a camera with respect to a 3D reconstruction or scene representation is a crucial step for many mixed reality and robotics applications. Given the vast amount of available data nowadays, many applications constrain storage and/or bandwidth to work efficiently. To satisfy these constraints, many applications compress a scene representation by reducing its number of 3D points. While state-of-the-art methods use $K$-cover-based algorithms to compress a scene, they are slow and hard to tune. To enhance speed and facilitate parameter tuning, this work introduces a novel approach that compresses a scene representation by means of a constrained quadratic program (QP). Because this QP resembles a one-class support vector machine, we derive a variant of the sequential minimal optimization to solve it. Our approach uses the points corresponding to the support vectors as the subset of points to represent a scene. We also present an efficient initialization method that allows our method to converge quickly. Our experiments on publicly available datasets show that our approach compresses a scene representation quickly while delivering accurate pose estimates.



### Progressively Volumetrized Deep Generative Models for Data-Efficient Contextual Learning of MR Image Recovery
- **Arxiv ID**: http://arxiv.org/abs/2011.13913v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.13913v4)
- **Published**: 2020-11-27 18:55:56+00:00
- **Updated**: 2022-03-12 11:36:28+00:00
- **Authors**: Mahmut Yurt, Muzaffer Özbey, Salman Ul Hassan Dar, Berk Tınaz, Kader Karlı Oğuz, Tolga Çukur
- **Comment**: None
- **Journal**: None
- **Summary**: Magnetic resonance imaging (MRI) offers the flexibility to image a given anatomic volume under a multitude of tissue contrasts. Yet, scan time considerations put stringent limits on the quality and diversity of MRI data. The gold-standard approach to alleviate this limitation is to recover high-quality images from data undersampled across various dimensions, most commonly the Fourier domain or contrast sets. A primary distinction among recovery methods is whether the anatomy is processed per volume or per cross-section. Volumetric models offer enhanced capture of global contextual information, but they can suffer from suboptimal learning due to elevated model complexity. Cross-sectional models with lower complexity offer improved learning behavior, yet they ignore contextual information across the longitudinal dimension of the volume. Here, we introduce a novel progressive volumetrization strategy for generative models (ProvoGAN) that serially decomposes complex volumetric image recovery tasks into successive cross-sectional mappings task-optimally ordered across individual rectilinear dimensions. ProvoGAN effectively captures global context and recovers fine-structural details across all dimensions, while maintaining low model complexity and improved learning behaviour. Comprehensive demonstrations on mainstream MRI reconstruction and synthesis tasks show that ProvoGAN yields superior performance to state-of-the-art volumetric and cross-sectional models.



### Task Programming: Learning Data Efficient Behavior Representations
- **Arxiv ID**: http://arxiv.org/abs/2011.13917v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.13917v2)
- **Published**: 2020-11-27 18:58:32+00:00
- **Updated**: 2021-03-29 17:59:47+00:00
- **Authors**: Jennifer J. Sun, Ann Kennedy, Eric Zhan, David J. Anderson, Yisong Yue, Pietro Perona
- **Comment**: To appear in as an Oral in CVPR 2021. Code:
  https://github.com/neuroethology/TREBA. Project page:
  https://sites.google.com/view/task-programming
- **Journal**: None
- **Summary**: Specialized domain knowledge is often necessary to accurately annotate training sets for in-depth analysis, but can be burdensome and time-consuming to acquire from domain experts. This issue arises prominently in automated behavior analysis, in which agent movements or actions of interest are detected from video tracking data. To reduce annotation effort, we present TREBA: a method to learn annotation-sample efficient trajectory embedding for behavior analysis, based on multi-task self-supervised learning. The tasks in our method can be efficiently engineered by domain experts through a process we call "task programming", which uses programs to explicitly encode structured knowledge from domain experts. Total domain expert effort can be reduced by exchanging data annotation time for the construction of a small number of programmed tasks. We evaluate this trade-off using data from behavioral neuroscience, in which specialized domain knowledge is used to identify behaviors. We present experimental results in three datasets across two domains: mice and fruit flies. Using embeddings from TREBA, we reduce annotation burden by up to a factor of 10 without compromising accuracy compared to state-of-the-art features. Our results thus suggest that task programming and self-supervision can be an effective way to reduce annotation effort for domain experts.



### Unsupervised part representation by Flow Capsules
- **Arxiv ID**: http://arxiv.org/abs/2011.13920v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.13920v2)
- **Published**: 2020-11-27 18:59:42+00:00
- **Updated**: 2021-02-19 18:07:46+00:00
- **Authors**: Sara Sabour, Andrea Tagliasacchi, Soroosh Yazdani, Geoffrey E. Hinton, David J. Fleet
- **Comment**: None
- **Journal**: None
- **Summary**: Capsule networks aim to parse images into a hierarchy of objects, parts and relations. While promising, they remain limited by an inability to learn effective low level part descriptions. To address this issue we propose a way to learn primary capsule encoders that detect atomic parts from a single image. During training we exploit motion as a powerful perceptual cue for part definition, with an expressive decoder for part generation within a layered image model with occlusion. Experiments demonstrate robust part discovery in the presence of multiple objects, cluttered backgrounds, and occlusion. The part decoder infers the underlying shape masks, effectively filling in occluded regions of the detected shapes. We evaluate FlowCapsules on unsupervised part segmentation and unsupervised image classification.



### D-NeRF: Neural Radiance Fields for Dynamic Scenes
- **Arxiv ID**: http://arxiv.org/abs/2011.13961v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.13961v1)
- **Published**: 2020-11-27 19:06:50+00:00
- **Updated**: 2020-11-27 19:06:50+00:00
- **Authors**: Albert Pumarola, Enric Corona, Gerard Pons-Moll, Francesc Moreno-Noguer
- **Comment**: None
- **Journal**: None
- **Summary**: Neural rendering techniques combining machine learning with geometric reasoning have arisen as one of the most promising approaches for synthesizing novel views of a scene from a sparse set of images. Among these, stands out the Neural radiance fields (NeRF), which trains a deep network to map 5D input coordinates (representing spatial location and viewing direction) into a volume density and view-dependent emitted radiance. However, despite achieving an unprecedented level of photorealism on the generated images, NeRF is only applicable to static scenes, where the same spatial location can be queried from different images. In this paper we introduce D-NeRF, a method that extends neural radiance fields to a dynamic domain, allowing to reconstruct and render novel images of objects under rigid and non-rigid motions from a \emph{single} camera moving around the scene. For this purpose we consider time as an additional input to the system, and split the learning process in two main stages: one that encodes the scene into a canonical space and another that maps this canonical representation into the deformed scene at a particular time. Both mappings are simultaneously learned using fully-connected networks. Once the networks are trained, D-NeRF can render novel images, controlling both the camera view and the time variable, and thus, the object movement. We demonstrate the effectiveness of our approach on scenes with objects under rigid, articulated and non-rigid motions. Code, model weights and the dynamic scenes dataset will be released.



### Self supervised contrastive learning for digital histopathology
- **Arxiv ID**: http://arxiv.org/abs/2011.13971v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.13971v2)
- **Published**: 2020-11-27 19:18:45+00:00
- **Updated**: 2021-09-07 20:02:12+00:00
- **Authors**: Ozan Ciga, Tony Xu, Anne L. Martel
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised learning has been a long-standing goal of machine learning and is especially important for medical image analysis, where the learning can compensate for the scarcity of labeled datasets. A promising subclass of unsupervised learning is self-supervised learning, which aims to learn salient features using the raw input as the learning signal. In this paper, we use a contrastive self-supervised learning method called SimCLR that achieved state-of-the-art results on natural-scene images and apply this method to digital histopathology by collecting and pretraining on 57 histopathology datasets without any labels. We find that combining multiple multi-organ datasets with different types of staining and resolution properties improves the quality of the learned features. Furthermore, we find using more images for pretraining leads to a better performance in multiple downstream tasks. Linear classifiers trained on top of the learned features show that networks pretrained on digital histopathology datasets perform better than ImageNet pretrained networks, boosting task performances by more than 28% in F1 scores on average. These findings may also be useful when applying newer contrastive techniques to histopathology data. Pretrained PyTorch models are made publicly available at https://github.com/ozanciga/self-supervised-histopathology.



### Trends in deep learning for medical hyperspectral image analysis
- **Arxiv ID**: http://arxiv.org/abs/2011.13974v1
- **DOI**: 10.1109/ACCESS.2021.3068392
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.13974v1)
- **Published**: 2020-11-27 19:42:06+00:00
- **Updated**: 2020-11-27 19:42:06+00:00
- **Authors**: Uzair Khan, Paheding Sidike, Colin Elkin, Vijay Devabhaktuni
- **Comment**: None
- **Journal**: in IEEE Access, vol. 9, pp. 79534-79548, 2021
- **Summary**: Deep learning algorithms have seen acute growth of interest in their applications throughout several fields of interest in the last decade, with medical hyperspectral imaging being a particularly promising domain. So far, to the best of our knowledge, there is no review paper that discusses the implementation of deep learning for medical hyperspectral imaging, which is what this review paper aims to accomplish by examining publications that currently utilize deep learning to perform effective analysis of medical hyperspectral imagery. This paper discusses deep learning concepts that are relevant and applicable to medical hyperspectral imaging analysis, several of which have been implemented since the boom in deep learning. This will comprise of reviewing the use of deep learning for classification, segmentation, and detection in order to investigate the analysis of medical hyperspectral imaging. Lastly, we discuss the current and future challenges pertaining to this discipline and the possible efforts to overcome such trials.



### Active Learning in CNNs via Expected Improvement Maximization
- **Arxiv ID**: http://arxiv.org/abs/2011.14015v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, I.2.6; I.5.4; I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/2011.14015v1)
- **Published**: 2020-11-27 22:06:52+00:00
- **Updated**: 2020-11-27 22:06:52+00:00
- **Authors**: Udai G. Nagpal, David A Knowles
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning models such as Convolutional Neural Networks (CNNs) have demonstrated high levels of effectiveness in a variety of domains, including computer vision and more recently, computational biology. However, training effective models often requires assembling and/or labeling large datasets, which may be prohibitively time-consuming or costly. Pool-based active learning techniques have the potential to mitigate these issues, leveraging models trained on limited data to selectively query unlabeled data points from a pool in an attempt to expedite the learning process. Here we present "Dropout-based Expected IMprOvementS" (DEIMOS), a flexible and computationally-efficient approach to active learning that queries points that are expected to maximize the model's improvement across a representative sample of points. The proposed framework enables us to maintain a prediction covariance matrix capturing model uncertainty, and to dynamically update this matrix in order to generate diverse batches of points in the batch-mode setting. Our active learning results demonstrate that DEIMOS outperforms several existing baselines across multiple regression and classification tasks taken from computer vision and genomics.



### Rethinking Text Segmentation: A Novel Dataset and A Text-Specific Refinement Approach
- **Arxiv ID**: http://arxiv.org/abs/2011.14021v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.14021v1)
- **Published**: 2020-11-27 22:50:09+00:00
- **Updated**: 2020-11-27 22:50:09+00:00
- **Authors**: Xingqian Xu, Zhifei Zhang, Zhaowen Wang, Brian Price, Zhonghao Wang, Humphrey Shi
- **Comment**: None
- **Journal**: None
- **Summary**: Text segmentation is a prerequisite in many real-world text-related tasks, e.g., text style transfer, and scene text removal. However, facing the lack of high-quality datasets and dedicated investigations, this critical prerequisite has been left as an assumption in many works, and has been largely overlooked by current research. To bridge this gap, we proposed TextSeg, a large-scale fine-annotated text dataset with six types of annotations: word- and character-wise bounding polygons, masks and transcriptions. We also introduce Text Refinement Network (TexRNet), a novel text segmentation approach that adapts to the unique properties of text, e.g. non-convex boundary, diverse texture, etc., which often impose burdens on traditional segmentation models. In our TexRNet, we propose text specific network designs to address such challenges, including key features pooling and attention-based similarity checking. We also introduce trimap and discriminator losses that show significant improvement on text segmentation. Extensive experiments are carried out on both our TextSeg dataset and other existing datasets. We demonstrate that TexRNet consistently improves text segmentation performance by nearly 2% compared to other state-of-the-art segmentation methods. Our dataset and code will be made available at https://github.com/SHI-Labs/Rethinking-Text-Segmentation.



### General Multi-label Image Classification with Transformers
- **Arxiv ID**: http://arxiv.org/abs/2011.14027v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.14027v1)
- **Published**: 2020-11-27 23:20:35+00:00
- **Updated**: 2020-11-27 23:20:35+00:00
- **Authors**: Jack Lanchantin, Tianlu Wang, Vicente Ordonez, Yanjun Qi
- **Comment**: 13 pages, 7 figures
- **Journal**: None
- **Summary**: Multi-label image classification is the task of predicting a set of labels corresponding to objects, attributes or other entities present in an image. In this work we propose the Classification Transformer (C-Tran), a general framework for multi-label image classification that leverages Transformers to exploit the complex dependencies among visual features and labels. Our approach consists of a Transformer encoder trained to predict a set of target labels given an input set of masked labels, and visual features from a convolutional neural network. A key ingredient of our method is a label mask training objective that uses a ternary encoding scheme to represent the state of the labels as positive, negative, or unknown during training. Our model shows state-of-the-art performance on challenging datasets such as COCO and Visual Genome. Moreover, because our model explicitly represents the uncertainty of labels during training, it is more general by allowing us to produce improved results for images with partial or extra label annotations during inference. We demonstrate this additional capability in the COCO, Visual Genome, News500, and CUB image datasets.



