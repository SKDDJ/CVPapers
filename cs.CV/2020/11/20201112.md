# Arxiv Papers in cs.CV on 2020-11-12
### CheXphotogenic: Generalization of Deep Learning Models for Chest X-ray Interpretation to Photos of Chest X-rays
- **Arxiv ID**: http://arxiv.org/abs/2011.06129v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.06129v1)
- **Published**: 2020-11-12 00:16:51+00:00
- **Updated**: 2020-11-12 00:16:51+00:00
- **Authors**: Pranav Rajpurkar, Anirudh Joshi, Anuj Pareek, Jeremy Irvin, Andrew Y. Ng, Matthew Lungren
- **Comment**: Machine Learning for Health (ML4H) at NeurIPS 2020 - Extended
  Abstract
- **Journal**: None
- **Summary**: The use of smartphones to take photographs of chest x-rays represents an appealing solution for scaled deployment of deep learning models for chest x-ray interpretation. However, the performance of chest x-ray algorithms on photos of chest x-rays has not been thoroughly investigated. In this study, we measured the diagnostic performance for 8 different chest x-ray models when applied to photos of chest x-rays. All models were developed by different groups and submitted to the CheXpert challenge, and re-applied to smartphone photos of x-rays in the CheXphoto dataset without further tuning. We found that several models had a drop in performance when applied to photos of chest x-rays, but even with this drop, some models still performed comparably to radiologists. Further investigation could be directed towards understanding how different model training procedures may affect model generalization to photos of chest x-rays.



### Deep Sketch-Based Modeling: Tips and Tricks
- **Arxiv ID**: http://arxiv.org/abs/2011.06133v3
- **DOI**: 10.1109/3DV50981.2020.00064
- **Categories**: **cs.CV**, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2011.06133v3)
- **Published**: 2020-11-12 00:34:08+00:00
- **Updated**: 2021-04-26 09:23:32+00:00
- **Authors**: Yue Zhong, Yulia Gryaditskaya, Honggang Zhang, Yi-Zhe Song
- **Comment**: None
- **Journal**: None
- **Summary**: Deep image-based modeling received lots of attention in recent years, yet the parallel problem of sketch-based modeling has only been briefly studied, often as a potential application. In this work, for the first time, we identify the main differences between sketch and image inputs: (i) style variance, (ii) imprecise perspective, and (iii) sparsity. We discuss why each of these differences can pose a challenge, and even make a certain class of image-based methods inapplicable. We study alternative solutions to address each of the difference. By doing so, we drive out a few important insights: (i) sparsity commonly results in an incorrect prediction of foreground versus background, (ii) diversity of human styles, if not taken into account, can lead to very poor generalization properties, and finally (iii) unless a dedicated sketching interface is used, one can not expect sketches to match a perspective of a fixed viewpoint. Finally, we compare a set of representative deep single-image modeling solutions and show how their performance can be improved to tackle sketch input by taking into consideration the identified critical differences.



### Universal Embeddings for Spatio-Temporal Tagging of Self-Driving Logs
- **Arxiv ID**: http://arxiv.org/abs/2011.06165v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2011.06165v1)
- **Published**: 2020-11-12 02:18:16+00:00
- **Updated**: 2020-11-12 02:18:16+00:00
- **Authors**: Sean Segal, Eric Kee, Wenjie Luo, Abbas Sadat, Ersin Yumer, Raquel Urtasun
- **Comment**: CoRL 2020 (Oral)
- **Journal**: None
- **Summary**: In this paper, we tackle the problem of spatio-temporal tagging of self-driving scenes from raw sensor data. Our approach learns a universal embedding for all tags, enabling efficient tagging of many attributes and faster learning of new attributes with limited data. Importantly, the embedding is spatio-temporally aware, allowing the model to naturally output spatio-temporal tag values. Values can then be pooled over arbitrary regions, in order to, for example, compute the pedestrian density in front of the SDV, or determine if a car is blocking another car at a 4-way intersection. We demonstrate the effectiveness of our approach on a new large scale self-driving dataset, SDVScenes, containing 15 attributes relating to vehicle and pedestrian density, the actions of each actor, the speed of each actor, interactions between actors, and the topology of the road map.



### Bi-tuning of Pre-trained Representations
- **Arxiv ID**: http://arxiv.org/abs/2011.06182v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.06182v1)
- **Published**: 2020-11-12 03:32:25+00:00
- **Updated**: 2020-11-12 03:32:25+00:00
- **Authors**: Jincheng Zhong, Ximei Wang, Zhi Kou, Jianmin Wang, Mingsheng Long
- **Comment**: None
- **Journal**: None
- **Summary**: It is common within the deep learning community to first pre-train a deep neural network from a large-scale dataset and then fine-tune the pre-trained model to a specific downstream task. Recently, both supervised and unsupervised pre-training approaches to learning representations have achieved remarkable advances, which exploit the discriminative knowledge of labels and the intrinsic structure of data, respectively. It follows natural intuition that both discriminative knowledge and intrinsic structure of the downstream task can be useful for fine-tuning, however, existing fine-tuning methods mainly leverage the former and discard the latter. A question arises: How to fully explore the intrinsic structure of data for boosting fine-tuning? In this paper, we propose Bi-tuning, a general learning framework to fine-tuning both supervised and unsupervised pre-trained representations to downstream tasks. Bi-tuning generalizes the vanilla fine-tuning by integrating two heads upon the backbone of pre-trained representations: a classifier head with an improved contrastive cross-entropy loss to better leverage the label information in an instance-contrast way, and a projector head with a newly-designed categorical contrastive learning loss to fully exploit the intrinsic structure of data in a category-consistent way. Comprehensive experiments confirm that Bi-tuning achieves state-of-the-art results for fine-tuning tasks of both supervised and unsupervised pre-trained models by large margins (e.g. 10.7\% absolute rise in accuracy on CUB in low-data regime).



### Gaussian RAM: Lightweight Image Classification via Stochastic Retina-Inspired Glimpse and Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/2011.06190v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.06190v1)
- **Published**: 2020-11-12 04:27:06+00:00
- **Updated**: 2020-11-12 04:27:06+00:00
- **Authors**: Dongseok Shim, H. Jin Kim
- **Comment**: ICCAS 2020 Accepted and Student Best Paper Finalist
- **Journal**: None
- **Summary**: Previous studies on image classification have mainly focused on the performance of the networks, not on real-time operation or model compression. We propose a Gaussian Deep Recurrent visual Attention Model (GDRAM)- a reinforcement learning based lightweight deep neural network for large scale image classification that outperforms the conventional CNN (Convolutional Neural Network) which uses the entire image as input. Highly inspired by the biological visual recognition process, our model mimics the stochastic location of the retina with Gaussian distribution. We evaluate the model on Large cluttered MNIST, Large CIFAR-10 and Large CIFAR-100 datasets which are resized to 128 in both width and height.



### Domain Generalization in Biosignal Classification
- **Arxiv ID**: http://arxiv.org/abs/2011.06207v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.06207v1)
- **Published**: 2020-11-12 05:15:46+00:00
- **Updated**: 2020-11-12 05:15:46+00:00
- **Authors**: Theekshana Dissanayake, Tharindu Fernando, Simon Denman, Houman Ghaemmaghami, Sridha Sridharan, Clinton Fookes
- **Comment**: None
- **Journal**: None
- **Summary**: Objective: When training machine learning models, we often assume that the training data and evaluation data are sampled from the same distribution. However, this assumption is violated when the model is evaluated on another unseen but similar database, even if that database contains the same classes. This problem is caused by domain-shift and can be solved using two approaches: domain adaptation and domain generalization. Simply, domain adaptation methods can access data from unseen domains during training; whereas in domain generalization, the unseen data is not available during training. Hence, domain generalization concerns models that perform well on inaccessible, domain-shifted data. Method: Our proposed domain generalization method represents an unseen domain using a set of known basis domains, afterwhich we classify the unseen domain using classifier fusion. To demonstrate our system, we employ a collection of heart sound databases that contain normal and abnormal sounds (classes). Results: Our proposed classifier fusion method achieves accuracy gains of up to 16% for four completely unseen domains. Conclusion: Recognizing the complexity induced by the inherent temporal nature of biosignal data, the two-stage method proposed in this study is able to effectively simplify the whole process of domain generalization while demonstrating good results on unseen domains and the adopted basis domains. Significance: To our best knowledge, this is the first study that investigates domain generalization for biosignal data. Our proposed learning strategy can be used to effectively learn domain-relevant features while being aware of the class differences in the data.



### A Transfer Learning Framework for Anomaly Detection Using Model of Normality
- **Arxiv ID**: http://arxiv.org/abs/2011.06210v1
- **DOI**: 10.1109/IEMCON51383.2020.9284916
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.06210v1)
- **Published**: 2020-11-12 05:26:32+00:00
- **Updated**: 2020-11-12 05:26:32+00:00
- **Authors**: Sulaiman Aburakhia, Tareq Tayeh, Ryan Myers, Abdallah Shami
- **Comment**: 7 pages, 4 figures, 2 tables, conference: The 11th Annual IEEE
  Information Technology, Electronics and Mobile Communication Conference "IEEE
  IEMCON", Vancouver, Canada, November 2020. IEEE IEMCON'20' best paper award
  in the category of Industrial Automation and Control Systems Technology
- **Journal**: None
- **Summary**: Convolutional Neural Network (CNN) techniques have proven to be very useful in image-based anomaly detection applications. CNN can be used as deep features extractor where other anomaly detection techniques are applied on these features. For this scenario, using transfer learning is common since pretrained models provide deep feature representations that are useful for anomaly detection tasks. Consequentially, anomaly can be detected by applying similarly measure between extracted features and a defined model of normality. A key factor in such approaches is the decision threshold used for detecting anomaly. While most of the proposed methods focus on the approach itself, slight attention has been paid to address decision threshold settings. In this paper, we tackle this problem and propose a welldefined method to set the working-point decision threshold that improves detection accuracy. We introduce a transfer learning framework for anomaly detection based on similarity measure with a Model of Normality (MoN) and show that with the proposed threshold settings, a significant performance improvement can be achieved. Moreover, the framework has low complexity with relaxed computational requirements.



### Unimodal Cyclic Regularization for Training Multimodal Image Registration Networks
- **Arxiv ID**: http://arxiv.org/abs/2011.06214v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.06214v1)
- **Published**: 2020-11-12 05:37:30+00:00
- **Updated**: 2020-11-12 05:37:30+00:00
- **Authors**: Zhe Xu, Jiangpeng Yan, Jie Luo, William Wells, Xiu Li, Jayender Jagadeesan
- **Comment**: 5 pages, 5 figures
- **Journal**: None
- **Summary**: The loss function of an unsupervised multimodal image registration framework has two terms, i.e., a metric for similarity measure and regularization. In the deep learning era, researchers proposed many approaches to automatically learn the similarity metric, which has been shown effective in improving registration performance. However, for the regularization term, most existing multimodal registration approaches still use a hand-crafted formula to impose artificial properties on the estimated deformation field. In this work, we propose a unimodal cyclic regularization training pipeline, which learns task-specific prior knowledge from simpler unimodal registration, to constrain the deformation field of multimodal registration. In the experiment of abdominal CT-MR registration, the proposed method yields better results over conventional regularization methods, especially for severely deformed local regions.



### Unsupervised Multimodal Image Registration with Adaptative Gradient Guidance
- **Arxiv ID**: http://arxiv.org/abs/2011.06216v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2011.06216v1)
- **Published**: 2020-11-12 05:47:20+00:00
- **Updated**: 2020-11-12 05:47:20+00:00
- **Authors**: Zhe Xu, Jiangpeng Yan, Jie Luo, Xiu Li, Jayender Jagadeesan
- **Comment**: 5 pages, under review
- **Journal**: None
- **Summary**: Multimodal image registration (MIR) is a fundamental procedure in many image-guided therapies. Recently, unsupervised learning-based methods have demonstrated promising performance over accuracy and efficiency in deformable image registration. However, the estimated deformation fields of the existing methods fully rely on the to-be-registered image pair. It is difficult for the networks to be aware of the mismatched boundaries, resulting in unsatisfactory organ boundary alignment. In this paper, we propose a novel multimodal registration framework, which leverages the deformation fields estimated from both: (i) the original to-be-registered image pair, (ii) their corresponding gradient intensity maps, and adaptively fuses them with the proposed gated fusion module. With the help of auxiliary gradient-space guidance, the network can concentrate more on the spatial relationship of the organ boundary. Experimental results on two clinically acquired CT-MRI datasets demonstrate the effectiveness of our proposed approach.



### Adding Knowledge to Unsupervised Algorithms for the Recognition of Intent
- **Arxiv ID**: http://arxiv.org/abs/2011.06219v1
- **DOI**: 10.1007/s11263-020-01404-0
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.06219v1)
- **Published**: 2020-11-12 05:57:09+00:00
- **Updated**: 2020-11-12 05:57:09+00:00
- **Authors**: Stuart Synakowski, Qianli Feng, Aleix Martinez
- **Comment**: This is a pre-print of an article published in International Journal
  of Computer Vision. The final authenticated version is available online at:
  https://doi.org/10.1007/s11263-020-01404-0
- **Journal**: None
- **Summary**: Computer vision algorithms performance are near or superior to humans in the visual problems including object recognition (especially those of fine-grained categories), segmentation, and 3D object reconstruction from 2D views. Humans are, however, capable of higher-level image analyses. A clear example, involving theory of mind, is our ability to determine whether a perceived behavior or action was performed intentionally or not. In this paper, we derive an algorithm that can infer whether the behavior of an agent in a scene is intentional or unintentional based on its 3D kinematics, using the knowledge of self-propelled motion, Newtonian motion and their relationship. We show how the addition of this basic knowledge leads to a simple, unsupervised algorithm. To test the derived algorithm, we constructed three dedicated datasets from abstract geometric animation to realistic videos of agents performing intentional and non-intentional actions. Experiments on these datasets show that our algorithm can recognize whether an action is intentional or not, even without training data. The performance is comparable to various supervised baselines quantitatively, with sensible intentionality segmentation qualitatively.



### Decomposing Normal and Abnormal Features of Medical Images for Content-based Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2011.06224v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.06224v1)
- **Published**: 2020-11-12 06:25:49+00:00
- **Updated**: 2020-11-12 06:25:49+00:00
- **Authors**: Kazuma Kobayashi, Ryuichiro Hataya, Yusuke Kurose, Tatsuya Harada, Ryuji Hamamoto
- **Comment**: Machine Learning for Health (ML4H) at NeurIPS 2020 - Extended
  Abstract
- **Journal**: None
- **Summary**: Medical images can be decomposed into normal and abnormal features, which is considered as the compositionality. Based on this idea, we propose an encoder-decoder network to decompose a medical image into two discrete latent codes: a normal anatomy code and an abnormal anatomy code. Using these latent codes, we demonstrate a similarity retrieval by focusing on either normal or abnormal features of medical images.



### A Review of Uncertainty Quantification in Deep Learning: Techniques, Applications and Challenges
- **Arxiv ID**: http://arxiv.org/abs/2011.06225v4
- **DOI**: 10.1016/j.inffus.2021.05.008
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.06225v4)
- **Published**: 2020-11-12 06:41:05+00:00
- **Updated**: 2021-01-06 01:58:12+00:00
- **Authors**: Moloud Abdar, Farhad Pourpanah, Sadiq Hussain, Dana Rezazadegan, Li Liu, Mohammad Ghavamzadeh, Paul Fieguth, Xiaochun Cao, Abbas Khosravi, U Rajendra Acharya, Vladimir Makarenkov, Saeid Nahavandi
- **Comment**: None
- **Journal**: 2021
- **Summary**: Uncertainty quantification (UQ) plays a pivotal role in reduction of uncertainties during both optimization and decision making processes. It can be applied to solve a variety of real-world applications in science and engineering. Bayesian approximation and ensemble learning techniques are two most widely-used UQ methods in the literature. In this regard, researchers have proposed different UQ methods and examined their performance in a variety of applications such as computer vision (e.g., self-driving cars and object detection), image processing (e.g., image restoration), medical image analysis (e.g., medical image classification and segmentation), natural language processing (e.g., text classification, social media texts and recidivism risk-scoring), bioinformatics, etc. This study reviews recent advances in UQ methods used in deep learning. Moreover, we also investigate the application of these methods in reinforcement learning (RL). Then, we outline a few important applications of UQ methods. Finally, we briefly highlight the fundamental research challenges faced by UQ methods and discuss the future research directions in this field.



### DSAM: A Distance Shrinking with Angular Marginalizing Loss for High Performance Vehicle Re-identificatio
- **Arxiv ID**: http://arxiv.org/abs/2011.06228v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.06228v3)
- **Published**: 2020-11-12 06:48:31+00:00
- **Updated**: 2021-09-09 02:59:59+00:00
- **Authors**: Jiangtao Kong, Yu Cheng, Benjia Zhou, Kai Li, Junliang Xing
- **Comment**: None
- **Journal**: None
- **Summary**: Vehicle Re-identification (ReID) is an important yet challenging problem in computer vision. Compared to other visual objects like faces and persons, vehicles simultaneously exhibit much larger intraclass viewpoint variations and interclass visual similarities, making most exiting loss functions designed for face recognition and person ReID unsuitable for vehicle ReID. To obtain a high-performance vehicle ReID model, we present a novel Distance Shrinking with Angular Marginalizing (DSAM) loss function to perform hybrid learning in both the Original Feature Space (OFS) and the Feature Angular Space (FAS) using the local verification and the global identification information. Specifically, it shrinks the distance between samples of the same class locally in the Original Feature Space while keeps samples of different classes far away in the Feature Angular Space. The shrinking and marginalizing operations are performed during each iteration of the training process and are suitable for different SoftMax based loss functions. We evaluate the DSAM loss function on three large vehicle ReID datasets with detailed analyses and extensive comparisons with many competing vehicle ReID methods. Experimental results show that our DSAM loss enhances the SoftMax loss by a large margin on the PKU-VD1-Large dataset: 10.41% for mAP, 5.29% for cmc1, and 4.60% for cmc5. Moreover, the mAP is increased by 9.34% on the PKU-VehicleID dataset and 6.13% on the VeRi-776 dataset. Source code will be released to facilitate further studies in this research direction.



### Automated Model Compression by Jointly Applied Pruning and Quantization
- **Arxiv ID**: http://arxiv.org/abs/2011.06231v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2011.06231v1)
- **Published**: 2020-11-12 07:06:29+00:00
- **Updated**: 2020-11-12 07:06:29+00:00
- **Authors**: Wenting Tang, Xingxing Wei, Bo Li
- **Comment**: None
- **Journal**: None
- **Summary**: In the traditional deep compression framework, iteratively performing network pruning and quantization can reduce the model size and computation cost to meet the deployment requirements. However, such a step-wise application of pruning and quantization may lead to suboptimal solutions and unnecessary time consumption. In this paper, we tackle this issue by integrating network pruning and quantization as a unified joint compression problem and then use AutoML to automatically solve it. We find the pruning process can be regarded as the channel-wise quantization with 0 bit. Thus, the separate two-step pruning and quantization can be simplified as the one-step quantization with mixed precision. This unification not only simplifies the compression pipeline but also avoids the compression divergence. To implement this idea, we propose the automated model compression by jointly applied pruning and quantization (AJPQ). AJPQ is designed with a hierarchical architecture: the layer controller controls the layer sparsity, and the channel controller decides the bit-width for each kernel. Following the same importance criterion, the layer controller and the channel controller collaboratively decide the compression strategy. With the help of reinforcement learning, our one-step compression is automatically achieved. Compared with the state-of-the-art automated compression methods, our method obtains a better accuracy while reducing the storage considerably. For fixed precision quantization, AJPQ can reduce more than five times model size and two times computation with a slight performance increase for Skynet in remote sensing object detection. When mixed-precision is allowed, AJPQ can reduce five times model size with only 1.06% top-5 accuracy decline for MobileNet in the classification task.



### PoseTrackReID: Dataset Description
- **Arxiv ID**: http://arxiv.org/abs/2011.06243v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.06243v1)
- **Published**: 2020-11-12 07:44:25+00:00
- **Updated**: 2020-11-12 07:44:25+00:00
- **Authors**: Andreas Doering, Di Chen, Shanshan Zhang, Bernt Schiele, Juergen Gall
- **Comment**: None
- **Journal**: None
- **Summary**: Current datasets for video-based person re-identification (re-ID) do not include structural knowledge in form of human pose annotations for the persons of interest. Nonetheless, pose information is very helpful to disentangle useful feature information from background or occlusion noise. Especially real-world scenarios, such as surveillance, contain a lot of occlusions in human crowds or by obstacles. On the other hand, video-based person re-ID can benefit other tasks such as multi-person pose tracking in terms of robust feature matching. For that reason, we present PoseTrackReID, a large-scale dataset for multi-person pose tracking and video-based person re-ID. With PoseTrackReID, we want to bridge the gap between person re-ID and multi-person pose tracking. Additionally, this dataset provides a good benchmark for current state-of-the-art methods on multi-frame person re-ID.



### VCE: Variational Convertor-Encoder for One-Shot Generalization
- **Arxiv ID**: http://arxiv.org/abs/2011.06246v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.06246v1)
- **Published**: 2020-11-12 07:58:14+00:00
- **Updated**: 2020-11-12 07:58:14+00:00
- **Authors**: Chengshuai Li, Shuai Han, Jianping Xing
- **Comment**: None
- **Journal**: None
- **Summary**: Variational Convertor-Encoder (VCE) converts an image to various styles; we present this novel architecture for the problem of one-shot generalization and its transfer to new tasks not seen before without additional training. We also improve the performance of variational auto-encoder (VAE) to filter those blurred points using a novel algorithm proposed by us, namely large margin VAE (LMVAE). Two samples with the same property are input to the encoder, and then a convertor is required to processes one of them from the noisy outputs of the encoder; finally, the noise represents a variety of transformation rules and is used to convert new images. The algorithm that combines and improves the condition variational auto-encoder (CVAE) and introspective VAE, we propose this new framework aim to transform graphics instead of generating them; it is used for the one-shot generative process. No sequential inference algorithmic is needed in training. Compared to recent Omniglot datasets, the results show that our model produces more realistic and diverse images.



### SVAM: Saliency-guided Visual Attention Modeling by Autonomous Underwater Robots
- **Arxiv ID**: http://arxiv.org/abs/2011.06252v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2011.06252v2)
- **Published**: 2020-11-12 08:17:21+00:00
- **Updated**: 2022-04-14 15:51:39+00:00
- **Authors**: Md Jahidul Islam, Ruobing Wang, Junaed Sattar
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a holistic approach to saliency-guided visual attention modeling (SVAM) for use by autonomous underwater robots. Our proposed model, named SVAM-Net, integrates deep visual features at various scales and semantics for effective salient object detection (SOD) in natural underwater images. The SVAM-Net architecture is configured in a unique way to jointly accommodate bottom-up and top-down learning within two separate branches of the network while sharing the same encoding layers. We design dedicated spatial attention modules (SAMs) along these learning pathways to exploit the coarse-level and fine-level semantic features for SOD at four stages of abstractions. The bottom-up branch performs a rough yet reasonably accurate saliency estimation at a fast rate, whereas the deeper top-down branch incorporates a residual refinement module (RRM) that provides fine-grained localization of the salient objects. Extensive performance evaluation of SVAM-Net on benchmark datasets clearly demonstrates its effectiveness for underwater SOD. We also validate its generalization performance by several ocean trials' data that include test images of diverse underwater scenes and waterbodies, and also images with unseen natural objects. Moreover, we analyze its computational feasibility for robotic deployments and demonstrate its utility in several important use cases of visual attention modeling.



### Learning to Segment Dynamic Objects using SLAM Outliers
- **Arxiv ID**: http://arxiv.org/abs/2011.06259v2
- **DOI**: 10.1109/ICPR48806.2021.9412341
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.06259v2)
- **Published**: 2020-11-12 08:36:54+00:00
- **Updated**: 2022-10-15 17:15:37+00:00
- **Authors**: Adrian Bojko, Romain Dupont, Mohamed Tamaazousti, Hervé Le Borgne
- **Comment**: Published in the proceedings of ICPR 2020 (25th International
  Conference on Pattern Recognition):
  https://ieeexplore.ieee.org/abstract/document/9412341
- **Journal**: 25th International Conference on Pattern Recognition (ICPR), 2021,
  pp. 9780-9787
- **Summary**: We present a method to automatically learn to segment dynamic objects using SLAM outliers. It requires only one monocular sequence per dynamic object for training and consists in localizing dynamic objects using SLAM outliers, creating their masks, and using these masks to train a semantic segmentation network. We integrate the trained network in ORB-SLAM 2 and LDSO. At runtime we remove features on dynamic objects, making the SLAM unaffected by them. We also propose a new stereo dataset and new metrics to evaluate SLAM robustness. Our dataset includes consensus inversions, i.e., situations where the SLAM uses more features on dynamic objects that on the static background. Consensus inversions are challenging for SLAM as they may cause major SLAM failures. Our approach performs better than the State-of-the-Art on the TUM RGB-D dataset in monocular mode and on our dataset in both monocular and stereo modes.



### UNOC: Understanding Occlusion for Embodied Presence in Virtual Reality
- **Arxiv ID**: http://arxiv.org/abs/2012.03680v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2012.03680v1)
- **Published**: 2020-11-12 09:31:09+00:00
- **Updated**: 2020-11-12 09:31:09+00:00
- **Authors**: Mathias Parger, Chengcheng Tang, Yuanlu Xu, Christopher Twigg, Lingling Tao, Yijing Li, Robert Wang, Markus Steinberger
- **Comment**: None
- **Journal**: None
- **Summary**: Tracking body and hand motions in the 3D space is essential for social and self-presence in augmented and virtual environments. Unlike the popular 3D pose estimation setting, the problem is often formulated as inside-out tracking based on embodied perception (e.g., egocentric cameras, handheld sensors). In this paper, we propose a new data-driven framework for inside-out body tracking, targeting challenges of omnipresent occlusions in optimization-based methods (e.g., inverse kinematics solvers). We first collect a large-scale motion capture dataset with both body and finger motions using optical markers and inertial sensors. This dataset focuses on social scenarios and captures ground truth poses under self-occlusions and body-hand interactions. We then simulate the occlusion patterns in head-mounted camera views on the captured ground truth using a ray casting algorithm and learn a deep neural network to infer the occluded body parts. In the experiments, we show that our method is able to generate high-fidelity embodied poses by applying the proposed method on the task of real-time inside-out body tracking, finger motion synthesis, and 3-point inverse kinematics.



### Image Anomaly Detection by Aggregating Deep Pyramidal Representations
- **Arxiv ID**: http://arxiv.org/abs/2011.06288v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2011.06288v1)
- **Published**: 2020-11-12 09:58:27+00:00
- **Updated**: 2020-11-12 09:58:27+00:00
- **Authors**: Pankaj Mishra, Claudio Piciarelli, Gian Luca Foresti
- **Comment**: Published in First International Conference of Industrial Machine
  Learning ICPR2020
- **Journal**: None
- **Summary**: Anomaly detection consists in identifying, within a dataset, those samples that significantly differ from the majority of the data, representing the normal class. It has many practical applications, e.g. ranging from defective product detection in industrial systems to medical imaging. This paper focuses on image anomaly detection using a deep neural network with multiple pyramid levels to analyze the image features at different scales. We propose a network based on encoding-decoding scheme, using a standard convolutional autoencoders, trained on normal data only in order to build a model of normality. Anomalies can be detected by the inability of the network to reconstruct its input. Experimental results show a good accuracy on MNIST, FMNIST and the recent MVTec Anomaly Detection dataset



### Real-Time Intermediate Flow Estimation for Video Frame Interpolation
- **Arxiv ID**: http://arxiv.org/abs/2011.06294v12
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.06294v12)
- **Published**: 2020-11-12 10:12:06+00:00
- **Updated**: 2022-07-13 06:51:21+00:00
- **Authors**: Zhewei Huang, Tianyuan Zhang, Wen Heng, Boxin Shi, Shuchang Zhou
- **Comment**: Accepted to ECCV 2022
- **Journal**: None
- **Summary**: Real-time video frame interpolation (VFI) is very useful in video processing, media players, and display devices. We propose RIFE, a Real-time Intermediate Flow Estimation algorithm for VFI. To realize a high-quality flow-based VFI method, RIFE uses a neural network named IFNet that can estimate the intermediate flows end-to-end with much faster speed. A privileged distillation scheme is designed for stable IFNet training and improve the overall performance. RIFE does not rely on pre-trained optical flow models and can support arbitrary-timestep frame interpolation with the temporal encoding input. Experiments demonstrate that RIFE achieves state-of-the-art performance on several public benchmarks. Compared with the popular SuperSlomo and DAIN methods, RIFE is 4--27 times faster and produces better results. Furthermore, RIFE can be extended to wider applications thanks to temporal encoding. The code is available at https://github.com/megvii-research/ECCV2022-RIFE.



### Improving Model Accuracy for Imbalanced Image Classification Tasks by Adding a Final Batch Normalization Layer: An Empirical Study
- **Arxiv ID**: http://arxiv.org/abs/2011.06319v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.06319v1)
- **Published**: 2020-11-12 11:27:40+00:00
- **Updated**: 2020-11-12 11:27:40+00:00
- **Authors**: Veysel Kocaman, Ofer M. Shir, Thomas Bäck
- **Comment**: Accepted for presentation and inclusion in ICPR 2020, the 25th
  International Conference on Pattern Recognition
- **Journal**: None
- **Summary**: Some real-world domains, such as Agriculture and Healthcare, comprise early-stage disease indications whose recording constitutes a rare event, and yet, whose precise detection at that stage is critical. In this type of highly imbalanced classification problems, which encompass complex features, deep learning (DL) is much needed because of its strong detection capabilities. At the same time, DL is observed in practice to favor majority over minority classes and consequently suffer from inaccurate detection of the targeted early-stage indications. To simulate such scenarios, we artificially generate skewness (99% vs. 1%) for certain plant types out of the PlantVillage dataset as a basis for classification of scarce visual cues through transfer learning. By randomly and unevenly picking healthy and unhealthy samples from certain plant types to form a training set, we consider a base experiment as fine-tuning ResNet34 and VGG19 architectures and then testing the model performance on a balanced dataset of healthy and unhealthy images. We empirically observe that the initial F1 test score jumps from 0.29 to 0.95 for the minority class upon adding a final Batch Normalization (BN) layer just before the output layer in VGG19. We demonstrate that utilizing an additional BN layer before the output layer in modern CNN architectures has a considerable impact in terms of minimizing the training time and testing error for minority classes in highly imbalanced data sets. Moreover, when the final BN is employed, minimizing the loss function may not be the best way to assure a high F1 test score for minority classes in such problems. That is, the network might perform better even if it is not confident enough while making a prediction; leading to another discussion about why softmax output is not a good uncertainty measure for DL models.



### Unsupervised MR Motion Artifact Deep Learning using Outlier-Rejecting Bootstrap Aggregation
- **Arxiv ID**: http://arxiv.org/abs/2011.06337v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2011.06337v1)
- **Published**: 2020-11-12 12:10:58+00:00
- **Updated**: 2020-11-12 12:10:58+00:00
- **Authors**: Gyutaek Oh, Jeong Eun Lee, Jong Chul Ye
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, deep learning approaches for MR motion artifact correction have been extensively studied. Although these approaches have shown high performance and reduced computational complexity compared to classical methods, most of them require supervised training using paired artifact-free and artifact-corrupted images, which may prohibit its use in many important clinical applications. For example, transient severe motion (TSM) due to acute transient dyspnea in Gd-EOB-DTPA-enhanced MR is difficult to control and model for paired data generation. To address this issue, here we propose a novel unsupervised deep learning scheme through outlier-rejecting bootstrap subsampling and aggregation. This is inspired by the observation that motions usually cause sparse k-space outliers in the phase encoding direction, so k-space subsampling along the phase encoding direction can remove some outliers and the aggregation step can further improve the results from the reconstruction network. Our method does not require any paired data because the training step only requires artifact-free images. Furthermore, to address the smoothing from potential bias to the artifact-free images, the network is trained in an unsupervised manner using optimal transport driven cycleGAN. We verify that our method can be applied for artifact correction from simulated motion as well as real motion from TSM successfully, outperforming existing state-of-the-art deep learning methods.



### StrObe: Streaming Object Detection from LiDAR Packets
- **Arxiv ID**: http://arxiv.org/abs/2011.06425v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2011.06425v2)
- **Published**: 2020-11-12 14:57:44+00:00
- **Updated**: 2020-11-13 17:59:04+00:00
- **Authors**: Davi Frossard, Simon Suo, Sergio Casas, James Tu, Rui Hu, Raquel Urtasun
- **Comment**: To be presented at the 4th Conference on Robot Learning (CoRL 2020)
- **Journal**: None
- **Summary**: Many modern robotics systems employ LiDAR as their main sensing modality due to its geometrical richness. Rolling shutter LiDARs are particularly common, in which an array of lasers scans the scene from a rotating base. Points are emitted as a stream of packets, each covering a sector of the 360{\deg} coverage. Modern perception algorithms wait for the full sweep to be built before processing the data, which introduces an additional latency. For typical 10Hz LiDARs this will be 100ms. As a consequence, by the time an output is produced, it no longer accurately reflects the state of the world. This poses a challenge, as robotics applications require minimal reaction times, such that maneuvers can be quickly planned in the event of a safety-critical situation. In this paper we propose StrObe, a novel approach that minimizes latency by ingesting LiDAR packets and emitting a stream of detections without waiting for the full sweep to be built. StrObe reuses computations from previous packets and iteratively updates a latent spatial representation of the scene, which acts as a memory, as new evidence comes in, resulting in accurate low-latency perception. We demonstrate the effectiveness of our approach on a large scale real-world dataset, showing that StrObe far outperforms the state-of-the-art when latency is taken into account, and matches the performance in the traditional setting.



### Same Object, Different Grasps: Data and Semantic Knowledge for Task-Oriented Grasping
- **Arxiv ID**: http://arxiv.org/abs/2011.06431v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.06431v2)
- **Published**: 2020-11-12 15:08:15+00:00
- **Updated**: 2020-11-13 15:28:44+00:00
- **Authors**: Adithyavairavan Murali, Weiyu Liu, Kenneth Marino, Sonia Chernova, Abhinav Gupta
- **Comment**: Accepted to Conference on Robot Learning (CoRL) 2020
- **Journal**: None
- **Summary**: Despite the enormous progress and generalization in robotic grasping in recent years, existing methods have yet to scale and generalize task-oriented grasping to the same extent. This is largely due to the scale of the datasets both in terms of the number of objects and tasks studied. We address these concerns with the TaskGrasp dataset which is more diverse both in terms of objects and tasks, and an order of magnitude larger than previous datasets. The dataset contains 250K task-oriented grasps for 56 tasks and 191 objects along with their RGB-D information. We take advantage of this new breadth and diversity in the data and present the GCNGrasp framework which uses the semantic knowledge of objects and tasks encoded in a knowledge graph to generalize to new object instances, classes and even new tasks. Our framework shows a significant improvement of around 12% on held-out settings compared to baseline methods which do not use semantics. We demonstrate that our dataset and model are applicable for the real world by executing task-oriented grasps on a real robot on unknown objects. Code, data and supplementary video could be found at https://sites.google.com/view/taskgrasp



### A deep Q-Learning based Path Planning and Navigation System for Firefighting Environments
- **Arxiv ID**: http://arxiv.org/abs/2011.06450v1
- **DOI**: 10.5220/0010267102670277
- **Categories**: **cs.AI**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.06450v1)
- **Published**: 2020-11-12 15:43:17+00:00
- **Updated**: 2020-11-12 15:43:17+00:00
- **Authors**: Manish Bhattarai, Manel Martinez-Ramon
- **Comment**: Accepted to ICAART2021
- **Journal**: None
- **Summary**: Live fire creates a dynamic, rapidly changing environment that presents a worthy challenge for deep learning and artificial intelligence methodologies to assist firefighters with scene comprehension in maintaining their situational awareness, tracking and relay of important features necessary for key decisions as they tackle these catastrophic events. We propose a deep Q-learning based agent who is immune to stress induced disorientation and anxiety and thus able to make clear decisions for navigation based on the observed and stored facts in live fire environments. As a proof of concept, we imitate structural fire in a gaming engine called Unreal Engine which enables the interaction of the agent with the environment. The agent is trained with a deep Q-learning algorithm based on a set of rewards and penalties as per its actions on the environment. We exploit experience replay to accelerate the learning process and augment the learning of the agent with human-derived experiences. The agent trained under this deep Q-learning approach outperforms agents trained through alternative path planning systems and demonstrates this methodology as a promising foundation on which to build a path planning navigation assistant capable of safely guiding fire fighters through live fire environments.



### 3D-OES: Viewpoint-Invariant Object-Factorized Environment Simulators
- **Arxiv ID**: http://arxiv.org/abs/2011.06464v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.06464v1)
- **Published**: 2020-11-12 16:15:52+00:00
- **Updated**: 2020-11-12 16:15:52+00:00
- **Authors**: Hsiao-Yu Fish Tung, Zhou Xian, Mihir Prabhudesai, Shamit Lal, Katerina Fragkiadaki
- **Comment**: None
- **Journal**: None
- **Summary**: We propose an action-conditioned dynamics model that predicts scene changes caused by object and agent interactions in a viewpoint-invariant 3D neural scene representation space, inferred from RGB-D videos. In this 3D feature space, objects do not interfere with one another and their appearance persists over time and across viewpoints. This permits our model to predict future scenes long in the future by simply "moving" 3D object features based on cumulative object motion predictions. Object motion predictions are computed by a graph neural network that operates over the object features extracted from the 3D neural scene representation. Our model's simulations can be decoded by a neural renderer into2D image views from any desired viewpoint, which aids the interpretability of our latent 3D simulation space. We show our model generalizes well its predictions across varying number and appearances of interacting objects as well as across camera viewpoints, outperforming existing 2D and 3D dynamics models. We further demonstrate sim-to-real transfer of the learnt dynamics by applying our model trained solely in simulation to model-based control for pushing objects to desired locations under clutter on a real robotic setup



### Content-based Image Retrieval and the Semantic Gap in the Deep Learning Era
- **Arxiv ID**: http://arxiv.org/abs/2011.06490v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2011.06490v1)
- **Published**: 2020-11-12 17:00:08+00:00
- **Updated**: 2020-11-12 17:00:08+00:00
- **Authors**: Björn Barz, Joachim Denzler
- **Comment**: CBIR workshop at ICPR 2020
- **Journal**: None
- **Summary**: Content-based image retrieval has seen astonishing progress over the past decade, especially for the task of retrieving images of the same object that is depicted in the query image. This scenario is called instance or object retrieval and requires matching fine-grained visual patterns between images. Semantics, however, do not play a crucial role. This brings rise to the question: Do the recent advances in instance retrieval transfer to more generic image retrieval scenarios? To answer this question, we first provide a brief overview of the most relevant milestones of instance retrieval. We then apply them to a semantic image retrieval task and find that they perform inferior to much less sophisticated and more generic methods in a setting that requires image understanding. Following this, we review existing approaches to closing this so-called semantic gap by integrating prior world knowledge. We conclude that the key problem for the further advancement of semantic image retrieval lies in the lack of a standardized task definition and an appropriate benchmark dataset.



### Fit2Form: 3D Generative Model for Robot Gripper Form Design
- **Arxiv ID**: http://arxiv.org/abs/2011.06498v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG, I.2.9; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2011.06498v1)
- **Published**: 2020-11-12 17:09:36+00:00
- **Updated**: 2020-11-12 17:09:36+00:00
- **Authors**: Huy Ha, Shubham Agrawal, Shuran Song
- **Comment**: Conference on Robot Learning 2020
- **Journal**: None
- **Summary**: The 3D shape of a robot's end-effector plays a critical role in determining it's functionality and overall performance. Many industrial applications rely on task-specific gripper designs to ensure the system's robustness and accuracy. However, the process of manual hardware design is both costly and time-consuming, and the quality of the resulting design is dependent on the engineer's experience and domain expertise, which can easily be out-dated or inaccurate. The goal of this work is to use machine learning algorithms to automate the design of task-specific gripper fingers. We propose Fit2Form, a 3D generative design framework that generates pairs of finger shapes to maximize design objectives (i.e., grasp success, stability, and robustness) for target grasp objects. We model the design objectives by training a Fitness network to predict their values for pairs of gripper fingers and their corresponding grasp objects. This Fitness network then provides supervision to a 3D Generative network that produces a pair of 3D finger geometries for the target grasp object. Our experiments demonstrate that the proposed 3D generative design framework generates parallel jaw gripper finger shapes that achieve more stable and robust grasps compared to other general-purpose and task-specific gripper design algorithms. Video can be found at https://youtu.be/utKHP3qb1bg.



### Reinforcement Learning with Videos: Combining Offline Observations with Interaction
- **Arxiv ID**: http://arxiv.org/abs/2011.06507v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2011.06507v2)
- **Published**: 2020-11-12 17:15:48+00:00
- **Updated**: 2021-11-04 20:07:57+00:00
- **Authors**: Karl Schmeckpeper, Oleh Rybkin, Kostas Daniilidis, Sergey Levine, Chelsea Finn
- **Comment**: None
- **Journal**: Conference on Robot Learning (2020)
- **Summary**: Reinforcement learning is a powerful framework for robots to acquire skills from experience, but often requires a substantial amount of online data collection. As a result, it is difficult to collect sufficiently diverse experiences that are needed for robots to generalize broadly. Videos of humans, on the other hand, are a readily available source of broad and interesting experiences. In this paper, we consider the question: can we perform reinforcement learning directly on experience collected by humans? This problem is particularly difficult, as such videos are not annotated with actions and exhibit substantial visual domain shift relative to the robot's embodiment. To address these challenges, we propose a framework for reinforcement learning with videos (RLV). RLV learns a policy and value function using experience collected by humans in combination with data collected by robots. In our experiments, we find that RLV is able to leverage such videos to learn challenging vision-based skills with less than half as many samples as RL methods that learn from scratch.



### Roof fall hazard detection with convolutional neural networks using transfer learning
- **Arxiv ID**: http://arxiv.org/abs/2012.03681v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2012.03681v1)
- **Published**: 2020-11-12 17:16:36+00:00
- **Updated**: 2020-11-12 17:16:36+00:00
- **Authors**: Ergin Isleyen, Sebnem Duzgun, McKell R. Carter
- **Comment**: None
- **Journal**: None
- **Summary**: Roof falls due to geological conditions are major safety hazards in mining and tunneling industries, causing lost work times, injuries, and fatalities. Several large-opening limestone mines in the Eastern and Midwestern United States have roof fall problems caused by high horizontal stresses. The typical hazard management approach for this type of roof fall hazard relies heavily on visual inspections and expert knowledge. In this study, we propose an artificial intelligence (AI) based system for the detection roof fall hazards caused by high horizontal stresses. We use images depicting hazardous and non-hazardous roof conditions to develop a convolutional neural network for autonomous detection of hazardous roof conditions. To compensate for limited input data, we utilize a transfer learning approach. In transfer learning, an already-trained network is used as a starting point for classification in a similar domain. Results confirm that this approach works well for classifying roof conditions as hazardous or safe, achieving a statistical accuracy of 86%. However, accuracy alone is not enough to ensure a reliable hazard management system. System constraints and reliability are improved when the features being used by the network are understood. Therefore, we used a deep learning interpretation technique called integrated gradients to identify the important geologic features in each image for prediction. The analysis of integrated gradients shows that the system mimics expert judgment on roof fall hazard detection. The system developed in this paper demonstrates the potential of deep learning in geological hazard management to complement human experts, and likely to become an essential part of autonomous tunneling operations in those cases where hazard identification heavily depends on expert knowledge.



### Shared Prior Learning of Energy-Based Models for Image Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2011.06539v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NA, eess.IV, math.NA, math.OC, 49J15, 65C30, 65K10, 65L09, 68U10
- **Links**: [PDF](http://arxiv.org/pdf/2011.06539v2)
- **Published**: 2020-11-12 17:56:05+00:00
- **Updated**: 2020-11-13 08:54:13+00:00
- **Authors**: Thomas Pinetz, Erich Kobler, Thomas Pock, Alexander Effland
- **Comment**: 37 pages, 19 figures
- **Journal**: None
- **Summary**: We propose a novel learning-based framework for image reconstruction particularly designed for training without ground truth data, which has three major building blocks: energy-based learning, a patch-based Wasserstein loss functional, and shared prior learning. In energy-based learning, the parameters of an energy functional composed of a learned data fidelity term and a data-driven regularizer are computed in a mean-field optimal control problem. In the absence of ground truth data, we change the loss functional to a patch-based Wasserstein functional, in which local statistics of the output images are compared to uncorrupted reference patches. Finally, in shared prior learning, both aforementioned optimal control problems are optimized simultaneously with shared learned parameters of the regularizer to further enhance unsupervised image reconstruction. We derive several time discretization schemes of the gradient flow and verify their consistency in terms of Mosco convergence. In numerous numerical experiments, we demonstrate that the proposed method generates state-of-the-art results for various image reconstruction applications--even if no ground truth images are available for training.



### Empirical Performance Analysis of Conventional Deep Learning Models for Recognition of Objects in 2-D Images
- **Arxiv ID**: http://arxiv.org/abs/2011.06639v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2011.06639v1)
- **Published**: 2020-11-12 20:14:03+00:00
- **Updated**: 2020-11-12 20:14:03+00:00
- **Authors**: Sangeeta Satish Rao, Nikunj Phutela, V R Badri Prasad
- **Comment**: None
- **Journal**: None
- **Summary**: Artificial Neural Networks, an essential part of Deep Learning, are derived from the structure and functionality of the human brain. It has a broad range of applications ranging from medical analysis to automated driving. Over the past few years, deep learning techniques have improved drastically - models can now be customized to a much greater extent by varying the network architecture, network parameters, among others. We have varied parameters like learning rate, filter size, the number of hidden layers, stride size and the activation function among others to analyze the performance of the model and thus produce a model with the highest performance. The model classifies images into 3 categories, namely, cars, faces and aeroplanes.



### Disassemblable Fieldwork CT Scanner Using a 3D-printed Calibration Phantom
- **Arxiv ID**: http://arxiv.org/abs/2011.06671v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.ins-det
- **Links**: [PDF](http://arxiv.org/pdf/2011.06671v1)
- **Published**: 2020-11-12 22:07:29+00:00
- **Updated**: 2020-11-12 22:07:29+00:00
- **Authors**: Florian Schiffers, Thomas Bochynek, Andre Aichert, Tobias Würfl, Michael Rubenstein, Oliver Cossairt
- **Comment**: This paper was originally published at the 6th International
  Conference on Image Formation in X-Ray Computed Tomography (CTmeeting 2020)
- **Journal**: CT Meeting 2020
- **Summary**: The use of computed tomography (CT) imaging has become of increasing interest to academic areas outside of the field of medical imaging and industrial inspection, e.g., to biology and cultural heritage research. The pecularities of these fields, however, sometimes require that objects need to be imaged on-site, e.g., in field-work conditions or in museum collections. Under these circumstances, it is often not possible to use a commercial device and a custom solution is the only viable option. In order to achieve high image quality under adverse conditions, reliable calibration and trajectory reproduction are usually key requirements for any custom CT scanning system. Here, we introduce the construction of a low-cost disassemblable CT scanner that allows calibration even when trajectory reproduction is not possible due to the limitations imposed by the project conditions. Using 3D-printed in-image calibration phantoms, we compute a projection matrix directly from each captured X-ray projection. We describe our method in detail and show successful tomographic reconstructions of several specimen as proof of concept.



### Trajectory Prediction in Autonomous Driving with a Lane Heading Auxiliary Loss
- **Arxiv ID**: http://arxiv.org/abs/2011.06679v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2011.06679v2)
- **Published**: 2020-11-12 22:51:25+00:00
- **Updated**: 2021-01-28 18:47:34+00:00
- **Authors**: Ross Greer, Nachiket Deo, Mohan Trivedi
- **Comment**: 8 pages, 18 figures
- **Journal**: None
- **Summary**: Predicting a vehicle's trajectory is an essential ability for autonomous vehicles navigating through complex urban traffic scenes. Bird's-eye-view roadmap information provides valuable information for making trajectory predictions, and while state-of-the-art models extract this information via image convolution, auxiliary loss functions can augment patterns inferred from deep learning by further encoding common knowledge of social and legal driving behaviors. Since human driving behavior is inherently multimodal, models which allow for multimodal output tend to outperform single-prediction models on standard metrics. We propose a loss function which enhances such models by enforcing expected driving rules on all predicted modes. Our contribution to trajectory prediction is twofold; we propose a new metric which addresses failure cases of the off-road rate metric by penalizing trajectories that oppose the ascribed heading (flow direction) of a driving lane, and we show this metric to be differentiable and therefore suitable as an auxiliary loss function. We then use this auxiliary loss to extend the the standard multiple trajectory prediction (MTP) and MultiPath models, achieving improved results on the nuScenes prediction benchmark by predicting trajectories which better conform to the lane-following rules of the road.



### Adversarial Image Color Transformations in Explicit Color Filter Space
- **Arxiv ID**: http://arxiv.org/abs/2011.06690v3
- **DOI**: 10.1109/TIFS.2023.3275057
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.06690v3)
- **Published**: 2020-11-12 23:51:37+00:00
- **Updated**: 2023-06-16 10:19:13+00:00
- **Authors**: Zhengyu Zhao, Zhuoran Liu, Martha Larson
- **Comment**: Published at IEEE Transactions on Information Forensics and Security
  2023. Code is available at
  https://github.com/ZhengyuZhao/ACE/tree/master/Journal_version
- **Journal**: None
- **Summary**: Deep Neural Networks have been shown to be vulnerable to adversarial images. Conventional attacks strive for indistinguishable adversarial images with strictly restricted perturbations. Recently, researchers have moved to explore distinguishable yet non-suspicious adversarial images and demonstrated that color transformation attacks are effective. In this work, we propose Adversarial Color Filter (AdvCF), a novel color transformation attack that is optimized with gradient information in the parameter space of a simple color filter. In particular, our color filter space is explicitly specified so that we are able to provide a systematic analysis of model robustness against adversarial color transformations, from both the attack and defense perspectives. In contrast, existing color transformation attacks do not offer the opportunity for systematic analysis due to the lack of such an explicit space. We further demonstrate the effectiveness of our AdvCF in fooling image classifiers and also compare it with other color transformation attacks regarding their robustness to defenses and image acceptability through an extensive user study. We also highlight the human-interpretability of AdvCF and show its superiority over the state-of-the-art human-interpretable color transformation attack on both image acceptability and efficiency. Additional results provide interesting new insights into model robustness against AdvCF in another three visual tasks.



