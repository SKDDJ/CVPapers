# Arxiv Papers in cs.CV on 2020-11-04
### DR-Unet104 for Multimodal MRI brain tumor segmentation
- **Arxiv ID**: http://arxiv.org/abs/2011.02840v2
- **DOI**: 10.1007/978-3-030-72087-2_36
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.02840v2)
- **Published**: 2020-11-04 01:24:26+00:00
- **Updated**: 2021-05-04 14:25:49+00:00
- **Authors**: Jordan Colman, Lei Zhang, Wenting Duan, Xujiong Ye
- **Comment**: Part of the Multimodal Brain Tumor Segmentation 2020 Challenge
  conference proceedings
- **Journal**: BrainLes 2020. Lecture Notes in Computer Science, vol 12659, pp
  410-419
- **Summary**: In this paper we propose a 2D deep residual Unet with 104 convolutional layers (DR-Unet104) for lesion segmentation in brain MRIs. We make multiple additions to the Unet architecture, including adding the 'bottleneck' residual block to the Unet encoder and adding dropout after each convolution block stack. We verified the effect of introducing the regularisation of dropout with small rate (e.g. 0.2) on the architecture, and found a dropout of 0.2 improved the overall performance compared to no dropout, or a dropout of 0.5. We evaluated the proposed architecture as part of the Multimodal Brain Tumor Segmentation (BraTS) 2020 Challenge and compared our method to DeepLabV3+ with a ResNet-V2-152 backbone. We found that the DR-Unet104 achieved a mean dice score coefficient of 0.8862, 0.6756 and 0.6721 for validation data, whole tumor, enhancing tumor and tumor core respectively, an overall improvement on 0.8770, 0.65242 and 0.68134 achieved by DeepLabV3+. Our method produced a final mean DSC of 0.8673, 0.7514 and 0.7983 on whole tumor, enhancing tumor and tumor core on the challenge's testing data. We produced a competitive lesion segmentation architecture, despite only 2D convolutions, having the added benefit that it can be used on lower power computers than a 3D architecture. The source code and trained model for this work is openly available at https://github.com/jordan-colman/DR-Unet104.



### Realtime CNN-based Keypoint Detector with Sobel Filter and CNN-based Descriptor Trained with Keypoint Candidates
- **Arxiv ID**: http://arxiv.org/abs/2011.02119v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.02119v1)
- **Published**: 2020-11-04 04:11:37+00:00
- **Updated**: 2020-11-04 04:11:37+00:00
- **Authors**: Xun Yuan, Ke Hu, Song Chen
- **Comment**: None
- **Journal**: None
- **Summary**: The local feature detector and descriptor are essential in many computer vision tasks, such as SLAM and 3D reconstruction. In this paper, we introduce two separate CNNs, lightweight SobelNet and DesNet, to detect key points and to compute dense local descriptors. The detector and the descriptor work in parallel. Sobel filter provides the edge structure of the input images as the input of CNN. The locations of key points will be obtained after exerting the non-maximum suppression (NMS) process on the output map of the CNN. We design Gaussian loss for the training process of SobelNet to detect corner points as keypoints. At the same time, the input of DesNet is the original grayscale image, and circle loss is used to train DesNet. Besides, output maps of SobelNet are needed while training DesNet. We have evaluated our method on several benchmarks including HPatches benchmark, ETH benchmark, and FM-Bench. SobelNet achieves better or comparable performance with less computation compared with SOTA methods in recent years. The inference time of an image of 640x480 is 7.59ms and 1.09ms for SobelNet and DesNet respectively on RTX 2070 SUPER.



### Learning Discriminative Representations for Fine-Grained Diabetic Retinopathy Grading
- **Arxiv ID**: http://arxiv.org/abs/2011.02120v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.02120v1)
- **Published**: 2020-11-04 04:16:55+00:00
- **Updated**: 2020-11-04 04:16:55+00:00
- **Authors**: Li Tian, Liyan Ma, Zhijie Wen, Shaorong Xie, Yupeng Xu
- **Comment**: 5 pages
- **Journal**: None
- **Summary**: Diabetic retinopathy (DR) is one of the leading causes of blindness. However, no specific symptoms of early DR lead to a delayed diagnosis, which results in disease progression in patients. To determine the disease severity levels, ophthalmologists need to focus on the discriminative parts of the fundus images. In recent years, deep learning has achieved great success in medical image analysis. However, most works directly employ algorithms based on convolutional neural networks (CNNs), which ignore the fact that the difference among classes is subtle and gradual. Hence, we consider automatic image grading of DR as a fine-grained classification task, and construct a bilinear model to identify the pathologically discriminative areas. In order to leverage the ordinal information among classes, we use an ordinal regression method to obtain the soft labels. In addition, other than only using a categorical loss to train our network, we also introduce the metric loss to learn a more discriminative feature space. Experimental results demonstrate the superior performance of the proposed method on two public IDRiD and DeepDR datasets.



### A Two-Stage Cascade Model with Variational Autoencoders and Attention Gates for MRI Brain Tumor Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2011.02881v2
- **DOI**: 10.1007/978-3-030-72084-1_39
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.02881v2)
- **Published**: 2020-11-04 05:55:06+00:00
- **Updated**: 2020-11-28 06:48:15+00:00
- **Authors**: Chenggang Lyu, Hai Shu
- **Comment**: None
- **Journal**: Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic
  Brain Injuries (BrainLes 2020)
- **Summary**: Automatic MRI brain tumor segmentation is of vital importance for the disease diagnosis, monitoring, and treatment planning. In this paper, we propose a two-stage encoder-decoder based model for brain tumor subregional segmentation. Variational autoencoder regularization is utilized in both stages to prevent the overfitting issue. The second-stage network adopts attention gates and is trained additionally using an expanded dataset formed by the first-stage outputs. On the BraTS 2020 validation dataset, the proposed method achieves the mean Dice score of 0.9041, 0.8350, and 0.7958, and Hausdorff distance (95%) of 4.953, 6.299, and 23.608 for the whole tumor, tumor core, and enhancing tumor, respectively. The corresponding results on the BraTS 2020 testing dataset are 0.8729, 0.8357, and 0.8205 for Dice score, and 11.4288, 19.9690, and 15.6711 for Hausdorff distance. The code is publicly available at https://github.com/shu-hai/two-stage-VAE-Attention-gate-BraTS2020.



### Deep Image Compositing
- **Arxiv ID**: http://arxiv.org/abs/2011.02146v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.02146v1)
- **Published**: 2020-11-04 06:12:24+00:00
- **Updated**: 2020-11-04 06:12:24+00:00
- **Authors**: He Zhang, Jianming Zhang, Federico Perazzi, Zhe Lin, Vishal M. Patel
- **Comment**: WACV-2021. A better portrait segmentation technology has been shipped
  in Photoshop 2020. Check this out if you are not sure how to use it.
  https://www.youtube.com/watch?v=v_kitSYKr3s&t=138s
- **Journal**: None
- **Summary**: Image compositing is a task of combining regions from different images to compose a new image. A common use case is background replacement of portrait images. To obtain high quality composites, professionals typically manually perform multiple editing steps such as segmentation, matting and foreground color decontamination, which is very time consuming even with sophisticated photo editing tools. In this paper, we propose a new method which can automatically generate high-quality image compositing without any user input. Our method can be trained end-to-end to optimize exploitation of contextual and color information of both foreground and background images, where the compositing quality is considered in the optimization. Specifically, inspired by Laplacian pyramid blending, a dense-connected multi-stream fusion network is proposed to effectively fuse the information from the foreground and background images at different scales. In addition, we introduce a self-taught strategy to progressively train from easy to complex cases to mitigate the lack of training data. Experiments show that the proposed method can automatically generate high-quality composites and outperforms existing methods both qualitatively and quantitatively.



### Do Noises Bother Human and Neural Networks In the Same Way? A Medical Image Analysis Perspective
- **Arxiv ID**: http://arxiv.org/abs/2011.02155v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.02155v1)
- **Published**: 2020-11-04 06:58:09+00:00
- **Updated**: 2020-11-04 06:58:09+00:00
- **Authors**: Shao-Cheng Wen, Yu-Jen Chen, Zihao Liu, Wujie Wen, Xiaowei Xu, Yiyu Shi, Tsung-Yi Ho, Qianjun Jia, Meiping Huang, Jian Zhuang
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning had already demonstrated its power in medical images, including denoising, classification, segmentation, etc. All these applications are proposed to automatically analyze medical images beforehand, which brings more information to radiologists during clinical assessment for accuracy improvement. Recently, many medical denoising methods had shown their significant artifact reduction result and noise removal both quantitatively and qualitatively. However, those existing methods are developed around human-vision, i.e., they are designed to minimize the noise effect that can be perceived by human eyes. In this paper, we introduce an application-guided denoising framework, which focuses on denoising for the following neural networks. In our experiments, we apply the proposed framework to different datasets, models, and use cases. Experimental results show that our proposed framework can achieve a better result than human-vision denoising network.



### CoT-AMFlow: Adaptive Modulation Network with Co-Teaching Strategy for Unsupervised Optical Flow Estimation
- **Arxiv ID**: http://arxiv.org/abs/2011.02156v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2011.02156v1)
- **Published**: 2020-11-04 07:01:25+00:00
- **Updated**: 2020-11-04 07:01:25+00:00
- **Authors**: Hengli Wang, Rui Fan, Ming Liu
- **Comment**: 13 pages, 3 figures and 6 tables. This paper is accepted by CoRL 2020
- **Journal**: None
- **Summary**: The interpretation of ego motion and scene change is a fundamental task for mobile robots. Optical flow information can be employed to estimate motion in the surroundings. Recently, unsupervised optical flow estimation has become a research hotspot. However, unsupervised approaches are often easy to be unreliable on partially occluded or texture-less regions. To deal with this problem, we propose CoT-AMFlow in this paper, an unsupervised optical flow estimation approach. In terms of the network architecture, we develop an adaptive modulation network that employs two novel module types, flow modulation modules (FMMs) and cost volume modulation modules (CMMs), to remove outliers in challenging regions. As for the training paradigm, we adopt a co-teaching strategy, where two networks simultaneously teach each other about challenging regions to further improve accuracy. Experimental results on the MPI Sintel, KITTI Flow and Middlebury Flow benchmarks demonstrate that our CoT-AMFlow outperforms all other state-of-the-art unsupervised approaches, while still running in real time. Our project page is available at https://sites.google.com/view/cot-amflow.



### An Improved Attention for Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/2011.02164v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2011.02164v3)
- **Published**: 2020-11-04 07:34:54+00:00
- **Updated**: 2021-06-03 19:59:08+00:00
- **Authors**: Tanzila Rahman, Shih-Han Chou, Leonid Sigal, Giuseppe Carenini
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: We consider the problem of Visual Question Answering (VQA). Given an image and a free-form, open-ended, question, expressed in natural language, the goal of VQA system is to provide accurate answer to this question with respect to the image. The task is challenging because it requires simultaneous and intricate understanding of both visual and textual information. Attention, which captures intra- and inter-modal dependencies, has emerged as perhaps the most widely used mechanism for addressing these challenges. In this paper, we propose an improved attention-based architecture to solve VQA. We incorporate an Attention on Attention (AoA) module within encoder-decoder framework, which is able to determine the relation between attention results and queries. Attention module generates weighted average for each query. On the other hand, AoA module first generates an information vector and an attention gate using attention results and current context; and then adds another attention to generate final attended information by multiplying the two. We also propose multimodal fusion module to combine both visual and textual information. The goal of this fusion module is to dynamically decide how much information should be considered from each modality. Extensive experiments on VQA-v2 benchmark dataset show that our method achieves the state-of-the-art performance.



### DAIS: Automatic Channel Pruning via Differentiable Annealing Indicator Search
- **Arxiv ID**: http://arxiv.org/abs/2011.02166v2
- **DOI**: 10.1109/TNNLS.2022.3161284
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.02166v2)
- **Published**: 2020-11-04 07:43:01+00:00
- **Updated**: 2022-04-07 07:29:46+00:00
- **Authors**: Yushuo Guan, Ning Liu, Pengyu Zhao, Zhengping Che, Kaigui Bian, Yanzhi Wang, Jian Tang
- **Comment**: Accepted to IEEE TNNLS
- **Journal**: None
- **Summary**: The convolutional neural network has achieved great success in fulfilling computer vision tasks despite large computation overhead against efficient deployment. Structured (channel) pruning is usually applied to reduce the model redundancy while preserving the network structure, such that the pruned network can be easily deployed in practice. However, existing structured pruning methods require hand-crafted rules which may lead to tremendous pruning space. In this paper, we introduce Differentiable Annealing Indicator Search (DAIS) that leverages the strength of neural architecture search in the channel pruning and automatically searches for the effective pruned model with given constraints on computation overhead. Specifically, DAIS relaxes the binarized channel indicators to be continuous and then jointly learns both indicators and model parameters via bi-level optimization. To bridge the non-negligible discrepancy between the continuous model and the target binarized model, DAIS proposes an annealing-based procedure to steer the indicator convergence towards binarized states. Moreover, DAIS designs various regularizations based on a priori structural knowledge to control the pruning sparsity and to improve model performance. Experimental results show that DAIS outperforms state-of-the-art pruning methods on CIFAR-10, CIFAR-100, and ImageNet.



### Covariance Self-Attention Dual Path UNet for Rectal Tumor Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2011.02880v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.02880v2)
- **Published**: 2020-11-04 08:01:19+00:00
- **Updated**: 2021-01-05 09:38:09+00:00
- **Authors**: Haijun Gao, Bochuan Zheng, Dazhi Pan, Xiangyin Zeng
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning algorithms are preferable for rectal tumor segmentation. However, it is still a challenge task to accurately segment and identify the locations and sizes of rectal tumors by using deep learning methods. To increase the capability of extracting enough feature information for rectal tumor segmentation, we propose a Covariance Self-Attention Dual Path UNet (CSA-DPUNet). The proposed network mainly includes two improvements on UNet: 1) modify UNet that has only one path structure to consist of two contracting path and two expansive paths (nam new network as DPUNet), which can help extract more feature information from CT images; 2) employ the criss-cross self-attention module into DPUNet, meanwhile, replace the original calculation method of correlation operation with covariance operation, which can further enhances the characterization ability of DPUNet and improves the segmentation accuracy of rectal tumors. Experiments illustrate that compared with the current state-of-the-art results, CSA-DPUNet brings 15.31%, 7.2%, 11.8%, and 9.5% improvement in Dice coefficient, P, R, F1, respectively, which demonstrates that our proposed CSA-DPUNet is effective for rectal tumor segmentation.



### Leveraging Temporal Joint Depths for Improving 3D Human Pose Estimation in Video
- **Arxiv ID**: http://arxiv.org/abs/2011.02172v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.02172v1)
- **Published**: 2020-11-04 08:23:41+00:00
- **Updated**: 2020-11-04 08:23:41+00:00
- **Authors**: Naoki Kato, Hiroto Honda, Yusuke Uchida
- **Comment**: None
- **Journal**: None
- **Summary**: The effectiveness of the approaches to predict 3D poses from 2D poses estimated in each frame of a video has been demonstrated for 3D human pose estimation. However, 2D poses without appearance information of persons have much ambiguity with respect to the joint depths. In this paper, we propose to estimate a 3D pose in each frame of a video and refine it considering temporal information. The proposed approach reduces the ambiguity of the joint depths and improves the 3D pose estimation accuracy.



### Robust building footprint extraction from big multi-sensor data using deep competition network
- **Arxiv ID**: http://arxiv.org/abs/2011.02879v3
- **DOI**: 10.5194/isprs-archives-XLII-4-W18-615-2019
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2011.02879v3)
- **Published**: 2020-11-04 09:04:38+00:00
- **Updated**: 2020-11-28 13:06:36+00:00
- **Authors**: Mehdi Khoshboresh-Masouleh, Mohammad R. Saradjian
- **Comment**: 8 pages, 5 figures
- **Journal**: The International Archives of the Photogrammetry, Remote Sensing
  and Spatial Information Sciences, Volume XLII-4/W18, 2019
- **Summary**: Building footprint extraction (BFE) from multi-sensor data such as optical images and light detection and ranging (LiDAR) point clouds is widely used in various fields of remote sensing applications. However, it is still challenging research topic due to relatively inefficient building extraction techniques from variety of complex scenes in multi-sensor data. In this study, we develop and evaluate a deep competition network (DCN) that fuses very high spatial resolution optical remote sensing images with LiDAR data for robust BFE. DCN is a deep superpixelwise convolutional encoder-decoder architecture using the encoder vector quantization with classified structure. DCN consists of five encoding-decoding blocks with convolutional weights for robust binary representation (superpixel) learning. DCN is trained and tested in a big multi-sensor dataset obtained from the state of Indiana in the United States with multiple building scenes. Comparison results of the accuracy assessment showed that DCN has competitive BFE performance in comparison with other deep semantic binary segmentation architectures. Therefore, we conclude that the proposed model is a suitable solution to the robust BFE from big multi-sensor data.



### Hyperspectral classification of blood-like substances using machine learning methods combined with genetic algorithms in transductive and inductive scenarios
- **Arxiv ID**: http://arxiv.org/abs/2011.02188v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2011.02188v1)
- **Published**: 2020-11-04 09:18:16+00:00
- **Updated**: 2020-11-04 09:18:16+00:00
- **Authors**: Filip Pałka, Wojciech Książek, Paweł Pławiak, Michał Romaszewski, Kamil Książek
- **Comment**: None
- **Journal**: None
- **Summary**: This study is focused on applying genetic algorithms (GA) to model and band selection in hyperspectral image classification. We use a forensic-inspired data set of seven hyperspectral images with blood and five visually similar substances to test GA-optimised classifiers in two scenarios: when the training and test data come from the same image and when they come from different images, which is a more challenging task due to significant spectra differences. In our experiments we compare GA with a classic model optimisation through grid search. Our results show that GA-based model optimisation can reduce the number of bands and create an accurate classifier that outperforms the GS-based reference models, provided that during model optimisation it has access to examples similar to test data. We illustrate this with experiment highlighting the importance of a validation set.



### Weed Density and Distribution Estimation for Precision Agriculture using Semi-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2011.02193v2
- **DOI**: 10.1109/ACCESS.2021.3057912
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2011.02193v2)
- **Published**: 2020-11-04 09:35:53+00:00
- **Updated**: 2021-02-18 14:05:01+00:00
- **Authors**: Shantam Shorewala, Armaan Ashfaque, Sidharth R, Ujjwal Verma
- **Comment**: Included few details about the dataset. This paper has been accepted
  in IEEE Access. Citation: S. Shorewala, A. Ashfaque, S. R and U. Verma, "Weed
  Density and Distribution Estimation for Precision Agriculture using
  Semi-Supervised Learning," in IEEE Access, doi: 10.1109/ACCESS.2021.3057912
- **Journal**: IEEE Access, vol 9, 2021
- **Summary**: Uncontrolled growth of weeds can severely affect the crop yield and quality. Unrestricted use of herbicide for weed removal alters biodiversity and cause environmental pollution. Instead, identifying weed-infested regions can aid selective chemical treatment of these regions. Advances in analyzing farm images have resulted in solutions to identify weed plants. However, a majority of these approaches are based on supervised learning methods which requires huge amount of manually annotated images. As a result, these supervised approaches are economically infeasible for the individual farmer because of the wide variety of plant species being cultivated. In this paper, we propose a deep learning-based semi-supervised approach for robust estimation of weed density and distribution across farmlands using only limited color images acquired from autonomous robots. This weed density and distribution can be useful in a site-specific weed management system for selective treatment of infected areas using autonomous robots. In this work, the foreground vegetation pixels containing crops and weeds are first identified using a Convolutional Neural Network (CNN) based unsupervised segmentation. Subsequently, the weed infected regions are identified using a fine-tuned CNN, eliminating the need for designing hand-crafted features. The approach is validated on two datasets of different crop/weed species (1) Crop Weed Field Image Dataset (CWFID), which consists of carrot plant images and the (2) Sugar Beets dataset. The proposed method is able to localize weed-infested regions a maximum recall of 0.99 and estimate weed density with a maximum accuracy of 82.13%. Hence, the proposed approach is shown to generalize to different plant species without the need for extensive labeled data.



### Affine invariant triangulations
- **Arxiv ID**: http://arxiv.org/abs/2011.02197v1
- **DOI**: None
- **Categories**: **cs.CG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.02197v1)
- **Published**: 2020-11-04 09:41:16+00:00
- **Updated**: 2020-11-04 09:41:16+00:00
- **Authors**: Prosenjit Bose, Pilar Cano, Rodrigo I. Silveira
- **Comment**: None
- **Journal**: None
- **Summary**: We study affine invariant 2D triangulation methods. That is, methods that produce the same triangulation for a point set $S$ for any (unknown) affine transformation of $S$. Our work is based on a method by Nielson [A characterization of an affine invariant triangulation. Geom. Mod, 191-210. Springer, 1993] that uses the inverse of the covariance matrix of $S$ to define an affine invariant norm, denoted $A_{S}$, and an affine invariant triangulation, denoted ${DT}_{A_{S}}[S]$. We revisit the $A_{S}$-norm from a geometric perspective, and show that ${DT}_{A_{S}}[S]$ can be seen as a standard Delaunay triangulation of a transformed point set based on $S$. We prove that it retains all of its well-known properties such as being 1-tough, containing a perfect matching, and being a constant spanner of the complete geometric graph of $S$. We show that the $A_{S}$-norm extends to a hierarchy of related geometric structures such as the minimum spanning tree, nearest neighbor graph, Gabriel graph, relative neighborhood graph, and higher order versions of these graphs. In addition, we provide different affine invariant sorting methods of a point set $S$ and of the vertices of a polygon $P$ that can be combined with known algorithms to obtain other affine invariant triangulation methods of $S$ and of $P$.



### Few-Shot Font Generation with Deep Metric Learning
- **Arxiv ID**: http://arxiv.org/abs/2011.02206v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.02206v1)
- **Published**: 2020-11-04 10:12:10+00:00
- **Updated**: 2020-11-04 10:12:10+00:00
- **Authors**: Haruka Aoki, Koki Tsubota, Hikaru Ikuta, Kiyoharu Aizawa
- **Comment**: Accepted to ICPR 2020
- **Journal**: None
- **Summary**: Designing fonts for languages with a large number of characters, such as Japanese and Chinese, is an extremely labor-intensive and time-consuming task. In this study, we addressed the problem of automatically generating Japanese typographic fonts from only a few font samples, where the synthesized glyphs are expected to have coherent characteristics, such as skeletons, contours, and serifs. Existing methods often fail to generate fine glyph images when the number of style reference glyphs is extremely limited. Herein, we proposed a simple but powerful framework for extracting better style features. This framework introduces deep metric learning to style encoders. We performed experiments using black-and-white and shape-distinctive font datasets and demonstrated the effectiveness of the proposed framework.



### Crack Detection as a Weakly-Supervised Problem: Towards Achieving Less Annotation-Intensive Crack Detectors
- **Arxiv ID**: http://arxiv.org/abs/2011.02208v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.02208v1)
- **Published**: 2020-11-04 10:14:33+00:00
- **Updated**: 2020-11-04 10:14:33+00:00
- **Authors**: Yuki Inoue, Hiroto Nagayoshi
- **Comment**: Accepted to ICPR 2020
- **Journal**: None
- **Summary**: Automatic crack detection is a critical task that has the potential to drastically reduce labor-intensive building and road inspections currently being done manually. Recent studies in this field have significantly improved the detection accuracy. However, the methods often heavily rely on costly annotation processes. In addition, to handle a wide variety of target domains, new batches of annotations are usually required for each new environment. This makes the data annotation cost a significant bottleneck when deploying crack detection systems in real life. To resolve this issue, we formulate the crack detection problem as a weakly-supervised problem and propose a two-branched framework. By combining predictions of a supervised model trained on low quality annotations with predictions based on pixel brightness, our framework is less affected by the annotation quality. Experimental results show that the proposed framework retains high detection accuracy even when provided with low quality annotations. Implementation of the proposed framework is publicly available at https://github.com/hitachi-rd-cv/weakly-sup-crackdet.



### Low cost enhanced security face recognition with stereo cameras
- **Arxiv ID**: http://arxiv.org/abs/2011.02222v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.02222v1)
- **Published**: 2020-11-04 10:55:40+00:00
- **Updated**: 2020-11-04 10:55:40+00:00
- **Authors**: Biel Tura Vecino, Martí Cobos, Philippe Salembier
- **Comment**: 5 pages, 9 figures, code available at
  https://github.com/bieltura/Automotive_face_detector
- **Journal**: None
- **Summary**: This article explores a face recognition alternative which seeks to contribute to resolve current security vulnerabilities in most recognition architectures. Current low cost facial authentication software in the market can be fooled by a printed picture of a face due to the lack of depth information. The presented software creates a depth map of the face with the help of a stereo setup, offering a higher level of security than traditional recognition programs. Analysis of the person's identity and facial depth map are processed through deep convolutional neural networks, providing a secure low cost real-time face authentication method.



### Registration Loss Learning for Deep Probabilistic Point Set Registration
- **Arxiv ID**: http://arxiv.org/abs/2011.02229v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.02229v1)
- **Published**: 2020-11-04 11:05:44+00:00
- **Updated**: 2020-11-04 11:05:44+00:00
- **Authors**: Felix Järemo Lawin, Per-Erik Forssén
- **Comment**: 3DV 2020
- **Journal**: None
- **Summary**: Probabilistic methods for point set registration have interesting theoretical properties, such as linear complexity in the number of used points, and they easily generalize to joint registration of multiple point sets. In this work, we improve their recognition performance to match state of the art. This is done by incorporating learned features, by adding a von Mises-Fisher feature model in each mixture component, and by using learned attention weights. We learn these jointly using a registration loss learning strategy (RLL) that directly uses the registration error as a loss, by back-propagating through the registration iterations. This is possible as the probabilistic registration is fully differentiable, and the result is a learning framework that is truly end-to-end. We perform extensive experiments on the 3DMatch and Kitti datasets. The experiments demonstrate that our approach benefits significantly from the integration of the learned features and our learning strategy, outperforming the state-of-the-art on Kitti. Code is available at https://github.com/felja633/RLLReg.



### The Forchheim Image Database for Camera Identification in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2011.02241v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.02241v1)
- **Published**: 2020-11-04 11:54:54+00:00
- **Updated**: 2020-11-04 11:54:54+00:00
- **Authors**: Benjamin Hadwiger, Christian Riess
- **Comment**: None
- **Journal**: None
- **Summary**: Image provenance can represent crucial knowledge in criminal investigation and journalistic fact checking. In the last two decades, numerous algorithms have been proposed for obtaining information on the source camera and distribution history of an image. For a fair ranking of these techniques, it is important to rigorously assess their performance on practically relevant test cases. To this end, a number of datasets have been proposed. However, we argue that there is a gap in existing databases: to our knowledge, there is currently no dataset that simultaneously satisfies two goals, namely a) to cleanly separate scene content and forensic traces, and b) to support realistic post-processing like social media recompression. In this work, we propose the Forchheim Image Database (FODB) to close this gap. It consists of more than 23,000 images of 143 scenes by 27 smartphone cameras, and it allows to cleanly separate image content from forensic artifacts. Each image is provided in 6 different qualities: the original camera-native version, and five copies from social networks. We demonstrate the usefulness of FODB in an evaluation of methods for camera identification. We report three findings. First, the recently proposed general-purpose EfficientNet remarkably outperforms several dedicated forensic CNNs both on clean and compressed images. Second, classifiers obtain a performance boost even on unknown post-processing after augmentation by artificial degradations. Third, FODB's clean separation of scene content and forensic traces imposes important, rigorous boundary conditions for algorithm benchmarking.



### BGGAN: Bokeh-Glass Generative Adversarial Network for Rendering Realistic Bokeh
- **Arxiv ID**: http://arxiv.org/abs/2011.02242v1
- **DOI**: 10.1007/978-3-030-67070-2_14
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2011.02242v1)
- **Published**: 2020-11-04 11:56:34+00:00
- **Updated**: 2020-11-04 11:56:34+00:00
- **Authors**: Ming Qian, Congyu Qiao, Jiamin Lin, Zhenyu Guo, Chenghua Li, Cong Leng, Jian Cheng
- **Comment**: accepted by ECCV workshop 2020
- **Journal**: Proceedings of the European Conference on Computer Vision
  Workshops. 2020: 229-244
- **Summary**: A photo captured with bokeh effect often means objects in focus are sharp while the out-of-focus areas are all blurred. DSLR can easily render this kind of effect naturally. However, due to the limitation of sensors, smartphones cannot capture images with depth-of-field effects directly. In this paper, we propose a novel generator called Glass-Net, which generates bokeh images not relying on complex hardware. Meanwhile, the GAN-based method and perceptual loss are combined for rendering a realistic bokeh effect in the stage of finetuning the model. Moreover, Instance Normalization(IN) is reimplemented in our network, which ensures our tflite model with IN can be accelerated on smartphone GPU. Experiments show that our method is able to render a high-quality bokeh effect and process one $1024 \times 1536$ pixel image in 1.9 seconds on all smartphone chipsets. This approach ranked First in AIM 2020 Rendering Realistic Bokeh Challenge Track 1 \& Track 2.



### Video Generative Adversarial Networks: A Review
- **Arxiv ID**: http://arxiv.org/abs/2011.02250v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2011.02250v1)
- **Published**: 2020-11-04 12:16:05+00:00
- **Updated**: 2020-11-04 12:16:05+00:00
- **Authors**: Nuha Aldausari, Arcot Sowmya, Nadine Marcus, Gelareh Mohammadi
- **Comment**: 24 pages
- **Journal**: None
- **Summary**: With the increasing interest in the content creation field in multiple sectors such as media, education, and entertainment, there is an increasing trend in the papers that uses AI algorithms to generate content such as images, videos, audio, and text. Generative Adversarial Networks (GANs) in one of the promising models that synthesizes data samples that are similar to real data samples. While the variations of GANs models, in general, have been covered to some extent in several survey papers, to the best of our knowledge, this is among the first survey papers that reviews the state-of-the-art video GANs models. This paper first categorized GANs review papers into general GANs review papers, image GANs review papers, and special field GANs review papers such as anomaly detection, medical imaging, or cybersecurity. The paper then summarizes the main improvements in GANs frameworks that are not initially developed for the video domain but have been adopted in multiple video GANs variations. Then, a comprehensive review of video GANs models is provided under two main divisions according to the presence or non-presence of a condition. The conditional models then further grouped according to the type of condition into audio, text, video, and image. The paper is concluded by highlighting the main challenges and limitations of the current video GANs models. A comprehensive list of datasets, applied loss functions, and evaluation metrics is provided in the supplementary material.



### Handwriting Classification for the Analysis of Art-Historical Documents
- **Arxiv ID**: http://arxiv.org/abs/2011.02264v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.02264v1)
- **Published**: 2020-11-04 13:06:46+00:00
- **Updated**: 2020-11-04 13:06:46+00:00
- **Authors**: Christian Bartz, Hendrik Rätz, Christoph Meinel
- **Comment**: Code available at
  https://github.com/hendraet/handwriting-classification
- **Journal**: None
- **Summary**: Digitized archives contain and preserve the knowledge of generations of scholars in millions of documents. The size of these archives calls for automatic analysis since a manual analysis by specialists is often too expensive. In this paper, we focus on the analysis of handwriting in scanned documents from the art-historic archive of the WPI. Since the archive consists of documents written in several languages and lacks annotated training data for the creation of recognition models, we propose the task of handwriting classification as a new step for a handwriting OCR pipeline. We propose a handwriting classification model that labels extracted text fragments, eg, numbers, dates, or words, based on their visual structure. Such a classification supports historians by highlighting documents that contain a specific class of text without the need to read the entire content. To this end, we develop and compare several deep learning-based models for text classification. In extensive experiments, we show the advantages and disadvantages of our proposed approach and discuss possible usage scenarios on a real-world dataset.



### S3-Net: A Fast and Lightweight Video Scene Understanding Network by Single-shot Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2011.02265v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.02265v1)
- **Published**: 2020-11-04 13:09:26+00:00
- **Updated**: 2020-11-04 13:09:26+00:00
- **Authors**: Yuan Cheng, Yuchao Yang, Hai-Bao Chen, Ngai Wong, Hao Yu
- **Comment**: WACV2021
- **Journal**: None
- **Summary**: Real-time understanding in video is crucial in various AI applications such as autonomous driving. This work presents a fast single-shot segmentation strategy for video scene understanding. The proposed net, called S3-Net, quickly locates and segments target sub-scenes, meanwhile extracts structured time-series semantic features as inputs to an LSTM-based spatio-temporal model. Utilizing tensorization and quantization techniques, S3-Net is intended to be lightweight for edge computing. Experiments using CityScapes, UCF11, HMDB51 and MOMENTS datasets demonstrate that the proposed S3-Net achieves an accuracy improvement of 8.1% versus the 3D-CNN based approach on UCF11, a storage reduction of 6.9x and an inference speed of 22.8 FPS on CityScapes with a GTX1080Ti GPU.



### Pixel-wise Dense Detector for Image Inpainting
- **Arxiv ID**: http://arxiv.org/abs/2011.02293v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.02293v2)
- **Published**: 2020-11-04 13:45:27+00:00
- **Updated**: 2020-11-17 09:27:30+00:00
- **Authors**: Ruisong Zhang, Weize Quan, Baoyuan Wu, Zhifeng Li, Dong-Ming Yan
- **Comment**: 12 pages, 9 figures, accepted by Computer Graphics Forum,
  supplementary material link:
  https://evergrow.github.io/GDN_Inpainting_files/GDN_Inpainting_Supplement.pdf
- **Journal**: None
- **Summary**: Recent GAN-based image inpainting approaches adopt an average strategy to discriminate the generated image and output a scalar, which inevitably lose the position information of visual artifacts. Moreover, the adversarial loss and reconstruction loss (e.g., l1 loss) are combined with tradeoff weights, which are also difficult to tune. In this paper, we propose a novel detection-based generative framework for image inpainting, which adopts the min-max strategy in an adversarial process. The generator follows an encoder-decoder architecture to fill the missing regions, and the detector using weakly supervised learning localizes the position of artifacts in a pixel-wise manner. Such position information makes the generator pay attention to artifacts and further enhance them. More importantly, we explicitly insert the output of the detector into the reconstruction loss with a weighting criterion, which balances the weight of the adversarial loss and reconstruction loss automatically rather than manual operation. Experiments on multiple public datasets show the superior performance of the proposed framework. The source code is available at https://github.com/Evergrow/GDN_Inpainting.



### Effective Fusion Factor in FPN for Tiny Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2011.02298v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.02298v2)
- **Published**: 2020-11-04 13:55:10+00:00
- **Updated**: 2020-11-09 09:40:08+00:00
- **Authors**: Yuqi Gong, Xuehui Yu, Yao Ding, Xiaoke Peng, Jian Zhao, Zhenjun Han
- **Comment**: accepted by WACV2021
- **Journal**: None
- **Summary**: FPN-based detectors have made significant progress in general object detection, e.g., MS COCO and PASCAL VOC. However, these detectors fail in certain application scenarios, e.g., tiny object detection. In this paper, we argue that the top-down connections between adjacent layers in FPN bring two-side influences for tiny object detection, not only positive. We propose a novel concept, fusion factor, to control information that deep layers deliver to shallow layers, for adapting FPN to tiny object detection. After series of experiments and analysis, we explore how to estimate an effective value of fusion factor for a particular dataset by a statistical method. The estimation is dependent on the number of objects distributed in each layer. Comprehensive experiments are conducted on tiny object detection datasets, e.g., TinyPerson and Tiny CityPersons. Our results show that when configuring FPN with a proper fusion factor, the network is able to achieve significant performance gains over the baseline on tiny object detection datasets. Codes and models will be released.



### FDRN: A Fast Deformable Registration Network for Medical Images
- **Arxiv ID**: http://arxiv.org/abs/2011.02307v4
- **DOI**: 10.1002/mp.15011
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2011.02307v4)
- **Published**: 2020-11-04 14:09:51+00:00
- **Updated**: 2021-06-23 18:43:32+00:00
- **Authors**: Kaicong Sun, Sven Simon
- **Comment**: None
- **Journal**: None
- **Summary**: Deformable image registration is a fundamental task in medical imaging. Due to the large computational complexity of deformable registration of volumetric images, conventional iterative methods usually face the tradeoff between the registration accuracy and the computation time in practice. In order to boost the registration performance in both accuracy and runtime, we propose a fast convolutional neural network. Specially, to efficiently utilize the memory resources and enlarge the model capacity, we adopt additive forwarding instead of channel concatenation and deepen the network in each encoder and decoder stage. To facilitate the learning efficiency, we leverage skip connection within the encoder and decoder stages to enable residual learning and employ an auxiliary loss at the bottom layer with lowest resolution to involve deep supervision. Particularly, the low-resolution auxiliary loss is weighted by an exponentially decayed parameter during the training phase. In conjunction with the main loss in high-resolution grid, a coarse-to-fine learning strategy is achieved. Last but not least, we introduce an auxiliary loss based on the segmentation prior to improve the registration performance in Dice score. Comparing to the auxiliary loss using average Dice score, the proposed multi-label segmentation loss does not induce additional memory cost in the training phase and can be employed on images with arbitrary amount of categories. In the experiments, we show FDRN outperforms the existing state-of-the-art registration methods for brain MR images by resorting to the compact network structure and efficient learning. Besides, FDRN is a generalized framework for image registration which is not confined to a particular type of medical images or anatomy.



### Deep Multimodality Learning for UAV Video Aesthetic Quality Assessment
- **Arxiv ID**: http://arxiv.org/abs/2011.02356v1
- **DOI**: 10.1109/TMM.2019.2960656
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.02356v1)
- **Published**: 2020-11-04 15:37:49+00:00
- **Updated**: 2020-11-04 15:37:49+00:00
- **Authors**: Qi Kuang, Xin Jin, Qinping Zhao, Bin Zhou
- **Comment**: IEEE Trans. on Multimedia, 2020
- **Journal**: None
- **Summary**: Despite the growing number of unmanned aerial vehicles (UAVs) and aerial videos, there is a paucity of studies focusing on the aesthetics of aerial videos that can provide valuable information for improving the aesthetic quality of aerial photography. In this article, we present a method of deep multimodality learning for UAV video aesthetic quality assessment. More specifically, a multistream framework is designed to exploit aesthetic attributes from multiple modalities, including spatial appearance, drone camera motion, and scene structure. A novel specially designed motion stream network is proposed for this new multistream framework. We construct a dataset with 6,000 UAV video shots captured by drone cameras. Our model can judge whether a UAV video was shot by professional photographers or amateurs together with the scene type classification. The experimental results reveal that our method outperforms the video classification methods and traditional SVM-based methods for video aesthetics. In addition, we present three application examples of UAV video grading, professional segment detection and aesthetic-based UAV path planning using the proposed method.



### Subtensor Quantization for Mobilenets
- **Arxiv ID**: http://arxiv.org/abs/2011.08009v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.08009v1)
- **Published**: 2020-11-04 15:41:47+00:00
- **Updated**: 2020-11-04 15:41:47+00:00
- **Authors**: Thu Dinh, Andrey Melnikov, Vasilios Daskalopoulos, Sek Chai
- **Comment**: Embedded Vision Workshop, 16th European Conference on Computer Vision
  (ECCV), Aug 2020
- **Journal**: None
- **Summary**: Quantization for deep neural networks (DNN) have enabled developers to deploy models with less memory and more efficient low-power inference. However, not all DNN designs are friendly to quantization. For example, the popular Mobilenet architecture has been tuned to reduce parameter size and computational latency with separable depth-wise convolutions, but not all quantization algorithms work well and the accuracy can suffer against its float point versions. In this paper, we analyzed several root causes of quantization loss and proposed alternatives that do not rely on per-channel or training-aware approaches. We evaluate the image classification task on ImageNet dataset, and our post-training quantized 8-bit inference top-1 accuracy in within 0.7% of the floating point version.



### SD-Measure: A Social Distancing Detector
- **Arxiv ID**: http://arxiv.org/abs/2011.02365v1
- **DOI**: 10.1109/CICN49253.2020.9242628
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.02365v1)
- **Published**: 2020-11-04 15:47:14+00:00
- **Updated**: 2020-11-04 15:47:14+00:00
- **Authors**: Savyasachi Gupta, Rudraksh Kapil, Goutham Kanahasabai, Shreyas Srinivas Joshi, Aniruddha Srinivas Joshi
- **Comment**: Contains 6 pages & 7 figures. Published in 12th CICN 2020
- **Journal**: 12th CICN, 2020, pp. 306-311
- **Summary**: The practice of social distancing is imperative to curbing the spread of contagious diseases and has been globally adopted as a non-pharmaceutical prevention measure during the COVID-19 pandemic. This work proposes a novel framework named SD-Measure for detecting social distancing from video footages. The proposed framework leverages the Mask R-CNN deep neural network to detect people in a video frame. To consistently identify whether social distancing is practiced during the interaction between people, a centroid tracking algorithm is utilised to track the subjects over the course of the footage. With the aid of authentic algorithms for approximating the distance of people from the camera and between themselves, we determine whether the social distancing guidelines are being adhered to. The framework attained a high accuracy value in conjunction with a low false alarm rate when tested on Custom Video Footage Dataset (CVFD) and Custom Personal Images Dataset (CPID), where it manifested its effectiveness in determining whether social distancing guidelines were practiced.



### Deep Learning Framework to Detect Face Masks from Video Footage
- **Arxiv ID**: http://arxiv.org/abs/2011.02371v1
- **DOI**: 10.1109/CICN49253.2020.9242625
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.02371v1)
- **Published**: 2020-11-04 16:02:03+00:00
- **Updated**: 2020-11-04 16:02:03+00:00
- **Authors**: Aniruddha Srinivas Joshi, Shreyas Srinivas Joshi, Goutham Kanahasabai, Rudraksh Kapil, Savyasachi Gupta
- **Comment**: Contains 6 pages and 6 figures. Published in 12th CICN 2020
- **Journal**: 12th CICN, 2020, pp. 435-440
- **Summary**: The use of facial masks in public spaces has become a social obligation since the wake of the COVID-19 global pandemic and the identification of facial masks can be imperative to ensure public safety. Detection of facial masks in video footages is a challenging task primarily due to the fact that the masks themselves behave as occlusions to face detection algorithms due to the absence of facial landmarks in the masked regions. In this work, we propose an approach for detecting facial masks in videos using deep learning. The proposed framework capitalizes on the MTCNN face detection model to identify the faces and their corresponding facial landmarks present in the video frame. These facial images and cues are then processed by a neoteric classifier that utilises the MobileNetV2 architecture as an object detector for identifying masked regions. The proposed framework was tested on a dataset which is a collection of videos capturing the movement of people in public spaces while complying with COVID-19 safety protocols. The proposed methodology demonstrated its effectiveness in detecting facial masks by achieving high precision, recall, and accuracy.



### A Follow-the-Leader Strategy using Hierarchical Deep Neural Networks with Grouped Convolutions
- **Arxiv ID**: http://arxiv.org/abs/2011.07948v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2011.07948v4)
- **Published**: 2020-11-04 16:04:42+00:00
- **Updated**: 2021-04-28 18:43:50+00:00
- **Authors**: Jose Solomon, Francois Charette
- **Comment**: 11 pages, 7 figures, 3 tables
- **Journal**: None
- **Summary**: The task of following-the-leader is implemented using a hierarchical Deep Neural Network (DNN) end-to-end driving model to match the direction and speed of a target pedestrian. The model uses a classifier DNN to determine if the pedestrian is within the field of view of the camera sensor. If the pedestrian is present, the image stream from the camera is fed to a regression DNN which simultaneously adjusts the autonomous vehicle's steering and throttle to keep cadence with the pedestrian. If the pedestrian is not visible, the vehicle uses a straightforward exploratory search strategy to reacquire the tracking objective. The classifier and regression DNNs incorporate grouped convolutions to boost model performance as well as to significantly reduce parameter count and compute latency. The models are trained on the Intelligence Processing Unit (IPU) to leverage its fine-grain compute capabilities in order to minimize time-to-train. The results indicate very robust tracking behavior on the part of the autonomous vehicle in terms of its steering and throttle profiles, while requiring minimal data collection to produce. The throughput in terms of processing training samples has been boosted by the use of the IPU in conjunction with grouped convolutions by a factor ~3.5 for training of the classifier and a factor of ~7 for the regression network. A recording of the vehicle tracking a pedestrian has been produced and is available on the web. This is a preprint of an article published in SN Computer Science. The final authenticated version is available online at: https://doi.org/https://doi.org/10.1007/s42979-021-00572-1.



### Noise Reduction to Compute Tissue Mineral Density and Trabecular Bone Volume Fraction from Low Resolution QCT
- **Arxiv ID**: http://arxiv.org/abs/2011.02382v1
- **DOI**: 10.1016/j.compmedimag.2020.101816
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.02382v1)
- **Published**: 2020-11-04 16:17:24+00:00
- **Updated**: 2020-11-04 16:17:24+00:00
- **Authors**: Felix Thomsen, José M. Fuertes García, Manuel Lucena, Juan Pisula, Rodrigo de Luis García, Jan Broggrefe, Claudio Delrieux
- **Comment**: A revised version of this manuscript was accepted for publication in
  Computerized Medical Imaging and Graphics
- **Journal**: None
- **Summary**: We propose a 3D neural network with specific loss functions for quantitative computed tomography (QCT) noise reduction to compute micro-structural parameters such as tissue mineral density (TMD) and bone volume ratio (BV/TV) with significantly higher accuracy than using no or standard noise reduction filters. The vertebra-phantom study contained high resolution peripheral and clinical CT scans with simulated in vivo CT noise and nine repetitions of three different tube currents (100, 250 and 360 mAs). Five-fold cross validation was performed on 20466 purely spongy pairs of noisy and ground-truth patches. Comparison of training and test errors revealed high robustness against over-fitting. While not showing effects for the assessment of BMD and voxel-wise densities, the filter improved thoroughly the computation of TMD and BV/TV with respect to the unfiltered data. Root-mean-square and accuracy errors of low resolution TMD and BV/TV decreased to less than 17% of the initial values. Furthermore filtered low resolution scans revealed still more TMD- and BV/TV-relevant information than high resolution CT scans, either unfiltered or filtered with two state-of-the-art standard denoising methods. The proposed architecture is threshold and rotational invariant, applicable on a wide range of image resolutions at once, and likely serves for an accurate computation of further micro-structural parameters. Furthermore, it is less prone for over-fitting than neural networks that compute structural parameters directly. In conclusion, the method is potentially important for the diagnosis of osteoporosis and other bone diseases since it allows to assess relevant 3D micro-structural information from standard low exposure CT protocols such as 100 mAs and 120 kVp.



### Filter Pruning using Hierarchical Group Sparse Regularization for Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2011.02389v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.02389v1)
- **Published**: 2020-11-04 16:29:41+00:00
- **Updated**: 2020-11-04 16:29:41+00:00
- **Authors**: Kakeru Mitsuno, Takio Kurita
- **Comment**: Accepted to ICPR 2020
- **Journal**: None
- **Summary**: Since the convolutional neural networks are often trained with redundant parameters, it is possible to reduce redundant kernels or filters to obtain a compact network without dropping the classification accuracy. In this paper, we propose a filter pruning method using the hierarchical group sparse regularization. It is shown in our previous work that the hierarchical group sparse regularization is effective in obtaining sparse networks in which filters connected to unnecessary channels are automatically close to zero. After training the convolutional neural network with the hierarchical group sparse regularization, the unnecessary filters are selected based on the increase of the classification loss of the randomly selected training samples to obtain a compact network. It is shown that the proposed method can reduce more than 50% parameters of ResNet for CIFAR-10 with only 0.3% decrease in the accuracy of test samples. Also, 34% parameters of ResNet are reduced for TinyImageNet-200 with higher accuracy than the baseline network.



### Channel Planting for Deep Neural Networks using Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/2011.02390v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.02390v1)
- **Published**: 2020-11-04 16:29:59+00:00
- **Updated**: 2020-11-04 16:29:59+00:00
- **Authors**: Kakeru Mitsuno, Yuichiro Nomura, Takio Kurita
- **Comment**: Accepted to ICPR 2020
- **Journal**: None
- **Summary**: In recent years, deeper and wider neural networks have shown excellent performance in computer vision tasks, while their enormous amount of parameters results in increased computational cost and overfitting. Several methods have been proposed to compress the size of the networks without reducing network performance. Network pruning can reduce redundant and unnecessary parameters from a network. Knowledge distillation can transfer the knowledge of deeper and wider networks to smaller networks. The performance of the smaller network obtained by these methods is bounded by the predefined network. Neural architecture search has been proposed, which can search automatically the architecture of the networks to break the structure limitation. Also, there is a dynamic configuration method to train networks incrementally as sub-networks. In this paper, we present a novel incremental training algorithm for deep neural networks called planting. Our planting can search the optimal network architecture with smaller number of parameters for improving the network performance by augmenting channels incrementally to layers of the initial networks while keeping the earlier trained parameters fixed. Also, we propose using the knowledge distillation method for training the channels planted. By transferring the knowledge of deeper and wider networks, we can grow the networks effectively and efficiently. We evaluate the effectiveness of the proposed method on different datasets such as CIFAR-10/100 and STL-10. For the STL-10 dataset, we show that we are able to achieve comparable performance with only 7% parameters compared to the larger network and reduce the overfitting caused by a small amount of the data.



### Fairness in Biometrics: a figure of merit to assess biometric verification systems
- **Arxiv ID**: http://arxiv.org/abs/2011.02395v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.02395v2)
- **Published**: 2020-11-04 16:46:37+00:00
- **Updated**: 2021-03-30 07:23:41+00:00
- **Authors**: Tiago de Freitas Pereira, Sébastien Marcel
- **Comment**: 11 pages
- **Journal**: None
- **Summary**: Machine learning-based (ML) systems are being largely deployed since the last decade in a myriad of scenarios impacting several instances in our daily lives. With this vast sort of applications, aspects of fairness start to rise in the spotlight due to the social impact that this can get in minorities. In this work aspects of fairness in biometrics are addressed. First, we introduce the first figure of merit that is able to evaluate and compare fairness aspects between multiple biometric verification systems, the so-called Fairness Discrepancy Rate (FDR). A use case with two synthetic biometric systems is introduced and demonstrates the potential of this figure of merit in extreme cases of fair and unfair behavior. Second, a use case using face biometrics is presented where several systems are evaluated compared with this new figure of merit using three public datasets exploring gender and race demographics.



### Graph Based Temporal Aggregation for Video Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2011.02426v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.02426v1)
- **Published**: 2020-11-04 17:23:14+00:00
- **Updated**: 2020-11-04 17:23:14+00:00
- **Authors**: Arvind Srinivasan, Aprameya Bharadwaj, Aveek Saha, Subramanyam Natarajan
- **Comment**: 6 pages, 6 figures, 7 tables
- **Journal**: None
- **Summary**: Large scale video retrieval is a field of study with a lot of ongoing research. Most of the work in the field is on video retrieval through text queries using techniques such as VSE++. However, there is little research done on video retrieval through image queries, and the work that has been done in this field either uses image queries from within the video dataset or iterates through videos frame by frame. These approaches are not generalized for queries from outside the dataset and do not scale well for large video datasets. To overcome these issues, we propose a new approach for video retrieval through image queries where an undirected graph is constructed from the combined set of frames from all videos to be searched. The node features of this graph are used in the task of video retrieval. Experimentation is done on the MSR-VTT dataset by using query images from outside the dataset. To evaluate this novel approach P@5, P@10 and P@20 metrics are calculated. Two different ResNet models namely, ResNet-152 and ResNet-50 are used in this study.



### Super-Resolution of Real-World Faces
- **Arxiv ID**: http://arxiv.org/abs/2011.02427v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.02427v2)
- **Published**: 2020-11-04 17:25:54+00:00
- **Updated**: 2022-02-08 03:56:30+00:00
- **Authors**: Saurabh Goswami, Aakanksha, Rajagopalan A. N
- **Comment**: 15 pages
- **Journal**: None
- **Summary**: Real low-resolution (LR) face images contain degradations which are too varied and complex to be captured by known downsampling kernels and signal-independent noises. So, in order to successfully super-resolve real faces, a method needs to be robust to a wide range of noise, blur, compression artifacts etc. Some of the recent works attempt to model these degradations from a dataset of real images using a Generative Adversarial Network (GAN). They generate synthetically degraded LR images and use them with corresponding real high-resolution(HR) image to train a super-resolution (SR) network using a combination of a pixel-wise loss and an adversarial loss. In this paper, we propose a two module super-resolution network where the feature extractor module extracts robust features from the LR image, and the SR module generates an HR estimate using only these robust features. We train a degradation GAN to convert bicubically downsampled clean images to real degraded images, and interpolate between the obtained degraded LR image and its clean LR counterpart. This interpolated LR image is then used along with it's corresponding HR counterpart to train the super-resolution network from end to end. Entropy Regularized Wasserstein Divergence is used to force the encoded features learnt from the clean and degraded images to closely resemble those extracted from the interpolated image to ensure robustness.



### Muti-view Mouse Social Behaviour Recognition with Deep Graphical Model
- **Arxiv ID**: http://arxiv.org/abs/2011.02451v2
- **DOI**: 10.1109/TIP.2021.3083079
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.02451v2)
- **Published**: 2020-11-04 18:09:58+00:00
- **Updated**: 2021-06-30 07:44:00+00:00
- **Authors**: Zheheng Jiang, Feixiang Zhou, Aite Zhao, Xin Li, Ling Li, Dacheng Tao, Xuelong Li, Huiyu Zhou
- **Comment**: 17 pages, 11 figures
- **Journal**: None
- **Summary**: Home-cage social behaviour analysis of mice is an invaluable tool to assess therapeutic efficacy of neurodegenerative diseases. Despite tremendous efforts made within the research community, single-camera video recordings are mainly used for such analysis. Because of the potential to create rich descriptions of mouse social behaviors, the use of multi-view video recordings for rodent observations is increasingly receiving much attention. However, identifying social behaviours from various views is still challenging due to the lack of correspondence across data sources. To address this problem, we here propose a novel multiview latent-attention and dynamic discriminative model that jointly learns view-specific and view-shared sub-structures, where the former captures unique dynamics of each view whilst the latter encodes the interaction between the views. Furthermore, a novel multi-view latent-attention variational autoencoder model is introduced in learning the acquired features, enabling us to learn discriminative features in each view. Experimental results on the standard CRMI13 and our multi-view Parkinson's Disease Mouse Behaviour (PDMB) datasets demonstrate that our model outperforms the other state of the arts technologies and effectively deals with the imbalanced data problem.



### Introducing a new high-resolution handwritten digits data set with writer characteristics
- **Arxiv ID**: http://arxiv.org/abs/2011.07946v3
- **DOI**: 10.1007/s42979-022-01494-2
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.07946v3)
- **Published**: 2020-11-04 18:18:43+00:00
- **Updated**: 2022-04-13 21:46:00+00:00
- **Authors**: Cédric Beaulac, Jeffrey S. Rosenthal
- **Comment**: Data set available here :
  https://drive.google.com/drive/folders/1f2o1kjXLvcxRgtmMMuDkA2PQ5Zato4Or?usp=sharing
- **Journal**: SN COMPUT. SCI. 4, 66 (2023)
- **Summary**: The contributions in this article are two-fold. First, we introduce a new hand-written digit data set that we collected. It contains high-resolution images of hand-written The contributions in this article are two-fold. First, we introduce a new handwritten digit data set that we collected. It contains high-resolution images of handwritten digits together with various writer characteristics which are not available in the well-known MNIST database. The multiple writer characteristics gathered are a novelty of our data set and create new research opportunities. The data set is publicly available online. Second, we analyse this new data set. We begin with simple supervised tasks. We assess the predictability of the writer characteristics gathered, the effect of using some of those characteristics as predictors in classification task and the effect of higher resolution images on classification accuracy. We also explore semi-supervised applications; we can leverage the high quantity of handwritten digits data sets already existing online to improve the accuracy of various classifications task with noticeable success. Finally, we also demonstrate the generative perspective offered by this new data set; we are able to generate images that mimics the writing style of specific writers. The data set has unique and distinct features and our analysis establishes benchmarks and showcases some of the new opportunities made possible with this new data set.



### Monitoring the Impact of Wildfires on Tree Species with Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2011.02514v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.02514v2)
- **Published**: 2020-11-04 19:42:04+00:00
- **Updated**: 2020-11-12 06:28:38+00:00
- **Authors**: Wang Zhou, Levente Klein
- **Comment**: None
- **Journal**: Neural Information Processing Systems (NeurIPS 2020) Workshop
- **Summary**: One of the impacts of climate change is the difficulty of tree regrowth after wildfires over areas that traditionally were covered by certain tree species. Here a deep learning model is customized to classify land covers from four-band aerial imagery before and after wildfires to study the prolonged consequences of wildfires on tree species. The tree species labels are generated from manually delineated maps for five land cover classes: Conifer, Hardwood, Shrub, ReforestedTree and Barren land. With an accuracy of $92\%$ on the test split, the model is applied to three wildfires on data from 2009 to 2018. The model accurately delineates areas damaged by wildfires, changes in tree species and rebound of burned areas. The result shows clear evidence of wildfires impacting the local ecosystem and the outlined approach can help monitor reforested areas, observe changes in forest composition and track wildfire impact on tree species.



### Physics-Informed Neural Network Super Resolution for Advection-Diffusion Models
- **Arxiv ID**: http://arxiv.org/abs/2011.02519v2
- **DOI**: None
- **Categories**: **cs.CV**, physics.geo-ph
- **Links**: [PDF](http://arxiv.org/pdf/2011.02519v2)
- **Published**: 2020-11-04 19:56:11+00:00
- **Updated**: 2020-12-12 05:27:41+00:00
- **Authors**: Chulin Wang, Eloisa Bentivegna, Wang Zhou, Levente Klein, Bruce Elmegreen
- **Comment**: None
- **Journal**: Neural Information Processing Systems (NeurIPS 2020) Workshop
- **Summary**: Physics-informed neural networks (NN) are an emerging technique to improve spatial resolution and enforce physical consistency of data from physics models or satellite observations. A super-resolution (SR) technique is explored to reconstruct high-resolution images ($4\times$) from lower resolution images in an advection-diffusion model of atmospheric pollution plumes. SR performance is generally increased when the advection-diffusion equation constrains the NN in addition to conventional pixel-based constraints. The ability of SR techniques to also reconstruct missing data is investigated by randomly removing image pixels from the simulations and allowing the system to learn the content of missing data. Improvements in S/N of $11\%$ are demonstrated when physics equations are included in SR with $40\%$ pixel loss. Physics-informed NNs accurately reconstruct corrupted images and generate better results compared to the standard SR approaches.



### Hypersim: A Photorealistic Synthetic Dataset for Holistic Indoor Scene Understanding
- **Arxiv ID**: http://arxiv.org/abs/2011.02523v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2011.02523v5)
- **Published**: 2020-11-04 20:12:07+00:00
- **Updated**: 2021-08-18 03:16:16+00:00
- **Authors**: Mike Roberts, Jason Ramapuram, Anurag Ranjan, Atulit Kumar, Miguel Angel Bautista, Nathan Paczan, Russ Webb, Joshua M. Susskind
- **Comment**: Accepted for publication at the International Conference on Computer
  Vision (ICCV) 2021
- **Journal**: None
- **Summary**: For many fundamental scene understanding tasks, it is difficult or impossible to obtain per-pixel ground truth labels from real images. We address this challenge by introducing Hypersim, a photorealistic synthetic dataset for holistic indoor scene understanding. To create our dataset, we leverage a large repository of synthetic scenes created by professional artists, and we generate 77,400 images of 461 indoor scenes with detailed per-pixel labels and corresponding ground truth geometry. Our dataset: (1) relies exclusively on publicly available 3D assets; (2) includes complete scene geometry, material information, and lighting information for every scene; (3) includes dense per-pixel semantic instance segmentations and complete camera information for every image; and (4) factors every image into diffuse reflectance, diffuse illumination, and a non-diffuse residual term that captures view-dependent lighting effects.   We analyze our dataset at the level of scenes, objects, and pixels, and we analyze costs in terms of money, computation time, and annotation effort. Remarkably, we find that it is possible to generate our entire dataset from scratch, for roughly half the cost of training a popular open-source natural language processing model. We also evaluate sim-to-real transfer performance on two real-world scene understanding tasks - semantic segmentation and 3D shape prediction - where we find that pre-training on our dataset significantly improves performance on both tasks, and achieves state-of-the-art performance on the most challenging Pix3D test set. All of our rendered image data, as well as all the code we used to generate our dataset and perform our experiments, is available online.



### Mutual Modality Learning for Video Action Classification
- **Arxiv ID**: http://arxiv.org/abs/2011.02543v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.02543v1)
- **Published**: 2020-11-04 21:20:08+00:00
- **Updated**: 2020-11-04 21:20:08+00:00
- **Authors**: Stepan Komkov, Maksim Dzabraev, Aleksandr Petiushko
- **Comment**: None
- **Journal**: None
- **Summary**: The construction of models for video action classification progresses rapidly. However, the performance of those models can still be easily improved by ensembling with the same models trained on different modalities (e.g. Optical flow). Unfortunately, it is computationally expensive to use several modalities during inference. Recent works examine the ways to integrate advantages of multi-modality into a single RGB-model. Yet, there is still a room for improvement. In this paper, we explore the various methods to embed the ensemble power into a single model. We show that proper initialization, as well as mutual modality learning, enhances single-modality models. As a result, we achieve state-of-the-art results in the Something-Something-v2 benchmark.



### Uncertainty-Aware Voxel based 3D Object Detection and Tracking with von-Mises Loss
- **Arxiv ID**: http://arxiv.org/abs/2011.02553v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.02553v1)
- **Published**: 2020-11-04 21:53:31+00:00
- **Updated**: 2020-11-04 21:53:31+00:00
- **Authors**: Yuanxin Zhong, Minghan Zhu, Huei Peng
- **Comment**: Submitted to ICRA 2021
- **Journal**: None
- **Summary**: Object detection and tracking is a key task in autonomy. Specifically, 3D object detection and tracking have been an emerging hot topic recently. Although various methods have been proposed for object detection, uncertainty in the 3D detection and tracking tasks has been less explored. Uncertainty helps us tackle the error in the perception system and improve robustness. In this paper, we propose a method for improving target tracking performance by adding uncertainty regression to the SECOND detector, which is one of the most representative algorithms of 3D object detection. Our method estimates positional and dimensional uncertainties with Gaussian Negative Log-Likelihood (NLL) Loss for estimation and introduces von-Mises NLL Loss for angular uncertainty estimation. We fed the uncertainty output into a classical object tracking framework and proved that our method increased the tracking performance compared against the vanilla tracker with constant covariance assumption.



### DUDE: Deep Unsigned Distance Embeddings for Hi-Fidelity Representation of Complex 3D Surfaces
- **Arxiv ID**: http://arxiv.org/abs/2011.02570v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.02570v2)
- **Published**: 2020-11-04 22:49:05+00:00
- **Updated**: 2020-12-14 03:40:00+00:00
- **Authors**: Rahul Venkatesh, Sarthak Sharma, Aurobrata Ghosh, Laszlo Jeni, Maneesh Singh
- **Comment**: None
- **Journal**: None
- **Summary**: High fidelity representation of shapes with arbitrary topology is an important problem for a variety of vision and graphics applications. Owing to their limited resolution, classical discrete shape representations using point clouds, voxels and meshes produce low quality results when used in these applications. Several implicit 3D shape representation approaches using deep neural networks have been proposed leading to significant improvements in both quality of representations as well as the impact on downstream applications. However, these methods can only be used to represent topologically closed shapes which greatly limits the class of shapes that they can represent. As a consequence, they also often require clean, watertight meshes for training. In this work, we propose DUDE - a Deep Unsigned Distance Embedding method which alleviates both of these shortcomings. DUDE is a disentangled shape representation that utilizes an unsigned distance field (uDF) to represent proximity to a surface, and a normal vector field (nVF) to represent surface orientation. We show that a combination of these two (uDF+nVF) can be used to learn high fidelity representations for arbitrary open/closed shapes. As opposed to prior work such as DeepSDF, our shape representations can be directly learnt from noisy triangle soups, and do not need watertight meshes. Additionally, we propose novel algorithms for extracting and rendering iso-surfaces from the learnt representations. We validate DUDE on benchmark 3D datasets and demonstrate that it produces significant improvements over the state of the art.



### Multi-layer Feature Aggregation for Deep Scene Parsing Models
- **Arxiv ID**: http://arxiv.org/abs/2011.02572v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.02572v1)
- **Published**: 2020-11-04 23:07:07+00:00
- **Updated**: 2020-11-04 23:07:07+00:00
- **Authors**: Litao Yu, Yongsheng Gao, Jun Zhou, Jian Zhang, Qiang Wu
- **Comment**: None
- **Journal**: None
- **Summary**: Scene parsing from images is a fundamental yet challenging problem in visual content understanding. In this dense prediction task, the parsing model assigns every pixel to a categorical label, which requires the contextual information of adjacent image patches. So the challenge for this learning task is to simultaneously describe the geometric and semantic properties of objects or a scene. In this paper, we explore the effective use of multi-layer feature outputs of the deep parsing networks for spatial-semantic consistency by designing a novel feature aggregation module to generate the appropriate global representation prior, to improve the discriminative power of features. The proposed module can auto-select the intermediate visual features to correlate the spatial and semantic information. At the same time, the multiple skip connections form a strong supervision, making the deep parsing network easy to train. Extensive experiments on four public scene parsing datasets prove that the deep parsing network equipped with the proposed feature aggregation module can achieve very promising results.



### Learning and Evaluating Representations for Deep One-class Classification
- **Arxiv ID**: http://arxiv.org/abs/2011.02578v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.02578v2)
- **Published**: 2020-11-04 23:33:41+00:00
- **Updated**: 2021-03-25 23:11:23+00:00
- **Authors**: Kihyuk Sohn, Chun-Liang Li, Jinsung Yoon, Minho Jin, Tomas Pfister
- **Comment**: Published at International Conference on Learning Representation
  (ICLR) 2021. The first two authors contributed equally
- **Journal**: None
- **Summary**: We present a two-stage framework for deep one-class classification. We first learn self-supervised representations from one-class data, and then build one-class classifiers on learned representations. The framework not only allows to learn better representations, but also permits building one-class classifiers that are faithful to the target task. We argue that classifiers inspired by the statistical perspective in generative or discriminative models are more effective than existing approaches, such as a normality score from a surrogate classifier. We thoroughly evaluate different self-supervised representation learning algorithms under the proposed framework for one-class classification. Moreover, we present a novel distribution-augmented contrastive learning that extends training distributions via data augmentation to obstruct the uniformity of contrastive representations. In experiments, we demonstrate state-of-the-art performance on visual domain one-class classification benchmarks, including novelty and anomaly detection. Finally, we present visual explanations, confirming that the decision-making process of deep one-class classifiers is intuitive to humans. The code is available at https://github.com/google-research/deep_representation_one_class.



### Improved Algorithm for Seamlessly Creating Infinite Loops from a Video Clip, while Preserving Variety in Textures
- **Arxiv ID**: http://arxiv.org/abs/2011.02579v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.02579v1)
- **Published**: 2020-11-04 23:38:51+00:00
- **Updated**: 2020-11-04 23:38:51+00:00
- **Authors**: Kunjal Panchal
- **Comment**: None
- **Journal**: None
- **Summary**: This project implements the paper "Video Textures" by Szeliski. The aim is to create a "Moving Picture" or as we popularly call it, a GIF; which is "somewhere between a photograph and a video". The idea is to input a video which has some repeated motion (the texture), such as a flag waving, rain, or a candle flame. The output is a new video that infinitely extends the original video in a seamless way. In practice, the output isn't really infinte, but is instead looped using a video player and is sufficiently long as to appear to never repeat.   Our goal from this implementation was to: improve distance metric by switching from a crude sum of squared distance to most sophisticated wavelet-based distance; add intensity normalization, cross-fading and morphing to the suggested basic algorithm. We also experiment on the trade-off between variety and smoothness.



### DeepReg: a deep learning toolkit for medical image registration
- **Arxiv ID**: http://arxiv.org/abs/2011.02580v1
- **DOI**: 10.21105/joss.02705
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.02580v1)
- **Published**: 2020-11-04 23:39:02+00:00
- **Updated**: 2020-11-04 23:39:02+00:00
- **Authors**: Yunguan Fu, Nina Montaña Brown, Shaheer U. Saeed, Adrià Casamitjana, Zachary M. C. Baum, Rémi Delaunay, Qianye Yang, Alexander Grimwood, Zhe Min, Stefano B. Blumberg, Juan Eugenio Iglesias, Dean C. Barratt, Ester Bonmati, Daniel C. Alexander, Matthew J. Clarkson, Tom Vercauteren, Yipeng Hu
- **Comment**: Accepted in The Journal of Open Source Software (JOSS)
- **Journal**: None
- **Summary**: DeepReg (https://github.com/DeepRegNet/DeepReg) is a community-supported open-source toolkit for research and education in medical image registration using deep learning.



