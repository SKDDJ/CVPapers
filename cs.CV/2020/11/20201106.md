# Arxiv Papers in cs.CV on 2020-11-06
### Affinity LCFCN: Learning to Segment Fish with Weak Supervision
- **Arxiv ID**: http://arxiv.org/abs/2011.03149v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.03149v1)
- **Published**: 2020-11-06 00:33:20+00:00
- **Updated**: 2020-11-06 00:33:20+00:00
- **Authors**: Issam Laradji, Alzayat Saleh, Pau Rodriguez, Derek Nowrouzezahrai, Mostafa Rahimi Azghadi, David Vazquez
- **Comment**: None
- **Journal**: None
- **Summary**: Aquaculture industries rely on the availability of accurate fish body measurements, e.g., length, width and mass. Manual methods that rely on physical tools like rulers are time and labour intensive. Leading automatic approaches rely on fully-supervised segmentation models to acquire these measurements but these require collecting per-pixel labels -- also time consuming and laborious: i.e., it can take up to two minutes per fish to generate accurate segmentation labels, almost always requiring at least some manual intervention. We propose an automatic segmentation model efficiently trained on images labeled with only point-level supervision, where each fish is annotated with a single click. This labeling process requires significantly less manual intervention, averaging roughly one second per fish. Our approach uses a fully convolutional neural network with one branch that outputs per-pixel scores and another that outputs an affinity matrix. We aggregate these two outputs using a random walk to obtain the final, refined per-pixel segmentation output. We train the entire model end-to-end with an LCFCN loss, resulting in our A-LCFCN method. We validate our model on the DeepFish dataset, which contains many fish habitats from the north-eastern Australian region. Our experimental results confirm that A-LCFCN outperforms a fully-supervised segmentation model at fixed annotation budget. Moreover, we show that A-LCFCN achieves better segmentation results than LCFCN and a standard baseline. We have released the code at \url{https://github.com/IssamLaradji/affinity_lcfcn}.



### Confusable Learning for Large-class Few-Shot Classification
- **Arxiv ID**: http://arxiv.org/abs/2011.03154v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.03154v1)
- **Published**: 2020-11-06 01:44:37+00:00
- **Updated**: 2020-11-06 01:44:37+00:00
- **Authors**: Bingcong Li, Bo Han, Zhuowei Wang, Jing Jiang, Guodong Long
- **Comment**: None
- **Journal**: None
- **Summary**: Few-shot image classification is challenging due to the lack of ample samples in each class. Such a challenge becomes even tougher when the number of classes is very large, i.e., the large-class few-shot scenario. In this novel scenario, existing approaches do not perform well because they ignore confusable classes, namely similar classes that are difficult to distinguish from each other. These classes carry more information. In this paper, we propose a biased learning paradigm called Confusable Learning, which focuses more on confusable classes. Our method can be applied to mainstream meta-learning algorithms. Specifically, our method maintains a dynamically updating confusion matrix, which analyzes confusable classes in the dataset. Such a confusion matrix helps meta learners to emphasize on confusable classes. Comprehensive experiments on Omniglot, Fungi, and ImageNet demonstrate the efficacy of our method over state-of-the-art baselines.



### GHFP: Gradually Hard Filter Pruning
- **Arxiv ID**: http://arxiv.org/abs/2011.03170v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2011.03170v1)
- **Published**: 2020-11-06 03:09:52+00:00
- **Updated**: 2020-11-06 03:09:52+00:00
- **Authors**: Linhang Cai, Zhulin An, Yongjun Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Filter pruning is widely used to reduce the computation of deep learning, enabling the deployment of Deep Neural Networks (DNNs) in resource-limited devices. Conventional Hard Filter Pruning (HFP) method zeroizes pruned filters and stops updating them, thus reducing the search space of the model. On the contrary, Soft Filter Pruning (SFP) simply zeroizes pruned filters, keeping updating them in the following training epochs, thus maintaining the capacity of the network. However, SFP, together with its variants, converges much slower than HFP due to its larger search space. Our question is whether SFP-based methods and HFP can be combined to achieve better performance and speed up convergence. Firstly, we generalize SFP-based methods and HFP to analyze their characteristics. Then we propose a Gradually Hard Filter Pruning (GHFP) method to smoothly switch from SFP-based methods to HFP during training and pruning, thus maintaining a large search space at first, gradually reducing the capacity of the model to ensure a moderate convergence speed. Experimental results on CIFAR-10/100 show that our method achieves the state-of-the-art performance.



### ULSD: Unified Line Segment Detection across Pinhole, Fisheye, and Spherical Cameras
- **Arxiv ID**: http://arxiv.org/abs/2011.03174v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.03174v1)
- **Published**: 2020-11-06 03:30:17+00:00
- **Updated**: 2020-11-06 03:30:17+00:00
- **Authors**: Hao Li, Huai Yu, Wen Yang, Lei Yu, Sebastian Scherer
- **Comment**: None
- **Journal**: None
- **Summary**: Line segment detection is essential for high-level tasks in computer vision and robotics. Currently, most stateof-the-art (SOTA) methods are dedicated to detecting straight line segments in undistorted pinhole images, thus distortions on fisheye or spherical images may largely degenerate their performance. Targeting at the unified line segment detection (ULSD) for both distorted and undistorted images, we propose to represent line segments with the Bezier curve model. Then the line segment detection is tackled by the Bezier curve regression with an end-to-end network, which is model-free and without any undistortion preprocessing. Experimental results on the pinhole, fisheye, and spherical image datasets validate the superiority of the proposed ULSD to the SOTA methods both in accuracy and efficiency (40.6fps for pinhole images). The source code is available at https://github.com/lh9171338/Unified-LineSegment-Detection.



### Automatic Brain Tumor Segmentation with Scale Attention Network
- **Arxiv ID**: http://arxiv.org/abs/2011.03188v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.03188v3)
- **Published**: 2020-11-06 04:45:49+00:00
- **Updated**: 2020-11-25 19:16:09+00:00
- **Authors**: Yading Yuan
- **Comment**: 10 pages, 3 figures, to appear in LNCS Brain Tumore Segmentation
  (BraTS)
- **Journal**: None
- **Summary**: Automatic segmentation of brain tumors is an essential but challenging step for extracting quantitative imaging biomarkers for accurate tumor detection, diagnosis, prognosis, treatment planning and assessment. Multimodal Brain Tumor Segmentation Challenge 2020 (BraTS 2020) provides a common platform for comparing different automatic algorithms on multi-parametric Magnetic Resonance Imaging (mpMRI) in tasks of 1) Brain tumor segmentation MRI scans; 2) Prediction of patient overall survival (OS) from pre-operative MRI scans; 3) Distinction of true tumor recurrence from treatment related effects and 4) Evaluation of uncertainty measures in segmentation. We participate the image segmentation challenge by developing a fully automatic segmentation network based on encoder-decoder architecture. In order to better integrate information across different scales, we propose a dynamic scale attention mechanism that incorporates low-level details with high-level semantics from feature maps at different scales. Our framework was trained using the 369 challenge training cases provided by BraTS 2020, and achieved an average Dice Similarity Coefficient (DSC) of 0.8828, 0.8433 and 0.8177, as well as 95% Hausdorff distance (in millimeter) of 5.2176, 17.9697 and 13.4298 on 166 testing cases for whole tumor, tumor core and enhanced tumor, respectively, which ranked itself as the 3rd place among 693 registrations in the BraTS 2020 challenge.



### Towards a quantitative assessment of neurodegeneration in Alzheimer's disease
- **Arxiv ID**: http://arxiv.org/abs/2011.04465v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.04465v1)
- **Published**: 2020-11-06 05:56:29+00:00
- **Updated**: 2020-11-06 05:56:29+00:00
- **Authors**: Oleg Michailovich, Rinat Mukhometzianov
- **Comment**: 26 pages, 5 figures, submitted to IEEE Access
- **Journal**: None
- **Summary**: Alzheimer's disease (AD) is an irreversible neurodegenerative disorder that progressively destroys memory and other cognitive domains of the brain. While effective therapeutic management of AD is still in development, it seems reasonable to expect their prospective outcomes to depend on the severity of baseline pathology. For this reason, substantial research efforts have been invested in the development of effective means of non-invasive diagnosis of AD at its earliest possible stages. In pursuit of the same objective, the present paper addresses the problem of the quantitative diagnosis of AD by means of Diffusion Magnetic Resonance Imaging (dMRI). In particular, the paper introduces the notion of a pathology specific imaging contrast (PSIC), which, in addition to supplying a valuable diagnostic score, can serve as a means of visual representation of the spatial extent of neurodegeneration. The values of PSIC are computed by a dedicated deep neural network (DNN), which has been specially adapted to the processing of dMRI signals. Once available, such values can be used for several important purposes, including stratification of study subjects. In particular, experiments confirm the DNN-based classification can outperform a wide range of alternative approaches in application to the basic problem of stratification of cognitively normal (CN) and AD subjects. Notwithstanding its preliminary nature, this result suggests a strong rationale for further extension and improvement of the explorative methodology described in this paper.



### Learning a Geometric Representation for Data-Efficient Depth Estimation via Gradient Field and Contrastive Loss
- **Arxiv ID**: http://arxiv.org/abs/2011.03207v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2011.03207v2)
- **Published**: 2020-11-06 06:47:19+00:00
- **Updated**: 2021-03-17 05:59:46+00:00
- **Authors**: Dongseok Shim, H. Jin Kim
- **Comment**: IEEE ICRA 2021 accepted
- **Journal**: None
- **Summary**: Estimating a depth map from a single RGB image has been investigated widely for localization, mapping, and 3-dimensional object detection. Recent studies on a single-view depth estimation are mostly based on deep Convolutional neural Networks (ConvNets) which require a large amount of training data paired with densely annotated labels. Depth annotation tasks are both expensive and inefficient, so it is inevitable to leverage RGB images which can be collected very easily to boost the performance of ConvNets without depth labels. However, most self-supervised learning algorithms are focused on capturing the semantic information of images to improve the performance in classification or object detection, not in depth estimation. In this paper, we show that existing self-supervised methods do not perform well on depth estimation and propose a gradient-based self-supervised learning algorithm with momentum contrastive loss to help ConvNets extract the geometric information with unlabeled images. As a result, the network can estimate the depth map accurately with a relatively small amount of annotated data. To show that our method is independent of the model structure, we evaluate our method with two different monocular depth estimation algorithms. Our method outperforms the previous state-of-the-art self-supervised learning algorithms and shows the efficiency of labeled data in triple compared to random initialization on the NYU Depth v2 dataset.



### Task-relevant Representation Learning for Networked Robotic Perception
- **Arxiv ID**: http://arxiv.org/abs/2011.03216v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.IT, cs.NI, cs.SY, eess.SY, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/2011.03216v1)
- **Published**: 2020-11-06 07:39:08+00:00
- **Updated**: 2020-11-06 07:39:08+00:00
- **Authors**: Manabu Nakanoya, Sandeep Chinchali, Alexandros Anemogiannis, Akul Datta, Sachin Katti, Marco Pavone
- **Comment**: None
- **Journal**: None
- **Summary**: Today, even the most compute-and-power constrained robots can measure complex, high data-rate video and LIDAR sensory streams. Often, such robots, ranging from low-power drones to space and subterranean rovers, need to transmit high-bitrate sensory data to a remote compute server if they are uncertain or cannot scalably run complex perception or mapping tasks locally. However, today's representations for sensory data are mostly designed for human, not robotic, perception and thus often waste precious compute or wireless network resources to transmit unimportant parts of a scene that are unnecessary for a high-level robotic task. This paper presents an algorithm to learn task-relevant representations of sensory data that are co-designed with a pre-trained robotic perception model's ultimate objective. Our algorithm aggressively compresses robotic sensory data by up to 11x more than competing methods. Further, it achieves high accuracy and robust generalization on diverse tasks including Mars terrain classification with low-power deep learning accelerators, neural motion planning, and environmental timeseries classification.



### Efficient image retrieval using multi neural hash codes and bloom filters
- **Arxiv ID**: http://arxiv.org/abs/2011.03234v2
- **DOI**: 10.1109/INOCON50539.2020.9298228
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2011.03234v2)
- **Published**: 2020-11-06 08:46:31+00:00
- **Updated**: 2020-11-18 12:09:08+00:00
- **Authors**: Sourin Chakrabarti
- **Comment**: 2020 IEEE International Conference for Innovation in Technology.
  Asian Journal for Convergence in Technology(AJCT) Volume VI Issue III
- **Journal**: None
- **Summary**: This paper aims to deliver an efficient and modified approach for image retrieval using multiple neural hash codes and limiting the number of queries using bloom filters by identifying false positives beforehand. Traditional approaches involving neural networks for image retrieval tasks tend to use higher layers for feature extraction. But it has been seen that the activations of lower layers have proven to be more effective in a number of scenarios. In our approach, we have leveraged the use of local deep convolutional neural networks which combines the powers of both the features of lower and higher layers for creating feature maps which are then compressed using PCA and fed to a bloom filter after binary sequencing using a modified multi k-means approach. The feature maps obtained are further used in the image retrieval process in a hierarchical coarse-to-fine manner by first comparing the images in the higher layers for semantically similar images and then gradually moving towards the lower layers searching for structural similarities. While searching, the neural hashes for the query image are again calculated and queried in the bloom filter which tells us whether the query image is absent in the set or maybe present. If the bloom filter doesn't necessarily rule out the query, then it goes into the image retrieval process. This approach can be particularly helpful in cases where the image store is distributed since the approach supports parallel querying.



### Channel Pruning via Multi-Criteria based on Weight Dependency
- **Arxiv ID**: http://arxiv.org/abs/2011.03240v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2011.03240v4)
- **Published**: 2020-11-06 09:12:00+00:00
- **Updated**: 2021-04-20 07:41:11+00:00
- **Authors**: Yangchun Yan, Rongzuo Guo, Chao Li, Kang Yang, Yongjun Xu
- **Comment**: 8 pages,IJCNN2021
- **Journal**: None
- **Summary**: Channel pruning has demonstrated its effectiveness in compressing ConvNets. In many related arts, the importance of an output feature map is only determined by its associated filter. However, these methods ignore a small part of weights in the next layer which disappears as the feature map is removed. They ignore the phenomenon of weight dependency. Besides, many pruning methods use only one criterion for evaluation and find a sweet spot of pruning structure and accuracy in a trial-and-error fashion, which can be time-consuming. In this paper, we proposed a channel pruning algorithm via multi-criteria based on weight dependency, CPMC, which can compress a pre-trained model directly. CPMC defines channel importance in three aspects, including its associated weight value, computational cost, and parameter quantity. According to the phenomenon of weight dependency, CPMC gets channel importance by assessing its associated filter and the corresponding partial weights in the next layer. Then CPMC uses global normalization to achieve cross-layer comparison. Finally, CPMC removes less important channels by global ranking. CPMC can compress various CNN models, including VGGNet, ResNet, and DenseNet on various image classification datasets. Extensive experiments have shown CPMC outperforms the others significantly.



### Hi-UCD: A Large-scale Dataset for Urban Semantic Change Detection in Remote Sensing Imagery
- **Arxiv ID**: http://arxiv.org/abs/2011.03247v7
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2011.03247v7)
- **Published**: 2020-11-06 09:20:54+00:00
- **Updated**: 2020-12-28 01:47:48+00:00
- **Authors**: Shiqi Tian, Ailong Ma, Zhuo Zheng, Yanfei Zhong
- **Comment**: Presented at NeurIPS 2020 Workshop on Machine Learning for the
  Developing World
- **Journal**: None
- **Summary**: With the acceleration of the urban expansion, urban change detection (UCD), as a significant and effective approach, can provide the change information with respect to geospatial objects for dynamical urban analysis. However, existing datasets suffer from three bottlenecks: (1) lack of high spatial resolution images; (2) lack of semantic annotation; (3) lack of long-range multi-temporal images. In this paper, we propose a large scale benchmark dataset, termed Hi-UCD. This dataset uses aerial images with a spatial resolution of 0.1 m provided by the Estonia Land Board, including three-time phases, and semantically annotated with nine classes of land cover to obtain the direction of ground objects change. It can be used for detecting and analyzing refined urban changes. We benchmark our dataset using some classic methods in binary and multi-class change detection. Experimental results show that Hi-UCD is challenging yet useful. We hope the Hi-UCD can become a strong benchmark accelerating future research.



### Learning Translation Invariance in CNNs
- **Arxiv ID**: http://arxiv.org/abs/2011.11757v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.11757v1)
- **Published**: 2020-11-06 09:39:27+00:00
- **Updated**: 2020-11-06 09:39:27+00:00
- **Authors**: Valerio Biscione, Jeffrey Bowers
- **Comment**: NeurIPS 2020 Workshop SVRHM
- **Journal**: None
- **Summary**: When seeing a new object, humans can immediately recognize it across different retinal locations: we say that the internal object representation is invariant to translation. It is commonly believed that Convolutional Neural Networks (CNNs) are architecturally invariant to translation thanks to the convolution and/or pooling operations they are endowed with. In fact, several works have found that these networks systematically fail to recognise new objects on untrained locations. In this work we show how, even though CNNs are not 'architecturally invariant' to translation, they can indeed 'learn' to be invariant to translation. We verified that this can be achieved by pretraining on ImageNet, and we found that it is also possible with much simpler datasets in which the items are fully translated across the input canvas. We investigated how this pretraining affected the internal network representations, finding that the invariance was almost always acquired, even though it was some times disrupted by further training due to catastrophic forgetting/interference. These experiments show how pretraining a network on an environment with the right 'latent' characteristics (a more naturalistic environment) can result in the network learning deep perceptual rules which would dramatically improve subsequent generalization.



### Modular Primitives for High-Performance Differentiable Rendering
- **Arxiv ID**: http://arxiv.org/abs/2011.03277v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.03277v1)
- **Published**: 2020-11-06 10:48:43+00:00
- **Updated**: 2020-11-06 10:48:43+00:00
- **Authors**: Samuli Laine, Janne Hellsten, Tero Karras, Yeongho Seol, Jaakko Lehtinen, Timo Aila
- **Comment**: None
- **Journal**: None
- **Summary**: We present a modular differentiable renderer design that yields performance superior to previous methods by leveraging existing, highly optimized hardware graphics pipelines. Our design supports all crucial operations in a modern graphics pipeline: rasterizing large numbers of triangles, attribute interpolation, filtered texture lookups, as well as user-programmable shading and geometry processing, all in high resolutions. Our modular primitives allow custom, high-performance graphics pipelines to be built directly within automatic differentiation frameworks such as PyTorch or TensorFlow. As a motivating application, we formulate facial performance capture as an inverse rendering problem and show that it can be solved efficiently using our tools. Our results indicate that this simple and straightforward approach achieves excellent geometric correspondence between rendered results and reference imagery.



### "What's This?" -- Learning to Segment Unknown Objects from Manipulation Sequences
- **Arxiv ID**: http://arxiv.org/abs/2011.03279v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.03279v2)
- **Published**: 2020-11-06 10:55:28+00:00
- **Updated**: 2021-06-17 09:00:03+00:00
- **Authors**: Wout Boerdijk, Martin Sundermeyer, Maximilian Durner, Rudolph Triebel
- **Comment**: 8 pages, 6 figures,in Proceedings of the IEEE International
  Conference on Robotics and Automation (ICRA), 2021
- **Journal**: None
- **Summary**: We present a novel framework for self-supervised grasped object segmentation with a robotic manipulator. Our method successively learns an agnostic foreground segmentation followed by a distinction between manipulator and object solely by observing the motion between consecutive RGB frames. In contrast to previous approaches, we propose a single, end-to-end trainable architecture which jointly incorporates motion cues and semantic knowledge. Furthermore, while the motion of the manipulator and the object are substantial cues for our algorithm, we present means to robustly deal with distraction objects moving in the background, as well as with completely static scenes. Our method neither depends on any visual registration of a kinematic robot or 3D object models, nor on precise hand-eye calibration or any additional sensor data. By extensive experimental evaluation we demonstrate the superiority of our framework and provide detailed insights on its capability of dealing with the aforementioned extreme cases of motion. We also show that training a semantic segmentation network with the automatically labeled data achieves results on par with manually annotated training data. Code and pretrained model are available at https://github.com/DLR-RM/DistinctNet.



### Event-VPR: End-to-End Weakly Supervised Network Architecture for Event-based Visual Place Recognition
- **Arxiv ID**: http://arxiv.org/abs/2011.03290v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2011.03290v1)
- **Published**: 2020-11-06 11:32:04+00:00
- **Updated**: 2020-11-06 11:32:04+00:00
- **Authors**: Delei Kong, Zheng Fang, Haojia Li, Kuanxu Hou, Sonya Coleman, Dermot Kerr
- **Comment**: None
- **Journal**: None
- **Summary**: Traditional visual place recognition (VPR) methods generally use frame-based cameras, which is easy to fail due to dramatic illumination changes or fast motions. In this paper, we propose an end-to-end visual place recognition network for event cameras, which can achieve good place recognition performance in challenging environments. The key idea of the proposed algorithm is firstly to characterize the event streams with the EST voxel grid, then extract features using a convolution network, and finally aggregate features using an improved VLAD network to realize end-to-end visual place recognition using event streams. To verify the effectiveness of the proposed algorithm, we compare the proposed method with classical VPR methods on the event-based driving datasets (MVSEC, DDD17) and the synthetic datasets (Oxford RobotCar). Experimental results show that the proposed method can achieve much better performance in challenging scenarios. To our knowledge, this is the first end-to-end event-based VPR method. The accompanying source code is available at https://github.com/kongdelei/Event-VPR.



### Learning to Orient Surfaces by Self-supervised Spherical CNNs
- **Arxiv ID**: http://arxiv.org/abs/2011.03298v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.03298v2)
- **Published**: 2020-11-06 11:43:57+00:00
- **Updated**: 2020-11-13 09:25:28+00:00
- **Authors**: Riccardo Spezialetti, Federico Stella, Marlon Marcon, Luciano Silva, Samuele Salti, Luigi Di Stefano
- **Comment**: Accepted to NeurIPS 2020
- **Journal**: None
- **Summary**: Defining and reliably finding a canonical orientation for 3D surfaces is key to many Computer Vision and Robotics applications. This task is commonly addressed by handcrafted algorithms exploiting geometric cues deemed as distinctive and robust by the designer. Yet, one might conjecture that humans learn the notion of the inherent orientation of 3D objects from experience and that machines may do so alike. In this work, we show the feasibility of learning a robust canonical orientation for surfaces represented as point clouds. Based on the observation that the quintessential property of a canonical orientation is equivariance to 3D rotations, we propose to employ Spherical CNNs, a recently introduced machinery that can learn equivariant representations defined on the Special Orthogonal group SO(3). Specifically, spherical correlations compute feature maps whose elements define 3D rotations. Our method learns such feature maps from raw data by a self-supervised training procedure and robustly selects a rotation to transform the input point cloud into a learned canonical orientation. Thereby, we realize the first end-to-end learning approach to define and extract the canonical orientation of 3D shapes, which we aptly dub Compass. Experiments on several public datasets prove its effectiveness at orienting local surface patches as well as whole objects.



### Deep coastal sea elements forecasting using U-Net based models
- **Arxiv ID**: http://arxiv.org/abs/2011.03303v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV, I.2; I.5
- **Links**: [PDF](http://arxiv.org/pdf/2011.03303v2)
- **Published**: 2020-11-06 12:02:31+00:00
- **Updated**: 2021-11-08 21:25:15+00:00
- **Authors**: Jesús García Fernández, Ismail Alaoui Abdellaoui, Siamak Mehrkanoon
- **Comment**: 12 pages, 11 figures
- **Journal**: None
- **Summary**: The supply and demand of energy is influenced by meteorological conditions. The relevance of accurate weather forecasts increases as the demand for renewable energy sources increases. The energy providers and policy makers require weather information to make informed choices and establish optimal plans according to the operational objectives. Due to the recent development of deep learning techniques applied to satellite imagery, weather forecasting that uses remote sensing data has also been the subject of major progress. The present paper investigates multiple steps ahead frame prediction for coastal sea elements in the Netherlands using U-Net based architectures. Hourly data from the Copernicus observation programme spanned over a period of 2 years has been used to train the models and make the forecasting, including seasonal predictions. We propose a variation of the U-Net architecture and further extend this novel model using residual connections, parallel convolutions and asymmetric convolutions in order to introduce three additional architectures. In particular, we show that the architecture equipped with parallel and asymmetric convolutions as well as skip connections outperforms the other three discussed models.



### Towards Efficient Scene Understanding via Squeeze Reasoning
- **Arxiv ID**: http://arxiv.org/abs/2011.03308v3
- **DOI**: 10.1109/TIP.2021.3099369
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.03308v3)
- **Published**: 2020-11-06 12:17:01+00:00
- **Updated**: 2021-07-20 06:29:08+00:00
- **Authors**: Xiangtai Li, Xia Li, Ansheng You, Li Zhang, Guangliang Cheng, Kuiyuan Yang, Yunhai Tong, Zhouchen Lin
- **Comment**: Accepted by IEEE-TIP
- **Journal**: None
- **Summary**: Graph-based convolutional model such as non-local block has shown to be effective for strengthening the context modeling ability in convolutional neural networks (CNNs). However, its pixel-wise computational overhead is prohibitive which renders it unsuitable for high resolution imagery. In this paper, we explore the efficiency of context graph reasoning and propose a novel framework called Squeeze Reasoning. Instead of propagating information on the spatial map, we first learn to squeeze the input feature into a channel-wise global vector and perform reasoning within the single vector where the computation cost can be significantly reduced. Specifically, we build the node graph in the vector where each node represents an abstract semantic concept. The refined feature within the same semantic category results to be consistent, which is thus beneficial for downstream tasks. We show that our approach can be modularized as an end-to-end trained block and can be easily plugged into existing networks. {Despite its simplicity and being lightweight, the proposed strategy allows us to establish the considerable results on different semantic segmentation datasets and shows significant improvements with respect to strong baselines on various other scene understanding tasks including object detection, instance segmentation and panoptic segmentation.} Code is available at \url{https://github.com/lxtGH/SFSegNets}.



### Self-Supervised Learning for Biological Sample Localization in 3D Tomographic Images
- **Arxiv ID**: http://arxiv.org/abs/2011.03353v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2011.03353v2)
- **Published**: 2020-11-06 13:34:00+00:00
- **Updated**: 2023-01-11 09:49:49+00:00
- **Authors**: Yaroslav Zharov, Alexey Ershov, Tilo Baumbach, Vincent Heuveline
- **Comment**: None
- **Journal**: None
- **Summary**: In synchrotron-based Computed Tomography (CT) there is a trade-off between spatial resolution, field of view and speed of positioning and alignment of samples. The problem is even more prominent for high-throughput tomography--an automated setup, capable of scanning large batches of samples without human interaction. As a result, in many applications, only 20-30% of the reconstructed volume contains the actual sample. Such data redundancy clutters the storage and increases processing time. Hence, an automated sample localization becomes an important practical problem. In this work, we describe two self-supervised losses designed for biological CT. We further demonstrate how to employ the uncertainty estimation for sample localization. This approach shows the ability to localize a sample with less than 1.5\% relative error and reduce the used storage by a factor of four. We also show that one of the proposed losses works reasonably well as a pre-training task for the semantic segmentation.



### Domain Adaptive Person Re-Identification via Coupling Optimization
- **Arxiv ID**: http://arxiv.org/abs/2011.03363v1
- **DOI**: 10.1145/3394171.3413904
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.03363v1)
- **Published**: 2020-11-06 14:01:03+00:00
- **Updated**: 2020-11-06 14:01:03+00:00
- **Authors**: Xiaobin Liu, Shiliang Zhang
- **Comment**: ACM MM 2020 Oral
- **Journal**: None
- **Summary**: Domain adaptive person Re-Identification (ReID) is challenging owing to the domain gap and shortage of annotations on target scenarios. To handle those two challenges, this paper proposes a coupling optimization method including the Domain-Invariant Mapping (DIM) method and the Global-Local distance Optimization (GLO), respectively. Different from previous methods that transfer knowledge in two stages, the DIM achieves a more efficient one-stage knowledge transfer by mapping images in labeled and unlabeled datasets to a shared feature space. GLO is designed to train the ReID model with unsupervised setting on the target domain. Instead of relying on existing optimization strategies designed for supervised training, GLO involves more images in distance optimization, and achieves better robustness to noisy label prediction. GLO also integrates distance optimizations in both the global dataset and local training batch, thus exhibits better training efficiency. Extensive experiments on three large-scale datasets, i.e., Market-1501, DukeMTMC-reID, and MSMT17, show that our coupling optimization outperforms state-of-the-art methods by a large margin. Our method also works well in unsupervised training, and even outperforms several recent domain adaptive methods.



### Disentangling 3D Prototypical Networks For Few-Shot Concept Learning
- **Arxiv ID**: http://arxiv.org/abs/2011.03367v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.03367v3)
- **Published**: 2020-11-06 14:08:27+00:00
- **Updated**: 2021-07-20 19:07:01+00:00
- **Authors**: Mihir Prabhudesai, Shamit Lal, Darshan Patil, Hsiao-Yu Tung, Adam W Harley, Katerina Fragkiadaki
- **Comment**: None
- **Journal**: None
- **Summary**: We present neural architectures that disentangle RGB-D images into objects' shapes and styles and a map of the background scene, and explore their applications for few-shot 3D object detection and few-shot concept classification. Our networks incorporate architectural biases that reflect the image formation process, 3D geometry of the world scene, and shape-style interplay. They are trained end-to-end self-supervised by predicting views in static scenes, alongside a small number of 3D object boxes. Objects and scenes are represented in terms of 3D feature grids in the bottleneck of the network. We show that the proposed 3D neural representations are compositional: they can generate novel 3D scene feature maps by mixing object shapes and styles, resizing and adding the resulting object 3D feature maps over background scene feature maps. We show that classifiers for object categories, color, materials, and spatial relationships trained over the disentangled 3D feature sub-spaces generalize better with dramatically fewer examples than the current state-of-the-art, and enable a visual question answering system that uses them as its modules to generalize one-shot to novel objects in the scene.



### Suppression of Correlated Noise with Similarity-based Unsupervised Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2011.03384v6
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2011.03384v6)
- **Published**: 2020-11-06 14:31:08+00:00
- **Updated**: 2022-01-05 18:47:20+00:00
- **Authors**: Chuang Niu, Mengzhou Li, Fenglei Fan, Weiwen Wu, Xiaodong Guo, Qing Lyu, Ge Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Image denoising is a prerequisite for downstream tasks in many fields. Low-dose and photon-counting computed tomography (CT) denoising can optimize diagnostic performance at minimized radiation dose. Supervised deep denoising methods are popular but require paired clean or noisy samples that are often unavailable in practice. Limited by the independent noise assumption, current unsupervised denoising methods cannot process correlated noises as in CT images. Here we propose the first-of-its-kind similarity-based unsupervised deep denoising approach, referred to as Noise2Sim, that works in a nonlocal and nonlinear fashion to suppress not only independent but also correlated noises. Theoretically, Noise2Sim is asymptotically equivalent to supervised learning methods under mild conditions. Experimentally, Nosie2Sim recovers intrinsic features from noisy low-dose CT and photon-counting CT images as effectively as or even better than supervised learning methods on practical datasets visually, quantitatively and statistically. Noise2Sim is a general unsupervised denoising approach and has great potential in diverse applications.



### Illumination Normalization by Partially Impossible Encoder-Decoder Cost Function
- **Arxiv ID**: http://arxiv.org/abs/2011.03428v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.03428v2)
- **Published**: 2020-11-06 15:25:26+00:00
- **Updated**: 2020-11-09 15:43:42+00:00
- **Authors**: Steve Dias Da Cruz, Bertram Taetz, Thomas Stifter, Didier Stricker
- **Comment**: This paper is accepted at IEEE Winter Conference on Applications of
  Computer Vision (WACV), 2021. Supplementary material is available under
  https://sviro.kl.dfki.de/downloads/papers/wacv2021_supplementary.pdf
- **Journal**: None
- **Summary**: Images recorded during the lifetime of computer vision based systems undergo a wide range of illumination and environmental conditions affecting the reliability of previously trained machine learning models. Image normalization is hence a valuable preprocessing component to enhance the models' robustness. To this end, we introduce a new strategy for the cost function formulation of encoder-decoder networks to average out all the unimportant information in the input images (e.g. environmental features and illumination changes) to focus on the reconstruction of the salient features (e.g. class instances). Our method exploits the availability of identical sceneries under different illumination and environmental conditions for which we formulate a partially impossible reconstruction target: the input image will not convey enough information to reconstruct the target in its entirety. Its applicability is assessed on three publicly available datasets. We combine the triplet loss as a regularizer in the latent space representation and a nearest neighbour search to improve the generalization to unseen illuminations and class instances. The importance of the aforementioned post-processing is highlighted on an automotive application. To this end, we release a synthetic dataset of sceneries from three different passenger compartments where each scenery is rendered under ten different illumination and environmental conditions: see https://sviro.kl.dfki.de



### Deep Cross-modal Hashing via Margin-dynamic-softmax Loss
- **Arxiv ID**: http://arxiv.org/abs/2011.03451v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2011.03451v2)
- **Published**: 2020-11-06 16:02:35+00:00
- **Updated**: 2021-05-18 15:02:27+00:00
- **Authors**: Rong-Cheng Tu, Xian-Ling Mao, Rongxin Tu, Binbin Bian, Wei Wei, Heyan Huang
- **Comment**: None
- **Journal**: None
- **Summary**: Due to their high retrieval efficiency and low storage cost for cross-modal search task, cross-modal hashing methods have attracted considerable attention. For the supervised cross-modal hashing methods, how to make the learned hash codes preserve semantic information sufficiently contained in the label of datapoints is the key to further enhance the retrieval performance. Hence, almost all supervised cross-modal hashing methods usually depends on defining a similarity between datapoints with the label information to guide the hashing model learning fully or partly. However, the defined similarity between datapoints can only capture the label information of datapoints partially and misses abundant semantic information, then hinders the further improvement of retrieval performance. Thus, in this paper, different from previous works, we propose a novel cross-modal hashing method without defining the similarity between datapoints, called Deep Cross-modal Hashing via \textit{Margin-dynamic-softmax Loss} (DCHML). Specifically, DCHML first trains a proxy hashing network to transform each category information of a dataset into a semantic discriminative hash code, called proxy hash code. Each proxy hash code can preserve the semantic information of its corresponding category well. Next, without defining the similarity between datapoints to supervise the training process of the modality-specific hashing networks , we propose a novel \textit{margin-dynamic-softmax loss} to directly utilize the proxy hashing codes as supervised information. Finally, by minimizing the novel \textit{margin-dynamic-softmax loss}, the modality-specific hashing networks can be trained to generate hash codes which can simultaneously preserve the cross-modal similarity and abundant semantic information well.



### A Comprehensive Comparison of Multi-Dimensional Image Denoising Methods
- **Arxiv ID**: http://arxiv.org/abs/2011.03462v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.03462v1)
- **Published**: 2020-11-06 16:28:17+00:00
- **Updated**: 2020-11-06 16:28:17+00:00
- **Authors**: Zhaoming Kong, Xiaowei Yang, Lifang He
- **Comment**: None
- **Journal**: None
- **Summary**: Filtering multi-dimensional images such as color images, color videos, multispectral images and magnetic resonance images is challenging in terms of both effectiveness and efficiency. Leveraging the nonlocal self-similarity (NLSS) characteristic of images and sparse representation in the transform domain, the block-matching and 3D filtering (BM3D) based methods show powerful denoising performance. Recently, numerous new approaches with different regularization terms, transforms and advanced deep neural network (DNN) architectures are proposed to improve denoising quality. In this paper, we extensively compare over 60 methods on both synthetic and real-world datasets. We also introduce a new color image and video dataset for benchmarking, and our evaluations are performed from four different perspectives including quantitative metrics, visual effects, human ratings and computational cost. Comprehensive experiments demonstrate: (i) the effectiveness and efficiency of the BM3D family for various denoising tasks, (ii) a simple matrix-based algorithm could produce similar results compared with its tensor counterparts, and (iii) several DNN models trained with synthetic Gaussian noise show state-of-the-art performance on real-world color image and video datasets. Despite the progress in recent years, we discuss shortcomings and possible extensions of existing techniques. Datasets and codes for evaluation are made publicly available at https://github.com/ZhaomingKong/Denoising-Comparison.



### Deep Transfer Learning for Automated Diagnosis of Skin Lesions from Photographs
- **Arxiv ID**: http://arxiv.org/abs/2011.04475v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2011.04475v3)
- **Published**: 2020-11-06 16:49:40+00:00
- **Updated**: 2020-11-18 14:30:04+00:00
- **Authors**: Emma Rocheteau, Doyoon Kim
- **Comment**: Machine Learning for Mobile Health (ML4MH) Workshop at NeurIPS 2020
- **Journal**: None
- **Summary**: Melanoma is not the most common form of skin cancer, but it is the most deadly. Currently, the disease is diagnosed by expert dermatologists, which is costly and requires timely access to medical treatment. Recent advances in deep learning have the potential to improve diagnostic performance, expedite urgent referrals and reduce burden on clinicians. Through smart phones, the technology could reach people who would not normally have access to such healthcare services, e.g. in remote parts of the world, due to financial constraints or in 2020, COVID-19 cancellations. To this end, we have investigated various transfer learning approaches by leveraging model parameters pre-trained on ImageNet with finetuning on melanoma detection. We compare EfficientNet, MnasNet, MobileNet, DenseNet, SqueezeNet, ShuffleNet, GoogleNet, ResNet, ResNeXt, VGG and a simple CNN with and without transfer learning. We find the mobile network, EfficientNet (with transfer learning) achieves the best mean performance with an area under the receiver operating characteristic curve (AUROC) of 0.931$\pm$0.005 and an area under the precision recall curve (AUPRC) of 0.840$\pm$0.010. This is significantly better than general practitioners (0.83$\pm$0.03 AUROC) and dermatologists (0.91$\pm$0.02 AUROC).



### Online Descriptor Enhancement via Self-Labelling Triplets for Visual Data Association
- **Arxiv ID**: http://arxiv.org/abs/2011.10471v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2011.10471v2)
- **Published**: 2020-11-06 17:42:04+00:00
- **Updated**: 2021-06-03 21:12:07+00:00
- **Authors**: Yorai Shaoul, Katherine Liu, Kyel Ok, Nicholas Roy
- **Comment**: Under review for the 2021 IEEE/RSJ International Conference on
  Intelligent Robots and Systems (IROS 2021). This work has been submitted to
  the IEEE for possible publication. Copyright may be transferred without
  notice, after which this version may no longer be accessible
- **Journal**: None
- **Summary**: Object-level data association is central to robotic applications such as tracking-by-detection and object-level simultaneous localization and mapping. While current learned visual data association methods outperform hand-crafted algorithms, many rely on large collections of domain-specific training examples that can be difficult to obtain without prior knowledge. Additionally, such methods often remain fixed during inference-time and do not harness observed information to better their performance. We propose a self-supervised method for incrementally refining visual descriptors to improve performance in the task of object-level visual data association. Our method optimizes deep descriptor generators online, by continuously training a widely available image classification network pre-trained with domain-independent data. We show that earlier layers in the network outperform later-stage layers for the data association task while also allowing for a 94% reduction in the number of parameters, enabling the online optimization. We show that self-labelling challenging triplets--choosing positive examples separated by large temporal distances and negative examples close in the descriptor space--improves the quality of the learned descriptors for the multi-object tracking task. Finally, we demonstrate that our approach surpasses other visual data-association methods applied to a tracking-by-detection task, and show that it provides better performance-gains when compared to other methods that attempt to adapt to observed information.



### Large-scale multilingual audio visual dubbing
- **Arxiv ID**: http://arxiv.org/abs/2011.03530v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2011.03530v1)
- **Published**: 2020-11-06 18:58:15+00:00
- **Updated**: 2020-11-06 18:58:15+00:00
- **Authors**: Yi Yang, Brendan Shillingford, Yannis Assael, Miaosen Wang, Wendi Liu, Yutian Chen, Yu Zhang, Eren Sezener, Luis C. Cobo, Misha Denil, Yusuf Aytar, Nando de Freitas
- **Comment**: 26 pages, 8 figures
- **Journal**: None
- **Summary**: We describe a system for large-scale audiovisual translation and dubbing, which translates videos from one language to another. The source language's speech content is transcribed to text, translated, and automatically synthesized into target language speech using the original speaker's voice. The visual content is translated by synthesizing lip movements for the speaker to match the translated audio, creating a seamless audiovisual experience in the target language. The audio and visual translation subsystems each contain a large-scale generic synthesis model trained on thousands of hours of data in the corresponding domain. These generic models are fine-tuned to a specific speaker before translation, either using an auxiliary corpus of data from the target speaker, or using the video to be translated itself as the input to the fine-tuning process. This report gives an architectural overview of the full system, as well as an in-depth discussion of the video dubbing component. The role of the audio and text components in relation to the full system is outlined, but their design is not discussed in detail. Translated and dubbed demo videos generated using our system can be viewed at https://www.youtube.com/playlist?list=PLSi232j2ZA6_1Exhof5vndzyfbxAhhEs5



### A Weakly Supervised Convolutional Network for Change Segmentation and Classification
- **Arxiv ID**: http://arxiv.org/abs/2011.03577v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, I.4
- **Links**: [PDF](http://arxiv.org/pdf/2011.03577v1)
- **Published**: 2020-11-06 20:20:45+00:00
- **Updated**: 2020-11-06 20:20:45+00:00
- **Authors**: Philipp Andermatt, Radu Timofte
- **Comment**: 17 pages, 5 figures. Accepted at the Machine Learning and Computing
  for Visual Semantic Analysis (MLCSA2020) Workshop which is held in
  conjunction with ACCV 2020
- **Journal**: None
- **Summary**: Fully supervised change detection methods require difficult to procure pixel-level labels, while weakly supervised approaches can be trained with image-level labels. However, most of these approaches require a combination of changed and unchanged image pairs for training. Thus, these methods can not directly be used for datasets where only changed image pairs are available. We present W-CDNet, a novel weakly supervised change detection network that can be trained with image-level semantic labels. Additionally, W-CDNet can be trained with two different types of datasets, either containing changed image pairs only or a mixture of changed and unchanged image pairs. Since we use image-level semantic labels for training, we simultaneously create a change mask and label the changed object for single-label images. W-CDNet employs a W-shaped siamese U-net to extract feature maps from an image pair which then get compared in order to create a raw change mask. The core part of our model, the Change Segmentation and Classification (CSC) module, learns an accurate change mask at a hidden layer by using a custom Remapping Block and then segmenting the current input image with the change mask. The segmented image is used to predict the image-level semantic label. The correct label can only be predicted if the change mask actually marks relevant change. This forces the model to learn an accurate change mask. We demonstrate the segmentation and classification performance of our approach and achieve top results on AICD and HRSCD, two public aerial imaging change detection datasets as well as on a Food Waste change detection dataset. Our code is available at https://github.com/PhiAbs/W-CDNet .



### Chest X-ray Image Phase Features for Improved Diagnosis of COVID-19 Using Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2011.03585v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.03585v2)
- **Published**: 2020-11-06 20:26:26+00:00
- **Updated**: 2021-04-14 16:08:35+00:00
- **Authors**: Xiao Qi, Lloyd Brown, David J. Foran, Ilker Hacihaliloglu
- **Comment**: 16 pages, 9 figures
- **Journal**: International Journal of Computer Assisted Radiology and Surgery,
  2021
- **Summary**: Recently, the outbreak of the novel Coronavirus disease 2019 (COVID-19) pandemic has seriously endangered human health and life. Due to limited availability of test kits, the need for auxiliary diagnostic approach has increased. Recent research has shown radiography of COVID-19 patient, such as CT and X-ray, contains salient information about the COVID-19 virus and could be used as an alternative diagnosis method. Chest X-ray (CXR) due to its faster imaging time, wide availability, low cost and portability gains much attention and becomes very promising. Computational methods with high accuracy and robustness are required for rapid triaging of patients and aiding radiologist in the interpretation of the collected data. In this study, we design a novel multi-feature convolutional neural network (CNN) architecture for multi-class improved classification of COVID-19 from CXR images. CXR images are enhanced using a local phase-based image enhancement method. The enhanced images, together with the original CXR data, are used as an input to our proposed CNN architecture. Using ablation studies, we show the effectiveness of the enhanced images in improving the diagnostic accuracy. We provide quantitative evaluation on two datasets and qualitative results for visual inspection. Quantitative evaluation is performed on data consisting of 8,851 normal (healthy), 6,045 pneumonia, and 3,323 Covid-19 CXR scans. In Dataset-1, our model achieves 95.57\% average accuracy for a three classes classification, 99\% precision, recall, and F1-scores for COVID-19 cases. For Dataset-2, we have obtained 94.44\% average accuracy, and 95\% precision, recall, and F1-scores for detection of COVID-19. Our proposed multi-feature guided CNN achieves improved results compared to single-feature CNN proving the importance of the local phase-based CXR image enhancement (https://github.com/endiqq/Fus-CNNs_COVID-19).



### HDR Imaging with Quanta Image Sensors: Theoretical Limits and Optimal Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2011.03614v2
- **DOI**: 10.1109/TCI.2020.3041093
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.03614v2)
- **Published**: 2020-11-06 22:08:03+00:00
- **Updated**: 2020-12-02 20:28:52+00:00
- **Authors**: Abhiram Gnanasambandam, Stanley H. Chan
- **Comment**: None
- **Journal**: IEEE Transactions on Computational Imaging, 2020
- **Summary**: High dynamic range (HDR) imaging is one of the biggest achievements in modern photography. Traditional solutions to HDR imaging are designed for and applied to CMOS image sensors (CIS). However, the mainstream one-micron CIS cameras today generally have a high read noise and low frame-rate. These, in turn, limit the acquisition speed and quality, making the cameras slow in the HDR mode. In this paper, we propose a new computational photography technique for HDR imaging. Recognizing the limitations of CIS, we use the Quanta Image Sensor (QIS) to trade the spatial-temporal resolution with bit-depth. QIS is a single-photon image sensor that has comparable pixel pitch to CIS but substantially lower dark current and read noise. We provide a complete theoretical characterization of the sensor in the context of HDR imaging, by proving the fundamental limits in the dynamic range that QIS can offer and the trade-offs with noise and speed. In addition, we derive an optimal reconstruction algorithm for single-bit and multi-bit QIS. Our algorithm is theoretically optimal for \emph{all} linear reconstruction schemes based on exposure bracketing. Experimental results confirm the validity of the theory and algorithm, based on synthetic and real QIS data.



### Unmasking Communication Partners: A Low-Cost AI Solution for Digitally Removing Head-Mounted Displays in VR-Based Telepresence
- **Arxiv ID**: http://arxiv.org/abs/2011.03630v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.03630v1)
- **Published**: 2020-11-06 23:17:12+00:00
- **Updated**: 2020-11-06 23:17:12+00:00
- **Authors**: Philipp Ladwig, Alexander Pech, Ralf Dörner, Christian Geiger
- **Comment**: 9 pages, IEEE 3rd International Conference on Artificial Intelligence
  & Virtual Reality
- **Journal**: None
- **Summary**: Face-to-face conversation in Virtual Reality (VR) is a challenge when participants wear head-mounted displays (HMD). A significant portion of a participant's face is hidden and facial expressions are difficult to perceive. Past research has shown that high-fidelity face reconstruction with personal avatars in VR is possible under laboratory conditions with high-cost hardware. In this paper, we propose one of the first low-cost systems for this task which uses only open source, free software and affordable hardware. Our approach is to track the user's face underneath the HMD utilizing a Convolutional Neural Network (CNN) and generate corresponding expressions with Generative Adversarial Networks (GAN) for producing RGBD images of the person's face. We use commodity hardware with low-cost extensions such as 3D-printed mounts and miniature cameras. Our approach learns end-to-end without manual intervention, runs in real time, and can be trained and executed on an ordinary gaming computer. We report evaluation results showing that our low-cost system does not achieve the same fidelity of research prototypes using high-end hardware and closed source software, but it is capable of creating individual facial avatars with person-specific characteristics in movements and expressions.



### Efficient Robust Watermarking Based on Quaternion Singular Value Decomposition and Coefficient Pair Selection
- **Arxiv ID**: http://arxiv.org/abs/2011.03631v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NA, math.NA, 65F55, I.4.1
- **Links**: [PDF](http://arxiv.org/pdf/2011.03631v1)
- **Published**: 2020-11-06 23:27:36+00:00
- **Updated**: 2020-11-06 23:27:36+00:00
- **Authors**: Yong Chen, Zhi-Gang Jia, Ya-Xin Peng, Yan Peng
- **Comment**: 11 figures, 3 tables
- **Journal**: None
- **Summary**: Quaternion singular value decomposition (QSVD) is a robust technique of digital watermarking which can extract high quality watermarks from watermarked images with low distortion. In this paper, QSVD technique is further investigated and an efficient robust watermarking scheme is proposed. The improved algebraic structure-preserving method is proposed to handle the problem of "explosion of complexity" occurred in the conventional QSVD design. Secret information is transmitted blindly by incorporating in QSVD two new strategies, namely, coefficient pair selection and adaptive embedding. Unlike conventional QSVD which embeds watermarks in a single imaginary unit, we propose to adaptively embed the watermark into the optimal hiding position using the Normalized Cross-Correlation (NC) method. This avoids the selection of coefficient pair with less correlation, and thus, it reduces embedding impact by decreasing the maximum modification of coefficient values. In this way, compared with conventional QSVD, the proposed watermarking strategy avoids more modifications to a single color image layer and a better visual quality of the watermarked image is observed. Meanwhile, adaptive QSVD resists some common geometric attacks, and it improves the robustness of conventional QSVD. With these improvements, our method outperforms conventional QSVD. Its superiority over other state-of-the-art methods is also demonstrated experimentally.



### Augmented Equivariant Attention Networks for Microscopy Image Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2011.03633v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2011.03633v4)
- **Published**: 2020-11-06 23:37:49+00:00
- **Updated**: 2022-06-02 23:47:01+00:00
- **Authors**: Yaochen Xie, Yu Ding, Shuiwang Ji
- **Comment**: 13 pages, 8 figures, accepted by TMI
- **Journal**: None
- **Summary**: It is time-consuming and expensive to take high-quality or high-resolution electron microscopy (EM) and fluorescence microscopy (FM) images. Taking these images could be even invasive to samples and may damage certain subtleties in the samples after long or intense exposures, often necessary for achieving high-quality or high resolution in the first place. Advances in deep learning enable us to perform image-to-image transformation tasks for various types of microscopy image reconstruction, computationally producing high-quality images from the physically acquired low-quality ones. When training image-to-image transformation models on pairs of experimentally acquired microscopy images, prior models suffer from performance loss due to their inability to capture inter-image dependencies and common features shared among images. Existing methods that take advantage of shared features in image classification tasks cannot be properly applied to image reconstruction tasks because they fail to preserve the equivariance property under spatial permutations, something essential in image-to-image transformation. To address these limitations, we propose the augmented equivariant attention networks (AEANets) with better capability to capture inter-image dependencies, while preserving the equivariance property. The proposed AEANets captures inter-image dependencies and shared features via two augmentations on the attention mechanism, which are the shared references and the batch-aware attention during training. We theoretically derive the equivariance property of the proposed augmented attention model and experimentally demonstrate its consistent superiority in both quantitative and visual results over the baseline methods.



### Motion Prediction on Self-driving Cars: A Review
- **Arxiv ID**: http://arxiv.org/abs/2011.03635v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2011.03635v1)
- **Published**: 2020-11-06 23:40:37+00:00
- **Updated**: 2020-11-06 23:40:37+00:00
- **Authors**: Shahrokh Paravarzar, Belqes Mohammad
- **Comment**: None
- **Journal**: None
- **Summary**: The autonomous vehicle motion prediction literature is reviewed. Motion prediction is the most challenging task in autonomous vehicles and self-drive cars. These challenges have been discussed. Later on, the state-of-theart has reviewed based on the most recent literature and the current challenges are discussed. The state-of-the-art consists of classical and physical methods, deep learning networks, and reinforcement learning. prons and cons of the methods and gap of the research presented in this review. Finally, the literature surrounding object tracking and motion will be presented. As a result, deep reinforcement learning is the best candidate to tackle self-driving cars.



