# Arxiv Papers in cs.CV on 2020-12-06
### Automatic sampling and training method for wood-leaf classification based on tree terrestrial point cloud
- **Arxiv ID**: http://arxiv.org/abs/2012.03152v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.03152v1)
- **Published**: 2020-12-06 00:18:41+00:00
- **Updated**: 2020-12-06 00:18:41+00:00
- **Authors**: Zichu Liu, Qing Zhang, Pei Wang, Yaxin Li, Jingqian Sun
- **Comment**: None
- **Journal**: None
- **Summary**: Terrestrial laser scanning technology provides an efficient and accuracy solution for acquiring three-dimensional information of plants. The leaf-wood classification of plant point cloud data is a fundamental step for some forestry and biological research. An automatic sampling and training method for classification was proposed based on tree point cloud data. The plane fitting method was used for selecting leaf sample points and wood sample points automatically, then two local features were calculated for training and classification by using support vector machine (SVM) algorithm. The point cloud data of ten trees were tested by using the proposed method and a manual selection method. The average correct classification rate and kappa coefficient are 0.9305 and 0.7904, respectively. The results show that the proposed method had better efficiency and accuracy comparing to the manual selection method.



### Any-Width Networks
- **Arxiv ID**: http://arxiv.org/abs/2012.03153v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2012.03153v1)
- **Published**: 2020-12-06 00:22:01+00:00
- **Updated**: 2020-12-06 00:22:01+00:00
- **Authors**: Thanh Vu, Marc Eder, True Price, Jan-Michael Frahm
- **Comment**: 8 pages. Published at CVPR 2020 Workshop on Efficient Deep Learning
  in Computer Vision. Code at https://github.com/thanhmvu/awn
- **Journal**: None
- **Summary**: Despite remarkable improvements in speed and accuracy, convolutional neural networks (CNNs) still typically operate as monolithic entities at inference time. This poses a challenge for resource-constrained practical applications, where both computational budgets and performance needs can vary with the situation. To address these constraints, we propose the Any-Width Network (AWN), an adjustable-width CNN architecture and associated training routine that allow for fine-grained control over speed and accuracy during inference. Our key innovation is the use of lower-triangular weight matrices which explicitly address width-varying batch statistics while being naturally suited for multi-width operations. We also show that this design facilitates an efficient training routine based on random width sampling. We empirically demonstrate that our proposed AWNs compare favorably to existing methods while providing maximally granular control during inference.



### Robust Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/2012.09732v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.09732v1)
- **Published**: 2020-12-06 00:33:17+00:00
- **Updated**: 2020-12-06 00:33:17+00:00
- **Authors**: Daniel Yarnell, Xian Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Automated captioning of photos is a mission that incorporates the difficulties of photo analysis and text generation. One essential feature of captioning is the concept of attention: how to determine what to specify and in which sequence. In this study, we leverage the Object Relation using adversarial robust cut algorithm, that grows upon this method by specifically embedding knowledge about the spatial association between input data through graph representation. Our experimental study represent the promising performance of our proposed method for image captioning.



### Food Classification with Convolutional Neural Networks and Multi-Class Linear Discernment Analysis
- **Arxiv ID**: http://arxiv.org/abs/2012.03170v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2012.03170v2)
- **Published**: 2020-12-06 03:28:58+00:00
- **Updated**: 2020-12-11 04:27:23+00:00
- **Authors**: Joshua Ball
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) have been successful in representing the fully-connected inferencing ability perceived to be seen in the human brain: they take full advantage of the hierarchy-style patterns commonly seen in complex data and develop more patterns using simple features. Countless implementations of CNNs have shown how strong their ability is to learn these complex patterns, particularly in the realm of image classification. However, the cost of getting a high performance CNN to a so-called "state of the art" level is computationally costly. Even when using transfer learning, which utilize the very deep layers from models such as MobileNetV2, CNNs still take a great amount of time and resources. Linear discriminant analysis (LDA), a generalization of Fisher's linear discriminant, can be implemented in a multi-class classification method to increase separability of class features while not needing a high performance system to do so for image classification. Similarly, we also believe LDA has great promise in performing well. In this paper, we discuss our process of developing a robust CNN for food classification as well as our effective implementation of multi-class LDA and prove that (1) CNN is superior to LDA for image classification and (2) why LDA should not be left out of the races for image classification, particularly for binary cases.



### Large-scale Robust Deep AUC Maximization: A New Surrogate Loss and Empirical Studies on Medical Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2012.03173v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, math.OC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2012.03173v2)
- **Published**: 2020-12-06 03:41:51+00:00
- **Updated**: 2021-09-07 19:18:12+00:00
- **Authors**: Zhuoning Yuan, Yan Yan, Milan Sonka, Tianbao Yang
- **Comment**: Accepted by ICCV2021
- **Journal**: International Conference on Computer Vision (ICCV2021)
- **Summary**: Deep AUC Maximization (DAM) is a new paradigm for learning a deep neural network by maximizing the AUC score of the model on a dataset. Most previous works of AUC maximization focus on the perspective of optimization by designing efficient stochastic algorithms, and studies on generalization performance of large-scale DAM on difficult tasks are missing. In this work, we aim to make DAM more practical for interesting real-world applications (e.g., medical image classification). First, we propose a new margin-based min-max surrogate loss function for the AUC score (named as AUC min-max-margin loss or simply AUC margin loss for short). It is more robust than the commonly used AUC square loss, while enjoying the same advantage in terms of large-scale stochastic optimization. Second, we conduct extensive empirical studies of our DAM method on four difficult medical image classification tasks, namely (i) classification of chest x-ray images for identifying many threatening diseases, (ii) classification of images of skin lesions for identifying melanoma, (iii) classification of mammogram for breast cancer screening, and (iv) classification of microscopic images for identifying tumor tissue. Our studies demonstrate that the proposed DAM method improves the performance of optimizing cross-entropy loss by a large margin, and also achieves better performance than optimizing the existing AUC square loss on these medical image classification tasks. Specifically, our DAM method has achieved the 1st place on Stanford CheXpert competition on Aug. 31, 2020. To the best of our knowledge, this is the first work that makes DAM succeed on large-scale medical image datasets. We also conduct extensive ablation studies to demonstrate the advantages of the new AUC margin loss over the AUC square loss on benchmark datasets. The proposed method is implemented in our open-sourced library LibAUC (www.libauc.org).



### Maximum Entropy Subspace Clustering Network
- **Arxiv ID**: http://arxiv.org/abs/2012.03176v3
- **DOI**: 10.1109/TCSVT.2021.3089480
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.03176v3)
- **Published**: 2020-12-06 03:50:49+00:00
- **Updated**: 2021-06-05 06:47:08+00:00
- **Authors**: Zhihao Peng, Yuheng Jia, Hui Liu, Junhui Hou, Qingfu Zhang
- **Comment**: IEEE Transactions on Circuits and Systems for Video Technology (2021)
- **Journal**: None
- **Summary**: Deep subspace clustering networks have attracted much attention in subspace clustering, in which an auto-encoder non-linearly maps the input data into a latent space, and a fully connected layer named self-expressiveness module is introduced to learn the affinity matrix via a typical regularization term (e.g., sparse or low-rank). However, the adopted regularization terms ignore the connectivity within each subspace, limiting their clustering performance. In addition, the adopted framework suffers from the coupling issue between the auto-encoder module and the self-expressiveness module, making the network training non-trivial. To tackle these two issues, we propose a novel deep subspace clustering method named Maximum Entropy Subspace Clustering Network (MESC-Net). Specifically, MESC-Net maximizes the entropy of the affinity matrix to promote the connectivity within each subspace, in which its elements corresponding to the same subspace are uniformly and densely distributed. Furthermore, we design a novel framework to explicitly decouple the auto-encoder module and the self-expressiveness module. We also theoretically prove that the learned affinity matrix satisfies the block-diagonal property under the independent subspaces. Extensive quantitative and qualitative results on commonly used benchmark datasets validate MESC-Net significantly outperforms state-of-the-art methods.



### Computer Stereo Vision for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2012.03194v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.03194v2)
- **Published**: 2020-12-06 06:54:03+00:00
- **Updated**: 2020-12-17 03:42:39+00:00
- **Authors**: Rui Fan, Li Wang, Mohammud Junaid Bocus, Ioannis Pitas
- **Comment**: Book chapter
- **Journal**: None
- **Summary**: As an important component of autonomous systems, autonomous car perception has had a big leap with recent advances in parallel computing architectures. With the use of tiny but full-feature embedded supercomputers, computer stereo vision has been prevalently applied in autonomous cars for depth perception. The two key aspects of computer stereo vision are speed and accuracy. They are both desirable but conflicting properties, as the algorithms with better disparity accuracy usually have higher computational complexity. Therefore, the main aim of developing a computer stereo vision algorithm for resource-limited hardware is to improve the trade-off between speed and accuracy. In this chapter, we introduce both the hardware and software aspects of computer stereo vision for autonomous car systems. Then, we discuss four autonomous car perception tasks, including 1) visual feature detection, description and matching, 2) 3D information acquisition, 3) object detection/recognition and 4) semantic image segmentation. The principles of computer stereo vision and parallel computing on multi-threading CPU and GPU architectures are then detailed.



### Depth Completion using Piecewise Planar Model
- **Arxiv ID**: http://arxiv.org/abs/2012.03195v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.03195v1)
- **Published**: 2020-12-06 07:11:46+00:00
- **Updated**: 2020-12-06 07:11:46+00:00
- **Authors**: Yiran Zhong, Yuchao Dai, Hongdong Li
- **Comment**: This work was accomplished in 2017
- **Journal**: None
- **Summary**: A depth map can be represented by a set of learned bases and can be efficiently solved in a closed form solution. However, one issue with this method is that it may create artifacts when colour boundaries are inconsistent with depth boundaries. In fact, this is very common in a natural image. To address this issue, we enforce a more strict model in depth recovery: a piece-wise planar model. More specifically, we represent the desired depth map as a collection of 3D planar and the reconstruction problem is formulated as the optimization of planar parameters. Such a problem can be formulated as a continuous CRF optimization problem and can be solved through particle based method (MP-PBP) \cite{Yamaguchi14}. Extensive experimental evaluations on the KITTI visual odometry dataset show that our proposed methods own high resistance to false object boundaries and can generate useful and visually pleasant 3D point clouds.



### Online Adaptation for Consistent Mesh Reconstruction in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2012.03196v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.03196v1)
- **Published**: 2020-12-06 07:22:27+00:00
- **Updated**: 2020-12-06 07:22:27+00:00
- **Authors**: Xueting Li, Sifei Liu, Shalini De Mello, Kihwan Kim, Xiaolong Wang, Ming-Hsuan Yang, Jan Kautz
- **Comment**: NeurIPS 2020, https://sites.google.com/nvidia.com/vmr-2020
- **Journal**: None
- **Summary**: This paper presents an algorithm to reconstruct temporally consistent 3D meshes of deformable object instances from videos in the wild. Without requiring annotations of 3D mesh, 2D keypoints, or camera pose for each video frame, we pose video-based reconstruction as a self-supervised online adaptation problem applied to any incoming test video. We first learn a category-specific 3D reconstruction model from a collection of single-view images of the same category that jointly predicts the shape, texture, and camera pose of an image. Then, at inference time, we adapt the model to a test video over time using self-supervised regularization terms that exploit temporal consistency of an object instance to enforce that all reconstructed meshes share a common texture map, a base shape, as well as parts. We demonstrate that our algorithm recovers temporally consistent and reliable 3D structures from videos of non-rigid objects including those of animals captured in the wild -- an extremely challenging task rarely addressed before.



### DGGAN: Depth-image Guided Generative Adversarial Networks for Disentangling RGB and Depth Images in 3D Hand Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2012.03197v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.03197v1)
- **Published**: 2020-12-06 07:23:21+00:00
- **Updated**: 2020-12-06 07:23:21+00:00
- **Authors**: Liangjian Chen, Shih-Yao Lin, Yusheng Xie, Yen-Yu Lin, Wei Fan, Xiaohui Xie
- **Comment**: None
- **Journal**: 2020 IEEE Winter Conference on Applications of Computer Vision
  (WACV)
- **Summary**: Estimating3D hand poses from RGB images is essentialto a wide range of potential applications, but is challengingowing to substantial ambiguity in the inference of depth in-formation from RGB images. State-of-the-art estimators ad-dress this problem by regularizing3D hand pose estimationmodels during training to enforce the consistency betweenthe predicted3D poses and the ground-truth depth maps.However, these estimators rely on both RGB images and thepaired depth maps during training. In this study, we proposea conditional generative adversarial network (GAN) model,called Depth-image Guided GAN (DGGAN), to generate re-alistic depth maps conditioned on the input RGB image, anduse the synthesized depth maps to regularize the3D handpose estimation model, therefore eliminating the need forground-truth depth maps. Experimental results on multiplebenchmark datasets show that the synthesized depth mapsproduced by DGGAN are quite effective in regularizing thepose estimation model, yielding new state-of-the-art resultsin estimation accuracy, notably reducing the mean3D end-point errors (EPE) by4.7%,16.5%, and6.8%on the RHD,STB and MHP datasets, respectively.



### Temporal-Aware Self-Supervised Learning for 3D Hand Pose and Mesh Estimation in Videos
- **Arxiv ID**: http://arxiv.org/abs/2012.03205v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.03205v1)
- **Published**: 2020-12-06 07:54:18+00:00
- **Updated**: 2020-12-06 07:54:18+00:00
- **Authors**: Liangjian Chen, Shih-Yao Lin, Yusheng Xie, Yen-Yu Lin, Xiaohui Xie
- **Comment**: None
- **Journal**: 2021 IEEE Winter Conference on Applications of Computer Vision
  (WACV)
- **Summary**: Estimating 3D hand pose directly from RGB imagesis challenging but has gained steady progress recently bytraining deep models with annotated 3D poses. Howeverannotating 3D poses is difficult and as such only a few 3Dhand pose datasets are available, all with limited samplesizes. In this study, we propose a new framework of training3D pose estimation models from RGB images without usingexplicit 3D annotations, i.e., trained with only 2D informa-tion. Our framework is motivated by two observations: 1)Videos provide richer information for estimating 3D posesas opposed to static images; 2) Estimated 3D poses oughtto be consistent whether the videos are viewed in the for-ward order or reverse order. We leverage these two obser-vations to develop a self-supervised learning model calledtemporal-aware self-supervised network (TASSN). By en-forcing temporal consistency constraints, TASSN learns 3Dhand poses and meshes from videos with only 2D keypointposition annotations. Experiments show that our modelachieves surprisingly good results, with 3D estimation ac-curacy on par with the state-of-the-art models trained with3D annotations, highlighting the benefit of the temporalconsistency in constraining 3D prediction models.



### MVHM: A Large-Scale Multi-View Hand Mesh Benchmark for Accurate 3D Hand Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2012.03206v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.03206v1)
- **Published**: 2020-12-06 07:55:08+00:00
- **Updated**: 2020-12-06 07:55:08+00:00
- **Authors**: Liangjian Chen, Shih-Yao Lin, Yusheng Xie, Yen-Yu Lin, Xiaohui Xie
- **Comment**: None
- **Journal**: 2021 IEEE Winter Conference on Applications of Computer Vision
  (WACV)
- **Summary**: Estimating 3D hand poses from a single RGB image is challenging because depth ambiguity leads the problem ill-posed. Training hand pose estimators with 3D hand mesh annotations and multi-view images often results in significant performance gains. However, existing multi-view datasets are relatively small with hand joints annotated by off-the-shelf trackers or automated through model predictions, both of which may be inaccurate and can introduce biases. Collecting a large-scale multi-view 3D hand pose images with accurate mesh and joint annotations is valuable but strenuous. In this paper, we design a spin match algorithm that enables a rigid mesh model matching with any target mesh ground truth. Based on the match algorithm, we propose an efficient pipeline to generate a large-scale multi-view hand mesh (MVHM) dataset with accurate 3D hand mesh and joint labels. We further present a multi-view hand pose estimation approach to verify that training a hand pose estimator with our generated dataset greatly enhances the performance. Experimental results show that our approach achieves the performance of 0.990 in $\text{AUC}_{\text{20-50}}$ on the MHP dataset compared to the previous state-of-the-art of 0.939 on this dataset. Our datasset is public available. \footnote{\url{https://github.com/Kuzphi/MVHM}} Our datasset is available at~\href{https://github.com/Kuzphi/MVHM}{\color{blue}{https://github.com/Kuzphi/MVHM}}.



### Factorizing Perception and Policy for Interactive Instruction Following
- **Arxiv ID**: http://arxiv.org/abs/2012.03208v3
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2012.03208v3)
- **Published**: 2020-12-06 07:59:22+00:00
- **Updated**: 2021-09-02 13:14:59+00:00
- **Authors**: Kunal Pratap Singh, Suvaansh Bhambri, Byeonghwi Kim, Roozbeh Mottaghi, Jonghyun Choi
- **Comment**: ICCV 2021
- **Journal**: None
- **Summary**: Performing simple household tasks based on language directives is very natural to humans, yet it remains an open challenge for AI agents. The 'interactive instruction following' task attempts to make progress towards building agents that jointly navigate, interact, and reason in the environment at every step. To address the multifaceted problem, we propose a model that factorizes the task into interactive perception and action policy streams with enhanced components and name it as MOCA, a Modular Object-Centric Approach. We empirically validate that MOCA outperforms prior arts by significant margins on the ALFRED benchmark with improved generalization.



### Skeleon-Based Typing Style Learning For Person Identification
- **Arxiv ID**: http://arxiv.org/abs/2012.03212v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.03212v1)
- **Published**: 2020-12-06 08:14:06+00:00
- **Updated**: 2020-12-06 08:14:06+00:00
- **Authors**: Lior Gelberg, David Mendlovic, Dan Raviv
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: We present a novel architecture for person identification based on typing-style, constructed of adaptive non-local spatio-temporal graph convolutional network. Since type style dynamics convey meaningful information that can be useful for person identification, we extract the joints positions and then learn their movements' dynamics. Our non-local approach increases our model's robustness to noisy input data while analyzing joints locations instead of RGB data provides remarkable robustness to alternating environmental conditions, e.g., lighting, noise, etc. We further present two new datasets for typing style based person identification task and extensive evaluation that displays our model's superior discriminative and generalization abilities, when compared with state-of-the-art skeleton-based models.



### Spatiotemporal tomography based on scattered multiangular signals and its application for resolving evolving clouds using moving platforms
- **Arxiv ID**: http://arxiv.org/abs/2012.03223v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.03223v1)
- **Published**: 2020-12-06 09:22:08+00:00
- **Updated**: 2020-12-06 09:22:08+00:00
- **Authors**: Roi Ronen, Yoav Y. Schechner, Eshkol Eytan
- **Comment**: 14 pages, 16 figures
- **Journal**: None
- **Summary**: We derive computed tomography (CT) of a time-varying volumetric translucent object, using a small number of moving cameras. We particularly focus on passive scattering tomography, which is a non-linear problem. We demonstrate the approach on dynamic clouds, as clouds have a major effect on Earth's climate. State of the art scattering CT assumes a static object. Existing 4D CT methods rely on a linear image formation model and often on significant priors. In this paper, the angular and temporal sampling rates needed for a proper recovery are discussed. If these rates are used, the paper leads to a representation of the time-varying object, which simplifies 4D CT tomography. The task is achieved using gradient-based optimization. We demonstrate this in physics-based simulations and in an experiment that had yielded real-world data.



### FuseVis: Interpreting neural networks for image fusion using per-pixel saliency visualization
- **Arxiv ID**: http://arxiv.org/abs/2012.08932v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2012.08932v1)
- **Published**: 2020-12-06 10:03:02+00:00
- **Updated**: 2020-12-06 10:03:02+00:00
- **Authors**: Nishant Kumar, Stefan Gumhold
- **Comment**: 30 pages, 9 figures, MDPI Journal (Computers)
- **Journal**: None
- **Summary**: Image fusion helps in merging two or more images to construct a more informative single fused image. Recently, unsupervised learning based convolutional neural networks (CNN) have been utilized for different types of image fusion tasks such as medical image fusion, infrared-visible image fusion for autonomous driving as well as multi-focus and multi-exposure image fusion for satellite imagery. However, it is challenging to analyze the reliability of these CNNs for the image fusion tasks since no groundtruth is available. This led to the use of a wide variety of model architectures and optimization functions yielding quite different fusion results. Additionally, due to the highly opaque nature of such neural networks, it is difficult to explain the internal mechanics behind its fusion results. To overcome these challenges, we present a novel real-time visualization tool, named FuseVis, with which the end-user can compute per-pixel saliency maps that examine the influence of the input image pixels on each pixel of the fused image. We trained several image fusion based CNNs on medical image pairs and then using our FuseVis tool, we performed case studies on a specific clinical application by interpreting the saliency maps from each of the fusion methods. We specifically visualized the relative influence of each input image on the predictions of the fused image and showed that some of the evaluated image fusion methods are better suited for the specific clinical application. To the best of our knowledge, currently, there is no approach for visual analysis of neural networks for image fusion. Therefore, this work opens up a new research direction to improve the interpretability of deep fusion networks. The FuseVis tool can also be adapted in other deep neural network based image processing applications to make them interpretable.



### Cross-Layer Distillation with Semantic Calibration
- **Arxiv ID**: http://arxiv.org/abs/2012.03236v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2012.03236v2)
- **Published**: 2020-12-06 11:16:07+00:00
- **Updated**: 2021-08-29 07:40:05+00:00
- **Authors**: Defang Chen, Jian-Ping Mei, Yuan Zhang, Can Wang, Yan Feng, Chun Chen
- **Comment**: Extended version of our previous AAAI-2021 paper
- **Journal**: None
- **Summary**: Knowledge distillation is a technique to enhance the generalization ability of a student model by exploiting outputs from a teacher model. Recently, feature-map based variants explore knowledge transfer between manually assigned teacher-student pairs in intermediate layers for further improvement. However, layer semantics may vary in different neural networks and semantic mismatch in manual layer associations will lead to performance degeneration due to negative regularization. To address this issue, we propose Semantic Calibration for cross-layer Knowledge Distillation (SemCKD), which automatically assigns proper target layers of the teacher model for each student layer with an attention mechanism. With a learned attention distribution, each student layer distills knowledge contained in multiple teacher layers rather than a specific intermediate layer for appropriate cross-layer supervision. We further provide theoretical analysis of the association weights and conduct extensive experiments to demonstrate the effectiveness of our approach. Code is avaliable at \url{https://github.com/DefangChen/SemCKD}.



### Esophageal Tumor Segmentation in CT Images using Dilated Dense Attention Unet (DDAUnet)
- **Arxiv ID**: http://arxiv.org/abs/2012.03242v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2012.03242v3)
- **Published**: 2020-12-06 11:42:52+00:00
- **Updated**: 2021-03-24 12:59:55+00:00
- **Authors**: Sahar Yousefi, Hessam Sokooti, Mohamed S. Elmahdy, Irene M. Lips, Mohammad T. Manzuri Shalmani, Roel T. Zinkstok, Frank J. W. M. Dankers, Marius Staring
- **Comment**: None
- **Journal**: None
- **Summary**: Manual or automatic delineation of the esophageal tumor in CT images is known to be very challenging. This is due to the low contrast between the tumor and adjacent tissues, the anatomical variation of the esophagus, as well as the occasional presence of foreign bodies (e.g. feeding tubes). Physicians therefore usually exploit additional knowledge such as endoscopic findings, clinical history, additional imaging modalities like PET scans. Achieving his additional information is time-consuming, while the results are error-prone and might lead to non-deterministic results. In this paper we aim to investigate if and to what extent a simplified clinical workflow based on CT alone, allows one to automatically segment the esophageal tumor with sufficient quality. For this purpose, we present a fully automatic end-to-end esophageal tumor segmentation method based on convolutional neural networks (CNNs). The proposed network, called Dilated Dense Attention Unet (DDAUnet), leverages spatial and channel attention gates in each dense block to selectively concentrate on determinant feature maps and regions. Dilated convolutional layers are used to manage GPU memory and increase the network receptive field. We collected a dataset of 792 scans from 288 distinct patients including varying anatomies with \mbox{air pockets}, feeding tubes and proximal tumors. Repeatability and reproducibility studies were conducted for three distinct splits of training and validation sets. The proposed network achieved a $\mathrm{DSC}$ value of $0.79 \pm 0.20$, a mean surface distance of $5.4 \pm 20.2mm$ and $95\%$ Hausdorff distance of $14.7 \pm 25.0mm$ for 287 test scans, demonstrating promising results with a simplified clinical workflow based on CT alone. Our code is publicly available via \url{https://github.com/yousefis/DenseUnet_Esophagus_Segmentation}.



### Learning to Reduce Defocus Blur by Realistically Modeling Dual-Pixel Data
- **Arxiv ID**: http://arxiv.org/abs/2012.03255v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2012.03255v2)
- **Published**: 2020-12-06 13:12:43+00:00
- **Updated**: 2021-08-17 04:16:10+00:00
- **Authors**: Abdullah Abuolaim, Mauricio Delbracio, Damien Kelly, Michael S. Brown, Peyman Milanfar
- **Comment**: None
- **Journal**: None
- **Summary**: Recent work has shown impressive results on data-driven defocus deblurring using the two-image views available on modern dual-pixel (DP) sensors. One significant challenge in this line of research is access to DP data. Despite many cameras having DP sensors, only a limited number provide access to the low-level DP sensor images. In addition, capturing training data for defocus deblurring involves a time-consuming and tedious setup requiring the camera's aperture to be adjusted. Some cameras with DP sensors (e.g., smartphones) do not have adjustable apertures, further limiting the ability to produce the necessary training data. We address the data capture bottleneck by proposing a procedure to generate realistic DP data synthetically. Our synthesis approach mimics the optical image formation found on DP sensors and can be applied to virtual scenes rendered with standard computer software. Leveraging these realistic synthetic DP images, we introduce a recurrent convolutional network (RCN) architecture that improves deblurring results and is suitable for use with single-frame and multi-frame data (e.g., video) captured by DP sensors. Finally, we show that our synthetic DP data is useful for training DNN models targeting video deblurring applications where access to DP data remains challenging.



### CoEdge: Cooperative DNN Inference with Adaptive Workload Partitioning over Heterogeneous Edge Devices
- **Arxiv ID**: http://arxiv.org/abs/2012.03257v1
- **DOI**: None
- **Categories**: **cs.NI**, cs.CV, cs.DC
- **Links**: [PDF](http://arxiv.org/pdf/2012.03257v1)
- **Published**: 2020-12-06 13:15:52+00:00
- **Updated**: 2020-12-06 13:15:52+00:00
- **Authors**: Liekang Zeng, Xu Chen, Zhi Zhou, Lei Yang, Junshan Zhang
- **Comment**: Accepted by IEEE/ACM Transactions on Networking, Nov. 2020
- **Journal**: None
- **Summary**: Recent advances in artificial intelligence have driven increasing intelligent applications at the network edge, such as smart home, smart factory, and smart city. To deploy computationally intensive Deep Neural Networks (DNNs) on resource-constrained edge devices, traditional approaches have relied on either offloading workload to the remote cloud or optimizing computation at the end device locally. However, the cloud-assisted approaches suffer from the unreliable and delay-significant wide-area network, and the local computing approaches are limited by the constrained computing capability. Towards high-performance edge intelligence, the cooperative execution mechanism offers a new paradigm, which has attracted growing research interest recently. In this paper, we propose CoEdge, a distributed DNN computing system that orchestrates cooperative DNN inference over heterogeneous edge devices. CoEdge utilizes available computation and communication resources at the edge and dynamically partitions the DNN inference workload adaptive to devices' computing capabilities and network conditions. Experimental evaluations based on a realistic prototype show that CoEdge outperforms status-quo approaches in saving energy with close inference latency, achieving up to 25.5%~66.9% energy reduction for four widely-adopted CNN models.



### Towards Better Object Detection in Scale Variation with Adaptive Feature Selection
- **Arxiv ID**: http://arxiv.org/abs/2012.03265v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.03265v2)
- **Published**: 2020-12-06 13:41:20+00:00
- **Updated**: 2020-12-09 13:43:09+00:00
- **Authors**: Zehui Gong, Dong Li
- **Comment**: None
- **Journal**: None
- **Summary**: It is a common practice to exploit pyramidal feature representation to tackle the problem of scale variation in object instances. However, most of them still predict the objects in a certain range of scales based solely or mainly on a single-level representation, yielding inferior detection performance. To this end, we propose a novel adaptive feature selection module (AFSM), to automatically learn the way to fuse multi-level representations in the channel dimension, in a data-driven manner. It significantly improves the performance of the detectors that have a feature pyramid structure, while introducing nearly free inference overhead. Moreover, a class-aware sampling mechanism (CASM) is proposed to tackle the class imbalance problem, by re-weighting the sampling ratio to each of the training images, based on the statistical characteristics of each class. This is crucial to improve the performance of the minor classes. Experimental results demonstrate the effectiveness of the proposed method, with 83.04% mAP at 15.96 FPS on the VOC dataset, and 39.48% AP on the VisDrone-DET validation subset, respectively, outperforming other state-of-the-art detectors considerably. The code is available at https://github.com/ZeHuiGong/AFSM.git.



### Bifold and Semantic Reasoning for Pedestrian Behavior Prediction
- **Arxiv ID**: http://arxiv.org/abs/2012.03298v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2012.03298v2)
- **Published**: 2020-12-06 15:57:11+00:00
- **Updated**: 2021-08-09 14:01:40+00:00
- **Authors**: Amir Rasouli, Mohsen Rohani, Jun Luo
- **Comment**: ICCV 2021. 11 pages; 5 Figures; 5 tables
- **Journal**: None
- **Summary**: Pedestrian behavior prediction is one of the major challenges for intelligent driving systems. Pedestrians often exhibit complex behaviors influenced by various contextual elements. To address this problem, we propose BiPed, a multitask learning framework that simultaneously predicts trajectories and actions of pedestrians by relying on multimodal data. Our method benefits from 1) a bifold encoding approach where different data modalities are processed independently allowing them to develop their own representations, and jointly to produce a representation for all modalities using shared parameters; 2) a novel interaction modeling technique that relies on categorical semantic parsing of the scenes to capture interactions between target pedestrians and their surroundings; and 3) a bifold prediction mechanism that uses both independent and shared decoding of multimodal representations. Using public pedestrian behavior benchmark datasets for driving, PIE and JAAD, we highlight the benefits of the proposed method for behavior prediction and show that our model achieves state-of-the-art performance and improves trajectory and action prediction by up to 22% and 9% respectively. We further investigate the contributions of the proposed reasoning techniques via extensive ablation studies.



### TediGAN: Text-Guided Diverse Face Image Generation and Manipulation
- **Arxiv ID**: http://arxiv.org/abs/2012.03308v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2012.03308v3)
- **Published**: 2020-12-06 16:20:19+00:00
- **Updated**: 2021-03-29 06:40:59+00:00
- **Authors**: Weihao Xia, Yujiu Yang, Jing-Hao Xue, Baoyuan Wu
- **Comment**: CVPR 2021. Code: https://github.com/weihaox/TediGAN Data:
  https://github.com/weihaox/Multi-Modal-CelebA-HQ Video:
  https://youtu.be/L8Na2f5viAM
- **Journal**: None
- **Summary**: In this work, we propose TediGAN, a novel framework for multi-modal image generation and manipulation with textual descriptions. The proposed method consists of three components: StyleGAN inversion module, visual-linguistic similarity learning, and instance-level optimization. The inversion module maps real images to the latent space of a well-trained StyleGAN. The visual-linguistic similarity learns the text-image matching by mapping the image and text into a common embedding space. The instance-level optimization is for identity preservation in manipulation. Our model can produce diverse and high-quality images with an unprecedented resolution at 1024. Using a control mechanism based on style-mixing, our TediGAN inherently supports image synthesis with multi-modal inputs, such as sketches or semantic labels, with or without instance guidance. To facilitate text-guided multi-modal synthesis, we propose the Multi-Modal CelebA-HQ, a large-scale dataset consisting of real face images and corresponding semantic segmentation map, sketch, and textual descriptions. Extensive experiments on the introduced dataset demonstrate the superior performance of our proposed method. Code and data are available at https://github.com/weihaox/TediGAN.



### Efficient Human Pose Estimation with Depthwise Separable Convolution and Person Centroid Guided Joint Grouping
- **Arxiv ID**: http://arxiv.org/abs/2012.03316v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.03316v1)
- **Published**: 2020-12-06 16:39:54+00:00
- **Updated**: 2020-12-06 16:39:54+00:00
- **Authors**: Jie Ou, Hong Wu
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose efficient and effective methods for 2D human pose estimation. A new ResBlock is proposed based on depthwise separable convolution and is utilized instead of the original one in Hourglass network. It can be further enhanced by replacing the vanilla depthwise convolution with a mixed depthwise convolution. Based on it, we propose a bottom-up multi-person pose estimation method. A rooted tree is used to represent human pose by introducing person centroid as the root which connects to all body joints directly or hierarchically. Two branches of sub-networks are used to predict the centroids, body joints and their offsets to their parent nodes. Joints are grouped by tracing along their offsets to the closest centroids. Experimental results on the MPII human dataset and the LSP dataset show that both our single-person and multi-person pose estimation methods can achieve competitive accuracies with low computational costs.



### Global Unifying Intrinsic Calibration for Spinning and Solid-State LiDARs
- **Arxiv ID**: http://arxiv.org/abs/2012.03321v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2012.03321v2)
- **Published**: 2020-12-06 16:55:58+00:00
- **Updated**: 2021-07-28 01:39:08+00:00
- **Authors**: Jiunn-Kai Huang, Chenxi Feng, Madhav Achar, Maani Ghaffari, Jessy W. Grizzle
- **Comment**: None
- **Journal**: None
- **Summary**: Sensor calibration, which can be intrinsic or extrinsic, is an essential step to achieve the measurement accuracy required for modern perception and navigation systems deployed on autonomous robots. To date, intrinsic calibration models for spinning LiDARs have been based on hypothesized based on their physical mechanisms, resulting in anywhere from three to ten parameters to be estimated from data, while no phenomenological models have yet been proposed for solid-state LiDARs. Instead of going down that road, we propose to abstract away from the physics of a LiDAR type (spinning vs solid-state, for example), and focus on the spatial geometry of the point cloud generated by the sensor. By modeling the calibration parameters as an element of a special matrix Lie Group, we achieve a unifying view of calibration for different types of LiDARs. We further prove mathematically that the proposed model is well-constrained (has a unique answer) given four appropriately orientated targets. The proof provides a guideline for target positioning in the form of a tetrahedron. Moreover, an existing Semidefinite programming global solver for SE(3) can be modified to compute efficiently the optimal calibration parameters. For solid state LiDARs, we illustrate how the method works in simulation. For spinning LiDARs, we show with experimental data that the proposed matrix Lie Group model performs equally well as physics-based models in terms of reducing the P2P distance, while being more robust to noise.



### A Pseudo-labelling Auto-Encoder for unsupervised image classification
- **Arxiv ID**: http://arxiv.org/abs/2012.03322v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2012.03322v2)
- **Published**: 2020-12-06 17:03:34+00:00
- **Updated**: 2021-05-31 17:16:14+00:00
- **Authors**: Aymene Mohammed Bouayed, Karim Atif, Rachid Deriche, Abdelhakim Saim
- **Comment**: 13 pages, 17 figures, 9 tables, title simplified, references added
- **Journal**: None
- **Summary**: In this paper, we introduce a unique variant of the denoising Auto-Encoder and combine it with the perceptual loss to classify images in an unsupervised manner. The proposed method, called Pseudo Labelling, consists of first applying a randomly sampled set of data augmentation transformations to each training image. As a result, each initial image can be considered as a pseudo-label to its corresponding augmented ones. Then, an Auto-Encoder is used to learn the mapping between each set of the augmented images and its corresponding pseudo-label. Furthermore, the perceptual loss is employed to take into consideration the existing dependencies between the pixels in the same neighbourhood of an image. This combination encourages the encoder to output richer encodings that are highly informative of the input's class. Consequently, the Auto-Encoder's performance on unsupervised image classification is improved in terms of stability, accuracy and consistency across all tested datasets. Previous state-of-the-art accuracy on the MNIST, CIFAR-10 and SVHN datasets is improved by 0.3\%, 3.11\% and 9.21\% respectively.



### Does the dataset meet your expectations? Explaining sample representation in image data
- **Arxiv ID**: http://arxiv.org/abs/2012.08642v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2012.08642v1)
- **Published**: 2020-12-06 18:16:28+00:00
- **Updated**: 2020-12-06 18:16:28+00:00
- **Authors**: Dhasarathy Parthasarathy, Anton Johansson
- **Comment**: Preprint of paper accepted at BNAIC/BeneLearn 2020
- **Journal**: None
- **Summary**: Since the behavior of a neural network model is adversely affected by a lack of diversity in training data, we present a method that identifies and explains such deficiencies. When a dataset is labeled, we note that annotations alone are capable of providing a human interpretable summary of sample diversity. This allows explaining any lack of diversity as the mismatch found when comparing the \textit{actual} distribution of annotations in the dataset with an \textit{expected} distribution of annotations, specified manually to capture essential label diversity. While, in many practical cases, labeling (samples $\rightarrow$ annotations) is expensive, its inverse, simulation (annotations $\rightarrow$ samples) can be cheaper. By mapping the expected distribution of annotations into test samples using parametric simulation, we present a method that explains sample representation using the mismatch in diversity between simulated and collected data. We then apply the method to examine a dataset of geometric shapes to qualitatively and quantitatively explain sample representation in terms of comprehensible aspects such as size, position, and pixel brightness.



### An Uncertainty-Driven GCN Refinement Strategy for Organ Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2012.03352v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2012.03352v1)
- **Published**: 2020-12-06 18:55:07+00:00
- **Updated**: 2020-12-06 18:55:07+00:00
- **Authors**: Roger D. Soberanis-Mukul, Nassir Navab, Shadi Albarqouni
- **Comment**: Accepted for publication at the Journal of Machine Learning for
  Biomedical Imaging (MELBA) https://melba-journal.org
- **Journal**: None
- **Summary**: Organ segmentation in CT volumes is an important pre-processing step in many computer assisted intervention and diagnosis methods. In recent years, convolutional neural networks have dominated the state of the art in this task. However, since this problem presents a challenging environment due to high variability in the organ's shape and similarity between tissues, the generation of false negative and false positive regions in the output segmentation is a common issue. Recent works have shown that the uncertainty analysis of the model can provide us with useful information about potential errors in the segmentation. In this context, we proposed a segmentation refinement method based on uncertainty analysis and graph convolutional networks. We employ the uncertainty levels of the convolutional network in a particular input volume to formulate a semi-supervised graph learning problem that is solved by training a graph convolutional network. To test our method we refine the initial output of a 2D U-Net. We validate our framework with the NIH pancreas dataset and the spleen dataset of the medical segmentation decathlon. We show that our method outperforms the state-of-the-art CRF refinement method by improving the dice score by 1% for the pancreas and 2% for spleen, with respect to the original U-Net's prediction. Finally, we perform a sensitivity analysis on the parameters of our proposal and discuss the applicability to other CNN architectures, the results, and current limitations of the model for future work in this research direction. For reproducibility purposes, we make our code publicly available at https://github.com/rodsom22/gcn_refinement.



### Shape From Tracing: Towards Reconstructing 3D Object Geometry and SVBRDF Material from Images via Differentiable Path Tracing
- **Arxiv ID**: http://arxiv.org/abs/2012.03939v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2012.03939v1)
- **Published**: 2020-12-06 18:55:35+00:00
- **Updated**: 2020-12-06 18:55:35+00:00
- **Authors**: Purvi Goel, Loudon Cohen, James Guesman, Vikas Thamizharasan, James Tompkin, Daniel Ritchie
- **Comment**: Will be published at 3DV 2020
- **Journal**: None
- **Summary**: Reconstructing object geometry and material from multiple views typically requires optimization. Differentiable path tracing is an appealing framework as it can reproduce complex appearance effects. However, it is difficult to use due to high computational cost. In this paper, we explore how to use differentiable ray tracing to refine an initial coarse mesh and per-mesh-facet material representation. In simulation, we find that it is possible to reconstruct fine geometric and material detail from low resolution input views, allowing high-quality reconstructions in a few hours despite the expense of path tracing. The reconstructions successfully disambiguate shading, shadow, and global illumination effects such as diffuse interreflection from material properties. We demonstrate the impact of different geometry initializations, including space carving, multi-view stereo, and 3D neural networks. Finally, with input captured using smartphone video and a consumer 360? camera for lighting estimation, we also show how to refine initial reconstructions of real-world objects in unconstrained environments.



### Rethinking FUN: Frequency-Domain Utilization Networks
- **Arxiv ID**: http://arxiv.org/abs/2012.03357v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.03357v1)
- **Published**: 2020-12-06 19:16:37+00:00
- **Updated**: 2020-12-06 19:16:37+00:00
- **Authors**: Kfir Goldberg, Stav Shapiro, Elad Richardson, Shai Avidan
- **Comment**: 9 pages, 7 figures
- **Journal**: None
- **Summary**: The search for efficient neural network architectures has gained much focus in recent years, where modern architectures focus not only on accuracy but also on inference time and model size. Here, we present FUN, a family of novel Frequency-domain Utilization Networks. These networks utilize the inherent efficiency of the frequency-domain by working directly in that domain, represented with the Discrete Cosine Transform. Using modern techniques and building blocks such as compound-scaling and inverted-residual layers we generate a set of such networks allowing one to balance between size, latency and accuracy while outperforming competing RGB-based models. Extensive evaluations verifies that our networks present strong alternatives to previous approaches. Moreover, we show that working in frequency domain allows for dynamic compression of the input at inference time without any explicit change to the architecture.



### Select, Label, and Mix: Learning Discriminative Invariant Feature Representations for Partial Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2012.03358v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.03358v2)
- **Published**: 2020-12-06 19:29:32+00:00
- **Updated**: 2023-01-03 21:52:44+00:00
- **Authors**: Aadarsh Sahoo, Rameswar Panda, Rogerio Feris, Kate Saenko, Abir Das
- **Comment**: Accepted to WACV 2023. Project page:
  https://cvir.github.io/projects/slm.html
- **Journal**: None
- **Summary**: Partial domain adaptation which assumes that the unknown target label space is a subset of the source label space has attracted much attention in computer vision. Despite recent progress, existing methods often suffer from three key problems: negative transfer, lack of discriminability, and domain invariance in the latent space. To alleviate the above issues, we develop a novel 'Select, Label, and Mix' (SLM) framework that aims to learn discriminative invariant feature representations for partial domain adaptation. First, we present an efficient "select" module that automatically filters out the outlier source samples to avoid negative transfer while aligning distributions across both domains. Second, the "label" module iteratively trains the classifier using both the labeled source domain data and the generated pseudo-labels for the target domain to enhance the discriminability of the latent space. Finally, the "mix" module utilizes domain mixup regularization jointly with the other two modules to explore more intrinsic structures across domains leading to a domain-invariant latent space for partial domain adaptation. Extensive experiments on several benchmark datasets for partial domain adaptation demonstrate the superiority of our proposed framework over state-of-the-art methods.



### Self-Training for Class-Incremental Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2012.03362v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2012.03362v3)
- **Published**: 2020-12-06 19:48:35+00:00
- **Updated**: 2022-03-11 01:28:42+00:00
- **Authors**: Lu Yu, Xialei Liu, Joost van de Weijer
- **Comment**: Accepted at TNNLS
- **Journal**: IEEE Transactions on Neural Networks and Learning Systems, 2022
- **Summary**: In class-incremental semantic segmentation, we have no access to the labeled data of previous tasks. Therefore, when incrementally learning new classes, deep neural networks suffer from catastrophic forgetting of previously learned knowledge. To address this problem, we propose to apply a self-training approach that leverages unlabeled data, which is used for the rehearsal of previous knowledge. Specifically, we first learn a temporary model for the current task, and then pseudo labels for the unlabeled data are computed by fusing information from the old model of the previous task and the current temporary model. Additionally, conflict reduction is proposed to resolve the conflicts of pseudo labels generated from both the old and temporary models. We show that maximizing self-entropy can further improve results by smoothing the overconfident predictions. Interestingly, in the experiments we show that the auxiliary data can be different from the training data and that even general-purpose but diverse auxiliary data can lead to large performance gains. The experiments demonstrate state-of-the-art results: obtaining a relative gain of up to 114% on Pascal-VOC 2012 and 8.5% on the more challenging ADE20K compared to previous state-of-the-art methods.



### Spatio-Temporal Graph Scattering Transform
- **Arxiv ID**: http://arxiv.org/abs/2012.03363v3
- **DOI**: None
- **Categories**: **eess.SP**, cs.AI, cs.CV, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2012.03363v3)
- **Published**: 2020-12-06 19:49:55+00:00
- **Updated**: 2021-02-09 05:08:41+00:00
- **Authors**: Chao Pan, Siheng Chen, Antonio Ortega
- **Comment**: 18 pages, ICLR 2021
- **Journal**: None
- **Summary**: Although spatio-temporal graph neural networks have achieved great empirical success in handling multiple correlated time series, they may be impractical in some real-world scenarios due to a lack of sufficient high-quality training data. Furthermore, spatio-temporal graph neural networks lack theoretical interpretation. To address these issues, we put forth a novel mathematically designed framework to analyze spatio-temporal data. Our proposed spatio-temporal graph scattering transform (ST-GST) extends traditional scattering transforms to the spatio-temporal domain. It performs iterative applications of spatio-temporal graph wavelets and nonlinear activation functions, which can be viewed as a forward pass of spatio-temporal graph convolutional networks without training. Since all the filter coefficients in ST-GST are mathematically designed, it is promising for the real-world scenarios with limited training data, and also allows for a theoretical analysis, which shows that the proposed ST-GST is stable to small perturbations of input signals and structures. Finally, our experiments show that i) ST-GST outperforms spatio-temporal graph convolutional networks by an increase of 35% in accuracy for MSR Action3D dataset; ii) it is better and computationally more efficient to design the transform based on separable spatio-temporal graphs than the joint ones; and iii) the nonlinearity in ST-GST is critical to empirical performance.



### Visual Aware Hierarchy Based Food Recognition
- **Arxiv ID**: http://arxiv.org/abs/2012.03368v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.03368v1)
- **Published**: 2020-12-06 20:25:31+00:00
- **Updated**: 2020-12-06 20:25:31+00:00
- **Authors**: Runyu Mao, Jiangpeng He, Zeman Shao, Sri Kalyan Yarlagadda, Fengqing Zhu
- **Comment**: MADiMA 2020
- **Journal**: None
- **Summary**: Food recognition is one of the most important components in image-based dietary assessment. However, due to the different complexity level of food images and inter-class similarity of food categories, it is challenging for an image-based food recognition system to achieve high accuracy for a variety of publicly available datasets. In this work, we propose a new two-step food recognition system that includes food localization and hierarchical food classification using Convolutional Neural Networks (CNNs) as the backbone architecture. The food localization step is based on an implementation of the Faster R-CNN method to identify food regions. In the food classification step, visually similar food categories can be clustered together automatically to generate a hierarchical structure that represents the semantic visual relations among food categories, then a multi-task CNN model is proposed to perform the classification task based on the visual aware hierarchical structure. Since the size and quality of dataset is a key component of data driven methods, we introduce a new food image dataset, VIPER-FoodNet (VFN) dataset, consists of 82 food categories with 15k images based on the most commonly consumed foods in the United States. A semi-automatic crowdsourcing tool is used to provide the ground-truth information for this dataset including food object bounding boxes and food object labels. Experimental results demonstrate that our system can significantly improve both classification and recognition performance on 4 publicly available datasets and the new VFN dataset.



### Proactive Pseudo-Intervention: Causally Informed Contrastive Learning For Interpretable Vision Models
- **Arxiv ID**: http://arxiv.org/abs/2012.03369v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2012.03369v2)
- **Published**: 2020-12-06 20:30:26+00:00
- **Updated**: 2021-04-29 21:28:56+00:00
- **Authors**: Dong Wang, Yuewei Yang, Chenyang Tao, Zhe Gan, Liqun Chen, Fanjie Kong, Ricardo Henao, Lawrence Carin
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks excel at comprehending complex visual signals, delivering on par or even superior performance to that of human experts. However, ad-hoc visual explanations of model decisions often reveal an alarming level of reliance on exploiting non-causal visual cues that strongly correlate with the target label in training data. As such, deep neural nets suffer compromised generalization to novel inputs collected from different sources, and the reverse engineering of their decision rules offers limited interpretability. To overcome these limitations, we present a novel contrastive learning strategy called {\it Proactive Pseudo-Intervention} (PPI) that leverages proactive interventions to guard against image features with no causal relevance. We also devise a novel causally informed salience mapping module to identify key image pixels to intervene, and show it greatly facilitates model interpretability. To demonstrate the utility of our proposals, we benchmark on both standard natural images and challenging medical image datasets. PPI-enhanced models consistently deliver superior performance relative to competing solutions, especially on out-of-domain predictions and data integration from heterogeneous sources. Further, our causally trained saliency maps are more succinct and meaningful relative to their non-causal counterparts.



### Art Style Classification with Self-Trained Ensemble of AutoEncoding Transformations
- **Arxiv ID**: http://arxiv.org/abs/2012.03377v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, I.5.1; I.5.2; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/2012.03377v1)
- **Published**: 2020-12-06 21:05:23+00:00
- **Updated**: 2020-12-06 21:05:23+00:00
- **Authors**: Akshay Joshi, Ankit Agrawal, Sushmita Nair
- **Comment**: 6
- **Journal**: None
- **Summary**: The artistic style of a painting is a rich descriptor that reveals both visual and deep intrinsic knowledge about how an artist uniquely portrays and expresses their creative vision. Accurate categorization of paintings across different artistic movements and styles is critical for large-scale indexing of art databases. However, the automatic extraction and recognition of these highly dense artistic features has received little to no attention in the field of computer vision research. In this paper, we investigate the use of deep self-supervised learning methods to solve the problem of recognizing complex artistic styles with high intra-class and low inter-class variation. Further, we outperform existing approaches by almost 20% on a highly class imbalanced WikiArt dataset with 27 art categories. To achieve this, we train the EnAET semi-supervised learning model (Wang et al., 2019) with limited annotated data samples and supplement it with self-supervised representations learned from an ensemble of spatial and non-spatial transformations.



### Multivariate Density Estimation with Deep Neural Mixture Models
- **Arxiv ID**: http://arxiv.org/abs/2012.03391v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, cs.LG, stat.ME, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2012.03391v1)
- **Published**: 2020-12-06 23:03:48+00:00
- **Updated**: 2020-12-06 23:03:48+00:00
- **Authors**: Edmondo Trentin
- **Comment**: Extended journal version of E. Trentin, "Maximum-Likelihood
  Estimation of Neural Mixture Densities: Model, Algorithm, and Preliminary
  Experimental Evaluation". In Proc. of ANNPR 2018: 178-189, Springer, 2018
- **Journal**: None
- **Summary**: Albeit worryingly underrated in the recent literature on machine learning in general (and, on deep learning in particular), multivariate density estimation is a fundamental task in many applications, at least implicitly, and still an open issue. With a few exceptions, deep neural networks (DNNs) have seldom been applied to density estimation, mostly due to the unsupervised nature of the estimation task, and (especially) due to the need for constrained training algorithms that ended up realizing proper probabilistic models that satisfy Kolmogorov's axioms. Moreover, in spite of the well-known improvement in terms of modeling capabilities yielded by mixture models over plain single-density statistical estimators, no proper mixtures of multivariate DNN-based component densities have been investigated so far. The paper fills this gap by extending our previous work on Neural Mixture Densities (NMMs) to multivariate DNN mixtures. A maximum-likelihood (ML) algorithm for estimating Deep NMMs (DNMMs) is handed out, which satisfies numerically a combination of hard and soft constraints aimed at ensuring satisfaction of Kolmogorov's axioms. The class of probability density functions that can be modeled to any degree of precision via DNMMs is formally defined. A procedure for the automatic selection of the DNMM architecture, as well as of the hyperparameters for its ML training algorithm, is presented (exploiting the probabilistic nature of the DNMM). Experimental results on univariate and multivariate data are reported on, corroborating the effectiveness of the approach and its superiority to the most popular statistical estimation techniques.



