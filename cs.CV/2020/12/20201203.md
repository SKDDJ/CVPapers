# Arxiv Papers in cs.CV on 2020-12-03
### Going Beyond Classification Accuracy Metrics in Model Compression
- **Arxiv ID**: http://arxiv.org/abs/2012.01604v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2012.01604v2)
- **Published**: 2020-12-03 00:00:41+00:00
- **Updated**: 2021-06-14 20:10:09+00:00
- **Authors**: Vinu Joseph, Shoaib Ahmed Siddiqui, Aditya Bhaskara, Ganesh Gopalakrishnan, Saurav Muralidharan, Michael Garland, Sheraz Ahmed, Andreas Dengel
- **Comment**: None
- **Journal**: None
- **Summary**: With the rise in edge-computing devices, there has been an increasing demand to deploy energy and resource-efficient models. A large body of research has been devoted to developing methods that can reduce the size of the model considerably without affecting the standard metrics such as top-1 accuracy. However, these pruning approaches tend to result in a significant mismatch in other metrics such as fairness across classes and explainability. To combat such misalignment, we propose a novel multi-part loss function inspired by the knowledge-distillation literature. Through extensive experiments, we demonstrate the effectiveness of our approach across different compression algorithms, architectures, tasks as well as datasets. In particular, we obtain up to $4.1\times$ reduction in the number of prediction mismatches between the compressed and reference models, and up to $5.7\times$ in cases where the reference model makes the correct prediction; all while making no changes to the compression algorithm, and minor modifications to the loss function. Furthermore, we demonstrate how inducing simple alignment between the predictions of the models naturally improves the alignment on other metrics including fairness and attributions. Our framework can thus serve as a simple plug-and-play component for compression algorithms in the future.



### Single-shot Path Integrated Panoptic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2012.01632v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.01632v2)
- **Published**: 2020-12-03 01:50:30+00:00
- **Updated**: 2020-12-04 02:34:32+00:00
- **Authors**: Sukjun Hwang, Seoung Wug Oh, Seon Joo Kim
- **Comment**: 10 pages, 5 figures
- **Journal**: None
- **Summary**: Panoptic segmentation, which is a novel task of unifying instance segmentation and semantic segmentation, has attracted a lot of attention lately. However, most of the previous methods are composed of multiple pathways with each pathway specialized to a designated segmentation task. In this paper, we propose to resolve panoptic segmentation in single-shot by integrating the execution flows. With the integrated pathway, a unified feature map called Panoptic-Feature is generated, which includes the information of both things and stuffs. Panoptic-Feature becomes more sophisticated by auxiliary problems that guide to cluster pixels that belong to the same instance and differentiate between objects of different classes. A collection of convolutional filters, where each filter represents either a thing or stuff, is applied to Panoptic-Feature at once, materializing the single-shot panoptic segmentation. Taking the advantages of both top-down and bottom-up approaches, our method, named SPINet, enjoys high efficiency and accuracy on major panoptic segmentation benchmarks: COCO and Cityscapes.



### Rel3D: A Minimally Contrastive Benchmark for Grounding Spatial Relations in 3D
- **Arxiv ID**: http://arxiv.org/abs/2012.01634v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.01634v1)
- **Published**: 2020-12-03 01:51:56+00:00
- **Updated**: 2020-12-03 01:51:56+00:00
- **Authors**: Ankit Goyal, Kaiyu Yang, Dawei Yang, Jia Deng
- **Comment**: Accepted to NeurIPS 2020
- **Journal**: None
- **Summary**: Understanding spatial relations (e.g., "laptop on table") in visual input is important for both humans and robots. Existing datasets are insufficient as they lack large-scale, high-quality 3D ground truth information, which is critical for learning spatial relations. In this paper, we fill this gap by constructing Rel3D: the first large-scale, human-annotated dataset for grounding spatial relations in 3D. Rel3D enables quantifying the effectiveness of 3D information in predicting spatial relations on large-scale human data. Moreover, we propose minimally contrastive data collection -- a novel crowdsourcing method for reducing dataset bias. The 3D scenes in our dataset come in minimally contrastive pairs: two scenes in a pair are almost identical, but a spatial relation holds in one and fails in the other. We empirically validate that minimally contrastive examples can diagnose issues with current relation detection models as well as lead to sample-efficient training. Code and data are available at https://github.com/princeton-vl/Rel3D.



### Meta-Generating Deep Attentive Metric for Few-shot Classification
- **Arxiv ID**: http://arxiv.org/abs/2012.01641v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.01641v1)
- **Published**: 2020-12-03 02:07:43+00:00
- **Updated**: 2020-12-03 02:07:43+00:00
- **Authors**: Lei Zhang, Fei Zhou, Wei Wei, Yanning Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Learning to generate a task-aware base learner proves a promising direction to deal with few-shot learning (FSL) problem. Existing methods mainly focus on generating an embedding model utilized with a fixed metric (eg, cosine distance) for nearest neighbour classification or directly generating a linear classier. However, due to the limited discriminative capacity of such a simple metric or classifier, these methods fail to generalize to challenging cases appropriately. To mitigate this problem, we present a novel deep metric meta-generation method that turns to an orthogonal direction, ie, learning to adaptively generate a specific metric for a new FSL task based on the task description (eg, a few labelled samples). In this study, we structure the metric using a three-layer deep attentive network that is flexible enough to produce a discriminative metric for each task. Moreover, different from existing methods that utilize an uni-modal weight distribution conditioned on labelled samples for network generation, the proposed meta-learner establishes a multi-modal weight distribution conditioned on cross-class sample pairs using a tailored variational autoencoder, which can separately capture the specific inter-class discrepancy statistics for each class and jointly embed the statistics for all classes into metric generation. By doing this, the generated metric can be appropriately adapted to a new FSL task with pleasing generalization performance. To demonstrate this, we test the proposed method on four benchmark FSL datasets and gain surprisingly obvious performance improvement over state-of-the-art competitors, especially in the challenging cases, eg, improve the accuracy from 26.14% to 46.69% in the 20-way 1-shot task on miniImageNet, while improve the accuracy from 45.2% to 68.72% in the 5-way 1-shot task on FC100. Code is available: https://github.com/NWPUZhoufei/DAM.



### Learning to Transfer Visual Effects from Videos to Images
- **Arxiv ID**: http://arxiv.org/abs/2012.01642v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.01642v2)
- **Published**: 2020-12-03 02:10:14+00:00
- **Updated**: 2020-12-17 17:32:50+00:00
- **Authors**: Christopher Thomas, Yale Song, Adriana Kovashka
- **Comment**: None
- **Journal**: None
- **Summary**: We study the problem of animating images by transferring spatio-temporal visual effects (such as melting) from a collection of videos. We tackle two primary challenges in visual effect transfer: 1) how to capture the effect we wish to distill; and 2) how to ensure that only the effect, rather than content or artistic style, is transferred from the source videos to the input image. To address the first challenge, we evaluate five loss functions; the most promising one encourages the generated animations to have similar optical flow and texture motions as the source videos. To address the second challenge, we only allow our model to move existing image pixels from the previous frame, rather than predicting unconstrained pixel values. This forces any visual effects to occur using the input image's pixels, preventing unwanted artistic style or content from the source video from appearing in the output. We evaluate our method in objective and subjective settings, and show interesting qualitative results which demonstrate objects undergoing atypical transformations, such as making a face melt or a deer bloom.



### Capturing implicit hierarchical structure in 3D biomedical images with self-supervised hyperbolic representations
- **Arxiv ID**: http://arxiv.org/abs/2012.01644v3
- **DOI**: None
- **Categories**: **cs.CV**, I.4.10
- **Links**: [PDF](http://arxiv.org/pdf/2012.01644v3)
- **Published**: 2020-12-03 02:15:31+00:00
- **Updated**: 2021-10-25 22:36:34+00:00
- **Authors**: Joy Hsu, Jeffrey Gu, Gong-Her Wu, Wah Chiu, Serena Yeung
- **Comment**: To appear at NeurIPS 2021
- **Journal**: None
- **Summary**: We consider the task of representation learning for unsupervised segmentation of 3D voxel-grid biomedical images. We show that models that capture implicit hierarchical relationships between subvolumes are better suited for this task. To that end, we consider encoder-decoder architectures with a hyperbolic latent space, to explicitly capture hierarchical relationships present in subvolumes of the data. We propose utilizing a 3D hyperbolic variational autoencoder with a novel gyroplane convolutional layer to map from the embedding space back to 3D images. To capture these relationships, we introduce an essential self-supervised loss -- in addition to the standard VAE loss -- which infers approximate hierarchies and encourages implicitly related subvolumes to be mapped closer in the embedding space. We present experiments on both synthetic data and biomedical data to validate our hypothesis.



### Towards Defending Multiple $\ell_p$-norm Bounded Adversarial Perturbations via Gated Batch Normalization
- **Arxiv ID**: http://arxiv.org/abs/2012.01654v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.01654v2)
- **Published**: 2020-12-03 02:26:01+00:00
- **Updated**: 2023-08-11 12:57:04+00:00
- **Authors**: Aishan Liu, Shiyu Tang, Xinyun Chen, Lei Huang, Haotong Qin, Xianglong Liu, Dacheng Tao
- **Comment**: Accepted on IJCV
- **Journal**: None
- **Summary**: There has been extensive evidence demonstrating that deep neural networks are vulnerable to adversarial examples, which motivates the development of defenses against adversarial attacks. Existing adversarial defenses typically improve model robustness against individual specific perturbation types (\eg, $\ell_{\infty}$-norm bounded adversarial examples). However, adversaries are likely to generate multiple types of perturbations in practice (\eg, $\ell_1$, $\ell_2$, and $\ell_{\infty}$ perturbations). Some recent methods improve model robustness against adversarial attacks in multiple $\ell_p$ balls, but their performance against each perturbation type is still far from satisfactory. In this paper, we observe that different $\ell_p$ bounded adversarial perturbations induce different statistical properties that can be separated and characterized by the statistics of Batch Normalization (BN). We thus propose Gated Batch Normalization (GBN) to adversarially train a perturbation-invariant predictor for defending multiple $\ell_p$ bounded adversarial perturbations. GBN consists of a multi-branch BN layer and a gated sub-network. Each BN branch in GBN is in charge of one perturbation type to ensure that the normalized output is aligned towards learning perturbation-invariant representation. Meanwhile, the gated sub-network is designed to separate inputs added with different perturbation types. We perform an extensive evaluation of our approach on commonly-used dataset including MNIST, CIFAR-10, and Tiny-ImageNet, and demonstrate that GBN outperforms previous defense proposals against multiple perturbation types (\ie, $\ell_1$, $\ell_2$, and $\ell_{\infty}$ perturbations) by large margins.



### Dual-Branch Network with Dual-Sampling Modulated Dice Loss for Hard Exudate Segmentation from Colour Fundus Images
- **Arxiv ID**: http://arxiv.org/abs/2012.01665v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.01665v2)
- **Published**: 2020-12-03 02:37:13+00:00
- **Updated**: 2021-05-06 06:59:31+00:00
- **Authors**: Qing Liu, Haotian Liu, Yixiong Liang
- **Comment**: None
- **Journal**: None
- **Summary**: Automated segmentation of hard exudates in colour fundus images is a challenge task due to issues of extreme class imbalance and enormous size variation. This paper aims to tackle these issues and proposes a dual-branch network with dual-sampling modulated Dice loss. It consists of two branches: large hard exudate biased learning branch and small hard exudate biased learning branch. Both of them are responsible for their own duty separately. Furthermore, we propose a dual-sampling modulated Dice loss for the training such that our proposed dual-branch network is able to segment hard exudates in different sizes. In detail, for the first branch, we use a uniform sampler to sample pixels from predicted segmentation mask for Dice loss calculation, which leads to this branch naturally be biased in favour of large hard exudates as Dice loss generates larger cost on misidentification of large hard exudates than small hard exudates. For the second branch, we use a re-balanced sampler to oversample hard exudate pixels and undersample background pixels for loss calculation. In this way, cost on misidentification of small hard exudates is enlarged, which enforces the parameters in the second branch fit small hard exudates well. Considering that large hard exudates are much easier to be correctly identified than small hard exudates, we propose an easy-to-difficult learning strategy by adaptively modulating the losses of two branches. We evaluate our proposed method on two public datasets and results demonstrate that ours achieves state-of-the-art performances.



### Interpretable Graph Capsule Networks for Object Recognition
- **Arxiv ID**: http://arxiv.org/abs/2012.01674v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.01674v3)
- **Published**: 2020-12-03 03:18:00+00:00
- **Updated**: 2021-03-07 16:50:54+00:00
- **Authors**: Jindong Gu, Volker Tresp
- **Comment**: None
- **Journal**: The Thirty-Fifth AAAI Conference on Artificial Intelligence
  (AAAI), 2021
- **Summary**: Capsule Networks, as alternatives to Convolutional Neural Networks, have been proposed to recognize objects from images. The current literature demonstrates many advantages of CapsNets over CNNs. However, how to create explanations for individual classifications of CapsNets has not been well explored. The widely used saliency methods are mainly proposed for explaining CNN-based classifications; they create saliency map explanations by combining activation values and the corresponding gradients, e.g., Grad-CAM. These saliency methods require a specific architecture of the underlying classifiers and cannot be trivially applied to CapsNets due to the iterative routing mechanism therein. To overcome the lack of interpretability, we can either propose new post-hoc interpretation methods for CapsNets or modifying the model to have build-in explanations. In this work, we explore the latter. Specifically, we propose interpretable Graph Capsule Networks (GraCapsNets), where we replace the routing part with a multi-head attention-based Graph Pooling approach. In the proposed model, individual classification explanations can be created effectively and efficiently. Our model also demonstrates some unexpected benefits, even though it replaces the fundamental part of CapsNets. Our GraCapsNets achieve better classification performance with fewer parameters and better adversarial robustness, when compared to CapsNets. Besides, GraCapsNets also keep other advantages of CapsNets, namely, disentangled representations and affine transformation robustness.



### Lookahead optimizer improves the performance of Convolutional Autoencoders for reconstruction of natural images
- **Arxiv ID**: http://arxiv.org/abs/2012.05694v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR, cs.LG, physics.data-an
- **Links**: [PDF](http://arxiv.org/pdf/2012.05694v1)
- **Published**: 2020-12-03 03:18:28+00:00
- **Updated**: 2020-12-03 03:18:28+00:00
- **Authors**: Sayan Nag
- **Comment**: None
- **Journal**: None
- **Summary**: Autoencoders are a class of artificial neural networks which have gained a lot of attention in the recent past. Using the encoder block of an autoencoder the input image can be compressed into a meaningful representation. Then a decoder is employed to reconstruct the compressed representation back to a version which looks like the input image. It has plenty of applications in the field of data compression and denoising. Another version of Autoencoders (AE) exist, called Variational AE (VAE) which acts as a generative model like GAN. Recently, an optimizer was introduced which is known as lookahead optimizer which significantly enhances the performances of Adam as well as SGD. In this paper, we implement Convolutional Autoencoders (CAE) and Convolutional Variational Autoencoders (CVAE) with lookahead optimizer (with Adam) and compare them with the Adam (only) optimizer counterparts. For this purpose, we have used a movie dataset comprising of natural images for the former case and CIFAR100 for the latter case. We show that lookahead optimizer (with Adam) improves the performance of CAEs for reconstruction of natural images.



### Motion-based Camera Localization System in Colonoscopy Videos
- **Arxiv ID**: http://arxiv.org/abs/2012.01690v3
- **DOI**: 10.1016/j.media.2021.102180
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2012.01690v3)
- **Published**: 2020-12-03 03:57:12+00:00
- **Updated**: 2021-02-11 20:09:22+00:00
- **Authors**: Heming Yao, Ryan W. Stidham, Zijun Gao, Jonathan Gryak, Kayvan Najarian
- **Comment**: None
- **Journal**: Medical image analysis. 2021 Oct 1;73:102180
- **Summary**: Optical colonoscopy is an essential diagnostic and prognostic tool for many gastrointestinal diseases, including cancer screening and staging, intestinal bleeding, diarrhea, abdominal symptom evaluation, and inflammatory bowel disease assessment. Automated assessment of colonoscopy is of interest considering the subjectivity present in qualitative human interpretations of colonoscopy findings. Localization of the camera is essential to interpreting the meaning and context of findings for diseases evaluated by colonoscopy. In this study, we propose a camera localization system to estimate the relative location of the camera and classify the colon into anatomical segments. The camera localization system begins with non-informative frame detection and removal. Then a self-training end-to-end convolutional neural network is built to estimate the camera motion, where several strategies are proposed to improve its robustness and generalization on endoscopic videos. Using the estimated camera motion a camera trajectory can be derived and a relative location index calculated. Based on the estimated location index, anatomical colon segment classification is performed by constructing a colon template. The proposed motion estimation algorithm was evaluated on an external dataset containing the ground truth for camera pose. The experimental results show that the performance of the proposed method is superior to other published methods. The relative location index estimation and anatomical region classification were further validated using colonoscopy videos collected from routine clinical practice. This validation yielded an average accuracy in classification of 0.754, which is substantially higher than the performances obtained using location indices built from other methods.



### Relational Learning for Skill Preconditions
- **Arxiv ID**: http://arxiv.org/abs/2012.01693v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2012.01693v1)
- **Published**: 2020-12-03 04:13:49+00:00
- **Updated**: 2020-12-03 04:13:49+00:00
- **Authors**: Mohit Sharma, Oliver Kroemer
- **Comment**: CoRL'20
- **Journal**: None
- **Summary**: To determine if a skill can be executed in any given environment, a robot needs to learn the preconditions for the skill. As robots begin to operate in dynamic and unstructured environments, precondition models will need to generalize to variable number of objects with different shapes and sizes. In this work, we focus on learning precondition models for manipulation skills in unconstrained environments. Our work is motivated by the intuition that many complex manipulation tasks, with multiple objects, can be simplified by focusing on less complex pairwise object relations. We propose an object-relation model that learns continuous representations for these pairwise object relations. Our object-relation model is trained completely in simulation, and once learned, is used by a separate precondition model to predict skill preconditions for real world tasks. We evaluate our precondition model on $3$ different manipulation tasks: sweeping, cutting, and unstacking. We show that our approach leads to significant improvements in predicting preconditions for all 3 tasks, across objects of different shapes and sizes.



### Content-Adaptive Pixel Discretization to Improve Model Robustness
- **Arxiv ID**: http://arxiv.org/abs/2012.01699v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2012.01699v4)
- **Published**: 2020-12-03 04:40:51+00:00
- **Updated**: 2022-10-11 04:11:37+00:00
- **Authors**: Ryan Feng, Wu-chi Feng, Atul Prakash
- **Comment**: None
- **Journal**: None
- **Summary**: Preprocessing defenses such as pixel discretization are appealing to remove adversarial attacks due to their simplicity. However, they have been shown to be ineffective except on simple datasets like MNIST. We hypothesize that existing discretization approaches failed because using a fixed codebook for the entire dataset limits their ability to balance image representation and codeword separability. We first formally prove that adaptive codebooks can provide stronger robustness guarantees than fixed codebooks as a preprocessing defense on some datasets. Based on that insight, we propose a content-adaptive pixel discretization defense called Essential Features, which discretizes the image to a per-image adaptive codebook to reduce the color space. We then find that Essential Features can be further optimized by applying adaptive blurring before the discretization to push perturbed pixel values back to their original value before determining the codebook. Against adaptive attacks, we show that content-adaptive pixel discretization extends the range of datasets that benefit in terms of both L_2 and L_infinity robustness where previously fixed codebooks were found to have failed. Our findings suggest that content-adaptive pixel discretization should be part of the repertoire for making models robust.



### Robust Federated Learning with Noisy Labels
- **Arxiv ID**: http://arxiv.org/abs/2012.01700v1
- **DOI**: 10.1109/MIS.2022.3151466
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2012.01700v1)
- **Published**: 2020-12-03 04:46:07+00:00
- **Updated**: 2020-12-03 04:46:07+00:00
- **Authors**: Seunghan Yang, Hyoungseob Park, Junyoung Byun, Changick Kim
- **Comment**: None
- **Journal**: IEEE Intelligent Systems, 2022
- **Summary**: Federated learning is a paradigm that enables local devices to jointly train a server model while keeping the data decentralized and private. In federated learning, since local data are collected by clients, it is hardly guaranteed that the data are correctly annotated. Although a lot of studies have been conducted to train the networks robust to these noisy data in a centralized setting, these algorithms still suffer from noisy labels in federated learning. Compared to the centralized setting, clients' data can have different noise distributions due to variations in their labeling systems or background knowledge of users. As a result, local models form inconsistent decision boundaries and their weights severely diverge from each other, which are serious problems in federated learning. To solve these problems, we introduce a novel federated learning scheme that the server cooperates with local models to maintain consistent decision boundaries by interchanging class-wise centroids. These centroids are central features of local data on each device, which are aligned by the server every communication round. Updating local models with the aligned centroids helps to form consistent decision boundaries among local models, although the noise distributions in clients' data are different from each other. To improve local model performance, we introduce a novel approach to select confident samples that are used for updating the model with given labels. Furthermore, we propose a global-guided pseudo-labeling method to update labels of unconfident samples by exploiting the global model. Our experimental results on the noisy CIFAR-10 dataset and the Clothing1M dataset show that our approach is noticeably effective in federated learning with noisy labels.



### AutoInt: Automatic Integration for Fast Neural Volume Rendering
- **Arxiv ID**: http://arxiv.org/abs/2012.01714v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2012.01714v2)
- **Published**: 2020-12-03 05:46:10+00:00
- **Updated**: 2021-05-23 02:23:47+00:00
- **Authors**: David B. Lindell, Julien N. P. Martel, Gordon Wetzstein
- **Comment**: None
- **Journal**: None
- **Summary**: Numerical integration is a foundational technique in scientific computing and is at the core of many computer vision applications. Among these applications, neural volume rendering has recently been proposed as a new paradigm for view synthesis, achieving photorealistic image quality. However, a fundamental obstacle to making these methods practical is the extreme computational and memory requirements caused by the required volume integrations along the rendered rays during training and inference. Millions of rays, each requiring hundreds of forward passes through a neural network are needed to approximate those integrations with Monte Carlo sampling. Here, we propose automatic integration, a new framework for learning efficient, closed-form solutions to integrals using coordinate-based neural networks. For training, we instantiate the computational graph corresponding to the derivative of the network. The graph is fitted to the signal to integrate. After optimization, we reassemble the graph to obtain a network that represents the antiderivative. By the fundamental theorem of calculus, this enables the calculation of any definite integral in two evaluations of the network. Applying this approach to neural rendering, we improve a tradeoff between rendering speed and image quality: improving render times by greater than 10 times with a tradeoff of slightly reduced image quality.



### Enabling Collaborative Video Sensing at the Edge through Convolutional Sharing
- **Arxiv ID**: http://arxiv.org/abs/2012.08643v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.08643v1)
- **Published**: 2020-12-03 06:29:09+00:00
- **Updated**: 2020-12-03 06:29:09+00:00
- **Authors**: Kasthuri Jayarajah, Dhanuja Wanniarachchige, Archan Misra
- **Comment**: None
- **Journal**: None
- **Summary**: While Deep Neural Network (DNN) models have provided remarkable advances in machine vision capabilities, their high computational complexity and model sizes present a formidable roadblock to deployment in AIoT-based sensing applications. In this paper, we propose a novel paradigm by which peer nodes in a network can collaborate to improve their accuracy on person detection, an exemplar machine vision task. The proposed methodology requires no re-training of the DNNs and incurs minimal processing latency as it extracts scene summaries from the collaborators and injects back into DNNs of the reference cameras, on-the-fly. Early results show promise with improvements in recall as high as 10% with a single collaborator, on benchmark datasets.



### Parallel Residual Bi-Fusion Feature Pyramid Network for Accurate Single-Shot Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2012.01724v5
- **DOI**: 10.1109/TIP.2021.3118953
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.01724v5)
- **Published**: 2020-12-03 06:51:20+00:00
- **Updated**: 2023-05-18 15:33:06+00:00
- **Authors**: Ping-Yang Chen, Ming-Ching Chang, Jun-Wei Hsieh, Yong-Sheng Chen
- **Comment**: accepted by IEEE transactions on Image Processing
- **Journal**: IEEE Transactions on Image Processing, vol. 30, pp. 9099-9111,
  2021
- **Summary**: This paper proposes the Parallel Residual Bi-Fusion Feature Pyramid Network (PRB-FPN) for fast and accurate single-shot object detection. Feature Pyramid (FP) is widely used in recent visual detection, however the top-down pathway of FP cannot preserve accurate localization due to pooling shifting. The advantage of FP is weakened as deeper backbones with more layers are used. In addition, it cannot keep up accurate detection of both small and large objects at the same time. To address these issues, we propose a new parallel FP structure with bi-directional (top-down and bottom-up) fusion and associated improvements to retain high-quality features for accurate localization. We provide the following design improvements: (1) A parallel bifusion FP structure with a bottom-up fusion module (BFM) to detect both small and large objects at once with high accuracy. (2) A concatenation and re-organization (CORE) module provides a bottom-up pathway for feature fusion, which leads to the bi-directional fusion FP that can recover lost information from lower-layer feature maps. (3) The CORE feature is further purified to retain richer contextual information. Such CORE purification in both top-down and bottom-up pathways can be finished in only a few iterations. (4) The adding of a residual design to CORE leads to a new Re-CORE module that enables easy training and integration with a wide range of deeper or lighter backbones. The proposed network achieves state-of-the-art performance on the UAVDT17 and MS COCO datasets. Code is available at https://github.com/pingyang1117/PRBNet_PyTorch.



### Dual Refinement Feature Pyramid Networks for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2012.01733v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.01733v2)
- **Published**: 2020-12-03 07:17:03+00:00
- **Updated**: 2020-12-04 02:57:40+00:00
- **Authors**: Jialiang Ma, Bin Chen
- **Comment**: None
- **Journal**: None
- **Summary**: FPN is a common component used in object detectors, it supplements multi-scale information by adjacent level features interpolation and summation. However, due to the existence of nonlinear operations and the convolutional layers with different output dimensions, the relationship between different levels is much more complex, the pixel-wise summation is not an efficient approach. In this paper, we first analyze the design defects from pixel level and feature map level. Then, we design a novel parameter-free feature pyramid networks named Dual Refinement Feature Pyramid Networks (DRFPN) for the problems. Specifically, DRFPN consists of two modules: Spatial Refinement Block (SRB) and Channel Refinement Block (CRB). SRB learns the location and content of sampling points based on contextual information between adjacent levels. CRB learns an adaptive channel merging method based on attention mechanism. Our proposed DRFPN can be easily plugged into existing FPN-based models. Without bells and whistles, for two-stage detectors, our model outperforms different FPN-based counterparts by 1.6 to 2.2 AP on the COCO detection benchmark, and 1.5 to 1.9 AP on the COCO segmentation benchmark. For one-stage detectors, DRFPN improves anchor-based RetinaNet by 1.9 AP and anchor-free FCOS by 1.3 AP when using ResNet50 as backbone. Extensive experiments verifies the robustness and generalization ability of DRFPN. The code will be made publicly available.



### Sparse Semi-Supervised Action Recognition with Active Learning
- **Arxiv ID**: http://arxiv.org/abs/2012.01740v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2012.01740v2)
- **Published**: 2020-12-03 07:48:31+00:00
- **Updated**: 2020-12-07 20:28:54+00:00
- **Authors**: Jingyuan Li, Eli Shlizerman
- **Comment**: None
- **Journal**: None
- **Summary**: Current state-of-the-art methods for skeleton-based action recognition are supervised and rely on labels. The reliance is limiting the performance due to the challenges involved in annotation and mislabeled data. Unsupervised methods have been introduced, however, they organize sequences into clusters and still require labels to associate clusters with actions. In this paper, we propose a novel approach for skeleton-based action recognition, called SESAR, that connects these approaches. SESAR leverages the information from both unlabeled data and a handful of sequences actively selected for labeling, combining unsupervised training with sparsely supervised guidance. SESAR is composed of two main components, where the first component learns a latent representation for unlabeled action sequences through an Encoder-Decoder RNN which reconstructs the sequences, and the second component performs active learning to select sequences to be labeled based on cluster and classification uncertainty. When the two components are simultaneously trained on skeleton-based action sequences, they correspond to a robust system for action recognition with only a handful of labeled samples. We evaluate our system on common datasets with multiple sequences and actions, such as NW UCLA, NTU RGB+D 60, and UWA3D. Our results outperform standalone skeleton-based supervised, unsupervised with cluster identification, and active-learning methods for action recognition when applied to sparse labeled samples, as low as 1% of the data.



### 3D-NVS: A 3D Supervision Approach for Next View Selection
- **Arxiv ID**: http://arxiv.org/abs/2012.01743v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2012.01743v1)
- **Published**: 2020-12-03 07:50:16+00:00
- **Updated**: 2020-12-03 07:50:16+00:00
- **Authors**: Kumar Ashutosh, Saurabh Kumar, Subhasis Chaudhuri
- **Comment**: Submitted to CVPR-21
- **Journal**: None
- **Summary**: We present a classification based approach for the next best view selection and show how we can plausibly obtain a supervisory signal for this task. The proposed approach is end-to-end trainable and aims to get the best possible 3D reconstruction quality with a pair of passively acquired 2D views. The proposed model consists of two stages: a classifier and a reconstructor network trained jointly via the indirect 3D supervision from ground truth voxels. While testing, the proposed method assumes no prior knowledge of the underlying 3D shape for selecting the next best view. We demonstrate the proposed method's effectiveness via detailed experiments on synthetic and real images and show how it provides improved reconstruction quality than the existing state of the art 3D reconstruction and the next best view prediction techniques.



### Unsupervised Alternating Optimization for Blind Hyperspectral Imagery Super-resolution
- **Arxiv ID**: http://arxiv.org/abs/2012.01745v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2012.01745v1)
- **Published**: 2020-12-03 07:52:32+00:00
- **Updated**: 2020-12-03 07:52:32+00:00
- **Authors**: Jiangtao Nie, Lei Zhang, Wei Wei, Zhiqiang Lang, Yanning Zhang
- **Comment**: 14 page, 13 figure
- **Journal**: None
- **Summary**: Despite the great success of deep model on Hyperspectral imagery (HSI) super-resolution(SR) for simulated data, most of them function unsatisfactory when applied to the real data, especially for unsupervised HSI SR methods. One of the main reason comes from the fact that the predefined degeneration models (e.g. blur in spatial domain) utilized by most HSI SR methods often exist great discrepancy with the real one, which results in these deep models overfit and ultimately degrade their performance on real data. To well mitigate such a problem, we explore the unsupervised blind HSI SR method. Specifically, we investigate how to effectively obtain the degeneration models in spatial and spectral domain, respectively, and makes them can well compatible with the fusion based SR reconstruction model. To this end, we first propose an alternating optimization based deep framework to estimate the degeneration models and reconstruct the latent image, with which the degeneration models estimation and HSI reconstruction can mutually promotes each other. Then, a meta-learning based mechanism is further proposed to pre-train the network, which can effectively improve the speed and generalization ability adapting to different complex degeneration. Experiments on three benchmark HSI SR datasets report an excellent superiority of the proposed method on handling blind HSI fusion problem over other competing methods.



### Triplet Entropy Loss: Improving The Generalisation of Short Speech Language Identification Systems
- **Arxiv ID**: http://arxiv.org/abs/2012.03775v1
- **DOI**: None
- **Categories**: **cs.SD**, cs.CV, cs.LG, eess.AS, I.2.7
- **Links**: [PDF](http://arxiv.org/pdf/2012.03775v1)
- **Published**: 2020-12-03 08:20:03+00:00
- **Updated**: 2020-12-03 08:20:03+00:00
- **Authors**: Ruan van der Merwe
- **Comment**: 22 pages, 26 figures, Code available at
  https://github.com/ruanvdmerwe/triplet-entropy-loss
- **Journal**: None
- **Summary**: We present several methods to improve the generalisation of language identification (LID) systems to new speakers and to new domains. These methods involve Spectral augmentation, where spectrograms are masked in the frequency or time bands during training and CNN architectures that are pre-trained on the Imagenet dataset. The paper also introduces the novel Triplet Entropy Loss training method, which involves training a network simultaneously using Cross Entropy and Triplet loss. It was found that all three methods improved the generalisation of the models, though not significantly. Even though the models trained using Triplet Entropy Loss showed a better understanding of the languages and higher accuracies, it appears as though the models still memorise word patterns present in the spectrograms rather than learning the finer nuances of a language. The research shows that Triplet Entropy Loss has great potential and should be investigated further, not only in language identification tasks but any classification task.



### Understanding Failures of Deep Networks via Robust Feature Extraction
- **Arxiv ID**: http://arxiv.org/abs/2012.01750v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.01750v3)
- **Published**: 2020-12-03 08:33:29+00:00
- **Updated**: 2021-06-12 07:15:51+00:00
- **Authors**: Sahil Singla, Besmira Nushi, Shital Shah, Ece Kamar, Eric Horvitz
- **Comment**: Accepted at CVPR, 2021
- **Journal**: None
- **Summary**: Traditional evaluation metrics for learned models that report aggregate scores over a test set are insufficient for surfacing important and informative patterns of failure over features and instances. We introduce and study a method aimed at characterizing and explaining failures by identifying visual attributes whose presence or absence results in poor performance. In distinction to previous work that relies upon crowdsourced labels for visual attributes, we leverage the representation of a separate robust model to extract interpretable features and then harness these features to identify failure modes. We further propose a visualization method aimed at enabling humans to understand the meaning encoded in such features and we test the comprehensibility of the features. An evaluation of the methods on the ImageNet dataset demonstrates that: (i) the proposed workflow is effective for discovering important failure modes, (ii) the visualization techniques help humans to understand the extracted features, and (iii) the extracted insights can assist engineers with error analysis and debugging.



### Pedestrian Trajectory Prediction using Context-Augmented Transformer Networks
- **Arxiv ID**: http://arxiv.org/abs/2012.01757v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.01757v2)
- **Published**: 2020-12-03 08:43:12+00:00
- **Updated**: 2021-08-08 15:16:14+00:00
- **Authors**: Khaled Saleh
- **Comment**: This work has been submitted for publication
- **Journal**: None
- **Summary**: Forecasting the trajectory of pedestrians in shared urban traffic environments is still considered one of the challenging problems facing the development of autonomous vehicles (AVs). In the literature, this problem is often tackled using recurrent neural networks (RNNs). Despite the powerful capabilities of RNNs in capturing the temporal dependency in the pedestrians' motion trajectories, they were argued to be challenged when dealing with longer sequential data. Thus, in this work, we are introducing a framework based on the transformer networks that were shown recently to be more efficient and outperformed RNNs in many sequential-based tasks. We relied on a fusion of the past positional information, agent interactions information and scene physical semantics information as an input to our framework in order to provide a robust trajectory prediction of pedestrians. We have evaluated our framework on two real-life datasets of pedestrians in shared urban traffic environments and it has outperformed the compared baseline approaches in both short-term and long-term prediction horizons.



### Beyond Cats and Dogs: Semi-supervised Classification of fuzzy labels with overclustering
- **Arxiv ID**: http://arxiv.org/abs/2012.01768v2
- **DOI**: 10.3390/s21196661
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.01768v2)
- **Published**: 2020-12-03 08:54:25+00:00
- **Updated**: 2021-10-19 12:16:16+00:00
- **Authors**: Lars Schmarje, Johannes Brünger, Monty Santarossa, Simon-Martin Schröder, Rainer Kiko, Reinhard Koch
- **Comment**: Reworked version available at arXiv:2110.06630, Published in Sensors
  2021 (see DOI link)
- **Journal**: None
- **Summary**: A long-standing issue with deep learning is the need for large and consistently labeled datasets. Although the current research in semi-supervised learning can decrease the required amount of annotated data by a factor of 10 or even more, this line of research still uses distinct classes like cats and dogs. However, in the real-world we often encounter problems where different experts have different opinions, thus producing fuzzy labels. We propose a novel framework for handling semi-supervised classifications of such fuzzy labels. Our framework is based on the idea of overclustering to detect substructures in these fuzzy labels. We propose a novel loss to improve the overclustering capability of our framework and show on the common image classification dataset STL-10 that it is faster and has better overclustering performance than previous work. On a real-world plankton dataset, we illustrate the benefit of overclustering for fuzzy labels and show that we beat previous state-of-the-art semisupervised methods. Moreover, we acquire 5 to 10% more consistent predictions of substructures.



### Flow-based Deformation Guidance for Unpaired Multi-Contrast MRI Image-to-Image Translation
- **Arxiv ID**: http://arxiv.org/abs/2012.01777v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2012.01777v1)
- **Published**: 2020-12-03 09:10:22+00:00
- **Updated**: 2020-12-03 09:10:22+00:00
- **Authors**: Toan Duc Bui, Manh Nguyen, Ngan Le, Khoa Luu
- **Comment**: Medical Image Computing and Computer Assisted Interventions
- **Journal**: None
- **Summary**: Image synthesis from corrupted contrasts increases the diversity of diagnostic information available for many neurological diseases. Recently the image-to-image translation has experienced significant levels of interest within medical research, beginning with the successful use of the Generative Adversarial Network (GAN) to the introduction of cyclic constraint extended to multiple domains. However, in current approaches, there is no guarantee that the mapping between the two image domains would be unique or one-to-one. In this paper, we introduce a novel approach to unpaired image-to-image translation based on the invertible architecture. The invertible property of the flow-based architecture assures a cycle-consistency of image-to-image translation without additional loss functions. We utilize the temporal information between consecutive slices to provide more constraints to the optimization for transforming one domain to another in unpaired volumetric medical images. To capture temporal structures in the medical images, we explore the displacement between the consecutive slices using a deformation field. In our approach, the deformation field is used as a guidance to keep the translated slides realistic and consistent across the translation. The experimental results have shown that the synthesized images using our proposed approach are able to archive a competitive performance in terms of mean squared error, peak signal-to-noise ratio, and structural similarity index when compared with the existing deep learning-based methods on three standard datasets, i.e. HCP, MRBrainS13, and Brats2019.



### NICER: Aesthetic Image Enhancement with Humans in the Loop
- **Arxiv ID**: http://arxiv.org/abs/2012.01778v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.AI, cs.CV, cs.LG, I.2.10; I.4.3; H.1.2
- **Links**: [PDF](http://arxiv.org/pdf/2012.01778v1)
- **Published**: 2020-12-03 09:14:10+00:00
- **Updated**: 2020-12-03 09:14:10+00:00
- **Authors**: Michael Fischer, Konstantin Kobs, Andreas Hotho
- **Comment**: The code can be found at https://github.com/mr-Mojo/NICER
- **Journal**: ACHI 2020, The Thirteenth International Conference on Advances in
  Computer-Human Interactions; 2020; pages 357-362
- **Summary**: Fully- or semi-automatic image enhancement software helps users to increase the visual appeal of photos and does not require in-depth knowledge of manual image editing. However, fully-automatic approaches usually enhance the image in a black-box manner that does not give the user any control over the optimization process, possibly leading to edited images that do not subjectively appeal to the user. Semi-automatic methods mostly allow for controlling which pre-defined editing step is taken, which restricts the users in their creativity and ability to make detailed adjustments, such as brightness or contrast. We argue that incorporating user preferences by guiding an automated enhancement method simplifies image editing and increases the enhancement's focus on the user. This work thus proposes the Neural Image Correction & Enhancement Routine (NICER), a neural network based approach to no-reference image enhancement in a fully-, semi-automatic or fully manual process that is interactive and user-centered. NICER iteratively adjusts image editing parameters in order to maximize an aesthetic score based on image style and content. Users can modify these parameters at any time and guide the optimization process towards a desired direction. This interactive workflow is a novelty in the field of human-computer interaction for image enhancement tasks. In a user study, we show that NICER can improve image aesthetics without user interaction and that allowing user interaction leads to diverse enhancement outcomes that are strongly preferred over the unedited image. We make our code publicly available to facilitate further research in this direction.



### Attributes Aware Face Generation with Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2012.01782v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.01782v1)
- **Published**: 2020-12-03 09:25:50+00:00
- **Updated**: 2020-12-03 09:25:50+00:00
- **Authors**: Zheng Yuan, Jie Zhang, Shiguang Shan, Xilin Chen
- **Comment**: 8 pages, 5 figures, 3 tables. Accepted by ICPR2020
- **Journal**: None
- **Summary**: Recent studies have shown remarkable success in face image generations. However, most of the existing methods only generate face images from random noise, and cannot generate face images according to the specific attributes. In this paper, we focus on the problem of face synthesis from attributes, which aims at generating faces with specific characteristics corresponding to the given attributes. To this end, we propose a novel attributes aware face image generator method with generative adversarial networks called AFGAN. Specifically, we firstly propose a two-path embedding layer and self-attention mechanism to convert binary attribute vector to rich attribute features. Then three stacked generators generate $64 \times 64$, $128 \times 128$ and $256 \times 256$ resolution face images respectively by taking the attribute features as input. In addition, an image-attribute matching loss is proposed to enhance the correlation between the generated images and input attributes. Extensive experiments on CelebA demonstrate the superiority of our AFGAN in terms of both qualitative and quantitative evaluations.



### SB-MTL: Score-based Meta Transfer-Learning for Cross-Domain Few-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2012.01784v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2012.01784v1)
- **Published**: 2020-12-03 09:29:35+00:00
- **Updated**: 2020-12-03 09:29:35+00:00
- **Authors**: John Cai, Bill Cai, Sheng Mei Shen
- **Comment**: None
- **Journal**: None
- **Summary**: While many deep learning methods have seen significant success in tackling the problem of domain adaptation and few-shot learning separately, far fewer methods are able to jointly tackle both problems in Cross-Domain Few-Shot Learning (CD-FSL). This problem is exacerbated under sharp domain shifts that typify common computer vision applications. In this paper, we present a novel, flexible and effective method to address the CD-FSL problem. Our method, called Score-based Meta Transfer-Learning (SB-MTL), combines transfer-learning and meta-learning by using a MAML-optimized feature encoder and a score-based Graph Neural Network. First, we have a feature encoder with specific layers designed to be fine-tuned. To do so, we apply a first-order MAML algorithm to find good initializations. Second, instead of directly taking the classification scores after fine-tuning, we interpret the scores as coordinates by mapping the pre-softmax classification scores onto a metric space. Subsequently, we apply a Graph Neural Network to propagate label information from the support set to the query set in our score-based metric space. We test our model on the Broader Study of Cross-Domain Few-Shot Learning (BSCD-FSL) benchmark, which includes a range of target domains with highly varying dissimilarity to the miniImagenet source domain. We observe significant improvements in accuracy across 5, 20 and 50 shot, and on the four target domains. In terms of average accuracy, our model outperforms previous transfer-learning methods by 5.93% and previous meta-learning methods by 14.28%.



### Object SLAM-Based Active Mapping and Robotic Grasping
- **Arxiv ID**: http://arxiv.org/abs/2012.01788v3
- **DOI**: 10.1109/3DV53792.2021.00144
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2012.01788v3)
- **Published**: 2020-12-03 09:36:55+00:00
- **Updated**: 2021-10-16 10:30:37+00:00
- **Authors**: Yanmin Wu, Yunzhou Zhang, Delong Zhu, Xin Chen, Sonya Coleman, Wenkai Sun, Xinggang Hu, Zhiqiang Deng
- **Comment**: Accepted for IEEE International Conference on 3D Vision (3DV), 2021.
  Project page: https://yanmin-wu.github.io/project/active-mapping/
- **Journal**: 2021 International Conference on 3D Vision (3DV), 2021, pp.
  1372-1381
- **Summary**: This paper presents the first active object mapping framework for complex robotic manipulation and autonomous perception tasks. The framework is built on an object SLAM system integrated with a simultaneous multi-object pose estimation process that is optimized for robotic grasping. Aiming to reduce the observation uncertainty on target objects and increase their pose estimation accuracy, we also design an object-driven exploration strategy to guide the object mapping process, enabling autonomous mapping and high-level perception. Combining the mapping module and the exploration strategy, an accurate object map that is compatible with robotic grasping can be generated. Additionally, quantitative evaluations also indicate that the proposed framework has a very high mapping accuracy. Experiments with manipulation (including object grasping and placement) and augmented reality significantly demonstrate the effectiveness and advantages of our proposed framework.



### Attribute-Guided Adversarial Training for Robustness to Natural Perturbations
- **Arxiv ID**: http://arxiv.org/abs/2012.01806v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2012.01806v3)
- **Published**: 2020-12-03 10:17:30+00:00
- **Updated**: 2021-04-08 03:25:14+00:00
- **Authors**: Tejas Gokhale, Rushil Anirudh, Bhavya Kailkhura, Jayaraman J. Thiagarajan, Chitta Baral, Yezhou Yang
- **Comment**: AAAI 2021. Camera Ready version + Appendix
- **Journal**: None
- **Summary**: While existing work in robust deep learning has focused on small pixel-level norm-based perturbations, this may not account for perturbations encountered in several real-world settings. In many such cases although test data might not be available, broad specifications about the types of perturbations (such as an unknown degree of rotation) may be known. We consider a setup where robustness is expected over an unseen test domain that is not i.i.d. but deviates from the training domain. While this deviation may not be exactly known, its broad characterization is specified a priori, in terms of attributes. We propose an adversarial training approach which learns to generate new samples so as to maximize exposure of the classifier to the attributes-space, without having access to the data from the test domain. Our adversarial training solves a min-max optimization problem, with the inner maximization generating adversarial perturbations, and the outer minimization finding model parameters by optimizing the loss on adversarial perturbations generated from the inner maximization. We demonstrate the applicability of our approach on three types of naturally occurring perturbations -- object-related shifts, geometric transformations, and common image corruptions. Our approach enables deep neural networks to be robust against a wide range of naturally occurring perturbations. We demonstrate the usefulness of the proposed approach by showing the robustness gains of deep neural networks trained using our adversarial training on MNIST, CIFAR-10, and a new variant of the CLEVR dataset.



### D-Unet: A Dual-encoder U-Net for Image Splicing Forgery Detection and Localization
- **Arxiv ID**: http://arxiv.org/abs/2012.01821v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.01821v3)
- **Published**: 2020-12-03 10:54:02+00:00
- **Updated**: 2022-05-22 06:21:38+00:00
- **Authors**: Bo Liu, Ranglei Wu, Xiuli Bi, Bin Xiao, Weisheng Li, Guoyin Wang, Xinbo Gao
- **Comment**: 13 pages, 13 figures
- **Journal**: None
- **Summary**: Recently, many detection methods based on convolutional neural networks (CNNs) have been proposed for image splicing forgery detection. Most of these detection methods focus on the local patches or local objects. In fact, image splicing forgery detection is a global binary classification task that distinguishes the tampered and non-tampered regions by image fingerprints. However, some specific image contents are hardly retained by CNN-based detection networks, but if included, would improve the detection accuracy of the networks. To resolve these issues, we propose a novel network called dual-encoder U-Net (D-Unet) for image splicing forgery detection, which employs an unfixed encoder and a fixed encoder. The unfixed encoder autonomously learns the image fingerprints that differentiate between the tampered and non-tampered regions, whereas the fixed encoder intentionally provides the direction information that assists the learning and detection of the network. This dual-encoder is followed by a spatial pyramid global-feature extraction module that expands the global insight of D-Unet for classifying the tampered and non-tampered regions more accurately. In an experimental comparison study of D-Unet and state-of-the-art methods, D-Unet outperformed the other methods in image-level and pixel-level detection, without requiring pre-training or training on a large number of forgery images. Moreover, it was stably robust to different attacks.



### SMDS-Net: Model Guided Spectral-Spatial Network for Hyperspectral Image Denoising
- **Arxiv ID**: http://arxiv.org/abs/2012.01829v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2012.01829v2)
- **Published**: 2020-12-03 11:05:01+00:00
- **Updated**: 2021-11-15 02:56:37+00:00
- **Authors**: Fengchao Xiong, Shuyin Tao, Jun Zhou, Jianfeng Lu, Jiantao Zhou, Yuntao Qian
- **Comment**: The experimental settings have been updated
- **Journal**: None
- **Summary**: Deep learning (DL) based hyperspectral images (HSIs) denoising approaches directly learn the nonlinear mapping between observed noisy images and underlying clean images. They normally do not consider the physical characteristics of HSIs, therefore making them lack of interpretability that is key to understand their denoising mechanism.. In order to tackle this problem, we introduce a novel model guided interpretable network for HSI denoising. Specifically, fully considering the spatial redundancy, spectral low-rankness and spectral-spatial properties of HSIs, we first establish a subspace based multi-dimensional sparse model. This model first projects the observed HSIs into a low-dimensional orthogonal subspace, and then represents the projected image with a multidimensional dictionary. After that, the model is unfolded into an end-to-end network named SMDS-Net whose fundamental modules are seamlessly connected with the denoising procedure and optimization of the model. This makes SMDS-Net convey clear physical meanings, i.e., learning the low-rankness and sparsity of HSIs. Finally, all key variables including dictionaries and thresholding parameters are obtained by the end-to-end training. Extensive experiments and comprehensive analysis confirm the denoising ability and interpretability of our method against the state-of-the-art HSI denoising methods.



### Image inpainting using frequency domain priors
- **Arxiv ID**: http://arxiv.org/abs/2012.01832v1
- **DOI**: 10.1117/1.JEI.30.2.023016
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2012.01832v1)
- **Published**: 2020-12-03 11:08:13+00:00
- **Updated**: 2020-12-03 11:08:13+00:00
- **Authors**: Hiya Roy, Subhajit Chaudhury, Toshihiko Yamasaki, Tatsuaki Hashimoto
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present a novel image inpainting technique using frequency domain information. Prior works on image inpainting predict the missing pixels by training neural networks using only the spatial domain information. However, these methods still struggle to reconstruct high-frequency details for real complex scenes, leading to a discrepancy in color, boundary artifacts, distorted patterns, and blurry textures. To alleviate these problems, we investigate if it is possible to obtain better performance by training the networks using frequency domain information (Discrete Fourier Transform) along with the spatial domain information. To this end, we propose a frequency-based deconvolution module that enables the network to learn the global context while selectively reconstructing the high-frequency components. We evaluate our proposed method on the publicly available datasets CelebA, Paris Streetview, and DTD texture dataset, and show that our method outperforms current state-of-the-art image inpainting techniques both qualitatively and quantitatively.



### Make One-Shot Video Object Segmentation Efficient Again
- **Arxiv ID**: http://arxiv.org/abs/2012.01866v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2012.01866v1)
- **Published**: 2020-12-03 12:21:23+00:00
- **Updated**: 2020-12-03 12:21:23+00:00
- **Authors**: Tim Meinhardt, Laura Leal-Taixe
- **Comment**: None
- **Journal**: None
- **Summary**: Video object segmentation (VOS) describes the task of segmenting a set of objects in each frame of a video. In the semi-supervised setting, the first mask of each object is provided at test time. Following the one-shot principle, fine-tuning VOS methods train a segmentation model separately on each given object mask. However, recently the VOS community has deemed such a test time optimization and its impact on the test runtime as unfeasible. To mitigate the inefficiencies of previous fine-tuning approaches, we present efficient One-Shot Video Object Segmentation (e-OSVOS). In contrast to most VOS approaches, e-OSVOS decouples the object detection task and predicts only local segmentation masks by applying a modified version of Mask R-CNN. The one-shot test runtime and performance are optimized without a laborious and handcrafted hyperparameter search. To this end, we meta learn the model initialization and learning rates for the test time optimization. To achieve optimal learning behavior, we predict individual learning rates at a neuron level. Furthermore, we apply an online adaptation to address the common performance degradation throughout a sequence by continuously fine-tuning the model on previous mask predictions supported by a frame-to-frame bounding box propagation. e-OSVOS provides state-of-the-art results on DAVIS 2016, DAVIS 2017, and YouTube-VOS for one-shot fine-tuning methods while reducing the test runtime substantially.   Code is available at https://github.com/dvl-tum/e-osvos.



### How to Exploit the Transferability of Learned Image Compression to Conventional Codecs
- **Arxiv ID**: http://arxiv.org/abs/2012.01874v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2012.01874v2)
- **Published**: 2020-12-03 12:34:51+00:00
- **Updated**: 2021-03-07 03:44:13+00:00
- **Authors**: Jan P. Klopp, Keng-Chi Liu, Liang-Gee Chen, Shao-Yi Chien
- **Comment**: 10 pages, 5 figures
- **Journal**: None
- **Summary**: Lossy image compression is often limited by the simplicity of the chosen loss measure. Recent research suggests that generative adversarial networks have the ability to overcome this limitation and serve as a multi-modal loss, especially for textures. Together with learned image compression, these two techniques can be used to great effect when relaxing the commonly employed tight measures of distortion. However, convolutional neural network based algorithms have a large computational footprint. Ideally, an existing conventional codec should stay in place, which would ensure faster adoption and adhering to a balanced computational envelope.   As a possible avenue to this goal, in this work, we propose and investigate how learned image coding can be used as a surrogate to optimize an image for encoding. The image is altered by a learned filter to optimise for a different performance measure or a particular task. Extending this idea with a generative adversarial network, we show how entire textures are replaced by ones that are less costly to encode but preserve sense of detail.   Our approach can remodel a conventional codec to adjust for the MS-SSIM distortion with over 20% rate improvement without any decoding overhead. On task-aware image compression, we perform favourably against a similar but codec-specific approach.



### Learning Two-Stream CNN for Multi-Modal Age-related Macular Degeneration Categorization
- **Arxiv ID**: http://arxiv.org/abs/2012.01879v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2012.01879v2)
- **Published**: 2020-12-03 12:50:36+00:00
- **Updated**: 2022-05-04 04:48:56+00:00
- **Authors**: Weisen Wang, Xirong Li, Zhiyan Xu, Weihong Yu, Jianchun Zhao, Dayong Ding, Youxin Chen
- **Comment**: Accepted by IEEE Journal of Biomedical and Health Informatics (J-BHI)
- **Journal**: None
- **Summary**: This paper tackles automated categorization of Age-related Macular Degeneration (AMD), a common macular disease among people over 50. Previous research efforts mainly focus on AMD categorization with a single-modal input, let it be a color fundus photograph (CFP) or an OCT B-scan image. By contrast, we consider AMD categorization given a multi-modal input, a direction that is clinically meaningful yet mostly unexplored. Contrary to the prior art that takes a traditional approach of feature extraction plus classifier training that cannot be jointly optimized, we opt for end-to-end multi-modal Convolutional Neural Networks (MM-CNN). Our MM-CNN is instantiated by a two-stream CNN, with spatially-invariant fusion to combine information from the CFP and OCT streams. In order to visually interpret the contribution of the individual modalities to the final prediction, we extend the class activation mapping (CAM) technique to the multi-modal scenario. For effective training of MM-CNN, we develop two data augmentation methods. One is GAN-based CFP/OCT image synthesis, with our novel use of CAMs as conditional input of a high-resolution image-to-image translation GAN. The other method is Loose Pairing, which pairs a CFP image and an OCT image on the basis of their classes instead of eye identities. Experiments on a clinical dataset consisting of 1,094 CFP images and 1,289 OCT images acquired from 1,093 distinct eyes show that the proposed solution obtains better F1 and Accuracy than multiple baselines for multi-modal AMD categorization. Code and data are available at https://github.com/li-xirong/mmc-amd.



### Temporal Pyramid Network for Pedestrian Trajectory Prediction with Multi-Supervision
- **Arxiv ID**: http://arxiv.org/abs/2012.01884v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.01884v2)
- **Published**: 2020-12-03 13:02:59+00:00
- **Updated**: 2020-12-04 02:11:49+00:00
- **Authors**: Rongqin Liang, Yuanman Li, Xia Li, yi tang, Jiantao Zhou, Wenbin Zou
- **Comment**: 9 pages, 5 figures
- **Journal**: None
- **Summary**: Predicting human motion behavior in a crowd is important for many applications, ranging from the natural navigation of autonomous vehicles to intelligent security systems of video surveillance. All the previous works model and predict the trajectory with a single resolution, which is rather inefficient and difficult to simultaneously exploit the long-range information (e.g., the destination of the trajectory), and the short-range information (e.g., the walking direction and speed at a certain time) of the motion behavior. In this paper, we propose a temporal pyramid network for pedestrian trajectory prediction through a squeeze modulation and a dilation modulation. Our hierarchical framework builds a feature pyramid with increasingly richer temporal information from top to bottom, which can better capture the motion behavior at various tempos. Furthermore, we propose a coarse-to-fine fusion strategy with multi-supervision. By progressively merging the top coarse features of global context to the bottom fine features of rich local context, our method can fully exploit both the long-range and short-range information of the trajectory. Experimental results on several benchmarks demonstrate the superiority of our method.



### Patch2Pix: Epipolar-Guided Pixel-Level Correspondences
- **Arxiv ID**: http://arxiv.org/abs/2012.01909v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.01909v3)
- **Published**: 2020-12-03 13:44:02+00:00
- **Updated**: 2021-03-26 19:54:41+00:00
- **Authors**: Qunjie Zhou, Torsten Sattler, Laura Leal-Taixe
- **Comment**: CVPR2021 Camera Ready Version
- **Journal**: None
- **Summary**: The classical matching pipeline used for visual localization typically involves three steps: (i) local feature detection and description, (ii) feature matching, and (iii) outlier rejection. Recently emerged correspondence networks propose to perform those steps inside a single network but suffer from low matching resolution due to the memory bottleneck. In this work, we propose a new perspective to estimate correspondences in a detect-to-refine manner, where we first predict patch-level match proposals and then refine them. We present Patch2Pix, a novel refinement network that refines match proposals by regressing pixel-level matches from the local regions defined by those proposals and jointly rejecting outlier matches with confidence scores. Patch2Pix is weakly supervised to learn correspondences that are consistent with the epipolar geometry of an input image pair. We show that our refinement network significantly improves the performance of correspondence networks on image matching, homography estimation, and localization tasks. In addition, we show that our learned refinement generalizes to fully-supervised methods without re-training, which leads us to state-of-the-art localization performance. The code is available at https://github.com/GrumpyZhou/patch2pix.



### Multi-mode Core Tensor Factorization based Low-Rankness and Its Applications to Tensor Completion
- **Arxiv ID**: http://arxiv.org/abs/2012.01918v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.IT, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/2012.01918v3)
- **Published**: 2020-12-03 13:57:00+00:00
- **Updated**: 2021-12-14 12:26:20+00:00
- **Authors**: Haijin Zeng
- **Comment**: None
- **Journal**: None
- **Summary**: Low-rank tensor completion has been widely used in computer vision and machine learning. This paper develops a novel multi-modal core tensor factorization (MCTF) method combined with a tensor low-rankness measure and a better nonconvex relaxation form of this measure (NC-MCTF). The proposed models encode low-rank insights for general tensors provided by Tucker and T-SVD, and thus are expected to simultaneously model spectral low-rankness in multiple orientations and accurately restore the data of intrinsic low-rank structure based on few observed entries. Furthermore, we study the MCTF and NC-MCTF regularization minimization problem, and design an effective block successive upper-bound minimization (BSUM) algorithm to solve them. This efficient solver can extend MCTF to various tasks, such as tensor completion. A series of experiments, including hyperspectral image (HSI), video and MRI completion, confirm the superior performance of the proposed method.



### Multi-Label Contrastive Learning for Abstract Visual Reasoning
- **Arxiv ID**: http://arxiv.org/abs/2012.01944v1
- **DOI**: 10.1109/TNNLS.2022.3185949
- **Categories**: **cs.AI**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2012.01944v1)
- **Published**: 2020-12-03 14:18:15+00:00
- **Updated**: 2020-12-03 14:18:15+00:00
- **Authors**: Mikołaj Małkiński, Jacek Mańdziuk
- **Comment**: None
- **Journal**: IEEE Transactions on Neural Networks and Learning Systems, 2022,
  Open Access, published 6 July 2022
- **Summary**: For a long time the ability to solve abstract reasoning tasks was considered one of the hallmarks of human intelligence. Recent advances in application of deep learning (DL) methods led, as in many other domains, to surpassing human abstract reasoning performance, specifically in the most popular type of such problems - the Raven's Progressive Matrices (RPMs). While the efficacy of DL systems is indeed impressive, the way they approach the RPMs is very different from that of humans. State-of-the-art systems solving RPMs rely on massive pattern-based training and sometimes on exploiting biases in the dataset, whereas humans concentrate on identification of the rules / concepts underlying the RPM (or generally a visual reasoning task) to be solved. Motivated by this cognitive difference, this work aims at combining DL with human way of solving RPMs and getting the best of both worlds. Specifically, we cast the problem of solving RPMs into multi-label classification framework where each RPM is viewed as a multi-label data point, with labels determined by the set of abstract rules underlying the RPM. For efficient training of the system we introduce a generalisation of the Noise Contrastive Estimation algorithm to the case of multi-label samples. Furthermore, we propose a new sparse rule encoding scheme for RPMs which, besides the new training algorithm, is the key factor contributing to the state-of-the-art performance. The proposed approach is evaluated on two most popular benchmark datasets (Balanced-RAVEN and PGM) and on both of them demonstrates an advantage over the current state-of-the-art results. Contrary to applications of contrastive learning methods reported in other domains, the state-of-the-art performance reported in the paper is achieved with no need for large batch sizes or strong data augmentation.



### Co-mining: Self-Supervised Learning for Sparsely Annotated Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2012.01950v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.01950v2)
- **Published**: 2020-12-03 14:23:43+00:00
- **Updated**: 2021-10-14 10:55:41+00:00
- **Authors**: Tiancai Wang, Tong Yang, Jiale Cao, Xiangyu Zhang
- **Comment**: Accepted to AAAI 2021. Code is available at
  https://github.com/megvii-research/Co-mining
- **Journal**: None
- **Summary**: Object detectors usually achieve promising results with the supervision of complete instance annotations. However, their performance is far from satisfactory with sparse instance annotations. Most existing methods for sparsely annotated object detection either re-weight the loss of hard negative samples or convert the unlabeled instances into ignored regions to reduce the interference of false negatives. We argue that these strategies are insufficient since they can at most alleviate the negative effect caused by missing annotations. In this paper, we propose a simple but effective mechanism, called Co-mining, for sparsely annotated object detection. In our Co-mining, two branches of a Siamese network predict the pseudo-label sets for each other. To enhance multi-view learning and better mine unlabeled instances, the original image and corresponding augmented image are used as the inputs of two branches of the Siamese network, respectively. Co-mining can serve as a general training mechanism applied to most of modern object detectors. Experiments are performed on MS COCO dataset with three different sparsely annotated settings using two typical frameworks: anchor-based detector RetinaNet and anchor-free detector FCOS. Experimental results show that our Co-mining with RetinaNet achieves 1.4%~2.1% improvements compared with different baselines and surpasses existing methods under the same sparsely annotated setting. Code is available at https://github.com/megvii-research/Co-mining.



### IMAGO: A family photo album dataset for a socio-historical analysis of the twentieth century
- **Arxiv ID**: http://arxiv.org/abs/2012.01955v1
- **DOI**: 10.1145/3507918
- **Categories**: **cs.CV**, cs.CY, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2012.01955v1)
- **Published**: 2020-12-03 14:28:58+00:00
- **Updated**: 2020-12-03 14:28:58+00:00
- **Authors**: Lorenzo Stacchio, Alessia Angeli, Giuseppe Lisanti, Daniela Calanca, Gustavo Marfia
- **Comment**: None
- **Journal**: None
- **Summary**: Although one of the most popular practices in photography since the end of the 19th century, an increase in scholarly interest in family photo albums dates back to the early 1980s. Such collections of photos may reveal sociological and historical insights regarding specific cultures and times. They are, however, in most cases scattered among private homes and only available on paper or photographic film, thus making their analysis by academics such as historians, social-cultural anthropologists and cultural theorists very cumbersome. In this paper, we analyze the IMAGO dataset including photos belonging to family albums assembled at the University of Bologna's Rimini campus since 2004. Following a deep learning-based approach, the IMAGO dataset has offered the opportunity of experimenting with photos taken between year 1845 and year 2009, with the goals of assessing the dates and the socio-historical contexts of the images, without use of any other sources of information. Exceeding our initial expectations, such analysis has revealed its merit not only in terms of the performance of the approach adopted in this work, but also in terms of the foreseeable implications and use for the benefit of socio-historical research. To the best of our knowledge, this is the first work that moves along this path in literature.



### A small note on variation in segmentation annotations
- **Arxiv ID**: http://arxiv.org/abs/2012.01975v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2012.01975v1)
- **Published**: 2020-12-03 14:55:23+00:00
- **Updated**: 2020-12-03 14:55:23+00:00
- **Authors**: Silas Nyboe Ørting
- **Comment**: None
- **Journal**: None
- **Summary**: We report on the results of a small crowdsourcing experiment conducted at a workshop on machine learning for segmentation held at the Danish Bio Imaging network meeting 2020. During the workshop we asked participants to manually segment mitochondria in three 2D patches. The aim of the experiment was to illustrate that manual annotations should not be seen as the ground truth, but as a reference standard that is subject to substantial variation. In this note we show how the large variation we observed in the segmentations can be reduced by removing the annotators with worst pair-wise agreement. Having removed the annotators with worst performance, we illustrate that the remaining variance is semantically meaningful and can be exploited to obtain segmentations of cell boundary and cell interior.



### Wisdom of Committees: An Overlooked Approach To Faster and More Accurate Models
- **Arxiv ID**: http://arxiv.org/abs/2012.01988v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.01988v6)
- **Published**: 2020-12-03 14:59:16+00:00
- **Updated**: 2022-02-17 17:29:51+00:00
- **Authors**: Xiaofang Wang, Dan Kondratyuk, Eric Christiansen, Kris M. Kitani, Yair Alon, Elad Eban
- **Comment**: ICLR 2022
- **Journal**: None
- **Summary**: Committee-based models (ensembles or cascades) construct models by combining existing pre-trained ones. While ensembles and cascades are well-known techniques that were proposed before deep learning, they are not considered a core building block of deep model architectures and are rarely compared to in recent literature on developing efficient models. In this work, we go back to basics and conduct a comprehensive analysis of the efficiency of committee-based models. We find that even the most simplistic method for building committees from existing, independently pre-trained models can match or exceed the accuracy of state-of-the-art models while being drastically more efficient. These simple committee-based models also outperform sophisticated neural architecture search methods (e.g., BigNAS). These findings hold true for several tasks, including image classification, video classification, and semantic segmentation, and various architecture families, such as ViT, EfficientNet, ResNet, MobileNetV2, and X3D. Our results show that an EfficientNet cascade can achieve a 5.4x speedup over B7 and a ViT cascade can achieve a 2.3x speedup over ViT-L-384 while being equally accurate.



### Localization of Malaria Parasites and White Blood Cells in Thick Blood Smears
- **Arxiv ID**: http://arxiv.org/abs/2012.01994v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, I.4.9; I.2.1
- **Links**: [PDF](http://arxiv.org/pdf/2012.01994v1)
- **Published**: 2020-12-03 15:14:38+00:00
- **Updated**: 2020-12-03 15:14:38+00:00
- **Authors**: Rose Nakasi, Aminah Zawedde, Ernest Mwebaze, Jeremy Francis Tusubira, Gilbert Maiga
- **Comment**: 6 pages, 3 figures,2020 NeurIPS ML4D workshop
- **Journal**: None
- **Summary**: Effectively determining malaria parasitemia is a critical aspect in assisting clinicians to accurately determine the severity of the disease and provide quality treatment. Microscopy applied to thick smear blood smears is the de facto method for malaria parasitemia determination. However, manual quantification of parasitemia is time consuming, laborious and requires considerable trained expertise which is particularly inadequate in highly endemic and low resourced areas. This study presents an end-to-end approach for localisation and count of malaria parasites and white blood cells (WBCs) which aid in the effective determination of parasitemia; the quantitative content of parasites in the blood. On a dataset of slices of images of thick blood smears, we build models to analyse the obtained digital images. To improve model performance due to the limited size of the dataset, data augmentation was applied. Our preliminary results show that our deep learning approach reliably detects and returns a count of malaria parasites and WBCs with a high precision and recall. We also evaluate our system against human experts and results indicate a strong correlation between our deep learning model counts and the manual expert counts (p=0.998 for parasites, p=0.987 for WBCs). This approach could potentially be applied to support malaria parasitemia determination especially in settings that lack sufficient Microscopists.



### Aerial Imagery Pixel-level Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2012.02024v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4.6
- **Links**: [PDF](http://arxiv.org/pdf/2012.02024v1)
- **Published**: 2020-12-03 16:09:09+00:00
- **Updated**: 2020-12-03 16:09:09+00:00
- **Authors**: Michael R. Heffels, Joaquin Vanschoren
- **Comment**: 30 pages, 15 figures, 4 tables. Code available through GitHub repo at
  https://github.com/mrheffels/aerial-imagery-segmentation
- **Journal**: None
- **Summary**: Aerial imagery can be used for important work on a global scale. Nevertheless, the analysis of this data using neural network architectures lags behind the current state-of-the-art on popular datasets such as PASCAL VOC, CityScapes and Camvid. In this paper we bridge the performance-gap between these popular datasets and aerial imagery data. Little work is done on aerial imagery with state-of-the-art neural network architectures in a multi-class setting. Our experiments concerning data augmentation, normalisation, image size and loss functions give insight into a high performance setup for aerial imagery segmentation datasets. Our work, using the state-of-the-art DeepLabv3+ Xception65 architecture, achieves a mean IOU of 70% on the DroneDeploy validation set. With this result, we clearly outperform the current publicly available state-of-the-art validation set mIOU (65%) performance with 5%. Furthermore, to our knowledge, there is no mIOU benchmark for the test set. Hence, we also propose a new benchmark on the DroneDeploy test set using the best performing DeepLabv3+ Xception65 architecture, with a mIOU score of 52.5%.



### Recovering Trajectories of Unmarked Joints in 3D Human Actions Using Latent Space Optimization
- **Arxiv ID**: http://arxiv.org/abs/2012.02043v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2012.02043v1)
- **Published**: 2020-12-03 16:25:07+00:00
- **Updated**: 2020-12-03 16:25:07+00:00
- **Authors**: Suhas Lohit, Rushil Anirudh, Pavan Turaga
- **Comment**: Accepted at WACV 2021
- **Journal**: None
- **Summary**: Motion capture (mocap) and time-of-flight based sensing of human actions are becoming increasingly popular modalities to perform robust activity analysis. Applications range from action recognition to quantifying movement quality for health applications. While marker-less motion capture has made great progress, in critical applications such as healthcare, marker-based systems, especially active markers, are still considered gold-standard. However, there are several practical challenges in both modalities such as visibility, tracking errors, and simply the need to keep marker setup convenient wherein movements are recorded with a reduced marker-set. This implies that certain joint locations will not even be marked-up, making downstream analysis of full body movement challenging. To address this gap, we first pose the problem of reconstructing the unmarked joint data as an ill-posed linear inverse problem. We recover missing joints for a given action by projecting it onto the manifold of human actions, this is achieved by optimizing the latent space representation of a deep autoencoder. Experiments on both mocap and Kinect datasets clearly demonstrate that the proposed method performs very well in recovering semantics of the actions and dynamics of missing joints. We will release all the code and models publicly.



### Neural Prototype Trees for Interpretable Fine-grained Image Recognition
- **Arxiv ID**: http://arxiv.org/abs/2012.02046v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2012.02046v2)
- **Published**: 2020-12-03 16:28:04+00:00
- **Updated**: 2021-04-15 08:49:31+00:00
- **Authors**: Meike Nauta, Ron van Bree, Christin Seifert
- **Comment**: Accepted to IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR) 2021. 11 pages, and 9 pages supplementary
- **Journal**: None
- **Summary**: Prototype-based methods use interpretable representations to address the black-box nature of deep learning models, in contrast to post-hoc explanation methods that only approximate such models. We propose the Neural Prototype Tree (ProtoTree), an intrinsically interpretable deep learning method for fine-grained image recognition. ProtoTree combines prototype learning with decision trees, and thus results in a globally interpretable model by design. Additionally, ProtoTree can locally explain a single prediction by outlining a decision path through the tree. Each node in our binary tree contains a trainable prototypical part. The presence or absence of this learned prototype in an image determines the routing through a node. Decision making is therefore similar to human reasoning: Does the bird have a red throat? And an elongated beak? Then it's a hummingbird! We tune the accuracy-interpretability trade-off using ensemble methods, pruning and binarizing. We apply pruning without sacrificing accuracy, resulting in a small tree with only 8 learned prototypes along a path to classify a bird from 200 species. An ensemble of 5 ProtoTrees achieves competitive accuracy on the CUB-200- 2011 and Stanford Cars data sets. Code is available at https://github.com/M-Nauta/ProtoTree



### CoCosNet v2: Full-Resolution Correspondence Learning for Image Translation
- **Arxiv ID**: http://arxiv.org/abs/2012.02047v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.02047v2)
- **Published**: 2020-12-03 16:28:05+00:00
- **Updated**: 2021-03-30 07:50:44+00:00
- **Authors**: Xingran Zhou, Bo Zhang, Ting Zhang, Pan Zhang, Jianmin Bao, Dong Chen, Zhongfei Zhang, Fang Wen
- **Comment**: CVPR 2021 oral presentation
- **Journal**: None
- **Summary**: We present the full-resolution correspondence learning for cross-domain images, which aids image translation. We adopt a hierarchical strategy that uses the correspondence from coarse level to guide the fine levels. At each hierarchy, the correspondence can be efficiently computed via PatchMatch that iteratively leverages the matchings from the neighborhood. Within each PatchMatch iteration, the ConvGRU module is employed to refine the current correspondence considering not only the matchings of larger context but also the historic estimates. The proposed CoCosNet v2, a GRU-assisted PatchMatch approach, is fully differentiable and highly efficient. When jointly trained with image translation, full-resolution semantic correspondence can be established in an unsupervised manner, which in turn facilitates the exemplar-based image translation. Experiments on diverse translation tasks show that CoCosNet v2 performs considerably better than state-of-the-art literature on producing high-resolution images.



### A Multi-task Contextual Atrous Residual Network for Brain Tumor Detection & Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2012.02073v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.02073v1)
- **Published**: 2020-12-03 17:04:16+00:00
- **Updated**: 2020-12-03 17:04:16+00:00
- **Authors**: Ngan Le, Kashu Yamazaki, Dat Truong, Kha Gia Quach, Marios Savvides
- **Comment**: Accepted in ICPR 2020
- **Journal**: None
- **Summary**: In recent years, deep neural networks have achieved state-of-the-art performance in a variety of recognition and segmentation tasks in medical imaging including brain tumor segmentation. We investigate that segmenting a brain tumor is facing to the imbalanced data problem where the number of pixels belonging to the background class (non tumor pixel) is much larger than the number of pixels belonging to the foreground class (tumor pixel). To address this problem, we propose a multi-task network which is formed as a cascaded structure. Our model consists of two targets, i.e., (i) effectively differentiate the brain tumor regions and (ii) estimate the brain tumor mask. The first objective is performed by our proposed contextual brain tumor detection network, which plays a role of an attention gate and focuses on the region around brain tumor only while ignoring the far neighbor background which is less correlated to the tumor. The second objective is built upon a 3D atrous residual network and under an encode-decode network in order to effectively segment both large and small objects (brain tumor). Our 3D atrous residual network is designed with a skip connection to enables the gradient from the deep layers to be directly propagated to shallow layers, thus, features of different depths are preserved and used for refining each other. In order to incorporate larger contextual information from volume MRI data, our network utilizes the 3D atrous convolution with various kernel sizes, which enlarges the receptive field of filters. Our proposed network has been evaluated on various datasets including BRATS2015, BRATS2017 and BRATS2018 datasets with both validation set and testing set. Our performance has been benchmarked by both region-based metrics and surface-based metrics. We also have conducted comparisons against state-of-the-art approaches.



### SSGD: A safe and efficient method of gradient descent
- **Arxiv ID**: http://arxiv.org/abs/2012.02076v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CR, cs.CV, cs.DC
- **Links**: [PDF](http://arxiv.org/pdf/2012.02076v2)
- **Published**: 2020-12-03 17:09:20+00:00
- **Updated**: 2021-04-26 04:33:08+00:00
- **Authors**: Jinhuan Duan, Xianxian Li, Shiqi Gao, Jinyan Wang, Zili Zhong
- **Comment**: None
- **Journal**: None
- **Summary**: With the vigorous development of artificial intelligence technology, various engineering technology applications have been implemented one after another. The gradient descent method plays an important role in solving various optimization problems, due to its simple structure, good stability and easy implementation. In multi-node machine learning system, the gradients usually need to be shared. Shared gradients are generally unsafe. Attackers can obtain training data simply by knowing the gradient information. In this paper, to prevent gradient leakage while keeping the accuracy of model, we propose the super stochastic gradient descent approach to update parameters by concealing the modulus length of gradient vectors and converting it or them into a unit vector. Furthermore, we analyze the security of super stochastic gradient descent approach. Our algorithm can defend against attacks on the gradient. Experiment results show that our approach is obviously superior to prevalent gradient descent approaches in terms of accuracy, robustness, and adaptability to large-scale batches.



### Towards Part-Based Understanding of RGB-D Scans
- **Arxiv ID**: http://arxiv.org/abs/2012.02094v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.02094v1)
- **Published**: 2020-12-03 17:30:02+00:00
- **Updated**: 2020-12-03 17:30:02+00:00
- **Authors**: Alexey Bokhovkin, Vladislav Ishimtsev, Emil Bogomolov, Denis Zorin, Alexey Artemov, Evgeny Burnaev, Angela Dai
- **Comment**: https://youtu.be/iuixmPNs4v4
- **Journal**: None
- **Summary**: Recent advances in 3D semantic scene understanding have shown impressive progress in 3D instance segmentation, enabling object-level reasoning about 3D scenes; however, a finer-grained understanding is required to enable interactions with objects and their functional understanding. Thus, we propose the task of part-based scene understanding of real-world 3D environments: from an RGB-D scan of a scene, we detect objects, and for each object predict its decomposition into geometric part masks, which composed together form the complete geometry of the observed object. We leverage an intermediary part graph representation to enable robust completion as well as building of part priors, which we use to construct the final part mask predictions. Our experiments demonstrate that guiding part understanding through part graph to part prior-based predictions significantly outperforms alternative approaches to the task of semantic part completion.



### Robust Instance Segmentation through Reasoning about Multi-Object Occlusion
- **Arxiv ID**: http://arxiv.org/abs/2012.02107v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.02107v3)
- **Published**: 2020-12-03 17:41:55+00:00
- **Updated**: 2021-04-01 13:32:03+00:00
- **Authors**: Xiaoding Yuan, Adam Kortylewski, Yihong Sun, Alan Yuille
- **Comment**: Accepted by CVPR 2021
- **Journal**: None
- **Summary**: Analyzing complex scenes with Deep Neural Networks is a challenging task, particularly when images contain multiple objects that partially occlude each other. Existing approaches to image analysis mostly process objects independently and do not take into account the relative occlusion of nearby objects. In this paper, we propose a deep network for multi-object instance segmentation that is robust to occlusion and can be trained from bounding box supervision only. Our work builds on Compositional Networks, which learn a generative model of neural feature activations to locate occluders and to classify objects based on their non-occluded parts. We extend their generative model to include multiple objects and introduce a framework for efficient inference in challenging occlusion scenarios. In particular, we obtain feed-forward predictions of the object classes and their instance and occluder segmentations. We introduce an Occlusion Reasoning Module (ORM) that locates erroneous segmentations and estimates the occlusion order to correct them. The improved segmentation masks are, in turn, integrated into the network in a top-down manner to improve the image classification. Our experiments on the KITTI INStance dataset (KINS) and a synthetic occlusion dataset demonstrate the effectiveness and robustness of our model at multi-object instance segmentation under occlusion. Code is publically available at https://github.com/XD7479/Multi-Object-Occlusion.



### SAFCAR: Structured Attention Fusion for Compositional Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2012.02109v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.02109v2)
- **Published**: 2020-12-03 17:45:01+00:00
- **Updated**: 2020-12-17 21:15:37+00:00
- **Authors**: Tae Soo Kim, Gregory D. Hager
- **Comment**: None
- **Journal**: None
- **Summary**: We present a general framework for compositional action recognition -- i.e. action recognition where the labels are composed out of simpler components such as subjects, atomic-actions and objects. The main challenge in compositional action recognition is that there is a combinatorially large set of possible actions that can be composed using basic components. However, compositionality also provides a structure that can be exploited. To do so, we develop and test a novel Structured Attention Fusion (SAF) self-attention mechanism to combine information from object detections, which capture the time-series structure of an action, with visual cues that capture contextual information. We show that our approach recognizes novel verb-noun compositions more effectively than current state of the art systems, and it generalizes to unseen action categories quite efficiently from only a few labeled examples. We validate our approach on the challenging Something-Else tasks from the Something-Something-V2 dataset. We further show that our framework is flexible and can generalize to a new domain by showing competitive results on the Charades-Fewshot dataset.



### Generalized Object Detection on Fisheye Cameras for Autonomous Driving: Dataset, Representations and Baseline
- **Arxiv ID**: http://arxiv.org/abs/2012.02124v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2012.02124v2)
- **Published**: 2020-12-03 18:00:16+00:00
- **Updated**: 2022-12-21 23:10:50+00:00
- **Authors**: Hazem Rashed, Eslam Mohamed, Ganesh Sistu, Varun Ravi Kumar, Ciaran Eising, Ahmad El-Sallab, Senthil Yogamani
- **Comment**: Camera ready version. Accepted for presentation at Winter Conference
  on Applications of Computer Vision 2021. Dataset is shared at
  https://drive.google.com/drive/folders/1bobmY2wlIBozeU5ZgPfYPqVAnpPw4QrM
- **Journal**: None
- **Summary**: Object detection is a comprehensively studied problem in autonomous driving. However, it has been relatively less explored in the case of fisheye cameras. The standard bounding box fails in fisheye cameras due to the strong radial distortion, particularly in the image's periphery. We explore better representations like oriented bounding box, ellipse, and generic polygon for object detection in fisheye images in this work. We use the IoU metric to compare these representations using accurate instance segmentation ground truth. We design a novel curved bounding box model that has optimal properties for fisheye distortion models. We also design a curvature adaptive perimeter sampling method for obtaining polygon vertices, improving relative mAP score by 4.9% compared to uniform sampling. Overall, the proposed polygon model improves mIoU relative accuracy by 40.3%. It is the first detailed study on object detection on fisheye cameras for autonomous driving scenarios to the best of our knowledge. The dataset comprising of 10,000 images along with all the object representations ground truth will be made public to encourage further research. We summarize our work in a short video with qualitative results at https://youtu.be/iLkOzvJpL-A.



### BERT-hLSTMs: BERT and Hierarchical LSTMs for Visual Storytelling
- **Arxiv ID**: http://arxiv.org/abs/2012.02128v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2012.02128v1)
- **Published**: 2020-12-03 18:07:28+00:00
- **Updated**: 2020-12-03 18:07:28+00:00
- **Authors**: Jing Su, Qingyun Dai, Frank Guerin, Mian Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: Visual storytelling is a creative and challenging task, aiming to automatically generate a story-like description for a sequence of images. The descriptions generated by previous visual storytelling approaches lack coherence because they use word-level sequence generation methods and do not adequately consider sentence-level dependencies. To tackle this problem, we propose a novel hierarchical visual storytelling framework which separately models sentence-level and word-level semantics. We use the transformer-based BERT to obtain embeddings for sentences and words. We then employ a hierarchical LSTM network: the bottom LSTM receives as input the sentence vector representation from BERT, to learn the dependencies between the sentences corresponding to images, and the top LSTM is responsible for generating the corresponding word vector representations, taking input from the bottom LSTM. Experimental results demonstrate that our model outperforms most closely related baselines under automatic evaluation metrics BLEU and CIDEr, and also show the effectiveness of our method with human evaluation.



### Graph-SIM: A Graph-based Spatiotemporal Interaction Modelling for Pedestrian Action Prediction
- **Arxiv ID**: http://arxiv.org/abs/2012.02148v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2012.02148v3)
- **Published**: 2020-12-03 18:28:27+00:00
- **Updated**: 2021-03-25 14:12:10+00:00
- **Authors**: Tiffany Yau, Saber Malekmohammadi, Amir Rasouli, Peter Lakner, Mohsen Rohani, Jun Luo
- **Comment**: 7 pages, 3 figures, 4 tables, accepted at ICRA 2021
- **Journal**: None
- **Summary**: One of the most crucial yet challenging tasks for autonomous vehicles in urban environments is predicting the future behaviour of nearby pedestrians, especially at points of crossing. Predicting behaviour depends on many social and environmental factors, particularly interactions between road users. Capturing such interactions requires a global view of the scene and dynamics of the road users in three-dimensional space. This information, however, is missing from the current pedestrian behaviour benchmark datasets. Motivated by these challenges, we propose 1) a novel graph-based model for predicting pedestrian crossing action. Our method models pedestrians' interactions with nearby road users through clustering and relative importance weighting of interactions using features obtained from the bird's-eye-view. 2) We introduce a new dataset that provides 3D bounding box and pedestrian behavioural annotations for the existing nuScenes dataset. On the new data, our approach achieves state-of-the-art performance by improving on various metrics by more than 15% in comparison to existing methods. The dataset is available at https://github.com/huawei-noah/datasets/PePScenes.



### MakeupBag: Disentangling Makeup Extraction and Application
- **Arxiv ID**: http://arxiv.org/abs/2012.02157v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.02157v1)
- **Published**: 2020-12-03 18:44:24+00:00
- **Updated**: 2020-12-03 18:44:24+00:00
- **Authors**: Dokhyam Hoshen
- **Comment**: None
- **Journal**: None
- **Summary**: This paper introduces MakeupBag, a novel method for automatic makeup style transfer. Our proposed technique can transfer a new makeup style from a reference face image to another previously unseen facial photograph. We solve makeup disentanglement and facial makeup application as separable objectives, in contrast to other current deep methods that entangle the two tasks. MakeupBag presents a significant advantage for our approach as it allows customization and pixel specific modification of the extracted makeup style, which is not possible using current methods. Extensive experiments, both qualitative and numerical, are conducted demonstrating the high quality and accuracy of the images produced by our method. Furthermore, in contrast to most other current methods, MakeupBag tackles both classical and extreme and costume makeup transfer. In a comparative analysis, MakeupBag is shown to outperform current state-of-the-art approaches.



### Self-labeled Conditional GANs
- **Arxiv ID**: http://arxiv.org/abs/2012.02162v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.02162v1)
- **Published**: 2020-12-03 18:46:46+00:00
- **Updated**: 2020-12-03 18:46:46+00:00
- **Authors**: Mehdi Noroozi
- **Comment**: None
- **Journal**: None
- **Summary**: This paper introduces a novel and fully unsupervised framework for conditional GAN training in which labels are automatically obtained from data. We incorporate a clustering network into the standard conditional GAN framework that plays against the discriminator. With the generator, it aims to find a shared structured mapping for associating pseudo-labels with the real and fake images. Our generator outperforms unconditional GANs in terms of FID with significant margins on large scale datasets like ImageNet and LSUN. It also outperforms class conditional GANs trained on human labels on CIFAR10 and CIFAR100 where fine-grained annotations or a large number of samples per class are not available. Additionally, our clustering network exceeds the state-of-the-art on CIFAR100 clustering.



### Visualization of Supervised and Self-Supervised Neural Networks via Attribution Guided Factorization
- **Arxiv ID**: http://arxiv.org/abs/2012.02166v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.02166v1)
- **Published**: 2020-12-03 18:48:39+00:00
- **Updated**: 2020-12-03 18:48:39+00:00
- **Authors**: Shir Gur, Ameen Ali, Lior Wolf
- **Comment**: None
- **Journal**: None
- **Summary**: Neural network visualization techniques mark image locations by their relevancy to the network's classification. Existing methods are effective in highlighting the regions that affect the resulting classification the most. However, as we show, these methods are limited in their ability to identify the support for alternative classifications, an effect we name {\em the saliency bias} hypothesis. In this work, we integrate two lines of research: gradient-based methods and attribution-based methods, and develop an algorithm that provides per-class explainability. The algorithm back-projects the per pixel local influence, in a manner that is guided by the local attributions, while correcting for salient features that would otherwise bias the explanation. In an extensive battery of experiments, we demonstrate the ability of our methods to class-specific visualization, and not just the predicted label. Remarkably, the method obtains state of the art results in benchmarks that are commonly applied to gradient-based methods as well as in those that are employed mostly for evaluating attribution methods. Using a new unsupervised procedure, our method is also successful in demonstrating that self-supervised methods learn semantic information.



### Multimodal Spatio-Temporal Deep Learning Approach for Neonatal Postoperative Pain Assessment
- **Arxiv ID**: http://arxiv.org/abs/2012.02175v1
- **DOI**: 10.1016/j.compbiomed.2020.104150
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2012.02175v1)
- **Published**: 2020-12-03 18:52:35+00:00
- **Updated**: 2020-12-03 18:52:35+00:00
- **Authors**: Md Sirajus Salekin, Ghada Zamzmi, Dmitry Goldgof, Rangachar Kasturi, Thao Ho, Yu Sun
- **Comment**: Accepted in Computers in Biology and Medicine, 2020
- **Journal**: None
- **Summary**: The current practice for assessing neonatal postoperative pain relies on bedside caregivers. This practice is subjective, inconsistent, slow, and discontinuous. To develop a reliable medical interpretation, several automated approaches have been proposed to enhance the current practice. These approaches are unimodal and focus mainly on assessing neonatal procedural (acute) pain. As pain is a multimodal emotion that is often expressed through multiple modalities, the multimodal assessment of pain is necessary especially in case of postoperative (acute prolonged) pain. Additionally, spatio-temporal analysis is more stable over time and has been proven to be highly effective at minimizing misclassification errors. In this paper, we present a novel multimodal spatio-temporal approach that integrates visual and vocal signals and uses them for assessing neonatal postoperative pain. We conduct comprehensive experiments to investigate the effectiveness of the proposed approach. We compare the performance of the multimodal and unimodal postoperative pain assessment, and measure the impact of temporal information integration. The experimental results, on a real-world dataset, show that the proposed multimodal spatio-temporal approach achieves the highest AUC (0.87) and accuracy (79%), which are on average 6.67% and 6.33% higher than unimodal approaches. The results also show that the integration of temporal information markedly improves the performance as compared to the non-temporal approach as it captures changes in the pain dynamic. These results demonstrate that the proposed approach can be used as a viable alternative to manual assessment, which would tread a path toward fully automated pain monitoring in clinical settings, point-of-care testing, and homes.



### DeepVideoMVS: Multi-View Stereo on Video with Recurrent Spatio-Temporal Fusion
- **Arxiv ID**: http://arxiv.org/abs/2012.02177v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2012.02177v3)
- **Published**: 2020-12-03 18:54:03+00:00
- **Updated**: 2021-07-21 18:12:44+00:00
- **Authors**: Arda Düzçeker, Silvano Galliani, Christoph Vogel, Pablo Speciale, Mihai Dusmanu, Marc Pollefeys
- **Comment**: CVPR 2021
- **Journal**: None
- **Summary**: We propose an online multi-view depth prediction approach on posed video streams, where the scene geometry information computed in the previous time steps is propagated to the current time step in an efficient and geometrically plausible way. The backbone of our approach is a real-time capable, lightweight encoder-decoder that relies on cost volumes computed from pairs of images. We extend it by placing a ConvLSTM cell at the bottleneck layer, which compresses an arbitrary amount of past information in its states. The novelty lies in propagating the hidden state of the cell by accounting for the viewpoint changes between time steps. At a given time step, we warp the previous hidden state into the current camera plane using the previous depth prediction. Our extension brings only a small overhead of computation time and memory consumption, while improving the depth predictions significantly. As a result, we outperform the existing state-of-the-art multi-view stereo methods on most of the evaluated metrics in hundreds of indoor scenes while maintaining a real-time performance. Code available: https://github.com/ardaduz/deep-video-mvs



### BasicVSR: The Search for Essential Components in Video Super-Resolution and Beyond
- **Arxiv ID**: http://arxiv.org/abs/2012.02181v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.02181v2)
- **Published**: 2020-12-03 18:56:14+00:00
- **Updated**: 2021-04-07 11:23:38+00:00
- **Authors**: Kelvin C. K. Chan, Xintao Wang, Ke Yu, Chao Dong, Chen Change Loy
- **Comment**: CVPR 2021 camera-ready
- **Journal**: None
- **Summary**: Video super-resolution (VSR) approaches tend to have more components than the image counterparts as they need to exploit the additional temporal dimension. Complex designs are not uncommon. In this study, we wish to untangle the knots and reconsider some most essential components for VSR guided by four basic functionalities, i.e., Propagation, Alignment, Aggregation, and Upsampling. By reusing some existing components added with minimal redesigns, we show a succinct pipeline, BasicVSR, that achieves appealing improvements in terms of speed and restoration quality in comparison to many state-of-the-art algorithms. We conduct systematic analysis to explain how such gain can be obtained and discuss the pitfalls. We further show the extensibility of BasicVSR by presenting an information-refill mechanism and a coupled propagation scheme to facilitate information aggregation. The BasicVSR and its extension, IconVSR, can serve as strong baselines for future VSR approaches.



### Learned Initializations for Optimizing Coordinate-Based Neural Representations
- **Arxiv ID**: http://arxiv.org/abs/2012.02189v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.02189v2)
- **Published**: 2020-12-03 18:59:52+00:00
- **Updated**: 2021-03-23 17:11:16+00:00
- **Authors**: Matthew Tancik, Ben Mildenhall, Terrance Wang, Divi Schmidt, Pratul P. Srinivasan, Jonathan T. Barron, Ren Ng
- **Comment**: Project page: https://www.matthewtancik.com/learnit
- **Journal**: None
- **Summary**: Coordinate-based neural representations have shown significant promise as an alternative to discrete, array-based representations for complex low dimensional signals. However, optimizing a coordinate-based network from randomly initialized weights for each new signal is inefficient. We propose applying standard meta-learning algorithms to learn the initial weight parameters for these fully-connected networks based on the underlying class of signals being represented (e.g., images of faces or 3D models of chairs). Despite requiring only a minor change in implementation, using these learned initial weights enables faster convergence during optimization and can serve as a strong prior over the signal class being modeled, resulting in better generalization when only partial observations of a given signal are available. We explore these benefits across a variety of tasks, including representing 2D images, reconstructing CT scans, and recovering 3D shapes and scenes from 2D image observations.



### pixelNeRF: Neural Radiance Fields from One or Few Images
- **Arxiv ID**: http://arxiv.org/abs/2012.02190v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2012.02190v3)
- **Published**: 2020-12-03 18:59:54+00:00
- **Updated**: 2021-05-30 17:52:24+00:00
- **Authors**: Alex Yu, Vickie Ye, Matthew Tancik, Angjoo Kanazawa
- **Comment**: CVPR 2021
- **Journal**: None
- **Summary**: We propose pixelNeRF, a learning framework that predicts a continuous neural scene representation conditioned on one or few input images. The existing approach for constructing neural radiance fields involves optimizing the representation to every scene independently, requiring many calibrated views and significant compute time. We take a step towards resolving these shortcomings by introducing an architecture that conditions a NeRF on image inputs in a fully convolutional manner. This allows the network to be trained across multiple scenes to learn a scene prior, enabling it to perform novel view synthesis in a feed-forward manner from a sparse set of views (as few as one). Leveraging the volume rendering approach of NeRF, our model can be trained directly from images with no explicit 3D supervision. We conduct extensive experiments on ShapeNet benchmarks for single image novel view synthesis tasks with held-out objects as well as entire unseen categories. We further demonstrate the flexibility of pixelNeRF by demonstrating it on multi-object ShapeNet scenes and real scenes from the DTU dataset. In all cases, pixelNeRF outperforms current state-of-the-art baselines for novel view synthesis and single image 3D reconstruction. For the video and code, please visit the project website: https://alexyu.net/pixelnerf



### Scan2Cap: Context-aware Dense Captioning in RGB-D Scans
- **Arxiv ID**: http://arxiv.org/abs/2012.02206v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2012.02206v1)
- **Published**: 2020-12-03 19:00:05+00:00
- **Updated**: 2020-12-03 19:00:05+00:00
- **Authors**: Dave Zhenyu Chen, Ali Gholami, Matthias Nießner, Angel X. Chang
- **Comment**: Video: https://youtu.be/AgmIpDbwTCY
- **Journal**: None
- **Summary**: We introduce the task of dense captioning in 3D scans from commodity RGB-D sensors. As input, we assume a point cloud of a 3D scene; the expected output is the bounding boxes along with the descriptions for the underlying objects. To address the 3D object detection and description problems, we propose Scan2Cap, an end-to-end trained method, to detect objects in the input scene and describe them in natural language. We use an attention mechanism that generates descriptive tokens while referring to the related components in the local context. To reflect object relations (i.e. relative spatial relations) in the generated captions, we use a message passing graph module to facilitate learning object relation features. Our method can effectively localize and describe 3D objects in scenes from the ScanRefer dataset, outperforming 2D baseline methods by a significant margin (27.61% CiDEr@0.5IoUimprovement).



### Traffic Surveillance using Vehicle License Plate Detection and Recognition in Bangladesh
- **Arxiv ID**: http://arxiv.org/abs/2012.02218v1
- **DOI**: 10.1109/ICECE51571.2020.9393109
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.02218v1)
- **Published**: 2020-12-03 19:16:49+00:00
- **Updated**: 2020-12-03 19:16:49+00:00
- **Authors**: Md. Saif Hassan Onim, Muhaiminul Islam Akash, Mahmudul Haque, Raiyan Ibne Hafiz
- **Comment**: None
- **Journal**: None
- **Summary**: Computer vision coupled with Deep Learning (DL) techniques bring out a substantial prospect in the field of traffic control, monitoring and law enforcing activities. This paper presents a YOLOv4 object detection model in which the Convolutional Neural Network (CNN) is trained and tuned for detecting the license plate of the vehicles of Bangladesh and recognizing characters using tesseract from the detected license plates. Here we also present a Graphical User Interface (GUI) based on Tkinter, a python package. The license plate detection model is trained with mean average precision (mAP) of 90.50% and performed in a single TESLA T4 GPU with an average of 14 frames per second (fps) on real time video footage.



### EVRNet: Efficient Video Restoration on Edge Devices
- **Arxiv ID**: http://arxiv.org/abs/2012.02228v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2012.02228v1)
- **Published**: 2020-12-03 19:42:38+00:00
- **Updated**: 2020-12-03 19:42:38+00:00
- **Authors**: Sachin Mehta, Amit Kumar, Fitsum Reda, Varun Nasery, Vikram Mulukutla, Rakesh Ranjan, Vikas Chandra
- **Comment**: Technical report
- **Journal**: None
- **Summary**: Video transmission applications (e.g., conferencing) are gaining momentum, especially in times of global health pandemic. Video signals are transmitted over lossy channels, resulting in low-quality received signals. To restore videos on recipient edge devices in real-time, we introduce an efficient video restoration network, EVRNet. EVRNet efficiently allocates parameters inside the network using alignment, differential, and fusion modules. With extensive experiments on video restoration tasks (deblocking, denoising, and super-resolution), we demonstrate that EVRNet delivers competitive performance to existing methods with significantly fewer parameters and MACs. For example, EVRNet has 260 times fewer parameters and 958 times fewer MACs than enhanced deformable convolution-based video restoration network (EDVR) for 4 times video super-resolution while its SSIM score is 0.018 less than EDVR. We also evaluated the performance of EVRNet under multiple distortions on unseen dataset to demonstrate its ability in modeling variable-length sequences under both camera and object motion.



### COVID-CLNet: COVID-19 Detection with Compressive Deep Learning Approaches
- **Arxiv ID**: http://arxiv.org/abs/2012.02234v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2012.02234v1)
- **Published**: 2020-12-03 19:56:48+00:00
- **Updated**: 2020-12-03 19:56:48+00:00
- **Authors**: Khalfalla Awedat, Almabrok Essa
- **Comment**: 7 pages, 6 figures
- **Journal**: None
- **Summary**: One of the most serious global health threat is COVID-19 pandemic. The emphasis on improving diagnosis and increasing the diagnostic capability helps stopping its spread significantly. Therefore, to assist the radiologist or other medical professional to detect and identify the COVID-19 cases in the shortest possible time, we propose a computer-aided detection (CADe) system that uses the computed tomography (CT) scan images. This proposed boosted deep learning network (CLNet) is based on the implementation of Deep Learning (DL) networks as a complementary to the Compressive Learning (CL). We utilize our inception feature extraction technique in the measurement domain using CL to represent the data features into a new space with less dimensionality before accessing the Convolutional Neural Network. All original features have been contributed equally in the new space using a sensing matrix. Experiments performed on different compressed methods show promising results for COVID-19 detection. In addition, our novel weighted method based on different sensing matrices that used to capture boosted features demonstrates an improvement in the performance of the proposed method.



### Explaining Predictions of Deep Neural Classifier via Activation Analysis
- **Arxiv ID**: http://arxiv.org/abs/2012.02248v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2012.02248v1)
- **Published**: 2020-12-03 20:36:19+00:00
- **Updated**: 2020-12-03 20:36:19+00:00
- **Authors**: Martin Stano, Wanda Benesova, Lukas Samuel Martak
- **Comment**: None
- **Journal**: None
- **Summary**: In many practical applications, deep neural networks have been typically deployed to operate as a black box predictor. Despite the high amount of work on interpretability and high demand on the reliability of these systems, they typically still have to include a human actor in the loop, to validate the decisions and handle unpredictable failures and unexpected corner cases. This is true in particular for failure-critical application domains, such as medical diagnosis. We present a novel approach to explain and support an interpretation of the decision-making process to a human expert operating a deep learning system based on Convolutional Neural Network (CNN). By modeling activation statistics on selected layers of a trained CNN via Gaussian Mixture Models (GMM), we develop a novel perceptual code in binary vector space that describes how the input sample is processed by the CNN. By measuring distances between pairs of samples in this perceptual encoding space, for any new input sample, we can now retrieve a set of most perceptually similar and dissimilar samples from an existing atlas of labeled samples, to support and clarify the decision made by the CNN model. Possible uses of this approach include for example Computer-Aided Diagnosis (CAD) systems working with medical imaging data, such as Magnetic Resonance Imaging (MRI) or Computed Tomography (CT) scans. We demonstrate the viability of our method in the domain of medical imaging for patient condition diagnosis, as the proposed decision explanation method via similar ground truth domain examples (e.g. from existing diagnosis archives) will be interpretable by the operating medical personnel. Our results indicate that our method is capable of detecting distinct prediction strategies that enable us to identify the most similar predictions from an existing atlas.



### Domain Adaptation on Semantic Segmentation for Aerial Images
- **Arxiv ID**: http://arxiv.org/abs/2012.02264v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.02264v2)
- **Published**: 2020-12-03 20:58:27+00:00
- **Updated**: 2020-12-11 16:09:12+00:00
- **Authors**: Ying Chen, Xu Ouyang, Kaiyue Zhu, Gady Agam
- **Comment**: None
- **Journal**: None
- **Summary**: Semantic segmentation has achieved significant advances in recent years. While deep neural networks perform semantic segmentation well, their success rely on pixel level supervision which is expensive and time-consuming. Further, training using data from one domain may not generalize well to data from a new domain due to a domain gap between data distributions in the different domains. This domain gap is particularly evident in aerial images where visual appearance depends on the type of environment imaged, season, weather, and time of day when the environment is imaged. Subsequently, this distribution gap leads to severe accuracy loss when using a pretrained segmentation model to analyze new data with different characteristics. In this paper, we propose a novel unsupervised domain adaptation framework to address domain shift in the context of aerial semantic image segmentation. To this end, we solve the problem of domain shift by learn the soft label distribution difference between the source and target domains. Further, we also apply entropy minimization on the target domain to produce high-confident prediction rather than using high-confident prediction by pseudo-labeling. We demonstrate the effectiveness of our domain adaptation framework using the challenge image segmentation dataset of ISPRS, and show improvement over state-of-the-art methods in terms of various metrics.



### Detecting Trojaned DNNs Using Counterfactual Attributions
- **Arxiv ID**: http://arxiv.org/abs/2012.02275v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.IT, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/2012.02275v1)
- **Published**: 2020-12-03 21:21:33+00:00
- **Updated**: 2020-12-03 21:21:33+00:00
- **Authors**: Karan Sikka, Indranil Sur, Susmit Jha, Anirban Roy, Ajay Divakaran
- **Comment**: None
- **Journal**: None
- **Summary**: We target the problem of detecting Trojans or backdoors in DNNs. Such models behave normally with typical inputs but produce specific incorrect predictions for inputs poisoned with a Trojan trigger. Our approach is based on a novel observation that the trigger behavior depends on a few ghost neurons that activate on trigger pattern and exhibit abnormally higher relative attribution for wrong decisions when activated. Further, these trigger neurons are also active on normal inputs of the target class. Thus, we use counterfactual attributions to localize these ghost neurons from clean inputs and then incrementally excite them to observe changes in the model's accuracy. We use this information for Trojan detection by using a deep set encoder that enables invariance to the number of model classes, architecture, etc. Our approach is implemented in the TrinityAI tool that exploits the synergies between trustworthiness, resilience, and interpretability challenges in deep learning. We evaluate our approach on benchmarks with high diversity in model architectures, triggers, etc. We show consistent gains (+10%) over state-of-the-art methods that rely on the susceptibility of the DNN to specific adversarial attacks, which in turn requires strong assumptions on the nature of the Trojan attack.



### BoxInst: High-Performance Instance Segmentation with Box Annotations
- **Arxiv ID**: http://arxiv.org/abs/2012.02310v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.02310v1)
- **Published**: 2020-12-03 22:27:55+00:00
- **Updated**: 2020-12-03 22:27:55+00:00
- **Authors**: Zhi Tian, Chunhua Shen, Xinlong Wang, Hao Chen
- **Comment**: Code is available at: https://git.io/AdelaiDet
- **Journal**: None
- **Summary**: We present a high-performance method that can achieve mask-level instance segmentation with only bounding-box annotations for training. While this setting has been studied in the literature, here we show significantly stronger performance with a simple design (e.g., dramatically improving previous best reported mask AP of 21.1% in Hsu et al. (2019) to 31.6% on the COCO dataset). Our core idea is to redesign the loss of learning masks in instance segmentation, with no modification to the segmentation network itself. The new loss functions can supervise the mask training without relying on mask annotations. This is made possible with two loss terms, namely, 1) a surrogate term that minimizes the discrepancy between the projections of the ground-truth box and the predicted mask; 2) a pairwise loss that can exploit the prior that proximal pixels with similar colors are very likely to have the same category label. Experiments demonstrate that the redesigned mask loss can yield surprisingly high-quality instance masks with only box annotations. For example, without using any mask annotations, with a ResNet-101 backbone and 3x training schedule, we achieve 33.2% mask AP on COCO test-dev split (vs. 39.1% of the fully supervised counterpart). Our excellent experiment results on COCO and Pascal VOC indicate that our method dramatically narrows the performance gap between weakly and fully supervised instance segmentation.   Code is available at: https://git.io/AdelaiDet



### Multicenter Assessment of Augmented Reality Registration Methods for Image-guided Interventions
- **Arxiv ID**: http://arxiv.org/abs/2012.02319v1
- **DOI**: None
- **Categories**: **physics.med-ph**, cs.CV, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2012.02319v1)
- **Published**: 2020-12-03 23:05:48+00:00
- **Updated**: 2020-12-03 23:05:48+00:00
- **Authors**: Ningcheng Li, Jonathan Wakim, Yilun Koethe, Timothy Huber, Terence Gade, Stephen Hunt, Brian Park
- **Comment**: 16 pages, 5 figures (plus 1 supplemental figure), and 1 table
- **Journal**: None
- **Summary**: Purpose: To evaluate manual and automatic registration times as well as accuracy with augmented reality during alignment of a holographic 3-dimensional (3D) model onto the real-world environment.   Method: 18 participants in various stages of clinical training across two academic centers registered a 3D CT phantom model onto a CT grid using the HoloLens 2 augmented reality headset 3 consecutive times. Registration times and accuracy were compared among different registration methods (hand gesture, Xbox controller, and automatic registration), levels of clinical experience, and consecutive attempts. Registration times were also compared with prior HoloLens 1 data.   Results: Mean aggregate manual registration times were 27.7, 24.3, and 72.8 seconds for one-handed gesture, two-handed gesture, and Xbox controller, respectively; mean automatic registration time was 5.3s (ANOVA p<0.0001). No significant difference in registration times was found among attendings, residents and fellows, and medical students (p>0.05). Significant improvements in registration times were detected across consecutive attempts using hand gestures (p<0.01). Compared with previously reported HoloLens 1 experience, hand gesture registration times were 81.7% faster (p<0.05). Registration accuracies were not significantly different across manual registration methods, measuring at 5.9, 9.5, and 8.6 mm with one-handed gesture, two-handed gesture, and Xbox controller, respectively (p>0.05).   Conclusions: Manual registration times decreased significantly with updated hand gesture maneuvers on HoloLens 2 versus HoloLens 1, approaching the registration times of automatic registration and outperforming Xbox controller mediated registration. These results will encourage wider clinical integration of HoloLens 2 in procedural medical care.



### Probabilistic Tracklet Scoring and Inpainting for Multiple Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/2012.02337v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.02337v2)
- **Published**: 2020-12-03 23:59:27+00:00
- **Updated**: 2020-12-10 04:11:29+00:00
- **Authors**: Fatemeh Saleh, Sadegh Aliakbarian, Hamid Rezatofighi, Mathieu Salzmann, Stephen Gould
- **Comment**: None
- **Journal**: None
- **Summary**: Despite the recent advances in multiple object tracking (MOT), achieved by joint detection and tracking, dealing with long occlusions remains a challenge. This is due to the fact that such techniques tend to ignore the long-term motion information. In this paper, we introduce a probabilistic autoregressive motion model to score tracklet proposals by directly measuring their likelihood. This is achieved by training our model to learn the underlying distribution of natural tracklets. As such, our model allows us not only to assign new detections to existing tracklets, but also to inpaint a tracklet when an object has been lost for a long time, e.g., due to occlusion, by sampling tracklets so as to fill the gap caused by misdetections. Our experiments demonstrate the superiority of our approach at tracking objects in challenging sequences; it outperforms the state of the art in most standard MOT metrics on multiple MOT benchmark datasets, including MOT16, MOT17, and MOT20.



