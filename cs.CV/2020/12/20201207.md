# Arxiv Papers in cs.CV on 2020-12-07
### CompFeat: Comprehensive Feature Aggregation for Video Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2012.03400v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.03400v1)
- **Published**: 2020-12-07 00:31:42+00:00
- **Updated**: 2020-12-07 00:31:42+00:00
- **Authors**: Yang Fu, Linjie Yang, Ding Liu, Thomas S. Huang, Humphrey Shi
- **Comment**: Accepted to AAAI 2021
- **Journal**: None
- **Summary**: Video instance segmentation is a complex task in which we need to detect, segment, and track each object for any given video. Previous approaches only utilize single-frame features for the detection, segmentation, and tracking of objects and they suffer in the video scenario due to several distinct challenges such as motion blur and drastic appearance change. To eliminate ambiguities introduced by only using single-frame features, we propose a novel comprehensive feature aggregation approach (CompFeat) to refine features at both frame-level and object-level with temporal and spatial context information. The aggregation process is carefully designed with a new attention mechanism which significantly increases the discriminative power of the learned features. We further improve the tracking capability of our model through a siamese design by incorporating both feature similarities and spatial similarities. Experiments conducted on the YouTube-VIS dataset validate the effectiveness of proposed CompFeat. Our code will be available at https://github.com/SHI-Labs/CompFeat-for-Video-Instance-Segmentation.



### PMP-Net: Point Cloud Completion by Learning Multi-step Point Moving Paths
- **Arxiv ID**: http://arxiv.org/abs/2012.03408v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.03408v3)
- **Published**: 2020-12-07 01:34:38+00:00
- **Updated**: 2021-06-12 13:52:34+00:00
- **Authors**: Xin Wen, Peng Xiang, Zhizhong Han, Yan-Pei Cao, Pengfei Wan, Wen Zheng, Yu-Shen Liu
- **Comment**: Accepted by CVPR 2021
- **Journal**: None
- **Summary**: The task of point cloud completion aims to predict the missing part for an incomplete 3D shape. A widely used strategy is to generate a complete point cloud from the incomplete one. However, the unordered nature of point clouds will degrade the generation of high-quality 3D shapes, as the detailed topology and structure of discrete points are hard to be captured by the generative process only using a latent code. In this paper, we address the above problem by reconsidering the completion task from a new perspective, where we formulate the prediction as a point cloud deformation process. Specifically, we design a novel neural network, named PMP-Net, to mimic the behavior of an earth mover. It moves each point of the incomplete input to complete the point cloud, where the total distance of point moving paths (PMP) should be shortest. Therefore, PMP-Net predicts a unique point moving path for each point according to the constraint of total point moving distances. As a result, the network learns a strict and unique correspondence on point-level, which can capture the detailed topology and structure relationships between the incomplete shape and the complete target, and thus improves the quality of the predicted complete shape. We conduct comprehensive experiments on Completion3D and PCN datasets, which demonstrate our advantages over the state-of-the-art point cloud completion methods.



### Boosting Image Super-Resolution Via Fusion of Complementary Information Captured by Multi-Modal Sensors
- **Arxiv ID**: http://arxiv.org/abs/2012.03417v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.03417v1)
- **Published**: 2020-12-07 02:15:28+00:00
- **Updated**: 2020-12-07 02:15:28+00:00
- **Authors**: Fan Wang, Jiangxin Yang, Yanlong Cao, Yanpeng Cao, Michael Ying Yang
- **Comment**: 8 pages, 7 figures
- **Journal**: None
- **Summary**: Image Super-Resolution (SR) provides a promising technique to enhance the image quality of low-resolution optical sensors, facilitating better-performing target detection and autonomous navigation in a wide range of robotics applications. It is noted that the state-of-the-art SR methods are typically trained and tested using single-channel inputs, neglecting the fact that the cost of capturing high-resolution images in different spectral domains varies significantly. In this paper, we attempt to leverage complementary information from a low-cost channel (visible/depth) to boost image quality of an expensive channel (thermal) using fewer parameters. To this end, we first present an effective method to virtually generate pixel-wise aligned visible and thermal images based on real-time 3D reconstruction of multi-modal data captured at various viewpoints. Then, we design a feature-level multispectral fusion residual network model to perform high-accuracy SR of thermal images by adaptively integrating co-occurrence features presented in multispectral images. Experimental results demonstrate that this new approach can effectively alleviate the ill-posed inverse problem of image SR by taking into account complementary information from an additional low-cost channel, significantly outperforming state-of-the-art SR approaches in terms of both accuracy and efficiency.



### Interpreting Deep Neural Networks with Relative Sectional Propagation by Analyzing Comparative Gradients and Hostile Activations
- **Arxiv ID**: http://arxiv.org/abs/2012.03434v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.03434v2)
- **Published**: 2020-12-07 03:11:07+00:00
- **Updated**: 2020-12-12 10:49:00+00:00
- **Authors**: Woo-Jeoung Nam, Jaesik Choi, Seong-Whan Lee
- **Comment**: 9 pages, 8 figures, Accepted paper in AAAI Conference on Artificial
  Intelligence (AAAI), 2021
- **Journal**: None
- **Summary**: The clear transparency of Deep Neural Networks (DNNs) is hampered by complex internal structures and nonlinear transformations along deep hierarchies. In this paper, we propose a new attribution method, Relative Sectional Propagation (RSP), for fully decomposing the output predictions with the characteristics of class-discriminative attributions and clear objectness. We carefully revisit some shortcomings of backpropagation-based attribution methods, which are trade-off relations in decomposing DNNs. We define hostile factor as an element that interferes with finding the attributions of the target and propagate it in a distinguishable way to overcome the non-suppressed nature of activated neurons. As a result, it is possible to assign the bi-polar relevance scores of the target (positive) and hostile (negative) attributions while maintaining each attribution aligned with the importance. We also present the purging techniques to prevent the decrement of the gap between the relevance scores of the target and hostile attributions during backward propagation by eliminating the conflicting units to channel attribution map. Therefore, our method makes it possible to decompose the predictions of DNNs with clearer class-discriminativeness and detailed elucidations of activation neurons compared to the conventional attribution methods. In a verified experimental environment, we report the results of the assessments: (i) Pointing Game, (ii) mIoU, and (iii) Model Sensitivity with PASCAL VOC 2007, MS COCO 2014, and ImageNet datasets. The results demonstrate that our method outperforms existing backward decomposition methods, including distinctive and intuitive visualizations.



### Selective Pseudo-Labeling with Reinforcement Learning for Semi-Supervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2012.03438v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.03438v1)
- **Published**: 2020-12-07 03:37:38+00:00
- **Updated**: 2020-12-07 03:37:38+00:00
- **Authors**: Bingyu Liu, Yuhong Guo, Jieping Ye, Weihong Deng
- **Comment**: None
- **Journal**: None
- **Summary**: Recent domain adaptation methods have demonstrated impressive improvement on unsupervised domain adaptation problems. However, in the semi-supervised domain adaptation (SSDA) setting where the target domain has a few labeled instances available, these methods can fail to improve performance. Inspired by the effectiveness of pseudo-labels in domain adaptation, we propose a reinforcement learning based selective pseudo-labeling method for semi-supervised domain adaptation. It is difficult for conventional pseudo-labeling methods to balance the correctness and representativeness of pseudo-labeled data. To address this limitation, we develop a deep Q-learning model to select both accurate and representative pseudo-labeled instances. Moreover, motivated by large margin loss's capacity on learning discriminative features with little data, we further propose a novel target margin loss for our base model training to improve its discriminability. Our proposed method is evaluated on several benchmark datasets for SSDA, and demonstrates superior performance to all the comparison methods.



### Hyperspectral Classification Based on Lightweight 3-D-CNN With Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/2012.03439v1
- **DOI**: 10.1109/TGRS.2019.2902568
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.03439v1)
- **Published**: 2020-12-07 03:44:35+00:00
- **Updated**: 2020-12-07 03:44:35+00:00
- **Authors**: Haokui Zhang, Ying Li, Yenan Jiang, Peng Wang, Qiang Shen, Chunhua Shen
- **Comment**: 16 pages. Accepted to IEEE Trans. Geosci. Remote Sens. Code is
  available at: https://github.com/hkzhang91/LWNet
- **Journal**: IEEE Transactions on Geoscience and Remote Sensing, 2019, 57(8):
  5813-5828
- **Summary**: Recently, hyperspectral image (HSI) classification approaches based on deep learning (DL) models have been proposed and shown promising performance. However, because of very limited available training samples and massive model parameters, DL methods may suffer from overfitting. In this paper, we propose an end-to-end 3-D lightweight convolutional neural network (CNN) (abbreviated as 3-D-LWNet) for limited samples-based HSI classification. Compared with conventional 3-D-CNN models, the proposed 3-D-LWNet has a deeper network structure, less parameters, and lower computation cost, resulting in better classification performance. To further alleviate the small sample problem, we also propose two transfer learning strategies: 1) cross-sensor strategy, in which we pretrain a 3-D model in the source HSI data sets containing a greater number of labeled samples and then transfer it to the target HSI data sets and 2) cross-modal strategy, in which we pretrain a 3-D model in the 2-D RGB image data sets containing a large number of samples and then transfer it to the target HSI data sets. In contrast to previous approaches, we do not impose restrictions over the source data sets, in which they do not have to be collected by the same sensors as the target data sets. Experiments on three public HSI data sets captured by different sensors demonstrate that our model achieves competitive performance for HSI classification compared to several state-of-the-art methods



### VideoMix: Rethinking Data Augmentation for Video Classification
- **Arxiv ID**: http://arxiv.org/abs/2012.03457v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.03457v1)
- **Published**: 2020-12-07 05:40:33+00:00
- **Updated**: 2020-12-07 05:40:33+00:00
- **Authors**: Sangdoo Yun, Seong Joon Oh, Byeongho Heo, Dongyoon Han, Jinhyung Kim
- **Comment**: 15 pages
- **Journal**: None
- **Summary**: State-of-the-art video action classifiers often suffer from overfitting. They tend to be biased towards specific objects and scene cues, rather than the foreground action content, leading to sub-optimal generalization performances. Recent data augmentation strategies have been reported to address the overfitting problems in static image classifiers. Despite the effectiveness on the static image classifiers, data augmentation has rarely been studied for videos. For the first time in the field, we systematically analyze the efficacy of various data augmentation strategies on the video classification task. We then propose a powerful augmentation strategy VideoMix. VideoMix creates a new training video by inserting a video cuboid into another video. The ground truth labels are mixed proportionally to the number of voxels from each video. We show that VideoMix lets a model learn beyond the object and scene biases and extract more robust cues for action recognition. VideoMix consistently outperforms other augmentation baselines on Kinetics and the challenging Something-Something-V2 benchmarks. It also improves the weakly-supervised action localization performance on THUMOS'14. VideoMix pretrained models exhibit improved accuracies on the video detection task (AVA).



### PFA-GAN: Progressive Face Aging with Generative Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/2012.03459v1
- **DOI**: 10.1109/TIFS.2020.3047753
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.03459v1)
- **Published**: 2020-12-07 05:45:13+00:00
- **Updated**: 2020-12-07 05:45:13+00:00
- **Authors**: Zhizhong Huang, Shouzhen Chen, Junping Zhang, Hongming Shan
- **Comment**: None
- **Journal**: IEEE Transactions on Information Forensics and Security, 2021
- **Summary**: Face aging is to render a given face to predict its future appearance, which plays an important role in the information forensics and security field as the appearance of the face typically varies with age. Although impressive results have been achieved with conditional generative adversarial networks (cGANs), the existing cGANs-based methods typically use a single network to learn various aging effects between any two different age groups. However, they cannot simultaneously meet three essential requirements of face aging -- including image quality, aging accuracy, and identity preservation -- and usually generate aged faces with strong ghost artifacts when the age gap becomes large. Inspired by the fact that faces gradually age over time, this paper proposes a novel progressive face aging framework based on generative adversarial network (PFA-GAN) to mitigate these issues. Unlike the existing cGANs-based methods, the proposed framework contains several sub-networks to mimic the face aging process from young to old, each of which only learns some specific aging effects between two adjacent age groups. The proposed framework can be trained in an end-to-end manner to eliminate accumulative artifacts and blurriness. Moreover, this paper introduces an age estimation loss to take into account the age distribution for an improved aging accuracy, and proposes to use the Pearson correlation coefficient as an evaluation metric measuring the aging smoothness for face aging methods. Extensively experimental results demonstrate superior performance over existing (c)GANs-based methods, including the state-of-the-art one, on two benchmarked datasets. The source code is available at~\url{https://github.com/Hzzone/PFA-GAN}.



### Attention-based Saliency Hashing for Ophthalmic Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2012.03466v1
- **DOI**: 10.1109/BIBM49941.2020.9313536
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2012.03466v1)
- **Published**: 2020-12-07 06:04:12+00:00
- **Updated**: 2020-12-07 06:04:12+00:00
- **Authors**: Jiansheng Fang, Yanwu Xu, Xiaoqing Zhang, Yan Hu, Jiang Liu
- **Comment**: 8 pages, 4 figures, BIBM2020 conference
- **Journal**: None
- **Summary**: Deep hashing methods have been proved to be effective for the large-scale medical image search assisting reference-based diagnosis for clinicians. However, when the salient region plays a maximal discriminative role in ophthalmic image, existing deep hashing methods do not fully exploit the learning ability of the deep network to capture the features of salient regions pointedly. The different grades or classes of ophthalmic images may be share similar overall performance but have subtle differences that can be differentiated by mining salient regions. To address this issue, we propose a novel end-to-end network, named Attention-based Saliency Hashing (ASH), for learning compact hash-code to represent ophthalmic images. ASH embeds a spatial-attention module to focus more on the representation of salient regions and highlights their essential role in differentiating ophthalmic images. Benefiting from the spatial-attention module, the information of salient regions can be mapped into the hash-code for similarity calculation. In the training stage, we input the image pairs to share the weights of the network, and a pairwise loss is designed to maximize the discriminability of the hash-code. In the retrieval stage, ASH obtains the hash-code by inputting an image with an end-to-end manner, then the hash-code is used to similarity calculation to return the most similar images. Extensive experiments on two different modalities of ophthalmic image datasets demonstrate that the proposed ASH can further improve the retrieval performance compared to the state-of-the-art deep hashing methods due to the huge contributions of the spatial-attention module.



### Multi-Instrumentalist Net: Unsupervised Generation of Music from Body Movements
- **Arxiv ID**: http://arxiv.org/abs/2012.03478v1
- **DOI**: None
- **Categories**: **cs.SD**, cs.CV, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2012.03478v1)
- **Published**: 2020-12-07 06:54:10+00:00
- **Updated**: 2020-12-07 06:54:10+00:00
- **Authors**: Kun Su, Xiulong Liu, Eli Shlizerman
- **Comment**: Please see associated video at
  https://www.youtube.com/watch?v=yo5OZKBbBh4
- **Journal**: None
- **Summary**: We propose a novel system that takes as an input body movements of a musician playing a musical instrument and generates music in an unsupervised setting. Learning to generate multi-instrumental music from videos without labeling the instruments is a challenging problem. To achieve the transformation, we built a pipeline named 'Multi-instrumentalistNet' (MI Net). At its base, the pipeline learns a discrete latent representation of various instruments music from log-spectrogram using a Vector Quantized Variational Autoencoder (VQ-VAE) with multi-band residual blocks. The pipeline is then trained along with an autoregressive prior conditioned on the musician's body keypoints movements encoded by a recurrent neural network. Joint training of the prior with the body movements encoder succeeds in the disentanglement of the music into latent features indicating the musical components and the instrumental features. The latent space results in distributions that are clustered into distinct instruments from which new music can be generated. Furthermore, the VQ-VAE architecture supports detailed music generation with additional conditioning. We show that a Midi can further condition the latent space such that the pipeline will generate the exact content of the music being played by the instrument in the video. We evaluate MI Net on two datasets containing videos of 13 instruments and obtain generated music of reasonable audio quality, easily associated with the corresponding instrument, and consistent with the music audio content.



### Meta Ordinal Regression Forest For Learning with Unsure Lung Nodules
- **Arxiv ID**: http://arxiv.org/abs/2012.03480v1
- **DOI**: 10.1109/BIBM49941.2020.9313554
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.03480v1)
- **Published**: 2020-12-07 06:59:43+00:00
- **Updated**: 2020-12-07 06:59:43+00:00
- **Authors**: Yiming Lei, Haiping Zhu, Junping Zhang, Hongming Shan
- **Comment**: None
- **Journal**: IEEE International Conference on Bioinformatics and Biomedicine
  (BIBM 2020)
- **Summary**: Deep learning-based methods have achieved promising performance in early detection and classification of lung nodules, most of which discard unsure nodules and simply deal with a binary classification -- malignant vs benign. Recently, an unsure data model (UDM) was proposed to incorporate those unsure nodules by formulating this problem as an ordinal regression, showing better performance over traditional binary classification. To further explore the ordinal relationship for lung nodule classification, this paper proposes a meta ordinal regression forest (MORF), which improves upon the state-of-the-art ordinal regression method, deep ordinal regression forest (DORF), in three major ways. First, MORF can alleviate the biases of the predictions by making full use of deep features while DORF needs to fix the composition of decision trees before training. Second, MORF has a novel grouped feature selection (GFS) module to re-sample the split nodes of decision trees. Last, combined with GFS, MORF is equipped with a meta learning-based weighting scheme to map the features selected by GFS to tree-wise weights while DORF assigns equal weights for all trees. Experimental results on the LIDC-IDRI dataset demonstrate superior performance over existing methods, including the state-of-the-art DORF.



### Rethinking Learnable Tree Filter for Generic Feature Transform
- **Arxiv ID**: http://arxiv.org/abs/2012.03482v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, 68T45
- **Links**: [PDF](http://arxiv.org/pdf/2012.03482v1)
- **Published**: 2020-12-07 07:16:47+00:00
- **Updated**: 2020-12-07 07:16:47+00:00
- **Authors**: Lin Song, Yanwei Li, Zhengkai Jiang, Zeming Li, Xiangyu Zhang, Hongbin Sun, Jian Sun, Nanning Zheng
- **Comment**: Accepted by NeurIPS-2020
- **Journal**: None
- **Summary**: The Learnable Tree Filter presents a remarkable approach to model structure-preserving relations for semantic segmentation. Nevertheless, the intrinsic geometric constraint forces it to focus on the regions with close spatial distance, hindering the effective long-range interactions. To relax the geometric constraint, we give the analysis by reformulating it as a Markov Random Field and introduce a learnable unary term. Besides, we propose a learnable spanning tree algorithm to replace the original non-differentiable one, which further improves the flexibility and robustness. With the above improvements, our method can better capture long-range dependencies and preserve structural details with linear complexity, which is extended to several vision tasks for more generic feature transform. Extensive experiments on object detection/instance segmentation demonstrate the consistent improvements over the original version. For semantic segmentation, we achieve leading performance (82.1% mIoU) on the Cityscapes benchmark without bells-and-whistles. Code is available at https://github.com/StevenGrove/LearnableTreeFilterV2.



### An Approach to Intelligent Pneumonia Detection and Integration
- **Arxiv ID**: http://arxiv.org/abs/2012.03487v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/2012.03487v1)
- **Published**: 2020-12-07 07:27:45+00:00
- **Updated**: 2020-12-07 07:27:45+00:00
- **Authors**: Bonaventure F. P. Dossou, Alena Iureva, Sayali R. Rajhans, Vamsi S. Pidikiti
- **Comment**: None
- **Journal**: None
- **Summary**: Each year, over 2.5 million people, most of them in developed countries, die from pneumonia [1]. Since many studies have proved pneumonia is successfully treatable when timely and correctly diagnosed, many of diagnosis aids have been developed, with AI-based methods achieving high accuracies [2]. However, currently, the usage of AI in pneumonia detection is limited, in particular, due to challenges in generalizing a locally achieved result. In this report, we propose a roadmap for creating and integrating a system that attempts to solve this challenge. We also address various technical, legal, ethical, and logistical issues, with a blueprint of possible solutions.



### CARAFE++: Unified Content-Aware ReAssembly of FEatures
- **Arxiv ID**: http://arxiv.org/abs/2012.04733v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.04733v1)
- **Published**: 2020-12-07 07:34:57+00:00
- **Updated**: 2020-12-07 07:34:57+00:00
- **Authors**: Jiaqi Wang, Kai Chen, Rui Xu, Ziwei Liu, Chen Change Loy, Dahua Lin
- **Comment**: Technical Report. Extended journal version of the conference paper
  that appeared as arXiv:1905.02188
- **Journal**: None
- **Summary**: Feature reassembly, i.e. feature downsampling and upsampling, is a key operation in a number of modern convolutional network architectures, e.g., residual networks and feature pyramids. Its design is critical for dense prediction tasks such as object detection and semantic/instance segmentation. In this work, we propose unified Content-Aware ReAssembly of FEatures (CARAFE++), a universal, lightweight and highly effective operator to fulfill this goal. CARAFE++ has several appealing properties: (1) Unlike conventional methods such as pooling and interpolation that only exploit sub-pixel neighborhood, CARAFE++ aggregates contextual information within a large receptive field. (2) Instead of using a fixed kernel for all samples (e.g. convolution and deconvolution), CARAFE++ generates adaptive kernels on-the-fly to enable instance-specific content-aware handling. (3) CARAFE++ introduces little computational overhead and can be readily integrated into modern network architectures. We conduct comprehensive evaluations on standard benchmarks in object detection, instance/semantic segmentation and image inpainting. CARAFE++ shows consistent and substantial gains across all the tasks (2.5% APbox, 2.1% APmask, 1.94% mIoU, 1.35 dB respectively) with negligible computational overhead. It shows great potential to serve as a strong building block for modern deep networks.



### Fine-grained Angular Contrastive Learning with Coarse Labels
- **Arxiv ID**: http://arxiv.org/abs/2012.03515v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.03515v3)
- **Published**: 2020-12-07 08:09:02+00:00
- **Updated**: 2021-06-22 09:39:03+00:00
- **Authors**: Guy Bukchin, Eli Schwartz, Kate Saenko, Ori Shahar, Rogerio Feris, Raja Giryes, Leonid Karlinsky
- **Comment**: None
- **Journal**: None
- **Summary**: Few-shot learning methods offer pre-training techniques optimized for easier later adaptation of the model to new classes (unseen during training) using one or a few examples. This adaptivity to unseen classes is especially important for many practical applications where the pre-trained label space cannot remain fixed for effective use and the model needs to be "specialized" to support new categories on the fly. One particularly interesting scenario, essentially overlooked by the few-shot literature, is Coarse-to-Fine Few-Shot (C2FS), where the training classes (e.g. animals) are of much `coarser granularity' than the target (test) classes (e.g. breeds). A very practical example of C2FS is when the target classes are sub-classes of the training classes. Intuitively, it is especially challenging as (both regular and few-shot) supervised pre-training tends to learn to ignore intra-class variability which is essential for separating sub-classes. In this paper, we introduce a novel 'Angular normalization' module that allows to effectively combine supervised and self-supervised contrastive pre-training to approach the proposed C2FS task, demonstrating significant gains in a broad study over multiple baselines and datasets. We hope that this work will help to pave the way for future research on this new, challenging, and very practical topic of C2FS classification.



### A Singular Value Perspective on Model Robustness
- **Arxiv ID**: http://arxiv.org/abs/2012.03516v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.03516v1)
- **Published**: 2020-12-07 08:09:07+00:00
- **Updated**: 2020-12-07 08:09:07+00:00
- **Authors**: Malhar Jere, Maghav Kumar, Farinaz Koushanfar
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) have made significant progress on several computer vision benchmarks, but are fraught with numerous non-human biases such as vulnerability to adversarial samples. Their lack of explainability makes identification and rectification of these biases difficult, and understanding their generalization behavior remains an open problem. In this work we explore the relationship between the generalization behavior of CNNs and the Singular Value Decomposition (SVD) of images. We show that naturally trained and adversarially robust CNNs exploit highly different features for the same dataset. We demonstrate that these features can be disentangled by SVD for ImageNet and CIFAR-10 trained networks. Finally, we propose Rank Integrated Gradients (RIG), the first rank-based feature attribution method to understand the dependence of CNNs on image rank.



### Fine-Grained Dynamic Head for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2012.03519v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, 68T45
- **Links**: [PDF](http://arxiv.org/pdf/2012.03519v1)
- **Published**: 2020-12-07 08:16:32+00:00
- **Updated**: 2020-12-07 08:16:32+00:00
- **Authors**: Lin Song, Yanwei Li, Zhengkai Jiang, Zeming Li, Hongbin Sun, Jian Sun, Nanning Zheng
- **Comment**: Accepted by NeurIPS-2020
- **Journal**: None
- **Summary**: The Feature Pyramid Network (FPN) presents a remarkable approach to alleviate the scale variance in object representation by performing instance-level assignments. Nevertheless, this strategy ignores the distinct characteristics of different sub-regions in an instance. To this end, we propose a fine-grained dynamic head to conditionally select a pixel-level combination of FPN features from different scales for each instance, which further releases the ability of multi-scale feature representation. Moreover, we design a spatial gate with the new activation function to reduce computational complexity dramatically through spatially sparse convolutions. Extensive experiments demonstrate the effectiveness and efficiency of the proposed method on several state-of-the-art detection benchmarks. Code is available at https://github.com/StevenGrove/DynamicHead.



### Backpropagating Linearly Improves Transferability of Adversarial Examples
- **Arxiv ID**: http://arxiv.org/abs/2012.03528v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2012.03528v1)
- **Published**: 2020-12-07 08:40:56+00:00
- **Updated**: 2020-12-07 08:40:56+00:00
- **Authors**: Yiwen Guo, Qizhang Li, Hao Chen
- **Comment**: Accepted by NeurIPS 2020
- **Journal**: None
- **Summary**: The vulnerability of deep neural networks (DNNs) to adversarial examples has drawn great attention from the community. In this paper, we study the transferability of such examples, which lays the foundation of many black-box attacks on DNNs. We revisit a not so new but definitely noteworthy hypothesis of Goodfellow et al.'s and disclose that the transferability can be enhanced by improving the linearity of DNNs in an appropriate manner. We introduce linear backpropagation (LinBP), a method that performs backpropagation in a more linear fashion using off-the-shelf attacks that exploit gradients. More specifically, it calculates forward as normal but backpropagates loss as if some nonlinear activations are not encountered in the forward pass. Experimental results demonstrate that this simple yet effective method obviously outperforms current state-of-the-arts in crafting transferable adversarial examples on CIFAR-10 and ImageNet, leading to more effective attacks on a variety of DNNs.



### Multi-temporal and multi-source remote sensing image classification by nonlinear relative normalization
- **Arxiv ID**: http://arxiv.org/abs/2012.04469v1
- **DOI**: 10.1016/j.isprsjprs.2016.07.004
- **Categories**: **eess.SP**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2012.04469v1)
- **Published**: 2020-12-07 08:46:11+00:00
- **Updated**: 2020-12-07 08:46:11+00:00
- **Authors**: Devis Tuia, Diego Marcos, Gustau Camps-Valls
- **Comment**: None
- **Journal**: ISPRS Journal of Photogrammetry and Remote Sensing 120, DOI:
  10.1016/j.isprsjprs.2016.07.004
- **Summary**: Remote sensing image classification exploiting multiple sensors is a very challenging problem: data from different modalities are affected by spectral distortions and mis-alignments of all kinds, and this hampers re-using models built for one image to be used successfully in other scenes. In order to adapt and transfer models across image acquisitions, one must be able to cope with datasets that are not co-registered, acquired under different illumination and atmospheric conditions, by different sensors, and with scarce ground references. Traditionally, methods based on histogram matching have been used. However, they fail when densities have very different shapes or when there is no corresponding band to be matched between the images. An alternative builds upon \emph{manifold alignment}. Manifold alignment performs a multidimensional relative normalization of the data prior to product generation that can cope with data of different dimensionality (e.g. different number of bands) and possibly unpaired examples. Aligning data distributions is an appealing strategy, since it allows to provide data spaces that are more similar to each other, regardless of the subsequent use of the transformed data. In this paper, we study a methodology that aligns data from different domains in a nonlinear way through {\em kernelization}. We introduce the Kernel Manifold Alignment (KEMA) method, which provides a flexible and discriminative projection map, exploits only a few labeled samples (or semantic ties) in each domain, and reduces to solving a generalized eigenvalue problem. We successfully test KEMA in multi-temporal and multi-source very high resolution classification tasks, as well as on the task of making a model invariant to shadowing for hyperspectral imaging.



### Active Learning Methods for Efficient Hybrid Biophysical Variable Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2012.04468v1
- **DOI**: 10.1109/LGRS.2016.2560799
- **Categories**: **eess.SP**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2012.04468v1)
- **Published**: 2020-12-07 08:56:40+00:00
- **Updated**: 2020-12-07 08:56:40+00:00
- **Authors**: ochem Verrelst, Sara Dethier, Juan Pablo Rivera, Jordi Muñoz-Marí, Gustau Camps-Valls, José Moreno
- **Comment**: None
- **Journal**: IEEE Geoscience and Remote Sensing Letters, vol. 13, no. 7, pp.
  1012-1016, July 2016
- **Summary**: Kernel-based machine learning regression algorithms (MLRAs) are potentially powerful methods for being implemented into operational biophysical variable retrieval schemes. However, they face difficulties in coping with large training datasets. With the increasing amount of optical remote sensing data made available for analysis and the possibility of using a large amount of simulated data from radiative transfer models (RTMs) to train kernel MLRAs, efficient data reduction techniques will need to be implemented. Active learning (AL) methods enable to select the most informative samples in a dataset. This letter introduces six AL methods for achieving optimized biophysical variable estimation with a manageable training dataset, and their implementation into a Matlab-based MLRA toolbox for semi-automatic use. The AL methods were analyzed on their efficiency of improving the estimation accuracy of leaf area index and chlorophyll content based on PROSAIL simulations. Each of the implemented methods outperformed random sampling, improving retrieval accuracy with lower sampling rates. Practically, AL methods open opportunities to feed advanced MLRAs with RTM-generated training data for development of operational retrieval models.



### End-to-End Object Detection with Fully Convolutional Network
- **Arxiv ID**: http://arxiv.org/abs/2012.03544v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2012.03544v3)
- **Published**: 2020-12-07 09:14:55+00:00
- **Updated**: 2021-03-26 03:38:55+00:00
- **Authors**: Jianfeng Wang, Lin Song, Zeming Li, Hongbin Sun, Jian Sun, Nanning Zheng
- **Comment**: Accepted to CVPR2021
- **Journal**: None
- **Summary**: Mainstream object detectors based on the fully convolutional network has achieved impressive performance. While most of them still need a hand-designed non-maximum suppression (NMS) post-processing, which impedes fully end-to-end training. In this paper, we give the analysis of discarding NMS, where the results reveal that a proper label assignment plays a crucial role. To this end, for fully convolutional detectors, we introduce a Prediction-aware One-To-One (POTO) label assignment for classification to enable end-to-end detection, which obtains comparable performance with NMS. Besides, a simple 3D Max Filtering (3DMF) is proposed to utilize the multi-scale features and improve the discriminability of convolutions in the local region. With these techniques, our end-to-end framework achieves competitive performance against many state-of-the-art detectors with NMS on COCO and CrowdHuman datasets. The code is available at https://github.com/Megvii-BaseDetection/DeFCN .



### Learned Block Iterative Shrinkage Thresholding Algorithm for Photothermal Super Resolution Imaging
- **Arxiv ID**: http://arxiv.org/abs/2012.03547v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, physics.app-ph, physics.comp-ph
- **Links**: [PDF](http://arxiv.org/pdf/2012.03547v2)
- **Published**: 2020-12-07 09:27:16+00:00
- **Updated**: 2020-12-10 14:15:57+00:00
- **Authors**: Samim Ahmadi, Jan Christian Hauffen, Linh Kästner, Peter Jung, Giuseppe Caire, Mathias Ziegler
- **Comment**: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible. 11 pages, 10 figures
- **Journal**: None
- **Summary**: Block-sparse regularization is already well-known in active thermal imaging and is used for multiple measurement based inverse problems. The main bottleneck of this method is the choice of regularization parameters which differs for each experiment. To avoid time-consuming manually selected regularization parameter, we propose a learned block-sparse optimization approach using an iterative algorithm unfolded into a deep neural network. More precisely, we show the benefits of using a learned block iterative shrinkage thresholding algorithm that is able to learn the choice of regularization parameters. In addition, this algorithm enables the determination of a suitable weight matrix to solve the underlying inverse problem. Therefore, in this paper we present the algorithm and compare it with state of the art block iterative shrinkage thresholding using synthetically generated test data and experimental test data from active thermography for defect reconstruction. Our results show that the use of the learned block-sparse optimization approach provides smaller normalized mean square errors for a small fixed number of iterations than without learning. Thus, this new approach allows to improve the convergence speed and only needs a few iterations to generate accurate defect reconstruction in photothermal super resolution imaging.



### Spectral band selection for vegetation properties retrieval using Gaussian processes regression
- **Arxiv ID**: http://arxiv.org/abs/2012.08640v1
- **DOI**: 10.1016/j.jag.2016.07.016
- **Categories**: **cs.CV**, eess.IV, stat.AP
- **Links**: [PDF](http://arxiv.org/pdf/2012.08640v1)
- **Published**: 2020-12-07 09:28:33+00:00
- **Updated**: 2020-12-07 09:28:33+00:00
- **Authors**: Jochem Verrelst, Juan Pablo Rivera, Anatoly Gitelson, Jesus Delegido, José Moreno, Gustau Camps-Valls
- **Comment**: None
- **Journal**: International Journal of Applied Earth Observation and
  Geoinformation Volume 52, October 2016, Pages 554-567
- **Summary**: With current and upcoming imaging spectrometers, automated band analysis techniques are needed to enable efficient identification of most informative bands to facilitate optimized processing of spectral data into estimates of biophysical variables. This paper introduces an automated spectral band analysis tool (BAT) based on Gaussian processes regression (GPR) for the spectral analysis of vegetation properties. The GPR-BAT procedure sequentially backwards removes the least contributing band in the regression model for a given variable until only one band is kept. GPR-BAT is implemented within the framework of the free ARTMO's MLRA (machine learning regression algorithms) toolbox, which is dedicated to the transforming of optical remote sensing images into biophysical products. GPR-BAT allows (1) to identify the most informative bands in relating spectral data to a biophysical variable, and (2) to find the least number of bands that preserve optimized accurate predictions. This study concludes that a wise band selection of hyperspectral data is strictly required for optimal vegetation properties mapping.



### Robustness Investigation on Deep Learning CT Reconstruction for Real-Time Dose Optimization
- **Arxiv ID**: http://arxiv.org/abs/2012.03579v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2012.03579v1)
- **Published**: 2020-12-07 10:55:54+00:00
- **Updated**: 2020-12-07 10:55:54+00:00
- **Authors**: Chang Liu, Yixing Huang, Joscha Maier, Laura Klein, Marc Kachelrieß, Andreas Maier
- **Comment**: Proceedings for "2020 IEEE Nuclear Science Symposium and Medical
  Imaging Conference"
- **Journal**: None
- **Summary**: In computed tomography (CT), automatic exposure control (AEC) is frequently used to reduce radiation dose exposure to patients. For organ-specific AEC, a preliminary CT reconstruction is necessary to estimate organ shapes for dose optimization, where only a few projections are allowed for real-time reconstruction. In this work, we investigate the performance of automated transform by manifold approximation (AUTOMAP) in such applications. For proof of concept, we investigate its performance on the MNIST dataset first, where the dataset containing all the 10 digits are randomly split into a training set and a test set. We train the AUTOMAP model for image reconstruction from 2 projections or 4 projections directly. The test results demonstrate that AUTOMAP is able to reconstruct most digits well with a false rate of 1.6% and 6.8% respectively. In our subsequent experiment, the MNIST dataset is split in a way that the training set contains 9 digits only while the test set contains the excluded digit only, for instance "2". In the test results, the digit "2"s are falsely predicted as "3" or "5" when using 2 projections for reconstruction, reaching a false rate of 94.4%. For the application in medical images, AUTOMAP is also trained on patients' CT images. The test images reach an average root-mean-square error of 290 HU. Although the coarse body outlines are well reconstructed, some organs are misshaped.



### DIPPAS: A Deep Image Prior PRNU Anonymization Scheme
- **Arxiv ID**: http://arxiv.org/abs/2012.03581v2
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2012.03581v2)
- **Published**: 2020-12-07 10:56:50+00:00
- **Updated**: 2021-10-18 10:17:33+00:00
- **Authors**: Francesco Picetti, Sara Mandelli, Paolo Bestagini, Vincenzo Lipari, Stefano Tubaro
- **Comment**: None
- **Journal**: None
- **Summary**: Source device identification is an important topic in image forensics since it allows to trace back the origin of an image. Its forensics counter-part is source device anonymization, that is, to mask any trace on the image that can be useful for identifying the source device. A typical trace exploited for source device identification is the Photo Response Non-Uniformity (PRNU), a noise pattern left by the device on the acquired images. In this paper, we devise a methodology for suppressing such a trace from natural images without significant impact on image quality. Specifically, we turn PRNU anonymization into an optimization problem in a Deep Image Prior (DIP) framework. In a nutshell, a Convolutional Neural Network (CNN) acts as generator and returns an image that is anonymized with respect to the source PRNU, still maintaining high visual quality. With respect to widely-adopted deep learning paradigms, our proposed CNN is not trained on a set of input-target pairs of images. Instead, it is optimized to reconstruct the PRNU-free image from the original image under analysis itself. This makes the approach particularly suitable in scenarios where large heterogeneous databases are analyzed and prevents any problem due to lack of generalization. Through numerical examples on publicly available datasets, we prove our methodology to be effective compared to state-of-the-art techniques.



### Self-Supervision Closes the Gap Between Weak and Strong Supervision in Histology
- **Arxiv ID**: http://arxiv.org/abs/2012.03583v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2012.03583v1)
- **Published**: 2020-12-07 10:59:38+00:00
- **Updated**: 2020-12-07 10:59:38+00:00
- **Authors**: Olivier Dehaene, Axel Camara, Olivier Moindrot, Axel de Lavergne, Pierre Courtiol
- **Comment**: Accepted as a poster for the ML4H 2020 NeurIPS workshop
- **Journal**: None
- **Summary**: One of the biggest challenges for applying machine learning to histopathology is weak supervision: whole-slide images have billions of pixels yet often only one global label. The state of the art therefore relies on strongly-supervised model training using additional local annotations from domain experts. However, in the absence of detailed annotations, most weakly-supervised approaches depend on a frozen feature extractor pre-trained on ImageNet. We identify this as a key weakness and propose to train an in-domain feature extractor on histology images using MoCo v2, a recent self-supervised learning algorithm. Experimental results on Camelyon16 and TCGA show that the proposed extractor greatly outperforms its ImageNet counterpart. In particular, our results improve the weakly-supervised state of the art on Camelyon16 from 91.4% to 98.7% AUC, thereby closing the gap with strongly-supervised models that reach 99.3% AUC. Through these experiments, we demonstrate that feature extractors trained via self-supervised learning can act as drop-in replacements to significantly improve existing machine learning techniques in histology. Lastly, we show that the learned embedding space exhibits biologically meaningful separation of tissue structures.



### PSGCNet: A Pyramidal Scale and Global Context Guided Network for Dense Object Counting in Remote Sensing Images
- **Arxiv ID**: http://arxiv.org/abs/2012.03597v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.03597v3)
- **Published**: 2020-12-07 11:35:56+00:00
- **Updated**: 2022-02-24 13:20:17+00:00
- **Authors**: Guangshuai Gao, Qingjie Liu, Zhenghui Hu, Lu Li, Qi Wen, Yunhong Wang
- **Comment**: Accepted by TGRS
- **Journal**: None
- **Summary**: Object counting, which aims to count the accurate number of object instances in images, has been attracting more and more attention. However, challenges such as large scale variation, complex background interference, and non-uniform density distribution greatly limit the counting accuracy, particularly striking in remote sensing imagery. To mitigate the above issues, this paper proposes a novel framework for dense object counting in remote sensing images, which incorporates a pyramidal scale module (PSM) and a global context module (GCM), dubbed PSGCNet, where PSM is used to adaptively capture multi-scale information and GCM is to guide the model to select suitable scales generated from PSM. Moreover, a reliable supervision manner improved from Bayesian and Counting loss (BCL) is utilized to learn the density probability and then compute the count expectation at each annotation. It can relieve non-uniform density distribution to a certain extent. Extensive experiments on four remote sensing counting datasets demonstrate the effectiveness of the proposed method and the superiority of it compared with state-of-the-arts. Additionally, experiments extended on four commonly used crowd counting datasets further validate the generalization ability of the model. Code is available at https://github.com/gaoguangshuai/PSGCNet.



### Efficient Kernel based Matched Filter Approach for Segmentation of Retinal Blood Vessels
- **Arxiv ID**: http://arxiv.org/abs/2012.03601v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2012.03601v1)
- **Published**: 2020-12-07 11:41:00+00:00
- **Updated**: 2020-12-07 11:41:00+00:00
- **Authors**: Sushil Kumar Saroj, Vikas Ratna, Rakesh Kumar, Nagendra Pratap Singh
- **Comment**: None
- **Journal**: None
- **Summary**: Retinal blood vessels structure contains information about diseases like obesity, diabetes, hypertension and glaucoma. This information is very useful in identification and treatment of these fatal diseases. To obtain this information, there is need to segment these retinal vessels. Many kernel based methods have been given for segmentation of retinal vessels but their kernels are not appropriate to vessel profile cause poor performance. To overcome this, a new and efficient kernel based matched filter approach has been proposed. The new matched filter is used to generate the matched filter response (MFR) image. We have applied Otsu thresholding method on obtained MFR image to extract the vessels. We have conducted extensive experiments to choose best value of parameters for the proposed matched filter kernel. The proposed approach has examined and validated on two online available DRIVE and STARE datasets. The proposed approach has specificity 98.50%, 98.23% and accuracy 95.77 %, 95.13% for DRIVE and STARE dataset respectively. Obtained results confirm that the proposed method has better performance than others. The reason behind increased performance is due to appropriate proposed kernel which matches retinal blood vessel profile more accurately.



### Ada-Segment: Automated Multi-loss Adaptation for Panoptic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2012.03603v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.03603v1)
- **Published**: 2020-12-07 11:43:10+00:00
- **Updated**: 2020-12-07 11:43:10+00:00
- **Authors**: Gengwei Zhang, Yiming Gao, Hang Xu, Hao Zhang, Zhenguo Li, Xiaodan Liang
- **Comment**: Accepted by AAAI2021
- **Journal**: None
- **Summary**: Panoptic segmentation that unifies instance segmentation and semantic segmentation has recently attracted increasing attention. While most existing methods focus on designing novel architectures, we steer toward a different perspective: performing automated multi-loss adaptation (named Ada-Segment) on the fly to flexibly adjust multiple training losses over the course of training using a controller trained to capture the learning dynamics. This offers a few advantages: it bypasses manual tuning of the sensitive loss combination, a decisive factor for panoptic segmentation; it allows to explicitly model the learning dynamics, and reconcile the learning of multiple objectives (up to ten in our experiments); with an end-to-end architecture, it generalizes to different datasets without the need of re-tuning hyperparameters or re-adjusting the training process laboriously. Our Ada-Segment brings 2.7% panoptic quality (PQ) improvement on COCO val split from the vanilla baseline, achieving the state-of-the-art 48.5% PQ on COCO test-dev split and 32.9% PQ on ADE20K dataset. The extensive ablation studies reveal the ever-changing dynamics throughout the training process, necessitating the incorporation of an automated and adaptive learning strategy as presented in this paper.



### Noise2Kernel: Adaptive Self-Supervised Blind Denoising using a Dilated Convolutional Kernel Architecture
- **Arxiv ID**: http://arxiv.org/abs/2012.03623v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2012.03623v1)
- **Published**: 2020-12-07 12:13:17+00:00
- **Updated**: 2020-12-07 12:13:17+00:00
- **Authors**: Kanggeun Lee, Won-Ki Jeong
- **Comment**: None
- **Journal**: None
- **Summary**: With the advent of recent advances in unsupervised learning, efficient training of a deep network for image denoising without pairs of noisy and clean images has become feasible. However, most current unsupervised denoising methods are built on the assumption of zero-mean noise under the signal-independent condition. This assumption causes blind denoising techniques to suffer brightness shifting problems on images that are greatly corrupted by extreme noise such as salt-and-pepper noise. Moreover, most blind denoising methods require a random masking scheme for training to ensure the invariance of the denoising process. In this paper, we propose a dilated convolutional network that satisfies an invariant property, allowing efficient kernel-based training without random masking. We also propose an adaptive self-supervision loss to circumvent the requirement of zero-mean constraint, which is specifically effective in removing salt-and-pepper or hybrid noise where a prior knowledge of noise statistics is not readily available. We demonstrate the efficacy of the proposed method by comparing it with state-of-the-art denoising methods using various examples.



### Sparsity-driven Digital Terrain Model Extraction
- **Arxiv ID**: http://arxiv.org/abs/2012.08639v1
- **DOI**: 10.1109/IGARSS.2018.8517569
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.08639v1)
- **Published**: 2020-12-07 12:29:01+00:00
- **Updated**: 2020-12-07 12:29:01+00:00
- **Authors**: Fatih Nar, Erdal Yilmaz, Gustau Camps-Valls
- **Comment**: Preprint. Paper published in IGARSS 2018 - 2018 IEEE International
  Geoscience and Remote Sensing Symposium, Valencia, 2018, pp. 1316-1319
- **Journal**: None
- **Summary**: We here introduce an automatic Digital Terrain Model (DTM) extraction method. The proposed sparsity-driven DTM extractor (SD-DTM) takes a high-resolution Digital Surface Model (DSM) as an input and constructs a high-resolution DTM using the variational framework. To obtain an accurate DTM, an iterative approach is proposed for the minimization of the target variational cost function. Accuracy of the SD-DTM is shown in a real-world DSM data set. We show the efficiency and effectiveness of the approach both visually and quantitatively via residual plots in illustrative terrain types.



### Confidence-aware Non-repetitive Multimodal Transformers for TextCaps
- **Arxiv ID**: http://arxiv.org/abs/2012.03662v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2012.03662v3)
- **Published**: 2020-12-07 13:20:12+00:00
- **Updated**: 2021-03-21 14:28:04+00:00
- **Authors**: Zhaokai Wang, Renda Bao, Qi Wu, Si Liu
- **Comment**: 9 pages; Accepted by AAAI 2021
- **Journal**: None
- **Summary**: When describing an image, reading text in the visual scene is crucial to understand the key information. Recent work explores the TextCaps task, i.e. image captioning with reading Optical Character Recognition (OCR) tokens, which requires models to read text and cover them in generated captions. Existing approaches fail to generate accurate descriptions because of their (1) poor reading ability; (2) inability to choose the crucial words among all extracted OCR tokens; (3) repetition of words in predicted captions. To this end, we propose a Confidence-aware Non-repetitive Multimodal Transformers (CNMT) to tackle the above challenges. Our CNMT consists of a reading, a reasoning and a generation modules, in which Reading Module employs better OCR systems to enhance text reading ability and a confidence embedding to select the most noteworthy tokens. To address the issue of word redundancy in captions, our Generation Module includes a repetition mask to avoid predicting repeated word in captions. Our model outperforms state-of-the-art models on TextCaps dataset, improving from 81.0 to 93.0 in CIDEr. Our source code is publicly available.



### Machine Learning Information Fusion in Earth Observation: A Comprehensive Review of Methods, Applications and Data Sources
- **Arxiv ID**: http://arxiv.org/abs/2012.05795v1
- **DOI**: 10.1016/j.inffus.2020.07.004
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2012.05795v1)
- **Published**: 2020-12-07 13:35:08+00:00
- **Updated**: 2020-12-07 13:35:08+00:00
- **Authors**: S. Salcedo-Sanz, P. Ghamisi, M. Piles, M. Werner, L. Cuadra, A. Moreno-Martínez, E. Izquierdo-Verdiguier, J. Muñoz-Marí, Amirhosein Mosavi, G. Camps-Valls
- **Comment**: None
- **Journal**: Information Fusion, Volume 63, November 2020, Pages 256-272
- **Summary**: This paper reviews the most important information fusion data-driven algorithms based on Machine Learning (ML) techniques for problems in Earth observation. Nowadays we observe and model the Earth with a wealth of observations, from a plethora of different sensors, measuring states, fluxes, processes and variables, at unprecedented spatial and temporal resolutions. Earth observation is well equipped with remote sensing systems, mounted on satellites and airborne platforms, but it also involves in-situ observations, numerical models and social media data streams, among other data sources. Data-driven approaches, and ML techniques in particular, are the natural choice to extract significant information from this data deluge. This paper produces a thorough review of the latest work on information fusion for Earth observation, with a practical intention, not only focusing on describing the most relevant previous works in the field, but also the most important Earth observation applications where ML information fusion has obtained significant results. We also review some of the most currently used data sets, models and sources for Earth observation problems, describing their importance and how to obtain the data when needed. Finally, we illustrate the application of ML data fusion with a representative set of case studies, as well as we discuss and outlook the near future of the field.



### An Enriched Automated PV Registry: Combining Image Recognition and 3D Building Data
- **Arxiv ID**: http://arxiv.org/abs/2012.03690v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2012.03690v1)
- **Published**: 2020-12-07 13:45:08+00:00
- **Updated**: 2020-12-07 13:45:08+00:00
- **Authors**: Benjamin Rausch, Kevin Mayer, Marie-Louise Arlt, Gunther Gust, Philipp Staudt, Christof Weinhardt, Dirk Neumann, Ram Rajagopal
- **Comment**: Tackling Climate Change with Machine Learning at NeurIPS 2020
  (Spotlight talk)
- **Journal**: None
- **Summary**: While photovoltaic (PV) systems are installed at an unprecedented rate, reliable information on an installation level remains scarce. As a result, automatically created PV registries are a timely contribution to optimize grid planning and operations. This paper demonstrates how aerial imagery and three-dimensional building data can be combined to create an address-level PV registry, specifying area, tilt, and orientation angles. We demonstrate the benefits of this approach for PV capacity estimation. In addition, this work presents, for the first time, a comparison between automated and officially-created PV registries. Our results indicate that our enriched automated registry proves to be useful to validate, update, and complement official registries.



### Unsupervised Pre-training for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/2012.03753v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.03753v2)
- **Published**: 2020-12-07 14:48:26+00:00
- **Updated**: 2021-04-25 04:51:41+00:00
- **Authors**: Dengpan Fu, Dongdong Chen, Jianmin Bao, Hao Yang, Lu Yuan, Lei Zhang, Houqiang Li, Dong Chen
- **Comment**: To appear at CVPR 2021
- **Journal**: None
- **Summary**: In this paper, we present a large scale unlabeled person re-identification (Re-ID) dataset "LUPerson" and make the first attempt of performing unsupervised pre-training for improving the generalization ability of the learned person Re-ID feature representation. This is to address the problem that all existing person Re-ID datasets are all of limited scale due to the costly effort required for data annotation. Previous research tries to leverage models pre-trained on ImageNet to mitigate the shortage of person Re-ID data but suffers from the large domain gap between ImageNet and person Re-ID data. LUPerson is an unlabeled dataset of 4M images of over 200K identities, which is 30X larger than the largest existing Re-ID dataset. It also covers a much diverse range of capturing environments (eg, camera settings, scenes, etc.). Based on this dataset, we systematically study the key factors for learning Re-ID features from two perspectives: data augmentation and contrastive loss. Unsupervised pre-training performed on this large-scale dataset effectively leads to a generic Re-ID feature that can benefit all existing person Re-ID methods. Using our pre-trained model in some basic frameworks, our methods achieve state-of-the-art results without bells and whistles on four widely used Re-ID datasets: CUHK03, Market1501, DukeMTMC, and MSMT17. Our results also show that the performance improvement is more significant on small-scale target datasets or under few-shot setting.



### Sparse Single Sweep LiDAR Point Cloud Segmentation via Learning Contextual Shape Priors from Scene Completion
- **Arxiv ID**: http://arxiv.org/abs/2012.03762v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.03762v1)
- **Published**: 2020-12-07 14:58:25+00:00
- **Updated**: 2020-12-07 14:58:25+00:00
- **Authors**: Xu Yan, Jiantao Gao, Jie Li, Ruimao Zhang, Zhen Li, Rui Huang, Shuguang Cui
- **Comment**: To appear in AAAI 2021. Codes are available at
  https://github.com/yanx27/JS3C-Net
- **Journal**: None
- **Summary**: LiDAR point cloud analysis is a core task for 3D computer vision, especially for autonomous driving. However, due to the severe sparsity and noise interference in the single sweep LiDAR point cloud, the accurate semantic segmentation is non-trivial to achieve. In this paper, we propose a novel sparse LiDAR point cloud semantic segmentation framework assisted by learned contextual shape priors. In practice, an initial semantic segmentation (SS) of a single sweep point cloud can be achieved by any appealing network and then flows into the semantic scene completion (SSC) module as the input. By merging multiple frames in the LiDAR sequence as supervision, the optimized SSC module has learned the contextual shape priors from sequential LiDAR data, completing the sparse single sweep point cloud to the dense one. Thus, it inherently improves SS optimization through fully end-to-end training. Besides, a Point-Voxel Interaction (PVI) module is proposed to further enhance the knowledge fusion between SS and SSC tasks, i.e., promoting the interaction of incomplete local geometry of point cloud and complete voxel-wise global structure. Furthermore, the auxiliary SSC and PVI modules can be discarded during inference without extra burden for SS. Extensive experiments confirm that our JS3C-Net achieves superior performance on both SemanticKITTI and SemanticPOSS benchmarks, i.e., 4% and 3% improvement correspondingly.



### Learning Tactile Models for Factor Graph-based Estimation
- **Arxiv ID**: http://arxiv.org/abs/2012.03768v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2012.03768v2)
- **Published**: 2020-12-07 15:09:31+00:00
- **Updated**: 2021-03-28 19:34:52+00:00
- **Authors**: Paloma Sodhi, Michael Kaess, Mustafa Mukadam, Stuart Anderson
- **Comment**: Accepted to ICRA 2021
- **Journal**: None
- **Summary**: We're interested in the problem of estimating object states from touch during manipulation under occlusions. In this work, we address the problem of estimating object poses from touch during planar pushing. Vision-based tactile sensors provide rich, local image measurements at the point of contact. A single such measurement, however, contains limited information and multiple measurements are needed to infer latent object state. We solve this inference problem using a factor graph. In order to incorporate tactile measurements in the graph, we need local observation models that can map high-dimensional tactile images onto a low-dimensional state space. Prior work has used low-dimensional force measurements or engineered functions to interpret tactile measurements. These methods, however, can be brittle and difficult to scale across objects and sensors. Our key insight is to directly learn tactile observation models that predict the relative pose of the sensor given a pair of tactile images. These relative poses can then be incorporated as factors within a factor graph. We propose a two-stage approach: first we learn local tactile observation models supervised with ground truth data, and then integrate these models along with physics and geometric factors within a factor graph optimizer. We demonstrate reliable object tracking using only tactile feedback for 150 real-world planar pushing sequences with varying trajectories across three object shapes. Supplementary video: https://youtu.be/y1kBfSmi8w0



### Pose-Guided Human Animation from a Single Image in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2012.03796v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.03796v2)
- **Published**: 2020-12-07 15:38:29+00:00
- **Updated**: 2021-11-22 02:24:58+00:00
- **Authors**: Jae Shin Yoon, Lingjie Liu, Vladislav Golyanik, Kripasindhu Sarkar, Hyun Soo Park, Christian Theobalt
- **Comment**: 14 pages including Appendix
- **Journal**: None
- **Summary**: We present a new pose transfer method for synthesizing a human animation from a single image of a person controlled by a sequence of body poses. Existing pose transfer methods exhibit significant visual artifacts when applying to a novel scene, resulting in temporal inconsistency and failures in preserving the identity and textures of the person. To address these limitations, we design a compositional neural network that predicts the silhouette, garment labels, and textures. Each modular network is explicitly dedicated to a subtask that can be learned from the synthetic data. At the inference time, we utilize the trained network to produce a unified representation of appearance and its labels in UV coordinates, which remains constant across poses. The unified representation provides an incomplete yet strong guidance to generating the appearance in response to the pose change. We use the trained network to complete the appearance and render it with the background. With these strategies, we are able to synthesize human animations that can preserve the identity and appearance of the person in a temporally coherent way without any fine-tuning of the network on the testing scene. Experiments show that our method outperforms the state-of-the-arts in terms of synthesis quality, temporal coherence, and generalization ability.



### Perspectives on Sim2Real Transfer for Robotics: A Summary of the R:SS 2020 Workshop
- **Arxiv ID**: http://arxiv.org/abs/2012.03806v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2012.03806v1)
- **Published**: 2020-12-07 15:48:26+00:00
- **Updated**: 2020-12-07 15:48:26+00:00
- **Authors**: Sebastian Höfer, Kostas Bekris, Ankur Handa, Juan Camilo Gamboa, Florian Golemo, Melissa Mozifian, Chris Atkeson, Dieter Fox, Ken Goldberg, John Leonard, C. Karen Liu, Jan Peters, Shuran Song, Peter Welinder, Martha White
- **Comment**: Summary of the "2nd Workshop on Closing the Reality Gap in Sim2Real
  Transfer for Robotics" held in conjunction with "Robotics: Science and System
  2020". Website: https://sim2real.github.io/
- **Journal**: None
- **Summary**: This report presents the debates, posters, and discussions of the Sim2Real workshop held in conjunction with the 2020 edition of the "Robotics: Science and System" conference. Twelve leaders of the field took competing debate positions on the definition, viability, and importance of transferring skills from simulation to the real world in the context of robotics problems. The debaters also joined a large panel discussion, answering audience questions and outlining the future of Sim2Real in robotics. Furthermore, we invited extended abstracts to this workshop which are summarized in this report. Based on the workshop, this report concludes with directions for practitioners exploiting this technology and for researchers further exploring open problems in this area.



### Self-supervised asymmetric deep hashing with margin-scalable constraint
- **Arxiv ID**: http://arxiv.org/abs/2012.03820v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.03820v3)
- **Published**: 2020-12-07 16:09:37+00:00
- **Updated**: 2021-07-23 15:46:37+00:00
- **Authors**: Zhengyang Yu, Song Wu, Zhihao Dou, Erwin M. Bakker
- **Comment**: None
- **Journal**: None
- **Summary**: Due to its effectivity and efficiency, deep hashing approaches are widely used for large-scale visual search. However, it is still challenging to produce compact and discriminative hash codes for images associated with multiple semantics for two main reasons, 1) similarity constraints designed in most of the existing methods are based upon an oversimplified similarity assignment(i.e., 0 for instance pairs sharing no label, 1 for instance pairs sharing at least 1 label), 2) the exploration in multi-semantic relevance are insufficient or even neglected in many of the existing methods. These problems significantly limit the discrimination of generated hash codes. In this paper, we propose a novel self-supervised asymmetric deep hashing method with a margin-scalable constraint(SADH) approach to cope with these problems. SADH implements a self-supervised network to sufficiently preserve semantic information in a semantic feature dictionary and a semantic code dictionary for the semantics of the given dataset, which efficiently and precisely guides a feature learning network to preserve multilabel semantic information using an asymmetric learning strategy. By further exploiting semantic dictionaries, a new margin-scalable constraint is employed for both precise similarity searching and robust hash code generation. Extensive empirical research on four popular benchmarks validates the proposed method and shows it outperforms several state-of-the-art approaches.



### The Role of Regularization in Shaping Weight and Node Pruning Dependency and Dynamics
- **Arxiv ID**: http://arxiv.org/abs/2012.03827v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2012.03827v1)
- **Published**: 2020-12-07 16:22:20+00:00
- **Updated**: 2020-12-07 16:22:20+00:00
- **Authors**: Yael Ben-Guigui, Jacob Goldberger, Tammy Riklin-Raviv
- **Comment**: None
- **Journal**: None
- **Summary**: The pressing need to reduce the capacity of deep neural networks has stimulated the development of network dilution methods and their analysis. While the ability of $L_1$ and $L_0$ regularization to encourage sparsity is often mentioned, $L_2$ regularization is seldom discussed in this context. We present a novel framework for weight pruning by sampling from a probability function that favors the zeroing of smaller weights. In addition, we examine the contribution of $L_1$ and $L_2$ regularization to the dynamics of node pruning while optimizing for weight pruning. We then demonstrate the effectiveness of the proposed stochastic framework when used together with a weight decay regularizer on popular classification models in removing 50% of the nodes in an MLP for MNIST classification, 60% of the filters in VGG-16 for CIFAR10 classification, and on medical image models in removing 60% of the channels in a U-Net for instance segmentation and 50% of the channels in CNN model for COVID-19 detection. For these node-pruned networks, we also present competitive weight pruning results that are only slightly less accurate than the original, dense networks.



### Estimating Crop Primary Productivity with Sentinel-2 and Landsat 8 using Machine Learning Methods Trained with Radiative Transfer Simulations
- **Arxiv ID**: http://arxiv.org/abs/2012.12101v1
- **DOI**: 10.1016/j.rse.2019.03.002
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.12101v1)
- **Published**: 2020-12-07 16:23:13+00:00
- **Updated**: 2020-12-07 16:23:13+00:00
- **Authors**: Aleksandra Wolanin, Gustau Camps-Valls, Luis Gómez-Chova, Gonzalo Mateo-García, Christiaan van der Tol, Yongguang Zhang, Luis Guanter
- **Comment**: None
- **Journal**: None
- **Summary**: Satellite remote sensing has been widely used in the last decades for agricultural applications, {both for assessing vegetation condition and for subsequent yield prediction.} Existing remote sensing-based methods to estimate gross primary productivity (GPP), which is an important variable to indicate crop photosynthetic function and stress, typically rely on empirical or semi-empirical approaches, which tend to over-simplify photosynthetic mechanisms. In this work, we take advantage of all parallel developments in mechanistic photosynthesis modeling and satellite data availability for advanced monitoring of crop productivity. In particular, we combine process-based modeling with the soil-canopy energy balance radiative transfer model (SCOPE) with Sentinel-2 {and Landsat 8} optical remote sensing data and machine learning methods in order to estimate crop GPP. Our model successfully estimates GPP across a variety of C3 crop types and environmental conditions even though it does not use any local information from the corresponding sites. This highlights its potential to map crop productivity from new satellite sensors at a global scale with the help of current Earth observation cloud computing platforms.



### MERANet: Facial Micro-Expression Recognition using 3D Residual Attention Network
- **Arxiv ID**: http://arxiv.org/abs/2012.04581v2
- **DOI**: 10.1145/3490035.3490260
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.04581v2)
- **Published**: 2020-12-07 16:41:42+00:00
- **Updated**: 2022-01-23 05:45:59+00:00
- **Authors**: Viswanatha Reddy Gajjala, Sai Prasanna Teja Reddy, Snehasis Mukherjee, Shiv Ram Dubey
- **Comment**: Published in Twelfth Indian Conference on Computer Vision, Graphics
  and Image Processing (ICVGIP), 2021
- **Journal**: None
- **Summary**: Micro-expression has emerged as a promising modality in affective computing due to its high objectivity in emotion detection. Despite the higher recognition accuracy provided by the deep learning models, there are still significant scope for improvements in micro-expression recognition techniques. The presence of micro-expressions in small-local regions of the face, as well as the limited size of available databases, continue to limit the accuracy in recognizing micro-expressions. In this work, we propose a facial micro-expression recognition model using 3D residual attention network named MERANet to tackle such challenges. The proposed model takes advantage of spatial-temporal attention and channel attention together, to learn deeper fine-grained subtle features for classification of emotions. Further, the proposed model encompasses both spatial and temporal information simultaneously using the 3D kernels and residual connections. Moreover, the channel features and spatio-temporal features are re-calibrated using the channel and spatio-temporal attentions, respectively in each residual module. Our attention mechanism enables the model to learn to focus on different facial areas of interest. The experiments are conducted on benchmark facial micro-expression datasets. A superior performance is observed as compared to the state-of-the-art for facial micro-expression recognition on benchmark data.



### CycleQSM: Unsupervised QSM Deep Learning using Physics-Informed CycleGAN
- **Arxiv ID**: http://arxiv.org/abs/2012.03842v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2012.03842v1)
- **Published**: 2020-12-07 16:46:15+00:00
- **Updated**: 2020-12-07 16:46:15+00:00
- **Authors**: Gyutaek Oh, Hyokyoung Bae, Hyun-Seo Ahn, Sung-Hong Park, Jong Chul Ye
- **Comment**: None
- **Journal**: None
- **Summary**: Quantitative susceptibility mapping (QSM) is a useful magnetic resonance imaging (MRI) technique which provides spatial distribution of magnetic susceptibility values of tissues. QSMs can be obtained by deconvolving the dipole kernel from phase images, but the spectral nulls in the dipole kernel make the inversion ill-posed. In recent times, deep learning approaches have shown a comparable QSM reconstruction performance as the classic approaches, despite the fast reconstruction time. Most of the existing deep learning methods are, however, based on supervised learning, so matched pairs of input phase images and the ground-truth maps are needed. Moreover, it was reported that the supervised learning often leads to underestimated QSM values. To address this, here we propose a novel unsupervised QSM deep learning method using physics-informed cycleGAN, which is derived from optimal transport perspective. In contrast to the conventional cycleGAN, our novel cycleGAN has only one generator and one discriminator thanks to the known dipole kernel. Experimental results confirm that the proposed method provides more accurate QSM maps compared to the existing deep learning approaches, and provide competitive performance to the best classical approaches despite the ultra-fast reconstruction.



### Are DNNs fooled by extremely unrecognizable images?
- **Arxiv ID**: http://arxiv.org/abs/2012.03843v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.03843v2)
- **Published**: 2020-12-07 16:47:33+00:00
- **Updated**: 2022-03-26 07:48:32+00:00
- **Authors**: Soichiro Kumano, Hiroshi Kera, Toshihiko Yamasaki
- **Comment**: None
- **Journal**: None
- **Summary**: Fooling images are a potential threat to deep neural networks (DNNs). These images are not recognizable to humans as natural objects, such as dogs and cats, but are misclassified by DNNs as natural-object classes with high confidence scores. Despite their original design concept, existing fooling images retain some features that are characteristic of the target objects if looked into closely. Hence, DNNs can react to these features. In this paper, we address the question of whether there can be fooling images with no characteristic pattern of natural objects locally or globally. As a minimal case, we introduce single-color images with a few pixels altered, called sparse fooling images (SFIs). We first prove that SFIs always exist under mild conditions for linear and nonlinear models and reveal that complex models are more likely to be vulnerable to SFI attacks. With two SFI generation methods, we demonstrate that in deeper layers, SFIs end up with similar features to those of natural images, and consequently, fool DNNs successfully. Among other layers, we discovered that the max pooling layer causes the vulnerability against SFIs. The defense against SFIs and transferability are also discussed. This study highlights the new vulnerability of DNNs by introducing a novel class of images that distributes extremely far from natural images.



### End-to-end Handwritten Paragraph Text Recognition Using a Vertical Attention Network
- **Arxiv ID**: http://arxiv.org/abs/2012.03868v2
- **DOI**: 10.1109/TPAMI.2022.3144899
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.03868v2)
- **Published**: 2020-12-07 17:31:20+00:00
- **Updated**: 2021-12-03 14:30:54+00:00
- **Authors**: Denis Coquenet, Clément Chatelain, Thierry Paquet
- **Comment**: None
- **Journal**: IEEE Transactions on Pattern Analysis and Machine Intelligence
  2022
- **Summary**: Unconstrained handwritten text recognition remains challenging for computer vision systems. Paragraph text recognition is traditionally achieved by two models: the first one for line segmentation and the second one for text line recognition. We propose a unified end-to-end model using hybrid attention to tackle this task. This model is designed to iteratively process a paragraph image line by line. It can be split into three modules. An encoder generates feature maps from the whole paragraph image. Then, an attention module recurrently generates a vertical weighted mask enabling to focus on the current text line features. This way, it performs a kind of implicit line segmentation. For each text line features, a decoder module recognizes the character sequence associated, leading to the recognition of a whole paragraph. We achieve state-of-the-art character error rate at paragraph level on three popular datasets: 1.91% for RIMES, 4.45% for IAM and 3.59% for READ 2016. Our code and trained model weights are available at https://github.com/FactoDeepLearning/VerticalAttentionOCR.



### Traffic flow prediction using Deep Sedenion Networks
- **Arxiv ID**: http://arxiv.org/abs/2012.03874v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.03874v2)
- **Published**: 2020-12-07 17:37:56+00:00
- **Updated**: 2020-12-11 10:31:58+00:00
- **Authors**: Alabi Bojesomo, Panos Liatsis, Hasan Al Marzouqi
- **Comment**: 5 pages, 4 figures, 1 table, 2 equations
- **Journal**: None
- **Summary**: In this paper, we present our solution to the Traffic4cast2020 traffic prediction challenge. In this competition, participants are to predict future traffic parameters (speed and volume) in three different cities: Berlin, Istanbul and Moscow. The information provided includes nine channels where the first eight represent the speed and volume for four different direction of traffic (NE, NW, SE and SW), while the last channel is used to indicate presence of traffic incidents. The expected output should have the first 8 channels of the input at six future timing intervals (5, 10, 15, 30, 45, and 60min), while a one hour duration of past traffic data, in 5mins intervals, are provided as input. We solve the problem using a novel sedenion U-Net neural network. Sedenion networks provide the means for efficient encoding of correlated multimodal datasets. We use 12 of the 15 sedenion imaginary parts for the dynamic inputs and the real sedenion component is used for the static input. The sedenion output of the network is used to represent the multimodal traffic predictions. Proposed system achieved a validation MSE of 1.33e-3 and a test MSE of 1.31e-3.



### IHashNet: Iris Hashing Network based on efficient multi-index hashing
- **Arxiv ID**: http://arxiv.org/abs/2012.03881v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2012.03881v1)
- **Published**: 2020-12-07 17:50:57+00:00
- **Updated**: 2020-12-07 17:50:57+00:00
- **Authors**: Avantika Singh, Chirag Vashist, Pratyush Gaurav, Aditya Nigam, Rameshwar Pratap
- **Comment**: None
- **Journal**: None
- **Summary**: Massive biometric deployments are pervasive in today's world. But despite the high accuracy of biometric systems, their computational efficiency degrades drastically with an increase in the database size. Thus, it is essential to index them. An ideal indexing scheme needs to generate codes that preserve the intra-subject similarity as well as inter-subject dissimilarity. Here, in this paper, we propose an iris indexing scheme using real-valued deep iris features binarized to iris bar codes (IBC) compatible with the indexing structure. Firstly, for extracting robust iris features, we have designed a network utilizing the domain knowledge of ordinal filtering and learning their nonlinear combinations. Later these real-valued features are binarized. Finally, for indexing the iris dataset, we have proposed a loss that can transform the binary feature into an improved feature compatible with the Multi-Index Hashing scheme. This loss function ensures the hamming distance equally distributed among all the contiguous disjoint sub-strings. To the best of our knowledge, this is the first work in the iris indexing domain that presents an end-to-end iris indexing structure. Experimental results on four datasets are presented to depict the efficacy of the proposed approach.



### Online Photometric Calibration of Automatic Gain Thermal Infrared Cameras
- **Arxiv ID**: http://arxiv.org/abs/2012.14292v2
- **DOI**: 10.1109/LRA.2021.3061401
- **Categories**: **eess.IV**, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2012.14292v2)
- **Published**: 2020-12-07 17:51:54+00:00
- **Updated**: 2021-01-11 16:09:21+00:00
- **Authors**: Manash Pratim Das, Larry Matthies, Shreyansh Daftry
- **Comment**: 8 pages, 6 figures, Pre-Print. This work has been submitted to the
  IEEE for possible publication
- **Journal**: None
- **Summary**: Thermal infrared cameras are increasingly being used in various applications such as robot vision, industrial inspection and medical imaging, thanks to their improved resolution and portability. However, the performance of traditional computer vision techniques developed for electro-optical imagery does not directly translate to the thermal domain due to two major reasons: these algorithms require photometric assumptions to hold, and methods for photometric calibration of RGB cameras cannot be applied to thermal-infrared cameras due to difference in data acquisition and sensor phenomenology. In this paper, we take a step in this direction, and introduce a novel algorithm for online photometric calibration of thermal-infrared cameras. Our proposed method does not require any specific driver/hardware support and hence can be applied to any commercial off-the-shelf thermal IR camera. We present this in the context of visual odometry and SLAM algorithms, and demonstrate the efficacy of our proposed system through extensive experiments for both standard benchmark datasets, and real-world field tests with a thermal-infrared camera in natural outdoor environments.



### Using Persistent Homology Topological Features to Characterize Medical Images: Case Studies on Lung and Brain Cancers
- **Arxiv ID**: http://arxiv.org/abs/2012.12102v4
- **DOI**: None
- **Categories**: **cs.CV**, stat.AP, stat.ME
- **Links**: [PDF](http://arxiv.org/pdf/2012.12102v4)
- **Published**: 2020-12-07 18:16:59+00:00
- **Updated**: 2023-02-28 04:56:21+00:00
- **Authors**: Chul Moon, Qiwei Li, Guanghua Xiao
- **Comment**: None
- **Journal**: None
- **Summary**: Tumor shape is a key factor that affects tumor growth and metastasis. This paper proposes a topological feature computed by persistent homology to characterize tumor progression from digital pathology and radiology images and examines its effect on the time-to-event data. The proposed topological features are invariant to scale-preserving transformation and can summarize various tumor shape patterns. The topological features are represented in functional space and used as functional predictors in a functional Cox proportional hazards model. The proposed model enables interpretable inference about the association between topological shape features and survival risks. Two case studies are conducted using consecutive 133 lung cancer and 77 brain tumor patients. The results of both studies show that the topological features predict survival prognosis after adjusting clinical variables, and the predicted high-risk groups have worse survival outcomes than the low-risk groups. Also, the topological shape features found to be positively associated with survival hazards are irregular and heterogeneous shape patterns, which are known to be related to tumor progression.



### Model Compression Using Optimal Transport
- **Arxiv ID**: http://arxiv.org/abs/2012.03907v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2012.03907v1)
- **Published**: 2020-12-07 18:35:33+00:00
- **Updated**: 2020-12-07 18:35:33+00:00
- **Authors**: Suhas Lohit, Michael Jones
- **Comment**: None
- **Journal**: None
- **Summary**: Model compression methods are important to allow for easier deployment of deep learning models in compute, memory and energy-constrained environments such as mobile phones. Knowledge distillation is a class of model compression algorithm where knowledge from a large teacher network is transferred to a smaller student network thereby improving the student's performance. In this paper, we show how optimal transport-based loss functions can be used for training a student network which encourages learning student network parameters that help bring the distribution of student features closer to that of the teacher features. We present image classification results on CIFAR-100, SVHN and ImageNet and show that the proposed optimal transport loss functions perform comparably to or better than other loss functions.



### Learning Video Instance Segmentation with Recurrent Graph Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2012.03911v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.03911v1)
- **Published**: 2020-12-07 18:41:35+00:00
- **Updated**: 2020-12-07 18:41:35+00:00
- **Authors**: Joakim Johnander, Emil Brissman, Martin Danelljan, Michael Felsberg
- **Comment**: None
- **Journal**: None
- **Summary**: Most existing approaches to video instance segmentation comprise multiple modules that are heuristically combined to produce the final output. Formulating a purely learning-based method instead, which models both the temporal aspect as well as a generic track management required to solve the video instance segmentation task, is a highly challenging problem. In this work, we propose a novel learning formulation, where the entire video instance segmentation problem is modelled jointly. We fit a flexible model to our formulation that, with the help of a graph neural network, processes all available new information in each frame. Past information is considered and processed via a recurrent connection. We demonstrate the effectiveness of the proposed approach in comprehensive experiments. Our approach, operating at over 25 FPS, outperforms previous video real-time methods. We further conduct detailed ablative experiments that validate the different aspects of our approach.



### MultiON: Benchmarking Semantic Map Memory using Multi-Object Navigation
- **Arxiv ID**: http://arxiv.org/abs/2012.03912v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2012.03912v1)
- **Published**: 2020-12-07 18:42:38+00:00
- **Updated**: 2020-12-07 18:42:38+00:00
- **Authors**: Saim Wani, Shivansh Patel, Unnat Jain, Angel X. Chang, Manolis Savva
- **Comment**: Project page: https://shivanshpatel35.github.io/multi-ON/ ; the first
  three authors contributed equally
- **Journal**: None
- **Summary**: Navigation tasks in photorealistic 3D environments are challenging because they require perception and effective planning under partial observability. Recent work shows that map-like memory is useful for long-horizon navigation tasks. However, a focused investigation of the impact of maps on navigation tasks of varying complexity has not yet been performed. We propose the multiON task, which requires navigation to an episode-specific sequence of objects in a realistic environment. MultiON generalizes the ObjectGoal navigation task and explicitly tests the ability of navigation agents to locate previously observed goal objects. We perform a set of multiON experiments to examine how a variety of agent models perform across a spectrum of navigation task complexities. Our experiments show that: i) navigation performance degrades dramatically with escalating task complexity; ii) a simple semantic map agent performs surprisingly well relative to more complex neural image feature map agents; and iii) even oracle map agents achieve relatively low performance, indicating the potential for future work in training embodied navigation agents using maps. Video summary: https://youtu.be/yqTlHNIcgnY



### NeRD: Neural Reflectance Decomposition from Image Collections
- **Arxiv ID**: http://arxiv.org/abs/2012.03918v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2012.03918v4)
- **Published**: 2020-12-07 18:45:57+00:00
- **Updated**: 2021-08-26 15:49:03+00:00
- **Authors**: Mark Boss, Raphael Braun, Varun Jampani, Jonathan T. Barron, Ce Liu, Hendrik P. A. Lensch
- **Comment**: Accepted at ICCV 2021
- **Journal**: None
- **Summary**: Decomposing a scene into its shape, reflectance, and illumination is a challenging but important problem in computer vision and graphics. This problem is inherently more challenging when the illumination is not a single light source under laboratory conditions but is instead an unconstrained environmental illumination. Though recent work has shown that implicit representations can be used to model the radiance field of an object, most of these techniques only enable view synthesis and not relighting. Additionally, evaluating these radiance fields is resource and time-intensive. We propose a neural reflectance decomposition (NeRD) technique that uses physically-based rendering to decompose the scene into spatially varying BRDF material properties. In contrast to existing techniques, our input images can be captured under different illumination conditions. In addition, we also propose techniques to convert the learned reflectance volume into a relightable textured mesh enabling fast real-time rendering with novel illuminations. We demonstrate the potential of the proposed approach with experiments on both synthetic and real datasets, where we are able to obtain high-quality relightable 3D assets from image collections. The datasets and code is available on the project page: https://markboss.me/publication/2021-nerd/



### NeRV: Neural Reflectance and Visibility Fields for Relighting and View Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2012.03927v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2012.03927v1)
- **Published**: 2020-12-07 18:56:08+00:00
- **Updated**: 2020-12-07 18:56:08+00:00
- **Authors**: Pratul P. Srinivasan, Boyang Deng, Xiuming Zhang, Matthew Tancik, Ben Mildenhall, Jonathan T. Barron
- **Comment**: Project page: https://people.eecs.berkeley.edu/~pratul/nerv
- **Journal**: None
- **Summary**: We present a method that takes as input a set of images of a scene illuminated by unconstrained known lighting, and produces as output a 3D representation that can be rendered from novel viewpoints under arbitrary lighting conditions. Our method represents the scene as a continuous volumetric function parameterized as MLPs whose inputs are a 3D location and whose outputs are the following scene properties at that input location: volume density, surface normal, material parameters, distance to the first surface intersection in any direction, and visibility of the external environment in any direction. Together, these allow us to render novel views of the object under arbitrary lighting, including indirect illumination effects. The predicted visibility and surface intersection fields are critical to our model's ability to simulate direct and indirect illumination during training, because the brute-force techniques used by prior work are intractable for lighting conditions outside of controlled setups with a single light. Our method outperforms alternative approaches for recovering relightable 3D scene representations, and performs well in complex lighting settings that have posed a significant challenge to prior work.



### Identity-Driven DeepFake Detection
- **Arxiv ID**: http://arxiv.org/abs/2012.03930v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.03930v1)
- **Published**: 2020-12-07 18:59:08+00:00
- **Updated**: 2020-12-07 18:59:08+00:00
- **Authors**: Xiaoyi Dong, Jianmin Bao, Dongdong Chen, Weiming Zhang, Nenghai Yu, Dong Chen, Fang Wen, Baining Guo
- **Comment**: None
- **Journal**: None
- **Summary**: DeepFake detection has so far been dominated by ``artifact-driven'' methods and the detection performance significantly degrades when either the type of image artifacts is unknown or the artifacts are simply too hard to find. In this work, we present an alternative approach: Identity-Driven DeepFake Detection. Our approach takes as input the suspect image/video as well as the target identity information (a reference image or video). We output a decision on whether the identity in the suspect image/video is the same as the target identity. Our motivation is to prevent the most common and harmful DeepFakes that spread false information of a targeted person. The identity-based approach is fundamentally different in that it does not attempt to detect image artifacts. Instead, it focuses on whether the identity in the suspect image/video is true. To facilitate research on identity-based detection, we present a new large scale dataset ``Vox-DeepFake", in which each suspect content is associated with multiple reference images collected from videos of a target identity. We also present a simple identity-based detection algorithm called the OuterFace, which may serve as a baseline for further research. Even trained without fake videos, the OuterFace algorithm achieves superior detection accuracy and generalizes well to different DeepFake methods, and is robust with respect to video degradation techniques -- a performance not achievable with existing detection algorithms.



### GenScan: A Generative Method for Populating Parametric 3D Scan Datasets
- **Arxiv ID**: http://arxiv.org/abs/2012.03998v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.03998v1)
- **Published**: 2020-12-07 19:09:38+00:00
- **Updated**: 2020-12-07 19:09:38+00:00
- **Authors**: Mohammad Keshavarzi, Oladapo Afolabi, Luisa Caldas, Allen Y. Yang, Avideh Zakhor
- **Comment**: None
- **Journal**: None
- **Summary**: The availability of rich 3D datasets corresponding to the geometrical complexity of the built environments is considered an ongoing challenge for 3D deep learning methodologies. To address this challenge, we introduce GenScan, a generative system that populates synthetic 3D scan datasets in a parametric fashion. The system takes an existing captured 3D scan as an input and outputs alternative variations of the building layout including walls, doors, and furniture with corresponding textures. GenScan is a fully automated system that can also be manually controlled by a user through an assigned user interface. Our proposed system utilizes a combination of a hybrid deep neural network and a parametrizer module to extract and transform elements of a given 3D scan. GenScan takes advantage of style transfer techniques to generate new textures for the generated scenes. We believe our system would facilitate data augmentation to expand the currently limited 3D geometry datasets commonly used in 3D computer vision, generative design, and general 3D deep learning tasks.



### Learning an Animatable Detailed 3D Face Model from In-The-Wild Images
- **Arxiv ID**: http://arxiv.org/abs/2012.04012v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.04012v2)
- **Published**: 2020-12-07 19:30:45+00:00
- **Updated**: 2021-06-02 17:52:42+00:00
- **Authors**: Yao Feng, Haiwen Feng, Michael J. Black, Timo Bolkart
- **Comment**: SIGGRAPH 2021
- **Journal**: ACM Transactions on Graphics (ToG), Vol. 40, No. 4, Article 88.
  Publication date: August 2021
- **Summary**: While current monocular 3D face reconstruction methods can recover fine geometric details, they suffer several limitations. Some methods produce faces that cannot be realistically animated because they do not model how wrinkles vary with expression. Other methods are trained on high-quality face scans and do not generalize well to in-the-wild images. We present the first approach that regresses 3D face shape and animatable details that are specific to an individual but change with expression. Our model, DECA (Detailed Expression Capture and Animation), is trained to robustly produce a UV displacement map from a low-dimensional latent representation that consists of person-specific detail parameters and generic expression parameters, while a regressor is trained to predict detail, shape, albedo, expression, pose and illumination parameters from a single image. To enable this, we introduce a novel detail-consistency loss that disentangles person-specific details from expression-dependent wrinkles. This disentanglement allows us to synthesize realistic person-specific wrinkles by controlling expression parameters while keeping person-specific details unchanged. DECA is learned from in-the-wild images with no paired 3D supervision and achieves state-of-the-art shape reconstruction accuracy on two benchmarks. Qualitative results on in-the-wild data demonstrate DECA's robustness and its ability to disentangle identity- and expression-dependent details enabling animation of reconstructed faces. The model and code are publicly available at https://deca.is.tue.mpg.de.



### Generating unseen complex scenes: are we there yet?
- **Arxiv ID**: http://arxiv.org/abs/2012.04027v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2012.04027v1)
- **Published**: 2020-12-07 20:04:39+00:00
- **Updated**: 2020-12-07 20:04:39+00:00
- **Authors**: Arantxa Casanova, Michal Drozdzal, Adriana Romero-Soriano
- **Comment**: None
- **Journal**: None
- **Summary**: Although recent complex scene conditional generation models generate increasingly appealing scenes, it is very hard to assess which models perform better and why. This is often due to models being trained to fit different data splits, and defining their own experimental setups. In this paper, we propose a methodology to compare complex scene conditional generation models, and provide an in-depth analysis that assesses the ability of each model to (1) fit the training distribution and hence perform well on seen conditionings, (2) to generalize to unseen conditionings composed of seen object combinations, and (3) generalize to unseen conditionings composed of unseen object combinations. As a result, we observe that recent methods are able to generate recognizable scenes given seen conditionings, and exploit compositionality to generalize to unseen conditionings with seen object combinations. However, all methods suffer from noticeable image quality degradation when asked to generate images from conditionings composed of unseen object combinations. Moreover, through our analysis, we identify the advantages of different pipeline components, and find that (1) encouraging compositionality through instance-wise spatial conditioning normalizations increases robustness to both types of unseen conditionings, (2) using semantically aware losses such as the scene-graph perceptual similarity helps improve some dimensions of the generation process, and (3) enhancing the quality of generated masks and the quality of the individual objects are crucial steps to improve robustness to both types of unseen conditionings.



### Rotation-Invariant Point Convolution With Multiple Equivariant Alignments
- **Arxiv ID**: http://arxiv.org/abs/2012.04048v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.04048v1)
- **Published**: 2020-12-07 20:47:46+00:00
- **Updated**: 2020-12-07 20:47:46+00:00
- **Authors**: Hugues Thomas
- **Comment**: 3DV 2020
- **Journal**: None
- **Summary**: Recent attempts at introducing rotation invariance or equivariance in 3D deep learning approaches have shown promising results, but these methods still struggle to reach the performances of standard 3D neural networks. In this work we study the relation between equivariance and invariance in 3D point convolutions. We show that using rotation-equivariant alignments, it is possible to make any convolutional layer rotation-invariant. Furthermore, we improve this simple alignment procedure by using the alignment themselves as features in the convolution, and by combining multiple alignments together. With this core layer, we design rotation-invariant architectures which improve state-of-the-art results in both object classification and semantic segmentation and reduces the gap between rotation-invariant and standard 3D deep learning approaches.



### Semantic and Geometric Modeling with Neural Message Passing in 3D Scene Graphs for Hierarchical Mechanical Search
- **Arxiv ID**: http://arxiv.org/abs/2012.04060v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2012.04060v2)
- **Published**: 2020-12-07 21:04:34+00:00
- **Updated**: 2021-05-23 20:08:59+00:00
- **Authors**: Andrey Kurenkov, Roberto Martín-Martín, Jeff Ichnowski, Ken Goldberg, Silvio Savarese
- **Comment**: None
- **Journal**: None
- **Summary**: Searching for objects in indoor organized environments such as homes or offices is part of our everyday activities. When looking for a target object, we jointly reason about the rooms and containers the object is likely to be in; the same type of container will have a different probability of having the target depending on the room it is in. We also combine geometric and semantic information to infer what container is best to search, or what other objects are best to move, if the target object is hidden from view. We propose to use a 3D scene graph representation to capture the hierarchical, semantic, and geometric aspects of this problem. To exploit this representation in a search process, we introduce Hierarchical Mechanical Search (HMS), a method that guides an agent's actions towards finding a target object specified with a natural language description. HMS is based on a novel neural network architecture that uses neural message passing of vectors with visual, geometric, and linguistic information to allow HMS to reason across layers of the graph while combining semantic and geometric cues. HMS is evaluated on a novel dataset of 500 3D scene graphs with dense placements of semantically related objects in storage locations, and is shown to be significantly better than several baselines at finding objects and close to the oracle policy in terms of the median number of actions required. Additional qualitative results can be found at https://ai.stanford.edu/mech-search/hms.



### A New Window Loss Function for Bone Fracture Detection and Localization in X-ray Images with Point-based Annotation
- **Arxiv ID**: http://arxiv.org/abs/2012.04066v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.04066v2)
- **Published**: 2020-12-07 21:19:04+00:00
- **Updated**: 2021-01-04 15:55:50+00:00
- **Authors**: Xinyu Zhang, Yirui Wang, Chi-Tung Cheng, Le Lu, Adam P. Harrison, Jing Xiao, Chien-Hung Liao, Shun Miao
- **Comment**: Accepted to AAAI-2021
- **Journal**: None
- **Summary**: Object detection methods are widely adopted for computer-aided diagnosis using medical images. Anomalous findings are usually treated as objects that are described by bounding boxes. Yet, many pathological findings, e.g., bone fractures, cannot be clearly defined by bounding boxes, owing to considerable instance, shape and boundary ambiguities. This makes bounding box annotations, and their associated losses, highly ill-suited. In this work, we propose a new bone fracture detection method for X-ray images, based on a labor effective and flexible annotation scheme suitable for abnormal findings with no clear object-level spatial extents or boundaries. Our method employs a simple, intuitive, and informative point-based annotation protocol to mark localized pathology information. To address the uncertainty in the fracture scales annotated via point(s), we convert the annotations into pixel-wise supervision that uses lower and upper bounds with positive, negative, and uncertain regions. A novel Window Loss is subsequently proposed to only penalize the predictions outside of the uncertain regions. Our method has been extensively evaluated on 4410 pelvic X-ray images of unique patients. Experiments demonstrate that our method outperforms previous state-of-the-art image classification and object detection baselines by healthy margins, with an AUROC of 0.983 and FROC score of 89.6%.



### Efficient Nonlinear RX Anomaly Detectors
- **Arxiv ID**: http://arxiv.org/abs/2012.05799v1
- **DOI**: 10.1109/LGRS.2020.2970582
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2012.05799v1)
- **Published**: 2020-12-07 21:57:54+00:00
- **Updated**: 2020-12-07 21:57:54+00:00
- **Authors**: José A. Padrón Hidalgo, Adrián Pérez-Suay, Fatih Nar, Gustau Camps-Valls
- **Comment**: None
- **Journal**: None
- **Summary**: Current anomaly detection algorithms are typically challenged by either accuracy or efficiency. More accurate nonlinear detectors are typically slow and not scalable. In this letter, we propose two families of techniques to improve the efficiency of the standard kernel Reed-Xiaoli (RX) method for anomaly detection by approximating the kernel function with either {\em data-independent} random Fourier features or {\em data-dependent} basis with the Nystr\"om approach. We compare all methods for both real multi- and hyperspectral images. We show that the proposed efficient methods have a lower computational cost and they perform similar (or outperform) the standard kernel RX algorithm thanks to their implicit regularization effect. Last but not least, the Nystr\"om approach has an improved power of detection.



### Deformable Gabor Feature Networks for Biomedical Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2012.04109v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.04109v1)
- **Published**: 2020-12-07 23:25:32+00:00
- **Updated**: 2020-12-07 23:25:32+00:00
- **Authors**: Xuan Gong, Xin Xia, Wentao Zhu, Baochang Zhang, David Doermann, Lian Zhuo
- **Comment**: 9 pages, 6 figures
- **Journal**: None
- **Summary**: In recent years, deep learning has dominated progress in the field of medical image analysis. We find however, that the ability of current deep learning approaches to represent the complex geometric structures of many medical images is insufficient. One limitation is that deep learning models require a tremendous amount of data, and it is very difficult to obtain a sufficient amount with the necessary detail. A second limitation is that there are underlying features of these medical images that are well established, but the black-box nature of existing convolutional neural networks (CNNs) do not allow us to exploit them. In this paper, we revisit Gabor filters and introduce a deformable Gabor convolution (DGConv) to expand deep networks interpretability and enable complex spatial variations. The features are learned at deformable sampling locations with adaptive Gabor convolutions to improve representativeness and robustness to complex objects. The DGConv replaces standard convolutional layers and is easily trained end-to-end, resulting in deformable Gabor feature network (DGFN) with few additional parameters and minimal additional training cost. We introduce DGFN for addressing deep multi-instance multi-label classification on the INbreast dataset for mammograms and on the ChestX-ray14 dataset for pulmonary x-ray images.



### SuperFront: From Low-resolution to High-resolution Frontal Face Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2012.04111v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.04111v1)
- **Published**: 2020-12-07 23:30:28+00:00
- **Updated**: 2020-12-07 23:30:28+00:00
- **Authors**: Yu Yin, Joseph P. Robinson, Songyao Jiang, Yue Bai, Can Qin, Yun Fu
- **Comment**: None
- **Journal**: None
- **Summary**: Advances in face rotation, along with other face-based generative tasks, are more frequent as we advance further in topics of deep learning. Even as impressive milestones are achieved in synthesizing faces, the importance of preserving identity is needed in practice and should not be overlooked. Also, the difficulty should not be more for data with obscured faces, heavier poses, and lower quality. Existing methods tend to focus on samples with variation in pose, but with the assumption data is high in quality. We propose a generative adversarial network (GAN) -based model to generate high-quality, identity preserving frontal faces from one or multiple low-resolution (LR) faces with extreme poses. Specifically, we propose SuperFront-GAN (SF-GAN) to synthesize a high-resolution (HR), frontal face from one-to-many LR faces with various poses and with the identity-preserved. We integrate a super-resolution (SR) side-view module into SF-GAN to preserve identity information and fine details of the side-views in HR space, which helps model reconstruct high-frequency information of faces (i.e., periocular, nose, and mouth regions). Moreover, SF-GAN accepts multiple LR faces as input, and improves each added sample. We squeeze additional gain in performance with an orthogonal constraint in the generator to penalize redundant latent representations and, hence, diversify the learned features space. Quantitative and qualitative results demonstrate the superiority of SF-GAN over others.



### Adaptive Enhancement of Extreme Low-Light Images
- **Arxiv ID**: http://arxiv.org/abs/2012.04112v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2012.04112v3)
- **Published**: 2020-12-07 23:31:59+00:00
- **Updated**: 2023-04-04 14:59:59+00:00
- **Authors**: Evgeny Hershkovitch Neiterman, Michael Klyuchka, Gil Ben-Artzi
- **Comment**: None
- **Journal**: None
- **Summary**: Existing methods for enhancing dark images captured in a very low-light environment assume that the intensity level of the optimal output image is known and already included in the training set. However, this assumption often does not hold, leading to output images that contain visual imperfections such as dark regions or low contrast. To facilitate the training and evaluation of adaptive models that can overcome this limitation, we have created a dataset of 1500 raw images taken in both indoor and outdoor low-light conditions. Based on our dataset, we introduce a deep learning model capable of enhancing input images with a wide range of intensity levels at runtime, including ones that are not seen during training. Our experimental results demonstrate that our proposed dataset combined with our model can consistently and effectively enhance images across a wide range of diverse and challenging scenarios.



