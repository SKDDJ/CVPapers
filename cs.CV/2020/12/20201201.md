# Arxiv Papers in cs.CV on 2020-12-01
### Crowd-Sourced Road Quality Mapping in the Developing World
- **Arxiv ID**: http://arxiv.org/abs/2012.00179v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2012.00179v1)
- **Published**: 2020-12-01 00:10:36+00:00
- **Updated**: 2020-12-01 00:10:36+00:00
- **Authors**: Benjamin Choi, John Kamalu
- **Comment**: Presented at NeurIPS 2020 Workshop on Machine Learning for the
  Developing World
- **Journal**: None
- **Summary**: Road networks are among the most essential components of a country's infrastructure. By facilitating the movement and exchange of goods, people, and ideas, they support economic and cultural activity both within and across borders. Up-to-date mapping of the the geographical distribution of roads and their quality is essential in high-impact applications ranging from land use planning to wilderness conservation. Mapping presents a particularly pressing challenge in developing countries, where documentation is poor and disproportionate amounts of road construction are expected to occur in the coming decades. We present a new crowd-sourced approach capable of assessing road quality and identify key challenges and opportunities in the transferability of deep learning based methods across domains.



### Open Source 3-D Filament Diameter Sensor for Recycling, Winding and Additive Manufacturing Machines
- **Arxiv ID**: http://arxiv.org/abs/2012.00191v1
- **DOI**: 10.1115/1.4050762
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.00191v1)
- **Published**: 2020-12-01 00:54:47+00:00
- **Updated**: 2020-12-01 00:54:47+00:00
- **Authors**: Aliaksei L. Petsiuk, Joshua M. Pearce
- **Comment**: 25 pages, 16 figures, 2 tables
- **Journal**: None
- **Summary**: To overcome the challenge of upcycling plastic waste into 3-D printing filament in the distributed recycling and additive manufacturing systems, this study designs, builds, tests and validates an open source 3-D filament diameter sensor for recycling and winding machines. The modular system for multi-axis optical control of the diameter of the recycled 3-D-printer filament makes it possible to analyze the surface structure of the processed filament, save the history of measurements along the entire length of the spool, as well as mark defective areas. The sensor is developed as an independent module and integrated into a recyclebot. The diameter sensor was tested on different kinds of polymers (ABS, PLA) different sources of plastic (recycled 3-D prints and virgin plastic waste) and different colors including clear plastic. The results of the diameter measurements using the camera were compared with the manual measurements, and the measurements obtained with a one-dimensional digital light caliper. The results found that the developed open source filament sensing method allows users to obtain significantly more information in comparison with basic one-dimensional light sensors and using the received data not only for more accurate diameter measurements, but also for a detailed analysis of the recycled filament surface. The developed method ensures greater availability of plastics recycling technologies for the manufacturing community and stimulates the growth of composite materials creation. The presented system can greatly enhance the user possibilities and serve as a starting point for a complete recycling control system that will regulate motor parameters to achieve the desired filament diameter with acceptable deviations and even control the extrusion rate on a printer to recover from filament irregularities.



### How to fine-tune deep neural networks in few-shot learning?
- **Arxiv ID**: http://arxiv.org/abs/2012.00204v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2012.00204v1)
- **Published**: 2020-12-01 01:20:59+00:00
- **Updated**: 2020-12-01 01:20:59+00:00
- **Authors**: Peng Peng, Jiugen Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning has been widely used in data-intensive applications. However, training a deep neural network often requires a large data set. When there is not enough data available for training, the performance of deep learning models is even worse than that of shallow networks. It has been proved that few-shot learning can generalize to new tasks with few training samples. Fine-tuning of a deep model is simple and effective few-shot learning method. However, how to fine-tune deep learning models (fine-tune convolution layer or BN layer?) still lack deep investigation. Hence, we study how to fine-tune deep models through experimental comparison in this paper. Furthermore, the weight of the models is analyzed to verify the feasibility of the fine-tuning method.



### UPFlow: Upsampling Pyramid for Unsupervised Optical Flow Learning
- **Arxiv ID**: http://arxiv.org/abs/2012.00212v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.00212v2)
- **Published**: 2020-12-01 01:57:46+00:00
- **Updated**: 2021-06-02 05:26:17+00:00
- **Authors**: Kunming Luo, Chuan Wang, Shuaicheng Liu, Haoqiang Fan, Jue Wang, Jian Sun
- **Comment**: None
- **Journal**: None
- **Summary**: We present an unsupervised learning approach for optical flow estimation by improving the upsampling and learning of pyramid network. We design a self-guided upsample module to tackle the interpolation blur problem caused by bilinear upsampling between pyramid levels. Moreover, we propose a pyramid distillation loss to add supervision for intermediate levels via distilling the finest flow as pseudo labels. By integrating these two components together, our method achieves the best performance for unsupervised optical flow learning on multiple leading benchmarks, including MPI-SIntel, KITTI 2012 and KITTI 2015. In particular, we achieve EPE=1.4 on KITTI 2012 and F1=9.38% on KITTI 2015, which outperform the previous state-of-the-art methods by 22.2% and 15.7%, respectively.



### Point2Skeleton: Learning Skeletal Representations from Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2012.00230v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.00230v2)
- **Published**: 2020-12-01 03:04:09+00:00
- **Updated**: 2021-04-07 04:24:20+00:00
- **Authors**: Cheng Lin, Changjian Li, Yuan Liu, Nenglun Chen, Yi-King Choi, Wenping Wang
- **Comment**: Accepted to CVPR2021 (oral). Project:
  https://github.com/clinplayer/Point2Skeleton
- **Journal**: None
- **Summary**: We introduce Point2Skeleton, an unsupervised method to learn skeletal representations from point clouds. Existing skeletonization methods are limited to tubular shapes and the stringent requirement of watertight input, while our method aims to produce more generalized skeletal representations for complex structures and handle point clouds. Our key idea is to use the insights of the medial axis transform (MAT) to capture the intrinsic geometric and topological natures of the original input points. We first predict a set of skeletal points by learning a geometric transformation, and then analyze the connectivity of the skeletal points to form skeletal mesh structures. Extensive evaluations and comparisons show our method has superior performance and robustness. The learned skeletal representation will benefit several unsupervised tasks for point clouds, such as surface reconstruction and segmentation.



### RaP-Net: A Region-wise and Point-wise Weighting Network to Extract Robust Features for Indoor Localization
- **Arxiv ID**: http://arxiv.org/abs/2012.00234v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2012.00234v3)
- **Published**: 2020-12-01 03:12:09+00:00
- **Updated**: 2021-08-22 14:33:17+00:00
- **Authors**: Dongjiang Li, Jinyu Miao, Xuesong Shi, Yuxin Tian, Qiwei Long, Tianyu Cai, Ping Guo, Hongfei Yu, Wei Yang, Haosong Yue, Qi Wei, Fei Qiao
- **Comment**: IROS 2021
- **Journal**: None
- **Summary**: Feature extraction plays an important role in visual localization. Unreliable features on dynamic objects or repetitive regions will interfere with feature matching and challenge indoor localization greatly. To address the problem, we propose a novel network, RaP-Net, to simultaneously predict region-wise invariability and point-wise reliability, and then extract features by considering both of them. We also introduce a new dataset, named OpenLORIS-Location, to train the proposed network. The dataset contains 1553 images from 93 indoor locations. Various appearance changes between images of the same location are included and can help the model to learn the invariability in typical indoor scenes. Experimental results show that the proposed RaP-Net trained with OpenLORIS-Location dataset achieves excellent performance in the feature matching task and significantly outperforms state-of-the-arts feature algorithms in indoor localization. The RaP-Net code and dataset are available at https://github.com/ivipsourcecode/RaP-Net.



### Sim2Real for Self-Supervised Monocular Depth and Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2012.00238v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2012.00238v1)
- **Published**: 2020-12-01 03:25:02+00:00
- **Updated**: 2020-12-01 03:25:02+00:00
- **Authors**: Nithin Raghavan, Punarjay Chakravarty, Shubham Shrivastava
- **Comment**: 9 pages, 3 figures
- **Journal**: None
- **Summary**: Image-based learning methods for autonomous vehicle perception tasks require large quantities of labelled, real data in order to properly train without overfitting, which can often be incredibly costly. While leveraging the power of simulated data can potentially aid in mitigating these costs, networks trained in the simulation domain usually fail to perform adequately when applied to images in the real domain. Recent advances in domain adaptation have indicated that a shared latent space assumption can help to bridge the gap between the simulation and real domains, allowing the transference of the predictive capabilities of a network from the simulation domain to the real domain. We demonstrate that a twin VAE-based architecture with a shared latent space and auxiliary decoders is able to bridge the sim2real gap without requiring any paired, ground-truth data in the real domain. Using only paired, ground-truth data in the simulation domain, this architecture has the potential to generate perception tasks such as depth and segmentation maps. We compare this method to networks trained in a supervised manner to indicate the merit of these results.



### 3D Guided Weakly Supervised Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2012.00242v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.00242v1)
- **Published**: 2020-12-01 03:34:15+00:00
- **Updated**: 2020-12-01 03:34:15+00:00
- **Authors**: Weixuan Sun, Jing Zhang, Nick Barnes
- **Comment**: None
- **Journal**: None
- **Summary**: Pixel-wise clean annotation is necessary for fully-supervised semantic segmentation, which is laborious and expensive to obtain. In this paper, we propose a weakly supervised 2D semantic segmentation model by incorporating sparse bounding box labels with available 3D information, which is much easier to obtain with advanced sensors. We manually labeled a subset of the 2D-3D Semantics(2D-3D-S) dataset with bounding boxes, and introduce our 2D-3D inference module to generate accurate pixel-wise segment proposal masks. Guided by 3D information, we first generate a point cloud of objects and calculate objectness probability score for each point. Then we project the point cloud with objectness probabilities back to 2D images followed by a refinement step to obtain segment proposals, which are treated as pseudo labels to train a semantic segmentation network. Our method works in a recursive manner to gradually refine the above-mentioned segment proposals. Extensive experimental results on the 2D-3D-S dataset show that the proposed method can generate accurate segment proposals when bounding box labels are available on only a small subset of training images. Performance comparison with recent state-of-the-art methods further illustrates the effectiveness of our method.



### A New Action Recognition Framework for Video Highlights Summarization in Sporting Events
- **Arxiv ID**: http://arxiv.org/abs/2012.00253v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2012.00253v1)
- **Published**: 2020-12-01 04:14:40+00:00
- **Updated**: 2020-12-01 04:14:40+00:00
- **Authors**: Cheng Yan, Xin Li, Guoqiang Li
- **Comment**: 18 pages, 3 figures, 4 tables
- **Journal**: None
- **Summary**: To date, machine learning for human action recognition in video has been widely implemented in sports activities. Although some studies have been successful in the past, precision is still the most significant concern. In this study, we present a high-accuracy framework to automatically clip the sports video stream by using a three-level prediction algorithm based on two classical open-source structures, i.e., YOLO-v3 and OpenPose. It is found that by using a modest amount of sports video training data, our methodology can perform sports activity highlights clipping accurately. Comparing with the previous systems, our methodology shows some advantages in accuracy. This study may serve as a new clipping system to extend the potential applications of the video summarization in sports field, as well as facilitates the development of match analysis system.



### Confluence: A Robust Non-IoU Alternative to Non-Maxima Suppression in Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2012.00257v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2012.00257v3)
- **Published**: 2020-12-01 04:22:01+00:00
- **Updated**: 2022-08-03 00:40:26+00:00
- **Authors**: Andrew Shepley, Greg Falzon, Paul Kwan
- **Comment**: 16 pages
- **Journal**: None
- **Summary**: Confluence is a novel non-Intersection over Union (IoU) alternative to Non-Maxima Suppression (NMS) in bounding box post-processing in object detection. It overcomes the inherent limitations of IoU-based NMS variants to provide a more stable, consistent predictor of bounding box clustering by using a normalized Manhattan Distance inspired proximity metric to represent bounding box clustering. Unlike Greedy and Soft NMS, it does not rely solely on classification confidence scores to select optimal bounding boxes, instead selecting the box which is closest to every other box within a given cluster and removing highly confluent neighboring boxes. Confluence is experimentally validated on the MS COCO and CrowdHuman benchmarks, improving Average Precision by up to 2.3-3.8% and Average Recall by up to 5.3-7.2% when compared against de-facto standard and state of the art NMS variants. Quantitative results are supported by extensive qualitative analysis and threshold sensitivity analysis experiments support the conclusion that Confluence is more robust than NMS variants. Confluence represents a paradigm shift in bounding box processing, with potential to replace IoU in bounding box regression processes.



### FairFaceGAN: Fairness-aware Facial Image-to-Image Translation
- **Arxiv ID**: http://arxiv.org/abs/2012.00282v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.00282v2)
- **Published**: 2020-12-01 05:43:46+00:00
- **Updated**: 2020-12-02 05:38:48+00:00
- **Authors**: Sunhee Hwang, Sungho Park, Dohyung Kim, Mirae Do, Hyeran Byun
- **Comment**: The 31st British Machine Vision Conference (BMVC 2020)
- **Journal**: None
- **Summary**: In this paper, we introduce FairFaceGAN, a fairness-aware facial Image-to-Image translation model, mitigating the problem of unwanted translation in protected attributes (e.g., gender, age, race) during facial attributes editing. Unlike existing models, FairFaceGAN learns fair representations with two separate latents - one related to the target attributes to translate, and the other unrelated to them. This strategy enables FairFaceGAN to separate the information about protected attributes and that of target attributes. It also prevents unwanted translation in protected attributes while target attributes editing. To evaluate the degree of fairness, we perform two types of experiments on CelebA dataset. First, we compare the fairness-aware classification performances when augmenting data by existing image translation methods and FairFaceGAN respectively. Moreover, we propose a new fairness metric, namely Frechet Protected Attribute Distance (FPAD), which measures how well protected attributes are preserved. Experimental results demonstrate that FairFaceGAN shows consistent improvements in terms of fairness over the existing image translation models. Further, we also evaluate image translation performances, where FairFaceGAN shows competitive results, compared to those of existing methods.



### CycleGAN without checkerboard artifacts for counter-forensics of fake-image detection
- **Arxiv ID**: http://arxiv.org/abs/2012.00287v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2012.00287v1)
- **Published**: 2020-12-01 06:08:37+00:00
- **Updated**: 2020-12-01 06:08:37+00:00
- **Authors**: Takayuki Osakabe, Miki Tanaka, Yuma Kinoshita, Hitoshi Kiya
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a novel CycleGAN without checkerboard artifacts for counter-forensics of fake-image detection. Recent rapid advances in image manipulation tools and deep image synthesis techniques, such as Generative Adversarial Networks (GANs) have easily generated fake images, so detecting manipulated images has become an urgent issue. Most state-of-the-art forgery detection methods assume that images include checkerboard artifacts which are generated by using DNNs. Accordingly, we propose a novel CycleGAN without any checkerboard artifacts for counter-forensics of fake-mage detection methods for the first time, as an example of GANs without checkerboard artifacts.



### Dual Pixel Exploration: Simultaneous Depth Estimation and Image Restoration
- **Arxiv ID**: http://arxiv.org/abs/2012.00301v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.00301v1)
- **Published**: 2020-12-01 06:53:57+00:00
- **Updated**: 2020-12-01 06:53:57+00:00
- **Authors**: Liyuan Pan, Shah Chowdhury, Richard Hartley, Miaomiao Liu, Hongguang Zhang, Hongdong Li
- **Comment**: None
- **Journal**: None
- **Summary**: The dual-pixel (DP) hardware works by splitting each pixel in half and creating an image pair in a single snapshot. Several works estimate depth/inverse depth by treating the DP pair as a stereo pair. However, dual-pixel disparity only occurs in image regions with the defocus blur. The heavy defocus blur in DP pairs affects the performance of matching-based depth estimation approaches. Instead of removing the blur effect blindly, we study the formation of the DP pair which links the blur and the depth information. In this paper, we propose a mathematical DP model which can benefit depth estimation by the blur. These explorations motivate us to propose an end-to-end DDDNet (DP-based Depth and Deblur Network) to jointly estimate the depth and restore the image. Moreover, we define a reblur loss, which reflects the relationship of the DP image formation process with depth information, to regularise our depth estimate in training. To meet the requirement of a large amount of data for learning, we propose the first DP image simulator which allows us to create datasets with DP pairs from any existing RGBD dataset. As a side contribution, we collect a real dataset for further research. Extensive experimental evaluation on both synthetic and real datasets shows that our approach achieves competitive performance compared to state-of-the-art approaches.



### A Stitching Algorithm for Automated Surface Inspection of Rotationally Symmetric Components
- **Arxiv ID**: http://arxiv.org/abs/2012.00308v3
- **DOI**: None
- **Categories**: **cs.CV**, 68U10
- **Links**: [PDF](http://arxiv.org/pdf/2012.00308v3)
- **Published**: 2020-12-01 07:03:45+00:00
- **Updated**: 2021-06-21 16:26:53+00:00
- **Authors**: Tobias Schlagenhauf, Tim Brander, Juergen Fleischer
- **Comment**: 9 pages, 13 figures
- **Journal**: In: CIRP Journal of Manufacturing Science and Technology (35), S.
  169-177 (2021)
- **Summary**: This paper provides a novel approach to stitching surface images of rotationally symmetric parts. It presents a process pipeline that uses a feature-based stitching approach to create a distortion-free and true-to-life image from a video file. The developed process thus enables, for example, condition monitoring without having to view many individual images. For validation purposes, this will be demonstrated in the paper using the concrete example of a worn ball screw drive spindle. The developed algorithm aims at reproducing the functional principle of a line scan camera system, whereby the physical measuring systems are replaced by a feature-based approach. For evaluation of the stitching algorithms, metrics are used, some of which have only been developed in this work or have been supplemented by test procedures already in use. The applicability of the developed algorithm is not only limited to machine tool spindles. Instead, the developed method allows a general approach to the surface inspection of various rotationally symmetric components and can therefore be used in a variety of industrial applications. Deep-learning-based detection Algorithms can easily be implemented to generate a complete pipeline for failure detection and condition monitoring on rotationally symmetric parts.



### Unsupervised Part Discovery via Feature Alignment
- **Arxiv ID**: http://arxiv.org/abs/2012.00313v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.00313v1)
- **Published**: 2020-12-01 07:25:00+00:00
- **Updated**: 2020-12-01 07:25:00+00:00
- **Authors**: Mengqi Guo, Yutong Bai, Zhishuai Zhang, Adam Kortylewski, Alan Yuille
- **Comment**: 10 pages, 9 figures, submitted to CVPR 2021
- **Journal**: None
- **Summary**: Understanding objects in terms of their individual parts is important, because it enables a precise understanding of the objects' geometrical structure, and enhances object recognition when the object is seen in a novel pose or under partial occlusion. However, the manual annotation of parts in large scale datasets is time consuming and expensive. In this paper, we aim at discovering object parts in an unsupervised manner, i.e., without ground-truth part or keypoint annotations. Our approach builds on the intuition that objects of the same class in a similar pose should have their parts aligned at similar spatial locations. We exploit the property that neural network features are largely invariant to nuisance variables and the main remaining source of variations between images of the same object category is the object pose. Specifically, given a training image, we find a set of similar images that show instances of the same object category in the same pose, through an affine alignment of their corresponding feature maps. The average of the aligned feature maps serves as pseudo ground-truth annotation for a supervised training of the deep network backbone. During inference, part detection is simple and fast, without any extra modules or overheads other than a feed-forward neural network. Our experiments on several datasets from different domains verify the effectiveness of the proposed method. For example, we achieve 37.8 mAP on VehiclePart, which is at least 4.2 better than previous methods.



### Diverse Temporal Aggregation and Depthwise Spatiotemporal Factorization for Efficient Video Classification
- **Arxiv ID**: http://arxiv.org/abs/2012.00317v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.00317v3)
- **Published**: 2020-12-01 07:40:06+00:00
- **Updated**: 2021-04-22 01:40:41+00:00
- **Authors**: Youngwan Lee, Hyung-Il Kim, Kimin Yun, Jinyoung Moon
- **Comment**: None
- **Journal**: None
- **Summary**: Video classification researches that have recently attracted attention are the fields of temporal modeling and 3D efficient architecture. However, the temporal modeling methods are not efficient or the 3D efficient architecture is less interested in temporal modeling. For bridging the gap between them, we propose an efficient temporal modeling 3D architecture, called VoV3D, that consists of a temporal one-shot aggregation (T-OSA) module and depthwise factorized component, D(2+1)D. The T-OSA is devised to build a feature hierarchy by aggregating temporal features with different temporal receptive fields. Stacking this T-OSA enables the network itself to model short-range as well as long-range temporal relationships across frames without any external modules. Inspired by kernel factorization and channel factorization, we also design a depthwise spatiotemporal factorization module, named, D(2+1)D that decomposes a 3D depthwise convolution into two spatial and temporal depthwise convolutions for making our network more lightweight and efficient. By using the proposed temporal modeling method (T-OSA), and the efficient factorized component (D(2+1)D), we construct two types of VoV3D networks, VoV3D-M and VoV3D-L. Thanks to its efficiency and effectiveness of temporal modeling, VoV3D-L has 6x fewer model parameters and 16x less computation, surpassing a state-of-the-art temporal modeling method on both Something-Something and Kinetics-400. Furthermore, VoV3D shows better temporal modeling ability than a state-of-the-art efficient 3D architecture, X3D having comparable model capacity. We hope that VoV3D can serve as a baseline for efficient video classification.



### Fast Class-wise Updating for Online Hashing
- **Arxiv ID**: http://arxiv.org/abs/2012.00318v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/2012.00318v1)
- **Published**: 2020-12-01 07:41:54+00:00
- **Updated**: 2020-12-01 07:41:54+00:00
- **Authors**: Mingbao Lin, Rongrong Ji, Xiaoshuai Sun, Baochang Zhang, Feiyue Huang, Yonghong Tian, Dacheng Tao
- **Comment**: Accepted by IEEE Transactions on Pattern Analysis and Machine
  Intelligence (TPAMI)
- **Journal**: None
- **Summary**: Online image hashing has received increasing research attention recently, which processes large-scale data in a streaming fashion to update the hash functions on-the-fly. To this end, most existing works exploit this problem under a supervised setting, i.e., using class labels to boost the hashing performance, which suffers from the defects in both adaptivity and efficiency: First, large amounts of training batches are required to learn up-to-date hash functions, which leads to poor online adaptivity. Second, the training is time-consuming, which contradicts with the core need of online learning. In this paper, a novel supervised online hashing scheme, termed Fast Class-wise Updating for Online Hashing (FCOH), is proposed to address the above two challenges by introducing a novel and efficient inner product operation. To achieve fast online adaptivity, a class-wise updating method is developed to decompose the binary code learning and alternatively renew the hash functions in a class-wise fashion, which well addresses the burden on large amounts of training batches. Quantitatively, such a decomposition further leads to at least 75% storage saving. To further achieve online efficiency, we propose a semi-relaxation optimization, which accelerates the online training by treating different binary constraints independently. Without additional constraints and variables, the time complexity is significantly reduced. Such a scheme is also quantitatively shown to well preserve past information during updating hashing functions. We have quantitatively demonstrated that the collective effort of class-wise updating and semi-relaxation optimization provides a superior performance comparing to various state-of-the-art methods, which is verified through extensive experiments on three widely-used datasets.



### Disentangling Label Distribution for Long-tailed Visual Recognition
- **Arxiv ID**: http://arxiv.org/abs/2012.00321v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2012.00321v2)
- **Published**: 2020-12-01 07:56:53+00:00
- **Updated**: 2021-03-20 15:22:19+00:00
- **Authors**: Youngkyu Hong, Seungju Han, Kwanghee Choi, Seokjun Seo, Beomsu Kim, Buru Chang
- **Comment**: CVPR 2021
- **Journal**: None
- **Summary**: The current evaluation protocol of long-tailed visual recognition trains the classification model on the long-tailed source label distribution and evaluates its performance on the uniform target label distribution. Such protocol has questionable practicality since the target may also be long-tailed. Therefore, we formulate long-tailed visual recognition as a label shift problem where the target and source label distributions are different. One of the significant hurdles in dealing with the label shift problem is the entanglement between the source label distribution and the model prediction. In this paper, we focus on disentangling the source label distribution from the model prediction. We first introduce a simple but overlooked baseline method that matches the target label distribution by post-processing the model prediction trained by the cross-entropy loss and the Softmax function. Although this method surpasses state-of-the-art methods on benchmark datasets, it can be further improved by directly disentangling the source label distribution from the model prediction in the training phase. Thus, we propose a novel method, LAbel distribution DisEntangling (LADE) loss based on the optimal bound of Donsker-Varadhan representation. LADE achieves state-of-the-art performance on benchmark datasets such as CIFAR-100-LT, Places-LT, ImageNet-LT, and iNaturalist 2018. Moreover, LADE outperforms existing methods on various shifted target label distributions, showing the general adaptability of our proposed method.



### Low Bandwidth Video-Chat Compression using Deep Generative Models
- **Arxiv ID**: http://arxiv.org/abs/2012.00328v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2012.00328v1)
- **Published**: 2020-12-01 08:17:00+00:00
- **Updated**: 2020-12-01 08:17:00+00:00
- **Authors**: Maxime Oquab, Pierre Stock, Oran Gafni, Daniel Haziza, Tao Xu, Peizhao Zhang, Onur Celebi, Yana Hasson, Patrick Labatut, Bobo Bose-Kolanu, Thibault Peyronel, Camille Couprie
- **Comment**: 11 pages
- **Journal**: None
- **Summary**: To unlock video chat for hundreds of millions of people hindered by poor connectivity or unaffordable data costs, we propose to authentically reconstruct faces on the receiver's device using facial landmarks extracted at the sender's side and transmitted over the network. In this context, we discuss and evaluate the benefits and disadvantages of several deep adversarial approaches. In particular, we explore quality and bandwidth trade-offs for approaches based on static landmarks, dynamic landmarks or segmentation maps. We design a mobile-compatible architecture based on the first order animation model of Siarohin et al. In addition, we leverage SPADE blocks to refine results in important areas such as the eyes and lips. We compress the networks down to about 3MB, allowing models to run in real time on iPhone 8 (CPU). This approach enables video calling at a few kbits per second, an order of magnitude lower than currently available alternatives.



### Semi-Supervised Noisy Student Pre-training on EfficientNet Architectures for Plant Pathology Classification
- **Arxiv ID**: http://arxiv.org/abs/2012.00332v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2012.00332v1)
- **Published**: 2020-12-01 08:34:03+00:00
- **Updated**: 2020-12-01 08:34:03+00:00
- **Authors**: Sedrick Scott Keh
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, deep learning has vastly improved the identification and diagnosis of various diseases in plants. In this report, we investigate the problem of pathology classification using images of a single leaf. We explore the use of standard benchmark models such as VGG16, ResNet101, and DenseNet 161 to achieve a 0.945 score on the task. Furthermore, we explore the use of the newer EfficientNet model, improving the accuracy to 0.962. Finally, we introduce the state-of-the-art idea of semi-supervised Noisy Student training to the EfficientNet, resulting in significant improvements in both accuracy and convergence rate. The final ensembled Noisy Student model performs very well on the task, achieving a test score of 0.982.



### Ultra-low bitrate video conferencing using deep image animation
- **Arxiv ID**: http://arxiv.org/abs/2012.00346v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2012.00346v1)
- **Published**: 2020-12-01 09:06:34+00:00
- **Updated**: 2020-12-01 09:06:34+00:00
- **Authors**: Goluck Konuko, Giuseppe Valenzise, Stéphane Lathuilière
- **Comment**: 5 pages
- **Journal**: None
- **Summary**: In this work we propose a novel deep learning approach for ultra-low bitrate video compression for video conferencing applications. To address the shortcomings of current video compression paradigms when the available bandwidth is extremely limited, we adopt a model-based approach that employs deep neural networks to encode motion information as keypoint displacement and reconstruct the video signal at the decoder side. The overall system is trained in an end-to-end fashion minimizing a reconstruction error on the encoder output. Objective and subjective quality evaluation experiments demonstrate that the proposed approach provides an average bitrate reduction for the same visual quality of more than 80% compared to HEVC.



### HORAE: an annotated dataset of books of hours
- **Arxiv ID**: http://arxiv.org/abs/2012.00351v1
- **DOI**: 10.1145/3352631.3352633
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.00351v1)
- **Published**: 2020-12-01 09:25:38+00:00
- **Updated**: 2020-12-01 09:25:38+00:00
- **Authors**: Mélodie Boillet, Marie-Laurence Bonhomme, Dominique Stutzmann, Christopher Kermorvant
- **Comment**: None
- **Journal**: HIP 5 (2019) 7-12
- **Summary**: We introduce in this paper a new dataset of annotated pages from books of hours, a type of handwritten prayer books owned and used by rich lay people in the late middle ages. The dataset was created for conducting historical research on the evolution of the religious mindset in Europe at this period since the book of hours represent one of the major sources of information thanks both to their rich illustrations and the different types of religious sources they contain. We first describe how the corpus was collected and manually annotated then present the evaluation of a state-of-the-art system for text line detection and for zone detection and typing. The corpus is freely available for research.



### Robust and Accurate Object Velocity Detection by Stereo Camera for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2012.00353v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.00353v1)
- **Published**: 2020-12-01 09:29:59+00:00
- **Updated**: 2020-12-01 09:29:59+00:00
- **Authors**: Toru Saito, Toshimi Okubo, Naoki Takahashi
- **Comment**: None
- **Journal**: IEEE Intelligent Vehicles Symposium, 2020
- **Summary**: Although the number of camera-based sensors mounted on vehicles has recently increased dramatically, robust and accurate object velocity detection is difficult. Additionally, it is still common to use radar as a fusion system. We have developed a method to accurately detect the velocity of object using a camera, based on a large-scale dataset collected over 20 years by the automotive manufacturer, SUBARU. The proposed method consists of three methods: an High Dynamic Range (HDR) detection method that fuses multiple stereo disparity images, a fusion method that combines the results of monocular and stereo recognitions, and a new velocity calculation method. The evaluation was carried out using measurement devices and a test course that can quantitatively reproduce severe environment by mounting the developed stereo camera on an actual vehicle.



### Rethinking Positive Aggregation and Propagation of Gradients in Gradient-based Saliency Methods
- **Arxiv ID**: http://arxiv.org/abs/2012.00362v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2012.00362v1)
- **Published**: 2020-12-01 09:38:54+00:00
- **Updated**: 2020-12-01 09:38:54+00:00
- **Authors**: Ashkan Khakzar, Soroosh Baselizadeh, Nassir Navab
- **Comment**: ICML 2020 - Workshop on Human Interpretability in Machine Learning -
  Spotlight paper - Video at http://whi2020.online/poster_40.html
- **Journal**: None
- **Summary**: Saliency methods interpret the prediction of a neural network by showing the importance of input elements for that prediction. A popular family of saliency methods utilize gradient information. In this work, we empirically show that two approaches for handling the gradient information, namely positive aggregation, and positive propagation, break these methods. Though these methods reflect visually salient information in the input, they do not explain the model prediction anymore as the generated saliency maps are insensitive to the predicted output and are insensitive to model parameter randomization. Specifically for methods that aggregate the gradients of a chosen layer such as GradCAM++ and FullGrad, exclusively aggregating positive gradients is detrimental. We further support this by proposing several variants of aggregation methods with positive handling of gradient information. For methods that backpropagate gradient information such as LRP, RectGrad, and Guided Backpropagation, we show the destructive effect of exclusively propagating positive gradient information.



### Pre-Trained Image Processing Transformer
- **Arxiv ID**: http://arxiv.org/abs/2012.00364v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2012.00364v4)
- **Published**: 2020-12-01 09:42:46+00:00
- **Updated**: 2021-11-08 07:08:21+00:00
- **Authors**: Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua Liu, Siwei Ma, Chunjing Xu, Chao Xu, Wen Gao
- **Comment**: CVPR 2021
- **Journal**: None
- **Summary**: As the computing power of modern hardware is increasing strongly, pre-trained deep learning models (e.g., BERT, GPT-3) learned on large-scale datasets have shown their effectiveness over conventional methods. The big progress is mainly contributed to the representation ability of transformer and its variant architectures. In this paper, we study the low-level computer vision task (e.g., denoising, super-resolution and deraining) and develop a new pre-trained model, namely, image processing transformer (IPT). To maximally excavate the capability of transformer, we present to utilize the well-known ImageNet benchmark for generating a large amount of corrupted image pairs. The IPT model is trained on these images with multi-heads and multi-tails. In addition, the contrastive learning is introduced for well adapting to different image processing tasks. The pre-trained model can therefore efficiently employed on desired task after fine-tuning. With only one pre-trained model, IPT outperforms the current state-of-the-art methods on various low-level benchmarks. Code is available at https://github.com/huawei-noah/Pretrained-IPT and https://gitee.com/mindspore/mindspore/tree/master/model_zoo/research/cv/IPT



### ReproducedPapers.org: Openly teaching and structuring machine learning reproducibility
- **Arxiv ID**: http://arxiv.org/abs/2012.01172v1
- **DOI**: 10.1007/978-3-030-76423-4_1
- **Categories**: **cs.CY**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2012.01172v1)
- **Published**: 2020-12-01 11:19:45+00:00
- **Updated**: 2020-12-01 11:19:45+00:00
- **Authors**: Burak Yildiz, Hayley Hung, Jesse H. Krijthe, Cynthia C. S. Liem, Marco Loog, Gosia Migut, Frans Oliehoek, Annibale Panichella, Przemyslaw Pawelczak, Stjepan Picek, Mathijs de Weerdt, Jan van Gemert
- **Comment**: Accepted to RRPR 2020: Third Workshop on Reproducible Research in
  Pattern Recognition
- **Journal**: None
- **Summary**: We present ReproducedPapers.org: an open online repository for teaching and structuring machine learning reproducibility. We evaluate doing a reproduction project among students and the added value of an online reproduction repository among AI researchers. We use anonymous self-assessment surveys and obtained 144 responses. Results suggest that students who do a reproduction project place more value on scientific reproductions and become more critical thinkers. Students and AI researchers agree that our online reproduction repository is valuable.



### Learning to Generalize Unseen Domains via Memory-based Multi-Source Meta-Learning for Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2012.00417v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.00417v3)
- **Published**: 2020-12-01 11:38:16+00:00
- **Updated**: 2021-05-07 09:21:53+00:00
- **Authors**: Yuyang Zhao, Zhun Zhong, Fengxiang Yang, Zhiming Luo, Yaojin Lin, Shaozi Li, Nicu Sebe
- **Comment**: CVPR 2021
- **Journal**: None
- **Summary**: Recent advances in person re-identification (ReID) obtain impressive accuracy in the supervised and unsupervised learning settings. However, most of the existing methods need to train a new model for a new domain by accessing data. Due to public privacy, the new domain data are not always accessible, leading to a limited applicability of these methods. In this paper, we study the problem of multi-source domain generalization in ReID, which aims to learn a model that can perform well on unseen domains with only several labeled source domains. To address this problem, we propose the Memory-based Multi-Source Meta-Learning (M$^3$L) framework to train a generalizable model for unseen domains. Specifically, a meta-learning strategy is introduced to simulate the train-test process of domain generalization for learning more generalizable models. To overcome the unstable meta-optimization caused by the parametric classifier, we propose a memory-based identification loss that is non-parametric and harmonizes with meta-learning. We also present a meta batch normalization layer (MetaBN) to diversify meta-test features, further establishing the advantage of meta-learning. Experiments demonstrate that our M$^3$L can effectively enhance the generalization ability of the model for unseen domains and can outperform the state-of-the-art methods on four large-scale ReID datasets.



### Weakly-Supervised Arbitrary-Shaped Text Detection with Expectation-Maximization Algorithm
- **Arxiv ID**: http://arxiv.org/abs/2012.00424v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.00424v1)
- **Published**: 2020-12-01 11:45:39+00:00
- **Updated**: 2020-12-01 11:45:39+00:00
- **Authors**: Mengbiao Zhao, Wei Feng, Fei Yin, Xu-Yao Zhang, Cheng-Lin Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Arbitrary-shaped text detection is an important and challenging task in computer vision. Most existing methods require heavy data labeling efforts to produce polygon-level text region labels for supervised training. In order to reduce the cost in data labeling, we study weakly-supervised arbitrary-shaped text detection for combining various weak supervision forms (e.g., image-level tags, coarse, loose and tight bounding boxes), which are far easier for annotation. We propose an Expectation-Maximization (EM) based weakly-supervised learning framework to train an accurate arbitrary-shaped text detector using only a small amount of polygon-level annotated data combined with a large amount of weakly annotated data. Meanwhile, we propose a contour-based arbitrary-shaped text detector, which is suitable for incorporating weakly-supervised learning. Extensive experiments on three arbitrary-shaped text benchmarks (CTW1500, Total-Text and ICDAR-ArT) show that (1) using only 10% strongly annotated data and 90% weakly annotated data, our method yields comparable performance to state-of-the-art methods, (2) with 100% strongly annotated data, our method outperforms existing methods on all three benchmarks. We will make the weakly annotated datasets publicly available in the future.



### Unsupervised Segmentation for Terracotta Warrior Point Cloud (SRG-Net)
- **Arxiv ID**: http://arxiv.org/abs/2012.00433v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2012.00433v2)
- **Published**: 2020-12-01 12:02:55+00:00
- **Updated**: 2022-03-27 10:23:29+00:00
- **Authors**: Yao Hu, Guohua Geng, Kang Li, Wei Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: The repairing work of terracotta warriors in Emperor Qinshihuang Mausoleum Site Museum is handcrafted by experts, and the increasing amounts of unearthed pieces of terracotta warriors make the archaeologists too challenging to conduct the restoration of terracotta warriors efficiently. We hope to segment the 3D point cloud data of the terracotta warriors automatically and store the fragment data in the database to assist the archaeologists in matching the actual fragments with the ones in the database, which could result in higher repairing efficiency of terracotta warriors. Moreover, the existing 3D neural network research is mainly focusing on supervised classification, clustering, unsupervised representation, and reconstruction. There are few pieces of researches concentrating on unsupervised point cloud part segmentation. In this paper, we present SRG-Net for 3D point clouds of terracotta warriors to address these problems. Firstly, we adopt a customized seed-region-growing algorithm to segment the point cloud coarsely. Then we present a supervised segmentation and unsupervised reconstruction networks to learn the characteristics of 3D point clouds. Finally, we combine the SRG algorithm with our improved CNN(convolution neural network) using a refinement method. This pipeline is called SRG-Net, which aims at conducting segmentation tasks on the terracotta warriors. Our proposed SRG-Net is evaluated on the terracotta warrior data and ShapeNet dataset by measuring the accuracy and the latency. The experimental results show that our SRG-Net outperforms the state-of-the-art methods. Our code is available at https://github.com/hyoau/SRG-Net.



### A Unified Structure for Efficient RGB and RGB-D Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2012.00437v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.00437v1)
- **Published**: 2020-12-01 12:12:03+00:00
- **Updated**: 2020-12-01 12:12:03+00:00
- **Authors**: Peng Peng, Yong-Jie Li
- **Comment**: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible
- **Journal**: None
- **Summary**: Salient object detection (SOD) has been well studied in recent years, especially using deep neural networks. However, SOD with RGB and RGB-D images is usually treated as two different tasks with different network structures that need to be designed specifically. In this paper, we proposed a unified and efficient structure with a cross-attention context extraction (CRACE) module to address both tasks of SOD efficiently. The proposed CRACE module receives and appropriately fuses two (for RGB SOD) or three (for RGB-D SOD) inputs. The simple unified feature pyramid network (FPN)-like structure with CRACE modules conveys and refines the results under the multi-level supervisions of saliency and boundaries. The proposed structure is simple yet effective; the rich context information of RGB and depth can be appropriately extracted and fused by the proposed structure efficiently. Experimental results show that our method outperforms other state-of-the-art methods in both RGB and RGB-D SOD tasks on various datasets and in terms of most metrics.



### Just Ask: Learning to Answer Questions from Millions of Narrated Videos
- **Arxiv ID**: http://arxiv.org/abs/2012.00451v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2012.00451v3)
- **Published**: 2020-12-01 12:59:20+00:00
- **Updated**: 2021-08-12 15:15:32+00:00
- **Authors**: Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, Cordelia Schmid
- **Comment**: Accepted at ICCV 2021 (Oral); 20 pages; 14 figures
- **Journal**: None
- **Summary**: Recent methods for visual question answering rely on large-scale annotated datasets. Manual annotation of questions and answers for videos, however, is tedious, expensive and prevents scalability. In this work, we propose to avoid manual annotation and generate a large-scale training dataset for video question answering making use of automatic cross-modal supervision. We leverage a question generation transformer trained on text data and use it to generate question-answer pairs from transcribed video narrations. Given narrated videos, we then automatically generate the HowToVQA69M dataset with 69M video-question-answer triplets. To handle the open vocabulary of diverse answers in this dataset, we propose a training procedure based on a contrastive loss between a video-question multi-modal transformer and an answer transformer. We introduce the zero-shot VideoQA task and show excellent results, in particular for rare answers. Furthermore, we demonstrate our method to significantly outperform the state of the art on MSRVTT-QA, MSVD-QA, ActivityNet-QA and How2QA. Finally, for a detailed evaluation we introduce iVQA, a new VideoQA dataset with reduced language biases and high-quality redundant manual annotations. Our code, datasets and trained models are available at https://antoyang.github.io/just-ask.html.



### Counting People by Estimating People Flows
- **Arxiv ID**: http://arxiv.org/abs/2012.00452v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2012.00452v2)
- **Published**: 2020-12-01 12:59:24+00:00
- **Updated**: 2021-08-03 14:30:28+00:00
- **Authors**: Weizhe Liu, Mathieu Salzmann, Pascal Fua
- **Comment**: Accepted to IEEE Transactions on Pattern Analysis and Machine
  Intelligence. Extension of Our ECCV 2020 Paper: arXiv:1911.10782
- **Journal**: None
- **Summary**: Modern methods for counting people in crowded scenes rely on deep networks to estimate people densities in individual images. As such, only very few take advantage of temporal consistency in video sequences, and those that do only impose weak smoothness constraints across consecutive frames. In this paper, we advocate estimating people flows across image locations between consecutive images and inferring the people densities from these flows instead of directly regressing them. This enables us to impose much stronger constraints encoding the conservation of the number of people. As a result, it significantly boosts performance without requiring a more complex architecture. Furthermore, it allows us to exploit the correlation between people flow and optical flow to further improve the results. We also show that leveraging people conservation constraints in both a spatial and temporal manner makes it possible to train a deep crowd counting model in an active learning setting with much fewer annotations. This significantly reduces the annotation cost while still leading to similar performance to the full supervision case.



### Globally Optimal Relative Pose Estimation with Gravity Prior
- **Arxiv ID**: http://arxiv.org/abs/2012.00458v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.00458v2)
- **Published**: 2020-12-01 13:09:59+00:00
- **Updated**: 2021-02-05 02:38:55+00:00
- **Authors**: Yaqing Ding, Daniel Barath, Jian Yang, Hui Kong, Zuzana Kukelova
- **Comment**: None
- **Journal**: None
- **Summary**: Smartphones, tablets and camera systems used, e.g., in cars and UAVs, are typically equipped with IMUs (inertial measurement units) that can measure the gravity vector accurately. Using this additional information, the $y$-axes of the cameras can be aligned, reducing their relative orientation to a single degree-of-freedom. With this assumption, we propose a novel globally optimal solver, minimizing the algebraic error in the least-squares sense, to estimate the relative pose in the over-determined case. Based on the epipolar constraint, we convert the optimization problem into solving two polynomials with only two unknowns. Also, a fast solver is proposed using the first-order approximation of the rotation. The proposed solvers are compared with the state-of-the-art ones on four real-world datasets with approx. 50000 image pairs in total. Moreover, we collected a dataset, by a smartphone, consisting of 10933 image pairs, gravity directions, and ground truth 3D reconstructions.



### Minimal Solutions for Panoramic Stitching Given Gravity Prior
- **Arxiv ID**: http://arxiv.org/abs/2012.00465v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.00465v1)
- **Published**: 2020-12-01 13:17:36+00:00
- **Updated**: 2020-12-01 13:17:36+00:00
- **Authors**: Yaqing Ding, Daniel Barath, Zuzana Kukelova
- **Comment**: None
- **Journal**: None
- **Summary**: When capturing panoramas, people tend to align their cameras with the vertical axis, i.e., the direction of gravity. Moreover, modern devices, such as smartphones and tablets, are equipped with an IMU (Inertial Measurement Unit) that can measure the gravity vector accurately. Using this prior, the y-axes of the cameras can be aligned or assumed to be already aligned, reducing their relative orientation to 1-DOF (degree of freedom). Exploiting this assumption, we propose new minimal solutions to panoramic image stitching of images taken by cameras with coinciding optical centers, i.e., undergoing pure rotation. We consider four practical camera configurations, assuming unknown fixed or varying focal length with or without radial distortion. The solvers are tested both on synthetic scenes and on more than 500k real image pairs from the Sun360 dataset and from scenes captured by us using two smartphones equipped with IMUs. It is shown, that they outperform the state-of-the-art both in terms of accuracy and processing time.



### Boosting CNN-based primary quantization matrix estimation of double JPEG images via a classification-like architecture
- **Arxiv ID**: http://arxiv.org/abs/2012.00468v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GT
- **Links**: [PDF](http://arxiv.org/pdf/2012.00468v2)
- **Published**: 2020-12-01 13:20:11+00:00
- **Updated**: 2021-03-17 19:54:31+00:00
- **Authors**: Benedetta Tondi, Andrea Costranzo, Dequ Huang, Bin Li
- **Comment**: None
- **Journal**: None
- **Summary**: Estimating the primary quantization matrix of double JPEG compressed images is a problem of relevant importance in image forensics since it allows to infer important information about the past history of an image. In addition, the inconsistencies of the primary quantization matrices across different image regions can be used to localize splicing in double JPEG tampered images. Traditional model-based approaches work under specific assumptions on the relationship between the first and second compression qualities and on the alignment of the JPEG grid. Recently, a deep learning-based estimator capable to work under a wide variety of conditions has been proposed, that outperforms tailored existing methods in most of the cases. The method is based on a Convolutional Neural Network (CNN) that is trained to solve the estimation as a standard regression problem. By exploiting the integer nature of the quantization coefficients, in this paper, we propose a deep learning technique that performs the estimation by resorting to a simil-classification architecture. The CNN is trained with a loss function that takes into account both the accuracy and the Mean Square Error (MSE) of the estimation. Results confirm the superior performance of the proposed technique, compared to the state-of-the art methods based on statistical analysis and, in particular, deep learning regression. Moreover, the capability of the method to work under general operative conditions, regarding the alignment of the second compression grid with the one of first compression and the combinations of the JPEG qualities of former and second compression, is very relevant in practical applications, where these information are unknown a priori.



### Farthest sampling segmentation of triangulated surfaces
- **Arxiv ID**: http://arxiv.org/abs/2012.00478v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV, cs.NA, math.NA, 68U05, 05C50, 15A60, 65F15, 62H30
- **Links**: [PDF](http://arxiv.org/pdf/2012.00478v1)
- **Published**: 2020-12-01 13:31:44+00:00
- **Updated**: 2020-12-01 13:31:44+00:00
- **Authors**: Victoria Hernández-Mederos, Dimas Martínez, Jorge Estrada-Sarlabous, Valia Guerra-Ones
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we introduce Farthest Sampling Segmentation (FSS), a new method for segmentation of triangulated surfaces, which consists of two fundamental steps: the computation of a submatrix $W^k$ of the affinity matrix $W$ and the application of the k-means clustering algorithm to the rows of $W^k$. The submatrix $W^k$ is obtained computing the affinity between all triangles and only a few special triangles: those which are farthest in the defined metric. This is equivalent to select a sample of columns of $W$ without constructing it completely. The proposed method is computationally cheaper than other segmentation algorithms, since it only calculates few columns of $W$ and it does not require the eigendecomposition of $W$ or of any submatrix of $W$.   We prove that the orthogonal projection of $W$ on the space generated by the columns of $W^k$ coincides with the orthogonal projection of $W$ on the space generated by the $k$ eigenvectors computed by Nystr\"om's method using the columns of $W^k$ as a sample of $W$. Further, it is shown that for increasing size $k$, the proximity relationship among the rows of $W^k$ tends to faithfully reflect the proximity among the corresponding rows of $W$.   The FSS method does not depend on parameters that must be tuned by hand and it is very flexible, since it can handle any metric to define the distance between triangles. Numerical experiments with several metrics and a large variety of 3D triangular meshes show that the segmentations obtained computing less than the 10% of columns $W$ are as good as those obtained from clustering the rows of the full matrix $W$.



### Boosting the Performance of Semi-Supervised Learning with Unsupervised Clustering
- **Arxiv ID**: http://arxiv.org/abs/2012.00504v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2012.00504v1)
- **Published**: 2020-12-01 14:19:14+00:00
- **Updated**: 2020-12-01 14:19:14+00:00
- **Authors**: Boaz Lerner, Guy Shiran, Daphna Weinshall
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, Semi-Supervised Learning (SSL) has shown much promise in leveraging unlabeled data while being provided with very few labels. In this paper, we show that ignoring the labels altogether for whole epochs intermittently during training can significantly improve performance in the small sample regime. More specifically, we propose to train a network on two tasks jointly. The primary classification task is exposed to both the unlabeled and the scarcely annotated data, whereas the secondary task seeks to cluster the data without any labels. As opposed to hand-crafted pretext tasks frequently used in self-supervision, our clustering phase utilizes the same classification network and head in an attempt to relax the primary task and propagate the information from the labels without overfitting them. On top of that, the self-supervised technique of classifying image rotations is incorporated during the unsupervised learning phase to stabilize training. We demonstrate our method's efficacy in boosting several state-of-the-art SSL algorithms, significantly improving their results and reducing running time in various standard semi-supervised benchmarks, including 92.6% accuracy on CIFAR-10 and 96.9% on SVHN, using only 4 labels per class in each task. We also notably improve the results in the extreme cases of 1,2 and 3 labels per class, and show that features learned by our model are more meaningful for separating the data.



### One-Pixel Attack Deceives Computer-Assisted Diagnosis of Cancer
- **Arxiv ID**: http://arxiv.org/abs/2012.00517v6
- **DOI**: 10.1145/3483207.3483224
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2012.00517v6)
- **Published**: 2020-12-01 14:27:28+00:00
- **Updated**: 2021-11-02 08:17:03+00:00
- **Authors**: Joni Korpihalkola, Tuomo Sipola, Samir Puuska, Tero Kokkonen
- **Comment**: None
- **Journal**: 2021 4th International Conference on Signal Processing and Machine
  Learning (SPML 2021) (2021) 100-106
- **Summary**: Computer vision and machine learning can be used to automate various tasks in cancer diagnostic and detection. If an attacker can manipulate the automated processing, the results can be devastating and in the worst case lead to wrong diagnosis and treatment. In this research, the goal is to demonstrate the use of one-pixel attacks in a real-life scenario with a real pathology dataset, TUPAC16, which consists of digitized whole-slide images. We attack against the IBM CODAIT's MAX breast cancer detector using adversarial images. These adversarial examples are found using differential evolution to perform the one-pixel modification to the images in the dataset. The results indicate that a minor one-pixel modification of a whole slide image under analysis can affect the diagnosis by reversing the automatic diagnosis result. The attack poses a threat from the cyber security perspective: the one-pixel method can be used as an attack vector by a motivated attacker.



### Robustness Out of the Box: Compositional Representations Naturally Defend Against Black-Box Patch Attacks
- **Arxiv ID**: http://arxiv.org/abs/2012.00558v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.00558v1)
- **Published**: 2020-12-01 15:04:23+00:00
- **Updated**: 2020-12-01 15:04:23+00:00
- **Authors**: Christian Cosgrove, Adam Kortylewski, Chenglin Yang, Alan Yuille
- **Comment**: None
- **Journal**: None
- **Summary**: Patch-based adversarial attacks introduce a perceptible but localized change to the input that induces misclassification. While progress has been made in defending against imperceptible attacks, it remains unclear how patch-based attacks can be resisted. In this work, we study two different approaches for defending against black-box patch attacks. First, we show that adversarial training, which is successful against imperceptible attacks, has limited effectiveness against state-of-the-art location-optimized patch attacks. Second, we find that compositional deep networks, which have part-based representations that lead to innate robustness to natural occlusion, are robust to patch attacks on PASCAL3D+ and the German Traffic Sign Recognition Benchmark, without adversarial training. Moreover, the robustness of compositional models outperforms that of adversarially trained standard models by a large margin. However, on GTSRB, we observe that they have problems discriminating between similar traffic signs with fine-grained differences. We overcome this limitation by introducing part-based finetuning, which improves fine-grained recognition. By leveraging compositional representations, this is the first work that defends against black-box patch attacks without expensive adversarial training. This defense is more robust than adversarial training and more interpretable because it can locate and ignore adversarial patches.



### Facetwise Mesh Refinement for Multi-View Stereo
- **Arxiv ID**: http://arxiv.org/abs/2012.00564v1
- **DOI**: None
- **Categories**: **cs.CV**, 68T45, I.4.5
- **Links**: [PDF](http://arxiv.org/pdf/2012.00564v1)
- **Published**: 2020-12-01 15:16:56+00:00
- **Updated**: 2020-12-01 15:16:56+00:00
- **Authors**: Andrea Romanoni, Matteo Matteucci
- **Comment**: Accepted as Oral ICPR2020
- **Journal**: None
- **Summary**: Mesh refinement is a fundamental step for accurate Multi-View Stereo. It modifies the geometry of an initial manifold mesh to minimize the photometric error induced in a set of camera pairs. This initial mesh is usually the output of volumetric 3D reconstruction based on min-cut over Delaunay Triangulations. Such methods produce a significant amount of non-manifold vertices, therefore they require a vertex split step to explicitly repair them. In this paper, we extend this method to preemptively fix the non-manifold vertices by reasoning directly on the Delaunay Triangulation and avoid most vertex splits. The main contribution of this paper addresses the problem of choosing the camera pairs adopted by the refinement process. We treat the problem as a mesh labeling process, where each label corresponds to a camera pair. Differently from the state-of-the-art methods, which use each camera pair to refine all the visible parts of the mesh, we choose, for each facet, the best pair that enforces both the overall visibility and coverage. The refinement step is applied for each facet using only the camera pair selected. This facetwise refinement helps the process to be applied in the most evenly way possible.



### Boosting Adversarial Attacks on Neural Networks with Better Optimizer
- **Arxiv ID**: http://arxiv.org/abs/2012.00567v2
- **DOI**: 10.1155/2021/9983309
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.00567v2)
- **Published**: 2020-12-01 15:18:19+00:00
- **Updated**: 2021-06-09 12:40:43+00:00
- **Authors**: Heng Yin, Hengwei Zhang, Jindong Wang, Ruiyu Dou
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional neural networks have outperformed humans in image recognition tasks, but they remain vulnerable to attacks from adversarial examples. Since these data are crafted by adding imperceptible noise to normal images, their existence poses potential security threats to deep learning systems. Sophisticated adversarial examples with strong attack performance can also be used as a tool to evaluate the robustness of a model. However, the success rate of adversarial attacks can be further improved in black-box environments. Therefore, this study combines a modified Adam gradient descent algorithm with the iterative gradient-based attack method. The proposed Adam Iterative Fast Gradient Method is then used to improve the transferability of adversarial examples. Extensive experiments on ImageNet showed that the proposed method offers a higher attack success rate than existing iterative methods. By extending our method, we achieved a state-of-the-art attack success rate of 95.0% on defense models.



### Multi-level Knowledge Distillation via Knowledge Alignment and Correlation
- **Arxiv ID**: http://arxiv.org/abs/2012.00573v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.00573v2)
- **Published**: 2020-12-01 15:27:15+00:00
- **Updated**: 2021-06-04 00:11:35+00:00
- **Authors**: Fei Ding, Yin Yang, Hongxin Hu, Venkat Krovi, Feng Luo
- **Comment**: 15 pages, 11 tables, 4 figures
- **Journal**: None
- **Summary**: Knowledge distillation (KD) has become an important technique for model compression and knowledge transfer. In this work, we first perform a comprehensive analysis of the knowledge transferred by different KD methods. We demonstrate that traditional KD methods, which minimize the KL divergence of softmax outputs between networks, are related to the knowledge alignment of an individual sample only. Meanwhile, recent contrastive learning-based KD methods mainly transfer relational knowledge between different samples, namely, knowledge correlation. While it is important to transfer the full knowledge from teacher to student, we introduce the Multi-level Knowledge Distillation (MLKD) by effectively considering both knowledge alignment and correlation. MLKD is task-agnostic and model-agnostic, and can easily transfer knowledge from supervised or self-supervised pretrained teachers. We show that MLKD can improve the reliability and transferability of learned representations. Experiments demonstrate that MLKD outperforms other state-of-the-art methods on a large number of experimental settings including different (a) pretraining strategies (b) network architectures (c) datasets (d) tasks.



### DeFMO: Deblurring and Shape Recovery of Fast Moving Objects
- **Arxiv ID**: http://arxiv.org/abs/2012.00595v3
- **DOI**: 10.1109/CVPR46437.2021.00346
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2012.00595v3)
- **Published**: 2020-12-01 16:02:04+00:00
- **Updated**: 2021-03-30 09:14:34+00:00
- **Authors**: Denys Rozumnyi, Martin R. Oswald, Vittorio Ferrari, Jiri Matas, Marc Pollefeys
- **Comment**: CVPR 2021 camera-ready
- **Journal**: 2021 IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)
- **Summary**: Objects moving at high speed appear significantly blurred when captured with cameras. The blurry appearance is especially ambiguous when the object has complex shape or texture. In such cases, classical methods, or even humans, are unable to recover the object's appearance and motion. We propose a method that, given a single image with its estimated background, outputs the object's appearance and position in a series of sub-frames as if captured by a high-speed camera (i.e. temporal super-resolution). The proposed generative model embeds an image of the blurred object into a latent space representation, disentangles the background, and renders the sharp appearance. Inspired by the image formation model, we design novel self-supervised loss function terms that boost performance and show good generalization capabilities. The proposed DeFMO method is trained on a complex synthetic dataset, yet it performs well on real-world data from several datasets. DeFMO outperforms the state of the art and generates high-quality temporal super-resolution frames.



### NPAS: A Compiler-aware Framework of Unified Network Pruning and Architecture Search for Beyond Real-Time Mobile Acceleration
- **Arxiv ID**: http://arxiv.org/abs/2012.00596v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2012.00596v3)
- **Published**: 2020-12-01 16:03:40+00:00
- **Updated**: 2021-06-17 00:07:48+00:00
- **Authors**: Zhengang Li, Geng Yuan, Wei Niu, Pu Zhao, Yanyu Li, Yuxuan Cai, Xuan Shen, Zheng Zhan, Zhenglun Kong, Qing Jin, Zhiyu Chen, Sijia Liu, Kaiyuan Yang, Bin Ren, Yanzhi Wang, Xue Lin
- **Comment**: Accepted as an oral paper in the Conference on Computer Vision and
  Pattern Recognition (CVPR), 2021
- **Journal**: None
- **Summary**: With the increasing demand to efficiently deploy DNNs on mobile edge devices, it becomes much more important to reduce unnecessary computation and increase the execution speed. Prior methods towards this goal, including model compression and network architecture search (NAS), are largely performed independently and do not fully consider compiler-level optimizations which is a must-do for mobile acceleration. In this work, we first propose (i) a general category of fine-grained structured pruning applicable to various DNN layers, and (ii) a comprehensive, compiler automatic code generation framework supporting different DNNs and different pruning schemes, which bridge the gap of model compression and NAS. We further propose NPAS, a compiler-aware unified network pruning, and architecture search. To deal with large search space, we propose a meta-modeling procedure based on reinforcement learning with fast evaluation and Bayesian optimization, ensuring the total number of training epochs comparable with representative NAS frameworks. Our framework achieves 6.7ms, 5.9ms, 3.9ms ImageNet inference times with 78.2%, 75% (MobileNet-V3 level), and 71% (MobileNet-V2 level) Top-1 accuracy respectively on an off-the-shelf mobile phone, consistently outperforming prior work.



### Enabling Fingerprint Presentation Attacks: Fake Fingerprint Fabrication Techniques and Recognition Performance
- **Arxiv ID**: http://arxiv.org/abs/2012.00606v1
- **DOI**: None
- **Categories**: **cs.CV**, I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/2012.00606v1)
- **Published**: 2020-12-01 16:20:32+00:00
- **Updated**: 2020-12-01 16:20:32+00:00
- **Authors**: Christof Kauba, Luca Debiasi, Andreas Uhl
- **Comment**: 23 pages
- **Journal**: None
- **Summary**: Fake fingerprint representation pose a severe threat for fingerprint based authentication systems. Despite advances in presentation attack detection technologies, which are often integrated directly into the fingerprint scanner devices, many fingerprint scanners are still susceptible to presentation attacks using physical fake fingerprint representation. In this work we evaluate five different commercial-off-the-shelf fingerprint scanners based on different sensing technologies, including optical, optical multispectral, passive capacitive, active capacitive and thermal regarding their susceptibility to presentation attacks using fake fingerprint representations. Several different materials to create the fake representation are tested and evaluated, including wax, cast, latex, silicone, different types of glue, window colours, modelling clay, etc. The quantitative evaluation includes assessing the fingerprint quality of the samples captured from the fake representations as well as comparison experiments where the achieved matching scores of the fake representations against the corresponding real fingerprints indicate the effectiveness of the fake representations. Our results confirmed that all except one of the tested devices are susceptible to at least one type/material of fake fingerprint representations.



### Overcoming the limitations of patch-based learning to detect cancer in whole slide images
- **Arxiv ID**: http://arxiv.org/abs/2012.00617v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2012.00617v1)
- **Published**: 2020-12-01 16:37:18+00:00
- **Updated**: 2020-12-01 16:37:18+00:00
- **Authors**: Ozan Ciga, Tony Xu, Sharon Nofech-Mozes, Shawna Noy, Fang-I Lu, Anne L. Martel
- **Comment**: None
- **Journal**: None
- **Summary**: Whole slide images (WSIs) pose unique challenges when training deep learning models. They are very large which makes it necessary to break each image down into smaller patches for analysis, image features have to be extracted at multiple scales in order to capture both detail and context, and extreme class imbalances may exist. Significant progress has been made in the analysis of these images, thanks largely due to the availability of public annotated datasets. We postulate, however, that even if a method scores well on a challenge task, this success may not translate to good performance in a more clinically relevant workflow. Many datasets consist of image patches which may suffer from data curation bias; other datasets are only labelled at the whole slide level and the lack of annotations across an image may mask erroneous local predictions so long as the final decision is correct. In this paper, we outline the differences between patch or slide-level classification versus methods that need to localize or segment cancer accurately across the whole slide, and we experimentally verify that best practices differ in both cases. We apply a binary cancer detection network on post neoadjuvant therapy breast cancer WSIs to find the tumor bed outlining the extent of cancer, a task which requires sensitivity and precision across the whole slide. We extensively study multiple design choices and their effects on the outcome, including architectures and augmentations. Furthermore, we propose a negative data sampling strategy, which drastically reduces the false positive rate (7% on slide level) and improves each metric pertinent to our problem, with a 15% reduction in the error of tumor extent.



### We are More than Our Joints: Predicting how 3D Bodies Move
- **Arxiv ID**: http://arxiv.org/abs/2012.00619v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.00619v2)
- **Published**: 2020-12-01 16:41:04+00:00
- **Updated**: 2021-04-02 13:04:34+00:00
- **Authors**: Yan Zhang, Michael J. Black, Siyu Tang
- **Comment**: camera ready, cvpr
- **Journal**: None
- **Summary**: A key step towards understanding human behavior is the prediction of 3D human motion. Successful solutions have many applications in human tracking, HCI, and graphics. Most previous work focuses on predicting a time series of future 3D joint locations given a sequence 3D joints from the past. This Euclidean formulation generally works better than predicting pose in terms of joint rotations. Body joint locations, however, do not fully constrain 3D human pose, leaving degrees of freedom undefined, making it hard to animate a realistic human from only the joints. Note that the 3D joints can be viewed as a sparse point cloud. Thus the problem of human motion prediction can be seen as point cloud prediction. With this observation, we instead predict a sparse set of locations on the body surface that correspond to motion capture markers. Given such markers, we fit a parametric body model to recover the 3D shape and pose of the person. These sparse surface markers also carry detailed information about human movement that is not present in the joints, increasing the naturalness of the predicted motions. Using the AMASS dataset, we train MOJO, which is a novel variational autoencoder that generates motions from latent frequencies. MOJO preserves the full temporal resolution of the input motion, and sampling from the latent frequencies explicitly introduces high-frequency components into the generated motion. We note that motion prediction methods accumulate errors over time, resulting in joints or markers that diverge from true human bodies. To address this, we fit SMPL-X to the predictions at each time step, projecting the solution back onto the space of valid bodies. These valid markers are then propagated in time. Experiments show that our method produces state-of-the-art results and realistic 3D body animations. The code for research purposes is at https://yz-cnsdqz.github.io/MOJO/MOJO.html



### Structured Context Enhancement Network for Mouse Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2012.00630v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.00630v3)
- **Published**: 2020-12-01 16:50:19+00:00
- **Updated**: 2021-08-01 16:52:20+00:00
- **Authors**: Feixiang Zhou, Zheheng Jiang, Zhihua Liu, Fang Chen, Long Chen, Lei Tong, Zhile Yang, Haikuan Wang, Minrui Fei, Ling Li, Huiyu Zhou
- **Comment**: IEEE Transactions on Circuits and Systems for Video Technology
- **Journal**: None
- **Summary**: Automated analysis of mouse behaviours is crucial for many applications in neuroscience. However, quantifying mouse behaviours from videos or images remains a challenging problem, where pose estimation plays an important role in describing mouse behaviours. Although deep learning based methods have made promising advances in human pose estimation, they cannot be directly applied to pose estimation of mice due to different physiological natures. Particularly, since mouse body is highly deformable, it is a challenge to accurately locate different keypoints on the mouse body. In this paper, we propose a novel Hourglass network based model, namely Graphical Model based Structured Context Enhancement Network (GM-SCENet) where two effective modules, i.e., Structured Context Mixer (SCM) and Cascaded Multi-Level Supervision (CMLS) are subsequently implemented. SCM can adaptively learn and enhance the proposed structured context information of each mouse part by a novel graphical model that takes into account the motion difference between body parts. Then, the CMLS module is designed to jointly train the proposed SCM and the Hourglass network by generating multi-level information, increasing the robustness of the whole network.Using the multi-level prediction information from SCM and CMLS, we develop an inference method to ensure the accuracy of the localisation results. Finally, we evaluate our proposed approach against several baselines...



### Unpaired Image-to-Image Translation via Latent Energy Transport
- **Arxiv ID**: http://arxiv.org/abs/2012.00649v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2012.00649v3)
- **Published**: 2020-12-01 17:18:58+00:00
- **Updated**: 2021-05-23 19:54:38+00:00
- **Authors**: Yang Zhao, Changyou Chen
- **Comment**: CVPR2021. Code: https://github.com/YangNaruto/latent-energy-transport
- **Journal**: None
- **Summary**: Image-to-image translation aims to preserve source contents while translating to discriminative target styles between two visual domains. Most works apply adversarial learning in the ambient image space, which could be computationally expensive and challenging to train. In this paper, we propose to deploy an energy-based model (EBM) in the latent space of a pretrained autoencoder for this task. The pretrained autoencoder serves as both a latent code extractor and an image reconstruction worker. Our model, LETIT, is based on the assumption that two domains share the same latent space, where latent representation is implicitly decomposed as a content code and a domain-specific style code. Instead of explicitly extracting the two codes and applying adaptive instance normalization to combine them, our latent EBM can implicitly learn to transport the source style code to the target style code while preserving the content code, an advantage over existing image translation methods. This simplified solution is also more efficient in the one-sided unpaired image translation setting. Qualitative and quantitative comparisons demonstrate superior translation quality and faithfulness for content preservation. Our model is the first to be applicable to 1024$\times$1024-resolution unpaired image translation to the best of our knowledge.



### Decoder-side Cross Resolution Synthesis for Video Compression Enhancement
- **Arxiv ID**: http://arxiv.org/abs/2012.00650v4
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2012.00650v4)
- **Published**: 2020-12-01 17:23:53+00:00
- **Updated**: 2021-12-18 05:12:12+00:00
- **Authors**: Ming Lu, Tong Chen, Zhenyu Dai, Dong Wang, Dandan Ding, Zhan Ma
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a decoder-side Cross Resolution Synthesis (CRS) module to pursue better compression efficiency beyond the latest Versatile Video Coding (VVC), where we encode intra frames at original high resolution (HR), compress inter frames at a lower resolution (LR), and then super-resolve decoded LR inter frames with the help from preceding HR intra and neighboring LR inter frames.   For a LR inter frame, a motion alignment and aggregation network (MAN) is devised to produce temporally aggregated motion representation to best guarantee the temporal smoothness; Another texture compensation network (TCN) is utilized to generate texture representation from decoded HR intra frame for better augmenting spatial details; Finally, a similarity-driven fusion engine synthesizes motion and texture representations to upscale LR inter frames for the removal of compression and resolution re-sampling noises.   We enhance the VVC using proposed CRS, showing averaged 8.76% and 11.93% Bj{\o}ntegaard Delta Rate (BD-Rate) gains against the latest VVC anchor in Random Access (RA) and Low-delay P (LDP) settings respectively. In addition, experimental comparisons to the state-of-the-art super-resolution (SR) based VVC enhancement methods, and ablation studies are conducted to further report superior efficiency and generalization of the proposed algorithm. All materials will be made to public at https://njuvision.github.io/CRS for reproducible research.



### Cross-modal registration using point clouds and graph-matching in the context of correlative microscopies
- **Arxiv ID**: http://arxiv.org/abs/2012.00656v1
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.TO, 05C60, I.4; J.3
- **Links**: [PDF](http://arxiv.org/pdf/2012.00656v1)
- **Published**: 2020-12-01 17:27:00+00:00
- **Updated**: 2020-12-01 17:27:00+00:00
- **Authors**: Stephan Kunne, Guillaume Potier, Jean Mérot, Perrine Paul-Gilloteaux
- **Comment**: in Proceedings of iTWIST'20, Paper-ID: 22, Nantes, France, December,
  2-4, 2020
- **Journal**: None
- **Summary**: Correlative microscopy aims at combining two or more modalities to gain more information than the one provided by one modality on the same biological structure. Registration is needed at different steps of correlative microscopies workflows. Biologists want to select the image content used for registration not to introduce bias in the correlation of unknown structures. Intensity-based methods might not allow this selection and might be too slow when the images are very large. We propose an approach based on point clouds created from selected content by the biologist. These point clouds may be prone to big differences in densities but also missing parts and outliers. In this paper we present a method of registration for point clouds based on graph building and graph matching, and compare the method to iterative closest point based methods.



### Emotion Detection using Image Processing in Python
- **Arxiv ID**: http://arxiv.org/abs/2012.00659v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.00659v1)
- **Published**: 2020-12-01 17:34:35+00:00
- **Updated**: 2020-12-01 17:34:35+00:00
- **Authors**: Raghav Puri, Archit Gupta, Manas Sikri, Mohit Tiwari, Nitish Pathak, Shivendra Goel
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, user's emotion using its facial expressions will be detected. These expressions can be derived from the live feed via system's camera or any pre-exisiting image available in the memory. Emotions possessed by humans can be recognized and has a vast scope of study in the computer vision industry upon which several researches have already been done. The work has been implemented using Python (2.7, Open Source Computer Vision Library (OpenCV) and NumPy. The scanned image(testing dataset) is being compared to the training dataset and thus emotion is predicted. The objective of this paper is to develop a system which can analyze the image and predict the expression of the person. The study proves that this procedure is workable and produces valid results.



### Learning Disentangled Latent Factors from Paired Data in Cross-Modal Retrieval: An Implicit Identifiable VAE Approach
- **Arxiv ID**: http://arxiv.org/abs/2012.00682v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2012.00682v1)
- **Published**: 2020-12-01 17:47:50+00:00
- **Updated**: 2020-12-01 17:47:50+00:00
- **Authors**: Minyoung Kim, Ricardo Guerrero, Vladimir Pavlovic
- **Comment**: None
- **Journal**: None
- **Summary**: We deal with the problem of learning the underlying disentangled latent factors that are shared between the paired bi-modal data in cross-modal retrieval. Our assumption is that the data in both modalities are complex, structured, and high dimensional (e.g., image and text), for which the conventional deep auto-encoding latent variable models such as the Variational Autoencoder (VAE) often suffer from difficulty of accurate decoder training or realistic synthesis. A suboptimally trained decoder can potentially harm the model's capability of identifying the true factors. In this paper we propose a novel idea of the implicit decoder, which completely removes the ambient data decoding module from a latent variable model, via implicit encoder inversion that is achieved by Jacobian regularization of the low-dimensional embedding function. Motivated from the recent Identifiable VAE (IVAE) model, we modify it to incorporate the query modality data as conditioning auxiliary input, which allows us to prove that the true parameters of the model can be identified under some regularity conditions. Tested on various datasets where the true factors are fully/partially available, our model is shown to identify the factors accurately, significantly outperforming conventional encoder-decoder latent variable models. We also test our model on the Recipe1M, the large-scale food image/recipe dataset, where the learned factors by our approach highly coincide with the most pronounced food factors that are widely agreed on, including savoriness, wateriness, and greenness.



### Fully Convolutional Networks for Panoptic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2012.00720v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.00720v2)
- **Published**: 2020-12-01 18:31:41+00:00
- **Updated**: 2021-04-03 04:28:54+00:00
- **Authors**: Yanwei Li, Hengshuang Zhao, Xiaojuan Qi, Liwei Wang, Zeming Li, Jian Sun, Jiaya Jia
- **Comment**: CVPR2021 Oral
- **Journal**: None
- **Summary**: In this paper, we present a conceptually simple, strong, and efficient framework for panoptic segmentation, called Panoptic FCN. Our approach aims to represent and predict foreground things and background stuff in a unified fully convolutional pipeline. In particular, Panoptic FCN encodes each object instance or stuff category into a specific kernel weight with the proposed kernel generator and produces the prediction by convolving the high-resolution feature directly. With this approach, instance-aware and semantically consistent properties for things and stuff can be respectively satisfied in a simple generate-kernel-then-segment workflow. Without extra boxes for localization or instance separation, the proposed approach outperforms previous box-based and -free models with high efficiency on COCO, Cityscapes, and Mapillary Vistas datasets with single scale input. Our code is made publicly available at https://github.com/Jia-Research-Lab/PanopticFCN.



### RAFT-3D: Scene Flow using Rigid-Motion Embeddings
- **Arxiv ID**: http://arxiv.org/abs/2012.00726v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.00726v2)
- **Published**: 2020-12-01 18:38:18+00:00
- **Updated**: 2021-04-06 17:11:38+00:00
- **Authors**: Zachary Teed, Jia Deng
- **Comment**: None
- **Journal**: None
- **Summary**: We address the problem of scene flow: given a pair of stereo or RGB-D video frames, estimate pixelwise 3D motion. We introduce RAFT-3D, a new deep architecture for scene flow. RAFT-3D is based on the RAFT model developed for optical flow but iteratively updates a dense field of pixelwise SE3 motion instead of 2D motion. A key innovation of RAFT-3D is rigid-motion embeddings, which represent a soft grouping of pixels into rigid objects. Integral to rigid-motion embeddings is Dense-SE3, a differentiable layer that enforces geometric consistency of the embeddings. Experiments show that RAFT-3D achieves state-of-the-art performance. On FlyingThings3D, under the two-view evaluation, we improved the best published accuracy (d < 0.05) from 34.3% to 83.7%. On KITTI, we achieve an error of 5.77, outperforming the best published method (6.31), despite using no object instance supervision. Code is available at https://github.com/princeton-vl/RAFT-3D.



### GLEAN: Generative Latent Bank for Large-Factor Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2012.00739v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.00739v1)
- **Published**: 2020-12-01 18:56:14+00:00
- **Updated**: 2020-12-01 18:56:14+00:00
- **Authors**: Kelvin C. K. Chan, Xintao Wang, Xiangyu Xu, Jinwei Gu, Chen Change Loy
- **Comment**: Tech report, 19 pages, 19 figures. A high-resolution version of this
  paper can be found at https://ckkelvinchan.github.io/
- **Journal**: None
- **Summary**: We show that pre-trained Generative Adversarial Networks (GANs), e.g., StyleGAN, can be used as a latent bank to improve the restoration quality of large-factor image super-resolution (SR). While most existing SR approaches attempt to generate realistic textures through learning with adversarial loss, our method, Generative LatEnt bANk (GLEAN), goes beyond existing practices by directly leveraging rich and diverse priors encapsulated in a pre-trained GAN. But unlike prevalent GAN inversion methods that require expensive image-specific optimization at runtime, our approach only needs a single forward pass to generate the upscaled image. GLEAN can be easily incorporated in a simple encoder-bank-decoder architecture with multi-resolution skip connections. Switching the bank allows the method to deal with images from diverse categories, e.g., cat, building, human face, and car. Images upscaled by GLEAN show clear improvements in terms of fidelity and texture faithfulness in comparison to existing methods.



### MaX-DeepLab: End-to-End Panoptic Segmentation with Mask Transformers
- **Arxiv ID**: http://arxiv.org/abs/2012.00759v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.00759v3)
- **Published**: 2020-12-01 19:00:00+00:00
- **Updated**: 2021-07-12 21:16:19+00:00
- **Authors**: Huiyu Wang, Yukun Zhu, Hartwig Adam, Alan Yuille, Liang-Chieh Chen
- **Comment**: CVPR 2021
- **Journal**: None
- **Summary**: We present MaX-DeepLab, the first end-to-end model for panoptic segmentation. Our approach simplifies the current pipeline that depends heavily on surrogate sub-tasks and hand-designed components, such as box detection, non-maximum suppression, thing-stuff merging, etc. Although these sub-tasks are tackled by area experts, they fail to comprehensively solve the target task. By contrast, our MaX-DeepLab directly predicts class-labeled masks with a mask transformer, and is trained with a panoptic quality inspired loss via bipartite matching. Our mask transformer employs a dual-path architecture that introduces a global memory path in addition to a CNN path, allowing direct communication with any CNN layers. As a result, MaX-DeepLab shows a significant 7.1% PQ gain in the box-free regime on the challenging COCO dataset, closing the gap between box-based and box-free methods for the first time. A small variant of MaX-DeepLab improves 3.0% PQ over DETR with similar parameters and M-Adds. Furthermore, MaX-DeepLab, without test time augmentation, achieves new state-of-the-art 51.3% PQ on COCO test-dev set. Code is available at https://github.com/google-research/deeplab2.



### Dynamic Feature Pyramid Networks for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2012.00779v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.00779v2)
- **Published**: 2020-12-01 19:03:55+00:00
- **Updated**: 2021-07-03 14:05:04+00:00
- **Authors**: Mingjian Zhu, Kai Han, Changbin Yu, Yunhe Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Feature pyramid network (FPN) is a critical component in modern object detection frameworks. The performance gain in most of the existing FPN variants is mainly attributed to the increase of computational burden. An attempt to enhance the FPN is enriching the spatial information by expanding the receptive fields, which is promising to largely improve the detection accuracy. In this paper, we first investigate how expanding the receptive fields affect the accuracy and computational costs of FPN. We explore a baseline model called inception FPN in which each lateral connection contains convolution filters with different kernel sizes. Moreover, we point out that not all objects need such a complicated calculation and propose a new dynamic FPN (DyFPN). The output features of DyFPN will be calculated by using the adaptively selected branch according to a dynamic gating operation. Therefore, the proposed method can provide a more efficient dynamic inference for achieving a better trade-off between accuracy and computational cost. Extensive experiments conducted on MS-COCO benchmark demonstrate that the proposed DyFPN significantly improves performance with the optimal allocation of computation resources. For instance, replacing inception FPN with DyFPN reduces about 40% of its FLOPs while maintaining similar high performance.



### Pose-based Sign Language Recognition using GCN and BERT
- **Arxiv ID**: http://arxiv.org/abs/2012.00781v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.00781v1)
- **Published**: 2020-12-01 19:10:50+00:00
- **Updated**: 2020-12-01 19:10:50+00:00
- **Authors**: Anirudh Tunga, Sai Vidyaranya Nuthalapati, Juan Wachs
- **Comment**: None
- **Journal**: None
- **Summary**: Sign language recognition (SLR) plays a crucial role in bridging the communication gap between the hearing and vocally impaired community and the rest of the society. Word-level sign language recognition (WSLR) is the first important step towards understanding and interpreting sign language. However, recognizing signs from videos is a challenging task as the meaning of a word depends on a combination of subtle body motions, hand configurations, and other movements. Recent pose-based architectures for WSLR either model both the spatial and temporal dependencies among the poses in different frames simultaneously or only model the temporal information without fully utilizing the spatial information.   We tackle the problem of WSLR using a novel pose-based approach, which captures spatial and temporal information separately and performs late fusion. Our proposed architecture explicitly captures the spatial interactions in the video using a Graph Convolutional Network (GCN). The temporal dependencies between the frames are captured using Bidirectional Encoder Representations from Transformers (BERT). Experimental results on WLASL, a standard word-level sign language recognition dataset show that our model significantly outperforms the state-of-the-art on pose-based methods by achieving an improvement in the prediction accuracy by up to 5%.



### Adversarial Robustness Across Representation Spaces
- **Arxiv ID**: http://arxiv.org/abs/2012.00802v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.00802v1)
- **Published**: 2020-12-01 19:55:58+00:00
- **Updated**: 2020-12-01 19:55:58+00:00
- **Authors**: Pranjal Awasthi, George Yu, Chun-Sung Ferng, Andrew Tomkins, Da-Cheng Juan
- **Comment**: None
- **Journal**: None
- **Summary**: Adversarial robustness corresponds to the susceptibility of deep neural networks to imperceptible perturbations made at test time. In the context of image tasks, many algorithms have been proposed to make neural networks robust to adversarial perturbations made to the input pixels. These perturbations are typically measured in an $\ell_p$ norm. However, robustness often holds only for the specific attack used for training. In this work we extend the above setting to consider the problem of training of deep neural networks that can be made simultaneously robust to perturbations applied in multiple natural representation spaces. For the case of image data, examples include the standard pixel representation as well as the representation in the discrete cosine transform~(DCT) basis. We design a theoretically sound algorithm with formal guarantees for the above problem. Furthermore, our guarantees also hold when the goal is to require robustness with respect to multiple $\ell_p$ norm based attacks. We then derive an efficient practical implementation and demonstrate the effectiveness of our approach on standard datasets for image classification.



### A Three-Stage Self-Training Framework for Semi-Supervised Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2012.00827v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.00827v2)
- **Published**: 2020-12-01 21:00:27+00:00
- **Updated**: 2022-01-09 22:14:55+00:00
- **Authors**: Rihuan Ke, Angelica Aviles-Rivero, Saurabh Pandey, Saikumar Reddy, Carola-Bibiane Schönlieb
- **Comment**: Accepted for publication by IEEE Transactions on Image Processing
- **Journal**: None
- **Summary**: Semantic segmentation has been widely investigated in the community, in which the state of the art techniques are based on supervised models. Those models have reported unprecedented performance at the cost of requiring a large set of high quality segmentation masks. To obtain such annotations is highly expensive and time consuming, in particular, in semantic segmentation where pixel-level annotations are required. In this work, we address this problem by proposing a holistic solution framed as a three-stage self-training framework for semi-supervised semantic segmentation. The key idea of our technique is the extraction of the pseudo-masks statistical information to decrease uncertainty in the predicted probability whilst enforcing segmentation consistency in a multi-task fashion. We achieve this through a three-stage solution. Firstly, we train a segmentation network to produce rough pseudo-masks which predicted probability is highly uncertain. Secondly, we then decrease the uncertainty of the pseudo-masks using a multi-task model that enforces consistency whilst exploiting the rich statistical information of the data. We compare our approach with existing methods for semi-supervised semantic segmentation and demonstrate its state-of-the-art performance with extensive experiments.



### Data Augmentation with norm-VAE for Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2012.00848v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2012.00848v1)
- **Published**: 2020-12-01 21:41:08+00:00
- **Updated**: 2020-12-01 21:41:08+00:00
- **Authors**: Qian Wang, Fanlin Meng, Toby P. Breckon
- **Comment**: 12 pages
- **Journal**: None
- **Summary**: We address the Unsupervised Domain Adaptation (UDA) problem in image classification from a new perspective. In contrast to most existing works which either align the data distributions or learn domain-invariant features, we directly learn a unified classifier for both domains within a high-dimensional homogeneous feature space without explicit domain adaptation. To this end, we employ the effective Selective Pseudo-Labelling (SPL) techniques to take advantage of the unlabelled samples in the target domain. Surprisingly, data distribution discrepancy across the source and target domains can be well handled by a computationally simple classifier (e.g., a shallow Multi-Layer Perceptron) trained in the original feature space. Besides, we propose a novel generative model norm-VAE to generate synthetic features for the target domain as a data augmentation strategy to enhance classifier training. Experimental results on several benchmark datasets demonstrate the pseudo-labelling strategy itself can lead to comparable performance to many state-of-the-art methods whilst the use of norm-VAE for feature augmentation can further improve the performance in most cases. As a result, our proposed methods (i.e. naive-SPL and norm-VAE-SPL) can achieve new state-of-the-art performance with the average accuracy of 93.4% and 90.4% on Office-Caltech and ImageCLEF-DA datasets, and comparable performance on Digits, Office31 and Office-Home datasets with the average accuracy of 97.2%, 87.6% and 67.9% respectively.



### FFD: Fast Feature Detector
- **Arxiv ID**: http://arxiv.org/abs/2012.00859v1
- **DOI**: 10.1109/TIP.2020.3042057
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.00859v1)
- **Published**: 2020-12-01 21:56:35+00:00
- **Updated**: 2020-12-01 21:56:35+00:00
- **Authors**: Morteza Ghahremani, Yonghuai Liu, Bernard Tiddeman
- **Comment**: None
- **Journal**: IEEE Transactions on Image Processing, 2021
- **Summary**: Scale-invariance, good localization and robustness to noise and distortions are the main properties that a local feature detector should possess. Most existing local feature detectors find excessive unstable feature points that increase the number of keypoints to be matched and the computational time of the matching step. In this paper, we show that robust and accurate keypoints exist in the specific scale-space domain. To this end, we first formulate the superimposition problem into a mathematical model and then derive a closed-form solution for multiscale analysis. The model is formulated via difference-of-Gaussian (DoG) kernels in the continuous scale-space domain, and it is proved that setting the scale-space pyramid's blurring ratio and smoothness to 2 and 0.627, respectively, facilitates the detection of reliable keypoints. For the applicability of the proposed model to discrete images, we discretize it using the undecimated wavelet transform and the cubic spline function. Theoretically, the complexity of our method is less than 5\% of that of the popular baseline Scale Invariant Feature Transform (SIFT). Extensive experimental results show the superiority of the proposed feature detector over the existing representative hand-crafted and learning-based techniques in accuracy and computational time. The code and supplementary materials can be found at~{\url{https://github.com/mogvision/FFD}}.



### Towards Good Practices in Self-supervised Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2012.00868v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2012.00868v1)
- **Published**: 2020-12-01 22:13:43+00:00
- **Updated**: 2020-12-01 22:13:43+00:00
- **Authors**: Srikar Appalaraju, Yi Zhu, Yusheng Xie, István Fehérvári
- **Comment**: None
- **Journal**: Neural Information Processing Systems (NeurIPS Self-Supervision
  Workshop 2020)
- **Summary**: Self-supervised representation learning has seen remarkable progress in the last few years. More recently, contrastive instance learning has shown impressive results compared to its supervised learning counterparts. However, even with the ever increased interest in contrastive instance learning, it is still largely unclear why these methods work so well. In this paper, we aim to unravel some of the mysteries behind their success, which are the good practices. Through an extensive empirical analysis, we hope to not only provide insights but also lay out a set of best practices that led to the success of recent work in self-supervised representation learning.



### A compact sequence encoding scheme for online human activity recognition in HRI applications
- **Arxiv ID**: http://arxiv.org/abs/2012.00873v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.00873v1)
- **Published**: 2020-12-01 22:33:09+00:00
- **Updated**: 2020-12-01 22:33:09+00:00
- **Authors**: Georgios Tsatiris, Kostas Karpouzis, Stefanos Kollias
- **Comment**: None
- **Journal**: None
- **Summary**: Human activity recognition and analysis has always been one of the most active areas of pattern recognition and machine intelligence, with applications in various fields, including but not limited to exertion games, surveillance, sports analytics and healthcare. Especially in Human-Robot Interaction, human activity understanding plays a crucial role as household robotic assistants are a trend of the near future. However, state-of-the-art infrastructures that can support complex machine intelligence tasks are not always available, and may not be for the average consumer, as robotic hardware is expensive. In this paper we propose a novel action sequence encoding scheme which efficiently transforms spatio-temporal action sequences into compact representations, using Mahalanobis distance-based shape features and the Radon transform. This representation can be used as input for a lightweight convolutional neural network. Experiments show that the proposed pipeline, when based on state-of-the-art human pose estimation techniques, can provide a robust end-to-end online action recognition scheme, deployable on hardware lacking extreme computing capabilities.



### DiffusionNet: Discretization Agnostic Learning on Surfaces
- **Arxiv ID**: http://arxiv.org/abs/2012.00888v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CG, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2012.00888v3)
- **Published**: 2020-12-01 23:24:22+00:00
- **Updated**: 2022-01-07 14:43:15+00:00
- **Authors**: Nicholas Sharp, Souhaib Attaiki, Keenan Crane, Maks Ovsjanikov
- **Comment**: Published in ACM Transactions on Graphics, presented at SIGGRAPH 2022
- **Journal**: None
- **Summary**: We introduce a new general-purpose approach to deep learning on 3D surfaces, based on the insight that a simple diffusion layer is highly effective for spatial communication. The resulting networks are automatically robust to changes in resolution and sampling of a surface -- a basic property which is crucial for practical applications. Our networks can be discretized on various geometric representations such as triangle meshes or point clouds, and can even be trained on one representation then applied to another. We optimize the spatial support of diffusion as a continuous network parameter ranging from purely local to totally global, removing the burden of manually choosing neighborhood sizes. The only other ingredients in the method are a multi-layer perceptron applied independently at each point, and spatial gradient features to support directional filters. The resulting networks are simple, robust, and efficient. Here, we focus primarily on triangle mesh surfaces, and demonstrate state-of-the-art results for a variety of tasks including surface classification, segmentation, and non-rigid correspondence.



### Deep Multi-Scale Features Learning for Distorted Image Quality Assessment
- **Arxiv ID**: http://arxiv.org/abs/2012.01980v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2012.01980v1)
- **Published**: 2020-12-01 23:39:01+00:00
- **Updated**: 2020-12-01 23:39:01+00:00
- **Authors**: Wei Zhou, Zhibo Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Image quality assessment (IQA) aims to estimate human perception based image visual quality. Although existing deep neural networks (DNNs) have shown significant effectiveness for tackling the IQA problem, it still needs to improve the DNN-based quality assessment models by exploiting efficient multi-scale features. In this paper, motivated by the human visual system (HVS) combining multi-scale features for perception, we propose to use pyramid features learning to build a DNN with hierarchical multi-scale features for distorted image quality prediction. Our model is based on both residual maps and distorted images in luminance domain, where the proposed network contains spatial pyramid pooling and feature pyramid from the network structure. Our proposed network is optimized in a deep end-to-end supervision manner. To validate the effectiveness of the proposed method, extensive experiments are conducted on four widely-used image quality assessment databases, demonstrating the superiority of our algorithm.



### Displacement-Invariant Cost Computation for Efficient Stereo Matching
- **Arxiv ID**: http://arxiv.org/abs/2012.00899v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.00899v1)
- **Published**: 2020-12-01 23:58:16+00:00
- **Updated**: 2020-12-01 23:58:16+00:00
- **Authors**: Yiran Zhong, Charles Loop, Wonmin Byeon, Stan Birchfield, Yuchao Dai, Kaihao Zhang, Alexey Kamenev, Thomas Breuel, Hongdong Li, Jan Kautz
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: Although deep learning-based methods have dominated stereo matching leaderboards by yielding unprecedented disparity accuracy, their inference time is typically slow, on the order of seconds for a pair of 540p images. The main reason is that the leading methods employ time-consuming 3D convolutions applied to a 4D feature volume. A common way to speed up the computation is to downsample the feature volume, but this loses high-frequency details. To overcome these challenges, we propose a \emph{displacement-invariant cost computation module} to compute the matching costs without needing a 4D feature volume. Rather, costs are computed by applying the same 2D convolution network on each disparity-shifted feature map pair independently. Unlike previous 2D convolution-based methods that simply perform context mapping between inputs and disparity maps, our proposed approach learns to match features between the two images. We also propose an entropy-based refinement strategy to refine the computed disparity map, which further improves speed by avoiding the need to compute a second disparity map on the right image. Extensive experiments on standard datasets (SceneFlow, KITTI, ETH3D, and Middlebury) demonstrate that our method achieves competitive accuracy with much less inference time. On typical image sizes, our method processes over 100 FPS on a desktop GPU, making our method suitable for time-critical applications such as autonomous driving. We also show that our approach generalizes well to unseen datasets, outperforming 4D-volumetric methods.



