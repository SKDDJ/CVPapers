# Arxiv Papers in cs.CV on 2020-12-02
### ReMP: Rectified Metric Propagation for Few-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2012.00904v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2012.00904v1)
- **Published**: 2020-12-02 00:07:53+00:00
- **Updated**: 2020-12-02 00:07:53+00:00
- **Authors**: Yang Zhao, Chunyuan Li, Ping Yu, Changyou Chen
- **Comment**: Tech Report, 11 pages
- **Journal**: None
- **Summary**: Few-shot learning features the capability of generalizing from a few examples. In this paper, we first identify that a discriminative feature space, namely a rectified metric space, that is learned to maintain the metric consistency from training to testing, is an essential component to the success of metric-based few-shot learning. Numerous analyses indicate that a simple modification of the objective can yield substantial performance gains. The resulting approach, called rectified metric propagation (ReMP), further optimizes an attentive prototype propagation network, and applies a repulsive force to make confident predictions. Extensive experiments demonstrate that the proposed ReMP is effective and efficient, and outperforms the state of the arts on various standard few-shot learning datasets.



### Visually Imperceptible Adversarial Patch Attacks on Digital Images
- **Arxiv ID**: http://arxiv.org/abs/2012.00909v3
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2012.00909v3)
- **Published**: 2020-12-02 00:47:56+00:00
- **Updated**: 2021-04-27 06:14:28+00:00
- **Authors**: Yaguan Qian, Jiamin Wang, Bin Wang, Shaoning Zeng, Zhaoquan Gu, Shouling Ji, Wassim Swaileh
- **Comment**: None
- **Journal**: None
- **Summary**: The vulnerability of deep neural networks (DNNs) to adversarial examples has attracted more attention. Many algorithms have been proposed to craft powerful adversarial examples. However, most of these algorithms modified the global or local region of pixels without taking network explanations into account. Hence, the perturbations are redundant, which are easily detected by human eyes. In this paper, we propose a novel method to generate local region perturbations. The main idea is to find a contributing feature region (CFR) of an image by simulating the human attention mechanism and then add perturbations to CFR. Furthermore, a soft mask matrix is designed on the basis of an activation map to finely represent the contributions of each pixel in CFR. With this soft mask, we develop a new loss function with inverse temperature to search for optimal perturbations in CFR. Due to the network explanations, the perturbations added to CFR are more effective than those added to other regions. Extensive experiments conducted on CIFAR-10 and ILSVRC2012 demonstrate the effectiveness of the proposed method, including attack success rate, imperceptibility, and transferability.



### MEVA: A Large-Scale Multiview, Multimodal Video Dataset for Activity Detection
- **Arxiv ID**: http://arxiv.org/abs/2012.00914v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.00914v1)
- **Published**: 2020-12-02 01:02:06+00:00
- **Updated**: 2020-12-02 01:02:06+00:00
- **Authors**: Kellie Corona, Katie Osterdahl, Roderic Collins, Anthony Hoogs
- **Comment**: 9 pages, 11 figures, to appear at WACV 2021. Dataset is available at
  https://mevadata.org
- **Journal**: None
- **Summary**: We present the Multiview Extended Video with Activities (MEVA) dataset, a new and very-large-scale dataset for human activity recognition. Existing security datasets either focus on activity counts by aggregating public video disseminated due to its content, which typically excludes same-scene background video, or they achieve persistence by observing public areas and thus cannot control for activity content. Our dataset is over 9300 hours of untrimmed, continuous video, scripted to include diverse, simultaneous activities, along with spontaneous background activity. We have annotated 144 hours for 37 activity types, marking bounding boxes of actors and props. Our collection observed approximately 100 actors performing scripted scenarios and spontaneous background activity over a three-week period at an access-controlled venue, collecting in multiple modalities with overlapping and non-overlapping indoor and outdoor viewpoints. The resulting data includes video from 38 RGB and thermal IR cameras, 42 hours of UAV footage, as well as GPS locations for the actors. 122 hours of annotation are sequestered in support of the NIST Activity in Extended Video (ActEV) challenge; the other 22 hours of annotation and the corresponding video are available on our website, along with an additional 306 hours of ground camera data, 4.6 hours of UAV data, and 9.6 hours of GPS logs. Additional derived data includes camera models geo-registering the outdoor cameras and a dense 3D point cloud model of the outdoor scene. The data was collected with IRB oversight and approval and released under a CC-BY-4.0 license.



### CPF: Learning a Contact Potential Field to Model the Hand-Object Interaction
- **Arxiv ID**: http://arxiv.org/abs/2012.00924v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.00924v4)
- **Published**: 2020-12-02 01:45:30+00:00
- **Updated**: 2021-09-11 03:31:45+00:00
- **Authors**: Lixin Yang, Xinyu Zhan, Kailin Li, Wenqiang Xu, Jiefeng Li, Cewu Lu
- **Comment**: ICCV 2021, (reduce PDF file size)
- **Journal**: None
- **Summary**: Modeling the hand-object (HO) interaction not only requires estimation of the HO pose, but also pays attention to the contact due to their interaction. Significant progress has been made in estimating hand and object separately with deep learning methods, simultaneous HO pose estimation and contact modeling has not yet been fully explored. In this paper, we present an explicit contact representation namely Contact Potential Field (CPF), and a learning-fitting hybrid framework namely MIHO to Modeling the Interaction of Hand and Object. In CPF, we treat each contacting HO vertex pair as a spring-mass system. Hence the whole system forms a potential field with minimal elastic energy at the grasp position. Extensive experiments on the two commonly used benchmarks have demonstrated that our method can achieve state-of-the-art in several reconstruction metrics, and allow us to produce more physically plausible HO pose even when the ground-truth exhibits severe interpenetration or disjointedness. Our code is available at https://github.com/lixiny/CPF.



### SemiNLL: A Framework of Noisy-Label Learning by Semi-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2012.00925v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2012.00925v1)
- **Published**: 2020-12-02 01:49:47+00:00
- **Updated**: 2020-12-02 01:49:47+00:00
- **Authors**: Zhuowei Wang, Jing Jiang, Bo Han, Lei Feng, Bo An, Gang Niu, Guodong Long
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning with noisy labels is a challenging task. Recent prominent methods that build on a specific sample selection (SS) strategy and a specific semi-supervised learning (SSL) model achieved state-of-the-art performance. Intuitively, better performance could be achieved if stronger SS strategies and SSL models are employed. Following this intuition, one might easily derive various effective noisy-label learning methods using different combinations of SS strategies and SSL models, which is, however, reinventing the wheel in essence. To prevent this problem, we propose SemiNLL, a versatile framework that combines SS strategies and SSL models in an end-to-end manner. Our framework can absorb various SS strategies and SSL backbones, utilizing their power to achieve promising performance. We also instantiate our framework with different combinations, which set the new state of the art on benchmark-simulated and real-world datasets with noisy labels.



### pi-GAN: Periodic Implicit Generative Adversarial Networks for 3D-Aware Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2012.00926v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2012.00926v2)
- **Published**: 2020-12-02 01:57:46+00:00
- **Updated**: 2021-04-05 23:18:10+00:00
- **Authors**: Eric R. Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu, Gordon Wetzstein
- **Comment**: None
- **Journal**: None
- **Summary**: We have witnessed rapid progress on 3D-aware image synthesis, leveraging recent advances in generative visual models and neural rendering. Existing approaches however fall short in two ways: first, they may lack an underlying 3D representation or rely on view-inconsistent rendering, hence synthesizing images that are not multi-view consistent; second, they often depend upon representation network architectures that are not expressive enough, and their results thus lack in image quality. We propose a novel generative model, named Periodic Implicit Generative Adversarial Networks ($\pi$-GAN or pi-GAN), for high-quality 3D-aware image synthesis. $\pi$-GAN leverages neural representations with periodic activation functions and volumetric rendering to represent scenes as view-consistent 3D representations with fine detail. The proposed approach obtains state-of-the-art results for 3D-aware image synthesis with multiple real and synthetic datasets.



### Improving Accuracy of Binary Neural Networks using Unbalanced Activation Distribution
- **Arxiv ID**: http://arxiv.org/abs/2012.00938v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2012.00938v2)
- **Published**: 2020-12-02 02:49:53+00:00
- **Updated**: 2021-03-30 04:52:40+00:00
- **Authors**: Hyungjun Kim, Jihoon Park, Changhun Lee, Jae-Joon Kim
- **Comment**: CVPR 2021, 10 pages, 10 figures
- **Journal**: None
- **Summary**: Binarization of neural network models is considered as one of the promising methods to deploy deep neural network models on resource-constrained environments such as mobile devices. However, Binary Neural Networks (BNNs) tend to suffer from severe accuracy degradation compared to the full-precision counterpart model. Several techniques were proposed to improve the accuracy of BNNs. One of the approaches is to balance the distribution of binary activations so that the amount of information in the binary activations becomes maximum. Based on extensive analysis, in stark contrast to previous work, we argue that unbalanced activation distribution can actually improve the accuracy of BNNs. We also show that adjusting the threshold values of binary activation functions results in the unbalanced distribution of the binary activation, which increases the accuracy of BNN models. Experimental results show that the accuracy of previous BNN models (e.g. XNOR-Net and Bi-Real-Net) can be improved by simply shifting the threshold values of binary activation functions without requiring any other modification.



### Tensor Completion via Convolutional Sparse Coding Regularization
- **Arxiv ID**: http://arxiv.org/abs/2012.00944v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2012.00944v2)
- **Published**: 2020-12-02 03:12:10+00:00
- **Updated**: 2021-05-06 08:29:49+00:00
- **Authors**: Zhebin Wu, Tianchi Liao, Chuan Chen, Cong Liu, Zibin Zheng, Xiongjun Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Tensor data often suffer from missing value problem due to the complex high-dimensional structure while acquiring them. To complete the missing information, lots of Low-Rank Tensor Completion (LRTC) methods have been proposed, most of which depend on the low-rank property of tensor data. In this way, the low-rank component of the original data could be recovered roughly. However, the shortcoming is that the detail information can not be fully restored, no matter the Sum of the Nuclear Norm (SNN) nor the Tensor Nuclear Norm (TNN) based methods. On the contrary, in the field of signal processing, Convolutional Sparse Coding (CSC) can provide a good representation of the high-frequency component of the image, which is generally associated with the detail component of the data. Nevertheless, CSC can not handle the low-frequency component well. To this end, we propose two novel methods, LRTC-CSC-I and LRTC-CSC-II, which adopt CSC as a supplementary regularization for LRTC to capture the high-frequency components. Therefore, the LRTC-CSC methods can not only solve the missing value problem but also recover the details. Moreover, the regularizer CSC can be trained with small samples due to the sparsity characteristic. Extensive experiments show the effectiveness of LRTC-CSC methods, and quantitative evaluation indicates that the performance of our models are superior to state-of-the-art methods.



### Two-Stage Single Image Reflection Removal with Reflection-Aware Guidance
- **Arxiv ID**: http://arxiv.org/abs/2012.00945v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.00945v2)
- **Published**: 2020-12-02 03:14:57+00:00
- **Updated**: 2021-02-21 07:53:31+00:00
- **Authors**: Yu Li, Ming Liu, Yaling Yi, Qince Li, Dongwei Ren, Wangmeng Zuo
- **Comment**: None
- **Journal**: None
- **Summary**: Removing undesired reflection from an image captured through a glass surface is a very challenging problem with many practical application scenarios. For improving reflection removal, cascaded deep models have been usually adopted to estimate the transmission in a progressive manner. However, most existing methods are still limited in exploiting the result in prior stage for guiding transmission estimation. In this paper, we present a novel two-stage network with reflection-aware guidance (RAGNet) for single image reflection removal (SIRR). To be specific, the reflection layer is firstly estimated due to that it generally is much simpler and is relatively easier to estimate. Reflectionaware guidance (RAG) module is then elaborated for better exploiting the estimated reflection in predicting transmission layer. By incorporating feature maps from the estimated reflection and observation, RAG can be used (i) to mitigate the effect of reflection from the observation, and (ii) to generate mask in partial convolution for mitigating the effect of deviating from linear combination hypothesis. A dedicated mask loss is further presented for reconciling the contributions of encoder and decoder features. Experiments on five commonly used datasets demonstrate the quantitative and qualitative superiority of our RAGNet in comparison to the state-of-the-art SIRR methods. The source code and pre-trained model are available at https://github.com/liyucs/RAGNet.



### Wide-Area Crowd Counting: Multi-View Fusion Networks for Counting in Large Scenes
- **Arxiv ID**: http://arxiv.org/abs/2012.00946v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.00946v2)
- **Published**: 2020-12-02 03:20:30+00:00
- **Updated**: 2022-05-02 16:45:19+00:00
- **Authors**: Qi Zhang, Antoni B. Chan
- **Comment**: Accepted to IJCV
- **Journal**: None
- **Summary**: Crowd counting in single-view images has achieved outstanding performance on existing counting datasets. However, single-view counting is not applicable to large and wide scenes (e.g., public parks, long subway platforms, or event spaces) because a single camera cannot capture the whole scene in adequate detail for counting, e.g., when the scene is too large to fit into the field-of-view of the camera, too long so that the resolution is too low on faraway crowds, or when there are too many large objects that occlude large portions of the crowd. Therefore, to solve the wide-area counting task requires multiple cameras with overlapping fields-of-view. In this paper, we propose a deep neural network framework for multi-view crowd counting, which fuses information from multiple camera views to predict a scene-level density map on the ground-plane of the 3D world. We consider three versions of the fusion framework: the late fusion model fuses camera-view density map; the naive early fusion model fuses camera-view feature maps; and the multi-view multi-scale early fusion model ensures that features aligned to the same ground-plane point have consistent scales. A rotation selection module further ensures consistent rotation alignment of the features. We test our 3 fusion models on 3 multi-view counting datasets, PETS2009, DukeMTMC, and a newly collected multi-view counting dataset containing a crowded street intersection. Our methods achieve state-of-the-art results compared to other multi-view counting baselines.



### Ship Detection: Parameter Server Variant
- **Arxiv ID**: http://arxiv.org/abs/2012.00953v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.DC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2012.00953v1)
- **Published**: 2020-12-02 03:39:24+00:00
- **Updated**: 2020-12-02 03:39:24+00:00
- **Authors**: Benjamin Smith
- **Comment**: 20 pages
- **Journal**: None
- **Summary**: Deep learning ship detection in satellite optical imagery suffers from false positive occurrences with clouds, landmasses, and man-made objects that interfere with correct classification of ships, typically limiting class accuracy scores to 88\%. This work explores the tensions between customization strategies, class accuracy rates, training times, and costs in cloud based solutions. We demonstrate how a custom U-Net can achieve 92\% class accuracy over a validation dataset and 68\% over a target dataset with 90\% confidence. We also compare a single node architecture with a parameter server variant whose workers act as a boosting mechanism. The parameter server variant outperforms class accuracy on the target dataset reaching 73\% class accuracy compared to the best single node approach. A comparative investigation on the systematic performance of the single node and parameter server variant architectures is discussed with support from empirical findings.



### PWCLO-Net: Deep LiDAR Odometry in 3D Point Clouds Using Hierarchical Embedding Mask Optimization
- **Arxiv ID**: http://arxiv.org/abs/2012.00972v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.00972v2)
- **Published**: 2020-12-02 05:23:41+00:00
- **Updated**: 2021-04-02 05:20:05+00:00
- **Authors**: Guangming Wang, Xinrui Wu, Zhe Liu, Hesheng Wang
- **Comment**: Accepted to CVPR 2021
- **Journal**: None
- **Summary**: A novel 3D point cloud learning model for deep LiDAR odometry, named PWCLO-Net, using hierarchical embedding mask optimization is proposed in this paper. In this model, the Pyramid, Warping, and Cost volume (PWC) structure for the LiDAR odometry task is built to refine the estimated pose in a coarse-to-fine approach hierarchically. An attentive cost volume is built to associate two point clouds and obtain embedding motion patterns. Then, a novel trainable embedding mask is proposed to weigh the local motion patterns of all points to regress the overall pose and filter outlier points. The estimated current pose is used to warp the first point cloud to bridge the distance to the second point cloud, and then the cost volume of the residual motion is built. At the same time, the embedding mask is optimized hierarchically from coarse to fine to obtain more accurate filtering information for pose refinement. The trainable pose warp-refinement process is iteratively used to make the pose estimation more robust for outliers. The superior performance and effectiveness of our LiDAR odometry model are demonstrated on KITTI odometry dataset. Our method outperforms all recent learning-based methods and outperforms the geometry-based approach, LOAM with mapping optimization, on most sequences of KITTI odometry dataset.Our source codes will be released on https://github.com/IRMVLab/PWCLONet.



### Learning Vector Quantized Shape Code for Amodal Blastomere Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2012.00985v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.00985v1)
- **Published**: 2020-12-02 06:17:28+00:00
- **Updated**: 2020-12-02 06:17:28+00:00
- **Authors**: Won-Dong Jang, Donglai Wei, Xingxuan Zhang, Brian Leahy, Helen Yang, James Tompkin, Dalit Ben-Yosef, Daniel Needleman, Hanspeter Pfister
- **Comment**: 9 pages including references, 9 figures, 5 tables, in preparation for
  journal submission
- **Journal**: None
- **Summary**: Blastomere instance segmentation is important for analyzing embryos' abnormality. To measure the accurate shapes and sizes of blastomeres, their amodal segmentation is necessary. Amodal instance segmentation aims to recover the complete silhouette of an object even when the object is not fully visible. For each detected object, previous methods directly regress the target mask from input features. However, images of an object under different amounts of occlusion should have the same amodal mask output, which makes it harder to train the regression model. To alleviate the problem, we propose to classify input features into intermediate shape codes and recover complete object shapes from them. First, we pre-train the Vector Quantized Variational Autoencoder (VQ-VAE) model to learn these discrete shape codes from ground truth amodal masks. Then, we incorporate the VQ-VAE model into the amodal instance segmentation pipeline with an additional refinement module. We also detect an occlusion map to integrate occlusion information with a backbone feature. As such, our network faithfully detects bounding boxes of amodal objects. On an internal embryo cell image benchmark, the proposed method outperforms previous state-of-the-art methods. To show generalizability, we show segmentation results on the public KINS natural image benchmark. To examine the learned shape codes and model design choices, we perform ablation studies on a synthetic dataset of simple overlaid shapes. Our method would enable accurate measurement of blastomeres in in vitro fertilization (IVF) clinics, which potentially can increase IVF success rate.



### PV-RAFT: Point-Voxel Correlation Fields for Scene Flow Estimation of Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2012.00987v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.00987v2)
- **Published**: 2020-12-02 06:20:54+00:00
- **Updated**: 2021-05-12 08:44:23+00:00
- **Authors**: Yi Wei, Ziyi Wang, Yongming Rao, Jiwen Lu, Jie Zhou
- **Comment**: Accepted to CVPR 2021
- **Journal**: None
- **Summary**: In this paper, we propose a Point-Voxel Recurrent All-Pairs Field Transforms (PV-RAFT) method to estimate scene flow from point clouds. Since point clouds are irregular and unordered, it is challenging to efficiently extract features from all-pairs fields in the 3D space, where all-pairs correlations play important roles in scene flow estimation. To tackle this problem, we present point-voxel correlation fields, which capture both local and long-range dependencies of point pairs. To capture point-based correlations, we adopt the K-Nearest Neighbors search that preserves fine-grained information in the local region. By voxelizing point clouds in a multi-scale manner, we construct pyramid correlation voxels to model long-range correspondences. Integrating these two types of correlations, our PV-RAFT makes use of all-pairs relations to handle both small and large displacements. We evaluate the proposed method on the FlyingThings3D and KITTI Scene Flow 2015 datasets. Experimental results show that PV-RAFT outperforms state-of-the-art methods by remarkable margins.



### An Once-for-All Budgeted Pruning Framework for ConvNets Considering Input Resolution
- **Arxiv ID**: http://arxiv.org/abs/2012.00996v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.00996v1)
- **Published**: 2020-12-02 07:09:12+00:00
- **Updated**: 2020-12-02 07:09:12+00:00
- **Authors**: Wenyu Sun, Jian Cao, Pengtao Xu, Xiangcheng Liu, Pu Li
- **Comment**: None
- **Journal**: None
- **Summary**: We propose an efficient once-for-all budgeted pruning framework (OFARPruning) to find many compact network structures close to winner tickets in the early training stage considering the effect of input resolution during the pruning process. In structure searching stage, we utilize cosine similarity to measure the similarity of the pruning mask to get high-quality network structures with low energy and time consumption. After structure searching stage, our proposed method randomly sample the compact structures with different pruning rates and input resolution to achieve joint optimization. Ultimately, we can obtain a cohort of compact networks adaptive to various resolution to meet dynamic FLOPs constraints on different edge devices with only once training. The experiments based on image classification and object detection show that OFARPruning has a higher accuracy than the once-for-all compression methods such as US-Net and MutualNet (1-2% better with less FLOPs), and achieve the same even higher accuracy as the conventional pruning methods (72.6% vs. 70.5% on MobileNetv2 under 170 MFLOPs) with much higher efficiency.



### q-SNE: Visualizing Data using q-Gaussian Distributed Stochastic Neighbor Embedding
- **Arxiv ID**: http://arxiv.org/abs/2012.00999v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.00999v1)
- **Published**: 2020-12-02 07:21:59+00:00
- **Updated**: 2020-12-02 07:21:59+00:00
- **Authors**: Motoshi Abe, Junichi Miyao, Takio Kurita
- **Comment**: This paper is accepted ICPR2020. Code on Python is here
  (https://github.com/i13abe/q-SNE)
- **Journal**: None
- **Summary**: The dimensionality reduction has been widely introduced to use the high-dimensional data for regression, classification, feature analysis, and visualization. As the one technique of dimensionality reduction, a stochastic neighbor embedding (SNE) was introduced. The SNE leads powerful results to visualize high-dimensional data by considering the similarity between the local Gaussian distributions of high and low-dimensional space. To improve the SNE, a t-distributed stochastic neighbor embedding (t-SNE) was also introduced. To visualize high-dimensional data, the t-SNE leads to more powerful and flexible visualization on 2 or 3-dimensional mapping than the SNE by using a t-distribution as the distribution of low-dimensional data. Recently, Uniform manifold approximation and projection (UMAP) is proposed as a dimensionality reduction technique. We present a novel technique called a q-Gaussian distributed stochastic neighbor embedding (q-SNE). The q-SNE leads to more powerful and flexible visualization on 2 or 3-dimensional mapping than the t-SNE and the SNE by using a q-Gaussian distribution as the distribution of low-dimensional data. The q-Gaussian distribution includes the Gaussian distribution and the t-distribution as the special cases with q=1.0 and q=2.0. Therefore, the q-SNE can also express the t-SNE and the SNE by changing the parameter q, and this makes it possible to find the best visualization by choosing the parameter q. We show the performance of q-SNE as visualization on 2-dimensional mapping and classification by k-Nearest Neighbors (k-NN) classifier in embedded space compared with SNE, t-SNE, and UMAP by using the datasets MNIST, COIL-20, OlivettiFaces, FashionMNIST, and Glove.



### Artist, Style And Year Classification Using Face Recognition And Clustering With Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2012.01009v1
- **DOI**: 10.5121/csit.2020.101604
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.01009v1)
- **Published**: 2020-12-02 08:01:27+00:00
- **Updated**: 2020-12-02 08:01:27+00:00
- **Authors**: Doruk Pancaroglu
- **Comment**: 14 pages, SIPO 2020
- **Journal**: None
- **Summary**: Artist, year and style classification of fine-art paintings are generally achieved using standard image classification methods, image segmentation, or more recently, convolutional neural networks (CNNs). This works aims to use newly developed face recognition methods such as FaceNet that use CNNs to cluster fine-art paintings using the extracted faces in the paintings, which are found abundantly. A dataset consisting of over 80,000 paintings from over 1000 artists is chosen, and three separate face recognition and clustering tasks are performed. The produced clusters are analyzed by the file names of the paintings and the clusters are named by their majority artist, year range, and style. The clusters are further analyzed and their performance metrics are calculated. The study shows promising results as the artist, year, and styles are clustered with an accuracy of 58.8, 63.7, and 81.3 percent, while the clusters have an average purity of 63.1, 72.4, and 85.9 percent.



### Learning Order Parameters from Videos of Dynamical Phases for Skyrmions with Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2012.06308v1
- **DOI**: 10.1103/PhysRevApplied.16.014005
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2012.06308v1)
- **Published**: 2020-12-02 08:17:32+00:00
- **Updated**: 2020-12-02 08:17:32+00:00
- **Authors**: Weidi Wang, Zeyuan Wang, Yinghui Zhang, Bo Sun, Ke Xia
- **Comment**: 15 pages, 15 figures
- **Journal**: Phys. Rev. Applied 16, 014005 (2021)
- **Summary**: The ability to recognize dynamical phenomena (e.g., dynamical phases) and dynamical processes in physical events from videos, then to abstract physical concepts and reveal physical laws, lies at the core of human intelligence. The main purposes of this paper are to use neural networks for classifying the dynamical phases of some videos and to demonstrate that neural networks can learn physical concepts from them. To this end, we employ multiple neural networks to recognize the static phases (image format) and dynamical phases (video format) of a particle-based skyrmion model. Our results show that neural networks, without any prior knowledge, can not only correctly classify these phases, but also predict the phase boundaries which agree with those obtained by simulation. We further propose a parameter visualization scheme to interpret what neural networks have learned. We show that neural networks can learn two order parameters from videos of dynamical phases and predict the critical values of two order parameters. Finally, we demonstrate that only two order parameters are needed to identify videos of skyrmion dynamical phases. It shows that this parameter visualization scheme can be used to determine how many order parameters are needed to fully recognize the input phases. Our work sheds light on the future use of neural networks in discovering new physical concepts and revealing unknown yet physical laws from videos.



### MAAD-Face: A Massively Annotated Attribute Dataset for Face Images
- **Arxiv ID**: http://arxiv.org/abs/2012.01030v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.01030v2)
- **Published**: 2020-12-02 08:54:26+00:00
- **Updated**: 2021-06-28 06:40:49+00:00
- **Authors**: Philipp Terhörst, Daniel Fährmann, Jan Niklas Kolf, Naser Damer, Florian Kirchbuchner, Arjan Kuijper
- **Comment**: Accepted in IEEE Transactions on Information Forensics and Security
- **Journal**: None
- **Summary**: Soft-biometrics play an important role in face biometrics and related fields since these might lead to biased performances, threatens the user's privacy, or are valuable for commercial aspects. Current face databases are specifically constructed for the development of face recognition applications. Consequently, these databases contain large amount of face images but lack in the number of attribute annotations and the overall annotation correctness. In this work, we propose MAADFace, a new face annotations database that is characterized by the large number of its high-quality attribute annotations. MAADFace is build on the VGGFace2 database and thus, consists of 3.3M faces of over 9k individuals. Using a novel annotation transfer-pipeline that allows an accurate label-transfer from multiple source-datasets to a target-dataset, MAAD-Face consists of 123.9M attribute annotations of 47 different binary attributes. Consequently, it provides 15 and 137 times more attribute labels than CelebA and LFW. Our investigation on the annotation quality by three human evaluators demonstrated the superiority of the MAAD-Face annotations over existing databases. Additionally, we make use of the large amount of high-quality annotations from MAAD-Face to study the viability of soft-biometrics for recognition, providing insights about which attributes support genuine and imposter decisions. The MAAD-Face annotations dataset is publicly available.



### A Photogrammetry-based Framework to Facilitate Image-based Modeling and Automatic Camera Tracking
- **Arxiv ID**: http://arxiv.org/abs/2012.01044v1
- **DOI**: 10.5220/0010319801060112
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2012.01044v1)
- **Published**: 2020-12-02 09:26:37+00:00
- **Updated**: 2020-12-02 09:26:37+00:00
- **Authors**: Sebastian Bullinger, Christoph Bodensteiner, Michael Arens
- **Comment**: None
- **Journal**: In Proceedings of the 16th International Joint Conference on
  Computer Vision, Imaging and Computer Graphics Theory and Applications -
  Volume 1: GRAPP, 106-112, 2021
- **Summary**: We propose a framework that extends Blender to exploit Structure from Motion (SfM) and Multi-View Stereo (MVS) techniques for image-based modeling tasks such as sculpting or camera and motion tracking. Applying SfM allows us to determine camera motions without manually defining feature tracks or calibrating the cameras used to capture the image data. With MVS we are able to automatically compute dense scene models, which is not feasible with the built-in tools of Blender. Currently, our framework supports several state-of-the-art SfM and MVS pipelines. The modular system design enables us to integrate further approaches without additional effort. The framework is publicly available as an open source software package.



### Learning Universal Shape Dictionary for Realtime Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2012.01050v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.01050v1)
- **Published**: 2020-12-02 09:44:49+00:00
- **Updated**: 2020-12-02 09:44:49+00:00
- **Authors**: Tutian Tang, Wenqiang Xu, Ruolin Ye, Lixin Yang, Cewu Lu
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel explicit shape representation for instance segmentation.   Based on how to model the object shape, current instance segmentation systems can be divided into two categories, implicit and explicit models. The implicit methods, which represent the object mask/contour by intractable network parameters, and produce it through pixel-wise classification, are predominant. However, the explicit methods, which parameterize the shape with simple and explainable models, are less explored. Since the operations to generate the final shape are light-weighted, the explicit methods have a clear speed advantage over implicit methods, which is crucial for real-world applications. The proposed USD-Seg adopts a linear model, sparse coding with dictionary, for object shapes.   First, it learns a dictionary from a large collection of shape datasets, making any shape being able to be decomposed into a linear combination through the dictionary.   Hence the name "Universal Shape Dictionary".   Then it adds a simple shape vector regression head to ordinary object detector, giving the detector segmentation ability with minimal overhead.   For quantitative evaluation, we use both average precision (AP) and the proposed Efficiency of AP (AP$_E$) metric, which intends to also measure the computational consumption of the framework to cater to the requirements of real-world applications. We report experimental results on the challenging COCO dataset, in which our single model on a single Titan Xp GPU achieves 35.8 AP and 27.8 AP$_E$ at 65 fps with YOLOv4 as base detector, 34.1 AP and 28.6 AP$_E$ at 12 fps with FCOS as base detector.



### A Temporally Consistent Image-based Sun Tracking Algorithm for Solar Energy Forecasting Applications
- **Arxiv ID**: http://arxiv.org/abs/2012.01059v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2012.01059v3)
- **Published**: 2020-12-02 09:59:45+00:00
- **Updated**: 2022-10-25 14:31:31+00:00
- **Authors**: Quentin Paletta, Joan Lasenby
- **Comment**: Accepted as a workshop paper at NeurIPS 2020
- **Journal**: None
- **Summary**: Improving irradiance forecasting is critical to further increase the share of solar in the energy mix. On a short time scale, fish-eye cameras on the ground are used to capture cloud displacements causing the local variability of the electricity production. As most of the solar radiation comes directly from the Sun, current forecasting approaches use its position in the image as a reference to interpret the cloud cover dynamics. However, existing Sun tracking methods rely on external data and a calibration of the camera, which requires access to the device. To address these limitations, this study introduces an image-based Sun tracking algorithm to localise the Sun in the image when it is visible and interpolate its daily trajectory from past observations. We validate the method on a set of sky images collected over a year at SIRTA's lab. Experimental results show that the proposed method provides robust smooth Sun trajectories with a mean absolute error below 1% of the image size.



### PlueckerNet: Learn to Register 3D Line Reconstructions
- **Arxiv ID**: http://arxiv.org/abs/2012.01096v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.01096v1)
- **Published**: 2020-12-02 11:31:56+00:00
- **Updated**: 2020-12-02 11:31:56+00:00
- **Authors**: Liu Liu, Hongdong Li, Haodong Yao, Ruyi Zha
- **Comment**: 12 pages
- **Journal**: None
- **Summary**: Aligning two partially-overlapped 3D line reconstructions in Euclidean space is challenging, as we need to simultaneously solve correspondences and relative pose between line reconstructions. This paper proposes a neural network based method and it has three modules connected in sequence: (i) a Multilayer Perceptron (MLP) based network takes Pluecker representations of lines as inputs, to extract discriminative line-wise features and matchabilities (how likely each line is going to have a match), (ii) an Optimal Transport (OT) layer takes two-view line-wise features and matchabilities as inputs to estimate a 2D joint probability matrix, with each item describes the matchness of a line pair, and (iii) line pairs with Top-K matching probabilities are fed to a 2-line minimal solver in a RANSAC framework to estimate a six Degree-of-Freedom (6-DoF) rigid transformation. Experiments on both indoor and outdoor datasets show that the registration (rotation and translation) precision of our method outperforms baselines significantly.



### Efficient Depth Completion Using Learned Bases
- **Arxiv ID**: http://arxiv.org/abs/2012.01110v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.01110v1)
- **Published**: 2020-12-02 11:57:37+00:00
- **Updated**: 2020-12-02 11:57:37+00:00
- **Authors**: Yiran Zhong, Yuchao Dai, Hongdong Li
- **Comment**: This work was accomplished in 2017
- **Journal**: None
- **Summary**: In this paper, we propose a new global geometry constraint for depth completion. By assuming depth maps often lay on low dimensional subspaces, a dense depth map can be approximated by a weighted sum of full-resolution principal depth bases. The principal components of depth fields can be learned from natural depth maps. The given sparse depth points are served as a data term to constrain the weighting process. When the input depth points are too sparse, the recovered dense depth maps are often over smoothed. To address this issue, we add a colour-guided auto-regression model as another regularization term. It assumes the reconstructed depth maps should share the same nonlocal similarity in the accompanying colour image. Our colour-guided PCA depth completion method has closed-form solutions, thus can be efficiently solved and is significantly more accurate than PCA only method. Extensive experiments on KITTI and Middlebury datasets demonstrate the superior performance of our proposed method.



### Single-Shot Freestyle Dance Reenactment
- **Arxiv ID**: http://arxiv.org/abs/2012.01158v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2012.01158v2)
- **Published**: 2020-12-02 12:57:43+00:00
- **Updated**: 2021-03-21 14:11:57+00:00
- **Authors**: Oran Gafni, Oron Ashual, Lior Wolf
- **Comment**: None
- **Journal**: None
- **Summary**: The task of motion transfer between a source dancer and a target person is a special case of the pose transfer problem, in which the target person changes their pose in accordance with the motions of the dancer.   In this work, we propose a novel method that can reanimate a single image by arbitrary video sequences, unseen during training. The method combines three networks: (i) a segmentation-mapping network, (ii) a realistic frame-rendering network, and (iii) a face refinement network. By separating this task into three stages, we are able to attain a novel sequence of realistic frames, capturing natural motion and appearance. Our method obtains significantly better visual quality than previous methods and is able to animate diverse body types and appearances, which are captured in challenging poses, as shown in the experiments and supplementary video.



### Sparse Convolutions on Continuous Domains for Point Cloud and Event Stream Networks
- **Arxiv ID**: http://arxiv.org/abs/2012.01170v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.01170v1)
- **Published**: 2020-12-02 13:05:02+00:00
- **Updated**: 2020-12-02 13:05:02+00:00
- **Authors**: Dominic Jack, Frederic Maire, Simon Denman, Anders Eriksson
- **Comment**: ACCV2020
- **Journal**: None
- **Summary**: Image convolutions have been a cornerstone of a great number of deep learning advances in computer vision. The research community is yet to settle on an equivalent operator for sparse, unstructured continuous data like point clouds and event streams however. We present an elegant sparse matrix-based interpretation of the convolution operator for these cases, which is consistent with the mathematical definition of convolution and efficient during training. On benchmark point cloud classification problems we demonstrate networks built with these operations can train an order of magnitude or more faster than top existing methods, whilst maintaining comparable accuracy and requiring a tiny fraction of the memory. We also apply our operator to event stream processing, achieving state-of-the-art results on multiple tasks with streams of hundreds of thousands of events.



### Assessing the Influencing Factors on the Accuracy of Underage Facial Age Estimation
- **Arxiv ID**: http://arxiv.org/abs/2012.01179v1
- **DOI**: 10.1109/CyberSecurity49315.2020.9138851
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.01179v1)
- **Published**: 2020-12-02 13:09:56+00:00
- **Updated**: 2020-12-02 13:09:56+00:00
- **Authors**: Felix Anda, Brett A. Becker, David Lillis, Nhien-An Le-Khac, Mark Scanlon
- **Comment**: None
- **Journal**: The 6th IEEE International Conference on Cyber Security and
  Protection of Digital Services (Cyber Security), Dublin, Ireland, June 2020
- **Summary**: Swift response to the detection of endangered minors is an ongoing concern for law enforcement. Many child-focused investigations hinge on digital evidence discovery and analysis. Automated age estimation techniques are needed to aid in these investigations to expedite this evidence discovery process, and decrease investigator exposure to traumatic material. Automated techniques also show promise in decreasing the overflowing backlog of evidence obtained from increasing numbers of devices and online services. A lack of sufficient training data combined with natural human variance has been long hindering accurate automated age estimation -- especially for underage subjects. This paper presented a comprehensive evaluation of the performance of two cloud age estimation services (Amazon Web Service's Rekognition service and Microsoft Azure's Face API) against a dataset of over 21,800 underage subjects. The objective of this work is to evaluate the influence that certain human biometric factors, facial expressions, and image quality (i.e. blur, noise, exposure and resolution) have on the outcome of automated age estimation services. A thorough evaluation allows us to identify the most influential factors to be overcome in future age estimation systems.



### Classifying bacteria clones using attention-based deep multiple instance learning interpreted by persistence homology
- **Arxiv ID**: http://arxiv.org/abs/2012.01189v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2012.01189v2)
- **Published**: 2020-12-02 13:20:39+00:00
- **Updated**: 2021-07-23 11:27:58+00:00
- **Authors**: Adriana Borowa, Dawid Rymarczyk, Dorota Ochońska, Monika Brzychczy-Włoch, Bartosz Zieliński
- **Comment**: Published at the International Joint Conferences on Neural Networks
- **Journal**: 978-0-7381-3366-9/21, 2021
- **Summary**: In this work, we analyze if it is possible to distinguish between different clones of the same bacteria species (Klebsiella pneumoniae) based only on microscopic images. It is a challenging task, previously considered impossible due to the high clones similarity. For this purpose, we apply a multi-step algorithm with attention-based multiple instance learning. Except for obtaining accuracy at the level of 0.9, we introduce extensive interpretability based on CellProfiler and persistence homology, increasing the understandability and trust in the model.



### Meta-Cognition-Based Simple And Effective Approach To Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2012.01201v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2012.01201v1)
- **Published**: 2020-12-02 13:36:51+00:00
- **Updated**: 2020-12-02 13:36:51+00:00
- **Authors**: Sannidhi P Kumar, Chandan Gautam, Suresh Sundaram
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, many researchers have attempted to improve deep learning-based object detection models, both in terms of accuracy and operational speeds. However, frequently, there is a trade-off between speed and accuracy of such models, which encumbers their use in practical applications such as autonomous navigation. In this paper, we explore a meta-cognitive learning strategy for object detection to improve generalization ability while at the same time maintaining detection speed. The meta-cognitive method selectively samples the object instances in the training dataset to reduce overfitting. We use YOLO v3 Tiny as a base model for the work and evaluate the performance using the MS COCO dataset. The experimental results indicate an improvement in absolute precision of 2.6% (minimum), and 4.4% (maximum), with no overhead to inference time.



### Learning Delaunay Surface Elements for Mesh Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2012.01203v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2012.01203v2)
- **Published**: 2020-12-02 13:42:07+00:00
- **Updated**: 2021-05-06 17:17:14+00:00
- **Authors**: Marie-Julie Rakotosaona, Paul Guerrero, Noam Aigerman, Niloy Mitra, Maks Ovsjanikov
- **Comment**: None
- **Journal**: CVPR 2021
- **Summary**: We present a method for reconstructing triangle meshes from point clouds. Existing learning-based methods for mesh reconstruction mostly generate triangles individually, making it hard to create manifold meshes. We leverage the properties of 2D Delaunay triangulations to construct a mesh from manifold surface elements. Our method first estimates local geodesic neighborhoods around each point. We then perform a 2D projection of these neighborhoods using a learned logarithmic map. A Delaunay triangulation in this 2D domain is guaranteed to produce a manifold patch, which we call a Delaunay surface element. We synchronize the local 2D projections of neighboring elements to maximize the manifoldness of the reconstructed mesh. Our results show that we achieve better overall manifoldness of our reconstructed meshes than current methods to reconstruct meshes with arbitrary topology. Our code, data and pretrained models can be found online: https://github.com/mrakotosaon/dse-meshing



### Unsupervised Neural Domain Adaptation for Document Image Binarization
- **Arxiv ID**: http://arxiv.org/abs/2012.01204v2
- **DOI**: 10.1016/j.patcog.2021.108099
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.01204v2)
- **Published**: 2020-12-02 13:42:38+00:00
- **Updated**: 2021-07-01 09:44:37+00:00
- **Authors**: Francisco J. Castellanos, Antonio-Javier Gallego, Jorge Calvo-Zaragoza
- **Comment**: None
- **Journal**: Pattern Recognition, 2021, Vol. 119, p. 108099
- **Summary**: Binarization is a well-known image processing task, whose objective is to separate the foreground of an image from the background. One of the many tasks for which it is useful is that of preprocessing document images in order to identify relevant information, such as text or symbols. The wide variety of document types, alphabets, and formats makes binarization challenging. There are multiple proposals with which to solve this problem, from classical manually-adjusted methods, to more recent approaches based on machine learning. The latter techniques require a large amount of training data in order to obtain good results; however, labeling a portion of each existing collection of documents is not feasible in practice. This is a common problem in supervised learning, which can be addressed by using the so-called Domain Adaptation (DA) techniques. These techniques take advantage of the knowledge learned in one domain, for which labeled data are available, to apply it to other domains for which there are no labeled data. This paper proposes a method that combines neural networks and DA in order to carry out unsupervised document binarization. However, when both the source and target domains are very similar, this adaptation could be detrimental. Our methodology, therefore, first measures the similarity between domains in an innovative manner in order to determine whether or not it is appropriate to apply the adaptation process. The results reported in the experimentation, when evaluating up to 20 possible combinations among five different domains, show that our proposal successfully deals with the binarization of new document domains without the need for labeled data.



### Learning Spatial Attention for Face Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2012.01211v2
- **DOI**: 10.1109/TIP.2020.3043093
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.01211v2)
- **Published**: 2020-12-02 13:54:25+00:00
- **Updated**: 2020-12-04 12:33:42+00:00
- **Authors**: Chaofeng Chen, Dihong Gong, Hao Wang, Zhifeng Li, Kwan-Yee K. Wong
- **Comment**: TIP 2020. Codes are available at
  https://github.com/chaofengc/Face-SPARNet
- **Journal**: None
- **Summary**: General image super-resolution techniques have difficulties in recovering detailed face structures when applying to low resolution face images. Recent deep learning based methods tailored for face images have achieved improved performance by jointly trained with additional task such as face parsing and landmark prediction. However, multi-task learning requires extra manually labeled data. Besides, most of the existing works can only generate relatively low resolution face images (e.g., $128\times128$), and their applications are therefore limited. In this paper, we introduce a novel SPatial Attention Residual Network (SPARNet) built on our newly proposed Face Attention Units (FAUs) for face super-resolution. Specifically, we introduce a spatial attention mechanism to the vanilla residual blocks. This enables the convolutional layers to adaptively bootstrap features related to the key face structures and pay less attention to those less feature-rich regions. This makes the training more effective and efficient as the key face structures only account for a very small portion of the face image. Visualization of the attention maps shows that our spatial attention network can capture the key face structures well even for very low resolution faces (e.g., $16\times16$). Quantitative comparisons on various kinds of metrics (including PSNR, SSIM, identity similarity, and landmark detection) demonstrate the superiority of our method over current state-of-the-arts. We further extend SPARNet with multi-scale discriminators, named as SPARNetHD, to produce high resolution results (i.e., $512\times512$). We show that SPARNetHD trained with synthetic data cannot only produce high quality and high resolution outputs for synthetically degraded face images, but also show good generalization ability to real world low quality face images.



### Curiosity-driven 3D Object Detection Without Labels
- **Arxiv ID**: http://arxiv.org/abs/2012.01230v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.01230v3)
- **Published**: 2020-12-02 14:17:16+00:00
- **Updated**: 2021-10-15 19:11:40+00:00
- **Authors**: David Griffiths, Jan Boehm, Tobias Ritschel
- **Comment**: 19 pages, 17 figures
- **Journal**: None
- **Summary**: In this paper we set out to solve the task of 6-DOF 3D object detection from 2D images, where the only supervision is a geometric representation of the objects we aim to find. In doing so, we remove the need for 6-DOF labels (i.e., position, orientation etc.), allowing our network to be trained on unlabeled images in a self-supervised manner. We achieve this through a neural network which learns an explicit scene parameterization which is subsequently passed into a differentiable renderer. We analyze why analysis-by-synthesis-like losses for supervision of 3D scene structure using differentiable rendering is not practical, as it almost always gets stuck in local minima of visual ambiguities. This can be overcome by a novel form of training, where an additional network is employed to steer the optimization itself to explore the entire parameter space i.e., to be curious, and hence, to resolve those ambiguities and find workable minima.



### Channel Attention Networks for Robust MR Fingerprinting Matching
- **Arxiv ID**: http://arxiv.org/abs/2012.01241v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2012.01241v1)
- **Published**: 2020-12-02 14:34:40+00:00
- **Updated**: 2020-12-02 14:34:40+00:00
- **Authors**: Refik Soyak, Ebru Navruz, Eda Ozgu Ersoy, Gastao Cruz, Claudia Prieto, Andrew P. King, Devrim Unay, Ilkay Oksuz
- **Comment**: None
- **Journal**: None
- **Summary**: Magnetic Resonance Fingerprinting (MRF) enables simultaneous mapping of multiple tissue parameters such as T1 and T2 relaxation times. The working principle of MRF relies on varying acquisition parameters pseudo-randomly, so that each tissue generates its unique signal evolution during scanning. Even though MRF provides faster scanning, it has disadvantages such as erroneous and slow generation of the corresponding parametric maps, which needs to be improved. Moreover, there is a need for explainable architectures for understanding the guiding signals to generate accurate parametric maps. In this paper, we addressed both of these shortcomings by proposing a novel neural network architecture consisting of a channel-wise attention module and a fully convolutional network. The proposed approach, evaluated over 3 simulated MRF signals, reduces error in the reconstruction of tissue parameters by 8.88% for T1 and 75.44% for T2 with respect to state-of-the-art methods. Another contribution of this study is a new channel selection method: attention-based channel selection. Furthermore, the effect of patch size and temporal frames of MRF signal on channel reduction are analyzed by employing a channel-wise attention.



### Vision-based Drone Flocking in Outdoor Environments
- **Arxiv ID**: http://arxiv.org/abs/2012.01245v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG, cs.MA
- **Links**: [PDF](http://arxiv.org/pdf/2012.01245v2)
- **Published**: 2020-12-02 14:44:40+00:00
- **Updated**: 2021-02-16 10:13:38+00:00
- **Authors**: Fabian Schilling, Fabrizio Schiano, Dario Floreano
- **Comment**: 8 pages, 8 figures, accepted for publication in the IEEE Robotics and
  Automation Letters (RA-L) on February 2, 2021
- **Journal**: None
- **Summary**: Decentralized deployment of drone swarms usually relies on inter-agent communication or visual markers that are mounted on the vehicles to simplify their mutual detection. This letter proposes a vision-based detection and tracking algorithm that enables groups of drones to navigate without communication or visual markers. We employ a convolutional neural network to detect and localize nearby agents onboard the quadcopters in real-time. Rather than manually labeling a dataset, we automatically annotate images to train the neural network using background subtraction by systematically flying a quadcopter in front of a static camera. We use a multi-agent state tracker to estimate the relative positions and velocities of nearby agents, which are subsequently fed to a flocking algorithm for high-level control. The drones are equipped with multiple cameras to provide omnidirectional visual inputs. The camera setup ensures the safety of the flock by avoiding blind spots regardless of the agent configuration. We evaluate the approach with a group of three real quadcopters that are controlled using the proposed vision-based flocking algorithm. The results show that the drones can safely navigate in an outdoor environment despite substantial background clutter and difficult lighting conditions. The source code, image dataset, and trained detection model are available at https://github.com/lis-epfl/vswarm.



### Attention-gating for improved radio galaxy classification
- **Arxiv ID**: http://arxiv.org/abs/2012.01248v2
- **DOI**: 10.1093/mnras/staa3946
- **Categories**: **astro-ph.GA**, astro-ph.IM, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2012.01248v2)
- **Published**: 2020-12-02 14:49:53+00:00
- **Updated**: 2021-02-01 13:09:41+00:00
- **Authors**: Micah Bowles, Anna M. M. Scaife, Fiona Porter, Hongming Tang, David J. Bastien
- **Comment**: 18 pages, 16 figures, Published in MNRAS
- **Journal**: MNRAS 501 (2021) 4579-4595
- **Summary**: In this work we introduce attention as a state of the art mechanism for classification of radio galaxies using convolutional neural networks. We present an attention-based model that performs on par with previous classifiers while using more than 50% fewer parameters than the next smallest classic CNN application in this field. We demonstrate quantitatively how the selection of normalisation and aggregation methods used in attention-gating can affect the output of individual models, and show that the resulting attention maps can be used to interpret the classification choices made by the model. We observe that the salient regions identified by the our model align well with the regions an expert human classifier would attend to make equivalent classifications. We show that while the selection of normalisation and aggregation may only minimally affect the performance of individual models, it can significantly affect the interpretability of the respective attention maps and by selecting a model which aligns well with how astronomers classify radio sources by eye, a user can employ the model in a more effective manner.



### Chair Segments: A Compact Benchmark for the Study of Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2012.01250v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2012.01250v1)
- **Published**: 2020-12-02 14:54:03+00:00
- **Updated**: 2020-12-02 14:54:03+00:00
- **Authors**: Leticia Pinto-Alva, Ian K. Torres, Rosangel Garcia, Ziyan Yang, Vicente Ordonez
- **Comment**: 10 pages, 7 figures
- **Journal**: None
- **Summary**: Over the years, datasets and benchmarks have had an outsized influence on the design of novel algorithms. In this paper, we introduce ChairSegments, a novel and compact semi-synthetic dataset for object segmentation. We also show empirical findings in transfer learning that mirror recent findings for image classification. We particularly show that models that are fine-tuned from a pretrained set of weights lie in the same basin of the optimization landscape. ChairSegments consists of a diverse set of prototypical images of chairs with transparent backgrounds composited into a diverse array of backgrounds. We aim for ChairSegments to be the equivalent of the CIFAR-10 dataset but for quickly designing and iterating over novel model architectures for segmentation. On Chair Segments, a U-Net model can be trained to full convergence in only thirty minutes using a single GPU. Finally, while this dataset is semi-synthetic, it can be a useful proxy for real data, leading to state-of-the-art accuracy on the Object Discovery dataset when used as a source of pretraining.



### Suppressing Spoof-irrelevant Factors for Domain-agnostic Face Anti-spoofing
- **Arxiv ID**: http://arxiv.org/abs/2012.01271v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.01271v1)
- **Published**: 2020-12-02 15:27:19+00:00
- **Updated**: 2020-12-02 15:27:19+00:00
- **Authors**: Taewook Kim, Yonghyun Kim
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Face anti-spoofing aims to prevent false authentications of face recognition systems by distinguishing whether an image is originated from a human face or a spoof medium. We propose a novel method called Doubly Adversarial Suppression Network (DASN) for domain-agnostic face anti-spoofing; DASN improves the generalization ability to unseen domains by learning to effectively suppress spoof-irrelevant factors (SiFs) (e.g., camera sensors, illuminations). To achieve our goal, we introduce two types of adversarial learning schemes. In the first adversarial learning scheme, multiple SiFs are suppressed by deploying multiple discrimination heads that are trained against an encoder. In the second adversarial learning scheme, each of the discrimination heads is also adversarially trained to suppress a spoof factor, and the group of the secondary spoof classifier and the encoder aims to intensify the spoof factor by overcoming the suppression. We evaluate the proposed method on four public benchmark datasets, and achieve remarkable evaluation results. The results demonstrate the effectiveness of the proposed method.



### Generating Descriptions for Sequential Images with Local-Object Attention and Global Semantic Context Modelling
- **Arxiv ID**: http://arxiv.org/abs/2012.01295v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2012.01295v1)
- **Published**: 2020-12-02 16:07:32+00:00
- **Updated**: 2020-12-02 16:07:32+00:00
- **Authors**: Jing Su, Chenghua Lin, Mian Zhou, Qingyun Dai, Haoyu Lv
- **Comment**: Accepted by INLG 2018
- **Journal**: None
- **Summary**: In this paper, we propose an end-to-end CNN-LSTM model for generating descriptions for sequential images with a local-object attention mechanism. To generate coherent descriptions, we capture global semantic context using a multi-layer perceptron, which learns the dependencies between sequential images. A paralleled LSTM network is exploited for decoding the sequence descriptions. Experimental results show that our model outperforms the baseline across three different evaluation metrics on the datasets published by Microsoft.



### A Framework and Dataset for Abstract Art Generation via CalligraphyGAN
- **Arxiv ID**: http://arxiv.org/abs/2012.00744v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.00744v1)
- **Published**: 2020-12-02 16:24:20+00:00
- **Updated**: 2020-12-02 16:24:20+00:00
- **Authors**: Jinggang Zhuo, Ling Fan, Harry Jiannan Wang
- **Comment**: Accepted by NeurIPS 2020 Workshop on Machine Learning for Creativity
  and Design, Vancouver, Canada
- **Journal**: None
- **Summary**: With the advancement of deep learning, artificial intelligence (AI) has made many breakthroughs in recent years and achieved superhuman performance in various tasks such as object detection, reading comprehension, and video games. Generative Modeling, such as various Generative Adversarial Networks (GAN) models, has been applied to generate paintings and music. Research in Natural Language Processing (NLP) also had a leap forward in 2018 since the release of the pre-trained contextual neural language models such as BERT and recently released GPT3. Despite the exciting AI applications aforementioned, AI is still significantly lagging behind humans in creativity, which is often considered the ultimate moonshot for AI. Our work is inspired by Chinese calligraphy, which is a unique form of visual art where the character itself is an aesthetic painting. We also draw inspirations from paintings of the Abstract Expressionist movement in the 1940s and 1950s, such as the work by American painter Franz Kline. In this paper, we present a creative framework based on Conditional Generative Adversarial Networks and Contextual Neural Language Model to generate abstract artworks that have intrinsic meaning and aesthetic value, which is different from the existing work, such as image captioning and text-to-image generation, where the texts are the descriptions of the images. In addition, we have publicly released a Chinese calligraphy image dataset and demonstrate our framework using a prototype system and a user study.



### An Improved Iterative Neural Network for High-Quality Image-Domain Material Decomposition in Dual-Energy CT
- **Arxiv ID**: http://arxiv.org/abs/2012.01986v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2012.01986v4)
- **Published**: 2020-12-02 16:27:38+00:00
- **Updated**: 2022-01-22 04:17:43+00:00
- **Authors**: Zhipeng Li, Yong Long, Il Yong Chun
- **Comment**: None
- **Journal**: None
- **Summary**: Dual-energy computed tomography (DECT) has been widely used in many applications that need material decomposition. Image-domain methods directly decompose material images from high- and low-energy attenuation images, and thus, are susceptible to noise and artifacts on attenuation images. The purpose of this study is to develop an improved iterative neural network (INN) for high-quality image-domain material decomposition in DECT, and to study its properties. We propose a new INN architecture for DECT material decomposition. The proposed INN architecture uses distinct cross-material convolutional neural network (CNN) in image refining modules, and uses image decomposition physics in image reconstruction modules. The distinct cross-material CNN refiners incorporate distinct encoding-decoding filters and cross-material model that captures correlations between different materials. We study the distinct cross-material CNN refiner with patch-based reformulation and tight-frame condition. Numerical experiments with extended cardiactorso (XCAT) phantom and clinical data show that the proposed INN significantly improves the image quality over several image-domain material decomposition methods, including a conventional model-based image decomposition (MBID) method using an edge-preserving regularizer, a recent MBID method using pre-learned material-wise sparsifying transforms, and a noniterative deep CNN method. Our study with patch-based reformulations reveals that learned filters of distinct cross-material CNN refiners can approximately satisfy the tight-frame condition.



### Top-1 CORSMAL Challenge 2020 Submission: Filling Mass Estimation Using Multi-modal Observations of Human-robot Handovers
- **Arxiv ID**: http://arxiv.org/abs/2012.01311v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, cs.LG, cs.RO, cs.SD
- **Links**: [PDF](http://arxiv.org/pdf/2012.01311v1)
- **Published**: 2020-12-02 16:31:03+00:00
- **Updated**: 2020-12-02 16:31:03+00:00
- **Authors**: Vladimir Iashin, Francesca Palermo, Gökhan Solak, Claudio Coppola
- **Comment**: Code: https://github.com/v-iashin/CORSMAL Docker:
  https://hub.docker.com/r/iashin/corsmal
- **Journal**: None
- **Summary**: Human-robot object handover is a key skill for the future of human-robot collaboration. CORSMAL 2020 Challenge focuses on the perception part of this problem: the robot needs to estimate the filling mass of a container held by a human. Although there are powerful methods in image processing and audio processing individually, answering such a problem requires processing data from multiple sensors together. The appearance of the container, the sound of the filling, and the depth data provide essential information. We propose a multi-modal method to predict three key indicators of the filling mass: filling type, filling level, and container capacity. These indicators are then combined to estimate the filling mass of a container. Our method obtained Top-1 overall performance among all submissions to CORSMAL 2020 Challenge on both public and private subsets while showing no evidence of overfitting. Our source code is publicly available: https://github.com/v-iashin/CORSMAL



### Red Blood Cell Segmentation with Overlapping Cell Separation and Classification on Imbalanced Dataset
- **Arxiv ID**: http://arxiv.org/abs/2012.01321v6
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2012.01321v6)
- **Published**: 2020-12-02 16:49:51+00:00
- **Updated**: 2023-03-06 10:41:29+00:00
- **Authors**: Korranat Naruenatthanaset, Thanarat H. Chalidabhongse, Duangdao Palasuwan, Nantheera Anantrasirichai, Attakorn Palasuwan
- **Comment**: This work has been submitted to Intelligent Systems with Applications
  (ISWA) for possible publication. Copyright may be transferred without notice,
  after which this version may no longer be accessible
- **Journal**: None
- **Summary**: Automated red blood cell (RBC) classification on blood smear images helps hematologists to analyze RBC lab results in a reduced time and cost. However, overlapping cells can cause incorrect predicted results, and so they have to be separated into multiple single RBCs before classifying. To classify multiple classes with deep learning, imbalance problems are common in medical imaging because normal samples are always higher than rare disease samples. This paper presents a new method to segment and classify RBCs from blood smear images, specifically to tackle cell overlapping and data imbalance problems. Focusing on overlapping cell separation, our segmentation process first estimates ellipses to represent RBCs. The method detects the concave points and then finds the ellipses using directed ellipse fitting. The accuracy from 20 blood smear images was 0.889. Classification requires balanced training datasets. However, some RBC types are rare. The imbalance ratio of this dataset was 34.538 for 12 RBC classes from 20,875 individual RBC samples. The use of machine learning for RBC classification with an imbalanced dataset is hence more challenging than many other applications. We analyzed techniques to deal with this problem. The best accuracy and F1-score were 0.921 and 0.8679, respectively, using EfficientNet-B1 with augmentation. Experimental results showed that the weight balancing technique with augmentation had the potential to deal with imbalance problems by improving the F1-score on minority classes, while data augmentation significantly improved the overall classification performance.



### Siamese Basis Function Networks for Data-efficient Defect Classification in Technical Domains
- **Arxiv ID**: http://arxiv.org/abs/2012.01338v10
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, 68U10, J.2
- **Links**: [PDF](http://arxiv.org/pdf/2012.01338v10)
- **Published**: 2020-12-02 17:16:42+00:00
- **Updated**: 2021-09-28 04:43:11+00:00
- **Authors**: Tobias Schlagenhauf, Faruk Yildirim, Benedikt Brückner
- **Comment**: In the current version two contributing authors where added which
  where unfortunately missed in the last version
- **Journal**: None
- **Summary**: Training deep learning models in technical domains is often accompanied by the challenge that although the task is clear, insufficient data for training is available. In this work, we propose a novel approach based on the combination of Siamese networks and radial basis function networks to perform data-efficient classification without pretraining by measuring the distance between images in semantic space in a data-efficient manner. We develop the models using three technical datasets, the NEU dataset, the BSD dataset, and the TEX dataset. In addition to the technical domain, we show the general applicability to classical datasets (cifar10 and MNIST) as well. The approach is tested against state-of-the-art models (Resnet50 and Resnet101) by stepwise reduction of the number of samples available for training. The authors show that the proposed approach outperforms the state-of-the-art models in the low data regime.



### Cross-Modal Retrieval and Synthesis (X-MRS): Closing the Modality Gap in Shared Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2012.01345v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2012.01345v3)
- **Published**: 2020-12-02 17:27:00+00:00
- **Updated**: 2021-09-30 10:53:33+00:00
- **Authors**: Ricardo Guerrero, Hai Xuan Pham, Vladimir Pavlovic
- **Comment**: None
- **Journal**: None
- **Summary**: Computational food analysis (CFA) naturally requires multi-modal evidence of a particular food, e.g., images, recipe text, etc. A key to making CFA possible is multi-modal shared representation learning, which aims to create a joint representation of the multiple views (text and image) of the data. In this work we propose a method for food domain cross-modal shared representation learning that preserves the vast semantic richness present in the food data. Our proposed method employs an effective transformer-based multilingual recipe encoder coupled with a traditional image embedding architecture. Here, we propose the use of imperfect multilingual translations to effectively regularize the model while at the same time adding functionality across multiple languages and alphabets. Experimental analysis on the public Recipe1M dataset shows that the representation learned via the proposed method significantly outperforms the current state-of-the-arts (SOTA) on retrieval tasks. Furthermore, the representational power of the learned representation is demonstrated through a generative food image synthesis model conditioned on recipe embeddings. Synthesized images can effectively reproduce the visual appearance of paired samples, indicating that the learned representation captures the joint semantics of both the textual recipe and its visual content, thus narrowing the modality gap.



### $DA^3$:Dynamic Additive Attention Adaption for Memory-EfficientOn-Device Multi-Domain Learning
- **Arxiv ID**: http://arxiv.org/abs/2012.01362v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.01362v3)
- **Published**: 2020-12-02 18:03:18+00:00
- **Updated**: 2021-10-06 17:37:40+00:00
- **Authors**: Li Yang, Adnan Siraj Rakin, Deliang Fan
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: Nowadays, one practical limitation of deep neural network (DNN) is its high degree of specialization to a single task or domain (e.g., one visual domain). It motivates researchers to develop algorithms that can adapt DNN model to multiple domains sequentially, while still performing well on the past domains, which is known as multi-domain learning. Almost all conventional methods only focus on improving accuracy with minimal parameter update, while ignoring high computing and memory cost during training, which makes it difficult to deploy multi-domain learning into more and more widely used resource-limited edge devices, like mobile phones, IoT, embedded systems, etc. We observe that large memory used for activation storage is the bottleneck that largely limits the training time and cost on edge devices. To reduce training memory usage, while keeping the domain adaption accuracy performance, we propose Dynamic Additive Attention Adaption ($DA^3$), a novel memory-efficient on-device multi-domain learning method. $DA^3$ learns a novel additive attention adaptor module, while freezing the weights of the pre-trained backbone model for each domain. Differentiating from prior works, such module not only mitigates activation memory buffering for reducing memory usage during training but also serves as a dynamic gating mechanism to reduce the computation cost for fast inference. We validate $DA^3$ on multiple datasets against state-of-the-art methods, which shows great improvement in both accuracy and training time. Moreover, we deployed $DA^3$ into the popular NIVDIA Jetson Nano edge GPU, where the measured experimental results show our proposed $DA^3$ reduces the on-device training memory consumption by 19-37x, and training time by 2x, in comparison to the baseline methods (e.g., standard fine-tuning, Parallel and Series Res. adaptor, and Piggyback).



### Cross-Descriptor Visual Localization and Mapping
- **Arxiv ID**: http://arxiv.org/abs/2012.01377v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.01377v2)
- **Published**: 2020-12-02 18:19:51+00:00
- **Updated**: 2021-09-21 14:44:17+00:00
- **Authors**: Mihai Dusmanu, Ondrej Miksik, Johannes L. Schönberger, Marc Pollefeys
- **Comment**: Accepted at ICCV 2021. 18 pages, 15 figures, 6 tables
- **Journal**: None
- **Summary**: Visual localization and mapping is the key technology underlying the majority of mixed reality and robotics systems. Most state-of-the-art approaches rely on local features to establish correspondences between images. In this paper, we present three novel scenarios for localization and mapping which require the continuous update of feature representations and the ability to match across different feature types. While localization and mapping is a fundamental computer vision problem, the traditional setup supposes the same local features are used throughout the evolution of a map. Thus, whenever the underlying features are changed, the whole process is repeated from scratch. However, this is typically impossible in practice, because raw images are often not stored and re-building the maps could lead to loss of the attached digital content. To overcome the limitations of current approaches, we present the first principled solution to cross-descriptor localization and mapping. Our data-driven approach is agnostic to the feature descriptor type, has low computational requirements, and scales linearly with the number of description algorithms. Extensive experiments demonstrate the effectiveness of our approach on state-of-the-art benchmarks for a variety of handcrafted and learned features.



### A Self-Supervised Feature Map Augmentation (FMA) Loss and Combined Augmentations Finetuning to Efficiently Improve the Robustness of CNNs
- **Arxiv ID**: http://arxiv.org/abs/2012.01386v1
- **DOI**: 10.1145/3385958.3430477
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2012.01386v1)
- **Published**: 2020-12-02 18:32:14+00:00
- **Updated**: 2020-12-02 18:32:14+00:00
- **Authors**: Nikhil Kapoor, Chun Yuan, Jonas Löhdefink, Roland Zimmermann, Serin Varghese, Fabian Hüger, Nico Schmidt, Peter Schlicht, Tim Fingscheidt
- **Comment**: Accepted at ACM CSCS 2020 (8 pages, 4 figures)
- **Journal**: None
- **Summary**: Deep neural networks are often not robust to semantically-irrelevant changes in the input. In this work we address the issue of robustness of state-of-the-art deep convolutional neural networks (CNNs) against commonly occurring distortions in the input such as photometric changes, or the addition of blur and noise. These changes in the input are often accounted for during training in the form of data augmentation. We have two major contributions: First, we propose a new regularization loss called feature-map augmentation (FMA) loss which can be used during finetuning to make a model robust to several distortions in the input. Second, we propose a new combined augmentations (CA) finetuning strategy, that results in a single model that is robust to several augmentation types at the same time in a data-efficient manner. We use the CA strategy to improve an existing state-of-the-art method called stability training (ST). Using CA, on an image classification task with distorted images, we achieve an accuracy improvement of on average 8.94% with FMA and 8.86% with ST absolute on CIFAR-10 and 8.04% with FMA and 8.27% with ST absolute on ImageNet, compared to 1.98% and 2.12%, respectively, with the well known data augmentation method, while keeping the clean baseline performance.



### Fine-grained activity recognition for assembly videos
- **Arxiv ID**: http://arxiv.org/abs/2012.01392v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.01392v1)
- **Published**: 2020-12-02 18:38:17+00:00
- **Updated**: 2020-12-02 18:38:17+00:00
- **Authors**: Jonathan D. Jones, Cathryn Cortesa, Amy Shelton, Barbara Landau, Sanjeev Khudanpur, Gregory D. Hager
- **Comment**: 8 pages, 6 figures. Submitted to RA-L/ICRA 2021
- **Journal**: None
- **Summary**: In this paper we address the task of recognizing assembly actions as a structure (e.g. a piece of furniture or a toy block tower) is built up from a set of primitive objects. Recognizing the full range of assembly actions requires perception at a level of spatial detail that has not been attempted in the action recognition literature to date. We extend the fine-grained activity recognition setting to address the task of assembly action recognition in its full generality by unifying assembly actions and kinematic structures within a single framework. We use this framework to develop a general method for recognizing assembly actions from observation sequences, along with observation features that take advantage of a spatial assembly's special structure. Finally, we evaluate our method empirically on two application-driven data sources: (1) An IKEA furniture-assembly dataset, and (2) A block-building dataset. On the first, our system recognizes assembly actions with an average framewise accuracy of 70% and an average normalized edit distance of 10%. On the second, which requires fine-grained geometric reasoning to distinguish between assemblies, our system attains an average normalized edit distance of 23% -- a relative improvement of 69% over prior work.



### Learning View-Disentangled Human Pose Representation by Contrastive Cross-View Mutual Information Maximization
- **Arxiv ID**: http://arxiv.org/abs/2012.01405v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.01405v2)
- **Published**: 2020-12-02 18:55:35+00:00
- **Updated**: 2021-03-26 04:05:44+00:00
- **Authors**: Long Zhao, Yuxiao Wang, Jiaping Zhao, Liangzhe Yuan, Jennifer J. Sun, Florian Schroff, Hartwig Adam, Xi Peng, Dimitris Metaxas, Ting Liu
- **Comment**: Accepted to CVPR 2021 (Oral presentation). Code is available at
  https://github.com/google-research/google-research/tree/master/poem
- **Journal**: None
- **Summary**: We introduce a novel representation learning method to disentangle pose-dependent as well as view-dependent factors from 2D human poses. The method trains a network using cross-view mutual information maximization (CV-MIM) which maximizes mutual information of the same pose performed from different viewpoints in a contrastive learning manner. We further propose two regularization terms to ensure disentanglement and smoothness of the learned representations. The resulting pose representations can be used for cross-view action recognition. To evaluate the power of the learned representations, in addition to the conventional fully-supervised action recognition settings, we introduce a novel task called single-shot cross-view action recognition. This task trains models with actions from only one single viewpoint while models are evaluated on poses captured from all possible viewpoints. We evaluate the learned representations on standard benchmarks for action recognition, and show that (i) CV-MIM performs competitively compared with the state-of-the-art models in the fully-supervised scenarios; (ii) CV-MIM outperforms other competing methods by a large margin in the single-shot cross-view setting; (iii) and the learned representations can significantly boost the performance when reducing the amount of supervised training data. Our code is made publicly available at https://github.com/google-research/google-research/tree/master/poem



### PatchmatchNet: Learned Multi-View Patchmatch Stereo
- **Arxiv ID**: http://arxiv.org/abs/2012.01411v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.01411v1)
- **Published**: 2020-12-02 18:59:02+00:00
- **Updated**: 2020-12-02 18:59:02+00:00
- **Authors**: Fangjinhua Wang, Silvano Galliani, Christoph Vogel, Pablo Speciale, Marc Pollefeys
- **Comment**: None
- **Journal**: None
- **Summary**: We present PatchmatchNet, a novel and learnable cascade formulation of Patchmatch for high-resolution multi-view stereo. With high computation speed and low memory requirement, PatchmatchNet can process higher resolution imagery and is more suited to run on resource limited devices than competitors that employ 3D cost volume regularization. For the first time we introduce an iterative multi-scale Patchmatch in an end-to-end trainable architecture and improve the Patchmatch core algorithm with a novel and learned adaptive propagation and evaluation scheme for each iteration. Extensive experiments show a very competitive performance and generalization for our method on DTU, Tanks & Temples and ETH3D, but at a significantly higher efficiency than all existing top-performing models: at least two and a half times faster than state-of-the-art methods with twice less memory usage.



### Neural Deformation Graphs for Globally-consistent Non-rigid Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2012.01451v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2012.01451v1)
- **Published**: 2020-12-02 19:00:13+00:00
- **Updated**: 2020-12-02 19:00:13+00:00
- **Authors**: Aljaž Božič, Pablo Palafox, Michael Zollhöfer, Justus Thies, Angela Dai, Matthias Nießner
- **Comment**: Video: https://youtu.be/vyq36eFkdWo
- **Journal**: None
- **Summary**: We introduce Neural Deformation Graphs for globally-consistent deformation tracking and 3D reconstruction of non-rigid objects. Specifically, we implicitly model a deformation graph via a deep neural network. This neural deformation graph does not rely on any object-specific structure and, thus, can be applied to general non-rigid deformation tracking. Our method globally optimizes this neural graph on a given sequence of depth camera observations of a non-rigidly moving object. Based on explicit viewpoint consistency as well as inter-frame graph and surface consistency constraints, the underlying network is trained in a self-supervised fashion. We additionally optimize for the geometry of the object with an implicit deformable multi-MLP shape representation. Our approach does not assume sequential input data, thus enabling robust tracking of fast motions or even temporally disconnected recordings. Our experiments demonstrate that our Neural Deformation Graphs outperform state-of-the-art non-rigid reconstruction approaches both qualitatively and quantitatively, with 64% improved reconstruction and 62% improved deformation tracking performance.



### ACE-Net: Fine-Level Face Alignment through Anchors and Contours Estimation
- **Arxiv ID**: http://arxiv.org/abs/2012.01461v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.01461v2)
- **Published**: 2020-12-02 19:04:12+00:00
- **Updated**: 2022-01-09 07:40:21+00:00
- **Authors**: Jihua Huang, Amir Tamrakar
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel facial Anchors and Contours Estimation framework, ACE-Net, for fine-level face alignment tasks. ACE-Net predicts facial anchors and contours that are richer than traditional facial landmarks while overcoming ambiguities and inconsistencies in their definitions. We introduce a weakly supervised loss enabling ACE-Net to learn from existing facial landmarks datasets without the need for reannotation. Instead, synthetic data, from which GT contours can be easily obtained, is used during training to bridge the density gap between landmarks and true facial contours. We evaluate the face alignment accuracy of ACE-Net with respect to the HELEN dataset which has 194 annotated facial landmarks, while it is trained with only 68 or 36 landmarks from the 300-W dataset. We show that ACE-Net generated contours are better than contours interpolated straight from the 68 GT landmarks and ACE-Net also outperforms models trained only with full supervision from GT landmarks-based contours.



### Video Anomaly Detection by Estimating Likelihood of Representations
- **Arxiv ID**: http://arxiv.org/abs/2012.01468v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2012.01468v1)
- **Published**: 2020-12-02 19:16:22+00:00
- **Updated**: 2020-12-02 19:16:22+00:00
- **Authors**: Yuqi Ouyang, Victor Sanchez
- **Comment**: Accepted to ICPR 2020
- **Journal**: None
- **Summary**: Video anomaly detection is a challenging task not only because it involves solving many sub-tasks such as motion representation, object localization and action recognition, but also because it is commonly considered as an unsupervised learning problem that involves detecting outliers. Traditionally, solutions to this task have focused on the mapping between video frames and their low-dimensional features, while ignoring the spatial connections of those features. Recent solutions focus on analyzing these spatial connections by using hard clustering techniques, such as K-Means, or applying neural networks to map latent features to a general understanding, such as action attributes. In order to solve video anomaly in the latent feature space, we propose a deep probabilistic model to transfer this task into a density estimation problem where latent manifolds are generated by a deep denoising autoencoder and clustered by expectation maximization. Evaluations on several benchmarks datasets show the strengths of our model, achieving outstanding performance on challenging datasets.



### Fair Attribute Classification through Latent Space De-biasing
- **Arxiv ID**: http://arxiv.org/abs/2012.01469v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.01469v3)
- **Published**: 2020-12-02 19:18:58+00:00
- **Updated**: 2021-04-02 17:57:47+00:00
- **Authors**: Vikram V. Ramaswamy, Sunnie S. Y. Kim, Olga Russakovsky
- **Comment**: Accepted to CVPR 2021, code can be found at
  https://github.com/princetonvisualai/gan-debiasing
- **Journal**: None
- **Summary**: Fairness in visual recognition is becoming a prominent and critical topic of discussion as recognition systems are deployed at scale in the real world. Models trained from data in which target labels are correlated with protected attributes (e.g., gender, race) are known to learn and exploit those correlations. In this work, we introduce a method for training accurate target classifiers while mitigating biases that stem from these correlations. We use GANs to generate realistic-looking images, and perturb these images in the underlying latent space to generate training data that is balanced for each protected attribute. We augment the original dataset with this perturbed generated data, and empirically demonstrate that target classifiers trained on the augmented dataset exhibit a number of both quantitative and qualitative benefits. We conduct a thorough evaluation across multiple target labels and protected attributes in the CelebA dataset, and provide an in-depth analysis and comparison to existing literature in the space.



### CovSegNet: A Multi Encoder-Decoder Architecture for Improved Lesion Segmentation of COVID-19 Chest CT Scans
- **Arxiv ID**: http://arxiv.org/abs/2012.01473v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2012.01473v1)
- **Published**: 2020-12-02 19:26:35+00:00
- **Updated**: 2020-12-02 19:26:35+00:00
- **Authors**: Tanvir Mahmud, Md Awsafur Rahman, Shaikh Anowarul Fattah, Sun-Yuan Kung
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic lung lesions segmentation of chest CT scans is considered a pivotal stage towards accurate diagnosis and severity measurement of COVID-19. Traditional U-shaped encoder-decoder architecture and its variants suffer from diminutions of contextual information in pooling/upsampling operations with increased semantic gaps among encoded and decoded feature maps as well as instigate vanishing gradient problems for its sequential gradient propagation that result in sub-optimal performance. Moreover, operating with 3D CT-volume poses further limitations due to the exponential increase of computational complexity making the optimization difficult. In this paper, an automated COVID-19 lesion segmentation scheme is proposed utilizing a highly efficient neural network architecture, namely CovSegNet, to overcome these limitations. Additionally, a two-phase training scheme is introduced where a deeper 2D-network is employed for generating ROI-enhanced CT-volume followed by a shallower 3D-network for further enhancement with more contextual information without increasing computational burden. Along with the traditional vertical expansion of Unet, we have introduced horizontal expansion with multi-stage encoder-decoder modules for achieving optimum performance. Additionally, multi-scale feature maps are integrated into the scale transition process to overcome the loss of contextual information. Moreover, a multi-scale fusion module is introduced with a pyramid fusion scheme to reduce the semantic gaps between subsequent encoder/decoder modules while facilitating the parallel optimization for efficient gradient propagation. Outstanding performances have been achieved in three publicly available datasets that largely outperform other state-of-the-art approaches. The proposed scheme can be easily extended for achieving optimum segmentation performances in a wide variety of applications.



### Leveraging Uncertainty from Deep Learning for Trustworthy Materials Discovery Workflows
- **Arxiv ID**: http://arxiv.org/abs/2012.01478v2
- **DOI**: None
- **Categories**: **cond-mat.mtrl-sci**, cs.CV, cs.LG, physics.app-ph
- **Links**: [PDF](http://arxiv.org/pdf/2012.01478v2)
- **Published**: 2020-12-02 19:34:16+00:00
- **Updated**: 2021-04-22 23:29:30+00:00
- **Authors**: Jize Zhang, Bhavya Kailkhura, T. Yong-Jin Han
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we leverage predictive uncertainty of deep neural networks to answer challenging questions material scientists usually encounter in machine learning based materials applications workflows. First, we show that by leveraging predictive uncertainty, a user can determine the required training data set size necessary to achieve a certain classification accuracy. Next, we propose uncertainty guided decision referral to detect and refrain from making decisions on confusing samples. Finally, we show that predictive uncertainty can also be used to detect out-of-distribution test samples. We find that this scheme is accurate enough to detect a wide range of real-world shifts in data, e.g., changes in the image acquisition conditions or changes in the synthesis conditions. Using microstructure information from scanning electron microscope (SEM) images as an example use case, we show that leveraging uncertainty-aware deep learning can significantly improve the performance and dependability of classification models.



### Contour Transformer Network for One-shot Segmentation of Anatomical Structures
- **Arxiv ID**: http://arxiv.org/abs/2012.01480v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.01480v1)
- **Published**: 2020-12-02 19:42:18+00:00
- **Updated**: 2020-12-02 19:42:18+00:00
- **Authors**: Yuhang Lu, Kang Zheng, Weijian Li, Yirui Wang, Adam P. Harrison, Chihung Lin, Song Wang, Jing Xiao, Le Lu, Chang-Fu Kuo, Shun Miao
- **Comment**: None
- **Journal**: None
- **Summary**: Accurate segmentation of anatomical structures is vital for medical image analysis. The state-of-the-art accuracy is typically achieved by supervised learning methods, where gathering the requisite expert-labeled image annotations in a scalable manner remains a main obstacle. Therefore, annotation-efficient methods that permit to produce accurate anatomical structure segmentation are highly desirable. In this work, we present Contour Transformer Network (CTN), a one-shot anatomy segmentation method with a naturally built-in human-in-the-loop mechanism. We formulate anatomy segmentation as a contour evolution process and model the evolution behavior by graph convolutional networks (GCNs). Training the CTN model requires only one labeled image exemplar and leverages additional unlabeled data through newly introduced loss functions that measure the global shape and appearance consistency of contours. On segmentation tasks of four different anatomies, we demonstrate that our one-shot learning method significantly outperforms non-learning-based methods and performs competitively to the state-of-the-art fully supervised deep learning methods. With minimal human-in-the-loop editing feedback, the segmentation performance can be further improved to surpass the fully supervised methods.



### Data-driven Analysis of Turbulent Flame Images
- **Arxiv ID**: http://arxiv.org/abs/2012.01485v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.01485v1)
- **Published**: 2020-12-02 19:46:17+00:00
- **Updated**: 2020-12-02 19:46:17+00:00
- **Authors**: Rathziel Roncancio, Jupyoung Kim, Aly El Gamal, Jay P. Gore
- **Comment**: AIAA Science and Technology Conference 2021
- **Journal**: None
- **Summary**: Turbulent premixed flames are important for power generation using gas turbines. Improvements in characterization and understanding of turbulent flames continue particularly for transient events like ignition and extinction. Pockets or islands of unburned material are features of turbulent flames during these events. These features are directly linked to heat release rates and hydrocarbons emissions. Unburned material pockets in turbulent CH$_4$/air premixed flames with CO$_2$ addition were investigated using OH Planar Laser-Induced Fluorescence images. Convolutional Neural Networks (CNN) were used to classify images containing unburned pockets for three turbulent flames with 0%, 5%, and 10% CO$_2$ addition. The CNN model was constructed using three convolutional layers and two fully connected layers using dropout and weight decay. The CNN model achieved accuracies of 91.72%, 89.35% and 85.80% for the three flames, respectively.



### Braille to Text Translation for Bengali Language: A Geometric Approach
- **Arxiv ID**: http://arxiv.org/abs/2012.01494v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.01494v1)
- **Published**: 2020-12-02 19:57:29+00:00
- **Updated**: 2020-12-02 19:57:29+00:00
- **Authors**: Minhas Kamal, Dr. Amin Ahsan Ali, Dr. Muhammad Asif Hossain Khan, Dr. Mohammad Shoyaib
- **Comment**: None
- **Journal**: In Jahangirnagar University Journal of Information Technology
  (JJIT), pp. 93-111, 2018
- **Summary**: Braille is the only system to visually impaired people for reading and writing. However, general people cannot read Braille. So, teachers and relatives find it hard to assist them with learning. Almost every major language has software solutions for this translation purpose. However, in Bengali there is an absence of this useful tool. Here, we propose Braille to Text Translator, which takes image of these tactile alphabets, and translates them to plain text. Image deterioration, scan-time page rotation, and braille dot deformation are the principal issues in this scheme. All of these challenges are directly checked using special image processing and geometric structure analysis. The technique yields 97.25% accuracy in recognizing Braille characters.



### Few-Shot Classification with Feature Map Reconstruction Networks
- **Arxiv ID**: http://arxiv.org/abs/2012.01506v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2012.01506v2)
- **Published**: 2020-12-02 20:19:09+00:00
- **Updated**: 2021-04-27 17:54:22+00:00
- **Authors**: Davis Wertheimer, Luming Tang, Bharath Hariharan
- **Comment**: Accepted to CVPR 2021. Updated to match most recent version. Code is
  available at https://github.com/Tsingularity/FRN
- **Journal**: None
- **Summary**: In this paper we reformulate few-shot classification as a reconstruction problem in latent space. The ability of the network to reconstruct a query feature map from support features of a given class predicts membership of the query in that class. We introduce a novel mechanism for few-shot classification by regressing directly from support features to query features in closed form, without introducing any new modules or large-scale learnable parameters. The resulting Feature Map Reconstruction Networks are both more performant and computationally efficient than previous approaches. We demonstrate consistent and substantial accuracy gains on four fine-grained benchmarks with varying neural architectures. Our model is also competitive on the non-fine-grained mini-ImageNet and tiered-ImageNet benchmarks with minimal bells and whistles.



### Temporal Representation Learning on Monocular Videos for 3D Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2012.01511v5
- **DOI**: 10.1109/TPAMI.2022.3215307
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2012.01511v5)
- **Published**: 2020-12-02 20:27:35+00:00
- **Updated**: 2022-11-25 17:00:59+00:00
- **Authors**: Sina Honari, Victor Constantin, Helge Rhodin, Mathieu Salzmann, Pascal Fua
- **Comment**: Accepted in "IEEE Transactions on Pattern Analysis and Machine
  Intelligence (TPAMI)"
- **Journal**: IEEE Transactions on Pattern Analysis and Machine Intelligence,
  2022
- **Summary**: In this paper we propose an unsupervised feature extraction method to capture temporal information on monocular videos, where we detect and encode subject of interest in each frame and leverage contrastive self-supervised (CSS) learning to extract rich latent vectors. Instead of simply treating the latent features of nearby frames as positive pairs and those of temporally-distant ones as negative pairs as in other CSS approaches, we explicitly disentangle each latent vector into a time-variant component and a time-invariant one. We then show that applying contrastive loss only to the time-variant features and encouraging a gradual transition on them between nearby and away frames while also reconstructing the input, extract rich temporal features, well-suited for human pose estimation. Our approach reduces error by about 50% compared to the standard CSS strategies, outperforms other unsupervised single-view methods and matches the performance of multi-view techniques. When 2D pose is available, our approach can extract even richer latent features and improve the 3D pose estimation accuracy, outperforming other state-of-the-art weakly supervised methods.



### From Goals, Waypoints & Paths To Long Term Human Trajectory Forecasting
- **Arxiv ID**: http://arxiv.org/abs/2012.01526v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2012.01526v1)
- **Published**: 2020-12-02 21:01:29+00:00
- **Updated**: 2020-12-02 21:01:29+00:00
- **Authors**: Karttikeya Mangalam, Yang An, Harshayu Girase, Jitendra Malik
- **Comment**: 14 pages, 7 figures (including 2 GIFs)
- **Journal**: None
- **Summary**: Human trajectory forecasting is an inherently multi-modal problem. Uncertainty in future trajectories stems from two sources: (a) sources that are known to the agent but unknown to the model, such as long term goals and (b)sources that are unknown to both the agent & the model, such as intent of other agents & irreducible randomness indecisions. We propose to factorize this uncertainty into its epistemic & aleatoric sources. We model the epistemic un-certainty through multimodality in long term goals and the aleatoric uncertainty through multimodality in waypoints& paths. To exemplify this dichotomy, we also propose a novel long term trajectory forecasting setting, with prediction horizons upto a minute, an order of magnitude longer than prior works. Finally, we presentY-net, a scene com-pliant trajectory forecasting network that exploits the pro-posed epistemic & aleatoric structure for diverse trajectory predictions across long prediction horizons.Y-net significantly improves previous state-of-the-art performance on both (a) The well studied short prediction horizon settings on the Stanford Drone & ETH/UCY datasets and (b) The proposed long prediction horizon setting on the re-purposed Stanford Drone & Intersection Drone datasets.



### Increased performance in DDM analysis by calculating structure functions through Fourier transform in time
- **Arxiv ID**: http://arxiv.org/abs/2012.05695v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, physics.app-ph, physics.data-an
- **Links**: [PDF](http://arxiv.org/pdf/2012.05695v1)
- **Published**: 2020-12-02 21:12:45+00:00
- **Updated**: 2020-12-02 21:12:45+00:00
- **Authors**: M. Norouzisadeh, G. Cerchiari, F. Croccolo
- **Comment**: 6 pages, 5 figures
- **Journal**: None
- **Summary**: Differential Dynamic Microscopy (DDM) is the combination of optical microscopy to statistical analysis to obtain information about the dynamical behaviour of a variety of samples spanning from soft matter physics to biology. In DDM, the dynamical evolution of the samples is investigated separately at different length scales and extracted from a set of images recorded at different times. A specific result of interest is the structure function that can be computed via spatial Fourier transforms and differences of signals. In this work, we present an algorithm to efficiently process a set of images according to the DDM analysis scheme. We bench-marked the new approach against the state-of-the-art algorithm reported in previous work. The new implementation computes the DDM analysis faster, thanks to an additional Fourier transform in time instead of performing differences of signals. This allows obtaining very fast analysis also in CPU based machine. In order to test the new code, we performed the DDM analysis over sets of more than 1000 images with and without the help of GPU hardware acceleration. As an example, for images of $512 \times 512$ pixels, the new algorithm is 10 times faster than the previous GPU code. Without GPU hardware acceleration and for the same set of images, we found that the new algorithm is 300 faster than the old one both running only on the CPU.



### Differential Morphed Face Detection Using Deep Siamese Networks
- **Arxiv ID**: http://arxiv.org/abs/2012.01541v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.01541v2)
- **Published**: 2020-12-02 21:30:11+00:00
- **Updated**: 2020-12-05 01:51:50+00:00
- **Authors**: Sobhan Soleymani, Baaria Chaudhary, Ali Dabouei, Jeremy Dawson, Nasser M. Nasrabadi
- **Comment**: MultiMedia FORensics in the WILD (MMForWILD 2020)
- **Journal**: None
- **Summary**: Although biometric facial recognition systems are fast becoming part of security applications, these systems are still vulnerable to morphing attacks, in which a facial reference image can be verified as two or more separate identities. In border control scenarios, a successful morphing attack allows two or more people to use the same passport to cross borders. In this paper, we propose a novel differential morph attack detection framework using a deep Siamese network. To the best of our knowledge, this is the first research work that makes use of a Siamese network architecture for morph attack detection. We compare our model with other classical and deep learning models using two distinct morph datasets, VISAPP17 and MorGAN. We explore the embedding space generated by the contrastive loss using three decision making frameworks using Euclidean distance, feature difference and a support vector machine classifier, and feature concatenation and a support vector machine classifier.



### Mutual Information Maximization on Disentangled Representations for Differential Morph Detection
- **Arxiv ID**: http://arxiv.org/abs/2012.01542v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.01542v1)
- **Published**: 2020-12-02 21:31:02+00:00
- **Updated**: 2020-12-02 21:31:02+00:00
- **Authors**: Sobhan Soleymani, Ali Dabouei, Fariborz Taherkhani, Jeremy Dawson, Nasser M. Nasrabadi
- **Comment**: IEEE Winter Conference on Applications of Computer Vision (WACV 2021)
- **Journal**: None
- **Summary**: In this paper, we present a novel differential morph detection framework, utilizing landmark and appearance disentanglement. In our framework, the face image is represented in the embedding domain using two disentangled but complementary representations. The network is trained by triplets of face images, in which the intermediate image inherits the landmarks from one image and the appearance from the other image. This initially trained network is further trained for each dataset using contrastive representations. We demonstrate that, by employing appearance and landmark disentanglement, the proposed framework can provide state-of-the-art differential morph detection performance. This functionality is achieved by the using distances in landmark, appearance, and ID domains. The performance of the proposed framework is evaluated using three morph datasets generated with different methodologies.



### From a Fourier-Domain Perspective on Adversarial Examples to a Wiener Filter Defense for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2012.01558v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2012.01558v2)
- **Published**: 2020-12-02 22:06:04+00:00
- **Updated**: 2021-04-21 15:44:10+00:00
- **Authors**: Nikhil Kapoor, Andreas Bär, Serin Varghese, Jan David Schneider, Fabian Hüger, Peter Schlicht, Tim Fingscheidt
- **Comment**: Accepted by The International Joint Conference on Neural Network
  (IJCNN) 2021
- **Journal**: None
- **Summary**: Despite recent advancements, deep neural networks are not robust against adversarial perturbations. Many of the proposed adversarial defense approaches use computationally expensive training mechanisms that do not scale to complex real-world tasks such as semantic segmentation, and offer only marginal improvements. In addition, fundamental questions on the nature of adversarial perturbations and their relation to the network architecture are largely understudied. In this work, we study the adversarial problem from a frequency domain perspective. More specifically, we analyze discrete Fourier transform (DFT) spectra of several adversarial images and report two major findings: First, there exists a strong connection between a model architecture and the nature of adversarial perturbations that can be observed and addressed in the frequency domain. Second, the observed frequency patterns are largely image- and attack-type independent, which is important for the practical impact of any defense making use of such patterns. Motivated by these findings, we additionally propose an adversarial defense method based on the well-known Wiener filters that captures and suppresses adversarial frequencies in a data-driven manner. Our proposed method not only generalizes across unseen attacks but also beats five existing state-of-the-art methods across two models in a variety of attack settings.



### Holistic 3D Human and Scene Mesh Estimation from Single View Images
- **Arxiv ID**: http://arxiv.org/abs/2012.01591v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.01591v2)
- **Published**: 2020-12-02 23:22:03+00:00
- **Updated**: 2021-04-16 17:30:41+00:00
- **Authors**: Zhenzhen Weng, Serena Yeung
- **Comment**: None
- **Journal**: None
- **Summary**: The 3D world limits the human body pose and the human body pose conveys information about the surrounding objects. Indeed, from a single image of a person placed in an indoor scene, we as humans are adept at resolving ambiguities of the human pose and room layout through our knowledge of the physical laws and prior perception of the plausible object and human poses. However, few computer vision models fully leverage this fact. In this work, we propose an end-to-end trainable model that perceives the 3D scene from a single RGB image, estimates the camera pose and the room layout, and reconstructs both human body and object meshes. By imposing a set of comprehensive and sophisticated losses on all aspects of the estimations, we show that our model outperforms existing human body mesh methods and indoor scene reconstruction methods. To the best of our knowledge, this is the first model that outputs both object and human predictions at the mesh level, and performs joint optimization on the scene and human poses.



