# Arxiv Papers in cs.CV on 2020-12-05
### Driver Glance Classification In-the-wild: Towards Generalization Across Domains and Subjects
- **Arxiv ID**: http://arxiv.org/abs/2012.02906v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.02906v3)
- **Published**: 2020-12-05 00:23:01+00:00
- **Updated**: 2021-11-09 06:21:17+00:00
- **Authors**: Sandipan Banerjee, Ajjen Joshi, Jay Turcot, Bryan Reimer, Taniya Mishra
- **Comment**: IEEE FG 2021
- **Journal**: None
- **Summary**: Distracted drivers are dangerous drivers. Equipping advanced driver assistance systems (ADAS) with the ability to detect driver distraction can help prevent accidents and improve driver safety. In order to detect driver distraction, an ADAS must be able to monitor their visual attention. We propose a model that takes as input a patch of the driver's face along with a crop of the eye-region and classifies their glance into 6 coarse regions-of-interest (ROIs) in the vehicle. We demonstrate that an hourglass network, trained with an additional reconstruction loss, allows the model to learn stronger contextual feature representations than a traditional encoder-only classification module. To make the system robust to subject-specific variations in appearance and behavior, we design a personalized hourglass model tuned with an auxiliary input representing the driver's baseline glance behavior. Finally, we present a weakly supervised multi-domain training regimen that enables the hourglass to jointly learn representations from different domains (varying in camera type, angle), utilizing unlabeled samples and thereby reducing annotation cost.



### What Makes a "Good" Data Augmentation in Knowledge Distillation -- A Statistical Perspective
- **Arxiv ID**: http://arxiv.org/abs/2012.02909v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2012.02909v3)
- **Published**: 2020-12-05 00:32:04+00:00
- **Updated**: 2023-02-21 21:59:57+00:00
- **Authors**: Huan Wang, Suhas Lohit, Mike Jones, Yun Fu
- **Comment**: Camera Ready of NeurIPS'22. Code:
  https://github.com/MingSun-Tse/Good-DA-in-KD
- **Journal**: None
- **Summary**: Knowledge distillation (KD) is a general neural network training approach that uses a teacher model to guide the student model. Existing works mainly study KD from the network output side (e.g., trying to design a better KD loss function), while few have attempted to understand it from the input side. Especially, its interplay with data augmentation (DA) has not been well understood. In this paper, we ask: Why do some DA schemes (e.g., CutMix) inherently perform much better than others in KD? What makes a "good" DA in KD? Our investigation from a statistical perspective suggests that a good DA scheme should reduce the covariance of the teacher-student cross-entropy. A practical metric, the stddev of teacher's mean probability (T. stddev), is further presented and well justified empirically. Besides the theoretical understanding, we also introduce a new entropy-based data-mixing DA scheme, CutMixPick, to further enhance CutMix. Extensive empirical studies support our claims and demonstrate how we can harvest considerable performance gains simply by using a better DA scheme in knowledge distillation.



### Cosine-Pruned Medial Axis: A new method for isometric equivariant and noise-free medial axis extraction
- **Arxiv ID**: http://arxiv.org/abs/2012.02910v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.02910v1)
- **Published**: 2020-12-05 00:44:05+00:00
- **Updated**: 2020-12-05 00:44:05+00:00
- **Authors**: Diego Patiño, John Branch
- **Comment**: None
- **Journal**: None
- **Summary**: We present the CPMA, a new method for medial axis pruning with noise robustness and equivariance to isometric transformations. Our method leverages the discrete cosine transform to create smooth versions of a shape $\Omega$. We use the smooth shapes to compute a score function $\scorefunction$ that filters out spurious branches from the medial axis. We extensively compare the CPMA with state-of-the-art pruning methods and highlight our method's noise robustness and isometric equivariance. We found that our pruning approach achieves competitive results and yields stable medial axes even in scenarios with significant contour perturbations.



### Multi-head Knowledge Distillation for Model Compression
- **Arxiv ID**: http://arxiv.org/abs/2012.02911v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2012.02911v1)
- **Published**: 2020-12-05 00:49:14+00:00
- **Updated**: 2020-12-05 00:49:14+00:00
- **Authors**: Huan Wang, Suhas Lohit, Michael Jones, Yun Fu
- **Comment**: Copyright: 2020 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works
- **Journal**: None
- **Summary**: Several methods of knowledge distillation have been developed for neural network compression. While they all use the KL divergence loss to align the soft outputs of the student model more closely with that of the teacher, the various methods differ in how the intermediate features of the student are encouraged to match those of the teacher. In this paper, we propose a simple-to-implement method using auxiliary classifiers at intermediate layers for matching features, which we refer to as multi-head knowledge distillation (MHKD). We add loss terms for training the student that measure the dissimilarity between student and teacher outputs of the auxiliary classifiers. At the same time, the proposed method also provides a natural way to measure differences at the intermediate layers even though the dimensions of the internal teacher and student features may be different. Through several experiments in image classification on multiple datasets we show that the proposed method outperforms prior relevant approaches presented in the literature.



### iGibson 1.0: a Simulation Environment for Interactive Tasks in Large Realistic Scenes
- **Arxiv ID**: http://arxiv.org/abs/2012.02924v6
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2012.02924v6)
- **Published**: 2020-12-05 02:14:17+00:00
- **Updated**: 2021-08-10 04:45:16+00:00
- **Authors**: Bokui Shen, Fei Xia, Chengshu Li, Roberto Martín-Martín, Linxi Fan, Guanzhi Wang, Claudia Pérez-D'Arpino, Shyamal Buch, Sanjana Srivastava, Lyne P. Tchapmi, Micael E. Tchapmi, Kent Vainio, Josiah Wong, Li Fei-Fei, Silvio Savarese
- **Comment**: None
- **Journal**: 2021 IEEE/RSJ International Conference on Intelligent Robots and
  Systems (IROS 2021)
- **Summary**: We present iGibson 1.0, a novel simulation environment to develop robotic solutions for interactive tasks in large-scale realistic scenes. Our environment contains 15 fully interactive home-sized scenes with 108 rooms populated with rigid and articulated objects. The scenes are replicas of real-world homes, with distribution and the layout of objects aligned to those of the real world. iGibson 1.0 integrates several key features to facilitate the study of interactive tasks: i) generation of high-quality virtual sensor signals (RGB, depth, segmentation, LiDAR, flow and so on), ii) domain randomization to change the materials of the objects (both visual and physical) and/or their shapes, iii) integrated sampling-based motion planners to generate collision-free trajectories for robot bases and arms, and iv) intuitive human-iGibson interface that enables efficient collection of human demonstrations. Through experiments, we show that the full interactivity of the scenes enables agents to learn useful visual representations that accelerate the training of downstream manipulation tasks. We also show that iGibson 1.0 features enable the generalization of navigation agents, and that the human-iGibson interface and integrated motion planners facilitate efficient imitation learning of human demonstrated (mobile) manipulation behaviors. iGibson 1.0 is open-source, equipped with comprehensive examples and documentation. For more information, visit our project website: http://svl.stanford.edu/igibson/



### Cirrus: A Long-range Bi-pattern LiDAR Dataset
- **Arxiv ID**: http://arxiv.org/abs/2012.02938v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.02938v1)
- **Published**: 2020-12-05 03:18:31+00:00
- **Updated**: 2020-12-05 03:18:31+00:00
- **Authors**: Ze Wang, Sihao Ding, Ying Li, Jonas Fenn, Sohini Roychowdhury, Andreas Wallin, Lane Martin, Scott Ryvola, Guillermo Sapiro, Qiang Qiu
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we introduce Cirrus, a new long-range bi-pattern LiDAR public dataset for autonomous driving tasks such as 3D object detection, critical to highway driving and timely decision making. Our platform is equipped with a high-resolution video camera and a pair of LiDAR sensors with a 250-meter effective range, which is significantly longer than existing public datasets. We record paired point clouds simultaneously using both Gaussian and uniform scanning patterns. Point density varies significantly across such a long range, and different scanning patterns further diversify object representation in LiDAR. In Cirrus, eight categories of objects are exhaustively annotated in the LiDAR point clouds for the entire effective range. To illustrate the kind of studies supported by this new dataset, we introduce LiDAR model adaptation across different ranges, scanning patterns, and sensor devices. Promising results show the great potential of this new dataset to the robotics and computer vision communities.



### FloodNet: A High Resolution Aerial Imagery Dataset for Post Flood Scene Understanding
- **Arxiv ID**: http://arxiv.org/abs/2012.02951v1
- **DOI**: None
- **Categories**: **cs.CV**, 68T45, I.4.6
- **Links**: [PDF](http://arxiv.org/pdf/2012.02951v1)
- **Published**: 2020-12-05 05:15:36+00:00
- **Updated**: 2020-12-05 05:15:36+00:00
- **Authors**: Maryam Rahnemoonfar, Tashnim Chowdhury, Argho Sarkar, Debvrat Varshney, Masoud Yari, Robin Murphy
- **Comment**: 11 pages
- **Journal**: None
- **Summary**: Visual scene understanding is the core task in making any crucial decision in any computer vision system. Although popular computer vision datasets like Cityscapes, MS-COCO, PASCAL provide good benchmarks for several tasks (e.g. image classification, segmentation, object detection), these datasets are hardly suitable for post disaster damage assessments. On the other hand, existing natural disaster datasets include mainly satellite imagery which have low spatial resolution and a high revisit period. Therefore, they do not have a scope to provide quick and efficient damage assessment tasks. Unmanned Aerial Vehicle(UAV) can effortlessly access difficult places during any disaster and collect high resolution imagery that is required for aforementioned tasks of computer vision. To address these issues we present a high resolution UAV imagery, FloodNet, captured after the hurricane Harvey. This dataset demonstrates the post flooded damages of the affected areas. The images are labeled pixel-wise for semantic segmentation task and questions are produced for the task of visual question answering. FloodNet poses several challenges including detection of flooded roads and buildings and distinguishing between natural water and flooded water. With the advancement of deep learning algorithms, we can analyze the impact of any disaster which can make a precise understanding of the affected areas. In this paper, we compare and contrast the performances of baseline methods for image classification, semantic segmentation, and visual question answering on our dataset.



### SpeakingFaces: A Large-Scale Multimodal Dataset of Voice Commands with Visual and Thermal Video Streams
- **Arxiv ID**: http://arxiv.org/abs/2012.02961v3
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2012.02961v3)
- **Published**: 2020-12-05 06:49:42+00:00
- **Updated**: 2021-05-01 05:27:42+00:00
- **Authors**: Madina Abdrakhmanova, Askat Kuzdeuov, Sheikh Jarju, Yerbolat Khassanov, Michael Lewis, Huseyin Atakan Varol
- **Comment**: 20 pages, 9 figures, 5 tables
- **Journal**: None
- **Summary**: We present SpeakingFaces as a publicly-available large-scale multimodal dataset developed to support machine learning research in contexts that utilize a combination of thermal, visual, and audio data streams; examples include human-computer interaction, biometric authentication, recognition systems, domain transfer, and speech recognition. SpeakingFaces is comprised of aligned high-resolution thermal and visual spectra image streams of fully-framed faces synchronized with audio recordings of each subject speaking approximately 100 imperative phrases. Data were collected from 142 subjects, yielding over 13,000 instances of synchronized data (~3.8 TB). For technical validation, we demonstrate two baseline examples. The first baseline shows classification by gender, utilizing different combinations of the three data streams in both clean and noisy environments. The second example consists of thermal-to-visual facial image translation, as an instance of domain transfer.



### Multi Scale Temporal Graph Networks For Skeleton-based Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2012.02970v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2012.02970v1)
- **Published**: 2020-12-05 08:08:25+00:00
- **Updated**: 2020-12-05 08:08:25+00:00
- **Authors**: Tingwei Li, Ruiwen Zhang, Qing Li
- **Comment**: None
- **Journal**: 2020,Computer Science & Information Technology (CS & IT)
- **Summary**: Graph convolutional networks (GCNs) can effectively capture the features of related nodes and improve the performance of the model. More attention is paid to employing GCN in Skeleton-Based action recognition. But existing methods based on GCNs have two problems. First, the consistency of temporal and spatial features is ignored for extracting features node by node and frame by frame. To obtain spatiotemporal features simultaneously, we design a generic representation of skeleton sequences for action recognition and propose a novel model called Temporal Graph Networks (TGN). Secondly, the adjacency matrix of the graph describing the relation of joints is mostly dependent on the physical connection between joints. To appropriately describe the relations between joints in the skeleton graph, we propose a multi-scale graph strategy, adopting a full-scale graph, part-scale graph, and core-scale graph to capture the local features of each joint and the contour features of important joints. Experiments were carried out on two large datasets and results show that TGN with our graph strategy outperforms state-of-the-art methods.



### A grid-point detection method based on U-net for a structured light system
- **Arxiv ID**: http://arxiv.org/abs/2012.08641v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2012.08641v1)
- **Published**: 2020-12-05 09:33:43+00:00
- **Updated**: 2020-12-05 09:33:43+00:00
- **Authors**: Dieuthuy Pham, Minhtuan Ha, Changyan Xiao
- **Comment**: http://airccse.org/csit/V10N16.html
- **Journal**: None
- **Summary**: Accurate detection of the feature points of the projected pattern plays an extremely important role in one-shot 3D reconstruction systems, especially for the ones using a grid pattern. To solve this problem, this paper proposes a grid-point detection method based on U-net. A specific dataset is designed that includes the images captured with the two-shot imaging method and the ones acquired with the one-shot imaging method. Among them, the images in the first group after labeled as the ground truth images and the images captured at the same pose with the one-shot method are cut into small patches with the size of 64x64 pixels then feed to the training set. The remaining of the images in the second group is the test set. The experimental results show that our method can achieve a better detecting performance with higher accuracy in comparison with the previous methods.



### Spatially-Adaptive Pixelwise Networks for Fast Image Translation
- **Arxiv ID**: http://arxiv.org/abs/2012.02992v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.02992v1)
- **Published**: 2020-12-05 10:02:03+00:00
- **Updated**: 2020-12-05 10:02:03+00:00
- **Authors**: Tamar Rott Shaham, Michael Gharbi, Richard Zhang, Eli Shechtman, Tomer Michaeli
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a new generator architecture, aimed at fast and efficient high-resolution image-to-image translation. We design the generator to be an extremely lightweight function of the full-resolution image. In fact, we use pixel-wise networks; that is, each pixel is processed independently of others, through a composition of simple affine transformations and nonlinearities. We take three important steps to equip such a seemingly simple function with adequate expressivity. First, the parameters of the pixel-wise networks are spatially varying so they can represent a broader function class than simple 1x1 convolutions. Second, these parameters are predicted by a fast convolutional network that processes an aggressively low-resolution representation of the input; Third, we augment the input image with a sinusoidal encoding of spatial coordinates, which provides an effective inductive bias for generating realistic novel high-frequency image content. As a result, our model is up to 18x faster than state-of-the-art baselines. We achieve this speedup while generating comparable visual quality across different image resolutions and translation domains.



### Attention-Driven Dynamic Graph Convolutional Network for Multi-Label Image Recognition
- **Arxiv ID**: http://arxiv.org/abs/2012.02994v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.02994v1)
- **Published**: 2020-12-05 10:10:12+00:00
- **Updated**: 2020-12-05 10:10:12+00:00
- **Authors**: Jin Ye, Junjun He, Xiaojiang Peng, Wenhao Wu, Yu Qiao
- **Comment**: This paper has been accepted by ECCV 2020 (Source codes have been
  released on https://github.com/Yejin0111/ADD-GCN
- **Journal**: None
- **Summary**: Recent studies often exploit Graph Convolutional Network (GCN) to model label dependencies to improve recognition accuracy for multi-label image recognition. However, constructing a graph by counting the label co-occurrence possibilities of the training data may degrade model generalizability, especially when there exist occasional co-occurrence objects in test images. Our goal is to eliminate such bias and enhance the robustness of the learnt features. To this end, we propose an Attention-Driven Dynamic Graph Convolutional Network (ADD-GCN) to dynamically generate a specific graph for each image. ADD-GCN adopts a Dynamic Graph Convolutional Network (D-GCN) to model the relation of content-aware category representations that are generated by a Semantic Attention Module (SAM). Extensive experiments on public multi-label benchmarks demonstrate the effectiveness of our method, which achieves mAPs of 85.2%, 96.0%, and 95.5% on MS-COCO, VOC2007, and VOC2012, respectively, and outperforms current state-of-the-art methods with a clear margin. All codes can be found at https://github.com/Yejin0111/ADD-GCN.



### ProMask: Probability Mask for Skeleton Detection
- **Arxiv ID**: http://arxiv.org/abs/2012.03003v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.03003v1)
- **Published**: 2020-12-05 10:54:36+00:00
- **Updated**: 2020-12-05 10:54:36+00:00
- **Authors**: Xiuxiu Bai, Lele Ye, Zhe Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Detecting object skeletons in natural images presents challenging, due to varied object scales, the complexity of backgrounds and various noises. The skeleton is a highly compressing shape representation, which can bring some essential advantages but cause the difficulties of detection. This skeleton line occupies a rare proportion of an image and is overly sensitive to spatial position. Inspired by these issues, we propose the ProMask, which is a novel skeleton detection model. The ProMask includes the probability mask and vector router. The skeleton probability mask representation explicitly encodes skeletons with segmentation signals, which can provide more supervised information to learn and pay more attention to ground-truth skeleton pixels. Moreover, the vector router module possesses two sets of orthogonal basis vectors in a two-dimensional space, which can dynamically adjust the predicted skeleton position. We evaluate our method on the well-known skeleton datasets, realizing the better performance than state-of-the-art approaches. Especially, ProMask significantly outperforms the competitive DeepFlux by 6.2% on the challenging SYM-PASCAL dataset. We consider that our proposed skeleton probability mask could serve as a solid baseline for future skeleton detection, since it is very effective and it requires about 10 lines of code.



### Automatic Segmentation and Location Learning of Neonatal Cerebral Ventricles in 3D Ultrasound Data Combining CNN and CPPN
- **Arxiv ID**: http://arxiv.org/abs/2012.03014v2
- **DOI**: 10.1016/j.compbiomed.2021.104268
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2012.03014v2)
- **Published**: 2020-12-05 11:57:26+00:00
- **Updated**: 2021-02-20 14:48:22+00:00
- **Authors**: Matthieu Martin, Bruno Sciolla, Michaël Sdika, Philippe Quétin, Philippe Delachartre
- **Comment**: 18 pages, 14 figures
- **Journal**: None
- **Summary**: Preterm neonates are highly likely to suffer from ventriculomegaly, a dilation of the Cerebral Ventricular System (CVS). This condition can develop into life-threatening hydrocephalus and is correlated with future neuro-developmental impairments. Consequently, it must be detected and monitored by physicians. In clinical routing, manual 2D measurements are performed on 2D ultrasound (US) images to estimate the CVS volume but this practice is imprecise due to the unavailability of 3D information. A way to tackle this problem would be to develop automatic CVS segmentation algorithms for 3D US data. In this paper, we investigate the potential of 2D and 3D Convolutional Neural Networks (CNN) to solve this complex task and propose to use Compositional Pattern Producing Network (CPPN) to enable the CNNs to learn CVS location. Our database was composed of 25 3D US volumes collected on 21 preterm nenonates at the age of $35.8 \pm 1.6$ gestational weeks. We found that the CPPN enables to encode CVS location, which increases the accuracy of the CNNs when they have few layers. Accuracy of the 2D and 3D CNNs reached intraobserver variability (IOV) in the case of dilated ventricles with Dice of $0.893 \pm 0.008$ and $0.886 \pm 0.004$ respectively (IOV = $0.898 \pm 0.008$) and with volume errors of $0.45 \pm 0.42$ cm$^3$ and $0.36 \pm 0.24$ cm$^3$ respectively (IOV = $0.41 \pm 0.05$ cm$^3$). 3D CNNs were more accurate than 2D CNNs in the case of normal ventricles with Dice of $0.797 \pm 0.041$ against $0.776 \pm 0.038$ (IOV = $0.816 \pm 0.009$) and volume errors of $0.35 \pm 0.29$ cm$^3$ against $0.35 \pm 0.24$ cm$^3$ (IOV = $0.2 \pm 0.11$ cm$^3$). The best segmentation time of volumes of size $320 \times 320 \times 320$ was obtained by a 2D CNN in $3.5 \pm 0.2$ s.



### CIA-SSD: Confident IoU-Aware Single-Stage Object Detector From Point Cloud
- **Arxiv ID**: http://arxiv.org/abs/2012.03015v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.03015v1)
- **Published**: 2020-12-05 12:00:08+00:00
- **Updated**: 2020-12-05 12:00:08+00:00
- **Authors**: Wu Zheng, Weiliang Tang, Sijin Chen, Li Jiang, Chi-Wing Fu
- **Comment**: Accepted by the 35th AAAI Conference on Artificial Intelligence
  (AAAI), 2021
- **Journal**: None
- **Summary**: Existing single-stage detectors for locating objects in point clouds often treat object localization and category classification as separate tasks, so the localization accuracy and classification confidence may not well align. To address this issue, we present a new single-stage detector named the Confident IoU-Aware Single-Stage object Detector (CIA-SSD). First, we design the lightweight Spatial-Semantic Feature Aggregation module to adaptively fuse high-level abstract semantic features and low-level spatial features for accurate predictions of bounding boxes and classification confidence. Also, the predicted confidence is further rectified with our designed IoU-aware confidence rectification module to make the confidence more consistent with the localization accuracy. Based on the rectified confidence, we further formulate the Distance-variant IoU-weighted NMS to obtain smoother regressions and avoid redundant predictions. We experiment CIA-SSD on 3D car detection in the KITTI test set and show that it attains top performance in terms of the official ranking metric (moderate AP 80.28%) and above 32 FPS inference speed, outperforming all prior single-stage detectors. The code is available at https://github.com/Vegeta2020/CIA-SSD.



### Depth estimation from 4D light field videos
- **Arxiv ID**: http://arxiv.org/abs/2012.03021v2
- **DOI**: 10.1117/12.2591012
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2012.03021v2)
- **Published**: 2020-12-05 12:33:15+00:00
- **Updated**: 2020-12-08 08:25:32+00:00
- **Authors**: Takahiro Kinoshita, Satoshi Ono
- **Comment**: 6 pages, 6 figures, International Workshop on Advanced Image
  Technology (IWAIT) 2021
- **Journal**: None
- **Summary**: Depth (disparity) estimation from 4D Light Field (LF) images has been a research topic for the last couple of years. Most studies have focused on depth estimation from static 4D LF images while not considering temporal information, i.e., LF videos. This paper proposes an end-to-end neural network architecture for depth estimation from 4D LF videos. This study also constructs a medium-scale synthetic 4D LF video dataset that can be used for training deep learning-based methods. Experimental results using synthetic and real-world 4D LF videos show that temporal information contributes to the improvement of depth estimation accuracy in noisy regions. Dataset and code is available at: https://mediaeng-lfv.github.io/LFV_Disparity_Estimation



### ParaNet: Deep Regular Representation for 3D Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2012.03028v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.03028v1)
- **Published**: 2020-12-05 13:19:55+00:00
- **Updated**: 2020-12-05 13:19:55+00:00
- **Authors**: Qijian Zhang, Junhui Hou, Yue Qian, Juyong Zhang, Ying He
- **Comment**: None
- **Journal**: None
- **Summary**: Although convolutional neural networks have achieved remarkable success in analyzing 2D images/videos, it is still non-trivial to apply the well-developed 2D techniques in regular domains to the irregular 3D point cloud data. To bridge this gap, we propose ParaNet, a novel end-to-end deep learning framework, for representing 3D point clouds in a completely regular and nearly lossless manner. To be specific, ParaNet converts an irregular 3D point cloud into a regular 2D color image, named point geometry image (PGI), where each pixel encodes the spatial coordinates of a point. In contrast to conventional regular representation modalities based on multi-view projection and voxelization, the proposed representation is differentiable and reversible. Technically, ParaNet is composed of a surface embedding module, which parameterizes 3D surface points onto a unit square, and a grid resampling module, which resamples the embedded 2D manifold over regular dense grids. Note that ParaNet is unsupervised, i.e., the training simply relies on reference-free geometry constraints. The PGIs can be seamlessly coupled with a task network established upon standard and mature techniques for 2D images/videos to realize a specific task for 3D point clouds. We evaluate ParaNet over shape classification and point cloud upsampling, in which our solutions perform favorably against the existing state-of-the-art methods. We believe such a paradigm will open up many possibilities to advance the progress of deep learning-based point cloud processing and understanding.



### Understanding Bird's-Eye View of Road Semantics using an Onboard Camera
- **Arxiv ID**: http://arxiv.org/abs/2012.03040v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2012.03040v2)
- **Published**: 2020-12-05 14:39:14+00:00
- **Updated**: 2022-01-14 13:20:15+00:00
- **Authors**: Yigit Baran Can, Alexander Liniger, Ozan Unal, Danda Paudel, Luc Van Gool
- **Comment**: IEEE Robotics and Automation Letters
- **Journal**: None
- **Summary**: Autonomous navigation requires scene understanding of the action-space to move or anticipate events. For planner agents moving on the ground plane, such as autonomous vehicles, this translates to scene understanding in the bird's-eye view (BEV). However, the onboard cameras of autonomous cars are customarily mounted horizontally for a better view of the surrounding. In this work, we study scene understanding in the form of online estimation of semantic BEV maps using the video input from a single onboard camera. We study three key aspects of this task, image-level understanding, BEV level understanding, and the aggregation of temporal information. Based on these three pillars we propose a novel architecture that combines these three aspects. In our extensive experiments, we demonstrate that the considered aspects are complementary to each other for BEV understanding. Furthermore, the proposed architecture significantly surpasses the current state-of-the-art. Code: https://github.com/ybarancan/BEV_feat_stitch.



### Self-Supervised Visual Representation Learning from Hierarchical Grouping
- **Arxiv ID**: http://arxiv.org/abs/2012.03044v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.03044v1)
- **Published**: 2020-12-05 14:54:08+00:00
- **Updated**: 2020-12-05 14:54:08+00:00
- **Authors**: Xiao Zhang, Michael Maire
- **Comment**: Accepted by NeurIPS 2020
- **Journal**: None
- **Summary**: We create a framework for bootstrapping visual representation learning from a primitive visual grouping capability. We operationalize grouping via a contour detector that partitions an image into regions, followed by merging of those regions into a tree hierarchy. A small supervised dataset suffices for training this grouping primitive. Across a large unlabeled dataset, we apply this learned primitive to automatically predict hierarchical region structure. These predictions serve as guidance for self-supervised contrastive feature learning: we task a deep network with producing per-pixel embeddings whose pairwise distances respect the region hierarchy. Experiments demonstrate that our approach can serve as state-of-the-art generic pre-training, benefiting downstream tasks. We additionally explore applications to semantic region search and video-based object instance tracking.



### A Survey on Deep Learning with Noisy Labels: How to train your model when you cannot trust on the annotations?
- **Arxiv ID**: http://arxiv.org/abs/2012.03061v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2012.03061v1)
- **Published**: 2020-12-05 15:45:20+00:00
- **Updated**: 2020-12-05 15:45:20+00:00
- **Authors**: Filipe R. Cordeiro, Gustavo Carneiro
- **Comment**: Paper published at SIBRAPI, 2020 (camera ready version)
- **Journal**: 2020 33rd SIBGRAPI Conference on Graphics, Patterns and Images
  (SIBGRAPI)
- **Summary**: Noisy Labels are commonly present in data sets automatically collected from the internet, mislabeled by non-specialist annotators, or even specialists in a challenging task, such as in the medical field. Although deep learning models have shown significant improvements in different domains, an open issue is their ability to memorize noisy labels during training, reducing their generalization potential. As deep learning models depend on correctly labeled data sets and label correctness is difficult to guarantee, it is crucial to consider the presence of noisy labels for deep learning training. Several approaches have been proposed in the literature to improve the training of deep learning models in the presence of noisy labels. This paper presents a survey on the main techniques in literature, in which we classify the algorithm in the following groups: robust losses, sample weighting, sample selection, meta-learning, and combined approaches. We also present the commonly used experimental setup, data sets, and results of the state-of-the-art models.



### Dynamic Neural Radiance Fields for Monocular 4D Facial Avatar Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2012.03065v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2012.03065v1)
- **Published**: 2020-12-05 16:01:16+00:00
- **Updated**: 2020-12-05 16:01:16+00:00
- **Authors**: Guy Gafni, Justus Thies, Michael Zollhöfer, Matthias Nießner
- **Comment**: Video: https://youtu.be/m7oROLdQnjk | Project page:
  https://gafniguy.github.io/4D-Facial-Avatars/
- **Journal**: None
- **Summary**: We present dynamic neural radiance fields for modeling the appearance and dynamics of a human face. Digitally modeling and reconstructing a talking human is a key building-block for a variety of applications. Especially, for telepresence applications in AR or VR, a faithful reproduction of the appearance including novel viewpoints or head-poses is required. In contrast to state-of-the-art approaches that model the geometry and material properties explicitly, or are purely image-based, we introduce an implicit representation of the head based on scene representation networks. To handle the dynamics of the face, we combine our scene representation network with a low-dimensional morphable model which provides explicit control over pose and expressions. We use volumetric rendering to generate images from this hybrid representation and demonstrate that such a dynamic neural scene representation can be learned from monocular input data only, without the need of a specialized capture setup. In our experiments, we show that this learned volumetric representation allows for photo-realistic image generation that surpasses the quality of state-of-the-art video-based reenactment methods.



### MyFood: A Food Segmentation and Classification System to Aid Nutritional Monitoring
- **Arxiv ID**: http://arxiv.org/abs/2012.03087v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.03087v1)
- **Published**: 2020-12-05 17:40:05+00:00
- **Updated**: 2020-12-05 17:40:05+00:00
- **Authors**: Charles N. C. Freitas, Filipe R. Cordeiro, Valmir Macario
- **Comment**: Paper published at SIBRAPI 2020 (Camera ready version)
- **Journal**: 2020 33rd SIBGRAPI Conference on Graphics, Patterns and Images
  (SIBGRAPI)
- **Summary**: The absence of food monitoring has contributed significantly to the increase in the population's weight. Due to the lack of time and busy routines, most people do not control and record what is consumed in their diet. Some solutions have been proposed in computer vision to recognize food images, but few are specialized in nutritional monitoring. This work presents the development of an intelligent system that classifies and segments food presented in images to help the automatic monitoring of user diet and nutritional intake. This work shows a comparative study of state-of-the-art methods for image classification and segmentation, applied to food recognition. In our methodology, we compare the FCN, ENet, SegNet, DeepLabV3+, and Mask RCNN algorithms. We build a dataset composed of the most consumed Brazilian food types, containing nine classes and a total of 1250 images. The models were evaluated using the following metrics: Intersection over Union, Sensitivity, Specificity, Balanced Precision, and Positive Predefined Value. We also propose an system integrated into a mobile application that automatically recognizes and estimates the nutrients in a meal, assisting people with better nutritional monitoring. The proposed solution showed better results than the existing ones in the market. The dataset is publicly available at the following link http://doi.org/10.5281/zenodo.4041488



### Semantic Segmentation of Medium-Resolution Satellite Imagery using Conditional Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2012.03093v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2012.03093v1)
- **Published**: 2020-12-05 18:18:45+00:00
- **Updated**: 2020-12-05 18:18:45+00:00
- **Authors**: Aditya Kulkarni, Tharun Mohandoss, Daniel Northrup, Ernest Mwebaze, Hamed Alemohammad
- **Comment**: Presented at the AI for Earth Sciences Workshop at NeurIPS 2020
- **Journal**: None
- **Summary**: Semantic segmentation of satellite imagery is a common approach to identify patterns and detect changes around the planet. Most of the state-of-the-art semantic segmentation models are trained in a fully supervised way using Convolutional Neural Network (CNN). The generalization property of CNN is poor for satellite imagery because the data can be very diverse in terms of landscape types, image resolutions, and scarcity of labels for different geographies and seasons. Hence, the performance of CNN doesn't translate well to images from unseen regions or seasons. Inspired by Conditional Generative Adversarial Networks (CGAN) based approach of image-to-image translation for high-resolution satellite imagery, we propose a CGAN framework for land cover classification using medium-resolution Sentinel-2 imagery. We find that the CGAN model outperforms the CNN model of similar complexity by a significant margin on an unseen imbalanced test dataset.



### When Do Curricula Work?
- **Arxiv ID**: http://arxiv.org/abs/2012.03107v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2012.03107v3)
- **Published**: 2020-12-05 19:41:30+00:00
- **Updated**: 2021-02-09 17:38:58+00:00
- **Authors**: Xiaoxia Wu, Ethan Dyer, Behnam Neyshabur
- **Comment**: ICLR 2021
- **Journal**: None
- **Summary**: Inspired by human learning, researchers have proposed ordering examples during training based on their difficulty. Both curriculum learning, exposing a network to easier examples early in training, and anti-curriculum learning, showing the most difficult examples first, have been suggested as improvements to the standard i.i.d. training. In this work, we set out to investigate the relative benefits of ordered learning. We first investigate the \emph{implicit curricula} resulting from architectural and optimization bias and find that samples are learned in a highly consistent order. Next, to quantify the benefit of \emph{explicit curricula}, we conduct extensive experiments over thousands of orderings spanning three kinds of learning: curriculum, anti-curriculum, and random-curriculum -- in which the size of the training dataset is dynamically increased over time, but the examples are randomly ordered. We find that for standard benchmark datasets, curricula have only marginal benefits, and that randomly ordered samples perform as well or better than curricula and anti-curricula, suggesting that any benefit is entirely due to the dynamic training set size. Inspired by common use cases of curriculum learning in practice, we investigate the role of limited training time budget and noisy data in the success of curriculum learning. Our experiments demonstrate that curriculum, but not anti-curriculum can indeed improve the performance either with limited training time budget or in existence of noisy data.



### Generating Synthetic Multispectral Satellite Imagery from Sentinel-2
- **Arxiv ID**: http://arxiv.org/abs/2012.03108v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2012.03108v1)
- **Published**: 2020-12-05 19:41:33+00:00
- **Updated**: 2020-12-05 19:41:33+00:00
- **Authors**: Tharun Mohandoss, Aditya Kulkarni, Daniel Northrup, Ernest Mwebaze, Hamed Alemohammad
- **Comment**: Presented at the AI for Earth Sciences Workshop at NeurIPS 2020
- **Journal**: None
- **Summary**: Multi-spectral satellite imagery provides valuable data at global scale for many environmental and socio-economic applications. Building supervised machine learning models based on these imagery, however, may require ground reference labels which are not available at global scale. Here, we propose a generative model to produce multi-resolution multi-spectral imagery based on Sentinel-2 data. The resulting synthetic images are indistinguishable from real ones by humans. This technique paves the road for future work to generate labeled synthetic imagery that can be used for data augmentation in data scarce regions and applications.



### Spectral Distribution Aware Image Generation
- **Arxiv ID**: http://arxiv.org/abs/2012.03110v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2012.03110v2)
- **Published**: 2020-12-05 19:46:48+00:00
- **Updated**: 2020-12-30 15:33:15+00:00
- **Authors**: Steffen Jung, Margret Keuper
- **Comment**: Accepted at AAAI 2021 (conference version). Code:
  https://github.com/steffen-jung/SpectralGAN
- **Journal**: None
- **Summary**: Recent advances in deep generative models for photo-realistic images have led to high quality visual results. Such models learn to generate data from a given training distribution such that generated images can not be easily distinguished from real images by the human eye. Yet, recent work on the detection of such fake images pointed out that they are actually easily distinguishable by artifacts in their frequency spectra. In this paper, we propose to generate images according to the frequency distribution of the real data by employing a spectral discriminator. The proposed discriminator is lightweight, modular and works stably with different commonly used GAN losses. We show that the resulting models can better generate images with realistic frequency spectra, which are thus harder to detect by this cue.



### LandCoverNet: A global benchmark land cover classification training dataset
- **Arxiv ID**: http://arxiv.org/abs/2012.03111v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2012.03111v1)
- **Published**: 2020-12-05 19:48:57+00:00
- **Updated**: 2020-12-05 19:48:57+00:00
- **Authors**: Hamed Alemohammad, Kevin Booth
- **Comment**: Presented at the AI for Earth Sciences Workshop at NeurIPS 2020
- **Journal**: None
- **Summary**: Regularly updated and accurate land cover maps are essential for monitoring 14 of the 17 Sustainable Development Goals. Multispectral satellite imagery provide high-quality and valuable information at global scale that can be used to develop land cover classification models. However, such a global application requires a geographically diverse training dataset. Here, we present LandCoverNet, a global training dataset for land cover classification based on Sentinel-2 observations at 10m spatial resolution. Land cover class labels are defined based on annual time-series of Sentinel-2, and verified by consensus among three human annotators.



### It's All Around You: Range-Guided Cylindrical Network for 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2012.03121v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CG, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2012.03121v1)
- **Published**: 2020-12-05 21:02:18+00:00
- **Updated**: 2020-12-05 21:02:18+00:00
- **Authors**: Meytal Rapoport-Lavie, Dan Raviv
- **Comment**: None
- **Journal**: None
- **Summary**: Modern perception systems in the field of autonomous driving rely on 3D data analysis. LiDAR sensors are frequently used to acquire such data due to their increased resilience to different lighting conditions. Although rotating LiDAR scanners produce ring-shaped patterns in space, most networks analyze their data using an orthogonal voxel sampling strategy. This work presents a novel approach for analyzing 3D data produced by 360-degree depth scanners, utilizing a more suitable coordinate system, which is aligned with the scanning pattern. Furthermore, we introduce a novel notion of range-guided convolutions, adapting the receptive field by distance from the ego vehicle and the object's scale. Our network demonstrates powerful results on the nuScenes challenge, comparable to current state-of-the-art architectures. The backbone architecture introduced in this work can be easily integrated onto other pipelines as well.



### Development and Characterization of a Chest CT Atlas
- **Arxiv ID**: http://arxiv.org/abs/2012.03124v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2012.03124v1)
- **Published**: 2020-12-05 21:20:57+00:00
- **Updated**: 2020-12-05 21:20:57+00:00
- **Authors**: Kaiwen Xu, Riqiang Gao, Mirza S. Khan, Shunxing Bao, Yucheng Tang, Steve A. Deppen, Yuankai Huo, Kim L. Sandler, Pierre P. Massion, Mattias P. Heinrich, Bennett A. Landman
- **Comment**: Accepted by SPIE2021 Medical Imaging (oral)
- **Journal**: None
- **Summary**: A major goal of lung cancer screening is to identify individuals with particular phenotypes that are associated with high risk of cancer. Identifying relevant phenotypes is complicated by the variation in body position and body composition. In the brain, standardized coordinate systems (e.g., atlases) have enabled separate consideration of local features from gross/global structure. To date, no analogous standard atlas has been presented to enable spatial mapping and harmonization in chest computational tomography (CT). In this paper, we propose a thoracic atlas built upon a large low dose CT (LDCT) database of lung cancer screening program. The study cohort includes 466 male and 387 female subjects with no screening detected malignancy (age 46-79 years, mean 64.9 years). To provide spatial mapping, we optimize a multi-stage inter-subject non-rigid registration pipeline for the entire thoracic space. We evaluate the optimized pipeline relative to two baselines with alternative non-rigid registration module: the same software with default parameters and an alternative software. We achieve a significant improvement in terms of registration success rate based on manual QA. For the entire study cohort, the optimized pipeline achieves a registration success rate of 91.7%. The application validity of the developed atlas is evaluated in terms of discriminative capability for different anatomic phenotypes, including body mass index (BMI), chronic obstructive pulmonary disease (COPD), and coronary artery calcification (CAC).



### Simultaneous Corn and Soybean Yield Prediction from Remote Sensing Data Using Deep Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/2012.03129v3
- **DOI**: 10.1038/s41598-021-89779-z
- **Categories**: **cs.CV**, cs.LG, eess.IV, q-bio.QM, stat.AP
- **Links**: [PDF](http://arxiv.org/pdf/2012.03129v3)
- **Published**: 2020-12-05 22:09:07+00:00
- **Updated**: 2021-06-03 15:20:12+00:00
- **Authors**: Saeed Khaki, Hieu Pham, Lizhi Wang
- **Comment**: 14 pages, 8 figures, 7 tables
- **Journal**: Scientific Reports, 11(1), 1-14 (2021)
- **Summary**: Large-scale crop yield estimation is, in part, made possible due to the availability of remote sensing data allowing for the continuous monitoring of crops throughout their growth cycle. Having this information allows stakeholders the ability to make real-time decisions to maximize yield potential. Although various models exist that predict yield from remote sensing data, there currently does not exist an approach that can estimate yield for multiple crops simultaneously, and thus leads to more accurate predictions. A model that predicts the yield of multiple crops and concurrently considers the interaction between multiple crop yields. We propose a new convolutional neural network model called YieldNet which utilizes a novel deep learning framework that uses transfer learning between corn and soybean yield predictions by sharing the weights of the backbone feature extractor. Additionally, to consider the multi-target response variable, we propose a new loss function. We conduct our experiment using data from 1,132 counties for corn and 1,076 counties for soybean across the United States. Numerical results demonstrate that our proposed method accurately predicts corn and soybean yield from one to four months before the harvest with a MAE being 8.74% and 8.70% of the average yield, respectively, and is competitive to other state-of-the-art approaches.



### Selective Eye-gaze Augmentation To Enhance Imitation Learning In Atari Games
- **Arxiv ID**: http://arxiv.org/abs/2012.03145v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2012.03145v1)
- **Published**: 2020-12-05 23:35:55+00:00
- **Updated**: 2020-12-05 23:35:55+00:00
- **Authors**: Chaitanya Thammineni, Hemanth Manjunatha, Ehsan T. Esfahani
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents the selective use of eye-gaze information in learning human actions in Atari games. Vast evidence suggests that our eye movement convey a wealth of information about the direction of our attention and mental states and encode the information necessary to complete a task. Based on this evidence, we hypothesize that selective use of eye-gaze, as a clue for attention direction, will enhance the learning from demonstration. For this purpose, we propose a selective eye-gaze augmentation (SEA) network that learns when to use the eye-gaze information. The proposed network architecture consists of three sub-networks: gaze prediction, gating, and action prediction network. Using the prior 4 game frames, a gaze map is predicted by the gaze prediction network which is used for augmenting the input frame. The gating network will determine whether the predicted gaze map should be used in learning and is fed to the final network to predict the action at the current frame. To validate this approach, we use publicly available Atari Human Eye-Tracking And Demonstration (Atari-HEAD) dataset consists of 20 Atari games with 28 million human demonstrations and 328 million eye-gazes (over game frames) collected from four subjects. We demonstrate the efficacy of selective eye-gaze augmentation in comparison with state of the art Attention Guided Imitation Learning (AGIL), Behavior Cloning (BC). The results indicate that the selective augmentation approach (the SEA network) performs significantly better than the AGIL and BC. Moreover, to demonstrate the significance of selective use of gaze through the gating network, we compare our approach with the random selection of the gaze. Even in this case, the SEA network performs significantly better validating the advantage of selectively using the gaze in demonstration learning.



