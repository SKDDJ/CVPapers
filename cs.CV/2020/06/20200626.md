# Arxiv Papers in cs.CV on 2020-06-26
### Cascaded Convolutional Neural Networks with Perceptual Loss for Low Dose CT Denoising
- **Arxiv ID**: http://arxiv.org/abs/2006.14738v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.14738v1)
- **Published**: 2020-06-26 00:35:26+00:00
- **Updated**: 2020-06-26 00:35:26+00:00
- **Authors**: Sepehr Ataei, Dr. Javad Alirezaie, Dr. Paul Babyn
- **Comment**: None
- **Journal**: None
- **Summary**: Low Dose CT Denoising research aims to reduce the risks of radiation exposure to patients. Recently researchers have used deep learning to denoise low dose CT images with promising results. However, approaches that use mean-squared-error (MSE) tend to over smooth the image resulting in loss of fine structural details in low contrast regions of the image. These regions are often crucial for diagnosis and must be preserved in order for Low dose CT to be used effectively in practice. In this work we use a cascade of two neural networks, the first of which aims to reconstruct normal dose CT from low dose CT by minimizing perceptual loss, and the second which predicts the difference between the ground truth and prediction from the perceptual loss network. We show that our method outperforms related works and more effectively reconstructs fine structural details in low contrast regions of the image.



### Graph Optimal Transport for Cross-Domain Alignment
- **Arxiv ID**: http://arxiv.org/abs/2006.14744v3
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.14744v3)
- **Published**: 2020-06-26 01:14:23+00:00
- **Updated**: 2020-07-24 20:04:49+00:00
- **Authors**: Liqun Chen, Zhe Gan, Yu Cheng, Linjie Li, Lawrence Carin, Jingjing Liu
- **Comment**: None
- **Journal**: ICML 2020
- **Summary**: Cross-domain alignment between two sets of entities (e.g., objects in an image, words in a sentence) is fundamental to both computer vision and natural language processing. Existing methods mainly focus on designing advanced attention mechanisms to simulate soft alignment, with no training signals to explicitly encourage alignment. The learned attention matrices are also dense and lacks interpretability. We propose Graph Optimal Transport (GOT), a principled framework that germinates from recent advances in Optimal Transport (OT). In GOT, cross-domain alignment is formulated as a graph matching problem, by representing entities into a dynamically-constructed graph. Two types of OT distances are considered: (i) Wasserstein distance (WD) for node (entity) matching; and (ii) Gromov-Wasserstein distance (GWD) for edge (structure) matching. Both WD and GWD can be incorporated into existing neural network models, effectively acting as a drop-in regularizer. The inferred transport plan also yields sparse and self-normalized alignment, enhancing the interpretability of the learned model. Experiments show consistent outperformance of GOT over baselines across a wide range of tasks, including image-text retrieval, visual question answering, image captioning, machine translation, and text summarization.



### Point Proposal Network for Reconstructing 3D Particle Endpoints with Sub-Pixel Precision in Liquid Argon Time Projection Chambers
- **Arxiv ID**: http://arxiv.org/abs/2006.14745v3
- **DOI**: 10.1103/PhysRevD.104.032004
- **Categories**: **hep-ex**, cs.CV, physics.ins-det
- **Links**: [PDF](http://arxiv.org/pdf/2006.14745v3)
- **Published**: 2020-06-26 01:18:43+00:00
- **Updated**: 2020-07-10 15:30:44+00:00
- **Authors**: Laura Dominé, Pierre Côte de Soux, François Drielsma, Dae Heun Koh, Ran Itay, Qing Lin, Kazuhiro Terao, Ka Vang Tsang, Tracy L. Usher
- **Comment**: None
- **Journal**: Phys. Rev. D 104, 032004 (2021)
- **Summary**: Liquid Argon Time Projection Chambers (LArTPC) are particle imaging detectors recording 2D or 3D images of trajectories of charged particles. Identifying points of interest in these images, namely the initial and terminal points of track-like particle trajectories such as muons and protons, and the initial points of electromagnetic shower-like particle trajectories such as electrons and gamma rays, is a crucial step of identifying and analyzing these particles and impacts the inference of physics signals such as neutrino interaction. The Point Proposal Network is designed to discover these specific points of interest. The algorithm predicts with a sub-voxel precision their spatial location, and also determines the category of the identified points of interest. Using as a benchmark the PILArNet public LArTPC data sample in which the voxel resolution is 3mm/voxel, our algorithm successfully predicted 96.8% and 97.8% of 3D points within a distance of 3 and 10~voxels from the provided true point locations respectively. For the predicted 3D points within 3 voxels of the closest true point locations, the median distance is found to be 0.25 voxels, achieving the sub-voxel level precision. In addition, we report our analysis of the mistakes where our algorithm prediction differs from the provided true point positions by more than 10~voxels. Among 50 mistakes visually scanned, 25 were due to the definition of true position location, 15 were legitimate mistakes where a physicist cannot visually disagree with the algorithm's prediction, and 10 were genuine mistakes that we wish to improve in the future. Further, using these predicted points, we demonstrate a simple algorithm to cluster 3D voxels into individual track-like particle trajectories with a clustering efficiency, purity, and Adjusted Rand Index of 96%, 93%, and 91% respectively.



### Deepfake Detection using Spatiotemporal Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/2006.14749v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.14749v1)
- **Published**: 2020-06-26 01:32:31+00:00
- **Updated**: 2020-06-26 01:32:31+00:00
- **Authors**: Oscar de Lima, Sean Franklin, Shreshtha Basu, Blake Karwoski, Annet George
- **Comment**: None
- **Journal**: None
- **Summary**: Better generative models and larger datasets have led to more realistic fake videos that can fool the human eye but produce temporal and spatial artifacts that deep learning approaches can detect. Most current Deepfake detection methods only use individual video frames and therefore fail to learn from temporal information. We created a benchmark of the performance of spatiotemporal convolutional methods using the Celeb-DF dataset. Our methods outperformed state-of-the-art frame-based detection methods. Code for our paper is publicly available at https://github.com/oidelima/Deepfake-Detection.



### Meta Deformation Network: Meta Functionals for Shape Correspondence
- **Arxiv ID**: http://arxiv.org/abs/2006.14758v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.14758v1)
- **Published**: 2020-06-26 02:28:51+00:00
- **Updated**: 2020-06-26 02:28:51+00:00
- **Authors**: Daohan Lu, Yi Fang
- **Comment**: None
- **Journal**: None
- **Summary**: We present a new technique named "Meta Deformation Network" for 3D shape matching via deformation, in which a deep neural network maps a reference shape onto the parameters of a second neural network whose task is to give the correspondence between a learned template and query shape via deformation. We categorize the second neural network as a meta-function, or a function generated by another function, as its parameters are dynamically given by the first network on a per-input basis. This leads to a straightforward overall architecture and faster execution speeds, without loss in the quality of the deformation of the template. We show in our experiments that Meta Deformation Network leads to improvements on the MPI-FAUST Inter Challenge over designs that utilized a conventional decoder design that has non-dynamic parameters.



### Lesion Mask-based Simultaneous Synthesis of Anatomic and MolecularMR Images using a GAN
- **Arxiv ID**: http://arxiv.org/abs/2006.14761v3
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.14761v3)
- **Published**: 2020-06-26 02:50:09+00:00
- **Updated**: 2020-08-26 19:03:46+00:00
- **Authors**: Pengfei Guo, Puyang Wang, Jinyuan Zhou, Vishal M. Patel, Shanshan Jiang
- **Comment**: MICCAI 2020
- **Journal**: None
- **Summary**: Data-driven automatic approaches have demonstrated their great potential in resolving various clinical diagnostic dilemmas for patients with malignant gliomas in neuro-oncology with the help of conventional and advanced molecular MR images. However, the lack of sufficient annotated MRI data has vastly impeded the development of such automatic methods. Conventional data augmentation approaches, including flipping, scaling, rotation, and distortion are not capable of generating data with diverse image content. In this paper, we propose a method, called synthesis of anatomic and molecular MR images network (SAMR), which can simultaneously synthesize data from arbitrary manipulated lesion information on multiple anatomic and molecular MRI sequences, including T1-weighted (T1w), gadolinium enhanced T1w (Gd-T1w), T2-weighted (T2w), fluid-attenuated inversion recovery (FLAIR), and amide proton transfer-weighted (APTw). The proposed framework consists of a stretch-out up-sampling module, a brain atlas encoder, a segmentation consistency module, and multi-scale label-wise discriminators. Extensive experiments on real clinical data demonstrate that the proposed model can perform significantly better than the state-of-the-art synthesis methods.



### Pushing the Limit of Unsupervised Learning for Ultrasound Image Artifact Removal
- **Arxiv ID**: http://arxiv.org/abs/2006.14773v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.14773v1)
- **Published**: 2020-06-26 03:21:56+00:00
- **Updated**: 2020-06-26 03:21:56+00:00
- **Authors**: Shujaat Khan, Jaeyoung Huh, Jong Chul Ye
- **Comment**: None
- **Journal**: None
- **Summary**: Ultrasound (US) imaging is a fast and non-invasive imaging modality which is widely used for real-time clinical imaging applications without concerning about radiation hazard. Unfortunately, it often suffers from poor visual quality from various origins, such as speckle noises, blurring, multi-line acquisition (MLA), limited RF channels, small number of view angles for the case of plane wave imaging, etc. Classical methods to deal with these problems include image-domain signal processing approaches using various adaptive filtering and model-based approaches. Recently, deep learning approaches have been successfully used for ultrasound imaging field. However, one of the limitations of these approaches is that paired high quality images for supervised training are difficult to obtain in many practical applications. In this paper, inspired by the recent theory of unsupervised learning using optimal transport driven cycleGAN (OT-cycleGAN), we investigate applicability of unsupervised deep learning for US artifact removal problems without matched reference data. Experimental results for various tasks such as deconvolution, speckle removal, limited data artifact removal, etc. confirmed that our unsupervised learning method provides comparable results to supervised learning for many practical applications.



### Blind Image Deconvolution using Student's-t Prior with Overlapping Group Sparsity
- **Arxiv ID**: http://arxiv.org/abs/2006.14780v1
- **DOI**: 10.1109/ICASSP.2017.7952470
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.14780v1)
- **Published**: 2020-06-26 03:34:44+00:00
- **Updated**: 2020-06-26 03:34:44+00:00
- **Authors**: In S. Jeon, Deokyoung Kang, Suk I. Yoo
- **Comment**: None
- **Journal**: 2017 IEEE International Conference on Acoustics, Speech and Signal
  Processing (ICASSP)
- **Summary**: In this paper, we solve blind image deconvolution problem that is to remove blurs form a signal degraded image without any knowledge of the blur kernel. Since the problem is ill-posed, an image prior plays a significant role in accurate blind deconvolution. Traditional image prior assumes coefficients in filtered domains are sparse. However, it is assumed here that there exist additional structures over the sparse coefficients. Accordingly, we propose new problem formulation for the blind image deconvolution, which utilizes the structural information by coupling Student's-t image prior with overlapping group sparsity. The proposed method resulted in an effective blind deconvolution algorithm that outperforms other state-of-the-art algorithms.



### On Equivariant and Invariant Learning of Object Landmark Representations
- **Arxiv ID**: http://arxiv.org/abs/2006.14787v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.14787v2)
- **Published**: 2020-06-26 04:06:56+00:00
- **Updated**: 2021-04-02 20:23:49+00:00
- **Authors**: Zezhou Cheng, Jong-Chyi Su, Subhransu Maji
- **Comment**: Project Page:
  https://people.cs.umass.edu/~zezhoucheng/contrastive_landmark Code:
  https://github.com/cvl-umass/ContrastLandmark
- **Journal**: None
- **Summary**: Given a collection of images, humans are able to discover landmarks by modeling the shared geometric structure across instances. This idea of geometric equivariance has been widely used for the unsupervised discovery of object landmark representations. In this paper, we develop a simple and effective approach by combining instance-discriminative and spatially-discriminative contrastive learning. We show that when a deep network is trained to be invariant to geometric and photometric transformations, representations emerge from its intermediate layers that are highly predictive of object landmarks. Stacking these across layers in a "hypercolumn" and projecting them using spatially-contrastive learning further improves their performance on matching and few-shot landmark regression tasks. We also present a unified view of existing equivariant and invariant representation learning approaches through the lens of contrastive learning, shedding light on the nature of invariances learned. Experiments on standard benchmarks for landmark learning, as well as a new challenging one we propose, show that the proposed approach surpasses prior state-of-the-art.



### Ricci Curvature Based Volumetric Segmentation of the Auditory Ossicles
- **Arxiv ID**: http://arxiv.org/abs/2006.14788v3
- **DOI**: None
- **Categories**: **cs.CV**, math.DG
- **Links**: [PDF](http://arxiv.org/pdf/2006.14788v3)
- **Published**: 2020-06-26 04:09:15+00:00
- **Updated**: 2022-03-02 10:09:36+00:00
- **Authors**: Na Lei, Jisui Huang, Yuxue Ren, Emil Saucan, Zhenchang Wang
- **Comment**: There is a fundamental problem with the layout of our paper, and we
  should design a general segmentation framework rather than just focusing on
  the ossicles
- **Journal**: None
- **Summary**: The auditory ossicles that are located in the middle ear are the smallest bones in the human body. Their damage will result in hearing loss. It is therefore important to be able to automatically diagnose ossicles' diseases based on Computed Tomography (CT) 3D imaging. However CT images usually include the whole head area, which is much larger than the bones of interest, thus the localization of the ossicles, followed by segmentation, both play a significant role in automatic diagnosis. The commonly employed local segmentation methods require manually selected initial points, which is a highly time consuming process. We therefore propose a completely automatic method to locate the ossicles which requires neither templates, nor manual labels. It relies solely on the connective properties of the auditory ossicles themselves, and their relationship with the surrounding tissue fluid. For the segmentation task, we define a novel energy function and obtain the shape of the ossicles from the 3D CT image by minimizing this new energy. Compared to the state-of-the-art methods which usually use the gradient operator and some normalization terms, we propose to add a Ricci curvature term to the commonly employed energy function. We compare our proposed method with the state-of-the-art methods and show that the performance of discrete Forman-Ricci curvature is superior to the others.



### Storing Encoded Episodes as Concepts for Continual Learning
- **Arxiv ID**: http://arxiv.org/abs/2007.06637v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.06637v1)
- **Published**: 2020-06-26 04:15:56+00:00
- **Updated**: 2020-06-26 04:15:56+00:00
- **Authors**: Ali Ayub, Alan R. Wagner
- **Comment**: Accepted at ICML2020 (Workshop on Lifelong Learning)
- **Journal**: None
- **Summary**: The two main challenges faced by continual learning approaches are catastrophic forgetting and memory limitations on the storage of data. To cope with these challenges, we propose a novel, cognitively-inspired approach which trains autoencoders with Neural Style Transfer to encode and store images. Reconstructed images from encoded episodes are replayed when training the classifier model on a new task to avoid catastrophic forgetting. The loss function for the reconstructed images is weighted to reduce its effect during classifier training to cope with image degradation. When the system runs out of memory the encoded episodes are converted into centroids and covariance matrices, which are used to generate pseudo-images during classifier training, keeping classifier performance stable with less memory. Our approach increases classification accuracy by 13-17% over state-of-the-art methods on benchmark datasets, while requiring 78% less storage space.



### Text Detection on Roughly Placed Books by Leveraging a Learning-based Model Trained with Another Domain Data
- **Arxiv ID**: http://arxiv.org/abs/2006.14808v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.14808v1)
- **Published**: 2020-06-26 05:53:23+00:00
- **Updated**: 2020-06-26 05:53:23+00:00
- **Authors**: Riku Anegawa, Masayoshi Aritsugi
- **Comment**: None
- **Journal**: None
- **Summary**: Text detection enables us to extract rich information from images. In this paper, we focus on how to generate bounding boxes that are appropriate to grasp text areas on books to help implement automatic text detection. We attempt not to improve a learning-based model by training it with an enough amount of data in the target domain but to leverage it, which has been already trained with another domain data. We develop algorithms that construct the bounding boxes by improving and leveraging the results of a learning-based method. Our algorithms can utilize different learning-based approaches to detect scene texts. Experimental evaluations demonstrate that our algorithms work well in various situations where books are roughly placed.



### Few-Shot Anomaly Detection for Polyp Frames from Colonoscopy
- **Arxiv ID**: http://arxiv.org/abs/2006.14811v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.14811v1)
- **Published**: 2020-06-26 06:08:46+00:00
- **Updated**: 2020-06-26 06:08:46+00:00
- **Authors**: Yu Tian, Gabriel Maicas, Leonardo Zorron Cheng Tao Pu, Rajvinder Singh, Johan W. Verjans, Gustavo Carneiro
- **Comment**: Accept at MICCAI 2020
- **Journal**: None
- **Summary**: Anomaly detection methods generally target the learning of a normal image distribution (i.e., inliers showing healthy cases) and during testing, samples relatively far from the learned distribution are classified as anomalies (i.e., outliers showing disease cases). These approaches tend to be sensitive to outliers that lie relatively close to inliers (e.g., a colonoscopy image with a small polyp). In this paper, we address the inappropriate sensitivity to outliers by also learning from inliers. We propose a new few-shot anomaly detection method based on an encoder trained to maximise the mutual information between feature embeddings and normal images, followed by a few-shot score inference network, trained with a large set of inliers and a substantially smaller set of outliers. We evaluate our proposed method on the clinical problem of detecting frames containing polyps from colonoscopy video sequences, where the training set has 13350 normal images (i.e., without polyps) and less than 100 abnormal images (i.e., with polyps). The results of our proposed model on this data set reveal a state-of-the-art detection result, while the performance based on different number of anomaly samples is relatively stable after approximately 40 abnormal training images.



### A survey of loss functions for semantic segmentation
- **Arxiv ID**: http://arxiv.org/abs/2006.14822v4
- **DOI**: 10.1109/CIBCB48159.2020.9277638
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.14822v4)
- **Published**: 2020-06-26 06:49:18+00:00
- **Updated**: 2020-09-03 01:14:34+00:00
- **Authors**: Shruti Jadon
- **Comment**: 5 pages, 5 figures, 2 tables
- **Journal**: 2020 IEEE International Conference on Computational Intelligence
  in Bioinformatics and Computational Biology
- **Summary**: Image Segmentation has been an active field of research as it has a wide range of applications, ranging from automated disease detection to self-driving cars. In the past five years, various papers came up with different objective loss functions used in different cases such as biased data, sparse segmentation, etc. In this paper, we have summarized some of the well-known loss functions widely used for Image Segmentation and listed out the cases where their usage can help in fast and better convergence of a model. Furthermore, we have also introduced a new log-cosh dice loss function and compared its performance on the NBFS skull-segmentation open-source data-set with widely used loss functions. We also showcased that certain loss functions perform well across all data-sets and can be taken as a good baseline choice in unknown data distribution scenarios. Our code is available at Github: https://github.com/shruti-jadon/Semantic-Segmentation-Loss-Functions.



### Expandable YOLO: 3D Object Detection from RGB-D Images
- **Arxiv ID**: http://arxiv.org/abs/2006.14837v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.14837v1)
- **Published**: 2020-06-26 07:32:30+00:00
- **Updated**: 2020-06-26 07:32:30+00:00
- **Authors**: Masahiro Takahashi, Alessandro Moro, Yonghoon Ji, Kazunori Umeda
- **Comment**: 5 pages, 8 figures
- **Journal**: None
- **Summary**: This paper aims at constructing a light-weight object detector that inputs a depth and a color image from a stereo camera. Specifically, by extending the network architecture of YOLOv3 to 3D in the middle, it is possible to output in the depth direction. In addition, Intersection over Uninon (IoU) in 3D space is introduced to confirm the accuracy of region extraction results. In the field of deep learning, object detectors that use distance information as input are actively studied for utilizing automated driving. However, the conventional detector has a large network structure, and the real-time property is impaired. The effectiveness of the detector constructed as described above is verified using datasets. As a result of this experiment, the proposed model is able to output 3D bounding boxes and detect people whose part of the body is hidden. Further, the processing speed of the model is 44.35 fps.



### Not all Failure Modes are Created Equal: Training Deep Neural Networks for Explicable (Mis)Classification
- **Arxiv ID**: http://arxiv.org/abs/2006.14841v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.14841v2)
- **Published**: 2020-06-26 07:37:33+00:00
- **Updated**: 2021-11-01 22:34:14+00:00
- **Authors**: Alberto Olmo, Sailik Sengupta, Subbarao Kambhampati
- **Comment**: None
- **Journal**: None
- **Summary**: Deep Neural Networks are often brittle on image classification tasks and known to misclassify inputs. While these misclassifications may be inevitable, all failure modes cannot be considered equal. Certain misclassifications (eg. classifying the image of a dog to an airplane) can perplex humans and result in the loss of human trust in the system. Even worse, these errors (eg. a person misclassified as a primate) can have odious societal impacts. Thus, in this work, we aim to reduce inexplicable errors. To address this challenge, we first discuss methods to obtain the class-level semantics that capture the human's expectation ($M^h$) regarding which classes are semantically close {\em vs.} ones that are far away. We show that for popular image benchmarks (like CIFAR-10, CIFAR-100, ImageNet), class-level semantics can be readily obtained by leveraging either human subject studies or publicly available human-curated knowledge bases. Second, we propose the use of Weighted Loss Functions (WLFs) to penalize misclassifications by the weight of their inexplicability. Finally, we show that training (or fine-tuning) existing classifiers with the proposed methods lead to Deep Neural Networks that have (1) comparable top-1 accuracy, (2) more explicable failure modes on both in-distribution and out-of-distribution (OOD) test data, and (3) incur significantly less cost in the gathering of additional human labels compared to existing works.



### An Automatic Reader of Identity Documents
- **Arxiv ID**: http://arxiv.org/abs/2006.14853v1
- **DOI**: 10.1109/SMC.2019.8914438
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.14853v1)
- **Published**: 2020-06-26 08:22:40+00:00
- **Updated**: 2020-06-26 08:22:40+00:00
- **Authors**: Filippo Attivissimo, Nicola Giaquinto, Marco Scarpetta, Maurizio Spadavecchia
- **Comment**: 6 pages, 9 figures
- **Journal**: None
- **Summary**: Identity documents automatic reading and verification is an appealing technology for nowadays service industry, since this task is still mostly performed manually, leading to waste of economic and time resources. In this paper the prototype of a novel automatic reading system of identity documents is presented. The system has been thought to extract data of the main Italian identity documents from photographs of acceptable quality, like those usually required to online subscribers of various services. The document is first localized inside the photo, and then classified; finally, text recognition is executed. A synthetic dataset has been used, both for neural networks training, and for performance evaluation of the system. The synthetic dataset avoided privacy issues linked to the use of real photos of real documents, which will be used, instead, for future developments of the system.



### Orthogonal Deep Models As Defense Against Black-Box Attacks
- **Arxiv ID**: http://arxiv.org/abs/2006.14856v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.14856v1)
- **Published**: 2020-06-26 08:29:05+00:00
- **Updated**: 2020-06-26 08:29:05+00:00
- **Authors**: Mohammad A. A. K. Jalwana, Naveed Akhtar, Mohammed Bennamoun, Ajmal Mian
- **Comment**: Accepted in IEEE Access
- **Journal**: None
- **Summary**: Deep learning has demonstrated state-of-the-art performance for a variety of challenging computer vision tasks. On one hand, this has enabled deep visual models to pave the way for a plethora of critical applications like disease prognostics and smart surveillance. On the other, deep learning has also been found vulnerable to adversarial attacks, which calls for new techniques to defend deep models against these attacks. Among the attack algorithms, the black-box schemes are of serious practical concern since they only need publicly available knowledge of the targeted model. We carefully analyze the inherent weakness of deep models in black-box settings where the attacker may develop the attack using a model similar to the targeted model. Based on our analysis, we introduce a novel gradient regularization scheme that encourages the internal representation of a deep model to be orthogonal to another, even if the architectures of the two models are similar. Our unique constraint allows a model to concomitantly endeavour for higher accuracy while maintaining near orthogonal alignment of gradients with respect to a reference model. Detailed empirical study verifies that controlled misalignment of gradients under our orthogonality objective significantly boosts a model's robustness against transferable black-box adversarial attacks. In comparison to regular models, the orthogonal models are significantly more robust to a range of $l_p$ norm bounded perturbations. We verify the effectiveness of our technique on a variety of large-scale models.



### AutoSNAP: Automatically Learning Neural Architectures for Instrument Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2006.14858v1
- **DOI**: 10.1007/978-3-030-59716-0_36
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.14858v1)
- **Published**: 2020-06-26 08:34:47+00:00
- **Updated**: 2020-06-26 08:34:47+00:00
- **Authors**: David Kügler, Marc Uecker, Arjan Kuijper, Anirban Mukhopadhyay
- **Comment**: Accepted at MICCAI 2020 Preparing code for release at
  https://github.com/MECLabTUDA/AutoSNAP
- **Journal**: None
- **Summary**: Despite recent successes, the advances in Deep Learning have not yet been fully translated to Computer Assisted Intervention (CAI) problems such as pose estimation of surgical instruments. Currently, neural architectures for classification and segmentation tasks are adopted ignoring significant discrepancies between CAI and these tasks. We propose an automatic framework (AutoSNAP) for instrument pose estimation problems, which discovers and learns the architectures for neural networks. We introduce 1)~an efficient testing environment for pose estimation, 2)~a powerful architecture representation based on novel Symbolic Neural Architecture Patterns (SNAPs), and 3)~an optimization of the architecture using an efficient search scheme. Using AutoSNAP, we discover an improved architecture (SNAPNet) which outperforms both the hand-engineered i3PosNet and the state-of-the-art architecture search method DARTS.



### A Flexible Framework for Designing Trainable Priors with Adaptive Smoothing and Game Encoding
- **Arxiv ID**: http://arxiv.org/abs/2006.14859v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.14859v2)
- **Published**: 2020-06-26 08:34:54+00:00
- **Updated**: 2020-11-09 10:00:10+00:00
- **Authors**: Bruno Lecouat, Jean Ponce, Julien Mairal
- **Comment**: NeurIPS 2020
- **Journal**: None
- **Summary**: We introduce a general framework for designing and training neural network layers whose forward passes can be interpreted as solving non-smooth convex optimization problems, and whose architectures are derived from an optimization algorithm. We focus on convex games, solved by local agents represented by the nodes of a graph and interacting through regularization functions. This approach is appealing for solving imaging problems, as it allows the use of classical image priors within deep models that are trainable end to end. The priors used in this presentation include variants of total variation, Laplacian regularization, bilateral filtering, sparse coding on learned dictionaries, and non-local self similarities. Our models are fully interpretable as well as parameter and data efficient. Our experiments demonstrate their effectiveness on a large diversity of tasks ranging from image denoising and compressed sensing for fMRI to dense stereo matching.



### Domain Contrast for Domain Adaptive Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2006.14863v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.14863v1)
- **Published**: 2020-06-26 08:45:36+00:00
- **Updated**: 2020-06-26 08:45:36+00:00
- **Authors**: Feng Liu, Xiaoxong Zhang, Fang Wan, Xiangyang Ji, Qixiang Ye
- **Comment**: None
- **Journal**: None
- **Summary**: We present Domain Contrast (DC), a simple yet effective approach inspired by contrastive learning for training domain adaptive detectors. DC is deduced from the error bound minimization perspective of a transferred model, and is implemented with cross-domain contrast loss which is plug-and-play. By minimizing cross-domain contrast loss, DC guarantees the transferability of detectors while naturally alleviating the class imbalance issue in the target domain. DC can be applied at either image level or region level, consistently improving detectors' transferability and discriminability. Extensive experiments on commonly used benchmarks show that DC improves the baseline and state-of-the-art by significant margins, while demonstrating great potential for large domain divergence.



### RPM-Net: Recurrent Prediction of Motion and Parts from Point Cloud
- **Arxiv ID**: http://arxiv.org/abs/2006.14865v1
- **DOI**: 10.1145/3355089.3356573
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.14865v1)
- **Published**: 2020-06-26 08:51:11+00:00
- **Updated**: 2020-06-26 08:51:11+00:00
- **Authors**: Zihao Yan, Ruizhen Hu, Xingguang Yan, Luanmin Chen, Oliver van Kaick, Hao Zhang, Hui Huang
- **Comment**: Accepted to SIGGRAPH Asia 2019, project page at
  https://vcc.tech/research/2019/RPMNet
- **Journal**: ACM Transactions on Graphics (Proceedings of SIGGRAPH Asia),
  volume 38, number 6, pages 240:1--240:15, year 2019
- **Summary**: We introduce RPM-Net, a deep learning-based approach which simultaneously infers movable parts and hallucinates their motions from a single, un-segmented, and possibly partial, 3D point cloud shape. RPM-Net is a novel Recurrent Neural Network (RNN), composed of an encoder-decoder pair with interleaved Long Short-Term Memory (LSTM) components, which together predict a temporal sequence of pointwise displacements for the input point cloud. At the same time, the displacements allow the network to learn movable parts, resulting in a motion-based shape segmentation. Recursive applications of RPM-Net on the obtained parts can predict finer-level part motions, resulting in a hierarchical object segmentation. Furthermore, we develop a separate network to estimate part mobilities, e.g., per-part motion parameters, from the segmented motion sequence. Both networks learn deep predictive models from a training set that exemplifies a variety of mobilities for diverse objects. We show results of simultaneous motion and part predictions from synthetic and real scans of 3D objects exhibiting a variety of part mobilities, possibly involving multiple movable parts.



### An Interactive Data Visualization and Analytics Tool to Evaluate Mobility and Sociability Trends During COVID-19
- **Arxiv ID**: http://arxiv.org/abs/2006.14882v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/2006.14882v1)
- **Published**: 2020-06-26 09:27:53+00:00
- **Updated**: 2020-06-26 09:27:53+00:00
- **Authors**: Fan Zuo, Jingxing Wang, Jingqin Gao, Kaan Ozbay, Xuegang Jeff Ban, Yubin Shen, Hong Yang, Shri Iyer
- **Comment**: None
- **Journal**: None
- **Summary**: The COVID-19 outbreak has dramatically changed travel behavior in affected cities. The C2SMART research team has been investigating the impact of COVID-19 on mobility and sociability. New York City (NYC) and Seattle, two of the cities most affected by COVID-19 in the U.S. were included in our initial study. An all-in-one dashboard with data mining and cloud computing capabilities was developed for interactive data analytics and visualization to facilitate the understanding of the impact of the outbreak and corresponding policies such as social distancing on transportation systems. This platform is updated regularly and continues to evolve with the addition of new data, impact metrics, and visualizations to assist public and decision-makers to make informed decisions. This paper presents the architecture of the COVID related mobility data dashboard and preliminary mobility and sociability metrics for NYC and Seattle.



### Ensemble Transfer Learning for Emergency Landing Field Identification on Moderate Resource Heterogeneous Kubernetes Cluster
- **Arxiv ID**: http://arxiv.org/abs/2006.14887v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2006.14887v2)
- **Published**: 2020-06-26 09:40:32+00:00
- **Updated**: 2020-08-31 09:35:34+00:00
- **Authors**: Andreas Klos, Marius Rosenbaum, Wolfram Schiffmann
- **Comment**: None
- **Journal**: None
- **Summary**: The full loss of thrust of an aircraft requires fast and reliable decisions of the pilot. If no published landing field is within reach, an emergency landing field must be selected. The choice of a suitable emergency landing field denotes a crucial task to avoid unnecessary damage of the aircraft, risk for the civil population as well as the crew and all passengers on board. Especially in case of instrument meteorological conditions it is indispensable to use a database of suitable emergency landing fields. Thus, based on public available digital orthographic photos and digital surface models, we created various datasets with different sample sizes to facilitate training and testing of neural networks. Each dataset consists of a set of data layers. The best compositions of these data layers as well as the best performing transfer learning models are selected. Subsequently, certain hyperparameters of the chosen models for each sample size are optimized with Bayesian and Bandit optimization. The hyperparameter tuning is performed with a self-made Kubernetes cluster. The models outputs were investigated with respect to the input data by the utilization of layer-wise relevance propagation. With optimized models we created an ensemble model to improve the segmentation performance. Finally, an area around the airport of Arnsberg in North Rhine-Westphalia was segmented and emergency landing fields are identified, while the verification of the final approach's obstacle clearance is left unconsidered. These emergency landing fields are stored in a PostgreSQL database.



### An Investigation of Traffic Density Changes inside Wuhan during the COVID-19 Epidemic with GF-2 Time-Series Images
- **Arxiv ID**: http://arxiv.org/abs/2006.16098v2
- **DOI**: None
- **Categories**: **cs.CV**, 68U10, I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/2006.16098v2)
- **Published**: 2020-06-26 10:30:12+00:00
- **Updated**: 2021-07-15 07:26:58+00:00
- **Authors**: Chen Wu, Yinong Guo, Haonan Guo, Jingwen Yuan, Lixiang Ru, Hongruixuan Chen, Bo Du, Liangpei Zhang
- **Comment**: 35 pages, 9 figures, submitted to International Journal of Applied
  Earth Observation and Geoinformation
- **Journal**: None
- **Summary**: In order to mitigate the spread of COVID-19, Wuhan was the first city to implement strict lockdown policy in 2020. Even though numerous researches have discussed the travel restriction between cities and provinces, few studies focus on the effect of transportation control inside the city due to the lack of the measurement and available data in Wuhan. Since the public transports have been shut down in the beginning of city lockdown, the change of traffic density is a good indicator to reflect the intracity population flow. Therefore, in this paper, we collected time-series high-resolution remote sensing images with the resolution of 1m acquired before, during and after Wuhan lockdown by GF-2 satellite. Vehicles on the road were extracted and counted for the statistics of traffic density to reflect the changes of human transmissions in the whole period of Wuhan lockdown. Open Street Map was used to obtain observation road surfaces, and a vehicle detection method combing morphology filter and deep learning was utilized to extract vehicles with the accuracy of 62.56%. According to the experimental results, the traffic density of Wuhan dropped with the percentage higher than 80%, and even higher than 90% on main roads during city lockdown; after lockdown lift, the traffic density recovered to the normal rate. Traffic density distributions also show the obvious reduction and increase throughout the whole study area. The significant reduction and recovery of traffic density indicates that the lockdown policy in Wuhan show effectiveness in controlling human transmission inside the city, and the city returned to normal after lockdown lift.



### Fast Multi-Level Foreground Estimation
- **Arxiv ID**: http://arxiv.org/abs/2006.14970v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.14970v1)
- **Published**: 2020-06-26 13:16:13+00:00
- **Updated**: 2020-06-26 13:16:13+00:00
- **Authors**: Thomas Germer, Tobias Uelwer, Stefan Conrad, Stefan Harmeling
- **Comment**: Accepted at the 25th International Conference on Pattern Recognition
  2020 (ICPR)
- **Journal**: None
- **Summary**: Alpha matting aims to estimate the translucency of an object in a given image. The resulting alpha matte describes pixel-wise to what amount foreground and background colors contribute to the color of the composite image. While most methods in literature focus on estimating the alpha matte, the process of estimating the foreground colors given the input image and its alpha matte is often neglected, although foreground estimation is an essential part of many image editing workflows. In this work, we propose a novel method for foreground estimation given the alpha matte. We demonstrate that our fast multi-level approach yields results that are comparable with the state-of-the-art while outperforming those methods in computational runtime and memory usage.



### Suggestive Annotation of Brain Tumour Images with Gradient-guided Sampling
- **Arxiv ID**: http://arxiv.org/abs/2006.14984v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.14984v2)
- **Published**: 2020-06-26 13:39:49+00:00
- **Updated**: 2020-07-03 11:34:10+00:00
- **Authors**: Chengliang Dai, Shuo Wang, Yuanhan Mo, Kaichen Zhou, Elsa Angelini, Yike Guo, Wenjia Bai
- **Comment**: Paper accepted by MICCAI 2020
- **Journal**: None
- **Summary**: Machine learning has been widely adopted for medical image analysis in recent years given its promising performance in image segmentation and classification tasks. As a data-driven science, the success of machine learning, in particular supervised learning, largely depends on the availability of manually annotated datasets. For medical imaging applications, such annotated datasets are not easy to acquire. It takes a substantial amount of time and resource to curate an annotated medical image set. In this paper, we propose an efficient annotation framework for brain tumour images that is able to suggest informative sample images for human experts to annotate. Our experiments show that training a segmentation model with only 19% suggestively annotated patient scans from BraTS 2019 dataset can achieve a comparable performance to training a model on the full dataset for whole tumour segmentation task. It demonstrates a promising way to save manual annotation cost and improve data efficiency in medical imaging applications.



### High Resolution Zero-Shot Domain Adaptation of Synthetically Rendered Face Images
- **Arxiv ID**: http://arxiv.org/abs/2006.15031v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.15031v1)
- **Published**: 2020-06-26 15:00:04+00:00
- **Updated**: 2020-06-26 15:00:04+00:00
- **Authors**: Stephan J. Garbin, Marek Kowalski, Matthew Johnson, Jamie Shotton
- **Comment**: None
- **Journal**: None
- **Summary**: Generating photorealistic images of human faces at scale remains a prohibitively difficult task using computer graphics approaches. This is because these require the simulation of light to be photorealistic, which in turn requires physically accurate modelling of geometry, materials, and light sources, for both the head and the surrounding scene. Non-photorealistic renders however are increasingly easy to produce. In contrast to computer graphics approaches, generative models learned from more readily available 2D image data have been shown to produce samples of human faces that are hard to distinguish from real data. The process of learning usually corresponds to a loss of control over the shape and appearance of the generated images. For instance, even simple disentangling tasks such as modifying the hair independently of the face, which is trivial to accomplish in a computer graphics approach, remains an open research question. In this work, we propose an algorithm that matches a non-photorealistic, synthetically generated image to a latent vector of a pretrained StyleGAN2 model which, in turn, maps the vector to a photorealistic image of a person of the same pose, expression, hair, and lighting. In contrast to most previous work, we require no synthetic training data. To the best of our knowledge, this is the first algorithm of its kind to work at a resolution of 1K and represents a significant leap forward in visual realism.



### SAR2SAR: a semi-supervised despeckling algorithm for SAR images
- **Arxiv ID**: http://arxiv.org/abs/2006.15037v3
- **DOI**: 10.1109/JSTARS.2021.3071864
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.15037v3)
- **Published**: 2020-06-26 15:07:28+00:00
- **Updated**: 2021-04-13 09:41:33+00:00
- **Authors**: Emanuele Dalsasso, Loïc Denis, Florence Tupin
- **Comment**: The manuscript is the accepted version of IEEE STARS. Code is made
  available at https://gitlab.telecom-paris.fr/RING/SAR2SAR
- **Journal**: IEEE Journal of Selected Topics in Applied Earth Observations and
  Remote Sensing (Early Access), 2020
- **Summary**: Speckle reduction is a key step in many remote sensing applications. By strongly affecting synthetic aperture radar (SAR) images, it makes them difficult to analyse. Due to the difficulty to model the spatial correlation of speckle, a deep learning algorithm with self-supervision is proposed in this paper: SAR2SAR. Multi-temporal time series are leveraged and the neural network learns to restore SAR images by only looking at noisy acquisitions. To this purpose, the recently proposed noise2noise framework has been employed. The strategy to adapt it to SAR despeckling is presented, based on a compensation of temporal changes and a loss function adapted to the statistics of speckle.   A study with synthetic speckle noise is presented to compare the performances of the proposed method with other state-of-the-art filters. Then, results on real images are discussed, to show the potential of the proposed algorithm. The code is made available to allow testing and reproducible research in this field.



### Object-Centric Learning with Slot Attention
- **Arxiv ID**: http://arxiv.org/abs/2006.15055v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.15055v2)
- **Published**: 2020-06-26 15:31:57+00:00
- **Updated**: 2020-10-14 08:51:40+00:00
- **Authors**: Francesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg Heigold, Jakob Uszkoreit, Alexey Dosovitskiy, Thomas Kipf
- **Comment**: NeurIPS 2020. Code available at
  https://github.com/google-research/google-research/tree/master/slot_attention
- **Journal**: None
- **Summary**: Learning object-centric representations of complex scenes is a promising step towards enabling efficient abstract reasoning from low-level perceptual features. Yet, most deep learning approaches learn distributed representations that do not capture the compositional properties of natural scenes. In this paper, we present the Slot Attention module, an architectural component that interfaces with perceptual representations such as the output of a convolutional neural network and produces a set of task-dependent abstract representations which we call slots. These slots are exchangeable and can bind to any object in the input by specializing through a competitive procedure over multiple rounds of attention. We empirically demonstrate that Slot Attention can extract object-centric representations that enable generalization to unseen compositions when trained on unsupervised object discovery and supervised property prediction tasks.



### Cross-Supervised Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2006.15056v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.15056v2)
- **Published**: 2020-06-26 15:33:48+00:00
- **Updated**: 2020-06-29 03:11:46+00:00
- **Authors**: Zitian Chen, Zhiqiang Shen, Jiahui Yu, Erik Learned-Miller
- **Comment**: None
- **Journal**: None
- **Summary**: After learning a new object category from image-level annotations (with no object bounding boxes), humans are remarkably good at precisely localizing those objects. However, building good object localizers (i.e., detectors) currently requires expensive instance-level annotations. While some work has been done on learning detectors from weakly labeled samples (with only class labels), these detectors do poorly at localization. In this work, we show how to build better object detectors from weakly labeled images of new categories by leveraging knowledge learned from fully labeled base categories. We call this novel learning paradigm cross-supervised object detection. We propose a unified framework that combines a detection head trained from instance-level annotations and a recognition head learned from image-level annotations, together with a spatial correlation module that bridges the gap between detection and recognition. These contributions enable us to better detect novel objects with image-level annotations in complex multi-object scenes such as the COCO dataset.



### A Loss Function for Generative Neural Networks Based on Watson's Perceptual Model
- **Arxiv ID**: http://arxiv.org/abs/2006.15057v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.15057v3)
- **Published**: 2020-06-26 15:36:11+00:00
- **Updated**: 2021-01-06 11:16:21+00:00
- **Authors**: Steffen Czolbe, Oswin Krause, Ingemar Cox, Christian Igel
- **Comment**: Published at the 34th Conference on Neural Information Processing
  Systems (NeurIPS 2020)
- **Journal**: None
- **Summary**: To train Variational Autoencoders (VAEs) to generate realistic imagery requires a loss function that reflects human perception of image similarity. We propose such a loss function based on Watson's perceptual model, which computes a weighted distance in frequency space and accounts for luminance and contrast masking. We extend the model to color images, increase its robustness to translation by using the Fourier Transform, remove artifacts due to splitting the image into blocks, and make it differentiable. In experiments, VAEs trained with the new loss function generated realistic, high-quality image samples. Compared to using the Euclidean distance and the Structural Similarity Index, the images were less blurry; compared to deep neural network based losses, the new approach required less computational resources and generated images with less artifacts.



### Computing Light Transport Gradients using the Adjoint Method
- **Arxiv ID**: http://arxiv.org/abs/2006.15059v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV, cs.LG, physics.comp-ph
- **Links**: [PDF](http://arxiv.org/pdf/2006.15059v1)
- **Published**: 2020-06-26 15:38:14+00:00
- **Updated**: 2020-06-26 15:38:14+00:00
- **Authors**: Jos Stam
- **Comment**: 23 pages, 8 figures, unpublished manuscript
- **Journal**: None
- **Summary**: This paper proposes a new equation from continuous adjoint theory to compute the gradient of quantities governed by the Transport Theory of light. Unlike discrete gradients ala autograd, which work at the code level, we first formulate the continuous theory and then discretize it. The key insight of this paper is that computing gradients in Transport Theory is akin to computing the importance, a quantity adjoint to radiance that satisfies an adjoint equation. Importance tells us where to look for light that matters. This is one of the key insights of this paper. In fact, this mathematical journey started from a whimsical thought that these adjoints might be related. Computing gradients is therefore no more complicated than computing the importance field. This insight and the following paper hopefully will shed some light on this complicated problem and ease the implementations of gradient computations in existing path tracers.



### 4S-DT: Self Supervised Super Sample Decomposition for Transfer learning with application to COVID-19 detection
- **Arxiv ID**: http://arxiv.org/abs/2007.11450v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.11450v2)
- **Published**: 2020-06-26 16:09:31+00:00
- **Updated**: 2020-09-15 17:29:33+00:00
- **Authors**: Asmaa Abbas, Mohammed M. Abdelsamea, Mohamed Gaber
- **Comment**: None
- **Journal**: None
- **Summary**: Due to the high availability of large-scale annotated image datasets, knowledge transfer from pre-trained models showed outstanding performance in medical image classification. However, building a robust image classification model for datasets with data irregularity or imbalanced classes can be a very challenging task, especially in the medical imaging domain. In this paper, we propose a novel deep convolutional neural network, we called Self Supervised Super Sample Decomposition for Transfer learning (4S-DT) model. 4S-DT encourages a coarse-to-fine transfer learning from large-scale image recognition tasks to a specific chest X-ray image classification task using a generic self-supervised sample decomposition approach. Our main contribution is a novel self-supervised learning mechanism guided by a super sample decomposition of unlabelled chest X-ray images. 4S-DT helps in improving the robustness of knowledge transformation via a downstream learning strategy with a class-decomposition layer to simplify the local structure of the data. 4S-DT can deal with any irregularities in the image dataset by investigating its class boundaries using a downstream class-decomposition mechanism. We used 50,000 unlabelled chest X-ray images to achieve our coarse-to-fine transfer learning with an application to COVID-19 detection, as an exemplar. 4S-DT has achieved a high accuracy of 99.8% (95% CI: 99.44%, 99.98%) in the detection of COVID-19 cases on a large dataset and an accuracy of 97.54% (95%$ CI: 96.22%, 98.91%) on an extended test set enriched by augmented images of a small dataset, out of which all real COVID-19 cases were detected, which was the highest accuracy obtained when compared to other methods.



### COVID-19 Screening Using Residual Attention Network an Artificial Intelligence Approach
- **Arxiv ID**: http://arxiv.org/abs/2006.16106v3
- **DOI**: 10.1109/ICMLA51294.2020.00211
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.16106v3)
- **Published**: 2020-06-26 16:33:01+00:00
- **Updated**: 2020-10-20 16:54:38+00:00
- **Authors**: Vishal Sharma, Curtis Dyreson
- **Comment**: None
- **Journal**: 2020 19th IEEE International Conference on Machine Learning and
  Applications (ICMLA)
- **Summary**: Coronavirus Disease 2019 (COVID-19) is caused by severe acute respiratory syndrome coronavirus 2 virus (SARS-CoV-2). The virus transmits rapidly; it has a basic reproductive number R of 2.2-2.7. In March 2020, the World Health Organization declared the COVID-19 outbreak a pandemic. COVID-19 is currently affecting more than 200 countries with 6M active cases. An effective testing strategy for COVID-19 is crucial to controlling the outbreak but the demand for testing surpasses the availability of test kits that use Reverse Transcription Polymerase Chain Reaction (RT-PCR). In this paper, we present a technique to screen for COVID-19 using artificial intelligence. Our technique takes only seconds to screen for the presence of the virus in a patient. We collected a dataset of chest X-ray images and trained several popular deep convolution neural network-based models (VGG, MobileNet, Xception, DenseNet, InceptionResNet) to classify the chest X-rays. Unsatisfied with these models, we then designed and built a Residual Attention Network that was able to screen COVID-19 with a testing accuracy of 98% and a validation accuracy of 100%. A feature maps visual of our model show areas in a chest X-ray which are important for classification. Our work can help to increase the adaptation of AI-assisted applications in clinical practice. The code and dataset used in this project are available at https://github.com/vishalshar/covid-19-screening-using-RAN-on-X-ray-images.



### End-to-end training of deep kernel map networks for image classification
- **Arxiv ID**: http://arxiv.org/abs/2006.15088v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.15088v1)
- **Published**: 2020-06-26 16:37:41+00:00
- **Updated**: 2020-06-26 16:37:41+00:00
- **Authors**: Mingyuan Jiu, Hichem Sahbi
- **Comment**: None
- **Journal**: None
- **Summary**: Deep kernel map networks have shown excellent performances in various classification problems including image annotation. Their general recipe consists in aggregating several layers of singular value decompositions (SVDs) -- that map data from input spaces into high dimensional spaces -- while preserving the similarity of the underlying kernels. However, the potential of these deep map networks has not been fully explored as the original setting of these networks focuses mainly on the approximation quality of their kernels and ignores their discrimination power. In this paper, we introduce a novel "end-to-end" design for deep kernel map learning that balances the approximation quality of kernels and their discrimination power. Our method proceeds in two steps; first, layerwise SVD is applied in order to build initial deep kernel map approximations and then an "end-to-end" supervised learning is employed to further enhance their discrimination power while maintaining their efficiency. Extensive experiments, conducted on the challenging ImageCLEF annotation benchmark, show the high efficiency and the out-performance of this two-step process with respect to different related methods.



### ULSAM: Ultra-Lightweight Subspace Attention Module for Compact Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2006.15102v1
- **DOI**: 10.1109/WACV45572.2020.9093341
- **Categories**: **cs.CV**, I.5.1; I.5.2; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/2006.15102v1)
- **Published**: 2020-06-26 17:05:43+00:00
- **Updated**: 2020-06-26 17:05:43+00:00
- **Authors**: Rajat Saini, Nandan Kumar Jha, Bedanta Das, Sparsh Mittal, C. Krishna Mohan
- **Comment**: Accepted as a conference paper in 2020 IEEE Winter Conference on
  Applications of Computer Vision (WACV)
- **Journal**: WACV (2020) 1627-1636
- **Summary**: The capability of the self-attention mechanism to model the long-range dependencies has catapulted its deployment in vision models. Unlike convolution operators, self-attention offers infinite receptive field and enables compute-efficient modeling of global dependencies. However, the existing state-of-the-art attention mechanisms incur high compute and/or parameter overheads, and hence unfit for compact convolutional neural networks (CNNs). In this work, we propose a simple yet effective "Ultra-Lightweight Subspace Attention Mechanism" (ULSAM), which infers different attention maps for each feature map subspace. We argue that leaning separate attention maps for each feature subspace enables multi-scale and multi-frequency feature representation, which is more desirable for fine-grained image classification. Our method of subspace attention is orthogonal and complementary to the existing state-of-the-arts attention mechanisms used in vision models. ULSAM is end-to-end trainable and can be deployed as a plug-and-play module in the pre-existing compact CNNs. Notably, our work is the first attempt that uses a subspace attention mechanism to increase the efficiency of compact CNNs. To show the efficacy of ULSAM, we perform experiments with MobileNet-V1 and MobileNet-V2 as backbone architectures on ImageNet-1K and three fine-grained image classification datasets. We achieve $\approx$13% and $\approx$25% reduction in both the FLOPs and parameter counts of MobileNet-V2 with a 0.27% and more than 1% improvement in top-1 accuracy on the ImageNet-1K and fine-grained image classification datasets (respectively). Code and trained models are available at https://github.com/Nandan91/ULSAM.



### Person Re-identification by analyzing Dynamic Variations in Gait Sequences
- **Arxiv ID**: http://arxiv.org/abs/2006.15109v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.15109v1)
- **Published**: 2020-06-26 17:16:37+00:00
- **Updated**: 2020-06-26 17:16:37+00:00
- **Authors**: Sandesh Bharadwaj, Kunal Chanda
- **Comment**: Presented at ETCCS 2020, accepted for publication in Springer LNEE
  Proceedings
- **Journal**: None
- **Summary**: Gait recognition is a biometric technology that identifies individuals in a video sequence by analysing their style of walking or limb movement. However, this identification is generally sensitive to appearance changes and conventional feature descriptors such as Gait Energy Image (GEI) lose some of the dynamic information in the gait sequence. Active Energy Image (AEI) focuses more on dynamic motion changes than GEI and is more suited to deal with appearance changes. We propose a new approach, which allows recognizing people by analysing the dynamic motion variations and identifying people without using a database of predicted changes. In the proposed method, the active energy image is calculated by averaging the difference frames of the silhouette sequence and divided into multiple segments. Affine moment invariants are computed as gait features for each section. Next, matching weights are calculated based on the similarity between extracted features and those in the database. Finally, the subject is identified by the weighted combination of similarities in all segments. The CASIA-B Gait Database is used as the principal dataset for the experimental analysis.



### An Advert Creation System for 3D Product Placements
- **Arxiv ID**: http://arxiv.org/abs/2006.15131v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2006.15131v1)
- **Published**: 2020-06-26 17:41:50+00:00
- **Updated**: 2020-06-26 17:41:50+00:00
- **Authors**: Ivan Bacher, Hossein Javidnia, Soumyabrata Dev, Rahul Agrahari, Murhaf Hossari, Matthew Nicholson, Clare Conran, Jian Tang, Peng Song, David Corrigan, François Pitié
- **Comment**: Published in Proc. European Conference on Machine Learning and
  Principles and Practice of Knowledge Discovery in Databases (ECML-PKDD), 2020
- **Journal**: None
- **Summary**: Over the past decade, the evolution of video-sharing platforms has attracted a significant amount of investments on contextual advertising. The common contextual advertising platforms utilize the information provided by users to integrate 2D visual ads into videos. The existing platforms face many technical challenges such as ad integration with respect to occluding objects and 3D ad placement. This paper presents a Video Advertisement Placement & Integration (Adverts) framework, which is capable of perceiving the 3D geometry of the scene and camera motion to blend 3D virtual objects in videos and create the illusion of reality. The proposed framework contains several modules such as monocular depth estimation, object segmentation, background-foreground separation, alpha matting and camera tracking. Our experiments conducted using Adverts framework indicates the significant potential of this system in contextual ad integration, and pushing the limits of advertising industry using mixed reality technologies.



### Conditional Set Generation with Transformers
- **Arxiv ID**: http://arxiv.org/abs/2006.16841v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.16841v2)
- **Published**: 2020-06-26 17:52:27+00:00
- **Updated**: 2020-07-01 06:00:00+00:00
- **Authors**: Adam R Kosiorek, Hyunjik Kim, Danilo J Rezende
- **Comment**: 6 pages, 6 figures, ICML 2020 Workshop on Object-Oriented Learning
- **Journal**: None
- **Summary**: A set is an unordered collection of unique elements--and yet many machine learning models that generate sets impose an implicit or explicit ordering. Since model performance can depend on the choice of order, any particular ordering can lead to sub-optimal results. An alternative solution is to use a permutation-equivariant set generator, which does not specify an order-ing. An example of such a generator is the DeepSet Prediction Network (DSPN). We introduce the Transformer Set Prediction Network (TSPN), a flexible permutation-equivariant model for set prediction based on the transformer, that builds upon and outperforms DSPN in the quality of predicted set elements and in the accuracy of their predicted sizes. We test our model on MNIST-as-point-clouds (SET-MNIST) for point-cloud generation and on CLEVR for object detection.



### Bookworm continual learning: beyond zero-shot learning and continual learning
- **Arxiv ID**: http://arxiv.org/abs/2006.15176v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.15176v3)
- **Published**: 2020-06-26 19:07:18+00:00
- **Updated**: 2020-08-20 13:07:23+00:00
- **Authors**: Kai Wang, Luis Herranz, Anjan Dutta, Joost van de Weijer
- **Comment**: Accepted by TASK-CV workshop at ECCV 2020
- **Journal**: None
- **Summary**: We propose bookworm continual learning(BCL), a flexible setting where unseen classes can be inferred via a semantic model, and the visual model can be updated continually. Thus BCL generalizes both continual learning (CL) and zero-shot learning (ZSL). We also propose the bidirectional imagination (BImag) framework to address BCL where features of both past and future classes are generated. We observe that conditioning the feature generator on attributes can actually harm the continual learning ability, and propose two variants (joint class-attribute conditioning and asymmetric generation) to alleviate this problem.



### Region-of-interest guided Supervoxel Inpainting for Self-supervision
- **Arxiv ID**: http://arxiv.org/abs/2006.15186v1
- **DOI**: 10.1007/978-3-030-59710-8_49
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.15186v1)
- **Published**: 2020-06-26 19:28:20+00:00
- **Updated**: 2020-06-26 19:28:20+00:00
- **Authors**: Subhradeep Kayal, Shuai Chen, Marleen de Bruijne
- **Comment**: Accepted at MICCAI 2020
- **Journal**: None
- **Summary**: Self-supervised learning has proven to be invaluable in making best use of all of the available data in biomedical image segmentation. One particularly simple and effective mechanism to achieve self-supervision is inpainting, the task of predicting arbitrary missing areas based on the rest of an image. In this work, we focus on image inpainting as the self-supervised proxy task, and propose two novel structural changes to further enhance the performance of a deep neural network. We guide the process of generating images to inpaint by using supervoxel-based masking instead of random masking, and also by focusing on the area to be segmented in the primary task, which we term as the region-of-interest. We postulate that these additions force the network to learn semantics that are more attuned to the primary task, and test our hypotheses on two applications: brain tumour and white matter hyperintensities segmentation. We empirically show that our proposed approach consistently outperforms both supervised CNNs, without any self-supervision, and conventional inpainting-based self-supervision methods on both large and small training set sizes.



### Making DensePose fast and light
- **Arxiv ID**: http://arxiv.org/abs/2006.15190v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.15190v3)
- **Published**: 2020-06-26 19:42:20+00:00
- **Updated**: 2020-07-09 11:33:27+00:00
- **Authors**: Ruslan Rakhimov, Emil Bogomolov, Alexandr Notchenko, Fung Mao, Alexey Artemov, Denis Zorin, Evgeny Burnaev
- **Comment**: None
- **Journal**: None
- **Summary**: DensePose estimation task is a significant step forward for enhancing user experience computer vision applications ranging from augmented reality to cloth fitting. Existing neural network models capable of solving this task are heavily parameterized and a long way from being transferred to an embedded or mobile device. To enable Dense Pose inference on the end device with current models, one needs to support an expensive server-side infrastructure and have a stable internet connection. To make things worse, mobile and embedded devices do not always have a powerful GPU inside. In this work, we target the problem of redesigning the DensePose R-CNN model's architecture so that the final network retains most of its accuracy but becomes more light-weight and fast. To achieve that, we tested and incorporated many deep learning innovations from recent years, specifically performing an ablation study on 23 efficient backbone architectures, multiple two-stage detection pipeline modifications, and custom model quantization methods. As a result, we achieved $17\times$ model size reduction and $2\times$ latency improvement compared to the baseline model.



