# Arxiv Papers in cs.CV on 2020-06-16
### Dual-Resolution Correspondence Networks
- **Arxiv ID**: http://arxiv.org/abs/2006.08844v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.08844v2)
- **Published**: 2020-06-16 00:42:43+00:00
- **Updated**: 2020-10-28 17:16:58+00:00
- **Authors**: Xinghui Li, Kai Han, Shuda Li, Victor Adrian Prisacariu
- **Comment**: NeurIPS 2020, code at https://dualrcnet.active.vision/
- **Journal**: None
- **Summary**: We tackle the problem of establishing dense pixel-wise correspondences between a pair of images. In this work, we introduce Dual-Resolution Correspondence Networks (DualRC-Net), to obtain pixel-wise correspondences in a coarse-to-fine manner. DualRC-Net extracts both coarse- and fine- resolution feature maps. The coarse maps are used to produce a full but coarse 4D correlation tensor, which is then refined by a learnable neighbourhood consensus module. The fine-resolution feature maps are used to obtain the final dense correspondences guided by the refined coarse 4D correlation tensor. The selected coarse-resolution matching scores allow the fine-resolution features to focus only on a limited number of possible matches with high confidence. In this way, DualRC-Net dramatically increases matching reliability and localisation accuracy, while avoiding to apply the expensive 4D convolution kernels on fine-resolution feature maps. We comprehensively evaluate our method on large-scale public benchmarks including HPatches, InLoc, and Aachen Day-Night. It achieves the state-of-the-art results on all of them.



### Robust Recovery via Implicit Bias of Discrepant Learning Rates for Double Over-parameterization
- **Arxiv ID**: http://arxiv.org/abs/2006.08857v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, math.OC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.08857v1)
- **Published**: 2020-06-16 01:21:22+00:00
- **Updated**: 2020-06-16 01:21:22+00:00
- **Authors**: Chong You, Zhihui Zhu, Qing Qu, Yi Ma
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advances have shown that implicit bias of gradient descent on over-parameterized models enables the recovery of low-rank matrices from linear measurements, even with no prior knowledge on the intrinsic rank. In contrast, for robust low-rank matrix recovery from grossly corrupted measurements, over-parameterization leads to overfitting without prior knowledge on both the intrinsic rank and sparsity of corruption. This paper shows that with a double over-parameterization for both the low-rank matrix and sparse corruption, gradient descent with discrepant learning rates provably recovers the underlying matrix even without prior knowledge on neither rank of the matrix nor sparsity of the corruption. We further extend our approach for the robust recovery of natural images by over-parameterizing images with deep convolutional networks. Experiments show that our method handles different test images and varying corruption levels with a single learning pipeline where the network width and termination conditions do not need to be adjusted on a case-by-case basis. Underlying the success is again the implicit bias with discrepant learning rates on different over-parameterized parameters, which may bear on broader applications.



### GPU-accelerated Hierarchical Panoramic Image Feature Retrieval for Indoor Localization
- **Arxiv ID**: http://arxiv.org/abs/2006.08861v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.08861v1)
- **Published**: 2020-06-16 01:42:20+00:00
- **Updated**: 2020-06-16 01:42:20+00:00
- **Authors**: Feng Hu
- **Comment**: None
- **Journal**: None
- **Summary**: Indoor localization has many applications, such as commercial Location Based Services (LBS), robotic navigation, and assistive navigation for the blind. This paper formulates the indoor localization problem into a multimedia retrieving problem by modeling visual landmarks with a panoramic image feature, and calculating a user's location via GPU- accelerated parallel retrieving algorithm. To solve the scene similarity problem, we apply a multi-images based retrieval strategy and a 2D aggregation method to estimate the final retrieval location. Experiments on a campus building real data demonstrate real-time responses (14fps) and robust localization.



### CNN Acceleration by Low-rank Approximation with Quantized Factors
- **Arxiv ID**: http://arxiv.org/abs/2006.08878v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.08878v1)
- **Published**: 2020-06-16 02:28:05+00:00
- **Updated**: 2020-06-16 02:28:05+00:00
- **Authors**: Nikolay Kozyrskiy, Anh-Huy Phan
- **Comment**: None
- **Journal**: None
- **Summary**: The modern convolutional neural networks although achieve great results in solving complex computer vision tasks still cannot be effectively used in mobile and embedded devices due to the strict requirements for computational complexity, memory and power consumption. The CNNs have to be compressed and accelerated before deployment. In order to solve this problem the novel approach combining two known methods, low-rank tensor approximation in Tucker format and quantization of weights and feature maps (activations), is proposed. The greedy one-step and multi-step algorithms for the task of multilinear rank selection are proposed. The approach for quality restoration after applying Tucker decomposition and quantization is developed. The efficiency of our method is demonstrated for ResNet18 and ResNet34 on CIFAR-10, CIFAR-100 and Imagenet classification tasks. As a result of comparative analysis performed for other methods for compression and acceleration our approach showed its promising features.



### DeepCapture: Image Spam Detection Using Deep Learning and Data Augmentation
- **Arxiv ID**: http://arxiv.org/abs/2006.08885v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.08885v1)
- **Published**: 2020-06-16 02:50:04+00:00
- **Updated**: 2020-06-16 02:50:04+00:00
- **Authors**: Bedeuro Kim, Sharif Abuadbba, Hyoungshick Kim
- **Comment**: 15 pages, single column. ACISP 2020: Australasian Conference on
  Information Security and Privacy
- **Journal**: None
- **Summary**: Image spam emails are often used to evade text-based spam filters that detect spam emails with their frequently used keywords. In this paper, we propose a new image spam email detection tool called DeepCapture using a convolutional neural network (CNN) model. There have been many efforts to detect image spam emails, but there is a significant performance degrade against entirely new and unseen image spam emails due to overfitting during the training phase. To address this challenging issue, we mainly focus on developing a more robust model to address the overfitting problem. Our key idea is to build a CNN-XGBoost framework consisting of eight layers only with a large number of training samples using data augmentation techniques tailored towards the image spam detection task. To show the feasibility of DeepCapture, we evaluate its performance with publicly available datasets consisting of 6,000 spam and 2,313 non-spam image samples. The experimental results show that DeepCapture is capable of achieving an F1-score of 88%, which has a 6% improvement over the best existing spam detection model CNN-SVM with an F1-score of 82%. Moreover, DeepCapture outperformed existing image spam detection solutions against new and unseen image datasets.



### Exploiting Visual Semantic Reasoning for Video-Text Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2006.08889v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.08889v1)
- **Published**: 2020-06-16 02:56:46+00:00
- **Updated**: 2020-06-16 02:56:46+00:00
- **Authors**: Zerun Feng, Zhimin Zeng, Caili Guo, Zheng Li
- **Comment**: Accepted by IJCAI 2020. SOLE copyright holder is IJCAI (International
  Joint Conferences on Artificial Intelligence), all rights reserved.
  http://static.ijcai.org/2020-accepted_papers.html
- **Journal**: None
- **Summary**: Video retrieval is a challenging research topic bridging the vision and language areas and has attracted broad attention in recent years. Previous works have been devoted to representing videos by directly encoding from frame-level features. In fact, videos consist of various and abundant semantic relations to which existing methods pay less attention. To address this issue, we propose a Visual Semantic Enhanced Reasoning Network (ViSERN) to exploit reasoning between frame regions. Specifically, we consider frame regions as vertices and construct a fully-connected semantic correlation graph. Then, we perform reasoning by novel random walk rule-based graph convolutional networks to generate region features involved with semantic relations. With the benefit of reasoning, semantic interactions between regions are considered, while the impact of redundancy is suppressed. Finally, the region features are aggregated to form frame-level features for further encoding to measure video-text similarity. Extensive experiments on two public benchmark datasets validate the effectiveness of our method by achieving state-of-the-art performance due to the powerful semantic reasoning.



### Opportunities and Challenges in Explainable Artificial Intelligence (XAI): A Survey
- **Arxiv ID**: http://arxiv.org/abs/2006.11371v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.11371v2)
- **Published**: 2020-06-16 02:58:10+00:00
- **Updated**: 2020-06-23 01:48:56+00:00
- **Authors**: Arun Das, Paul Rad
- **Comment**: 24 pages, 20 figures, survey paper, submitting to IEEE
- **Journal**: None
- **Summary**: Nowadays, deep neural networks are widely used in mission critical systems such as healthcare, self-driving vehicles, and military which have direct impact on human lives. However, the black-box nature of deep neural networks challenges its use in mission critical applications, raising ethical and judicial concerns inducing lack of trust. Explainable Artificial Intelligence (XAI) is a field of Artificial Intelligence (AI) that promotes a set of tools, techniques, and algorithms that can generate high-quality interpretable, intuitive, human-understandable explanations of AI decisions. In addition to providing a holistic view of the current XAI landscape in deep learning, this paper provides mathematical summaries of seminal work. We start by proposing a taxonomy and categorizing the XAI techniques based on their scope of explanations, methodology behind the algorithms, and explanation level or usage which helps build trustworthy, interpretable, and self-explanatory deep learning models. We then describe the main principles used in XAI research and present the historical timeline for landmark studies in XAI from 2007 to 2020. After explaining each category of algorithms and approaches in detail, we then evaluate the explanation maps generated by eight XAI algorithms on image data, discuss the limitations of this approach, and provide potential future directions to improve XAI evaluation.



### Depth by Poking: Learning to Estimate Depth from Self-Supervised Grasping
- **Arxiv ID**: http://arxiv.org/abs/2006.08903v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.08903v1)
- **Published**: 2020-06-16 03:34:26+00:00
- **Updated**: 2020-06-16 03:34:26+00:00
- **Authors**: Ben Goodrich, Alex Kuefler, William D. Richards
- **Comment**: IEEE International Conference on Robotics and Automation (ICRA) 2020
- **Journal**: None
- **Summary**: Accurate depth estimation remains an open problem for robotic manipulation; even state of the art techniques including structured light and LiDAR sensors fail on reflective or transparent surfaces. We address this problem by training a neural network model to estimate depth from RGB-D images, using labels from physical interactions between a robot and its environment. Our network predicts, for each pixel in an input image, the z position that a robot's end effector would reach if it attempted to grasp or poke at the corresponding position. Given an autonomous grasping policy, our approach is self-supervised as end effector position labels can be recovered through forward kinematics, without human annotation. Although gathering such physical interaction data is expensive, it is necessary for training and routine operation of state of the art manipulation systems. Therefore, this depth estimator comes ``for free'' while collecting data for other tasks (e.g., grasping, pushing, placing). We show our approach achieves significantly lower root mean squared error than traditional structured light sensors and unsupervised deep learning methods on difficult, industry-scale jumbled bin datasets.



### Plug-and-Play Anomaly Detection with Expectation Maximization Filtering
- **Arxiv ID**: http://arxiv.org/abs/2006.08933v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.08933v1)
- **Published**: 2020-06-16 05:28:40+00:00
- **Updated**: 2020-06-16 05:28:40+00:00
- **Authors**: Muhammad Umar Karim Khan, Mishal Fatima, Chong-Min Kyung
- **Comment**: None
- **Journal**: None
- **Summary**: Anomaly detection in crowds enables early rescue response. A plug-and-play smart camera for crowd surveillance has numerous constraints different from typical anomaly detection: the training data cannot be used iteratively; there are no training labels; and training and classification needs to be performed simultaneously. We tackle all these constraints with our approach in this paper. We propose a Core Anomaly-Detection (CAD) neural network which learns the motion behavior of objects in the scene with an unsupervised method. On average over standard datasets, CAD with a single epoch of training shows a percentage increase in Area Under the Curve (AUC) of 4.66% and 4.9% compared to the best results with convolutional autoencoders and convolutional LSTM-based methods, respectively. With a single epoch of training, our method improves the AUC by 8.03% compared to the convolutional LSTM-based approach. We also propose an Expectation Maximization filter which chooses samples for training the core anomaly-detection network. The overall framework improves the AUC compared to future frame prediction-based approach by 24.87% when crowd anomaly detection is performed on a video stream. We believe our work is the first step towards using deep learning methods with autonomous plug-and-play smart cameras for crowd anomaly detection.



### Channel Relationship Prediction with Forget-Update Module for Few-shot Classification
- **Arxiv ID**: http://arxiv.org/abs/2006.08937v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.08937v1)
- **Published**: 2020-06-16 05:48:08+00:00
- **Updated**: 2020-06-16 05:48:08+00:00
- **Authors**: Minglei Yuan, Cunhao Cai, Tong Lu
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we proposed a pipeline for inferring the relationship of each class in support set and a query sample using forget-update module. We first propose a novel architectural module called "channel vector sequence construction module", which boosts the performance of sequence-prediction-model-based few-shot classification methods by collecting the overall information of all support samples and a query sample. The channel vector sequence generated by this module is organized in a way that each time step of the sequence contains the information from the corresponding channel of all support samples and the query sample to be inferred. Channel vector sequence is obtained by a convolutional neural network and a fully connected network, and the spliced channel vector sequence is spliced of the corresponding channel vectors of support samples and a query sample in the original channel order. Also, we propose a forget-update module consisting of stacked forget-update blocks. The forget block modify the original information with the learned weights and the update block establishes a dense connection for the model. The proposed pipeline, which consists of channel vector sequence construction module and forget-update module, can infer the relationship between the query sample and support samples in few-shot classification scenario. Experimental results show that the pipeline can achieve state-of-the-art results on miniImagenet, CUB dataset, and cross-domain scenario.



### Learning the Redundancy-free Features for Generalized Zero-Shot Object Recognition
- **Arxiv ID**: http://arxiv.org/abs/2006.08939v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.08939v2)
- **Published**: 2020-06-16 05:53:25+00:00
- **Updated**: 2021-05-23 06:09:22+00:00
- **Authors**: Zongyan Han, Zhenyong Fu, Jian Yang
- **Comment**: Some researchers and we have found KNN results in 1st version are
  incorrect, due to a careless mistake in the code. Concretely, the parameters
  for accuracy function of KNN were organized in the wrong order by mistake.
  The softmax results are correct. We have removed all KNN results and remove
  the SOTA claims. According to the Program Chairs' suggestion, we have made
  errata request to CVF and IEEE
- **Journal**: None
- **Summary**: Zero-shot object recognition or zero-shot learning aims to transfer the object recognition ability among the semantically related categories, such as fine-grained animal or bird species. However, the images of different fine-grained objects tend to merely exhibit subtle differences in appearance, which will severely deteriorate zero-shot object recognition. To reduce the superfluous information in the fine-grained objects, in this paper, we propose to learn the redundancy-free features for generalized zero-shot learning. We achieve our motivation by projecting the original visual features into a new (redundancy-free) feature space and then restricting the statistical dependence between these two feature spaces. Furthermore, we require the projected features to keep and even strengthen the category relationship in the redundancy-free feature space. In this way, we can remove the redundant information from the visual features without losing the discriminative information. We extensively evaluate the performance on four benchmark datasets. The results show that our redundancy-free feature based generalized zero-shot learning (RFF-GZSL) approach can achieve competitive results compared with the state-of-the-arts.



### Global Feature Aggregation for Accident Anticipation
- **Arxiv ID**: http://arxiv.org/abs/2006.08942v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.08942v1)
- **Published**: 2020-06-16 06:17:15+00:00
- **Updated**: 2020-06-16 06:17:15+00:00
- **Authors**: Mishal Fatima, Muhammad Umar Karim Khan, Chong Min Kyung
- **Comment**: None
- **Journal**: None
- **Summary**: Anticipation of accidents ahead of time in autonomous and non-autonomous vehicles aids in accident avoidance. In order to recognize abnormal events such as traffic accidents in a video sequence, it is important that the network takes into account interactions of objects in a given frame. We propose a novel Feature Aggregation (FA) block that refines each object's features by computing a weighted sum of the features of all objects in a frame. We use FA block along with Long Short Term Memory (LSTM) network to anticipate accidents in the video sequences. We report mean Average Precision (mAP) and Average Time-to-Accident (ATTA) on Street Accident (SA) dataset. Our proposed method achieves the highest score for risk anticipation by predicting accidents 0.32 sec and 0.75 sec earlier compared to the best results with Adaptive Loss and dynamic parameter prediction based methods respectively.



### How Much Can I Trust You? -- Quantifying Uncertainties in Explaining Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2006.09000v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.09000v1)
- **Published**: 2020-06-16 08:54:42+00:00
- **Updated**: 2020-06-16 08:54:42+00:00
- **Authors**: Kirill Bykov, Marina M. -C. Höhne, Klaus-Robert Müller, Shinichi Nakajima, Marius Kloft
- **Comment**: 12 pages, 10 figures
- **Journal**: None
- **Summary**: Explainable AI (XAI) aims to provide interpretations for predictions made by learning machines, such as deep neural networks, in order to make the machines more transparent for the user and furthermore trustworthy also for applications in e.g. safety-critical areas. So far, however, no methods for quantifying uncertainties of explanations have been conceived, which is problematic in domains where a high confidence in explanations is a prerequisite. We therefore contribute by proposing a new framework that allows to convert any arbitrary explanation method for neural networks into an explanation method for Bayesian neural networks, with an in-built modeling of uncertainties. Within the Bayesian framework a network's weights follow a distribution that extends standard single explanation scores and heatmaps to distributions thereof, in this manner translating the intrinsic network model uncertainties into a quantification of explanation uncertainties. This allows us for the first time to carve out uncertainties associated with a model explanation and subsequently gauge the appropriate level of explanation confidence for a user (using percentiles). We demonstrate the effectiveness and usefulness of our approach extensively in various experiments, both qualitatively and quantitatively.



### RL-CycleGAN: Reinforcement Learning Aware Simulation-To-Real
- **Arxiv ID**: http://arxiv.org/abs/2006.09001v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.09001v1)
- **Published**: 2020-06-16 08:58:07+00:00
- **Updated**: 2020-06-16 08:58:07+00:00
- **Authors**: Kanishka Rao, Chris Harris, Alex Irpan, Sergey Levine, Julian Ibarz, Mohi Khansari
- **Comment**: Proceedings of the IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR 2020)
- **Journal**: None
- **Summary**: Deep neural network based reinforcement learning (RL) can learn appropriate visual representations for complex tasks like vision-based robotic grasping without the need for manually engineering or prior learning a perception system. However, data for RL is collected via running an agent in the desired environment, and for applications like robotics, running a robot in the real world may be extremely costly and time consuming. Simulated training offers an appealing alternative, but ensuring that policies trained in simulation can transfer effectively into the real world requires additional machinery. Simulations may not match reality, and typically bridging the simulation-to-reality gap requires domain knowledge and task-specific engineering. We can automate this process by employing generative models to translate simulated images into realistic ones. However, this sort of translation is typically task-agnostic, in that the translated images may not preserve all features that are relevant to the task. In this paper, we introduce the RL-scene consistency loss for image translation, which ensures that the translation operation is invariant with respect to the Q-values associated with the image. This allows us to learn a task-aware translation. Incorporating this loss into unsupervised domain translation, we obtain RL-CycleGAN, a new approach for simulation-to-real-world transfer for reinforcement learning. In evaluations of RL-CycleGAN on two vision-based robotics grasping tasks, we show that RL-CycleGAN offers a substantial improvement over a number of prior methods for sim-to-real transfer, attaining excellent real-world performance with only a modest number of real-world observations.



### Improved Techniques for Training Score-Based Generative Models
- **Arxiv ID**: http://arxiv.org/abs/2006.09011v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.09011v2)
- **Published**: 2020-06-16 09:17:17+00:00
- **Updated**: 2020-10-23 19:37:51+00:00
- **Authors**: Yang Song, Stefano Ermon
- **Comment**: NeurIPS 2020
- **Journal**: None
- **Summary**: Score-based generative models can produce high quality image samples comparable to GANs, without requiring adversarial optimization. However, existing training procedures are limited to images of low resolution (typically below 32x32), and can be unstable under some settings. We provide a new theoretical analysis of learning and sampling from score models in high dimensional spaces, explaining existing failure modes and motivating new solutions that generalize across datasets. To enhance stability, we also propose to maintain an exponential moving average of model weights. With these improvements, we can effortlessly scale score-based generative models to images with unprecedented resolutions ranging from 64x64 to 256x256. Our score-based models can generate high-fidelity samples that rival best-in-class GANs on various image datasets, including CelebA, FFHQ, and multiple LSUN categories.



### Real-time Universal Style Transfer on High-resolution Images via Zero-channel Pruning
- **Arxiv ID**: http://arxiv.org/abs/2006.09029v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.09029v2)
- **Published**: 2020-06-16 09:50:14+00:00
- **Updated**: 2020-06-23 03:37:40+00:00
- **Authors**: Jie An, Tao Li, Haozhi Huang, Li Shen, Xuan Wang, Yongyi Tang, Jinwen Ma, Wei Liu, Jiebo Luo
- **Comment**: None
- **Journal**: None
- **Summary**: Extracting effective deep features to represent content and style information is the key to universal style transfer. Most existing algorithms use VGG19 as the feature extractor, which incurs a high computational cost and impedes real-time style transfer on high-resolution images. In this work, we propose a lightweight alternative architecture - ArtNet, which is based on GoogLeNet, and later pruned by a novel channel pruning method named Zero-channel Pruning specially designed for style transfer approaches. Besides, we propose a theoretically sound sandwich swap transform (S2) module to transfer deep features, which can create a pleasing holistic appearance and good local textures with an improved content preservation ability. By using ArtNet and S2, our method is 2.3 to 107.4 times faster than state-of-the-art approaches. The comprehensive experiments demonstrate that ArtNet can achieve universal, real-time, and high-quality style transfer on high-resolution images simultaneously, (68.03 FPS on 512 times 512 images).



### Deep Learning based Segmentation of Fish in Noisy Forward Looking MBES Images
- **Arxiv ID**: http://arxiv.org/abs/2006.09034v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2006.09034v1)
- **Published**: 2020-06-16 09:57:38+00:00
- **Updated**: 2020-06-16 09:57:38+00:00
- **Authors**: Jesper Haahr Christensen, Lars Valdemar Mogensen, Ole Ravn
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we investigate a Deep Learning (DL) approach to fish segmentation in a small dataset of noisy low-resolution images generated by a forward-looking multibeam echosounder (MBES). We build on recent advances in DL and Convolutional Neural Networks (CNNs) for semantic segmentation and demonstrate an end-to-end approach for a fish/non-fish probability prediction for all range-azimuth positions projected by an imaging sonar. We use self-collected datasets from the Danish Sound and the Faroe Islands to train and test our model and present techniques to obtain satisfying performance and generalization even with a low-volume dataset. We show that our model proves the desired performance and has learned to harness the importance of semantic context and take this into account to separate noise and non-targets from real targets. Furthermore, we present techniques to deploy models on low-cost embedded platforms to obtain higher performance fit for edge environments - where compute and power are restricted by size/cost - for testing and prototyping.



### Fine-Tuning DARTS for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2006.09042v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.09042v1)
- **Published**: 2020-06-16 10:00:45+00:00
- **Updated**: 2020-06-16 10:00:45+00:00
- **Authors**: Muhammad Suhaib Tanveer, Muhammad Umar Karim Khan, Chong-Min Kyung
- **Comment**: 8 pages, 6 figures
- **Journal**: None
- **Summary**: Neural Architecture Search (NAS) has gained attraction due to superior classification performance. Differential Architecture Search (DARTS) is a computationally light method. To limit computational resources DARTS makes numerous approximations. These approximations result in inferior performance. We propose to fine-tune DARTS using fixed operations as they are independent of these approximations. Our method offers a good trade-off between the number of parameters and classification accuracy. Our approach improves the top-1 accuracy on Fashion-MNIST, CompCars, and MIO-TCD datasets by 0.56%, 0.50%, and 0.39%, respectively compared to the state-of-the-art approaches. Our approach performs better than DARTS, improving the accuracy by 0.28%, 1.64%, 0.34%, 4.5%, and 3.27% compared to DARTS, on CIFAR-10, CIFAR-100, Fashion-MNIST, CompCars, and MIO-TCD datasets, respectively.



### Improved Deep Point Cloud Geometry Compression
- **Arxiv ID**: http://arxiv.org/abs/2006.09043v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, eess.SP, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.09043v2)
- **Published**: 2020-06-16 10:03:14+00:00
- **Updated**: 2020-06-24 12:56:23+00:00
- **Authors**: Maurice Quach, Giuseppe Valenzise, Frederic Dufaux
- **Comment**: Code is available at https://github.com/mauriceqch/pcc_geo_cnn_v2
- **Journal**: None
- **Summary**: Point clouds have been recognized as a crucial data structure for 3D content and are essential in a number of applications such as virtual and mixed reality, autonomous driving, cultural heritage, etc. In this paper, we propose a set of contributions to improve deep point cloud compression, i.e.: using a scale hyperprior model for entropy coding; employing deeper transforms; a different balancing weight in the focal loss; optimal thresholding for decoding; and sequential model training. In addition, we present an extensive ablation study on the impact of each of these factors, in order to provide a better understanding about why they improve RD performance. An optimal combination of the proposed improvements achieves BD-PSNR gains over G-PCC trisoup and octree of 5.50 (6.48) dB and 6.84 (5.95) dB, respectively, when using the point-to-point (point-to-plane) metric. Code is available at https://github.com/mauriceqch/pcc_geo_cnn_v2 .



### Multi-Precision Policy Enforced Training (MuPPET): A precision-switching strategy for quantised fixed-point training of CNNs
- **Arxiv ID**: http://arxiv.org/abs/2006.09049v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.09049v1)
- **Published**: 2020-06-16 10:14:36+00:00
- **Updated**: 2020-06-16 10:14:36+00:00
- **Authors**: Aditya Rajagopal, Diederik Adriaan Vink, Stylianos I. Venieris, Christos-Savvas Bouganis
- **Comment**: Accepted at the 37th International Conference on Machine Learning
  (ICML), 2020
- **Journal**: None
- **Summary**: Large-scale convolutional neural networks (CNNs) suffer from very long training times, spanning from hours to weeks, limiting the productivity and experimentation of deep learning practitioners. As networks grow in size and complexity, training time can be reduced through low-precision data representations and computations. However, in doing so the final accuracy suffers due to the problem of vanishing gradients. Existing state-of-the-art methods combat this issue by means of a mixed-precision approach utilising two different precision levels, FP32 (32-bit floating-point) and FP16/FP8 (16-/8-bit floating-point), leveraging the hardware support of recent GPU architectures for FP16 operations to obtain performance gains. This work pushes the boundary of quantised training by employing a multilevel optimisation approach that utilises multiple precisions including low-precision fixed-point representations. The novel training strategy, MuPPET, combines the use of multiple number representation regimes together with a precision-switching mechanism that decides at run time the transition point between precision regimes. Overall, the proposed strategy tailors the training process to the hardware-level capabilities of the target hardware architecture and yields improvements in training time and energy efficiency compared to state-of-the-art approaches. Applying MuPPET on the training of AlexNet, ResNet18 and GoogLeNet on ImageNet (ILSVRC12) and targeting an NVIDIA Turing GPU, MuPPET achieves the same accuracy as standard full-precision training with training-time speedup of up to 1.84$\times$ and an average speedup of 1.58$\times$ across the networks.



### Momentum Contrastive Learning for Few-Shot COVID-19 Diagnosis from Chest CT Images
- **Arxiv ID**: http://arxiv.org/abs/2006.13276v1
- **DOI**: 10.1016/j.patcog.2021.107826
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.13276v1)
- **Published**: 2020-06-16 10:14:58+00:00
- **Updated**: 2020-06-16 10:14:58+00:00
- **Authors**: Xiaocong Chen, Lina Yao, Tao Zhou, Jinming Dong, Yu Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: The current pandemic, caused by the outbreak of a novel coronavirus (COVID-19) in December 2019, has led to a global emergency that has significantly impacted economies, healthcare systems and personal wellbeing all around the world. Controlling the rapidly evolving disease requires highly sensitive and specific diagnostics. While real-time RT-PCR is the most commonly used, these can take up to 8 hours, and require significant effort from healthcare professionals. As such, there is a critical need for a quick and automatic diagnostic system. Diagnosis from chest CT images is a promising direction. However, current studies are limited by the lack of sufficient training samples, as acquiring annotated CT images is time-consuming. To this end, we propose a new deep learning algorithm for the automated diagnosis of COVID-19, which only requires a few samples for training. Specifically, we use contrastive learning to train an encoder which can capture expressive feature representations on large and publicly available lung datasets and adopt the prototypical network for classification. We validate the efficacy of the proposed model in comparison with other competing methods on two publicly available and annotated COVID-19 CT datasets. Our results demonstrate the superior performance of our model for the accurate diagnosis of COVID-19 based on chest CT images.



### Multi-Objective CNN Based Algorithm for SAR Despeckling
- **Arxiv ID**: http://arxiv.org/abs/2006.09050v4
- **DOI**: 10.1109/TGRS.2020.3034852
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.09050v4)
- **Published**: 2020-06-16 10:15:42+00:00
- **Updated**: 2020-10-30 13:55:51+00:00
- **Authors**: Sergio Vitale, Giampaolo Ferraioli, Vito Pascazio
- **Comment**: None
- **Journal**: IEEE Transactions on Geoscience and Remote Sensing, (2020) 1-14
- **Summary**: Deep learning (DL) in remote sensing has nowadays become an effective operative tool: it is largely used in applications such as change detection, image restoration, segmentation, detection and classification. With reference to synthetic aperture radar (SAR) domain the application of DL techniques is not straightforward due to non trivial interpretation of SAR images, specially caused by the presence of speckle. Several deep learning solutions for SAR despeckling have been proposed in the last few years. Most of these solutions focus on the definition of different network architectures with similar cost functions not involving SAR image properties. In this paper, a convolutional neural network (CNN) with a multi-objective cost function taking care of spatial and statistical properties of the SAR image is proposed. This is achieved by the definition of a peculiar loss function obtained by the weighted combination of three different terms. Each of this term is dedicated mainly to one of the following SAR image characteristics: spatial details, speckle statistical properties and strong scatterers identification. Their combination allows to balance these effects. Moreover, a specifically designed architecture is proposed for effectively extract distinctive features within the considered framework. Experiments on simulated and real SAR images show the accuracy of the proposed method compared to the State-of-Art despeckling algorithms, both from quantitative and qualitative point of view. The importance of considering such SAR properties in the cost function is crucial for a correct noise rejection and details preservation in different underlined scenarios, such as homogeneous, heterogeneous and extremely heterogeneous.



### Mucko: Multi-Layer Cross-Modal Knowledge Reasoning for Fact-based Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/2006.09073v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.09073v3)
- **Published**: 2020-06-16 11:03:37+00:00
- **Updated**: 2020-11-04 01:36:36+00:00
- **Authors**: Zihao Zhu, Jing Yu, Yujing Wang, Yajing Sun, Yue Hu, Qi Wu
- **Comment**: None
- **Journal**: None
- **Summary**: Fact-based Visual Question Answering (FVQA) requires external knowledge beyond visible content to answer questions about an image, which is challenging but indispensable to achieve general VQA. One limitation of existing FVQA solutions is that they jointly embed all kinds of information without fine-grained selection, which introduces unexpected noises for reasoning the final answer. How to capture the question-oriented and information-complementary evidence remains a key challenge to solve the problem. In this paper, we depict an image by a multi-modal heterogeneous graph, which contains multiple layers of information corresponding to the visual, semantic and factual features. On top of the multi-layer graph representations, we propose a modality-aware heterogeneous graph convolutional network to capture evidence from different layers that is most relevant to the given question. Specifically, the intra-modal graph convolution selects evidence from each modality and cross-modal graph convolution aggregates relevant information across different modalities. By stacking this process multiple times, our model performs iterative reasoning and predicts the optimal answer by analyzing all question-oriented evidence. We achieve a new state-of-the-art performance on the FVQA task and demonstrate the effectiveness and interpretability of our model with extensive experiments.



### Progressive Skeletonization: Trimming more fat from a network at initialization
- **Arxiv ID**: http://arxiv.org/abs/2006.09081v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.09081v5)
- **Published**: 2020-06-16 11:32:47+00:00
- **Updated**: 2021-03-19 13:06:16+00:00
- **Authors**: Pau de Jorge, Amartya Sanyal, Harkirat S. Behl, Philip H. S. Torr, Gregory Rogez, Puneet K. Dokania
- **Comment**: None
- **Journal**: None
- **Summary**: Recent studies have shown that skeletonization (pruning parameters) of networks \textit{at initialization} provides all the practical benefits of sparsity both at inference and training time, while only marginally degrading their performance. However, we observe that beyond a certain level of sparsity (approx $95\%$), these approaches fail to preserve the network performance, and to our surprise, in many cases perform even worse than trivial random pruning. To this end, we propose an objective to find a skeletonized network with maximum {\em foresight connection sensitivity} (FORCE) whereby the trainability, in terms of connection sensitivity, of a pruned network is taken into consideration. We then propose two approximate procedures to maximize our objective (1) Iterative SNIP: allows parameters that were unimportant at earlier stages of skeletonization to become important at later stages; and (2) FORCE: iterative process that allows exploration by allowing already pruned parameters to resurrect at later stages of skeletonization. Empirical analyses on a large suite of experiments show that our approach, while providing at least as good a performance as other recent approaches on moderate pruning levels, provides remarkably improved performance on higher pruning levels (could remove up to $99.5\%$ parameters while keeping the networks trainable). Code can be found in https://github.com/naver/force.



### UCSG-Net -- Unsupervised Discovering of Constructive Solid Geometry Tree
- **Arxiv ID**: http://arxiv.org/abs/2006.09102v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.09102v3)
- **Published**: 2020-06-16 12:13:37+00:00
- **Updated**: 2020-10-20 17:32:15+00:00
- **Authors**: Kacper Kania, Maciej Zięba, Tomasz Kajdanowicz
- **Comment**: Accepted to Thirty-fourth Conference on Neural Information Processing
  Systems (NeurIPS 2020). Project page: https://kacperkan.github.io/ucsgnet.
  Project video: https://www.youtube.com/watch?v=s1p4UHtUG3g&feature=emb_title.
  Comments: 13 pages, 7 figures; apply reviewers' remarks, fix the reference to
  the CSG-Net work
- **Journal**: None
- **Summary**: Signed distance field (SDF) is a prominent implicit representation of 3D meshes. Methods that are based on such representation achieved state-of-the-art 3D shape reconstruction quality. However, these methods struggle to reconstruct non-convex shapes. One remedy is to incorporate a constructive solid geometry framework (CSG) that represents a shape as a decomposition into primitives. It allows to embody a 3D shape of high complexity and non-convexity with a simple tree representation of Boolean operations. Nevertheless, existing approaches are supervised and require the entire CSG parse tree that is given upfront during the training process. On the contrary, we propose a model that extracts a CSG parse tree without any supervision - UCSG-Net. Our model predicts parameters of primitives and binarizes their SDF representation through differentiable indicator function. It is achieved jointly with discovering the structure of a Boolean operators tree. The model selects dynamically which operator combination over primitives leads to the reconstruction of high fidelity. We evaluate our method on 2D and 3D autoencoding tasks. We show that the predicted parse tree representation is interpretable and can be used in CAD software.



### Learning from Demonstration with Weakly Supervised Disentanglement
- **Arxiv ID**: http://arxiv.org/abs/2006.09107v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.09107v2)
- **Published**: 2020-06-16 12:29:51+00:00
- **Updated**: 2021-03-26 12:15:52+00:00
- **Authors**: Yordan Hristov, Subramanian Ramamoorthy
- **Comment**: 18 pages, 16 figures, accepted at the International Conference on
  Learning Representations (ICLR) 2021, supplementary website at
  https://sites.google.com/view/weak-label-lfd
- **Journal**: None
- **Summary**: Robotic manipulation tasks, such as wiping with a soft sponge, require control from multiple rich sensory modalities. Human-robot interaction, aimed at teaching robots, is difficult in this setting as there is potential for mismatch between human and machine comprehension of the rich data streams. We treat the task of interpretable learning from demonstration as an optimisation problem over a probabilistic generative model. To account for the high-dimensionality of the data, a high-capacity neural network is chosen to represent the model. The latent variables in this model are explicitly aligned with high-level notions and concepts that are manifested in a set of demonstrations. We show that such alignment is best achieved through the use of labels from the end user, in an appropriately restricted vocabulary, in contrast to the conventional approach of the designer picking a prior over the latent variables. Our approach is evaluated in the context of two table-top robot manipulation tasks performed by a PR2 robot -- that of dabbing liquids with a sponge (forcefully pressing a sponge and moving it along a surface) and pouring between different containers. The robot provides visual information, arm joint positions and arm joint efforts. We have made videos of the tasks and data available - see supplementary materials at: https://sites.google.com/view/weak-label-lfd.



### 1st place solution for AVA-Kinetics Crossover in AcitivityNet Challenge 2020
- **Arxiv ID**: http://arxiv.org/abs/2006.09116v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.09116v1)
- **Published**: 2020-06-16 12:52:59+00:00
- **Updated**: 2020-06-16 12:52:59+00:00
- **Authors**: Siyu Chen, Junting Pan, Guanglu Song, Manyuan Zhang, Hao Shao, Ziyi Lin, Jing Shao, Hongsheng Li, Yu Liu
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2006.07976
- **Journal**: None
- **Summary**: This technical report introduces our winning solution to the spatio-temporal action localization track, AVA-Kinetics Crossover, in ActivityNet Challenge 2020. Our entry is mainly based on Actor-Context-Actor Relation Network. We describe technical details for the new AVA-Kinetics dataset, together with some experimental results. Without any bells and whistles, we achieved 39.62 mAP on the test set of AVA-Kinetics, which outperforms other entries by a large margin. Code will be available at: https://github.com/Siyu-C/ACAR-Net.



### End-to-End Real-time Catheter Segmentation with Optical Flow-Guided Warping during Endovascular Intervention
- **Arxiv ID**: http://arxiv.org/abs/2006.09117v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2006.09117v1)
- **Published**: 2020-06-16 12:53:27+00:00
- **Updated**: 2020-06-16 12:53:27+00:00
- **Authors**: Anh Nguyen, Dennis Kundrat, Giulio Dagnino, Wenqiang Chi, Mohamed E. M. K. Abdelaziz, Yao Guo, YingLiang Ma, Trevor M. Y. Kwok, Celia Riga, Guang-Zhong Yang
- **Comment**: ICRA 2020
- **Journal**: None
- **Summary**: Accurate real-time catheter segmentation is an important pre-requisite for robot-assisted endovascular intervention. Most of the existing learning-based methods for catheter segmentation and tracking are only trained on small-scale datasets or synthetic data due to the difficulties of ground-truth annotation. Furthermore, the temporal continuity in intraoperative imaging sequences is not fully utilised. In this paper, we present FW-Net, an end-to-end and real-time deep learning framework for endovascular intervention. The proposed FW-Net has three modules: a segmentation network with encoder-decoder architecture, a flow network to extract optical flow information, and a novel flow-guided warping function to learn the frame-to-frame temporal continuity. We show that by effectively learning temporal continuity, the network can successfully segment and track the catheters in real-time sequences using only raw ground-truth for training. Detailed validation results confirm that our FW-Net outperforms state-of-the-art techniques while achieving real-time performance.



### Rethinking the Role of Gradient-Based Attribution Methods for Model Interpretability
- **Arxiv ID**: http://arxiv.org/abs/2006.09128v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.09128v2)
- **Published**: 2020-06-16 13:17:32+00:00
- **Updated**: 2021-03-03 09:42:58+00:00
- **Authors**: Suraj Srinivas, Francois Fleuret
- **Comment**: Oral Presentation at ICLR 2021
- **Journal**: None
- **Summary**: Current methods for the interpretability of discriminative deep neural networks commonly rely on the model's input-gradients, i.e., the gradients of the output logits w.r.t. the inputs. The common assumption is that these input-gradients contain information regarding $p_{\theta} ( y \mid x)$, the model's discriminative capabilities, thus justifying their use for interpretability. However, in this work we show that these input-gradients can be arbitrarily manipulated as a consequence of the shift-invariance of softmax without changing the discriminative function. This leaves an open question: if input-gradients can be arbitrary, why are they highly structured and explanatory in standard models?   We investigate this by re-interpreting the logits of standard softmax-based classifiers as unnormalized log-densities of the data distribution and show that input-gradients can be viewed as gradients of a class-conditional density model $p_{\theta}(x \mid y)$ implicit within the discriminative model. This leads us to hypothesize that the highly structured and explanatory nature of input-gradients may be due to the alignment of this class-conditional model $p_{\theta}(x \mid y)$ with that of the ground truth data distribution $p_{\text{data}} (x \mid y)$. We test this hypothesis by studying the effect of density alignment on gradient explanations. To achieve this alignment we use score-matching, and propose novel approximations to this algorithm to enable training large-scale models.   Our experiments show that improving the alignment of the implicit density model with the data distribution enhances gradient structure and explanatory power while reducing this alignment has the opposite effect. Overall, our finding that input-gradients capture information regarding an implicit generative model implies that we need to re-think their use for interpreting discriminative models.



### AlphaGAN: Fully Differentiable Architecture Search for Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2006.09134v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.09134v3)
- **Published**: 2020-06-16 13:27:30+00:00
- **Updated**: 2021-08-07 07:53:29+00:00
- **Authors**: Yuesong Tian, Li Shen, Li Shen, Guinan Su, Zhifeng Li, Wei Liu
- **Comment**: In IEEE Transactions on Pattern Analysis and Machine Intelligence
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) are formulated as minimax game problems, whereby generators attempt to approach real data distributions by virtue of adversarial learning against discriminators. The intrinsic problem complexity poses the challenge to enhance the performance of generative networks. In this work, we aim to boost model learning from the perspective of network architectures, by incorporating recent progress on automated architecture search into GANs. To this end, we propose a fully differentiable search framework for generative adversarial networks, dubbed alphaGAN. The searching process is formalized as solving a bi-level minimax optimization problem, in which the outer-level objective aims for seeking a suitable network architecture towards pure Nash Equilibrium conditioned on the generator and the discriminator network parameters optimized with a traditional GAN loss in the inner level. The entire optimization performs a first-order method by alternately minimizing the two-level objective in a fully differentiable manner, enabling architecture search to be completed in an enormous search space. Extensive experiments on CIFAR-10 and STL-10 datasets show that our algorithm can obtain high-performing architectures only with 3-GPU hours on a single GPU in the search space comprised of approximate 2 ? 1011 possible configurations. We also provide a comprehensive analysis on the behavior of the searching process and the properties of searched architectures, which would benefit further research on architectures for generative models. Pretrained models and codes are available at https://github.com/yuesongtian/AlphaGAN.



### Improving accuracy and speeding up Document Image Classification through parallel systems
- **Arxiv ID**: http://arxiv.org/abs/2006.09141v1
- **DOI**: 10.1007/978-3-030-50417-5_29
- **Categories**: **cs.CV**, cs.DC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.09141v1)
- **Published**: 2020-06-16 13:36:07+00:00
- **Updated**: 2020-06-16 13:36:07+00:00
- **Authors**: Javier Ferrando, Juan Luis Dominguez, Jordi Torres, Raul Garcia, David Garcia, Daniel Garrido, Jordi Cortada, Mateo Valero
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a study showing the benefits of the EfficientNet models compared with heavier Convolutional Neural Networks (CNNs) in the Document Classification task, essential problem in the digitalization process of institutions. We show in the RVL-CDIP dataset that we can improve previous results with a much lighter model and present its transfer learning capabilities on a smaller in-domain dataset such as Tobacco3482. Moreover, we present an ensemble pipeline which is able to boost solely image input by combining image model predictions with the ones generated by BERT model on extracted text by OCR. We also show that the batch size can be effectively increased without hindering its accuracy so that the training process can be sped up by parallelizing throughout multiple GPUs, decreasing the computational time needed. Lastly, we expose the training performance differences between PyTorch and Tensorflow Deep Learning frameworks.



### Cogradient Descent for Bilinear Optimization
- **Arxiv ID**: http://arxiv.org/abs/2006.09142v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.09142v1)
- **Published**: 2020-06-16 13:41:54+00:00
- **Updated**: 2020-06-16 13:41:54+00:00
- **Authors**: Li'an Zhuo, Baochang Zhang, Linlin Yang, Hanlin Chen, Qixiang Ye, David Doermann, Guodong Guo, Rongrong Ji
- **Comment**: 9 pages, 6 figures
- **Journal**: None
- **Summary**: Conventional learning methods simplify the bilinear model by regarding two intrinsically coupled factors independently, which degrades the optimization procedure. One reason lies in the insufficient training due to the asynchronous gradient descent, which results in vanishing gradients for the coupled variables. In this paper, we introduce a Cogradient Descent algorithm (CoGD) to address the bilinear problem, based on a theoretical framework to coordinate the gradient of hidden variables via a projection function. We solve one variable by considering its coupling relationship with the other, leading to a synchronous gradient descent to facilitate the optimization procedure. Our algorithm is applied to solve problems with one variable under the sparsity constraint, which is widely used in the learning paradigm. We validate our CoGD considering an extensive set of applications including image reconstruction, inpainting, and network pruning. Experiments show that it improves the state-of-the-art by a significant margin.



### Reconstruction of turbulent data with deep generative models for semantic inpainting from TURB-Rot database
- **Arxiv ID**: http://arxiv.org/abs/2006.09179v2
- **DOI**: 10.1103/PhysRevFluids.6.050503
- **Categories**: **physics.flu-dyn**, cond-mat.stat-mech, cs.CV, cs.LG, nlin.CD
- **Links**: [PDF](http://arxiv.org/pdf/2006.09179v2)
- **Published**: 2020-06-16 14:26:07+00:00
- **Updated**: 2021-06-14 09:26:53+00:00
- **Authors**: M. Buzzicotti, F. Bonaccorso, P. Clark Di Leoni, L. Biferale
- **Comment**: None
- **Journal**: Phys. Rev. Fluids 6, 050503 (2021)
- **Summary**: We study the applicability of tools developed by the computer vision community for features learning and semantic image inpainting to perform data reconstruction of fluid turbulence configurations. The aim is twofold. First, we explore on a quantitative basis, the capability of Convolutional Neural Networks embedded in a Deep Generative Adversarial Model (Deep-GAN) to generate missing data in turbulence, a paradigmatic high dimensional chaotic system. In particular, we investigate their use in reconstructing two-dimensional damaged snapshots extracted from a large database of numerical configurations of 3d turbulence in the presence of rotation, a case with multi-scale random features where both large-scale organised structures and small-scale highly intermittent and non-Gaussian fluctuations are present. Second, following a reverse engineering approach, we aim to rank the input flow properties (features) in terms of their qualitative and quantitative importance to obtain a better set of reconstructed fields. We present two approaches both based on Context Encoders. The first one infers the missing data via a minimization of the L2 pixel-wise reconstruction loss, plus a small adversarial penalisation. The second searches for the closest encoding of the corrupted flow configuration from a previously trained generator. Finally, we present a comparison with a different data assimilation tool, based on Nudging, an equation-informed unbiased protocol, well known in the numerical weather prediction community. The TURB-Rot database, http://smart-turb.roma2.infn.it, of roughly 300K 2d turbulent images is released and details on how to download it are given.



### AVLnet: Learning Audio-Visual Language Representations from Instructional Videos
- **Arxiv ID**: http://arxiv.org/abs/2006.09199v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.MM, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2006.09199v2)
- **Published**: 2020-06-16 14:38:03+00:00
- **Updated**: 2021-06-29 18:44:50+00:00
- **Authors**: Andrew Rouditchenko, Angie Boggust, David Harwath, Brian Chen, Dhiraj Joshi, Samuel Thomas, Kartik Audhkhasi, Hilde Kuehne, Rameswar Panda, Rogerio Feris, Brian Kingsbury, Michael Picheny, Antonio Torralba, James Glass
- **Comment**: A version of this work has been accepted to Interspeech 2021
- **Journal**: None
- **Summary**: Current methods for learning visually grounded language from videos often rely on text annotation, such as human generated captions or machine generated automatic speech recognition (ASR) transcripts. In this work, we introduce the Audio-Video Language Network (AVLnet), a self-supervised network that learns a shared audio-visual embedding space directly from raw video inputs. To circumvent the need for text annotation, we learn audio-visual representations from randomly segmented video clips and their raw audio waveforms. We train AVLnet on HowTo100M, a large corpus of publicly available instructional videos, and evaluate on image retrieval and video retrieval tasks, achieving state-of-the-art performance. We perform analysis of AVLnet's learned representations, showing our model utilizes speech and natural sounds to learn audio-visual concepts. Further, we propose a tri-modal model that jointly processes raw audio, video, and text captions from videos to learn a multi-modal semantic embedding space useful for text-video retrieval. Our code, data, and trained models will be released at avlnet.csail.mit.edu



### Visual Identification of Individual Holstein-Friesian Cattle via Deep Metric Learning
- **Arxiv ID**: http://arxiv.org/abs/2006.09205v3
- **DOI**: 10.1016/j.compag.2021.106133
- **Categories**: **cs.CV**, I.4.8; I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/2006.09205v3)
- **Published**: 2020-06-16 14:41:55+00:00
- **Updated**: 2020-10-14 10:58:30+00:00
- **Authors**: William Andrew, Jing Gao, Siobhan Mullan, Neill Campbell, Andrew W Dowsey, Tilo Burghardt
- **Comment**: 41 pages, 18 figures, 2 tables; Submitted to Computers and
  Electronics in Agriculture ; Source code and network weights available at
  https://github.com/CWOA/MetricLearningIdentification ; OpenCows2020 dataset
  available at https://doi.org/10.5523/bris.10m32xl88x2b61zlkkgz3fml17
- **Journal**: Computers and Electronics in Agriculture 185, 106133 (2021)
- **Summary**: Holstein-Friesian cattle exhibit individually-characteristic black and white coat patterns visually akin to those arising from Turing's reaction-diffusion systems. This work takes advantage of these natural markings in order to automate visual detection and biometric identification of individual Holstein-Friesians via convolutional neural networks and deep metric learning techniques. Existing approaches rely on markings, tags or wearables with a variety of maintenance requirements, whereas we present a totally hands-off method for the automated detection, localisation, and identification of individual animals from overhead imaging in an open herd setting, i.e. where new additions to the herd are identified without re-training. We propose the use of SoftMax-based reciprocal triplet loss to address the identification problem and evaluate the techniques in detail against fixed herd paradigms. We find that deep metric learning systems show strong performance even when many cattle unseen during system training are to be identified and re-identified -- achieving 93.8% accuracy when trained on just half of the population. This work paves the way for facilitating the non-intrusive monitoring of cattle applicable to precision farming and surveillance for automated productivity, health and welfare monitoring, and to veterinary research such as behavioural analysis, disease outbreak tracing, and more. Key parts of the source code, network weights and datasets are available publicly.



### MS-TCN++: Multi-Stage Temporal Convolutional Network for Action Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2006.09220v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.09220v2)
- **Published**: 2020-06-16 14:50:47+00:00
- **Updated**: 2020-09-02 10:18:47+00:00
- **Authors**: Shijie Li, Yazan Abu Farha, Yun Liu, Ming-Ming Cheng, Juergen Gall
- **Comment**: IEEE Transactions on Pattern Analysis and Machine Intelligence. arXiv
  admin note: substantial text overlap with arXiv:1903.01945
- **Journal**: None
- **Summary**: With the success of deep learning in classifying short trimmed videos, more attention has been focused on temporally segmenting and classifying activities in long untrimmed videos. State-of-the-art approaches for action segmentation utilize several layers of temporal convolution and temporal pooling. Despite the capabilities of these approaches in capturing temporal dependencies, their predictions suffer from over-segmentation errors. In this paper, we propose a multi-stage architecture for the temporal action segmentation task that overcomes the limitations of the previous approaches. The first stage generates an initial prediction that is refined by the next ones. In each stage we stack several layers of dilated temporal convolutions covering a large receptive field with few parameters. While this architecture already performs well, lower layers still suffer from a small receptive field. To address this limitation, we propose a dual dilated layer that combines both large and small receptive fields. We further decouple the design of the first stage from the refining stages to address the different requirements of these stages. Extensive evaluation shows the effectiveness of the proposed model in capturing long-range dependencies and recognizing action segments. Our models achieve state-of-the-art results on three datasets: 50Salads, Georgia Tech Egocentric Activities (GTEA), and the Breakfast dataset.



### DSDANet: Deep Siamese Domain Adaptation Convolutional Neural Network for Cross-domain Change Detection
- **Arxiv ID**: http://arxiv.org/abs/2006.09225v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.09225v1)
- **Published**: 2020-06-16 15:00:54+00:00
- **Updated**: 2020-06-16 15:00:54+00:00
- **Authors**: Hongruixuan Chen, Chen Wu, Bo Du, Liangpei Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Change detection (CD) is one of the most vital applications in remote sensing. Recently, deep learning has achieved promising performance in the CD task. However, the deep models are task-specific and CD data set bias often exists, hence it is inevitable that deep CD models would suffer degraded performance after transferring it from original CD data set to new ones, making manually label numerous samples in the new data set unavoidable, which costs a large amount of time and human labor. How to learn a transferable CD model in the data set with enough labeled data (original domain) but can well detect changes in another data set without labeled data (target domain)? This is defined as the cross-domain change detection problem. In this paper, we propose a novel deep siamese domain adaptation convolutional neural network (DSDANet) architecture for cross-domain CD. In DSDANet, a siamese convolutional neural network first extracts spatial-spectral features from multi-temporal images. Then, through multi-kernel maximum mean discrepancy (MK-MMD), the learned feature representation is embedded into a reproducing kernel Hilbert space (RKHS), in which the distribution of two domains can be explicitly matched. By optimizing the network parameters and kernel coefficients with the source labeled data and target unlabeled data, DSDANet can learn transferrable feature representation that can bridge the discrepancy between two domains. To the best of our knowledge, it is the first time that such a domain adaptation-based deep network is proposed for CD. The theoretical analysis and experimental results demonstrate the effectiveness and potential of the proposed method.



### Foreground-Background Imbalance Problem in Deep Object Detectors: A Review
- **Arxiv ID**: http://arxiv.org/abs/2006.09238v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.09238v1)
- **Published**: 2020-06-16 15:15:53+00:00
- **Updated**: 2020-06-16 15:15:53+00:00
- **Authors**: Joya Chen, Qi Wu, Dong Liu, Tong Xu
- **Comment**: Accepted by IEEE MIPR 2020
- **Journal**: None
- **Summary**: Recent years have witnessed the remarkable developments made by deep learning techniques for object detection, a fundamentally challenging problem of computer vision. Nevertheless, there are still difficulties in training accurate deep object detectors, one of which is owing to the foreground-background imbalance problem. In this paper, we survey the recent advances about the solutions to the imbalance problem. First, we analyze the characteristics of the imbalance problem in different kinds of deep detectors, including one-stage and two-stage ones. Second, we divide the existing solutions into two categories: sampling heuristics and non-sampling schemes, and review them in detail. Third, we experimentally compare the performance of some state-of-the-art solutions on the COCO benchmark. Promising directions for future work are also discussed.



### Two-Dimensional Non-Line-of-Sight Scene Estimation from a Single Edge Occluder
- **Arxiv ID**: http://arxiv.org/abs/2006.09241v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.09241v1)
- **Published**: 2020-06-16 15:19:20+00:00
- **Updated**: 2020-06-16 15:19:20+00:00
- **Authors**: Sheila W. Seidel, John Murray-Bruce, Yanting Ma, Christopher Yu, William T. Freeman, Vivek K Goyal
- **Comment**: 14 pages, 15 figures
- **Journal**: None
- **Summary**: Passive non-line-of-sight imaging methods are often faster and stealthier than their active counterparts, requiring less complex and costly equipment. However, many of these methods exploit motion of an occluder or the hidden scene, or require knowledge or calibration of complicated occluders. The edge of a wall is a known and ubiquitous occluding structure that may be used as an aperture to image the region hidden behind it. Light from around the corner is cast onto the floor forming a fan-like penumbra rather than a sharp shadow. Subtle variations in the penumbra contain a remarkable amount of information about the hidden scene. Previous work has leveraged the vertical nature of the edge to demonstrate 1D (in angle measured around the corner) reconstructions of moving and stationary hidden scenery from as little as a single photograph of the penumbra. In this work, we introduce a second reconstruction dimension: range measured from the edge. We derive a new forward model, accounting for radial falloff, and propose two inversion algorithms to form 2D reconstructions from a single photograph of the penumbra. Performances of both algorithms are demonstrated on experimental data corresponding to several different hidden scene configurations. A Cramer-Rao bound analysis further demonstrates the feasibility (and utility) of the 2D corner camera.



### AcED: Accurate and Edge-consistent Monocular Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/2006.09243v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2006.09243v1)
- **Published**: 2020-06-16 15:21:00+00:00
- **Updated**: 2020-06-16 15:21:00+00:00
- **Authors**: Kunal Swami, Prasanna Vishnu Bondada, Pankaj Kumar Bajpai
- **Comment**: Accepted in IEEE ICIP 2020
- **Journal**: None
- **Summary**: Single image depth estimation is a challenging problem. The current state-of-the-art method formulates the problem as that of ordinal regression. However, the formulation is not fully differentiable and depth maps are not generated in an end-to-end fashion. The method uses a na\"ive threshold strategy to determine per-pixel depth labels, which results in significant discretization errors. For the first time, we formulate a fully differentiable ordinal regression and train the network in end-to-end fashion. This enables us to include boundary and smoothness constraints in the optimization function, leading to smooth and edge-consistent depth maps. A novel per-pixel confidence map computation for depth refinement is also proposed. Extensive evaluation of the proposed model on challenging benchmarks reveals its superiority over recent state-of-the-art methods, both quantitatively and qualitatively. Additionally, we demonstrate practical utility of the proposed method for single camera bokeh solution using in-house dataset of challenging real-life images.



### Structured and Localized Image Restoration
- **Arxiv ID**: http://arxiv.org/abs/2006.09261v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.09261v1)
- **Published**: 2020-06-16 15:43:12+00:00
- **Updated**: 2020-06-16 15:43:12+00:00
- **Authors**: Thomas Eboli, Alex Nowak-Vila, Jian Sun, Francis Bach, Jean Ponce, Alessandro Rudi
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel approach to image restoration that leverages ideas from localized structured prediction and non-linear multi-task learning. We optimize a penalized energy function regularized by a sum of terms measuring the distance between patches to be restored and clean patches from an external database gathered beforehand. The resulting estimator comes with strong statistical guarantees leveraging local dependency properties of overlapping patches. We derive the corresponding algorithms for energies based on the mean-squared and Euclidean norm errors. Finally, we demonstrate the practical effectiveness of our model on different image restoration problems using standard benchmarks.



### How Secure is Distributed Convolutional Neural Network on IoT Edge Devices?
- **Arxiv ID**: http://arxiv.org/abs/2006.09276v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/2006.09276v1)
- **Published**: 2020-06-16 16:10:09+00:00
- **Updated**: 2020-06-16 16:10:09+00:00
- **Authors**: Hawzhin Mohammed, Tolulope A. Odetola, Syed Rafay Hasan
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNN) has found successful adoption in many applications. The deployment of CNN on resource-constrained edge devices have proved challenging. CNN distributed deployment across different edge devices has been adopted. In this paper, we propose Trojan attacks on CNN deployed across a distributed edge network across different nodes. We propose five stealthy attack scenarios for distributed CNN inference. These attacks are divided into trigger and payload circuitry. These attacks are tested on deep learning models (LeNet, AlexNet). The results show how the degree of vulnerability of individual layers and how critical they are to the final classification.



### 70 years of machine learning in geoscience in review
- **Arxiv ID**: http://arxiv.org/abs/2006.13311v3
- **DOI**: 10.1016/bs.agph.2020.08.002
- **Categories**: **physics.geo-ph**, cs.CV, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.13311v3)
- **Published**: 2020-06-16 16:32:27+00:00
- **Updated**: 2020-08-26 12:34:24+00:00
- **Authors**: Jesper Sören Dramsch
- **Comment**: 36 pages, 17 figures, book chapter
- **Journal**: None
- **Summary**: This review gives an overview of the development of machine learning in geoscience. A thorough analysis of the co-developments of machine learning applications throughout the last 70 years relates the recent enthusiasm for machine learning to developments in geoscience. I explore the shift of kriging towards a mainstream machine learning method and the historic application of neural networks in geoscience, following the general trend of machine learning enthusiasm through the decades. Furthermore, this chapter explores the shift from mathematical fundamentals and knowledge in software development towards skills in model validation, applied statistics, and integrated subject matter expertise. The review is interspersed with code examples to complement the theoretical foundations and illustrate model validation and machine learning explainability for science. The scope of this review includes various shallow machine learning methods, e.g. Decision Trees, Random Forests, Support-Vector Machines, and Gaussian Processes, as well as, deep neural networks, including feed-forward neural networks, convolutional neural networks, recurrent neural networks and generative adversarial networks. Regarding geoscience, the review has a bias towards geophysics but aims to strike a balance with geochemistry, geostatistics, and geology, however excludes remote sensing, as this would exceed the scope. In general, I aim to provide context for the recent enthusiasm surrounding deep learning with respect to research, hardware, and software developments that enable successful application of shallow and deep machine learning in all disciplines of Earth science.



### The shape and simplicity biases of adversarially robust ImageNet-trained CNNs
- **Arxiv ID**: http://arxiv.org/abs/2006.09373v6
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.09373v6)
- **Published**: 2020-06-16 16:38:16+00:00
- **Updated**: 2022-09-12 13:13:07+00:00
- **Authors**: Peijie Chen, Chirag Agarwal, Anh Nguyen
- **Comment**: None
- **Journal**: None
- **Summary**: Increasingly more similarities between human vision and convolutional neural networks (CNNs) have been revealed in the past few years. Yet, vanilla CNNs often fall short in generalizing to adversarial or out-of-distribution (OOD) examples which humans demonstrate superior performance. Adversarial training is a leading learning algorithm for improving the robustness of CNNs on adversarial and OOD data; however, little is known about the properties, specifically the shape bias and internal features learned inside adversarially-robust CNNs. In this paper, we perform a thorough, systematic study to understand the shape bias and some internal mechanisms that enable the generalizability of AlexNet, GoogLeNet, and ResNet-50 models trained via adversarial training. We find that while standard ImageNet classifiers have a strong texture bias, their R counterparts rely heavily on shapes. Remarkably, adversarial training induces three simplicity biases into hidden neurons in the process of "robustifying" CNNs. That is, each convolutional neuron in R networks often changes to detecting (1) pixel-wise smoother patterns, i.e., a mechanism that blocks high-frequency noise from passing through the network; (2) more lower-level features i.e. textures and colors (instead of objects);and (3) fewer types of inputs. Our findings reveal the interesting mechanisms that made networks more adversarially robust and also explain some recent findings e.g., why R networks benefit from a much larger capacity (Xie et al. 2020) and can act as a strong image prior in image synthesis (Santurkar et al. 2019).



### A New Run-based Connected Component Labeling for Efficiently Analyzing and Processing Holes
- **Arxiv ID**: http://arxiv.org/abs/2006.09299v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.09299v1)
- **Published**: 2020-06-16 16:40:00+00:00
- **Updated**: 2020-06-16 16:40:00+00:00
- **Authors**: Florian Lemaitre, Lionel Lacassagne
- **Comment**: 5 pages
- **Journal**: None
- **Summary**: This article introduces a new connected component labeling and analysis algorithm for foreground and background labeling that computes the adjacency tree. The computation of features (bounding boxes, first statistical moments, Euler number) is done on-the-fly. The transitive closure enables an efficient hole processing that can be filled while their features are merged with the surrounding connected component without the need to rescan the image. A comparison with existing algorithms shows that this new algorithm can do all these computations faster than algorithms processing black and white components.



### Unsupervised Pansharpening Based on Self-Attention Mechanism
- **Arxiv ID**: http://arxiv.org/abs/2006.09303v3
- **DOI**: 10.1109/TGRS.2020.3009207
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.09303v3)
- **Published**: 2020-06-16 16:46:30+00:00
- **Updated**: 2020-08-30 11:48:18+00:00
- **Authors**: Ying Qu, Razieh Kaviani Baghbaderani, Hairong Qi, Chiman Kwan
- **Comment**: Accepted by TGRS
- **Journal**: None
- **Summary**: Pansharpening is to fuse a multispectral image (MSI) of low-spatial-resolution (LR) but rich spectral characteristics with a panchromatic image (PAN) of high-spatial-resolution (HR) but poor spectral characteristics. Traditional methods usually inject the extracted high-frequency details from PAN into the up-sampled MSI. Recent deep learning endeavors are mostly supervised assuming the HR MSI is available, which is unrealistic especially for satellite images. Nonetheless, these methods could not fully exploit the rich spectral characteristics in the MSI. Due to the wide existence of mixed pixels in satellite images where each pixel tends to cover more than one constituent material, pansharpening at the subpixel level becomes essential. In this paper, we propose an unsupervised pansharpening (UP) method in a deep-learning framework to address the above challenges based on the self-attention mechanism (SAM), referred to as UP-SAM. The contribution of this paper is three-fold. First, the self-attention mechanism is proposed where the spatial varying detail extraction and injection functions are estimated according to the attention representations indicating spectral characteristics of the MSI with sub-pixel accuracy. Second, such attention representations are derived from mixed pixels with the proposed stacked attention network powered with a stick-breaking structure to meet the physical constraints of mixed pixel formulations. Third, the detail extraction and injection functions are spatial varying based on the attention representations, which largely improves the reconstruction accuracy. Extensive experimental results demonstrate that the proposed approach is able to reconstruct sharper MSI of different types, with more details and less spectral distortion as compared to the state-of-the-art.



### Learning About Objects by Learning to Interact with Them
- **Arxiv ID**: http://arxiv.org/abs/2006.09306v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.09306v2)
- **Published**: 2020-06-16 16:47:50+00:00
- **Updated**: 2020-10-23 23:46:17+00:00
- **Authors**: Martin Lohmann, Jordi Salvador, Aniruddha Kembhavi, Roozbeh Mottaghi
- **Comment**: NeurIPS 2020
- **Journal**: None
- **Summary**: Much of the remarkable progress in computer vision has been focused around fully supervised learning mechanisms relying on highly curated datasets for a variety of tasks. In contrast, humans often learn about their world with little to no external supervision. Taking inspiration from infants learning from their environment through play and interaction, we present a computational framework to discover objects and learn their physical properties along this paradigm of Learning from Interaction. Our agent, when placed within the near photo-realistic and physics-enabled AI2-THOR environment, interacts with its world and learns about objects, their geometric extents and relative masses, without any external guidance. Our experiments reveal that this agent learns efficiently and effectively; not just for objects it has interacted with before, but also for novel instances from seen categories as well as novel object categories.



### Lung Segmentation and Nodule Detection in Computed Tomography Scan using a Convolutional Neural Network Trained Adversarially using Turing Test Loss
- **Arxiv ID**: http://arxiv.org/abs/2006.09308v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.09308v1)
- **Published**: 2020-06-16 16:51:53+00:00
- **Updated**: 2020-06-16 16:51:53+00:00
- **Authors**: Rakshith Sathish, Rachana Sathish, Ramanathan Sethuraman, Debdoot Sheet
- **Comment**: Accepted at 42nd Annual International Conferences of the IEEE
  Engineering in Medicine and Biology Society (2020)
- **Journal**: None
- **Summary**: Lung cancer is the most common form of cancer found worldwide with a high mortality rate. Early detection of pulmonary nodules by screening with a low-dose computed tomography (CT) scan is crucial for its effective clinical management. Nodules which are symptomatic of malignancy occupy about 0.0125 - 0.025\% of volume in a CT scan of a patient. Manual screening of all slices is a tedious task and presents a high risk of human errors. To tackle this problem we propose a computationally efficient two stage framework. In the first stage, a convolutional neural network (CNN) trained adversarially using Turing test loss segments the lung region. In the second stage, patches sampled from the segmented region are then classified to detect the presence of nodules. The proposed method is experimentally validated on the LUNA16 challenge dataset with a dice coefficient of $0.984\pm0.0007$ for 10-fold cross-validation.



### Deep Multimodal Transfer-Learned Regression in Data-Poor Domains
- **Arxiv ID**: http://arxiv.org/abs/2006.09310v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.09310v1)
- **Published**: 2020-06-16 16:52:44+00:00
- **Updated**: 2020-06-16 16:52:44+00:00
- **Authors**: Levi McClenny, Mulugeta Haile, Vahid Attari, Brian Sadler, Ulisses Braga-Neto, Raymundo Arroyave
- **Comment**: None
- **Journal**: None
- **Summary**: In many real-world applications of deep learning, estimation of a target may rely on various types of input data modes, such as audio-video, image-text, etc. This task can be further complicated by a lack of sufficient data. Here we propose a Deep Multimodal Transfer-Learned Regressor (DMTL-R) for multimodal learning of image and feature data in a deep regression architecture effective at predicting target parameters in data-poor domains. Our model is capable of fine-tuning a given set of pre-trained CNN weights on a small amount of training image data, while simultaneously conditioning on feature information from a complimentary data mode during network training, yielding more accurate single-target or multi-target regression than can be achieved using the images or the features alone. We present results using phase-field simulation microstructure images with an accompanying set of physical features, using pre-trained weights from various well-known CNN architectures, which demonstrate the efficacy of the proposed multimodal approach.



### Domain Adaptation with Morphologic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2006.09322v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, cs.LG, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2006.09322v1)
- **Published**: 2020-06-16 17:06:02+00:00
- **Updated**: 2020-06-16 17:06:02+00:00
- **Authors**: Jonathan Klein, Sören Pirk, Dominik L. Michels
- **Comment**: This work has been supported by KAUST under individual baseline
  funding
- **Journal**: None
- **Summary**: We present a novel domain adaptation framework that uses morphologic segmentation to translate images from arbitrary input domains (real and synthetic) into a uniform output domain. Our framework is based on an established image-to-image translation pipeline that allows us to first transform the input image into a generalized representation that encodes morphology and semantics - the edge-plus-segmentation map (EPS) - which is then transformed into an output domain. Images transformed into the output domain are photo-realistic and free of artifacts that are commonly present across different real (e.g. lens flare, motion blur, etc.) and synthetic (e.g. unrealistic textures, simplified geometry, etc.) data sets. Our goal is to establish a preprocessing step that unifies data from multiple sources into a common representation that facilitates training downstream tasks in computer vision. This way, neural networks for existing tasks can be trained on a larger variety of training data, while they are also less affected by overfitting to specific data sets. We showcase the effectiveness of our approach by qualitatively and quantitatively evaluating our method on four data sets of simulated and real data of urban scenes. Additional results can be found on the project website available at http://jonathank.de/research/eps/ .



### What's in the Image? Explorable Decoding of Compressed Images
- **Arxiv ID**: http://arxiv.org/abs/2006.09332v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.09332v2)
- **Published**: 2020-06-16 17:15:44+00:00
- **Updated**: 2021-03-27 09:01:59+00:00
- **Authors**: Yuval Bahat, Tomer Michaeli
- **Comment**: None
- **Journal**: None
- **Summary**: The ever-growing amounts of visual contents captured on a daily basis necessitate the use of lossy compression methods in order to save storage space and transmission bandwidth. While extensive research efforts are devoted to improving compression techniques, every method inevitably discards information. Especially at low bit rates, this information often corresponds to semantically meaningful visual cues, so that decompression involves significant ambiguity. In spite of this fact, existing decompression algorithms typically produce only a single output, and do not allow the viewer to explore the set of images that map to the given compressed code. In this work we propose the first image decompression method to facilitate user-exploration of the diverse set of natural images that could have given rise to the compressed input code, thus granting users the ability to determine what could and what could not have been there in the original scene. Specifically, we develop a novel deep-network based decoder architecture for the ubiquitous JPEG standard, which allows traversing the set of decompressed images that are consistent with the compressed JPEG file. To allow for simple user interaction, we develop a graphical user interface comprising several intuitive exploration tools, including an automatic tool for examining specific solutions of interest. We exemplify our framework on graphical, medical and forensic use cases, demonstrating its wide range of potential applications.



### LiDARsim: Realistic LiDAR Simulation by Leveraging the Real World
- **Arxiv ID**: http://arxiv.org/abs/2006.09348v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.09348v1)
- **Published**: 2020-06-16 17:44:35+00:00
- **Updated**: 2020-06-16 17:44:35+00:00
- **Authors**: Sivabalan Manivasagam, Shenlong Wang, Kelvin Wong, Wenyuan Zeng, Mikita Sazanovich, Shuhan Tan, Bin Yang, Wei-Chiu Ma, Raquel Urtasun
- **Comment**: CVPR 2020 (Oral)
- **Journal**: None
- **Summary**: We tackle the problem of producing realistic simulations of LiDAR point clouds, the sensor of preference for most self-driving vehicles. We argue that, by leveraging real data, we can simulate the complex world more realistically compared to employing virtual worlds built from CAD/procedural models. Towards this goal, we first build a large catalog of 3D static maps and 3D dynamic objects by driving around several cities with our self-driving fleet. We can then generate scenarios by selecting a scene from our catalog and "virtually" placing the self-driving vehicle (SDV) and a set of dynamic objects from the catalog in plausible locations in the scene. To produce realistic simulations, we develop a novel simulator that captures both the power of physics-based and learning-based simulation. We first utilize ray casting over the 3D scene and then use a deep neural network to produce deviations from the physics-based simulation, producing realistic LiDAR point clouds. We showcase LiDARsim's usefulness for perception algorithms-testing on long-tail events and end-to-end closed-loop evaluation on safety-critical scenarios.



### Building One-Shot Semi-supervised (BOSS) Learning up to Fully Supervised Performance
- **Arxiv ID**: http://arxiv.org/abs/2006.09363v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.09363v2)
- **Published**: 2020-06-16 17:56:00+00:00
- **Updated**: 2021-01-26 18:31:29+00:00
- **Authors**: Leslie N. Smith, Adam Conovaloff
- **Comment**: Revised version
- **Journal**: None
- **Summary**: Reaching the performance of fully supervised learning with unlabeled data and only labeling one sample per class might be ideal for deep learning applications. We demonstrate for the first time the potential for building one-shot semi-supervised (BOSS) learning on Cifar-10 and SVHN up to attain test accuracies that are comparable to fully supervised learning. Our method combines class prototype refining, class balancing, and self-training. A good prototype choice is essential and we propose a technique for obtaining iconic examples. In addition, we demonstrate that class balancing methods substantially improve accuracy results in semi-supervised learning to levels that allow self-training to reach the level of fully supervised learning performance. Rigorous empirical evaluations provide evidence that labeling large datasets is not necessary for training deep neural networks. We made our code available at https://github.com/lnsmith54/BOSS to facilitate replication and for use with future real-world applications.



### Semantic Curiosity for Active Visual Learning
- **Arxiv ID**: http://arxiv.org/abs/2006.09367v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.09367v1)
- **Published**: 2020-06-16 17:59:24+00:00
- **Updated**: 2020-06-16 17:59:24+00:00
- **Authors**: Devendra Singh Chaplot, Helen Jiang, Saurabh Gupta, Abhinav Gupta
- **Comment**: See project webpage at
  https://devendrachaplot.github.io/projects/SemanticCuriosity
- **Journal**: None
- **Summary**: In this paper, we study the task of embodied interactive learning for object detection. Given a set of environments (and some labeling budget), our goal is to learn an object detector by having an agent select what data to obtain labels for. How should an exploration policy decide which trajectory should be labeled? One possibility is to use a trained object detector's failure cases as an external reward. However, this will require labeling millions of frames required for training RL policies, which is infeasible. Instead, we explore a self-supervised approach for training our exploration policy by introducing a notion of semantic curiosity. Our semantic curiosity policy is based on a simple observation -- the detection outputs should be consistent. Therefore, our semantic curiosity rewards trajectories with inconsistent labeling behavior and encourages the exploration policy to explore such areas. The exploration policy trained via semantic curiosity generalizes to novel scenes and helps train an object detector that outperforms baselines trained with other possible alternatives such as random exploration, prediction-error curiosity, and coverage-maximizing exploration.



### Noise2Inpaint: Learning Referenceless Denoising by Inpainting Unrolling
- **Arxiv ID**: http://arxiv.org/abs/2006.09450v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, eess.SP, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2006.09450v2)
- **Published**: 2020-06-16 18:46:42+00:00
- **Updated**: 2020-11-19 16:56:54+00:00
- **Authors**: Burhaneddin Yaman, Seyed Amir Hossein Hosseini, Mehmet Akçakaya
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning based image denoising methods have been recently popular due to their improved performance. Traditionally, these methods are trained in a supervised manner, requiring a set of noisy input and clean target image pairs. More recently, self-supervised approaches have been proposed to learn denoising from only noisy images. These methods assume that noise across pixels is statistically independent, and the underlying image pixels show spatial correlations across neighborhoods. These methods rely on a masking approach that divides the image pixels into two disjoint sets, where one is used as input to the network while the other is used to define the loss. However, these previous self-supervised approaches rely on a purely data-driven regularization neural network without explicitly taking the masking model into account. In this work, building on these self-supervised approaches, we introduce Noise2Inpaint (N2I), a training approach that recasts the denoising problem into a regularized image inpainting framework. This allows us to use an objective function, which can incorporate different statistical properties of the noise as needed. We use algorithm unrolling to unroll an iterative optimization for solving this objective function and train the unrolled network end-to-end. The training paradigm follows the masking approach from previous works, splitting the pixels into two disjoint sets. Importantly, one of these is now used to impose data fidelity in the unrolled network, while the other still defines the loss. We demonstrate that N2I performs successful denoising on real-world datasets, while better preserving details compared to its purely data-driven counterpart Noise2Self.



### Interpretable multimodal fusion networks reveal mechanisms of brain cognition
- **Arxiv ID**: http://arxiv.org/abs/2006.09454v1
- **DOI**: None
- **Categories**: **q-bio.NC**, cs.CV, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.09454v1)
- **Published**: 2020-06-16 18:52:50+00:00
- **Updated**: 2020-06-16 18:52:50+00:00
- **Authors**: Wenxing Hu, Xianghe Meng, Yuntong Bai, Aiying Zhang, Biao Cai, Gemeng Zhang, Tony W. Wilson, Julia M. Stephen, Vince D. Calhoun, Yu-Ping Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Multimodal fusion benefits disease diagnosis by providing a more comprehensive perspective. Developing algorithms is challenging due to data heterogeneity and the complex within- and between-modality associations. Deep-network-based data-fusion models have been developed to capture the complex associations and the performance in diagnosis has been improved accordingly. Moving beyond diagnosis prediction, evaluation of disease mechanisms is critically important for biomedical research. Deep-network-based data-fusion models, however, are difficult to interpret, bringing about difficulties for studying biological mechanisms. In this work, we develop an interpretable multimodal fusion model, namely gCAM-CCL, which can perform automated diagnosis and result interpretation simultaneously. The gCAM-CCL model can generate interpretable activation maps, which quantify pixel-level contributions of the input features. This is achieved by combining intermediate feature maps using gradient-based weights. Moreover, the estimated activation maps are class-specific, and the captured cross-data associations are interest/label related, which further facilitates class-specific analysis and biological mechanism analysis. We validate the gCAM-CCL model on a brain imaging-genetic study, and show gCAM-CCL's performed well for both classification and mechanism analysis. Mechanism analysis suggests that during task-fMRI scans, several object recognition related regions of interests (ROIs) are first activated and then several downstream encoding ROIs get involved. Results also suggest that the higher cognition performing group may have stronger neurotransmission signaling while the lower cognition performing group may have problem in brain/neuron development, resulting from genetic variations.



### On the Inference of Soft Biometrics from Typing Patterns Collected in a Multi-device Environment
- **Arxiv ID**: http://arxiv.org/abs/2006.09501v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY, cs.HC, I.3.6
- **Links**: [PDF](http://arxiv.org/pdf/2006.09501v1)
- **Published**: 2020-06-16 20:25:58+00:00
- **Updated**: 2020-06-16 20:25:58+00:00
- **Authors**: Vishaal Udandarao, Mohit Agrawal, Rajesh Kumar, Rajiv Ratn Shah
- **Comment**: The first two authors contributed equally. The code is available upon
  request. Please contact the last author
- **Journal**: The Sixth IEEE International Conference on Multimedia Big Data,
  August 2020
- **Summary**: In this paper, we study the inference of gender, major/minor (computer science, non-computer science), typing style, age, and height from the typing patterns collected from 117 individuals in a multi-device environment. The inference of the first three identifiers was considered as classification tasks, while the rest as regression tasks. For classification tasks, we benchmark the performance of six classical machine learning (ML) and four deep learning (DL) classifiers. On the other hand, for regression tasks, we evaluated three ML and four DL-based regressors. The overall experiment consisted of two text-entry (free and fixed) and four device (Desktop, Tablet, Phone, and Combined) configurations. The best arrangements achieved accuracies of 96.15%, 93.02%, and 87.80% for typing style, gender, and major/minor, respectively, and mean absolute errors of 1.77 years and 2.65 inches for age and height, respectively. The results are promising considering the variety of application scenarios that we have listed in this work.



### Gradient Amplification: An efficient way to train deep neural networks
- **Arxiv ID**: http://arxiv.org/abs/2006.10560v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.10560v1)
- **Published**: 2020-06-16 20:30:55+00:00
- **Updated**: 2020-06-16 20:30:55+00:00
- **Authors**: Sunitha Basodi, Chunyan Ji, Haiping Zhang, Yi Pan
- **Comment**: 9 page document with 7 figures and one results table
- **Journal**: Big Data Mining and Analytics 2020
- **Summary**: Improving performance of deep learning models and reducing their training times are ongoing challenges in deep neural networks. There are several approaches proposed to address these challenges one of which is to increase the depth of the neural networks. Such deeper networks not only increase training times, but also suffer from vanishing gradients problem while training. In this work, we propose gradient amplification approach for training deep learning models to prevent vanishing gradients and also develop a training strategy to enable or disable gradient amplification method across several epochs with different learning rates. We perform experiments on VGG-19 and resnet (Resnet-18 and Resnet-34) models, and study the impact of amplification parameters on these models in detail. Our proposed approach improves performance of these deep learning models even at higher learning rates, thereby allowing these models to achieve higher performance with reduced training time.



### A generalizable saliency map-based interpretation of model outcome
- **Arxiv ID**: http://arxiv.org/abs/2006.09504v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.09504v2)
- **Published**: 2020-06-16 20:34:42+00:00
- **Updated**: 2020-06-19 04:51:53+00:00
- **Authors**: Shailja Thakur, Sebastian Fischmeister
- **Comment**: None
- **Journal**: None
- **Summary**: One of the significant challenges of deep neural networks is that the complex nature of the network prevents human comprehension of the outcome of the network. Consequently, the applicability of complex machine learning models is limited in the safety-critical domains, which incurs risk to life and property. To fully exploit the capabilities of complex neural networks, we propose a non-intrusive interpretability technique that uses the input and output of the model to generate a saliency map. The method works by empirically optimizing a randomly initialized input mask by localizing and weighing individual pixels according to their sensitivity towards the target class. Our experiments show that the proposed model interpretability approach performs better than the existing saliency map-based approaches methods at localizing the relevant input pixels.   Furthermore, to obtain a global perspective on the target-specific explanation, we propose a saliency map reconstruction approach to generate acceptable variations of the salient inputs from the space of input data distribution for which the model outcome remains unaltered. Experiments show that our interpretability method can reconstruct the salient part of the input with a classification accuracy of 89%.



### On sparse connectivity, adversarial robustness, and a novel model of the artificial neuron
- **Arxiv ID**: http://arxiv.org/abs/2006.09510v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.09510v1)
- **Published**: 2020-06-16 20:45:08+00:00
- **Updated**: 2020-06-16 20:45:08+00:00
- **Authors**: Sergey Bochkanov
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks have achieved human-level accuracy on almost all perceptual benchmarks. It is interesting that these advances were made using two ideas that are decades old: (a) an artificial neuron based on a linear summator and (b) SGD training.   However, there are important metrics beyond accuracy: computational efficiency and stability against adversarial perturbations. In this paper, we propose two closely connected methods to improve these metrics on contour recognition tasks: (a) a novel model of an artificial neuron, a "strong neuron," with low hardware requirements and inherent robustness against adversarial perturbations and (b) a novel constructive training algorithm that generates sparse networks with $O(1)$ connections per neuron.   We demonstrate the feasibility of our approach through experiments on SVHN and GTSRB benchmarks. We achieved an impressive 10x-100x reduction in operations count (10x when compared with other sparsification approaches, 100x when compared with dense networks) and a substantial reduction in hardware requirements (8-bit fixed-point math was used) with no reduction in model accuracy. Superior stability against adversarial perturbations (exceeding that of adversarial training) was achieved without any counteradversarial measures, relying on the robustness of strong neurons alone. We also proved that constituent blocks of our strong neuron are the only activation functions with perfect stability against adversarial attacks.



### Visual Chirality
- **Arxiv ID**: http://arxiv.org/abs/2006.09512v1
- **DOI**: 10.1109/CVPR42600.2020.01231
- **Categories**: **cs.CV**, I.4
- **Links**: [PDF](http://arxiv.org/pdf/2006.09512v1)
- **Published**: 2020-06-16 20:48:23+00:00
- **Updated**: 2020-06-16 20:48:23+00:00
- **Authors**: Zhiqiu Lin, Jin Sun, Abe Davis, Noah Snavely
- **Comment**: Published at CVPR 2020, Best Paper Nomination, Oral Presentation.
  Project Page: https://linzhiqiu.github.io/papers/chirality/
- **Journal**: CVPR (2020), 12292-12300
- **Summary**: How can we tell whether an image has been mirrored? While we understand the geometry of mirror reflections very well, less has been said about how it affects distributions of imagery at scale, despite widespread use for data augmentation in computer vision. In this paper, we investigate how the statistics of visual data are changed by reflection. We refer to these changes as "visual chirality", after the concept of geometric chirality - the notion of objects that are distinct from their mirror image. Our analysis of visual chirality reveals surprising results, including low-level chiral signals pervading imagery stemming from image processing in cameras, to the ability to discover visual chirality in images of people and faces. Our work has implications for data augmentation, self-supervised learning, and image forensics.



### COVID-CXNet: Detecting COVID-19 in Frontal Chest X-ray Images using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2006.13807v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.13807v2)
- **Published**: 2020-06-16 21:31:02+00:00
- **Updated**: 2020-07-29 01:53:46+00:00
- **Authors**: Arman Haghanifar, Mahdiyar Molahasani Majdabadi, Younhee Choi, S. Deivalakshmi, Seokbum Ko
- **Comment**: The editor has asked for confidence intervals. In this version,
  confidence intervals are added to the manuscript
- **Journal**: None
- **Summary**: One of the primary clinical observations for screening the infectious by the novel coronavirus is capturing a chest x-ray image. In most of the patients, a chest x-ray contains abnormalities, such as consolidation, which are the results of COVID-19 viral pneumonia. In this study, research is conducted on efficiently detecting imaging features of this type of pneumonia using deep convolutional neural networks in a large dataset. It is demonstrated that simple models, alongside the majority of pretrained networks in the literature, focus on irrelevant features for decision-making. In this paper, numerous chest x-ray images from various sources are collected, and the largest publicly accessible dataset is prepared. Finally, using the transfer learning paradigm, the well-known CheXNet model is utilized for developing COVID-CXNet. This powerful model is capable of detecting the novel coronavirus pneumonia based on relevant and meaningful features with precise localization. COVID-CXNet is a step towards a fully automated and robust COVID-19 detection system.



### Explanation-based Weakly-supervised Learning of Visual Relations with Graph Networks
- **Arxiv ID**: http://arxiv.org/abs/2006.09562v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.09562v2)
- **Published**: 2020-06-16 23:14:52+00:00
- **Updated**: 2020-07-17 21:01:44+00:00
- **Authors**: Federico Baldassarre, Kevin Smith, Josephine Sullivan, Hossein Azizpour
- **Comment**: Published at the European Conference on Computer Vision, ECCV 2020
  (Poster)
- **Journal**: None
- **Summary**: Visual relationship detection is fundamental for holistic image understanding. However, the localization and classification of (subject, predicate, object) triplets remain challenging tasks, due to the combinatorial explosion of possible relationships, their long-tailed distribution in natural images, and an expensive annotation process. This paper introduces a novel weakly-supervised method for visual relationship detection that relies on minimal image-level predicate labels. A graph neural network is trained to classify predicates in images from a graph representation of detected objects, implicitly encoding an inductive bias for pairwise relations. We then frame relationship detection as the explanation of such a predicate classifier, i.e. we obtain a complete relation by recovering the subject and object of a predicted predicate. We present results comparable to recent fully- and weakly-supervised methods on three diverse and challenging datasets: HICO-DET for human-object interaction, Visual Relationship Detection for generic object-to-object relations, and UnRel for unusual triplets; demonstrating robustness to non-comprehensive annotations and good few-shot generalization.



### Validation and generalization of pixel-wise relevance in convolutional neural networks trained for face classification
- **Arxiv ID**: http://arxiv.org/abs/2006.16795v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.16795v1)
- **Published**: 2020-06-16 23:20:40+00:00
- **Updated**: 2020-06-16 23:20:40+00:00
- **Authors**: Jñani Crawford, Eshed Margalit, Kalanit Grill-Spector, Sonia Poltoratski
- **Comment**: 10 pages, 7 figures
- **Journal**: None
- **Summary**: The increased use of convolutional neural networks for face recognition in science, governance, and broader society has created an acute need for methods that can show how these 'black box' decisions are made. To be interpretable and useful to humans, such a method should convey a model's learned classification strategy in a way that is robust to random initializations or spurious correlations in input data. To this end, we applied the decompositional pixel-wise attribution method of layer-wise relevance propagation (LRP) to resolve the decisions of several classes of VGG-16 models trained for face recognition. We then quantified how these relevance measures vary with and generalize across key model parameters, such as the pretraining dataset (ImageNet or VGGFace), the finetuning task (gender or identity classification), and random initializations of model weights. Using relevance-based image masking, we find that relevance maps for face classification prove generally stable across random initializations, and can generalize across finetuning tasks. However, there is markedly less generalization across pretraining datasets, indicating that ImageNet- and VGGFace-trained models sample face information differently even as they achieve comparably high classification performance. Fine-grained analyses of relevance maps across models revealed asymmetries in generalization that point to specific benefits of choice parameters, and suggest that it may be possible to find an underlying set of important face image pixels that drive decisions across convolutional neural networks and tasks. Finally, we evaluated model decision weighting against human measures of similarity, providing a novel framework for interpreting face recognition decisions across human and machine.



### Mining Label Distribution Drift in Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2006.09565v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.09565v1)
- **Published**: 2020-06-16 23:41:42+00:00
- **Updated**: 2020-06-16 23:41:42+00:00
- **Authors**: Peizhao Li, Zhengming Ding, Hongfu Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised domain adaptation targets to transfer task knowledge from labeled source domain to related yet unlabeled target domain, and is catching extensive interests from academic and industrial areas. Although tremendous efforts along this direction have been made to minimize the domain divergence, unfortunately, most of existing methods only manage part of the picture by aligning feature representations from different domains. Beyond the discrepancy in feature space, the gap between known source label and unknown target label distribution, recognized as label distribution drift, is another crucial factor raising domain divergence, and has not been paid enough attention and well explored. From this point, in this paper, we first experimentally reveal how label distribution drift brings negative effects on current domain adaptation methods. Next, we propose Label distribution Matching Domain Adversarial Network (LMDAN) to handle data distribution shift and label distribution drift jointly. In LMDAN, label distribution drift problem is addressed by the proposed source samples weighting strategy, which select samples to contribute to positive adaptation and avoid negative effects brought by the mismatched in label distribution. Finally, different from general domain adaptation experiments, we modify domain adaptation datasets to create the considerable label distribution drift between source and target domain. Numerical results and empirical model analysis show that LMDAN delivers superior performance compared to other state-of-the-art domain adaptation methods under such scenarios.



