# Arxiv Papers in cs.CV on 2020-06-28
### Interpretable and Trustworthy Deepfake Detection via Dynamic Prototypes
- **Arxiv ID**: http://arxiv.org/abs/2006.15473v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.15473v2)
- **Published**: 2020-06-28 00:25:34+00:00
- **Updated**: 2021-01-15 02:13:45+00:00
- **Authors**: Loc Trinh, Michael Tsang, Sirisha Rambhatla, Yan Liu
- **Comment**: To appear in the 2021 IEEE Winter Conference on Applications of
  Computer Vision (WACV 21')
- **Journal**: None
- **Summary**: In this paper we propose a novel human-centered approach for detecting forgery in face images, using dynamic prototypes as a form of visual explanations. Currently, most state-of-the-art deepfake detections are based on black-box models that process videos frame-by-frame for inference, and few closely examine their temporal inconsistencies. However, the existence of such temporal artifacts within deepfake videos is key in detecting and explaining deepfakes to a supervising human. To this end, we propose Dynamic Prototype Network (DPNet) -- an interpretable and effective solution that utilizes dynamic representations (i.e., prototypes) to explain deepfake temporal artifacts. Extensive experimental results show that DPNet achieves competitive predictive performance, even on unseen testing datasets such as Google's DeepFakeDetection, DeeperForensics, and Celeb-DF, while providing easy referential explanations of deepfake dynamics. On top of DPNet's prototypical framework, we further formulate temporal logic specifications based on these dynamics to check our model's compliance to desired temporal behaviors, hence providing trustworthiness for such critical detection systems.



### Frequency learning for image classification
- **Arxiv ID**: http://arxiv.org/abs/2006.15476v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.15476v1)
- **Published**: 2020-06-28 00:32:47+00:00
- **Updated**: 2020-06-28 00:32:47+00:00
- **Authors**: José Augusto Stuchi, Levy Boccato, Romis Attux
- **Comment**: None
- **Journal**: None
- **Summary**: Machine learning applied to computer vision and signal processing is achieving results comparable to the human brain on specific tasks due to the great improvements brought by the deep neural networks (DNN). The majority of state-of-the-art architectures nowadays are DNN related, but only a few explore the frequency domain to extract useful information and improve the results, like in the image processing field. In this context, this paper presents a new approach for exploring the Fourier transform of the input images, which is composed of trainable frequency filters that boost discriminative components in the spectrum. Additionally, we propose a slicing procedure to allow the network to learn both global and local features from the frequency-domain representations of the image blocks. The proposed method proved to be competitive with respect to well-known DNN architectures in the selected experiments, with the advantage of being a simpler and lightweight model. This work also raises the discussion on how the state-of-the-art DNNs architectures can exploit not only spatial features, but also the frequency, in order to improve its performance when solving real world problems.



### Automated Stitching of Coral Reef Images and Extraction of Features for Damselfish Shoaling Behavior Analysis
- **Arxiv ID**: http://arxiv.org/abs/2006.15478v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.15478v1)
- **Published**: 2020-06-28 00:56:51+00:00
- **Updated**: 2020-06-28 00:56:51+00:00
- **Authors**: Riza Rae Pineda, Kristofer delas Peñas, Dana Manogan
- **Comment**: None
- **Journal**: None
- **Summary**: Behavior analysis of animals involves the observation of intraspecific and interspecific interactions among various organisms in the environment. Collective behavior such as herding in farm animals, flocking of birds, and shoaling and schooling of fish provide information on its benefits on collective survival, fitness, reproductive patterns, group decision-making, and effects in animal epidemiology. In marine ethology, the investigation of behavioral patterns in schooling species can provide supplemental information in the planning and management of marine resources. Currently, damselfish species, although prevalent in tropical waters, have no adequate established base behavior information. This limits reef managers in efficiently planning for stress and disaster responses in protecting the reef. Visual marine data captured in the wild are scarce and prone to multiple scene variations, primarily caused by motion and changes in the natural environment. The gathered videos of damselfish by this research exhibit several scene distortions caused by erratic camera motions during acquisition. To effectively analyze shoaling behavior given the issues posed by capturing data in the wild, we propose a pre-processing system that utilizes color correction and image stitching techniques and extracts behavior features for manual analysis.



### Bottom-Up Human Pose Estimation by Ranking Heatmap-Guided Adaptive Keypoint Estimates
- **Arxiv ID**: http://arxiv.org/abs/2006.15480v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.15480v1)
- **Published**: 2020-06-28 01:14:59+00:00
- **Updated**: 2020-06-28 01:14:59+00:00
- **Authors**: Ke Sun, Zigang Geng, Depu Meng, Bin Xiao, Dong Liu, Zhaoxiang Zhang, Jingdong Wang
- **Comment**: None
- **Journal**: None
- **Summary**: The typical bottom-up human pose estimation framework includes two stages, keypoint detection and grouping. Most existing works focus on developing grouping algorithms, e.g., associative embedding, and pixel-wise keypoint regression that we adopt in our approach. We present several schemes that are rarely or unthoroughly studied before for improving keypoint detection and grouping (keypoint regression) performance. First, we exploit the keypoint heatmaps for pixel-wise keypoint regression instead of separating them for improving keypoint regression. Second, we adopt a pixel-wise spatial transformer network to learn adaptive representations for handling the scale and orientation variance to further improve keypoint regression quality. Last, we present a joint shape and heatvalue scoring scheme to promote the estimated poses that are more likely to be true poses. Together with the tradeoff heatmap estimation loss for balancing the background and keypoint pixels and thus improving heatmap estimation quality, we get the state-of-the-art bottom-up human pose estimation result. Code is available at https://github.com/HRNet/HRNet-Bottom-up-Pose-Estimation.



### Laplacian Regularized Few-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2006.15486v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.15486v3)
- **Published**: 2020-06-28 02:17:52+00:00
- **Updated**: 2021-04-28 15:17:38+00:00
- **Authors**: Imtiaz Masud Ziko, Jose Dolz, Eric Granger, Ismail Ben Ayed
- **Comment**: ICML 2020 paper
- **Journal**: None
- **Summary**: We propose a transductive Laplacian-regularized inference for few-shot tasks. Given any feature embedding learned from the base classes, we minimize a quadratic binary-assignment function containing two terms: (1) a unary term assigning query samples to the nearest class prototype, and (2) a pairwise Laplacian term encouraging nearby query samples to have consistent label assignments. Our transductive inference does not re-train the base model, and can be viewed as a graph clustering of the query set, subject to supervision constraints from the support set. We derive a computationally efficient bound optimizer of a relaxation of our function, which computes independent (parallel) updates for each query sample, while guaranteeing convergence. Following a simple cross-entropy training on the base classes, and without complex meta-learning strategies, we conducted comprehensive experiments over five few-shot learning benchmarks. Our LaplacianShot consistently outperforms state-of-the-art methods by significant margins across different models, settings, and data sets. Furthermore, our transductive inference is very fast, with computational times that are close to inductive inference, and can be used for large-scale few-shot tasks.



### Video Representation Learning with Visual Tempo Consistency
- **Arxiv ID**: http://arxiv.org/abs/2006.15489v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.15489v2)
- **Published**: 2020-06-28 02:46:44+00:00
- **Updated**: 2020-12-18 03:02:33+00:00
- **Authors**: Ceyuan Yang, Yinghao Xu, Bo Dai, Bolei Zhou
- **Comment**: Technical report. Models are available at
  https://github.com/decisionforce/VTHCL
- **Journal**: None
- **Summary**: Visual tempo, which describes how fast an action goes, has shown its potential in supervised action recognition. In this work, we demonstrate that visual tempo can also serve as a self-supervision signal for video representation learning. We propose to maximize the mutual information between representations of slow and fast videos via hierarchical contrastive learning (VTHCL). Specifically, by sampling the same instance at slow and fast frame rates respectively, we can obtain slow and fast video frames which share the same semantics but contain different visual tempos. Video representations learned from VTHCL achieve the competitive performances under the self-supervision evaluation protocol for action recognition on UCF-101 (82.1\%) and HMDB-51 (49.2\%). Moreover, comprehensive experiments suggest that the learned representations are generalized well to other downstream tasks including action detection on AVA and action anticipation on Epic-Kitchen. Finally, we propose Instance Correspondence Map (ICM) to visualize the shared semantics captured by contrastive learning.



### 1st Place Solution for Waymo Open Dataset Challenge -- 3D Detection and Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2006.15505v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.15505v1)
- **Published**: 2020-06-28 04:49:39+00:00
- **Updated**: 2020-06-28 04:49:39+00:00
- **Authors**: Zhuangzhuang Ding, Yihan Hu, Runzhou Ge, Li Huang, Sijia Chen, Yu Wang, Jie Liao
- **Comment**: None
- **Journal**: None
- **Summary**: In this technical report, we introduce our winning solution "HorizonLiDAR3D" for the 3D detection track and the domain adaptation track in Waymo Open Dataset Challenge at CVPR 2020. Many existing 3D object detectors include prior-based anchor box design to account for different scales and aspect ratios and classes of objects, which limits its capability of generalization to a different dataset or domain and requires post-processing (e.g. Non-Maximum Suppression (NMS)). We proposed a one-stage, anchor-free and NMS-free 3D point cloud object detector AFDet, using object key-points to encode the 3D attributes, and to learn an end-to-end point cloud object detection without the need of hand-engineering or learning the anchors. AFDet serves as a strong baseline in our winning solution and significant improvements are made over this baseline during the challenges. Specifically, we design stronger networks and enhance the point cloud data using densification and point painting. To leverage camera information, we append/paint additional attributes to each point by projecting them to camera space and gathering image-based perception information. The final detection performance also benefits from model ensemble and Test-Time Augmentation (TTA) in both the 3D detection track and the domain adaptation track. Our solution achieves the 1st place with 77.11% mAPH/L2 and 69.49% mAPH/L2 respectively on the 3D detection track and the domain adaptation track.



### 1st Place Solutions for Waymo Open Dataset Challenges -- 2D and 3D Tracking
- **Arxiv ID**: http://arxiv.org/abs/2006.15506v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.15506v1)
- **Published**: 2020-06-28 04:49:59+00:00
- **Updated**: 2020-06-28 04:49:59+00:00
- **Authors**: Yu Wang, Sijia Chen, Li Huang, Runzhou Ge, Yihan Hu, Zhuangzhuang Ding, Jie Liao
- **Comment**: None
- **Journal**: None
- **Summary**: This technical report presents the online and real-time 2D and 3D multi-object tracking (MOT) algorithms that reached the 1st places on both Waymo Open Dataset 2D tracking and 3D tracking challenges. An efficient and pragmatic online tracking-by-detection framework named HorizonMOT is proposed for camera-based 2D tracking in the image space and LiDAR-based 3D tracking in the 3D world space. Within the tracking-by-detection paradigm, our trackers leverage our high-performing detectors used in the 2D/3D detection challenges and achieved 45.13% 2D MOTA/L2 and 63.45% 3D MOTA/L2 in the 2D/3D tracking challenges.



### 2nd Place Solution for Waymo Open Dataset Challenge -- 2D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2006.15507v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.15507v1)
- **Published**: 2020-06-28 04:50:16+00:00
- **Updated**: 2020-06-28 04:50:16+00:00
- **Authors**: Sijia Chen, Yu Wang, Li Huang, Runzhou Ge, Yihan Hu, Zhuangzhuang Ding, Jie Liao
- **Comment**: None
- **Journal**: None
- **Summary**: A practical autonomous driving system urges the need to reliably and accurately detect vehicles and persons. In this report, we introduce a state-of-the-art 2D object detection system for autonomous driving scenarios. Specifically, we integrate both popular two-stage detector and one-stage detector with anchor free fashion to yield a robust detection. Furthermore, we train multiple expert models and design a greedy version of the auto ensemble scheme that automatically merges detections from different models. Notably, our overall detection system achieves 70.28 L2 mAP on the Waymo Open Dataset v1.2, ranking the 2nd place in the 2D detection track of the Waymo Open Dataset Challenges.



### Enhancement of a CNN-Based Denoiser Based on Spatial and Spectral Analysis
- **Arxiv ID**: http://arxiv.org/abs/2006.15517v1
- **DOI**: 10.1109/ICIP.2019.8804295
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.15517v1)
- **Published**: 2020-06-28 05:25:50+00:00
- **Updated**: 2020-06-28 05:25:50+00:00
- **Authors**: Rui Zhao, Kin-Man Lam, Daniel P. K. Lun
- **Comment**: ICIP 2019
- **Journal**: None
- **Summary**: Convolutional neural network (CNN)-based image denoising methods have been widely studied recently, because of their high-speed processing capability and good visual quality. However, most of the existing CNN-based denoisers learn the image prior from the spatial domain, and suffer from the problem of spatially variant noise, which limits their performance in real-world image denoising tasks. In this paper, we propose a discrete wavelet denoising CNN (WDnCNN), which restores images corrupted by various noise with a single model. Since most of the content or energy of natural images resides in the low-frequency spectrum, their transformed coefficients in the frequency domain are highly imbalanced. To address this issue, we present a band normalization module (BNM) to normalize the coefficients from different parts of the frequency spectrum. Moreover, we employ a band discriminative training (BDT) criterion to enhance the model regression. We evaluate the proposed WDnCNN, and compare it with other state-of-the-art denoisers. Experimental results show that WDnCNN achieves promising performance in both synthetic and real noise reduction, making it a potential solution to many practical image denoising applications.



### Predictive and Generative Neural Networks for Object Functionality
- **Arxiv ID**: http://arxiv.org/abs/2006.15520v1
- **DOI**: 10.1145/3197517.3201287
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.15520v1)
- **Published**: 2020-06-28 05:40:05+00:00
- **Updated**: 2020-06-28 05:40:05+00:00
- **Authors**: Ruizhen Hu, Zihao Yan, Jingwen Zhang, Oliver van Kaick, Ariel Shamir, Hao Zhang, Hui Huang
- **Comment**: Accepted to SIGGRAPH 2018, project page at
  https://vcc.tech/research/2018/ICON4
- **Journal**: ACM Transactions on Graphics (Proc. SIGGRAPH), volume 37, number
  4, pages 151:1--151:14, year 2018
- **Summary**: Humans can predict the functionality of an object even without any surroundings, since their knowledge and experience would allow them to "hallucinate" the interaction or usage scenarios involving the object. We develop predictive and generative deep convolutional neural networks to replicate this feat. Specifically, our work focuses on functionalities of man-made 3D objects characterized by human-object or object-object interactions. Our networks are trained on a database of scene contexts, called interaction contexts, each consisting of a central object and one or more surrounding objects, that represent object functionalities. Given a 3D object in isolation, our functional similarity network (fSIM-NET), a variation of the triplet network, is trained to predict the functionality of the object by inferring functionality-revealing interaction contexts. fSIM-NET is complemented by a generative network (iGEN-NET) and a segmentation network (iSEG-NET). iGEN-NET takes a single voxelized 3D object with a functionality label and synthesizes a voxelized surround, i.e., the interaction context which visually demonstrates the corresponding functionality. iSEG-NET further separates the interacting objects into different groups according to their interaction types.



### MgSvF: Multi-Grained Slow vs. Fast Framework for Few-Shot Class-Incremental Learning
- **Arxiv ID**: http://arxiv.org/abs/2006.15524v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.15524v3)
- **Published**: 2020-06-28 06:12:49+00:00
- **Updated**: 2021-03-02 16:47:28+00:00
- **Authors**: Hanbin Zhao, Yongjian Fu, Mintong Kang, Qi Tian, Fei Wu, Xi Li
- **Comment**: None
- **Journal**: None
- **Summary**: As a challenging problem, few-shot class-incremental learning (FSCIL) continually learns a sequence of tasks, confronting the dilemma between slow forgetting of old knowledge and fast adaptation to new knowledge. In this paper, we concentrate on this "slow vs. fast" (SvF) dilemma to determine which knowledge components to be updated in a slow fashion or a fast fashion, and thereby balance old-knowledge preservation and new-knowledge adaptation. We propose a multi-grained SvF learning strategy to cope with the SvF dilemma from two different grains: intra-space (within the same feature space) and inter-space (between two different feature spaces). The proposed strategy designs a novel frequency-aware regularization to boost the intra-space SvF capability, and meanwhile develops a new feature space composition operation to enhance the inter-space SvF learning performance. With the multi-grained SvF learning strategy, our method outperforms the state-of-the-art approaches by a large margin.



### DeepACC:Automate Chromosome Classification based on Metaphase Images using Deep Learning Framework Fused with Prior Knowledge
- **Arxiv ID**: http://arxiv.org/abs/2006.15528v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.15528v2)
- **Published**: 2020-06-28 07:04:41+00:00
- **Updated**: 2021-08-11 10:58:52+00:00
- **Authors**: Chunlong Luo, Tianqi Yu, Yufan Luo, Manqing Wang, Fuhai Yu, Yinhao Li, Chan Tian, Jie Qiao, Li Xiao
- **Comment**: This work is supported by a fund from another hospital. Only Li Xiao
  conceived the idea and supervised Chunlong Luo to complete the work, the data
  provider did not participate in the research process. Thus, the authorships
  and institutional information are not correct. After careful consideration, I
  decide to withdraw this preprint version
- **Journal**: None
- **Summary**: Chromosome classification is an important but difficult and tedious task in karyotyping. Previous methods only classify manually segmented single chromosome, which is far from clinical practice. In this work, we propose a detection based method, DeepACC, to locate and fine classify chromosomes simultaneously based on the whole metaphase image. We firstly introduce the Additive Angular Margin Loss to enhance the discriminative power of model. To alleviate batch effects, we transform decision boundary of each class case-by-case through a siamese network which make full use of prior knowledges that chromosomes usually appear in pairs. Furthermore, we take the clinically seven group criterion as a prior knowledge and design an additional Group Inner-Adjacency Loss to further reduce inter-class similarities. 3390 metaphase images from clinical laboratory are collected and labelled to evaluate the performance. Results show that the new design brings encouraging performance gains comparing to the state-of-the-art baselines.



### Compositional Convolutional Neural Networks: A Robust and Interpretable Model for Object Recognition under Occlusion
- **Arxiv ID**: http://arxiv.org/abs/2006.15538v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.15538v1)
- **Published**: 2020-06-28 08:18:19+00:00
- **Updated**: 2020-06-28 08:18:19+00:00
- **Authors**: Adam Kortylewski, Qing Liu, Angtian Wang, Yihong Sun, Alan Yuille
- **Comment**: None
- **Journal**: None
- **Summary**: Computer vision systems in real-world applications need to be robust to partial occlusion while also being explainable. In this work, we show that black-box deep convolutional neural networks (DCNNs) have only limited robustness to partial occlusion. We overcome these limitations by unifying DCNNs with part-based models into Compositional Convolutional Neural Networks (CompositionalNets) - an interpretable deep architecture with innate robustness to partial occlusion. Specifically, we propose to replace the fully connected classification head of DCNNs with a differentiable compositional model that can be trained end-to-end. The structure of the compositional model enables CompositionalNets to decompose images into objects and context, as well as to further decompose object representations in terms of individual parts and the objects' pose. The generative nature of our compositional model enables it to localize occluders and to recognize objects based on their non-occluded parts. We conduct extensive experiments in terms of image classification and object detection on images of artificially occluded objects from the PASCAL3D+ and ImageNet dataset, and real images of partially occluded vehicles from the MS-COCO dataset. Our experiments show that CompositionalNets made from several popular DCNN backbones (VGG-16, ResNet50, ResNext) improve by a large margin over their non-compositional counterparts at classifying and detecting partially occluded objects. Furthermore, they can localize occluders accurately despite being trained with class-level supervision only. Finally, we demonstrate that CompositionalNets provide human interpretable predictions as their individual components can be understood as detecting parts and estimating an objects' viewpoint.



### DHARI Report to EPIC-Kitchens 2020 Object Detection Challenge
- **Arxiv ID**: http://arxiv.org/abs/2006.15553v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.15553v1)
- **Published**: 2020-06-28 09:29:48+00:00
- **Updated**: 2020-06-28 09:29:48+00:00
- **Authors**: Kaide Li, Bingyan Liao, Laifeng Hu, Yaonong Wang
- **Comment**: Challenge Winner in the EPIC-Kitchens 2020 Object Detection
  Challenge(EPIC@CVPR2020)
- **Journal**: None
- **Summary**: In this report, we describe the technical details of oursubmission to the EPIC-Kitchens Object Detection Challenge.Duck filling and mix-up techniques are firstly introduced to augment the data and significantly improve the robustness of the proposed method. Then we propose GRE-FPN and Hard IoU-imbalance Sampler methods to extract more representative global object features. To bridge the gap of category imbalance, Class Balance Sampling is utilized and greatly improves the test results. Besides, some training and testing strategies are also exploited, such as Stochastic Weight Averaging and multi-scale testing. Experimental results demonstrate that our approach can significantly improve the mean Average Precision (mAP) of object detection on both the seen and unseen test sets of EPICKitchens.



### When and How Can Deep Generative Models be Inverted?
- **Arxiv ID**: http://arxiv.org/abs/2006.15555v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.15555v1)
- **Published**: 2020-06-28 09:37:52+00:00
- **Updated**: 2020-06-28 09:37:52+00:00
- **Authors**: Aviad Aberdam, Dror Simon, Michael Elad
- **Comment**: None
- **Journal**: None
- **Summary**: Deep generative models (e.g. GANs and VAEs) have been developed quite extensively in recent years. Lately, there has been an increased interest in the inversion of such a model, i.e. given a (possibly corrupted) signal, we wish to recover the latent vector that generated it. Building upon sparse representation theory, we define conditions that are applicable to any inversion algorithm (gradient descent, deep encoder, etc.), under which such generative models are invertible with a unique solution. Importantly, the proposed analysis is applicable to any trained model, and does not depend on Gaussian i.i.d. weights. Furthermore, we introduce two layer-wise inversion pursuit algorithms for trained generative networks of arbitrary depth, and accompany these with recovery guarantees. Finally, we validate our theoretical results numerically and show that our method outperforms gradient descent when inverting such generators, both for clean and corrupted signals.



### SAR Image Despeckling by Deep Neural Networks: from a pre-trained model to an end-to-end training strategy
- **Arxiv ID**: http://arxiv.org/abs/2006.15559v4
- **DOI**: 10.3390/rs12162636
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.15559v4)
- **Published**: 2020-06-28 09:47:31+00:00
- **Updated**: 2020-09-21 12:24:43+00:00
- **Authors**: Emanuele Dalsasso, Xiangli Yang, Loïc Denis, Florence Tupin, Wen Yang
- **Comment**: Article accepted for publication on Remote Sensing, MDPI. Notebook
  with Colab compatibility is available at
  https://gitlab.telecom-paris.fr/RING/SAR-CNN
- **Journal**: Remote Sens. 2020, 12(16), 2636
- **Summary**: Speckle reduction is a longstanding topic in synthetic aperture radar (SAR) images. Many different schemes have been proposed for the restoration of intensity SAR images. Among the different possible approaches, methods based on convolutional neural networks (CNNs) have recently shown to reach state-of-the-art performance for SAR image restoration. CNN training requires good training data: many pairs of speckle-free / speckle-corrupted images. This is an issue in SAR applications, given the inherent scarcity of speckle-free images. To handle this problem, this paper analyzes different strategies one can adopt, depending on the speckle removal task one wishes to perform and the availability of multitemporal stacks of SAR data. The first strategy applies a CNN model, trained to remove additive white Gaussian noise from natural images, to a recently proposed SAR speckle removal framework: MuLoG (MUlti-channel LOgarithm with Gaussian denoising). No training on SAR images is performed, the network is readily applied to speckle reduction tasks. The second strategy considers a novel approach to construct a reliable dataset of speckle-free SAR images necessary to train a CNN model. Finally, a hybrid approach is also analyzed: the CNN used to remove additive white Gaussian noise is trained on speckle-free SAR images. The proposed methods are compared to other state-of-the-art speckle removal filters, to evaluate the quality of denoising and to discuss the pros and cons of the different strategies. Along with the paper, we make available the weights of the trained network to allow its usage by other researchers.



### Dynamic Sampling Networks for Efficient Action Recognition in Videos
- **Arxiv ID**: http://arxiv.org/abs/2006.15560v1
- **DOI**: 10.1109/TIP.2020.3007826
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.15560v1)
- **Published**: 2020-06-28 09:48:29+00:00
- **Updated**: 2020-06-28 09:48:29+00:00
- **Authors**: Yin-Dong Zheng, Zhaoyang Liu, Tong Lu, Limin Wang
- **Comment**: To appear in IEEE Transaction on Image Processing
- **Journal**: None
- **Summary**: The existing action recognition methods are mainly based on clip-level classifiers such as two-stream CNNs or 3D CNNs, which are trained from the randomly selected clips and applied to densely sampled clips during testing. However, this standard setting might be suboptimal for training classifiers and also requires huge computational overhead when deployed in practice. To address these issues, we propose a new framework for action recognition in videos, called {\em Dynamic Sampling Networks} (DSN), by designing a dynamic sampling module to improve the discriminative power of learned clip-level classifiers and as well increase the inference efficiency during testing. Specifically, DSN is composed of a sampling module and a classification module, whose objective is to learn a sampling policy to on-the-fly select which clips to keep and train a clip-level classifier to perform action recognition based on these selected clips, respectively. In particular, given an input video, we train an observation network in an associative reinforcement learning setting to maximize the rewards of the selected clips with a correct prediction. We perform extensive experiments to study different aspects of the DSN framework on four action recognition datasets: UCF101, HMDB51, THUMOS14, and ActivityNet v1.3. The experimental results demonstrate that DSN is able to greatly improve the inference efficiency by only using less than half of the clips, which can still obtain a slightly better or comparable recognition accuracy to the state-of-the-art approaches.



### Joint Hand-object 3D Reconstruction from a Single Image with Cross-branch Feature Fusion
- **Arxiv ID**: http://arxiv.org/abs/2006.15561v2
- **DOI**: 10.1109/TIP.2021.3068645
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.15561v2)
- **Published**: 2020-06-28 09:50:25+00:00
- **Updated**: 2021-03-22 07:38:29+00:00
- **Authors**: Yujin Chen, Zhigang Tu, Di Kang, Ruizhi Chen, Linchao Bao, Zhengyou Zhang, Junsong Yuan
- **Comment**: Accepted by IEEE Transactions on Image Processing (TIP)
- **Journal**: None
- **Summary**: Accurate 3D reconstruction of the hand and object shape from a hand-object image is important for understanding human-object interaction as well as human daily activities. Different from bare hand pose estimation, hand-object interaction poses a strong constraint on both the hand and its manipulated object, which suggests that hand configuration may be crucial contextual information for the object, and vice versa. However, current approaches address this task by training a two-branch network to reconstruct the hand and object separately with little communication between the two branches. In this work, we propose to consider hand and object jointly in feature space and explore the reciprocity of the two branches. We extensively investigate cross-branch feature fusion architectures with MLP or LSTM units. Among the investigated architectures, a variant with LSTM units that enhances object feature with hand feature shows the best performance gain. Moreover, we employ an auxiliary depth estimation module to augment the input RGB image with the estimated depth map, which further improves the reconstruction accuracy. Experiments conducted on public datasets demonstrate that our approach significantly outperforms existing approaches in terms of the reconstruction accuracy of objects.



### MvMM-RegNet: A new image registration framework based on multivariate mixture model and neural network estimation
- **Arxiv ID**: http://arxiv.org/abs/2006.15573v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.15573v2)
- **Published**: 2020-06-28 11:19:15+00:00
- **Updated**: 2020-07-14 04:38:16+00:00
- **Authors**: Xinzhe Luo, Xiahai Zhuang
- **Comment**: Accepted for publication at MICCAI 2020; Code is available from
  https://github.com/xzluo97/MvMM-RegNet
- **Journal**: None
- **Summary**: Current deep-learning-based registration algorithms often exploit intensity-based similarity measures as the loss function, where dense correspondence between a pair of moving and fixed images is optimized through backpropagation during training. However, intensity-based metrics can be misleading when the assumption of intensity class correspondence is violated, especially in cross-modality or contrast-enhanced images. Moreover, existing learning-based registration methods are predominantly applicable to pairwise registration and are rarely extended to groupwise registration or simultaneous registration with multiple images. In this paper, we propose a new image registration framework based on multivariate mixture model (MvMM) and neural network estimation. A generative model consolidating both appearance and anatomical information is established to derive a novel loss function capable of implementing groupwise registration. We highlight the versatility of the proposed framework for various applications on multimodal cardiac images, including single-atlas-based segmentation (SAS) via pairwise registration and multi-atlas segmentation (MAS) unified by groupwise registration. We evaluated performance on two publicly available datasets, i.e. MM-WHS-2017 and MS-CMRSeg-2019. The results show that the proposed framework achieved an average Dice score of $0.871\pm 0.025$ for whole-heart segmentation on MR images and $0.783\pm 0.082$ for myocardium segmentation on LGE MR images.



### SMPR: Single-Stage Multi-Person Pose Regression
- **Arxiv ID**: http://arxiv.org/abs/2006.15576v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.15576v2)
- **Published**: 2020-06-28 11:26:38+00:00
- **Updated**: 2020-11-30 15:57:52+00:00
- **Authors**: Junqi Lin, Huixin Miao, Junjie Cao, Zhixun Su, Risheng Liu
- **Comment**: 9 pages, 8 figures
- **Journal**: None
- **Summary**: Existing multi-person pose estimators can be roughly divided into two-stage approaches (top-down and bottom-up approaches) and one-stage approaches. The two-stage methods either suffer high computational redundancy for additional person detectors or group keypoints heuristically after predicting all the instance-free keypoints. The recently proposed single-stage methods do not rely on the above two extra stages but have lower performance than the latest bottom-up approaches. In this work, a novel single-stage multi-person pose regression, termed SMPR, is presented. It follows the paradigm of dense prediction and predicts instance-aware keypoints from every location. Besides feature aggregation, we propose better strategies to define positive pose hypotheses for training which all play an important role in dense pose estimation. The network also learns the scores of estimated poses. The pose scoring strategy further improves the pose estimation performance by prioritizing superior poses during non-maximum suppression (NMS). We show that our method not only outperforms existing single-stage methods and but also be competitive with the latest bottom-up methods, with 70.2 AP and 77.5 AP75 on the COCO test-dev pose benchmark. Code is available at https://github.com/cmdi-dlut/SMPR.



### Generalisable 3D Fabric Architecture for Streamlined Universal Multi-Dataset Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2006.15578v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.15578v4)
- **Published**: 2020-06-28 11:35:23+00:00
- **Updated**: 2022-11-29 00:54:21+00:00
- **Authors**: Siyu Liu, Wei Dai, Craig Engstrom, Jurgen Fripp, Stuart Crozier, Jason A. Dowling, Shekhar S. Chandra
- **Comment**: None
- **Journal**: None
- **Summary**: Data scarcity is common in deep learning models for medical image segmentation. Previous works proposed multi-dataset learning, either simultaneously or via transfer learning to expand training sets. However, medical image datasets have diverse-sized images and features, and developing a model simultaneously for multiple datasets is challenging. This work proposes Fabric Image Representation Encoding Network (FIRENet), a universal architecture for simultaneous multi-dataset segmentation and transfer learning involving arbitrary numbers of dataset(s). To handle different-sized image and feature, a 3D fabric module is used to encapsulate many multi-scale sub-architectures. An optimal combination of these sub-architectures can be implicitly learnt to best suit the target dataset(s). For diverse-scale feature extraction, a 3D extension of atrous spatial pyramid pooling (ASPP3D) is used in each fabric node for a fine-grained coverage of rich-scale image features. In the first experiment, FIRENet performed 3D universal bone segmentation of multiple musculoskeletal datasets of the human knee, shoulder and hip joints and exhibited excellent simultaneous multi-dataset segmentation performance. When tested for transfer learning, FIRENet further exhibited excellent single dataset performance (when pre-training on a prostate dataset), as well as significantly improved universal bone segmentation performance. The following experiment involves the simultaneous segmentation of the 10 Medical Segmentation Decathlon (MSD) challenge datasets. FIRENet demonstrated good multi-dataset segmentation results and inter-dataset adaptability of highly diverse image sizes. In both experiments, FIRENet's streamlined multi-dataset learning with one unified network that requires no hyper-parameter tuning.



### A lateral semicircular canal segmentation based geometric calibration for human temporal bone CT Image
- **Arxiv ID**: http://arxiv.org/abs/2006.15588v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.15588v1)
- **Published**: 2020-06-28 12:36:08+00:00
- **Updated**: 2020-06-28 12:36:08+00:00
- **Authors**: Xiaoguang Li, Peng Fu, Hongxia Yin, ZhenChang Wang, Li Zhuo, Hui Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Computed Tomography (CT) of the temporal bone has become an important method for diagnosing ear diseases. Due to the different posture of the subject and the settings of CT scanners, the CT image of the human temporal bone should be geometrically calibrated to ensure the symmetry of the bilateral anatomical structure. Manual calibration is a time-consuming task for radiologists and an important pre-processing step for further computer-aided CT analysis. We propose an automatic calibration algorithm for temporal bone CT images. The lateral semicircular canals (LSCs) are segmented as anchors at first. Then, we define a standard 3D coordinate system. The key step is the LSC segmentation. We design a novel 3D LSC segmentation encoder-decoder network, which introduces a 3D dilated convolution and a multi-pooling scheme for feature fusion in the encoding stage. The experimental results show that our LSC segmentation network achieved a higher segmentation accuracy. Our proposed method can help to perform calibration of temporal bone CT images efficiently.



### Localization Uncertainty Estimation for Anchor-Free Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2006.15607v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.15607v6)
- **Published**: 2020-06-28 13:49:30+00:00
- **Updated**: 2022-07-06 05:18:04+00:00
- **Authors**: Youngwan Lee, Joong-won Hwang, Hyung-Il Kim, Kimin Yun, Yongjin Kwon, Yuseok Bae, Sung Ju Hwang
- **Comment**: None
- **Journal**: None
- **Summary**: Since many safety-critical systems, such as surgical robots and autonomous driving cars operate in unstable environments with sensor noise and incomplete data, it is desirable for object detectors to take the localization uncertainty into account. However, there are several limitations of the existing uncertainty estimation methods for anchor-based object detection. 1) They model the uncertainty of the heterogeneous object properties with different characteristics and scales, such as location (center point) and scale (width, height), which could be difficult to estimate. 2) They model box offsets as Gaussian distributions, which is not compatible with the ground truth bounding boxes that follow the Dirac delta distribution. 3) Since anchor-based methods are sensitive to anchor hyper-parameters, their localization uncertainty could also be highly sensitive to the choice of hyper-parameters. To tackle these limitations, we propose a new localization uncertainty estimation method called UAD for anchor-free object detection. Our method captures the uncertainty in four directions of box offsets (left, right, top, bottom) that are homogeneous, so that it can tell which direction is uncertain, and provide a quantitative value of uncertainty in [0, 1]. To enable such uncertainty estimation, we design a new uncertainty loss, negative power log-likelihood loss, to measure the localization uncertainty by weighting the likelihood loss by its IoU, which alleviates the model misspecification problem. Furthermore, we propose an uncertainty-aware focal loss for reflecting the estimated uncertainty to the classification score. Experimental results on COCO datasets demonstrate that our method significantly improves FCOS, by up to 1.8 points, without sacrificing computational efficiency.



### Reinforcement Learning Based Handwritten Digit Recognition with Two-State Q-Learning
- **Arxiv ID**: http://arxiv.org/abs/2007.01193v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.01193v2)
- **Published**: 2020-06-28 14:23:36+00:00
- **Updated**: 2020-08-10 10:17:30+00:00
- **Authors**: Abdul Mueed Hafiz, Ghulam Mohiuddin Bhat
- **Comment**: None
- **Journal**: None
- **Summary**: We present a simple yet efficient Hybrid Classifier based on Deep Learning and Reinforcement Learning. Q-Learning is used with two Q-states and four actions. Conventional techniques use feature maps extracted from Convolutional Neural Networks (CNNs) and include them in the Qstates along with past history. This leads to difficulties with these approaches as the number of states is very large number due to high dimensions of the feature maps. Since our method uses only two Q-states it is simple and has much lesser number of parameters to optimize and also thus has a straightforward reward function. Also, the approach uses unexplored actions for image processing vis-a-vis other contemporary techniques. Three datasets have been used for benchmarking of the approach. These are the MNIST Digit Image Dataset, the USPS Digit Image Dataset and the MATLAB Digit Image Dataset. The performance of the proposed hybrid classifier has been compared with other contemporary techniques like a well-established Reinforcement Learning Technique, AlexNet, CNN-Nearest Neighbor Classifier and CNNSupport Vector Machine Classifier. Our approach outperforms these contemporary hybrid classifiers on all the three datasets used.



### Shadow Removal by a Lightness-Guided Network with Training on Unpaired Data
- **Arxiv ID**: http://arxiv.org/abs/2006.15617v1
- **DOI**: 10.1109/TIP.2020.3048677
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.15617v1)
- **Published**: 2020-06-28 14:31:18+00:00
- **Updated**: 2020-06-28 14:31:18+00:00
- **Authors**: Zhihao Liu, Hui Yin, Yang Mi, Mengyang Pu, Song Wang
- **Comment**: Submitted to IEEE TIP
- **Journal**: None
- **Summary**: Shadow removal can significantly improve the image visual quality and has many applications in computer vision. Deep learning methods based on CNNs have become the most effective approach for shadow removal by training on either paired data, where both the shadow and underlying shadow-free versions of an image are known, or unpaired data, where shadow and shadow-free training images are totally different with no correspondence. In practice, CNN training on unpaired data is more preferred given the easiness of training data collection. In this paper, we present a new Lightness-Guided Shadow Removal Network (LG-ShadowNet) for shadow removal by training on unpaired data. In this method, we first train a CNN module to compensate for the lightness and then train a second CNN module with the guidance of lightness information from the first CNN module for final shadow removal. We also introduce a loss function to further utilise the colour prior of existing data. Extensive experiments on widely used ISTD, adjusted ISTD and USR datasets demonstrate that the proposed method outperforms the state-of-the-art methods with training on unpaired data.



### Analogical Image Translation for Fog Generation
- **Arxiv ID**: http://arxiv.org/abs/2006.15618v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.15618v1)
- **Published**: 2020-06-28 14:33:31+00:00
- **Updated**: 2020-06-28 14:33:31+00:00
- **Authors**: Rui Gong, Dengxin Dai, Yuhua Chen, Wen Li, Luc Van Gool
- **Comment**: 18 pages, 9 figures, 7 tables
- **Journal**: None
- **Summary**: Image-to-image translation is to map images from a given \emph{style} to another given \emph{style}. While exceptionally successful, current methods assume the availability of training images in both source and target domains, which does not always hold in practice. Inspired by humans' reasoning capability of analogy, we propose analogical image translation (AIT). Given images of two styles in the source domain: $\mathcal{A}$ and $\mathcal{A}^\prime$, along with images $\mathcal{B}$ of the first style in the target domain, learn a model to translate $\mathcal{B}$ to $\mathcal{B}^\prime$ in the target domain, such that $\mathcal{A}:\mathcal{A}^\prime ::\mathcal{B}:\mathcal{B}^\prime$. AIT is especially useful for translation scenarios in which training data of one style is hard to obtain but training data of the same two styles in another domain is available. For instance, in the case from normal conditions to extreme, rare conditions, obtaining real training images for the latter case is challenging but obtaining synthetic data for both cases is relatively easy. In this work, we are interested in adding adverse weather effects, more specifically fog effects, to images taken in clear weather. To circumvent the challenge of collecting real foggy images, AIT learns with synthetic clear-weather images, synthetic foggy images and real clear-weather images to add fog effects onto real clear-weather images without seeing any real foggy images during training. AIT achieves this zero-shot image translation capability by coupling a supervised training scheme in the synthetic domain, a cycle consistency strategy in the real domain, an adversarial training scheme between the two domains, and a novel network design. Experiments show the effectiveness of our method for zero-short image translation and its benefit for downstream tasks such as semantic foggy scene understanding.



### Offline Handwritten Chinese Text Recognition with Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2006.15619v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.15619v1)
- **Published**: 2020-06-28 14:34:38+00:00
- **Updated**: 2020-06-28 14:34:38+00:00
- **Authors**: Brian Liu, Xianchao Xu, Yu Zhang
- **Comment**: 6 pages, 5 figures, and 3 tables
- **Journal**: None
- **Summary**: Deep learning based methods have been dominating the text recognition tasks in different and multilingual scenarios. The offline handwritten Chinese text recognition (HCTR) is one of the most challenging tasks because it involves thousands of characters, variant writing styles and complex data collection process. Recently, the recurrent-free architectures for text recognition appears to be competitive as its highly parallelism and comparable results. In this paper, we build the models using only the convolutional neural networks and use CTC as the loss function. To reduce the overfitting, we apply dropout after each max-pooling layer and with extreme high rate on the last one before the linear layer. The CASIA-HWDB database is selected to tune and evaluate the proposed models. With the existing text samples as templates, we randomly choose isolated character samples to synthesis more text samples for training. We finally achieve 6.81% character error rate (CER) on the ICDAR 2013 competition set, which is the best published result without language model correction.



### A Survey on Instance Segmentation: State of the art
- **Arxiv ID**: http://arxiv.org/abs/2007.00047v1
- **DOI**: 10.1007/s13735-020-00195-x
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.00047v1)
- **Published**: 2020-06-28 14:39:20+00:00
- **Updated**: 2020-06-28 14:39:20+00:00
- **Authors**: Abdul Mueed Hafiz, Ghulam Mohiuddin Bhat
- **Comment**: Int J Multimed Info Retr (2020)
- **Journal**: None
- **Summary**: Object detection or localization is an incremental step in progression from coarse to fine digital image inference. It not only provides the classes of the image objects, but also provides the location of the image objects which have been classified. The location is given in the form of bounding boxes or centroids. Semantic segmentation gives fine inference by predicting labels for every pixel in the input image. Each pixel is labelled according to the object class within which it is enclosed. Furthering this evolution, instance segmentation gives different labels for separate instances of objects belonging to the same class. Hence, instance segmentation may be defined as the technique of simultaneously solving the problem of object detection as well as that of semantic segmentation. In this survey paper on instance segmentation -- its background, issues, techniques, evolution, popular datasets, related work up to the state of the art and future scope have been discussed. The paper provides valuable information for those who want to do research in the field of instance segmentation.



### Fast Training of Deep Networks with One-Class CNNs
- **Arxiv ID**: http://arxiv.org/abs/2007.00046v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.00046v2)
- **Published**: 2020-06-28 14:53:45+00:00
- **Updated**: 2020-07-22 11:51:16+00:00
- **Authors**: Abdul Mueed Hafiz, Ghulam Mohiuddin Bhat
- **Comment**: Camera Ready: 2nd International Conference on Cybernetics, Cognition
  and Machine Learning Applications(ICCCMLA), 2020, India
- **Journal**: None
- **Summary**: One-class CNNs have shown promise in novelty detection. However, very less work has been done on extending them to multiclass classification. The proposed approach is a viable effort in this direction. It uses one-class CNNs i.e., it trains one CNN per class, for multiclass classification. An ensemble of such one-class CNNs is used for multiclass classification. The benefits of the approach are generally better recognition accuracy while taking almost even half or two-thirds of the training time of a conventional multi-class deep network. The proposed approach has been applied successfully to face recognition and object recognition tasks. For face recognition, a 1000 frame RGB video, featuring many faces together, has been used for benchmarking of the proposed approach. Its database is available on request via e-mail. For object recognition, the Caltech-101 Image Database and 17Flowers Dataset have also been used. The experimental results support the claims made.



### Image Classification by Reinforcement Learning with Two-State Q-Learning
- **Arxiv ID**: http://arxiv.org/abs/2007.01298v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.01298v3)
- **Published**: 2020-06-28 14:54:48+00:00
- **Updated**: 2020-10-31 13:23:49+00:00
- **Authors**: Abdul Mueed Hafiz
- **Comment**: HICO-2021 Camera Ready Paper
- **Journal**: None
- **Summary**: In this paper, a simple and efficient Hybrid Classifier is presented which is based on deep learning and reinforcement learning. Here, Q-Learning has been used with two states and 'two or three' actions. Other techniques found in the literature use feature map extracted from Convolutional Neural Networks and use these in the Q-states along with past history. This leads to technical difficulties in these approaches because the number of states is high due to large dimensions of the feature map. Because the proposed technique uses only two Q-states it is straightforward and consequently has much lesser number of optimization parameters, and thus also has a simple reward function. Also, the proposed technique uses novel actions for processing images as compared to other techniques found in literature. The performance of the proposed technique is compared with other recent algorithms like ResNet50, InceptionV3, etc. on popular databases including ImageNet, Cats and Dogs Dataset, and Caltech-101 Dataset. The proposed approach outperforms others techniques on all the datasets used.



### Improving VQA and its Explanations \\ by Comparing Competing Explanations
- **Arxiv ID**: http://arxiv.org/abs/2006.15631v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.15631v1)
- **Published**: 2020-06-28 15:11:40+00:00
- **Updated**: 2020-06-28 15:11:40+00:00
- **Authors**: Jialin Wu, Liyan Chen, Raymond J. Mooney
- **Comment**: None
- **Journal**: None
- **Summary**: Most recent state-of-the-art Visual Question Answering (VQA) systems are opaque black boxes that are only trained to fit the answer distribution given the question and visual content. As a result, these systems frequently take shortcuts, focusing on simple visual concepts or question priors. This phenomenon becomes more problematic as the questions become complex that requires more reasoning and commonsense knowledge. To address this issue, we present a novel framework that uses explanations for competing answers to help VQA systems select the correct answer. By training on human textual explanations, our framework builds better representations for the questions and visual content, and then reweights confidences in the answer candidates using either generated or retrieved explanations from the training set. We evaluate our framework on the VQA-X dataset, which has more difficult questions with human explanations, achieving new state-of-the-art results on both VQA and its explanations.



### K-Nearest Neighbour and Support Vector Machine Hybrid Classification
- **Arxiv ID**: http://arxiv.org/abs/2007.00045v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.00045v1)
- **Published**: 2020-06-28 15:26:56+00:00
- **Updated**: 2020-06-28 15:26:56+00:00
- **Authors**: A. M. Hafiz
- **Comment**: None
- **Journal**: International Journal of Imaging and Robotics, Vol.19, No.4,
  pp.33-41, CESER Publications (2019)
- **Summary**: In this paper, a novel K-Nearest Neighbour and Support Vector Machine hybrid classification technique has been proposed that is simple and robust. It is based on the concept of discriminative nearest neighbourhood classification. The technique consists of using K-Nearest Neighbour Classification for test samples satisfying a proximity condition. The patterns which do not pass the proximity condition are separated. This is followed by sifting the training set for a fixed number of patterns for every class which are closest to each separated test pattern respectively, based on the Euclidean distance metric. Subsequently, for every separated test sample, a Support Vector Machine is trained on the sifted training set patterns associated with it, and classification for the test sample is done. The proposed technique has been compared to the state of art in this research area. Three datasets viz. the United States Postal Service (USPS) Handwritten Digit Dataset, MNIST Dataset, and an Arabic numeral dataset, the Modified Arabic Digits Database, MADB, have been used to evaluate the performance of the algorithm. The algorithm generally outperforms the other algorithms with which it has been compared.



### Digit Image Recognition Using an Ensemble of One-Versus-All Deep Network Classifiers
- **Arxiv ID**: http://arxiv.org/abs/2007.01192v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.01192v2)
- **Published**: 2020-06-28 15:37:39+00:00
- **Updated**: 2020-10-31 13:04:35+00:00
- **Authors**: Abdul Mueed Hafiz, Mahmoud Hassaballah
- **Comment**: ICTCS 2020 Camera Ready Paper
- **Journal**: None
- **Summary**: In multiclass deep network classifiers, the burden of classifying samples of different classes is put on a single classifier. As the result the optimum classification accuracy is not obtained. Also training times are large due to running the CNN training on single CPU/GPU. However it is known that using ensembles of classifiers increases the performance. Also, the training times can be reduced by running each member of the ensemble on a separate processor. Ensemble learning has been used in the past for traditional methods to a varying extent and is a hot topic. With the advent of deep learning, ensemble learning has been applied to the former as well. However, an area which is unexplored and has potential is One-Versus-All (OVA) deep ensemble learning. In this paper we explore it and show that by using OVA ensembles of deep networks, improvements in performance of deep networks can be obtained. As shown in this paper, the classification capability of deep networks can be further increased by using an ensemble of binary classification (OVA) deep networks. We implement a novel technique for the case of digit image recognition and test and evaluate it on the same. In the proposed approach, a single OVA deep network classifier is dedicated to each category. Subsequently, OVA deep network ensembles have been investigated. Every network in an ensemble has been trained by an OVA training technique using the Stochastic Gradient Descent with Momentum Algorithm (SGDMA). For classification of a test sample, the sample is presented to each network in the ensemble. After prediction score voting, the network with the largest score is assumed to have classified the sample. The experimentation has been done on the MNIST digit dataset, the USPS+ digit dataset, and MATLAB digit image dataset. Our proposed technique outperforms the baseline on digit image recognition for all datasets.



### Learning Goals from Failure
- **Arxiv ID**: http://arxiv.org/abs/2006.15657v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.15657v2)
- **Published**: 2020-06-28 17:16:49+00:00
- **Updated**: 2020-12-13 01:44:08+00:00
- **Authors**: Dave Epstein, Carl Vondrick
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a framework that predicts the goals behind observable human action in video. Motivated by evidence in developmental psychology, we leverage video of unintentional action to learn video representations of goals without direct supervision. Our approach models videos as contextual trajectories that represent both low-level motion and high-level action features. Experiments and visualizations show our trained model is able to predict the underlying goals in video of unintentional action. We also propose a method to "automatically correct" unintentional action by leveraging gradient signals of our model to adjust latent trajectories. Although the model is trained with minimal supervision, it is competitive with or outperforms baselines trained on large (supervised) datasets of successfully executed goals, showing that observing unintentional action is crucial to learning about goals in video. Project page: https://aha.cs.columbia.edu/



### Geometry-Inspired Top-k Adversarial Perturbations
- **Arxiv ID**: http://arxiv.org/abs/2006.15669v6
- **DOI**: 10.1109/WACV51458.2022.00411
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.15669v6)
- **Published**: 2020-06-28 18:05:57+00:00
- **Updated**: 2021-11-23 14:58:22+00:00
- **Authors**: Nurislam Tursynbek, Aleksandr Petiushko, Ivan Oseledets
- **Comment**: WACV 2022
- **Journal**: 2022 IEEE/CVF Winter Conference on Applications of Computer Vision
  (WACV)
- **Summary**: The brittleness of deep image classifiers to small adversarial input perturbations has been extensively studied in the last several years. However, the main objective of existing perturbations is primarily limited to change the correctly predicted Top-1 class by an incorrect one, which does not intend to change the Top-k prediction. In many digital real-world scenarios Top-k prediction is more relevant. In this work, we propose a fast and accurate method of computing Top-k adversarial examples as a simple multi-objective optimization. We demonstrate its efficacy and performance by comparing it to other adversarial example crafting techniques. Moreover, based on this method, we propose Top-k Universal Adversarial Perturbations, image-agnostic tiny perturbations that cause the true class to be absent among the Top-k prediction for the majority of natural images. We experimentally show that our approach outperforms baseline methods and even improves existing techniques of finding Universal Adversarial Perturbations.



### Harvesting, Detecting, and Characterizing Liver Lesions from Large-scale Multi-phase CT Data via Deep Dynamic Texture Learning
- **Arxiv ID**: http://arxiv.org/abs/2006.15691v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.15691v2)
- **Published**: 2020-06-28 19:55:34+00:00
- **Updated**: 2020-08-30 16:51:39+00:00
- **Authors**: Yuankai Huo, Jinzheng Cai, Chi-Tung Cheng, Ashwin Raju, Ke Yan, Bennett A. Landman, Jing Xiao, Le Lu, Chien-Hung Liao, Adam P. Harrison
- **Comment**: None
- **Journal**: None
- **Summary**: Non-invasive radiological-based lesion characterization and identification, e.g., to differentiate cancer subtypes, has long been a major aim to enhance oncological diagnosis and treatment procedures. Here we study a specific population of human subjects, with the hope of reducing the need for invasive surgical biopsies of liver cancer patients, which can cause many harmful side-effects. To this end, we propose a fully-automated and multi-stage liver tumor characterization framework designed for dynamic contrast computed tomography (CT). Our system comprises four sequential processes of tumor proposal detection, tumor harvesting, primary tumor site selection, and deep texture-based tumor characterization. Our main contributions are that, (1) we propose a 3D non-isotropic anchor-free detection method for liver lesions; (2) we present and validate spatially adaptivedeep texture (SaDT) learning, which allows for more precise characterization of liver lesions; (3) using a semi-automatic process, we bootstrap off of 200 gold standard annotations to curate another 1001 patients. Experimental evaluations demonstrate that our new data curation strategy, combined with the SaDT deep dynamic texture analysis, can effectively improve the mean F1 scores by >8.6% compared with baselines, in differentiating four major liver lesion types. Our F1 score of (hepatocellular carcinoma versus remaining subclasses) is 0.763, which is higher than reported human observer performance using dynamic CT and comparable to an advanced magnetic resonance imagery protocol. Apart from demonstrating the benefits of our data curation approach and physician-inspired workflow, these results also indicate that analyzing texture features, instead of standard object-based analysis, is a promising strategy for lesion differentiation.



### Simulation of Brain Resection for Cavity Segmentation Using Self-Supervised and Semi-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2006.15693v1
- **DOI**: 10.1007/978-3-030-59716-0_12
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.15693v1)
- **Published**: 2020-06-28 20:03:39+00:00
- **Updated**: 2020-06-28 20:03:39+00:00
- **Authors**: Fernando Pérez-García, Roman Rodionov, Ali Alim-Marvasti, Rachel Sparks, John S. Duncan, Sébastien Ourselin
- **Comment**: 13 pages, 6 figures, accepted at the International Conference on
  Medical Image Computing and Computer Assisted Intervention (MICCAI) 2020
- **Journal**: Medical Image Computing and Computer Assisted Intervention -
  MICCAI 2020. Lecture Notes in Computer Science, vol 12263. Springer, Cham
- **Summary**: Resective surgery may be curative for drug-resistant focal epilepsy, but only 40% to 70% of patients achieve seizure freedom after surgery. Retrospective quantitative analysis could elucidate patterns in resected structures and patient outcomes to improve resective surgery. However, the resection cavity must first be segmented on the postoperative MR image. Convolutional neural networks (CNNs) are the state-of-the-art image segmentation technique, but require large amounts of annotated data for training. Annotation of medical images is a time-consuming process requiring highly-trained raters, and often suffering from high inter-rater variability. Self-supervised learning can be used to generate training instances from unlabeled data. We developed an algorithm to simulate resections on preoperative MR images. We curated a new dataset, EPISURG, comprising 431 postoperative and 269 preoperative MR images from 431 patients who underwent resective surgery. In addition to EPISURG, we used three public datasets comprising 1813 preoperative MR images for training. We trained a 3D CNN on artificially resected images created on the fly during training, using images from 1) EPISURG, 2) public datasets and 3) both. To evaluate trained models, we calculate Dice score (DSC) between model segmentations and 200 manual annotations performed by three human raters. The model trained on data with manual annotations obtained a median (interquartile range) DSC of 65.3 (30.6). The DSC of our best-performing model, trained with no manual annotations, is 81.7 (14.2). For comparison, inter-rater agreement between human annotators was 84.0 (9.9). We demonstrate a training method for CNNs using simulated resection cavities that can accurately segment real resection cavities, without manual annotations.



### MIMC-VINS: A Versatile and Resilient Multi-IMU Multi-Camera Visual-Inertial Navigation System
- **Arxiv ID**: http://arxiv.org/abs/2006.15699v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.15699v1)
- **Published**: 2020-06-28 20:16:08+00:00
- **Updated**: 2020-06-28 20:16:08+00:00
- **Authors**: Kevin Eckenhoff, Patrick Geneva, Guoquan Huang
- **Comment**: 20 pages, 10 figures, 13 tables
- **Journal**: None
- **Summary**: As cameras and inertial sensors are becoming ubiquitous in mobile devices and robots, it holds great potential to design visual-inertial navigation systems (VINS) for efficient versatile 3D motion tracking which utilize any (multiple) available cameras and inertial measurement units (IMUs) and are resilient to sensor failures or measurement depletion. To this end, rather than the standard VINS paradigm using a minimal sensing suite of a single camera and IMU, in this paper we design a real-time consistent multi-IMU multi-camera (MIMC)-VINS estimator that is able to seamlessly fuse multi-modal information from an arbitrary number of uncalibrated cameras and IMUs. Within an efficient multi-state constraint Kalman filter (MSCKF) framework, the proposed MIMC-VINS algorithm optimally fuses asynchronous measurements from all sensors, while providing smooth, uninterrupted, and accurate 3D motion tracking even if some sensors fail. The key idea of the proposed MIMC-VINS is to perform high-order on-manifold state interpolation to efficiently process all available visual measurements without increasing the computational burden due to estimating additional sensors' poses at asynchronous imaging times. In order to fuse the information from multiple IMUs, we propagate a joint system consisting of all IMU states while enforcing rigid-body constraints between the IMUs during the filter update stage. Lastly, we estimate online both spatiotemporal extrinsic and visual intrinsic parameters to make our system robust to errors in prior sensor calibration. The proposed system is extensively validated in both Monte-Carlo simulations and real-world experiments.



### Motion Pyramid Networks for Accurate and Efficient Cardiac Motion Estimation
- **Arxiv ID**: http://arxiv.org/abs/2006.15710v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.15710v3)
- **Published**: 2020-06-28 21:03:19+00:00
- **Updated**: 2020-09-15 23:13:18+00:00
- **Authors**: Hanchao Yu, Xiao Chen, Humphrey Shi, Terrence Chen, Thomas S. Huang, Shanhui Sun
- **Comment**: Accepted by MICCAI2020
- **Journal**: None
- **Summary**: Cardiac motion estimation plays a key role in MRI cardiac feature tracking and function assessment such as myocardium strain. In this paper, we propose Motion Pyramid Networks, a novel deep learning-based approach for accurate and efficient cardiac motion estimation. We predict and fuse a pyramid of motion fields from multiple scales of feature representations to generate a more refined motion field. We then use a novel cyclic teacher-student training strategy to make the inference end-to-end and further improve the tracking performance. Our teacher model provides more accurate motion estimation as supervision through progressive motion compensations. Our student model learns from the teacher model to estimate motion in a single step while maintaining accuracy. The teacher-student knowledge distillation is performed in a cyclic way for a further performance boost. Our proposed method outperforms a strong baseline model on two public available clinical datasets significantly, evaluated by a variety of metrics and the inference time. New evaluation metrics are also proposed to represent errors in a clinically meaningful manner.



### Generalizable Cone Beam CT Esophagus Segmentation Using Physics-Based Data Augmentation
- **Arxiv ID**: http://arxiv.org/abs/2006.15713v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.15713v2)
- **Published**: 2020-06-28 21:12:09+00:00
- **Updated**: 2021-01-30 22:33:15+00:00
- **Authors**: Sadegh R Alam, Tianfang Li, Pengpeng Zhang, Si-Yuan Zhang, Saad Nadeem
- **Comment**: Accepted to Physics in Medicine & Biology 2021
- **Journal**: None
- **Summary**: Automated segmentation of esophagus is critical in image guided/adaptive radiotherapy of lung cancer to minimize radiation-induced toxicities such as acute esophagitis. We developed a semantic physics-based data augmentation method for segmenting esophagus in both planning CT (pCT) and cone-beam CT (CBCT) using 3D convolutional neural networks. 191 cases with their pCT and CBCTs from four independent datasets were used to train a modified 3D-Unet architecture with a multi-objective loss function specifically designed for soft-tissue organs such as esophagus. Scatter artifacts and noise were extracted from week 1 CBCTs using power law adaptive histogram equalization method and induced to the corresponding pCT followed by reconstruction using CBCT reconstruction parameters. Moreover, we leverage physics-based artifact induced pCTs to drive the esophagus segmentation in real weekly CBCTs. Segmentations were evaluated using geometric Dice and Hausdorff distance as well as dosimetrically using mean esophagus dose and D5cc. Due to the physics-based data augmentation, our model trained just on the synthetic CBCTs was robust and generalizable enough to also produce state-of-the-art results on the pCTs and CBCTs, achieving 0.81 and 0.74 Dice overlap. Our physics-based data augmentation spans the realistic noise/artifact spectrum across patient CBCT/pCT data and can generalize well across modalities with the potential to improve the accuracy of treatment setup and response analysis.



### Uncertainty-aware multi-view co-training for semi-supervised medical image segmentation and domain adaptation
- **Arxiv ID**: http://arxiv.org/abs/2006.16806v1
- **DOI**: 10.1016/j.media.2020.101766
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.16806v1)
- **Published**: 2020-06-28 22:04:54+00:00
- **Updated**: 2020-06-28 22:04:54+00:00
- **Authors**: Yingda Xia, Dong Yang, Zhiding Yu, Fengze Liu, Jinzheng Cai, Lequan Yu, Zhuotun Zhu, Daguang Xu, Alan Yuille, Holger Roth
- **Comment**: 19 pages, 6 figures, to appear in Medical Image Analysis. This
  article is an extension of the conference paper arXiv:1811.12506
- **Journal**: Medical Image Analysis, 2020
- **Summary**: Although having achieved great success in medical image segmentation, deep learning-based approaches usually require large amounts of well-annotated data, which can be extremely expensive in the field of medical image analysis. Unlabeled data, on the other hand, is much easier to acquire. Semi-supervised learning and unsupervised domain adaptation both take the advantage of unlabeled data, and they are closely related to each other. In this paper, we propose uncertainty-aware multi-view co-training (UMCT), a unified framework that addresses these two tasks for volumetric medical image segmentation. Our framework is capable of efficiently utilizing unlabeled data for better performance. We firstly rotate and permute the 3D volumes into multiple views and train a 3D deep network on each view. We then apply co-training by enforcing multi-view consistency on unlabeled data, where an uncertainty estimation of each view is utilized to achieve accurate labeling. Experiments on the NIH pancreas segmentation dataset and a multi-organ segmentation dataset show state-of-the-art performance of the proposed framework on semi-supervised medical image segmentation. Under unsupervised domain adaptation settings, we validate the effectiveness of this work by adapting our multi-organ segmentation model to two pathological organs from the Medical Segmentation Decathlon Datasets. Additionally, we show that our UMCT-DA model can even effectively handle the challenging situation where labeled source data is inaccessible, demonstrating strong potentials for real-world applications.



### Unsupervised Learning of Video Representations via Dense Trajectory Clustering
- **Arxiv ID**: http://arxiv.org/abs/2006.15731v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.15731v1)
- **Published**: 2020-06-28 22:23:03+00:00
- **Updated**: 2020-06-28 22:23:03+00:00
- **Authors**: Pavel Tokmakov, Martial Hebert, Cordelia Schmid
- **Comment**: None
- **Journal**: None
- **Summary**: This paper addresses the task of unsupervised learning of representations for action recognition in videos. Previous works proposed to utilize future prediction, or other domain-specific objectives to train a network, but achieved only limited success. In contrast, in the relevant field of image representation learning, simpler, discrimination-based methods have recently bridged the gap to fully-supervised performance. We first propose to adapt two top performing objectives in this class - instance recognition and local aggregation, to the video domain. In particular, the latter approach iterates between clustering the videos in the feature space of a network and updating it to respect the cluster with a non-parametric classification loss. We observe promising performance, but qualitative analysis shows that the learned representations fail to capture motion patterns, grouping the videos based on appearance. To mitigate this issue, we turn to the heuristic-based IDT descriptors, that were manually designed to encode motion patterns in videos. We form the clusters in the IDT space, using these descriptors as a an unsupervised prior in the iterative local aggregation algorithm. Our experiments demonstrates that this approach outperform prior work on UCF101 and HMDB51 action recognition benchmarks. We also qualitatively analyze the learned representations and show that they successfully capture video dynamics.



### Roweisposes, Including Eigenposes, Supervised Eigenposes, and Fisherposes, for 3D Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2006.15736v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.15736v1)
- **Published**: 2020-06-28 22:46:12+00:00
- **Updated**: 2020-06-28 22:46:12+00:00
- **Authors**: Benyamin Ghojogh, Fakhri Karray, Mark Crowley
- **Comment**: key-words: Roweisposes, Roweis discriminant analysis, Fisherposes,
  eigenposes, supervised eigenposes, action recognition
- **Journal**: None
- **Summary**: Human action recognition is one of the important fields of computer vision and machine learning. Although various methods have been proposed for 3D action recognition, some of which are basic and some use deep learning, the need of basic methods based on generalized eigenvalue problem is sensed for action recognition. This need is especially sensed because of having similar basic methods in the field of face recognition such as eigenfaces and Fisherfaces. In this paper, we propose Roweisposes which uses Roweis discriminant analysis for generalized subspace learning. This method includes Fisherposes, eigenposes, supervised eigenposes, and double supervised eigenposes as its special cases. Roweisposes is a family of infinite number of action recongition methods which learn a discriminative subspace for embedding the body poses. Experiments on the TST, UTKinect, and UCFKinect datasets verify the effectiveness of the proposed method for action recognition.



