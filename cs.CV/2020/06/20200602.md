# Arxiv Papers in cs.CV on 2020-06-02
### A heterogeneous branch and multi-level classification network for person re-identification
- **Arxiv ID**: http://arxiv.org/abs/2006.01367v1
- **DOI**: 10.1016/j.neucom.2020.05.007
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.01367v1)
- **Published**: 2020-06-02 03:34:50+00:00
- **Updated**: 2020-06-02 03:34:50+00:00
- **Authors**: Jiabao Wang, Yang Li, Yangshuo Zhang, Zhuang Miao, Rui Zhang
- **Comment**: None
- **Journal**: Neurocomputing 404(2020)61-69
- **Summary**: Convolutional neural networks with multiple branches have recently been proved highly effective in person re-identification (re-ID). Researchers design multi-branch networks using part models, yet they always attribute the effectiveness to multiple parts. In addition, existing multi-branch networks always have isomorphic branches, which lack structural diversity. In order to improve this problem, we propose a novel Heterogeneous Branch and Multi-level Classification Network (HBMCN), which is designed based on the pre-trained ResNet-50 model. A new heterogeneous branch, SE-Res-Branch, is proposed based on the SE-Res module, which consists of the Squeeze-and-Excitation block and the residual block. Furthermore, a new multi-level classification joint objective function is proposed for the supervised learning of HBMCN, whereby multi-level features are extracted from multiple high-level layers and concatenated to represent a person. Based on three public person re-ID benchmarks (Market1501, DukeMTMC-reID and CUHK03), experimental results show that the proposed HBMCN reaches 94.4%, 85.7% and 73.8% in Rank-1, and 85.7%, 74.6% and 69.0% in mAP, achieving a state-of-the-art performance. Further analysis demonstrates that the specially designed heterogeneous branch performs better than an isomorphic branch, and multi-level classification provides more discriminative features compared to single-level classification. As a result, HBMCN provides substantial further improvements in person re-ID tasks.



### Adaptive convolutional neural networks for k-space data interpolation in fast magnetic resonance imaging
- **Arxiv ID**: http://arxiv.org/abs/2006.01385v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.01385v2)
- **Published**: 2020-06-02 04:29:33+00:00
- **Updated**: 2020-06-09 18:15:47+00:00
- **Authors**: Tianming Du, Honggang Zhang, Yuemeng Li, Hee Kwon Song, Yong Fan
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning in k-space has demonstrated great potential for image reconstruction from undersampled k-space data in fast magnetic resonance imaging (MRI). However, existing deep learning-based image reconstruction methods typically apply weight-sharing convolutional neural networks (CNNs) to k-space data without taking into consideration the k-space data's spatial frequency properties, leading to ineffective learning of the image reconstruction models. Moreover, complementary information of spatially adjacent slices is often ignored in existing deep learning methods. To overcome such limitations, we develop a deep learning algorithm, referred to as adaptive convolutional neural networks for k-space data interpolation (ACNN-k-Space), which adopts a residual Encoder-Decoder network architecture to interpolate the undersampled k-space data by integrating spatially contiguous slices as multi-channel input, along with k-space data from multiple coils if available. The network is enhanced by self-attention layers to adaptively focus on k-space data at different spatial frequencies and channels. We have evaluated our method on two public datasets and compared it with state-of-the-art existing methods. Ablation studies and experimental results demonstrate that our method effectively reconstructs images from undersampled k-space data and achieves significantly better image reconstruction performance than current state-of-the-art techniques.



### Exploring the role of Input and Output Layers of a Deep Neural Network in Adversarial Defense
- **Arxiv ID**: http://arxiv.org/abs/2006.01408v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.01408v1)
- **Published**: 2020-06-02 06:15:46+00:00
- **Updated**: 2020-06-02 06:15:46+00:00
- **Authors**: Jay N. Paranjape, Rahul Kumar Dubey, Vijendran V Gopalan
- **Comment**: 5 pages, 7 figures, to be presented at CONF-CDS 2020
- **Journal**: None
- **Summary**: Deep neural networks are learning models having achieved state of the art performance in many fields like prediction, computer vision, language processing and so on. However, it has been shown that certain inputs exist which would not trick a human normally, but may mislead the model completely. These inputs are known as adversarial inputs. These inputs pose a high security threat when such models are used in real world applications. In this work, we have analyzed the resistance of three different classes of fully connected dense networks against the rarely tested non-gradient based adversarial attacks. These classes are created by manipulating the input and output layers. We have proven empirically that owing to certain characteristics of the network, they provide a high robustness against these attacks, and can be used in fine tuning other models to increase defense against adversarial attacks.



### COVIDGR dataset and COVID-SDNet methodology for predicting COVID-19 based on Chest X-Ray images
- **Arxiv ID**: http://arxiv.org/abs/2006.01409v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.01409v3)
- **Published**: 2020-06-02 06:18:34+00:00
- **Updated**: 2020-11-11 06:15:00+00:00
- **Authors**: S. Tabik, A. Gómez-Ríos, J. L. Martín-Rodríguez, I. Sevillano-García, M. Rey-Area, D. Charte, E. Guirado, J. L. Suárez, J. Luengo, M. A. Valero-González, P. García-Villanova, E. Olmedo-Sánchez, F. Herrera
- **Comment**: Paper accepted in Journal of Biomedical And Health Informatics
- **Journal**: None
- **Summary**: Currently, Coronavirus disease (COVID-19), one of the most infectious diseases in the 21st century, is diagnosed using RT-PCR testing, CT scans and/or Chest X-Ray (CXR) images. CT (Computed Tomography) scanners and RT-PCR testing are not available in most medical centers and hence in many cases CXR images become the most time/cost effective tool for assisting clinicians in making decisions. Deep learning neural networks have a great potential for building COVID-19 triage systems and detecting COVID-19 patients, especially patients with low severity. Unfortunately, current databases do not allow building such systems as they are highly heterogeneous and biased towards severe cases. This paper is three-fold: (i) we demystify the high sensitivities achieved by most recent COVID-19 classification models, (ii) under a close collaboration with Hospital Universitario Cl\'inico San Cecilio, Granada, Spain, we built COVIDGR-1.0, a homogeneous and balanced database that includes all levels of severity, from normal with Positive RT-PCR, Mild, Moderate to Severe. COVIDGR-1.0 contains 426 positive and 426 negative PA (PosteroAnterior) CXR views and (iii) we propose COVID Smart Data based Network (COVID-SDNet) methodology for improving the generalization capacity of COVID-classification models. Our approach reaches good and stable results with an accuracy of $97.72\% \pm 0.95 \%$, $86.90\% \pm 3.20\%$, $61.80\% \pm 5.49\%$ in severe, moderate and mild COVID-19 severity levels (Paper accepted for publication in Journal of Biomedical and Health Informatics). Our approach could help in the early detection of COVID-19. COVIDGR-1.0 along with the severity level labels are available to the scientific community through this link https://dasci.es/es/transferencia/open-data/covidgr/.



### Transforming Multi-Concept Attention into Video Summarization
- **Arxiv ID**: http://arxiv.org/abs/2006.01410v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.01410v2)
- **Published**: 2020-06-02 06:23:50+00:00
- **Updated**: 2020-06-03 03:30:07+00:00
- **Authors**: Yen-Ting Liu, Yu-Jhe Li, Yu-Chiang Frank Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Video summarization is among challenging tasks in computer vision, which aims at identifying highlight frames or shots over a lengthy video input. In this paper, we propose an novel attention-based framework for video summarization with complex video data. Unlike previous works which only apply attention mechanism on the correspondence between frames, our multi-concept video self-attention (MC-VSA) model is presented to identify informative regions across temporal and concept video features, which jointly exploit context diversity over time and space for summarization purposes. Together with consistency between video and summary enforced in our framework, our model can be applied to both labeled and unlabeled data, making our method preferable to real-world applications. Extensive and complete experiments on two benchmarks demonstrate the effectiveness of our model both quantitatively and qualitatively, and confirms its superiority over the stateof-the-arts.



### Resolving Class Imbalance in Object Detection with Weighted Cross Entropy Losses
- **Arxiv ID**: http://arxiv.org/abs/2006.01413v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.01413v1)
- **Published**: 2020-06-02 06:36:12+00:00
- **Updated**: 2020-06-02 06:36:12+00:00
- **Authors**: Trong Huy Phan, Kazuma Yamamoto
- **Comment**: None
- **Journal**: None
- **Summary**: Object detection is an important task in computer vision which serves a lot of real-world applications such as autonomous driving, surveillance and robotics. Along with the rapid thrive of large-scale data, numerous state-of-the-art generalized object detectors (e.g. Faster R-CNN, YOLO, SSD) were developed in the past decade. Despite continual efforts in model modification and improvement in training strategies to boost detection accuracy, there are still limitations in performance of detectors when it comes to specialized datasets with uneven object class distributions. This originates from the common usage of Cross Entropy loss function for object classification sub-task that simply ignores the frequency of appearance of object class during training, and thus results in lower accuracies for object classes with fewer number of samples. Class-imbalance in general machine learning has been widely studied, however, little attention has been paid on the subject of object detection. In this paper, we propose to explore and overcome such problem by application of several weighted variants of Cross Entropy loss, for examples Balanced Cross Entropy, Focal Loss and Class-Balanced Loss Based on Effective Number of Samples to our object detector. Experiments with BDD100K (a highly class-imbalanced driving database acquired from on-vehicle cameras capturing mostly Car-class objects and other minority object classes such as Bus, Person and Motor) have proven better class-wise performances of detector trained with the afore-mentioned loss functions.



### Monocular Human Pose Estimation: A Survey of Deep Learning-based Methods
- **Arxiv ID**: http://arxiv.org/abs/2006.01423v1
- **DOI**: 10.1016/j.cviu.2019.102897
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.01423v1)
- **Published**: 2020-06-02 07:07:45+00:00
- **Updated**: 2020-06-02 07:07:45+00:00
- **Authors**: Yucheng Chen, Yingli Tian, Mingyi He
- **Comment**: This version corresponds to the pre-print of the paper accepted for
  Computer Vision and Image Understanding (CVIU)
- **Journal**: Computer Vision and Image Understanding (CVIU) 192 (2020) 102897
- **Summary**: Vision-based monocular human pose estimation, as one of the most fundamental and challenging problems in computer vision, aims to obtain posture of the human body from input images or video sequences. The recent developments of deep learning techniques have been brought significant progress and remarkable breakthroughs in the field of human pose estimation. This survey extensively reviews the recent deep learning-based 2D and 3D human pose estimation methods published since 2014. This paper summarizes the challenges, main frameworks, benchmark datasets, evaluation metrics, performance comparison, and discusses some promising future research directions.



### Image Super-Resolution with Cross-Scale Non-Local Attention and Exhaustive Self-Exemplars Mining
- **Arxiv ID**: http://arxiv.org/abs/2006.01424v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.01424v1)
- **Published**: 2020-06-02 07:08:58+00:00
- **Updated**: 2020-06-02 07:08:58+00:00
- **Authors**: Yiqun Mei, Yuchen Fan, Yuqian Zhou, Lichao Huang, Thomas S. Huang, Humphrey Shi
- **Comment**: CVPR2020
- **Journal**: None
- **Summary**: Deep convolution-based single image super-resolution (SISR) networks embrace the benefits of learning from large-scale external image resources for local recovery, yet most existing works have ignored the long-range feature-wise similarities in natural images. Some recent works have successfully leveraged this intrinsic feature correlation by exploring non-local attention modules. However, none of the current deep models have studied another inherent property of images: cross-scale feature correlation. In this paper, we propose the first Cross-Scale Non-Local (CS-NL) attention module with integration into a recurrent neural network. By combining the new CS-NL prior with local and in-scale non-local priors in a powerful recurrent fusion cell, we can find more cross-scale feature correlations within a single low-resolution (LR) image. The performance of SISR is significantly improved by exhaustively integrating all possible priors. Extensive experiments demonstrate the effectiveness of the proposed CS-NL module by setting new state-of-the-arts on multiple SISR benchmarks.



### A Multi-modal Neural Embeddings Approach for Detecting Mobile Counterfeit Apps: A Case Study on Google Play Store
- **Arxiv ID**: http://arxiv.org/abs/2006.02231v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2006.02231v1)
- **Published**: 2020-06-02 07:10:21+00:00
- **Updated**: 2020-06-02 07:10:21+00:00
- **Authors**: Naveen Karunanayake, Jathushan Rajasegaran, Ashanie Gunathillake, Suranga Seneviratne, Guillaume Jourjon
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1804.09882
- **Journal**: None
- **Summary**: Counterfeit apps impersonate existing popular apps in attempts to misguide users to install them for various reasons such as collecting personal information or spreading malware. Many counterfeits can be identified once installed, however even a tech-savvy user may struggle to detect them before installation. To this end, this paper proposes to leverage the recent advances in deep learning methods to create image and text embeddings so that counterfeit apps can be efficiently identified when they are submitted for publication. We show that a novel approach of combining content embeddings and style embeddings outperforms the baseline methods for image similarity such as SIFT, SURF, and various image hashing methods. We first evaluate the performance of the proposed method on two well-known datasets for evaluating image similarity methods and show that content, style, and combined embeddings increase precision@k and recall@k by 10%-15% and 12%-25%, respectively when retrieving five nearest neighbours. Second, specifically for the app counterfeit detection problem, combined content and style embeddings achieve 12% and 14% increase in precision@k and recall@k, respectively compared to the baseline methods. Third, we present an analysis of approximately 1.2 million apps from Google Play Store and identify a set of potential counterfeits for top-10,000 popular apps. Under a conservative assumption, we were able to find 2,040 potential counterfeits that contain malware in a set of 49,608 apps that showed high similarity to one of the top-10,000 popular apps in Google Play Store. We also find 1,565 potential counterfeits asking for at least five additional dangerous permissions than the original app and 1,407 potential counterfeits having at least five extra third party advertisement libraries.



### Distribution Aligned Multimodal and Multi-Domain Image Stylization
- **Arxiv ID**: http://arxiv.org/abs/2006.01431v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.01431v1)
- **Published**: 2020-06-02 07:25:53+00:00
- **Updated**: 2020-06-02 07:25:53+00:00
- **Authors**: Minxuan Lin, Fan Tang, Weiming Dong, Xiao Li, Chongyang Ma, Changsheng Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Multimodal and multi-domain stylization are two important problems in the field of image style transfer. Currently, there are few methods that can perform both multimodal and multi-domain stylization simultaneously. In this paper, we propose a unified framework for multimodal and multi-domain style transfer with the support of both exemplar-based reference and randomly sampled guidance. The key component of our method is a novel style distribution alignment module that eliminates the explicit distribution gaps between various style domains and reduces the risk of mode collapse. The multimodal diversity is ensured by either guidance from multiple images or random style code, while the multi-domain controllability is directly achieved by using a domain label. We validate our proposed framework on painting style transfer with a variety of different artistic styles and genres. Qualitative and quantitative comparisons with state-of-the-art methods demonstrate that our method can generate high-quality results of multi-domain styles and multimodal instances with reference style guidance or random sampled style.



### Recapture as You Want
- **Arxiv ID**: http://arxiv.org/abs/2006.01435v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.01435v1)
- **Published**: 2020-06-02 07:43:53+00:00
- **Updated**: 2020-06-02 07:43:53+00:00
- **Authors**: Chen Gao, Si Liu, Ran He, Shuicheng Yan, Bo Li
- **Comment**: 14 pages
- **Journal**: None
- **Summary**: With the increasing prevalence and more powerful camera systems of mobile devices, people can conveniently take photos in their daily life, which naturally brings the demand for more intelligent photo post-processing techniques, especially on those portrait photos. In this paper, we present a portrait recapture method enabling users to easily edit their portrait to desired posture/view, body figure and clothing style, which are very challenging to achieve since it requires to simultaneously perform non-rigid deformation of human body, invisible body-parts reasoning and semantic-aware editing. We decompose the editing procedure into semantic-aware geometric and appearance transformation. In geometric transformation, a semantic layout map is generated that meets user demands to represent part-level spatial constraints and further guides the semantic-aware appearance transformation. In appearance transformation, we design two novel modules, Semantic-aware Attentive Transfer (SAT) and Layout Graph Reasoning (LGR), to conduct intra-part transfer and inter-part reasoning, respectively. SAT module produces each human part by paying attention to the semantically consistent regions in the source portrait. It effectively addresses the non-rigid deformation issue and well preserves the intrinsic structure/appearance with rich texture details. LGR module utilizes body skeleton knowledge to construct a layout graph that connects all relevant part features, where graph reasoning mechanism is used to propagate information among part nodes to mine their relations. In this way, LGR module infers invisible body parts and guarantees global coherence among all the parts. Extensive experiments on DeepFashion, Market-1501 and in-the-wild photos demonstrate the effectiveness and superiority of our approach. Video demo is at: \url{https://youtu.be/vTyq9HL6jgw}.



### Learning to do multiframe wavefront sensing unsupervisedly: applications to blind deconvolution
- **Arxiv ID**: http://arxiv.org/abs/2006.01438v2
- **DOI**: 10.1051/0004-6361/202038552
- **Categories**: **astro-ph.IM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.01438v2)
- **Published**: 2020-06-02 08:02:12+00:00
- **Updated**: 2020-12-20 11:33:48+00:00
- **Authors**: A. Asensio Ramos, N. Olspert
- **Comment**: 11 pages, 4 figures, accepted for publication in A&A
- **Journal**: A&A 646, A100 (2021)
- **Summary**: Observations from ground based telescopes are affected by the presence of the Earth atmosphere, which severely perturbs them. The use of adaptive optics techniques has allowed us to partly beat this limitation. However, image selection or post-facto image reconstruction methods applied to bursts of short-exposure images are routinely needed to reach the diffraction limit. Deep learning has been recently proposed as an efficient way to accelerate these image reconstructions. Currently, these deep neural networks are trained with supervision, so that either standard deconvolution algorithms need to be applied a-priori or complex simulations of the solar magneto-convection need to be carried out to generate the training sets. Our aim here is to propose a general unsupervised training scheme that allows multiframe blind deconvolution deep learning systems to be trained simply with observations. The approach can be applied for the correction of point-like as well as extended objects. Leveraging the linear image formation theory and a probabilistic approach to the blind deconvolution problem produces a physically-motivated loss function. The optimization of this loss function allows an end-to-end training of a machine learning model composed of three neural networks. As examples, we apply this procedure to the deconvolution of stellar data from the FastCam instrument and to solar extended data from the Swedish Solar Telescope. The analysis demonstrates that the proposed neural model can be successfully trained without supervision using observations only. It provides estimations of the instantaneous wavefronts, from which a corrected image can be found using standard deconvolution technniques. The network model is roughly three orders of magnitude faster than applying standard deconvolution based on optimization and shows potential to be used on real-time at the telescope.



### CT-based COVID-19 Triage: Deep Multitask Learning Improves Joint Identification and Severity Quantification
- **Arxiv ID**: http://arxiv.org/abs/2006.01441v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.01441v3)
- **Published**: 2020-06-02 08:05:06+00:00
- **Updated**: 2020-11-26 05:32:20+00:00
- **Authors**: Mikhail Goncharov, Maxim Pisov, Alexey Shevtsov, Boris Shirokikh, Anvar Kurmukov, Ivan Blokhin, Valeria Chernina, Alexander Solovev, Victor Gombolevskiy, Sergey Morozov, Mikhail Belyaev
- **Comment**: None
- **Journal**: None
- **Summary**: The current COVID-19 pandemic overloads healthcare systems, including radiology departments. Though several deep learning approaches were developed to assist in CT analysis, nobody considered study triage directly as a computer science problem. We describe two basic setups: Identification of COVID-19 to prioritize studies of potentially infected patients to isolate them as early as possible; Severity quantification to highlight studies of severe patients and direct them to a hospital or provide emergency medical care. We formalize these tasks as binary classification and estimation of affected lung percentage. Though similar problems were well-studied separately, we show that existing methods provide reasonable quality only for one of these setups. We employ a multitask approach to consolidate both triage approaches and propose a convolutional neural network to combine all available labels within a single model. In contrast with the most popular multitask approaches, we add classification layers to the most spatially detailed upper part of U-Net instead of the bottom, less detailed latent representation. We train our model on approximately 2000 publicly available CT studies and test it with a carefully designed set consisting of 32 COVID-19 studies, 30 cases with bacterial pneumonia, 31 healthy patients, and 30 patients with other lung pathologies to emulate a typical patient flow in an out-patient hospital. The proposed multitask model outperforms the latent-based one and achieves ROC AUC scores ranging from 0.87+-01 (bacterial pneumonia) to 0.97+-01 (healthy controls) for Identification of COVID-19 and 0.97+-01 Spearman Correlation for Severity quantification. We release all the code and create a public leaderboard, where other community members can test their models on our test dataset.



### Perturbation Analysis of Gradient-based Adversarial Attacks
- **Arxiv ID**: http://arxiv.org/abs/2006.01456v1
- **DOI**: 10.1016/j.patrec.2020.04.034
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.01456v1)
- **Published**: 2020-06-02 08:51:37+00:00
- **Updated**: 2020-06-02 08:51:37+00:00
- **Authors**: Utku Ozbulak, Manvel Gasparyan, Wesley De Neve, Arnout Van Messem
- **Comment**: Accepted for publication in Pattern Recognition Letters, 2020
- **Journal**: Pattern Recognition Letters 2020, Volume 135, Pages 133-120
- **Summary**: After the discovery of adversarial examples and their adverse effects on deep learning models, many studies focused on finding more diverse methods to generate these carefully crafted samples. Although empirical results on the effectiveness of adversarial example generation methods against defense mechanisms are discussed in detail in the literature, an in-depth study of the theoretical properties and the perturbation effectiveness of these adversarial attacks has largely been lacking. In this paper, we investigate the objective functions of three popular methods for adversarial example generation: the L-BFGS attack, the Iterative Fast Gradient Sign attack, and Carlini & Wagner's attack (CW). Specifically, we perform a comparative and formal analysis of the loss functions underlying the aforementioned attacks while laying out large-scale experimental results on ImageNet dataset. This analysis exposes (1) the faster optimization speed as well as the constrained optimization space of the cross-entropy loss, (2) the detrimental effects of using the signature of the cross-entropy loss on optimization precision as well as optimization space, and (3) the slow optimization speed of the logit loss in the context of adversariality. Our experiments reveal that the Iterative Fast Gradient Sign attack, which is thought to be fast for generating adversarial examples, is the worst attack in terms of the number of iterations required to create adversarial examples in the setting of equal perturbation. Moreover, our experiments show that the underlying loss function of CW, which is criticized for being substantially slower than other adversarial attacks, is not that much slower than other loss functions. Finally, we analyze how well neural networks can identify adversarial perturbations generated by the attacks under consideration, hereby revisiting the idea of adversarial retraining on ImageNet.



### Channel Attention based Iterative Residual Learning for Depth Map Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2006.01469v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.01469v1)
- **Published**: 2020-06-02 09:12:23+00:00
- **Updated**: 2020-06-02 09:12:23+00:00
- **Authors**: Xibin Song, Yuchao Dai, Dingfu Zhou, Liu Liu, Wei Li, Hongdng Li, Ruigang Yang
- **Comment**: accepted by Conference on Computer Vision and Pattern Recognition
  2020
- **Journal**: None
- **Summary**: Despite the remarkable progresses made in deep-learning based depth map super-resolution (DSR), how to tackle real-world degradation in low-resolution (LR) depth maps remains a major challenge. Existing DSR model is generally trained and tested on synthetic dataset, which is very different from what would get from a real depth sensor. In this paper, we argue that DSR models trained under this setting are restrictive and not effective in dealing with real-world DSR tasks. We make two contributions in tackling real-world degradation of different depth sensors. First, we propose to classify the generation of LR depth maps into two types: non-linear downsampling with noise and interval downsampling, for which DSR models are learned correspondingly. Second, we propose a new framework for real-world DSR, which consists of four modules : 1) An iterative residual learning module with deep supervision to learn effective high-frequency components of depth maps in a coarse-to-fine manner; 2) A channel attention strategy to enhance channels with abundant high-frequency components; 3) A multi-stage fusion module to effectively re-exploit the results in the coarse-to-fine process; and 4) A depth refinement module to improve the depth map by TGV regularization and input loss. Extensive experiments on benchmarking datasets demonstrate the superiority of our method over current state-of-the-art DSR methods.



### Studying The Effect of MIL Pooling Filters on MIL Tasks
- **Arxiv ID**: http://arxiv.org/abs/2006.01561v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.01561v1)
- **Published**: 2020-06-02 12:33:03+00:00
- **Updated**: 2020-06-02 12:33:03+00:00
- **Authors**: Mustafa Umit Oner, Jared Marc Song Kye-Jet, Hwee Kuan Lee, Wing-Kin Sung
- **Comment**: 16 pages
- **Journal**: None
- **Summary**: There are different multiple instance learning (MIL) pooling filters used in MIL models. In this paper, we study the effect of different MIL pooling filters on the performance of MIL models in real world MIL tasks. We designed a neural network based MIL framework with 5 different MIL pooling filters: `max', `mean', `attention', `distribution' and `distribution with attention'. We also formulated 5 different MIL tasks on a real world lymph node metastases dataset. We found that the performance of our framework in a task is different for different filters. We also observed that the performances of the five pooling filters are also different from task to task. Hence, the selection of a correct MIL pooling filter for each MIL task is crucial for better performance. Furthermore, we noticed that models with `distribution' and `distribution with attention' pooling filters consistently perform well in almost all of the tasks. We attribute this phenomena to the amount of information captured by `distribution' based pooling filters. While point estimate based pooling filters, like `max' and `mean', produce point estimates of distributions, `distribution' based pooling filters capture the full information in distributions. Lastly, we compared the performance of our neural network model with `distribution' pooling filter with the performance of the best MIL methods in the literature on classical MIL datasets and our model outperformed the others.



### CNNs on Surfaces using Rotation-Equivariant Features
- **Arxiv ID**: http://arxiv.org/abs/2006.01570v1
- **DOI**: 10.1145/3386569.3392437
- **Categories**: **cs.CV**, cs.CG, cs.LG, I.2.10; I.3.5
- **Links**: [PDF](http://arxiv.org/pdf/2006.01570v1)
- **Published**: 2020-06-02 12:46:00+00:00
- **Updated**: 2020-06-02 12:46:00+00:00
- **Authors**: Ruben Wiersma, Elmar Eisemann, Klaus Hildebrandt
- **Comment**: 12 pages, 14 figures, 5 tables, to be published in ACM ToG (SIGGRAPH
  2020)
- **Journal**: None
- **Summary**: This paper is concerned with a fundamental problem in geometric deep learning that arises in the construction of convolutional neural networks on surfaces. Due to curvature, the transport of filter kernels on surfaces results in a rotational ambiguity, which prevents a uniform alignment of these kernels on the surface. We propose a network architecture for surfaces that consists of vector-valued, rotation-equivariant features. The equivariance property makes it possible to locally align features, which were computed in arbitrary coordinate systems, when aggregating features in a convolution layer. The resulting network is agnostic to the choices of coordinate systems for the tangent spaces on the surface. We implement our approach for triangle meshes. Based on circular harmonic functions, we introduce convolution filters for meshes that are rotation-equivariant at the discrete level. We evaluate the resulting networks on shape correspondence and shape classifications tasks and compare their performance to other approaches.



### A Multi-Task Comparator Framework for Kinship Verification
- **Arxiv ID**: http://arxiv.org/abs/2006.01615v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.01615v1)
- **Published**: 2020-06-02 14:00:09+00:00
- **Updated**: 2020-06-02 14:00:09+00:00
- **Authors**: Stefan Hörmann, Martin Knoche, Gerhard Rigoll
- **Comment**: To be published in IEEE FG 2020 - RFIW Workshop
- **Journal**: None
- **Summary**: Approaches for kinship verification often rely on cosine distances between face identification features. However, due to gender bias inherent in these features, it is hard to reliably predict whether two opposite-gender pairs are related. Instead of fine tuning the feature extractor network on kinship verification, we propose a comparator network to cope with this bias. After concatenating both features, cascaded local expert networks extract the information most relevant for their corresponding kinship relation. We demonstrate that our framework is robust against this gender bias and achieves comparable results on two tracks of the RFIW Challenge 2020. Moreover, we show how our framework can be further extended to handle partially known or unknown kinship relations.



### Give Me Something to Eat: Referring Expression Comprehension with Commonsense Knowledge
- **Arxiv ID**: http://arxiv.org/abs/2006.01629v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.01629v2)
- **Published**: 2020-06-02 14:12:43+00:00
- **Updated**: 2020-08-17 11:24:00+00:00
- **Authors**: Peng Wang, Dongyang Liu, Hui Li, Qi Wu
- **Comment**: Accepted by ACMMM2020
- **Journal**: None
- **Summary**: Conventional referring expression comprehension (REF) assumes people to query something from an image by describing its visual appearance and spatial location, but in practice, we often ask for an object by describing its affordance or other non-visual attributes, especially when we do not have a precise target. For example, sometimes we say 'Give me something to eat'. In this case, we need to use commonsense knowledge to identify the objects in the image. Unfortunately, these is no existing referring expression dataset reflecting this requirement, not to mention a model to tackle this challenge. In this paper, we collect a new referring expression dataset, called KB-Ref, containing 43k expressions on 16k images. In KB-Ref, to answer each expression (detect the target object referred by the expression), at least one piece of commonsense knowledge must be required. We then test state-of-the-art (SoTA) REF models on KB-Ref, finding that all of them present a large drop compared to their outstanding performance on general REF datasets. We also present an expression conditioned image and fact attention (ECIFA) network that extract information from correlated image regions and commonsense knowledge facts. Our method leads to a significant improvement over SoTA REF models, although there is still a gap between this strong baseline and human performance. The dataset and baseline models will be released.



### Interpretation of ResNet by Visualization of Preferred Stimulus in Receptive Fields
- **Arxiv ID**: http://arxiv.org/abs/2006.01645v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.01645v2)
- **Published**: 2020-06-02 14:25:26+00:00
- **Updated**: 2020-07-09 11:26:17+00:00
- **Authors**: Genta Kobayashi, Hayaru Shouno
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: One of the methods used in image recognition is the Deep Convolutional Neural Network (DCNN). DCNN is a model in which the expressive power of features is greatly improved by deepening the hidden layer of CNN. The architecture of CNNs is determined based on a model of the visual cortex of mammals. There is a model called Residual Network (ResNet) that has a skip connection. ResNet is an advanced model in terms of the learning method, but it has not been interpreted from a biological viewpoint. In this research, we investigate the receptive fields of a ResNet on the classification task in ImageNet. We find that ResNet has orientation selective neurons and double opponent color neurons. In addition, we suggest that some inactive neurons in the first layer of ResNet affect the classification task.



### Variational Inference and Learning of Piecewise-linear Dynamical Systems
- **Arxiv ID**: http://arxiv.org/abs/2006.01668v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.01668v2)
- **Published**: 2020-06-02 14:40:35+00:00
- **Updated**: 2020-11-02 14:37:07+00:00
- **Authors**: Xavier Alameda-Pineda, Vincent Drouard, Radu Horaud
- **Comment**: Submitted to IEEE Transactions on Neural Networks and Learning
  Systems
- **Journal**: None
- **Summary**: Modeling the temporal behavior of data is of primordial importance in many scientific and engineering fields. Baseline methods assume that both the dynamic and observation equations follow linear-Gaussian models. However, there are many real-world processes that cannot be characterized by a single linear behavior. Alternatively, it is possible to consider a piecewise-linear model which, combined with a switching mechanism, is well suited when several modes of behavior are needed. Nevertheless, switching dynamical systems are intractable because of their computational complexity increases exponentially with time. In this paper, we propose a variational approximation of piecewise linear dynamical systems. We provide full details of the derivation of two variational expectation-maximization algorithms, a filter and a smoother. We show that the model parameters can be split into two sets, static and dynamic parameters, and that the former parameters can be estimated off-line together with the number of linear modes, or the number of states of the switching variable. We apply the proposed method to a visual tracking problem, namely head-pose tracking, and we thoroughly compare our algorithm with several state of the art trackers.



### SeqXFilter: A Memory-efficient Denoising Filter for Dynamic Vision Sensors
- **Arxiv ID**: http://arxiv.org/abs/2006.01687v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.ET
- **Links**: [PDF](http://arxiv.org/pdf/2006.01687v1)
- **Published**: 2020-06-02 15:04:04+00:00
- **Updated**: 2020-06-02 15:04:04+00:00
- **Authors**: Shasha Guo, Lei Wang, Xiaofan Chen, Limeng Zhang, Ziyang Kang, Weixia Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Neuromorphic event-based dynamic vision sensors (DVS) have much faster sampling rates and a higher dynamic range than frame-based imaging sensors. However, they are sensitive to background activity (BA) events that are unwanted. There are some filters for tackling this problem based on spatio-temporal correlation. However, they are either memory-intensive or computing-intensive. We propose \emph{SeqXFilter}, a spatio-temporal correlation filter with only a past event window that has an O(1) space complexity and has simple computations. We explore the spatial correlation of an event with its past few events by analyzing the distribution of the events when applying different functions on the spatial distances. We find the best function to check the spatio-temporal correlation for an event for \emph{SeqXFilter}, best separating real events and noise events. We not only give the visual denoising effect of the filter but also use two metrics for quantitatively analyzing the filter's performance. Four neuromorphic event-based datasets, recorded from four DVS with different output sizes, are used for validation of our method. The experimental results show that \emph{SeqXFilter} achieves similar performance as baseline NNb filters, but with extremely small memory cost and simple computation logic.



### AnalogNet: Convolutional Neural Network Inference on Analog Focal Plane Sensor Processors
- **Arxiv ID**: http://arxiv.org/abs/2006.01765v2
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.01765v2)
- **Published**: 2020-06-02 16:44:43+00:00
- **Updated**: 2020-06-21 17:19:36+00:00
- **Authors**: Matthew Z. Wong, Benoit Guillard, Riku Murai, Sajad Saeedi, Paul H. J. Kelly
- **Comment**: 8 pages, 7 figures
- **Journal**: None
- **Summary**: We present a high-speed, energy-efficient Convolutional Neural Network (CNN) architecture utilising the capabilities of a unique class of devices known as analog Focal Plane Sensor Processors (FPSP), in which the sensor and the processor are embedded together on the same silicon chip. Unlike traditional vision systems, where the sensor array sends collected data to a separate processor for processing, FPSPs allow data to be processed on the imaging device itself. This unique architecture enables ultra-fast image processing and high energy efficiency, at the expense of limited processing resources and approximate computations. In this work, we show how to convert standard CNNs to FPSP code, and demonstrate a method of training networks to increase their robustness to analog computation errors. Our proposed architecture, coined AnalogNet, reaches a testing accuracy of 96.9% on the MNIST handwritten digits recognition task, at a speed of 2260 FPS, for a cost of 0.7 mJ per frame.



### A Novel Nudity Detection Algorithm for Web and Mobile Application Development
- **Arxiv ID**: http://arxiv.org/abs/2006.01780v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2006.01780v2)
- **Published**: 2020-06-02 17:00:47+00:00
- **Updated**: 2020-06-28 15:29:09+00:00
- **Authors**: Rahat Yeasin Emon
- **Comment**: 5 pages
- **Journal**: None
- **Summary**: In our current web and mobile application development runtime nude image content detection is very important. This paper presents a runtime nudity detection method for web and mobile application development. We use two parameters to detect the nude content of an image. One is the number of skin pixels another is face region. A skin color model based on RGB, HSV color spaces are used to detect skin pixels in an image. Google vision api is used to detect the face region. By the percentage of skin regions and face regions an image is identified nude or not. The success of this algorithm exists in detecting skin regions and face regions. The skin detection algorithm can detect skin 95% accurately with a low false-positive rate and the google vision api for web and mobile applications can detect face 99% accurately with less than 1 second time. From the experimental analysis, we have seen that the proposed algorithm can detect 95% percent accurately the nudity of an image.



### Geometric Graph Representations and Geometric Graph Convolutions for Deep Learning on Three-Dimensional (3D) Graphs
- **Arxiv ID**: http://arxiv.org/abs/2006.01785v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.01785v1)
- **Published**: 2020-06-02 17:08:59+00:00
- **Updated**: 2020-06-02 17:08:59+00:00
- **Authors**: Daniel T. Chang
- **Comment**: None
- **Journal**: None
- **Summary**: The geometry of three-dimensional (3D) graphs, consisting of nodes and edges, plays a crucial role in many important applications. An excellent example is molecular graphs, whose geometry influences important properties of a molecule including its reactivity and biological activity. To facilitate the incorporation of geometry in deep learning on 3D graphs, we define three types of geometric graph representations: positional, angle-geometric and distance-geometric. For proof of concept, we use the distance-geometric graph representation for geometric graph convolutions. Further, to utilize standard graph convolution networks, we employ a simple edge weight / edge distance correlation scheme, whose parameters can be fixed using reference values or determined through Bayesian hyperparameter optimization. The results of geometric graph convolutions, for the ESOL and Freesol datasets, show significant improvement over those of standard graph convolutions. Our work demonstrates the feasibility and promise of incorporating geometry, using the distance-geometric graph representation, in deep learning on 3D graphs.



### Shapley Value as Principled Metric for Structured Network Pruning
- **Arxiv ID**: http://arxiv.org/abs/2006.01795v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2006.01795v1)
- **Published**: 2020-06-02 17:26:49+00:00
- **Updated**: 2020-06-02 17:26:49+00:00
- **Authors**: Marco Ancona, Cengiz Öztireli, Markus Gross
- **Comment**: None
- **Journal**: None
- **Summary**: Structured pruning is a well-known technique to reduce the storage size and inference cost of neural networks. The usual pruning pipeline consists of ranking the network internal filters and activations with respect to their contributions to the network performance, removing the units with the lowest contribution, and fine-tuning the network to reduce the harm induced by pruning. Recent results showed that random pruning performs on par with other metrics, given enough fine-tuning resources. In this work, we show that this is not true on a low-data regime when fine-tuning is either not possible or not effective. In this case, reducing the harm caused by pruning becomes crucial to retain the performance of the network. First, we analyze the problem of estimating the contribution of hidden units with tools suggested by cooperative game theory and propose Shapley values as a principled ranking metric for this task. We compare with several alternatives proposed in the literature and discuss how Shapley values are theoretically preferable. Finally, we compare all ranking metrics on the challenging scenario of low-data pruning, where we demonstrate how Shapley values outperform other heuristics.



### Object-Independent Human-to-Robot Handovers using Real Time Robotic Vision
- **Arxiv ID**: http://arxiv.org/abs/2006.01797v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.01797v2)
- **Published**: 2020-06-02 17:29:20+00:00
- **Updated**: 2020-09-21 16:40:13+00:00
- **Authors**: Patrick Rosenberger, Akansel Cosgun, Rhys Newbury, Jun Kwan, Valerio Ortenzi, Peter Corke, Manfred Grafinger
- **Comment**: IEEE Robotics and Automation Letters (RA-L). Preprint Version.
  Accepted September, 2020. The code and videos can be found at
  https://patrosat.github.io/h2r_handovers/
- **Journal**: None
- **Summary**: We present an approach for safe and object-independent human-to-robot handovers using real time robotic vision and manipulation. We aim for general applicability with a generic object detector, a fast grasp selection algorithm and by using a single gripper-mounted RGB-D camera, hence not relying on external sensors. The robot is controlled via visual servoing towards the object of interest. Putting a high emphasis on safety, we use two perception modules: human body part segmentation and hand/finger segmentation. Pixels that are deemed to belong to the human are filtered out from candidate grasp poses, hence ensuring that the robot safely picks the object without colliding with the human partner. The grasp selection and perception modules run concurrently in real-time, which allows monitoring of the progress. In experiments with 13 objects, the robot was able to successfully take the object from the human in 81.9% of the trials.



### Practical sensorless aberration estimation for 3D microscopy with deep learning
- **Arxiv ID**: http://arxiv.org/abs/2006.01804v2
- **DOI**: 10.1364/OE.401933
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.01804v2)
- **Published**: 2020-06-02 17:39:32+00:00
- **Updated**: 2020-07-05 19:17:05+00:00
- **Authors**: Debayan Saha, Uwe Schmidt, Qinrong Zhang, Aurelien Barbotin, Qi Hu, Na Ji, Martin J. Booth, Martin Weigert, Eugene W. Myers
- **Comment**: None
- **Journal**: None
- **Summary**: Estimation of optical aberrations from volumetric intensity images is a key step in sensorless adaptive optics for 3D microscopy. Recent approaches based on deep learning promise accurate results at fast processing speeds. However, collecting ground truth microscopy data for training the network is typically very difficult or even impossible thereby limiting this approach in practice. Here, we demonstrate that neural networks trained only on simulated data yield accurate predictions for real experimental images. We validate our approach on simulated and experimental datasets acquired with two different microscopy modalities, and also compare the results to non-learned methods. Additionally, we study the predictability of individual aberrations with respect to their data requirements and find that the symmetry of the wavefront plays a crucial role. Finally, we make our implementation freely available as open source software in Python.



### Adversarial Item Promotion: Vulnerabilities at the Core of Top-N Recommenders that Use Images to Address Cold Start
- **Arxiv ID**: http://arxiv.org/abs/2006.01888v3
- **DOI**: None
- **Categories**: **cs.IR**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.01888v3)
- **Published**: 2020-06-02 19:12:13+00:00
- **Updated**: 2020-10-20 13:05:48+00:00
- **Authors**: Zhuoran Liu, Martha Larson
- **Comment**: Our code is available at https://github.com/liuzrcc/AIP
- **Journal**: None
- **Summary**: E-commerce platforms provide their customers with ranked lists of recommended items matching the customers' preferences. Merchants on e-commerce platforms would like their items to appear as high as possible in the top-N of these ranked lists. In this paper, we demonstrate how unscrupulous merchants can create item images that artificially promote their products, improving their rankings. Recommender systems that use images to address the cold start problem are vulnerable to this security risk. We describe a new type of attack, Adversarial Item Promotion (AIP), that strikes directly at the core of Top-N recommenders: the ranking mechanism itself. Existing work on adversarial images in recommender systems investigates the implications of conventional attacks, which target deep learning classifiers. In contrast, our AIP attacks are embedding attacks that seek to push features representations in a way that fools the ranker (not a classifier) and directly lead to item promotion. We introduce three AIP attacks insider attack, expert attack, and semantic attack, which are defined with respect to three successively more realistic attack models. Our experiments evaluate the danger of these attacks when mounted against three representative visually-aware recommender algorithms in a framework that uses images to address cold start. We also evaluate potential defenses, including adversarial training and find that common, currently-existing, techniques do not eliminate the danger of AIP attacks. In sum, we show that using images to address cold start opens recommender systems to potential threats with clear practical implications.



### Learning to Branch for Multi-Task Learning
- **Arxiv ID**: http://arxiv.org/abs/2006.01895v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.01895v2)
- **Published**: 2020-06-02 19:23:21+00:00
- **Updated**: 2020-06-09 05:18:55+00:00
- **Authors**: Pengsheng Guo, Chen-Yu Lee, Daniel Ulbricht
- **Comment**: Accepted at ICML 2020
- **Journal**: None
- **Summary**: Training multiple tasks jointly in one deep network yields reduced latency during inference and better performance over the single-task counterpart by sharing certain layers of a network. However, over-sharing a network could erroneously enforce over-generalization, causing negative knowledge transfer across tasks. Prior works rely on human intuition or pre-computed task relatedness scores for ad hoc branching structures. They provide sub-optimal end results and often require huge efforts for the trial-and-error process. In this work, we present an automated multi-task learning algorithm that learns where to share or branch within a network, designing an effective network topology that is directly optimized for multiple objectives across tasks. Specifically, we propose a novel tree-structured design space that casts a tree branching operation as a gumbel-softmax sampling procedure. This enables differentiable network splitting that is end-to-end trainable. We validate the proposed method on controlled synthetic data, CelebA, and Taskonomy.



### Automatic Differentiation for All Photons Imaging to See Inside Volumetric Scattering Media
- **Arxiv ID**: http://arxiv.org/abs/2006.01897v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.01897v1)
- **Published**: 2020-06-02 19:24:28+00:00
- **Updated**: 2020-06-02 19:24:28+00:00
- **Authors**: Tomohiro Maeda, Ankit Ranjan, Ramesh Raskar
- **Comment**: None
- **Journal**: None
- **Summary**: Imaging through dense scattering media - such as biological tissue, fog, and smoke - has applications in the medical and robotics fields. We propose a new framework using automatic differentiation for All Photons Imaging through homogeneous scattering media with unknown optical properties for non-invasive sensing and diagnostics. We overcome the need for the imaging target to be visible to the illumination source in All Photons Imaging, enabling practical and non-invasive imaging through turbid media with a simple optical setup. Our method does not require calibration to acquire the sensor position or optical properties of the media.



### The Convolution Exponential and Generalized Sylvester Flows
- **Arxiv ID**: http://arxiv.org/abs/2006.01910v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.01910v2)
- **Published**: 2020-06-02 19:43:36+00:00
- **Updated**: 2020-10-26 10:24:08+00:00
- **Authors**: Emiel Hoogeboom, Victor Garcia Satorras, Jakub M. Tomczak, Max Welling
- **Comment**: Accepted to Neural Information Processing Systems (NeurIPS) 2020
- **Journal**: None
- **Summary**: This paper introduces a new method to build linear flows, by taking the exponential of a linear transformation. This linear transformation does not need to be invertible itself, and the exponential has the following desirable properties: it is guaranteed to be invertible, its inverse is straightforward to compute and the log Jacobian determinant is equal to the trace of the linear transformation. An important insight is that the exponential can be computed implicitly, which allows the use of convolutional layers. Using this insight, we develop new invertible transformations named convolution exponentials and graph convolution exponentials, which retain the equivariance of their underlying transformations. In addition, we generalize Sylvester Flows and propose Convolutional Sylvester Flows which are based on the generalization and the convolution exponential as basis change. Empirically, we show that the convolution exponential outperforms other linear transformations in generative flows on CIFAR10 and the graph convolution exponential improves the performance of graph normalizing flows. In addition, we show that Convolutional Sylvester Flows improve performance over residual flows as a generative flow model measured in log-likelihood.



### Ear2Face: Deep Biometric Modality Mapping
- **Arxiv ID**: http://arxiv.org/abs/2006.01943v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.01943v1)
- **Published**: 2020-06-02 21:14:27+00:00
- **Updated**: 2020-06-02 21:14:27+00:00
- **Authors**: Dogucan Yaman, Fevziye Irem Eyiokur, Hazım Kemal Ekenel
- **Comment**: 13 pages, 4 figures
- **Journal**: None
- **Summary**: In this paper, we explore the correlation between different visual biometric modalities. For this purpose, we present an end-to-end deep neural network model that learns a mapping between the biometric modalities. Namely, our goal is to generate a frontal face image of a subject given his/her ear image as the input. We formulated the problem as a paired image-to-image translation task and collected datasets of ear and face image pairs from the Multi-PIE and FERET datasets to train our GAN-based models. We employed feature reconstruction and style reconstruction losses in addition to adversarial and pixel losses. We evaluated the proposed method both in terms of reconstruction quality and in terms of person identification accuracy. To assess the generalization capability of the learned mapping models, we also run cross-dataset experiments. That is, we trained the model on the FERET dataset and tested it on the Multi-PIE dataset and vice versa. We have achieved very promising results, especially on the FERET dataset, generating visually appealing face images from ear image inputs. Moreover, we attained a very high cross-modality person identification performance, for example, reaching 90.9% Rank-10 identification accuracy on the FERET dataset.



### Continual Learning of Predictive Models in Video Sequences via Variational Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/2006.01945v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.01945v1)
- **Published**: 2020-06-02 21:17:38+00:00
- **Updated**: 2020-06-02 21:17:38+00:00
- **Authors**: Damian Campo, Giulia Slavic, Mohamad Baydoun, Lucio Marcenaro, Carlo Regazzoni
- **Comment**: Manuscript accepted at the 27th IEEE International Conference on
  Image Processing (ICIP 2020)
- **Journal**: None
- **Summary**: This paper proposes a method for performing continual learning of predictive models that facilitate the inference of future frames in video sequences. For a first given experience, an initial Variational Autoencoder, together with a set of fully connected neural networks are utilized to respectively learn the appearance of video frames and their dynamics at the latent space level. By employing an adapted Markov Jump Particle Filter, the proposed method recognizes new situations and integrates them as predictive models avoiding catastrophic forgetting of previously learned tasks. For evaluating the proposed method, this article uses video sequences from a vehicle that performs different tasks in a controlled environment.



### NewtonianVAE: Proportional Control and Goal Identification from Pixels via Physical Latent Spaces
- **Arxiv ID**: http://arxiv.org/abs/2006.01959v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.01959v2)
- **Published**: 2020-06-02 21:41:38+00:00
- **Updated**: 2021-04-26 21:49:30+00:00
- **Authors**: Miguel Jaques, Michael Burke, Timothy Hospedales
- **Comment**: None
- **Journal**: None
- **Summary**: Learning low-dimensional latent state space dynamics models has been a powerful paradigm for enabling vision-based planning and learning for control. We introduce a latent dynamics learning framework that is uniquely designed to induce proportional controlability in the latent space, thus enabling the use of much simpler controllers than prior work. We show that our learned dynamics model enables proportional control from pixels, dramatically simplifies and accelerates behavioural cloning of vision-based controllers, and provides interpretable goal discovery when applied to imitation learning of switching controllers from demonstration.



### From two rolling shutters to one global shutter
- **Arxiv ID**: http://arxiv.org/abs/2006.01964v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.01964v1)
- **Published**: 2020-06-02 22:18:43+00:00
- **Updated**: 2020-06-02 22:18:43+00:00
- **Authors**: Cenek Albl, Zuzana Kukelova, Viktor Larsson, Tomas Pajdla, Konrad Schindler
- **Comment**: CVPR 2020
- **Journal**: None
- **Summary**: Most consumer cameras are equipped with electronic rolling shutter, leading to image distortions when the camera moves during image capture. We explore a surprisingly simple camera configuration that makes it possible to undo the rolling shutter distortion: two cameras mounted to have different rolling shutter directions. Such a setup is easy and cheap to build and it possesses the geometric constraints needed to correct rolling shutter distortion using only a sparse set of point correspondences between the two images. We derive equations that describe the underlying geometry for general and special motions and present an efficient method for finding their solutions. Our synthetic and real experiments demonstrate that our approach is able to remove large rolling shutter distortions of all types without relying on any specific scene structure.



### Grafted network for person re-identification
- **Arxiv ID**: http://arxiv.org/abs/2006.01967v2
- **DOI**: 10.1016/j.image.2019.115674
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.01967v2)
- **Published**: 2020-06-02 22:33:44+00:00
- **Updated**: 2020-06-06 05:25:28+00:00
- **Authors**: Jiabao Wang, Yang Li, Shanshan Jiao, Zhuang Miao, Rui Zhang
- **Comment**: None
- **Journal**: Signal Processing: Image Communication 80(2020)115674
- **Summary**: Convolutional neural networks have shown outstanding effectiveness in person re-identification (re-ID). However, the models always have large number of parameters and much computation for mobile application. In order to relieve this problem, we propose a novel grafted network (GraftedNet), which is designed by grafting a high-accuracy rootstock and a light-weighted scion. The rootstock is based on the former parts of ResNet-50 to provide a strong baseline, while the scion is a new designed module, composed of the latter parts of SqueezeNet, to compress the parameters. To extract more discriminative feature representation, a joint multi-level and part-based feature is proposed. In addition, to train GraftedNet efficiently, we propose an accompanying learning method, by adding an accompanying branch to train the model in training and removing it in testing for saving parameters and computation. On three public person re-ID benchmarks (Market1501, DukeMTMC-reID and CUHK03), the effectiveness of GraftedNet are evaluated and its components are analyzed. Experimental results show that the proposed GraftedNet achieves 93.02%, 85.3% and 76.2% in Rank-1 and 81.6%, 74.7% and 71.6% in mAP, with only 4.6M parameters.



### Quantifying the Uncertainty in Model Parameters Using Gaussian Process-Based Markov Chain Monte Carlo: An Application to Cardiac Electrophysiological Models
- **Arxiv ID**: http://arxiv.org/abs/2006.01983v1
- **DOI**: 10.1007/978-3-319-59050-9_18
- **Categories**: **stat.ML**, cs.CV, cs.LG, stat.AP
- **Links**: [PDF](http://arxiv.org/pdf/2006.01983v1)
- **Published**: 2020-06-02 23:48:15+00:00
- **Updated**: 2020-06-02 23:48:15+00:00
- **Authors**: Jwala Dhamala, John L. Sapp, B. Milan Horácek, Linwei Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Estimation of patient-specific model parameters is important for personalized modeling, although sparse and noisy clinical data can introduce significant uncertainty in the estimated parameter values. This importance source of uncertainty, if left unquantified, will lead to unknown variability in model outputs that hinder their reliable adoptions. Probabilistic estimation model parameters, however, remains an unresolved challenge because standard Markov Chain Monte Carlo sampling requires repeated model simulations that are computationally infeasible. A common solution is to replace the simulation model with a computationally-efficient surrogate for a faster sampling. However, by sampling from an approximation of the exact posterior probability density function (pdf) of the parameters, the efficiency is gained at the expense of sampling accuracy. In this paper, we address this issue by integrating surrogate modeling into Metropolis Hasting (MH) sampling of the exact posterior pdfs to improve its acceptance rate. It is done by first quickly constructing a Gaussian process (GP) surrogate of the exact posterior pdfs using deterministic optimization. This efficient surrogate is then used to modify commonly-used proposal distributions in MH sampling such that only proposals accepted by the surrogate will be tested by the exact posterior pdf for acceptance/rejection, reducing unnecessary model simulations at unlikely candidates. Synthetic and real-data experiments using the presented method show a significant gain in computational efficiency without compromising the accuracy. In addition, insights into the non-identifiability and heterogeneity of tissue properties can be gained from the obtained posterior distributions.



