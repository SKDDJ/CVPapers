# Arxiv Papers in cs.CV on 2020-06-06
### Simple Primary Colour Editing for Consumer Product Images
- **Arxiv ID**: http://arxiv.org/abs/2006.03743v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2006.03743v1)
- **Published**: 2020-06-06 00:24:56+00:00
- **Updated**: 2020-06-06 00:24:56+00:00
- **Authors**: Han Gong, Luwen Yu, Stephen Westland
- **Comment**: This is a working paper (pre-print). Code available at
  https://github.com/hangong/prod_recolor/
- **Journal**: None
- **Summary**: We present a simple primary colour editing method for consumer product images. We show that by using colour correction and colour blending, we can automate the pain-staking colour editing task and save time for consumer colour preference researchers. To improve the colour harmony between the primary colour and its complementary colours, our algorithm also tunes the other colours in the image. Preliminary experiment has shown some promising results compared with a state-of-the-art method and human editing.



### Auxiliary Signal-Guided Knowledge Encoder-Decoder for Medical Report Generation
- **Arxiv ID**: http://arxiv.org/abs/2006.03744v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.03744v1)
- **Published**: 2020-06-06 01:00:15+00:00
- **Updated**: 2020-06-06 01:00:15+00:00
- **Authors**: Mingjie Li, Fuyu Wang, Xiaojun Chang, Xiaodan Liang
- **Comment**: 11 pages, 3 figures
- **Journal**: None
- **Summary**: Beyond the common difficulties faced in the natural image captioning, medical report generation specifically requires the model to describe a medical image with a fine-grained and semantic-coherence paragraph that should satisfy both medical commonsense and logic. Previous works generally extract the global image features and attempt to generate a paragraph that is similar to referenced reports; however, this approach has two limitations. Firstly, the regions of primary interest to radiologists are usually located in a small area of the global image, meaning that the remainder parts of the image could be considered as irrelevant noise in the training procedure. Secondly, there are many similar sentences used in each medical report to describe the normal regions of the image, which causes serious data bias. This deviation is likely to teach models to generate these inessential sentences on a regular basis. To address these problems, we propose an Auxiliary Signal-Guided Knowledge Encoder-Decoder (ASGK) to mimic radiologists' working patterns. In more detail, ASGK integrates internal visual feature fusion and external medical linguistic information to guide medical knowledge transfer and learning. The core structure of ASGK consists of a medical graph encoder and a natural language decoder, inspired by advanced Generative Pre-Training (GPT). Experiments on the CX-CHR dataset and our COVID-19 CT Report dataset demonstrate that our proposed ASGK is able to generate a robust and accurate report, and moreover outperforms state-of-the-art methods on both medical terminology classification and paragraph generation metrics.



### GRNet: Gridding Residual Network for Dense Point Cloud Completion
- **Arxiv ID**: http://arxiv.org/abs/2006.03761v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.03761v4)
- **Published**: 2020-06-06 02:46:39+00:00
- **Updated**: 2020-07-20 11:22:05+00:00
- **Authors**: Haozhe Xie, Hongxun Yao, Shangchen Zhou, Jiageng Mao, Shengping Zhang, Wenxiu Sun
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: Estimating the complete 3D point cloud from an incomplete one is a key problem in many vision and robotics applications. Mainstream methods (e.g., PCN and TopNet) use Multi-layer Perceptrons (MLPs) to directly process point clouds, which may cause the loss of details because the structural and context of point clouds are not fully considered. To solve this problem, we introduce 3D grids as intermediate representations to regularize unordered point clouds. We therefore propose a novel Gridding Residual Network (GRNet) for point cloud completion. In particular, we devise two novel differentiable layers, named Gridding and Gridding Reverse, to convert between point clouds and 3D grids without losing structural information. We also present the differentiable Cubic Feature Sampling layer to extract features of neighboring points, which preserves context information. In addition, we design a new loss function, namely Gridding Loss, to calculate the L1 distance between the 3D grids of the predicted and ground truth point clouds, which is helpful to recover details. Experimental results indicate that the proposed GRNet performs favorably against state-of-the-art methods on the ShapeNet, Completion3D, and KITTI benchmarks.



### Deep Octree-based CNNs with Output-Guided Skip Connections for 3D Shape and Scene Completion
- **Arxiv ID**: http://arxiv.org/abs/2006.03762v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2006.03762v1)
- **Published**: 2020-06-06 02:51:26+00:00
- **Updated**: 2020-06-06 02:51:26+00:00
- **Authors**: Peng-Shuai Wang, Yang Liu, Xin Tong
- **Comment**: None
- **Journal**: None
- **Summary**: Acquiring complete and clean 3D shape and scene data is challenging due to geometric occlusion and insufficient views during 3D capturing. We present a simple yet effective deep learning approach for completing the input noisy and incomplete shapes or scenes. Our network is built upon the octree-based CNNs (O-CNN) with U-Net like structures, which enjoys high computational and memory efficiency and supports to construct a very deep network structure for 3D CNNs. A novel output-guided skip-connection is introduced to the network structure for better preserving the input geometry and learning geometry prior from data effectively. We show that with these simple adaptions -- output-guided skip-connection and deeper O-CNN (up to 70 layers), our network achieves state-of-the-art results in 3D shape completion and semantic scene computation.



### MAGNet: Multi-Region Attention-Assisted Grounding of Natural Language Queries at Phrase Level
- **Arxiv ID**: http://arxiv.org/abs/2006.03776v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.03776v1)
- **Published**: 2020-06-06 04:14:15+00:00
- **Updated**: 2020-06-06 04:14:15+00:00
- **Authors**: Amar Shrestha, Krittaphat Pugdeethosapol, Haowen Fang, Qinru Qiu
- **Comment**: Submitted to The 2020 European Conference on Computer Vision (ECCV
  2020)
- **Journal**: None
- **Summary**: Grounding free-form textual queries necessitates an understanding of these textual phrases and its relation to the visual cues to reliably reason about the described locations. Spatial attention networks are known to learn this relationship and focus its gaze on salient objects in the image. Thus, we propose to utilize spatial attention networks for image-level visual-textual fusion preserving local (word) and global (phrase) information to refine region proposals with an in-network Region Proposal Network (RPN) and detect single or multiple regions for a phrase query. We focus only on the phrase query - ground truth pair (referring expression) for a model independent of the constraints of the datasets i.e. additional attributes, context etc. For such referring expression dataset ReferIt game, our Multi-region Attention-assisted Grounding network (MAGNet) achieves over 12\% improvement over the state-of-the-art. Without the context from image captions and attribute information in Flickr30k Entities, we still achieve competitive results compared to the state-of-the-art.



### No-Reference Image Quality Assessment via Feature Fusion and Multi-Task Learning
- **Arxiv ID**: http://arxiv.org/abs/2006.03783v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.03783v1)
- **Published**: 2020-06-06 05:04:10+00:00
- **Updated**: 2020-06-06 05:04:10+00:00
- **Authors**: S. Alireza Golestaneh, Kris Kitani
- **Comment**: None
- **Journal**: None
- **Summary**: Blind or no-reference image quality assessment (NR-IQA) is a fundamental, unsolved, and yet challenging problem due to the unavailability of a reference image. It is vital to the streaming and social media industries that impact billions of viewers daily. Although previous NR-IQA methods leveraged different feature extraction approaches, the performance bottleneck still exists. In this paper, we propose a simple and yet effective general-purpose no-reference (NR) image quality assessment (IQA) framework based on multi-task learning. Our model employs distortion types as well as subjective human scores to predict image quality. We propose a feature fusion method to utilize distortion information to improve the quality score estimation task. In our experiments, we demonstrate that by utilizing multi-task learning and our proposed feature fusion method, our model yields better performance for the NR-IQA task. To demonstrate the effectiveness of our approach, we test our approach on seven standard datasets and show that we achieve state-of-the-art results on various datasets.



### Multi-Task Temporal Shift Attention Networks for On-Device Contactless Vitals Measurement
- **Arxiv ID**: http://arxiv.org/abs/2006.03790v2
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.03790v2)
- **Published**: 2020-06-06 06:31:24+00:00
- **Updated**: 2021-02-28 20:50:15+00:00
- **Authors**: Xin Liu, Josh Fromm, Shwetak Patel, Daniel McDuff
- **Comment**: preprint
- **Journal**: None
- **Summary**: Telehealth and remote health monitoring have become increasingly important during the SARS-CoV-2 pandemic and it is widely expected that this will have a lasting impact on healthcare practices. These tools can help reduce the risk of exposing patients and medical staff to infection, make healthcare services more accessible, and allow providers to see more patients. However, objective measurement of vital signs is challenging without direct contact with a patient. We present a video-based and on-device optical cardiopulmonary vital sign measurement approach. It leverages a novel multi-task temporal shift convolutional attention network (MTTS-CAN) and enables real-time cardiovascular and respiratory measurements on mobile platforms. We evaluate our system on an Advanced RISC Machine (ARM) CPU and achieve state-of-the-art accuracy while running at over 150 frames per second which enables real-time applications. Systematic experimentation on large benchmark datasets reveals that our approach leads to substantial (20%-50%) reductions in error and generalizes well across datasets.



### Deep Mining External Imperfect Data for Chest X-ray Disease Screening
- **Arxiv ID**: http://arxiv.org/abs/2006.03796v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.03796v1)
- **Published**: 2020-06-06 06:48:40+00:00
- **Updated**: 2020-06-06 06:48:40+00:00
- **Authors**: Luyang Luo, Lequan Yu, Hao Chen, Quande Liu, Xi Wang, Jiaqi Xu, Pheng-Ann Heng
- **Comment**: Accepted to IEEE Transactions on Medical Imaging
- **Journal**: None
- **Summary**: Deep learning approaches have demonstrated remarkable progress in automatic Chest X-ray analysis. The data-driven feature of deep models requires training data to cover a large distribution. Therefore, it is substantial to integrate knowledge from multiple datasets, especially for medical images. However, learning a disease classification model with extra Chest X-ray (CXR) data is yet challenging. Recent researches have demonstrated that performance bottleneck exists in joint training on different CXR datasets, and few made efforts to address the obstacle. In this paper, we argue that incorporating an external CXR dataset leads to imperfect training data, which raises the challenges. Specifically, the imperfect data is in two folds: domain discrepancy, as the image appearances vary across datasets; and label discrepancy, as different datasets are partially labeled. To this end, we formulate the multi-label thoracic disease classification problem as weighted independent binary tasks according to the categories. For common categories shared across domains, we adopt task-specific adversarial training to alleviate the feature differences. For categories existing in a single dataset, we present uncertainty-aware temporal ensembling of model predictions to mine the information from the missing labels further. In this way, our framework simultaneously models and tackles the domain and label discrepancies, enabling superior knowledge mining ability. We conduct extensive experiments on three datasets with more than 360,000 Chest X-ray images. Our method outperforms other competing models and sets state-of-the-art performance on the official NIH test set with 0.8349 AUC, demonstrating its effectiveness of utilizing the external dataset to improve the internal classification.



### Extracting Cellular Location of Human Proteins Using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2006.03800v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.03800v1)
- **Published**: 2020-06-06 07:15:11+00:00
- **Updated**: 2020-06-06 07:15:11+00:00
- **Authors**: Hanke Chen
- **Comment**: 5 page, 10 figures
- **Journal**: None
- **Summary**: Understanding and extracting the patterns of microscopy images has been a major challenge in the biomedical field. Although trained scientists can locate the proteins of interest within a human cell, this procedure is not efficient and accurate enough to process a large amount of data and it often leads to bias. To resolve this problem, we attempted to create an automatic image classifier using Machine Learning to locate human proteins with higher speed and accuracy than human beings. We implemented a Convolution Neural Network with Residue and Squeeze-Excitation layers classifier to locate given proteins of any type in a subcellular structure. After training the model using a series of techniques, it can locate thousands of proteins in 27 different human cell types into 28 subcellular locations, way significant than historical approaches. The model can classify 4,500 images per minute with an accuracy of 63.07%, surpassing human performance in accuracy (by 35%) and speed. Because our system can be implemented on different cell types, it opens a new vision of understanding in the biomedical field. From the locational information of the human proteins, doctors can easily detect cell's abnormal behaviors including viral infection, pathogen invasion, and malignant tumor development. Given the amount of data generalized by experiments are greater than that human can analyze, the model cut down the human resources and time needed to analyze data. Moreover, this locational information can be used in different scenarios like subcellular engineering, medical care, and etiology inspection.



### Learning Diagnosis of COVID-19 from a Single Radiological Image
- **Arxiv ID**: http://arxiv.org/abs/2006.12220v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.12220v1)
- **Published**: 2020-06-06 07:41:28+00:00
- **Updated**: 2020-06-06 07:41:28+00:00
- **Authors**: Pengyi Zhang, Yunxin Zhong, Xiaoying Tang, Yunlin Deng, Xiaoqiong Li
- **Comment**: None
- **Journal**: None
- **Summary**: Radiological image is currently adopted as the visual evidence for COVID-19 diagnosis in clinical. Using deep models to realize automated infection measurement and COVID-19 diagnosis is important for faster examination based on radiological imaging. Unfortunately, collecting large training data systematically in the early stage is difficult. To address this problem, we explore the feasibility of learning deep models for COVID-19 diagnosis from a single radiological image by resorting to synthesizing diverse radiological images. Specifically, we propose a novel conditional generative model, called CoSinGAN, which can be learned from a single radiological image with a given condition, i.e., the annotations of the lung and COVID-19 infection. Our CoSinGAN is able to capture the conditional distribution of visual finds of COVID-19 infection, and further synthesize diverse and high-resolution radiological images that match the input conditions precisely. Both deep classification and segmentation networks trained on synthesized samples from CoSinGAN achieve notable detection accuracy of COVID-19 infection. Such results are significantly better than the counterparts trained on the same extremely small number of real samples (1 or 2 real samples) by using strong data augmentation, and approximate to the counterparts trained on large dataset (2846 real images). It confirms our method can significantly reduce the performance gap between deep models trained on extremely small dataset and on large dataset, and thus has the potential to realize learning COVID-19 diagnosis from few radiological images in the early stage of COVID-19 pandemic. Our codes are made publicly available at https://github.com/PengyiZhang/CoSinGAN.



### An Empirical Analysis of the Impact of Data Augmentation on Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/2006.03810v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.03810v2)
- **Published**: 2020-06-06 08:20:48+00:00
- **Updated**: 2020-06-09 13:01:00+00:00
- **Authors**: Deepan Das, Haley Massa, Abhimanyu Kulkarni, Theodoros Rekatsinas
- **Comment**: None
- **Journal**: None
- **Summary**: Generalization Performance of Deep Learning models trained using Empirical Risk Minimization can be improved significantly by using Data Augmentation strategies such as simple transformations, or using Mixed Samples. We attempt to empirically analyze the impact of such strategies on the transfer of generalization between teacher and student models in a distillation setup. We observe that if a teacher is trained using any of the mixed sample augmentation strategies, such as MixUp or CutMix, the student model distilled from it is impaired in its generalization capabilities. We hypothesize that such strategies limit a model's capability to learn example-specific features, leading to a loss in quality of the supervision signal during distillation. We present a novel Class-Discrimination metric to quantitatively measure this dichotomy in performance and link it to the discriminative capacity induced by the different strategies on a network's latent space.



### UCLID-Net: Single View Reconstruction in Object Space
- **Arxiv ID**: http://arxiv.org/abs/2006.03817v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.03817v2)
- **Published**: 2020-06-06 09:15:56+00:00
- **Updated**: 2020-06-16 12:11:18+00:00
- **Authors**: Benoit Guillard, Edoardo Remelli, Pascal Fua
- **Comment**: Added supplementary material
- **Journal**: None
- **Summary**: Most state-of-the-art deep geometric learning single-view reconstruction approaches rely on encoder-decoder architectures that output either shape parametrizations or implicit representations. However, these representations rarely preserve the Euclidean structure of the 3D space objects exist in. In this paper, we show that building a geometry preserving 3-dimensional latent space helps the network concurrently learn global shape regularities and local reasoning in the object coordinate space and, as a result, boosts performance. We demonstrate both on ShapeNet synthetic images, which are often used for benchmarking purposes, and on real-world images that our approach outperforms state-of-the-art ones. Furthermore, the single-view pipeline naturally extends to multi-view reconstruction, which we also show.



### 3D Self-Supervised Methods for Medical Imaging
- **Arxiv ID**: http://arxiv.org/abs/2006.03829v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.03829v3)
- **Published**: 2020-06-06 09:56:58+00:00
- **Updated**: 2020-11-02 10:43:10+00:00
- **Authors**: Aiham Taleb, Winfried Loetzsch, Noel Danz, Julius Severin, Thomas Gaertner, Benjamin Bergner, Christoph Lippert
- **Comment**: Proceedings of NeurIPS 2020 (Please cite the proceedings version).
  For source code, see https://github.com/HealthML/self-supervised-3d-tasks
- **Journal**: None
- **Summary**: Self-supervised learning methods have witnessed a recent surge of interest after proving successful in multiple application fields. In this work, we leverage these techniques, and we propose 3D versions for five different self-supervised methods, in the form of proxy tasks. Our methods facilitate neural network feature learning from unlabeled 3D images, aiming to reduce the required cost for expert annotation. The developed algorithms are 3D Contrastive Predictive Coding, 3D Rotation prediction, 3D Jigsaw puzzles, Relative 3D patch location, and 3D Exemplar networks. Our experiments show that pretraining models with our 3D tasks yields more powerful semantic representations, and enables solving downstream tasks more accurately and efficiently, compared to training the models from scratch and to pretraining them on 2D slices. We demonstrate the effectiveness of our methods on three downstream tasks from the medical imaging domain: i) Brain Tumor Segmentation from 3D MRI, ii) Pancreas Tumor Segmentation from 3D CT, and iii) Diabetic Retinopathy Detection from 2D Fundus images. In each task, we assess the gains in data-efficiency, performance, and speed of convergence. Interestingly, we also find gains when transferring the learned representations, by our methods, from a large unlabeled 3D corpus to a small downstream-specific dataset. We achieve results competitive to state-of-the-art solutions at a fraction of the computational expense. We publish our implementations for the developed algorithms (both 3D and 2D versions) as an open-source library, in an effort to allow other researchers to apply and extend our methods on their datasets.



### A Sparse and Locally Coherent Morphable Face Model for Dense Semantic Correspondence Across Heterogeneous 3D Faces
- **Arxiv ID**: http://arxiv.org/abs/2006.03840v3
- **DOI**: 10.1109/TPAMI.2021.3090942
- **Categories**: **cs.CV**, cs.CG
- **Links**: [PDF](http://arxiv.org/pdf/2006.03840v3)
- **Published**: 2020-06-06 10:52:07+00:00
- **Updated**: 2021-06-24 13:52:18+00:00
- **Authors**: Claudio Ferrari, Stefano Berretti, Pietro Pala, Alberto Del Bimbo
- **Comment**: Accepted for publication in IEEE Transactions on Pattern Analysis and
  Machine Intelligence (TPAMI)
- **Journal**: None
- **Summary**: The 3D Morphable Model (3DMM) is a powerful statistical tool for representing 3D face shapes. To build a 3DMM, a training set of face scans in full point-to-point correspondence is required, and its modeling capabilities directly depend on the variability contained in the training data. Thus, to increase the descriptive power of the 3DMM, establishing a dense correspondence across heterogeneous scans with sufficient diversity in terms of identities, ethnicities, or expressions becomes essential. In this manuscript, we present a fully automatic approach that leverages a 3DMM to transfer its dense semantic annotation across raw 3D faces, establishing a dense correspondence between them. We propose a novel formulation to learn a set of sparse deformation components with local support on the face that, together with an original non-rigid deformation algorithm, allow the 3DMM to precisely fit unseen faces and transfer its semantic annotation. We extensively experimented our approach, showing it can effectively generalize to highly diverse samples and accurately establish a dense correspondence even in presence of complex facial expressions. The accuracy of the dense registration is demonstrated by building a heterogeneous, large-scale 3DMM from more than 9,000 fully registered scans obtained by joining three large datasets together.



### Instance segmentation of buildings using keypoints
- **Arxiv ID**: http://arxiv.org/abs/2006.03858v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.03858v1)
- **Published**: 2020-06-06 13:11:37+00:00
- **Updated**: 2020-06-06 13:11:37+00:00
- **Authors**: Qingyu Li, Lichao Mou, Yuansheng Hua, Yao Sun, Pu Jin, Yilei Shi, Xiao Xiang Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: Building segmentation is of great importance in the task of remote sensing imagery interpretation. However, the existing semantic segmentation and instance segmentation methods often lead to segmentation masks with blurred boundaries. In this paper, we propose a novel instance segmentation network for building segmentation in high-resolution remote sensing images. More specifically, we consider segmenting an individual building as detecting several keypoints. The detected keypoints are subsequently reformulated as a closed polygon, which is the semantic boundary of the building. By doing so, the sharp boundary of the building could be preserved. Experiments are conducted on selected Aerial Imagery for Roof Segmentation (AIRS) dataset, and our method achieves better performance in both quantitative and qualitative results with comparison to the state-of-the-art methods. Our network is a bottom-up instance segmentation method that could well preserve geometric details.



### Towards large-scale, automated, accurate detection of CCTV camera objects using computer vision. Applications and implications for privacy, safety, and cybersecurity. (Preprint)
- **Arxiv ID**: http://arxiv.org/abs/2006.03870v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/2006.03870v3)
- **Published**: 2020-06-06 13:49:09+00:00
- **Updated**: 2021-08-20 21:02:28+00:00
- **Authors**: Hannu Turtiainen, Andrei Costin, Tuomo Lahtinen, Lauri Sintonen, Timo Hamalainen
- **Comment**: None
- **Journal**: None
- **Summary**: In order to withstand the ever-increasing invasion of privacy by CCTV cameras and technologies, on par CCTV-aware solutions must exist that provide privacy, safety, and cybersecurity features. We argue that a first important step towards such CCTV-aware solutions must be a mapping system (e.g., Google Maps, OpenStreetMap) that provides both privacy and safety routing and navigation options. However, this in turn requires that the mapping system contains updated information on CCTV cameras' exact geo-location, coverage area, and possibly other meta-data (e.g., resolution, facial recognition features, operator). Such information is however missing from current mapping systems, and there are several ways to fix this. One solution is to perform CCTV camera detection on geo-location tagged images, e.g., street view imagery on various platforms, user images publicly posted in image sharing platforms such as Flickr. Unfortunately, to the best of our knowledge, there are no computer vision models for CCTV camera object detection as well as no mapping system that supports privacy and safety routing options.   To close these gaps, with this paper we introduce CCTVCV -- the first and only computer vision MS COCO-compatible models that are able to accurately detect CCTV and video surveillance cameras in images and video frames. To this end, our best detectors were built using 8387 images that were manually reviewed and annotated to contain 10419 CCTV camera instances, and achieve an accuracy of up to 98.7%. Moreover, we build and evaluate multiple models, present a comprehensive comparison of their performance, and outline core challenges associated with such research.



### MMA Regularization: Decorrelating Weights of Neural Networks by Maximizing the Minimal Angles
- **Arxiv ID**: http://arxiv.org/abs/2006.06527v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.06527v2)
- **Published**: 2020-06-06 14:03:16+00:00
- **Updated**: 2021-03-23 09:14:49+00:00
- **Authors**: Zhennan Wang, Canqun Xiang, Wenbin Zou, Chen Xu
- **Comment**: NeurIPS2020
- **Journal**: https://proceedings.neurips.cc/paper/2020/file/dcd2f3f312b6705fb06f4f9f1b55b55c-Paper.pdf
- **Summary**: The strong correlation between neurons or filters can significantly weaken the generalization ability of neural networks. Inspired by the well-known Tammes problem, we propose a novel diversity regularization method to address this issue, which makes the normalized weight vectors of neurons or filters distributed on a hypersphere as uniformly as possible, through maximizing the minimal pairwise angles (MMA). This method can easily exert its effect by plugging the MMA regularization term into the loss function with negligible computational overhead. The MMA regularization is simple, efficient, and effective. Therefore, it can be used as a basic regularization method in neural network training. Extensive experiments demonstrate that MMA regularization is able to enhance the generalization ability of various modern models and achieves considerable performance improvements on CIFAR100 and TinyImageNet datasets. In addition, experiments on face verification show that MMA regularization is also effective for feature learning. Code is available at: https://github.com/wznpub/MMA_Regularization.



### ARID: A New Dataset for Recognizing Action in the Dark
- **Arxiv ID**: http://arxiv.org/abs/2006.03876v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.03876v4)
- **Published**: 2020-06-06 14:25:52+00:00
- **Updated**: 2022-08-19 05:41:15+00:00
- **Authors**: Yuecong Xu, Jianfei Yang, Haozhi Cao, Kezhi Mao, Jianxiong Yin, Simon See
- **Comment**: 6 pages, 7 figures, Data available at
  https://xuyu0010.github.io/arid, simplified title, extension of IJCAIW
  version published by Springer
  (https://link.springer.com/chapter/10.1007/978-981-16-0575-8_6)
- **Journal**: None
- **Summary**: The task of action recognition in dark videos is useful in various scenarios, e.g., night surveillance and self-driving at night. Though progress has been made in the action recognition task for videos in normal illumination, few have studied action recognition in the dark. This is partly due to the lack of sufficient datasets for such a task. In this paper, we explored the task of action recognition in dark videos. We bridge the gap of the lack of data for this task by collecting a new dataset: the Action Recognition in the Dark (ARID) dataset. It consists of over 3,780 video clips with 11 action categories. To the best of our knowledge, it is the first dataset focused on human actions in dark videos. To gain further understandings of our ARID dataset, we analyze the ARID dataset in detail and exhibited its necessity over synthetic dark videos. Additionally, we benchmarked the performance of several current action recognition models on our dataset and explored potential methods for increasing their performances. Our results show that current action recognition models and frame enhancement methods may not be effective solutions for the task of action recognition in dark videos.



### The Criminality From Face Illusion
- **Arxiv ID**: http://arxiv.org/abs/2006.03895v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/2006.03895v2)
- **Published**: 2020-06-06 15:45:05+00:00
- **Updated**: 2020-11-18 16:13:49+00:00
- **Authors**: Kevin W. Bowyer, Michael King, Walter Scheirer, Kushal Vangara
- **Comment**: None
- **Journal**: IEEE Transactions on Technology and Society, 2020
- **Summary**: The automatic analysis of face images can generate predictions about a person's gender, age, race, facial expression, body mass index, and various other indices and conditions. A few recent publications have claimed success in analyzing an image of a person's face in order to predict the person's status as Criminal / Non-Criminal. Predicting criminality from face may initially seem similar to other facial analytics, but we argue that attempts to create a criminality-from-face algorithm are necessarily doomed to fail, that apparently promising experimental results in recent publications are an illusion resulting from inadequate experimental design, and that there is potentially a large social cost to belief in the criminality from face illusion.



### Ensemble Network for Ranking Images Based on Visual Appeal
- **Arxiv ID**: http://arxiv.org/abs/2006.03898v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.03898v1)
- **Published**: 2020-06-06 15:51:38+00:00
- **Updated**: 2020-06-06 15:51:38+00:00
- **Authors**: Sachin Singh, Victor Sanchez, Tanaya Guha
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a computational framework for ranking images (group photos in particular) taken at the same event within a short time span. The ranking is expected to correspond with human perception of overall appeal of the images. We hypothesize and provide evidence through subjective analysis that the factors that appeal to humans are its emotional content, aesthetics and image quality. We propose a network which is an ensemble of three information channels, each predicting a score corresponding to one of the three visual appeal factors. For group emotion estimation, we propose a convolutional neural network (CNN) based architecture for predicting group emotion from images. This new architecture enforces the network to put emphasis on the important regions in the images, and achieves comparable results to the state-of-the-art. Next, we develop a network for the image ranking task that combines group emotion, aesthetics and image quality scores. Owing to the unavailability of suitable databases, we created a new database of manually annotated group photos taken during various social events. We present experimental results on this database and other benchmark databases whenever available. Overall, our experiments show that the proposed framework can reliably predict the overall appeal of images with results closely corresponding to human ranking.



### A Robust Attentional Framework for License Plate Recognition in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2006.03919v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.03919v2)
- **Published**: 2020-06-06 17:11:52+00:00
- **Updated**: 2020-06-09 03:06:11+00:00
- **Authors**: Linjiang Zhang, Peng Wang, Hui Li, Zhen Li, Chunhua Shen, Yanning Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Recognizing car license plates in natural scene images is an important yet still challenging task in realistic applications. Many existing approaches perform well for license plates collected under constrained conditions, eg, shooting in frontal and horizontal view-angles and under good lighting conditions. However, their performance drops significantly in an unconstrained environment that features rotation, distortion, occlusion, blurring, shading or extreme dark or bright conditions. In this work, we propose a robust framework for license plate recognition in the wild. It is composed of a tailored CycleGAN model for license plate image generation and an elaborate designed image-to-sequence network for plate recognition. On one hand, the CycleGAN based plate generation engine alleviates the exhausting human annotation work. Massive amount of training data can be obtained with a more balanced character distribution and various shooting conditions, which helps to boost the recognition accuracy to a large extent. On the other hand, the 2D attentional based license plate recognizer with an Xception-based CNN encoder is capable of recognizing license plates with different patterns under various scenarios accurately and robustly. Without using any heuristics rule or post-processing, our method achieves the state-of-the-art performance on four public datasets, which demonstrates the generality and robustness of our framework. Moreover, we released a new license plate dataset, named "CLPD", with 1200 images from all 31 provinces in mainland China. The dataset can be available from: https://github.com/wangpengnorman/CLPD_dataset.



### Robust watermarking with double detector-discriminator approach
- **Arxiv ID**: http://arxiv.org/abs/2006.03921v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.03921v1)
- **Published**: 2020-06-06 17:15:45+00:00
- **Updated**: 2020-06-06 17:15:45+00:00
- **Authors**: Marcin Plata, Piotr Syga
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we present a novel deep framework for a watermarking - a technique of embedding a transparent message into an image in a way that allows retrieving the message from a (perturbed) copy, so that copyright infringement can be tracked. For this technique, it is essential to extract the information from the image even after imposing some digital processing operations on it. Our framework outperforms recent methods in the context of robustness against not only spectrum of attacks (e.g. rotation, resizing, Gaussian smoothing) but also against compression, especially JPEG. The bit accuracy of our method is at least 0.86 for all types of distortions. We also achieved 0.90 bit accuracy for JPEG while recent methods provided at most 0.83. Our method retains high transparency and capacity as well. Moreover, we present our double detector-discriminator approach - a scheme to detect and discriminate if the image contains the embedded message or not, which is crucial for real-life watermarking systems and up to now was not investigated using neural networks. With this, we design a testing formula to validate our extended approach and compared it with a common procedure. We also present an alternative method of balancing between image quality and robustness on attacks which is easily applicable to the framework.



### Self-supervising Fine-grained Region Similarities for Large-scale Image Localization
- **Arxiv ID**: http://arxiv.org/abs/2006.03926v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.03926v2)
- **Published**: 2020-06-06 17:31:52+00:00
- **Updated**: 2020-07-09 06:21:08+00:00
- **Authors**: Yixiao Ge, Haibo Wang, Feng Zhu, Rui Zhao, Hongsheng Li
- **Comment**: Accepted in ECCV 2020 (Spotlight), code is available at
  https://github.com/yxgeee/SFRS
- **Journal**: None
- **Summary**: The task of large-scale retrieval-based image localization is to estimate the geographical location of a query image by recognizing its nearest reference images from a city-scale dataset. However, the general public benchmarks only provide noisy GPS labels associated with the training images, which act as weak supervisions for learning image-to-image similarities. Such label noise prevents deep neural networks from learning discriminative features for accurate localization. To tackle this challenge, we propose to self-supervise image-to-region similarities in order to fully explore the potential of difficult positive images alongside their sub-regions. The estimated image-to-region similarities can serve as extra training supervision for improving the network in generations, which could in turn gradually refine the fine-grained similarities to achieve optimal performance. Our proposed self-enhanced image-to-region similarity labels effectively deal with the training bottleneck in the state-of-the-art pipelines without any additional parameters or manual annotations in both training and inference. Our method outperforms state-of-the-arts on the standard localization benchmarks by noticeable margins and shows excellent generalization capability on multiple image retrieval datasets.



### An Efficient $k$-modes Algorithm for Clustering Categorical Datasets
- **Arxiv ID**: http://arxiv.org/abs/2006.03936v3
- **DOI**: 10.1002/SAM.11546
- **Categories**: **stat.ME**, cs.CV, stat.AP, stat.CO, stat.ML, 62H30, 62Pxx, 62-04, 62-08, I.5.3; G.3
- **Links**: [PDF](http://arxiv.org/pdf/2006.03936v3)
- **Published**: 2020-06-06 18:41:36+00:00
- **Updated**: 2021-06-23 20:18:20+00:00
- **Authors**: Karin S. Dorman, Ranjan Maitra
- **Comment**: 16 pages, 10 figures, 5 tables
- **Journal**: None
- **Summary**: Mining clusters from data is an important endeavor in many applications. The $k$-means method is a popular, efficient, and distribution-free approach for clustering numerical-valued data, but does not apply for categorical-valued observations. The $k$-modes method addresses this lacuna by replacing the Euclidean with the Hamming distance and the means with the modes in the $k$-means objective function. We provide a novel, computationally efficient implementation of $k$-modes, called OTQT. We prove that OTQT finds updates to improve the objective function that are undetectable to existing $k$-modes algorithms. Although slightly slower per iteration due to algorithmic complexity, OTQT is always more accurate per iteration and almost always faster (and only barely slower on some datasets) to the final optimum. Thus, we recommend OTQT as the preferred, default algorithm for $k$-modes optimization.



### UMLS-ChestNet: A deep convolutional neural network for radiological findings, differential diagnoses and localizations of COVID-19 in chest x-rays
- **Arxiv ID**: http://arxiv.org/abs/2006.05274v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.05274v1)
- **Published**: 2020-06-06 19:24:35+00:00
- **Updated**: 2020-06-06 19:24:35+00:00
- **Authors**: Germán González, Aurelia Bustos, José María Salinas, María de la Iglesia-Vaya, Joaquín Galant, Carlos Cano-Espinosa, Xavier Barber, Domingo Orozco-Beltrán, Miguel Cazorla, Antonio Pertusa
- **Comment**: 17 pages, 3 figures, 3 tables
- **Journal**: None
- **Summary**: In this work we present a method for the detection of radiological findings, their location and differential diagnoses from chest x-rays. Unlike prior works that focus on the detection of few pathologies, we use a hierarchical taxonomy mapped to the Unified Medical Language System (UMLS) terminology to identify 189 radiological findings, 22 differential diagnosis and 122 anatomic locations, including ground glass opacities, infiltrates, consolidations and other radiological findings compatible with COVID-19. We train the system on one large database of 92,594 frontal chest x-rays (AP or PA, standing, supine or decubitus) and a second database of 2,065 frontal images of COVID-19 patients identified by at least one positive Polymerase Chain Reaction (PCR) test. The reference labels are obtained through natural language processing of the radiological reports. On 23,159 test images, the proposed neural network obtains an AUC of 0.94 for the diagnosis of COVID-19. To our knowledge, this work uses the largest chest x-ray dataset of COVID-19 positive cases to date and is the first one to use a hierarchical labeling schema and to provide interpretability of the results, not only by using network attention methods, but also by indicating the radiological findings that have led to the diagnosis.



### Self-Supervised Dynamic Networks for Covariate Shift Robustness
- **Arxiv ID**: http://arxiv.org/abs/2006.03952v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.03952v1)
- **Published**: 2020-06-06 19:37:20+00:00
- **Updated**: 2020-06-06 19:37:20+00:00
- **Authors**: Tomer Cohen, Noy Shulman, Hai Morgenstern, Roey Mechrez, Erez Farhan
- **Comment**: None
- **Journal**: None
- **Summary**: As supervised learning still dominates most AI applications, test-time performance is often unexpected. Specifically, a shift of the input covariates, caused by typical nuisances like background-noise, illumination variations or transcription errors, can lead to a significant decrease in prediction accuracy. Recently, it was shown that incorporating self-supervision can significantly improve covariate shift robustness. In this work, we propose Self-Supervised Dynamic Networks (SSDN): an input-dependent mechanism, inspired by dynamic networks, that allows a self-supervised network to predict the weights of the main network, and thus directly handle covariate shifts at test-time. We present the conceptual and empirical advantages of the proposed method on the problem of image classification under different covariate shifts, and show that it significantly outperforms comparable methods.



### Enhancing Facial Data Diversity with Style-based Face Aging
- **Arxiv ID**: http://arxiv.org/abs/2006.03985v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.03985v1)
- **Published**: 2020-06-06 21:53:44+00:00
- **Updated**: 2020-06-06 21:53:44+00:00
- **Authors**: Markos Georgopoulos, James Oldfield, Mihalis A. Nicolaou, Yannis Panagakis, Maja Pantic
- **Comment**: IEEE CVPR 2020 WORKSHOP ON FAIR, DATA EFFICIENT AND TRUSTED COMPUTER
  VISION
- **Journal**: None
- **Summary**: A significant limiting factor in training fair classifiers relates to the presence of dataset bias. In particular, face datasets are typically biased in terms of attributes such as gender, age, and race. If not mitigated, bias leads to algorithms that exhibit unfair behaviour towards such groups. In this work, we address the problem of increasing the diversity of face datasets with respect to age. Concretely, we propose a novel, generative style-based architecture for data augmentation that captures fine-grained aging patterns by conditioning on multi-resolution age-discriminative representations. By evaluating on several age-annotated datasets in both single- and cross-database experiments, we show that the proposed method outperforms state-of-the-art algorithms for age transfer, especially in the case of age groups that lie in the tails of the label distribution. We further show significantly increased diversity in the augmented datasets, outperforming all compared methods according to established metrics.



### MeshSDF: Differentiable Iso-Surface Extraction
- **Arxiv ID**: http://arxiv.org/abs/2006.03997v2
- **DOI**: None
- **Categories**: **cs.CV**, 30L05, I.2.10; I.4.8; J.6
- **Links**: [PDF](http://arxiv.org/pdf/2006.03997v2)
- **Published**: 2020-06-06 23:44:05+00:00
- **Updated**: 2020-10-31 15:45:02+00:00
- **Authors**: Edoardo Remelli, Artem Lukoianov, Stephan R. Richter, Benoît Guillard, Timur Bagautdinov, Pierre Baque, Pascal Fua
- **Comment**: 22 pages, 16 figures, Neural Information Processing Systems (NeurIPS
  2020)
- **Journal**: None
- **Summary**: Geometric Deep Learning has recently made striking progress with the advent of continuous Deep Implicit Fields. They allow for detailed modeling of watertight surfaces of arbitrary topology while not relying on a 3D Euclidean grid, resulting in a learnable parameterization that is not limited in resolution.   Unfortunately, these methods are often not suitable for applications that require an explicit mesh-based surface representation because converting an implicit field to such a representation relies on the Marching Cubes algorithm, which cannot be differentiated with respect to the underlying implicit field.   In this work, we remove this limitation and introduce a differentiable way to produce explicit surface mesh representations from Deep Signed Distance Functions. Our key insight is that by reasoning on how implicit field perturbations impact local surface geometry, one can ultimately differentiate the 3D location of surface samples with respect to the underlying deep implicit field. We exploit this to define MeshSDF, an end-to-end differentiable mesh representation which can vary its topology.   We use two different applications to validate our theoretical insight: Single-View Reconstruction via Differentiable Rendering and Physically-Driven Shape Optimization. In both cases our differentiable parameterization gives us an edge over state-of-the-art algorithms.



