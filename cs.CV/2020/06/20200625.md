# Arxiv Papers in cs.CV on 2020-06-25
### SRFlow: Learning the Super-Resolution Space with Normalizing Flow
- **Arxiv ID**: http://arxiv.org/abs/2006.14200v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.14200v2)
- **Published**: 2020-06-25 06:34:04+00:00
- **Updated**: 2020-07-31 14:55:35+00:00
- **Authors**: Andreas Lugmayr, Martin Danelljan, Luc Van Gool, Radu Timofte
- **Comment**: ECCV 2020 Spotlight | git.io/SRFlow
- **Journal**: None
- **Summary**: Super-resolution is an ill-posed problem, since it allows for multiple predictions for a given low-resolution image. This fundamental fact is largely ignored by state-of-the-art deep learning based approaches. These methods instead train a deterministic mapping using combinations of reconstruction and adversarial losses. In this work, we therefore propose SRFlow: a normalizing flow based super-resolution method capable of learning the conditional distribution of the output given the low-resolution input. Our model is trained in a principled manner using a single loss, namely the negative log-likelihood. SRFlow therefore directly accounts for the ill-posed nature of the problem, and learns to predict diverse photo-realistic high-resolution images. Moreover, we utilize the strong image posterior learned by SRFlow to design flexible image manipulation techniques, capable of enhancing super-resolved images by, e.g., transferring content from other images. We perform extensive experiments on faces, as well as on super-resolution in general. SRFlow outperforms state-of-the-art GAN-based approaches in terms of both PSNR and perceptual quality metrics, while allowing for diversity through the exploration of the space of super-resolved solutions.



### Searching towards Class-Aware Generators for Conditional Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2006.14208v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.14208v2)
- **Published**: 2020-06-25 07:05:28+00:00
- **Updated**: 2021-04-06 02:07:12+00:00
- **Authors**: Peng Zhou, Lingxi Xie, Xiaopeng Zhang, Bingbing Ni, Qi Tian
- **Comment**: Code is available at \url{https://github.com/PeterouZh/NAS_cGAN}
- **Journal**: None
- **Summary**: Conditional Generative Adversarial Networks (cGAN) were designed to generate images based on the provided conditions, \eg, class-level distributions. However, existing methods have used the same generating architecture for all classes. This paper presents a novel idea that adopts NAS to find a distinct architecture for each class. The search space contains regular and class-modulated convolutions, where the latter is designed to introduce class-specific information while avoiding the reduction of training data for each class generator. The search algorithm follows a weight-sharing pipeline with mixed-architecture optimization so that the search cost does not grow with the number of classes. To learn the sampling policy, a Markov decision process is embedded into the search algorithm and a moving average is applied for better stability. We evaluate our approach on CIFAR10 and CIFAR100. Besides achieving better image generation quality in terms of FID scores, we discover several insights that are helpful in designing cGAN models. Code is available at https://github.com/PeterouZh/NAS_cGAN.



### Deep Residual 3D U-Net for Joint Segmentation and Texture Classification of Nodules in Lung
- **Arxiv ID**: http://arxiv.org/abs/2006.14215v2
- **DOI**: 10.1007/978-3-030-50516-5_37
- **Categories**: **eess.IV**, cs.CV, cs.LG, I.2.1; I.2.10; I.4.6; I.5.1
- **Links**: [PDF](http://arxiv.org/pdf/2006.14215v2)
- **Published**: 2020-06-25 07:20:41+00:00
- **Updated**: 2020-06-26 05:08:18+00:00
- **Authors**: Alexandr G. Rassadin
- **Comment**: 10 pages, 5 figures, 2 tables, accepted for publication at ICIAR 2020
  (LNDb Grand Challenge)
- **Journal**: ICIAR 2020: Image Analysis and Recognition pp 419-427
- **Summary**: In this work we present a method for lung nodules segmentation, their texture classification and subsequent follow-up recommendation from the CT image of lung. Our method consists of neural network model based on popular U-Net architecture family but modified for the joint nodule segmentation and its texture classification tasks and an ensemble-based model for the follow-up recommendation. This solution was evaluated within the LNDb medical imaging challenge and produced the best nodule segmentation result on the final leaderboard.



### Fine granularity access in interactive compression of 360-degree images based on rate-adaptive channel codes
- **Arxiv ID**: http://arxiv.org/abs/2006.14239v2
- **DOI**: 10.1109/TMM.2020.3017890
- **Categories**: **cs.MM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.14239v2)
- **Published**: 2020-06-25 08:13:48+00:00
- **Updated**: 2020-08-21 13:45:21+00:00
- **Authors**: Navid Mahmoudian Bidgoli, Thomas Maugey, Aline Roumy
- **Comment**: accepted to be published in IEEE Transactions on Multimedia
- **Journal**: None
- **Summary**: In this paper, we propose a new interactive compression scheme for omnidirectional images. This requires two characteristics: efficient compression of data, to lower the storage cost, and random access ability to extract part of the compressed stream requested by the user (for reducing the transmission rate). For efficient compression, data needs to be predicted by a series of references that have been pre-defined and compressed. This contrasts with the spirit of random accessibility. We propose a solution for this problem based on incremental codes implemented by rate-adaptive channel codes. This scheme encodes the image while adapting to any user request and leads to an efficient coding that is flexible in extracting data depending on the available information at the decoder. Therefore, only the information that is needed to be displayed at the user's side is transmitted during the user's request, as if the request was already known at the encoder. The experimental results demonstrate that our coder obtains a better transmission rate than the state-of-the-art tile-based methods at a small cost in storage. Moreover, the transmission rate grows gradually with the size of the request and avoids a staircase effect, which shows the perfect suitability of our coder for interactive transmission.



### SS-CAM: Smoothed Score-CAM for Sharper Visual Feature Localization
- **Arxiv ID**: http://arxiv.org/abs/2006.14255v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.14255v3)
- **Published**: 2020-06-25 08:51:54+00:00
- **Updated**: 2020-11-12 13:02:55+00:00
- **Authors**: Haofan Wang, Rakshit Naidu, Joy Michael, Soumya Snigdha Kundu
- **Comment**: 7 pages, 4 figures and 4 tables
- **Journal**: None
- **Summary**: Interpretation of the underlying mechanisms of Deep Convolutional Neural Networks has become an important aspect of research in the field of deep learning due to their applications in high-risk environments. To explain these black-box architectures there have been many methods applied so the internal decisions can be analyzed and understood. In this paper, built on the top of Score-CAM, we introduce an enhanced visual explanation in terms of visual sharpness called SS-CAM, which produces centralized localization of object features within an image through a smooth operation. We evaluate our method on the ILSVRC 2012 Validation dataset, which outperforms Score-CAM on both faithfulness and localization tasks.



### SASO: Joint 3D Semantic-Instance Segmentation via Multi-scale Semantic Association and Salient Point Clustering Optimization
- **Arxiv ID**: http://arxiv.org/abs/2006.15015v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2006.15015v1)
- **Published**: 2020-06-25 08:55:25+00:00
- **Updated**: 2020-06-25 08:55:25+00:00
- **Authors**: Jingang Tan, Lili Chen, Kangru Wang, Jingquan Peng, Jiamao Li, Xiaolin Zhang
- **Comment**: 8 pages,6 figures
- **Journal**: None
- **Summary**: We propose a novel 3D point cloud segmentation framework named SASO, which jointly performs semantic and instance segmentation tasks. For semantic segmentation task, inspired by the inherent correlation among objects in spatial context, we propose a Multi-scale Semantic Association (MSA) module to explore the constructive effects of the semantic context information. For instance segmentation task, different from previous works that utilize clustering only in inference procedure, we propose a Salient Point Clustering Optimization (SPCO) module to introduce a clustering procedure into the training process and impel the network focusing on points that are difficult to be distinguished. In addition, because of the inherent structures of indoor scenes, the imbalance problem of the category distribution is rarely considered but severely limits the performance of 3D scene perception. To address this issue, we introduce an adaptive Water Filling Sampling (WFS) algorithm to balance the category distribution of training data. Extensive experiments demonstrate that our method outperforms the state-of-the-art methods on benchmark datasets in both semantic segmentation and instance segmentation tasks.



### SACT: Self-Aware Multi-Space Feature Composition Transformer for Multinomial Attention for Video Captioning
- **Arxiv ID**: http://arxiv.org/abs/2006.14262v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2006.14262v1)
- **Published**: 2020-06-25 09:11:49+00:00
- **Updated**: 2020-06-25 09:11:49+00:00
- **Authors**: Chiranjib Sur
- **Comment**: None
- **Journal**: None
- **Summary**: Video captioning works on the two fundamental concepts, feature detection and feature composition. While modern day transformers are beneficial in composing features, they lack the fundamental problems of selecting and understanding of the contents. As the feature length increases, it becomes increasingly important to include provisions for improved capturing of the pertinent contents. In this work, we have introduced a new concept of Self-Aware Composition Transformer (SACT) that is capable of generating Multinomial Attention (MultAtt) which is a way of generating distributions of various combinations of frames. Also, multi-head attention transformer works on the principle of combining all possible contents for attention, which is good for natural language classification, but has limitations for video captioning. Video contents have repetitions and require parsing of important contents for better content composition. In this work, we have introduced SACT for more selective attention and combined them for different attention heads for better capturing of the usable contents for any applications. To address the problem of diversification and encourage selective utilization, we propose the Self-Aware Composition Transformer model for dense video captioning and apply the technique on two benchmark datasets like ActivityNet and YouCookII.



### Self-Segregating and Coordinated-Segregating Transformer for Focused Deep Multi-Modular Network for Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/2006.14264v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2006.14264v1)
- **Published**: 2020-06-25 09:17:03+00:00
- **Updated**: 2020-06-25 09:17:03+00:00
- **Authors**: Chiranjib Sur
- **Comment**: None
- **Journal**: None
- **Summary**: Attention mechanism has gained huge popularity due to its effectiveness in achieving high accuracy in different domains. But attention is opportunistic and is not justified by the content or usability of the content. Transformer like structure creates all/any possible attention(s). We define segregating strategies that can prioritize the contents for the applications for enhancement of performance. We defined two strategies: Self-Segregating Transformer (SST) and Coordinated-Segregating Transformer (CST) and used it to solve visual question answering application. Self-segregation strategy for attention contributes in better understanding and filtering the information that can be most helpful for answering the question and create diversity of visual-reasoning for attention. This work can easily be used in many other applications that involve repetition and multiple frames of features and would reduce the commonality of the attentions to a great extent. Visual Question Answering (VQA) requires understanding and coordination of both images and textual interpretations. Experiments demonstrate that segregation strategies for cascaded multi-head transformer attention outperforms many previous works and achieved considerable improvement for VQA-v2 dataset benchmark.



### Empirical Analysis of Overfitting and Mode Drop in GAN Training
- **Arxiv ID**: http://arxiv.org/abs/2006.14265v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.14265v1)
- **Published**: 2020-06-25 09:17:32+00:00
- **Updated**: 2020-06-25 09:17:32+00:00
- **Authors**: Yasin Yazici, Chuan-Sheng Foo, Stefan Winkler, Kim-Hui Yap, Vijay Chandrasekhar
- **Comment**: To appear in ICIP2020
- **Journal**: None
- **Summary**: We examine two key questions in GAN training, namely overfitting and mode drop, from an empirical perspective. We show that when stochasticity is removed from the training procedure, GANs can overfit and exhibit almost no mode drop. Our results shed light on important characteristics of the GAN training procedure. They also provide evidence against prevailing intuitions that GANs do not memorize the training set, and that mode dropping is mainly due to properties of the GAN objective rather than how it is optimized during training.



### Attention-based Graph ResNet for Motor Intent Detection from Raw EEG signals
- **Arxiv ID**: http://arxiv.org/abs/2007.13484v1
- **DOI**: None
- **Categories**: **eess.SP**, cs.AI, cs.CV, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.13484v1)
- **Published**: 2020-06-25 09:29:48+00:00
- **Updated**: 2020-06-25 09:29:48+00:00
- **Authors**: Shuyue Jia, Yimin Hou, Yan Shi, Yang Li
- **Comment**: None
- **Journal**: None
- **Summary**: In previous studies, decoding electroencephalography (EEG) signals has not considered the topological relationship of EEG electrodes. However, the latest neuroscience has suggested brain network connectivity. Thus, the exhibited interaction between EEG channels might not be appropriately measured via Euclidean distance. To fill the gap, an attention-based graph residual network, a novel structure of Graph Convolutional Neural Network (GCN), was presented to detect human motor intents from raw EEG signals, where the topological structure of EEG electrodes was built as a graph. Meanwhile, deep residual learning with a full-attention architecture was introduced to address the degradation problem concerning deeper networks in raw EEG motor imagery (MI) data. Individual variability, the critical and longstanding challenge underlying EEG signals, has been successfully handled with the state-of-the-art performance, 98.08% accuracy at the subject level, 94.28% for 20 subjects. Numerical results were promising that the implementation of the graph-structured topology was superior to decode raw EEG data. The innovative deep learning approach was expected to entail a universal method towards both neuroscience research and real-world EEG-based practical applications, e.g., seizure prediction.



### PropagationNet: Propagate Points to Curve to Learn Structure Information
- **Arxiv ID**: http://arxiv.org/abs/2006.14308v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.14308v1)
- **Published**: 2020-06-25 11:08:59+00:00
- **Updated**: 2020-06-25 11:08:59+00:00
- **Authors**: Xiehe Huang, Weihong Deng, Haifeng Shen, Xiubao Zhang, Jieping Ye
- **Comment**: 10 pages, 8 figures, 8 tables, CVPR2020
- **Journal**: The IEEE/CVF Conference on Computer Vision and Pattern Recognition
  (CVPR), 2020, pp. 7265-7274
- **Summary**: Deep learning technique has dramatically boosted the performance of face alignment algorithms. However, due to large variability and lack of samples, the alignment problem in unconstrained situations, \emph{e.g}\onedot large head poses, exaggerated expression, and uneven illumination, is still largely unsolved. In this paper, we explore the instincts and reasons behind our two proposals, \emph{i.e}\onedot Propagation Module and Focal Wing Loss, to tackle the problem. Concretely, we present a novel structure-infused face alignment algorithm based on heatmap regression via propagating landmark heatmaps to boundary heatmaps, which provide structure information for further attention map generation. Moreover, we propose a Focal Wing Loss for mining and emphasizing the difficult samples under in-the-wild condition. In addition, we adopt methods like CoordConv and Anti-aliased CNN from other fields that address the shift-variance problem of CNN for face alignment. When implementing extensive experiments on different benchmarks, \emph{i.e}\onedot WFLW, 300W, and COFW, our method outperforms state-of-the-arts by a significant margin. Our proposed approach achieves 4.05\% mean error on WFLW, 2.93\% mean error on 300W full-set, and 3.71\% mean error on COFW.



### Deep Learning for Cornea Microscopy Blind Deblurring
- **Arxiv ID**: http://arxiv.org/abs/2006.14319v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.14319v1)
- **Published**: 2020-06-25 11:50:35+00:00
- **Updated**: 2020-06-25 11:50:35+00:00
- **Authors**: Toussain Cardot, Pilar Marxer, Ivan Snozzi
- **Comment**: None
- **Journal**: None
- **Summary**: The goal of this project is to build a deep-learning solution that deblurs cornea scans, used for medical examination. The spherical shape of the eye prevents ophtamologist from having completely sharp image. Provided with a stack of corneas from confocal images, our approach is to build a model that performs an upscaling of the images using an SR (Super Resolution) Network.



### Perfusion Quantification from Endoscopic Videos: Learning to Read Tumor Signatures
- **Arxiv ID**: http://arxiv.org/abs/2006.14321v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2006.14321v1)
- **Published**: 2020-06-25 11:53:20+00:00
- **Updated**: 2020-06-25 11:53:20+00:00
- **Authors**: Sergiy Zhuk, Jonathan P. Epperlein, Rahul Nair, Seshu Thirupati, Pol Mac Aonghusa, Ronan Cahill, Donal O'Shea
- **Comment**: To be published in 23rd International Conference on Medical Image
  Computing & Computer Assisted Intervention (MICCAI 2020)
- **Journal**: None
- **Summary**: Intra-operative identification of malignant versus benign or healthy tissue is a major challenge in fluorescence guided cancer surgery. We propose a perfusion quantification method for computer-aided interpretation of subtle differences in dynamic perfusion patterns which can be used to distinguish between normal tissue and benign or malignant tumors intra-operatively in real-time by using multispectral endoscopic videos. The method exploits the fact that vasculature arising from cancer angiogenesis gives tumors differing perfusion patterns from the surrounding tissue, and defines a signature of tumor which could be used to differentiate tumors from normal tissues. Experimental evaluation of our method on a cohort of colorectal cancer surgery endoscopic videos suggests that the proposed tumor signature is able to successfully discriminate between healthy, cancerous and benign tissue with 95% accuracy.



### Collaborative Boundary-aware Context Encoding Networks for Error Map Prediction
- **Arxiv ID**: http://arxiv.org/abs/2006.14345v1
- **DOI**: 10.1016/j.patcog.2021.108515
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.14345v1)
- **Published**: 2020-06-25 12:42:01+00:00
- **Updated**: 2020-06-25 12:42:01+00:00
- **Authors**: Zhenxi Zhang, Chunna Tian, Jie Li, Zhusi Zhong, Zhicheng Jiao, Xinbo Gao
- **Comment**: None
- **Journal**: Pattern Recognition PR_108515 ,2022
- **Summary**: Medical image segmentation is usually regarded as one of the most important intermediate steps in clinical situations and medical imaging research. Thus, accurately assessing the segmentation quality of the automatically generated predictions is essential for guaranteeing the reliability of the results of the computer-assisted diagnosis (CAD). Many researchers apply neural networks to train segmentation quality regression models to estimate the segmentation quality of a new data cohort without labeled ground truth. Recently, a novel idea is proposed that transforming the segmentation quality assessment (SQA) problem intothe pixel-wise error map prediction task in the form of segmentation. However, the simple application of vanilla segmentation structures in medical image fails to detect some small and thin error regions of the auto-generated masks with complex anatomical structures. In this paper, we propose collaborative boundaryaware context encoding networks called AEP-Net for error prediction task. Specifically, we propose a collaborative feature transformation branch for better feature fusion between images and masks, and precise localization of error regions. Further, we propose a context encoding module to utilize the global predictor from the error map to enhance the feature representation and regularize the networks. We perform experiments on IBSR v2.0 dataset and ACDC dataset. The AEP-Net achieves an average DSC of 0.8358, 0.8164 for error prediction task,and shows a high Pearson correlation coefficient of 0.9873 between the actual segmentation accuracy and the predicted accuracy inferred from the predicted error map on IBSR v2.0 dataset, which verifies the efficacy of our AEP-Net.



### Epoch-evolving Gaussian Process Guided Learning
- **Arxiv ID**: http://arxiv.org/abs/2006.14347v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.14347v1)
- **Published**: 2020-06-25 12:45:17+00:00
- **Updated**: 2020-06-25 12:45:17+00:00
- **Authors**: Jiabao Cui, Xuewei Li, Bin Li, Hanbin Zhao, Bourahla Omar, Xi Li
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a novel learning scheme called epoch-evolving Gaussian Process Guided Learning (GPGL), which aims at characterizing the correlation information between the batch-level distribution and the global data distribution. Such correlation information is encoded as context labels and needs renewal every epoch. With the guidance of the context label and ground truth label, GPGL scheme provides a more efficient optimization through updating the model parameters with a triangle consistency loss. Furthermore, our GPGL scheme can be further generalized and naturally applied to the current deep models, outperforming the existing batch-based state-of-the-art models on mainstream datasets (CIFAR-10, CIFAR-100, and Tiny-ImageNet) remarkably.



### Discontinuous and Smooth Depth Completion with Binary Anisotropic Diffusion Tensor
- **Arxiv ID**: http://arxiv.org/abs/2006.14374v1
- **DOI**: 10.1109/LRA.2020.3005890
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2006.14374v1)
- **Published**: 2020-06-25 13:16:47+00:00
- **Updated**: 2020-06-25 13:16:47+00:00
- **Authors**: Yasuhiro Yao, Menandro Roxas, Ryoichi Ishikawa, Shingo Ando, Jun Shimamura, Takeshi Oishi
- **Comment**: 8 pages 6 figures
- **Journal**: None
- **Summary**: We propose an unsupervised real-time dense depth completion from a sparse depth map guided by a single image. Our method generates a smooth depth map while preserving discontinuity between different objects. Our key idea is a Binary Anisotropic Diffusion Tensor (B-ADT) which can completely eliminate smoothness constraint at intended positions and directions by applying it to variational regularization. We also propose an Image-guided Nearest Neighbor Search (IGNNS) to derive a piecewise constant depth map which is used for B-ADT derivation and in the data term of the variational energy. Our experiments show that our method can outperform previous unsupervised and semi-supervised depth completion methods in terms of accuracy. Moreover, since our resulting depth map preserves the discontinuity between objects, the result can be converted to a visually plausible point cloud. This is remarkable since previous methods generate unnatural surface-like artifacts between discontinuous objects.



### Training Variational Networks with Multi-Domain Simulations: Speed-of-Sound Image Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2006.14395v1
- **DOI**: 10.1109/TUFFC.2020.3010186
- **Categories**: **eess.IV**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2006.14395v1)
- **Published**: 2020-06-25 13:32:08+00:00
- **Updated**: 2020-06-25 13:32:08+00:00
- **Authors**: Melanie Bernhardt, Valery Vishnevskiy, Richard Rau, Orcun Goksel
- **Comment**: None
- **Journal**: None
- **Summary**: Speed-of-sound has been shown as a potential biomarker for breast cancer imaging, successfully differentiating malignant tumors from benign ones. Speed-of-sound images can be reconstructed from time-of-flight measurements from ultrasound images acquired using conventional handheld ultrasound transducers. Variational Networks (VN) have recently been shown to be a potential learning-based approach for optimizing inverse problems in image reconstruction. Despite earlier promising results, these methods however do not generalize well from simulated to acquired data, due to the domain shift. In this work, we present for the first time a VN solution for a pulse-echo SoS image reconstruction problem using diverging waves with conventional transducers and single-sided tissue access. This is made possible by incorporating simulations with varying complexity into training. We use loop unrolling of gradient descent with momentum, with an exponentially weighted loss of outputs at each unrolled iteration in order to regularize training. We learn norms as activation functions regularized to have smooth forms for robustness to input distribution variations. We evaluate reconstruction quality on ray-based and full-wave simulations as well as on tissue-mimicking phantom data, in comparison to a classical iterative (L-BFGS) optimization of this image reconstruction problem. We show that the proposed regularization techniques combined with multi-source domain training yield substantial improvements in the domain adaptation capabilities of VN, reducing median RMSE by 54% on a wave-based simulation dataset compared to the baseline VN. We also show that on data acquired from a tissue-mimicking breast phantom the proposed VN provides improved reconstruction in 12 milliseconds.



### DanHAR: Dual Attention Network For Multimodal Human Activity Recognition Using Wearable Sensors
- **Arxiv ID**: http://arxiv.org/abs/2006.14435v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.14435v4)
- **Published**: 2020-06-25 14:17:33+00:00
- **Updated**: 2021-07-21 08:21:37+00:00
- **Authors**: Wenbin Gao, Lei Zhang, Qi Teng, Jun He, Hao Wu
- **Comment**: None
- **Journal**: None
- **Summary**: Human activity recognition (HAR) in ubiquitous computing has been beginning to incorporate attention into the context of deep neural networks (DNNs), in which the rich sensing data from multimodal sensors such as accelerometer and gyroscope is used to infer human activities. Recently, two attention methods are proposed via combining with Gated Recurrent Units (GRU) and Long Short-Term Memory (LSTM) network, which can capture the dependencies of sensing signals in both spatial and temporal domains simultaneously. However, recurrent networks often have a weak feature representing power compared with convolutional neural networks (CNNs). On the other hand, two attention, i.e., hard attention and soft attention, are applied in temporal domains via combining with CNN, which pay more attention to the target activity from a long sequence. However, they can only tell where to focus and miss channel information, which plays an important role in deciding what to focus. As a result, they fail to address the spatial-temporal dependencies of multimodal sensing signals, compared with attention-based GRU or LSTM. In the paper, we propose a novel dual attention method called DanHAR, which introduces the framework of blending channel attention and temporal attention on a CNN, demonstrating superiority in improving the comprehensibility for multimodal HAR. Extensive experiments on four public HAR datasets and weakly labeled dataset show that DanHAR achieves state-of-the-art performance with negligible overhead of parameters. Furthermore, visualizing analysis is provided to show that our attention can amplifies more important sensor modalities and timesteps during classification, which agrees well with human common intuition.



### Scalable Spectral Clustering with Nystrom Approximation: Practical and Theoretical Aspects
- **Arxiv ID**: http://arxiv.org/abs/2006.14470v2
- **DOI**: 10.1109/OJSP.2020.3039330
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.14470v2)
- **Published**: 2020-06-25 15:10:56+00:00
- **Updated**: 2021-01-31 02:10:30+00:00
- **Authors**: Farhad Pourkamali-Anaraki
- **Comment**: Published in IEEE Open Journal of Signal Processing
- **Journal**: in IEEE Open Journal of Signal Processing, vol. 1, pp. 242-256,
  2020
- **Summary**: Spectral clustering techniques are valuable tools in signal processing and machine learning for partitioning complex data sets. The effectiveness of spectral clustering stems from constructing a non-linear embedding based on creating a similarity graph and computing the spectral decomposition of the Laplacian matrix. However, spectral clustering methods fail to scale to large data sets because of high computational cost and memory usage. A popular approach for addressing these problems utilizes the Nystrom method, an efficient sampling-based algorithm for computing low-rank approximations to large positive semi-definite matrices. This paper demonstrates how the previously popular approach of Nystrom-based spectral clustering has severe limitations. Existing time-efficient methods ignore critical information by prematurely reducing the rank of the similarity matrix associated with sampled points. Also, current understanding is limited regarding how utilizing the Nystrom approximation will affect the quality of spectral embedding approximations. To address the limitations, this work presents a principled spectral clustering algorithm that exploits spectral properties of the similarity matrix associated with sampled points to regulate accuracy-efficiency trade-offs. We provide theoretical results to reduce the current gap and present numerical experiments with real and synthetic data. Empirical results demonstrate the efficacy and efficiency of the proposed method compared to existing spectral clustering techniques based on the Nystrom method and other efficient methods. The overarching goal of this work is to provide an improved baseline for future research directions to accelerate spectral clustering.



### One Thousand and One Hours: Self-driving Motion Prediction Dataset
- **Arxiv ID**: http://arxiv.org/abs/2006.14480v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2006.14480v2)
- **Published**: 2020-06-25 15:23:41+00:00
- **Updated**: 2020-11-16 21:16:49+00:00
- **Authors**: John Houston, Guido Zuidhof, Luca Bergamini, Yawei Ye, Long Chen, Ashesh Jain, Sammy Omari, Vladimir Iglovikov, Peter Ondruska
- **Comment**: Presente at CoRL2020
- **Journal**: None
- **Summary**: Motivated by the impact of large-scale datasets on ML systems we present the largest self-driving dataset for motion prediction to date, containing over 1,000 hours of data. This was collected by a fleet of 20 autonomous vehicles along a fixed route in Palo Alto, California, over a four-month period. It consists of 170,000 scenes, where each scene is 25 seconds long and captures the perception output of the self-driving system, which encodes the precise positions and motions of nearby vehicles, cyclists, and pedestrians over time. On top of this, the dataset contains a high-definition semantic map with 15,242 labelled elements and a high-definition aerial view over the area. We show that using a dataset of this size dramatically improves performance for key self-driving problems. Combined with the provided software kit, this collection forms the largest and most detailed dataset to date for the development of self-driving machine learning tasks, such as motion forecasting, motion planning and simulation. The full dataset is available at http://level5.lyft.com/.



### Uncovering the Connections Between Adversarial Transferability and Knowledge Transferability
- **Arxiv ID**: http://arxiv.org/abs/2006.14512v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.14512v4)
- **Published**: 2020-06-25 16:04:47+00:00
- **Updated**: 2021-07-08 19:17:09+00:00
- **Authors**: Kaizhao Liang, Jacky Y. Zhang, Boxin Wang, Zhuolin Yang, Oluwasanmi Koyejo, Bo Li
- **Comment**: Accepted to ICML 2021
- **Journal**: None
- **Summary**: Knowledge transferability, or transfer learning, has been widely adopted to allow a pre-trained model in the source domain to be effectively adapted to downstream tasks in the target domain. It is thus important to explore and understand the factors affecting knowledge transferability. In this paper, as the first work, we analyze and demonstrate the connections between knowledge transferability and another important phenomenon--adversarial transferability, \emph{i.e.}, adversarial examples generated against one model can be transferred to attack other models. Our theoretical studies show that adversarial transferability indicates knowledge transferability and vice versa. Moreover, based on the theoretical insights, we propose two practical adversarial transferability metrics to characterize this process, serving as bidirectional indicators between adversarial and knowledge transferability. We conduct extensive experiments for different scenarios on diverse datasets, showing a positive correlation between adversarial transferability and knowledge transferability. Our findings will shed light on future research about effective knowledge transfer learning and adversarial transferability analyses.



### Smooth Adversarial Training
- **Arxiv ID**: http://arxiv.org/abs/2006.14536v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2006.14536v2)
- **Published**: 2020-06-25 16:34:39+00:00
- **Updated**: 2021-07-11 00:56:58+00:00
- **Authors**: Cihang Xie, Mingxing Tan, Boqing Gong, Alan Yuille, Quoc V. Le
- **Comment**: tech report
- **Journal**: None
- **Summary**: It is commonly believed that networks cannot be both accurate and robust, that gaining robustness means losing accuracy. It is also generally believed that, unless making networks larger, network architectural elements would otherwise matter little in improving adversarial robustness. Here we present evidence to challenge these common beliefs by a careful study about adversarial training. Our key observation is that the widely-used ReLU activation function significantly weakens adversarial training due to its non-smooth nature. Hence we propose smooth adversarial training (SAT), in which we replace ReLU with its smooth approximations to strengthen adversarial training. The purpose of smooth activation functions in SAT is to allow it to find harder adversarial examples and compute better gradient updates during adversarial training.   Compared to standard adversarial training, SAT improves adversarial robustness for "free", i.e., no drop in accuracy and no increase in computational cost. For example, without introducing additional computations, SAT significantly enhances ResNet-50's robustness from 33.0% to 42.3%, while also improving accuracy by 0.9% on ImageNet. SAT also works well with larger networks: it helps EfficientNet-L1 to achieve 82.2% accuracy and 58.6% robustness on ImageNet, outperforming the previous state-of-the-art defense by 9.5% for accuracy and 11.6% for robustness. Models are available at https://github.com/cihangxie/SmoothAdversarialTraining.



### Estimating Displaced Populations from Overhead
- **Arxiv ID**: http://arxiv.org/abs/2006.14547v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.14547v2)
- **Published**: 2020-06-25 16:45:11+00:00
- **Updated**: 2020-12-21 17:41:21+00:00
- **Authors**: Armin Hadzic, Gordon Christie, Jeffrey Freeman, Amber Dismer, Stevan Bullard, Ashley Greiner, Nathan Jacobs, Ryan Mukherjee
- **Comment**: Fixed typo in abstract
- **Journal**: None
- **Summary**: We introduce a deep learning approach to perform fine-grained population estimation for displacement camps using high-resolution overhead imagery. We train and evaluate our approach on drone imagery cross-referenced with population data for refugee camps in Cox's Bazar, Bangladesh in 2018 and 2019. Our proposed approach achieves 7.02% mean absolute percent error on sequestered camp imagery. We believe our experiments with real-world displacement camp data constitute an important step towards the development of tools that enable the humanitarian community to effectively and rapidly respond to the global displacement crisis.



### Lifted Disjoint Paths with Application in Multiple Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/2006.14550v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.DM
- **Links**: [PDF](http://arxiv.org/pdf/2006.14550v1)
- **Published**: 2020-06-25 16:49:08+00:00
- **Updated**: 2020-06-25 16:49:08+00:00
- **Authors**: Andrea Hornakova, Roberto Henschel, Bodo Rosenhahn, Paul Swoboda
- **Comment**: ICML 2020, Codebase available at
  https://github.com/AndreaHor/LifT_Solver
- **Journal**: None
- **Summary**: We present an extension to the disjoint paths problem in which additional \emph{lifted} edges are introduced to provide path connectivity priors. We call the resulting optimization problem the lifted disjoint paths problem. We show that this problem is NP-hard by reduction from integer multicommodity flow and 3-SAT. To enable practical global optimization, we propose several classes of linear inequalities that produce a high-quality LP-relaxation. Additionally, we propose efficient cutting plane algorithms for separating the proposed linear inequalities. The lifted disjoint path problem is a natural model for multiple object tracking and allows an elegant mathematical formulation for long range temporal interactions. Lifted edges help to prevent id switches and to re-identify persons. Our lifted disjoint paths tracker achieves nearly optimal assignments with respect to input detections. As a consequence, it leads on all three main benchmarks of the MOT challenge, improving significantly over state-of-the-art.



### Anomaly Detection using Deep Reconstruction and Forecasting for Autonomous Systems
- **Arxiv ID**: http://arxiv.org/abs/2006.14556v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2006.14556v1)
- **Published**: 2020-06-25 17:00:01+00:00
- **Updated**: 2020-06-25 17:00:01+00:00
- **Authors**: Nadarasar Bahavan, Navaratnarajah Suman, Sulhi Cader, Ruwinda Ranganayake, Damitha Seneviratne, Vinu Maddumage, Gershom Seneviratne, Yasinha Supun, Isuru Wijesiri, Suchitha Dehigaspitiya, Dumindu Tissera, Chamira Edussooriya
- **Comment**: Runners Up - IEEE Signal Processing Cup 2020
- **Journal**: None
- **Summary**: We propose self-supervised deep algorithms to detect anomalies in heterogeneous autonomous systems using frontal camera video and IMU readings. Given that the video and IMU data are not synchronized, each of them are analyzed separately. The vision-based system, which utilizes a conditional GAN, analyzes immediate-past three frames and attempts to predict the next frame. The frame is classified as either an anomalous case or a normal case based on the degree of difference estimated using the prediction error and a threshold. The IMU-based system utilizes two approaches to classify the timestamps; the first being an LSTM autoencoder which reconstructs three consecutive IMU vectors and the second being an LSTM forecaster which is utilized to predict the next vector using the previous three IMU vectors. Based on the reconstruction error, the prediction error, and a threshold, the timestamp is classified as either an anomalous case or a normal case. The composition of algorithms won runners up at the IEEE Signal Processing Cup anomaly detection challenge 2020. In the competition dataset of camera frames consisting of both normal and anomalous cases, we achieve a test accuracy of 94% and an F1-score of 0.95. Furthermore, we achieve an accuracy of 100% on a test set containing normal IMU data, and an F1-score of 0.98 on the test set of abnormal IMU data.



### Dynamically Mitigating Data Discrepancy with Balanced Focal Loss for Replay Attack Detection
- **Arxiv ID**: http://arxiv.org/abs/2006.14563v3
- **DOI**: 10.1109/ICPR48806.2021.9412749
- **Categories**: **cs.CV**, cs.LG, eess.AS, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.14563v3)
- **Published**: 2020-06-25 17:06:47+00:00
- **Updated**: 2023-01-18 04:17:47+00:00
- **Authors**: Yongqiang Dou, Haocheng Yang, Maolin Yang, Yanyan Xu, Dengfeng Ke
- **Comment**: The 25th International Conference on Pattern Recognition (ICPR2020)
- **Journal**: None
- **Summary**: It becomes urgent to design effective anti-spoofing algorithms for vulnerable automatic speaker verification systems due to the advancement of high-quality playback devices. Current studies mainly treat anti-spoofing as a binary classification problem between bonafide and spoofed utterances, while lack of indistinguishable samples makes it difficult to train a robust spoofing detector. In this paper, we argue that for anti-spoofing, it needs more attention for indistinguishable samples over easily-classified ones in the modeling process, to make correct discrimination a top priority. Therefore, to mitigate the data discrepancy between training and inference, we propose D3M, to leverage a balanced focal loss function as the training objective to dynamically scale the loss based on the traits of the sample itself. Besides, in the experiments, we select three kinds of features that contain both magnitude-based and phase-based information to form complementary and informative features. Experimental results on the ASVspoof2019 dataset demonstrate the superiority of the proposed methods by comparison between our systems and top-performing ones. Systems trained with the balanced focal loss perform significantly better than conventional cross-entropy loss. With complementary features, our fusion system with only three kinds of features outperforms other systems containing five or more complex single models by 22.5% for min-tDCF and 7% for EER, achieving a min-tDCF and an EER of 0.0124 and 0.55% respectively. Furthermore, we present and discuss the evaluation results on real replay data apart from the simulated ASVspoof2019 data, indicating that research for anti-spoofing still has a long way to go. Source code, analysis data, and other details are publicly available at https://github.com/asvspoof/D3M.



### Multimarginal Wasserstein Barycenter for Stain Normalization and Augmentation
- **Arxiv ID**: http://arxiv.org/abs/2006.14566v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.14566v1)
- **Published**: 2020-06-25 17:09:40+00:00
- **Updated**: 2020-06-25 17:09:40+00:00
- **Authors**: Saad Nadeem, Travis Hollmann, Allen Tannenbaum
- **Comment**: To appear in MICCAI 2020
- **Journal**: None
- **Summary**: Variations in hematoxylin and eosin (H&E) stained images (due to clinical lab protocols, scanners, etc) directly impact the quality and accuracy of clinical diagnosis, and hence it is important to control for these variations for a reliable diagnosis. In this work, we present a new approach based on the multimarginal Wasserstein barycenter to normalize and augment H&E stained images given one or more references. Specifically, we provide a mathematically robust way of naturally incorporating additional images as intermediate references to drive stain normalization and augmentation simultaneously. The presented approach showed superior results quantitatively and qualitatively as compared to state-of-the-art methods for stain normalization. We further validated our stain normalization and augmentations in the nuclei segmentation task on a publicly available dataset, achieving state-of-the-art results against competing approaches.



### Backdoor Attacks Against Deep Learning Systems in the Physical World
- **Arxiv ID**: http://arxiv.org/abs/2006.14580v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.14580v4)
- **Published**: 2020-06-25 17:26:20+00:00
- **Updated**: 2021-09-07 17:42:01+00:00
- **Authors**: Emily Wenger, Josephine Passananti, Arjun Bhagoji, Yuanshun Yao, Haitao Zheng, Ben Y. Zhao
- **Comment**: Accepted to the 2021 Conference on Computer Vision and Pattern
  Recognition (CVPR 2021); 14 pages
- **Journal**: None
- **Summary**: Backdoor attacks embed hidden malicious behaviors into deep learning models, which only activate and cause misclassifications on model inputs containing a specific trigger. Existing works on backdoor attacks and defenses, however, mostly focus on digital attacks that use digitally generated patterns as triggers. A critical question remains unanswered: can backdoor attacks succeed using physical objects as triggers, thus making them a credible threat against deep learning systems in the real world? We conduct a detailed empirical study to explore this question for facial recognition, a critical deep learning task. Using seven physical objects as triggers, we collect a custom dataset of 3205 images of ten volunteers and use it to study the feasibility of physical backdoor attacks under a variety of real-world conditions. Our study reveals two key findings. First, physical backdoor attacks can be highly successful if they are carefully configured to overcome the constraints imposed by physical objects. In particular, the placement of successful triggers is largely constrained by the target model's dependence on key facial features. Second, four of today's state-of-the-art defenses against (digital) backdoors are ineffective against physical backdoors, because the use of physical objects breaks core assumptions used to construct these defenses. Our study confirms that (physical) backdoor attacks are not a hypothetical phenomenon but rather pose a serious real-world threat to critical classification tasks. We need new and more robust defenses against backdoors in the physical world.



### SmallBigNet: Integrating Core and Contextual Views for Video Classification
- **Arxiv ID**: http://arxiv.org/abs/2006.14582v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.14582v1)
- **Published**: 2020-06-25 17:29:57+00:00
- **Updated**: 2020-06-25 17:29:57+00:00
- **Authors**: Xianhang Li, Yali Wang, Zhipeng Zhou, Yu Qiao
- **Comment**: CVPR2020
- **Journal**: None
- **Summary**: Temporal convolution has been widely used for video classification. However, it is performed on spatio-temporal contexts in a limited view, which often weakens its capacity of learning video representation. To alleviate this problem, we propose a concise and novel SmallBig network, with the cooperation of small and big views. For the current time step, the small view branch is used to learn the core semantics, while the big view branch is used to capture the contextual semantics. Unlike traditional temporal convolution, the big view branch can provide the small view branch with the most activated video features from a broader 3D receptive field. Via aggregating such big-view contexts, the small view branch can learn more robust and discriminative spatio-temporal representations for video classification. Furthermore, we propose to share convolution in the small and big view branch, which improves model compactness as well as alleviates overfitting. As a result, our SmallBigNet achieves a comparable model size like 2D CNNs, while boosting accuracy like 3D CNNs. We conduct extensive experiments on the large-scale video benchmarks, e.g., Kinetics400, Something-Something V1 and V2. Our SmallBig network outperforms a number of recent state-of-the-art approaches, in terms of accuracy and/or efficiency. The codes and models will be available on https://github.com/xhl-video/SmallBigNet.



### A causal view of compositional zero-shot recognition
- **Arxiv ID**: http://arxiv.org/abs/2006.14610v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.14610v2)
- **Published**: 2020-06-25 17:51:22+00:00
- **Updated**: 2020-11-01 17:26:29+00:00
- **Authors**: Yuval Atzmon, Felix Kreuk, Uri Shalit, Gal Chechik
- **Comment**: (1) Accepted to NeurIPS 2020 (Spotlight) (2) Project page is at
  https://github.com/nv-research-israel/causal_comp (3) A video of our
  spotlight talk is at https://www.youtube.com/watch?v=IUAmwBylvyc
- **Journal**: None
- **Summary**: People easily recognize new visual categories that are new combinations of known components. This compositional generalization capacity is critical for learning in real-world domains like vision and language because the long tail of new combinations dominates the distribution. Unfortunately, learning systems struggle with compositional generalization because they often build on features that are correlated with class labels even if they are not "essential" for the class. This leads to consistent misclassification of samples from a new distribution, like new combinations of known components.   Here we describe an approach for compositional generalization that builds on causal ideas. First, we describe compositional zero-shot learning from a causal perspective, and propose to view zero-shot inference as finding "which intervention caused the image?". Second, we present a causal-inspired embedding model that learns disentangled representations of elementary components of visual objects from correlated (confounded) training data. We evaluate this approach on two datasets for predicting new combinations of attribute-object pairs: A well-controlled synthesized images dataset and a real-world dataset which consists of fine-grained types of shoes. We show improvements compared to strong baselines.



### Learning to simulate complex scenes
- **Arxiv ID**: http://arxiv.org/abs/2006.14611v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.14611v1)
- **Published**: 2020-06-25 17:51:34+00:00
- **Updated**: 2020-06-25 17:51:34+00:00
- **Authors**: Zhenfeng Xue, Weijie Mao, Liang Zheng
- **Comment**: 13 pages, 13 figures
- **Journal**: None
- **Summary**: Data simulation engines like Unity are becoming an increasingly important data source that allows us to acquire ground truth labels conveniently. Moreover, we can flexibly edit the content of an image in the engine, such as objects (position, orientation) and environments (illumination, occlusion). When using simulated data as training sets, its editable content can be leveraged to mimic the distribution of real-world data, and thus reduce the content difference between the synthetic and real domains. This paper explores content adaptation in the context of semantic segmentation, where the complex street scenes are fully synthesized using 19 classes of virtual objects from a first person driver perspective and controlled by 23 attributes. To optimize the attribute values and obtain a training set of similar content to real-world data, we propose a scalable discretization-and-relaxation (SDR) approach. Under a reinforcement learning framework, we formulate attribute optimization as a random-to-optimized mapping problem using a neural network. Our method has three characteristics. 1) Instead of editing attributes of individual objects, we focus on global attributes that have large influence on the scene structure, such as object density and illumination. 2) Attributes are quantized to discrete values, so as to reduce search space and training complexity. 3) Correlated attributes are jointly optimized in a group, so as to avoid meaningless scene structures and find better convergence points. Experiment shows our system can generate reasonable and useful scenes, from which we obtain promising real-world segmentation accuracy compared with existing synthetic training sets.



### Space-Time Correspondence as a Contrastive Random Walk
- **Arxiv ID**: http://arxiv.org/abs/2006.14613v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.14613v2)
- **Published**: 2020-06-25 17:56:05+00:00
- **Updated**: 2020-12-03 18:59:03+00:00
- **Authors**: Allan Jabri, Andrew Owens, Alexei A. Efros
- **Comment**: NeurIPS 2020 camera ready version -- Code at
  github.com/ajabri/videowalk
- **Journal**: None
- **Summary**: This paper proposes a simple self-supervised approach for learning a representation for visual correspondence from raw video. We cast correspondence as prediction of links in a space-time graph constructed from video. In this graph, the nodes are patches sampled from each frame, and nodes adjacent in time can share a directed edge. We learn a representation in which pairwise similarity defines transition probability of a random walk, so that long-range correspondence is computed as a walk along the graph. We optimize the representation to place high probability along paths of similarity. Targets for learning are formed without supervision, by cycle-consistency: the objective is to maximize the likelihood of returning to the initial node when walking along a graph constructed from a palindrome of frames. Thus, a single path-level constraint implicitly supervises chains of intermediate comparisons. When used as a similarity metric without adaptation, the learned representation outperforms the self-supervised state-of-the-art on label propagation tasks involving objects, semantic parts, and pose. Moreover, we demonstrate that a technique we call edge dropout, as well as self-supervised adaptation at test-time, further improve transfer for object-centric correspondence.



### LayoutTransformer: Layout Generation and Completion with Self-attention
- **Arxiv ID**: http://arxiv.org/abs/2006.14615v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.14615v2)
- **Published**: 2020-06-25 17:56:34+00:00
- **Updated**: 2021-09-30 16:44:42+00:00
- **Authors**: Kamal Gupta, Justin Lazarow, Alessandro Achille, Larry Davis, Vijay Mahadevan, Abhinav Shrivastava
- **Comment**: To appear at ICCV 2021
- **Journal**: None
- **Summary**: We address the problem of scene layout generation for diverse domains such as images, mobile applications, documents, and 3D objects. Most complex scenes, natural or human-designed, can be expressed as a meaningful arrangement of simpler compositional graphical primitives. Generating a new layout or extending an existing layout requires understanding the relationships between these primitives. To do this, we propose LayoutTransformer, a novel framework that leverages self-attention to learn contextual relationships between layout elements and generate novel layouts in a given domain. Our framework allows us to generate a new layout either from an empty set or from an initial seed set of primitives, and can easily scale to support an arbitrary of primitives per layout. Furthermore, our analyses show that the model is able to automatically capture the semantic properties of the primitives. We propose simple improvements in both representation of layout primitives, as well as training methods to demonstrate competitive performance in very diverse data domains such as object bounding boxes in natural images(COCO bounding box), documents (PubLayNet), mobile applications (RICO dataset) as well as 3D shapes (Part-Net). Code and other materials will be made available at https://kampta.github.io/layout.



### An Analysis of SVD for Deep Rotation Estimation
- **Arxiv ID**: http://arxiv.org/abs/2006.14616v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.14616v1)
- **Published**: 2020-06-25 17:58:28+00:00
- **Updated**: 2020-06-25 17:58:28+00:00
- **Authors**: Jake Levinson, Carlos Esteves, Kefan Chen, Noah Snavely, Angjoo Kanazawa, Afshin Rostamizadeh, Ameesh Makadia
- **Comment**: None
- **Journal**: None
- **Summary**: Symmetric orthogonalization via SVD, and closely related procedures, are well-known techniques for projecting matrices onto $O(n)$ or $SO(n)$. These tools have long been used for applications in computer vision, for example optimal 3D alignment problems solved by orthogonal Procrustes, rotation averaging, or Essential matrix decomposition. Despite its utility in different settings, SVD orthogonalization as a procedure for producing rotation matrices is typically overlooked in deep learning models, where the preferences tend toward classic representations like unit quaternions, Euler angles, and axis-angle, or more recently-introduced methods. Despite the importance of 3D rotations in computer vision and robotics, a single universally effective representation is still missing. Here, we explore the viability of SVD orthogonalization for 3D rotations in neural networks. We present a theoretical analysis that shows SVD is the natural choice for projecting onto the rotation group. Our extensive quantitative analysis shows simply replacing existing representations with the SVD orthogonalization procedure obtains state of the art performance in many deep learning applications covering both supervised and unsupervised training.



### Parametric Instance Classification for Unsupervised Visual Feature Learning
- **Arxiv ID**: http://arxiv.org/abs/2006.14618v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.14618v1)
- **Published**: 2020-06-25 17:59:13+00:00
- **Updated**: 2020-06-25 17:59:13+00:00
- **Authors**: Yue Cao, Zhenda Xie, Bin Liu, Yutong Lin, Zheng Zhang, Han Hu
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents parametric instance classification (PIC) for unsupervised visual feature learning. Unlike the state-of-the-art approaches which do instance discrimination in a dual-branch non-parametric fashion, PIC directly performs a one-branch parametric instance classification, revealing a simple framework similar to supervised classification and without the need to address the information leakage issue. We show that the simple PIC framework can be as effective as the state-of-the-art approaches, i.e. SimCLR and MoCo v2, by adapting several common component settings used in the state-of-the-art approaches. We also propose two novel techniques to further improve effectiveness and practicality of PIC: 1) a sliding-window data scheduler, instead of the previous epoch-based data scheduler, which addresses the extremely infrequent instance visiting issue in PIC and improves the effectiveness; 2) a negative sampling and weight update correction approach to reduce the training time and GPU memory consumption, which also enables application of PIC to almost unlimited training images. We hope that the PIC framework can serve as a simple baseline to facilitate future study.



### Determining Image similarity with Quasi-Euclidean Metric
- **Arxiv ID**: http://arxiv.org/abs/2006.14644v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.14644v1)
- **Published**: 2020-06-25 18:12:21+00:00
- **Updated**: 2020-06-25 18:12:21+00:00
- **Authors**: Vibhor Singh, Vishesh Devgan, Ishu Anand
- **Comment**: None
- **Journal**: None
- **Summary**: Image similarity is a core concept in Image Analysis due to its extensive application in computer vision, image processing, and pattern recognition. The objective of our study is to evaluate Quasi-Euclidean metric as an image similarity measure and analyze how it fares against the existing standard ways like SSIM and Euclidean metric. In this paper, we analyzed the similarity between two images from our own novice dataset and assessed its performance against the Euclidean distance metric and SSIM. We also present experimental results along with evidence indicating that our proposed implementation when applied to our novice dataset, furnished different results than standard metrics in terms of effectiveness and accuracy. In some cases, our methodology projected remarkable performance and it is also interesting to note that our implementation proves to be a step ahead in recognizing similarity when compared to



### Can 3D Adversarial Logos Cloak Humans?
- **Arxiv ID**: http://arxiv.org/abs/2006.14655v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.14655v2)
- **Published**: 2020-06-25 18:34:33+00:00
- **Updated**: 2020-11-27 07:18:55+00:00
- **Authors**: Yi Wang, Jingyang Zhou, Tianlong Chen, Sijia Liu, Shiyu Chang, Chandrajit Bajaj, Zhangyang Wang
- **Comment**: None
- **Journal**: None
- **Summary**: With the trend of adversarial attacks, researchers attempt to fool trained object detectors in 2D scenes. Among many of them, an intriguing new form of attack with potential real-world usage is to append adversarial patches (e.g. logos) to images. Nevertheless, much less have we known about adversarial attacks from 3D rendering views, which is essential for the attack to be persistently strong in the physical world. This paper presents a new 3D adversarial logo attack: we construct an arbitrary shape logo from a 2D texture image and map this image into a 3D adversarial logo via a texture mapping called logo transformation. The resulting 3D adversarial logo is then viewed as an adversarial texture enabling easy manipulation of its shape and position. This greatly extends the versatility of adversarial training for computer graphics synthesized imagery. Contrary to the traditional adversarial patch, this new form of attack is mapped into the 3D object world and back-propagates to the 2D image domain through differentiable rendering. In addition, and unlike existing adversarial patches, our new 3D adversarial logo is shown to fool state-of-the-art deep object detectors robustly under model rotations, leading to one step further for realistic attacks in the physical world. Our codes are available at https://github.com/TAMU-VITA/3D_Adversarial_Logo.



### SPSG: Self-Supervised Photometric Scene Generation from RGB-D Scans
- **Arxiv ID**: http://arxiv.org/abs/2006.14660v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.14660v2)
- **Published**: 2020-06-25 18:58:23+00:00
- **Updated**: 2021-04-28 15:15:45+00:00
- **Authors**: Angela Dai, Yawar Siddiqui, Justus Thies, Julien Valentin, Matthias Nießner
- **Comment**: Video: https://youtu.be/1cj962m9zqo
- **Journal**: None
- **Summary**: We present SPSG, a novel approach to generate high-quality, colored 3D models of scenes from RGB-D scan observations by learning to infer unobserved scene geometry and color in a self-supervised fashion. Our self-supervised approach learns to jointly inpaint geometry and color by correlating an incomplete RGB-D scan with a more complete version of that scan. Notably, rather than relying on 3D reconstruction losses to inform our 3D geometry and color reconstruction, we propose adversarial and perceptual losses operating on 2D renderings in order to achieve high-resolution, high-quality colored reconstructions of scenes. This exploits the high-resolution, self-consistent signal from individual raw RGB-D frames, in contrast to fused 3D reconstructions of the frames which exhibit inconsistencies from view-dependent effects, such as color balancing or pose inconsistencies. Thus, by informing our 3D scene generation directly through 2D signal, we produce high-quality colored reconstructions of 3D scenes, outperforming state of the art on both synthetic and real data.



### Fully Convolutional Open Set Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2006.14673v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.14673v1)
- **Published**: 2020-06-25 19:40:20+00:00
- **Updated**: 2020-06-25 19:40:20+00:00
- **Authors**: Hugo Oliveira, Caio Silva, Gabriel L. S. Machado, Keiller Nogueira, Jefersson A. dos Santos
- **Comment**: Submitted to the Machine Learning Journal
- **Journal**: None
- **Summary**: In semantic segmentation knowing about all existing classes is essential to yield effective results with the majority of existing approaches. However, these methods trained in a Closed Set of classes fail when new classes are found in the test phase. It means that they are not suitable for Open Set scenarios, which are very common in real-world computer vision and remote sensing applications. In this paper, we discuss the limitations of Closed Set segmentation and propose two fully convolutional approaches to effectively address Open Set semantic segmentation: OpenFCN and OpenPCS. OpenFCN is based on the well-known OpenMax algorithm, configuring a new application of this approach in segmentation settings. OpenPCS is a fully novel approach based on feature-space from DNN activations that serve as features for computing PCA and multi-variate gaussian likelihood in a lower dimensional space. Experiments were conducted on the well-known Vaihingen and Potsdam segmentation datasets. OpenFCN showed little-to-no improvement when compared to the simpler and much more time efficient SoftMax thresholding, while being between some orders of magnitude slower. OpenPCS achieved promising results in almost all experiments by overcoming both OpenFCN and SoftMax thresholding. OpenPCS is also a reasonable compromise between the runtime performances of the extremely fast SoftMax thresholding and the extremely slow OpenFCN, being close able to run close to real-time. Experiments also indicate that OpenPCS is effective, robust and suitable for Open Set segmentation, being able to improve the recognition of unknown class pixels without reducing the accuracy on the known class pixels.



### Duodepth: Static Gesture Recognition Via Dual Depth Sensors
- **Arxiv ID**: http://arxiv.org/abs/2006.14691v1
- **DOI**: 10.1109/ICIP.2019.8803665
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.14691v1)
- **Published**: 2020-06-25 20:41:47+00:00
- **Updated**: 2020-06-25 20:41:47+00:00
- **Authors**: Ilya Chugunov, Avideh Zakhor
- **Comment**: 26th International Conference on Image Processing
- **Journal**: 2019 IEEE International Conference on Image Processing (ICIP),
  Taipei, Taiwan, 2019, pp. 3467-3471
- **Summary**: Static gesture recognition is an effective non-verbal communication channel between a user and their devices; however many modern methods are sensitive to the relative pose of the user's hands with respect to the capture device, as parts of the gesture can become occluded. We present two methodologies for gesture recognition via synchronized recording from two depth cameras to alleviate this occlusion problem. One is a more classic approach using iterative closest point registration to accurately fuse point clouds and a single PointNet architecture for classification, and the other is a dual Point-Net architecture for classification without registration. On a manually collected data-set of 20,100 point clouds we show a 39.2% reduction in misclassification for the fused point cloud method, and 53.4% for the dual PointNet, when compared to a standard single camera pipeline.



### Adaptive additive classification-based loss for deep metric learning
- **Arxiv ID**: http://arxiv.org/abs/2006.14693v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.14693v1)
- **Published**: 2020-06-25 20:45:22+00:00
- **Updated**: 2020-06-25 20:45:22+00:00
- **Authors**: Istvan Fehervari, Ives Macedo
- **Comment**: None
- **Journal**: None
- **Summary**: Recent works have shown that deep metric learning algorithms can benefit from weak supervision from another input modality. This additional modality can be incorporated directly into the popular triplet-based loss function as distances. Also recently, classification loss and proxy-based metric learning have been observed to lead to faster convergence as well as better retrieval results, all the while without requiring complex and costly sampling strategies. In this paper we propose an extension to the existing adaptive margin for classification-based deep metric learning. Our extension introduces a separate margin for each negative proxy per sample. These margins are computed during training from precomputed distances of the classes in the other modality. Our results set a new state-of-the-art on both on the Amazon fashion retrieval dataset as well as on the public DeepFashion dataset. This was observed with both fastText- and BERT-based embeddings for the additional textual modality. Our results were achieved with faster convergence and lower code complexity than the prior state-of-the-art.



### Learning Data Augmentation with Online Bilevel Optimization for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2006.14699v2
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.14699v2)
- **Published**: 2020-06-25 21:01:52+00:00
- **Updated**: 2020-11-10 16:11:57+00:00
- **Authors**: Saypraseuth Mounsaveng, Issam Laradji, Ismail Ben Ayed, David Vazquez, Marco Pedersoli
- **Comment**: None
- **Journal**: None
- **Summary**: Data augmentation is a key practice in machine learning for improving generalization performance. However, finding the best data augmentation hyperparameters requires domain knowledge or a computationally demanding search. We address this issue by proposing an efficient approach to automatically train a network that learns an effective distribution of transformations to improve its generalization. Using bilevel optimization, we directly optimize the data augmentation parameters using a validation set. This framework can be used as a general solution to learn the optimal data augmentation jointly with an end task model like a classifier. Results show that our joint training method produces an image classification accuracy that is comparable to or better than carefully hand-crafted data augmentation. Yet, it does not need an expensive external validation loop on the data augmentation hyperparameters.



### Deep Q-Network-Driven Catheter Segmentation in 3D US by Hybrid Constrained Semi-Supervised Learning and Dual-UNet
- **Arxiv ID**: http://arxiv.org/abs/2006.14702v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.14702v1)
- **Published**: 2020-06-25 21:10:04+00:00
- **Updated**: 2020-06-25 21:10:04+00:00
- **Authors**: Hongxu Yang, Caifeng Shan, Alexander F. Kolen, Peter H. N. de With
- **Comment**: Accepted by MICCAI 2020
- **Journal**: None
- **Summary**: Catheter segmentation in 3D ultrasound is important for computer-assisted cardiac intervention. However, a large amount of labeled images are required to train a successful deep convolutional neural network (CNN) to segment the catheter, which is expensive and time-consuming. In this paper, we propose a novel catheter segmentation approach, which requests fewer annotations than the supervised learning method, but nevertheless achieves better performance. Our scheme considers a deep Q learning as the pre-localization step, which avoids voxel-level annotation and which can efficiently localize the target catheter. With the detected catheter, patch-based Dual-UNet is applied to segment the catheter in 3D volumetric data. To train the Dual-UNet with limited labeled images and leverage information of unlabeled images, we propose a novel semi-supervised scheme, which exploits unlabeled images based on hybrid constraints from predictions. Experiments show the proposed scheme achieves a higher performance than state-of-the-art semi-supervised methods, while it demonstrates that our method is able to learn from large-scale unlabeled images.



### CPL-SLAM: Efficient and Certifiably Correct Planar Graph-Based SLAM Using the Complex Number Representation
- **Arxiv ID**: http://arxiv.org/abs/2007.06708v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2007.06708v1)
- **Published**: 2020-06-25 21:17:50+00:00
- **Updated**: 2020-06-25 21:17:50+00:00
- **Authors**: Taosha Fan, Hanlin Wang, Michael Rubenstein, Todd Murphey
- **Comment**: None
- **Journal**: IEEE Transactions on Robotics, 2020
- **Summary**: In this paper, we consider the problem of planar graph-based simultaneous localization and mapping (SLAM) that involves both poses of the autonomous agent and positions of observed landmarks. We present CPL-SLAM, an efficient and certifiably correct algorithm to solve planar graph-based SLAM using the complex number representation. We formulate and simplify planar graph-based SLAM as the maximum likelihood estimation (MLE) on the product of unit complex numbers, and relax this nonconvex quadratic complex optimization problem to convex complex semidefinite programming (SDP). Furthermore, we simplify the corresponding complex semidefinite programming to Riemannian staircase optimization (RSO) on the complex oblique manifold that can be solved with the Riemannian trust region (RTR) method. In addition, we prove that the SDP relaxation and RSO simplification are tight as long as the noise magnitude is below a certain threshold. The efficacy of this work is validated through applications of CPL-SLAM and comparisons with existing state-of-the-art methods on planar graph-based SLAM, which indicates that our proposed algorithm is capable of solving planar graph-based SLAM certifiably, and is more efficient in numerical computation and more robust to measurement noise than existing state-of-the-art methods. The C++ code for CPL-SLAM is available at https://github.com/MurpheyLab/CPL-SLAM.



### Perspective Plane Program Induction from a Single Image
- **Arxiv ID**: http://arxiv.org/abs/2006.14708v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.14708v1)
- **Published**: 2020-06-25 21:18:58+00:00
- **Updated**: 2020-06-25 21:18:58+00:00
- **Authors**: Yikai Li, Jiayuan Mao, Xiuming Zhang, William T. Freeman, Joshua B. Tenenbaum, Jiajun Wu
- **Comment**: CVPR 2020. First two authors contributed equally. Project page:
  http://p3i.csail.mit.edu/
- **Journal**: None
- **Summary**: We study the inverse graphics problem of inferring a holistic representation for natural images. Given an input image, our goal is to induce a neuro-symbolic, program-like representation that jointly models camera poses, object locations, and global scene structures. Such high-level, holistic scene representations further facilitate low-level image manipulation tasks such as inpainting. We formulate this problem as jointly finding the camera pose and scene structure that best describe the input image. The benefits of such joint inference are two-fold: scene regularity serves as a new cue for perspective correction, and in turn, correct perspective correction leads to a simplified scene structure, similar to how the correct shape leads to the most regular texture in shape from texture. Our proposed framework, Perspective Plane Program Induction (P3I), combines search-based and gradient-based algorithms to efficiently solve the problem. P3I outperforms a set of baselines on a collection of Internet images, across tasks including camera pose estimation, global structure inference, and down-stream image manipulation tasks.



### Investigating and Exploiting Image Resolution for Transfer Learning-based Skin Lesion Classification
- **Arxiv ID**: http://arxiv.org/abs/2006.14715v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.14715v1)
- **Published**: 2020-06-25 21:51:24+00:00
- **Updated**: 2020-06-25 21:51:24+00:00
- **Authors**: Amirreza Mahbod, Gerald Schaefer, Chunliang Wang, Rupert Ecker, Georg Dorffner, Isabella Ellinger
- **Comment**: Accepted for the 25th International Conference on Pattern Recognition
  (ICPR 2020)
- **Journal**: None
- **Summary**: Skin cancer is among the most common cancer types. Dermoscopic image analysis improves the diagnostic accuracy for detection of malignant melanoma and other pigmented skin lesions when compared to unaided visual inspection. Hence, computer-based methods to support medical experts in the diagnostic procedure are of great interest. Fine-tuning pre-trained convolutional neural networks (CNNs) has been shown to work well for skin lesion classification. Pre-trained CNNs are usually trained with natural images of a fixed image size which is typically significantly smaller than captured skin lesion images and consequently dermoscopic images are downsampled for fine-tuning. However, useful medical information may be lost during this transformation. In this paper, we explore the effect of input image size on skin lesion classification performance of fine-tuned CNNs. For this, we resize dermoscopic images to different resolutions, ranging from 64x64 to 768x768 pixels and investigate the resulting classification performance of three well-established CNNs, namely DenseNet-121, ResNet-18, and ResNet-50. Our results show that using very small images (of size 64x64 pixels) degrades the classification performance, while images of size 128x128 pixels and above support good performance with larger image sizes leading to slightly improved classification. We further propose a novel fusion approach based on a three-level ensemble strategy that exploits multiple fine-tuned networks trained with dermoscopic images at various sizes. When applied on the ISIC 2017 skin lesion classification challenge, our fusion approach yields an area under the receiver operating characteristic curve of 89.2% and 96.6% for melanoma classification and seborrheic keratosis classification, respectively, outperforming state-of-the-art algorithms.



### Teaching CNNs to mimic Human Visual Cognitive Process & regularise Texture-Shape bias
- **Arxiv ID**: http://arxiv.org/abs/2006.14722v2
- **DOI**: 10.1109/ICASSP43922.2022.9747796
- **Categories**: **cs.CV**, cs.AI, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.14722v2)
- **Published**: 2020-06-25 22:32:54+00:00
- **Updated**: 2022-01-10 00:43:26+00:00
- **Authors**: Satyam Mohla, Anshul Nasery, Biplab Banerjee
- **Comment**: Submitted at ICASSP 2022; 5 Pages; LaTex;
- **Journal**: 2022 IEEE International Conference on Acoustics, Speech and Signal
  Processing (ICASSP)
- **Summary**: Recent experiments in computer vision demonstrate texture bias as the primary reason for supreme results in models employing Convolutional Neural Networks (CNNs), conflicting with early works claiming that these networks identify objects using shape. It is believed that the cost function forces the CNN to take a greedy approach and develop a proclivity for local information like texture to increase accuracy, thus failing to explore any global statistics. We propose CognitiveCNN, a new intuitive architecture, inspired from feature integration theory in psychology to utilise human interpretable feature like shape, texture, edges etc. to reconstruct, and classify the image. We define novel metrics to quantify the "relevance" of "abstract information" present in these modalities using attention maps. We further introduce a regularisation method which ensures that each modality like shape, texture etc. gets proportionate influence in a given task, as it does for reconstruction; and perform experiments to show the resulting boost in accuracy and robustness, besides imparting explainability to these CNNs for achieving superior performance in object recognition.



### Unsupervised Video Decomposition using Spatio-temporal Iterative Inference
- **Arxiv ID**: http://arxiv.org/abs/2006.14727v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.14727v1)
- **Published**: 2020-06-25 22:57:17+00:00
- **Updated**: 2020-06-25 22:57:17+00:00
- **Authors**: Polina Zablotskaia, Edoardo A. Dominici, Leonid Sigal, Andreas M. Lehrmann
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised multi-object scene decomposition is a fast-emerging problem in representation learning. Despite significant progress in static scenes, such models are unable to leverage important dynamic cues present in video. We propose a novel spatio-temporal iterative inference framework that is powerful enough to jointly model complex multi-object representations and explicit temporal dependencies between latent variables across frames. This is achieved by leveraging 2D-LSTM, temporally conditioned inference and generation within the iterative amortized inference for posterior refinement. Our method improves the overall quality of decompositions, encodes information about the objects' dynamics, and can be used to predict trajectories of each object separately. Additionally, we show that our model has a high accuracy even without color information. We demonstrate the decomposition, segmentation, and prediction capabilities of our model and show that it outperforms the state-of-the-art on several benchmark datasets, one of which was curated for this work and will be made publicly available.



