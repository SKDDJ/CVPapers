# Arxiv Papers in cs.CV on 2020-06-17
### Pain Intensity Estimation from Mobile Video Using 2D and 3D Facial Keypoints
- **Arxiv ID**: http://arxiv.org/abs/2006.12246v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.12246v1)
- **Published**: 2020-06-17 00:18:29+00:00
- **Updated**: 2020-06-17 00:18:29+00:00
- **Authors**: Matthew Lee, Lyndon Kennedy, Andreas Girgensohn, Lynn Wilcox, John Song En Lee, Chin Wen Tan, Ban Leong Sng
- **Comment**: None
- **Journal**: None
- **Summary**: Managing post-surgical pain is critical for successful surgical outcomes. One of the challenges of pain management is accurately assessing the pain level of patients. Self-reported numeric pain ratings are limited because they are subjective, can be affected by mood, and can influence the patient's perception of pain when making comparisons. In this paper, we introduce an approach that analyzes 2D and 3D facial keypoints of post-surgical patients to estimate their pain intensity level. Our approach leverages the previously unexplored capabilities of a smartphone to capture a dense 3D representation of a person's face as input for pain intensity level estimation. Our contributions are adata collection study with post-surgical patients to collect ground-truth labeled sequences of 2D and 3D facial keypoints for developing a pain estimation algorithm, a pain estimation model that uses multiple instance learning to overcome inherent limitations in facial keypoint sequences, and the preliminary results of the pain estimation model using 2D and 3D features with comparisons of alternate approaches.



### Cross-Correlated Attention Networks for Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2006.09597v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.09597v1)
- **Published**: 2020-06-17 01:47:23+00:00
- **Updated**: 2020-06-17 01:47:23+00:00
- **Authors**: Jieming Zhou, Soumava Kumar Roy, Pengfei Fang, Mehrtash Harandi, Lars Petersson
- **Comment**: Accepted by Image and Vision Computing
- **Journal**: Image and Vision Computing, Vol. 100, 2020, p. 103931
- **Summary**: Deep neural networks need to make robust inference in the presence of occlusion, background clutter, pose and viewpoint variations -- to name a few -- when the task of person re-identification is considered. Attention mechanisms have recently proven to be successful in handling the aforementioned challenges to some degree. However previous designs fail to capture inherent inter-dependencies between the attended features; leading to restricted interactions between the attention blocks. In this paper, we propose a new attention module called Cross-Correlated Attention (CCA); which aims to overcome such limitations by maximizing the information gain between different attended regions. Moreover, we also propose a novel deep network that makes use of different attention mechanisms to learn robust and discriminative representations of person images. The resulting model is called the Cross-Correlated Attention Network (CCAN). Extensive experiments demonstrate that the CCAN comfortably outperforms current state-of-the-art algorithms by a tangible margin.



### Exploring Sparsity in Image Super-Resolution for Efficient Inference
- **Arxiv ID**: http://arxiv.org/abs/2006.09603v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.09603v2)
- **Published**: 2020-06-17 02:08:26+00:00
- **Updated**: 2021-04-01 11:23:48+00:00
- **Authors**: Longguang Wang, Xiaoyu Dong, Yingqian Wang, Xinyi Ying, Zaiping Lin, Wei An, Yulan Guo
- **Comment**: Accepted by CVPR 2021
- **Journal**: None
- **Summary**: Current CNN-based super-resolution (SR) methods process all locations equally with computational resources being uniformly assigned in space. However, since missing details in low-resolution (LR) images mainly exist in regions of edges and textures, less computational resources are required for those flat regions. Therefore, existing CNN-based methods involve redundant computation in flat regions, which increases their computational cost and limits their applications on mobile devices. In this paper, we explore the sparsity in image SR to improve inference efficiency of SR networks. Specifically, we develop a Sparse Mask SR (SMSR) network to learn sparse masks to prune redundant computation. Within our SMSR, spatial masks learn to identify "important" regions while channel masks learn to mark redundant channels in those "unimportant" regions. Consequently, redundant computation can be accurately localized and skipped while maintaining comparable performance. It is demonstrated that our SMSR achieves state-of-the-art performance with 41%/33%/27% FLOPs being reduced for x2/3/4 SR. Code is available at: https://github.com/LongguangWang/SMSR.



### Multi-Subspace Neural Network for Image Recognition
- **Arxiv ID**: http://arxiv.org/abs/2006.09618v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.09618v1)
- **Published**: 2020-06-17 02:55:34+00:00
- **Updated**: 2020-06-17 02:55:34+00:00
- **Authors**: Chieh-Ning Fang, Chin-Teng Lin
- **Comment**: None
- **Journal**: None
- **Summary**: In image classification task, feature extraction is always a big issue. Intra-class variability increases the difficulty in designing the extractors. Furthermore, hand-crafted feature extractor cannot simply adapt new situation. Recently, deep learning has drawn lots of attention on automatically learning features from data. In this study, we proposed multi-subspace neural network (MSNN) which integrates key components of the convolutional neural network (CNN), receptive field, with subspace concept. Associating subspace with the deep network is a novel designing, providing various viewpoints of data. Basis vectors, trained by adaptive subspace self-organization map (ASSOM) span the subspace, serve as a transfer function to access axial components and define the receptive field to extract basic patterns of data without distorting the topology in the visual task. Moreover, the multiple-subspace strategy is implemented as parallel blocks to adapt real-world data and contribute various interpretations of data hoping to be more robust dealing with intra-class variability issues. To this end, handwritten digit and object image datasets (i.e., MNIST and COIL-20) for classification are employed to validate the proposed MSNN architecture. Experimental results show MSNN is competitive to other state-of-the-art approaches.



### Learning Visual Commonsense for Robust Scene Graph Generation
- **Arxiv ID**: http://arxiv.org/abs/2006.09623v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.09623v2)
- **Published**: 2020-06-17 03:07:53+00:00
- **Updated**: 2020-07-18 11:10:45+00:00
- **Authors**: Alireza Zareian, Zhecan Wang, Haoxuan You, Shih-Fu Chang
- **Comment**: To be presented at ECCV 2020
- **Journal**: None
- **Summary**: Scene graph generation models understand the scene through object and predicate recognition, but are prone to mistakes due to the challenges of perception in the wild. Perception errors often lead to nonsensical compositions in the output scene graph, which do not follow real-world rules and patterns, and can be corrected using commonsense knowledge. We propose the first method to acquire visual commonsense such as affordance and intuitive physics automatically from data, and use that to improve the robustness of scene understanding. To this end, we extend Transformer models to incorporate the structure of scene graphs, and train our Global-Local Attention Transformer on a scene graph corpus. Once trained, our model can be applied on any scene graph generation model and correct its obvious mistakes, resulting in more semantically plausible scene graphs. Through extensive experiments, we show our model learns commonsense better than any alternative, and improves the accuracy of state-of-the-art scene graph generation methods.



### Visor: Privacy-Preserving Video Analytics as a Cloud Service
- **Arxiv ID**: http://arxiv.org/abs/2006.09628v2
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.09628v2)
- **Published**: 2020-06-17 03:25:11+00:00
- **Updated**: 2020-06-23 04:37:24+00:00
- **Authors**: Rishabh Poddar, Ganesh Ananthanarayanan, Srinath Setty, Stavros Volos, Raluca Ada Popa
- **Comment**: USENIX Security 2020
- **Journal**: None
- **Summary**: Video-analytics-as-a-service is becoming an important offering for cloud providers. A key concern in such services is privacy of the videos being analyzed. While trusted execution environments (TEEs) are promising options for preventing the direct leakage of private video content, they remain vulnerable to side-channel attacks.   We present Visor, a system that provides confidentiality for the user's video stream as well as the ML models in the presence of a compromised cloud platform and untrusted co-tenants. Visor executes video pipelines in a hybrid TEE that spans both the CPU and GPU. It protects the pipeline against side-channel attacks induced by data-dependent access patterns of video modules, and also addresses leakage in the CPU-GPU communication channel. Visor is up to $1000\times$ faster than na\"ive oblivious solutions, and its overheads relative to a non-oblivious baseline are limited to $2\times$--$6\times$.



### Self-Supervised Representation Learning for Visual Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/2006.09654v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.09654v1)
- **Published**: 2020-06-17 04:37:29+00:00
- **Updated**: 2020-06-17 04:37:29+00:00
- **Authors**: Rabia Ali, Muhammad Umar Karim Khan, Chong Min Kyung
- **Comment**: None
- **Journal**: None
- **Summary**: Self-supervised learning allows for better utilization of unlabelled data. The feature representation obtained by self-supervision can be used in downstream tasks such as classification, object detection, segmentation, and anomaly detection. While classification, object detection, and segmentation have been investigated with self-supervised learning, anomaly detection needs more attention. We consider the problem of anomaly detection in images and videos, and present a new visual anomaly detection technique for videos. Numerous seminal and state-of-the-art self-supervised methods are evaluated for anomaly detection on a variety of image datasets. The best performing image-based self-supervised representation learning method is then used for video anomaly detection to see the importance of spatial features in visual anomaly detection in videos. We also propose a simple self-supervision approach for learning temporal coherence across video frames without the use of any optical flow information. At its core, our method identifies the frame indices of a jumbled video sequence allowing it to learn the spatiotemporal features of the video. This intuitive approach shows superior performance of visual anomaly detection compared to numerous methods for images and videos on UCF101 and ILSVRC2015 video datasets.



### Implicit Neural Representations with Periodic Activation Functions
- **Arxiv ID**: http://arxiv.org/abs/2006.09661v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.09661v1)
- **Published**: 2020-06-17 05:13:33+00:00
- **Updated**: 2020-06-17 05:13:33+00:00
- **Authors**: Vincent Sitzmann, Julien N. P. Martel, Alexander W. Bergman, David B. Lindell, Gordon Wetzstein
- **Comment**: Project website: https://vsitzmann.github.io/siren/ Project video:
  https://youtu.be/Q2fLWGBeaiI
- **Journal**: None
- **Summary**: Implicitly defined, continuous, differentiable signal representations parameterized by neural networks have emerged as a powerful paradigm, offering many possible benefits over conventional representations. However, current network architectures for such implicit neural representations are incapable of modeling signals with fine detail, and fail to represent a signal's spatial and temporal derivatives, despite the fact that these are essential to many physical signals defined implicitly as the solution to partial differential equations. We propose to leverage periodic activation functions for implicit neural representations and demonstrate that these networks, dubbed sinusoidal representation networks or Sirens, are ideally suited for representing complex natural signals and their derivatives. We analyze Siren activation statistics to propose a principled initialization scheme and demonstrate the representation of images, wavefields, video, sound, and their derivatives. Further, we show how Sirens can be leveraged to solve challenging boundary value problems, such as particular Eikonal equations (yielding signed distance functions), the Poisson equation, and the Helmholtz and wave equations. Lastly, we combine Sirens with hypernetworks to learn priors over the space of Siren functions.



### MetaSDF: Meta-learning Signed Distance Functions
- **Arxiv ID**: http://arxiv.org/abs/2006.09662v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.09662v1)
- **Published**: 2020-06-17 05:14:53+00:00
- **Updated**: 2020-06-17 05:14:53+00:00
- **Authors**: Vincent Sitzmann, Eric R. Chan, Richard Tucker, Noah Snavely, Gordon Wetzstein
- **Comment**: Project website: https://vsitzmann.github.io/metasdf/
- **Journal**: None
- **Summary**: Neural implicit shape representations are an emerging paradigm that offers many potential benefits over conventional discrete representations, including memory efficiency at a high spatial resolution. Generalizing across shapes with such neural implicit representations amounts to learning priors over the respective function space and enables geometry reconstruction from partial or noisy observations. Existing generalization methods rely on conditioning a neural network on a low-dimensional latent code that is either regressed by an encoder or jointly optimized in the auto-decoder framework. Here, we formalize learning of a shape space as a meta-learning problem and leverage gradient-based meta-learning algorithms to solve this task. We demonstrate that this approach performs on par with auto-decoder based approaches while being an order of magnitude faster at test-time inference. We further demonstrate that the proposed gradient-based method outperforms encoder-decoder based methods that leverage pooling-based set encoders.



### XRayGAN: Consistency-preserving Generation of X-ray Images from Radiology Reports
- **Arxiv ID**: http://arxiv.org/abs/2006.10552v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.10552v1)
- **Published**: 2020-06-17 05:32:14+00:00
- **Updated**: 2020-06-17 05:32:14+00:00
- **Authors**: Xingyi Yang, Nandiraju Gireesh, Eric Xing, Pengtao Xie
- **Comment**: None
- **Journal**: None
- **Summary**: To effectively train medical students to become qualified radiologists, a large number of X-ray images collected from patients with diverse medical conditions are needed. However, due to data privacy concerns, such images are typically difficult to obtain. To address this problem, we develop methods to generate view-consistent, high-fidelity, and high-resolution X-ray images from radiology reports to facilitate radiology training of medical students. This task is presented with several challenges. First, from a single report, images with different views (e.g., frontal, lateral) need to be generated. How to ensure consistency of these images (i.e., make sure they are about the same patient)? Second, X-ray images are required to have high resolution. Otherwise, many details of diseases would be lost. How to generate high-resolutions images? Third, radiology reports are long and have complicated structure. How to effectively understand their semantics to generate high-fidelity images that accurately reflect the contents of the reports? To address these three challenges, we propose an XRayGAN composed of three modules: (1) a view consistency network that maximizes the consistency between generated frontal-view and lateral-view images; (2) a multi-scale conditional GAN that progressively generates a cascade of images with increasing resolution; (3) a hierarchical attentional encoder that learns the latent semantics of a radiology report by capturing its hierarchical linguistic structure and various levels of clinical importance of words and sentences. Experiments on two radiology datasets demonstrate the effectiveness of our methods. To our best knowledge, this work represents the first one generating consistent and high-resolution X-ray images from radiology reports. The code is available at https://github.com/UCSD-AI4H/XRayGAN.



### Enhancing Few-Shot Image Classification with Unlabelled Examples
- **Arxiv ID**: http://arxiv.org/abs/2006.12245v6
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.12245v6)
- **Published**: 2020-06-17 05:42:47+00:00
- **Updated**: 2021-10-21 17:59:05+00:00
- **Authors**: Peyman Bateni, Jarred Barber, Jan-Willem van de Meent, Frank Wood
- **Comment**: None
- **Journal**: None
- **Summary**: We develop a transductive meta-learning method that uses unlabelled instances to improve few-shot image classification performance. Our approach combines a regularized Mahalanobis-distance-based soft k-means clustering procedure with a modified state of the art neural adaptive feature extractor to achieve improved test-time classification accuracy using unlabelled data. We evaluate our method on transductive few-shot learning tasks, in which the goal is to jointly predict labels for query (test) examples given a set of support (training) examples. We achieve state of the art performance on the Meta-Dataset, mini-ImageNet and tiered-ImageNet benchmarks. All trained models and code have been made publicly available at github.com/plai-group/simple-cnaps.



### Unsupervised Learning of Global Registration of Temporal Sequence of Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2006.12378v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.12378v1)
- **Published**: 2020-06-17 06:00:36+00:00
- **Updated**: 2020-06-17 06:00:36+00:00
- **Authors**: Lingjing Wang, Yi Shi, Xiang Li, Yi Fang
- **Comment**: None
- **Journal**: None
- **Summary**: Global registration of point clouds aims to find an optimal alignment of a sequence of 2D or 3D point sets. In this paper, we present a novel method that takes advantage of current deep learning techniques for unsupervised learning of global registration from a temporal sequence of point clouds. Our key novelty is that we introduce a deep Spatio-Temporal REPresentation (STREP) feature, which describes the geometric essence of both temporal and spatial relationship of the sequence of point clouds acquired with sensors in an unknown environment. In contrast to the previous practice that treats each time step (pair-wise registration) individually, our unsupervised model starts with optimizing a sequence of latent STREP feature, which is then decoded to a temporally and spatially continuous sequence of geometric transformations to globally align multiple point clouds. We have evaluated our proposed approach over both simulated 2D and real 3D datasets and the experimental results demonstrate that our method can beat other techniques by taking into account the temporal information in deep feature learning.



### Revealing the Invisible with Model and Data Shrinking for Composite-database Micro-expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/2006.09674v1
- **DOI**: 10.1109/TIP.2020.3018222
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.09674v1)
- **Published**: 2020-06-17 06:19:24+00:00
- **Updated**: 2020-06-17 06:19:24+00:00
- **Authors**: Zhaoqiang Xia, Wei Peng, Huai-Qian Khor, Xiaoyi Feng, Guoying Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: Composite-database micro-expression recognition is attracting increasing attention as it is more practical to real-world applications. Though the composite database provides more sample diversity for learning good representation models, the important subtle dynamics are prone to disappearing in the domain shift such that the models greatly degrade their performance, especially for deep models. In this paper, we analyze the influence of learning complexity, including the input complexity and model complexity, and discover that the lower-resolution input data and shallower-architecture model are helpful to ease the degradation of deep models in composite-database task. Based on this, we propose a recurrent convolutional network (RCN) to explore the shallower-architecture and lower-resolution input data, shrinking model and input complexities simultaneously. Furthermore, we develop three parameter-free modules (i.e., wide expansion, shortcut connection and attention unit) to integrate with RCN without increasing any learnable parameters. These three modules can enhance the representation ability in various perspectives while preserving not-very-deep architecture for lower-resolution data. Besides, three modules can further be combined by an automatic strategy (a neural architecture search strategy) and the searched architecture becomes more robust. Extensive experiments on MEGC2019 dataset (composited of existing SMIC, CASME II and SAMM datasets) have verified the influence of learning complexity and shown that RCNs with three modules and the searched combination outperform the state-of-the-art approaches.



### A Real-time Action Representation with Temporal Encoding and Deep Compression
- **Arxiv ID**: http://arxiv.org/abs/2006.09675v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.09675v1)
- **Published**: 2020-06-17 06:30:43+00:00
- **Updated**: 2020-06-17 06:30:43+00:00
- **Authors**: Kun Liu, Wu Liu, Huadong Ma, Mingkui Tan, Chuang Gan
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks have achieved remarkable success for video-based action recognition. However, most of existing approaches cannot be deployed in practice due to the high computational cost. To address this challenge, we propose a new real-time convolutional architecture, called Temporal Convolutional 3D Network (T-C3D), for action representation. T-C3D learns video action representations in a hierarchical multi-granularity manner while obtaining a high process speed. Specifically, we propose a residual 3D Convolutional Neural Network (CNN) to capture complementary information on the appearance of a single frame and the motion between consecutive frames. Based on this CNN, we develop a new temporal encoding method to explore the temporal dynamics of the whole video. Furthermore, we integrate deep compression techniques with T-C3D to further accelerate the deployment of models via reducing the size of the model. By these means, heavy calculations can be avoided when doing the inference, which enables the method to deal with videos beyond real-time speed while keeping promising performance. Our method achieves clear improvements on UCF101 action recognition benchmark against state-of-the-art real-time methods by 5.4% in terms of accuracy and 2 times faster in terms of inference speed with a less than 5MB storage model. We validate our approach by studying its action representation performance on four different benchmarks over three different tasks. Extensive experiments demonstrate comparable recognition performance to the state-of-the-art methods. The source code and the pre-trained models are publicly available at https://github.com/tc3d.



### FrostNet: Towards Quantization-Aware Network Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/2006.09679v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.09679v4)
- **Published**: 2020-06-17 06:40:43+00:00
- **Updated**: 2020-11-30 10:09:33+00:00
- **Authors**: Taehoon Kim, YoungJoon Yoo, Jihoon Yang
- **Comment**: None
- **Journal**: None
- **Summary**: INT8 quantization has become one of the standard techniques for deploying convolutional neural networks (CNNs) on edge devices to reduce the memory and computational resource usages. By analyzing quantized performances of existing mobile-target network architectures, we can raise an issue regarding the importance of network architecture for optimal INT8 quantization. In this paper, we present a new network architecture search (NAS) procedure to find a network that guarantees both full-precision (FLOAT32) and quantized (INT8) performances. We first propose critical but straightforward optimization method which enables quantization-aware training (QAT) : floating-point statistic assisting (StatAssist) and stochastic gradient boosting (GradBoost). By integrating the gradient-based NAS with StatAssist and GradBoost, we discovered a quantization-efficient network building block, Frost bottleneck. Furthermore, we used Frost bottleneck as the building block for hardware-aware NAS to obtain quantization-efficient networks, FrostNets, which show improved quantization performances compared to other mobile-target networks while maintaining competitive FLOAT32 performance. Our FrostNets achieve higher recognition accuracy than existing CNNs with comparable latency when quantized, due to higher latency reduction rate (average 65%).



### 3D Shape Reconstruction from Free-Hand Sketches
- **Arxiv ID**: http://arxiv.org/abs/2006.09694v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.09694v2)
- **Published**: 2020-06-17 07:43:10+00:00
- **Updated**: 2022-01-19 03:35:23+00:00
- **Authors**: Jiayun Wang, Jierui Lin, Qian Yu, Runtao Liu, Yubei Chen, Stella X. Yu
- **Comment**: None
- **Journal**: None
- **Summary**: Sketches are the most abstract 2D representations of real-world objects. Although a sketch usually has geometrical distortion and lacks visual cues, humans can effortlessly envision a 3D object from it. This suggests that sketches encode the information necessary for reconstructing 3D shapes. Despite great progress achieved in 3D reconstruction from distortion-free line drawings, such as CAD and edge maps, little effort has been made to reconstruct 3D shapes from free-hand sketches. We study this task and aim to enhance the power of sketches in 3D-related applications such as interactive design and VR/AR games.   Unlike previous works, which mostly study distortion-free line drawings, our 3D shape reconstruction is based on free-hand sketches. A major challenge for free-hand sketch 3D reconstruction comes from the insufficient training data and free-hand sketch diversity, e.g. individualized sketching styles. We thus propose data generation and standardization mechanisms. Instead of distortion-free line drawings, synthesized sketches are adopted as input training data. Additionally, we propose a sketch standardization module to handle different sketch distortions and styles. Extensive experiments demonstrate the effectiveness of our model and its strong generalizability to various free-hand sketches. Our code is publicly available at https://github.com/samaonline/3D-Shape-Reconstruction-from-Free-Hand-Sketches.



### Adversarial Defense by Latent Style Transformations
- **Arxiv ID**: http://arxiv.org/abs/2006.09701v2
- **DOI**: 10.1109/TIFS.2022.3155975
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.09701v2)
- **Published**: 2020-06-17 07:56:36+00:00
- **Updated**: 2022-02-22 10:23:36+00:00
- **Authors**: Shuo Wang, Surya Nepal, Alsharif Abuadbba, Carsten Rudolph, Marthie Grobler
- **Comment**: None
- **Journal**: None
- **Summary**: Machine learning models have demonstrated vulnerability to adversarial attacks, more specifically misclassification of adversarial examples.   In this paper, we investigate an attack-agnostic defense against adversarial attacks on high-resolution images by detecting suspicious inputs.   The intuition behind our approach is that the essential characteristics of a normal image are generally consistent with non-essential style transformations, e.g., slightly changing the facial expression of human portraits.   In contrast, adversarial examples are generally sensitive to such transformations.   In our approach to detect adversarial instances, we propose an in\underline{V}ertible \underline{A}utoencoder based on the \underline{S}tyleGAN2 generator via \underline{A}dversarial training (VASA) to inverse images to disentangled latent codes that reveal hierarchical styles.   We then build a set of edited copies with non-essential style transformations by performing latent shifting and reconstruction, based on the correspondences between latent codes and style transformations.   The classification-based consistency of these edited copies is used to distinguish adversarial instances.



### Neural Anisotropy Directions
- **Arxiv ID**: http://arxiv.org/abs/2006.09717v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.09717v2)
- **Published**: 2020-06-17 08:36:28+00:00
- **Updated**: 2020-10-14 10:21:58+00:00
- **Authors**: Guillermo Ortiz-Jimenez, Apostolos Modas, Seyed-Mohsen Moosavi-Dezfooli, Pascal Frossard
- **Comment**: Accepted to the 34th Conference on Neural Information Processing
  Systems (NeurIPS 2020) (39 pages, 22 figures)
- **Journal**: None
- **Summary**: In this work, we analyze the role of the network architecture in shaping the inductive bias of deep classifiers. To that end, we start by focusing on a very simple problem, i.e., classifying a class of linearly separable distributions, and show that, depending on the direction of the discriminative feature of the distribution, many state-of-the-art deep convolutional neural networks (CNNs) have a surprisingly hard time solving this simple task. We then define as neural anisotropy directions (NADs) the vectors that encapsulate the directional inductive bias of an architecture. These vectors, which are specific for each architecture and hence act as a signature, encode the preference of a network to separate the input data based on some particular features. We provide an efficient method to identify NADs for several CNN architectures and thus reveal their directional inductive biases. Furthermore, we show that, for the CIFAR-10 dataset, NADs characterize the features used by CNNs to discriminate between different classes.



### LRPD: Long Range 3D Pedestrian Detection Leveraging Specific Strengths of LiDAR and RGB
- **Arxiv ID**: http://arxiv.org/abs/2006.09738v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.09738v1)
- **Published**: 2020-06-17 09:27:38+00:00
- **Updated**: 2020-06-17 09:27:38+00:00
- **Authors**: Michael Fürst, Oliver Wasenmüller, Didier Stricker
- **Comment**: 7 Pages, 5 Figures, Autonomous Vehicles, 3D Object Detection
- **Journal**: None
- **Summary**: While short range 3D pedestrian detection is sufficient for emergency breaking, long range detections are required for smooth breaking and gaining trust in autonomous vehicles. The current state-of-the-art on the KITTI benchmark performs suboptimal in detecting the position of pedestrians at long range. Thus, we propose an approach specifically targeting long range 3D pedestrian detection (LRPD), leveraging the density of RGB and the precision of LiDAR. Therefore, for proposals, RGB instance segmentation and LiDAR point based proposal generation are combined, followed by a second stage using both sensor modalities symmetrically. This leads to a significant improvement in mAP on long range compared to the current state-of-the art. The evaluation of our LRPD approach was done on the pedestrians from the KITTI benchmark.



### Probabilistic orientation estimation with matrix Fisher distributions
- **Arxiv ID**: http://arxiv.org/abs/2006.09740v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.09740v1)
- **Published**: 2020-06-17 09:28:19+00:00
- **Updated**: 2020-06-17 09:28:19+00:00
- **Authors**: D. Mohlin, G. Bianchi, J. Sullivan
- **Comment**: 20 pages, 11 figures, submitted to NeurIPS
- **Journal**: None
- **Summary**: This paper focuses on estimating probability distributions over the set of 3D rotations ($SO(3)$) using deep neural networks. Learning to regress models to the set of rotations is inherently difficult due to differences in topology between $\mathbb{R}^N$ and $SO(3)$. We overcome this issue by using a neural network to output the parameters for a matrix Fisher distribution since these parameters are homeomorphic to $\mathbb{R}^9$. By using a negative log likelihood loss for this distribution we get a loss which is convex with respect to the network outputs. By optimizing this loss we improve state-of-the-art on several challenging applicable datasets, namely Pascal3D+, ModelNet10-$SO(3)$ and UPNA head pose.



### ChestX-Det10: Chest X-ray Dataset on Detection of Thoracic Abnormalities
- **Arxiv ID**: http://arxiv.org/abs/2006.10550v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.10550v3)
- **Published**: 2020-06-17 10:15:50+00:00
- **Updated**: 2020-10-19 06:25:25+00:00
- **Authors**: Jingyu Liu, Jie Lian, Yizhou Yu
- **Comment**: None
- **Journal**: None
- **Summary**: Instance level detection of thoracic diseases or abnormalities are crucial for automatic diagnosis in chest X-ray images. Most existing works on chest X-rays focus on disease classification and weakly supervised localization. In order to push forward the research on disease classification and localization on chest X-rays. We provide a new benchmark called ChestX-Det10, including box-level annotations of 10 categories of disease/abnormality of $\sim$ 3,500 images. The annotations are located at https://github.com/Deepwise-AILab/ChestX-Det10-Dataset.



### Evaluation of 3D CNN Semantic Mapping for Rover Navigation
- **Arxiv ID**: http://arxiv.org/abs/2006.09761v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2006.09761v1)
- **Published**: 2020-06-17 10:24:29+00:00
- **Updated**: 2020-06-17 10:24:29+00:00
- **Authors**: Sebastiano Chiodini, Luca Torresin, Marco Pertile, Stefano Debei
- **Comment**: To be presented at the 7th IEEE International Workshop on Metrology
  for Aerospace (MetroAerospace)
- **Journal**: None
- **Summary**: Terrain assessment is a key aspect for autonomous exploration rovers, surrounding environment recognition is required for multiple purposes, such as optimal trajectory planning and autonomous target identification. In this work we present a technique to generate accurate three-dimensional semantic maps for Martian environment. The algorithm uses as input a stereo image acquired by a camera mounted on a rover. Firstly, images are labeled with DeepLabv3+, which is an encoder-decoder Convolutional Neural Networl (CNN). Then, the labels obtained by the semantic segmentation are combined to stereo depth-maps in a Voxel representation. We evaluate our approach on the ESA Katwijk Beach Planetary Rover Dataset.



### Maximum Roaming Multi-Task Learning
- **Arxiv ID**: http://arxiv.org/abs/2006.09762v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.09762v4)
- **Published**: 2020-06-17 10:25:41+00:00
- **Updated**: 2021-05-19 09:20:37+00:00
- **Authors**: Lucas Pascal, Pietro Michiardi, Xavier Bost, Benoit Huet, Maria A. Zuluaga
- **Comment**: Accepted at the 35th AAAI Conference on Artificial Intelligence (AAAI
  2021)
- **Journal**: Proceedings of the AAAI Conference on Artificial Intelligence:
  35(10), 9331-9341 (2021)
- **Summary**: Multi-task learning has gained popularity due to the advantages it provides with respect to resource usage and performance. Nonetheless, the joint optimization of parameters with respect to multiple tasks remains an active research topic. Sub-partitioning the parameters between different tasks has proven to be an efficient way to relax the optimization constraints over the shared weights, may the partitions be disjoint or overlapping. However, one drawback of this approach is that it can weaken the inductive bias generally set up by the joint task optimization. In this work, we present a novel way to partition the parameter space without weakening the inductive bias. Specifically, we propose Maximum Roaming, a method inspired by dropout that randomly varies the parameter partitioning, while forcing them to visit as many tasks as possible at a regulated frequency, so that the network fully adapts to each update. We study the properties of our method through experiments on a variety of visual multi-task data sets. Experimental results suggest that the regularization brought by roaming has more impact on performance than usual partitioning optimization strategies. The overall method is flexible, easily applicable, provides superior regularization and consistently achieves improved performances compared to recent multi-task learning formulations.



### Mitosis Detection Under Limited Annotation: A Joint Learning Approach
- **Arxiv ID**: http://arxiv.org/abs/2006.09772v2
- **DOI**: 10.1109/ISBI45749.2020.9098431
- **Categories**: **cs.CV**, cs.LG, eess.IV, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2006.09772v2)
- **Published**: 2020-06-17 10:46:29+00:00
- **Updated**: 2020-07-02 08:37:08+00:00
- **Authors**: Pushpak Pati, Antonio Foncubierta-Rodriguez, Orcun Goksel, Maria Gabrani
- **Comment**: 2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI)
- **Journal**: None
- **Summary**: Mitotic counting is a vital prognostic marker of tumor proliferation in breast cancer. Deep learning-based mitotic detection is on par with pathologists, but it requires large labeled data for training. We propose a deep classification framework for enhancing mitosis detection by leveraging class label information, via softmax loss, and spatial distribution information among samples, via distance metric learning. We also investigate strategies towards steadily providing informative samples to boost the learning. The efficacy of the proposed framework is established through evaluation on ICPR 2012 and AMIDA 2013 mitotic data. Our framework significantly improves the detection with small training data and achieves on par or superior performance compared to state-of-the-art methods for using the entire training data.



### Self-supervised Knowledge Distillation for Few-shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2006.09785v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.09785v2)
- **Published**: 2020-06-17 11:27:00+00:00
- **Updated**: 2020-08-04 05:22:39+00:00
- **Authors**: Jathushan Rajasegaran, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, Mubarak Shah
- **Comment**: None
- **Journal**: None
- **Summary**: Real-world contains an overwhelmingly large number of object classes, learning all of which at once is infeasible. Few shot learning is a promising learning paradigm due to its ability to learn out of order distributions quickly with only a few samples. Recent works [7, 41] show that simply learning a good feature embedding can outperform more sophisticated meta-learning and metric learning algorithms for few-shot learning. In this paper, we propose a simple approach to improve the representation capacity of deep neural networks for few-shot learning tasks. We follow a two-stage learning process: First, we train a neural network to maximize the entropy of the feature embedding, thus creating an optimal output manifold using a self-supervised auxiliary loss. In the second stage, we minimize the entropy on feature embedding by bringing self-supervised twins together, while constraining the manifold with student-teacher distillation. Our experiments show that, even in the first stage, self-supervision can outperform current state-of-the-art methods, with further gains achieved by our second stage distillation process. Our codes are available at: https://github.com/brjathu/SKD.



### Sketch-Guided Scenery Image Outpainting
- **Arxiv ID**: http://arxiv.org/abs/2006.09788v2
- **DOI**: 10.1109/TIP.2021.3054477
- **Categories**: **cs.CV**, cs.GT
- **Links**: [PDF](http://arxiv.org/pdf/2006.09788v2)
- **Published**: 2020-06-17 11:34:36+00:00
- **Updated**: 2021-01-26 12:19:13+00:00
- **Authors**: Yaxiong Wang, Yunchao Wei, Xueming Qian, Li Zhu, Yi Yang
- **Comment**: Accepted by TIP
- **Journal**: None
- **Summary**: The outpainting results produced by existing approaches are often too random to meet users' requirement. In this work, we take the image outpainting one step forward by allowing users to harvest personal custom outpainting results using sketches as the guidance. To this end, we propose an encoder-decoder based network to conduct sketch-guided outpainting, where two alignment modules are adopted to impose the generated content to be realistic and consistent with the provided sketches. First, we apply a holistic alignment module to make the synthesized part be similar to the real one from the global view. Second, we reversely produce the sketches from the synthesized part and encourage them be consistent with the ground-truth ones using a sketch alignment module. In this way, the learned generator will be imposed to pay more attention to fine details and be sensitive to the guiding sketches. To our knowledge, this work is the first attempt to explore the challenging yet meaningful conditional scenery image outpainting. We conduct extensive experiments on two collected benchmarks to qualitatively and quantitatively validate the effectiveness of our approach compared with the other state-of-the-art generative models.



### Optimizing Grouped Convolutions on Edge Devices
- **Arxiv ID**: http://arxiv.org/abs/2006.09791v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.DC, stat.ML, I.2.6; D.3.4; C.1.4
- **Links**: [PDF](http://arxiv.org/pdf/2006.09791v1)
- **Published**: 2020-06-17 11:48:37+00:00
- **Updated**: 2020-06-17 11:48:37+00:00
- **Authors**: Perry Gibson, José Cano, Jack Turner, Elliot J. Crowley, Michael O'Boyle, Amos Storkey
- **Comment**: Camera ready version to be published at ASAP 2020 - The 31st IEEE
  International Conference on Application-specific Systems, Architectures and
  Processors. 8 pages, 6 figures
- **Journal**: None
- **Summary**: When deploying a deep neural network on constrained hardware, it is possible to replace the network's standard convolutions with grouped convolutions. This allows for substantial memory savings with minimal loss of accuracy. However, current implementations of grouped convolutions in modern deep learning frameworks are far from performing optimally in terms of speed. In this paper we propose Grouped Spatial Pack Convolutions (GSPC), a new implementation of grouped convolutions that outperforms existing solutions. We implement GSPC in TVM, which provides state-of-the-art performance on edge devices. We analyze a set of networks utilizing different types of grouped convolutions and evaluate their performance in terms of inference time on several edge devices. We observe that our new implementation scales well with the number of groups and provides the best inference times in all settings, improving the existing implementations of grouped convolutions in TVM, PyTorch and TensorFlow Lite by 3.4x, 8x and 4x on average respectively. Code is available at https://github.com/gecLAB/tvm-GSPC/



### MOSQUITO-NET: A deep learning based CADx system for malaria diagnosis along with model interpretation using GradCam and class activation maps
- **Arxiv ID**: http://arxiv.org/abs/2006.10547v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.10547v2)
- **Published**: 2020-06-17 13:00:30+00:00
- **Updated**: 2020-06-19 05:57:59+00:00
- **Authors**: Aayush Kumar, Sanat B Singh, Suresh Chandra Satapathy, Minakhi Rout
- **Comment**: arXiv admin note: text overlap with arXiv:2003.09871 by other authors
- **Journal**: None
- **Summary**: Malaria is considered one of the deadliest diseases in today world which causes thousands of deaths per year. The parasites responsible for malaria are scientifically known as Plasmodium which infects the red blood cells in human beings. The parasites are transmitted by a female class of mosquitos known as Anopheles. The diagnosis of malaria requires identification and manual counting of parasitized cells by medical practitioners in microscopic blood smears. Due to the unavailability of resources, its diagnostic accuracy is largely affected by large scale screening. State of the art Computer-aided diagnostic techniques based on deep learning algorithms such as CNNs, with end to end feature extraction and classification, have widely contributed to various image recognition tasks. In this paper, we evaluate the performance of custom made convnet Mosquito-Net, to classify the infected and uninfected cells for malaria diagnosis which could be deployed on the edge and mobile devices owing to its fewer parameters and less computation power. Therefore, it can be wildly preferred for diagnosis in remote and countryside areas where there is a lack of medical facilities.



### Burst Photography for Learning to Enhance Extremely Dark Images
- **Arxiv ID**: http://arxiv.org/abs/2006.09845v2
- **DOI**: 10.1109/TIP.2021.3125394
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.09845v2)
- **Published**: 2020-06-17 13:19:07+00:00
- **Updated**: 2021-11-19 20:09:40+00:00
- **Authors**: Ahmet Serdar Karadeniz, Erkut Erdem, Aykut Erdem
- **Comment**: Published in IEEE Transactions on Image Processing
- **Journal**: None
- **Summary**: Capturing images under extremely low-light conditions poses significant challenges for the standard camera pipeline. Images become too dark and too noisy, which makes traditional enhancement techniques almost impossible to apply. Recently, learning-based approaches have shown very promising results for this task since they have substantially more expressive capabilities to allow for improved quality. Motivated by these studies, in this paper, we aim to leverage burst photography to boost the performance and obtain much sharper and more accurate RGB images from extremely dark raw images. The backbone of our proposed framework is a novel coarse-to-fine network architecture that generates high-quality outputs progressively. The coarse network predicts a low-resolution, denoised raw image, which is then fed to the fine network to recover fine-scale details and realistic textures. To further reduce the noise level and improve the color accuracy, we extend this network to a permutation invariant structure so that it takes a burst of low-light images as input and merges information from multiple images at the feature-level. Our experiments demonstrate that our approach leads to perceptually more pleasing results than the state-of-the-art methods by producing more detailed and considerably higher quality images.



### Shallow Feature Based Dense Attention Network for Crowd Counting
- **Arxiv ID**: http://arxiv.org/abs/2006.09853v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.09853v1)
- **Published**: 2020-06-17 13:34:42+00:00
- **Updated**: 2020-06-17 13:34:42+00:00
- **Authors**: Yunqi Miao, Zijia Lin, Guiguang Ding, Jungong Han
- **Comment**: None
- **Journal**: None
- **Summary**: While the performance of crowd counting via deep learning has been improved dramatically in the recent years, it remains an ingrained problem due to cluttered backgrounds and varying scales of people within an image. In this paper, we propose a Shallow feature based Dense Attention Network (SDANet) for crowd counting from still images, which diminishes the impact of backgrounds via involving a shallow feature based attention model, and meanwhile, captures multi-scale information via densely connecting hierarchical image features. Specifically, inspired by the observation that backgrounds and human crowds generally have noticeably different responses in shallow features, we decide to build our attention model upon shallow-feature maps, which results in accurate background-pixel detection. Moreover, considering that the most representative features of people across different scales can appear in different layers of a feature extraction network, to better keep them all, we propose to densely connect hierarchical image features of different layers and subsequently encode them for estimating crowd density. Experimental results on three benchmark datasets clearly demonstrate the superiority of SDANet when dealing with different scenarios. Particularly, on the challenging UCF CC 50 dataset, our method outperforms other existing methods by a large margin, as is evident from a remarkable 11.9% Mean Absolute Error (MAE) drop of our SDANet.



### Intelligent Protection & Classification of Transients in Two-Core Symmetric Phase Angle Regulating Transformers
- **Arxiv ID**: http://arxiv.org/abs/2006.09865v1
- **DOI**: 10.1109/ACCESS.2021.3081015
- **Categories**: **eess.SP**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.09865v1)
- **Published**: 2020-06-17 13:42:58+00:00
- **Updated**: 2020-06-17 13:42:58+00:00
- **Authors**: Pallav Kumar Bera, Can Isik
- **Comment**: None
- **Journal**: IEEE Access, vol. 9, pp. 72937-72948, 2021
- **Summary**: This paper investigates the applicability of time and time-frequency features based classifiers to distinguish internal faults and other transients - magnetizing inrush, sympathetic inrush, external faults with current transformer saturation, and overexcitation - for Indirect Symmetrical Phase Angle Regulating Transformers (ISPAR). Then the faulty transformer unit (series/exciting) of the ISPAR is located, or else the transient disturbance is identified. An event detector detects variation in differential currents and registers one-cycle of 3-phase post transient samples which are used to extract the time and time-frequency features for training seven classifiers. Three different sets of features - wavelet coefficients, time-domain features, and combination of time and wavelet energy - obtained from exhaustive search using Decision Tree, random forest feature selection, and maximum Relevance Minimum Redundancy are used. The internal fault is detected with a balanced accuracy of 99.9%, the faulty unit is localized with balanced accuracy of 98.7% and the no-fault transients are classified with balanced accuracy of 99.5%. The results show potential for accurate internal fault detection and localization, and transient identification. The proposed scheme can supervise the operation of existing microprocessor-based differential relays resulting in higher stability and dependability. The ISPAR is modeled and the transients are simulated in PSCAD/EMTDC by varying several parameters.



### Self-Supervised Joint Learning Framework of Depth Estimation via Implicit Cues
- **Arxiv ID**: http://arxiv.org/abs/2006.09876v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.09876v3)
- **Published**: 2020-06-17 13:56:59+00:00
- **Updated**: 2020-06-26 07:11:18+00:00
- **Authors**: Jianrong Wang, Ge Zhang, Zhenyu Wu, XueWei Li, Li Liu
- **Comment**: None
- **Journal**: None
- **Summary**: In self-supervised monocular depth estimation, the depth discontinuity and motion objects' artifacts are still challenging problems. Existing self-supervised methods usually utilize a single view to train the depth estimation network. Compared with static views, abundant dynamic properties between video frames are beneficial to refined depth estimation, especially for dynamic objects. In this work, we propose a novel self-supervised joint learning framework for depth estimation using consecutive frames from monocular and stereo videos. The main idea is using an implicit depth cue extractor which leverages dynamic and static cues to generate useful depth proposals. These cues can predict distinguishable motion contours and geometric scene structures. Furthermore, a new high-dimensional attention module is introduced to extract clear global transformation, which effectively suppresses uncertainty of local descriptors in high-dimensional space, resulting in a more reliable optimization in learning framework. Experiments demonstrate that the proposed framework outperforms the state-of-the-art(SOTA) on KITTI and Make3D datasets.



### Using Wavelets and Spectral Methods to Study Patterns in Image-Classification Datasets
- **Arxiv ID**: http://arxiv.org/abs/2006.09879v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NA, eess.IV, math.NA
- **Links**: [PDF](http://arxiv.org/pdf/2006.09879v1)
- **Published**: 2020-06-17 13:58:24+00:00
- **Updated**: 2020-06-17 13:58:24+00:00
- **Authors**: Roozbeh Yousefzadeh, Furong Huang
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning models extract, before a final classification layer, features or patterns which are key for their unprecedented advantageous performance. However, the process of complex nonlinear feature extraction is not well understood, a major reason why interpretation, adversarial robustness, and generalization of deep neural nets are all open research problems. In this paper, we use wavelet transformation and spectral methods to analyze the contents of image classification datasets, extract specific patterns from the datasets and find the associations between patterns and classes. We show that each image can be written as the summation of a finite number of rank-1 patterns in the wavelet space, providing a low rank approximation that captures the structures and patterns essential for learning. Regarding the studies on memorization vs learning, our results clearly reveal disassociation of patterns from classes, when images are randomly labeled. Our method can be used as a pattern recognition approach to understand and interpret learnability of these datasets. It may also be used for gaining insights about the features and patterns that deep classifiers learn from the datasets.



### Unsupervised Learning of Visual Features by Contrasting Cluster Assignments
- **Arxiv ID**: http://arxiv.org/abs/2006.09882v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.09882v5)
- **Published**: 2020-06-17 14:00:42+00:00
- **Updated**: 2021-01-08 17:01:05+00:00
- **Authors**: Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, Armand Joulin
- **Comment**: NeurIPS 2020
- **Journal**: None
- **Summary**: Unsupervised image representations have significantly reduced the gap with supervised pretraining, notably with the recent achievements of contrastive learning methods. These contrastive methods typically work online and rely on a large number of explicit pairwise feature comparisons, which is computationally challenging. In this paper, we propose an online algorithm, SwAV, that takes advantage of contrastive methods without requiring to compute pairwise comparisons. Specifically, our method simultaneously clusters the data while enforcing consistency between cluster assignments produced for different augmentations (or views) of the same image, instead of comparing features directly as in contrastive learning. Simply put, we use a swapped prediction mechanism where we predict the cluster assignment of a view from the representation of another view. Our method can be trained with large and small batches and can scale to unlimited amounts of data. Compared to previous contrastive methods, our method is more memory efficient since it does not require a large memory bank or a special momentum network. In addition, we also propose a new data augmentation strategy, multi-crop, that uses a mix of views with different resolutions in place of two full-resolution views, without increasing the memory or compute requirements much. We validate our findings by achieving 75.3% top-1 accuracy on ImageNet with ResNet-50, as well as surpassing supervised pretraining on all the considered transfer tasks.



### Vision-Aided Dynamic Blockage Prediction for 6G Wireless Communication Networks
- **Arxiv ID**: http://arxiv.org/abs/2006.09902v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.IT, eess.SP, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/2006.09902v2)
- **Published**: 2020-06-17 14:37:38+00:00
- **Updated**: 2020-06-18 03:09:15+00:00
- **Authors**: Gouranga Charan, Muhammad Alrabeiah, Ahmed Alkhateeb
- **Comment**: The dataset and code files will be available soon on the ViWi
  website: https://www.viwi-dataset.net/
- **Journal**: None
- **Summary**: Unlocking the full potential of millimeter-wave and sub-terahertz wireless communication networks hinges on realizing unprecedented low-latency and high-reliability requirements. The challenge in meeting those requirements lies partly in the sensitivity of signals in the millimeter-wave and sub-terahertz frequency ranges to blockages. One promising way to tackle that challenge is to help a wireless network develop a sense of its surrounding using machine learning. This paper attempts to do that by utilizing deep learning and computer vision. It proposes a novel solution that proactively predicts \textit{dynamic} link blockages. More specifically, it develops a deep neural network architecture that learns from observed sequences of RGB images and beamforming vectors how to predict possible future link blockages. The proposed architecture is evaluated on a publicly available dataset that represents a synthetic dynamic communication scenario with multiple moving users and blockages. It scores a link-blockage prediction accuracy in the neighborhood of 86\%, a performance that is unlikely to be matched without utilizing visual data.



### Learning Colour Representations of Search Queries
- **Arxiv ID**: http://arxiv.org/abs/2006.09904v1
- **DOI**: 10.1145/3397271.3401095
- **Categories**: **cs.IR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.09904v1)
- **Published**: 2020-06-17 14:38:44+00:00
- **Updated**: 2020-06-17 14:38:44+00:00
- **Authors**: Paridhi Maheshwari, Manoj Ghuhan, Vishwa Vinay
- **Comment**: Accepted as a full paper at SIGIR 2020
- **Journal**: None
- **Summary**: Image search engines rely on appropriately designed ranking features that capture various aspects of the content semantics as well as the historic popularity. In this work, we consider the role of colour in this relevance matching process. Our work is motivated by the observation that a significant fraction of user queries have an inherent colour associated with them. While some queries contain explicit colour mentions (such as 'black car' and 'yellow daisies'), other queries have implicit notions of colour (such as 'sky' and 'grass'). Furthermore, grounding queries in colour is not a mapping to a single colour, but a distribution in colour space. For instance, a search for 'trees' tends to have a bimodal distribution around the colours green and brown. We leverage historical clickthrough data to produce a colour representation for search queries and propose a recurrent neural network architecture to encode unseen queries into colour space. We also show how this embedding can be learnt alongside a cross-modal relevance ranker from impression logs where a subset of the result images were clicked. We demonstrate that the use of a query-image colour distance feature leads to an improvement in the ranker performance as measured by users' preferences of clicked versus skipped images.



### FISHING Net: Future Inference of Semantic Heatmaps In Grids
- **Arxiv ID**: http://arxiv.org/abs/2006.09917v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.09917v1)
- **Published**: 2020-06-17 14:56:08+00:00
- **Updated**: 2020-06-17 14:56:08+00:00
- **Authors**: Noureldin Hendy, Cooper Sloan, Feng Tian, Pengfei Duan, Nick Charchut, Yuesong Xie, Chuang Wang, James Philbin
- **Comment**: None
- **Journal**: None
- **Summary**: For autonomous robots to navigate a complex environment, it is crucial to understand the surrounding scene both geometrically and semantically. Modern autonomous robots employ multiple sets of sensors, including lidars, radars, and cameras. Managing the different reference frames and characteristics of the sensors, and merging their observations into a single representation complicates perception. Choosing a single unified representation for all sensors simplifies the task of perception and fusion. In this work, we present an end-to-end pipeline that performs semantic segmentation and short term prediction using a top-down representation. Our approach consists of an ensemble of neural networks which take in sensor data from different sensor modalities and transform them into a single common top-down semantic grid representation. We find this representation favorable as it is agnostic to sensor-specific reference frames and captures both the semantic and geometric information for the surrounding scene. Because the modalities share a single output representation, they can be easily aggregated to produce a fused output. In this work we predict short-term semantic grids but the framework can be extended to other tasks. This approach offers a simple, extensible, end-to-end approach for multi-modal perception and prediction.



### Contrastive Learning for Weakly Supervised Phrase Grounding
- **Arxiv ID**: http://arxiv.org/abs/2006.09920v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.09920v3)
- **Published**: 2020-06-17 15:00:53+00:00
- **Updated**: 2020-08-05 21:53:38+00:00
- **Authors**: Tanmay Gupta, Arash Vahdat, Gal Chechik, Xiaodong Yang, Jan Kautz, Derek Hoiem
- **Comment**: ECCV 2020 (spotlight paper), Project page:
  http://tanmaygupta.info/info-ground
- **Journal**: None
- **Summary**: Phrase grounding, the problem of associating image regions to caption words, is a crucial component of vision-language tasks. We show that phrase grounding can be learned by optimizing word-region attention to maximize a lower bound on mutual information between images and caption words. Given pairs of images and captions, we maximize compatibility of the attention-weighted regions and the words in the corresponding caption, compared to non-corresponding pairs of images and captions. A key idea is to construct effective negative captions for learning through language model guided word substitutions. Training with our negatives yields a $\sim10\%$ absolute gain in accuracy over randomly-sampled negatives from the training data. Our weakly supervised phrase grounding model trained on COCO-Captions shows a healthy gain of $5.7\%$ to achieve $76.7\%$ accuracy on Flickr30K Entities benchmark.



### CoSE: Compositional Stroke Embeddings
- **Arxiv ID**: http://arxiv.org/abs/2006.09930v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.09930v2)
- **Published**: 2020-06-17 15:22:54+00:00
- **Updated**: 2020-11-30 18:50:51+00:00
- **Authors**: Emre Aksan, Thomas Deselaers, Andrea Tagliasacchi, Otmar Hilliges
- **Comment**: None
- **Journal**: Advances in Neural Information Processing Systems 2020
- **Summary**: We present a generative model for complex free-form structures such as stroke-based drawing tasks. While previous approaches rely on sequence-based models for drawings of basic objects or handwritten text, we propose a model that treats drawings as a collection of strokes that can be composed into complex structures such as diagrams (e.g., flow-charts). At the core of the approach lies a novel autoencoder that projects variable-length strokes into a latent space of fixed dimension. This representation space allows a relational model, operating in latent space, to better capture the relationship between strokes and to predict subsequent strokes. We demonstrate qualitatively and quantitatively that our proposed approach is able to model the appearance of individual strokes, as well as the compositional structure of larger diagram drawings. Our approach is suitable for interactive use cases such as auto-completing diagrams. We make code and models publicly available at https://eth-ait.github.io/cose.



### Human-Expert-Level Brain Tumor Detection Using Deep Learning with Data Distillation and Augmentation
- **Arxiv ID**: http://arxiv.org/abs/2006.12285v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.12285v3)
- **Published**: 2020-06-17 15:52:28+00:00
- **Updated**: 2020-07-16 12:32:26+00:00
- **Authors**: Diyuan Lu, Nenad Polomac, Iskra Gacheva, Elke Hattingen, Jochen Triesch
- **Comment**: Submitted to IEEE Transactions on Neural Networks and Learning
  Systems
- **Journal**: None
- **Summary**: The application of Deep Learning (DL) for medical diagnosis is often hampered by two problems. First, the amount of training data may be scarce, as it is limited by the number of patients who have acquired the condition to be diagnosed. Second, the training data may be corrupted by various types of noise. Here, we study the problem of brain tumor detection from magnetic resonance spectroscopy (MRS) data, where both types of problems are prominent. To overcome these challenges, we propose a new method for training a deep neural network that distills particularly representative training examples and augments the training data by mixing these samples from one class with those from the same and other classes to create additional training samples. We demonstrate that this technique substantially improves performance, allowing our method to reach human-expert-level accuracy with just a few thousand training examples. Interestingly, the network learns to rely on features of the data that are usually ignored by human experts, suggesting new directions for future research.



### Universally Quantized Neural Compression
- **Arxiv ID**: http://arxiv.org/abs/2006.09952v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.IT, cs.LG, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/2006.09952v2)
- **Published**: 2020-06-17 15:59:24+00:00
- **Updated**: 2020-10-21 17:10:20+00:00
- **Authors**: Eirikur Agustsson, Lucas Theis
- **Comment**: Authors contributed equally
- **Journal**: None
- **Summary**: A popular approach to learning encoders for lossy compression is to use additive uniform noise during training as a differentiable approximation to test-time quantization. We demonstrate that a uniform noise channel can also be implemented at test time using universal quantization (Ziv, 1985). This allows us to eliminate the mismatch between training and test phases while maintaining a completely differentiable loss function. Implementing the uniform noise channel is a special case of the more general problem of communicating a sample, which we prove is computationally hard if we do not make assumptions about its distribution. However, the uniform special case is efficient as well as easy to implement and thus of great interest from a practical point of view. Finally, we show that quantization can be obtained as a limiting case of a soft quantizer applied to the uniform noise channel, bridging compression with and without quantization.



### When We First Met: Visual-Inertial Person Localization for Co-Robot Rendezvous
- **Arxiv ID**: http://arxiv.org/abs/2006.09959v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2006.09959v2)
- **Published**: 2020-06-17 16:15:01+00:00
- **Updated**: 2020-11-03 13:57:23+00:00
- **Authors**: Xi Sun, Xinshuo Weng, Kris Kitani
- **Comment**: Published in IROS 2020. Project website is
  http://www.xinshuoweng.com/projects/VIPL/
- **Journal**: None
- **Summary**: We aim to enable robots to visually localize a target person through the aid of an additional sensing modality -- the target person's 3D inertial measurements. The need for such technology may arise when a robot is to meet person in a crowd for the first time or when an autonomous vehicle must rendezvous with a rider amongst a crowd without knowing the appearance of the person in advance. A person's inertial information can be measured with a wearable device such as a smart-phone and can be shared selectively with an autonomous system during the rendezvous. We propose a method to learn a visual-inertial feature space in which the motion of a person in video can be easily matched to the motion measured by a wearable inertial measurement unit (IMU). The transformation of the two modalities into the joint feature space is learned through the use of a contrastive loss which forces inertial motion features and video motion features generated by the same person to lie close in the joint feature space. To validate our approach, we compose a dataset of over 60,000 video segments of moving people along with wearable IMU data. Our experiments show that our proposed method is able to accurately localize a target person with 80.7% accuracy using only 5 seconds of IMU data and video.



### WhoAmI: An Automatic Tool for Visual Recognition of Tiger and Leopard Individuals in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2006.09962v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.09962v1)
- **Published**: 2020-06-17 16:17:46+00:00
- **Updated**: 2020-06-17 16:17:46+00:00
- **Authors**: Rita Pucci, Jitendra Shankaraiah, Devcharan Jathanna, Ullas Karanth, Kartic Subr
- **Comment**: None
- **Journal**: None
- **Summary**: Photographs of wild animals in their natural habitats can be recorded unobtrusively via cameras that are triggered by motion nearby. The installation of such camera traps is becoming increasingly common across the world. Although this is a convenient source of invaluable data for biologists, ecologists and conservationists, the arduous task of poring through potentially millions of pictures each season introduces prohibitive costs and frustrating delays. We develop automatic algorithms that are able to detect animals, identify the species of animals and to recognize individual animals for two species. we propose the first fully-automatic tool that can recognize specific individuals of leopard and tiger due to their characteristic body markings. We adopt a class of supervised learning approach of machine learning where a Deep Convolutional Neural Network (DCNN) is trained using several instances of manually-labelled images for each of the three classification tasks. We demonstrate the effectiveness of our approach on a data set of camera-trap images recorded in the jungles of Southern India.



### High-Fidelity Generative Image Compression
- **Arxiv ID**: http://arxiv.org/abs/2006.09965v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.09965v3)
- **Published**: 2020-06-17 16:21:10+00:00
- **Updated**: 2020-10-23 08:55:23+00:00
- **Authors**: Fabian Mentzer, George Toderici, Michael Tschannen, Eirikur Agustsson
- **Comment**: This is the Camera Ready version for NeurIPS 2020. Project page:
  https://hific.github.io
- **Journal**: None
- **Summary**: We extensively study how to combine Generative Adversarial Networks and learned compression to obtain a state-of-the-art generative lossy compression system. In particular, we investigate normalization layers, generator and discriminator architectures, training strategies, as well as perceptual losses. In contrast to previous work, i) we obtain visually pleasing reconstructions that are perceptually similar to the input, ii) we operate in a broad range of bitrates, and iii) our approach can be applied to high-resolution images. We bridge the gap between rate-distortion-perception theory and practice by evaluating our approach both quantitatively with various perceptual metrics, and with a user study. The study shows that our method is preferred to previous approaches even if they use more than 2x the bitrate.



### Classifier-independent Lower-Bounds for Adversarial Robustness
- **Arxiv ID**: http://arxiv.org/abs/2006.09989v6
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2006.09989v6)
- **Published**: 2020-06-17 16:46:39+00:00
- **Updated**: 2020-11-10 00:32:30+00:00
- **Authors**: Elvis Dohmatob
- **Comment**: None
- **Journal**: None
- **Summary**: We theoretically analyse the limits of robustness to test-time adversarial and noisy examples in classification. Our work focuses on deriving bounds which uniformly apply to all classifiers (i.e all measurable functions from features to labels) for a given problem. Our contributions are two-fold. (1) We use optimal transport theory to derive variational formulae for the Bayes-optimal error a classifier can make on a given classification problem, subject to adversarial attacks. The optimal adversarial attack is then an optimal transport plan for a certain binary cost-function induced by the specific attack model, and can be computed via a simple algorithm based on maximal matching on bipartite graphs. (2) We derive explicit lower-bounds on the Bayes-optimal error in the case of the popular distance-based attacks. These bounds are universal in the sense that they depend on the geometry of the class-conditional distributions of the data, but not on a particular classifier. Our results are in sharp contrast with the existing literature, wherein adversarial vulnerability of classifiers is derived as a consequence of nonzero ordinary test error.



### Noise or Signal: The Role of Image Backgrounds in Object Recognition
- **Arxiv ID**: http://arxiv.org/abs/2006.09994v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.09994v1)
- **Published**: 2020-06-17 16:54:43+00:00
- **Updated**: 2020-06-17 16:54:43+00:00
- **Authors**: Kai Xiao, Logan Engstrom, Andrew Ilyas, Aleksander Madry
- **Comment**: None
- **Journal**: None
- **Summary**: We assess the tendency of state-of-the-art object recognition models to depend on signals from image backgrounds. We create a toolkit for disentangling foreground and background signal on ImageNet images, and find that (a) models can achieve non-trivial accuracy by relying on the background alone, (b) models often misclassify images even in the presence of correctly classified foregrounds--up to 87.5% of the time with adversarially chosen backgrounds, and (c) more accurate models tend to depend on backgrounds less. Our analysis of backgrounds brings us closer to understanding which correlations machine learning models use, and how they determine models' out of distribution performance.



### Deeply Learned Spectral Total Variation Decomposition
- **Arxiv ID**: http://arxiv.org/abs/2006.10004v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.10004v2)
- **Published**: 2020-06-17 17:10:43+00:00
- **Updated**: 2020-10-21 17:03:54+00:00
- **Authors**: Tamara G. Grossmann, Yury Korolev, Guy Gilboa, Carola-Bibiane Schönlieb
- **Comment**: None
- **Journal**: None
- **Summary**: Non-linear spectral decompositions of images based on one-homogeneous functionals such as total variation have gained considerable attention in the last few years. Due to their ability to extract spectral components corresponding to objects of different size and contrast, such decompositions enable filtering, feature transfer, image fusion and other applications. However, obtaining this decomposition involves solving multiple non-smooth optimisation problems and is therefore computationally highly intensive. In this paper, we present a neural network approximation of a non-linear spectral decomposition. We report up to four orders of magnitude ($\times 10,000$) speedup in processing of mega-pixel size images, compared to classical GPU implementations. Our proposed network, TVSpecNET, is able to implicitly learn the underlying PDE and, despite being entirely data driven, inherits invariances of the model based transform. To the best of our knowledge, this is the first approach towards learning a non-linear spectral decomposition of images. Not only do we gain a staggering computational advantage, but this approach can also be seen as a step towards studying neural networks that can decompose an image into spectral components defined by a user rather than a handcrafted functional.



### Fast Object Classification and Meaningful Data Representation of Segmented Lidar Instances
- **Arxiv ID**: http://arxiv.org/abs/2006.10011v1
- **DOI**: 10.1109/ITSC45102.2020.9294217
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.10011v1)
- **Published**: 2020-06-17 17:16:38+00:00
- **Updated**: 2020-06-17 17:16:38+00:00
- **Authors**: Lukas Hahn, Frederik Hasecke, Anton Kummert
- **Comment**: 6 pages, 5 figures, 4 tables; accepted to appear in IEEE ITSC 2020
- **Journal**: 2020 IEEE 23rd International Conference on Intelligent
  Transportation Systems (ITSC), Rhodes, Greece, 2020, pp. 1-6
- **Summary**: Object detection algorithms for Lidar data have seen numerous publications in recent years, reporting good results on dataset benchmarks oriented towards automotive requirements. Nevertheless, many of these are not deployable to embedded vehicle systems, as they require immense computational power to be executed close to real time. In this work, we propose a way to facilitate real-time Lidar object classification on CPU. We show how our approach uses segmented object instances to extract important features, enabling a computationally efficient batch-wise classification. For this, we introduce a data representation which translates three-dimensional information into small image patches, using decomposed normal vector images. We couple this with dedicated object statistics to handle edge cases. We apply our method on the tasks of object detection and semantic segmentation, as well as the relatively new challenge of panoptic segmentation. Through evaluation, we show, that our algorithm is capable of producing good results on public data, while running in real time on CPU without using specific optimisation.



### OGAN: Disrupting Deepfakes with an Adversarial Attack that Survives Training
- **Arxiv ID**: http://arxiv.org/abs/2006.12247v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.12247v2)
- **Published**: 2020-06-17 17:18:29+00:00
- **Updated**: 2020-11-25 11:47:30+00:00
- **Authors**: Eran Segalis, Eran Galili
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Recent advances in autoencoders and generative models have given rise to effective video forgery methods, used for generating so-called "deepfakes". Mitigation research is mostly focused on post-factum deepfake detection and not on prevention. We complement these efforts by introducing a novel class of adversarial attacks---training-resistant attacks---which can disrupt face-swapping autoencoders whether or not its adversarial images have been included in the training set of said autoencoders. We propose the Oscillating GAN (OGAN) attack, a novel attack optimized to be training-resistant, which introduces spatial-temporal distortions to the output of face-swapping autoencoders. To implement OGAN, we construct a bilevel optimization problem, where we train a generator and a face-swapping model instance against each other. Specifically, we pair each input image with a target distortion, and feed them into a generator that produces an adversarial image. This image will exhibit the distortion when a face-swapping autoencoder is applied to it. We solve the optimization problem by training the generator and the face-swapping model simultaneously using an iterative process of alternating optimization. Next, we analyze the previously published Distorting Attack and show it is training-resistant, though it is outperformed by our suggested OGAN. Finally, we validate both attacks using a popular implementation of FaceSwap, and show that they transfer across different target models and target faces, including faces the adversarial attacks were not trained on. More broadly, these results demonstrate the existence of training-resistant adversarial attacks, potentially applicable to a wide range of domains.



### Deep Learning feature selection to unhide demographic recommender systems factors
- **Arxiv ID**: http://arxiv.org/abs/2006.12379v1
- **DOI**: 10.1007/s00521-020-05494-2
- **Categories**: **cs.IR**, cs.CV, cs.LG, cs.NE, stat.ML, I.5.1
- **Links**: [PDF](http://arxiv.org/pdf/2006.12379v1)
- **Published**: 2020-06-17 17:36:48+00:00
- **Updated**: 2020-06-17 17:36:48+00:00
- **Authors**: Jesús Bobadilla, Ángel González-Prieto, Fernando Ortega, Raúl Lara-Cabrera
- **Comment**: 20 pages, 14 figures, 1 table
- **Journal**: Neural Computing and Applications, 1-18, 2020
- **Summary**: Extracting demographic features from hidden factors is an innovative concept that provides multiple and relevant applications. The matrix factorization model generates factors which do not incorporate semantic knowledge. This paper provides a deep learning-based method: DeepUnHide, able to extract demographic information from the users and items factors in collaborative filtering recommender systems. The core of the proposed method is the gradient-based localization used in the image processing literature to highlight the representative areas of each classification class. Validation experiments make use of two public datasets and current baselines. Results show the superiority of DeepUnHide to make feature selection and demographic classification, compared to the state of art of feature selection methods. Relevant and direct applications include recommendations explanation, fairness in collaborative filtering and recommendation to groups of users.



### Big Self-Supervised Models are Strong Semi-Supervised Learners
- **Arxiv ID**: http://arxiv.org/abs/2006.10029v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.10029v2)
- **Published**: 2020-06-17 17:48:22+00:00
- **Updated**: 2020-10-26 03:09:28+00:00
- **Authors**: Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, Geoffrey Hinton
- **Comment**: NeurIPS'2020. Code and pretrained models at
  https://github.com/google-research/simclr
- **Journal**: None
- **Summary**: One paradigm for learning from few labeled examples while making best use of a large amount of unlabeled data is unsupervised pretraining followed by supervised fine-tuning. Although this paradigm uses unlabeled data in a task-agnostic way, in contrast to common approaches to semi-supervised learning for computer vision, we show that it is surprisingly effective for semi-supervised learning on ImageNet. A key ingredient of our approach is the use of big (deep and wide) networks during pretraining and fine-tuning. We find that, the fewer the labels, the more this approach (task-agnostic use of unlabeled data) benefits from a bigger network. After fine-tuning, the big network can be further improved and distilled into a much smaller one with little loss in classification accuracy by using the unlabeled examples for a second time, but in a task-specific way. The proposed semi-supervised learning algorithm can be summarized in three steps: unsupervised pretraining of a big ResNet model using SimCLRv2, supervised fine-tuning on a few labeled examples, and distillation with unlabeled examples for refining and transferring the task-specific knowledge. This procedure achieves 73.9% ImageNet top-1 accuracy with just 1% of the labels ($\le$13 labeled images per class) using ResNet-50, a $10\times$ improvement in label efficiency over the previous state-of-the-art. With 10% of labels, ResNet-50 trained with our method achieves 77.5% top-1 accuracy, outperforming standard supervised training with all of the labels.



### Semantic Visual Navigation by Watching YouTube Videos
- **Arxiv ID**: http://arxiv.org/abs/2006.10034v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2006.10034v2)
- **Published**: 2020-06-17 17:56:00+00:00
- **Updated**: 2020-10-27 05:46:46+00:00
- **Authors**: Matthew Chang, Arjun Gupta, Saurabh Gupta
- **Comment**: NeurIPS 2020. Project website with code, models, and videos:
  https://matthewchang.github.io/value-learning-from-video
- **Journal**: None
- **Summary**: Semantic cues and statistical regularities in real-world environment layouts can improve efficiency for navigation in novel environments. This paper learns and leverages such semantic cues for navigating to objects of interest in novel environments, by simply watching YouTube videos. This is challenging because YouTube videos don't come with labels for actions or goals, and may not even showcase optimal behavior. Our method tackles these challenges through the use of Q-learning on pseudo-labeled transition quadruples (image, action, next image, reward). We show that such off-policy Q-learning from passive data is able to learn meaningful semantic cues for navigation. These cues, when used in a hierarchical navigation policy, lead to improved efficiency at the ObjectGoal task in visually realistic simulations. We observe a relative improvement of 15-83% over end-to-end RL, behavior cloning, and classical methods, while using minimal direct interaction.



### LSD-C: Linearly Separable Deep Clusters
- **Arxiv ID**: http://arxiv.org/abs/2006.10039v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.10039v1)
- **Published**: 2020-06-17 17:58:10+00:00
- **Updated**: 2020-06-17 17:58:10+00:00
- **Authors**: Sylvestre-Alvise Rebuffi, Sebastien Ehrhardt, Kai Han, Andrea Vedaldi, Andrew Zisserman
- **Comment**: Code available at https://github.com/srebuffi/lsd-clusters
- **Journal**: None
- **Summary**: We present LSD-C, a novel method to identify clusters in an unlabeled dataset. Our algorithm first establishes pairwise connections in the feature space between the samples of the minibatch based on a similarity metric. Then it regroups in clusters the connected samples and enforces a linear separation between clusters. This is achieved by using the pairwise connections as targets together with a binary cross-entropy loss on the predictions that the associated pairs of samples belong to the same cluster. This way, the feature representation of the network will evolve such that similar samples in this feature space will belong to the same linearly separated cluster. Our method draws inspiration from recent semi-supervised learning practice and proposes to combine our clustering algorithm with self-supervised pretraining and strong data augmentation. We show that our approach significantly outperforms competitors on popular public image benchmarks including CIFAR 10/100, STL 10 and MNIST, as well as the document classification dataset Reuters 10K.



### Learning to Detect 3D Reflection Symmetry for Single-View Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2006.10042v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.10042v1)
- **Published**: 2020-06-17 17:58:59+00:00
- **Updated**: 2020-06-17 17:58:59+00:00
- **Authors**: Yichao Zhou, Shichen Liu, Yi Ma
- **Comment**: None
- **Journal**: None
- **Summary**: 3D reconstruction from a single RGB image is a challenging problem in computer vision. Previous methods are usually solely data-driven, which lead to inaccurate 3D shape recovery and limited generalization capability. In this work, we focus on object-level 3D reconstruction and present a geometry-based end-to-end deep learning framework that first detects the mirror plane of reflection symmetry that commonly exists in man-made objects and then predicts depth maps by finding the intra-image pixel-wise correspondence of the symmetry. Our method fully utilizes the geometric cues from symmetry during the test time by building plane-sweep cost volumes, a powerful tool that has been used in multi-view stereopsis. To our knowledge, this is the first work that uses the concept of cost volumes in the setting of single-image 3D reconstruction. We conduct extensive experiments on the ShapeNet dataset and find that our reconstruction method significantly outperforms the previous state-of-the-art single-view 3D reconstruction networks in term of the accuracy of camera poses and depth maps, without requiring objects being completely symmetric. Code is available at https://github.com/zhou13/symmetrynet.



### Overcoming Statistical Shortcuts for Open-ended Visual Counting
- **Arxiv ID**: http://arxiv.org/abs/2006.10079v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.10079v2)
- **Published**: 2020-06-17 18:02:01+00:00
- **Updated**: 2020-07-01 11:04:02+00:00
- **Authors**: Corentin Dancette, Remi Cadene, Xinlei Chen, Matthieu Cord
- **Comment**: 17 pages, 8 figures
- **Journal**: None
- **Summary**: Machine learning models tend to over-rely on statistical shortcuts. These spurious correlations between parts of the input and the output labels does not hold in real-world settings. We target this issue on the recent open-ended visual counting task which is well suited to study statistical shortcuts. We aim to develop models that learn a proper mechanism of counting regardless of the output label. First, we propose the Modifying Count Distribution (MCD) protocol, which penalizes models that over-rely on statistical shortcuts. It is based on pairs of training and testing sets that do not follow the same count label distribution such as the odd-even sets. Intuitively, models that have learned a proper mechanism of counting on odd numbers should perform well on even numbers. Secondly, we introduce the Spatial Counting Network (SCN), which is dedicated to visual analysis and counting based on natural language questions. Our model selects relevant image regions, scores them with fusion and self-attention mechanisms, and provides a final counting score. We apply our protocol on the recent dataset, TallyQA, and show superior performances compared to state-of-the-art models. We also demonstrate the ability of our model to select the correct instances to count in the image. Code and datasets are available: https://github.com/cdancette/spatial-counting-network



### An Online Method for A Class of Distributionally Robust Optimization with Non-Convex Objectives
- **Arxiv ID**: http://arxiv.org/abs/2006.10138v5
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.10138v5)
- **Published**: 2020-06-17 20:19:25+00:00
- **Updated**: 2021-11-12 15:52:00+00:00
- **Authors**: Qi Qi, Zhishuai Guo, Yi Xu, Rong Jin, Tianbao Yang
- **Comment**: 25 pages, 9 figures
- **Journal**: None
- **Summary**: In this paper, we propose a practical online method for solving a class of distributionally robust optimization (DRO) with non-convex objectives, which has important applications in machine learning for improving the robustness of neural networks. In the literature, most methods for solving DRO are based on stochastic primal-dual methods. However, primal-dual methods for DRO suffer from several drawbacks: (1) manipulating a high-dimensional dual variable corresponding to the size of data is time expensive; (2) they are not friendly to online learning where data is coming sequentially. To address these issues, we consider a class of DRO with an KL divergence regularization on the dual variables, transform the min-max problem into a compositional minimization problem, and propose practical duality-free online stochastic methods without requiring a large mini-batch size. We establish the state-of-the-art complexities of the proposed methods with and without a Polyak-\L ojasiewicz (PL) condition of the objective. Empirical studies on large-scale deep learning tasks (i) demonstrate that our method can speed up the training by more than 2 times than baseline methods and save days of training time on a large-scale dataset with $\sim$ 265K images, and (ii) verify the supreme performance of DRO over Empirical Risk Minimization (ERM) on imbalanced datasets. Of independent interest, the proposed method can be also used for solving a family of stochastic compositional problems with state-of-the-art complexities.



### Are you wearing a mask? Improving mask detection from speech using augmentation by cycle-consistent GANs
- **Arxiv ID**: http://arxiv.org/abs/2006.10147v2
- **DOI**: None
- **Categories**: **eess.AS**, cs.CV, cs.SD
- **Links**: [PDF](http://arxiv.org/pdf/2006.10147v2)
- **Published**: 2020-06-17 20:46:50+00:00
- **Updated**: 2020-07-25 21:52:21+00:00
- **Authors**: Nicolae-Cătălin Ristea, Radu Tudor Ionescu
- **Comment**: Accepted at INTERSPEECH 2020
- **Journal**: None
- **Summary**: The task of detecting whether a person wears a face mask from speech is useful in modelling speech in forensic investigations, communication between surgeons or people protecting themselves against infectious diseases such as COVID-19. In this paper, we propose a novel data augmentation approach for mask detection from speech. Our approach is based on (i) training Generative Adversarial Networks (GANs) with cycle-consistency loss to translate unpaired utterances between two classes (with mask and without mask), and on (ii) generating new training utterances using the cycle-consistent GANs, assigning opposite labels to each translated utterance. Original and translated utterances are converted into spectrograms which are provided as input to a set of ResNet neural networks with various depths. The networks are combined into an ensemble through a Support Vector Machines (SVM) classifier. With this system, we participated in the Mask Sub-Challenge (MSC) of the INTERSPEECH 2020 Computational Paralinguistics Challenge, surpassing the baseline proposed by the organizers by 2.8%. Our data augmentation technique provided a performance boost of 0.9% on the private test set. Furthermore, we show that our data augmentation approach yields better results than other baseline and state-of-the-art augmentation methods.



### Deep Network for Scatterer Distribution Estimation for Ultrasound Image Simulation
- **Arxiv ID**: http://arxiv.org/abs/2006.10166v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.10166v1)
- **Published**: 2020-06-17 21:25:13+00:00
- **Updated**: 2020-06-17 21:25:13+00:00
- **Authors**: Lin Zhang, Valery Vishnevskiy, Orcun Goksel
- **Comment**: None
- **Journal**: None
- **Summary**: Simulation-based ultrasound training can be an essential educational tool. Realistic ultrasound image appearance with typical speckle texture can be modeled as convolution of a point spread function with point scatterers representing tissue microstructure. Such scatterer distribution, however, is in general not known and its estimation for a given tissue type is fundamentally an ill-posed inverse problem. In this paper, we demonstrate a convolutional neural network approach for probabilistic scatterer estimation from observed ultrasound data. We herein propose to impose a known statistical distribution on scatterers and learn the mapping between ultrasound image and distribution parameter map by training a convolutional neural network on synthetic images. In comparison with several existing approaches, we demonstrate in numerical simulations and with in-vivo images that the synthesized images from scatterer representations estimated with our approach closely match the observations with varying acquisition parameters such as compression and rotation of the imaged domain.



### Variational State-Space Models for Localisation and Dense 3D Mapping in 6 DoF
- **Arxiv ID**: http://arxiv.org/abs/2006.10178v3
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.10178v3)
- **Published**: 2020-06-17 22:06:35+00:00
- **Updated**: 2021-03-15 17:11:08+00:00
- **Authors**: Atanas Mirchev, Baris Kayalibay, Patrick van der Smagt, Justin Bayer
- **Comment**: Update for ICLR2021
- **Journal**: None
- **Summary**: We solve the problem of 6-DoF localisation and 3D dense reconstruction in spatial environments as approximate Bayesian inference in a deep state-space model. Our approach leverages both learning and domain knowledge from multiple-view geometry and rigid-body dynamics. This results in an expressive predictive model of the world, often missing in current state-of-the-art visual SLAM solutions. The combination of variational inference, neural networks and a differentiable raycaster ensures that our model is amenable to end-to-end gradient-based optimisation. We evaluate our approach on realistic unmanned aerial vehicle flight data, nearing the performance of state-of-the-art visual-inertial odometry systems. We demonstrate the applicability of the model to generative prediction and planning.



### TearingNet: Point Cloud Autoencoder to Learn Topology-Friendly Representations
- **Arxiv ID**: http://arxiv.org/abs/2006.10187v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.10187v4)
- **Published**: 2020-06-17 22:42:43+00:00
- **Updated**: 2021-09-04 18:02:55+00:00
- **Authors**: Jiahao Pang, Duanshun Li, Dong Tian
- **Comment**: Accepted at CVPR 2021
- **Journal**: None
- **Summary**: Topology matters. Despite the recent success of point cloud processing with geometric deep learning, it remains arduous to capture the complex topologies of point cloud data with a learning model. Given a point cloud dataset containing objects with various genera, or scenes with multiple objects, we propose an autoencoder, TearingNet, which tackles the challenging task of representing the point clouds using a fixed-length descriptor. Unlike existing works directly deforming predefined primitives of genus zero (e.g., a 2D square patch) to an object-level point cloud, our TearingNet is characterized by a proposed Tearing network module and a Folding network module interacting with each other iteratively. Particularly, the Tearing network module learns the point cloud topology explicitly. By breaking the edges of a primitive graph, it tears the graph into patches or with holes to emulate the topology of a target point cloud, leading to faithful reconstructions. Experimentation shows the superiority of our proposal in terms of reconstructing point clouds as well as generating more topology-friendly representations than benchmarks.



### Head2Head++: Deep Facial Attributes Re-Targeting
- **Arxiv ID**: http://arxiv.org/abs/2006.10199v2
- **DOI**: 10.1109/TBIOM.2021.3049576
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.10199v2)
- **Published**: 2020-06-17 23:38:37+00:00
- **Updated**: 2021-09-28 15:01:59+00:00
- **Authors**: Michail Christos Doukas, Mohammad Rami Koujan, Viktoriia Sharmanska, Anastasios Roussos
- **Comment**: Published in IEEE Transactions on Biometrics, Behavior, and Identity
  Science (Volume: 3, Issue: 1, Jan. 2021)
- **Journal**: None
- **Summary**: Facial video re-targeting is a challenging problem aiming to modify the facial attributes of a target subject in a seamless manner by a driving monocular sequence. We leverage the 3D geometry of faces and Generative Adversarial Networks (GANs) to design a novel deep learning architecture for the task of facial and head reenactment. Our method is different to purely 3D model-based approaches, or recent image-based methods that use Deep Convolutional Neural Networks (DCNNs) to generate individual frames. We manage to capture the complex non-rigid facial motion from the driving monocular performances and synthesise temporally consistent videos, with the aid of a sequential Generator and an ad-hoc Dynamics Discriminator network. We conduct a comprehensive set of quantitative and qualitative tests and demonstrate experimentally that our proposed method can successfully transfer facial expressions, head pose and eye gaze from a source video to a target subject, in a photo-realistic and faithful fashion, better than other state-of-the-art methods. Most importantly, our system performs end-to-end reenactment in nearly real-time speed (18 fps).



### HyNet: Learning Local Descriptor with Hybrid Similarity Measure and Triplet Loss
- **Arxiv ID**: http://arxiv.org/abs/2006.10202v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.10202v3)
- **Published**: 2020-06-17 23:49:45+00:00
- **Updated**: 2020-11-09 16:25:48+00:00
- **Authors**: Yurun Tian, Axel Barroso-Laguna, Tony Ng, Vassileios Balntas, Krystian Mikolajczyk
- **Comment**: None
- **Journal**: None
- **Summary**: Recent works show that local descriptor learning benefits from the use of L2 normalisation, however, an in-depth analysis of this effect lacks in the literature. In this paper, we investigate how L2 normalisation affects the back-propagated descriptor gradients during training. Based on our observations, we propose HyNet, a new local descriptor that leads to state-of-the-art results in matching. HyNet introduces a hybrid similarity measure for triplet margin loss, a regularisation term constraining the descriptor norm, and a new network architecture that performs L2 normalisation of all intermediate feature maps and the output descriptors. HyNet surpasses previous methods by a significant margin on standard benchmarks that include patch matching, verification, and retrieval, as well as outperforming full end-to-end methods on 3D reconstruction tasks.



### BlazePose: On-device Real-time Body Pose tracking
- **Arxiv ID**: http://arxiv.org/abs/2006.10204v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.10204v1)
- **Published**: 2020-06-17 23:52:46+00:00
- **Updated**: 2020-06-17 23:52:46+00:00
- **Authors**: Valentin Bazarevsky, Ivan Grishchenko, Karthik Raveendran, Tyler Zhu, Fan Zhang, Matthias Grundmann
- **Comment**: 4 pages, 6 figures; CVPR Workshop on Computer Vision for Augmented
  and Virtual Reality, Seattle, WA, USA, 2020
- **Journal**: None
- **Summary**: We present BlazePose, a lightweight convolutional neural network architecture for human pose estimation that is tailored for real-time inference on mobile devices. During inference, the network produces 33 body keypoints for a single person and runs at over 30 frames per second on a Pixel 2 phone. This makes it particularly suited to real-time use cases like fitness tracking and sign language recognition. Our main contributions include a novel body pose tracking solution and a lightweight body pose estimation neural network that uses both heatmaps and regression to keypoint coordinates.



