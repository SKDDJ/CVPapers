# Arxiv Papers in cs.CV on 2020-06-05
### Discovering Parametric Activation Functions
- **Arxiv ID**: http://arxiv.org/abs/2006.03179v5
- **DOI**: 10.1016/j.neunet.2022.01.001
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.03179v5)
- **Published**: 2020-06-05 00:25:33+00:00
- **Updated**: 2022-01-21 19:39:36+00:00
- **Authors**: Garrett Bingham, Risto Miikkulainen
- **Comment**: Published in Neural Networks. 34 pages, 10 figures, 11 tables
- **Journal**: Neural Networks, Volume 148, 2022, Pages 48-65, ISSN 0893-6080
- **Summary**: Recent studies have shown that the choice of activation function can significantly affect the performance of deep learning networks. However, the benefits of novel activation functions have been inconsistent and task dependent, and therefore the rectified linear unit (ReLU) is still the most commonly used. This paper proposes a technique for customizing activation functions automatically, resulting in reliable improvements in performance. Evolutionary search is used to discover the general form of the function, and gradient descent to optimize its parameters for different parts of the network and over the learning process. Experiments with four different neural network architectures on the CIFAR-10 and CIFAR-100 image classification datasets show that this approach is effective. It discovers both general activation functions and specialized functions for different architectures, consistently improving accuracy over ReLU and other activation functions by significant margins. The approach can therefore be used as an automated optimization step in applying deep learning to new tasks.



### MSDU-net: A Multi-Scale Dilated U-net for Blur Detection
- **Arxiv ID**: http://arxiv.org/abs/2006.03182v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.03182v1)
- **Published**: 2020-06-05 00:30:38+00:00
- **Updated**: 2020-06-05 00:30:38+00:00
- **Authors**: Fan Yang, Xiao Xiao
- **Comment**: None
- **Journal**: None
- **Summary**: Blur detection is the separation of blurred and clear regions of an image, which is an important and challenging task in computer vision. In this work, we regard blur detection as an image segmentation problem. Inspired by the success of the U-net architecture for image segmentation, we design a Multi-Scale Dilated convolutional neural network based on U-net, which we call MSDU-net. The MSDU-net uses a group of multi-scale feature extractors with dilated convolutions to extract texture information at different scales. The U-shape architecture of the MSDU-net fuses the different-scale texture features and generates a semantic feature which allows us to achieve better results on the blur detection task. We show that using the MSDU-net we are able to outperform other state of the art blur detection methods on two publicly available benchmarks.



### Pick-Object-Attack: Type-Specific Adversarial Attack for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2006.03184v3
- **DOI**: 10.1016/j.cviu.2021.103257
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.03184v3)
- **Published**: 2020-06-05 00:37:09+00:00
- **Updated**: 2021-08-22 02:46:12+00:00
- **Authors**: Omid Mohamad Nezami, Akshay Chaturvedi, Mark Dras, Utpal Garain
- **Comment**: None
- **Journal**: None
- **Summary**: Many recent studies have shown that deep neural models are vulnerable to adversarial samples: images with imperceptible perturbations, for example, can fool image classifiers. In this paper, we present the first type-specific approach to generating adversarial examples for object detection, which entails detecting bounding boxes around multiple objects present in the image and classifying them at the same time, making it a harder task than against image classification. We specifically aim to attack the widely used Faster R-CNN by changing the predicted label for a particular object in an image: where prior work has targeted one specific object (a stop sign), we generalise to arbitrary objects, with the key challenge being the need to change the labels of all bounding boxes for all instances of that object type. To do so, we propose a novel method, named Pick-Object-Attack. Pick-Object-Attack successfully adds perturbations only to bounding boxes for the targeted object, preserving the labels of other detected objects in the image. In terms of perceptibility, the perturbations induced by the method are very small. Furthermore, for the first time, we examine the effect of adversarial attacks on object detection in terms of a downstream task, image captioning; we show that where a method that can modify all object types leads to very obvious changes in captions, the changes from our constrained attack are much less apparent.



### Scene Image Representation by Foreground, Background and Hybrid Features
- **Arxiv ID**: http://arxiv.org/abs/2006.03199v1
- **DOI**: 10.1016/j.eswa.2021.115285
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.03199v1)
- **Published**: 2020-06-05 01:55:24+00:00
- **Updated**: 2020-06-05 01:55:24+00:00
- **Authors**: Chiranjibi Sitaula, Yong Xiang, Sunil Aryal, Xuequan Lu
- **Comment**: Submitted to Expert Systems with Applications (ESWA), 28 pages and 17
  images
- **Journal**: Expert Systems with Applications (ESWA), 2021
- **Summary**: Previous methods for representing scene images based on deep learning primarily consider either the foreground or background information as the discriminating clues for the classification task. However, scene images also require additional information (hybrid) to cope with the inter-class similarity and intra-class variation problems. In this paper, we propose to use hybrid features in addition to foreground and background features to represent scene images. We suppose that these three types of information could jointly help to represent scene image more accurately. To this end, we adopt three VGG-16 architectures pre-trained on ImageNet, Places, and Hybrid (both ImageNet and Places) datasets for the corresponding extraction of foreground, background and hybrid information. All these three types of deep features are further aggregated to achieve our final features for the representation of scene images. Extensive experiments on two large benchmark scene datasets (MIT-67 and SUN-397) show that our method produces the state-of-the-art classification performance.



### Egocentric Object Manipulation Graphs
- **Arxiv ID**: http://arxiv.org/abs/2006.03201v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2006.03201v1)
- **Published**: 2020-06-05 02:03:25+00:00
- **Updated**: 2020-06-05 02:03:25+00:00
- **Authors**: Eadom Dessalene, Michael Maynord, Chinmaya Devaraj, Cornelia Fermuller, Yiannis Aloimonos
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce Egocentric Object Manipulation Graphs (Ego-OMG) - a novel representation for activity modeling and anticipation of near future actions integrating three components: 1) semantic temporal structure of activities, 2) short-term dynamics, and 3) representations for appearance. Semantic temporal structure is modeled through a graph, embedded through a Graph Convolutional Network, whose states model characteristics of and relations between hands and objects. These state representations derive from all three levels of abstraction, and span segments delimited by the making and breaking of hand-object contact. Short-term dynamics are modeled in two ways: A) through 3D convolutions, and B) through anticipating the spatiotemporal end points of hand trajectories, where hands come into contact with objects. Appearance is modeled through deep spatiotemporal features produced through existing methods. We note that in Ego-OMG it is simple to swap these appearance features, and thus Ego-OMG is complementary to most existing action anticipation methods. We evaluate Ego-OMG on the EPIC Kitchens Action Anticipation Challenge. The consistency of the egocentric perspective of EPIC Kitchens allows for the utilization of the hand-centric cues upon which Ego-OMG relies. We demonstrate state-of-the-art performance, outranking all other previous published methods by large margins and ranking first on the unseen test set and second on the seen test set of the EPIC Kitchens Action Anticipation Challenge. We attribute the success of Ego-OMG to the modeling of semantic structure captured over long timespans. We evaluate the design choices made through several ablation studies. Code will be released upon acceptance



### Black-box Explanation of Object Detectors via Saliency Maps
- **Arxiv ID**: http://arxiv.org/abs/2006.03204v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.03204v2)
- **Published**: 2020-06-05 02:13:35+00:00
- **Updated**: 2021-06-10 22:36:13+00:00
- **Authors**: Vitali Petsiuk, Rajiv Jain, Varun Manjunatha, Vlad I. Morariu, Ashutosh Mehra, Vicente Ordonez, Kate Saenko
- **Comment**: CVPR 2021 (oral). Project page
  https://cs-people.bu.edu/vpetsiuk/drise/
- **Journal**: None
- **Summary**: We propose D-RISE, a method for generating visual explanations for the predictions of object detectors. Utilizing the proposed similarity metric that accounts for both localization and categorization aspects of object detection allows our method to produce saliency maps that show image areas that most affect the prediction. D-RISE can be considered "black-box" in the software testing sense, as it only needs access to the inputs and outputs of an object detector. Compared to gradient-based methods, D-RISE is more general and agnostic to the particular type of object detector being tested, and does not need knowledge of the inner workings of the model. We show that D-RISE can be easily applied to different object detectors including one-stage detectors such as YOLOv3 and two-stage detectors such as Faster-RCNN. We present a detailed analysis of the generated visual explanations to highlight the utilization of context and possible biases learned by object detectors.



### Content-Aware Inter-Scale Cost Aggregation for Stereo Matching
- **Arxiv ID**: http://arxiv.org/abs/2006.03209v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.03209v1)
- **Published**: 2020-06-05 02:38:34+00:00
- **Updated**: 2020-06-05 02:38:34+00:00
- **Authors**: Chengtang Yao, Yunde Jia, Huijun Di, Yuwei Wu, Lidong Yu
- **Comment**: 19 pages, 14 figures, 5 tables
- **Journal**: None
- **Summary**: Cost aggregation is a key component of stereo matching for high-quality depth estimation. Most methods use multi-scale processing to downsample cost volume for proper context information, but will cause loss of details when upsampling. In this paper, we present a content-aware inter-scale cost aggregation method that adaptively aggregates and upsamples the cost volume from coarse-scale to fine-scale by learning dynamic filter weights according to the content of the left and right views on the two scales. Our method achieves reliable detail recovery when upsampling through the aggregation of information across different scales. Furthermore, a novel decomposition strategy is proposed to efficiently construct the 3D filter weights and aggregate the 3D cost volume, which greatly reduces the computation cost. We first learn the 2D similarities via the feature maps on the two scales, and then build the 3D filter weights based on the 2D similarities from the left and right views. After that, we split the aggregation in a full 3D spatial-disparity space into the aggregation in 1D disparity space and 2D spatial space. Experiment results on Scene Flow dataset, KITTI2015 and Middlebury demonstrate the effectiveness of our method.



### Content and Context Features for Scene Image Representation
- **Arxiv ID**: http://arxiv.org/abs/2006.03217v3
- **DOI**: 10.1016/j.knosys.2021.107470
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.03217v3)
- **Published**: 2020-06-05 03:19:13+00:00
- **Updated**: 2021-04-24 05:37:31+00:00
- **Authors**: Chiranjibi Sitaula, Sunil Aryal, Yong Xiang, Anish Basnet, Xuequan Lu
- **Comment**: Submitted to Knowledge-Based Systems (Elsevier) for consideration
- **Journal**: Knowledge-based Systems (Elsevier), 2021
- **Summary**: Existing research in scene image classification has focused on either content features (e.g., visual information) or context features (e.g., annotations). As they capture different information about images which can be complementary and useful to discriminate images of different classes, we suppose the fusion of them will improve classification results. In this paper, we propose new techniques to compute content features and context features, and then fuse them together. For content features, we design multi-scale deep features based on background and foreground information in images. For context features, we use annotations of similar images available in the web to design a filter words (codebook). Our experiments in three widely used benchmark scene datasets using support vector machine classifier reveal that our proposed context and content features produce better results than existing context and content features, respectively. The fusion of the proposed two types of features significantly outperform numerous state-of-the-art features.



### FP-Stereo: Hardware-Efficient Stereo Vision for Embedded Applications
- **Arxiv ID**: http://arxiv.org/abs/2006.03250v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AR, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2006.03250v4)
- **Published**: 2020-06-05 06:17:43+00:00
- **Updated**: 2020-07-01 09:20:48+00:00
- **Authors**: Jieru Zhao, Tingyuan Liang, Liang Feng, Wenchao Ding, Sharad Sinha, Wei Zhang, Shaojie Shen
- **Comment**: IEEE International Conference on Field Programmable Logic and
  Applications (FPL), 2020
- **Journal**: None
- **Summary**: Fast and accurate depth estimation, or stereo matching, is essential in embedded stereo vision systems, requiring substantial design effort to achieve an appropriate balance among accuracy, speed and hardware cost. To reduce the design effort and achieve the right balance, we propose FP-Stereo for building high-performance stereo matching pipelines on FPGAs automatically. FP-Stereo consists of an open-source hardware-efficient library, allowing designers to obtain the desired implementation instantly. Diverse methods are supported in our library for each stage of the stereo matching pipeline and a series of techniques are developed to exploit the parallelism and reduce the resource overhead. To improve the usability, FP-Stereo can generate synthesizable C code of the FPGA accelerator with our optimized HLS templates automatically. To guide users for the right design choice meeting specific application requirements, detailed comparisons are performed on various configurations of our library to investigate the accuracy/speed/cost trade-off. Experimental results also show that FP-Stereo outperforms the state-of-the-art FPGA design from all aspects, including 6.08% lower error, 2x faster speed, 30% less resource usage and 40% less energy consumption. Compared to GPU designs, FP-Stereo achieves the same accuracy at a competitive speed while consuming much less energy.



### TCDesc: Learning Topology Consistent Descriptors
- **Arxiv ID**: http://arxiv.org/abs/2006.03254v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.03254v1)
- **Published**: 2020-06-05 06:46:30+00:00
- **Updated**: 2020-06-05 06:46:30+00:00
- **Authors**: Honghu Pan, Fanyang Meng, Zhenyu He, Yongsheng Liang, Wei Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Triplet loss is widely used for learning local descriptors from image patch. However, triplet loss only minimizes the Euclidean distance between matching descriptors and maximizes that between the non-matching descriptors, which neglects the topology similarity between two descriptor sets. In this paper, we propose topology measure besides Euclidean distance to learn topology consistent descriptors by considering kNN descriptors of positive sample. First we establish a novel topology vector for each descriptor followed by Locally Linear Embedding (LLE) to indicate the topological relation among the descriptor and its kNN descriptors. Then we define topology distance between descriptors as the difference of their topology vectors. Last we employ the dynamic weighting strategy to fuse Euclidean distance and topology distance of matching descriptors and take the fusion result as the positive sample distance in the triplet loss. Experimental results on several benchmarks show that our method performs better than state-of-the-arts results and effectively improves the performance of triplet loss.



### Real-time Human Activity Recognition Using Conditionally Parametrized Convolutions on Mobile and Wearable Devices
- **Arxiv ID**: http://arxiv.org/abs/2006.03259v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.03259v2)
- **Published**: 2020-06-05 07:06:42+00:00
- **Updated**: 2020-06-13 07:55:34+00:00
- **Authors**: Xin Cheng, Lei Zhang, Yin Tang, Yue Liu, Hao Wu, Jun He
- **Comment**: 10 pages,14 figures
- **Journal**: None
- **Summary**: Recently, deep learning has represented an important research trend in human activity recognition (HAR). In particular, deep convolutional neural networks (CNNs) have achieved state-of-the-art performance on various HAR datasets. For deep learning, improvements in performance have to heavily rely on increasing model size or capacity to scale to larger and larger datasets, which inevitably leads to the increase of operations. A high number of operations in deep leaning increases computational cost and is not suitable for real-time HAR using mobile and wearable sensors. Though shallow learning techniques often are lightweight, they could not achieve good performance. Therefore, deep learning methods that can balance the trade-off between accuracy and computation cost is highly needed, which to our knowledge has seldom been researched. In this paper, we for the first time propose a computation efficient CNN using conditionally parametrized convolution for real-time HAR on mobile and wearable devices. We evaluate the proposed method on four public benchmark HAR datasets consisting of WISDM dataset, PAMAP2 dataset, UNIMIB-SHAR dataset, and OPPORTUNITY dataset, achieving state-of-the-art accuracy without compromising computation cost. Various ablation experiments are performed to show how such a network with large capacity is clearly preferable to baseline while requiring a similar amount of operations. The method can be used as a drop-in replacement for the existing deep HAR architectures and easily deployed onto mobile and wearable devices for real-time HAR applications.



### Convolutional Neural Networks for Global Human Settlements Mapping from Sentinel-2 Satellite Imagery
- **Arxiv ID**: http://arxiv.org/abs/2006.03267v2
- **DOI**: 10.1007/s00521-020-05449-7
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.03267v2)
- **Published**: 2020-06-05 07:28:19+00:00
- **Updated**: 2020-10-29 09:14:57+00:00
- **Authors**: Christina Corbane, Vasileios Syrris, Filip Sabo, Panagiotis Politis, Michele Melchiorri, Martino Pesaresi, Pierre Soille, Thomas Kemper
- **Comment**: 51 pages including supplementary material, 13 Figures in the main
  manuscript, under review in Neural Computing and Applications journal
- **Journal**: Neural computing and Applications, 2020
- **Summary**: Spatially consistent and up-to-date maps of human settlements are crucial for addressing policies related to urbanization and sustainability, especially in the era of an increasingly urbanized world.The availability of open and free Sentinel-2 data of the Copernicus Earth Observation program offers a new opportunity for wall-to-wall mapping of human settlements at a global scale.This paper presents a deep-learning-based framework for a fully automated extraction of built-up areas at a spatial resolution of 10 m from a global composite of Sentinel-2 imagery.A multi-neuro modeling methodology building on a simple Convolution Neural Networks architecture for pixel-wise image classification of built-up areas is developed.The core features of the proposed model are the image patch of size 5 x 5 pixels adequate for describing built-up areas from Sentinel-2 imagery and the lightweight topology with a total number of 1,448,578 trainable parameters and 4 2D convolutional layers and 2 flattened layers.The deployment of the model on the global Sentinel-2 image composite provides the most detailed and complete map reporting about built-up areas for reference year 2018. The validation of the results with an independent reference data-set of building footprints covering 277 sites across the world establishes the reliability of the built-up layer produced by the proposed framework and the model robustness.



### Biometric Quality: Review and Application to Face Recognition with FaceQnet
- **Arxiv ID**: http://arxiv.org/abs/2006.03298v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.03298v3)
- **Published**: 2020-06-05 08:33:22+00:00
- **Updated**: 2021-02-28 07:46:41+00:00
- **Authors**: Javier Hernandez-Ortega, Javier Galbally, Julian Fierrez, Laurent Beslay
- **Comment**: None
- **Journal**: None
- **Summary**: "The output of a computerised system can only be as accurate as the information entered into it." This rather trivial statement is the basis behind one of the driving concepts in biometric recognition: biometric quality. Quality is nowadays widely regarded as the number one factor responsible for the good or bad performance of automated biometric systems. It refers to the ability of a biometric sample to be used for recognition purposes and produce consistent, accurate, and reliable results. Such a subjective term is objectively estimated by the so-called biometric quality metrics. These algorithms play nowadays a pivotal role in the correct functioning of systems, providing feedback to the users and working as invaluable audit tools. In spite of their unanimously accepted relevance, some of the most used and deployed biometric characteristics are lacking behind in the development of these methods. This is the case of face recognition. After a gentle introduction to the general topic of biometric quality and a review of past efforts in face quality metrics, in the present work, we address the need for better face quality metrics by developing FaceQnet. FaceQnet is a novel open-source face quality assessment tool, inspired and powered by deep learning technology, which assigns a scalar quality measure to facial images, as prediction of their recognition accuracy. Two versions of FaceQnet have been thoroughly evaluated both in this work and also independently by NIST, showing the soundness of the approach and its competitiveness with respect to current state-of-the-art metrics. Even though our work is presented here particularly in the framework of face biometrics, the proposed methodology for building a fully automated quality metric can be very useful and easily adapted to other artificial intelligence tasks.



### Multi-modal Feature Fusion with Feature Attention for VATEX Captioning Challenge 2020
- **Arxiv ID**: http://arxiv.org/abs/2006.03315v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.03315v1)
- **Published**: 2020-06-05 09:00:36+00:00
- **Updated**: 2020-06-05 09:00:36+00:00
- **Authors**: Ke Lin, Zhuoxin Gan, Liwei Wang
- **Comment**: None
- **Journal**: None
- **Summary**: This report describes our model for VATEX Captioning Challenge 2020. First, to gather information from multiple domains, we extract motion, appearance, semantic and audio features. Then we design a feature attention module to attend on different feature when decoding. We apply two types of decoders, top-down and X-LAN and ensemble these models to get the final result. The proposed method outperforms official baseline with a significant gap. We achieve 76.0 CIDEr and 50.0 CIDEr on English and Chinese private test set. We rank 2nd on both English and Chinese private test leaderboard.



### MANTRA: Memory Augmented Networks for Multiple Trajectory Prediction
- **Arxiv ID**: http://arxiv.org/abs/2006.03340v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.03340v2)
- **Published**: 2020-06-05 09:49:59+00:00
- **Updated**: 2021-06-03 22:52:06+00:00
- **Authors**: Francesco Marchetti, Federico Becattini, Lorenzo Seidenari, Alberto Del Bimbo
- **Comment**: Accepted at CVPR20
- **Journal**: None
- **Summary**: Autonomous vehicles are expected to drive in complex scenarios with several independent non cooperating agents. Path planning for safely navigating in such environments can not just rely on perceiving present location and motion of other agents. It requires instead to predict such variables in a far enough future. In this paper we address the problem of multimodal trajectory prediction exploiting a Memory Augmented Neural Network. Our method learns past and future trajectory embeddings using recurrent neural networks and exploits an associative external memory to store and retrieve such embeddings. Trajectory prediction is then performed by decoding in-memory future encodings conditioned with the observed past. We incorporate scene knowledge in the decoding state by learning a CNN on top of semantic scene maps. Memory growth is limited by learning a writing controller based on the predictive capability of existing embeddings. We show that our method is able to natively perform multi-modal trajectory prediction obtaining state-of-the art results on three datasets. Moreover, thanks to the non-parametric nature of the memory module, we show how once trained our system can continuously improve by ingesting novel patterns.



### Explaining Autonomous Driving by Learning End-to-End Visual Attention
- **Arxiv ID**: http://arxiv.org/abs/2006.03347v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.03347v1)
- **Published**: 2020-06-05 10:12:31+00:00
- **Updated**: 2020-06-05 10:12:31+00:00
- **Authors**: Luca Cultrera, Lorenzo Seidenari, Federico Becattini, Pietro Pala, Alberto Del Bimbo
- **Comment**: accepted at CVPR20 Workshop on Safe Artificial Intelligence for
  Automated Driving (SAIAD20)
- **Journal**: None
- **Summary**: Current deep learning based autonomous driving approaches yield impressive results also leading to in-production deployment in certain controlled scenarios. One of the most popular and fascinating approaches relies on learning vehicle controls directly from data perceived by sensors. This end-to-end learning paradigm can be applied both in classical supervised settings and using reinforcement learning. Nonetheless the main drawback of this approach as also in other learning problems is the lack of explainability. Indeed, a deep network will act as a black-box outputting predictions depending on previously seen driving patterns without giving any feedback on why such decisions were taken. While to obtain optimal performance it is not critical to obtain explainable outputs from a learned agent, especially in such a safety critical field, it is of paramount importance to understand how the network behaves. This is particularly relevant to interpret failures of such systems. In this work we propose to train an imitation learning based agent equipped with an attention model. The attention model allows us to understand what part of the image has been deemed most important. Interestingly, the use of attention also leads to superior performance in a standard benchmark using the CARLA driving simulator.



### Uncertainty-Aware CNNs for Depth Completion: Uncertainty from Beginning to End
- **Arxiv ID**: http://arxiv.org/abs/2006.03349v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.03349v1)
- **Published**: 2020-06-05 10:18:35+00:00
- **Updated**: 2020-06-05 10:18:35+00:00
- **Authors**: Abdelrahman Eldesokey, Michael Felsberg, Karl Holmquist, Mikael Persson
- **Comment**: CVPR2020 (8 pages + supplementary)
- **Journal**: None
- **Summary**: The focus in deep learning research has been mostly to push the limits of prediction accuracy. However, this was often achieved at the cost of increased complexity, raising concerns about the interpretability and the reliability of deep networks. Recently, an increasing attention has been given to untangling the complexity of deep networks and quantifying their uncertainty for different computer vision tasks. Differently, the task of depth completion has not received enough attention despite the inherent noisy nature of depth sensors. In this work, we thus focus on modeling the uncertainty of depth data in depth completion starting from the sparse noisy input all the way to the final prediction.   We propose a novel approach to identify disturbed measurements in the input by learning an input confidence estimator in a self-supervised manner based on the normalized convolutional neural networks (NCNNs). Further, we propose a probabilistic version of NCNNs that produces a statistically meaningful uncertainty measure for the final prediction. When we evaluate our approach on the KITTI dataset for depth completion, we outperform all the existing Bayesian Deep Learning approaches in terms of prediction accuracy, quality of the uncertainty measure, and the computational efficiency. Moreover, our small network with 670k parameters performs on-par with conventional approaches with millions of parameters. These results give strong evidence that separating the network into parallel uncertainty and prediction streams leads to state-of-the-art performance with accurate uncertainty estimates.



### Learning to Rank Learning Curves
- **Arxiv ID**: http://arxiv.org/abs/2006.03361v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.03361v1)
- **Published**: 2020-06-05 10:49:52+00:00
- **Updated**: 2020-06-05 10:49:52+00:00
- **Authors**: Martin Wistuba, Tejaswini Pedapati
- **Comment**: Accepted at the International Conference on Machine Learning (ICML)
  2020
- **Journal**: None
- **Summary**: Many automated machine learning methods, such as those for hyperparameter and neural architecture optimization, are computationally expensive because they involve training many different model configurations. In this work, we present a new method that saves computational budget by terminating poor configurations early on in the training. In contrast to existing methods, we consider this task as a ranking and transfer learning problem. We qualitatively show that by optimizing a pairwise ranking loss and leveraging learning curves from other datasets, our model is able to effectively rank learning curves without having to observe many or very long learning curves. We further demonstrate that our method can be used to accelerate a neural architecture search by a factor of up to 100 without a significant performance degradation of the discovered architecture. In further experiments we analyze the quality of ranking, the influence of different model components as well as the predictive behavior of the model.



### Structurally aware bidirectional unpaired image to image translation between CT and MR
- **Arxiv ID**: http://arxiv.org/abs/2006.03374v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.03374v1)
- **Published**: 2020-06-05 11:21:56+00:00
- **Updated**: 2020-06-05 11:21:56+00:00
- **Authors**: Vismay Agrawal, Avinash Kori, Vikas Kumar Anand, Ganapathy Krishnamurthi
- **Comment**: 9 pages, 4 figures
- **Journal**: None
- **Summary**: Magnetic Resonance (MR) Imaging and Computed Tomography (CT) are the primary diagnostic imaging modalities quite frequently used for surgical planning and analysis. A general problem with medical imaging is that the acquisition process is quite expensive and time-consuming. Deep learning techniques like generative adversarial networks (GANs) can help us to leverage the possibility of an image to image translation between multiple imaging modalities, which in turn helps in saving time and cost. These techniques will help to conduct surgical planning under CT with the feedback of MRI information. While previous studies have shown paired and unpaired image synthesis from MR to CT, image synthesis from CT to MR still remains a challenge, since it involves the addition of extra tissue information. In this manuscript, we have implemented two different variations of Generative Adversarial Networks exploiting the cycling consistency and structural similarity between both CT and MR image modalities on a pelvis dataset, thus facilitating a bidirectional exchange of content and style between these image modalities. The proposed GANs translate the input medical images by different mechanisms, and hence generated images not only appears realistic but also performs well across various comparison metrics, and these images have also been cross verified with a radiologist. The radiologist verification has shown that slight variations in generated MR and CT images may not be exactly the same as their true counterpart but it can be used for medical purposes.



### A Dataset and Benchmarks for Multimedia Social Analysis
- **Arxiv ID**: http://arxiv.org/abs/2006.08335v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2006.08335v1)
- **Published**: 2020-06-05 11:33:01+00:00
- **Updated**: 2020-06-05 11:33:01+00:00
- **Authors**: Bofan Xue, David Chan, John Canny
- **Comment**: Published as a workshop paper at "Multimodality Learning" (CVPR 2020)
- **Journal**: None
- **Summary**: We present a new publicly available dataset with the goal of advancing multi-modality learning by offering vision and language data within the same context. This is achieved by obtaining data from a social media website with posts containing multiple paired images/videos and text, along with comment trees containing images/videos and/or text. With a total of 677k posts, 2.9 million post images, 488k post videos, 1.4 million comment images, 4.6 million comment videos, and 96.9 million comments, data from different modalities can be jointly used to improve performances for a variety of tasks such as image captioning, image classification, next frame prediction, sentiment analysis, and language modeling. We present a wide range of statistics for our dataset. Finally, we provide baseline performance analysis for one of the regression tasks using pre-trained models and several fully connected networks.



### Detection of prostate cancer in whole-slide images through end-to-end training with image-level labels
- **Arxiv ID**: http://arxiv.org/abs/2006.03394v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.03394v1)
- **Published**: 2020-06-05 12:11:35+00:00
- **Updated**: 2020-06-05 12:11:35+00:00
- **Authors**: Hans Pinckaers, Wouter Bulten, Jeroen van der Laak, Geert Litjens
- **Comment**: None
- **Journal**: None
- **Summary**: Prostate cancer is the most prevalent cancer among men in Western countries, with 1.1 million new diagnoses every year. The gold standard for the diagnosis of prostate cancer is a pathologists' evaluation of prostate tissue.   To potentially assist pathologists deep-learning-based cancer detection systems have been developed. Many of the state-of-the-art models are patch-based convolutional neural networks, as the use of entire scanned slides is hampered by memory limitations on accelerator cards. Patch-based systems typically require detailed, pixel-level annotations for effective training. However, such annotations are seldom readily available, in contrast to the clinical reports of pathologists, which contain slide-level labels. As such, developing algorithms which do not require manual pixel-wise annotations, but can learn using only the clinical report would be a significant advancement for the field.   In this paper, we propose to use a streaming implementation of convolutional layers, to train a modern CNN (ResNet-34) with 21 million parameters end-to-end on 4712 prostate biopsies. The method enables the use of entire biopsy images at high-resolution directly by reducing the GPU memory requirements by 2.4 TB. We show that modern CNNs, trained using our streaming approach, can extract meaningful features from high-resolution images without additional heuristics, reaching similar performance as state-of-the-art patch-based and multiple-instance learning methods. By circumventing the need for manual annotations, this approach can function as a blueprint for other tasks in histopathological diagnosis.   The source code to reproduce the streaming models is available at https://github.com/DIAGNijmegen/pathology-streaming-pipeline .



### Learning Neural Light Transport
- **Arxiv ID**: http://arxiv.org/abs/2006.03427v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.03427v1)
- **Published**: 2020-06-05 13:26:05+00:00
- **Updated**: 2020-06-05 13:26:05+00:00
- **Authors**: Paul Sanzenbacher, Lars Mescheder, Andreas Geiger
- **Comment**: 31 pages, 17 figures
- **Journal**: None
- **Summary**: In recent years, deep generative models have gained significance due to their ability to synthesize natural-looking images with applications ranging from virtual reality to data augmentation for training computer vision models. While existing models are able to faithfully learn the image distribution of the training set, they often lack controllability as they operate in 2D pixel space and do not model the physical image formation process. In this work, we investigate the importance of 3D reasoning for photorealistic rendering. We present an approach for learning light transport in static and dynamic 3D scenes using a neural network with the goal of predicting photorealistic images. In contrast to existing approaches that operate in the 2D image domain, our approach reasons in both 3D and 2D space, thus enabling global illumination effects and manipulation of 3D scene geometry. Experimentally, we find that our model is able to produce photorealistic renderings of static and dynamic scenes. Moreover, it compares favorably to baselines which combine path tracing and image denoising at the same computational budget.



### Acoustic Anomaly Detection for Machine Sounds based on Image Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/2006.03429v2
- **DOI**: 10.5220/0010185800490056
- **Categories**: **eess.AS**, cs.CV, cs.LG, cs.SD
- **Links**: [PDF](http://arxiv.org/pdf/2006.03429v2)
- **Published**: 2020-06-05 13:29:12+00:00
- **Updated**: 2020-12-11 12:06:30+00:00
- **Authors**: Robert Müller, Fabian Ritz, Steffen Illium, Claudia Linnhoff-Popien
- **Comment**: ICAART 2021, 8 pages, 2 figures, 1 table
- **Journal**: None
- **Summary**: In industrial applications, the early detection of malfunctioning factory machinery is crucial. In this paper, we consider acoustic malfunction detection via transfer learning. Contrary to the majority of current approaches which are based on deep autoencoders, we propose to extract features using neural networks that were pretrained on the task of image classification. We then use these features to train a variety of anomaly detection models and show that this improves results compared to convolutional autoencoders in recordings of four different factory machines in noisy environments. Moreover, we find that features extracted from ResNet based networks yield better results than those from AlexNet and Squeezenet. In our setting, Gaussian Mixture Models and One-Class Support Vector Machines achieve the best anomaly detection performance.



### Artificial Intelligence-based Clinical Decision Support for COVID-19 -- Where Art Thou?
- **Arxiv ID**: http://arxiv.org/abs/2006.03434v1
- **DOI**: None
- **Categories**: **cs.CY**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.03434v1)
- **Published**: 2020-06-05 13:34:47+00:00
- **Updated**: 2020-06-05 13:34:47+00:00
- **Authors**: Mathias Unberath, Kimia Ghobadi, Scott Levin, Jeremiah Hinson, Gregory D Hager
- **Comment**: Invited perspective piece on AI in the fight against COVID-19 to
  appear in Advanced Intelligent Systems
- **Journal**: None
- **Summary**: The COVID-19 crisis has brought about new clinical questions, new workflows, and accelerated distributed healthcare needs. While artificial intelligence (AI)-based clinical decision support seemed to have matured, the application of AI-based tools for COVID-19 has been limited to date. In this perspective piece, we identify opportunities and requirements for AI-based clinical decision support systems and highlight challenges that impact "AI readiness" for rapidly emergent healthcare challenges.



### Incorporating Image Gradients as Secondary Input Associated with Input Image to Improve the Performance of the CNN Model
- **Arxiv ID**: http://arxiv.org/abs/2006.04570v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.04570v1)
- **Published**: 2020-06-05 14:01:52+00:00
- **Updated**: 2020-06-05 14:01:52+00:00
- **Authors**: Vijay Pandey, Shashi Bhushan Jha
- **Comment**: None
- **Journal**: None
- **Summary**: CNN is very popular neural network architecture in modern days. It is primarily most used tool for vision related task to extract the important features from the given image. Moreover, CNN works as a filter to extract the important features using convolutional operation in distinct layers. In existing CNN architectures, to train the network on given input, only single form of given input is fed to the network. In this paper, new architecture has been proposed where given input is passed in more than one form to the network simultaneously by sharing the layers with both forms of input. We incorporate image gradient as second form of the input associated with the original input image and allowing both inputs to flow in the network using same number of parameters to improve the performance of the model for better generalization. The results of the proposed CNN architecture, applying on diverse set of datasets such as MNIST, CIFAR10 and CIFAR100 show superior result compared to the benchmark CNN architecture considering inputs in single form.



### Segmentation of Surgical Instruments for Minimally-Invasive Robot-Assisted Procedures Using Generative Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2006.03486v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.03486v1)
- **Published**: 2020-06-05 14:39:41+00:00
- **Updated**: 2020-06-05 14:39:41+00:00
- **Authors**: Iñigo Azqueta-Gavaldon, Florian Fröhlich, Klaus Strobl, Rudolph Triebel
- **Comment**: None
- **Journal**: None
- **Summary**: This work proves that semantic segmentation on minimally invasive surgical instruments can be improved by using training data that has been augmented through domain adaptation. The benefit of this method is twofold. Firstly, it suppresses the need of manually labeling thousands of images by transforming synthetic data into realistic-looking data. To achieve this, a CycleGAN model is used, which transforms a source dataset to approximate the domain distribution of a target dataset. Secondly, this newly generated data with perfect labels is utilized to train a semantic segmentation neural network, U-Net. This method shows generalization capabilities on data with variability regarding its rotation- position- and lighting conditions. Nevertheless, one of the caveats of this approach is that the model is unable to generalize well to other surgical instruments with a different shape from the one used for training. This is driven by the lack of a high variance in the geometric distribution of the training data. Future work will focus on making the model more scale-invariant and able to adapt to other types of surgical instruments previously unseen by the training.



### VALUE: Large Scale Voting-based Automatic Labelling for Urban Environments
- **Arxiv ID**: http://arxiv.org/abs/2006.03492v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2006.03492v1)
- **Published**: 2020-06-05 14:47:03+00:00
- **Updated**: 2020-06-05 14:47:03+00:00
- **Authors**: Giacomo Dabisias, Emanuele Ruffaldi, Hugo Grimmett, Peter Ondruska
- **Comment**: Presented at ICRA-2018 conference, 20-25th May 2018, Brisbane,
  Australia
- **Journal**: None
- **Summary**: This paper presents a simple and robust method for the automatic localisation of static 3D objects in large-scale urban environments. By exploiting the potential to merge a large volume of noisy but accurately localised 2D image data, we achieve superior performance in terms of both robustness and accuracy of the recovered 3D information. The method is based on a simple distributed voting schema which can be fully distributed and parallelised to scale to large-scale scenarios. To evaluate the method we collected city-scale data sets from New York City and San Francisco consisting of almost 400k images spanning the area of 40 km$^2$ and used it to accurately recover the 3D positions of traffic lights. We demonstrate a robust performance and also show that the solution improves in quality over time as the amount of data increases.



### A Meta-Bayesian Model of Intentional Visual Search
- **Arxiv ID**: http://arxiv.org/abs/2006.03531v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.03531v1)
- **Published**: 2020-06-05 16:10:35+00:00
- **Updated**: 2020-06-05 16:10:35+00:00
- **Authors**: Maell Cullen, Jonathan Monney, M. Berk Mirza, Rosalyn Moran
- **Comment**: Submitted to NeurIPS 2020
- **Journal**: None
- **Summary**: We propose a computational model of visual search that incorporates Bayesian interpretations of the neural mechanisms that underlie categorical perception and saccade planning. To enable meaningful comparisons between simulated and human behaviours, we employ a gaze-contingent paradigm that required participants to classify occluded MNIST digits through a window that followed their gaze. The conditional independencies imposed by a separation of time scales in this task are embodied by constraints on the hierarchical structure of our model; planning and decision making are cast as a partially observable Markov Decision Process while proprioceptive and exteroceptive signals are integrated by a dynamic model that facilitates approximate inference on visual information and its latent causes. Our model is able to recapitulate human behavioural metrics such as classification accuracy while retaining a high degree of interpretability, which we demonstrate by recovering subject-specific parameters from observed human behaviour.



### Novel Object Viewpoint Estimation through Reconstruction Alignment
- **Arxiv ID**: http://arxiv.org/abs/2006.03586v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.03586v1)
- **Published**: 2020-06-05 17:58:14+00:00
- **Updated**: 2020-06-05 17:58:14+00:00
- **Authors**: Mohamed El Banani, Jason J. Corso, David F. Fouhey
- **Comment**: To appear at CVPR 2020. Project page:
  https://mbanani.github.io/novelviewpoints/
- **Journal**: None
- **Summary**: The goal of this paper is to estimate the viewpoint for a novel object. Standard viewpoint estimation approaches generally fail on this task due to their reliance on a 3D model for alignment or large amounts of class-specific training data and their corresponding canonical pose. We overcome those limitations by learning a reconstruct and align approach. Our key insight is that although we do not have an explicit 3D model or a predefined canonical pose, we can still learn to estimate the object's shape in the viewer's frame and then use an image to provide our reference model or canonical pose. In particular, we propose learning two networks: the first maps images to a 3D geometry-aware feature bottleneck and is trained via an image-to-image translation loss; the second learns whether two instances of features are aligned. At test time, our model finds the relative transformation that best aligns the bottleneck features of our test image to a reference image. We evaluate our method on novel object viewpoint estimation by generalizing across different datasets, analyzing the impact of our different modules, and providing a qualitative analysis of the learned features to identify what representations are being learnt for alignment.



### Data Augmentation using Generative Adversarial Networks (GANs) for GAN-based Detection of Pneumonia and COVID-19 in Chest X-ray Images
- **Arxiv ID**: http://arxiv.org/abs/2006.03622v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2006.03622v2)
- **Published**: 2020-06-05 18:30:20+00:00
- **Updated**: 2021-01-12 20:27:04+00:00
- **Authors**: Saman Motamed, Patrik Rogalla, Farzad Khalvati
- **Comment**: None
- **Journal**: None
- **Summary**: Successful training of convolutional neural networks (CNNs) requires a substantial amount of data. With small datasets networks generalize poorly. Data Augmentation techniques improve the generalizability of neural networks by using existing training data more effectively. Standard data augmentation methods, however, produce limited plausible alternative data. Generative Adversarial Networks (GANs) have been utilized to generate new data and improve the performance of CNNs. Nevertheless, data augmentation techniques for training GANs are under-explored compared to CNNs. In this work, we propose a new GAN architecture for augmentation of chest X-rays for semi-supervised detection of pneumonia and COVID-19 using generative models. We show that the proposed GAN can be used to effectively augment data and improve classification accuracy of disease in chest X-rays for pneumonia and COVID-19. We compare our augmentation GAN model with Deep Convolutional GAN and traditional augmentation methods (rotate, zoom, etc) on two different X-ray datasets and show our GAN-based augmentation method surpasses other augmentation methods for training a GAN in detecting anomalies in X-ray images.



### Equivariant Maps for Hierarchical Structures
- **Arxiv ID**: http://arxiv.org/abs/2006.03627v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, math.GR, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.03627v2)
- **Published**: 2020-06-05 18:42:12+00:00
- **Updated**: 2020-11-24 01:54:52+00:00
- **Authors**: Renhao Wang, Marjan Albooyeh, Siamak Ravanbakhsh
- **Comment**: None
- **Journal**: None
- **Summary**: While using invariant and equivariant maps, it is possible to apply deep learning to a range of primitive data structures, a formalism for dealing with hierarchy is lacking. This is a significant issue because many practical structures are hierarchies of simple building blocks; some examples include sequences of sets, graphs of graphs, or multiresolution images. Observing that the symmetry of a hierarchical structure is the "wreath product" of symmetries of the building blocks, we express the equivariant map for the hierarchy using an intuitive combination of the equivariant linear layers of the building blocks. More generally, we show that any equivariant map for the hierarchy has this form. To demonstrate the effectiveness of this approach to model design, we consider its application in the semantic segmentation of point-cloud data. By voxelizing the point cloud, we impose a hierarchy of translation and permutation symmetries on the data and report state-of-the-art on Semantic3D, S3DIS, and vKITTI, that include some of the largest real-world point-cloud benchmarks.



### Hierarchical Class-Based Curriculum Loss
- **Arxiv ID**: http://arxiv.org/abs/2006.03629v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.03629v1)
- **Published**: 2020-06-05 18:48:57+00:00
- **Updated**: 2020-06-05 18:48:57+00:00
- **Authors**: Palash Goyal, Shalini Ghosh
- **Comment**: None
- **Journal**: None
- **Summary**: Classification algorithms in machine learning often assume a flat label space. However, most real world data have dependencies between the labels, which can often be captured by using a hierarchy. Utilizing this relation can help develop a model capable of satisfying the dependencies and improving model accuracy and interpretability. Further, as different levels in the hierarchy correspond to different granularities, penalizing each label equally can be detrimental to model learning. In this paper, we propose a loss function, hierarchical curriculum loss, with two properties: (i) satisfy hierarchical constraints present in the label space, and (ii) provide non-uniform weights to labels based on their levels in the hierarchy, learned implicitly by the training paradigm. We theoretically show that the proposed loss function is a tighter bound of 0-1 loss compared to any other loss satisfying the hierarchical constraints. We test our loss function on real world image data sets, and show that it significantly substantially outperforms multiple baselines.



### SparseFusion: Dynamic Human Avatar Modeling from Sparse RGBD Images
- **Arxiv ID**: http://arxiv.org/abs/2006.03630v1
- **DOI**: 10.1109/TMM.2020.3001506
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.03630v1)
- **Published**: 2020-06-05 18:53:36+00:00
- **Updated**: 2020-06-05 18:53:36+00:00
- **Authors**: Xinxin Zuo, Sen Wang, Jiangbin Zheng, Weiwei Yu, Minglun Gong, Ruigang Yang, Li Cheng
- **Comment**: Accepted by TMM
- **Journal**: None
- **Summary**: In this paper, we propose a novel approach to reconstruct 3D human body shapes based on a sparse set of RGBD frames using a single RGBD camera. We specifically focus on the realistic settings where human subjects move freely during the capture. The main challenge is how to robustly fuse these sparse frames into a canonical 3D model, under pose changes and surface occlusions. This is addressed by our new framework consisting of the following steps. First, based on a generative human template, for every two frames having sufficient overlap, an initial pairwise alignment is performed; It is followed by a global non-rigid registration procedure, in which partial results from RGBD frames are collected into a unified 3D shape, under the guidance of correspondences from the pairwise alignment; Finally, the texture map of the reconstructed human model is optimized to deliver a clear and spatially consistent texture. Empirical evaluations on synthetic and real datasets demonstrate both quantitatively and qualitatively the superior performance of our framework in reconstructing complete 3D human models with high fidelity. It is worth noting that our framework is flexible, with potential applications going beyond shape reconstruction. As an example, we showcase its use in reshaping and reposing to a new avatar.



### Robust Face Verification via Disentangled Representations
- **Arxiv ID**: http://arxiv.org/abs/2006.03638v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.03638v2)
- **Published**: 2020-06-05 19:17:02+00:00
- **Updated**: 2020-06-23 16:43:02+00:00
- **Authors**: Marius Arvinte, Ahmed H. Tewfik, Sriram Vishwanath
- **Comment**: Preprint
- **Journal**: None
- **Summary**: We introduce a robust algorithm for face verification, i.e., deciding whether twoimages are of the same person or not. Our approach is a novel take on the idea ofusing deep generative networks for adversarial robustness. We use the generativemodel during training as an online augmentation method instead of a test-timepurifier that removes adversarial noise. Our architecture uses a contrastive loss termand a disentangled generative model to sample negative pairs. Instead of randomlypairing two real images, we pair an image with its class-modified counterpart whilekeeping its content (pose, head tilt, hair, etc.) intact. This enables us to efficientlysample hard negative pairs for the contrastive loss. We experimentally show that, when coupled with adversarial training, the proposed scheme converges with aweak inner solver and has a higher clean and robust accuracy than state-of-the-art-methods when evaluated against white-box physical attacks.



### Knowledge transfer between bridges for drive-by monitoring using adversarial and multi-task learning
- **Arxiv ID**: http://arxiv.org/abs/2006.03641v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.03641v1)
- **Published**: 2020-06-05 19:18:45+00:00
- **Updated**: 2020-06-05 19:18:45+00:00
- **Authors**: Jingxiao Liu, Mario Bergés, Jacobo Bielak, Hae Young Noh
- **Comment**: None
- **Journal**: None
- **Summary**: Monitoring bridge health using the vibrations of drive-by vehicles has various benefits, such as low cost and no need for direct installation or on-site maintenance of equipment on the bridge. However, many such approaches require labeled data from every bridge, which is expensive and time-consuming, if not impossible, to obtain. This is further exacerbated by having multiple diagnostic tasks, such as damage quantification and localization. One way to address this issue is to directly apply the supervised model trained for one bridge to other bridges, although this may significantly reduce the accuracy because of distribution mismatch between different bridges'data. To alleviate these problems, we introduce a transfer learning framework using domain-adversarial training and multi-task learning to detect, localize and quantify damage. Specifically, we train a deep network in an adversarial way to learn features that are 1) sensitive to damage and 2) invariant to different bridges. In addition, to improve the error propagation from one task to the next, our framework learns shared features for all the tasks using multi-task learning. We evaluate our framework using lab-scale experiments with two different bridges. On average, our framework achieves 94%, 97% and 84% accuracy for damage detection, localization and quantification, respectively. within one damage severity level.



### RIT-Eyes: Rendering of near-eye images for eye-tracking applications
- **Arxiv ID**: http://arxiv.org/abs/2006.03642v1
- **DOI**: 10.1145/3385955.3407935
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.03642v1)
- **Published**: 2020-06-05 19:18:50+00:00
- **Updated**: 2020-06-05 19:18:50+00:00
- **Authors**: Nitinraj Nair, Rakshit Kothari, Aayush K. Chaudhary, Zhizhuo Yang, Gabriel J. Diaz, Jeff B. Pelz, Reynold J. Bailey
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks for video-based eye tracking have demonstrated resilience to noisy environments, stray reflections, and low resolution. However, to train these networks, a large number of manually annotated images are required. To alleviate the cumbersome process of manual labeling, computer graphics rendering is employed to automatically generate a large corpus of annotated eye images under various conditions. In this work, we introduce a synthetic eye image generation platform that improves upon previous work by adding features such as an active deformable iris, an aspherical cornea, retinal retro-reflection, gaze-coordinated eye-lid deformations, and blinks. To demonstrate the utility of our platform, we render images reflecting the represented gaze distributions inherent in two publicly available datasets, NVGaze and OpenEDS. We also report on the performance of two semantic segmentation architectures (SegNet and RITnet) trained on rendered images and tested on the original datasets.



### AutoHAS: Efficient Hyperparameter and Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/2006.03656v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.03656v3)
- **Published**: 2020-06-05 19:57:24+00:00
- **Updated**: 2021-04-07 06:55:00+00:00
- **Authors**: Xuanyi Dong, Mingxing Tan, Adams Wei Yu, Daiyi Peng, Bogdan Gabrys, Quoc V. Le
- **Comment**: Accepted to 2nd Workshop on Neural Architecture Search at ICLR 2021
- **Journal**: None
- **Summary**: Efficient hyperparameter or architecture search methods have shown remarkable results, but each of them is only applicable to searching for either hyperparameters (HPs) or architectures. In this work, we propose a unified pipeline, AutoHAS, to efficiently search for both architectures and hyperparameters. AutoHAS learns to alternately update the shared network weights and a reinforcement learning (RL) controller, which learns the probability distribution for the architecture candidates and HP candidates. A temporary weight is introduced to store the updated weight from the selected HPs (by the controller), and a validation accuracy based on this temporary weight serves as a reward to update the controller. In experiments, we show AutoHAS is efficient and generalizable to different search spaces, baselines and datasets. In particular, AutoHAS can improve the accuracy over popular network architectures, such as ResNet and EfficientNet, on CIFAR-10/100, ImageNet, and four more other datasets.



### Hierarchical, rotation-equivariant neural networks to select structural models of protein complexes
- **Arxiv ID**: http://arxiv.org/abs/2006.09275v2
- **DOI**: 10.1002/prot.26033
- **Categories**: **q-bio.BM**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.09275v2)
- **Published**: 2020-06-05 20:17:12+00:00
- **Updated**: 2021-01-23 00:47:10+00:00
- **Authors**: Stephan Eismann, Raphael J. L. Townshend, Nathaniel Thomas, Milind Jagota, Bowen Jing, Ron O. Dror
- **Comment**: 11 pages, 5 figures + SI: Updated based on the published version in
  PROTEINS. Presented at NeurIPS 2019 workshop Learning Meaningful
  Representations of Life
- **Journal**: None
- **Summary**: Predicting the structure of multi-protein complexes is a grand challenge in biochemistry, with major implications for basic science and drug discovery. Computational structure prediction methods generally leverage pre-defined structural features to distinguish accurate structural models from less accurate ones. This raises the question of whether it is possible to learn characteristics of accurate models directly from atomic coordinates of protein complexes, with no prior assumptions. Here we introduce a machine learning method that learns directly from the 3D positions of all atoms to identify accurate models of protein complexes, without using any pre-computed physics-inspired or statistical terms. Our neural network architecture combines multiple ingredients that together enable end-to-end learning from molecular structures containing tens of thousands of atoms: a point-based representation of atoms, equivariance with respect to rotation and translation, local convolutions, and hierarchical subsampling operations. When used in combination with previously developed scoring functions, our network substantially improves the identification of accurate structural models among a large set of possible models. Our network can also be used to predict the accuracy of a given structural model in absolute terms. The architecture we present is readily applicable to other tasks involving learning on 3D structures of large atomic systems.



### Visual Transformers: Token-based Image Representation and Processing for Computer Vision
- **Arxiv ID**: http://arxiv.org/abs/2006.03677v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.03677v4)
- **Published**: 2020-06-05 20:49:49+00:00
- **Updated**: 2020-11-20 00:10:51+00:00
- **Authors**: Bichen Wu, Chenfeng Xu, Xiaoliang Dai, Alvin Wan, Peizhao Zhang, Zhicheng Yan, Masayoshi Tomizuka, Joseph Gonzalez, Kurt Keutzer, Peter Vajda
- **Comment**: None
- **Journal**: None
- **Summary**: Computer vision has achieved remarkable success by (a) representing images as uniformly-arranged pixel arrays and (b) convolving highly-localized features. However, convolutions treat all image pixels equally regardless of importance; explicitly model all concepts across all images, regardless of content; and struggle to relate spatially-distant concepts. In this work, we challenge this paradigm by (a) representing images as semantic visual tokens and (b) running transformers to densely model token relationships. Critically, our Visual Transformer operates in a semantic token space, judiciously attending to different image parts based on context. This is in sharp contrast to pixel-space transformers that require orders-of-magnitude more compute. Using an advanced training recipe, our VTs significantly outperform their convolutional counterparts, raising ResNet accuracy on ImageNet top-1 by 4.6 to 7 points while using fewer FLOPs and parameters. For semantic segmentation on LIP and COCO-stuff, VT-based feature pyramid networks (FPN) achieve 0.35 points higher mIoU while reducing the FPN module's FLOPs by 6.5x.



### Evaluating the Disentanglement of Deep Generative Models through Manifold Topology
- **Arxiv ID**: http://arxiv.org/abs/2006.03680v5
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.03680v5)
- **Published**: 2020-06-05 20:54:11+00:00
- **Updated**: 2021-03-17 21:46:59+00:00
- **Authors**: Sharon Zhou, Eric Zelikman, Fred Lu, Andrew Y. Ng, Gunnar Carlsson, Stefano Ermon
- **Comment**: Published at ICLR 2021
- **Journal**: None
- **Summary**: Learning disentangled representations is regarded as a fundamental task for improving the generalization, robustness, and interpretability of generative models. However, measuring disentanglement has been challenging and inconsistent, often dependent on an ad-hoc external model or specific to a certain dataset. To address this, we present a method for quantifying disentanglement that only uses the generative model, by measuring the topological similarity of conditional submanifolds in the learned representation. This method showcases both unsupervised and supervised variants. To illustrate the effectiveness and applicability of our method, we empirically evaluate several state-of-the-art models across multiple datasets. We find that our method ranks models similarly to existing methods. We make ourcode publicly available at https://github.com/stanfordmlgroup/disentanglement.



### Texture Interpolation for Probing Visual Perception
- **Arxiv ID**: http://arxiv.org/abs/2006.03698v2
- **DOI**: None
- **Categories**: **q-bio.NC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.03698v2)
- **Published**: 2020-06-05 21:28:36+00:00
- **Updated**: 2020-10-22 18:05:27+00:00
- **Authors**: Jonathan Vacher, Aida Davila, Adam Kohn, Ruben Coen-Cagli
- **Comment**: Paper + ref: 12 pages and 7 figures | Supplementary: 16 pages and 16
  figures Accepted to NeurIPS 2020
- **Journal**: None
- **Summary**: Texture synthesis models are important tools for understanding visual processing. In particular, statistical approaches based on neurally relevant features have been instrumental in understanding aspects of visual perception and of neural coding. New deep learning-based approaches further improve the quality of synthetic textures. Yet, it is still unclear why deep texture synthesis performs so well, and applications of this new framework to probe visual perception are scarce. Here, we show that distributions of deep convolutional neural network (CNN) activations of a texture are well described by elliptical distributions and therefore, following optimal transport theory, constraining their mean and covariance is sufficient to generate new texture samples. Then, we propose the natural geodesics (ie the shortest path between two points) arising with the optimal transport metric to interpolate between arbitrary textures. Compared to other CNN-based approaches, our interpolation method appears to match more closely the geometry of texture perception, and our mathematical framework is better suited to study its statistical nature. We apply our method by measuring the perceptual scale associated to the interpolation parameter in human observers, and the neural sensitivity of different areas of visual cortex in macaque monkeys.



### Dilated Convolutions with Lateral Inhibitions for Semantic Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2006.03708v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.03708v5)
- **Published**: 2020-06-05 21:55:43+00:00
- **Updated**: 2022-01-19 13:12:50+00:00
- **Authors**: Yujiang Wang, Mingzhi Dong, Jie Shen, Yiming Lin, Maja Pantic
- **Comment**: Code is avaible at https://github.com/mapleandfire/LI-Convs
- **Journal**: None
- **Summary**: Dilated convolutions are widely used in deep semantic segmentation models as they can enlarge the filters' receptive field without adding additional weights nor sacrificing spatial resolution. However, as dilated convolutional filters do not possess positional knowledge about the pixels on semantically meaningful contours, they could lead to ambiguous predictions on object boundaries. In addition, although dilating the filter can expand its receptive field, the total number of sampled pixels remains unchanged, which usually comprises a small fraction of the receptive field's total area. Inspired by the Lateral Inhibition (LI) mechanisms in human visual systems, we propose the dilated convolution with lateral inhibitions (LI-Convs) to overcome these limitations. Introducing LI mechanisms improves the convolutional filter's sensitivity to semantic object boundaries. Moreover, since LI-Convs also implicitly take the pixels from the laterally inhibited zones into consideration, they can also extract features at a denser scale. By integrating LI-Convs into the Deeplabv3+ architecture, we propose the Lateral Inhibited Atrous Spatial Pyramid Pooling (LI-ASPP), the Lateral Inhibited MobileNet-V2 (LI-MNV2) and the Lateral Inhibited ResNet (LI-ResNet). Experimental results on three benchmark datasets (PASCAL VOC 2012, CelebAMask-HQ and ADE20K) show that our LI-based segmentation models outperform the baseline on all of them, thus verify the effectiveness and generality of the proposed LI-Convs.



### Applied Awareness: Test-Driven GUI Development using Computer Vision and Cryptography
- **Arxiv ID**: http://arxiv.org/abs/2006.03725v1
- **DOI**: None
- **Categories**: **cs.SE**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.03725v1)
- **Published**: 2020-06-05 22:46:48+00:00
- **Updated**: 2020-06-05 22:46:48+00:00
- **Authors**: Donald Beaver
- **Comment**: 8 pages. Submitted for conference publication
- **Journal**: None
- **Summary**: Graphical user interface testing is significantly challenging, and automating it even more so. Test-driven development is impractical: it generally requires an initial implementation of the GUI to generate golden images or to construct interactive test scenarios, and subsequent maintenance is costly. While computer vision has been applied to several aspects of GUI testing, we demonstrate a novel and immediately applicable approach of interpreting GUI presentation in terms of backend communications, modeling "awareness" in the fashion employed by cryptographic proofs of security. This focus on backend communication circumvents deficiencies in typical testing methodologies that rely on platform-dependent UI affordances or accessibility features. Our interdisciplinary work is ready for off-the-shelf practice: we report self-contained, practical implementation with both online and offline validation, using simple designer specifications at the outset and specifically avoiding any requirements for a bootstrap implementation or golden images. In addition to practical implementation, ties to formal verification methods in cryptography are explored and explained, providing fertile perspectives on assurance in UI and interpretability in AI.



### WOAD: Weakly Supervised Online Action Detection in Untrimmed Videos
- **Arxiv ID**: http://arxiv.org/abs/2006.03732v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.03732v2)
- **Published**: 2020-06-05 23:08:41+00:00
- **Updated**: 2021-05-18 18:19:25+00:00
- **Authors**: Mingfei Gao, Yingbo Zhou, Ran Xu, Richard Socher, Caiming Xiong
- **Comment**: CVPR2021
- **Journal**: None
- **Summary**: Online action detection in untrimmed videos aims to identify an action as it happens, which makes it very important for real-time applications. Previous methods rely on tedious annotations of temporal action boundaries for training, which hinders the scalability of online action detection systems. We propose WOAD, a weakly supervised framework that can be trained using only video-class labels. WOAD contains two jointly-trained modules, i.e., temporal proposal generator (TPG) and online action recognizer (OAR). Supervised by video-class labels, TPG works offline and targets at accurately mining pseudo frame-level labels for OAR. With the supervisory signals from TPG, OAR learns to conduct action detection in an online fashion. Experimental results on THUMOS'14, ActivityNet1.2 and ActivityNet1.3 show that our weakly-supervised method largely outperforms weakly-supervised baselines and achieves comparable performance to the previous strongly-supervised methods. Beyond that, WOAD is flexible to leverage strong supervision when it is available. When strongly supervised, our method obtains the state-of-the-art results in the tasks of both online per-frame action recognition and online detection of action start.



### Unsupervised Abnormality Detection Using Heterogeneous Autonomous Systems
- **Arxiv ID**: http://arxiv.org/abs/2006.03733v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.03733v2)
- **Published**: 2020-06-05 23:09:58+00:00
- **Updated**: 2020-07-14 16:34:23+00:00
- **Authors**: Sayeed Shafayet Chowdhury, Kazi Mejbaul Islam, Rouhan Noor
- **Comment**: None
- **Journal**: None
- **Summary**: Anomaly detection (AD) in a surveillance scenario is an emerging and challenging field of research. For autonomous vehicles like drones or cars, it is immensely important to distinguish between normal and abnormal states in real-time. Additionally, we also need to detect any device malfunction. But the nature and degree of abnormality may vary depending upon the actual environment and adversary. As a result, it is impractical to model all cases a-priori and use supervised methods to classify. Also, an autonomous vehicle provides various data types like images and other analog or digital sensor data, all of which can be useful in anomaly detection if leveraged fruitfully. To that effect, in this paper, a heterogeneous system is proposed which estimates the degree of abnormality of an unmanned surveillance drone, analyzing real-time image and IMU (Inertial Measurement Unit) sensor data in an unsupervised manner. Here, we have demonstrated a Convolutional Neural Network (CNN) architecture, named AngleNet to estimate the angle between a normal image and another image under consideration, which provides us with a measure of anomaly of the device. Moreover, the IMU data are used in autoencoder to predict abnormality. Finally, the results from these two algorithms are ensembled to estimate the final degree of abnormality. The proposed method performs satisfactorily on the IEEE SP Cup-2020 dataset with an accuracy of 97.3%. Additionally, we have also tested this approach on an in-house dataset to validate its robustness.



