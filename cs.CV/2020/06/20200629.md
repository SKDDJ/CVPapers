# Arxiv Papers in cs.CV on 2020-06-29
### Confidence-rich grid mapping
- **Arxiv ID**: http://arxiv.org/abs/2006.15754v1
- **DOI**: 10.1177/0278364919839762
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.15754v1)
- **Published**: 2020-06-29 00:21:30+00:00
- **Updated**: 2020-06-29 00:21:30+00:00
- **Authors**: Ali-akbar Agha-mohammadi, Eric Heiden, Karol Hausman, Gaurav S. Sukhatme
- **Comment**: Published at International Journal of Robotics Research (IJRR) 2019
  (https://journals.sagepub.com/doi/10.1177/0278364919839762)
- **Journal**: The International Journal of Robotics Research, 38(12-13),
  1352-1374 (2019)
- **Summary**: Representing the environment is a fundamental task in enabling robots to act autonomously in unknown environments. In this work, we present confidence-rich mapping (CRM), a new algorithm for spatial grid-based mapping of the 3D environment. CRM augments the occupancy level at each voxel by its confidence value. By explicitly storing and evolving confidence values using the CRM filter, CRM extends traditional grid mapping in three ways: first, it partially maintains the probabilistic dependence among voxels. Second, it relaxes the need for hand-engineering an inverse sensor model and proposes the concept of sensor cause model that can be derived in a principled manner from the forward sensor model. Third, and most importantly, it provides consistent confidence values over the occupancy estimation that can be reliably used in collision risk evaluation and motion planning. CRM runs online and enables mapping environments where voxels might be partially occupied. We demonstrate the performance of the method on various datasets and environments in simulation and on physical systems. We show in real-world experiments that, in addition to achieving maps that are more accurate than traditional methods, the proposed filtering scheme demonstrates a much higher level of consistency between its error and the reported confidence, hence, enabling a more reliable collision risk evaluation for motion planning.



### EmotionNet Nano: An Efficient Deep Convolutional Neural Network Design for Real-time Facial Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/2006.15759v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.15759v1)
- **Published**: 2020-06-29 00:48:05+00:00
- **Updated**: 2020-06-29 00:48:05+00:00
- **Authors**: James Ren Hou Lee, Linda Wang, Alexander Wong
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: While recent advances in deep learning have led to significant improvements in facial expression classification (FEC), a major challenge that remains a bottleneck for the widespread deployment of such systems is their high architectural and computational complexities. This is especially challenging given the operational requirements of various FEC applications, such as safety, marketing, learning, and assistive living, where real-time requirements on low-cost embedded devices is desired. Motivated by this need for a compact, low latency, yet accurate system capable of performing FEC in real-time on low-cost embedded devices, this study proposes EmotionNet Nano, an efficient deep convolutional neural network created through a human-machine collaborative design strategy, where human experience is combined with machine meticulousness and speed in order to craft a deep neural network design catered towards real-time embedded usage. Two different variants of EmotionNet Nano are presented, each with a different trade-off between architectural and computational complexity and accuracy. Experimental results using the CK+ facial expression benchmark dataset demonstrate that the proposed EmotionNet Nano networks demonstrated accuracies comparable to state-of-the-art in FEC networks, while requiring significantly fewer parameters (e.g., 23$\times$ fewer at a higher accuracy). Furthermore, we demonstrate that the proposed EmotionNet Nano networks achieved real-time inference speeds (e.g. $>25$ FPS and $>70$ FPS at 15W and 30W, respectively) and high energy efficiency (e.g. $>1.7$ images/sec/watt at 15W) on an ARM embedded processor, thus further illustrating the efficacy of EmotionNet Nano for deployment on embedded devices.



### Active Ensemble Deep Learning for Polarimetric Synthetic Aperture Radar Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2006.15771v1
- **DOI**: 10.1109/LGRS.2020.3005076
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.15771v1)
- **Published**: 2020-06-29 01:40:54+00:00
- **Updated**: 2020-06-29 01:40:54+00:00
- **Authors**: Sheng-Jie Liu, Haowen Luo, Qian Shi
- **Comment**: Accepted by GRSL
- **Journal**: None
- **Summary**: Although deep learning has achieved great success in image classification tasks, its performance is subject to the quantity and quality of training samples. For classification of polarimetric synthetic aperture radar (PolSAR) images, it is nearly impossible to annotate the images from visual interpretation. Therefore, it is urgent for remote sensing scientists to develop new techniques for PolSAR image classification under the condition of very few training samples. In this letter, we take the advantage of active learning and propose active ensemble deep learning (AEDL) for PolSAR image classification. We first show that only 35\% of the predicted labels of a deep learning model's snapshots near its convergence were exactly the same. The disagreement between snapshots is non-negligible. From the perspective of multiview learning, the snapshots together serve as a good committee to evaluate the importance of unlabeled instances. Using the snapshots committee to give out the informativeness of unlabeled data, the proposed AEDL achieved better performance on two real PolSAR images compared with standard active learning strategies. It achieved the same classification accuracy with only 86% and 55% of the training samples compared with breaking ties active learning and random selection for the Flevoland dataset.



### A Benchmark dataset for both underwater image enhancement and underwater object detection
- **Arxiv ID**: http://arxiv.org/abs/2006.15789v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.15789v1)
- **Published**: 2020-06-29 03:12:50+00:00
- **Updated**: 2020-06-29 03:12:50+00:00
- **Authors**: Long Chen, Lei Tong, Feixiang Zhou, Zheheng Jiang, Zhenyang Li, Jialin Lv, Junyu Dong, Huiyu Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: Underwater image enhancement is such an important vision task due to its significance in marine engineering and aquatic robot. It is usually work as a pre-processing step to improve the performance of high level vision tasks such as underwater object detection. Even though many previous works show the underwater image enhancement algorithms can boost the detection accuracy of the detectors, no work specially focus on investigating the relationship between these two tasks. This is mainly because existing underwater datasets lack either bounding box annotations or high quality reference images, based on which detection accuracy or image quality assessment metrics are calculated. To investigate how the underwater image enhancement methods influence the following underwater object detection tasks, in this paper, we provide a large-scale underwater object detection dataset with both bounding box annotations and high quality reference images, namely OUC dataset. The OUC dataset provides a platform for researchers to comprehensive study the influence of underwater image enhancement algorithms on the underwater object detection task.



### End-to-End Differentiable Learning to HDR Image Synthesis for Multi-exposure Images
- **Arxiv ID**: http://arxiv.org/abs/2006.15833v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.15833v2)
- **Published**: 2020-06-29 06:47:07+00:00
- **Updated**: 2020-12-18 07:19:13+00:00
- **Authors**: Jung Hee Kim, Siyeong Lee, Suk-Ju Kang
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, high dynamic range (HDR) image reconstruction based on the multiple exposure stack from a given single exposure utilizes a deep learning framework to generate high-quality HDR images. These conventional networks focus on the exposure transfer task to reconstruct the multi-exposure stack. Therefore, they often fail to fuse the multi-exposure stack into a perceptually pleasant HDR image as the inversion artifacts occur. We tackle the problem in stack reconstruction-based methods by proposing a novel framework with a fully differentiable high dynamic range imaging (HDRI) process. By explicitly using the loss, which compares the network's output with the ground truth HDR image, our framework enables a neural network that generates the multiple exposure stack for HDRI to train stably. In other words, our differentiable HDR synthesis layer helps the deep neural network to train to create multi-exposure stacks while reflecting the precise correlations between multi-exposure images in the HDRI process. In addition, our network uses the image decomposition and the recursive process to facilitate the exposure transfer task and to adaptively respond to recursion frequency. The experimental results show that the proposed network outperforms the state-of-the-art quantitative and qualitative results in terms of both the exposure transfer tasks and the whole HDRI process.



### Deep Doubly Supervised Transfer Network for Diagnosis of Breast Cancer with Imbalanced Ultrasound Imaging Modalities
- **Arxiv ID**: http://arxiv.org/abs/2007.06634v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.06634v1)
- **Published**: 2020-06-29 07:32:07+00:00
- **Updated**: 2020-06-29 07:32:07+00:00
- **Authors**: Han Xiangmin, Wang Jun, Zhou Weijun, Chang Cai, Ying Shihui, Shi Jun
- **Comment**: Accepted by MICCAI 2020
- **Journal**: None
- **Summary**: Elastography ultrasound (EUS) provides additional bio-mechanical in-formation about lesion for B-mode ultrasound (BUS) in the diagnosis of breast cancers. However, joint utilization of both BUS and EUS is not popular due to the lack of EUS devices in rural hospitals, which arouses a novel modality im-balance problem in computer-aided diagnosis (CAD) for breast cancers. Current transfer learning (TL) pay little attention to this special issue of clinical modality imbalance, that is, the source domain (EUS modality) has fewer labeled samples than those in the target domain (BUS modality). Moreover, these TL methods cannot fully use the label information to explore the intrinsic relation between two modalities and then guide the promoted knowledge transfer. To this end, we propose a novel doubly supervised TL network (DDSTN) that integrates the Learning Using Privileged Information (LUPI) paradigm and the Maximum Mean Discrepancy (MMD) criterion into a unified deep TL framework. The proposed algorithm can not only make full use of the shared labels to effectively guide knowledge transfer by LUPI paradigm, but also perform additional super-vised transfer between unpaired data. We further introduce the MMD criterion to enhance the knowledge transfer. The experimental results on the breast ultra-sound dataset indicate that the proposed DDSTN outperforms all the compared state-of-the-art algorithms for the BUS-based CAD.



### OpenDVC: An Open Source Implementation of the DVC Video Compression Method
- **Arxiv ID**: http://arxiv.org/abs/2006.15862v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.15862v2)
- **Published**: 2020-06-29 08:22:08+00:00
- **Updated**: 2020-08-03 18:45:14+00:00
- **Authors**: Ren Yang, Luc Van Gool, Radu Timofte
- **Comment**: Technical report of OpenDVC; the project page is at
  https://github.com/RenYang-home/OpenDVC
- **Journal**: None
- **Summary**: We introduce an open source Tensorflow implementation of the Deep Video Compression (DVC) method in this technical report. DVC is the first end-to-end optimized learned video compression method, achieving better MS-SSIM performance than the Low-Delay P (LDP) very fast setting of x265 and comparable PSNR performance with x265 (LDP very fast). At the time of writing this report, several learned video compression methods are superior to DVC, but currently none of them provides open source codes. We hope that our OpenDVC codes are able to provide a useful model for further development, and facilitate future researches on learned video compression. Different from the original DVC, which is only optimized for PSNR, we release not only the PSNR-optimized re-implementation, denoted by OpenDVC (PSNR), but also the MS-SSIM-optimized model OpenDVC (MS-SSIM). Our OpenDVC (MS-SSIM) model provides a more convincing baseline for MS-SSIM optimized methods, which can only compare with the PSNR optimized DVC in the past. The OpenDVC source codes and pre-trained models are publicly released at https://github.com/RenYang-home/OpenDVC.



### Abnormal activity capture from passenger flow of elevator based on unsupervised learning and fine-grained multi-label recognition
- **Arxiv ID**: http://arxiv.org/abs/2006.15873v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.15873v1)
- **Published**: 2020-06-29 08:50:20+00:00
- **Updated**: 2020-06-29 08:50:20+00:00
- **Authors**: Chunhua Jia, Wenhai Yi, Yu Wu, Hui Huang, Lei Zhang, Leilei Wu
- **Comment**: 9 pages, 8 figures, submitted to 34th Conference on Neural
  Information Processing System(NeurIPS 2020)
- **Journal**: None
- **Summary**: We present a work-flow which aims at capturing residents' abnormal activities through the passenger flow of elevator in multi-storey residence buildings. Camera and sensors (hall sensor, photoelectric sensor, gyro, accelerometer, barometer, and thermometer) with internet connection are mounted in elevator to collect image and data. Computer vision algorithms such as instance segmentation, multi-label recognition, embedding and clustering are applied to generalize passenger flow of elevator, i.e. how many people and what kinds of people get in and out of the elevator on each floor. More specifically in our implementation we propose GraftNet, a solution for fine-grained multi-label recognition task, to recognize human attributes, e.g. gender, age, appearance, and occupation. Then anomaly detection of unsupervised learning is hierarchically applied on the passenger flow data to capture abnormal or even illegal activities of the residents which probably bring safety hazard, e.g. drug dealing, pyramid sale gathering, prostitution, and over crowded residence. Experiment shows effects are there, and the captured records will be directly reported to our customer(property managers) for further confirmation.



### Improving Few-Shot Learning using Composite Rotation based Auxiliary Task
- **Arxiv ID**: http://arxiv.org/abs/2006.15919v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.15919v2)
- **Published**: 2020-06-29 10:21:35+00:00
- **Updated**: 2020-11-22 17:39:51+00:00
- **Authors**: Pratik Mazumder, Pravendra Singh, Vinay P. Namboodiri
- **Comment**: Accepted in WACV 2021
- **Journal**: None
- **Summary**: In this paper, we propose an approach to improve few-shot classification performance using a composite rotation based auxiliary task. Few-shot classification methods aim to produce neural networks that perform well for classes with a large number of training samples and classes with less number of training samples. They employ techniques to enable the network to produce highly discriminative features that are also very generic. Generally, the better the quality and generic-nature of the features produced by the network, the better is the performance of the network on few-shot learning. Our approach aims to train networks to produce such features by using a self-supervised auxiliary task. Our proposed composite rotation based auxiliary task performs rotation at two levels, i.e., rotation of patches inside the image (inner rotation) and rotation of the whole image (outer rotation) and assigns one out of 16 rotation classes to the modified image. We then simultaneously train for the composite rotation prediction task along with the original classification task, which forces the network to learn high-quality generic features that help improve the few-shot classification performance. We experimentally show that our approach performs better than existing few-shot learning methods on multiple benchmark datasets.



### Interpreting and Disentangling Feature Components of Various Complexity from DNNs
- **Arxiv ID**: http://arxiv.org/abs/2006.15920v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.15920v1)
- **Published**: 2020-06-29 10:24:27+00:00
- **Updated**: 2020-06-29 10:24:27+00:00
- **Authors**: Jie Ren, Mingjie Li, Zexu Liu, Quanshi Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: This paper aims to define, quantify, and analyze the feature complexity that is learned by a DNN. We propose a generic definition for the feature complexity. Given the feature of a certain layer in the DNN, our method disentangles feature components of different complexity orders from the feature. We further design a set of metrics to evaluate the reliability, the effectiveness, and the significance of over-fitting of these feature components. Furthermore, we successfully discover a close relationship between the feature complexity and the performance of DNNs. As a generic mathematical tool, the feature complexity and the proposed metrics can also be used to analyze the success of network compression and knowledge distillation.



### Hybrid Tensor Decomposition in Neural Network Compression
- **Arxiv ID**: http://arxiv.org/abs/2006.15938v3
- **DOI**: 10.1016/j.neunet.2020.09.006
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.15938v3)
- **Published**: 2020-06-29 11:16:22+00:00
- **Updated**: 2020-09-21 02:14:21+00:00
- **Authors**: Bijiao Wu, Dingheng Wang, Guangshe Zhao, Lei Deng, Guoqi Li
- **Comment**: submitted to <<Neural Networks>> on Apr.18,2020; accepted on
  Sep.08,2020
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) have enabled impressive breakthroughs in various artificial intelligence (AI) applications recently due to its capability of learning high-level features from big data. However, the current demand of DNNs for computational resources especially the storage consumption is growing due to that the increasing sizes of models are being required for more and more complicated applications. To address this problem, several tensor decomposition methods including tensor-train (TT) and tensor-ring (TR) have been applied to compress DNNs and shown considerable compression effectiveness. In this work, we introduce the hierarchical Tucker (HT), a classical but rarely-used tensor decomposition method, to investigate its capability in neural network compression. We convert the weight matrices and convolutional kernels to both HT and TT formats for comparative study, since the latter is the most widely used decomposition method and the variant of HT. We further theoretically and experimentally discover that the HT format has better performance on compressing weight matrices, while the TT format is more suited for compressing convolutional kernels. Based on this phenomenon we propose a strategy of hybrid tensor decomposition by combining TT and HT together to compress convolutional and fully connected parts separately and attain better accuracy than only using the TT or HT format on convolutional neural networks (CNNs). Our work illuminates the prospects of hybrid tensor decomposition for neural network compression.



### Adversarial Multi-Source Transfer Learning in Healthcare: Application to Glucose Prediction for Diabetic People
- **Arxiv ID**: http://arxiv.org/abs/2006.15940v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.15940v1)
- **Published**: 2020-06-29 11:17:50+00:00
- **Updated**: 2020-06-29 11:17:50+00:00
- **Authors**: Maxime De Bois, Mounîm A. El Yacoubi, Mehdi Ammi
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning has yet to revolutionize general practices in healthcare, despite promising results for some specific tasks. This is partly due to data being in insufficient quantities hurting the training of the models. To address this issue, data from multiple health actors or patients could be combined by capitalizing on their heterogeneity through the use of transfer learning.   To improve the quality of the transfer between multiple sources of data, we propose a multi-source adversarial transfer learning framework that enables the learning of a feature representation that is similar across the sources, and thus more general and more easily transferable. We apply this idea to glucose forecasting for diabetic people using a fully convolutional neural network. The evaluation is done by exploring various transfer scenarios with three datasets characterized by their high inter and intra variability.   While transferring knowledge is beneficial in general, we show that the statistical and clinical accuracies can be further improved by using of the adversarial training methodology, surpassing the current state-of-the-art results. In particular, it shines when using data from different datasets, or when there is too little data in an intra-dataset situation. To understand the behavior of the models, we analyze the learnt feature representations and propose a new metric in this regard. Contrary to a standard transfer, the adversarial transfer does not discriminate the patients and datasets, helping the learning of a more general feature representation.   The adversarial training framework improves the learning of a general feature representation in a multi-source environment, enhancing the knowledge transfer to an unseen target.   The proposed method can help improve the efficiency of data shared by different health actors in the training of deep models.



### Multi-level colonoscopy malignant tissue detection with adversarial CAC-UNet
- **Arxiv ID**: http://arxiv.org/abs/2006.15954v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.15954v2)
- **Published**: 2020-06-29 11:49:58+00:00
- **Updated**: 2020-06-30 13:43:25+00:00
- **Authors**: Chuang Zhu, Ke Mei, Ting Peng, Yihao Luo, Jun Liu, Ying Wang, Mulan Jin
- **Comment**: accepted by Neurocomputing; winner of the MICCAI DigestPath 2019
  challenge on colonoscopy tissue segmentation and classification task
- **Journal**: None
- **Summary**: The automatic and objective medical diagnostic model can be valuable to achieve early cancer detection, and thus reducing the mortality rate. In this paper, we propose a highly efficient multi-level malignant tissue detection through the designed adversarial CAC-UNet. A patch-level model with a pre-prediction strategy and a malignancy area guided label smoothing is adopted to remove the negative WSIs, with which to lower the risk of false positive detection. For the selected key patches by multi-model ensemble, an adversarial context-aware and appearance consistency UNet (CAC-UNet) is designed to achieve robust segmentation. In CAC-UNet, mirror designed discriminators are able to seamlessly fuse the whole feature maps of the skillfully designed powerful backbone network without any information loss. Besides, a mask prior is further added to guide the accurate segmentation mask prediction through an extra mask-domain discriminator. The proposed scheme achieves the best results in MICCAI DigestPath2019 challenge on colonoscopy tissue segmentation and classification task. The full implementation details and the trained models are available at https://github.com/Raykoooo/CAC-UNet.



### Few-Shot Microscopy Image Cell Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2007.01671v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.01671v1)
- **Published**: 2020-06-29 12:12:10+00:00
- **Updated**: 2020-06-29 12:12:10+00:00
- **Authors**: Youssef Dawoud, Julia Hornauer, Gustavo Carneiro, Vasileios Belagiannis
- **Comment**: 16 pages, 4 figures, Accepted by ECML-PKDD 2020 conference
- **Journal**: None
- **Summary**: Automatic cell segmentation in microscopy images works well with the support of deep neural networks trained with full supervision. Collecting and annotating images, though, is not a sustainable solution for every new microscopy database and cell type. Instead, we assume that we can access a plethora of annotated image data sets from different domains (sources) and a limited number of annotated image data sets from the domain of interest (target), where each domain denotes not only different image appearance but also a different type of cell segmentation problem. We pose this problem as meta-learning where the goal is to learn a generic and adaptable few-shot learning model from the available source domain data sets and cell segmentation tasks. The model can be afterwards fine-tuned on the few annotated images of the target domain that contains different image appearance and different cell type. In our meta-learning training, we propose the combination of three objective functions to segment the cells, move the segmentation results away from the classification boundary using cross-domain tasks, and learn an invariant representation between tasks of the source domains. Our experiments on five public databases show promising results from 1- to 10-shot meta-learning using standard segmentation neural network architectures.



### Explainable 3D Convolutional Neural Networks by Learning Temporal Transformations
- **Arxiv ID**: http://arxiv.org/abs/2006.15983v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.15983v1)
- **Published**: 2020-06-29 12:29:30+00:00
- **Updated**: 2020-06-29 12:29:30+00:00
- **Authors**: Gabriëlle Ras, Luca Ambrogioni, Pim Haselager, Marcel A. J. van Gerven, Umut Güçlü
- **Comment**: 10 pages, 5 figures, 4 tables
- **Journal**: None
- **Summary**: In this paper we introduce the temporally factorized 3D convolution (3TConv) as an interpretable alternative to the regular 3D convolution (3DConv). In a 3TConv the 3D convolutional filter is obtained by learning a 2D filter and a set of temporal transformation parameters, resulting in a sparse filter where the 2D slices are sequentially dependent on each other in the temporal dimension. We demonstrate that 3TConv learns temporal transformations that afford a direct interpretation. The temporal parameters can be used in combination with various existing 2D visualization methods. We also show that insight about what the model learns can be achieved by analyzing the transformation parameter statistics on a layer and model level. Finally, we implicitly demonstrate that, in popular ConvNets, the 2DConv can be replaced with a 3TConv and that the weights can be transferred to yield pretrained 3TConvs. pretrained 3TConvnets leverage more than a decade of work on traditional 2DConvNets by being able to make use of features that have been proven to deliver excellent results on image classification benchmarks.



### MoNet3D: Towards Accurate Monocular 3D Object Localization in Real Time
- **Arxiv ID**: http://arxiv.org/abs/2006.16007v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.16007v1)
- **Published**: 2020-06-29 12:48:57+00:00
- **Updated**: 2020-06-29 12:48:57+00:00
- **Authors**: Xichuan Zhou, Yicong Peng, Chunqiao Long, Fengbo Ren, Cong Shi
- **Comment**: None
- **Journal**: None
- **Summary**: Monocular multi-object detection and localization in 3D space has been proven to be a challenging task. The MoNet3D algorithm is a novel and effective framework that can predict the 3D position of each object in a monocular image and draw a 3D bounding box for each object. The MoNet3D method incorporates prior knowledge of the spatial geometric correlation of neighbouring objects into the deep neural network training process to improve the accuracy of 3D object localization. Experiments on the KITTI dataset show that the accuracy for predicting the depth and horizontal coordinates of objects in 3D space can reach 96.25\% and 94.74\%, respectively. Moreover, the method can realize the real-time image processing at 27.85 FPS, showing promising potential for embedded advanced driving-assistance system applications. Our code is publicly available at https://github.com/CQUlearningsystemgroup/YicongPeng.



### Intrinsic Autoencoders for Joint Neural Rendering and Intrinsic Image Decomposition
- **Arxiv ID**: http://arxiv.org/abs/2006.16011v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2006.16011v3)
- **Published**: 2020-06-29 12:53:58+00:00
- **Updated**: 2021-03-29 10:27:41+00:00
- **Authors**: Hassan Abu Alhaija, Siva Karthik Mustikovela, Justus Thies, Varun Jampani, Matthias Nießner, Andreas Geiger, Carsten Rother
- **Comment**: None
- **Journal**: None
- **Summary**: Neural rendering techniques promise efficient photo-realistic image synthesis while at the same time providing rich control over scene parameters by learning the physical image formation process. While several supervised methods have been proposed for this task, acquiring a dataset of images with accurately aligned 3D models is very difficult. The main contribution of this work is to lift this restriction by training a neural rendering algorithm from unpaired data. More specifically, we propose an autoencoder for joint generation of realistic images from synthetic 3D models while simultaneously decomposing real images into their intrinsic shape and appearance properties. In contrast to a traditional graphics pipeline, our approach does not require to specify all scene properties, such as material parameters and lighting by hand. Instead, we learn photo-realistic deferred rendering from a small set of 3D models and a larger set of unaligned real images, both of which are easy to acquire in practice. Simultaneously, we obtain accurate intrinsic decompositions of real images while not requiring paired ground truth. Our experiments confirm that a joint treatment of rendering and decomposition is indeed beneficial and that our approach outperforms state-of-the-art image-to-image translation baselines both qualitatively and quantitatively.



### Creating Artificial Modalities to Solve RGB Liveness
- **Arxiv ID**: http://arxiv.org/abs/2006.16028v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.16028v1)
- **Published**: 2020-06-29 13:19:22+00:00
- **Updated**: 2020-06-29 13:19:22+00:00
- **Authors**: Aleksandr Parkin, Oleg Grinchuk
- **Comment**: CVPRW2020
- **Journal**: None
- **Summary**: Special cameras that provide useful features for face anti-spoofing are desirable, but not always an option. In this work we propose a method to utilize the difference in dynamic appearance between bona fide and spoof samples by creating artificial modalities from RGB videos. We introduce two types of artificial transforms: rank pooling and optical flow, combined in end-to-end pipeline for spoof detection. We demonstrate that using intermediate representations that contain less identity and fine-grained features increase model robustness to unseen attacks as well as to unseen ethnicities. The proposed method achieves state-of-the-art on the largest cross-ethnicity face anti-spoofing dataset CASIA-SURF CeFA (RGB).



### Survey on the Analysis and Modeling of Visual Kinship: A Decade in the Making
- **Arxiv ID**: http://arxiv.org/abs/2006.16033v4
- **DOI**: 10.1109/tpami.2021.3063078
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.16033v4)
- **Published**: 2020-06-29 13:25:45+00:00
- **Updated**: 2021-02-24 04:04:20+00:00
- **Authors**: Joseph P Robinson, Ming Shao, Yun Fu
- **Comment**: None
- **Journal**: IEEE Transactions on pattern analysis and machine intelligence
  (2021)
- **Summary**: Kinship recognition is a challenging problem with many practical applications. With much progress and milestones having been reached after ten years - we are now able to survey the research and create new milestones. We review the public resources and data challenges that enabled and inspired many to hone-in on the views of automatic kinship recognition in the visual domain. The different tasks are described in technical terms and syntax consistent across the problem domain and the practical value of each discussed and measured. State-of-the-art methods for visual kinship recognition problems, whether to discriminate between or generate from, are examined. As part of such, we review systems proposed as part of a recent data challenge held in conjunction with the 2020 IEEE Conference on Automatic Face and Gesture Recognition. We establish a stronghold for the state of progress for the different problems in a consistent manner. This survey will serve as the central resource for the work of the next decade to build upon. For the tenth anniversary, the demo code is provided for the various kin-based tasks. Detecting relatives with visual recognition and classifying the relationship is an area with high potential for impact in research and practice.IEEE Transactions on pattern analysis and machine intelligence



### Forgery Detection in a Questioned Hyperspectral Document Image using K-means Clustering
- **Arxiv ID**: http://arxiv.org/abs/2006.16057v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/2006.16057v1)
- **Published**: 2020-06-29 13:51:24+00:00
- **Updated**: 2020-06-29 13:51:24+00:00
- **Authors**: Maria Yaseen, Rammal Aftab Ahmed, Rimsha Mahrukh
- **Comment**: 5 pages,6 figures
- **Journal**: None
- **Summary**: Hyperspectral imaging allows for analysis of images in several hundred of spectral bands depending on the spectral resolution of the imaging sensor. Hyperspectral document image is the one which has been captured by a hyperspectral camera so that the document can be observed in the different bands on the basis of their unique spectral signatures. To detect the forgery in a document various Ink mismatch detection techniques based on hyperspectral imaging have presented vast potential in differentiating visually similar inks. Inks of different materials exhibit different spectral signature even if they have the same color. Hyperspectral analysis of document images allows identification and discrimination of visually similar inks. Based on this analysis forensic experts can identify the authenticity of the document. In this paper an extensive ink mismatch detection technique is presented which uses KMean Clustering to identify different inks on the basis of their unique spectral response and separates them into different clusters.



### Unsupervised Landmark Learning from Unpaired Data
- **Arxiv ID**: http://arxiv.org/abs/2007.01053v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.01053v1)
- **Published**: 2020-06-29 13:57:20+00:00
- **Updated**: 2020-06-29 13:57:20+00:00
- **Authors**: Yinghao Xu, Ceyuan Yang, Ziwei Liu, Bo Dai, Bolei Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: Recent attempts for unsupervised landmark learning leverage synthesized image pairs that are similar in appearance but different in poses. These methods learn landmarks by encouraging the consistency between the original images and the images reconstructed from swapped appearances and poses. While synthesized image pairs are created by applying pre-defined transformations, they can not fully reflect the real variances in both appearances and poses. In this paper, we aim to open the possibility of learning landmarks on unpaired data (i.e. unaligned image pairs) sampled from a natural image collection, so that they can be different in both appearances and poses. To this end, we propose a cross-image cycle consistency framework ($C^3$) which applies the swapping-reconstruction strategy twice to obtain the final supervision. Moreover, a cross-image flow module is further introduced to impose the equivariance between estimated landmarks across images. Through comprehensive experiments, our proposed framework is shown to outperform strong baselines by a large margin. Besides quantitative results, we also provide visualization and interpretation on our learned models, which not only verifies the effectiveness of the learned landmarks, but also leads to important insights that are beneficial for future research.



### Patch SVDD: Patch-level SVDD for Anomaly Detection and Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2006.16067v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.16067v2)
- **Published**: 2020-06-29 14:19:47+00:00
- **Updated**: 2020-07-13 10:26:14+00:00
- **Authors**: Jihun Yi, Sungroh Yoon
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we address the problem of image anomaly detection and segmentation. Anomaly detection involves making a binary decision as to whether an input image contains an anomaly, and anomaly segmentation aims to locate the anomaly on the pixel level. Support vector data description (SVDD) is a long-standing algorithm used for an anomaly detection, and we extend its deep learning variant to the patch-based method using self-supervised learning. This extension enables anomaly segmentation and improves detection performance. As a result, anomaly detection and segmentation performances measured in AUROC on MVTec AD dataset increased by 9.8% and 7.0%, respectively, compared to the previous state-of-the-art methods. Our results indicate the efficacy of the proposed method and its potential for industrial application. Detailed analysis of the proposed method offers insights regarding its behavior, and the code is available online.



### Level Set Stereo for Cooperative Grouping with Occlusion
- **Arxiv ID**: http://arxiv.org/abs/2006.16094v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.16094v3)
- **Published**: 2020-06-29 14:51:08+00:00
- **Updated**: 2021-06-18 05:16:35+00:00
- **Authors**: Jialiang Wang, Todd Zickler
- **Comment**: ICIP 2021 Code and data: https://github.com/jialiangw/levelsetstereo
- **Journal**: None
- **Summary**: Localizing stereo boundaries is difficult because matching cues are absent in the occluded regions that are adjacent to them. We introduce an energy and level-set optimizer that improves boundaries by encoding the essential geometry of occlusions: The spatial extent of an occlusion must equal the amplitude of the disparity jump that causes it. In a collection of figure-ground scenes from Middlebury and Falling Things stereo datasets, the model provides more accurate boundaries than previous occlusion-handling techniques.



### Iris Recognition: Inherent Binomial Degrees of Freedom
- **Arxiv ID**: http://arxiv.org/abs/2006.16107v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.16107v1)
- **Published**: 2020-06-29 15:15:49+00:00
- **Updated**: 2020-06-29 15:15:49+00:00
- **Authors**: J. Michael Rozmus
- **Comment**: 4 pages, 6 figures
- **Journal**: None
- **Summary**: The distinctiveness of the human iris has been measured by first extracting a set of features from the iris, an encoding, and then comparing these encoded feature sets to determine how distinct they are from one another. For example, John Daugman measures the distinctiveness of the human iris at 244 degrees of freedom, that is, Daugman's encoding maps irises into the equivalent of 2 ^ 244 distinct possibilities [2]. This paper shows by direct pixel-by-pixel comparison of high-quality iris images that the inherent number of degrees of freedom embodied in the human iris, independent of any encoding, is at least 536. When the resolution of these images is gradually reduced, the number of degrees of freedom decreases smoothly to 123 for the lowest resolution images tested.



### GramGAN: Deep 3D Texture Synthesis From 2D Exemplars
- **Arxiv ID**: http://arxiv.org/abs/2006.16112v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2006.16112v2)
- **Published**: 2020-06-29 15:22:03+00:00
- **Updated**: 2020-06-30 10:33:59+00:00
- **Authors**: Tiziano Portenier, Siavash Bigdeli, Orcun Goksel
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel texture synthesis framework, enabling the generation of infinite, high-quality 3D textures given a 2D exemplar image. Inspired by recent advances in natural texture synthesis, we train deep neural models to generate textures by non-linearly combining learned noise frequencies. To achieve a highly realistic output conditioned on an exemplar patch, we propose a novel loss function that combines ideas from both style transfer and generative adversarial networks. In particular, we train the synthesis network to match the Gram matrices of deep features from a discriminator network. In addition, we propose two architectural concepts and an extrapolation strategy that significantly improve generalization performance. In particular, we inject both model input and condition into hidden network layers by learning to scale and bias hidden activations. Quantitative and qualitative evaluations on a diverse set of exemplars motivate our design decisions and show that our system performs superior to previous state of the art. Finally, we conduct a user study that confirms the benefits of our framework.



### Shape from Projections via Differentiable Forward Projector for Computed Tomography
- **Arxiv ID**: http://arxiv.org/abs/2006.16120v4
- **DOI**: 10.1016/j.ultramic.2021.113239
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.16120v4)
- **Published**: 2020-06-29 15:33:30+00:00
- **Updated**: 2021-03-11 08:31:41+00:00
- **Authors**: Jakeoung Koo, Anders B. Dahl, J. Andreas Bærentzen, Qiongyang Chen, Sara Bals, Vedrana A. Dahl
- **Comment**: Accepted in Ultramicroscopy
- **Journal**: None
- **Summary**: In computed tomography, the reconstruction is typically obtained on a voxel grid. In this work, however, we propose a mesh-based reconstruction method. For tomographic problems, 3D meshes have mostly been studied to simulate data acquisition, but not for reconstruction, for which a 3D mesh means the inverse process of estimating shapes from projections. In this paper, we propose a differentiable forward model for 3D meshes that bridge the gap between the forward model for 3D surfaces and optimization. We view the forward projection as a rendering process, and make it differentiable by extending recent work in differentiable rendering. We use the proposed forward model to reconstruct 3D shapes directly from projections. Experimental results for single-object problems show that the proposed method outperforms traditional voxel-based methods on noisy simulated data. We also apply the proposed method on electron tomography images of nanoparticles to demonstrate the applicability of the method on real data.



### Human Activity Recognition based on Dynamic Spatio-Temporal Relations
- **Arxiv ID**: http://arxiv.org/abs/2006.16132v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.16132v1)
- **Published**: 2020-06-29 15:49:34+00:00
- **Updated**: 2020-06-29 15:49:34+00:00
- **Authors**: Zhenyu Liu, Yaqiang Yao, Yan Liu, Yuening Zhu, Zhenchao Tao, Lei Wang, Yuhong Feng
- **Comment**: None
- **Journal**: None
- **Summary**: Human activity, which usually consists of several actions, generally covers interactions among persons and or objects. In particular, human actions involve certain spatial and temporal relationships, are the components of more complicated activity, and evolve dynamically over time. Therefore, the description of a single human action and the modeling of the evolution of successive human actions are two major issues in human activity recognition. In this paper, we develop a method for human activity recognition that tackles these two issues. In the proposed method, an activity is divided into several successive actions represented by spatio temporal patterns, and the evolution of these actions are captured by a sequential model. A refined comprehensive spatio temporal graph is utilized to represent a single action, which is a qualitative representation of a human action incorporating both the spatial and temporal relations of the participant objects. Next, a discrete hidden Markov model is applied to model the evolution of action sequences. Moreover, a fully automatic partition method is proposed to divide a long-term human activity video into several human actions based on variational objects and qualitative spatial relations. Finally, a hierarchical decomposition of the human body is introduced to obtain a discriminative representation for a single action. Experimental results on the Cornell Activity Dataset demonstrate the efficiency and effectiveness of the proposed approach, which will enable long videos of human activity to be better recognized.



### Large Deformation Diffeomorphic Image Registration with Laplacian Pyramid Networks
- **Arxiv ID**: http://arxiv.org/abs/2006.16148v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.16148v2)
- **Published**: 2020-06-29 16:10:40+00:00
- **Updated**: 2020-06-30 07:23:39+00:00
- **Authors**: Tony C. W. Mok, Albert C. S. Chung
- **Comment**: Paper accepted by MICCAI 2020
- **Journal**: None
- **Summary**: Deep learning-based methods have recently demonstrated promising results in deformable image registration for a wide range of medical image analysis tasks. However, existing deep learning-based methods are usually limited to small deformation settings, and desirable properties of the transformation including bijective mapping and topology preservation are often being ignored by these approaches. In this paper, we propose a deep Laplacian Pyramid Image Registration Network, which can solve the image registration optimization problem in a coarse-to-fine fashion within the space of diffeomorphic maps. Extensive quantitative and qualitative evaluations on two MR brain scan datasets show that our method outperforms the existing methods by a significant margin while maintaining desirable diffeomorphic properties and promising registration speed.



### Automatic Operating Room Surgical Activity Recognition for Robot-Assisted Surgery
- **Arxiv ID**: http://arxiv.org/abs/2006.16166v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.16166v1)
- **Published**: 2020-06-29 16:30:31+00:00
- **Updated**: 2020-06-29 16:30:31+00:00
- **Authors**: Aidean Sharghi, Helene Haugerud, Daniel Oh, Omid Mohareri
- **Comment**: International Conference on Medical Image Computing and Computer
  Assisted Intervention (MICCAI'20)
- **Journal**: None
- **Summary**: Automatic recognition of surgical activities in the operating room (OR) is a key technology for creating next generation intelligent surgical devices and workflow monitoring/support systems. Such systems can potentially enhance efficiency in the OR, resulting in lower costs and improved care delivery to the patients. In this paper, we investigate automatic surgical activity recognition in robot-assisted operations. We collect the first large-scale dataset including 400 full-length multi-perspective videos from a variety of robotic surgery cases captured using Time-of-Flight cameras. We densely annotate the videos with 10 most recognized and clinically relevant classes of activities. Furthermore, we investigate state-of-the-art computer vision action recognition techniques and adapt them for the OR environment and the dataset. First, we fine-tune the Inflated 3D ConvNet (I3D) for clip-level activity recognition on our dataset and use it to extract features from the videos. These features are then fed to a stack of 3 Temporal Gaussian Mixture layers which extracts context from neighboring clips, and eventually go through a Long Short Term Memory network to learn the order of activities in full-length videos. We extensively assess the model and reach a peak performance of 88% mean Average Precision.



### Unsupervised Learning Consensus Model for Dynamic Texture Videos Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2006.16177v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.16177v1)
- **Published**: 2020-06-29 16:40:59+00:00
- **Updated**: 2020-06-29 16:40:59+00:00
- **Authors**: Lazhar Khelifi, Max Mignotte
- **Comment**: None
- **Journal**: None
- **Summary**: Dynamic texture (DT) segmentation, and video processing in general, is currently widely dominated by methods based on deep neural networks that require the deployment of a large number of layers. Although this parametric approach has shown superior performances for the dynamic texture segmentation, all current deep learning methods suffer from a significant main weakness related to the lack of a sufficient reference annotation to train models and to make them functional. This study explores the unsupervised segmentation approach that can be used in the absence of training data to segment new videos. We present an effective unsupervised learning consensus model for the segmentation of dynamic texture (ULCM). This model is designed to merge different segmentation maps that contain multiple and weak quality regions in order to achieve a more accurate final result of segmentation. The diverse labeling fields required for the combination process are obtained by a simplified grouping scheme applied to an input video (on the basis of a three orthogonal planes: xy, yt and xt). In the proposed model, the set of values of the requantized local binary patterns (LBP) histogram around the pixel to be classified are used as features which represent both the spatial and temporal information replicated in the video. Experiments conducted on the challenging SynthDB dataset show that, contrary to current dynamic texture segmentation approaches that either require parameter estimation or a training step, ULCM is significantly faster, easier to code, simple and has limited parameters. Further qualitative experiments based on the YUP++ dataset prove the efficiently and competitively of the ULCM.



### Self-Supervised MultiModal Versatile Networks
- **Arxiv ID**: http://arxiv.org/abs/2006.16228v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.16228v2)
- **Published**: 2020-06-29 17:50:23+00:00
- **Updated**: 2020-10-30 17:53:59+00:00
- **Authors**: Jean-Baptiste Alayrac, Adrià Recasens, Rosalia Schneider, Relja Arandjelović, Jason Ramapuram, Jeffrey De Fauw, Lucas Smaira, Sander Dieleman, Andrew Zisserman
- **Comment**: To appear in the Thirty-Fourth Annual Conference on Neural
  Information Processing Systems (NeurIPS 2020)
- **Journal**: None
- **Summary**: Videos are a rich source of multi-modal supervision. In this work, we learn representations using self-supervision by leveraging three modalities naturally present in videos: visual, audio and language streams. To this end, we introduce the notion of a multimodal versatile network -- a network that can ingest multiple modalities and whose representations enable downstream tasks in multiple modalities. In particular, we explore how best to combine the modalities, such that fine-grained representations of the visual and audio modalities can be maintained, whilst also integrating text into a common embedding. Driven by versatility, we also introduce a novel process of deflation, so that the networks can be effortlessly applied to the visual data in the form of video or a static image. We demonstrate how such networks trained on large collections of unlabelled video data can be applied on video, video-text, image and audio tasks. Equipped with these representations, we obtain state-of-the-art performance on multiple challenging benchmarks including UCF101, HMDB51, Kinetics600, AudioSet and ESC-50 when compared to previous self-supervised work. Our models are publicly available.



### The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization
- **Arxiv ID**: http://arxiv.org/abs/2006.16241v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.16241v3)
- **Published**: 2020-06-29 17:59:10+00:00
- **Updated**: 2021-07-24 04:28:58+00:00
- **Authors**: Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, Justin Gilmer
- **Comment**: ICCV 2021; Datasets, code, and models available at
  https://github.com/hendrycks/imagenet-r
- **Journal**: None
- **Summary**: We introduce four new real-world distribution shift datasets consisting of changes in image style, image blurriness, geographic location, camera operation, and more. With our new datasets, we take stock of previously proposed methods for improving out-of-distribution robustness and put them to the test. We find that using larger models and artificial data augmentations can improve robustness on real-world distribution shifts, contrary to claims in prior work. We find improvements in artificial robustness benchmarks can transfer to real-world distribution shifts, contrary to claims in prior work. Motivated by our observation that data augmentations can help with real-world distribution shifts, we also introduce a new data augmentation method which advances the state-of-the-art and outperforms models pretrained with 1000 times more labeled data. Overall we find that some methods consistently help with distribution shifts in texture and local image statistics, but these methods do not help with some other distribution shifts like geographic changes. Our results show that future research must study multiple distribution shifts simultaneously, as we demonstrate that no evaluated method consistently improves robustness.



### The Heterogeneity Hypothesis: Finding Layer-Wise Differentiated Network Architectures
- **Arxiv ID**: http://arxiv.org/abs/2006.16242v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.16242v2)
- **Published**: 2020-06-29 17:59:26+00:00
- **Updated**: 2021-05-23 21:23:58+00:00
- **Authors**: Yawei Li, Wen Li, Martin Danelljan, Kai Zhang, Shuhang Gu, Luc Van Gool, Radu Timofte
- **Comment**: CVPR2021 paper
- **Journal**: None
- **Summary**: In this paper, we tackle the problem of convolutional neural network design. Instead of focusing on the design of the overall architecture, we investigate a design space that is usually overlooked, i.e. adjusting the channel configurations of predefined networks. We find that this adjustment can be achieved by shrinking widened baseline networks and leads to superior performance. Based on that, we articulate the heterogeneity hypothesis: with the same training protocol, there exists a layer-wise differentiated network architecture (LW-DNA) that can outperform the original network with regular channel configurations but with a lower level of model complexity.   The LW-DNA models are identified without extra computational cost or training time compared with the original network. This constraint leads to controlled experiments which direct the focus to the importance of layer-wise specific channel configurations. LW-DNA models come with advantages related to overfitting, i.e. the relative relationship between model complexity and dataset size. Experiments are conducted on various networks and datasets for image classification, visual tracking and image restoration. The resultant LW-DNA models consistently outperform the baseline models. Code is available at https://github.com/ofsoundof/Heterogeneity_Hypothesis.



### Scaling Symbolic Methods using Gradients for Neural Model Explanation
- **Arxiv ID**: http://arxiv.org/abs/2006.16322v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.16322v4)
- **Published**: 2020-06-29 19:12:22+00:00
- **Updated**: 2021-05-05 14:13:39+00:00
- **Authors**: Subham Sekhar Sahoo, Subhashini Venugopalan, Li Li, Rishabh Singh, Patrick Riley
- **Comment**: None
- **Journal**: None
- **Summary**: Symbolic techniques based on Satisfiability Modulo Theory (SMT) solvers have been proposed for analyzing and verifying neural network properties, but their usage has been fairly limited owing to their poor scalability with larger networks. In this work, we propose a technique for combining gradient-based methods with symbolic techniques to scale such analyses and demonstrate its application for model explanation. In particular, we apply this technique to identify minimal regions in an input that are most relevant for a neural network's prediction. Our approach uses gradient information (based on Integrated Gradients) to focus on a subset of neurons in the first layer, which allows our technique to scale to large networks. The corresponding SMT constraints encode the minimal input mask discovery problem such that after masking the input, the activations of the selected neurons are still above a threshold. After solving for the minimal masks, our approach scores the mask regions to generate a relative ordering of the features within the mask. This produces a saliency map which explains "where a model is looking" when making a prediction. We evaluate our technique on three datasets - MNIST, ImageNet, and Beer Reviews, and demonstrate both quantitatively and qualitatively that the regions generated by our approach are sparser and achieve higher saliency scores compared to the gradient-based methods alone. Code and examples are at - https://github.com/google-research/google-research/tree/master/smug_saliency



### Asymmetric metric learning for knowledge transfer
- **Arxiv ID**: http://arxiv.org/abs/2006.16331v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.16331v1)
- **Published**: 2020-06-29 19:28:36+00:00
- **Updated**: 2020-06-29 19:28:36+00:00
- **Authors**: Mateusz Budnik, Yannis Avrithis
- **Comment**: None
- **Journal**: None
- **Summary**: Knowledge transfer from large teacher models to smaller student models has recently been studied for metric learning, focusing on fine-grained classification. In this work, focusing on instance-level image retrieval, we study an asymmetric testing task, where the database is represented by the teacher and queries by the student. Inspired by this task, we introduce asymmetric metric learning, a novel paradigm of using asymmetric representations at training. This acts as a simple combination of knowledge transfer with the original metric learning task.   We systematically evaluate different teacher and student models, metric learning and knowledge transfer loss functions on the new asymmetric testing as well as the standard symmetric testing task, where database and queries are represented by the same model. We find that plain regression is surprisingly effective compared to more complex knowledge transfer mechanisms, working best in asymmetric testing. Interestingly, our asymmetric metric learning approach works best in symmetric testing, allowing the student to even outperform the teacher.



### Medical Imaging with Deep Learning: MIDL 2020 -- Short Paper Track
- **Arxiv ID**: http://arxiv.org/abs/2007.02319v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.02319v1)
- **Published**: 2020-06-29 20:01:51+00:00
- **Updated**: 2020-06-29 20:01:51+00:00
- **Authors**: Tal Arbel, Ismail Ben Ayed, Marleen de Bruijne, Maxime Descoteaux, Herve Lombaert, Chris Pal
- **Comment**: Accepted extended abstracts can also be found at
  https://openreview.net/group?id=MIDL.io/2020/Conference#abstract-accept-papers
- **Journal**: None
- **Summary**: This compendium gathers all the accepted extended abstracts from the Third International Conference on Medical Imaging with Deep Learning (MIDL 2020), held in Montreal, Canada, 6-9 July 2020. Note that only accepted extended abstracts are listed here, the Proceedings of the MIDL 2020 Full Paper Track are published in the Proceedings of Machine Learning Research (PMLR).



### Material Recognition for Automated Progress Monitoring using Deep Learning Methods
- **Arxiv ID**: http://arxiv.org/abs/2006.16344v2
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.16344v2)
- **Published**: 2020-06-29 20:06:26+00:00
- **Updated**: 2021-04-17 01:18:00+00:00
- **Authors**: Hadi Mahami, Navid Ghassemi, Mohammad Tayarani Darbandy, Afshin Shoeibi, Sadiq Hussain, Farnad Nasirzadeh, Roohallah Alizadehsani, Darius Nahavandi, Abbas Khosravi, Saeid Nahavandi
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advancements in Artificial intelligence, especially deep learning, has changed many fields irreversibly by introducing state of the art methods for automation. Construction monitoring has not been an exception; as a part of construction monitoring systems, material classification and recognition have drawn the attention of deep learning and machine vision researchers. However, to create production-ready systems, there is still a long path to cover. Real-world problems such as varying illuminations and reaching acceptable accuracies need to be addressed in order to create robust systems. In this paper, we have addressed these issues and reached a state of the art performance, i.e., 97.35% accuracy rate for this task. Also, a new dataset containing 1231 images of 11 classes taken from several construction sites is gathered and publicly published to help other researchers in this field.



### Vehicle Attribute Recognition by Appearance: Computer Vision Methods for Vehicle Type, Make and Model Classification
- **Arxiv ID**: http://arxiv.org/abs/2006.16400v1
- **DOI**: 10.1007/s11265-020-01567-6
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.16400v1)
- **Published**: 2020-06-29 21:33:06+00:00
- **Updated**: 2020-06-29 21:33:06+00:00
- **Authors**: Xingyang Ni, Heikki Huttunen
- **Comment**: Published in Journal of Signal Processing Systems
- **Journal**: None
- **Summary**: This paper studies vehicle attribute recognition by appearance. In the literature, image-based target recognition has been extensively investigated in many use cases, such as facial recognition, but less so in the field of vehicle attribute recognition. We survey a number of algorithms that identify vehicle properties ranging from coarse-grained level (vehicle type) to fine-grained level (vehicle make and model). Moreover, we discuss two alternative approaches for these tasks, including straightforward classification and a more flexible metric learning method. Furthermore, we design a simulated real-world scenario for vehicle attribute recognition and present an experimental comparison of the two approaches.



### Learning Patterns of Tourist Movement and Photography from Geotagged Photos at Archaeological Heritage Sites in Cuzco, Peru
- **Arxiv ID**: http://arxiv.org/abs/2006.16424v1
- **DOI**: 10.1016/j.tourman.2020.104165
- **Categories**: **cs.SI**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.16424v1)
- **Published**: 2020-06-29 22:49:59+00:00
- **Updated**: 2020-06-29 22:49:59+00:00
- **Authors**: Nicole D. Payntar, Wei-Lin Hsiao, R. Alan Covey, Kristen Grauman
- **Comment**: Accepted to Tourism Management
- **Journal**: None
- **Summary**: The popularity of media sharing platforms in recent decades has provided an abundance of open source data that remains underutilized by heritage scholars. By pairing geotagged internet photographs with machine learning and computer vision algorithms, we build upon the current theoretical discourse of anthropology associated with visuality and heritage tourism to identify travel patterns across a known archaeological heritage circuit, and quantify visual culture and experiences in Cuzco, Peru. Leveraging large-scale in-the-wild tourist photos, our goals are to (1) understand how the intensification of tourism intersects with heritage regulations and social media, aiding in the articulation of travel patterns across Cuzco's heritage landscape; and to (2) assess how aesthetic preferences and visuality become entangled with the rapidly evolving expectations of tourists, whose travel narratives are curated on social media and grounded in historic site representations.



### Biologically Inspired Mechanisms for Adversarial Robustness
- **Arxiv ID**: http://arxiv.org/abs/2006.16427v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.16427v1)
- **Published**: 2020-06-29 23:07:34+00:00
- **Updated**: 2020-06-29 23:07:34+00:00
- **Authors**: Manish V. Reddy, Andrzej Banburski, Nishka Pant, Tomaso Poggio
- **Comment**: 25 pages, 15 figures
- **Journal**: None
- **Summary**: A convolutional neural network strongly robust to adversarial perturbations at reasonable computational and performance cost has not yet been demonstrated. The primate visual ventral stream seems to be robust to small perturbations in visual stimuli but the underlying mechanisms that give rise to this robust perception are not understood. In this work, we investigate the role of two biologically plausible mechanisms in adversarial robustness. We demonstrate that the non-uniform sampling performed by the primate retina and the presence of multiple receptive fields with a range of receptive field sizes at each eccentricity improve the robustness of neural networks to small adversarial perturbations. We verify that these two mechanisms do not suffer from gradient obfuscation and study their contribution to adversarial robustness through ablation studies.



### Efficient Continuous Pareto Exploration in Multi-Task Learning
- **Arxiv ID**: http://arxiv.org/abs/2006.16434v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.16434v2)
- **Published**: 2020-06-29 23:36:20+00:00
- **Updated**: 2020-08-26 20:48:16+00:00
- **Authors**: Pingchuan Ma, Tao Du, Wojciech Matusik
- **Comment**: ICML 2020 camera-ready. Code:
  https://github.com/mit-gfx/ContinuousParetoMTL
- **Journal**: None
- **Summary**: Tasks in multi-task learning often correlate, conflict, or even compete with each other. As a result, a single solution that is optimal for all tasks rarely exists. Recent papers introduced the concept of Pareto optimality to this field and directly cast multi-task learning as multi-objective optimization problems, but solutions returned by existing methods are typically finite, sparse, and discrete. We present a novel, efficient method that generates locally continuous Pareto sets and Pareto fronts, which opens up the possibility of continuous analysis of Pareto optimal solutions in machine learning problems. We scale up theoretical results in multi-objective optimization to modern machine learning problems by proposing a sample-based sparse linear system, for which standard Hessian-free solvers in machine learning can be applied. We compare our method to the state-of-the-art algorithms and demonstrate its usage of analyzing local Pareto sets on various multi-task classification and regression problems. The experimental results confirm that our algorithm reveals the primary directions in local Pareto sets for trade-off balancing, finds more solutions with different trade-offs efficiently, and scales well to tasks with millions of parameters.



