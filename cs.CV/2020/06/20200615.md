# Arxiv Papers in cs.CV on 2020-06-15
### Iterative Nadaraya-Watson Distribution Transfer for Colour Grading
- **Arxiv ID**: http://arxiv.org/abs/2006.09208v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2006.09208v1)
- **Published**: 2020-06-15 00:14:03+00:00
- **Updated**: 2020-06-15 00:14:03+00:00
- **Authors**: Hana Alghamdi, Rozenn Dahyot
- **Comment**: 6 pages, 6 figures, 4 tables. arXiv admin note: substantial text
  overlap with arXiv:2005.09015
- **Journal**: None
- **Summary**: We propose a new method with Nadaraya-Watson that maps one N-dimensional distribution to another taking into account available information about correspondences. We extend the 2D/3D problem to higher dimensions by encoding overlapping neighborhoods of data points and solve the high dimensional problem in 1D space using an iterative projection approach. To show potentials of this mapping, we apply it to colour transfer between two images that exhibit overlapped scene. Experiments show quantitative and qualitative improvements over previous state of the art colour transfer methods.



### Multiple Video Frame Interpolation via Enhanced Deformable Separable Convolution
- **Arxiv ID**: http://arxiv.org/abs/2006.08070v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.08070v2)
- **Published**: 2020-06-15 01:10:59+00:00
- **Updated**: 2021-01-25 09:10:57+00:00
- **Authors**: Xianhang Cheng, Zhenzhong Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Generating non-existing frames from a consecutive video sequence has been an interesting and challenging problem in the video processing field. Typical kernel-based interpolation methods predict pixels with a single convolution process that convolves source frames with spatially adaptive local kernels, which circumvents the time-consuming, explicit motion estimation in the form of optical flow. However, when scene motion is larger than the pre-defined kernel size, these methods are prone to yield less plausible results. In addition, they cannot directly generate a frame at an arbitrary temporal position because the learned kernels are tied to the midpoint in time between the input frames. In this paper, we try to solve these problems and propose a novel non-flow kernel-based approach that we refer to as enhanced deformable separable convolution (EDSC) to estimate not only adaptive kernels, but also offsets, masks and biases to make the network obtain information from non-local neighborhood. During the learning process, different intermediate time step can be involved as a control variable by means of an extension of coord-conv trick, allowing the estimated components to vary with different input temporal information. This makes our method capable to produce multiple in-between frames. Furthermore, we investigate the relationships between our method and other typical kernel- and flow-based methods. Experimental results show that our method performs favorably against the state-of-the-art methods across a broad range of datasets. Code will be publicly available on URL: \url{https://github.com/Xianhang/EDSC-pytorch}.



### Geo-PIFu: Geometry and Pixel Aligned Implicit Functions for Single-view Human Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2006.08072v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.08072v2)
- **Published**: 2020-06-15 01:11:48+00:00
- **Updated**: 2020-12-07 00:20:35+00:00
- **Authors**: Tong He, John Collomosse, Hailin Jin, Stefano Soatto
- **Comment**: None
- **Journal**: None
- **Summary**: We propose Geo-PIFu, a method to recover a 3D mesh from a monocular color image of a clothed person. Our method is based on a deep implicit function-based representation to learn latent voxel features using a structure-aware 3D U-Net, to constrain the model in two ways: first, to resolve feature ambiguities in query point encoding, second, to serve as a coarse human shape proxy to regularize the high-resolution mesh and encourage global shape regularity. We show that, by both encoding query points and constraining global shape using latent voxel features, the reconstruction we obtain for clothed human meshes exhibits less shape distortion and improved surface details compared to competing methods. We evaluate Geo-PIFu on a recent human mesh public dataset that is $10 \times$ larger than the private commercial dataset used in PIFu and previous derivative work. On average, we exceed the state of the art by $42.7\%$ reduction in Chamfer and Point-to-Surface Distances, and $19.4\%$ reduction in normal estimation errors.



### Generalized Adversarially Learned Inference
- **Arxiv ID**: http://arxiv.org/abs/2006.08089v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.08089v3)
- **Published**: 2020-06-15 02:18:13+00:00
- **Updated**: 2020-12-21 15:34:08+00:00
- **Authors**: Yatin Dandi, Homanga Bharadhwaj, Abhishek Kumar, Piyush Rai
- **Comment**: AAAI 2021 (accepted for publication)
- **Journal**: None
- **Summary**: Allowing effective inference of latent vectors while training GANs can greatly increase their applicability in various downstream tasks. Recent approaches, such as ALI and BiGAN frameworks, develop methods of inference of latent variables in GANs by adversarially training an image generator along with an encoder to match two joint distributions of image and latent vector pairs. We generalize these approaches to incorporate multiple layers of feedback on reconstructions, self-supervision, and other forms of supervision based on prior or learned knowledge about the desired solutions. We achieve this by modifying the discriminator's objective to correctly identify more than two joint distributions of tuples of an arbitrary number of random variables consisting of images, latent vectors, and other variables generated through auxiliary tasks, such as reconstruction and inpainting or as outputs of suitable pre-trained models. We design a non-saturating maximization objective for the generator-encoder pair and prove that the resulting adversarial game corresponds to a global optimum that simultaneously matches all the distributions. Within our proposed framework, we introduce a novel set of techniques for providing self-supervised feedback to the model based on properties, such as patch-level correspondence and cycle consistency of reconstructions. Through comprehensive experiments, we demonstrate the efficacy, scalability, and flexibility of the proposed approach for a variety of tasks.



### Emotion Recognition in Audio and Video Using Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2006.08129v1
- **DOI**: None
- **Categories**: **eess.AS**, cs.CV, cs.LG, cs.SD, I.4.9; I.2.m
- **Links**: [PDF](http://arxiv.org/pdf/2006.08129v1)
- **Published**: 2020-06-15 04:50:18+00:00
- **Updated**: 2020-06-15 04:50:18+00:00
- **Authors**: Mandeep Singh, Yuan Fang
- **Comment**: 9 pages, 9 figures, 3 tables
- **Journal**: None
- **Summary**: Humans are able to comprehend information from multiple domains for e.g. speech, text and visual. With advancement of deep learning technology there has been significant improvement of speech recognition. Recognizing emotion from speech is important aspect and with deep learning technology emotion recognition has improved in accuracy and latency. There are still many challenges to improve accuracy. In this work, we attempt to explore different neural networks to improve accuracy of emotion recognition. With different architectures explored, we find (CNN+RNN) + 3DCNN multi-model architecture which processes audio spectrograms and corresponding video frames giving emotion prediction accuracy of 54.0% among 4 emotions and 71.75% among 3 emotions using IEMOCAP[2] dataset.



### Anomalous Motion Detection on Highway Using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2006.08143v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.08143v1)
- **Published**: 2020-06-15 05:40:11+00:00
- **Updated**: 2020-06-15 05:40:11+00:00
- **Authors**: Harpreet Singh, Emily M. Hand, Kostas Alexis
- **Comment**: to be published in IEEE ICIP 2020
- **Journal**: None
- **Summary**: Research in visual anomaly detection draws much interest due to its applications in surveillance. Common datasets for evaluation are constructed using a stationary camera overlooking a region of interest. Previous research has shown promising results in detecting spatial as well as temporal anomalies in these settings. The advent of self-driving cars provides an opportunity to apply visual anomaly detection in a more dynamic application yet no dataset exists in this type of environment. This paper presents a new anomaly detection dataset - the Highway Traffic Anomaly (HTA) dataset - for the problem of detecting anomalous traffic patterns from dash cam videos of vehicles on highways. We evaluate state-of-the-art deep learning anomaly detection models and propose novel variations to these methods. Our results show that state-of-the-art models built for settings with a stationary camera do not translate well to a more dynamic environment. The proposed variations to these SoTA methods show promising results on the new HTA dataset.



### Classifying degraded images over various levels of degradation
- **Arxiv ID**: http://arxiv.org/abs/2006.08145v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.08145v1)
- **Published**: 2020-06-15 05:43:07+00:00
- **Updated**: 2020-06-15 05:43:07+00:00
- **Authors**: Kazuki Endo, Masayuki Tanaka, Masatoshi Okutomi
- **Comment**: Accepted by the 27th IEEE International Conference on Image
  Processing (ICIP 2020)
- **Journal**: None
- **Summary**: Classification for degraded images having various levels of degradation is very important in practical applications. This paper proposes a convolutional neural network to classify degraded images by using a restoration network and an ensemble learning. The results demonstrate that the proposed network can classify degraded images over various levels of degradation well. This paper also reveals how the image-quality of training data for a classification network affects the classification performance of degraded images.



### NP-PROV: Neural Processes with Position-Relevant-Only Variances
- **Arxiv ID**: http://arxiv.org/abs/2007.00767v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.00767v1)
- **Published**: 2020-06-15 06:11:21+00:00
- **Updated**: 2020-06-15 06:11:21+00:00
- **Authors**: Xuesong Wang, Lina Yao, Xianzhi Wang, Feiping Nie
- **Comment**: 10 pages, 5 figures
- **Journal**: None
- **Summary**: Neural Processes (NPs) families encode distributions over functions to a latent representation, given context data, and decode posterior mean and variance at unknown locations. Since mean and variance are derived from the same latent space, they may fail on out-of-domain tasks where fluctuations in function values amplify the model uncertainty. We present a new member named Neural Processes with Position-Relevant-Only Variances (NP-PROV). NP-PROV hypothesizes that a target point close to a context point has small uncertainty, regardless of the function value at that position. The resulting approach derives mean and variance from a function-value-related space and a position-related-only latent space separately. Our evaluation on synthetic and real-world datasets reveals that NP-PROV can achieve state-of-the-art likelihood while retaining a bounded variance when drifts exist in the function value.



### Survey on Deep Multi-modal Data Analytics: Collaboration, Rivalry and Fusion
- **Arxiv ID**: http://arxiv.org/abs/2006.08159v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.08159v1)
- **Published**: 2020-06-15 06:42:04+00:00
- **Updated**: 2020-06-15 06:42:04+00:00
- **Authors**: Yang Wang
- **Comment**: Appearing at ACM TOMM, 26 pages
- **Journal**: None
- **Summary**: With the development of web technology, multi-modal or multi-view data has surged as a major stream for big data, where each modal/view encodes individual property of data objects. Often, different modalities are complementary to each other. Such fact motivated a lot of research attention on fusing the multi-modal feature spaces to comprehensively characterize the data objects. Most of the existing state-of-the-art focused on how to fuse the energy or information from multi-modal spaces to deliver a superior performance over their counterparts with single modal. Recently, deep neural networks have exhibited as a powerful architecture to well capture the nonlinear distribution of high-dimensional multimedia data, so naturally does for multi-modal data. Substantial empirical studies are carried out to demonstrate its advantages that are benefited from deep multi-modal methods, which can essentially deepen the fusion from multi-modal deep feature spaces. In this paper, we provide a substantial overview of the existing state-of-the-arts on the filed of multi-modal data analytics from shallow to deep spaces. Throughout this survey, we further indicate that the critical components for this field go to collaboration, adversarial competition and fusion over multi-modal spaces. Finally, we share our viewpoints regarding some future directions on this field.



### Filter design for small target detection on infrared imagery using normalized-cross-correlation layer
- **Arxiv ID**: http://arxiv.org/abs/2006.08162v1
- **DOI**: 10.3906/elk-1807-287
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.08162v1)
- **Published**: 2020-06-15 06:46:08+00:00
- **Updated**: 2020-06-15 06:46:08+00:00
- **Authors**: H. Seçkin Demir, Erdem Akagunduz
- **Comment**: None
- **Journal**: published in Turkish Journal of Electrical Engineering and
  Computer Sciences, vol.28, 302:317, 2020
- **Summary**: In this paper, we introduce a machine learning approach to the problem of infrared small target detection filter design. For this purpose, similarly to a convolutional layer of a neural network, the normalized-cross-correlational (NCC) layer, which we utilize for designing a target detection/recognition filter bank, is proposed. By employing the NCC layer in a neural network structure, we introduce a framework, in which supervised training is used to calculate the optimal filter shape and the optimum number of filters required for a specific target detection/recognition task on infrared images. We also propose the mean-absolute-deviation NCC (MAD-NCC) layer, an efficient implementation of the proposed NCC layer, designed especially for FPGA systems, in which square root operations are avoided for real-time computation. As a case study we work on dim-target detection on mid-wave infrared imagery and obtain the filters that can discriminate a dim target from various types of background clutter, specific to our operational concept.



### Neural gradients are near-lognormal: improved quantized and sparse training
- **Arxiv ID**: http://arxiv.org/abs/2006.08173v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.08173v3)
- **Published**: 2020-06-15 07:00:15+00:00
- **Updated**: 2020-10-12 14:18:25+00:00
- **Authors**: Brian Chmiel, Liad Ben-Uri, Moran Shkolnik, Elad Hoffer, Ron Banner, Daniel Soudry
- **Comment**: None
- **Journal**: None
- **Summary**: While training can mostly be accelerated by reducing the time needed to propagate neural gradients back throughout the model, most previous works focus on the quantization/pruning of weights and activations. These methods are often not applicable to neural gradients, which have very different statistical properties. Distinguished from weights and activations, we find that the distribution of neural gradients is approximately lognormal. Considering this, we suggest two closed-form analytical methods to reduce the computational and memory burdens of neural gradients. The first method optimizes the floating-point format and scale of the gradients. The second method accurately sets sparsity thresholds for gradient pruning. Each method achieves state-of-the-art results on ImageNet. To the best of our knowledge, this paper is the first to (1) quantize the gradients to 6-bit floating-point formats, or (2) achieve up to 85% gradient sparsity -- in each case without accuracy degradation. Reference implementation accompanies the paper.



### Dissimilarity Mixture Autoencoder for Deep Clustering
- **Arxiv ID**: http://arxiv.org/abs/2006.08177v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML, 68T10
- **Links**: [PDF](http://arxiv.org/pdf/2006.08177v4)
- **Published**: 2020-06-15 07:08:59+00:00
- **Updated**: 2021-07-15 16:33:02+00:00
- **Authors**: Juan S. Lara, Fabio A. González
- **Comment**: 20 pages (2.5 pages of references)
- **Journal**: None
- **Summary**: The dissimilarity mixture autoencoder (DMAE) is a neural network model for feature-based clustering that incorporates a flexible dissimilarity function and can be integrated into any kind of deep learning architecture. It internally represents a dissimilarity mixture model (DMM) that extends classical methods like K-Means, Gaussian mixture models, or Bregman clustering to any convex and differentiable dissimilarity function through the reinterpretation of probabilities as neural network representations. DMAE can be integrated with deep learning architectures into end-to-end models, allowing the simultaneous estimation of the clustering and neural network's parameters. Experimental evaluation was performed on image and text clustering benchmark datasets showing that DMAE is competitive in terms of unsupervised classification accuracy and normalized mutual information. The source code with the implementation of DMAE is publicly available at: https://github.com/juselara1/dmae



### Binary DAD-Net: Binarized Driveable Area Detection Network for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2006.08178v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.08178v1)
- **Published**: 2020-06-15 07:09:01+00:00
- **Updated**: 2020-06-15 07:09:01+00:00
- **Authors**: Alexander Frickenstein, Manoj Rohit Vemparala, Jakob Mayr, Naveen Shankar Nagaraja, Christian Unger, Federico Tombari, Walter Stechele
- **Comment**: IEEE International Conference on Robotics and Automation (ICRA) 2020
- **Journal**: None
- **Summary**: Driveable area detection is a key component for various applications in the field of autonomous driving (AD), such as ground-plane detection, obstacle detection and maneuver planning. Additionally, bulky and over-parameterized networks can be easily forgone and replaced with smaller networks for faster inference on embedded systems. The driveable area detection, posed as a two class segmentation task, can be efficiently modeled with slim binary networks. This paper proposes a novel binarized driveable area detection network (binary DAD-Net), which uses only binary weights and activations in the encoder, the bottleneck, and the decoder part. The latent space of the bottleneck is efficiently increased (x32 -> x16 downsampling) through binary dilated convolutions, learning more complex features. Along with automatically generated training data, the binary DAD-Net outperforms state-of-the-art semantic segmentation networks on public datasets. In comparison to a full-precision model, our approach has a x14.3 reduced compute complexity on an FPGA and it requires only 0.9MB memory resources. Therefore, commodity SIMD-based AD-hardware is capable of accelerating the binary DAD-Net.



### Infinite Feature Selection: A Graph-based Feature Filtering Approach
- **Arxiv ID**: http://arxiv.org/abs/2006.08184v1
- **DOI**: 10.1109/TPAMI.2020.3002843
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.08184v1)
- **Published**: 2020-06-15 07:20:40+00:00
- **Updated**: 2020-06-15 07:20:40+00:00
- **Authors**: Giorgio Roffo, Simone Melzi, Umberto Castellani, Alessandro Vinciarelli, Marco Cristani
- **Comment**: TPAMI PREPRINT 2020
- **Journal**: IEEE Transactions on Pattern Analysis and Machine Intelligence
  (2020),
- **Summary**: We propose a filtering feature selection framework that considers subsets of features as paths in a graph, where a node is a feature and an edge indicates pairwise (customizable) relations among features, dealing with relevance and redundancy principles. By two different interpretations (exploiting properties of power series of matrices and relying on Markov chains fundamentals) we can evaluate the values of paths (i.e., feature subsets) of arbitrary lengths, eventually go to infinite, from which we dub our framework Infinite Feature Selection (Inf-FS). Going to infinite allows to constrain the computational complexity of the selection process, and to rank the features in an elegant way, that is, considering the value of any path (subset) containing a particular feature. We also propose a simple unsupervised strategy to cut the ranking, so providing the subset of features to keep. In the experiments, we analyze diverse settings with heterogeneous features, for a total of 11 benchmarks, comparing against 18 widely-known comparative approaches. The results show that Inf-FS behaves better in almost any situation, that is, when the number of features to keep are fixed a priori, or when the decision of the subset cardinality is part of the process.



### AutoGAN-Distiller: Searching to Compress Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2006.08198v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.08198v2)
- **Published**: 2020-06-15 07:56:24+00:00
- **Updated**: 2020-07-06 15:41:44+00:00
- **Authors**: Yonggan Fu, Wuyang Chen, Haotao Wang, Haoran Li, Yingyan Lin, Zhangyang Wang
- **Comment**: Accepted at ICML2020
- **Journal**: None
- **Summary**: The compression of Generative Adversarial Networks (GANs) has lately drawn attention, due to the increasing demand for deploying GANs into mobile devices for numerous applications such as image translation, enhancement and editing. However, compared to the substantial efforts to compressing other deep models, the research on compressing GANs (usually the generators) remains at its infancy stage. Existing GAN compression algorithms are limited to handling specific GAN architectures and losses. Inspired by the recent success of AutoML in deep compression, we introduce AutoML to GAN compression and develop an AutoGAN-Distiller (AGD) framework. Starting with a specifically designed efficient search space, AGD performs an end-to-end discovery for new efficient generators, given the target computational resource constraints. The search is guided by the original GAN model via knowledge distillation, therefore fulfilling the compression. AGD is fully automatic, standalone (i.e., needing no trained discriminators), and generically applicable to various GAN models. We evaluate AGD in two representative GAN tasks: image translation and super resolution. Without bells and whistles, AGD yields remarkably lightweight yet more competitive compressed models, that largely outperform existing alternatives. Our codes and pretrained models are available at https://github.com/TAMU-VITA/AGD.



### AdamP: Slowing Down the Slowdown for Momentum Optimizers on Scale-invariant Weights
- **Arxiv ID**: http://arxiv.org/abs/2006.08217v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.08217v3)
- **Published**: 2020-06-15 08:35:15+00:00
- **Updated**: 2021-01-18 14:36:15+00:00
- **Authors**: Byeongho Heo, Sanghyuk Chun, Seong Joon Oh, Dongyoon Han, Sangdoo Yun, Gyuwan Kim, Youngjung Uh, Jung-Woo Ha
- **Comment**: Accepted at ICLR 2021. First two authors contributed equally
- **Journal**: None
- **Summary**: Normalization techniques are a boon for modern deep learning. They let weights converge more quickly with often better generalization performances. It has been argued that the normalization-induced scale invariance among the weights provides an advantageous ground for gradient descent (GD) optimizers: the effective step sizes are automatically reduced over time, stabilizing the overall training procedure. It is often overlooked, however, that the additional introduction of momentum in GD optimizers results in a far more rapid reduction in effective step sizes for scale-invariant weights, a phenomenon that has not yet been studied and may have caused unwanted side effects in the current practice. This is a crucial issue because arguably the vast majority of modern deep neural networks consist of (1) momentum-based GD (e.g. SGD or Adam) and (2) scale-invariant parameters. In this paper, we verify that the widely-adopted combination of the two ingredients lead to the premature decay of effective step sizes and sub-optimal model performances. We propose a simple and effective remedy, SGDP and AdamP: get rid of the radial component, or the norm-increasing direction, at each optimizer step. Because of the scale invariance, this modification only alters the effective step sizes without changing the effective update directions, thus enjoying the original convergence properties of GD optimizers. Given the ubiquity of momentum GD and scale invariance in machine learning, we have evaluated our methods against the baselines on 13 benchmarks. They range from vision tasks like classification (e.g. ImageNet), retrieval (e.g. CUB and SOP), and detection (e.g. COCO) to language modelling (e.g. WikiText) and audio classification (e.g. DCASE) tasks. We verify that our solution brings about uniform gains in those benchmarks. Source code is available at https://github.com/clovaai/AdamP.



### Differentiable Neural Architecture Transformation for Reproducible Architecture Improvement
- **Arxiv ID**: http://arxiv.org/abs/2006.08231v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.08231v1)
- **Published**: 2020-06-15 09:03:48+00:00
- **Updated**: 2020-06-15 09:03:48+00:00
- **Authors**: Do-Guk Kim, Heung-Chang Lee
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, Neural Architecture Search (NAS) methods are introduced and show impressive performance on many benchmarks. Among those NAS studies, Neural Architecture Transformer (NAT) aims to improve the given neural architecture to have better performance while maintaining computational costs. However, NAT has limitations about a lack of reproducibility. In this paper, we propose differentiable neural architecture transformation that is reproducible and efficient. The proposed method shows stable performance on various architectures. Extensive reproducibility experiments on two datasets, i.e., CIFAR-10 and Tiny Imagenet, present that the proposed method definitely outperforms NAT and be applicable to other models and datasets.



### Dense Non-Rigid Structure from Motion: A Manifold Viewpoint
- **Arxiv ID**: http://arxiv.org/abs/2006.09197v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CG
- **Links**: [PDF](http://arxiv.org/pdf/2006.09197v1)
- **Published**: 2020-06-15 09:15:54+00:00
- **Updated**: 2020-06-15 09:15:54+00:00
- **Authors**: Suryansh Kumar, Luc Van Gool, Carlos E. P. de Oliveira, Anoop Cherian, Yuchao Dai, Hongdong Li
- **Comment**: A comprehensive version that combines our cvpr 2018 and cvpr 2019
  work (Still under development and refinement, Initial Version). 13 Figures, 1
  Table. arXiv admin note: text overlap with arXiv:1902.01077
- **Journal**: None
- **Summary**: Non-Rigid Structure-from-Motion (NRSfM) problem aims to recover 3D geometry of a deforming object from its 2D feature correspondences across multiple frames. Classical approaches to this problem assume a small number of feature points and, ignore the local non-linearities of the shape deformation, and therefore, struggles to reliably model non-linear deformations. Furthermore, available dense NRSfM algorithms are often hurdled by scalability, computations, noisy measurements and, restricted to model just global deformation. In this paper, we propose algorithms that can overcome these limitations with the previous methods and, at the same time, can recover a reliable dense 3D structure of a non-rigid object with higher accuracy. Assuming that a deforming shape is composed of a union of local linear subspace and, span a global low-rank space over multiple frames enables us to efficiently model complex non-rigid deformations. To that end, each local linear subspace is represented using Grassmannians and, the global 3D shape across multiple frames is represented using a low-rank representation. We show that our approach significantly improves accuracy, scalability, and robustness against noise. Also, our representation naturally allows for simultaneous reconstruction and clustering framework which in general is observed to be more suitable for NRSfM problems. Our method currently achieves leading performance on the standard benchmark datasets.



### Self-Supervised Domain Mismatch Estimation for Autonomous Perception
- **Arxiv ID**: http://arxiv.org/abs/2006.08613v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.08613v1)
- **Published**: 2020-06-15 09:22:03+00:00
- **Updated**: 2020-06-15 09:22:03+00:00
- **Authors**: Jonas Löhdefink, Justin Fehrling, Marvin Klingner, Fabian Hüger, Peter Schlicht, Nico M. Schmidt, Tim Fingscheidt
- **Comment**: Proc. of CVPR - Workshops
- **Journal**: None
- **Summary**: Autonomous driving requires self awareness of its perception functions. Technically spoken, this can be realized by observers, which monitor the performance indicators of various perception modules. In this work we choose, exemplarily, a semantic segmentation to be monitored, and propose an autoencoder, trained in a self-supervised fashion on the very same training data as the semantic segmentation to be monitored. While the autoencoder's image reconstruction performance (PSNR) during online inference shows already a good predictive power w.r.t. semantic segmentation performance, we propose a novel domain mismatch metric DM as the earth mover's distance between a pre-stored PSNR distribution on training (source) data, and an online-acquired PSNR distribution on any inference (target) data. We are able to show by experiments that the DM metric has a strong rank order correlation with the semantic segmentation within its functional scope. We also propose a training domain-dependent threshold for the DM metric to define this functional scope.



### Learn to cycle: Time-consistent feature discovery for action recognition
- **Arxiv ID**: http://arxiv.org/abs/2006.08247v2
- **DOI**: 10.1016/j.patrec.2020.11.012
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.08247v2)
- **Published**: 2020-06-15 09:36:28+00:00
- **Updated**: 2020-06-23 14:06:36+00:00
- **Authors**: Alexandros Stergiou, Ronald Poppe
- **Comment**: None
- **Journal**: None
- **Summary**: Generalizing over temporal variations is a prerequisite for effective action recognition in videos. Despite significant advances in deep neural networks, it remains a challenge to focus on short-term discriminative motions in relation to the overall performance of an action. We address this challenge by allowing some flexibility in discovering relevant spatio-temporal features. We introduce Squeeze and Recursion Temporal Gates (SRTG), an approach that favors inputs with similar activations with potential temporal variations. We implement this idea with a novel CNN block that uses an LSTM to encapsulate feature dynamics, in conjunction with a temporal gate that is responsible for evaluating the consistency of the discovered dynamics and the modeled features. We show consistent improvement when using SRTG blocks, with only a minimal increase in the number of GFLOPs. On Kinetics-700, we perform on par with current state-of-the-art models, and outperform these on HACS, Moments in Time, UCF-101 and HMDB-51.



### Dermatologist vs Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2006.08254v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.08254v1)
- **Published**: 2020-06-15 09:44:57+00:00
- **Updated**: 2020-06-15 09:44:57+00:00
- **Authors**: Kaushil Mangaroliya, Mitt Shah
- **Comment**: None
- **Journal**: None
- **Summary**: Cancer, in general, is very deadly. Timely treatment of any cancer is the key to saving a life. Skin cancer is no exception. There have been thousands of Skin Cancer cases registered per year all over the world. There have been 123,000 deadly melanoma cases detected in a single year. This huge number is proven to be a cause of a high amount of UV rays present in the sunlight due to the degradation of the Ozone layer. If not detected at an early stage, skin cancer can lead to the death of the patient. Unavailability of proper resources such as expert dermatologists, state of the art testing facilities, and quick biopsy results have led researchers to develop a technology that can solve the above problem. Deep Learning is one such method that has offered extraordinary results. The Convolutional Neural Network proposed in this study out performs every pretrained models. We trained our model on the HAM10000 dataset which offers 10015 images belonging to 7 classes of skin disease. The model we proposed gave an accuracy of 89%. This model can predict deadly melanoma skin cancer with a great accuracy. Hopefully, this study can help save people's life where there is the unavailability of proper dermatological resources by bridging the gap using our proposed study.



### AMENet: Attentive Maps Encoder Network for Trajectory Prediction
- **Arxiv ID**: http://arxiv.org/abs/2006.08264v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.08264v2)
- **Published**: 2020-06-15 10:00:07+00:00
- **Updated**: 2021-01-13 15:57:30+00:00
- **Authors**: Hao Cheng, Wentong Liao, Michael Ying Yang, Bodo Rosenhahn, Monika Sester
- **Comment**: Accepted by ISPRS Journal of Photogrammetry and Remote Sensing
- **Journal**: None
- **Summary**: Trajectory prediction is critical for applications of planning safe future movements and remains challenging even for the next few seconds in urban mixed traffic. How an agent moves is affected by the various behaviors of its neighboring agents in different environments. To predict movements, we propose an end-to-end generative model named Attentive Maps Encoder Network (AMENet) that encodes the agent's motion and interaction information for accurate and realistic multi-path trajectory prediction. A conditional variational auto-encoder module is trained to learn the latent space of possible future paths based on attentive dynamic maps for interaction modeling and then is used to predict multiple plausible future trajectories conditioned on the observed past trajectories. The efficacy of AMENet is validated using two public trajectory prediction benchmarks Trajnet and InD.



### Deep-CAPTCHA: a deep learning based CAPTCHA solver for vulnerability assessment
- **Arxiv ID**: http://arxiv.org/abs/2006.08296v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.IT, cs.LG, math.IT, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.08296v2)
- **Published**: 2020-06-15 11:44:43+00:00
- **Updated**: 2020-06-24 19:55:33+00:00
- **Authors**: Zahra Noury, Mahdi Rezaei
- **Comment**: Version 2.0
- **Journal**: None
- **Summary**: CAPTCHA is a human-centred test to distinguish a human operator from bots, attacking programs, or other computerised agents that tries to imitate human intelligence. In this research, we investigate a way to crack visual CAPTCHA tests by an automated deep learning based solution. The goal of this research is to investigate the weaknesses and vulnerabilities of the CAPTCHA generator systems; hence, developing more robust CAPTCHAs, without taking the risks of manual try and fail efforts. We develop a Convolutional Neural Network called Deep-CAPTCHA to achieve this goal. The proposed platform is able to investigate both numerical and alphanumerical CAPTCHAs. To train and develop an efficient model, we have generated a dataset of 500,000 CAPTCHAs to train our model. In this paper, we present our customised deep neural network model, we review the research gaps, the existing challenges, and the solutions to cope with the issues. Our network's cracking accuracy leads to a high rate of 98.94% and 98.31% for the numerical and the alpha-numerical test datasets, respectively. That means more works is required to develop robust CAPTCHAs, to be non-crackable against automated artificial agents. As the outcome of this research, we identify some efficient techniques to improve the security of the CAPTCHAs, based on the performance analysis conducted on the Deep-CAPTCHA model.



### Mitigating Gender Bias in Captioning Systems
- **Arxiv ID**: http://arxiv.org/abs/2006.08315v7
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2006.08315v7)
- **Published**: 2020-06-15 12:16:19+00:00
- **Updated**: 2021-04-20 21:48:29+00:00
- **Authors**: Ruixiang Tang, Mengnan Du, Yuening Li, Zirui Liu, Na Zou, Xia Hu
- **Comment**: None
- **Journal**: None
- **Summary**: Image captioning has made substantial progress with huge supporting image collections sourced from the web. However, recent studies have pointed out that captioning datasets, such as COCO, contain gender bias found in web corpora. As a result, learning models could heavily rely on the learned priors and image context for gender identification, leading to incorrect or even offensive errors. To encourage models to learn correct gender features, we reorganize the COCO dataset and present two new splits COCO-GB V1 and V2 datasets where the train and test sets have different gender-context joint distribution. Models relying on contextual cues will suffer from huge gender prediction errors on the anti-stereotypical test data. Benchmarking experiments reveal that most captioning models learn gender bias, leading to high gender prediction errors, especially for women. To alleviate the unwanted bias, we propose a new Guided Attention Image Captioning model (GAIC) which provides self-guidance on visual attention to encourage the model to capture correct gender visual evidence. Experimental results validate that GAIC can significantly reduce gender prediction errors with a competitive caption quality. Our codes and the designed benchmark datasets are available at https://github.com/datamllab/Mitigating_Gender_Bias_In_Captioning_System.



### On the Preservation of Spatio-temporal Information in Machine Learning Applications
- **Arxiv ID**: http://arxiv.org/abs/2006.08321v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.08321v1)
- **Published**: 2020-06-15 12:22:36+00:00
- **Updated**: 2020-06-15 12:22:36+00:00
- **Authors**: Yigit Oktar, Mehmet Turkan
- **Comment**: None
- **Journal**: None
- **Summary**: In conventional machine learning applications, each data attribute is assumed to be orthogonal to others. Namely, every pair of dimension is orthogonal to each other and thus there is no distinction of in-between relations of dimensions. However, this is certainly not the case in real world signals which naturally originate from a spatio-temporal configuration. As a result, the conventional vectorization process disrupts all of the spatio-temporal information about the order/place of data whether it be $1$D, $2$D, $3$D, or $4$D. In this paper, the problem of orthogonality is first investigated through conventional $k$-means of images, where images are to be processed as vectors. As a solution, shift-invariant $k$-means is proposed in a novel framework with the help of sparse representations. A generalization of shift-invariant $k$-means, convolutional dictionary learning, is then utilized as an unsupervised feature extraction method for classification. Experiments suggest that Gabor feature extraction as a simulation of shallow convolutional neural networks provides a little better performance compared to convolutional dictionary learning. Many alternatives of convolutional-logic are also discussed for spatio-temporal information preservation, including a spatio-temporal hypercomplex encoding scheme.



### ORD: Object Relationship Discovery for Visual Dialogue Generation
- **Arxiv ID**: http://arxiv.org/abs/2006.08322v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2006.08322v1)
- **Published**: 2020-06-15 12:25:40+00:00
- **Updated**: 2020-06-15 12:25:40+00:00
- **Authors**: Ziwei Wang, Zi Huang, Yadan Luo, Huimin Lu
- **Comment**: None
- **Journal**: None
- **Summary**: With the rapid advancement of image captioning and visual question answering at single-round level, the question of how to generate multi-round dialogue about visual content has not yet been well explored.Existing visual dialogue methods encode the image into a fixed feature vector directly, concatenated with the question and history embeddings to predict the response.Some recent methods tackle the co-reference resolution problem using co-attention mechanism to cross-refer relevant elements from the image, history, and the target question.However, it remains challenging to reason visual relationships, since the fine-grained object-level information is omitted before co-attentive reasoning. In this paper, we propose an object relationship discovery (ORD) framework to preserve the object interactions for visual dialogue generation. Specifically, a hierarchical graph convolutional network (HierGCN) is proposed to retain the object nodes and neighbour relationships locally, and then refines the object-object connections globally to obtain the final graph embeddings. A graph attention is further incorporated to dynamically attend to this graph-structured representation at the response reasoning stage. Extensive experiments have proved that the proposed method can significantly improve the quality of dialogue by utilising the contextual information of visual relationships. The model achieves superior performance over the state-of-the-art methods on the Visual Dialog dataset, increasing MRR from 0.6222 to 0.6447, and recall@1 from 48.48% to 51.22%.



### Generating Master Faces for Use in Performing Wolf Attacks on Face Recognition Systems
- **Arxiv ID**: http://arxiv.org/abs/2006.08376v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.08376v1)
- **Published**: 2020-06-15 12:59:49+00:00
- **Updated**: 2020-06-15 12:59:49+00:00
- **Authors**: Huy H. Nguyen, Junichi Yamagishi, Isao Echizen, Sébastien Marcel
- **Comment**: Accepted to be Published in Proceedings of the 2020 International
  Joint Conference on Biometrics (IJCB 2020), Houston, USA
- **Journal**: None
- **Summary**: Due to its convenience, biometric authentication, especial face authentication, has become increasingly mainstream and thus is now a prime target for attackers. Presentation attacks and face morphing are typical types of attack. Previous research has shown that finger-vein- and fingerprint-based authentication methods are susceptible to wolf attacks, in which a wolf sample matches many enrolled user templates. In this work, we demonstrated that wolf (generic) faces, which we call "master faces," can also compromise face recognition systems and that the master face concept can be generalized in some cases. Motivated by recent similar work in the fingerprint domain, we generated high-quality master faces by using the state-of-the-art face generator StyleGAN in a process called latent variable evolution. Experiments demonstrated that even attackers with limited resources using only pre-trained models available on the Internet can initiate master face attacks. The results, in addition to demonstrating performance from the attacker's point of view, can also be used to clarify and improve the performance of face recognition systems and harden face authentication systems.



### Pixel Invisibility: Detecting Objects Invisible in Color Images
- **Arxiv ID**: http://arxiv.org/abs/2006.08383v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.08383v2)
- **Published**: 2020-06-15 13:10:17+00:00
- **Updated**: 2020-06-16 02:46:52+00:00
- **Authors**: Yongxin Wang, Duminda Wijesekera
- **Comment**: 8 pages, 7 figures, submitted to NIPS 2020
- **Journal**: None
- **Summary**: Despite recent success of object detectors using deep neural networks, their deployment on safety-critical applications such as self-driving cars remains questionable. This is partly due to the absence of reliable estimation for detectors' failure under operational conditions such as night, fog, dusk, dawn and glare. Such unquantifiable failures could lead to safety violations. In order to solve this problem, we created an algorithm that predicts a pixel-level invisibility map for color images that does not require manual labeling - that computes the probability that a pixel/region contains objects that are invisible in color domain, during various lighting conditions such as day, night and fog. We propose a novel use of cross modal knowledge distillation from color to infra-red domain using weakly-aligned image pairs from the day and construct indicators for the pixel-level invisibility based on the distances of their intermediate-level features. Quantitative experiments show the great performance of our pixel-level invisibility mask and also the effectiveness of distilled mid-level features on object detection in infra-red imagery.



### Spherical Motion Dynamics: Learning Dynamics of Neural Network with Normalization, Weight Decay, and SGD
- **Arxiv ID**: http://arxiv.org/abs/2006.08419v4
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.08419v4)
- **Published**: 2020-06-15 14:16:33+00:00
- **Updated**: 2020-11-27 06:10:50+00:00
- **Authors**: Ruosi Wan, Zhanxing Zhu, Xiangyu Zhang, Jian Sun
- **Comment**: Theoretical analysis on joint effect of normalization and weight
  decay
- **Journal**: None
- **Summary**: In this work, we comprehensively reveal the learning dynamics of neural network with normalization, weight decay (WD), and SGD (with momentum), named as Spherical Motion Dynamics (SMD). Most related works study SMD by focusing on "effective learning rate" in "equilibrium" condition, where weight norm remains unchanged. However, their discussions on why equilibrium condition can be reached in SMD is either absent or less convincing. Our work investigates SMD by directly exploring the cause of equilibrium condition. Specifically, 1) we introduce the assumptions that can lead to equilibrium condition in SMD, and prove that weight norm can converge at linear rate with given assumptions; 2) we propose "angular update" as a substitute for effective learning rate to measure the evolving of neural network in SMD, and prove angular update can also converge to its theoretical value at linear rate; 3) we verify our assumptions and theoretical results on various computer vision tasks including ImageNet and MSCOCO with standard settings. Experiment results show our theoretical findings agree well with empirical observations.



### SD-RSIC: Summarization Driven Deep Remote Sensing Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/2006.08432v2
- **DOI**: 10.1109/TGRS.2020.3031111
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.08432v2)
- **Published**: 2020-06-15 14:29:12+00:00
- **Updated**: 2020-10-13 10:09:15+00:00
- **Authors**: Gencer Sumbul, Sonali Nayak, Begüm Demir
- **Comment**: Accepted in the IEEE Transactions on Geoscience and Remote Sensing.
  For code visit: https://gitlab.tubit.tu-berlin.de/rsim/SD-RSIC
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) have been recently found popular for image captioning problems in remote sensing (RS). Existing DNN based approaches rely on the availability of a training set made up of a high number of RS images with their captions. However, captions of training images may contain redundant information (they can be repetitive or semantically similar to each other), resulting in information deficiency while learning a mapping from the image domain to the language domain. To overcome this limitation, in this paper, we present a novel Summarization Driven Remote Sensing Image Captioning (SD-RSIC) approach. The proposed approach consists of three main steps. The first step obtains the standard image captions by jointly exploiting convolutional neural networks (CNNs) with long short-term memory (LSTM) networks. The second step, unlike the existing RS image captioning methods, summarizes the ground-truth captions of each training image into a single caption by exploiting sequence to sequence neural networks and eliminates the redundancy present in the training set. The third step automatically defines the adaptive weights associated to each RS image to combine the standard captions with the summarized captions based on the semantic content of the image. This is achieved by a novel adaptive weighting strategy defined in the context of LSTM networks. Experimental results obtained on the RSCID, UCM-Captions and Sydney-Captions datasets show the effectiveness of the proposed approach compared to the state-of-the-art RS image captioning approaches. The code of the proposed approach is publicly available at https://gitlab.tubit.tu-berlin.de/rsim/SD-RSIC.



### 3D-ZeF: A 3D Zebrafish Tracking Benchmark Dataset
- **Arxiv ID**: http://arxiv.org/abs/2006.08466v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.08466v1)
- **Published**: 2020-06-15 15:15:20+00:00
- **Updated**: 2020-06-15 15:15:20+00:00
- **Authors**: Malte Pedersen, Joakim Bruslund Haurum, Stefan Hein Bengtson, Thomas B. Moeslund
- **Comment**: CVPR 2020. Project webpage: https://vap.aau.dk/3d-zef/
- **Journal**: None
- **Summary**: In this work we present a novel publicly available stereo based 3D RGB dataset for multi-object zebrafish tracking, called 3D-ZeF. Zebrafish is an increasingly popular model organism used for studying neurological disorders, drug addiction, and more. Behavioral analysis is often a critical part of such research. However, visual similarity, occlusion, and erratic movement of the zebrafish makes robust 3D tracking a challenging and unsolved problem. The proposed dataset consists of eight sequences with a duration between 15-120 seconds and 1-10 free moving zebrafish. The videos have been annotated with a total of 86,400 points and bounding boxes. Furthermore, we present a complexity score and a novel open-source modular baseline system for 3D tracking of zebrafish. The performance of the system is measured with respect to two detectors: a naive approach and a Faster R-CNN based fish head detector. The system reaches a MOTA of up to 77.6%. Links to the code and dataset is available at the project page https://vap.aau.dk/3d-zef



### Towards Incorporating Contextual Knowledge into the Prediction of Driving Behavior
- **Arxiv ID**: http://arxiv.org/abs/2006.08470v2
- **DOI**: 10.1109/ITSC45102.2020.9294665
- **Categories**: **cs.CV**, cs.LG, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.08470v2)
- **Published**: 2020-06-15 15:21:02+00:00
- **Updated**: 2020-07-04 14:41:43+00:00
- **Authors**: Florian Wirthmüller, Julian Schlechtriemen, Jochen Hipp, Manfred Reichert
- **Comment**: the article has been accepted for publication during the 23rd IEEE
  Intelligent Transportation Systems Conference (ITSC), 7 pages, 6 figures, 1
  table
- **Journal**: None
- **Summary**: Predicting the behavior of surrounding traffic participants is crucial for advanced driver assistance systems and autonomous driving. Most researchers however do not consider contextual knowledge when predicting vehicle motion. Extending former studies, we investigate how predictions are affected by external conditions. To do so, we categorize different kinds of contextual information and provide a carefully chosen definition as well as examples for external conditions. More precisely, we investigate how a state-of-the-art approach for lateral motion prediction is influenced by one selected external condition, namely the traffic density. Our investigations demonstrate that this kind of information is highly relevant in order to improve the performance of prediction algorithms. Therefore, this study constitutes the first step towards the integration of such information into automated vehicles. Moreover, our motion prediction approach is evaluated based on the public highD data set showing a maneuver prediction performance with areas under the ROC curve above 97% and a median lateral prediction error of only 0.18m on a prediction horizon of 5s.



### APQ: Joint Search for Network Architecture, Pruning and Quantization Policy
- **Arxiv ID**: http://arxiv.org/abs/2006.08509v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.08509v1)
- **Published**: 2020-06-15 16:09:17+00:00
- **Updated**: 2020-06-15 16:09:17+00:00
- **Authors**: Tianzhe Wang, Kuan Wang, Han Cai, Ji Lin, Zhijian Liu, Song Han
- **Comment**: Accepted by CVPR 2020
- **Journal**: None
- **Summary**: We present APQ for efficient deep learning inference on resource-constrained hardware. Unlike previous methods that separately search the neural architecture, pruning policy, and quantization policy, we optimize them in a joint manner. To deal with the larger design space it brings, a promising approach is to train a quantization-aware accuracy predictor to quickly get the accuracy of the quantized model and feed it to the search engine to select the best fit. However, training this quantization-aware accuracy predictor requires collecting a large number of quantized <model, accuracy> pairs, which involves quantization-aware finetuning and thus is highly time-consuming. To tackle this challenge, we propose to transfer the knowledge from a full-precision (i.e., fp32) accuracy predictor to the quantization-aware (i.e., int8) accuracy predictor, which greatly improves the sample efficiency. Besides, collecting the dataset for the fp32 accuracy predictor only requires to evaluate neural networks without any training cost by sampling from a pretrained once-for-all network, which is highly efficient. Extensive experiments on ImageNet demonstrate the benefits of our joint optimization approach. With the same accuracy, APQ reduces the latency/energy by 2x/1.3x over MobileNetV2+HAQ. Compared to the separate optimization approach (ProxylessNAS+AMC+HAQ), APQ achieves 2.3% higher ImageNet accuracy while reducing orders of magnitude GPU hours and CO2 emission, pushing the frontier for green AI that is environmental-friendly. The code and video are publicly available.



### The Limit of the Batch Size
- **Arxiv ID**: http://arxiv.org/abs/2006.08517v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.DC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.08517v1)
- **Published**: 2020-06-15 16:18:05+00:00
- **Updated**: 2020-06-15 16:18:05+00:00
- **Authors**: Yang You, Yuhui Wang, Huan Zhang, Zhao Zhang, James Demmel, Cho-Jui Hsieh
- **Comment**: None
- **Journal**: None
- **Summary**: Large-batch training is an efficient approach for current distributed deep learning systems. It has enabled researchers to reduce the ImageNet/ResNet-50 training from 29 hours to around 1 minute. In this paper, we focus on studying the limit of the batch size. We think it may provide a guidance to AI supercomputer and algorithm designers. We provide detailed numerical optimization instructions for step-by-step comparison. Moreover, it is important to understand the generalization and optimization performance of huge batch training. Hoffer et al. introduced "ultra-slow diffusion" theory to large-batch training. However, our experiments show contradictory results with the conclusion of Hoffer et al. We provide comprehensive experimental results and detailed analysis to study the limitations of batch size scaling and "ultra-slow diffusion" theory. For the first time we scale the batch size on ImageNet to at least a magnitude larger than all previous work, and provide detailed studies on the performance of many state-of-the-art optimization schemes under this setting. We propose an optimization recipe that is able to improve the top-1 test accuracy by 18% compared to the baseline.



### Domain Adaptation with Joint Learning for Generic, Optical Car Part Recognition and Detection Systems (Go-CaRD)
- **Arxiv ID**: http://arxiv.org/abs/2006.08521v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM, H.1.2; I.4.8; I.4.9; J.2; J.4
- **Links**: [PDF](http://arxiv.org/pdf/2006.08521v2)
- **Published**: 2020-06-15 16:28:53+00:00
- **Updated**: 2021-02-15 21:23:49+00:00
- **Authors**: Lukas Stappen, Xinchen Du, Vincent Karas, Stefan Müller, Björn W. Schuller
- **Comment**: Demonstration and instructions to obtain data and models:
  https://github.com/lstappen/GoCarD
- **Journal**: None
- **Summary**: Systems for the automatic recognition and detection of automotive parts are crucial in several emerging research areas in the development of intelligent vehicles. They enable, for example, the detection and modelling of interactions between human and the vehicle. In this paper, we quantitatively and qualitatively explore the efficacy of deep learning architectures for the classification and localisation of 29 interior and exterior vehicle regions on three novel datasets. Furthermore, we experiment with joint and transfer learning approaches across datasets and point out potential applications of our systems. Our best network architecture achieves an F1 score of 93.67 % for recognition, while our best localisation approach utilising state-of-the-art backbone networks achieve a mAP of 63.01 % for detection. The MuSe-CAR-Part dataset, which is based on a large variety of human-car interactions in videos, the weights of the best models, and the code is publicly available to academic parties for benchmarking and future research.



### Improved Conditional Flow Models for Molecule to Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2006.08532v1
- **DOI**: None
- **Categories**: **q-bio.BM**, cs.CV, cs.LG, eess.IV, q-bio.QM, 92-08
- **Links**: [PDF](http://arxiv.org/pdf/2006.08532v1)
- **Published**: 2020-06-15 16:39:50+00:00
- **Updated**: 2020-06-15 16:39:50+00:00
- **Authors**: Karren Yang, Samuel Goldman, Wengong Jin, Alex Lu, Regina Barzilay, Tommi Jaakkola, Caroline Uhler
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we aim to synthesize cell microscopy images under different molecular interventions, motivated by practical applications to drug development. Building on the recent success of graph neural networks for learning molecular embeddings and flow-based models for image generation, we propose Mol2Image: a flow-based generative model for molecule to cell image synthesis. To generate cell features at different resolutions and scale to high-resolution images, we develop a novel multi-scale flow architecture based on a Haar wavelet image pyramid. To maximize the mutual information between the generated images and the molecular interventions, we devise a training strategy based on contrastive learning. To evaluate our model, we propose a new set of metrics for biological image generation that are robust, interpretable, and relevant to practitioners. We show quantitatively that our method learns a meaningful embedding of the molecular intervention, which is translated into an image representation reflecting the biological effects of the intervention.



### Boosting Black-Box Attack with Partially Transferred Conditional Adversarial Distribution
- **Arxiv ID**: http://arxiv.org/abs/2006.08538v4
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.08538v4)
- **Published**: 2020-06-15 16:45:27+00:00
- **Updated**: 2021-03-18 08:56:09+00:00
- **Authors**: Yan Feng, Baoyuan Wu, Yanbo Fan, Li Liu, Zhifeng Li, Shutao Xia
- **Comment**: None
- **Journal**: None
- **Summary**: This work studies black-box adversarial attacks against deep neural networks (DNNs), where the attacker can only access the query feedback returned by the attacked DNN model, while other information such as model parameters or the training datasets are unknown. One promising approach to improve attack performance is utilizing the adversarial transferability between some white-box surrogate models and the target model (i.e., the attacked model). However, due to the possible differences on model architectures and training datasets between surrogate and target models, dubbed "surrogate biases", the contribution of adversarial transferability to improving the attack performance may be weakened. To tackle this issue, we innovatively propose a black-box attack method by developing a novel mechanism of adversarial transferability, which is robust to the surrogate biases. The general idea is transferring partial parameters of the conditional adversarial distribution (CAD) of surrogate models, while learning the untransferred parameters based on queries to the target model, to keep the flexibility to adjust the CAD of the target model on any new benign sample. Extensive experiments on benchmark datasets and attacking against real-world API demonstrate the superior attack performance of the proposed method.



### Visibility Guided NMS: Efficient Boosting of Amodal Object Detection in Crowded Traffic Scenes
- **Arxiv ID**: http://arxiv.org/abs/2006.08547v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.08547v1)
- **Published**: 2020-06-15 17:03:23+00:00
- **Updated**: 2020-06-15 17:03:23+00:00
- **Authors**: Nils Gählert, Niklas Hanselmann, Uwe Franke, Joachim Denzler
- **Comment**: Machine Learning for Autonomous Driving Workshop at the 33rd
  Conference on Neural Information Processing Systems (NeurIPS 2019),
  Vancouver, Canada
- **Journal**: None
- **Summary**: Object detection is an important task in environment perception for autonomous driving. Modern 2D object detection frameworks such as Yolo, SSD or Faster R-CNN predict multiple bounding boxes per object that are refined using Non-Maximum-Suppression (NMS) to suppress all but one bounding box. While object detection itself is fully end-to-end learnable and does not require any manual parameter selection, standard NMS is parametrized by an overlap threshold that has to be chosen by hand. In practice, this often leads to an inability of standard NMS strategies to distinguish different objects in crowded scenes in the presence of high mutual occlusion, e.g. for parked cars or crowds of pedestrians. Our novel Visibility Guided NMS (vg-NMS) leverages both pixel-based as well as amodal object detection paradigms and improves the detection performance especially for highly occluded objects with little computational overhead. We evaluate vg-NMS using KITTI, VIPER as well as the Synscapes dataset and show that it outperforms current state-of-the-art NMS.



### Now that I can see, I can improve: Enabling data-driven finetuning of CNNs on the edge
- **Arxiv ID**: http://arxiv.org/abs/2006.08554v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.08554v1)
- **Published**: 2020-06-15 17:16:45+00:00
- **Updated**: 2020-06-15 17:16:45+00:00
- **Authors**: Aditya Rajagopal, Christos-Savvas Bouganis
- **Comment**: Accepted for publication at CVPR2020 workshop - Efficient Deep
  Learning for Computer Vision
- **Journal**: None
- **Summary**: In today's world, a vast amount of data is being generated by edge devices that can be used as valuable training data to improve the performance of machine learning algorithms in terms of the achieved accuracy or to reduce the compute requirements of the model. However, due to user data privacy concerns as well as storage and communication bandwidth limitations, this data cannot be moved from the device to the data centre for further improvement of the model and subsequent deployment. As such there is a need for increased edge intelligence, where the deployed models can be fine-tuned on the edge, leading to improved accuracy and/or reducing the model's workload as well as its memory and power footprint. In the case of Convolutional Neural Networks (CNNs), both the weights of the network as well as its topology can be tuned to adapt to the data that it processes. This paper provides a first step towards enabling CNN finetuning on an edge device based on structured pruning. It explores the performance gains and costs of doing so and presents an extensible open-source framework that allows the deployment of such approaches on a wide range of network architectures and devices. The results show that on average, data-aware pruning with retraining can provide 10.2pp increased accuracy over a wide range of subsets, networks and pruning levels with a maximum improvement of 42.0pp over pruning and retraining in a manner agnostic to the data being processed by the network.



### Learning Diverse and Discriminative Representations via the Principle of Maximal Coding Rate Reduction
- **Arxiv ID**: http://arxiv.org/abs/2006.08558v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.IT, math.IT, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.08558v1)
- **Published**: 2020-06-15 17:23:55+00:00
- **Updated**: 2020-06-15 17:23:55+00:00
- **Authors**: Yaodong Yu, Kwan Ho Ryan Chan, Chong You, Chaobing Song, Yi Ma
- **Comment**: None
- **Journal**: None
- **Summary**: To learn intrinsic low-dimensional structures from high-dimensional data that most discriminate between classes, we propose the principle of Maximal Coding Rate Reduction ($\text{MCR}^2$), an information-theoretic measure that maximizes the coding rate difference between the whole dataset and the sum of each individual class. We clarify its relationships with most existing frameworks such as cross-entropy, information bottleneck, information gain, contractive and contrastive learning, and provide theoretical guarantees for learning diverse and discriminative features. The coding rate can be accurately computed from finite samples of degenerate subspace-like distributions and can learn intrinsic representations in supervised, self-supervised, and unsupervised settings in a unified manner. Empirically, the representations learned using this principle alone are significantly more robust to label corruptions in classification than those using cross-entropy, and can lead to state-of-the-art results in clustering mixed data from self-learned invariant features.



### Spectral DiffuserCam: lensless snapshot hyperspectral imaging with a spectral filter array
- **Arxiv ID**: http://arxiv.org/abs/2006.08565v2
- **DOI**: 10.1364/OPTICA.397214
- **Categories**: **eess.IV**, cs.CV, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/2006.08565v2)
- **Published**: 2020-06-15 17:31:17+00:00
- **Updated**: 2020-09-29 03:21:47+00:00
- **Authors**: Kristina Monakhova, Kyrollos Yanny, Neerja Aggarwal, Laura Waller
- **Comment**: 10 pages, 10 figures, Optica
- **Journal**: Optica 7, 1298-1307 (2020)
- **Summary**: Hyperspectral imaging is useful for applications ranging from medical diagnostics to agricultural crop monitoring; however, traditional scanning hyperspectral imagers are prohibitively slow and expensive for widespread adoption. Snapshot techniques exist but are often confined to bulky benchtop setups or have low spatio-spectral resolution. In this paper, we propose a novel, compact, and inexpensive computational camera for snapshot hyperspectral imaging. Our system consists of a tiled spectral filter array placed directly on the image sensor and a diffuser placed close to the sensor. Each point in the world maps to a unique pseudorandom pattern on the spectral filter array, which encodes multiplexed spatio-spectral information. By solving a sparsity-constrained inverse problem, we recover the hyperspectral volume with sub-super-pixel resolution. Our hyperspectral imaging framework is flexible and can be designed with contiguous or non-contiguous spectral filters that can be chosen for a given application. We provide theory for system design, demonstrate a prototype device, and present experimental results with high spatio-spectral resolution.



### Coherent Reconstruction of Multiple Humans from a Single Image
- **Arxiv ID**: http://arxiv.org/abs/2006.08586v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.08586v1)
- **Published**: 2020-06-15 17:51:45+00:00
- **Updated**: 2020-06-15 17:51:45+00:00
- **Authors**: Wen Jiang, Nikos Kolotouros, Georgios Pavlakos, Xiaowei Zhou, Kostas Daniilidis
- **Comment**: CVPR 2020. Project Page: https://jiangwenpl.github.io/multiperson/
- **Journal**: None
- **Summary**: In this work, we address the problem of multi-person 3D pose estimation from a single image. A typical regression approach in the top-down setting of this problem would first detect all humans and then reconstruct each one of them independently. However, this type of prediction suffers from incoherent results, e.g., interpenetration and inconsistent depth ordering between the people in the scene. Our goal is to train a single network that learns to avoid these problems and generate a coherent 3D reconstruction of all the humans in the scene. To this end, a key design choice is the incorporation of the SMPL parametric body model in our top-down framework, which enables the use of two novel losses. First, a distance field-based collision loss penalizes interpenetration among the reconstructed people. Second, a depth ordering-aware loss reasons about occlusions and promotes a depth ordering of people that leads to a rendering which is consistent with the annotated instance segmentation. This provides depth supervision signals to the network, even if the image has no explicit 3D annotations. The experiments show that our approach outperforms previous methods on standard 3D pose benchmarks, while our proposed losses enable more coherent reconstruction in natural images. The project website with videos, results, and code can be found at: https://jiangwenpl.github.io/multiperson



### Multiscale Deep Equilibrium Models
- **Arxiv ID**: http://arxiv.org/abs/2006.08656v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.08656v2)
- **Published**: 2020-06-15 18:07:44+00:00
- **Updated**: 2020-11-24 06:59:38+00:00
- **Authors**: Shaojie Bai, Vladlen Koltun, J. Zico Kolter
- **Comment**: NeurIPS 2020 Oral
- **Journal**: None
- **Summary**: We propose a new class of implicit networks, the multiscale deep equilibrium model (MDEQ), suited to large-scale and highly hierarchical pattern recognition domains. An MDEQ directly solves for and backpropagates through the equilibrium points of multiple feature resolutions simultaneously, using implicit differentiation to avoid storing intermediate states (and thus requiring only $O(1)$ memory consumption). These simultaneously-learned multi-resolution features allow us to train a single model on a diverse set of tasks and loss functions, such as using a single MDEQ to perform both image classification and semantic segmentation. We illustrate the effectiveness of this approach on two large-scale vision tasks: ImageNet classification and semantic segmentation on high-resolution images from the Cityscapes dataset. In both settings, MDEQs are able to match or exceed the performance of recent competitive computer vision models: the first time such performance and scale have been achieved by an implicit deep learning approach. The code and pre-trained models are at https://github.com/locuslab/mdeq .



### ESL: Entropy-guided Self-supervised Learning for Domain Adaptation in Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2006.08658v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.08658v1)
- **Published**: 2020-06-15 18:10:09+00:00
- **Updated**: 2020-06-15 18:10:09+00:00
- **Authors**: Antoine Saporta, Tuan-Hung Vu, Matthieu Cord, Patrick Pérez
- **Comment**: Accepted at the CVPR 2020 Workshop on Scalability in Autonomous
  Driving
- **Journal**: None
- **Summary**: While fully-supervised deep learning yields good models for urban scene semantic segmentation, these models struggle to generalize to new environments with different lighting or weather conditions for instance. In addition, producing the extensive pixel-level annotations that the task requires comes at a great cost. Unsupervised domain adaptation (UDA) is one approach that tries to address these issues in order to make such systems more scalable. In particular, self-supervised learning (SSL) has recently become an effective strategy for UDA in semantic segmentation. At the core of such methods lies `pseudo-labeling', that is, the practice of assigning high-confident class predictions as pseudo-labels, subsequently used as true labels, for target data. To collect pseudo-labels, previous works often rely on the highest softmax score, which we here argue as an unfavorable confidence measurement.   In this work, we propose Entropy-guided Self-supervised Learning (ESL), leveraging entropy as the confidence indicator for producing more accurate pseudo-labels. On different UDA benchmarks, ESL consistently outperforms strong SSL baselines and achieves state-of-the-art results.



### Predicting Livelihood Indicators from Community-Generated Street-Level Imagery
- **Arxiv ID**: http://arxiv.org/abs/2006.08661v6
- **DOI**: None
- **Categories**: **cs.CV**, I.2; I.4; K.4; E.1
- **Links**: [PDF](http://arxiv.org/pdf/2006.08661v6)
- **Published**: 2020-06-15 18:12:12+00:00
- **Updated**: 2021-02-26 19:45:49+00:00
- **Authors**: Jihyeon Lee, Dylan Grosz, Burak Uzkent, Sicheng Zeng, Marshall Burke, David Lobell, Stefano Ermon
- **Comment**: Accepted to AAAI 2021. Code:
  https://github.com/sustainlab-group/mapillarygcn
- **Journal**: None
- **Summary**: Major decisions from governments and other large organizations rely on measurements of the populace's well-being, but making such measurements at a broad scale is expensive and thus infrequent in much of the developing world. We propose an inexpensive, scalable, and interpretable approach to predict key livelihood indicators from public crowd-sourced street-level imagery. Such imagery can be cheaply collected and more frequently updated compared to traditional surveying methods, while containing plausibly relevant information for a range of livelihood indicators. We propose two approaches to learn from the street-level imagery: (1) a method that creates multi-household cluster representations by detecting informative objects and (2) a graph-based approach that captures the relationships between images. By visualizing what features are important to a model and how they are used, we can help end-user organizations understand the models and offer an alternate approach for index estimation that uses cheaply obtained roadway features. By comparing our results against ground data collected in nationally-representative household surveys, we demonstrate the performance of our approach in accurately predicting indicators of poverty, population, and health and its scalability by testing in two different countries, India and Kenya. Our code is available at https://github.com/sustainlab-group/mapillarygcn.



### Feature Space Saturation during Training
- **Arxiv ID**: http://arxiv.org/abs/2006.08679v5
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML, 68T07, I.2.6
- **Links**: [PDF](http://arxiv.org/pdf/2006.08679v5)
- **Published**: 2020-06-15 18:28:21+00:00
- **Updated**: 2021-11-22 14:11:35+00:00
- **Authors**: Mats L. Richter, Justin Shenk, Wolf Byttner, Anders Arpteg, Mikael Huss
- **Comment**: 45 pages, 41 figures; author order changed in v5 to reflect
  additional contribution; for code see http://github.com/MLRichter/phd-lab and
  http://github.com/delve-team/delve
- **Journal**: British Machine Vision Conference (BMVC) 2021
- **Summary**: We propose layer saturation - a simple, online-computable method for analyzing the information processing in neural networks. First, we show that a layer's output can be restricted to the eigenspace of its variance matrix without performance loss. We propose a computationally lightweight method for approximating the variance matrix during training. From the dimension of its lossless eigenspace we derive layer saturation - the ratio between the eigenspace dimension and layer width. We show that saturation seems to indicate which layers contribute to network performance. We demonstrate how to alter layer saturation in a neural network by changing network depth, filter sizes and input resolution. Furthermore, we show that well-chosen input resolution increases network performance by distributing the inference process more evenly across the network.



### Multi-Image Summarization: Textual Summary from a Set of Cohesive Images
- **Arxiv ID**: http://arxiv.org/abs/2006.08686v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.08686v1)
- **Published**: 2020-06-15 18:45:35+00:00
- **Updated**: 2020-06-15 18:45:35+00:00
- **Authors**: Nicholas Trieu, Sebastian Goodman, Pradyumna Narayana, Kazoo Sone, Radu Soricut
- **Comment**: 9 pages, 5 figures
- **Journal**: None
- **Summary**: Multi-sentence summarization is a well studied problem in NLP, while generating image descriptions for a single image is a well studied problem in Computer Vision. However, for applications such as image cluster labeling or web page summarization, summarizing a set of images is also a useful and challenging task. This paper proposes the new task of multi-image summarization, which aims to generate a concise and descriptive textual summary given a coherent set of input images. We propose a model that extends the image-captioning Transformer-based architecture for single image to multi-image. A dense average image feature aggregation network allows the model to focus on a coherent subset of attributes across the input images. We explore various input representations to the Transformer network and empirically show that aggregated image features are superior to individual image embeddings. We additionally show that the performance of the model is further improved by pretraining the model parameters on a single-image captioning task, which appears to be particularly effective in eliminating hallucinations in the output.



### DeshuffleGAN: A Self-Supervised GAN to Improve Structure Learning
- **Arxiv ID**: http://arxiv.org/abs/2006.08694v1
- **DOI**: 10.1109/ICIP40778.2020.9190774
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.08694v1)
- **Published**: 2020-06-15 19:06:07+00:00
- **Updated**: 2020-06-15 19:06:07+00:00
- **Authors**: Gulcin Baykal, Gozde Unal
- **Comment**: Accepted at ICIP 2020
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) triggered an increased interest in problem of image generation due to their improved output image quality and versatility for expansion towards new methods. Numerous GAN-based works attempt to improve generation by architectural and loss-based extensions. We argue that one of the crucial points to improve the GAN performance in terms of realism and similarity to the original data distribution is to be able to provide the model with a capability to learn the spatial structure in data. To that end, we propose the DeshuffleGAN to enhance the learning of the discriminator and the generator, via a self-supervision approach. Specifically, we introduce a deshuffling task that solves a puzzle of randomly shuffled image tiles, which in turn helps the DeshuffleGAN learn to increase its expressive capacity for spatial structure and realistic appearance. We provide experimental evidence for the performance improvement in generated images, compared to the baseline methods, which is consistently observed over two different datasets.



### Unsupervised Domain Adaptation for Semantic Segmentation of NIR Images through Generative Latent Search
- **Arxiv ID**: http://arxiv.org/abs/2006.08696v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.08696v2)
- **Published**: 2020-06-15 19:07:55+00:00
- **Updated**: 2020-07-17 12:07:42+00:00
- **Authors**: Prashant Pandey, Aayush Kumar Tyagi, Sameer Ambekar, Prathosh AP
- **Comment**: ECCV 2020 [Spotlight]
- **Journal**: None
- **Summary**: Segmentation of the pixels corresponding to human skin is an essential first step in multiple applications ranging from surveillance to heart-rate estimation from remote-photoplethysmography. However, the existing literature considers the problem only in the visible-range of the EM-spectrum which limits their utility in low or no light settings where the criticality of the application is higher. To alleviate this problem, we consider the problem of skin segmentation from the Near-infrared images. However, Deep learning based state-of-the-art segmentation techniques demands large amounts of labelled data that is unavailable for the current problem. Therefore we cast the skin segmentation problem as that of target-independent Unsupervised Domain Adaptation (UDA) where we use the data from the Red-channel of the visible-range to develop skin segmentation algorithm on NIR images. We propose a method for target-independent segmentation where the 'nearest-clone' of a target image in the source domain is searched and used as a proxy in the segmentation network trained only on the source domain. We prove the existence of 'nearest-clone' and propose a method to find it through an optimization algorithm over the latent space of a Deep generative model based on variational inference. We demonstrate the efficacy of the proposed method for NIR skin segmentation over the state-of-the-art UDA segmentation methods on the two newly created skin segmentation datasets in NIR domain despite not having access to the target NIR data. Additionally, we report state-of-the-art results for adaption from Synthia to Cityscapes which is a popular setting in Unsupervised Domain Adaptation for semantic segmentation. The code and datasets are available at https://github.com/ambekarsameer96/GLSS.



### HyperFlow: Representing 3D Objects as Surfaces
- **Arxiv ID**: http://arxiv.org/abs/2006.08710v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.08710v1)
- **Published**: 2020-06-15 19:18:02+00:00
- **Updated**: 2020-06-15 19:18:02+00:00
- **Authors**: Przemysław Spurek, Maciej Zięba, Jacek Tabor, Tomasz Trzciński
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we present HyperFlow - a novel generative model that leverages hypernetworks to create continuous 3D object representations in a form of lightweight surfaces (meshes), directly out of point clouds. Efficient object representations are essential for many computer vision applications, including robotic manipulation and autonomous driving. However, creating those representations is often cumbersome, because it requires processing unordered sets of point clouds. Therefore, it is either computationally expensive, due to additional optimization constraints such as permutation invariance, or leads to quantization losses introduced by binning point clouds into discrete voxels. Inspired by mesh-based representations of objects used in computer graphics, we postulate a fundamentally different approach and represent 3D objects as a family of surfaces. To that end, we devise a generative model that uses a hypernetwork to return the weights of a Continuous Normalizing Flows (CNF) target network. The goal of this target network is to map points from a probability distribution into a 3D mesh. To avoid numerical instability of the CNF on compact support distributions, we propose a new Spherical Log-Normal function which models density of 3D points around object surfaces mimicking noise introduced by 3D capturing devices. As a result, we obtain continuous mesh-based object representations that yield better qualitative results than competing approaches, while reducing training time by over an order of magnitude.



### Sky Optimization: Semantically aware image processing of skies in low-light photography
- **Arxiv ID**: http://arxiv.org/abs/2006.10172v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2006.10172v1)
- **Published**: 2020-06-15 20:19:12+00:00
- **Updated**: 2020-06-15 20:19:12+00:00
- **Authors**: Orly Liba, Longqi Cai, Yun-Ta Tsai, Elad Eban, Yair Movshovitz-Attias, Yael Pritch, Huizhong Chen, Jonathan T. Barron
- **Comment**: Published in Proceedings of the IEEE/CVF Conference on Computer
  Vision and Pattern Recognition Workshops. 2020
- **Journal**: None
- **Summary**: The sky is a major component of the appearance of a photograph, and its color and tone can strongly influence the mood of a picture. In nighttime photography, the sky can also suffer from noise and color artifacts. For this reason, there is a strong desire to process the sky in isolation from the rest of the scene to achieve an optimal look. In this work, we propose an automated method, which can run as a part of a camera pipeline, for creating accurate sky alpha-masks and using them to improve the appearance of the sky. Our method performs end-to-end sky optimization in less than half a second per image on a mobile device. We introduce a method for creating an accurate sky-mask dataset that is based on partially annotated images that are inpainted and refined by our modified weighted guided filter. We use this dataset to train a neural network for semantic sky segmentation. Due to the compute and power constraints of mobile devices, sky segmentation is performed at a low image resolution. Our modified weighted guided filter is used for edge-aware upsampling to resize the alpha-mask to a higher resolution. With this detailed mask we automatically apply post-processing steps to the sky in isolation, such as automatic spatially varying white-balance, brightness adjustments, contrast enhancement, and noise reduction.



### A Critical and Moving-Forward View on Quantum Image Processing
- **Arxiv ID**: http://arxiv.org/abs/2006.08747v1
- **DOI**: None
- **Categories**: **quant-ph**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.08747v1)
- **Published**: 2020-06-15 20:38:25+00:00
- **Updated**: 2020-06-15 20:38:25+00:00
- **Authors**: Fei Yan, Salvador E. Venegas-Andraca, Kaoru Hirota
- **Comment**: 16 pages, 4 figures. Under review
- **Journal**: None
- **Summary**: Physics and computer science have a long tradition of cross-fertilization. One of the latest outcomes of this mutually beneficial relationship is quantum information science, which comprises the study of information processing tasks that can be accomplished using quantum mechanical systems. Quantum Image Processing (QIMP) is an emergent field of quantum information science whose main goal is to strengthen our capacity for storing, processing, and retrieving visual information from images and video either by transitioning from digital to quantum paradigms or by complementing digital imaging with quantum techniques. The expectation is that harnessing the properties of quantum mechanical systems in QIMP will result in the realization of advanced technologies that will outperform, enhance or complement existing and upcoming digital technologies for image and video processing tasks.



### Towards Understanding the Effect of Leak in Spiking Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2006.08761v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.08761v1)
- **Published**: 2020-06-15 20:56:31+00:00
- **Updated**: 2020-06-15 20:56:31+00:00
- **Authors**: Sayeed Shafayet Chowdhury, Chankyu Lee, Kaushik Roy
- **Comment**: Sayeed Shafayet Chowdhury and Chankyu Lee contributed equally
- **Journal**: None
- **Summary**: Spiking Neural Networks (SNNs) are being explored to emulate the astounding capabilities of human brain that can learn and compute functions robustly and efficiently with noisy spiking activities. A variety of spiking neuron models have been proposed to resemble biological neuronal functionalities. With varying levels of bio-fidelity, these models often contain a leak path in their internal states, called membrane potentials. While the leaky models have been argued as more bioplausible, a comparative analysis between models with and without leak from a purely computational point of view demands attention. In this paper, we investigate the questions regarding the justification of leak and the pros and cons of using leaky behavior. Our experimental results reveal that leaky neuron model provides improved robustness and better generalization compared to models with no leak. However, leak decreases the sparsity of computation contrary to the common notion. Through a frequency domain analysis, we demonstrate the effect of leak in eliminating the high-frequency components from the input, thus enabling SNNs to be more robust against noisy spike-inputs.



### Total Deep Variation: A Stable Regularizer for Inverse Problems
- **Arxiv ID**: http://arxiv.org/abs/2006.08789v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NA, math.NA, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/2006.08789v1)
- **Published**: 2020-06-15 21:54:15+00:00
- **Updated**: 2020-06-15 21:54:15+00:00
- **Authors**: Erich Kobler, Alexander Effland, Karl Kunisch, Thomas Pock
- **Comment**: 30 pages, 12 figures. arXiv admin note: text overlap with
  arXiv:2001.05005
- **Journal**: None
- **Summary**: Various problems in computer vision and medical imaging can be cast as inverse problems. A frequent method for solving inverse problems is the variational approach, which amounts to minimizing an energy composed of a data fidelity term and a regularizer. Classically, handcrafted regularizers are used, which are commonly outperformed by state-of-the-art deep learning approaches. In this work, we combine the variational formulation of inverse problems with deep learning by introducing the data-driven general-purpose total deep variation regularizer. In its core, a convolutional neural network extracts local features on multiple scales and in successive blocks. This combination allows for a rigorous mathematical analysis including an optimal control formulation of the training problem in a mean-field setting and a stability analysis with respect to the initial values and the parameters of the regularizer. In addition, we experimentally verify the robustness against adversarial attacks and numerically derive upper bounds for the generalization error. Finally, we achieve state-of-the-art results for numerous imaging tasks.



### On the use of human reference data for evaluating automatic image descriptions
- **Arxiv ID**: http://arxiv.org/abs/2006.08792v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2006.08792v1)
- **Published**: 2020-06-15 21:57:27+00:00
- **Updated**: 2020-06-15 21:57:27+00:00
- **Authors**: Emiel van Miltenburg
- **Comment**: Originally presented as a (non-archival) poster at the VizWiz 2020
  workshop, collocated with CVPR 2020. See:
  https://vizwiz.org/workshops/2020-workshop/
- **Journal**: None
- **Summary**: Automatic image description systems are commonly trained and evaluated using crowdsourced, human-generated image descriptions. The best-performing system is then determined using some measure of similarity to the reference data (BLEU, Meteor, CIDER, etc). Thus, both the quality of the systems as well as the quality of the evaluation depends on the quality of the descriptions. As Section 2 will show, the quality of current image description datasets is insufficient. I argue that there is a need for more detailed guidelines that take into account the needs of visually impaired users, but also the feasibility of generating suitable descriptions. With high-quality data, evaluation of image description systems could use reference descriptions, but we should also look for alternatives.



### A study of the effect of the illumination model on the generation of synthetic training datasets
- **Arxiv ID**: http://arxiv.org/abs/2006.08819v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.NE, I.5.1; I.4.6; I.3.7
- **Links**: [PDF](http://arxiv.org/pdf/2006.08819v1)
- **Published**: 2020-06-15 23:22:24+00:00
- **Updated**: 2020-06-15 23:22:24+00:00
- **Authors**: Xin Zhang, Ning Jia, Ioannis Ivrissimtzis
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: The use of computer generated images to train Deep Neural Networks is a viable alternative to real images when the latter are scarce or expensive. In this paper, we study how the illumination model used by the rendering software affects the quality of the generated images. We created eight training sets, each one with a different illumination model, and tested them on three different network architectures, ResNet, U-Net and a combined architecture developed by us. The test set consisted of photos of 3D printed objects produced from the same CAD models used to generate the training set. The effect of the other parameters of the rendering process, such as textures and camera position, was randomized.   Our results show that the effect of the illumination model is important, comparable in significance to the network architecture. We also show that both light probes capturing natural environmental light, and modelled lighting environments, can give good results. In the case of light probes, we identified as two significant factors affecting performance the similarity between the light probe and the test environment, as well as the light probe's resolution. Regarding modelled lighting environment, similarity with the test environment was again identified as a significant factor.



### Cardiac Segmentation with Strong Anatomical Guarantees
- **Arxiv ID**: http://arxiv.org/abs/2006.08825v1
- **DOI**: 10.1109/TMI.2020.3003240
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.08825v1)
- **Published**: 2020-06-15 23:38:31+00:00
- **Updated**: 2020-06-15 23:38:31+00:00
- **Authors**: Nathan Painchaud, Youssef Skandarani, Thierry Judge, Olivier Bernard, Alain Lalande, Pierre-Marc Jodoin
- **Comment**: 11 pages, accepted for publication in IEEE TMI
- **Journal**: None
- **Summary**: Convolutional neural networks (CNN) have had unprecedented success in medical imaging and, in particular, in medical image segmentation. However, despite the fact that segmentation results are closer than ever to the inter-expert variability, CNNs are not immune to producing anatomically inaccurate segmentations, even when built upon a shape prior. In this paper, we present a framework for producing cardiac image segmentation maps that are guaranteed to respect pre-defined anatomical criteria, while remaining within the inter-expert variability. The idea behind our method is to use a well-trained CNN, have it process cardiac images, identify the anatomically implausible results and warp these results toward the closest anatomically valid cardiac shape. This warping procedure is carried out with a constrained variational autoencoder (cVAE) trained to learn a representation of valid cardiac shapes through a smooth, yet constrained, latent space. With this cVAE, we can project any implausible shape into the cardiac latent space and steer it toward the closest correct shape. We tested our framework on short-axis MRI as well as apical two and four-chamber view ultrasound images, two modalities for which cardiac shapes are drastically different. With our method, CNNs can now produce results that are both within the inter-expert variability and always anatomically plausible without having to rely on a shape prior.



