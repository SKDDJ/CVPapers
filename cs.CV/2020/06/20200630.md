# Arxiv Papers in cs.CV on 2020-06-30
### Object Detection Under Rainy Conditions for Autonomous Vehicles: A Review of State-of-the-Art and Emerging Techniques
- **Arxiv ID**: http://arxiv.org/abs/2006.16471v4
- **DOI**: 10.1109/MSP.2020.2984801
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.16471v4)
- **Published**: 2020-06-30 02:05:10+00:00
- **Updated**: 2021-02-12 02:16:15+00:00
- **Authors**: Mazin Hnewa, Hayder Radha
- **Comment**: None
- **Journal**: IEEE Signal Processing Magazine, vol. 38, no. 1, pp. 53-67, Jan.
  2021
- **Summary**: Advanced automotive active-safety systems, in general, and autonomous vehicles, in particular, rely heavily on visual data to classify and localize objects such as pedestrians, traffic signs and lights, and other nearby cars, to assist the corresponding vehicles maneuver safely in their environments. However, the performance of object detection methods could degrade rather significantly under challenging weather scenarios including rainy conditions. Despite major advancements in the development of deraining approaches, the impact of rain on object detection has largely been understudied, especially in the context of autonomous driving. The main objective of this paper is to present a tutorial on state-of-the-art and emerging techniques that represent leading candidates for mitigating the influence of rainy conditions on an autonomous vehicle's ability to detect objects. Our goal includes surveying and analyzing the performance of object detection methods trained and tested using visual data captured under clear and rainy conditions. Moreover, we survey and evaluate the efficacy and limitations of leading deraining approaches, deep-learning based domain adaptation, and image translation frameworks that are being considered for addressing the problem of object detection under rainy conditions. Experimental results of a variety of the surveyed techniques are presented as part of this tutorial.



### MSNet: A Multilevel Instance Segmentation Network for Natural Disaster Damage Assessment in Aerial Videos
- **Arxiv ID**: http://arxiv.org/abs/2006.16479v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.16479v2)
- **Published**: 2020-06-30 02:23:05+00:00
- **Updated**: 2020-12-31 23:06:25+00:00
- **Authors**: Xiaoyu Zhu, Junwei Liang, Alexander Hauptmann
- **Comment**: WACV 2021
- **Journal**: None
- **Summary**: In this paper, we study the problem of efficiently assessing building damage after natural disasters like hurricanes, floods or fires, through aerial video analysis. We make two main contributions. The first contribution is a new dataset, consisting of user-generated aerial videos from social media with annotations of instance-level building damage masks. This provides the first benchmark for quantitative evaluation of models to assess building damage using aerial videos. The second contribution is a new model, namely MSNet, which contains novel region proposal network designs and an unsupervised score refinement network for confidence score calibration in both bounding box and mask branches. We show that our model achieves state-of-the-art results compared to previous methods in our dataset. We will release our data, models and code.



### Method for the generation of depth images for view-based shape retrieval of 3D CAD model from partial point cloud
- **Arxiv ID**: http://arxiv.org/abs/2006.16500v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.16500v1)
- **Published**: 2020-06-30 03:18:16+00:00
- **Updated**: 2020-06-30 03:18:16+00:00
- **Authors**: Hyungki Kim, Moohyun Cha, Duhwan Mun
- **Comment**: 39 pages
- **Journal**: None
- **Summary**: A laser scanner can easily acquire the geometric data of physical environments in the form of a point cloud. Recognizing objects from a point cloud is often required for industrial 3D reconstruction, which should include not only geometry information but also semantic information. However, recognition process is often a bottleneck in 3D reconstruction because it requires expertise on domain knowledge and intensive labor. To address this problem, various methods have been developed to recognize objects by retrieving the corresponding model in the database from an input geometry query. In recent years, the technique of converting geometric data into an image and applying view-based 3D shape retrieval has demonstrated high accuracy. Depth image which encodes depth value as intensity of pixel is frequently used for view-based 3D shape retrieval. However, geometric data collected from objects is often incomplete due to the occlusions and the limit of line of sight. Image generated by occluded point clouds lowers the performance of view-based 3D object retrieval due to loss of information. In this paper, we propose a method of viewpoint and image resolution estimation method for view-based 3D shape retrieval from point cloud query. Automatic selection of viewpoint and image resolution by calculating the data acquisition rate and density from the sampled viewpoints and image resolutions are proposed. The retrieval performance from the images generated by the proposed method is experimented and compared for various dataset. Additionally, view-based 3D shape retrieval performance with deep convolutional neural network has been experimented with the proposed method.



### Vehicle Re-ID for Surround-view Camera System
- **Arxiv ID**: http://arxiv.org/abs/2006.16503v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.16503v1)
- **Published**: 2020-06-30 03:25:10+00:00
- **Updated**: 2020-06-30 03:25:10+00:00
- **Authors**: Zizhang Wu, Man Wang, Lingxiao Yin, Weiwei Sun, Jason Wang, Huangbin Wu
- **Comment**: CVPR 2020 workshop on Scalability in Autonomous Driving
- **Journal**: None
- **Summary**: The vehicle re-identification (ReID) plays a critical role in the perception system of autonomous driving, which attracts more and more attention in recent years. However, to our best knowledge, there is no existing complete solution for the surround-view system mounted on the vehicle. In this paper, we argue two main challenges in above scenario: i) In single camera view, it is difficult to recognize the same vehicle from the past image frames due to the fisheye distortion, occlusion, truncation, etc. ii) In multi-camera view, the appearance of the same vehicle varies greatly from different camera's viewpoints. Thus, we present an integral vehicle Re-ID solution to address these problems. Specifically, we propose a novel quality evaluation mechanism to balance the effect of tracking box's drift and target's consistency. Besides, we take advantage of the Re-ID network based on attention mechanism, then combined with a spatial constraint strategy to further boost the performance between different cameras. The experiments demonstrate that our solution achieves state-of-the-art accuracy while being real-time in practice. Besides, we will release the code and annotated fisheye dataset for the benefit of community.



### Uniform Priors for Data-Efficient Transfer
- **Arxiv ID**: http://arxiv.org/abs/2006.16524v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.16524v2)
- **Published**: 2020-06-30 04:39:36+00:00
- **Updated**: 2020-10-13 12:21:07+00:00
- **Authors**: Samarth Sinha, Karsten Roth, Anirudh Goyal, Marzyeh Ghassemi, Hugo Larochelle, Animesh Garg
- **Comment**: None
- **Journal**: None
- **Summary**: Deep Neural Networks have shown great promise on a variety of downstream applications; but their ability to adapt and generalize to new data and tasks remains a challenge. However, the ability to perform few or zero-shot adaptation to novel tasks is important for the scalability and deployment of machine learning models. It is therefore crucial to understand what makes for good, transfer-able features in deep networks that best allow for such adaptation. In this paper, we shed light on this by showing that features that are most transferable have high uniformity in the embedding space and propose a uniformity regularization scheme that encourages better transfer and feature reuse. We evaluate the regularization on its ability to facilitate adaptation to unseen tasks and data, for which we conduct a thorough experimental study covering four relevant, and distinct domains: few-shot Meta-Learning, Deep Metric Learning, Zero-Shot Domain Adaptation, as well as Out-of-Distribution classification. Across all experiments, we show that uniformity regularization consistently offers benefits over baseline methods and is able to achieve state-of-the-art performance in Deep Metric Learning and Meta-Learning.



### Actionable Attribution Maps for Scientific Machine Learning
- **Arxiv ID**: http://arxiv.org/abs/2006.16533v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.16533v1)
- **Published**: 2020-06-30 05:12:29+00:00
- **Updated**: 2020-06-30 05:12:29+00:00
- **Authors**: Shusen Liu, Bhavya Kailkhura, Jize Zhang, Anna M. Hiszpanski, Emily Robertson, Donald Loveland, T. Yong-Jin Han
- **Comment**: None
- **Journal**: None
- **Summary**: The scientific community has been increasingly interested in harnessing the power of deep learning to solve various domain challenges. However, despite the effectiveness in building predictive models, fundamental challenges exist in extracting actionable knowledge from the deep neural network due to their opaque nature. In this work, we propose techniques for exploring the behavior of deep learning models by injecting domain-specific actionable concepts as tunable ``knobs'' in the analysis pipeline. By incorporating the domain knowledge with generative modeling, we are not only able to better understand the behavior of these black-box models, but also provide scientists with actionable insights that can potentially lead to fundamental discoveries.



### Theory-Inspired Path-Regularized Differential Network Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/2006.16537v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, math.OC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.16537v2)
- **Published**: 2020-06-30 05:28:23+00:00
- **Updated**: 2020-10-12 12:12:55+00:00
- **Authors**: Pan Zhou, Caiming Xiong, Richard Socher, Steven C. H. Hoi
- **Comment**: NeurIPS 2020 (oral)
- **Journal**: None
- **Summary**: Despite its high search efficiency, differential architecture search (DARTS) often selects network architectures with dominated skip connections which lead to performance degradation. However, theoretical understandings on this issue remain absent, hindering the development of more advanced methods in a principled way. In this work, we solve this problem by theoretically analyzing the effects of various types of operations, e.g. convolution, skip connection and zero operation, to the network optimization. We prove that the architectures with more skip connections can converge faster than the other candidates, and thus are selected by DARTS. This result, for the first time, theoretically and explicitly reveals the impact of skip connections to fast network optimization and its competitive advantage over other types of operations in DARTS. Then we propose a theory-inspired path-regularized DARTS that consists of two key modules: (i) a differential group-structured sparse binary gate introduced for each operation to avoid unfair competition among operations, and (ii) a path-depth-wise regularization used to incite search exploration for deep architectures that often converge slower than shallow ones as shown in our theory and are not well explored during the search. Experimental results on image classification tasks validate its advantages.



### Hand-drawn Symbol Recognition of Surgical Flowsheet Graphs with Deep Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2006.16546v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.16546v1)
- **Published**: 2020-06-30 05:56:59+00:00
- **Updated**: 2020-06-30 05:56:59+00:00
- **Authors**: William Adorno III, Angela Yi, Marcel Durieux, Donald Brown
- **Comment**: 8 pages, 4 figures, BioInformatics And BioEngineering 2020
- **Journal**: None
- **Summary**: Perioperative data are essential to investigating the causes of adverse surgical outcomes. In some low to middle income countries, these data are computationally inaccessible due to a lack of digitization of surgical flowsheets. In this paper, we present a deep image segmentation approach using a U-Net architecture that can detect hand-drawn symbols on a flowsheet graph. The segmentation mask outputs are post-processed with techniques unique to each symbol to convert into numeric values. The U-Net method can detect, at the appropriate time intervals, the symbols for heart rate and blood pressure with over 99 percent accuracy. Over 95 percent of the predictions fall within an absolute error of five when compared to the actual value. The deep learning model outperformed template matching even with a small size of annotated images available for the training set.



### Tackling Occlusion in Siamese Tracking with Structured Dropouts
- **Arxiv ID**: http://arxiv.org/abs/2006.16571v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.16571v1)
- **Published**: 2020-06-30 07:09:33+00:00
- **Updated**: 2020-06-30 07:09:33+00:00
- **Authors**: Deepak K. Gupta, Efstratios Gavves, Arnold W. M. Smeulders
- **Comment**: Accepted at ICPR2020
- **Journal**: None
- **Summary**: Occlusion is one of the most difficult challenges in object tracking to model. This is because unlike other challenges, where data augmentation can be of help, occlusion is hard to simulate as the occluding object can be anything in any shape. In this paper, we propose a simple solution to simulate the effects of occlusion in the latent space. Specifically, we present structured dropout to mimick the change in latent codes under occlusion. We present three forms of dropout (channel dropout, segment dropout and slice dropout) with the various forms of occlusion in mind. To demonstrate its effectiveness, the dropouts are incorporated into two modern Siamese trackers (SiamFC and SiamRPN++). The outputs from multiple dropouts are combined using an encoder network to obtain the final prediction. Experiments on several tracking benchmarks show the benefits of structured dropouts, while due to their simplicity requiring only small changes to the existing tracker models.



### Early Exit or Not: Resource-Efficient Blind Quality Enhancement for Compressed Images
- **Arxiv ID**: http://arxiv.org/abs/2006.16581v5
- **DOI**: 10.1007/978-3-030-58517-4_17
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.16581v5)
- **Published**: 2020-06-30 07:38:47+00:00
- **Updated**: 2020-10-12 08:23:58+00:00
- **Authors**: Qunliang Xing, Mai Xu, Tianyi Li, Zhenyu Guan
- **Comment**: Accepted by ECCV 2020. v5 updates: enlarge character size; correct
  titlerunning; add publishment reference; add open-sourced url
- **Journal**: None
- **Summary**: Lossy image compression is pervasively conducted to save communication bandwidth, resulting in undesirable compression artifacts. Recently, extensive approaches have been proposed to reduce image compression artifacts at the decoder side; however, they require a series of architecture-identical models to process images with different quality, which are inefficient and resource-consuming. Besides, it is common in practice that compressed images are with unknown quality and it is intractable for existing approaches to select a suitable model for blind quality enhancement. In this paper, we propose a resource-efficient blind quality enhancement (RBQE) approach for compressed images. Specifically, our approach blindly and progressively enhances the quality of compressed images through a dynamic deep neural network (DNN), in which an early-exit strategy is embedded. Then, our approach can automatically decide to terminate or continue enhancement according to the assessed quality of enhanced images. Consequently, slight artifacts can be removed in a simpler and faster process, while the severe artifacts can be further removed in a more elaborate process. Extensive experiments demonstrate that our RBQE approach achieves state-of-the-art performance in terms of both blind quality enhancement and resource efficiency. The code is available at https://github.com/RyanXingQL/RBQE.



### On the Demystification of Knowledge Distillation: A Residual Network Perspective
- **Arxiv ID**: http://arxiv.org/abs/2006.16589v1
- **DOI**: None
- **Categories**: **cs.CV**, I.5.1; I.5.1
- **Links**: [PDF](http://arxiv.org/pdf/2006.16589v1)
- **Published**: 2020-06-30 08:00:13+00:00
- **Updated**: 2020-06-30 08:00:13+00:00
- **Authors**: Nandan Kumar Jha, Rajat Saini, Sparsh Mittal
- **Comment**: None
- **Journal**: None
- **Summary**: Knowledge distillation (KD) is generally considered as a technique for performing model compression and learned-label smoothing. However, in this paper, we study and investigate the KD approach from a new perspective: we study its efficacy in training a deeper network without any residual connections. We find that in most of the cases, non-residual student networks perform equally or better than their residual versions trained on raw data without KD (baseline network). Surprisingly, in some cases, they surpass the accuracy of baseline networks even with the inferior teachers. After a certain depth of non-residual student network, the accuracy drop, coming from the removal of residual connections, is substantial, and training with KD boosts the accuracy of the student up to a great extent; however, it does not fully recover the accuracy drop. Furthermore, we observe that the conventional teacher-student view of KD is incomplete and does not adequately explain our findings. We propose a novel interpretation of KD with the Trainee-Mentor hypothesis, which provides a holistic view of KD. We also present two viewpoints, loss landscape, and feature reuse, to explain the interplay between residual connections and KD. We substantiate our claims through extensive experiments on residual networks.



### A Framework for Learning Invariant Physical Relations in Multimodal Sensory Processing
- **Arxiv ID**: http://arxiv.org/abs/2006.16607v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.16607v1)
- **Published**: 2020-06-30 08:42:48+00:00
- **Updated**: 2020-06-30 08:42:48+00:00
- **Authors**: Du Xiaorui, Yavuzhan Erdem, Immanuel Schweizer, Cristian Axenie
- **Comment**: None
- **Journal**: None
- **Summary**: Perceptual learning enables humans to recognize and represent stimuli invariant to various transformations and build a consistent representation of the self and physical world. Such representations preserve the invariant physical relations among the multiple perceived sensory cues. This work is an attempt to exploit these principles in an engineered system. We design a novel neural network architecture capable of learning, in an unsupervised manner, relations among multiple sensory cues. The system combines computational principles, such as competition, cooperation, and correlation, in a neurally plausible computational substrate. It achieves that through a parallel and distributed processing architecture in which the relations among the multiple sensory quantities are extracted from time-sequenced data. We describe the core system functionality when learning arbitrary non-linear relations in low-dimensional sensory data. Here, an initial benefit rises from the fact that such a network can be engineered in a relatively straightforward way without prior information about the sensors and their interactions. Moreover, alleviating the need for tedious modelling and parametrization, the network converges to a consistent description of any arbitrary high-dimensional multisensory setup. We demonstrate this through a real-world learning problem, where, from standard RGB camera frames, the network learns the relations between physical quantities such as light intensity, spatial gradient, and optical flow, describing a visual scene. Overall, the benefits of such a framework lie in the capability to learn non-linear pairwise relations among sensory streams in an architecture that is stable under noise and missing sensor input.



### A Simple Domain Shifting Networkfor Generating Low Quality Images
- **Arxiv ID**: http://arxiv.org/abs/2006.16621v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.16621v1)
- **Published**: 2020-06-30 09:19:54+00:00
- **Updated**: 2020-06-30 09:19:54+00:00
- **Authors**: Guruprasad Hegde, Avinash Nittur Ramesh, Kanchana Vaishnavi Gandikota, Roman Obermaisser, Michael Moeller
- **Comment**: accepted ICPR 2020
- **Journal**: None
- **Summary**: Deep Learning systems have proven to be extremely successful for image recognition tasks for which significant amounts of training data is available, e.g., on the famous ImageNet dataset. We demonstrate that for robotics applications with cheap camera equipment, the low image quality, however,influences the classification accuracy, and freely available databases cannot be exploited in a straight forward way to train classifiers to be used on a robot. As a solution we propose to train a network on degrading the quality images in order to mimic specific low quality imaging systems. Numerical experiments demonstrate that classification networks trained by using images produced by our quality degrading network along with the high quality images outperform classification networks trained only on high quality data when used on a real robot system, while being significantly easier to use than competing zero-shot domain adaptation techniques.



### BitMix: Data Augmentation for Image Steganalysis
- **Arxiv ID**: http://arxiv.org/abs/2006.16625v1
- **DOI**: 10.1049/el.2020.1951
- **Categories**: **eess.IV**, cs.CR, cs.CV, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2006.16625v1)
- **Published**: 2020-06-30 09:36:21+00:00
- **Updated**: 2020-06-30 09:36:21+00:00
- **Authors**: In-Jae Yu, Wonhyuk Ahn, Seung-Hun Nam, Heung-Kyu Lee
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional neural networks (CNN) for image steganalysis demonstrate better performances with employing concepts from high-level vision tasks. The major employed concept is to use data augmentation to avoid overfitting due to limited data. To augment data without damaging the message embedding, only rotating multiples of 90 degrees or horizontally flipping are used in steganalysis, which generates eight fixed results from one sample. To overcome this limitation, we propose BitMix, a data augmentation method for spatial image steganalysis. BitMix mixes a cover and stego image pair by swapping the random patch and generates an embedding adaptive label with the ratio of the number of pixels modified in the swapped patch to those in the cover-stego pair. We explore optimal hyperparameters, the ratio of applying BitMix in the mini-batch, and the size of the bounding box for swapping patch. The results reveal that using BitMix improves the performance of spatial image steganalysis and better than other data augmentation methods.



### Primary Tumor Origin Classification of Lung Nodules in Spectral CT using Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/2006.16633v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.16633v1)
- **Published**: 2020-06-30 09:56:18+00:00
- **Updated**: 2020-06-30 09:56:18+00:00
- **Authors**: Linde S. Hesse, Pim A. de Jong, Josien P. W. Pluim, Veronika Cheplygina
- **Comment**: MSc thesis Linde Hesse
- **Journal**: None
- **Summary**: Early detection of lung cancer has been proven to decrease mortality significantly. A recent development in computed tomography (CT), spectral CT, can potentially improve diagnostic accuracy, as it yields more information per scan than regular CT. However, the shear workload involved with analyzing a large number of scans drives the need for automated diagnosis methods. Therefore, we propose a detection and classification system for lung nodules in CT scans. Furthermore, we want to observe whether spectral images can increase classifier performance. For the detection of nodules we trained a VGG-like 3D convolutional neural net (CNN). To obtain a primary tumor classifier for our dataset we pre-trained a 3D CNN with similar architecture on nodule malignancies of a large publicly available dataset, the LIDC-IDRI dataset. Subsequently we used this pre-trained network as feature extractor for the nodules in our dataset. The resulting feature vectors were classified into two (benign/malignant) and three (benign/primary lung cancer/metastases) classes using support vector machine (SVM). This classification was performed both on nodule- and scan-level. We obtained state-of-the art performance for detection and malignancy regression on the LIDC-IDRI database. Classification performance on our own dataset was higher for scan- than for nodule-level predictions. For the three-class scan-level classification we obtained an accuracy of 78\%. Spectral features did increase classifier performance, but not significantly. Our work suggests that a pre-trained feature extractor can be used as primary tumor origin classifier for lung nodules, eliminating the need for elaborate fine-tuning of a new network and large datasets. Code is available at \url{https://github.com/tueimage/lung-nodule-msc-2018}.



### OccInpFlow: Occlusion-Inpainting Optical Flow Estimation by Unsupervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2006.16637v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.16637v1)
- **Published**: 2020-06-30 10:01:32+00:00
- **Updated**: 2020-06-30 10:01:32+00:00
- **Authors**: Kunming Luo, Chuan Wang, Nianjin Ye, Shuaicheng Liu, Jue Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Occlusion is an inevitable and critical problem in unsupervised optical flow learning. Existing methods either treat occlusions equally as non-occluded regions or simply remove them to avoid incorrectness. However, the occlusion regions can provide effective information for optical flow learning. In this paper, we present OccInpFlow, an occlusion-inpainting framework to make full use of occlusion regions. Specifically, a new appearance-flow network is proposed to inpaint occluded flows based on the image content. Moreover, a boundary warp is proposed to deal with occlusions caused by displacement beyond image border. We conduct experiments on multiple leading flow benchmark data sets such as Flying Chairs, KITTI and MPI-Sintel, which demonstrate that the performance is significantly improved by our proposed occlusion handling framework.



### Rethinking CNN-Based Pansharpening: Guided Colorization of Panchromatic Images via GANs
- **Arxiv ID**: http://arxiv.org/abs/2006.16644v1
- **DOI**: 10.1109/TGRS.2020.3010441
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.16644v1)
- **Published**: 2020-06-30 10:12:37+00:00
- **Updated**: 2020-06-30 10:12:37+00:00
- **Authors**: Furkan Ozcelik, Ugur Alganci, Elif Sertel, Gozde Unal
- **Comment**: F. Ozcelik, U. Alganci, E. Sertel, G. Unal, "Rethinking CNN-Based
  Pansharpening: Guided Colorization of Panchromatic Images via GANs", IEEE
  Transactions on Geoscience and Remote Sensing (TGRS), in press, 2020
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNN)-based approaches have shown promising results in pansharpening of satellite images in recent years. However, they still exhibit limitations in producing high-quality pansharpening outputs. To that end, we propose a new self-supervised learning framework, where we treat pansharpening as a colorization problem, which brings an entirely novel perspective and solution to the problem compared to existing methods that base their solution solely on producing a super-resolution version of the multispectral image. Whereas CNN-based methods provide a reduced resolution panchromatic image as input to their model along with reduced resolution multispectral images, hence learn to increase their resolution together, we instead provide the grayscale transformed multispectral image as input, and train our model to learn the colorization of the grayscale input. We further address the fixed downscale ratio assumption during training, which does not generalize well to the full-resolution scenario. We introduce a noise injection into the training by randomly varying the downsampling ratios. Those two critical changes, along with the addition of adversarial training in the proposed PanColorization Generative Adversarial Networks (PanColorGAN) framework, help overcome the spatial detail loss and blur problems that are observed in CNN-based pansharpening. The proposed approach outperforms the previous CNN-based and traditional methods as demonstrated in our experiments.



### EasyQuant: Post-training Quantization via Scale Optimization
- **Arxiv ID**: http://arxiv.org/abs/2006.16669v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.16669v1)
- **Published**: 2020-06-30 10:43:02+00:00
- **Updated**: 2020-06-30 10:43:02+00:00
- **Authors**: Di Wu, Qi Tang, Yongle Zhao, Ming Zhang, Ying Fu, Debing Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: The 8 bits quantization has been widely applied to accelerate network inference in various deep learning applications. There are two kinds of quantization methods, training-based quantization and post-training quantization. Training-based approach suffers from a cumbersome training process, while post-training quantization may lead to unacceptable accuracy drop. In this paper, we present an efficient and simple post-training method via scale optimization, named EasyQuant (EQ),that could obtain comparable accuracy with the training-based method.Specifically, we first alternately optimize scales of weights and activations for all layers target at convolutional outputs to further obtain the high quantization precision. Then, we lower down bit width to INT7 both for weights and activations, and adopt INT16 intermediate storage and integer Winograd convolution implementation to accelerate inference.Experimental results on various computer vision tasks show that EQ outperforms the TensorRT method and can achieve near INT8 accuracy in 7 bits width post-training.



### EndoSLAM Dataset and An Unsupervised Monocular Visual Odometry and Depth Estimation Approach for Endoscopic Videos: Endo-SfMLearner
- **Arxiv ID**: http://arxiv.org/abs/2006.16670v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.16670v3)
- **Published**: 2020-06-30 10:43:27+00:00
- **Updated**: 2020-10-01 13:44:32+00:00
- **Authors**: Kutsev Bengisu Ozyoruk, Guliz Irem Gokceler, Gulfize Coskun, Kagan Incetan, Yasin Almalioglu, Faisal Mahmood, Eva Curto, Luis Perdigoto, Marina Oliveira, Hasan Sahin, Helder Araujo, Henrique Alexandrino, Nicholas J. Durr, Hunter B. Gilbert, Mehmet Turan
- **Comment**: 27 pages, 16 figures
- **Journal**: None
- **Summary**: Deep learning techniques hold promise to develop dense topography reconstruction and pose estimation methods for endoscopic videos. However, currently available datasets do not support effective quantitative benchmarking. In this paper, we introduce a comprehensive endoscopic SLAM dataset consisting of 3D point cloud data for six porcine organs, capsule and standard endoscopy recordings as well as synthetically generated data. A Panda robotic arm, two commercially available capsule endoscopes, two conventional endoscopes with different camera properties, and two high precision 3D scanners were employed to collect data from 8 ex-vivo porcine gastrointestinal (GI)-tract organs. In total, 35 sub-datasets are provided with 6D pose ground truth for the ex-vivo part: 18 sub-dataset for colon, 12 sub-datasets for stomach and 5 sub-datasets for small intestine, while four of these contain polyp-mimicking elevations carried out by an expert gastroenterologist. Synthetic capsule endoscopy frames from GI-tract with both depth and pose annotations are included to facilitate the study of simulation-to-real transfer learning algorithms. Additionally, we propound Endo-SfMLearner, an unsupervised monocular depth and pose estimation method that combines residual networks with spatial attention module in order to dictate the network to focus on distinguishable and highly textured tissue regions. The proposed approach makes use of a brightness-aware photometric loss to improve the robustness under fast frame-to-frame illumination changes. To exemplify the use-case of the EndoSLAM dataset, the performance of Endo-SfMLearner is extensively compared with the state-of-the-art. The codes and the link for the dataset are publicly available at https://github.com/CapsuleEndoscope/EndoSLAM. A video demonstrating the experimental setup and procedure is accessible through https://www.youtube.com/watch?v=G_LCe0aWWdQ.



### Cross-Scale Internal Graph Neural Network for Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2006.16673v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.16673v2)
- **Published**: 2020-06-30 10:48:40+00:00
- **Updated**: 2020-10-20 13:59:29+00:00
- **Authors**: Shangchen Zhou, Jiawei Zhang, Wangmeng Zuo, Chen Change Loy
- **Comment**: NeurIPS 2020
- **Journal**: None
- **Summary**: Non-local self-similarity in natural images has been well studied as an effective prior in image restoration. However, for single image super-resolution (SISR), most existing deep non-local methods (e.g., non-local neural networks) only exploit similar patches within the same scale of the low-resolution (LR) input image. Consequently, the restoration is limited to using the same-scale information while neglecting potential high-resolution (HR) cues from other scales. In this paper, we explore the cross-scale patch recurrence property of a natural image, i.e., similar patches tend to recur many times across different scales. This is achieved using a novel cross-scale internal graph neural network (IGNN). Specifically, we dynamically construct a cross-scale graph by searching k-nearest neighboring patches in the downsampled LR image for each query patch in the LR image. We then obtain the corresponding k HR neighboring patches in the LR image and aggregate them adaptively in accordance to the edge label of the constructed graph. In this way, the HR information can be passed from k HR neighboring patches to the LR query patch to help it recover more detailed textures. Besides, these internal image-specific LR/HR exemplars are also significant complements to the external information learned from the training dataset. Extensive experiments demonstrate the effectiveness of IGNN against the state-of-the-art SISR methods including existing non-local networks on standard benchmarks.



### Needle tip force estimation by deep learning from raw spectral OCT data
- **Arxiv ID**: http://arxiv.org/abs/2006.16675v1
- **DOI**: 10.1007/s11548-020-02224-w
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.16675v1)
- **Published**: 2020-06-30 10:49:54+00:00
- **Updated**: 2020-06-30 10:49:54+00:00
- **Authors**: M. Gromniak, N. Gessert, T. Saathoff, A. Schlaefer
- **Comment**: Accepted for publication in IJCARS
- **Journal**: None
- **Summary**: Purpose. Needle placement is a challenging problem for applications such as biopsy or brachytherapy. Tip force sensing can provide valuable feedback for needle navigation inside the tissue. For this purpose, fiber-optical sensors can be directly integrated into the needle tip. Optical coherence tomography (OCT) can be used to image tissue. Here, we study how to calibrate OCT to sense forces, e.g. during robotic needle placement.   Methods. We investigate whether using raw spectral OCT data without a typical image reconstruction can improve a deep learning-based calibration between optical signal and forces. For this purpose, we consider three different needles with a new, more robust design which are calibrated using convolutional neural networks (CNNs). We compare training the CNNs with the raw OCT signal and the reconstructed depth profiles.   Results. We find that using raw data as an input for the largest CNN model outperforms the use of reconstructed data with a mean absolute error of 5.81 mN compared to 8.04 mN.   Conclusions. We find that deep learning with raw spectral OCT data can improve learning for the task of force estimation. Our needle design and calibration approach constitute a very accurate fiber-optical sensor for measuring forces at the needle tip.



### Classification Confidence Estimation with Test-Time Data-Augmentation
- **Arxiv ID**: http://arxiv.org/abs/2006.16705v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.16705v1)
- **Published**: 2020-06-30 11:59:53+00:00
- **Updated**: 2020-06-30 11:59:53+00:00
- **Authors**: Yuval Bahat, Gregory Shakhnarovich
- **Comment**: None
- **Journal**: None
- **Summary**: Machine learning plays an increasingly significant role in many aspects of our lives (including medicine, transportation, security, justice and other domains), making the potential consequences of false predictions increasingly devastating. These consequences may be mitigated if we can automatically flag such false predictions and potentially assign them to alternative, more reliable mechanisms, that are possibly more costly and involve human attention. This suggests the task of detecting errors, which we tackle in this paper for the case of visual classification. To this end, we propose a novel approach for classification confidence estimation. We apply a set of semantics-preserving image transformations to the input image, and show how the resulting image sets can be used to estimate confidence in the classifier's prediction. We demonstrate the potential of our approach by extensively evaluating it on a wide variety of classifier architectures and datasets, including ResNext/ImageNet, achieving state of the art performance. This paper constitutes a significant revision of our earlier work in this direction (Bahat & Shakhnarovich, 2018).



### Beyond accuracy: quantifying trial-by-trial behaviour of CNNs and humans by measuring error consistency
- **Arxiv ID**: http://arxiv.org/abs/2006.16736v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, q-bio.NC, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2006.16736v3)
- **Published**: 2020-06-30 12:47:17+00:00
- **Updated**: 2020-12-18 15:39:48+00:00
- **Authors**: Robert Geirhos, Kristof Meding, Felix A. Wichmann
- **Comment**: NeurIPS 2020 camera ready
- **Journal**: None
- **Summary**: A central problem in cognitive science and behavioural neuroscience as well as in machine learning and artificial intelligence research is to ascertain whether two or more decision makers (be they brains or algorithms) use the same strategy. Accuracy alone cannot distinguish between strategies: two systems may achieve similar accuracy with very different strategies. The need to differentiate beyond accuracy is particularly pressing if two systems are near ceiling performance, like Convolutional Neural Networks (CNNs) and humans on visual object recognition. Here we introduce trial-by-trial error consistency, a quantitative analysis for measuring whether two decision making systems systematically make errors on the same inputs. Making consistent errors on a trial-by-trial basis is a necessary condition for similar processing strategies between decision makers. Our analysis is applicable to compare algorithms with algorithms, humans with humans, and algorithms with humans. When applying error consistency to object recognition we obtain three main findings: (1.) Irrespective of architecture, CNNs are remarkably consistent with one another. (2.) The consistency between CNNs and human observers, however, is little above what can be expected by chance alone -- indicating that humans and CNNs are likely implementing very different strategies. (3.) CORnet-S, a recurrent model termed the "current best model of the primate ventral visual stream", fails to capture essential characteristics of human behavioural data and behaves essentially like a standard purely feedforward ResNet-50 in our analysis. Taken together, error consistency analysis suggests that the strategies used by human and machine vision are still very different -- but we envision our general-purpose error consistency analysis to serve as a fruitful tool for quantifying future progress.



### Image-level Harmonization of Multi-Site Data using Image-and-Spatial Transformer Networks
- **Arxiv ID**: http://arxiv.org/abs/2006.16741v1
- **DOI**: 10.1007/978-3-030-59728-3_69
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.16741v1)
- **Published**: 2020-06-30 12:58:41+00:00
- **Updated**: 2020-06-30 12:58:41+00:00
- **Authors**: R. Robinson, Q. Dou, D. C. Castro, K. Kamnitsas, M. de Groot, R. M. Summers, D. Rueckert, B. Glocker
- **Comment**: Accepted at MICCAI 2020
- **Journal**: Medical Image Computing and Computer-Assisted Intervention (2020),
  pp. 710-719, LNCS 12267
- **Summary**: We investigate the use of image-and-spatial transformer networks (ISTNs) to tackle domain shift in multi-site medical imaging data. Commonly, domain adaptation (DA) is performed with little regard for explainability of the inter-domain transformation and is often conducted at the feature-level in the latent space. We employ ISTNs for DA at the image-level which constrains transformations to explainable appearance and shape changes. As proof-of-concept we demonstrate that ISTNs can be trained adversarially on a classification problem with simulated 2D data. For real-data validation, we construct two 3D brain MRI datasets from the Cam-CAN and UK Biobank studies to investigate domain shift due to acquisition and population differences. We show that age regression and sex classification models trained on ISTN output improve generalization when training on data from one and testing on the other site.



### Large-scale inference of liver fat with neural networks on UK Biobank body MRI
- **Arxiv ID**: http://arxiv.org/abs/2006.16777v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.16777v1)
- **Published**: 2020-06-30 13:33:30+00:00
- **Updated**: 2020-06-30 13:33:30+00:00
- **Authors**: Taro Langner, Robin Strand, Håkan Ahlström, Joel Kullberg
- **Comment**: None
- **Journal**: None
- **Summary**: The UK Biobank Imaging Study has acquired medical scans of more than 40,000 volunteer participants. The resulting wealth of anatomical information has been made available for research, together with extensive metadata including measurements of liver fat. These values play an important role in metabolic disease, but are only available for a minority of imaged subjects as their collection requires the careful work of image analysts on dedicated liver MRI. Another UK Biobank protocol is neck-to-knee body MRI for analysis of body composition. The resulting volumes can also quantify fat fractions, even though they were reconstructed with a two- instead of a three-point Dixon technique. In this work, a novel framework for automated inference of liver fat from UK Biobank neck-to-knee body MRI is proposed. A ResNet50 was trained for regression on two-dimensional slices from these scans and the reference values as target, without any need for ground truth segmentations. Once trained, it performs fast, objective, and fully automated predictions that require no manual intervention. On the given data, it closely emulates the reference method, reaching a level of agreement comparable to different gold standard techniques. The network learned to rectify non-linearities in the fat fraction values and identified several outliers in the reference. It outperformed a multi-atlas segmentation baseline and inferred new estimates for all imaged subjects lacking reference values, expanding the total number of liver fat measurements by factor six.



### Leveraging Temporal Information for 3D Detection and Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2006.16796v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.16796v1)
- **Published**: 2020-06-30 13:47:28+00:00
- **Updated**: 2020-06-30 13:47:28+00:00
- **Authors**: Cunjun Yu, Zhongang Cai, Daxuan Ren, Haiyu Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: Ever since the prevalent use of the LiDARs in autonomous driving, tremendous improvements have been made to the learning on the point clouds. However, recent progress largely focuses on detecting objects in a single 360-degree sweep, without extensively exploring the temporal information. In this report, we describe a simple way to pass such information in the learning pipeline by adding timestamps to the point clouds, which shows consistent improvements across all three classes.



### You Only Look Yourself: Unsupervised and Untrained Single Image Dehazing Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2006.16829v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.16829v1)
- **Published**: 2020-06-30 14:05:47+00:00
- **Updated**: 2020-06-30 14:05:47+00:00
- **Authors**: Boyun Li, Yuanbiao Gou, Shuhang Gu, Jerry Zitao Liu, Joey Tianyi Zhou, Xi Peng
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we study two challenging and less-touched problems in single image dehazing, namely, how to make deep learning achieve image dehazing without training on the ground-truth clean image (unsupervised) and a image collection (untrained). An unsupervised neural network will avoid the intensive labor collection of hazy-clean image pairs, and an untrained model is a ``real'' single image dehazing approach which could remove haze based on only the observed hazy image itself and no extra images is used. Motivated by the layer disentanglement idea, we propose a novel method, called you only look yourself (\textbf{YOLY}) which could be one of the first unsupervised and untrained neural networks for image dehazing. In brief, YOLY employs three jointly subnetworks to separate the observed hazy image into several latent layers, \textit{i.e.}, scene radiance layer, transmission map layer, and atmospheric light layer. After that, these three layers are further composed to the hazy image in a self-supervised manner. Thanks to the unsupervised and untrained characteristics of YOLY, our method bypasses the conventional training paradigm of deep models on hazy-clean pairs or a large scale dataset, thus avoids the labor-intensive data collection and the domain shift issue. Besides, our method also provides an effective learning-based haze transfer solution thanks to its layer disentanglement mechanism. Extensive experiments show the promising performance of our method in image dehazing compared with 14 methods on four databases.



### Can Your Face Detector Do Anti-spoofing? Face Presentation Attack Detection with a Multi-Channel Face Detector
- **Arxiv ID**: http://arxiv.org/abs/2006.16836v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.16836v2)
- **Published**: 2020-06-30 14:22:46+00:00
- **Updated**: 2020-07-29 09:14:54+00:00
- **Authors**: Anjith George, Sebastien Marcel
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: In a typical face recognition pipeline, the task of the face detector is to localize the face region. However, the face detector localizes regions that look like a face, irrespective of the liveliness of the face, which makes the entire system susceptible to presentation attacks. In this work, we try to reformulate the task of the face detector to detect real faces, thus eliminating the threat of presentation attacks. While this task could be challenging with visible spectrum images alone, we leverage the multi-channel information available from off the shelf devices (such as color, depth, and infrared channels) to design a multi-channel face detector. The proposed system can be used as a live-face detector obviating the need for a separate presentation attack detection module, making the system reliable in practice without any additional computational overhead. The main idea is to leverage a single-stage object detection framework, with a joint representation obtained from different channels for the PAD task. We have evaluated our approach in the multi-channel WMCA dataset containing a wide variety of attacks to show the effectiveness of the proposed framework.



### Boosting Deep Neural Networks with Geometrical Prior Knowledge: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2006.16867v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.16867v2)
- **Published**: 2020-06-30 14:56:05+00:00
- **Updated**: 2023-03-03 15:36:34+00:00
- **Authors**: Matthias Rath, Alexandru Paul Condurache
- **Comment**: None
- **Journal**: None
- **Summary**: Deep Neural Networks achieve state-of-the-art results in many different problem settings by exploiting vast amounts of training data. However, collecting, storing and - in the case of supervised learning - labelling the data is expensive and time-consuming. Additionally, assessing the networks' generalization abilities or predicting how the inferred output changes under input transformations is complicated since the networks are usually treated as a black box. Both of these problems can be mitigated by incorporating prior knowledge into the neural network. One promising approach, inspired by the success of convolutional neural networks in computer vision tasks, is to incorporate knowledge about symmetric geometrical transformations of the problem to solve that affect the output in a predictable way. This promises an increased data efficiency and more interpretable network outputs. In this survey, we try to give a concise overview about different approaches that incorporate geometrical prior knowledge into neural networks. Additionally, we connect those methods to 3D object detection for autonomous driving, where we expect promising results when applying those methods.



### Predicting Sample Collision with Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2006.16868v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.16868v1)
- **Published**: 2020-06-30 14:56:14+00:00
- **Updated**: 2020-06-30 14:56:14+00:00
- **Authors**: Tuan Tran, Jory Denny, Chinwe Ekenna
- **Comment**: 7 pages, 7 figures
- **Journal**: None
- **Summary**: Many state-of-art robotics applications require fast and efficient motion planning algorithms. Existing motion planning methods become less effective as the dimensionality of the robot and its workspace increases, especially the computational cost of collision detection routines. In this work, we present a framework to address the cost of expensive primitive operations in sampling-based motion planning. This framework determines the validity of a sample robot configuration through a novel combination of a Contractive AutoEncoder (CAE), which captures a occupancy grids representation of the robot's workspace, and a Multilayer Perceptron, which efficiently predicts the collision state of the robot from the CAE and the robot's configuration. We evaluate our framework on multiple planning problems with a variety of robots in 2D and 3D workspaces. The results show that (1) the framework is computationally efficient in all investigated problems, and (2) the framework generalizes well to new workspaces.



### FVV Live: Real-Time, Low-Cost, Free Viewpoint Video
- **Arxiv ID**: http://arxiv.org/abs/2006.16893v1
- **DOI**: 10.1109/ICMEW46912.2020.9105977
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2006.16893v1)
- **Published**: 2020-06-30 15:21:53+00:00
- **Updated**: 2020-06-30 15:21:53+00:00
- **Authors**: Daniel Berjón, Pablo Carballeira, Julián Cabrera, Carlos Carmona, Daniel Corregidor, César Díaz, Francisco Morán, Narciso García
- **Comment**: None
- **Journal**: None
- **Summary**: FVV Live is a novel real-time, low-latency, end-to-end free viewpoint system including capture, transmission, synthesis on an edge server and visualization and control on a mobile terminal. The system has been specially designed for low-cost and real-time operation, only using off-the-shelf components.



### Evaluation of Contemporary Convolutional Neural Network Architectures for Detecting COVID-19 from Chest Radiographs
- **Arxiv ID**: http://arxiv.org/abs/2007.01108v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.01108v1)
- **Published**: 2020-06-30 15:22:39+00:00
- **Updated**: 2020-06-30 15:22:39+00:00
- **Authors**: Nikita Albert
- **Comment**: None
- **Journal**: None
- **Summary**: Interpreting chest radiograph, a.ka. chest x-ray, images is a necessary and crucial diagnostic tool used by medical professionals to detect and identify many diseases that may plague a patient. Although the images themselves contain a wealth of valuable information, their usefulness may be limited by how well they are interpreted, especially when the reviewing radiologist may be fatigued or when or an experienced radiologist is unavailable. Research in the use of deep learning models to analyze chest radiographs yielded impressive results where, in some instances, the models outperformed practicing radiologists. Amidst the COVID-19 pandemic, researchers have explored and proposed the use of said deep models to detect COVID-19 infections from radiographs as a possible way to help ease the strain on medical resources. In this study, we train and evaluate three model architectures, proposed for chest radiograph analysis, under varying conditions, find issues that discount the impressive model performances proposed by contemporary studies on this subject, and propose methodologies to train models that yield more reliable results.. Code, scripts, pre-trained models, and visualizations are available at https://github.com/nalbert/COVID-detection-from-radiographs.



### ERNIE-ViL: Knowledge Enhanced Vision-Language Representations Through Scene Graph
- **Arxiv ID**: http://arxiv.org/abs/2006.16934v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2006.16934v3)
- **Published**: 2020-06-30 16:03:12+00:00
- **Updated**: 2021-03-19 05:17:32+00:00
- **Authors**: Fei Yu, Jiji Tang, Weichong Yin, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang
- **Comment**: Paper has been published in the AAAI2021 conference
- **Journal**: None
- **Summary**: We propose a knowledge-enhanced approach, ERNIE-ViL, which incorporates structured knowledge obtained from scene graphs to learn joint representations of vision-language. ERNIE-ViL tries to build the detailed semantic connections (objects, attributes of objects and relationships between objects) across vision and language, which are essential to vision-language cross-modal tasks. Utilizing scene graphs of visual scenes, ERNIE-ViL constructs Scene Graph Prediction tasks, i.e., Object Prediction, Attribute Prediction and Relationship Prediction tasks in the pre-training phase. Specifically, these prediction tasks are implemented by predicting nodes of different types in the scene graph parsed from the sentence. Thus, ERNIE-ViL can learn the joint representations characterizing the alignments of the detailed semantics across vision and language. After pre-training on large scale image-text aligned datasets, we validate the effectiveness of ERNIE-ViL on 5 cross-modal downstream tasks. ERNIE-ViL achieves state-of-the-art performances on all these tasks and ranks the first place on the VCR leaderboard with an absolute improvement of 3.7%.



### ITSELF: Iterative Saliency Estimation fLexible Framework
- **Arxiv ID**: http://arxiv.org/abs/2006.16956v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.16956v2)
- **Published**: 2020-06-30 16:51:31+00:00
- **Updated**: 2020-07-24 19:16:37+00:00
- **Authors**: Leonardo de Melo Joao, Felipe de Castro Belem, Alexandre Xavier Falcao
- **Comment**: None
- **Journal**: None
- **Summary**: Saliency object detection estimates the objects that most stand out in an image. The available unsupervised saliency estimators rely on a pre-determined set of assumptions of how humans perceive saliency to create discriminating features. By fixing the pre-selected assumptions as an integral part of their models, these methods cannot be easily extended for specific settings and different image domains. We then propose a superpixel-based ITerative Saliency Estimation fLexible Framework (ITSELF) that allows any user-defined assumptions to be added to the model when required. Thanks to recent advancements in superpixel segmentation algorithms, saliency-maps can be used to improve superpixel delineation. By combining a saliency-based superpixel algorithm to a superpixel-based saliency estimator, we propose a novel saliency/superpixel self-improving loop to iteratively enhance saliency maps. We compare ITSELF to two state-of-the-art saliency estimators on five metrics and six datasets, four of which are composed of natural-images, and two of biomedical-images. Experiments show that our approach is more robust than the compared methods, presenting competitive results on natural-image datasets and outperforming them on biomedical-image datasets.



### Improving robustness against common corruptions by covariate shift adaptation
- **Arxiv ID**: http://arxiv.org/abs/2006.16971v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.16971v2)
- **Published**: 2020-06-30 17:01:37+00:00
- **Updated**: 2020-10-23 04:37:23+00:00
- **Authors**: Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bringmann, Wieland Brendel, Matthias Bethge
- **Comment**: Accepted at the Thirty-fourth Conference on Neural Information
  Processing Systems. Web: https://domainadaptation.org/batchnorm/
- **Journal**: None
- **Summary**: Today's state-of-the-art machine vision models are vulnerable to image corruptions like blurring or compression artefacts, limiting their performance in many real-world applications. We here argue that popular benchmarks to measure model robustness against common corruptions (like ImageNet-C) underestimate model robustness in many (but not all) application scenarios. The key insight is that in many scenarios, multiple unlabeled examples of the corruptions are available and can be used for unsupervised online adaptation. Replacing the activation statistics estimated by batch normalization on the training set with the statistics of the corrupted images consistently improves the robustness across 25 different popular computer vision models. Using the corrected statistics, ResNet-50 reaches 62.2% mCE on ImageNet-C compared to 76.7% without adaptation. With the more robust DeepAugment+AugMix model, we improve the state of the art achieved by a ResNet50 model up to date from 53.6% mCE to 45.4% mCE. Even adapting to a single sample improves robustness for the ResNet-50 and AugMix models, and 32 samples are sufficient to improve the current state of the art for a ResNet-50 architecture. We argue that results with adapted statistics should be included whenever reporting scores in corruption benchmarks and other out-of-distribution generalization settings.



### Towards Robust LiDAR-based Perception in Autonomous Driving: General Black-box Adversarial Sensor Attack and Countermeasures
- **Arxiv ID**: http://arxiv.org/abs/2006.16974v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.16974v1)
- **Published**: 2020-06-30 17:07:45+00:00
- **Updated**: 2020-06-30 17:07:45+00:00
- **Authors**: Jiachen Sun, Yulong Cao, Qi Alfred Chen, Z. Morley Mao
- **Comment**: 18 pages, 27 figures, to be published in USENIX Security 2020
- **Journal**: None
- **Summary**: Perception plays a pivotal role in autonomous driving systems, which utilizes onboard sensors like cameras and LiDARs (Light Detection and Ranging) to assess surroundings. Recent studies have demonstrated that LiDAR-based perception is vulnerable to spoofing attacks, in which adversaries spoof a fake vehicle in front of a victim self-driving car by strategically transmitting laser signals to the victim's LiDAR sensor. However, existing attacks suffer from effectiveness and generality limitations. In this work, we perform the first study to explore the general vulnerability of current LiDAR-based perception architectures and discover that the ignored occlusion patterns in LiDAR point clouds make self-driving cars vulnerable to spoofing attacks. We construct the first black-box spoofing attack based on our identified vulnerability, which universally achieves around 80% mean success rates on all target models. We perform the first defense study, proposing CARLO to mitigate LiDAR spoofing attacks. CARLO detects spoofed data by treating ignored occlusion patterns as invariant physical features, which reduces the mean attack success rate to 5.5%. Meanwhile, we take the first step towards exploring a general architecture for robust LiDAR-based perception, and propose SVF that embeds the neglected physical features into end-to-end learning. SVF further reduces the mean attack success rate to around 2.3%.



### Self-Supervised Learning of a Biologically-Inspired Visual Texture Model
- **Arxiv ID**: http://arxiv.org/abs/2006.16976v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/2006.16976v1)
- **Published**: 2020-06-30 17:12:09+00:00
- **Updated**: 2020-06-30 17:12:09+00:00
- **Authors**: Nikhil Parthasarathy, Eero P. Simoncelli
- **Comment**: 15 pages, 7 figures
- **Journal**: None
- **Summary**: We develop a model for representing visual texture in a low-dimensional feature space, along with a novel self-supervised learning objective that is used to train it on an unlabeled database of texture images. Inspired by the architecture of primate visual cortex, the model uses a first stage of oriented linear filters (corresponding to cortical area V1), consisting of both rectified units (simple cells) and pooled phase-invariant units (complex cells). These responses are processed by a second stage (analogous to cortical area V2) consisting of convolutional filters followed by half-wave rectification and pooling to generate V2 'complex cell' responses. The second stage filters are trained on a set of unlabeled homogeneous texture images, using a novel contrastive objective that maximizes the distance between the distribution of V2 responses to individual images and the distribution of responses across all images. When evaluated on texture classification, the trained model achieves substantially greater data-efficiency than a variety of deep hierarchical model architectures. Moreover, we show that the learned model exhibits stronger representational similarity to texture responses of neural populations recorded in primate V2 than pre-trained deep CNNs.



### PriorGAN: Real Data Prior for Generative Adversarial Nets
- **Arxiv ID**: http://arxiv.org/abs/2006.16990v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.16990v1)
- **Published**: 2020-06-30 17:51:47+00:00
- **Updated**: 2020-06-30 17:51:47+00:00
- **Authors**: Shuyang Gu, Jianmin Bao, Dong Chen, Fang Wen
- **Comment**: None
- **Journal**: None
- **Summary**: Generative adversarial networks (GANs) have achieved rapid progress in learning rich data distributions. However, we argue about two main issues in existing techniques. First, the low quality problem where the learned distribution has massive low quality samples. Second, the missing modes problem where the learned distribution misses some certain regions of the real data distribution. To address these two issues, we propose a novel prior that captures the whole real data distribution for GANs, which are called PriorGANs. To be specific, we adopt a simple yet elegant Gaussian Mixture Model (GMM) to build an explicit probability distribution on the feature level for the whole real data. By maximizing the probability of generated data, we can push the low quality samples to high quality. Meanwhile, equipped with the prior, we can estimate the missing modes in the learned distribution and design a sampling strategy on the real data to solve the problem. The proposed real data prior can generalize to various training settings of GANs, such as LSGAN, WGAN-GP, SNGAN, and even the StyleGAN. Our experiments demonstrate that PriorGANs outperform the state-of-the-art on the CIFAR-10, FFHQ, LSUN-cat, and LSUN-bird datasets by large margins.



### Deep Isometric Learning for Visual Recognition
- **Arxiv ID**: http://arxiv.org/abs/2006.16992v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.16992v2)
- **Published**: 2020-06-30 17:53:13+00:00
- **Updated**: 2020-08-15 04:39:34+00:00
- **Authors**: Haozhi Qi, Chong You, Xiaolong Wang, Yi Ma, Jitendra Malik
- **Comment**: ICML 2020; Code: https://github.com/HaozhiQi/ISONet; Website:
  https://haozhiqi.github.io/ISONet
- **Journal**: None
- **Summary**: Initialization, normalization, and skip connections are believed to be three indispensable techniques for training very deep convolutional neural networks and obtaining state-of-the-art performance. This paper shows that deep vanilla ConvNets without normalization nor skip connections can also be trained to achieve surprisingly good performance on standard image recognition benchmarks. This is achieved by enforcing the convolution kernels to be near isometric during initialization and training, as well as by using a variant of ReLU that is shifted towards being isometric. Further experiments show that if combined with skip connections, such near isometric networks can achieve performances on par with (for ImageNet) and better than (for COCO) the standard ResNet, even without normalization at all. Our code is available at https://github.com/HaozhiQi/ISONet.



### Data-driven Regularization via Racecar Training for Generalizing Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2007.00024v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.00024v1)
- **Published**: 2020-06-30 18:00:41+00:00
- **Updated**: 2020-06-30 18:00:41+00:00
- **Authors**: You Xie, Nils Thuerey
- **Comment**: https://github.com/tum-pbs/racecar
- **Journal**: None
- **Summary**: We propose a novel training approach for improving the generalization in neural networks. We show that in contrast to regular constraints for orthogonality, our approach represents a {\em data-dependent} orthogonality constraint, and is closely related to singular value decompositions of the weight matrices. We also show how our formulation is easy to realize in practical network architectures via a reverse pass, which aims for reconstructing the full sequence of internal states of the network. Despite being a surprisingly simple change, we demonstrate that this forward-backward training approach, which we refer to as {\em racecar} training, leads to significantly more generic features being extracted from a given data set. Networks trained with our approach show more balanced mutual information between input and output throughout all layers, yield improved explainability and, exhibit improved performance for a variety of tasks and task transfers.



### Deep Feature Space: A Geometrical Perspective
- **Arxiv ID**: http://arxiv.org/abs/2007.00062v2
- **DOI**: 10.1109/TPAMI.2021.3094625
- **Categories**: **cs.CV**, cs.CG, cs.LG, I.2.6; I.5.1; F.4.1
- **Links**: [PDF](http://arxiv.org/pdf/2007.00062v2)
- **Published**: 2020-06-30 18:51:02+00:00
- **Updated**: 2021-07-01 16:03:09+00:00
- **Authors**: Ioannis Kansizoglou, Loukas Bampis, Antonios Gasteratos
- **Comment**: Accepted for publication in IEEE Transactions on Pattern Analysis and
  Machine Intelligence
- **Journal**: None
- **Summary**: One of the most prominent attributes of Neural Networks (NNs) constitutes their capability of learning to extract robust and descriptive features from high dimensional data, like images. Hence, such an ability renders their exploitation as feature extractors particularly frequent in an abundant of modern reasoning systems. Their application scope mainly includes complex cascade tasks, like multi-modal recognition and deep Reinforcement Learning (RL). However, NNs induce implicit biases that are difficult to avoid or to deal with and are not met in traditional image descriptors. Moreover, the lack of knowledge for describing the intra-layer properties -- and thus their general behavior -- restricts the further applicability of the extracted features. With the paper at hand, a novel way of visualizing and understanding the vector space before the NNs' output layer is presented, aiming to enlighten the deep feature vectors' properties under classification tasks. Main attention is paid to the nature of overfitting in the feature space and its adverse effect on further exploitation. We present the findings that can be derived from our model's formulation, and we evaluate them on realistic recognition scenarios, proving its prominence by improving the obtained results.



### Deep Geometric Texture Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2007.00074v1
- **DOI**: 10.1145/3386569.3392471
- **Categories**: **cs.GR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.00074v1)
- **Published**: 2020-06-30 19:36:38+00:00
- **Updated**: 2020-06-30 19:36:38+00:00
- **Authors**: Amir Hertz, Rana Hanocka, Raja Giryes, Daniel Cohen-Or
- **Comment**: SIGGRAPH 2020
- **Journal**: None
- **Summary**: Recently, deep generative adversarial networks for image generation have advanced rapidly; yet, only a small amount of research has focused on generative models for irregular structures, particularly meshes. Nonetheless, mesh generation and synthesis remains a fundamental topic in computer graphics. In this work, we propose a novel framework for synthesizing geometric textures. It learns geometric texture statistics from local neighborhoods (i.e., local triangular patches) of a single reference 3D model. It learns deep features on the faces of the input triangulation, which is used to subdivide and generate offsets across multiple scales, without parameterization of the reference or target mesh. Our network displaces mesh vertices in any direction (i.e., in the normal and tangential direction), enabling synthesis of geometric textures, which cannot be expressed by a simple 2D displacement map. Learning and synthesizing on local geometric patches enables a genus-oblivious framework, facilitating texture transfer between shapes of different genus.



### Similarity Search for Efficient Active Learning and Search of Rare Concepts
- **Arxiv ID**: http://arxiv.org/abs/2007.00077v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.00077v2)
- **Published**: 2020-06-30 19:46:10+00:00
- **Updated**: 2021-07-22 16:54:12+00:00
- **Authors**: Cody Coleman, Edward Chou, Julian Katz-Samuels, Sean Culatana, Peter Bailis, Alexander C. Berg, Robert Nowak, Roshan Sumbaly, Matei Zaharia, I. Zeki Yalniz
- **Comment**: None
- **Journal**: None
- **Summary**: Many active learning and search approaches are intractable for large-scale industrial settings with billions of unlabeled examples. Existing approaches search globally for the optimal examples to label, scaling linearly or even quadratically with the unlabeled data. In this paper, we improve the computational efficiency of active learning and search methods by restricting the candidate pool for labeling to the nearest neighbors of the currently labeled set instead of scanning over all of the unlabeled data. We evaluate several selection strategies in this setting on three large-scale computer vision datasets: ImageNet, OpenImages, and a de-identified and aggregated dataset of 10 billion images provided by a large internet company. Our approach achieved similar mean average precision and recall as the traditional global approach while reducing the computational cost of selection by up to three orders of magnitude, thus enabling web-scale active learning.



### Using Human Psychophysics to Evaluate Generalization in Scene Text Recognition Models
- **Arxiv ID**: http://arxiv.org/abs/2007.00083v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.00083v1)
- **Published**: 2020-06-30 19:51:26+00:00
- **Updated**: 2020-06-30 19:51:26+00:00
- **Authors**: Sahar Siddiqui, Elena Sizikova, Gemma Roig, Najib J. Majaj, Denis G. Pelli
- **Comment**: None
- **Journal**: None
- **Summary**: Scene text recognition models have advanced greatly in recent years. Inspired by human reading we characterize two important scene text recognition models by measuring their domains i.e. the range of stimulus images that they can read. The domain specifies the ability of readers to generalize to different word lengths, fonts, and amounts of occlusion. These metrics identify strengths and weaknesses of existing models. Relative to the attention-based (Attn) model, we discover that the connectionist temporal classification (CTC) model is more robust to noise and occlusion, and better at generalizing to different word lengths. Further, we show that in both models, adding noise to training images yields better generalization to occlusion. These results demonstrate the value of testing models till they break, complementing the traditional data science focus on optimizing performance.



### Semantic Segmentation With Multi Scale Spatial Attention For Self Driving Cars
- **Arxiv ID**: http://arxiv.org/abs/2007.12685v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.12685v3)
- **Published**: 2020-06-30 20:19:09+00:00
- **Updated**: 2020-09-30 22:56:11+00:00
- **Authors**: Abhinav Sagar, RajKumar Soundrapandiyan
- **Comment**: 11 pages, 6 figures
- **Journal**: None
- **Summary**: In this paper, we present a novel neural network using multi scale feature fusion at various scales for accurate and efficient semantic image segmentation. We used ResNet based feature extractor, dilated convolutional layers in downsampling part, atrous convolutional layers in the upsampling part and used concat operation to merge them. A new attention module is proposed to encode more contextual information and enhance the receptive field of the network. We present an in depth theoretical analysis of our network with training and optimization details. Our network was trained and tested on the Camvid dataset and Cityscapes dataset using mean accuracy per class and Intersection Over Union (IOU) as the evaluation metrics. Our model outperforms previous state of the art methods on semantic segmentation achieving mean IOU value of 74.12 while running at >100 FPS.



### Deep Learning for Vision-based Prediction: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2007.00095v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2007.00095v2)
- **Published**: 2020-06-30 20:26:46+00:00
- **Updated**: 2020-07-22 15:33:06+00:00
- **Authors**: Amir Rasouli
- **Comment**: None
- **Journal**: None
- **Summary**: Vision-based prediction algorithms have a wide range of applications including autonomous driving, surveillance, human-robot interaction, weather prediction. The objective of this paper is to provide an overview of the field in the past five years with a particular focus on deep learning approaches. For this purpose, we categorize these algorithms into video prediction, action prediction, trajectory prediction, body motion prediction, and other prediction applications. For each category, we highlight the common architectures, training methods and types of data used. In addition, we discuss the common evaluation metrics and datasets used for vision-based prediction tasks. A database of all the information presented in this survey including, cross-referenced according to papers, datasets and metrics, can be found online at https://github.com/aras62/vision-based-prediction.



### Robustness to Transformations Across Categories: Is Robustness To Transformations Driven by Invariant Neural Representations?
- **Arxiv ID**: http://arxiv.org/abs/2007.00112v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.00112v4)
- **Published**: 2020-06-30 21:18:08+00:00
- **Updated**: 2023-06-14 22:34:29+00:00
- **Authors**: Hojin Jang, Syed Suleman Abbas Zaidi, Xavier Boix, Neeraj Prasad, Sharon Gilad-Gutnick, Shlomit Ben-Ami, Pawan Sinha
- **Comment**: None
- **Journal**: None
- **Summary**: Deep Convolutional Neural Networks (DCNNs) have demonstrated impressive robustness to recognize objects under transformations (eg. blur or noise) when these transformations are included in the training set. A hypothesis to explain such robustness is that DCNNs develop invariant neural representations that remain unaltered when the image is transformed. However, to what extent this hypothesis holds true is an outstanding question, as robustness to transformations could be achieved with properties different from invariance, eg. parts of the network could be specialized to recognize either transformed or non-transformed images. This paper investigates the conditions under which invariant neural representations emerge by leveraging that they facilitate robustness to transformations beyond the training distribution. Concretely, we analyze a training paradigm in which only some object categories are seen transformed during training and evaluate whether the DCNN is robust to transformations across categories not seen transformed. Our results with state-of-the-art DCNNs indicate that invariant neural representations do not always drive robustness to transformations, as networks show robustness for categories seen transformed during training even in the absence of invariant neural representations. Invariance only emerges as the number of transformed categories in the training set is increased. This phenomenon is much more prominent with local transformations such as blurring and high-pass filtering than geometric transformations such as rotation and thinning, which entail changes in the spatial arrangement of the object. Our results contribute to a better understanding of invariant neural representations in deep learning and the conditions under which it spontaneously emerges.



### Long-term Pedestrian Trajectory Prediction using Mutable Intention Filter and Warp LSTM
- **Arxiv ID**: http://arxiv.org/abs/2007.00113v3
- **DOI**: 10.1109/LRA.2020.3047731
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.00113v3)
- **Published**: 2020-06-30 21:23:00+00:00
- **Updated**: 2021-06-21 01:33:10+00:00
- **Authors**: Zhe Huang, Aamir Hasan, Kazuki Shin, Ruohua Li, Katherine Driggs-Campbell
- **Comment**: Accepted by RA-L Special Issue on Long-Term Human Motion Prediction
- **Journal**: None
- **Summary**: Trajectory prediction is one of the key capabilities for robots to safely navigate and interact with pedestrians. Critical insights from human intention and behavioral patterns need to be integrated to effectively forecast long-term pedestrian behavior. Thus, we propose a framework incorporating a Mutable Intention Filter and a Warp LSTM (MIF-WLSTM) to simultaneously estimate human intention and perform trajectory prediction. The Mutable Intention Filter is inspired by particle filtering and genetic algorithms, where particles represent intention hypotheses that can be mutated throughout the pedestrian motion. Instead of predicting sequential displacement over time, our Warp LSTM learns to generate offsets on a full trajectory predicted by a nominal intention-aware linear model, which considers the intention hypotheses during filtering process. Through experiments on a publicly available dataset, we show that our method outperforms baseline approaches and demonstrate the robust performance of our method under abnormal intention-changing scenarios. Code is available at https://github.com/tedhuang96/mifwlstm.



### FathomNet: An underwater image training database for ocean exploration and discovery
- **Arxiv ID**: http://arxiv.org/abs/2007.00114v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.DB
- **Links**: [PDF](http://arxiv.org/pdf/2007.00114v3)
- **Published**: 2020-06-30 21:23:06+00:00
- **Updated**: 2020-07-10 04:14:21+00:00
- **Authors**: Océane Boulais, Ben Woodward, Brian Schlining, Lonny Lundsten, Kevin Barnard, Katy Croff Bell, Kakani Katija
- **Comment**: 8 pages, 6 figures
- **Journal**: None
- **Summary**: Thousands of hours of marine video data are collected annually from remotely operated vehicles (ROVs) and other underwater assets. However, current manual methods of analysis impede the full utilization of collected data for real time algorithms for ROV and large biodiversity analyses. FathomNet is a novel baseline image training set, optimized to accelerate development of modern, intelligent, and automated analysis of underwater imagery. Our seed data set consists of an expertly annotated and continuously maintained database with more than 26,000 hours of videotape, 6.8 million annotations, and 4,349 terms in the knowledge base. FathomNet leverages this data set by providing imagery, localizations, and class labels of underwater concepts in order to enable machine learning algorithm development. To date, there are more than 80,000 images and 106,000 localizations for 233 different classes, including midwater and benthic organisms. Our experiments consisted of training various deep learning algorithms with approaches to address weakly supervised localization, image labeling, object detection and classification which prove to be promising. While we find quality results on prediction for this new dataset, our results indicate that we are ultimately in need of a larger data set for ocean exploration.



### Accelerating Prostate Diffusion Weighted MRI using Guided Denoising Convolutional Neural Network: Retrospective Feasibility Study
- **Arxiv ID**: http://arxiv.org/abs/2007.00121v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2007.00121v1)
- **Published**: 2020-06-30 21:44:42+00:00
- **Updated**: 2020-06-30 21:44:42+00:00
- **Authors**: Elena A. Kaye, Emily A. Aherne, Cihan Duzgol, Ida Häggström, Erich Kobler, Yousef Mazaheri, Maggie M Fung, Zhigang Zhang, Ricardo Otazo, Herbert A. Vargas, Oguz Akin
- **Comment**: This manuscript has been accepted for publication in Radiology:
  Artificial Intelligence (https://pubs.rsna.org/journal/ai), which is
  published by the Radiological Society of North America (RSNA)
- **Journal**: None
- **Summary**: Purpose: To investigate feasibility of accelerating prostate diffusion-weighted imaging (DWI) by reducing the number of acquired averages and denoising the resulting image using a proposed guided denoising convolutional neural network (DnCNN). Materials and Methods: Raw data from the prostate DWI scans were retrospectively gathered (between July 2018 and July 2019) from six single-vendor MRI scanners. 118 data sets were used for training and validation (age: 64.3 +- 8 years) and 37 - for testing (age: 65.1 +- 7.3 years). High b-value diffusion-weighted (hb-DW) data were reconstructed into noisy images using two averages and reference images using all sixteen averages. A conventional DnCNN was modified into a guided DnCNN, which uses the low b-value DWI image as a guidance input. Quantitative and qualitative reader evaluations were performed on the denoised hb-DW images. A cumulative link mixed regression model was used to compare the readers scores. The agreement between the apparent diffusion coefficient (ADC) maps (denoised vs reference) was analyzed using Bland Altman analysis. Results: Compared to the DnCNN, the guided DnCNN produced denoised hb-DW images with higher peak signal-to-noise ratio and structural similarity index and lower normalized mean square error (p < 0.001). Compared to the reference images, the denoised images received higher image quality scores (p < 0.0001). The ADC values based on the denoised hb-DW images were in good agreement with the reference ADC values. Conclusion: Accelerating prostate DWI by reducing the number of acquired averages and denoising the resulting image using the proposed guided DnCNN is technically feasible.



### Modality-Agnostic Attention Fusion for visual search with text feedback
- **Arxiv ID**: http://arxiv.org/abs/2007.00145v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.00145v1)
- **Published**: 2020-06-30 22:55:02+00:00
- **Updated**: 2020-06-30 22:55:02+00:00
- **Authors**: Eric Dodds, Jack Culpepper, Simao Herdade, Yang Zhang, Kofi Boakye
- **Comment**: 14 pages, 8 figures
- **Journal**: None
- **Summary**: Image retrieval with natural language feedback offers the promise of catalog search based on fine-grained visual features that go beyond objects and binary attributes, facilitating real-world applications such as e-commerce. Our Modality-Agnostic Attention Fusion (MAAF) model combines image and text features and outperforms existing approaches on two visual search with modifying phrase datasets, Fashion IQ and CSS, and performs competitively on a dataset with only single-word modifications, Fashion200k. We also introduce two new challenging benchmarks adapted from Birds-to-Words and Spot-the-Diff, which provide new settings with rich language inputs, and we show that our approach without modification outperforms strong baselines. To better understand our model, we conduct detailed ablations on Fashion IQ and provide visualizations of the surprising phenomenon of words avoiding "attending" to the image region they refer to.



### Generating Adversarial Examples with an Optimized Quality
- **Arxiv ID**: http://arxiv.org/abs/2007.00146v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.00146v1)
- **Published**: 2020-06-30 23:05:12+00:00
- **Updated**: 2020-06-30 23:05:12+00:00
- **Authors**: Aminollah Khormali, DaeHun Nyang, David Mohaisen
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning models are widely used in a range of application areas, such as computer vision, computer security, etc. However, deep learning models are vulnerable to Adversarial Examples (AEs),carefully crafted samples to deceive those models. Recent studies have introduced new adversarial attack methods, but, to the best of our knowledge, none provided guaranteed quality for the crafted examples as part of their creation, beyond simple quality measures such as Misclassification Rate (MR). In this paper, we incorporateImage Quality Assessment (IQA) metrics into the design and generation process of AEs. We propose an evolutionary-based single- and multi-objective optimization approaches that generate AEs with high misclassification rate and explicitly improve the quality, thus indistinguishability, of the samples, while perturbing only a limited number of pixels. In particular, several IQA metrics, including edge analysis, Fourier analysis, and feature descriptors, are leveraged into the process of generating AEs. Unique characteristics of the evolutionary-based algorithm enable us to simultaneously optimize the misclassification rate and the IQA metrics of the AEs. In order to evaluate the performance of the proposed method, we conduct intensive experiments on different well-known benchmark datasets(MNIST, CIFAR, GTSRB, and Open Image Dataset V5), while considering various objective optimization configurations. The results obtained from our experiments, when compared with the exist-ing attack methods, validate our initial hypothesis that the use ofIQA metrics within generation process of AEs can substantially improve their quality, while maintaining high misclassification rate.Finally, transferability and human perception studies are provided, demonstrating acceptable performance.



### Early-Learning Regularization Prevents Memorization of Noisy Labels
- **Arxiv ID**: http://arxiv.org/abs/2007.00151v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.00151v2)
- **Published**: 2020-06-30 23:46:33+00:00
- **Updated**: 2020-10-22 22:18:22+00:00
- **Authors**: Sheng Liu, Jonathan Niles-Weed, Narges Razavian, Carlos Fernandez-Granda
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel framework to perform classification via deep learning in the presence of noisy annotations. When trained on noisy labels, deep neural networks have been observed to first fit the training data with clean labels during an "early learning" phase, before eventually memorizing the examples with false labels. We prove that early learning and memorization are fundamental phenomena in high-dimensional classification tasks, even in simple linear models, and give a theoretical explanation in this setting. Motivated by these findings, we develop a new technique for noisy classification tasks, which exploits the progress of the early learning phase. In contrast with existing approaches, which use the model output during early learning to detect the examples with clean labels, and either ignore or attempt to correct the false labels, we take a different route and instead capitalize on early learning via regularization. There are two key elements to our approach. First, we leverage semi-supervised learning techniques to produce target probabilities based on the model outputs. Second, we design a regularization term that steers the model towards these targets, implicitly preventing memorization of the false labels. The resulting framework is shown to provide robustness to noisy annotations on several standard benchmarks and real-world datasets, where it achieves results comparable to the state of the art.



