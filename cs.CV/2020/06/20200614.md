# Arxiv Papers in cs.CV on 2020-06-14
### 3D Reconstruction of Novel Object Shapes from Single Images
- **Arxiv ID**: http://arxiv.org/abs/2006.07752v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.07752v4)
- **Published**: 2020-06-14 00:34:26+00:00
- **Updated**: 2021-09-01 21:11:12+00:00
- **Authors**: Anh Thai, Stefan Stojanov, Vijay Upadhya, James M. Rehg
- **Comment**: First two authors contributed equally
- **Journal**: None
- **Summary**: Accurately predicting the 3D shape of any arbitrary object in any pose from a single image is a key goal of computer vision research. This is challenging as it requires a model to learn a representation that can infer both the visible and occluded portions of any object using a limited training set. A training set that covers all possible object shapes is inherently infeasible. Such learning-based approaches are inherently vulnerable to overfitting, and successfully implementing them is a function of both the architecture design and the training approach. We present an extensive investigation of factors specific to architecture design, training, experiment design, and evaluation that influence reconstruction performance and measurement. We show that our proposed SDFNet achieves state-of-the-art performance on seen and unseen shapes relative to existing methods GenRe and OccNet. We provide the first large-scale evaluation of single image shape reconstruction to unseen objects. The source code, data and trained models can be found on https://github.com/rehg-lab/3DShapeGen.



### FCOS: A simple and strong anchor-free object detector
- **Arxiv ID**: http://arxiv.org/abs/2006.09214v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.09214v3)
- **Published**: 2020-06-14 01:03:39+00:00
- **Updated**: 2020-10-12 14:04:33+00:00
- **Authors**: Zhi Tian, Chunhua Shen, Hao Chen, Tong He
- **Comment**: Accepted to IEEE Transactions on Pattern Analysis and Machine
  Intelligence. Extended version of arXiv:1904.01355; Code is at:
  https://git.io/AdelaiDet
- **Journal**: None
- **Summary**: In computer vision, object detection is one of most important tasks, which underpins a few instance-level recognition tasks and many downstream applications. Recently one-stage methods have gained much attention over two-stage approaches due to their simpler design and competitive performance. Here we propose a fully convolutional one-stage object detector (FCOS) to solve object detection in a per-pixel prediction fashion, analogue to other dense prediction problems such as semantic segmentation. Almost all state-of-the-art object detectors such as RetinaNet, SSD, YOLOv3, and Faster R-CNN rely on pre-defined anchor boxes. In contrast, our proposed detector FCOS is anchor box free, as well as proposal free. By eliminating the pre-defined set of anchor boxes, FCOS completely avoids the complicated computation related to anchor boxes such as calculating the intersection over union (IoU) scores during training. More importantly, we also avoid all hyper-parameters related to anchor boxes, which are often sensitive to the final detection performance. With the only post-processing non-maximum suppression (NMS), we demonstrate a much simpler and flexible detection framework achieving improved detection accuracy. We hope that the proposed FCOS framework can serve as a simple and strong alternative for many other instance-level tasks. Code and pre-trained models are available at: https://git.io/AdelaiDet



### Recurrent Distillation based Crowd Counting
- **Arxiv ID**: http://arxiv.org/abs/2006.07755v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.07755v1)
- **Published**: 2020-06-14 01:04:52+00:00
- **Updated**: 2020-06-14 01:04:52+00:00
- **Authors**: Yue Gu, Wenxi Liu
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, with the progress of deep learning technologies, crowd counting has been rapidly developed. In this work, we propose a simple yet effective crowd counting framework that is able to achieve the state-of-the-art performance on various crowded scenes. In particular, we first introduce a perspective-aware density map generation method that is able to produce ground-truth density maps from point annotations to train crowd counting model to accomplish superior performance than prior density map generation techniques. Besides, leveraging our density map generation method, we propose an iterative distillation algorithm to progressively enhance our model with identical network structures, without significantly sacrificing the dimension of the output density maps. In experiments, we demonstrate that, with our simple convolutional neural network architecture strengthened by our proposed training algorithm, our model is able to outperform or be comparable with the state-of-the-art methods. Furthermore, we also evaluate our density map generation approach and distillation algorithm in ablation studies.



### Domain Adaptation and Image Classification via Deep Conditional Adaptation Network
- **Arxiv ID**: http://arxiv.org/abs/2006.07776v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.07776v2)
- **Published**: 2020-06-14 02:56:01+00:00
- **Updated**: 2022-05-03 15:10:20+00:00
- **Authors**: Pengfei Ge, Chuan-Xian Ren, Dao-Qing Dai, Hong Yan
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised domain adaptation aims to generalize the supervised model trained on a source domain to an unlabeled target domain. Marginal distribution alignment of feature spaces is widely used to reduce the domain discrepancy between the source and target domains. However, it assumes that the source and target domains share the same label distribution, which limits their application scope. In this paper, we consider a more general application scenario where the label distributions of the source and target domains are not the same. In this scenario, marginal distribution alignment-based methods will be vulnerable to negative transfer. To address this issue, we propose a novel unsupervised domain adaptation method, Deep Conditional Adaptation Network (DCAN), based on conditional distribution alignment of feature spaces. To be specific, we reduce the domain discrepancy by minimizing the Conditional Maximum Mean Discrepancy between the conditional distributions of deep features on the source and target domains, and extract the discriminant information from target domain by maximizing the mutual information between samples and the prediction labels. In addition, DCAN can be used to address a special scenario, Partial unsupervised domain adaptation, where the target domain category is a subset of the source domain category. Experiments on both unsupervised domain adaptation and Partial unsupervised domain adaptation show that DCAN achieves superior classification performance over state-of-the-art methods.



### Cascaded deep monocular 3D human pose estimation with evolutionary training data
- **Arxiv ID**: http://arxiv.org/abs/2006.07778v3
- **DOI**: 10.1109/CVPR42600.2020.00621
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.07778v3)
- **Published**: 2020-06-14 03:09:52+00:00
- **Updated**: 2021-04-09 02:37:06+00:00
- **Authors**: Shichao Li, Lei Ke, Kevin Pratama, Yu-Wing Tai, Chi-Keung Tang, Kwang-Ting Cheng
- **Comment**: Accepted to CVPR 2020 as Oral Presentation
- **Journal**: None
- **Summary**: End-to-end deep representation learning has achieved remarkable accuracy for monocular 3D human pose estimation, yet these models may fail for unseen poses with limited and fixed training data. This paper proposes a novel data augmentation method that: (1) is scalable for synthesizing massive amount of training data (over 8 million valid 3D human poses with corresponding 2D projections) for training 2D-to-3D networks, (2) can effectively reduce dataset bias. Our method evolves a limited dataset to synthesize unseen 3D human skeletons based on a hierarchical human representation and heuristics inspired by prior knowledge. Extensive experiments show that our approach not only achieves state-of-the-art accuracy on the largest public benchmark, but also generalizes significantly better to unseen and rare poses. Code, pre-trained models and tools are available at this HTTPS URL.



### Segmentation task for fashion and apparel
- **Arxiv ID**: http://arxiv.org/abs/2006.11375v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.11375v1)
- **Published**: 2020-06-14 03:44:58+00:00
- **Updated**: 2020-06-14 03:44:58+00:00
- **Authors**: Hassler Castro, Mariana Ramirez
- **Comment**: 8 pages, 7 figures. To appear as one of the projects associated with
  the advance machine learning class at Universidad EAFIT, June 2019. Medellin,
  Colombia
- **Journal**: None
- **Summary**: The Fashion Industry is a strong and important industry in the global economy. Globalization has brought fast fashion, quick shifting consumer shopping preferences, more competition, and abundance in fashion shops and retailers, making it more difficult for professionals in the fashion industry to keep track of what fashion items people wear and how they combine them. This paper solves this problem by implementing several Deep Learning Architectures using the iMaterialist dataset consisting of 45,000 images with 46 different clothing and apparel categories.



### PrimA6D: Rotational Primitive Reconstruction for Enhanced and Robust 6D Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2006.07789v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2006.07789v2)
- **Published**: 2020-06-14 03:55:42+00:00
- **Updated**: 2020-07-03 10:39:13+00:00
- **Authors**: Myung-Hwan Jeon, Ayoung Kim
- **Comment**: RA-L and IROS 2020
- **Journal**: None
- **Summary**: In this paper, we introduce a rotational primitive prediction based 6D object pose estimation using a single image as an input. We solve for the 6D object pose of a known object relative to the camera using a single image with occlusion. Many recent state-of-the-art (SOTA) two-step approaches have exploited image keypoints extraction followed by PnP regression for pose estimation. Instead of relying on bounding box or keypoints on the object, we propose to learn orientation-induced primitive so as to achieve the pose estimation accuracy regardless of the object size. We leverage a Variational AutoEncoder (VAE) to learn this underlying primitive and its associated keypoints. The keypoints inferred from the reconstructed primitive image are then used to regress the rotation using PnP. Lastly, we compute the translation in a separate localization module to complete the entire 6D pose estimation. When evaluated over public datasets, the proposed method yields a notable improvement over the LINEMOD, the Occlusion LINEMOD, and the YCB-Video dataset. We further provide a synthetic-only trained case presenting comparable performance to the existing methods which require real images in the training phase.



### Generative 3D Part Assembly via Dynamic Graph Learning
- **Arxiv ID**: http://arxiv.org/abs/2006.07793v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.07793v3)
- **Published**: 2020-06-14 04:26:42+00:00
- **Updated**: 2020-12-23 05:50:35+00:00
- **Authors**: Jialei Huang, Guanqi Zhan, Qingnan Fan, Kaichun Mo, Lin Shao, Baoquan Chen, Leonidas Guibas, Hao Dong
- **Comment**: NeurIPS 2020
- **Journal**: None
- **Summary**: Autonomous part assembly is a challenging yet crucial task in 3D computer vision and robotics. Analogous to buying an IKEA furniture, given a set of 3D parts that can assemble a single shape, an intelligent agent needs to perceive the 3D part geometry, reason to propose pose estimations for the input parts, and finally call robotic planning and control routines for actuation. In this paper, we focus on the pose estimation subproblem from the vision side involving geometric and relational reasoning over the input part geometry. Essentially, the task of generative 3D part assembly is to predict a 6-DoF part pose, including a rigid rotation and translation, for each input part that assembles a single 3D shape as the final output. To tackle this problem, we propose an assembly-oriented dynamic graph learning framework that leverages an iterative graph neural network as a backbone. It explicitly conducts sequential part assembly refinements in a coarse-to-fine manner, exploits a pair of part relation reasoning module and part aggregation module for dynamically adjusting both part features and their relations in the part graph. We conduct extensive experiments and quantitative comparisons to three strong baseline methods, demonstrating the effectiveness of the proposed approach.



### Hyper RPCA: Joint Maximum Correntropy Criterion and Laplacian Scale Mixture Modeling On-the-Fly for Moving Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2006.07795v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.07795v1)
- **Published**: 2020-06-14 04:35:45+00:00
- **Updated**: 2020-06-14 04:35:45+00:00
- **Authors**: Zerui Shao, Yifei Pu, Jiliu Zhou, Bihan Wen, Yi Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Moving object detection is critical for automated video analysis in many vision-related tasks, such as surveillance tracking, video compression coding, etc. Robust Principal Component Analysis (RPCA), as one of the most popular moving object modelling methods, aims to separate the temporally varying (i.e., moving) foreground objects from the static background in video, assuming the background frames to be low-rank while the foreground to be spatially sparse. Classic RPCA imposes sparsity of the foreground component using l1-norm, and minimizes the modeling error via 2-norm. We show that such assumptions can be too restrictive in practice, which limits the effectiveness of the classic RPCA, especially when processing videos with dynamic background, camera jitter, camouflaged moving object, etc. In this paper, we propose a novel RPCA-based model, called Hyper RPCA, to detect moving objects on the fly. Different from classic RPCA, the proposed Hyper RPCA jointly applies the maximum correntropy criterion (MCC) for the modeling error, and Laplacian scale mixture (LSM) model for foreground objects. Extensive experiments have been conducted, and the results demonstrate that the proposed Hyper RPCA has competitive performance for foreground detection to the state-of-the-art algorithms on several well-known benchmark datasets.



### Structure by Architecture: Disentangled Representations without Regularization
- **Arxiv ID**: http://arxiv.org/abs/2006.07796v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.07796v3)
- **Published**: 2020-06-14 04:37:08+00:00
- **Updated**: 2021-07-04 11:59:35+00:00
- **Authors**: Felix Leeb, Guilia Lanzillotta, Yashas Annadani, Michel Besserve, Stefan Bauer, Bernhard Schölkopf
- **Comment**: Under review at NeurIPS 2021
- **Journal**: None
- **Summary**: We study the problem of self-supervised structured representation learning using autoencoders for generative modeling. Unlike most methods which rely on matching an arbitrary, relatively unstructured, prior distribution for sampling, we propose a sampling technique that relies solely on the independence of latent variables, thereby avoiding the trade-off between reconstruction quality and generative performance inherent to VAEs. We design a novel autoencoder architecture capable of learning a structured representation without the need for aggressive regularization. Our structural decoders learn a hierarchy of latent variables, akin to structural causal models, thereby ordering the information without any additional regularization. We demonstrate how these models learn a representation that improves results in a variety of downstream tasks including generation, disentanglement, and extrapolation using several challenging and natural image datasets.



### Geometry-Aware Instance Segmentation with Disparity Maps
- **Arxiv ID**: http://arxiv.org/abs/2006.07802v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.07802v1)
- **Published**: 2020-06-14 05:03:52+00:00
- **Updated**: 2020-06-14 05:03:52+00:00
- **Authors**: Cho-Ying Wu, Xiaoyan Hu, Michael Happold, Qiangeng Xu, Ulrich Neumann
- **Comment**: CVPR 2020 Workshop of Scalability in Autonomous Driving (WSAD).
  Please refer to WSAD site for details
- **Journal**: None
- **Summary**: Most previous works of outdoor instance segmentation for images only use color information. We explore a novel direction of sensor fusion to exploit stereo cameras. Geometric information from disparities helps separate overlapping objects of the same or different classes. Moreover, geometric information penalizes region proposals with unlikely 3D shapes thus suppressing false positive detections. Mask regression is based on 2D, 2.5D, and 3D ROI using the pseudo-lidar and image-based representations. These mask predictions are fused by a mask scoring process. However, public datasets only adopt stereo systems with shorter baseline and focal legnth, which limit measuring ranges of stereo cameras. We collect and utilize High-Quality Driving Stereo (HQDS) dataset, using much longer baseline and focal length with higher resolution. Our performance attains state of the art. Please refer to our project page. The full paper is available here.



### Relative Pose Estimation for Stereo Rolling Shutter Cameras
- **Arxiv ID**: http://arxiv.org/abs/2006.07807v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.07807v1)
- **Published**: 2020-06-14 05:58:39+00:00
- **Updated**: 2020-06-14 05:58:39+00:00
- **Authors**: Ke Wang, Bin Fan, Yuchao Dai
- **Comment**: Accepted by International Conference on Image Processing (ICIP 2020)
- **Journal**: None
- **Summary**: In this paper, we present a novel linear algorithm to estimate the 6 DoF relative pose from consecutive frames of stereo rolling shutter (RS) cameras. Our method is derived based on the assumption that stereo cameras undergo motion with constant velocity around the center of the baseline, which needs 9 pairs of correspondences on both left and right consecutive frames. The stereo RS images enable the recovery of depth maps from the semi-global matching (SGM) algorithm. With the estimated camera motion and depth map, we can correct the RS images to get the undistorted images without any scene structure assumption. Experiments on both simulated points and synthetic RS images demonstrate the effectiveness of our algorithm in relative pose estimation.



### ReLGAN: Generalization of Consistency for GAN with Disjoint Constraints and Relative Learning of Generative Processes for Multiple Transformation Learning
- **Arxiv ID**: http://arxiv.org/abs/2006.07809v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2006.07809v1)
- **Published**: 2020-06-14 06:03:30+00:00
- **Updated**: 2020-06-14 06:03:30+00:00
- **Authors**: Chiranjib Sur
- **Comment**: None
- **Journal**: None
- **Summary**: Image to image transformation has gained popularity from different research communities due to its enormous impact on different applications, including medical. In this work, we have introduced a generalized scheme for consistency for GAN architectures with two new concepts of Transformation Learning (TL) and Relative Learning (ReL) for enhanced learning image transformations. Consistency for GAN architectures suffered from inadequate constraints and failed to learn multiple and multi-modal transformations, which is inevitable for many medical applications. The main drawback is that it focused on creating an intermediate and workable hybrid, which is not permissible for the medical applications which focus on minute details. Another drawback is the weak interrelation between the two learning phases and TL and ReL have introduced improved coordination among them. We have demonstrated the capability of the novel network framework on public datasets. We emphasized that our novel architecture produced an improved neural image transformation version for the image, which is more acceptable to the medical community. Experiments and results demonstrated the effectiveness of our framework with enhancement compared to the previous works.



### Disentanglement for Discriminative Visual Recognition
- **Arxiv ID**: http://arxiv.org/abs/2006.07810v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.07810v1)
- **Published**: 2020-06-14 06:10:51+00:00
- **Updated**: 2020-06-14 06:10:51+00:00
- **Authors**: Xiaofeng Liu
- **Comment**: Manuscript for book "Recognition and perception of images" Willy
- **Journal**: None
- **Summary**: Recent successes of deep learning-based recognition rely on maintaining the content related to the main-task label. However, how to explicitly dispel the noisy signals for better generalization in a controllable manner remains an open issue. For instance, various factors such as identity-specific attributes, pose, illumination and expression affect the appearance of face images. Disentangling the identity-specific factors is potentially beneficial for facial expression recognition (FER). This chapter systematically summarize the detrimental factors as task-relevant/irrelevant semantic variations and unspecified latent variation. In this chapter, these problems are casted as either a deep metric learning problem or an adversarial minimax game in the latent space. For the former choice, a generalized adaptive (N+M)-tuplet clusters loss function together with the identity-aware hard-negative mining and online positive mining scheme can be used for identity-invariant FER. The better FER performance can be achieved by combining the deep metric loss and softmax loss in a unified two fully connected layer branches framework via joint optimization. For the latter solution, it is possible to equipping an end-to-end conditional adversarial network with the ability to decompose an input sample into three complementary parts. The discriminative representation inherits the desired invariance property guided by prior knowledge of the task, which is marginal independent to the task-relevant/irrelevant semantic and latent variations. The framework achieves top performance on a serial of tasks, including lighting, makeup, disguise-tolerant face recognition and facial attributes recognition. This chapter systematically summarize the popular and practical solution for disentanglement to achieve more discriminative visual recognition.



### 2D Image Relighting with Image-to-Image Translation
- **Arxiv ID**: http://arxiv.org/abs/2006.07816v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.07816v2)
- **Published**: 2020-06-14 06:39:53+00:00
- **Updated**: 2020-06-26 23:35:24+00:00
- **Authors**: Paul Gafton, Erick Maraz
- **Comment**: 12 pages, 52 Postscript figures, uses cvpr_eso.sty eso-pic.sty
  ruler.sty
- **Journal**: None
- **Summary**: With the advent of Generative Adversarial Networks (GANs), a finer level of control in manipulating various features of an image has become possible. One example of such fine manipulation is changing the position of the light source in a scene. This is fundamentally an ill-posed problem, since it requires understanding the scene geometry to generate proper lighting effects. This problem is not a trivial one and can become even more complicated if we want to change the direction of the light source from any direction to a specific one. Here we provide our attempt to solve this problem using GANs. Specifically, pix2pix [arXiv:1611.07004] trained with the dataset VIDIT [arXiv:2005.05460] which contains images of the same scene with different types of light temperature and 8 different light source positions (N, NE, E, SE, S, SW, W, NW). The results are 8 neural networks trained to be able to change the direction of the light source from any direction to one of the 8 previously mentioned. Additionally, we provide, as a tool, a simple CNN trained to identify the direction of the light source in an image.



### Alternating ConvLSTM: Learning Force Propagation with Alternate State Updates
- **Arxiv ID**: http://arxiv.org/abs/2006.07818v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2006.07818v1)
- **Published**: 2020-06-14 06:43:33+00:00
- **Updated**: 2020-06-14 06:43:33+00:00
- **Authors**: Congyue Deng, Tai-Jiang Mu, Shi-Min Hu
- **Comment**: None
- **Journal**: None
- **Summary**: Data-driven simulation is an important step-forward in computational physics when traditional numerical methods meet their limits. Learning-based simulators have been widely studied in past years; however, most previous works view simulation as a general spatial-temporal prediction problem and take little physical guidance in designing their neural network architectures. In this paper, we introduce the alternating convolutional Long Short-Term Memory (Alt-ConvLSTM) that models the force propagation mechanisms in a deformable object with near-uniform material properties. Specifically, we propose an accumulation state, and let the network update its cell state and the accumulation state alternately. We demonstrate how this novel scheme imitates the alternate updates of the first and second-order terms in the forward Euler method of numerical PDE solvers. Benefiting from this, our network only requires a small number of parameters, independent of the number of the simulated particles, and also retains the essential features in ConvLSTM, making it naturally applicable to sequential data with spatial inputs and outputs. We validate our Alt-ConvLSTM on human soft tissue simulation with thousands of particles and consistent body pose changes. Experimental results show that Alt-ConvLSTM efficiently models the material kinetic features and greatly outperforms vanilla ConvLSTM with only the single state update.



### Adaptively Meshed Video Stabilization
- **Arxiv ID**: http://arxiv.org/abs/2006.07820v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.07820v1)
- **Published**: 2020-06-14 06:51:23+00:00
- **Updated**: 2020-06-14 06:51:23+00:00
- **Authors**: Minda Zhao, Qiang Ling
- **Comment**: None
- **Journal**: None
- **Summary**: Video stabilization is essential for improving visual quality of shaky videos. The current video stabilization methods usually take feature trajectories in the background to estimate one global transformation matrix or several transformation matrices based on a fixed mesh, and warp shaky frames into their stabilized views. However, these methods may not model the shaky camera motion well in complicated scenes, such as scenes containing large foreground objects or strong parallax, and may result in notable visual artifacts in the stabilized videos. To resolve the above issues, this paper proposes an adaptively meshed method to stabilize a shaky video based on all of its feature trajectories and an adaptive blocking strategy. More specifically, we first extract feature trajectories of the shaky video and then generate a triangle mesh according to the distribution of the feature trajectories in each frame. Then transformations between shaky frames and their stabilized views over all triangular grids of the mesh are calculated to stabilize the shaky video. Since more feature trajectories can usually be extracted from all regions, including both background and foreground regions, a finer mesh will be obtained and provided for camera motion estimation and frame warping. We estimate the mesh-based transformations of each frame by solving a two-stage optimization problem. Moreover, foreground and background feature trajectories are no longer distinguished and both contribute to the estimation of the camera motion in the proposed optimization problem, which yields better estimation performance than previous works, particularly in challenging videos with large foreground objects or strong parallax.



### Working with scale: 2nd place solution to Product Detection in Densely Packed Scenes [Technical Report]
- **Arxiv ID**: http://arxiv.org/abs/2006.07825v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.07825v1)
- **Published**: 2020-06-14 07:17:21+00:00
- **Updated**: 2020-06-14 07:17:21+00:00
- **Authors**: Artem Kozlov
- **Comment**: None
- **Journal**: None
- **Summary**: This report describes a 2nd place solution of the detection challenge which is held within CVPR 2020 Retail-Vision workshop. Instead of going further considering previous results this work mainly aims to verify previously observed takeaways by re-experimenting. The reliability and reproducibility of the results are reached by incorporating a popular object detection toolbox - MMDetection. In this report, I firstly represent the results received for Faster-RCNN and RetinaNet models, which were taken for comparison in the original work. Then I describe the experiment results with more advanced models. The final section reviews two simple tricks for Faster-RCNN model that were used for my final submission: changing default anchor scale parameter and train-time image tiling. The source code is available at https://github.com/tyomj/product_detection.



### Few-shot Object Detection on Remote Sensing Images
- **Arxiv ID**: http://arxiv.org/abs/2006.07826v2
- **DOI**: 10.1109/TGRS.2021.3051383
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.07826v2)
- **Published**: 2020-06-14 07:18:10+00:00
- **Updated**: 2020-06-16 03:55:42+00:00
- **Authors**: Jingyu Deng, Xiang Li, Yi Fang
- **Comment**: 12pages, 7 figures
- **Journal**: TGRS. 60 (2021)
- **Summary**: In this paper, we deal with the problem of object detection on remote sensing images. Previous methods have developed numerous deep CNN-based methods for object detection on remote sensing images and the report remarkable achievements in detection performance and efficiency. However, current CNN-based methods mostly require a large number of annotated samples to train deep neural networks and tend to have limited generalization abilities for unseen object categories. In this paper, we introduce a few-shot learning-based method for object detection on remote sensing images where only a few annotated samples are provided for the unseen object categories. More specifically, our model contains three main components: a meta feature extractor that learns to extract feature representations from input images, a reweighting module that learn to adaptively assign different weights for each feature representation from the support images, and a bounding box prediction module that carries out object detection on the reweighted feature maps. We build our few-shot object detection model upon YOLOv3 architecture and develop a multi-scale object detection framework. Experiments on two benchmark datasets demonstrate that with only a few annotated samples our model can still achieve a satisfying detection performance on remote sensing images and the performance of our model is significantly better than the well-established baseline models.



### PCAAE: Principal Component Analysis Autoencoder for organising the latent space of generative networks
- **Arxiv ID**: http://arxiv.org/abs/2006.07827v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.07827v1)
- **Published**: 2020-06-14 07:40:45+00:00
- **Updated**: 2020-06-14 07:40:45+00:00
- **Authors**: Chi-Hieu Pham, Saïd Ladjal, Alasdair Newson
- **Comment**: Preprint with Appendix
- **Journal**: None
- **Summary**: Autoencoders and generative models produce some of the most spectacular deep learning results to date. However, understanding and controlling the latent space of these models presents a considerable challenge. Drawing inspiration from principal component analysis and autoencoder, we propose the Principal Component Analysis Autoencoder (PCAAE). This is a novel autoencoder whose latent space verifies two properties. Firstly, the dimensions are organised in decreasing importance with respect to the data at hand. Secondly, the components of the latent space are statistically independent. We achieve this by progressively increasing the latent space during training, and with a covariance loss applied to the latent codes. The resulting autoencoder produces a latent space which separates the intrinsic attributes of the data into different components of the latent space, in a completely unsupervised manner. We also describe an extension of our approach to the case of powerful, pre-trained GANs. We show results on both synthetic examples of shapes and on a state-of-the-art GAN. For example, we are able to separate the color shade scale of hair and skin, pose of faces and the gender in the CelebA, without accessing any labels. We compare the PCAAE with other state-of-the-art approaches, in particular with respect to the ability to disentangle attributes in the latent space. We hope that this approach will contribute to better understanding of the intrinsic latent spaces of powerful deep generative models.



### On Saliency Maps and Adversarial Robustness
- **Arxiv ID**: http://arxiv.org/abs/2006.07828v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.07828v2)
- **Published**: 2020-06-14 07:41:53+00:00
- **Updated**: 2020-07-13 15:02:59+00:00
- **Authors**: Puneet Mangla, Vedant Singh, Vineeth N Balasubramanian
- **Comment**: Accepted at ECML-PKDD 2020, Acknowledgements added
- **Journal**: None
- **Summary**: A Very recent trend has emerged to couple the notion of interpretability and adversarial robustness, unlike earlier efforts which solely focused on good interpretations or robustness against adversaries. Works have shown that adversarially trained models exhibit more interpretable saliency maps than their non-robust counterparts, and that this behavior can be quantified by considering the alignment between input image and saliency map. In this work, we provide a different perspective to this coupling, and provide a method, Saliency based Adversarial training (SAT), to use saliency maps to improve adversarial robustness of a model. In particular, we show that using annotations such as bounding boxes and segmentation masks, already provided with a dataset, as weak saliency maps, suffices to improve adversarial robustness with no additional effort to generate the perturbations themselves. Our empirical results on CIFAR-10, CIFAR-100, Tiny ImageNet and Flower-17 datasets consistently corroborate our claim, by showing improved adversarial robustness using our method. saliency maps. We also show how using finer and stronger saliency maps leads to more robust models, and how integrating SAT with existing adversarial training methods, further boosts performance of these existing methods.



### Multi-Miner: Object-Adaptive Region Mining for Weakly-Supervised Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2006.07834v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.07834v1)
- **Published**: 2020-06-14 08:00:42+00:00
- **Updated**: 2020-06-14 08:00:42+00:00
- **Authors**: Kuangqi Zhou, Qibin Hou, Zun Li, Jiashi Feng
- **Comment**: None
- **Journal**: None
- **Summary**: Object region mining is a critical step for weakly-supervised semantic segmentation. Most recent methods mine the object regions by expanding the seed regions localized by class activation maps. They generally do not consider the sizes of objects and apply a monotonous procedure to mining all the object regions. Thus their mined regions are often insufficient in number and scale for large objects, and on the other hand easily contaminated by surrounding backgrounds for small objects. In this paper, we propose a novel multi-miner framework to perform a region mining process that adapts to diverse object sizes and is thus able to mine more integral and finer object regions. Specifically, our multi-miner leverages a parallel modulator to check whether there are remaining object regions for each single object, and guide a category-aware generator to mine the regions of each object independently. In this way, the multi-miner adaptively takes more steps for large objects and fewer steps for small objects. Experiment results demonstrate that the multi-miner offers better region mining results and helps achieve better segmentation performance than state-of-the-art weakly-supervised semantic segmentation methods.



### A Generalized Asymmetric Dual-front Model for Active Contours and Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2006.07839v3
- **DOI**: 10.1109/TIP.2021.3078102
- **Categories**: **cs.CV**, cs.CG
- **Links**: [PDF](http://arxiv.org/pdf/2006.07839v3)
- **Published**: 2020-06-14 08:24:01+00:00
- **Updated**: 2021-05-04 09:19:18+00:00
- **Authors**: Da Chen, Jack Spencer, Jean-Marie Mirebeau, Ke Chen, Minglei Shu, Laurent D. Cohen
- **Comment**: Published in IEEE Transactions on Image Processing
- **Journal**: None
- **Summary**: The Voronoi diagram-based dual-front active contour models are known as a powerful and efficient way for addressing the image segmentation and domain partitioning problems. In the basic formulation of the dual-front models, the evolving contours can be considered as the interfaces of adjacent Voronoi regions. Among these dual-front models, a crucial ingredient is regarded as the geodesic metrics by which the geodesic distances and the corresponding Voronoi diagram can be estimated. In this paper, we introduce a type of asymmetric quadratic metrics dual-front model. The metrics considered are built by the integration of the image features and a vector field derived from the evolving contours. The use of the asymmetry enhancement can reduce the risk of contour shortcut or leakage problems especially when the initial contours are far away from the target boundaries or the images have complicated intensity distributions. Moreover, the proposed dual-front model can be applied for image segmentation in conjunction with various region-based homogeneity terms. The numerical experiments on both synthetic and real images show that the proposed dual-front model indeed achieves encouraging results.



### Towards Gender-Neutral Face Descriptors for Mitigating Bias in Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2006.07845v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.07845v2)
- **Published**: 2020-06-14 08:54:03+00:00
- **Updated**: 2020-09-17 06:25:31+00:00
- **Authors**: Prithviraj Dhar, Joshua Gleason, Hossein Souri, Carlos D. Castillo, Rama Chellappa
- **Comment**: Under submission
- **Journal**: None
- **Summary**: State-of-the-art deep networks implicitly encode gender information while being trained for face recognition. Gender is often viewed as an important attribute with respect to identifying faces. However, the implicit encoding of gender information in face descriptors has two major issues: (a.) It makes the descriptors susceptible to privacy leakage, i.e. a malicious agent can be trained to predict the face gender from such descriptors. (b.) It appears to contribute to gender bias in face recognition, i.e. we find a significant difference in the recognition accuracy of DCNNs on male and female faces. Therefore, we present a novel `Adversarial Gender De-biasing algorithm (AGENDA)' to reduce the gender information present in face descriptors obtained from previously trained face recognition networks. We show that AGENDA significantly reduces gender predictability of face descriptors. Consequently, we are also able to reduce gender bias in face verification while maintaining reasonable recognition performance.



### Continual General Chunking Problem and SyncMap
- **Arxiv ID**: http://arxiv.org/abs/2006.07853v4
- **DOI**: None
- **Categories**: **cs.AI**, cs.CL, cs.CV, cs.ET, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.07853v4)
- **Published**: 2020-06-14 09:39:56+00:00
- **Updated**: 2021-04-05 09:40:22+00:00
- **Authors**: Danilo Vasconcellos Vargas, Toshitake Asabuki
- **Comment**: None
- **Journal**: AAAI2021
- **Summary**: Humans possess an inherent ability to chunk sequences into their constituent parts. In fact, this ability is thought to bootstrap language skills and learning of image patterns which might be a key to a more animal-like type of intelligence. Here, we propose a continual generalization of the chunking problem (an unsupervised problem), encompassing fixed and probabilistic chunks, discovery of temporal and causal structures and their continual variations. Additionally, we propose an algorithm called SyncMap that can learn and adapt to changes in the problem by creating a dynamic map which preserves the correlation between variables. Results of SyncMap suggest that the proposed algorithm learn near optimal solutions, despite the presence of many types of structures and their continual variation. When compared to Word2vec, PARSER and MRIL, SyncMap surpasses or ties with the best algorithm on $66\%$ of the scenarios while being the second best in the remaining $34\%$. SyncMap's model-free simple dynamics and the absence of loss functions reveal that, perhaps surprisingly, much can be done with self-organization alone. Code available at https://github.com/zweifel/SyncMap.



### Cityscapes 3D: Dataset and Benchmark for 9 DoF Vehicle Detection
- **Arxiv ID**: http://arxiv.org/abs/2006.07864v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.07864v1)
- **Published**: 2020-06-14 10:56:27+00:00
- **Updated**: 2020-06-14 10:56:27+00:00
- **Authors**: Nils Gählert, Nicolas Jourdan, Marius Cordts, Uwe Franke, Joachim Denzler
- **Comment**: 2020 "Scalability in Autonomous Driving" CVPR Workshop
- **Journal**: None
- **Summary**: Detecting vehicles and representing their position and orientation in the three dimensional space is a key technology for autonomous driving. Recently, methods for 3D vehicle detection solely based on monocular RGB images gained popularity. In order to facilitate this task as well as to compare and drive state-of-the-art methods, several new datasets and benchmarks have been published. Ground truth annotations of vehicles are usually obtained using lidar point clouds, which often induces errors due to imperfect calibration or synchronization between both sensors. To this end, we propose Cityscapes 3D, extending the original Cityscapes dataset with 3D bounding box annotations for all types of vehicles. In contrast to existing datasets, our 3D annotations were labeled using stereo RGB images only and capture all nine degrees of freedom. This leads to a pixel-accurate reprojection in the RGB image and a higher range of annotations compared to lidar-based approaches. In order to ease multitask learning, we provide a pairing of 2D instance segments with 3D bounding boxes. In addition, we complement the Cityscapes benchmark suite with 3D vehicle detection based on the new annotations as well as metrics presented in this work. Dataset and benchmark are available online.



### Explicitly Modeled Attention Maps for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2006.07872v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.07872v2)
- **Published**: 2020-06-14 11:47:09+00:00
- **Updated**: 2021-03-18 14:18:57+00:00
- **Authors**: Andong Tan, Duc Tam Nguyen, Maximilian Dax, Matthias Nießner, Thomas Brox
- **Comment**: Accepted by AAAI2021
- **Journal**: None
- **Summary**: Self-attention networks have shown remarkable progress in computer vision tasks such as image classification. The main benefit of the self-attention mechanism is the ability to capture long-range feature interactions in attention-maps. However, the computation of attention-maps requires a learnable key, query, and positional encoding, whose usage is often not intuitive and computationally expensive. To mitigate this problem, we propose a novel self-attention module with explicitly modeled attention-maps using only a single learnable parameter for low computational overhead. The design of explicitly modeled attention-maps using geometric prior is based on the observation that the spatial context for a given pixel within an image is mostly dominated by its neighbors, while more distant pixels have a minor contribution. Concretely, the attention-maps are parametrized via simple functions (e.g., Gaussian kernel) with a learnable radius, which is modeled independently of the input content. Our evaluation shows that our method achieves an accuracy improvement of up to 2.2% over the ResNet-baselines in ImageNet ILSVRC and outperforms other self-attention methods such as AA-ResNet152 in accuracy by 0.9% with 6.4% fewer parameters and 6.7% fewer GFLOPs. This result empirically indicates the value of incorporating geometric prior into self-attention mechanism when applied in image classification.



### FenceMask: A Data Augmentation Approach for Pre-extracted Image Features
- **Arxiv ID**: http://arxiv.org/abs/2006.07877v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.07877v1)
- **Published**: 2020-06-14 12:16:16+00:00
- **Updated**: 2020-06-14 12:16:16+00:00
- **Authors**: Pu Li, Xiangyang Li, Xiang Long
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel data augmentation method named 'FenceMask' that exhibits outstanding performance in various computer vision tasks. It is based on the 'simulation of object occlusion' strategy, which aim to achieve the balance between object occlusion and information retention of the input data. By enhancing the sparsity and regularity of the occlusion block, our augmentation method overcome the difficulty of small object augmentation and notably improve performance over baselines. Sufficient experiments prove the performance of our method is better than other simulate object occlusion approaches. We tested it on CIFAR10, CIFAR100 and ImageNet datasets for Coarse-grained classification, COCO2017 and VisDrone datasets for detection, Oxford Flowers, Cornel Leaf and Stanford Dogs datasets for Fine-Grained Visual Categorization. Our method achieved significant performance improvement on Fine-Grained Visual Categorization task and VisDrone dataset.



### Optical Music Recognition: State of the Art and Major Challenges
- **Arxiv ID**: http://arxiv.org/abs/2006.07885v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.07885v2)
- **Published**: 2020-06-14 12:40:17+00:00
- **Updated**: 2020-06-22 16:33:59+00:00
- **Authors**: Elona Shatri, György Fazekas
- **Comment**: Author manuscript for TENOR 2020 conference. 11 pages with
  references, 3 figures
- **Journal**: None
- **Summary**: Optical Music Recognition (OMR) is concerned with transcribing sheet music into a machine-readable format. The transcribed copy should allow musicians to compose, play and edit music by taking a picture of a music sheet. Complete transcription of sheet music would also enable more efficient archival. OMR facilitates examining sheet music statistically or searching for patterns of notations, thus helping use cases in digital musicology too. Recently, there has been a shift in OMR from using conventional computer vision techniques towards a deep learning approach. In this paper, we review relevant works in OMR, including fundamental methods and significant outcomes, and highlight different stages of the OMR pipeline. These stages often lack standard input and output representation and standardised evaluation. Therefore, comparing different approaches and evaluating the impact of different processing methods can become rather complex. This paper provides recommendations for future work, addressing some of the highlighted issues and represents a position in furthering this important field of research.



### Multi-view Low-rank Preserving Embedding: A Novel Method for Multi-view Representation
- **Arxiv ID**: http://arxiv.org/abs/2006.10520v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.10520v1)
- **Published**: 2020-06-14 12:47:25+00:00
- **Updated**: 2020-06-14 12:47:25+00:00
- **Authors**: Xiangzhu Meng, Lin Feng, Huibing Wang
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, we have witnessed a surge of interest in multi-view representation learning, which is concerned with the problem of learning representations of multi-view data. When facing multiple views that are highly related but sightly different from each other, most of existing multi-view methods might fail to fully integrate multi-view information. Besides, correlations between features from multiple views always vary seriously, which makes multi-view representation challenging. Therefore, how to learn appropriate embedding from multi-view information is still an open problem but challenging. To handle this issue, this paper proposes a novel multi-view learning method, named Multi-view Low-rank Preserving Embedding (MvLPE). It integrates different views into one centroid view by minimizing the disagreement term, based on distance or similarity matrix among instances, between the centroid view and each view meanwhile maintaining low-rank reconstruction relations among samples for each view, which could make more full use of compatible and complementary information from multi-view features. Unlike existing methods with additive parameters, the proposed method could automatically allocate a suitable weight for each view in multi-view information fusion. However, MvLPE couldn't be directly solved, which makes the proposed MvLPE difficult to obtain an analytic solution. To this end, we approximate this solution based on stationary hypothesis and normalization post-processing to efficiently obtain the optimal solution. Furthermore, an iterative alternating strategy is provided to solve this multi-view representation problem. The experiments on six benchmark datasets demonstrate that the proposed method outperforms its counterparts while achieving very competitive performance.



### Team RUC_AIM3 Technical Report at Activitynet 2020 Task 2: Exploring Sequential Events Detection for Dense Video Captioning
- **Arxiv ID**: http://arxiv.org/abs/2006.07896v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.07896v1)
- **Published**: 2020-06-14 13:21:37+00:00
- **Updated**: 2020-06-14 13:21:37+00:00
- **Authors**: Yuqing Song, Shizhe Chen, Yida Zhao, Qin Jin
- **Comment**: Winner solution in CVPR 2020 Activitynet Dense Video Captioning
  challenge
- **Journal**: None
- **Summary**: Detecting meaningful events in an untrimmed video is essential for dense video captioning. In this work, we propose a novel and simple model for event sequence generation and explore temporal relationships of the event sequence in the video. The proposed model omits inefficient two-stage proposal generation and directly generates event boundaries conditioned on bi-directional temporal dependency in one pass. Experimental results show that the proposed event sequence generation model can generate more accurate and diverse events within a small number of proposals. For the event captioning, we follow our previous work to employ the intra-event captioning models into our pipeline system. The overall system achieves state-of-the-art performance on the dense-captioning events in video task with 9.894 METEOR score on the challenge testing set.



### Leveraging Multimodal Behavioral Analytics for Automated Job Interview Performance Assessment and Feedback
- **Arxiv ID**: http://arxiv.org/abs/2006.07909v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CL, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.07909v2)
- **Published**: 2020-06-14 14:20:42+00:00
- **Updated**: 2020-06-16 14:18:05+00:00
- **Authors**: Anumeha Agrawal, Rosa Anil George, Selvan Sunitha Ravi, Sowmya Kamath S, Anand Kumar M
- **Comment**: 9 pages, ACL 2020
- **Journal**: None
- **Summary**: Behavioral cues play a significant part in human communication and cognitive perception. In most professional domains, employee recruitment policies are framed such that both professional skills and personality traits are adequately assessed. Hiring interviews are structured to evaluate expansively a potential employee's suitability for the position - their professional qualifications, interpersonal skills, ability to perform in critical and stressful situations, in the presence of time and resource constraints, etc. Therefore, candidates need to be aware of their positive and negative attributes and be mindful of behavioral cues that might have adverse effects on their success. We propose a multimodal analytical framework that analyzes the candidate in an interview scenario and provides feedback for predefined labels such as engagement, speaking rate, eye contact, etc. We perform a comprehensive analysis that includes the interviewee's facial expressions, speech, and prosodic information, using the video, audio, and text transcripts obtained from the recorded interview. We use these multimodal data sources to construct a composite representation, which is used for training machine learning classifiers to predict the class labels. Such analysis is then used to provide constructive feedback to the interviewee for their behavioral cues and body language. Experimental validation showed that the proposed methodology achieved promising results.



### Multidimensional Wavelets for Scalable Image Decomposition: Orbital Wavelets
- **Arxiv ID**: http://arxiv.org/abs/2006.07920v1
- **DOI**: 10.1142/S0219691320500381
- **Categories**: **eess.SP**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.07920v1)
- **Published**: 2020-06-14 14:55:55+00:00
- **Updated**: 2020-06-14 14:55:55+00:00
- **Authors**: H. M. de Oliveira, V. V. Vermehren, R. J. Cintra
- **Comment**: Fixed typo in Figure 2(b) and Figure 3(b); 9 pages, 4 figures.
  International Journal of Wavelets, Multiresolution and Information
  Processing, 2020
- **Journal**: ijwmip vol.18, issue N 05, 2020
- **Summary**: Wavelets are closely related to the Schr\"odinger's wave functions and the interpretation of Born. Similarly to the appearance of atomic orbital, it is proposed to combine anti-symmetric wavelets into orbital wavelets. The proposed approach allows the increase of the dimension of wavelets through this process. New orbital 2D-wavelets are introduced for the decomposition of still images, showing that it is possible to perform an analysis simultaneous in two distinct scales. An example of such an image analysis is shown.



### TURB-Rot. A large database of 3d and 2d snapshots from turbulent rotating flows
- **Arxiv ID**: http://arxiv.org/abs/2006.07469v1
- **DOI**: None
- **Categories**: **physics.flu-dyn**, cs.CV, physics.data-an, physics.geo-ph, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.07469v1)
- **Published**: 2020-06-14 15:52:43+00:00
- **Updated**: 2020-06-14 15:52:43+00:00
- **Authors**: L. Biferale, F. Bonaccorso, M. Buzzicotti, P. Clark di Leoni
- **Comment**: None
- **Journal**: None
- **Summary**: We present TURB-Rot, a new open database of 3d and 2d snapshots of turbulent velocity fields, obtained by Direct Numerical Simulations (DNS) of the original Navier-Stokes equations in the presence of rotation. The aim is to provide the community interested in data-assimilation and/or computer vision with a new testing-ground made of roughly 300K complex images and fields. TURB-Rot data are characterized by multi-scales strongly non-Gaussian features and rough, non-differentiable, fields over almost two decades of scales. In addition, coming from fully resolved numerical simulations of the original partial differential equations, they offer the possibility to apply a wide range of approaches, from equation-free to physics-based models. TURB-Rot data are reachable at http://smart-turb.roma2.infn.it



### Meta Approach to Data Augmentation Optimization
- **Arxiv ID**: http://arxiv.org/abs/2006.07965v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.07965v1)
- **Published**: 2020-06-14 18:11:52+00:00
- **Updated**: 2020-06-14 18:11:52+00:00
- **Authors**: Ryuichiro Hataya, Jan Zdenek, Kazuki Yoshizoe, Hideki Nakayama
- **Comment**: None
- **Journal**: None
- **Summary**: Data augmentation policies drastically improve the performance of image recognition tasks, especially when the policies are optimized for the target data and tasks. In this paper, we propose to optimize image recognition models and data augmentation policies simultaneously to improve the performance using gradient descent. Unlike prior methods, our approach avoids using proxy tasks or reducing search space, and can directly improve the validation performance. Our method achieves efficient and scalable training by approximating the gradient of policies by implicit gradient with Neumann series approximation. We demonstrate that our approach can improve the performance of various image classification tasks, including ImageNet classification and fine-grained recognition, without using dataset-specific hyperparameter tuning.



### Actor-Context-Actor Relation Network for Spatio-Temporal Action Localization
- **Arxiv ID**: http://arxiv.org/abs/2006.07976v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.07976v3)
- **Published**: 2020-06-14 18:51:49+00:00
- **Updated**: 2021-04-20 20:30:27+00:00
- **Authors**: Junting Pan, Siyu Chen, Mike Zheng Shou, Yu Liu, Jing Shao, Hongsheng Li
- **Comment**: Accepted in CVPR 2021
- **Journal**: None
- **Summary**: Localizing persons and recognizing their actions from videos is a challenging task towards high-level video understanding. Recent advances have been achieved by modeling direct pairwise relations between entities. In this paper, we take one step further, not only model direct relations between pairs but also take into account indirect higher-order relations established upon multiple elements. We propose to explicitly model the Actor-Context-Actor Relation, which is the relation between two actors based on their interactions with the context. To this end, we design an Actor-Context-Actor Relation Network (ACAR-Net) which builds upon a novel High-order Relation Reasoning Operator and an Actor-Context Feature Bank to enable indirect relation reasoning for spatio-temporal action localization. Experiments on AVA and UCF101-24 datasets show the advantages of modeling actor-context-actor relations, and visualization of attention maps further verifies that our model is capable of finding relevant higher-order relations to support action detection. Notably, our method ranks first in the AVA-Kineticsaction localization task of ActivityNet Challenge 2020, out-performing other entries by a significant margin (+6.71mAP). Training code and models will be available at https://github.com/Siyu-C/ACAR-Net.



### Geodesic-HOF: 3D Reconstruction Without Cutting Corners
- **Arxiv ID**: http://arxiv.org/abs/2006.07981v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.07981v1)
- **Published**: 2020-06-14 18:59:06+00:00
- **Updated**: 2020-06-14 18:59:06+00:00
- **Authors**: Ziyun Wang, Eric A. Mitchell, Volkan Isler, Daniel D. Lee
- **Comment**: None
- **Journal**: None
- **Summary**: Single-view 3D object reconstruction is a challenging fundamental problem in computer vision, largely due to the morphological diversity of objects in the natural world. In particular, high curvature regions are not always captured effectively by methods trained using only set-based loss functions, resulting in reconstructions short-circuiting the surface or cutting corners. In particular, high curvature regions are not always captured effectively by methods trained using only set-based loss functions, resulting in reconstructions short-circuiting the surface or cutting corners. To address this issue, we propose learning an image-conditioned mapping function from a canonical sampling domain to a high dimensional space where the Euclidean distance is equal to the geodesic distance on the object. The first three dimensions of a mapped sample correspond to its 3D coordinates. The additional lifted components contain information about the underlying geodesic structure. Our results show that taking advantage of these learned lifted coordinates yields better performance for estimating surface normals and generating surfaces than using point cloud reconstructions alone. Further, we find that this learned geodesic embedding space provides useful information for applications such as unsupervised object decomposition.



### ShapeFlow: Learnable Deformations Among 3D Shapes
- **Arxiv ID**: http://arxiv.org/abs/2006.07982v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2006.07982v2)
- **Published**: 2020-06-14 19:03:35+00:00
- **Updated**: 2021-06-23 22:06:18+00:00
- **Authors**: Chiyu "Max" Jiang, Jingwei Huang, Andrea Tagliasacchi, Leonidas Guibas
- **Comment**: None
- **Journal**: None
- **Summary**: We present ShapeFlow, a flow-based model for learning a deformation space for entire classes of 3D shapes with large intra-class variations. ShapeFlow allows learning a multi-template deformation space that is agnostic to shape topology, yet preserves fine geometric details. Different from a generative space where a latent vector is directly decoded into a shape, a deformation space decodes a vector into a continuous flow that can advect a source shape towards a target. Such a space naturally allows the disentanglement of geometric style (coming from the source) and structural pose (conforming to the target). We parametrize the deformation between geometries as a learned continuous flow field via a neural network and show that such deformations can be guaranteed to have desirable properties, such as be bijectivity, freedom from self-intersections, or volume preservation. We illustrate the effectiveness of this learned deformation space for various downstream applications, including shape generation via deformation, geometric style transfer, unsupervised learning of a consistent parameterization for entire classes of shapes, and shape interpolation.



### GradAug: A New Regularization Method for Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2006.07989v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.07989v2)
- **Published**: 2020-06-14 19:30:34+00:00
- **Updated**: 2020-10-12 18:20:51+00:00
- **Authors**: Taojiannan Yang, Sijie Zhu, Chen Chen
- **Comment**: Accepted to NeurIPS 2020
- **Journal**: None
- **Summary**: We propose a new regularization method to alleviate over-fitting in deep neural networks. The key idea is utilizing randomly transformed training samples to regularize a set of sub-networks, which are originated by sampling the width of the original network, in the training process. As such, the proposed method introduces self-guided disturbances to the raw gradients of the network and therefore is termed as Gradient Augmentation (GradAug). We demonstrate that GradAug can help the network learn well-generalized and more diverse representations. Moreover, it is easy to implement and can be applied to various structures and applications. GradAug improves ResNet-50 to 78.79% on ImageNet classification, which is a new state-of-the-art accuracy. By combining with CutMix, it further boosts the performance to 79.67%, which outperforms an ensemble of advanced training tricks. The generalization ability is evaluated on COCO object detection and instance segmentation where GradAug significantly surpasses other state-of-the-art methods. GradAug is also robust to image distortions and FGSM adversarial attacks and is highly effective in low data regimes. Code is available at https://github.com/taoyang1122/GradAug



### Emergent Properties of Foveated Perceptual Systems
- **Arxiv ID**: http://arxiv.org/abs/2006.07991v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/2006.07991v3)
- **Published**: 2020-06-14 19:34:44+00:00
- **Updated**: 2021-06-22 21:21:08+00:00
- **Authors**: Arturo Deza, Talia Konkle
- **Comment**: An updated pre-print. Currently under review at NeurIPS 2021. Themes:
  Foveation, Machine Perception & Representation Learning
- **Journal**: None
- **Summary**: The goal of this work is to characterize the representational impact that foveation operations have for machine vision systems, inspired by the foveated human visual system, which has higher acuity at the center of gaze and texture-like encoding in the periphery. To do so, we introduce models consisting of a first-stage \textit{fixed} image transform followed by a second-stage \textit{learnable} convolutional neural network, and we varied the first stage component. The primary model has a foveated-textural input stage, which we compare to a model with foveated-blurred input and a model with spatially-uniform blurred input (both matched for perceptual compression), and a final reference model with minimal input-based compression. We find that: 1) the foveated-texture model shows similar scene classification accuracy as the reference model despite its compressed input, with greater i.i.d. generalization than the other models; 2) the foveated-texture model has greater sensitivity to high-spatial frequency information and greater robustness to occlusion, w.r.t the comparison models; 3) both the foveated systems, show a stronger center image-bias relative to the spatially-uniform systems even with a weight sharing constraint. Critically, these results are preserved over different classical CNN architectures throughout their learning dynamics. Altogether, this suggests that foveation with peripheral texture-based computations yields an efficient, distinct, and robust representational format of scene information, and provides symbiotic computational insight into the representational consequences that texture-based peripheral encoding may have for processing in the human visual system, while also potentially inspiring the next generation of computer vision models via spatially-adaptive computation. Code + Data available here: https://github.com/ArturoDeza/EmergentProperties



### Road Mapping in Low Data Environments with OpenStreetMap
- **Arxiv ID**: http://arxiv.org/abs/2006.07993v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.07993v1)
- **Published**: 2020-06-14 19:39:57+00:00
- **Updated**: 2020-06-14 19:39:57+00:00
- **Authors**: John Kamalu, Benjamin Choi
- **Comment**: None
- **Journal**: None
- **Summary**: Roads are among the most essential components of any country's infrastructure. By facilitating the movement and exchange of people, ideas, and goods, they support economic and cultural activity both within and across local and international borders. A comprehensive, up-to-date mapping of the geographical distribution of roads and their quality thus has the potential to act as an indicator for broader economic development. Such an indicator has a variety of high-impact applications, particularly in the planning of rural development projects where up-to-date infrastructure information is not available. This work investigates the viability of high resolution satellite imagery and crowd-sourced resources like OpenStreetMap in the construction of such a mapping. We experiment with state-of-the-art deep learning methods to explore the utility of OpenStreetMap data in road classification and segmentation tasks. We also compare the performance of models in different mask occlusion scenarios as well as out-of-country domains. Our comparison raises important pitfalls to consider in image-based infrastructure classification tasks, and shows the need for local training data specific to regions of interest for reliable performance.



### BatVision with GCC-PHAT Features for Better Sound to Vision Predictions
- **Arxiv ID**: http://arxiv.org/abs/2006.07995v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2006.07995v1)
- **Published**: 2020-06-14 19:49:58+00:00
- **Updated**: 2020-06-14 19:49:58+00:00
- **Authors**: Jesper Haahr Christensen, Sascha Hornauer, Stella Yu
- **Comment**: None
- **Journal**: None
- **Summary**: Inspired by sophisticated echolocation abilities found in nature, we train a generative adversarial network to predict plausible depth maps and grayscale layouts from sound. To achieve this, our sound-to-vision model processes binaural echo-returns from chirping sounds. We build upon previous work with BatVision that consists of a sound-to-vision model and a self-collected dataset using our mobile robot and low-cost hardware. We improve on the previous model by introducing several changes to the model, which leads to a better depth and grayscale estimation, and increased perceptual quality. Rather than using raw binaural waveforms as input, we generate generalized cross-correlation (GCC) features and use these as input instead. In addition, we change the model generator and base it on residual learning and use spectral normalization in the discriminator. We compare and present both quantitative and qualitative improvements over our previous BatVision model.



### CompressNet: Generative Compression at Extremely Low Bitrates
- **Arxiv ID**: http://arxiv.org/abs/2006.08003v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.08003v1)
- **Published**: 2020-06-14 20:03:12+00:00
- **Updated**: 2020-06-14 20:03:12+00:00
- **Authors**: Suraj Kiran Raman, Aditya Ramesh, Vijayakrishna Naganoor, Shubham Dash, Giridharan Kumaravelu, Honglak Lee
- **Comment**: None
- **Journal**: None
- **Summary**: Compressing images at extremely low bitrates (< 0.1 bpp) has always been a challenging task since the quality of reconstruction significantly reduces due to the strong imposed constraint on the number of bits allocated for the compressed data. With the increasing need to transfer large amounts of images with limited bandwidth, compressing images to very low sizes is a crucial task. However, the existing methods are not effective at extremely low bitrates. To address this need, we propose a novel network called CompressNet which augments a Stacked Autoencoder with a Switch Prediction Network (SAE-SPN). This helps in the reconstruction of visually pleasing images at these low bitrates (< 0.1 bpp). We benchmark the performance of our proposed method on the Cityscapes dataset, evaluating over different metrics at extremely low bitrates to show that our method outperforms the other state-of-the-art. In particular, at a bitrate of 0.07, CompressNet achieves 22% lower Perceptual Loss and 55% lower Frechet Inception Distance (FID) compared to the deep learning SOTA methods.



### RasterNet: Modeling Free-Flow Speed using LiDAR and Overhead Imagery
- **Arxiv ID**: http://arxiv.org/abs/2006.08021v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.08021v1)
- **Published**: 2020-06-14 21:03:52+00:00
- **Updated**: 2020-06-14 21:03:52+00:00
- **Authors**: Armin Hadzic, Hunter Blanton, Weilian Song, Mei Chen, Scott Workman, Nathan Jacobs
- **Comment**: None
- **Journal**: None
- **Summary**: Roadway free-flow speed captures the typical vehicle speed in low traffic conditions. Modeling free-flow speed is an important problem in transportation engineering with applications to a variety of design, operation, planning, and policy decisions of highway systems. Unfortunately, collecting large-scale historical traffic speed data is expensive and time consuming. Traditional approaches for estimating free-flow speed use geometric properties of the underlying road segment, such as grade, curvature, lane width, lateral clearance and access point density, but for many roads such features are unavailable. We propose a fully automated approach, RasterNet, for estimating free-flow speed without the need for explicit geometric features. RasterNet is a neural network that fuses large-scale overhead imagery and aerial LiDAR point clouds using a geospatially consistent raster structure. To support training and evaluation, we introduce a novel dataset combining free-flow speeds of road segments, overhead imagery, and LiDAR point clouds across the state of Kentucky. Our method achieves state-of-the-art results on a benchmark dataset.



