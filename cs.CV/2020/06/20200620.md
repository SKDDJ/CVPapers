# Arxiv Papers in cs.CV on 2020-06-20
### Towards Adaptive Benthic Habitat Mapping
- **Arxiv ID**: http://arxiv.org/abs/2006.11453v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.11453v1)
- **Published**: 2020-06-20 01:03:41+00:00
- **Updated**: 2020-06-20 01:03:41+00:00
- **Authors**: Jackson Shields, Oscar Pizarro, Stefan B. Williams
- **Comment**: To be published in ICRA2020 conference proceedings. 6 pages, 7
  figures
- **Journal**: None
- **Summary**: Autonomous Underwater Vehicles (AUVs) are increasingly being used to support scientific research and monitoring studies. One such application is in benthic habitat mapping where these vehicles collect seafloor imagery that complements broadscale bathymetric data collected using sonar. Using these two data sources, the relationship between remotely-sensed acoustic data and the sampled imagery can be learned, creating a habitat model. As the areas to be mapped are often very large and AUV systems collecting seafloor imagery can only sample from a small portion of the survey area, the information gathered should be maximised for each deployment. This paper illustrates how the habitat models themselves can be used to plan more efficient AUV surveys by identifying where to collect further samples in order to most improve the habitat model. A Bayesian neural network is used to predict visually-derived habitat classes when given broad-scale bathymetric data. This network can also estimate the uncertainty associated with a prediction, which can be deconstructed into its aleatoric (data) and epistemic (model) components. We demonstrate how these structured uncertainty estimates can be utilised to improve the model with fewer samples. Such adaptive approaches to benthic surveys have the potential to reduce costs by prioritizing further sampling efforts. We illustrate the effectiveness of the proposed approach using data collected by an AUV on offshore reefs in Tasmania, Australia.



### Exemplar Loss for Siamese Network in Visual Tracking
- **Arxiv ID**: http://arxiv.org/abs/2006.12987v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.12987v2)
- **Published**: 2020-06-20 01:47:53+00:00
- **Updated**: 2020-10-04 02:29:33+00:00
- **Authors**: Shuo Chang, YiFan Zhang, Sai Huang, Yuanyuan Yao, Zhiyong Feng
- **Comment**: The experiment results have some error. And the pdf format is not
  proper
- **Journal**: None
- **Summary**: Visual tracking plays an important role in perception system, which is a crucial part of intelligent transportation. Recently, Siamese network is a hot topic for visual tracking to estimate moving targets' trajectory, due to its superior accuracy and simple framework. In general, Siamese tracking algorithms, supervised by logistic loss and triplet loss, increase the value of inner product between exemplar template and positive sample while reduce the value of inner product with background sample. However, the distractors from different exemplars are not considered by mentioned loss functions, which limit the feature models' discrimination. In this paper, a new exemplar loss integrated with logistic loss is proposed to enhance the feature model's discrimination by reducing inner products among exemplars. Without the bells and whistles, the proposed algorithm outperforms the methods supervised by logistic loss or triplet loss. Numerical results suggest that the newly developed algorithm achieves comparable performance in public benchmarks.



### Video Playback Rate Perception for Self-supervisedSpatio-Temporal Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2006.11476v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.11476v1)
- **Published**: 2020-06-20 02:26:07+00:00
- **Updated**: 2020-06-20 02:26:07+00:00
- **Authors**: Yuan Yao, Chang Liu, Dezhao Luo, Yu Zhou, Qixiang Ye
- **Comment**: CVPR 2020
- **Journal**: None
- **Summary**: In self-supervised spatio-temporal representation learning, the temporal resolution and long-short term characteristics are not yet fully explored, which limits representation capabilities of learned models. In this paper, we propose a novel self-supervised method, referred to as video Playback Rate Perception (PRP), to learn spatio-temporal representation in a simple-yet-effective way. PRP roots in a dilated sampling strategy, which produces self-supervision signals about video playback rates for representation model learning. PRP is implemented with a feature encoder, a classification module, and a reconstructing decoder, to achieve spatio-temporal semantic retention in a collaborative discrimination-generation manner. The discriminative perception model follows a feature encoder to prefer perceiving low temporal resolution and long-term representation by classifying fast-forward rates. The generative perception model acts as a feature decoder to focus on comprehending high temporal resolution and short-term representation by introducing a motion-attention mechanism. PRP is applied on typical video target tasks including action recognition and video retrieval. Experiments show that PRP outperforms state-of-the-art self-supervised models with significant margins. Code is available at github.com/yuanyao366/PRP



### Unsupervised Image Classification for Deep Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2006.11480v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.11480v2)
- **Published**: 2020-06-20 02:57:06+00:00
- **Updated**: 2020-08-20 06:42:41+00:00
- **Authors**: Weijie Chen, Shiliang Pu, Di Xie, Shicai Yang, Yilu Guo, Luojun Lin
- **Comment**: Accepted by ECCV2020 Workshop VIPriors
- **Journal**: None
- **Summary**: Deep clustering against self-supervised learning is a very important and promising direction for unsupervised visual representation learning since it requires little domain knowledge to design pretext tasks. However, the key component, embedding clustering, limits its extension to the extremely large-scale dataset due to its prerequisite to save the global latent embedding of the entire dataset. In this work, we aim to make this framework more simple and elegant without performance decline. We propose an unsupervised image classification framework without using embedding clustering, which is very similar to standard supervised training manner. For detailed interpretation, we further analyze its relation with deep clustering and contrastive learning. Extensive experiments on ImageNet dataset have been conducted to prove the effectiveness of our method. Furthermore, the experiments on transfer learning benchmarks have verified its generalization to other downstream tasks, including multi-label image classification, object detection, semantic segmentation and few-shot image classification.



### Pseudo-LiDAR Point Cloud Interpolation Based on 3D Motion Representation and Spatial Supervision
- **Arxiv ID**: http://arxiv.org/abs/2006.11481v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.11481v1)
- **Published**: 2020-06-20 03:11:04+00:00
- **Updated**: 2020-06-20 03:11:04+00:00
- **Authors**: Haojie Liu, Kang Liao, Chunyu Lin, Yao Zhao, Yulan Guo
- **Comment**: 10 pages, 6 figures
- **Journal**: None
- **Summary**: Pseudo-LiDAR point cloud interpolation is a novel and challenging task in the field of autonomous driving, which aims to address the frequency mismatching problem between camera and LiDAR. Previous works represent the 3D spatial motion relationship induced by a coarse 2D optical flow, and the quality of interpolated point clouds only depends on the supervision of depth maps. As a result, the generated point clouds suffer from inferior global distributions and local appearances. To solve the above problems, we propose a Pseudo-LiDAR point cloud interpolation network to generates temporally and spatially high-quality point cloud sequences. By exploiting the scene flow between point clouds, the proposed network is able to learn a more accurate representation of the 3D spatial motion relationship. For the more comprehensive perception of the distribution of point cloud, we design a novel reconstruction loss function that implements the chamfer distance to supervise the generation of Pseudo-LiDAR point clouds in 3D space. In addition, we introduce a multi-modal deep aggregation module to facilitate the efficient fusion of texture and depth features. As the benefits of the improved motion representation, training loss function, and model structure, our approach gains significant improvements on the Pseudo-LiDAR point cloud interpolation task. The experimental results evaluated on KITTI dataset demonstrate the state-of-the-art performance of the proposed network, quantitatively and qualitatively.



### Unsupervised Vehicle Re-identification with Progressive Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2006.11486v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.11486v1)
- **Published**: 2020-06-20 03:59:41+00:00
- **Updated**: 2020-06-20 03:59:41+00:00
- **Authors**: Jinjia Peng, Yang Wang, Huibing Wang, Zhao Zhang, Xianping Fu, Meng Wang
- **Comment**: Appearing at IJCAI 2020
- **Journal**: None
- **Summary**: Vehicle re-identification (reID) aims at identifying vehicles across different non-overlapping cameras views. The existing methods heavily relied on well-labeled datasets for ideal performance, which inevitably causes fateful drop due to the severe domain bias between the training domain and the real-world scenes; worse still, these approaches required full annotations, which is labor-consuming. To tackle these challenges, we propose a novel progressive adaptation learning method for vehicle reID, named PAL, which infers from the abundant data without annotations. For PAL, a data adaptation module is employed for source domain, which generates the images with similar data distribution to unlabeled target domain as ``pseudo target samples''. These pseudo samples are combined with the unlabeled samples that are selected by a dynamic sampling strategy to make training faster. We further proposed a weighted label smoothing (WLS) loss, which considers the similarity between samples with different clusters to balance the confidence of pseudo labels. Comprehensive experimental results validate the advantages of PAL on both VehicleID and VeRi-776 dataset.



### Paying more attention to snapshots of Iterative Pruning: Improving Model Compression via Ensemble Distillation
- **Arxiv ID**: http://arxiv.org/abs/2006.11487v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.11487v3)
- **Published**: 2020-06-20 03:59:46+00:00
- **Updated**: 2020-08-14 05:41:26+00:00
- **Authors**: Duong H. Le, Trung-Nhan Vo, Nam Thoai
- **Comment**: BMVC 2020 - Camera ready
- **Journal**: None
- **Summary**: Network pruning is one of the most dominant methods for reducing the heavy inference cost of deep neural networks. Existing methods often iteratively prune networks to attain high compression ratio without incurring significant loss in performance. However, we argue that conventional methods for retraining pruned networks (i.e., using small, fixed learning rate) are inadequate as they completely ignore the benefits from snapshots of iterative pruning. In this work, we show that strong ensembles can be constructed from snapshots of iterative pruning, which achieve competitive performance and vary in network structure. Furthermore, we present simple, general and effective pipeline that generates strong ensembles of networks during pruning with large learning rate restarting, and utilizes knowledge distillation with those ensembles to improve the predictive power of compact models. In standard image classification benchmarks such as CIFAR and Tiny-Imagenet, we advance state-of-the-art pruning ratio of structured pruning by integrating simple l1-norm filters pruning into our pipeline. Specifically, we reduce 75-80% of total parameters and 65-70% MACs of numerous variants of ResNet architectures while having comparable or better performance than that of original networks. Code associate with this paper is made publicly available at https://github.com/lehduong/kesi.



### G-image Segmentation: Similarity-preserving Fuzzy C-Means with Spatial Information Constraint in Wavelet Space
- **Arxiv ID**: http://arxiv.org/abs/2006.11510v2
- **DOI**: 10.1109/TFUZZ.2020.3029285
- **Categories**: **cs.CV**, 62H30, I.4.6
- **Links**: [PDF](http://arxiv.org/pdf/2006.11510v2)
- **Published**: 2020-06-20 07:26:33+00:00
- **Updated**: 2020-07-01 01:43:13+00:00
- **Authors**: Cong Wang, Witold Pedrycz, ZhiWu Li, MengChu Zhou, Shuzhi Sam Ge
- **Comment**: This paper has been withdrawn by the author since some statements are
  not right as raised by other researchers
- **Journal**: IEEE Transactions on Fuzzy Systems, 2020
- **Summary**: G-images refer to image data defined on irregular graph domains. This work elaborates a similarity-preserving Fuzzy C-Means (FCM) algorithm for G-image segmentation and aims to develop techniques and tools for segmenting G-images. To preserve the membership similarity between an arbitrary image pixel and its neighbors, a Kullback-Leibler divergence term on membership partition is introduced as a part of FCM. As a result, similarity-preserving FCM is developed by considering spatial information of image pixels for its robustness enhancement. Due to superior characteristics of a wavelet space, the proposed FCM is performed in this space rather than Euclidean one used in conventional FCM to secure its high robustness. Experiments on synthetic and real-world G-images demonstrate that it indeed achieves higher robustness and performance than the state-of-the-art FCM algorithms. Moreover, it requires less computation than most of them.



### Neuro-Symbolic Visual Reasoning: Disentangling "Visual" from "Reasoning"
- **Arxiv ID**: http://arxiv.org/abs/2006.11524v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.NE, cs.SC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.11524v3)
- **Published**: 2020-06-20 08:48:29+00:00
- **Updated**: 2020-08-25 23:30:57+00:00
- **Authors**: Saeed Amizadeh, Hamid Palangi, Oleksandr Polozov, Yichen Huang, Kazuhito Koishida
- **Comment**: Published in Proceedings of the 37th International Conference on
  Machine Learning (ICML), Online, PMLR 119, 2020
- **Journal**: None
- **Summary**: Visual reasoning tasks such as visual question answering (VQA) require an interplay of visual perception with reasoning about the question semantics grounded in perception. However, recent advances in this area are still primarily driven by perception improvements (e.g. scene graph generation) rather than reasoning. Neuro-symbolic models such as Neural Module Networks bring the benefits of compositional reasoning to VQA, but they are still entangled with visual representation learning, and thus neural reasoning is hard to improve and assess on its own. To address this, we propose (1) a framework to isolate and evaluate the reasoning aspect of VQA separately from its perception, and (2) a novel top-down calibration technique that allows the model to answer reasoning questions even with imperfect perception. To this end, we introduce a differentiable first-order logic formalism for VQA that explicitly decouples question answering from visual perception. On the challenging GQA dataset, this framework is used to perform in-depth, disentangled comparisons between well-known VQA models leading to informative insights regarding the participating models as well as the task.



### Remote Sensing Image Scene Classification with Deep Neural Networks in JPEG 2000 Compressed Domain
- **Arxiv ID**: http://arxiv.org/abs/2006.11529v2
- **DOI**: 10.1109/TGRS.2020.3007523
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.11529v2)
- **Published**: 2020-06-20 09:13:38+00:00
- **Updated**: 2020-12-15 17:23:15+00:00
- **Authors**: Akshara Preethy Byju, Gencer Sumbul, Begüm Demir, Lorenzo Bruzzone
- **Comment**: Accepted to IEEE Transactions on Geoscience and Remote Sensing
- **Journal**: None
- **Summary**: To reduce the storage requirements, remote sensing (RS) images are usually stored in compressed format. Existing scene classification approaches using deep neural networks (DNNs) require to fully decompress the images, which is a computationally demanding task in operational applications. To address this issue, in this paper we propose a novel approach to achieve scene classification in JPEG 2000 compressed RS images. The proposed approach consists of two main steps: i) approximation of the finer resolution sub-bands of reversible biorthogonal wavelet filters used in JPEG 2000; and ii) characterization of the high-level semantic content of approximated wavelet sub-bands and scene classification based on the learnt descriptors. This is achieved by taking codestreams associated with the coarsest resolution wavelet sub-band as input to approximate finer resolution sub-bands using a number of transposed convolutional layers. Then, a series of convolutional layers models the high-level semantic content of the approximated wavelet sub-band. Thus, the proposed approach models the multiresolution paradigm given in the JPEG 2000 compression algorithm in an end-to-end trainable unified neural network. In the classification stage, the proposed approach takes only the coarsest resolution wavelet sub-bands as input, thereby reducing the time required to apply decoding. Experimental results performed on two benchmark aerial image archives demonstrate that the proposed approach significantly reduces the computational time with similar classification accuracies when compared to traditional RS scene classification approaches (which requires full image decompression).



### Pyramidal Convolution: Rethinking Convolutional Neural Networks for Visual Recognition
- **Arxiv ID**: http://arxiv.org/abs/2006.11538v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.11538v1)
- **Published**: 2020-06-20 10:19:29+00:00
- **Updated**: 2020-06-20 10:19:29+00:00
- **Authors**: Ionut Cosmin Duta, Li Liu, Fan Zhu, Ling Shao
- **Comment**: None
- **Journal**: None
- **Summary**: This work introduces pyramidal convolution (PyConv), which is capable of processing the input at multiple filter scales. PyConv contains a pyramid of kernels, where each level involves different types of filters with varying size and depth, which are able to capture different levels of details in the scene. On top of these improved recognition capabilities, PyConv is also efficient and, with our formulation, it does not increase the computational cost and parameters compared to standard convolution. Moreover, it is very flexible and extensible, providing a large space of potential network architectures for different applications. PyConv has the potential to impact nearly every computer vision task and, in this work, we present different architectures based on PyConv for four main tasks on visual recognition: image classification, video action classification/recognition, object detection and semantic image segmentation/parsing. Our approach shows significant improvements over all these core tasks in comparison with the baselines. For instance, on image recognition, our 50-layers network outperforms in terms of recognition performance on ImageNet dataset its counterpart baseline ResNet with 152 layers, while having 2.39 times less parameters, 2.52 times lower computational complexity and more than 3 times less layers. On image segmentation, our novel framework sets a new state-of-the-art on the challenging ADE20K benchmark for scene parsing. Code is available at: https://github.com/iduta/pyconv



### On Addressing the Impact of ISO Speed upon PRNU and Forgery Detection
- **Arxiv ID**: http://arxiv.org/abs/2006.11539v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.11539v1)
- **Published**: 2020-06-20 10:23:54+00:00
- **Updated**: 2020-06-20 10:23:54+00:00
- **Authors**: Yijun Quan, Chang-Tsun Li
- **Comment**: The paper is accepted to IEEE Transactions on Information Forensics
  and Security with the supplementary material
- **Journal**: None
- **Summary**: Photo Response Non-Uniformity (PRNU) has been used as a powerful device fingerprint for image forgery detection because image forgeries can be revealed by finding the absence of the PRNU in the manipulated areas. The correlation between an image's noise residual with the device's reference PRNU is often compared with a decision threshold to check the existence of the PRNU. A PRNU correlation predictor is usually used to determine this decision threshold assuming the correlation is content-dependent. However, we found that not only the correlation is content-dependent, but it also depends on the camera sensitivity setting. \textit{Camera sensitivity}, commonly known by the name of \textit{ISO speed}, is an important attribute in digital photography. In this work, we will show the PRNU correlation's dependency on ISO speed. Due to such dependency, we postulate that a correlation predictor is ISO speed-specific, i.e. \textit{reliable correlation predictions can only be made when a correlation predictor is trained with images of similar ISO speeds to the image in question}. We report the experiments we conducted to validate the postulate. It is realized that in the real-world, information about the ISO speed may not be available in the metadata to facilitate the implementation of our postulate in the correlation prediction process. We hence propose a method called Content-based Inference of ISO Speeds (CINFISOS) to infer the ISO speed from the image content.



### One PLOT to Show Them All: Visualization of Efficient Sets in Multi-Objective Landscapes
- **Arxiv ID**: http://arxiv.org/abs/2006.11547v1
- **DOI**: 10.1007/978-3-030-58115-2_11
- **Categories**: **cs.NE**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.11547v1)
- **Published**: 2020-06-20 11:03:11+00:00
- **Updated**: 2020-06-20 11:03:11+00:00
- **Authors**: Lennart Schäpermeier, Christian Grimme, Pascal Kerschke
- **Comment**: This version has been accepted for publication at the 16th
  International Conference on Parallel Problem Solving from Nature (PPSN XVI)
- **Journal**: Proceedings of the 16th International Conference on Parallel
  Problem Solving from Nature (PPSN XVI), pp. 154 - 167, Springer (2020)
- **Summary**: Visualization techniques for the decision space of continuous multi-objective optimization problems (MOPs) are rather scarce in research. For long, all techniques focused on global optimality and even for the few available landscape visualizations, e.g., cost landscapes, globality is the main criterion. In contrast, the recently proposed gradient field heatmaps (GFHs) emphasize the location and attraction basins of local efficient sets, but ignore the relation of sets in terms of solution quality.   In this paper, we propose a new and hybrid visualization technique, which combines the advantages of both approaches in order to represent local and global optimality together within a single visualization. Therefore, we build on the GFH approach but apply a new technique for approximating the location of locally efficient points and using the divergence of the multi-objective gradient vector field as a robust second-order condition. Then, the relative dominance relationship of the determined locally efficient points is used to visualize the complete landscape of the MOP. Augmented by information on the basins of attraction, this Plot of Landscapes with Optimal Trade-offs (PLOT) becomes one of the most informative multi-objective landscape visualization techniques available.



### Driver Intention Anticipation Based on In-Cabin and Driving Scene Monitoring
- **Arxiv ID**: http://arxiv.org/abs/2006.11557v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.11557v1)
- **Published**: 2020-06-20 11:56:32+00:00
- **Updated**: 2020-06-20 11:56:32+00:00
- **Authors**: Yao Rong, Zeynep Akata, Enkelejda Kasneci
- **Comment**: 8 pages, 9 figures
- **Journal**: IEEE Conference on Intelligent Transportation Systems (ITSC), 2020
- **Summary**: Numerous car accidents are caused by improper driving maneuvers. Serious injuries are however avoidable if such driving maneuvers are detected beforehand and the driver is assisted accordingly. In fact, various recent research has focused on the automated prediction of driving maneuver based on hand-crafted features extracted mainly from in-cabin driver videos. Since the outside view from the traffic scene may also contain informative features for driving maneuver prediction, we present a framework for the detection of the drivers' intention based on both in-cabin and traffic scene videos. More specifically, we (1) propose a Convolutional-LSTM (ConvLSTM)-based auto-encoder to extract motion features from the out-cabin traffic, (2) train a classifier which considers motions from both in- and outside of the cabin jointly for maneuver intention anticipation, (3) experimentally prove that the in- and outside image features have complementary information. Our evaluation based on the publicly available dataset Brain4cars shows that our framework achieves a prediction with the accuracy of 83.98% and F1-score of 84.3%.



### Deep Double-Side Learning Ensemble Model for Few-Shot Parkinson Speech Recognition
- **Arxiv ID**: http://arxiv.org/abs/2006.11593v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.11593v1)
- **Published**: 2020-06-20 15:14:41+00:00
- **Updated**: 2020-06-20 15:14:41+00:00
- **Authors**: Yongming Li, Lang Zhou, Lingyun Qin, Yuwei Zeng, Yuchuan Liu, Yan Lei, Pin Wang, Fan Li
- **Comment**: 15 pages, 4 figures
- **Journal**: None
- **Summary**: Diagnosis and therapeutic effect assessment of Parkinson disease based on voice data are very important,but its few-shot learning problem is challenging.Although deep learning is good at automatic feature extraction, it suffers from few-shot learning problem. Therefore, the general effective method is first conduct feature extraction based on prior knowledge, and then carry out feature reduction for subsequent classification. However, there are two major problems: 1) Structural information among speech features has not been mined and new features of higher quality have not been reconstructed. 2) Structural information between data samples has not been mined and new samples with higher quality have not been reconstructed. To solve these two problems, based on the existing Parkinson speech feature data set, a deep double-side learning ensemble model is designed in this paper that can reconstruct speech features and samples deeply and simultaneously. As to feature reconstruction, an embedded deep stacked group sparse auto-encoder is designed in this paper to conduct nonlinear feature transformation, so as to acquire new high-level deep features, and then the deep features are fused with original speech features by L1 regularization feature selection method. As to speech sample reconstruction, a deep sample learning algorithm is designed in this paper based on iterative mean clustering to conduct samples transformation, so as to obtain new high-level deep samples. Finally, the bagging ensemble learning mode is adopted to fuse the deep feature learning algorithm and the deep samples learning algorithm together, thereby constructing a deep double-side learning ensemble model. At the end of this paper, two representative speech datasets of Parkinson's disease were used for verification. The experimental results show that the proposed algorithm are effective.



### How do SGD hyperparameters in natural training affect adversarial robustness?
- **Arxiv ID**: http://arxiv.org/abs/2006.11604v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.11604v1)
- **Published**: 2020-06-20 16:04:44+00:00
- **Updated**: 2020-06-20 16:04:44+00:00
- **Authors**: Sandesh Kamath, Amit Deshpande, K V Subrahmanyam
- **Comment**: Preliminary version presented in ICML 2019 Workshop on "Understanding
  and Improving Generalization in Deep Learning" as "On Adversarial Robustness
  of Small vs Large Batch Training"
- **Journal**: None
- **Summary**: Learning rate, batch size and momentum are three important hyperparameters in the SGD algorithm. It is known from the work of Jastrzebski et al. arXiv:1711.04623 that large batch size training of neural networks yields models which do not generalize well. Yao et al. arXiv:1802.08241 observe that large batch training yields models that have poor adversarial robustness. In the same paper, the authors train models with different batch sizes and compute the eigenvalues of the Hessian of loss function. They observe that as the batch size increases, the dominant eigenvalues of the Hessian become larger. They also show that both adversarial training and small-batch training leads to a drop in the dominant eigenvalues of the Hessian or lowering its spectrum. They combine adversarial training and second order information to come up with a new large-batch training algorithm and obtain robust models with good generalization. In this paper, we empirically observe the effect of the SGD hyperparameters on the accuracy and adversarial robustness of networks trained with unperturbed samples. Jastrzebski et al. considered training models with a fixed learning rate to batch size ratio. They observed that higher the ratio, better is the generalization. We observe that networks trained with constant learning rate to batch size ratio, as proposed in Jastrzebski et al., yield models which generalize well and also have almost constant adversarial robustness, independent of the batch size. We observe that momentum is more effective with varying batch sizes and a fixed learning rate than with constant learning rate to batch size ratio based SGD training.



### Deep Polynomial Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2006.13026v2
- **DOI**: 10.1109/TPAMI.2021.3058891
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.13026v2)
- **Published**: 2020-06-20 16:23:32+00:00
- **Updated**: 2021-02-27 13:32:18+00:00
- **Authors**: Grigorios Chrysos, Stylianos Moschoglou, Giorgos Bouritsas, Jiankang Deng, Yannis Panagakis, Stefanos Zafeiriou
- **Comment**: Published in IEEE Transactions on Pattern Analysis and Machine
  Intelligence (T-PAMI). Code: https://github.com/grigorisg9gr/polynomial_nets.
  arXiv admin note: substantial text overlap with arXiv:2003.03828
- **Journal**: None
- **Summary**: Deep Convolutional Neural Networks (DCNNs) are currently the method of choice both for generative, as well as for discriminative learning in computer vision and machine learning. The success of DCNNs can be attributed to the careful selection of their building blocks (e.g., residual blocks, rectifiers, sophisticated normalization schemes, to mention but a few). In this paper, we propose $\Pi$-Nets, a new class of function approximators based on polynomial expansions. $\Pi$-Nets are polynomial neural networks, i.e., the output is a high-order polynomial of the input. The unknown parameters, which are naturally represented by high-order tensors, are estimated through a collective tensor factorization with factors sharing. We introduce three tensor decompositions that significantly reduce the number of parameters and show how they can be efficiently implemented by hierarchical neural networks. We empirically demonstrate that $\Pi$-Nets are very expressive and they even produce good results without the use of non-linear activation functions in a large battery of tasks and signals, i.e., images, graphs, and audio. When used in conjunction with activation functions, $\Pi$-Nets produce state-of-the-art results in three challenging tasks, i.e. image generation, face verification and 3D mesh representation learning. The source code is available at \url{https://github.com/grigorisg9gr/polynomial_nets}.



### FaceHack: Triggering backdoored facial recognition systems using facial characteristics
- **Arxiv ID**: http://arxiv.org/abs/2006.11623v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.11623v1)
- **Published**: 2020-06-20 17:39:23+00:00
- **Updated**: 2020-06-20 17:39:23+00:00
- **Authors**: Esha Sarkar, Hadjer Benkraouda, Michail Maniatakos
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advances in Machine Learning (ML) have opened up new avenues for its extensive use in real-world applications. Facial recognition, specifically, is used from simple friend suggestions in social-media platforms to critical security applications for biometric validation in automated immigration at airports. Considering these scenarios, security vulnerabilities to such ML algorithms pose serious threats with severe outcomes. Recent work demonstrated that Deep Neural Networks (DNNs), typically used in facial recognition systems, are susceptible to backdoor attacks; in other words,the DNNs turn malicious in the presence of a unique trigger. Adhering to common characteristics for being unnoticeable, an ideal trigger is small, localized, and typically not a part of the main im-age. Therefore, detection mechanisms have focused on detecting these distinct trigger-based outliers statistically or through their reconstruction. In this work, we demonstrate that specific changes to facial characteristics may also be used to trigger malicious behavior in an ML model. The changes in the facial attributes maybe embedded artificially using social-media filters or introduced naturally using movements in facial muscles. By construction, our triggers are large, adaptive to the input, and spread over the entire image. We evaluate the success of the attack and validate that it does not interfere with the performance criteria of the model. We also substantiate the undetectability of our triggers by exhaustively testing them with state-of-the-art defenses.



### Interpretation of 3D CNNs for Brain MRI Data Classification
- **Arxiv ID**: http://arxiv.org/abs/2006.15969v2
- **DOI**: None
- **Categories**: **q-bio.NC**, cs.CV, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.15969v2)
- **Published**: 2020-06-20 17:56:46+00:00
- **Updated**: 2020-10-14 16:14:44+00:00
- **Authors**: Maxim Kan, Ruslan Aliev, Anna Rudenko, Nikita Drobyshev, Nikita Petrashen, Ekaterina Kondrateva, Maxim Sharaev, Alexander Bernstein, Evgeny Burnaev
- **Comment**: 12 pages, 3 figures
- **Journal**: AIST2020
- **Summary**: Deep learning shows high potential for many medical image analysis tasks. Neural networks can work with full-size data without extensive preprocessing and feature generation and, thus, information loss. Recent work has shown that the morphological difference in specific brain regions can be found on MRI with the means of Convolution Neural Networks (CNN). However, interpretation of the existing models is based on a region of interest and can not be extended to voxel-wise image interpretation on a whole image. In the current work, we consider the classification task on a large-scale open-source dataset of young healthy subjects -- an exploration of brain differences between men and women. In this paper, we extend the previous findings in gender differences from diffusion-tensor imaging on T1 brain MRI scans. We provide the voxel-wise 3D CNN interpretation comparing the results of three interpretation methods: Meaningful Perturbations, Grad CAM and Guided Backpropagation, and contribute with the open-source library.



### G2D: Generate to Detect Anomaly
- **Arxiv ID**: http://arxiv.org/abs/2006.11629v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.11629v2)
- **Published**: 2020-06-20 18:02:50+00:00
- **Updated**: 2020-06-27 18:18:52+00:00
- **Authors**: Masoud Pourreza, Bahram Mohammadi, Mostafa Khaki, Samir Bouindour, Hichem Snoussi, Mohammad Sabokrou
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a novel method for irregularity detection. Previous researches solve this problem as a One-Class Classification (OCC) task where they train a reference model on all of the available samples. Then, they consider a test sample as an anomaly if it has a diversion from the reference model. Generative Adversarial Networks (GANs) have achieved the most promising results for OCC while implementing and training such networks, especially for the OCC task, is a cumbersome and computationally expensive procedure. To cope with the mentioned challenges, we present a simple but effective method to solve the irregularity detection as a binary classification task in order to make the implementation easier along with improving the detection performance. We learn two deep neural networks (generator and discriminator) in a GAN-style setting on merely the normal samples. During training, the generator gradually becomes an expert to generate samples which are similar to the normal ones. In the training phase, when the generator fails to produce normal data (in the early stages of learning and also prior to the complete convergence), it can be considered as an irregularity generator. In this way, we simultaneously generate the irregular samples. Afterward, we train a binary classifier on the generated anomalous samples along with the normal instances in order to be capable of detecting irregularities. The proposed framework applies to different related applications of outlier and anomaly detection in images and videos, respectively. The results confirm that our proposed method is superior to the baseline and state-of-the-art solutions.



### A Fast Stochastic Plug-and-Play ADMM for Imaging Inverse Problems
- **Arxiv ID**: http://arxiv.org/abs/2006.11630v2
- **DOI**: None
- **Categories**: **math.OC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.11630v2)
- **Published**: 2020-06-20 18:03:52+00:00
- **Updated**: 2020-06-23 08:47:00+00:00
- **Authors**: Junqi Tang, Mike Davies
- **Comment**: None
- **Journal**: None
- **Summary**: In this work we propose an efficient stochastic plug-and-play (PnP) algorithm for imaging inverse problems. The PnP stochastic gradient descent methods have been recently proposed and shown improved performance in some imaging applications over standard deterministic PnP methods. However, current stochastic PnP methods need to frequently compute the image denoisers which can be computationally expensive. To overcome this limitation, we propose a new stochastic PnP-ADMM method which is based on introducing stochastic gradient descent inner-loops within an inexact ADMM framework. We provide the theoretical guarantee on the fixed-point convergence for our algorithm under standard assumptions. Our numerical results demonstrate the effectiveness of our approach compared with state-of-the-art PnP methods.



### Estimating Model Uncertainty of Neural Networks in Sparse Information Form
- **Arxiv ID**: http://arxiv.org/abs/2006.11631v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.11631v1)
- **Published**: 2020-06-20 18:09:59+00:00
- **Updated**: 2020-06-20 18:09:59+00:00
- **Authors**: Jongseok Lee, Matthias Humt, Jianxiang Feng, Rudolph Triebel
- **Comment**: Accepted to the Thirty-seventh International Conference on Machine
  Learning (ICML) 2020
- **Journal**: None
- **Summary**: We present a sparse representation of model uncertainty for Deep Neural Networks (DNNs) where the parameter posterior is approximated with an inverse formulation of the Multivariate Normal Distribution (MND), also known as the information form. The key insight of our work is that the information matrix, i.e. the inverse of the covariance matrix tends to be sparse in its spectrum. Therefore, dimensionality reduction techniques such as low rank approximations (LRA) can be effectively exploited. To achieve this, we develop a novel sparsification algorithm and derive a cost-effective analytical sampler. As a result, we show that the information form can be scalably applied to represent model uncertainty in DNNs. Our exhaustive theoretical analysis and empirical evaluations on various benchmarks show the competitiveness of our approach over the current methods.



### A Bayesian Evaluation Framework for Subjectively Annotated Visual Recognition Tasks
- **Arxiv ID**: http://arxiv.org/abs/2007.06711v2
- **DOI**: 10.1016/j.patcog.2021.108395
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.06711v2)
- **Published**: 2020-06-20 18:35:33+00:00
- **Updated**: 2021-09-01 22:05:56+00:00
- **Authors**: Derek S. Prijatelj, Mel McCurrie, Walter J. Scheirer
- **Comment**: 21 pages. 6 figures. 2 tables. Supplementary Material as Appendix
  with 28 pages, 6 figures, 2 tables. First major revision for journal Pattern
  Recognition. Code to be included after publication at
  https://github.com/prijatelj/bayesian_eval_ground_truth-free
- **Journal**: None
- **Summary**: An interesting development in automatic visual recognition has been the emergence of tasks where it is not possible to assign objective labels to images, yet still feasible to collect annotations that reflect human judgements about them. Machine learning-based predictors for these tasks rely on supervised training that models the behavior of the annotators, i.e., what would the average person's judgement be for an image? A key open question for this type of work, especially for applications where inconsistency with human behavior can lead to ethical lapses, is how to evaluate the epistemic uncertainty of trained predictors, i.e., the uncertainty that comes from the predictor's model. We propose a Bayesian framework for evaluating black box predictors in this regime, agnostic to the predictor's internal structure. The framework specifies how to estimate the epistemic uncertainty that comes from the predictor with respect to human labels by approximating a conditional distribution and producing a credible interval for the predictions and their measures of performance. The framework is successfully applied to four image classification tasks that use subjective human judgements: facial beauty assessment, social attribute assignment, apparent age estimation, and ambiguous scene labeling.



### BRULÈ: Barycenter-Regularized Unsupervised Landmark Extraction
- **Arxiv ID**: http://arxiv.org/abs/2006.11643v3
- **DOI**: 10.1016/j.patcog.2022.108816
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.11643v3)
- **Published**: 2020-06-20 20:04:00+00:00
- **Updated**: 2021-03-30 09:18:05+00:00
- **Authors**: Iaroslav Bespalov, Nazar Buzun, Dmitry V. Dylov
- **Comment**: 10 main pages with 6 figures and 1 Table, 14 pages total with 6
  supplementary figures. I.B. and N.B. contributed equally. D.V.D. is
  corresponding author
- **Journal**: Pattern Recognition, V. 131, 108816, 2022
- **Summary**: Unsupervised retrieval of image features is vital for many computer vision tasks where the annotation is missing or scarce. In this work, we propose a new unsupervised approach to detect the landmarks in images, validating it on the popular task of human face key-points extraction. The method is based on the idea of auto-encoding the wanted landmarks in the latent space while discarding the non-essential information (and effectively preserving the interpretability). The interpretable latent space representation (the bottleneck containing nothing but the wanted key-points) is achieved by a new two-step regularization approach. The first regularization step evaluates transport distance from a given set of landmarks to some average value (the barycenter by Wasserstein distance). The second regularization step controls deviations from the barycenter by applying random geometric deformations synchronously to the initial image and to the encoded landmarks. We demonstrate the effectiveness of the approach both in unsupervised and semi-supervised training scenarios using 300-W, CelebA, and MAFL datasets. The proposed regularization paradigm is shown to prevent overfitting, and the detection quality is shown to improve beyond the state-of-the-art face models.



### A numerical framework for elastic surface matching, comparison, and interpolation
- **Arxiv ID**: http://arxiv.org/abs/2006.11652v2
- **DOI**: 10.1007/s11263-021-01476-6
- **Categories**: **cs.CV**, math.DG, 68U05, 49Q10, 58D10
- **Links**: [PDF](http://arxiv.org/pdf/2006.11652v2)
- **Published**: 2020-06-20 20:35:59+00:00
- **Updated**: 2021-06-10 15:14:23+00:00
- **Authors**: Martin Bauer, Nicolas Charon, Philipp Harms, Hsi-Wei Hsieh
- **Comment**: 21 pages, 11 figures, 1 table, 3 algorithms. Forthcoming in the
  International Journal of Computer Vision
- **Journal**: None
- **Summary**: Surface comparison and matching is a challenging problem in computer vision. While reparametrization-invariant Sobolev metrics provide meaningful elastic distances and point correspondences via the geodesic boundary value problem, solving this problem numerically tends to be difficult. Square root normal fields (SRNF) considerably simplify the computation of certain elastic distances between parametrized surfaces. Yet they leave open the issue of finding optimal reparametrizations, which induce elastic distances between unparametrized surfaces. This issue has concentrated much effort in recent years and led to the development of several numerical frameworks. In this paper, we take an alternative approach which bypasses the direct estimation of reparametrizations: we relax the geodesic boundary constraint using an auxiliary parametrization-blind varifold fidelity metric. This reformulation has several notable benefits. By avoiding altogether the need for reparametrizations, it provides the flexibility to deal with simplicial meshes of arbitrary topologies and sampling patterns. Moreover, the problem lends itself to a coarse-to-fine multi-resolution implementation, which makes the algorithm scalable to large meshes. Furthermore, this approach extends readily to higher-order feature maps such as square root curvature fields and is also able to include surface textures in the matching problem. We demonstrate these advantages on several examples, synthetic and real.



### Towards Understanding Label Smoothing
- **Arxiv ID**: http://arxiv.org/abs/2006.11653v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.11653v2)
- **Published**: 2020-06-20 20:36:17+00:00
- **Updated**: 2020-10-03 03:05:47+00:00
- **Authors**: Yi Xu, Yuanhong Xu, Qi Qian, Hao Li, Rong Jin
- **Comment**: None
- **Journal**: None
- **Summary**: Label smoothing regularization (LSR) has a great success in training deep neural networks by stochastic algorithms such as stochastic gradient descent and its variants. However, the theoretical understanding of its power from the view of optimization is still rare. This study opens the door to a deep understanding of LSR by initiating the analysis. In this paper, we analyze the convergence behaviors of stochastic gradient descent with label smoothing regularization for solving non-convex problems and show that an appropriate LSR can help to speed up the convergence by reducing the variance. More interestingly, we proposed a simple yet effective strategy, namely Two-Stage LAbel smoothing algorithm (TSLA), that uses LSR in the early training epochs and drops it off in the later training epochs. We observe from the improved convergence result of TSLA that it benefits from LSR in the first stage and essentially converges faster in the second stage. To the best of our knowledge, this is the first work for understanding the power of LSR via establishing convergence complexity of stochastic methods with LSR in non-convex optimization. We empirically demonstrate the effectiveness of the proposed method in comparison with baselines on training ResNet models over benchmark data sets.



### Adversarial Transfer of Pose Estimation Regression
- **Arxiv ID**: http://arxiv.org/abs/2006.11658v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2006.11658v2)
- **Published**: 2020-06-20 21:16:37+00:00
- **Updated**: 2020-11-23 13:45:18+00:00
- **Authors**: Boris Chidlovskii, Assem Sadek
- **Comment**: Published in ECCV'20 TASK-CV Workshop
- **Journal**: None
- **Summary**: We address the problem of camera pose estimation in visual localization. Current regression-based methods for pose estimation are trained and evaluated scene-wise. They depend on the coordinate frame of the training dataset and show a low generalization across scenes and datasets. We identify the dataset shift an important barrier to generalization and consider transfer learning as an alternative way towards a better reuse of pose estimation models. We revise domain adaptation techniques for classification and extend them to camera pose estimation, which is a multi-regression task. We develop a deep adaptation network for learning scene-invariant image representations and use adversarial learning to generate such representations for model transfer. We enrich the network with self-supervised learning and use the adaptability theory to validate the existence of scene-invariant representation of images in two given scenes. We evaluate our network on two public datasets, Cambridge Landmarks and 7Scene, demonstrate its superiority over several baselines and compare to the state of the art methods.



### Semantically Tied Paired Cycle Consistency for Any-Shot Sketch-based Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2006.11397v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.11397v1)
- **Published**: 2020-06-20 22:43:53+00:00
- **Updated**: 2020-06-20 22:43:53+00:00
- **Authors**: Anjan Dutta, Zeynep Akata
- **Comment**: In International Journal of Computer Vision (IJCV) 2020 (17 pages, 12
  figures, 5 tables). arXiv admin note: substantial text overlap with
  arXiv:1903.03372
- **Journal**: None
- **Summary**: Low-shot sketch-based image retrieval is an emerging task in computer vision, allowing to retrieve natural images relevant to hand-drawn sketch queries that are rarely seen during the training phase. Related prior works either require aligned sketch-image pairs that are costly to obtain or inefficient memory fusion layer for mapping the visual information to a semantic space. In this paper, we address any-shot, i.e. zero-shot and few-shot, sketch-based image retrieval (SBIR) tasks, where we introduce the few-shot setting for SBIR. For solving these tasks, we propose a semantically aligned paired cycle-consistent generative adversarial network (SEM-PCYC) for any-shot SBIR, where each branch of the generative adversarial network maps the visual information from sketch and image to a common semantic space via adversarial training. Each of these branches maintains cycle consistency that only requires supervision at the category level, and avoids the need of aligned sketch-image pairs. A classification criteria on the generators' outputs ensures the visual to semantic space mapping to be class-specific. Furthermore, we propose to combine textual and hierarchical side information via an auto-encoder that selects discriminating side information within a same end-to-end model. Our results demonstrate a significant boost in any-shot SBIR performance over the state-of-the-art on the extended version of the challenging Sketchy, TU-Berlin and QuickDraw datasets.



