# Arxiv Papers in cs.CV on 2020-06-27
### Attention-Guided Generative Adversarial Network to Address Atypical Anatomy in Modality Transfer
- **Arxiv ID**: http://arxiv.org/abs/2006.15264v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.15264v3)
- **Published**: 2020-06-27 02:50:39+00:00
- **Updated**: 2021-04-14 22:26:39+00:00
- **Authors**: Hajar Emami, Ming Dong, Carri K. Glide-Hurst
- **Comment**: IEEE 21st International Conference on Information Reuse and
  Integration for Data Science
- **Journal**: None
- **Summary**: Recently, interest in MR-only treatment planning using synthetic CTs (synCTs) has grown rapidly in radiation therapy. However, developing class solutions for medical images that contain atypical anatomy remains a major limitation. In this paper, we propose a novel spatial attention-guided generative adversarial network (attention-GAN) model to generate accurate synCTs using T1-weighted MRI images as the input to address atypical anatomy. Experimental results on fifteen brain cancer patients show that attention-GAN outperformed existing synCT models and achieved an average MAE of 85.22$\pm$12.08, 232.41$\pm$60.86, 246.38$\pm$42.67 Hounsfield units between synCT and CT-SIM across the entire head, bone and air regions, respectively. Qualitative analysis shows that attention-GAN has the ability to use spatially focused areas to better handle outliers, areas with complex anatomy or post-surgical regions, and thus offer strong potential for supporting near real-time MR-only treatment planning.



### Compressive MR Fingerprinting reconstruction with Neural Proximal Gradient iterations
- **Arxiv ID**: http://arxiv.org/abs/2006.15271v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.15271v3)
- **Published**: 2020-06-27 03:52:22+00:00
- **Updated**: 2020-07-06 11:51:40+00:00
- **Authors**: Dongdong Chen, Mike E. Davies, Mohammad Golbabaee
- **Comment**: To appear in MICCAI 2020
- **Journal**: None
- **Summary**: Consistency of the predictions with respect to the physical forward model is pivotal for reliably solving inverse problems. This consistency is mostly un-controlled in the current end-to-end deep learning methodologies proposed for the Magnetic Resonance Fingerprinting (MRF) problem. To address this, we propose ProxNet, a learned proximal gradient descent framework that directly incorporates the forward acquisition and Bloch dynamic models within a recurrent learning mechanism. The ProxNet adopts a compact neural proximal model for de-aliasing and quantitative inference, that can be flexibly trained on scarce MRF training datasets. Our numerical experiments show that the ProxNet can achieve a superior quantitative inference accuracy, much smaller storage requirement, and a comparable runtime to the recent deep learning MRF baselines, while being much faster than the dictionary matching schemes. Code has been released at https://github.com/edongdongchen/PGD-Net.



### Alpha-Net: Architecture, Models, and Applications
- **Arxiv ID**: http://arxiv.org/abs/2007.07221v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.07221v1)
- **Published**: 2020-06-27 05:05:01+00:00
- **Updated**: 2020-06-27 05:05:01+00:00
- **Authors**: Jishan Shaikh, Adya Sharma, Ankit Chouhan, Avinash Mahawar
- **Comment**: 13 pages, 8 figures, project paper preprint
- **Journal**: None
- **Summary**: Deep learning network training is usually computationally expensive and intuitively complex. We present a novel network architecture for custom training and weight evaluations. We reformulate the layers as ResNet-similar blocks with certain inputs and outputs of their own, the blocks (called Alpha blocks) on their connection configuration form their own network, combined with our novel loss function and normalization function form the complete Alpha-Net architecture. We provided the empirical mathematical formulation of network loss function for more understanding of accuracy estimation and further optimizations. We implemented Alpha-Net with 4 different layer configurations to express the architecture behavior comprehensively. On a custom dataset based on ImageNet benchmark, we evaluate Alpha-Net v1, v2, v3, and v4 for image recognition to give the accuracy of 78.2%, 79.1%, 79.5%, and 78.3% respectively. The Alpha-Net v3 gives improved accuracy of approx. 3% over the last state-of-the-art network ResNet 50 on ImageNet benchmark. We also present an analysis of our dataset with 256, 512, and 1024 layers and different versions of the loss function. Input representation is also crucial for training as initial preprocessing will take only a handful of features to make training less complex than it needs to be. We also compared network behavior with different layer structures, different loss functions, and different normalization functions for better quantitative modeling of Alpha-Net.



### A Retinex based GAN Pipeline to Utilize Paired and Unpaired Datasets for Enhancing Low Light Images
- **Arxiv ID**: http://arxiv.org/abs/2006.15304v2
- **DOI**: 10.1109/MERCon50084.2020.9185373
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.15304v2)
- **Published**: 2020-06-27 07:12:21+00:00
- **Updated**: 2021-10-22 03:37:02+00:00
- **Authors**: Harshana Weligampola, Gihan Jayatilaka, Suren Sritharan, Roshan Godaliyadda, Parakrama Ekanayaka, Roshan Ragel, Vijitha Herath
- **Comment**: None
- **Journal**: None
- **Summary**: Low light image enhancement is an important challenge for the development of robust computer vision algorithms. The machine learning approaches to this have been either unsupervised, supervised based on paired dataset or supervised based on unpaired dataset. This paper presents a novel deep learning pipeline that can learn from both paired and unpaired datasets. Convolution Neural Networks (CNNs) that are optimized to minimize standard loss, and Generative Adversarial Networks (GANs) that are optimized to minimize the adversarial loss are used to achieve different steps of the low light image enhancement process. Cycle consistency loss and a patched discriminator are utilized to further improve the performance. The paper also analyses the functionality and the performance of different components, hidden layers, and the entire pipeline.



### Video-Grounded Dialogues with Pretrained Generation Language Models
- **Arxiv ID**: http://arxiv.org/abs/2006.15319v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.15319v1)
- **Published**: 2020-06-27 08:24:26+00:00
- **Updated**: 2020-06-27 08:24:26+00:00
- **Authors**: Hung Le, Steven C. H. Hoi
- **Comment**: Accepted at ACL 2020 (Short Paper)
- **Journal**: Association for Computational Linguistics (2020) 5842-5848
- **Summary**: Pre-trained language models have shown remarkable success in improving various downstream NLP tasks due to their ability to capture dependencies in textual data and generate natural responses. In this paper, we leverage the power of pre-trained language models for improving video-grounded dialogue, which is very challenging and involves complex features of different dynamics: (1) Video features which can extend across both spatial and temporal dimensions; and (2) Dialogue features which involve semantic dependencies over multiple dialogue turns. We propose a framework by extending GPT-2 models to tackle these challenges by formulating video-grounded dialogue tasks as a sequence-to-sequence task, combining both visual and textual representation into a structured sequence, and fine-tuning a large pre-trained GPT-2 network. Our framework allows fine-tuning language models to capture dependencies across multiple modalities over different levels of information: spatio-temporal level in video and token-sentence level in dialogue context. We achieve promising improvement on the Audio-Visual Scene-Aware Dialogues (AVSD) benchmark from DSTC7, which supports a potential direction in this line of research.



### Interactive Deep Refinement Network for Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2006.15320v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.15320v1)
- **Published**: 2020-06-27 08:24:46+00:00
- **Updated**: 2020-06-27 08:24:46+00:00
- **Authors**: Titinunt Kitrungrotsakul, Iwamoto Yutaro, Lanfen Lin, Ruofeng Tong, Jingsong Li, Yen-Wei Chen
- **Comment**: 10 pages, 4 figures
- **Journal**: None
- **Summary**: Deep learning techniques have successfully been employed in numerous computer vision tasks including image segmentation. The techniques have also been applied to medical image segmentation, one of the most critical tasks in computer-aided diagnosis. Compared with natural images, the medical image is a gray-scale image with low-contrast (even with some invisible parts). Because some organs have similar intensity and texture with neighboring organs, there is usually a need to refine automatic segmentation results. In this paper, we propose an interactive deep refinement framework to improve the traditional semantic segmentation networks such as U-Net and fully convolutional network. In the proposed framework, we added a refinement network to traditional segmentation network to refine the segmentation results.Experimental results with public dataset revealed that the proposed method could achieve higher accuracy than other state-of-the-art methods.



### Compositional Video Synthesis with Action Graphs
- **Arxiv ID**: http://arxiv.org/abs/2006.15327v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.15327v4)
- **Published**: 2020-06-27 09:39:04+00:00
- **Updated**: 2021-06-10 21:07:15+00:00
- **Authors**: Amir Bar, Roei Herzig, Xiaolong Wang, Anna Rohrbach, Gal Chechik, Trevor Darrell, Amir Globerson
- **Comment**: ICML 2021 Camera Ready
- **Journal**: None
- **Summary**: Videos of actions are complex signals containing rich compositional structure in space and time. Current video generation methods lack the ability to condition the generation on multiple coordinated and potentially simultaneous timed actions. To address this challenge, we propose to represent the actions in a graph structure called Action Graph and present the new ``Action Graph To Video'' synthesis task. Our generative model for this task (AG2Vid) disentangles motion and appearance features, and by incorporating a scheduling mechanism for actions facilitates a timely and coordinated video generation. We train and evaluate AG2Vid on the CATER and Something-Something V2 datasets, and show that the resulting videos have better visual quality and semantic consistency compared to baselines. Finally, our model demonstrates zero-shot abilities by synthesizing novel compositions of the learned actions. For code and pretrained models, see the project page https://roeiherz.github.io/AG2Video



### A Tool for Automatic Estimation of Patient Position in Spinal CT Data
- **Arxiv ID**: http://arxiv.org/abs/2006.15330v1
- **DOI**: None
- **Categories**: **physics.med-ph**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.15330v1)
- **Published**: 2020-06-27 09:48:49+00:00
- **Updated**: 2020-06-27 09:48:49+00:00
- **Authors**: Roman Jakubicek, Tomas Vicar, Jiri Chmelik
- **Comment**: 6 pages, 3 figures, submitted on EMBEC 2020; the paper has not been
  reviewed yet
- **Journal**: None
- **Summary**: Much of the recently available research and challenge data lack the meta-data containing any information about the patient position. This paper presents a tool for automatic rotation of CT data into a standardized (HFS) patient position. The proposed method is based on the prediction of rotation angle with CNN, and it achieved nearly perfect results with an accuracy of 99.55 %. We provide implementations with easy to use an example for both Matlab and Python (PyTorch), which can be used, for example, for automatic rotation correction of VerSe2020 challenge data.



### Chroma Intra Prediction with attention-based CNN architectures
- **Arxiv ID**: http://arxiv.org/abs/2006.15349v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CC, cs.CV, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2006.15349v1)
- **Published**: 2020-06-27 12:11:17+00:00
- **Updated**: 2020-06-27 12:11:17+00:00
- **Authors**: Marc Górriz, Saverio Blasi, Alan F. Smeaton, Noel E. O'Connor, Marta Mrak
- **Comment**: 27th IEEE International Conference on Image Processing, 25-28 Oct
  2020, Abu Dhabi, United Arab Emirates
- **Journal**: None
- **Summary**: Neural networks can be used in video coding to improve chroma intra-prediction. In particular, usage of fully-connected networks has enabled better cross-component prediction with respect to traditional linear models. Nonetheless, state-of-the-art architectures tend to disregard the location of individual reference samples in the prediction process. This paper proposes a new neural network architecture for cross-component intra-prediction. The network uses a novel attention module to model spatial relations between reference and predicted samples. The proposed approach is integrated into the Versatile Video Coding (VVC) prediction pipeline. Experimental results demonstrate compression gains over the latest VVC anchor compared with state-of-the-art chroma intra-prediction methods based on neural networks.



### MiniNet: An extremely lightweight convolutional neural network for real-time unsupervised monocular depth estimation
- **Arxiv ID**: http://arxiv.org/abs/2006.15350v1
- **DOI**: 10.1016/j.isprsjprs.2020.06.004
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.15350v1)
- **Published**: 2020-06-27 12:13:22+00:00
- **Updated**: 2020-06-27 12:13:22+00:00
- **Authors**: Jun Liu, Qing Li, Rui Cao, Wenming Tang, Guoping Qiu
- **Comment**: None
- **Journal**: Volume 166, August 2020, Pages 255-267
- **Summary**: Predicting depth from a single image is an attractive research topic since it provides one more dimension of information to enable machines to better perceive the world. Recently, deep learning has emerged as an effective approach to monocular depth estimation. As obtaining labeled data is costly, there is a recent trend to move from supervised learning to unsupervised learning to obtain monocular depth. However, most unsupervised learning methods capable of achieving high depth prediction accuracy will require a deep network architecture which will be too heavy and complex to run on embedded devices with limited storage and memory spaces. To address this issue, we propose a new powerful network with a recurrent module to achieve the capability of a deep network while at the same time maintaining an extremely lightweight size for real-time high performance unsupervised monocular depth prediction from video sequences. Besides, a novel efficient upsample block is proposed to fuse the features from the associated encoder layer and recover the spatial size of features with the small number of model parameters. We validate the effectiveness of our approach via extensive experiments on the KITTI dataset. Our new model can run at a speed of about 110 frames per second (fps) on a single GPU, 37 fps on a single CPU, and 2 fps on a Raspberry Pi 3. Moreover, it achieves higher depth accuracy with nearly 33 times fewer model parameters than state-of-the-art models. To the best of our knowledge, this work is the first extremely lightweight neural network trained on monocular video sequences for real-time unsupervised monocular depth estimation, which opens up the possibility of implementing deep learning-based real-time unsupervised monocular depth prediction on low-cost embedded devices.



### Unsupervised Deep Representation Learning and Few-Shot Classification of PolSAR Images
- **Arxiv ID**: http://arxiv.org/abs/2006.15351v2
- **DOI**: 10.1109/TGRS.2020.3043191
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.15351v2)
- **Published**: 2020-06-27 12:15:32+00:00
- **Updated**: 2020-12-25 01:27:15+00:00
- **Authors**: Lamei Zhang, Siyu Zhang, Bin Zou, Hongwei Dong
- **Comment**: 16 pages, 16 figures
- **Journal**: None
- **Summary**: Deep learning and convolutional neural networks (CNNs) have made progress in polarimetric synthetic aperture radar (PolSAR) image classification over the past few years. However, a crucial issue has not been addressed, i.e., the requirement of CNNs for abundant labeled samples versus the insufficient human annotations of PolSAR images. It is well-known that following the supervised learning paradigm may lead to the overfitting of training data, and the lack of supervision information of PolSAR images undoubtedly aggravates this problem, which greatly affects the generalization performance of CNN-based classifiers in large-scale applications. To handle this problem, in this paper, learning transferrable representations from unlabeled PolSAR data through convolutional architectures is explored for the first time. Specifically, a PolSAR-tailored contrastive learning network (PCLNet) is proposed for unsupervised deep PolSAR representation learning and few-shot classification. Different from the utilization of optical processing methods, a diversity stimulation mechanism is constructed to narrow the application gap between optics and PolSAR. Beyond the conventional supervised methods, PCLNet develops an unsupervised pre-training phase based on the proxy objective of instance discrimination to learn useful representations from unlabeled PolSAR data. The acquired representations are transferred to the downstream task, i.e., few-shot PolSAR classification. Experiments on two widely-used PolSAR benchmark datasets confirm the validity of PCLNet. Besides, this work may enlighten how to efficiently utilize the massive unlabeled PolSAR data to alleviate the greedy demands of CNN-based methods for human annotations.



### An Evoked Potential-Guided Deep Learning Brain Representation For Visual Classification
- **Arxiv ID**: http://arxiv.org/abs/2006.15357v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.15357v1)
- **Published**: 2020-06-27 12:46:31+00:00
- **Updated**: 2020-06-27 12:46:31+00:00
- **Authors**: Xianglin Zheng, Zehong Cao, Quan Bai
- **Comment**: This paper is submitting to ICONIP 2020
- **Journal**: None
- **Summary**: The new perspective in visual classification aims to decode the feature representation of visual objects from human brain activities. Recording electroencephalogram (EEG) from the brain cortex has been seen as a prevalent approach to understand the cognition process of an image classification task. In this study, we proposed a deep learning framework guided by the visual evoked potentials, called the Event-Related Potential (ERP)-Long short-term memory (LSTM) framework, extracted by EEG signals for visual classification. In specific, we first extracted the ERP sequences from multiple EEG channels to response image stimuli-related information. Then, we trained an LSTM network to learn the feature representation space of visual objects for classification. In the experiment, 10 subjects were recorded by over 50,000 EEG trials from an image dataset with 6 categories, including a total of 72 exemplars. Our results showed that our proposed ERP-LSTM framework could achieve classification accuracies of cross-subject of 66.81% and 27.08% for categories (6 classes) and exemplars (72 classes), respectively. Our results outperformed that of using the existing visual classification frameworks, by improving classification accuracies in the range of 12.62% - 53.99%. Our findings suggested that decoding visual evoked potentials from EEG signals is an effective strategy to learn discriminative brain representations for visual classification.



### ReMarNet: Conjoint Relation and Margin Learning for Small-Sample Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2006.15366v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.15366v1)
- **Published**: 2020-06-27 13:50:20+00:00
- **Updated**: 2020-06-27 13:50:20+00:00
- **Authors**: Xiaoxu Li, Liyun Yu, Xiaochen Yang, Zhanyu Ma, Jing-Hao Xue, Jie Cao, Jun Guo
- **Comment**: IEEE TCSVT 2020
- **Journal**: None
- **Summary**: Despite achieving state-of-the-art performance, deep learning methods generally require a large amount of labeled data during training and may suffer from overfitting when the sample size is small. To ensure good generalizability of deep networks under small sample sizes, learning discriminative features is crucial. To this end, several loss functions have been proposed to encourage large intra-class compactness and inter-class separability. In this paper, we propose to enhance the discriminative power of features from a new perspective by introducing a novel neural network termed Relation-and-Margin learning Network (ReMarNet). Our method assembles two networks of different backbones so as to learn the features that can perform excellently in both of the aforementioned two classification mechanisms. Specifically, a relation network is used to learn the features that can support classification based on the similarity between a sample and a class prototype; at the meantime, a fully connected network with the cross entropy loss is used for classification via the decision boundary. Experiments on four image datasets demonstrate that our approach is effective in learning discriminative features from a small set of labeled samples and achieves competitive performance against state-of-the-art methods. Codes are available at https://github.com/liyunyu08/ReMarNet.



### MTStereo 2.0: improved accuracy of stereo depth estimation withMax-trees
- **Arxiv ID**: http://arxiv.org/abs/2006.15373v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.15373v1)
- **Published**: 2020-06-27 14:33:04+00:00
- **Updated**: 2020-06-27 14:33:04+00:00
- **Authors**: Rafael Brandt, Nicola Strisciuglio, Nicolai Petkov
- **Comment**: None
- **Journal**: None
- **Summary**: Efficient yet accurate extraction of depth from stereo image pairs is required by systems with low power resources, such as robotics and embedded systems. State-of-the-art stereo matching methods based on convolutional neural networks require intensive computations on GPUs and are difficult to deploy on embedded systems. In this paper, we propose a stereo matching method, called MTStereo 2.0, for limited-resource systems that require efficient and accurate depth estimation. It is based on a Max-tree hierarchical representation of image pairs, which we use to identify matching regions along image scan-lines. The method includes a cost function that considers similarity of region contextual information based on the Max-trees and a disparity border preserving cost aggregation approach. MTStereo 2.0 improves on its predecessor MTStereo 1.0 as it a) deploys a more robust cost function, b) performs more thorough detection of incorrect matches, c) computes disparity maps with pixel-level rather than node-level precision. MTStereo provides accurate sparse and semi-dense depth estimation and does not require intensive GPU computations like methods based on CNNs. Thus it can run on embedded and robotics devices with low-power requirements. We tested the proposed approach on several benchmark data sets, namely KITTI 2015, Driving, FlyingThings3D, Middlebury 2014, Monkaa and the TrimBot2020 garden data sets, and achieved competitive accuracy and efficiency. The code is available at https://github.com/rbrandt1/MaxTreeS.



### Light Pose Calibration for Camera-light Vision Systems
- **Arxiv ID**: http://arxiv.org/abs/2006.15389v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.15389v1)
- **Published**: 2020-06-27 15:56:13+00:00
- **Updated**: 2020-06-27 15:56:13+00:00
- **Authors**: Yifan Song, Furkan Elibol, Mengkun She, David Nakath, Kevin Köser
- **Comment**: None
- **Journal**: None
- **Summary**: Illuminating a scene with artificial light is a prerequisite for seeing in dark environments. However, nonuniform and dynamic illumination can deteriorate or even break computer vision approaches, for instance when operating a robot with headlights in the darkness. This paper presents a novel light calibration approach by taking multi-view and -distance images of a reference plane in order to provide pose information of the employed light sources to the computer vision system. By following a physical light propagation approach, under consideration of energy preservation, the estimation of light poses is solved by minimizing of the differences between real and rendered pixel intensities. During the evaluation we show the robustness and consistency of this method by statistically analyzing the light pose estimation results with different setups. Although the results are demonstrated using a rotationally-symmetric non-isotropic light, the method is suited also for non-symmetric lights.



### Deep Sea Robotic Imaging Simulator
- **Arxiv ID**: http://arxiv.org/abs/2006.15398v3
- **DOI**: 10.1007/978-3-030-68790-8_29
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.15398v3)
- **Published**: 2020-06-27 16:18:32+00:00
- **Updated**: 2021-02-23 12:07:18+00:00
- **Authors**: Yifan Song, David Nakath, Mengkun She, Furkan Elibol, Kevin Köser
- **Comment**: Accepted to ICPR 2021 Workshop: CVAUI 2020
- **Journal**: None
- **Summary**: Nowadays underwater vision systems are being widely applied in ocean research. However, the largest portion of the ocean - the deep sea - still remains mostly unexplored. Only relatively few image sets have been taken from the deep sea due to the physical limitations caused by technical challenges and enormous costs. Deep sea images are very different from the images taken in shallow waters and this area did not get much attention from the community. The shortage of deep sea images and the corresponding ground truth data for evaluation and training is becoming a bottleneck for the development of underwater computer vision methods. Thus, this paper presents a physical model-based image simulation solution, which uses an in-air texture and depth information as inputs, to generate underwater image sequences taken by robots in deep ocean scenarios. Different from shallow water conditions, artificial illumination plays a vital role in deep sea image formation as it strongly affects the scene appearance. Our radiometric image formation model considers both attenuation and scattering effects with co-moving spotlights in the dark. By detailed analysis and evaluation of the underwater image formation model, we propose a 3D lookup table structure in combination with a novel rendering strategy to improve simulation performance. This enables us to integrate an interactive deep sea robotic vision simulation in the Unmanned Underwater Vehicles simulator. To inspire further deep sea vision research by the community, we will release the source code of our deep sea image converter to the public.



### Invertible Concept-based Explanations for CNN Models with Non-negative Concept Activation Vectors
- **Arxiv ID**: http://arxiv.org/abs/2006.15417v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.15417v4)
- **Published**: 2020-06-27 17:57:26+00:00
- **Updated**: 2021-06-17 12:31:21+00:00
- **Authors**: Ruihan Zhang, Prashan Madumal, Tim Miller, Krista A. Ehinger, Benjamin I. P. Rubinstein
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional neural network (CNN) models for computer vision are powerful but lack explainability in their most basic form. This deficiency remains a key challenge when applying CNNs in important domains. Recent work on explanations through feature importance of approximate linear models has moved from input-level features (pixels or segments) to features from mid-layer feature maps in the form of concept activation vectors (CAVs). CAVs contain concept-level information and could be learned via clustering. In this work, we rethink the ACE algorithm of Ghorbani et~al., proposing an alternative invertible concept-based explanation (ICE) framework to overcome its shortcomings. Based on the requirements of fidelity (approximate models to target models) and interpretability (being meaningful to people), we design measurements and evaluate a range of matrix factorization methods with our framework. We find that non-negative concept activation vectors (NCAVs) from non-negative matrix factorization provide superior performance in interpretability and fidelity based on computational and human subject experiments. Our framework provides both local and global concept-level explanations for pre-trained CNN models.



### Counting Out Time: Class Agnostic Video Repetition Counting in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2006.15418v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.15418v1)
- **Published**: 2020-06-27 18:00:42+00:00
- **Updated**: 2020-06-27 18:00:42+00:00
- **Authors**: Debidatta Dwibedi, Yusuf Aytar, Jonathan Tompson, Pierre Sermanet, Andrew Zisserman
- **Comment**: Accepted at CVPR 2020. Project webpage:
  https://sites.google.com/view/repnet
- **Journal**: None
- **Summary**: We present an approach for estimating the period with which an action is repeated in a video. The crux of the approach lies in constraining the period prediction module to use temporal self-similarity as an intermediate representation bottleneck that allows generalization to unseen repetitions in videos in the wild. We train this model, called Repnet, with a synthetic dataset that is generated from a large unlabeled video collection by sampling short clips of varying lengths and repeating them with different periods and counts. This combination of synthetic data and a powerful yet constrained model, allows us to predict periods in a class-agnostic fashion. Our model substantially exceeds the state of the art performance on existing periodicity (PERTUBE) and repetition counting (QUVA) benchmarks. We also collect a new challenging dataset called Countix (~90 times larger than existing datasets) which captures the challenges of repetition counting in real-world videos. Project webpage: https://sites.google.com/view/repnet .



### On the generalization of learning-based 3D reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2006.15427v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.15427v1)
- **Published**: 2020-06-27 18:53:41+00:00
- **Updated**: 2020-06-27 18:53:41+00:00
- **Authors**: Miguel Angel Bautista, Walter Talbott, Shuangfei Zhai, Nitish Srivastava, Joshua M Susskind
- **Comment**: None
- **Journal**: None
- **Summary**: State-of-the-art learning-based monocular 3D reconstruction methods learn priors over object categories on the training set, and as a result struggle to achieve reasonable generalization to object categories unseen during training. In this paper we study the inductive biases encoded in the model architecture that impact the generalization of learning-based 3D reconstruction methods. We find that 3 inductive biases impact performance: the spatial extent of the encoder, the use of the underlying geometry of the scene to describe point features, and the mechanism to aggregate information from multiple views. Additionally, we propose mechanisms to enforce those inductive biases: a point representation that is aware of camera position, and a variance cost to aggregate information across views. Our model achieves state-of-the-art results on the standard ShapeNet 3D reconstruction benchmark in various settings.



### AerialMPTNet: Multi-Pedestrian Tracking in Aerial Imagery Using Temporal and Graphical Features
- **Arxiv ID**: http://arxiv.org/abs/2006.15457v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.15457v1)
- **Published**: 2020-06-27 22:02:29+00:00
- **Updated**: 2020-06-27 22:02:29+00:00
- **Authors**: Maximilian Kraus, Seyed Majid Azimi, Emec Ercelik, Reza Bahmanyar, Peter Reinartz, Alois Knoll
- **Comment**: ICPR 2020
- **Journal**: None
- **Summary**: Multi-pedestrian tracking in aerial imagery has several applications such as large-scale event monitoring, disaster management, search-and-rescue missions, and as input into predictive crowd dynamic models. Due to the challenges such as the large number and the tiny size of the pedestrians (e.g., 4 x 4 pixels) with their similar appearances as well as different scales and atmospheric conditions of the images with their extremely low frame rates (e.g., 2 fps), current state-of-the-art algorithms including the deep learning-based ones are unable to perform well. In this paper, we propose AerialMPTNet, a novel approach for multi-pedestrian tracking in geo-referenced aerial imagery by fusing appearance features from a Siamese Neural Network, movement predictions from a Long Short-Term Memory, and pedestrian interconnections from a GraphCNN. In addition, to address the lack of diverse aerial pedestrian tracking datasets, we introduce the Aerial Multi-Pedestrian Tracking (AerialMPT) dataset consisting of 307 frames and 44,740 pedestrians annotated. We believe that AerialMPT is the largest and most diverse dataset to this date and will be released publicly. We evaluate AerialMPTNet on AerialMPT and KIT AIS, and benchmark with several state-of-the-art tracking methods. Results indicate that AerialMPTNet significantly outperforms other methods on accuracy and time-efficiency.



