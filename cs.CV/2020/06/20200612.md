# Arxiv Papers in cs.CV on 2020-06-12
### SemifreddoNets: Partially Frozen Neural Networks for Efficient Computer Vision Systems
- **Arxiv ID**: http://arxiv.org/abs/2006.06888v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.06888v1)
- **Published**: 2020-06-12 00:31:54+00:00
- **Updated**: 2020-06-12 00:31:54+00:00
- **Authors**: Leo F Isikdogan, Bhavin V Nayak, Chyuan-Tyng Wu, Joao Peralta Moreira, Sushma Rao, Gilad Michael
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a system comprised of fixed-topology neural networks having partially frozen weights, named SemifreddoNets. SemifreddoNets work as fully-pipelined hardware blocks that are optimized to have an efficient hardware implementation. Those blocks freeze a certain portion of the parameters at every layer and replace the corresponding multipliers with fixed scalers. Fixing the weights reduces the silicon area, logic delay, and memory requirements, leading to significant savings in cost and power consumption. Unlike traditional layer-wise freezing approaches, SemifreddoNets make a profitable trade between the cost and flexibility by having some of the weights configurable at different scales and levels of abstraction in the model. Although fixing the topology and some of the weights somewhat limits the flexibility, we argue that the efficiency benefits of this strategy outweigh the advantages of a fully configurable model for many use cases. Furthermore, our system uses repeatable blocks, therefore it has the flexibility to adjust model complexity without requiring any hardware change. The hardware implementation of SemifreddoNets provides up to an order of magnitude reduction in silicon area and power consumption as compared to their equivalent implementation on a general-purpose accelerator.



### Online Sequential Extreme Learning Machines: Features Combined From Hundreds of Midlayers
- **Arxiv ID**: http://arxiv.org/abs/2006.06893v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2006.06893v1)
- **Published**: 2020-06-12 00:50:04+00:00
- **Updated**: 2020-06-12 00:50:04+00:00
- **Authors**: Chandra Swarathesh Addanki
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we develop an algorithm called hierarchal online sequential learning algorithm (H-OS-ELM) for single feed feedforward network with features combined from hundreds of midlayers, the algorithm can learn chunk by chunk with fixed or varying block size, we believe that the diverse selectivity of neurons in top layers which consists of encoded distributed information produced by the other neurons offers better computational advantage over inference accuracy. Thus this paper proposes a Hierarchical model framework combined with Online-Sequential learning algorithm, Firstly the model consists of subspace feature extractor which consists of subnetwork neuron, using the sub-features which is result of the feature extractor in first layer of the hierarchy we get rid of irrelevant factors which are of no use for the learning and iterate this process so that to recast the the subfeatures into the hierarchical model to be processed into more acceptable cognition. Secondly by using OS-Elm we are using non-iterative style for learning we are implementing a network which is wider and shallow which plays a important role in generalizing the overall performance which in turn boosts up the learning speed



### Iterate & Cluster: Iterative Semi-Supervised Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2006.06911v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.06911v1)
- **Published**: 2020-06-12 02:19:39+00:00
- **Updated**: 2020-06-12 02:19:39+00:00
- **Authors**: Jingyuan Li, Eli Shlizerman
- **Comment**: for associated video, see https://www.youtube.com/watch?v=ewuoz2tt73E
- **Journal**: None
- **Summary**: We propose a novel system for active semi-supervised feature-based action recognition. Given time sequences of features tracked during movements our system clusters the sequences into actions. Our system is based on encoder-decoder unsupervised methods shown to perform clustering by self-organization of their latent representation through the auto-regression task. These methods were tested on human action recognition benchmarks and outperformed non-feature based unsupervised methods and achieved comparable accuracy to skeleton-based supervised methods. However, such methods rely on K-Nearest Neighbours (KNN) associating sequences to actions, and general features with no annotated data would correspond to approximate clusters which could be further enhanced. Our system proposes an iterative semi-supervised method to address this challenge and to actively learn the association of clusters and actions. The method utilizes latent space embedding and clustering of the unsupervised encoder-decoder to guide the selection of sequences to be annotated in each iteration. Each iteration, the selection aims to enhance action recognition accuracy while choosing a small number of sequences for annotation. We test the approach on human skeleton-based action recognition benchmarks assuming that only annotations chosen by our method are available and on mouse movements videos recorded in lab experiments. We show that our system can boost recognition performance with only a small percentage of annotations. The system can be used as an interactive annotation tool to guide labeling efforts for 'in the wild' videos of various objects and actions to reach robust recognition.



### Potential Field Guided Actor-Critic Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/2006.06923v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.06923v1)
- **Published**: 2020-06-12 03:09:25+00:00
- **Updated**: 2020-06-12 03:09:25+00:00
- **Authors**: Weiya Ren
- **Comment**: 7 pages
- **Journal**: None
- **Summary**: In this paper, we consider the problem of actor-critic reinforcement learning. Firstly, we extend the actor-critic architecture to actor-critic-N architecture by introducing more critics beyond rewards. Secondly, we combine the reward-based critic with a potential-field-based critic to formulate the proposed potential field guided actor-critic reinforcement learning approach (actor-critic-2). This can be seen as a combination of the model-based gradients and the model-free gradients in policy improvement. State with large potential field often contains a strong prior information, such as pointing to the target at a long distance or avoiding collision by the side of an obstacle. In this situation, we should trust potential-field-based critic more as policy evaluation to accelerate policy improvement, where action policy tends to be guided. For example, in practical application, learning to avoid obstacles should be guided rather than learned by trial and error. State with small potential filed is often lack of information, for example, at the local minimum point or around the moving target. At this time, we should trust reward-based critic as policy evaluation more to evaluate the long-term return. In this case, action policy tends to explore. In addition, potential field evaluation can be combined with planning to estimate a better state value function. In this way, reward design can focus more on the final stage of reward, rather than reward shaping or phased reward. Furthermore, potential field evaluation can make up for the lack of communication in multi-agent cooperation problem, i.e., multi-agent each has a reward-based critic and a relative unified potential-field-based critic with prior information. Thirdly, simplified experiments on predator-prey game demonstrate the effectiveness of the proposed approach.



### Longitudinal Self-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2006.06930v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.06930v2)
- **Published**: 2020-06-12 03:35:17+00:00
- **Updated**: 2021-06-26 04:20:17+00:00
- **Authors**: Qingyu Zhao, Zixuan Liu, Ehsan Adeli, Kilian M. Pohl
- **Comment**: None
- **Journal**: None
- **Summary**: Machine learning analysis of longitudinal neuroimaging data is typically based on supervised learning, which requires a large number of ground-truth labels to be informative. As ground-truth labels are often missing or expensive to obtain in neuroscience, we avoid them in our analysis by combing factor disentanglement with self-supervised learning to identify changes and consistencies across the multiple MRIs acquired of each individual over time. Specifically, we propose a new definition of disentanglement by formulating a multivariate mapping between factors (e.g., brain age) associated with an MRI and a latent image representation. Then, factors that evolve across acquisitions of longitudinal sequences are disentangled from that mapping by self-supervised learning in such a way that changes in a single factor induce change along one direction in the representation space. We implement this model, named Longitudinal Self-Supervised Learning (LSSL), via a standard autoencoding structure with a cosine loss to disentangle brain age from the image representation. We apply LSSL to two longitudinal neuroimaging studies to highlight its strength in extracting the brain-age information from MRI and revealing informative characteristics associated with neurodegenerative and neuropsychological disorders. Moreover, the representations learned by LSSL facilitate supervised classification by recording faster convergence and higher (or similar) prediction accuracy compared to several other representation learning techniques.



### Does Unsupervised Architecture Representation Learning Help Neural Architecture Search?
- **Arxiv ID**: http://arxiv.org/abs/2006.06936v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.06936v2)
- **Published**: 2020-06-12 04:15:34+00:00
- **Updated**: 2020-10-24 21:54:36+00:00
- **Authors**: Shen Yan, Yu Zheng, Wei Ao, Xiao Zeng, Mi Zhang
- **Comment**: NeurIPS 2020 camera-ready. Code:
  https://github.com/MSU-MLSys-Lab/arch2vec
- **Journal**: None
- **Summary**: Existing Neural Architecture Search (NAS) methods either encode neural architectures using discrete encodings that do not scale well, or adopt supervised learning-based methods to jointly learn architecture representations and optimize architecture search on such representations which incurs search bias. Despite the widespread use, architecture representations learned in NAS are still poorly understood. We observe that the structural properties of neural architectures are hard to preserve in the latent space if architecture representation learning and search are coupled, resulting in less effective search performance. In this work, we find empirically that pre-training architecture representations using only neural architectures without their accuracies as labels considerably improve the downstream architecture search efficiency. To explain these observations, we visualize how unsupervised architecture representation learning better encourages neural architectures with similar connections and operators to cluster together. This helps to map neural architectures with similar performance to the same regions in the latent space and makes the transition of architectures in the latent space relatively smooth, which considerably benefits diverse downstream search strategies.



### The eyes know it: FakeET -- An Eye-tracking Database to Understand Deepfake Perception
- **Arxiv ID**: http://arxiv.org/abs/2006.06961v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.06961v2)
- **Published**: 2020-06-12 06:14:49+00:00
- **Updated**: 2020-06-18 22:02:13+00:00
- **Authors**: Parul Gupta, Komal Chugh, Abhinav Dhall, Ramanathan Subramanian
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: We present \textbf{FakeET}-- an eye-tracking database to understand human visual perception of \emph{deepfake} videos. Given that the principal purpose of deepfakes is to deceive human observers, FakeET is designed to understand and evaluate the ease with which viewers can detect synthetic video artifacts. FakeET contains viewing patterns compiled from 40 users via the \emph{Tobii} desktop eye-tracker for 811 videos from the \textit{Google Deepfake} dataset, with a minimum of two viewings per video. Additionally, EEG responses acquired via the \emph{Emotiv} sensor are also available. The compiled data confirms (a) distinct eye movement characteristics for \emph{real} vs \emph{fake} videos; (b) utility of the eye-track saliency maps for spatial forgery localization and detection, and (c) Error Related Negativity (ERN) triggers in the EEG responses, and the ability of the \emph{raw} EEG signal to distinguish between \emph{real} and \emph{fake} videos.



### Early Detection of Retinopathy of Prematurity (ROP) in Retinal Fundus Images Via Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2006.06968v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.06968v1)
- **Published**: 2020-06-12 07:04:13+00:00
- **Updated**: 2020-06-12 07:04:13+00:00
- **Authors**: Xin Guo, Yusuke Kikuchi, Guan Wang, Jinglin Yi, Qiong Zou, Rui Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: Retinopathy of prematurity (ROP) is an abnormal blood vessel development in the retina of a prematurely-born infant or an infant with low birth weight. ROP is one of the leading causes for infant blindness globally. Early detection of ROP is critical to slow down and avert the progression to vision impairment caused by ROP. Yet there is limited awareness of ROP even among medical professionals. Consequently, dataset for ROP is limited if ever available, and is in general extremely imbalanced in terms of the ratio between negative images and positive ones. In this study, we formulate the problem of detecting ROP in retinal fundus images in an optimization framework, and apply state-of-art convolutional neural network techniques to solve this problem. Experimental results based on our models achieve 100 percent sensitivity, 96 percent specificity, 98 percent accuracy, and 96 percent precision. In addition, our study shows that as the network gets deeper, more significant features can be extracted for better understanding of ROP.



### Multi Layer Neural Networks as Replacement for Pooling Operations
- **Arxiv ID**: http://arxiv.org/abs/2006.06969v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.06969v4)
- **Published**: 2020-06-12 07:08:38+00:00
- **Updated**: 2021-01-17 12:02:52+00:00
- **Authors**: Wolfgang Fuhl, Enkelejda Kasneci
- **Comment**: None
- **Journal**: None
- **Summary**: Pooling operations, which can be calculated at low cost and serve as a linear or nonlinear transfer function for data reduction, are found in almost every modern neural network. Countless modern approaches have already tackled replacing the common maximum value selection and mean value operations, not to mention providing a function that allows different functions to be selected through changing parameters. Additional neural networks are used to estimate the parameters of these pooling functions.Consequently, pooling layers may require supplementary parameters to increase the complexity of the whole model. In this work, we show that one perceptron can already be used effectively as a pooling operation without increasing the complexity of the model. This kind of pooling allows for the integration of multi-layer neural networks directly into a model as a pooling operation by restructuring the data and, as a result, learnin complex pooling operations. We compare our approach to tensor convolution with strides as a pooling operation and show that our approach is both effective and reduces complexity. The restructuring of the data in combination with multiple perceptrons allows for our approach to be used for upscaling, which can then be utilized for transposed convolutions in semantic segmentation.



### Towards Robust Pattern Recognition: A Review
- **Arxiv ID**: http://arxiv.org/abs/2006.06976v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.06976v1)
- **Published**: 2020-06-12 07:24:27+00:00
- **Updated**: 2020-06-12 07:24:27+00:00
- **Authors**: Xu-Yao Zhang, Cheng-Lin Liu, Ching Y. Suen
- **Comment**: None
- **Journal**: None
- **Summary**: The accuracies for many pattern recognition tasks have increased rapidly year by year, achieving or even outperforming human performance. From the perspective of accuracy, pattern recognition seems to be a nearly-solved problem. However, once launched in real applications, the high-accuracy pattern recognition systems may become unstable and unreliable, due to the lack of robustness in open and changing environments. In this paper, we present a comprehensive review of research towards robust pattern recognition from the perspective of breaking three basic and implicit assumptions: closed-world assumption, independent and identically distributed assumption, and clean and big data assumption, which form the foundation of most pattern recognition models. Actually, our brain is robust at learning concepts continually and incrementally, in complex, open and changing environments, with different contexts, modalities and tasks, by showing only a few examples, under weak or noisy supervision. These are the major differences between human intelligence and machine intelligence, which are closely related to the above three assumptions. After witnessing the significant progress in accuracy improvement nowadays, this review paper will enable us to analyze the shortcomings and limitations of current methods and identify future research directions for robust pattern recognition.



### Non-Negative Bregman Divergence Minimization for Deep Direct Density Ratio Estimation
- **Arxiv ID**: http://arxiv.org/abs/2006.06979v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.06979v3)
- **Published**: 2020-06-12 07:39:03+00:00
- **Updated**: 2021-07-17 09:25:37+00:00
- **Authors**: Masahiro Kato, Takeshi Teshima
- **Comment**: None
- **Journal**: None
- **Summary**: Density ratio estimation (DRE) is at the core of various machine learning tasks such as anomaly detection and domain adaptation. In existing studies on DRE, methods based on Bregman divergence (BD) minimization have been extensively studied. However, BD minimization when applied with highly flexible models, such as deep neural networks, tends to suffer from what we call train-loss hacking, which is a source of overfitting caused by a typical characteristic of empirical BD estimators. In this paper, to mitigate train-loss hacking, we propose a non-negative correction for empirical BD estimators. Theoretically, we confirm the soundness of the proposed method through a generalization error bound. Through our experiments, the proposed methods show a favorable performance in inlier-based outlier detection.



### Quantum Robust Fitting
- **Arxiv ID**: http://arxiv.org/abs/2006.06986v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.06986v3)
- **Published**: 2020-06-12 08:00:55+00:00
- **Updated**: 2020-10-09 11:02:05+00:00
- **Authors**: Tat-Jun Chin, David Suter, Shin-Fang Chng, James Quach
- **Comment**: Appears in: Asian Conference on Computer Vision 2020 (ACCV 2020)
- **Journal**: None
- **Summary**: Many computer vision applications need to recover structure from imperfect measurements of the real world. The task is often solved by robustly fitting a geometric model onto noisy and outlier-contaminated data. However, recent theoretical analyses indicate that many commonly used formulations of robust fitting in computer vision are not amenable to tractable solution and approximation. In this paper, we explore the usage of quantum computers for robust fitting. To do so, we examine and establish the practical usefulness of a robust fitting formulation inspired by Fourier analysis of Boolean functions. We then investigate a quantum algorithm to solve the formulation and analyse the computational speed-up possible over the classical algorithm. Our work thus proposes one of the first quantum treatments of robust fitting for computer vision.



### Weakly-supervised Temporal Action Localization by Uncertainty Modeling
- **Arxiv ID**: http://arxiv.org/abs/2006.07006v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.07006v3)
- **Published**: 2020-06-12 08:54:35+00:00
- **Updated**: 2020-12-17 07:12:38+00:00
- **Authors**: Pilhyeon Lee, Jinglu Wang, Yan Lu, Hyeran Byun
- **Comment**: Accepted by the 35th AAAI Conference on Artificial Intelligence (AAAI
  2021)
- **Journal**: None
- **Summary**: Weakly-supervised temporal action localization aims to learn detecting temporal intervals of action classes with only video-level labels. To this end, it is crucial to separate frames of action classes from the background frames (i.e., frames not belonging to any action classes). In this paper, we present a new perspective on background frames where they are modeled as out-of-distribution samples regarding their inconsistency. Then, background frames can be detected by estimating the probability of each frame being out-of-distribution, known as uncertainty, but it is infeasible to directly learn uncertainty without frame-level labels. To realize the uncertainty learning in the weakly-supervised setting, we leverage the multiple instance learning formulation. Moreover, we further introduce a background entropy loss to better discriminate background frames by encouraging their in-distribution (action) probabilities to be uniformly distributed over all action classes. Experimental results show that our uncertainty modeling is effective at alleviating the interference of background frames and brings a large performance gain without bells and whistles. We demonstrate that our model significantly outperforms state-of-the-art methods on the benchmarks, THUMOS'14 and ActivityNet (1.2 & 1.3). Our code is available at https://github.com/Pilhyeon/WTAL-Uncertainty-Modeling.



### Rethinking Sampling in 3D Point Cloud Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2006.07029v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.07029v1)
- **Published**: 2020-06-12 09:29:24+00:00
- **Updated**: 2020-06-12 09:29:24+00:00
- **Authors**: He Wang, Zetian Jiang, Li Yi, Kaichun Mo, Hao Su, Leonidas J. Guibas
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we examine the long-neglected yet important effects of point sampling patterns in point cloud GANs. Through extensive experiments, we show that sampling-insensitive discriminators (e.g.PointNet-Max) produce shape point clouds with point clustering artifacts while sampling-oversensitive discriminators (e.g.PointNet++, DGCNN) fail to guide valid shape generation. We propose the concept of sampling spectrum to depict the different sampling sensitivities of discriminators. We further study how different evaluation metrics weigh the sampling pattern against the geometry and propose several perceptual metrics forming a sampling spectrum of metrics. Guided by the proposed sampling spectrum, we discover a middle-point sampling-aware baseline discriminator, PointNet-Mix, which improves all existing point cloud generators by a large margin on sampling-related metrics. We point out that, though recent research has been focused on the generator design, the main bottleneck of point cloud GAN actually lies in the discriminator design. Our work provides both suggestions and tools for building future discriminators. We will release the code to facilitate future research.



### Benchmarking Unsupervised Object Representations for Video Sequences
- **Arxiv ID**: http://arxiv.org/abs/2006.07034v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.07034v2)
- **Published**: 2020-06-12 09:37:24+00:00
- **Updated**: 2021-06-29 07:24:56+00:00
- **Authors**: Marissa A. Weis, Kashyap Chitta, Yash Sharma, Wieland Brendel, Matthias Bethge, Andreas Geiger, Alexander S. Ecker
- **Comment**: None
- **Journal**: Journal of Machine Learning Research 22 (183): 1-61, 2021
- **Summary**: Perceiving the world in terms of objects and tracking them through time is a crucial prerequisite for reasoning and scene understanding. Recently, several methods have been proposed for unsupervised learning of object-centric representations. However, since these models were evaluated on different downstream tasks, it remains unclear how they compare in terms of basic perceptual abilities such as detection, figure-ground segmentation and tracking of objects. To close this gap, we design a benchmark with four data sets of varying complexity and seven additional test sets featuring challenging tracking scenarios relevant for natural videos. Using this benchmark, we compare the perceptual abilities of four object-centric approaches: ViMON, a video-extension of MONet, based on recurrent spatial attention, OP3, which exploits clustering via spatial mixture models, as well as TBA and SCALOR, which use explicit factorization via spatial transformers. Our results suggest that the architectures with unconstrained latent representations learn more powerful representations in terms of object detection, segmentation and tracking than the spatial transformer based architectures. We also observe that none of the methods are able to gracefully handle the most challenging tracking scenarios despite their synthetic nature, suggesting that our benchmark may provide fruitful guidance towards learning more robust object-centric video representations.



### Investigating the Impact of Pre-processing and Prediction Aggregation on the DeepFake Detection Task
- **Arxiv ID**: http://arxiv.org/abs/2006.07084v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.07084v3)
- **Published**: 2020-06-12 11:16:02+00:00
- **Updated**: 2020-10-19 10:22:15+00:00
- **Authors**: Polychronis Charitidis, Giorgos Kordopatis-Zilos, Symeon Papadopoulos, Ioannis Kompatsiaris
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advances in content generation technologies (widely known as DeepFakes) along with the online proliferation of manipulated media content render the detection of such manipulations a task of increasing importance. Even though there are many DeepFake detection methods, only a few focus on the impact of dataset preprocessing and the aggregation of frame-level to video-level prediction on model performance. In this paper, we propose a pre-processing step to improve the training data quality and examine its effect on the performance of DeepFake detection. We also propose and evaluate the effect of video-level prediction aggregation approaches. Experimental results show that the proposed pre-processing approach leads to considerable improvements in the performance of detection models, and the proposed prediction aggregation scheme further boosts the detection efficiency in cases where there are multiple faces in a video.



### Knowledge Distillation Meets Self-Supervision
- **Arxiv ID**: http://arxiv.org/abs/2006.07114v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.07114v2)
- **Published**: 2020-06-12 12:18:52+00:00
- **Updated**: 2020-07-13 09:14:27+00:00
- **Authors**: Guodong Xu, Ziwei Liu, Xiaoxiao Li, Chen Change Loy
- **Comment**: To appear in ECCV 2020. Code is available at:
  https://github.com/xuguodong03/SSKD
- **Journal**: None
- **Summary**: Knowledge distillation, which involves extracting the "dark knowledge" from a teacher network to guide the learning of a student network, has emerged as an important technique for model compression and transfer learning. Unlike previous works that exploit architecture-specific cues such as activation and attention for distillation, here we wish to explore a more general and model-agnostic approach for extracting "richer dark knowledge" from the pre-trained teacher model. We show that the seemingly different self-supervision task can serve as a simple yet powerful solution. For example, when performing contrastive learning between transformed entities, the noisy predictions of the teacher network reflect its intrinsic composition of semantic and pose information. By exploiting the similarity between those self-supervision signals as an auxiliary task, one can effectively transfer the hidden information from the teacher to the student. In this paper, we discuss practical ways to exploit those noisy self-supervision signals with selective transfer for distillation. We further show that self-supervision signals improve conventional distillation with substantial gains under few-shot and noisy-label scenarios. Given the richer knowledge mined from self-supervision, our knowledge distillation approach achieves state-of-the-art performance on standard benchmarks, i.e., CIFAR100 and ImageNet, under both similar-architecture and cross-architecture settings. The advantage is even more pronounced under the cross-architecture setting, where our method outperforms the state of the art CRD by an average of 2.3% in accuracy rate on CIFAR100 across six different teacher-student pairs.



### Attribute analysis with synthetic dataset for person re-identification
- **Arxiv ID**: http://arxiv.org/abs/2006.07139v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.07139v2)
- **Published**: 2020-06-12 12:51:47+00:00
- **Updated**: 2020-08-05 14:41:39+00:00
- **Authors**: Suncheng Xiang, Yuzhuo Fu, Guanjie You, Ting Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Person re-identification (re-ID) plays an important role in applications such as public security and video surveillance. Recently, learning from synthetic data, which benefits from the popularity of synthetic data engine, have achieved remarkable performance. However, existing synthetic datasets are in small size and lack of diversity, which hinders the development of person re-ID in real-world scenarios. To address this problem, firstly, we develop a large-scale synthetic data engine, the salient characteristic of this engine is controllable. Based on it, we build a large-scale synthetic dataset, which are diversified and customized from different attributes, such as illumination and viewpoint. Secondly, we quantitatively analyze the influence of dataset attributes on re-ID system. To our best knowledge, this is the first attempt to explicitly dissect person re-ID from the aspect of attribute on synthetic dataset. Comprehensive experiments help us have a deeper understanding of the fundamental problems in person re-ID. Our research also provides useful insights for dataset building and future practical usage.



### Move-to-Data: A new Continual Learning approach with Deep CNNs, Application for image-class recognition
- **Arxiv ID**: http://arxiv.org/abs/2006.07152v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.07152v1)
- **Published**: 2020-06-12 13:04:58+00:00
- **Updated**: 2020-06-12 13:04:58+00:00
- **Authors**: Miltiadis Poursanidis, Jenny Benois-Pineau, Akka Zemmari, Boris Mansenca, Aymar de Rugy
- **Comment**: None
- **Journal**: None
- **Summary**: In many real-life tasks of application of supervised learning approaches, all the training data are not available at the same time. The examples are lifelong image classification or recognition of environmental objects during interaction of instrumented persons with their environment, enrichment of an online-database with more images. It is necessary to pre-train the model at a "training recording phase" and then adjust it to the new coming data. This is the task of incremental/continual learning approaches. Amongst different problems to be solved by these approaches such as introduction of new categories in the model, refining existing categories to sub-categories and extending trained classifiers over them, ... we focus on the problem of adjusting pre-trained model with new additional training data for existing categories. We propose a fast continual learning layer at the end of the neuronal network. Obtained results are illustrated on the opensource CIFAR benchmark dataset. The proposed scheme yields similar performances as retraining but with drastically lower computational cost.



### Are we done with ImageNet?
- **Arxiv ID**: http://arxiv.org/abs/2006.07159v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.07159v1)
- **Published**: 2020-06-12 13:17:25+00:00
- **Updated**: 2020-06-12 13:17:25+00:00
- **Authors**: Lucas Beyer, Olivier J. Hénaff, Alexander Kolesnikov, Xiaohua Zhai, Aäron van den Oord
- **Comment**: All five authors contributed equally. New labels at
  https://github.com/google-research/reassessed-imagenet
- **Journal**: None
- **Summary**: Yes, and no. We ask whether recent progress on the ImageNet classification benchmark continues to represent meaningful generalization, or whether the community has started to overfit to the idiosyncrasies of its labeling procedure. We therefore develop a significantly more robust procedure for collecting human annotations of the ImageNet validation set. Using these new labels, we reassess the accuracy of recently proposed ImageNet classifiers, and find their gains to be substantially smaller than those reported on the original labels. Furthermore, we find the original ImageNet labels to no longer be the best predictors of this independently-collected set, indicating that their usefulness in evaluating vision models may be nearing an end. Nevertheless, we find our annotation procedure to have largely remedied the errors in the original labels, reinforcing ImageNet as a powerful benchmark for future research in visual recognition.



### ESAD: Endoscopic Surgeon Action Detection Dataset
- **Arxiv ID**: http://arxiv.org/abs/2006.07164v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2006.07164v1)
- **Published**: 2020-06-12 13:22:41+00:00
- **Updated**: 2020-06-12 13:22:41+00:00
- **Authors**: Vivek Singh Bawa, Gurkirt Singh, Francis KapingA, Inna Skarga-Bandurova, Alice Leporini, Carmela Landolfo, Armando Stabile, Francesco Setti, Riccardo Muradore, Elettra Oleari, Fabio Cuzzolin
- **Comment**: In context of SARAS ESAD Challeneg at MIDL
- **Journal**: None
- **Summary**: In this work, we take aim towards increasing the effectiveness of surgical assistant robots. We intended to make assistant robots safer by making them aware about the actions of surgeon, so it can take appropriate assisting actions. In other words, we aim to solve the problem of surgeon action detection in endoscopic videos. To this, we introduce a challenging dataset for surgeon action detection in real-world endoscopic videos. Action classes are picked based on the feedback of surgeons and annotated by medical professional. Given a video frame, we draw bounding box around surgical tool which is performing action and label it with action label. Finally, we presenta frame-level action detection baseline model based on recent advances in ob-ject detection. Results on our new dataset show that our presented dataset provides enough interesting challenges for future method and it can serveas strong benchmark corresponding research in surgeon action detection in endoscopic videos.



### Video Understanding as Machine Translation
- **Arxiv ID**: http://arxiv.org/abs/2006.07203v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.07203v2)
- **Published**: 2020-06-12 14:07:04+00:00
- **Updated**: 2020-09-17 19:41:31+00:00
- **Authors**: Bruno Korbar, Fabio Petroni, Rohit Girdhar, Lorenzo Torresani
- **Comment**: The authors have temporarily withdrawn this paper to reassess some of
  the experimental results
- **Journal**: None
- **Summary**: With the advent of large-scale multimodal video datasets, especially sequences with audio or transcribed speech, there has been a growing interest in self-supervised learning of video representations. Most prior work formulates the objective as a contrastive metric learning problem between the modalities. To enable effective learning, however, these strategies require a careful selection of positive and negative samples often combined with hand-designed curriculum policies. In this work we remove the need for negative sampling by taking a generative modeling approach that poses the objective as a translation problem between modalities. Such a formulation allows us to tackle a wide variety of downstream video understanding tasks by means of a single unified framework, without the need for large batches of negative samples common in contrastive metric learning. We experiment with the large-scale HowTo100M dataset for training, and report performance gains over the state-of-the-art on several downstream tasks including video classification (EPIC-Kitchens), question answering (TVQA), captioning (TVC, YouCook2, and MSR-VTT), and text-based clip retrieval (YouCook2 and MSR-VTT).



### Branch-Cooperative OSNet for Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2006.07206v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.07206v1)
- **Published**: 2020-06-12 14:09:23+00:00
- **Updated**: 2020-06-12 14:09:23+00:00
- **Authors**: Lei Zhang, Xiaofu Wu, Suofei Zhang, Zirui Yin
- **Comment**: 7 pages, 3 figures and 5 tables
- **Journal**: None
- **Summary**: Multi-branch is extensively studied for learning rich feature representation for person re-identification (Re-ID). In this paper, we propose a branch-cooperative architecture over OSNet, termed BC-OSNet, for person Re-ID. By stacking four cooperative branches, namely, a global branch, a local branch, a relational branch and a contrastive branch, we obtain powerful feature representation for person Re-ID. Extensive experiments show that the proposed BC-OSNet achieves state-of-art performance on the three popular datasets, including Market-1501, DukeMTMC-reID and CUHK03. In particular, it achieves mAP of 84.0% and rank-1 accuracy of 87.1% on the CUHK03_labeled.



### Sparse and Continuous Attention Mechanisms
- **Arxiv ID**: http://arxiv.org/abs/2006.07214v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CL, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.07214v3)
- **Published**: 2020-06-12 14:16:48+00:00
- **Updated**: 2020-10-29 08:39:54+00:00
- **Authors**: André F. T. Martins, António Farinhas, Marcos Treviso, Vlad Niculae, Pedro M. Q. Aguiar, Mário A. T. Figueiredo
- **Comment**: Accepted for spotlight presentation at NeurIPS 2020
- **Journal**: None
- **Summary**: Exponential families are widely used in machine learning; they include many distributions in continuous and discrete domains (e.g., Gaussian, Dirichlet, Poisson, and categorical distributions via the softmax transformation). Distributions in each of these families have fixed support. In contrast, for finite domains, there has been recent work on sparse alternatives to softmax (e.g. sparsemax and alpha-entmax), which have varying support, being able to assign zero probability to irrelevant categories. This paper expands that work in two directions: first, we extend alpha-entmax to continuous domains, revealing a link with Tsallis statistics and deformed exponential families. Second, we introduce continuous-domain attention mechanisms, deriving efficient gradient backpropagation algorithms for alpha in {1,2}. Experiments on attention-based text classification, machine translation, and visual question answering illustrate the use of continuous attention in 1D and 2D, showing that it allows attending to time intervals and compact regions.



### Local-Area-Learning Network: Meaningful Local Areas for Efficient Point Cloud Analysis
- **Arxiv ID**: http://arxiv.org/abs/2006.07226v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.07226v1)
- **Published**: 2020-06-12 14:32:28+00:00
- **Updated**: 2020-06-12 14:32:28+00:00
- **Authors**: Qendrim Bytyqi, Nicola Wolpert, Elmar Schömer
- **Comment**: None
- **Journal**: None
- **Summary**: Research in point cloud analysis with deep neural networks has made rapid progress in recent years. The pioneering work PointNet offered a direct analysis of point clouds. However, due to its architecture PointNet is not able to capture local structures. To overcome this drawback, the same authors have developed PointNet++ by applying PointNet to local areas. The local areas are defined by center points and their neighbors. In PointNet++ and its further developments the center points are determined with a Farthest Point Sampling (FPS) algorithm. This has the disadvantage that the center points in general do not have meaningful local areas. In this paper, we introduce the neural Local-Area-Learning Network (LocAL-Net) which places emphasis on the selection and characterization of the local areas. Our approach learns critical points that we use as center points. In order to strengthen the recognition of local structures, the points are given additional metric properties depending on the local areas. Finally, we derive and combine two global feature vectors, one from the whole point cloud and one from all local areas. Experiments on the datasets ModelNet10/40 and ShapeNet show that LocAL-Net is competitive for part segmentation. For classification LocAL-Net outperforms the state-of-the-arts.



### FedGAN: Federated Generative Adversarial Networks for Distributed Data
- **Arxiv ID**: http://arxiv.org/abs/2006.07228v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.MA, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.07228v2)
- **Published**: 2020-06-12 14:36:43+00:00
- **Updated**: 2020-06-15 06:38:12+00:00
- **Authors**: Mohammad Rasouli, Tao Sun, Ram Rajagopal
- **Comment**: 23 pages, 10 figures
- **Journal**: None
- **Summary**: We propose Federated Generative Adversarial Network (FedGAN) for training a GAN across distributed sources of non-independent-and-identically-distributed data sources subject to communication and privacy constraints. Our algorithm uses local generators and discriminators which are periodically synced via an intermediary that averages and broadcasts the generator and discriminator parameters. We theoretically prove the convergence of FedGAN with both equal and two time-scale updates of generator and discriminator, under standard assumptions, using stochastic approximations and communication efficient stochastic gradient descents. We experiment FedGAN on toy examples (2D system, mixed Gaussian, and Swiss role), image datasets (MNIST, CIFAR-10, and CelebA), and time series datasets (household electricity consumption and electric vehicle charging sessions). We show FedGAN converges and has similar performance to general distributed GAN, while reduces communication complexity. We also show its robustness to reduced communications.



### A Sliced Wasserstein Loss for Neural Texture Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2006.07229v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.07229v4)
- **Published**: 2020-06-12 14:37:11+00:00
- **Updated**: 2021-03-11 14:53:09+00:00
- **Authors**: Eric Heitz, Kenneth Vanhoey, Thomas Chambon, Laurent Belcour
- **Comment**: 9 pages, 14 figures, accepted at CVPR 2021
- **Journal**: IEEE/CVF Conference on Computer Vision and Pattern Recognition
  (CVPR) 2021
- **Summary**: We address the problem of computing a textural loss based on the statistics extracted from the feature activations of a convolutional neural network optimized for object recognition (e.g. VGG-19). The underlying mathematical problem is the measure of the distance between two distributions in feature space. The Gram-matrix loss is the ubiquitous approximation for this problem but it is subject to several shortcomings. Our goal is to promote the Sliced Wasserstein Distance as a replacement for it. It is theoretically proven,practical, simple to implement, and achieves results that are visually superior for texture synthesis by optimization or training generative neural networks.



### HMIC: Hierarchical Medical Image Classification, A Deep Learning Approach
- **Arxiv ID**: http://arxiv.org/abs/2006.07187v2
- **DOI**: 10.3390/info11060318
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.07187v2)
- **Published**: 2020-06-12 15:15:29+00:00
- **Updated**: 2020-06-23 22:59:38+00:00
- **Authors**: Kamran Kowsari, Rasoul Sali, Lubaina Ehsan, William Adorno, Asad Ali, Sean Moore, Beatrice Amadi, Paul Kelly, Sana Syed, Donald Brown
- **Comment**: None
- **Journal**: Information 11, no. 6 (2020): 318
- **Summary**: Image classification is central to the big data revolution in medicine. Improved information processing methods for diagnosis and classification of digital medical images have shown to be successful via deep learning approaches. As this field is explored, there are limitations to the performance of traditional supervised classifiers. This paper outlines an approach that is different from the current medical image classification tasks that view the issue as multi-class classification. We performed a hierarchical classification using our Hierarchical Medical Image classification (HMIC) approach. HMIC uses stacks of deep learning models to give particular comprehension at each level of the clinical picture hierarchy. For testing our performance, we use biopsy of the small bowel images that contain three categories in the parent level (Celiac Disease, Environmental Enteropathy, and histologically normal controls). For the child level, Celiac Disease Severity is classified into 4 classes (I, IIIa, IIIb, and IIIC).



### Multiple-Vehicle Tracking in the Highway Using Appearance Model and Visual Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/2006.07309v1
- **DOI**: 10.1109/MVIP49855.2020.9116905
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.07309v1)
- **Published**: 2020-06-12 16:46:12+00:00
- **Updated**: 2020-06-12 16:46:12+00:00
- **Authors**: Fateme Bafghi, Bijan Shoushtarian
- **Comment**: None
- **Journal**: None
- **Summary**: In recent decades, due to the groundbreaking improvements in machine vision, many daily tasks are performed by computers. One of these tasks is multiple-vehicle tracking, which is widely used in different areas such as video surveillance and traffic monitoring. This paper focuses on introducing an efficient novel approach with acceptable accuracy. This is achieved through an efficient appearance and motion model based on the features extracted from each object. For this purpose, two different approaches have been used to extract features, i.e. features extracted from a deep neural network, and traditional features. Then the results from these two approaches are compared with state-of-the-art trackers. The results are obtained by executing the methods on the UA-DETRACK benchmark. The first method led to 58.9% accuracy while the second method caused up to 15.9%. The proposed methods can still be improved by extracting more distinguishable features.



### CPR: Classifier-Projection Regularization for Continual Learning
- **Arxiv ID**: http://arxiv.org/abs/2006.07326v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.07326v2)
- **Published**: 2020-06-12 17:07:37+00:00
- **Updated**: 2021-04-19 09:30:56+00:00
- **Authors**: Sungmin Cha, Hsiang Hsu, Taebaek Hwang, Flavio P. Calmon, Taesup Moon
- **Comment**: ICLR 2021 camera ready version
- **Journal**: None
- **Summary**: We propose a general, yet simple patch that can be applied to existing regularization-based continual learning methods called classifier-projection regularization (CPR). Inspired by both recent results on neural networks with wide local minima and information theory, CPR adds an additional regularization term that maximizes the entropy of a classifier's output probability. We demonstrate that this additional term can be interpreted as a projection of the conditional probability given by a classifier's output to the uniform distribution. By applying the Pythagorean theorem for KL divergence, we then prove that this projection may (in theory) improve the performance of continual learning methods. In our extensive experimental results, we apply CPR to several state-of-the-art regularization-based continual learning methods and benchmark performance on popular image recognition datasets. Our results demonstrate that CPR indeed promotes a wide local minima and significantly improves both accuracy and plasticity while simultaneously mitigating the catastrophic forgetting of baseline continual learning methods. The codes and scripts for this work are available at https://github.com/csm9493/CPR_CL.



### GNN3DMOT: Graph Neural Network for 3D Multi-Object Tracking with Multi-Feature Learning
- **Arxiv ID**: http://arxiv.org/abs/2006.07327v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.07327v1)
- **Published**: 2020-06-12 17:08:14+00:00
- **Updated**: 2020-06-12 17:08:14+00:00
- **Authors**: Xinshuo Weng, Yongxin Wang, Yunze Man, Kris Kitani
- **Comment**: CVPR 2020. My website for all my research works:
  http://www.xinshuoweng.com/
- **Journal**: None
- **Summary**: 3D Multi-object tracking (MOT) is crucial to autonomous systems. Recent work uses a standard tracking-by-detection pipeline, where feature extraction is first performed independently for each object in order to compute an affinity matrix. Then the affinity matrix is passed to the Hungarian algorithm for data association. A key process of this standard pipeline is to learn discriminative features for different objects in order to reduce confusion during data association. In this work, we propose two techniques to improve the discriminative feature learning for MOT: (1) instead of obtaining features for each object independently, we propose a novel feature interaction mechanism by introducing the Graph Neural Network. As a result, the feature of one object is informed of the features of other objects so that the object feature can lean towards the object with similar feature (i.e., object probably with a same ID) and deviate from objects with dissimilar features (i.e., object probably with different IDs), leading to a more discriminative feature for each object; (2) instead of obtaining the feature from either 2D or 3D space in prior work, we propose a novel joint feature extractor to learn appearance and motion features from 2D and 3D space simultaneously. As features from different modalities often have complementary information, the joint feature can be more discriminate than feature from each individual modality. To ensure that the joint feature extractor does not heavily rely on one modality, we also propose an ensemble training paradigm. Through extensive evaluation, our proposed method achieves state-of-the-art performance on KITTI and nuScenes 3D MOT benchmarks. Our code will be made available at https://github.com/xinshuoweng/GNN3DMOT



### Robust Baggage Detection and Classification Based on Local Tri-directional Pattern
- **Arxiv ID**: http://arxiv.org/abs/2006.07345v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/2006.07345v3)
- **Published**: 2020-06-12 17:33:21+00:00
- **Updated**: 2021-02-01 04:14:21+00:00
- **Authors**: Shahbano, Muhammad Abdullah, Kashif Inayat
- **Comment**: None
- **Journal**: International Journal of Internet Technology and Secured
  Transactions (2021)
- **Summary**: In recent decades, the automatic video surveillance system has gained significant importance in computer vision community. The crucial objective of surveillance is monitoring and security in public places. In the traditional Local Binary Pattern, the feature description is somehow inaccurate, and the feature size is large enough. Therefore, to overcome these shortcomings, our research proposed a detection algorithm for a human with or without carrying baggage. The Local tri-directional pattern descriptor is exhibited to extract features of different human body parts including head, trunk, and limbs. Then with the help of support vector machine, extracted features are trained and evaluated. Experimental results on INRIA and MSMT17 V1 datasets show that LtriDP outperforms several state-of-the-art feature descriptors and validate its effectiveness.



### Residual Force Control for Agile Human Behavior Imitation and Extended Motion Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2006.07364v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.GR, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.07364v2)
- **Published**: 2020-06-12 17:56:16+00:00
- **Updated**: 2020-10-22 17:57:12+00:00
- **Authors**: Ye Yuan, Kris Kitani
- **Comment**: NeurIPS 2020. Code: https://github.com/Khrylx/RFC. Project page:
  https://www.ye-yuan.com/rfc
- **Journal**: None
- **Summary**: Reinforcement learning has shown great promise for synthesizing realistic human behaviors by learning humanoid control policies from motion capture data. However, it is still very challenging to reproduce sophisticated human skills like ballet dance, or to stably imitate long-term human behaviors with complex transitions. The main difficulty lies in the dynamics mismatch between the humanoid model and real humans. That is, motions of real humans may not be physically possible for the humanoid model. To overcome the dynamics mismatch, we propose a novel approach, residual force control (RFC), that augments a humanoid control policy by adding external residual forces into the action space. During training, the RFC-based policy learns to apply residual forces to the humanoid to compensate for the dynamics mismatch and better imitate the reference motion. Experiments on a wide range of dynamic motions demonstrate that our approach outperforms state-of-the-art methods in terms of convergence speed and the quality of learned motions. Notably, we showcase a physics-based virtual character empowered by RFC that can perform highly agile ballet dance moves such as pirouette, arabesque and jet\'e. Furthermore, we propose a dual-policy control framework, where a kinematic policy and an RFC-based policy work in tandem to synthesize multi-modal infinite-horizon human motions without any task guidance or user input. Our approach is the first humanoid control method that successfully learns from a large-scale human motion dataset (Human3.6M) and generates diverse long-term motions. Code and videos are available at https://www.ye-yuan.com/rfc.



### CoDeNet: Efficient Deployment of Input-Adaptive Object Detection on Embedded FPGAs
- **Arxiv ID**: http://arxiv.org/abs/2006.08357v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.08357v2)
- **Published**: 2020-06-12 17:56:47+00:00
- **Updated**: 2021-01-25 22:35:57+00:00
- **Authors**: Zhen Dong, Dequan Wang, Qijing Huang, Yizhao Gao, Yaohui Cai, Tian Li, Bichen Wu, Kurt Keutzer, John Wawrzynek
- **Comment**: Github repo: https://github.com/DequanWang/CoDeNet arXiv:2002.08357
  is the preliminary version of this paper
- **Journal**: FPGA 2021
- **Summary**: Deploying deep learning models on embedded systems has been challenging due to limited computing resources. The majority of existing work focuses on accelerating image classification, while other fundamental vision problems, such as object detection, have not been adequately addressed. Compared with image classification, detection problems are more sensitive to the spatial variance of objects, and therefore, require specialized convolutions to aggregate spatial information. To address this need, recent work introduces dynamic deformable convolution to augment regular convolutions. However, this will lead to inefficient memory accesses of inputs with existing hardware. In this work, we harness the flexibility of FPGAs to develop a novel object detection pipeline with deformable convolutions. We show the speed-accuracy tradeoffs for a set of algorithm modifications including irregular-access versus limited-range and fixed-shape. We then Co-Design a Network CoDeNet with the modified deformable convolution and quantize it to 4-bit weights and 8-bit activations. With our high-efficiency implementation, our solution reaches 26.9 frames per second with a tiny model size of 0.76 MB while achieving 61.7 AP50 on the standard object detection dataset, Pascal VOC. With our higher accuracy implementation, our model gets to 67.1 AP50 on Pascal VOC with only 2.9 MB of parameters-20.9x smaller but 10% more accurate than Tiny-YOLO.



### The DeepFake Detection Challenge (DFDC) Dataset
- **Arxiv ID**: http://arxiv.org/abs/2006.07397v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.07397v4)
- **Published**: 2020-06-12 18:15:55+00:00
- **Updated**: 2020-10-28 03:48:28+00:00
- **Authors**: Brian Dolhansky, Joanna Bitton, Ben Pflaum, Jikuo Lu, Russ Howes, Menglin Wang, Cristian Canton Ferrer
- **Comment**: None
- **Journal**: None
- **Summary**: Deepfakes are a recent off-the-shelf manipulation technique that allows anyone to swap two identities in a single video. In addition to Deepfakes, a variety of GAN-based face swapping methods have also been published with accompanying code. To counter this emerging threat, we have constructed an extremely large face swap video dataset to enable the training of detection models, and organized the accompanying DeepFake Detection Challenge (DFDC) Kaggle competition. Importantly, all recorded subjects agreed to participate in and have their likenesses modified during the construction of the face-swapped dataset. The DFDC dataset is by far the largest currently and publicly available face swap video dataset, with over 100,000 total clips sourced from 3,426 paid actors, produced with several Deepfake, GAN-based, and non-learned methods. In addition to describing the methods used to construct the dataset, we provide a detailed analysis of the top submissions from the Kaggle contest. We show although Deepfake detection is extremely difficult and still an unsolved problem, a Deepfake detection model trained only on the DFDC can generalize to real "in-the-wild" Deepfake videos, and such a model can be a valuable analysis tool when analyzing potentially Deepfaked videos. Training, validation and testing corpuses can be downloaded from https://ai.facebook.com/datasets/dfdc.



### BI-MAML: Balanced Incremental Approach for Meta Learning
- **Arxiv ID**: http://arxiv.org/abs/2006.07412v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.RO, q-bio.NC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.07412v1)
- **Published**: 2020-06-12 18:28:48+00:00
- **Updated**: 2020-06-12 18:28:48+00:00
- **Authors**: Yang Zheng, Jinlin Xiang, Kun Su, Eli Shlizerman
- **Comment**: Please see associated video at: https://youtu.be/4qlb-iG5SFo
- **Journal**: None
- **Summary**: We present a novel Balanced Incremental Model Agnostic Meta Learning system (BI-MAML) for learning multiple tasks. Our method implements a meta-update rule to incrementally adapt its model to new tasks without forgetting old tasks. Such a capability is not possible in current state-of-the-art MAML approaches. These methods effectively adapt to new tasks, however, suffer from 'catastrophic forgetting' phenomena, in which new tasks that are streamed into the model degrade the performance of the model on previously learned tasks. Our system performs the meta-updates with only a few-shots and can successfully accomplish them. Our key idea for achieving this is the design of balanced learning strategy for the baseline model. The strategy sets the baseline model to perform equally well on various tasks and incorporates time efficiency. The balanced learning strategy enables BI-MAML to both outperform other state-of-the-art models in terms of classification accuracy for existing tasks and also accomplish efficient adaption to similar new tasks with less required shots. We evaluate BI-MAML by conducting comparisons on two common benchmark datasets with multiple number of image classification tasks. BI-MAML performance demonstrates advantages in both accuracy and efficiency.



### Defending against GAN-based Deepfake Attacks via Transformation-aware Adversarial Faces
- **Arxiv ID**: http://arxiv.org/abs/2006.07421v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.07421v1)
- **Published**: 2020-06-12 18:51:57+00:00
- **Updated**: 2020-06-12 18:51:57+00:00
- **Authors**: Chaofei Yang, Lei Ding, Yiran Chen, Hai Li
- **Comment**: None
- **Journal**: None
- **Summary**: Deepfake represents a category of face-swapping attacks that leverage machine learning models such as autoencoders or generative adversarial networks. Although the concept of the face-swapping is not new, its recent technical advances make fake content (e.g., images, videos) more realistic and imperceptible to Humans. Various detection techniques for Deepfake attacks have been explored. These methods, however, are passive measures against Deepfakes as they are mitigation strategies after the high-quality fake content is generated. More importantly, we would like to think ahead of the attackers with robust defenses. This work aims to take an offensive measure to impede the generation of high-quality fake images or videos. Specifically, we propose to use novel transformation-aware adversarially perturbed faces as a defense against GAN-based Deepfake attacks. Different from the naive adversarial faces, our proposed approach leverages differentiable random image transformations during the generation. We also propose to use an ensemble-based approach to enhance the defense robustness against GAN-based Deepfake variants under the black-box setting. We show that training a Deepfake model with adversarial faces can lead to a significant degradation in the quality of synthesized faces. This degradation is twofold. On the one hand, the quality of the synthesized faces is reduced with more visual artifacts such that the synthesized faces are more obviously fake or less convincing to human observers. On the other hand, the synthesized faces can easily be detected based on various metrics.



### Targeted Adversarial Perturbations for Monocular Depth Prediction
- **Arxiv ID**: http://arxiv.org/abs/2006.08602v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.08602v2)
- **Published**: 2020-06-12 19:29:43+00:00
- **Updated**: 2020-12-08 22:28:46+00:00
- **Authors**: Alex Wong, Safa Cicek, Stefano Soatto
- **Comment**: None
- **Journal**: None
- **Summary**: We study the effect of adversarial perturbations on the task of monocular depth prediction. Specifically, we explore the ability of small, imperceptible additive perturbations to selectively alter the perceived geometry of the scene. We show that such perturbations can not only globally re-scale the predicted distances from the camera, but also alter the prediction to match a different target scene. We also show that, when given semantic or instance information, perturbations can fool the network to alter the depth of specific categories or instances in the scene, and even remove them while preserving the rest of the scene. To understand the effect of targeted perturbations, we conduct experiments on state-of-the-art monocular depth prediction methods. Our experiments reveal vulnerabilities in monocular depth prediction networks, and shed light on the biases and context learned by them.



### Learning-to-Learn Personalised Human Activity Recognition Models
- **Arxiv ID**: http://arxiv.org/abs/2006.07472v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.07472v1)
- **Published**: 2020-06-12 21:11:59+00:00
- **Updated**: 2020-06-12 21:11:59+00:00
- **Authors**: Anjana Wijekoon, Nirmalie Wiratunga
- **Comment**: 17 pages
- **Journal**: None
- **Summary**: Human Activity Recognition~(HAR) is the classification of human movement, captured using one or more sensors either as wearables or embedded in the environment~(e.g. depth cameras, pressure mats). State-of-the-art methods of HAR rely on having access to a considerable amount of labelled data to train deep architectures with many train-able parameters. This becomes prohibitive when tasked with creating models that are sensitive to personal nuances in human movement, explicitly present when performing exercises. In addition, it is not possible to collect training data to cover all possible subjects in the target population. Accordingly, learning personalised models with few data remains an interesting challenge for HAR research. We present a meta-learning methodology for learning to learn personalised HAR models for HAR; with the expectation that the end-user need only provides a few labelled data but can benefit from the rapid adaptation of a generic meta-model. We introduce two algorithms, Personalised MAML and Personalised Relation Networks inspired by existing Meta-Learning algorithms but optimised for learning HAR models that are adaptable to any person in health and well-being applications. A comparative study shows significant performance improvements against the state-of-the-art Deep Learning algorithms and the Few-shot Meta-Learning algorithms in multiple HAR domains.



### Early Blindness Detection Based on Retinal Images Using Ensemble Learning
- **Arxiv ID**: http://arxiv.org/abs/2006.07475v1
- **DOI**: 10.1109/ICCIT48885.2019.9038439
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.07475v1)
- **Published**: 2020-06-12 21:16:21+00:00
- **Updated**: 2020-06-12 21:16:21+00:00
- **Authors**: Niloy Sikder, Md. Sanaullah Chowdhury, Abu Shamim Mohammad Arif, Abdullah-Al Nahid
- **Comment**: 6 pages, 22nd International Conference of Computer and Information
  Technology (ICCIT), 18-20 December, 2019
- **Journal**: None
- **Summary**: Diabetic retinopathy (DR) is the primary cause of vision loss among grownup people around the world. In four out of five cases having diabetes for a prolonged period leads to DR. If detected early, more than 90 percent of the new DR occurrences can be prevented from turning into blindness through proper treatment. Despite having multiple treatment procedures available that are well capable to deal with DR, the negligence and failure of early detection cost most of the DR patients their precious eyesight. The recent developments in the field of Digital Image Processing (DIP) and Machine Learning (ML) have paved the way to use machines in this regard. The contemporary technologies allow us to develop devices capable of automatically detecting the condition of a persons eyes based on their retinal images. However, in practice, several factors hinder the quality of the captured images and impede the detection outcome. In this study, a novel early blind detection method has been proposed based on the color information extracted from retinal images using an ensemble learning algorithm. The method has been tested on a set of retinal images collected from people living in the rural areas of South Asia, which resulted in a 91 percent classification accuracy.



### Multispectral Biometrics System Framework: Application to Presentation Attack Detection
- **Arxiv ID**: http://arxiv.org/abs/2006.07489v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.07489v1)
- **Published**: 2020-06-12 22:09:35+00:00
- **Updated**: 2020-06-12 22:09:35+00:00
- **Authors**: Leonidas Spinoulas, Mohamed Hussein, David Geissbühler, Joe Mathai, Oswin G. Almeida, Guillaume Clivaz, Sébastien Marcel, Wael AbdAlmageed
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we present a general framework for building a biometrics system capable of capturing multispectral data from a series of sensors synchronized with active illumination sources. The framework unifies the system design for different biometric modalities and its realization on face, finger and iris data is described in detail. To the best of our knowledge, the presented design is the first to employ such a diverse set of electromagnetic spectrum bands, ranging from visible to long-wave-infrared wavelengths, and is capable of acquiring large volumes of data in seconds. Having performed a series of data collections, we run a comprehensive analysis on the captured data using a deep-learning classifier for presentation attack detection. Our study follows a data-centric approach attempting to highlight the strengths and weaknesses of each spectral band at distinguishing live from fake samples.



### OrigamiNet: Weakly-Supervised, Segmentation-Free, One-Step, Full Page Text Recognition by learning to unfold
- **Arxiv ID**: http://arxiv.org/abs/2006.07491v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.07491v1)
- **Published**: 2020-06-12 22:18:02+00:00
- **Updated**: 2020-06-12 22:18:02+00:00
- **Authors**: Mohamed Yousef, Tom E. Bishop
- **Comment**: Accepted to CVPR 2020
- **Journal**: None
- **Summary**: Text recognition is a major computer vision task with a big set of associated challenges. One of those traditional challenges is the coupled nature of text recognition and segmentation. This problem has been progressively solved over the past decades, going from segmentation based recognition to segmentation free approaches, which proved more accurate and much cheaper to annotate data for. We take a step from segmentation-free single line recognition towards segmentation-free multi-line / full page recognition. We propose a novel and simple neural network module, termed \textbf{OrigamiNet}, that can augment any CTC-trained, fully convolutional single line text recognizer, to convert it into a multi-line version by providing the model with enough spatial capacity to be able to properly collapse a 2D input signal into 1D without losing information. Such modified networks can be trained using exactly their same simple original procedure, and using only \textbf{unsegmented} image and text pairs. We carry out a set of interpretability experiments that show that our trained models learn an accurate implicit line segmentation. We achieve state-of-the-art character error rate on both IAM \& ICDAR 2017 HTR benchmarks for handwriting recognition, surpassing all other methods in the literature. On IAM we even surpass single line methods that use accurate localization information during training. Our code is available online at \url{https://github.com/IntuitionMachines/OrigamiNet}.



### Multi-Modal Fingerprint Presentation Attack Detection: Evaluation On A New Dataset
- **Arxiv ID**: http://arxiv.org/abs/2006.07498v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.07498v2)
- **Published**: 2020-06-12 22:38:23+00:00
- **Updated**: 2020-06-16 08:09:44+00:00
- **Authors**: Leonidas Spinoulas, Hengameh Mirzaalian, Mohamed Hussein, Wael AbdAlmageed
- **Comment**: None
- **Journal**: None
- **Summary**: Fingerprint presentation attack detection is becoming an increasingly challenging problem due to the continuous advancement of attack preparation techniques, which generate realistic-looking fake fingerprint presentations. In this work, rather than relying on legacy fingerprint images, which are widely used in the community, we study the usefulness of multiple recently introduced sensing modalities. Our study covers front-illumination imaging using short-wave-infrared, near-infrared, and laser illumination; and back-illumination imaging using near-infrared light. Toward studying the effectiveness of each of these unconventional sensing modalities and their fusion for liveness detection, we conducted a comprehensive analysis using a fully convolutional deep neural network framework. Our evaluation compares different combination of the new sensing modalities to legacy data from one of our collections as well as the public LivDet2015 dataset, showing the superiority of the new sensing modalities in most cases. It also covers the cases of known and unknown attacks and the cases of intra-dataset and inter-dataset evaluations. Our results indicate that the power of our approach stems from the nature of the captured data rather than the employed classification framework, which justifies the extra cost for hardware-based (or hybrid) solutions. We plan to publicly release one of our dataset collections.



### UniT: Unified Knowledge Transfer for Any-shot Object Detection and Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2006.07502v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.07502v3)
- **Published**: 2020-06-12 22:45:47+00:00
- **Updated**: 2021-03-03 20:34:48+00:00
- **Authors**: Siddhesh Khandelwal, Raghav Goyal, Leonid Sigal
- **Comment**: 22 Pages, 8 Figures, 13 Tables
- **Journal**: None
- **Summary**: Methods for object detection and segmentation rely on large scale instance-level annotations for training, which are difficult and time-consuming to collect. Efforts to alleviate this look at varying degrees and quality of supervision. Weakly-supervised approaches draw on image-level labels to build detectors/segmentors, while zero/few-shot methods assume abundant instance-level data for a set of base classes, and none to a few examples for novel classes. This taxonomy has largely siloed algorithmic designs. In this work, we aim to bridge this divide by proposing an intuitive and unified semi-supervised model that is applicable to a range of supervision: from zero to a few instance-level samples per novel class. For base classes, our model learns a mapping from weakly-supervised to fully-supervised detectors/segmentors. By learning and leveraging visual and lingual similarities between the novel and base classes, we transfer those mappings to obtain detectors/segmentors for novel classes; refining them with a few novel class instance-level annotated samples, if available. The overall model is end-to-end trainable and highly flexible. Through extensive experiments on MS-COCO and Pascal VOC benchmark datasets we show improved performance in a variety of settings.



