# Arxiv Papers in cs.CV on 2020-06-24
### GIFnets: Differentiable GIF Encoding Framework
- **Arxiv ID**: http://arxiv.org/abs/2006.13434v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.13434v1)
- **Published**: 2020-06-24 02:39:37+00:00
- **Updated**: 2020-06-24 02:39:37+00:00
- **Authors**: Innfarn Yoo, Xiyang Luo, Yilin Wang, Feng Yang, Peyman Milanfar
- **Comment**: None
- **Journal**: The IEEE/CVF Conference on Computer Vision and Pattern Recognition
  (CVPR), 2020, pp. 14473-14482
- **Summary**: Graphics Interchange Format (GIF) is a widely used image file format. Due to the limited number of palette colors, GIF encoding often introduces color banding artifacts. Traditionally, dithering is applied to reduce color banding, but introducing dotted-pattern artifacts. To reduce artifacts and provide a better and more efficient GIF encoding, we introduce a differentiable GIF encoding pipeline, which includes three novel neural networks: PaletteNet, DitherNet, and BandingNet. Each of these three networks provides an important functionality within the GIF encoding pipeline. PaletteNet predicts a near-optimal color palette given an input image. DitherNet manipulates the input image to reduce color banding artifacts and provides an alternative to traditional dithering. Finally, BandingNet is designed to detect color banding, and provides a new perceptual loss specifically for GIF images. As far as we know, this is the first fully differentiable GIF encoding pipeline based on deep neural networks and compatible with existing GIF decoders. User study shows that our algorithm is better than Floyd-Steinberg based GIF encoding.



### Learning Semantically Enhanced Feature for Fine-Grained Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2006.13457v3
- **DOI**: 10.1109/LSP.2020.3020227
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.13457v3)
- **Published**: 2020-06-24 03:41:12+00:00
- **Updated**: 2020-08-26 14:08:45+00:00
- **Authors**: Wei Luo, Hengmin Zhang, Jun Li, Xiu-Shen Wei
- **Comment**: Accepted by IEEE Signal Processing Letters. 5 pages, 4 figures, 4
  tables
- **Journal**: None
- **Summary**: We aim to provide a computationally cheap yet effective approach for fine-grained image classification (FGIC) in this letter. Unlike previous methods that rely on complex part localization modules, our approach learns fine-grained features by enhancing the semantics of sub-features of a global feature. Specifically, we first achieve the sub-feature semantic by arranging feature channels of a CNN into different groups through channel permutation. Meanwhile, to enhance the discriminability of sub-features, the groups are guided to be activated on object parts with strong discriminability by a weighted combination regularization. Our approach is parameter parsimonious and can be easily integrated into the backbone model as a plug-and-play module for end-to-end training with only image-level supervision. Experiments verified the effectiveness of our approach and validated its comparable performance to the state-of-the-art methods. Code is available at https://github.com/cswluo/SEF



### IA-MOT: Instance-Aware Multi-Object Tracking with Motion Consistency
- **Arxiv ID**: http://arxiv.org/abs/2006.13458v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.13458v1)
- **Published**: 2020-06-24 03:53:36+00:00
- **Updated**: 2020-06-24 03:53:36+00:00
- **Authors**: Jiarui Cai, Yizhou Wang, Haotian Zhang, Hung-Min Hsu, Chengqian Ma, Jenq-Neng Hwang
- **Comment**: The 5th Benchmarking Multi-Target Tracking (BMTT) Workshop, CVPR 2020
- **Journal**: None
- **Summary**: Multiple object tracking (MOT) is a crucial task in computer vision society. However, most tracking-by-detection MOT methods, with available detected bounding boxes, cannot effectively handle static, slow-moving and fast-moving camera scenarios simultaneously due to ego-motion and frequent occlusion. In this work, we propose a novel tracking framework, called "instance-aware MOT" (IA-MOT), that can track multiple objects in either static or moving cameras by jointly considering the instance-level features and object motions. First, robust appearance features are extracted from a variant of Mask R-CNN detector with an additional embedding head, by sending the given detections as the region proposals. Meanwhile, the spatial attention, which focuses on the foreground within the bounding boxes, is generated from the given instance masks and applied to the extracted embedding features. In the tracking stage, object instance masks are aligned by feature similarity and motion consistency using the Hungarian association algorithm. Moreover, object re-identification (ReID) is incorporated to recover ID switches caused by long-term occlusion or missing detection. Overall, when evaluated on the MOTS20 and KITTI-MOTS dataset, our proposed method won the first place in Track 3 of the BMTT Challenge in CVPR2020 workshops.



### ATSO: Asynchronous Teacher-Student Optimization for Semi-Supervised Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2006.13461v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.13461v3)
- **Published**: 2020-06-24 04:05:12+00:00
- **Updated**: 2020-08-07 01:18:45+00:00
- **Authors**: Xinyue Huo, Lingxi Xie, Jianzhong He, Zijie Yang, Qi Tian
- **Comment**: None
- **Journal**: None
- **Summary**: In medical image analysis, semi-supervised learning is an effective method to extract knowledge from a small amount of labeled data and a large amount of unlabeled data. This paper focuses on a popular pipeline known as self learning, and points out a weakness named lazy learning that refers to the difficulty for a model to learn from the pseudo labels generated by itself. To alleviate this issue, we propose ATSO, an asynchronous version of teacher-student optimization. ATSO partitions the unlabeled data into two subsets and alternately uses one subset to fine-tune the model and updates the label on the other subset. We evaluate ATSO on two popular medical image segmentation datasets and show its superior performance in various semi-supervised settings. With slight modification, ATSO transfers well to natural image segmentation for autonomous driving data.



### Learning Interclass Relations for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2006.13491v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.13491v1)
- **Published**: 2020-06-24 05:32:54+00:00
- **Updated**: 2020-06-24 05:32:54+00:00
- **Authors**: Muhamedrahimov Raouf, Bar Amir, Akselrod-Ballin Ayelet
- **Comment**: None
- **Journal**: None
- **Summary**: In standard classification, we typically treat class categories as independent of one-another. In many problems, however, we would be neglecting the natural relations that exist between categories, which are often dictated by an underlying biological or physical process. In this work, we propose novel formulations of the classification problem, based on a realization that the assumption of class-independence is a limiting factor that leads to the requirement of more training data. First, we propose manual ways to reduce our data needs by reintroducing knowledge about problem-specific interclass relations into the training process. Second, we propose a general approach to jointly learn categorical label representations that can implicitly encode natural interclass relations, alleviating the need for strong prior assumptions, which are not always available. We demonstrate this in the domain of medical images, where access to large amounts of labelled data is not trivial. Specifically, our experiments show the advantages of this approach in the classification of Intravenous Contrast enhancement phases in CT images, which encapsulate multiple interesting inter-class relations.



### Flexible Image Denoising with Multi-layer Conditional Feature Modulation
- **Arxiv ID**: http://arxiv.org/abs/2006.13500v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.13500v1)
- **Published**: 2020-06-24 06:00:00+00:00
- **Updated**: 2020-06-24 06:00:00+00:00
- **Authors**: Jiazhi Du, Xin Qiao, Zifei Yan, Hongzhi Zhang, Wangmeng Zuo
- **Comment**: None
- **Journal**: None
- **Summary**: For flexible non-blind image denoising, existing deep networks usually take both noisy image and noise level map as the input to handle various noise levels with a single model. However, in this kind of solution, the noise variance (i.e., noise level) is only deployed to modulate the first layer of convolution feature with channel-wise shifting, which is limited in balancing noise removal and detail preservation. In this paper, we present a novel flexible image enoising network (CFMNet) by equipping an U-Net backbone with multi-layer conditional feature modulation (CFM) modules. In comparison to channel-wise shifting only in the first layer, CFMNet can make better use of noise level information by deploying multiple layers of CFM. Moreover, each CFM module takes onvolutional features from both noisy image and noise level map as input for better trade-off between noise removal and detail preservation. Experimental results show that our CFMNet is effective in exploiting noise level information for flexible non-blind denoising, and performs favorably against the existing deep image denoising methods in terms of both quantitative metrics and visual quality.



### Dynamic Functional Connectivity and Graph Convolution Network for Alzheimer's Disease Classification
- **Arxiv ID**: http://arxiv.org/abs/2006.13510v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.13510v1)
- **Published**: 2020-06-24 06:45:25+00:00
- **Updated**: 2020-06-24 06:45:25+00:00
- **Authors**: Xingwei An, Yutao Zhou, Yang Di, Dong Ming
- **Comment**: None
- **Journal**: None
- **Summary**: Alzheimer's disease (AD) is the most prevalent form of dementia. Traditional methods cannot achieve efficient and accurate diagnosis of AD. In this paper, we introduce a novel method based on dynamic functional connectivity (dFC) that can effectively capture changes in the brain. We compare and combine four different types of features including amplitude of low-frequency fluctuation (ALFF), regional homogeneity (ReHo), dFC and the adjacency matrix of different brain structures between subjects. We use graph convolution network (GCN) which consider the similarity of brain structure between patients to solve the classification problem of non-Euclidean domains. The proposed method's accuracy and the area under the receiver operating characteristic curve achieved 91.3% and 98.4%. This result demonstrated that our proposed method can be used for detecting AD.



### Disentangle Perceptual Learning through Online Contrastive Learning
- **Arxiv ID**: http://arxiv.org/abs/2006.13511v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.13511v1)
- **Published**: 2020-06-24 06:48:38+00:00
- **Updated**: 2020-06-24 06:48:38+00:00
- **Authors**: Kangfu Mei, Yao Lu, Qiaosi Yi, Haoyu Wu, Juncheng Li, Rui Huang
- **Comment**: 12 pages, 8 figures
- **Journal**: None
- **Summary**: Pursuing realistic results according to human visual perception is the central concern in the image transformation tasks. Perceptual learning approaches like perceptual loss are empirically powerful for such tasks but they usually rely on the pre-trained classification network to provide features, which are not necessarily optimal in terms of visual perception of image transformation. In this paper, we argue that, among the features representation from the pre-trained classification network, only limited dimensions are related to human visual perception, while others are irrelevant, although both will affect the final image transformation results. Under such an assumption, we try to disentangle the perception-relevant dimensions from the representation through our proposed online contrastive learning. The resulted network includes the pre-training part and a feature selection layer, followed by the contrastive learning module, which utilizes the transformed results, target images, and task-oriented distorted images as the positive, negative, and anchor samples, respectively. The contrastive learning aims at activating the perception-relevant dimensions and suppressing the irrelevant ones by using the triplet loss, so that the original representation can be disentangled for better perceptual quality. Experiments on various image transformation tasks demonstrate the superiority of our framework, in terms of human visual perception, to the existing approaches using pre-trained networks and empirically designed losses.



### Deep Convolutional GANs for Car Image Generation
- **Arxiv ID**: http://arxiv.org/abs/2006.14380v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.14380v1)
- **Published**: 2020-06-24 06:56:56+00:00
- **Updated**: 2020-06-24 06:56:56+00:00
- **Authors**: Dong Hui Kim
- **Comment**: 6 pages, 8 figures
- **Journal**: None
- **Summary**: In this paper, we investigate the application of deep convolutional GANs on car image generation. We improve upon the commonly used DCGAN architecture by implementing Wasserstein loss to decrease mode collapse and introducing dropout at the end of the discrimiantor to introduce stochasticity. Furthermore, we introduce convolutional layers at the end of the generator to improve expressiveness and smooth noise. All of these improvements upon the DCGAN architecture comprise our proposal of the novel BoolGAN architecture, which is able to decrease the FID from 195.922 (baseline) to 165.966.



### 3D Pose Detection in Videos: Focusing on Occlusion
- **Arxiv ID**: http://arxiv.org/abs/2006.13517v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.13517v1)
- **Published**: 2020-06-24 07:01:17+00:00
- **Updated**: 2020-06-24 07:01:17+00:00
- **Authors**: Justin Wang, Edward Xu, Kangrui Xue, Lukasz Kidzinski
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we build upon existing methods for occlusion-aware 3D pose detection in videos. We implement a two stage architecture that consists of the stacked hourglass network to produce 2D pose predictions, which are then inputted into a temporal convolutional network to produce 3D pose predictions. To facilitate prediction on poses with occluded joints, we introduce an intuitive generalization of the cylinder man model used to generate occlusion labels. We find that the occlusion-aware network is able to achieve a mean-per-joint-position error 5 mm less than our linear baseline model on the Human3.6M dataset. Compared to our temporal convolutional network baseline, we achieve a comparable mean-per-joint-position error of 0.1 mm less at reduced computational cost.



### Adversarial Model for Rotated Indoor Scenes Planning
- **Arxiv ID**: http://arxiv.org/abs/2006.13527v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.13527v2)
- **Published**: 2020-06-24 07:29:07+00:00
- **Updated**: 2020-07-07 03:43:11+00:00
- **Authors**: Xinhan Di, Pengqian Yu, Hong Zhu, Lei Cai, Qiuyan Sheng, Changyu Sun
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose an adversarial model for producing furniture layout for interior scene synthesis when the interior room is rotated. The proposed model combines a conditional adversarial network, a rotation module, a mode module, and a rotation discriminator module. As compared with the prior work on scene synthesis, our proposed three modules enhance the ability of auto-layout generation and reduce the mode collapse during the rotation of the interior room. We conduct our experiments on a proposed real-world interior layout dataset that contains 14400 designs from the professional designers. Our numerical results demonstrate that the proposed model yields higher-quality layouts for four types of rooms, including the bedroom, the bathroom, the study room, and the tatami room.



### Affinity Fusion Graph-based Framework for Natural Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2006.13542v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.13542v3)
- **Published**: 2020-06-24 08:01:10+00:00
- **Updated**: 2021-01-15 03:33:23+00:00
- **Authors**: Yang Zhang, Moyun Liu, Jingwu He, Fei Pan, Yanwen Guo
- **Comment**: 11 pages, 10 figures
- **Journal**: None
- **Summary**: This paper proposes an affinity fusion graph framework to effectively connect different graphs with highly discriminating power and nonlinearity for natural image segmentation. The proposed framework combines adjacency-graphs and kernel spectral clustering based graphs (KSC-graphs) according to a new definition named affinity nodes of multi-scale superpixels. These affinity nodes are selected based on a better affiliation of superpixels, namely subspace-preserving representation which is generated by sparse subspace clustering based on subspace pursuit. Then a KSC-graph is built via a novel kernel spectral clustering to explore the nonlinear relationships among these affinity nodes. Moreover, an adjacency-graph at each scale is constructed, which is further used to update the proposed KSC-graph at affinity nodes. The fusion graph is built across different scales, and it is partitioned to obtain final segmentation result. Experimental results on the Berkeley segmentation dataset and Microsoft Research Cambridge dataset show the superiority of our framework in comparison with the state-of-the-art methods. The code is available at https://github.com/Yangzhangcst/AF-graph.



### Normalized Loss Functions for Deep Learning with Noisy Labels
- **Arxiv ID**: http://arxiv.org/abs/2006.13554v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.13554v1)
- **Published**: 2020-06-24 08:25:46+00:00
- **Updated**: 2020-06-24 08:25:46+00:00
- **Authors**: Xingjun Ma, Hanxun Huang, Yisen Wang, Simone Romano, Sarah Erfani, James Bailey
- **Comment**: Accepted to ICML 2020
- **Journal**: None
- **Summary**: Robust loss functions are essential for training accurate deep neural networks (DNNs) in the presence of noisy (incorrect) labels. It has been shown that the commonly used Cross Entropy (CE) loss is not robust to noisy labels. Whilst new loss functions have been designed, they are only partially robust. In this paper, we theoretically show by applying a simple normalization that: any loss can be made robust to noisy labels. However, in practice, simply being robust is not sufficient for a loss function to train accurate DNNs. By investigating several robust loss functions, we find that they suffer from a problem of underfitting. To address this, we propose a framework to build robust loss functions called Active Passive Loss (APL). APL combines two robust loss functions that mutually boost each other. Experiments on benchmark datasets demonstrate that the family of new loss functions created by our APL framework can consistently outperform state-of-the-art methods by large margins, especially under large noise rates such as 60% or 80% incorrect labels.



### Defending against adversarial attacks on medical imaging AI system, classification or detection?
- **Arxiv ID**: http://arxiv.org/abs/2006.13555v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.13555v1)
- **Published**: 2020-06-24 08:26:49+00:00
- **Updated**: 2020-06-24 08:26:49+00:00
- **Authors**: Xin Li, Deng Pan, Dongxiao Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: Medical imaging AI systems such as disease classification and segmentation are increasingly inspired and transformed from computer vision based AI systems. Although an array of adversarial training and/or loss function based defense techniques have been developed and proved to be effective in computer vision, defending against adversarial attacks on medical images remains largely an uncharted territory due to the following unique challenges: 1) label scarcity in medical images significantly limits adversarial generalizability of the AI system; 2) vastly similar and dominant fore- and background in medical images make it hard samples for learning the discriminating features between different disease classes; and 3) crafted adversarial noises added to the entire medical image as opposed to the focused organ target can make clean and adversarial examples more discriminate than that between different disease classes. In this paper, we propose a novel robust medical imaging AI framework based on Semi-Supervised Adversarial Training (SSAT) and Unsupervised Adversarial Detection (UAD), followed by designing a new measure for assessing systems adversarial risk. We systematically demonstrate the advantages of our robust medical imaging AI system over the existing adversarial defense techniques under diverse real-world settings of adversarial attacks using a benchmark OCT imaging data set.



### NINEPINS: Nuclei Instance Segmentation with Point Annotations
- **Arxiv ID**: http://arxiv.org/abs/2006.13556v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.13556v1)
- **Published**: 2020-06-24 08:28:52+00:00
- **Updated**: 2020-06-24 08:28:52+00:00
- **Authors**: Ting-An Yen, Hung-Chun Hsu, Pushpak Pati, Maria Gabrani, Antonio Foncubierta-Rodríguez, Pau-Choo Chung
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning-based methods are gaining traction in digital pathology, with an increasing number of publications and challenges that aim at easing the work of systematically and exhaustively analyzing tissue slides. These methods often achieve very high accuracies, at the cost of requiring large annotated datasets to train. This requirement is especially difficult to fulfill in the medical field, where expert knowledge is essential. In this paper we focus on nuclei segmentation, which generally requires experienced pathologists to annotate the nuclear areas in gigapixel histological images. We propose an algorithm for instance segmentation that uses pseudo-label segmentations generated automatically from point annotations, as a method to reduce the burden for pathologists. With the generated segmentation masks, the proposed method trains a modified version of HoVer-Net model to achieve instance segmentation. Experimental results show that the proposed method is robust to inaccuracies in point annotations and comparison with Hover-Net trained with fully annotated instance masks shows that a degradation in segmentation performance does not always imply a degradation in higher order tasks such as tissue classification.



### Learning for Video Compression with Recurrent Auto-Encoder and Recurrent Probability Model
- **Arxiv ID**: http://arxiv.org/abs/2006.13560v4
- **DOI**: 10.1109/JSTSP.2020.3043590
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.13560v4)
- **Published**: 2020-06-24 08:46:33+00:00
- **Updated**: 2020-12-06 10:07:34+00:00
- **Authors**: Ren Yang, Fabian Mentzer, Luc Van Gool, Radu Timofte
- **Comment**: Accepted for publication in IEEE Journal of Selected Topics in Signal
  Processing (J-STSP)
- **Journal**: IEEE Journal of Selected Topics in Signal Processing, 2021
- **Summary**: The past few years have witnessed increasing interests in applying deep learning to video compression. However, the existing approaches compress a video frame with only a few number of reference frames, which limits their ability to fully exploit the temporal correlation among video frames. To overcome this shortcoming, this paper proposes a Recurrent Learned Video Compression (RLVC) approach with the Recurrent Auto-Encoder (RAE) and Recurrent Probability Model (RPM). Specifically, the RAE employs recurrent cells in both the encoder and decoder. As such, the temporal information in a large range of frames can be used for generating latent representations and reconstructing compressed outputs. Furthermore, the proposed RPM network recurrently estimates the Probability Mass Function (PMF) of the latent representation, conditioned on the distribution of previous latent representations. Due to the correlation among consecutive frames, the conditional cross entropy can be lower than the independent cross entropy, thus reducing the bit-rate. The experiments show that our approach achieves the state-of-the-art learned video compression performance in terms of both PSNR and MS-SSIM. Moreover, our approach outperforms the default Low-Delay P (LDP) setting of x265 on PSNR, and also has better performance on MS-SSIM than the SSIM-tuned x265 and the slowest setting of x265. The codes are available at https://github.com/RenYang-home/RLVC.git.



### DISK: Learning local features with policy gradient
- **Arxiv ID**: http://arxiv.org/abs/2006.13566v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.13566v2)
- **Published**: 2020-06-24 08:57:38+00:00
- **Updated**: 2020-10-27 10:32:54+00:00
- **Authors**: Michał J. Tyszkiewicz, Pascal Fua, Eduard Trulls
- **Comment**: Camera-ready version for NeurIPS 2020
- **Journal**: None
- **Summary**: Local feature frameworks are difficult to learn in an end-to-end fashion, due to the discreteness inherent to the selection and matching of sparse keypoints. We introduce DISK (DIScrete Keypoints), a novel method that overcomes these obstacles by leveraging principles from Reinforcement Learning (RL), optimizing end-to-end for a high number of correct feature matches. Our simple yet expressive probabilistic model lets us keep the training and inference regimes close, while maintaining good enough convergence properties to reliably train from scratch. Our features can be extracted very densely while remaining discriminative, challenging commonly held assumptions about what constitutes a good keypoint, as showcased in Fig. 1, and deliver state-of-the-art results on three public benchmarks.



### Large-scale detection and categorization of oil spills from SAR images with deep learning
- **Arxiv ID**: http://arxiv.org/abs/2006.13575v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.13575v1)
- **Published**: 2020-06-24 09:32:31+00:00
- **Updated**: 2020-06-24 09:32:31+00:00
- **Authors**: Filippo Maria Bianchi, Martine M. Espeseth, Njål Borch
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a deep learning framework to detect and categorize oil spills in synthetic aperture radar (SAR) images at a large scale. By means of a carefully designed neural network model for image segmentation trained on an extensive dataset, we are able to obtain state-of-the-art performance in oil spill detection, achieving results that are comparable to results produced by human operators. We also introduce a classification task, which is novel in the context of oil spill detection in SAR. Specifically, after being detected, each oil spill is also classified according to different categories pertaining to its shape and texture characteristics. The classification results provide valuable insights for improving the design of oil spill services by world-leading providers. As the last contribution, we present our operational pipeline and a visualization tool for large-scale data, which allows to detect and analyze the historical presence of oil spills worldwide.



### Retrospective Loss: Looking Back to Improve Training of Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2006.13593v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.13593v1)
- **Published**: 2020-06-24 10:16:36+00:00
- **Updated**: 2020-06-24 10:16:36+00:00
- **Authors**: Surgan Jandial, Ayush Chopra, Mausoom Sarkar, Piyush Gupta, Balaji Krishnamurthy, Vineeth Balasubramanian
- **Comment**: Accepted at KDD 2020; The first two authors contributed equally
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) are powerful learning machines that have enabled breakthroughs in several domains. In this work, we introduce a new retrospective loss to improve the training of deep neural network models by utilizing the prior experience available in past model states during training. Minimizing the retrospective loss, along with the task-specific loss, pushes the parameter state at the current training step towards the optimal parameter state while pulling it away from the parameter state at a previous training step. Although a simple idea, we analyze the method as well as to conduct comprehensive sets of experiments across domains - images, speech, text, and graphs - to show that the proposed loss results in improved performance across input domains, tasks, and architectures.



### Comprehensive Information Integration Modeling Framework for Video Titling
- **Arxiv ID**: http://arxiv.org/abs/2006.13608v1
- **DOI**: 10.1145/3394486.3403325
- **Categories**: **cs.CV**, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2006.13608v1)
- **Published**: 2020-06-24 10:38:15+00:00
- **Updated**: 2020-06-24 10:38:15+00:00
- **Authors**: Shengyu Zhang, Ziqi Tan, Jin Yu, Zhou Zhao, Kun Kuang, Tan Jiang, Jingren Zhou, Hongxia Yang, Fei Wu
- **Comment**: 11 pages, 6 figures, to appear in KDD 2020 proceedings
- **Journal**: None
- **Summary**: In e-commerce, consumer-generated videos, which in general deliver consumers' individual preferences for the different aspects of certain products, are massive in volume. To recommend these videos to potential consumers more effectively, diverse and catchy video titles are critical. However, consumer-generated videos seldom accompany appropriate titles. To bridge this gap, we integrate comprehensive sources of information, including the content of consumer-generated videos, the narrative comment sentences supplied by consumers, and the product attributes, in an end-to-end modeling framework. Although automatic video titling is very useful and demanding, it is much less addressed than video captioning. The latter focuses on generating sentences that describe videos as a whole while our task requires the product-aware multi-grained video analysis. To tackle this issue, the proposed method consists of two processes, i.e., granular-level interaction modeling and abstraction-level story-line summarization. Specifically, the granular-level interaction modeling first utilizes temporal-spatial landmark cues, descriptive words, and abstractive attributes to builds three individual graphs and recognizes the intra-actions in each graph through Graph Neural Networks (GNN). Then the global-local aggregation module is proposed to model inter-actions across graphs and aggregate heterogeneous graphs into a holistic graph representation. The abstraction-level story-line summarization further considers both frame-level video features and the holistic graph to utilize the interactions between products and backgrounds, and generate the story-line topic of the video. We collect a large-scale dataset accordingly from real-world data in Taobao, a world-leading e-commerce platform, and will make the desensitized version publicly available to nourish further development of the research community...



### Recurrent Relational Memory Network for Unsupervised Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/2006.13611v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.13611v1)
- **Published**: 2020-06-24 10:44:35+00:00
- **Updated**: 2020-06-24 10:44:35+00:00
- **Authors**: Dan Guo, Yang Wang, Peipei Song, Meng Wang
- **Comment**: Appearing at IJCAI 2020
- **Journal**: None
- **Summary**: Unsupervised image captioning with no annotations is an emerging challenge in computer vision, where the existing arts usually adopt GAN (Generative Adversarial Networks) models. In this paper, we propose a novel memory-based network rather than GAN, named Recurrent Relational Memory Network ($R^2M$). Unlike complicated and sensitive adversarial learning that non-ideally performs for long sentence generation, $R^2M$ implements a concepts-to-sentence memory translator through two-stage memory mechanisms: fusion and recurrent memories, correlating the relational reasoning between common visual concepts and the generated words for long periods. $R^2M$ encodes visual context through unsupervised training on images, while enabling the memory to learn from irrelevant textual corpus via supervised fashion. Our solution enjoys less learnable parameters and higher computational efficiency than GAN-based methods, which heavily bear parameter sensitivity. We experimentally validate the superiority of $R^2M$ than state-of-the-arts on all benchmark datasets.



### Unifying Optimization Methods for Color Filter Design
- **Arxiv ID**: http://arxiv.org/abs/2006.13622v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.13622v1)
- **Published**: 2020-06-24 11:01:56+00:00
- **Updated**: 2020-06-24 11:01:56+00:00
- **Authors**: Graham Finlayson, Yuteng Zhu
- **Comment**: 11 pages, 3 figures, 1 table
- **Journal**: None
- **Summary**: Through optimization we can solve for a filter that when the camera views the world through this filter, it is more colorimetric. Previous work solved for the filter that best satisfied the Luther condition: the camera spectral sensitivities after filtering were approximately a linear transform from the CIE XYZ color matching functions. A more recent method optimized for the filter that maximized the Vora-Value (a measure which relates to the closeness of the vector spaces spanned by the camera sensors and human vision sensors). The optimized Luther- and Vora-filters are different from one another.   In this paper we begin by observing that the function defining the Vora-Value is equivalent to the Luther-condition optimization if we use the orthonormal basis of the XYZ color matching functions, i.e. we linearly transform the XYZ sensitivities to a set of orthonormal basis. In this formulation, the Luther-optimization algorithm is shown to almost optimize the Vora-Value. Moreover, experiments demonstrate that the modified orthonormal Luther-method finds the same color filter compared to the Vora-Value filter optimization. Significantly, our modified algorithm is simpler in formulation and also converges faster than the direct Vora-Value method.



### On the Empirical Neural Tangent Kernel of Standard Finite-Width Convolutional Neural Network Architectures
- **Arxiv ID**: http://arxiv.org/abs/2006.13645v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.13645v1)
- **Published**: 2020-06-24 11:40:36+00:00
- **Updated**: 2020-06-24 11:40:36+00:00
- **Authors**: Maxim Samarin, Volker Roth, David Belius
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: The Neural Tangent Kernel (NTK) is an important milestone in the ongoing effort to build a theory for deep learning. Its prediction that sufficiently wide neural networks behave as kernel methods, or equivalently as random feature models, has been confirmed empirically for certain wide architectures. It remains an open question how well NTK theory models standard neural network architectures of widths common in practice, trained on complex datasets such as ImageNet. We study this question empirically for two well-known convolutional neural network architectures, namely AlexNet and LeNet, and find that their behavior deviates significantly from their finite-width NTK counterparts. For wider versions of these networks, where the number of channels and widths of fully-connected layers are increased, the deviation decreases.



### Labelling unlabelled videos from scratch with multi-modal self-supervision
- **Arxiv ID**: http://arxiv.org/abs/2006.13662v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.13662v3)
- **Published**: 2020-06-24 12:28:17+00:00
- **Updated**: 2021-02-28 14:45:24+00:00
- **Authors**: Yuki M. Asano, Mandela Patrick, Christian Rupprecht, Andrea Vedaldi
- **Comment**: Accepted to NeurIPS 2020. Project page:
  https://www.robots.ox.ac.uk/~vgg/research/selavi, code:
  https://github.com/facebookresearch/selavi
- **Journal**: None
- **Summary**: A large part of the current success of deep learning lies in the effectiveness of data -- more precisely: labelled data. Yet, labelling a dataset with human annotation continues to carry high costs, especially for videos. While in the image domain, recent methods have allowed to generate meaningful (pseudo-) labels for unlabelled datasets without supervision, this development is missing for the video domain where learning feature representations is the current focus. In this work, we a) show that unsupervised labelling of a video dataset does not come for free from strong feature encoders and b) propose a novel clustering method that allows pseudo-labelling of a video dataset without any human annotations, by leveraging the natural correspondence between the audio and visual modalities. An extensive analysis shows that the resulting clusters have high semantic overlap to ground truth human labels. We further introduce the first benchmarking results on unsupervised labelling of common video datasets Kinetics, Kinetics-Sound, VGG-Sound and AVE.



### GMMLoc: Structure Consistent Visual Localization with Gaussian Mixture Models
- **Arxiv ID**: http://arxiv.org/abs/2006.13670v2
- **DOI**: 10.1109/LRA.2020.3005130
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.13670v2)
- **Published**: 2020-06-24 12:41:03+00:00
- **Updated**: 2020-10-05 11:07:53+00:00
- **Authors**: Huaiyang Huang, Haoyang Ye, Yuxiang Sun, Ming Liu
- **Comment**: IEEE Robotics and Automation Letters (RA-L); 8 pages, 6 figures; for
  demos and source code, see: http://sites.google.com/view/gmmloc/
- **Journal**: None
- **Summary**: Incorporating prior structure information into the visual state estimation could generally improve the localization performance. In this letter, we aim to address the paradox between accuracy and efficiency in coupling visual factors with structure constraints. To this end, we present a cross-modality method that tracks a camera in a prior map modelled by the Gaussian Mixture Model (GMM). With the pose estimated by the front-end initially, the local visual observations and map components are associated efficiently, and the visual structure from the triangulation is refined simultaneously. By introducing the hybrid structure factors into the joint optimization, the camera poses are bundle-adjusted with the local visual structure. By evaluating our complete system, namely GMMLoc, on the public dataset, we show how our system can provide a centimeter-level localization accuracy with only trivial computational overhead. In addition, the comparative studies with the state-of-the-art vision-dominant state estimators demonstrate the competitive performance of our method.



### Vision-Based Fall Event Detection in Complex Background Using Attention Guided Bi-directional LSTM
- **Arxiv ID**: http://arxiv.org/abs/2007.07773v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.07773v2)
- **Published**: 2020-06-24 13:19:21+00:00
- **Updated**: 2020-08-17 05:46:24+00:00
- **Authors**: Yong Chen, Lu Wang, Jiajia Hu, Mingbin Ye
- **Comment**: We have added a lot of experimental data and replaced all the
  pictures. The previous conclusions have undergone a lot of changes. This
  paper has almost no practical value
- **Journal**: None
- **Summary**: Fall event detection, as one of the greatest risks to the elderly, has been a hot research issue in the solitary scene in recent years. Nevertheless, there are few researches on the fall event detection in complex background. Different from most conventional background subtraction methods which depend on background modeling, Mask R-CNN method based on deep learning technique can clearly extract the moving object in noise background. We further propose an attention guided Bi-directional LSTM model for the final fall event detection. To demonstrate the efficiency, the proposed method is verified in the public dataset and self-build dataset. Evaluation of the algorithm performances in comparison with other state-of-the-art methods indicates that the proposed design is accurate and robust, which means it is suitable for the task of fall event detection in complex situation.



### Exploiting Non-Local Priors via Self-Convolution For Highly-Efficient Image Restoration
- **Arxiv ID**: http://arxiv.org/abs/2006.13714v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.13714v2)
- **Published**: 2020-06-24 13:24:37+00:00
- **Updated**: 2021-05-24 06:11:12+00:00
- **Authors**: Lanqing Guo, Zhiyuan Zha, Saiprasad Ravishankar, Bihan Wen
- **Comment**: Submitted to IEEE Transactions on Image Processing
- **Journal**: None
- **Summary**: Constructing effective image priors is critical to solving ill-posed inverse problems in image processing and imaging. Recent works proposed to exploit image non-local similarity for inverse problems by grouping similar patches and demonstrated state-of-the-art results in many applications. However, compared to classic methods based on filtering or sparsity, most of the non-local algorithms are time-consuming, mainly due to the highly inefficient and redundant block matching step, where the distance between each pair of overlapping patches needs to be computed. In this work, we propose a novel Self-Convolution operator to exploit image non-local similarity in a self-supervised way. The proposed Self-Convolution can generalize the commonly-used block matching step and produce equivalent results with much cheaper computation. Furthermore, by applying Self-Convolution, we propose an effective multi-modality image restoration scheme, which is much more efficient than conventional block matching for non-local modeling. Experimental results demonstrate that (1) Self-Convolution can significantly speed up most of the popular non-local image restoration algorithms, with two-fold to nine-fold faster block matching, and (2) the proposed multi-modality image restoration scheme achieves superior denoising results in both efficiency and effectiveness on RGB-NIR images. The code is publicly available at \href{https://github.com/GuoLanqing/Self-Convolution}.



### FBK-HUPBA Submission to the EPIC-Kitchens Action Recognition 2020 Challenge
- **Arxiv ID**: http://arxiv.org/abs/2006.13725v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.13725v1)
- **Published**: 2020-06-24 13:41:17+00:00
- **Updated**: 2020-06-24 13:41:17+00:00
- **Authors**: Swathikiran Sudhakaran, Sergio Escalera, Oswald Lanz
- **Comment**: Ranked 3rd in the EPIC-Kitchens action recognition challenge @ CVPR
  2020
- **Journal**: None
- **Summary**: In this report we describe the technical details of our submission to the EPIC-Kitchens Action Recognition 2020 Challenge. To participate in the challenge we deployed spatio-temporal feature extraction and aggregation models we have developed recently: Gate-Shift Module (GSM) [1] and EgoACO, an extension of Long Short-Term Attention (LSTA) [2]. We design an ensemble of GSM and EgoACO model families with different backbones and pre-training to generate the prediction scores. Our submission, visible on the public leaderboard with team name FBK-HUPBA, achieved a top-1 action recognition accuracy of 40.0% on S1 setting, and 25.71% on S2 setting, using only RGB.



### Imbalanced Gradients: A Subtle Cause of Overestimated Adversarial Robustness
- **Arxiv ID**: http://arxiv.org/abs/2006.13726v4
- **DOI**: 10.1007/s10994-023-06328-7
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.13726v4)
- **Published**: 2020-06-24 13:41:37+00:00
- **Updated**: 2023-03-29 13:57:28+00:00
- **Authors**: Xingjun Ma, Linxi Jiang, Hanxun Huang, Zejia Weng, James Bailey, Yu-Gang Jiang
- **Comment**: To appear in Machine Learning
- **Journal**: None
- **Summary**: Evaluating the robustness of a defense model is a challenging task in adversarial robustness research. Obfuscated gradients have previously been found to exist in many defense methods and cause a false signal of robustness. In this paper, we identify a more subtle situation called Imbalanced Gradients that can also cause overestimated adversarial robustness. The phenomenon of imbalanced gradients occurs when the gradient of one term of the margin loss dominates and pushes the attack towards to a suboptimal direction. To exploit imbalanced gradients, we formulate a Margin Decomposition (MD) attack that decomposes a margin loss into individual terms and then explores the attackability of these terms separately via a two-stage process. We also propose a multi-targeted and ensemble version of our MD attack. By investigating 24 defense models proposed since 2018, we find that 11 models are susceptible to a certain degree of imbalanced gradients and our MD attack can decrease their robustness evaluated by the best standalone baseline attack by more than 1%. We also provide an in-depth investigation on the likely causes of imbalanced gradients and effective countermeasures. Our code is available at https://github.com/HanxunH/MDAttack.



### A Novel and Reliable Deep Learning Web-Based Tool to Detect COVID-19 Infection from Chest CT-Scan
- **Arxiv ID**: http://arxiv.org/abs/2006.14419v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.14419v2)
- **Published**: 2020-06-24 13:47:54+00:00
- **Updated**: 2020-06-26 13:26:14+00:00
- **Authors**: Abdolkarim Saeedi, Maryam Saeedi, Arash Maghsoudi
- **Comment**: 9 pages, 8 figures, Improved English writing in the abstract,
  introduction and conclusion sections. Removed DenseNet typos underneath
  figures. Fixed some other minor typos
- **Journal**: None
- **Summary**: The corona virus is already spread around the world in many countries, and it has taken many lives. Furthermore, the world health organization (WHO) has announced that COVID-19 has reached the global epidemic stage. Early and reliable diagnosis using chest CT-scan can assist medical specialists in vital circumstances. In this work, we introduce a computer aided diagnosis (CAD) web service to detect COVID- 19 online. One of the largest public chest CT-scan databases, containing 746 participants was used in this experiment. A number of well-known deep neural network architectures consisting of ResNet, Inception and MobileNet were inspected to find the most efficient model for the hybrid system. A combination of the Densely connected convolutional network (DenseNet) in order to reduce image dimensions and Nu-SVM as an anti-overfitting bottleneck was chosen to distinguish between COVID-19 and healthy controls. The proposed methodology achieved 90.80% recall, 89.76% precision and 90.61% accuracy. The method also yields an AUC of 95.05%. Ultimately a flask web service is made public through ngrok using the trained models to provide a RESTful COVID-19 detector, which takes only 39 milliseconds to process one image. The source code is also available at https://github.com/KiLJ4EdeN/COVID_WEB. Based on the findings, it can be inferred that it is feasible to use the proposed technique as an automated tool for diagnosis of COVID-19.



### PhishGAN: Data Augmentation and Identification of Homoglpyh Attacks
- **Arxiv ID**: http://arxiv.org/abs/2006.13742v3
- **DOI**: 10.1109/CCCI49893.2020.9256804
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.13742v3)
- **Published**: 2020-06-24 13:59:09+00:00
- **Updated**: 2020-09-28 09:04:11+00:00
- **Authors**: Joon Sern Lee, Gui Peng David Yam, Jin Hao Chan
- **Comment**: 8 pages, 8 figures
- **Journal**: None
- **Summary**: Homoglyph attacks are a common technique used by hackers to conduct phishing. Domain names or links that are visually similar to actual ones are created via punycode to obfuscate the attack, making the victim more susceptible to phishing. For example, victims may mistake "|inkedin.com" for "linkedin.com" and in the process, divulge personal details to the fake website. Current State of The Art (SOTA) typically make use of string comparison algorithms (e.g. Levenshtein Distance), which are computationally heavy. One reason for this is the lack of publicly available datasets thus hindering the training of more advanced Machine Learning (ML) models. Furthermore, no one font is able to render all types of punycode correctly, posing a significant challenge to the creation of a dataset that is unbiased toward any particular font. This coupled with the vast number of internet domains pose a challenge in creating a dataset that can capture all possible variations. Here, we show how a conditional Generative Adversarial Network (GAN), PhishGAN, can be used to generate images of hieroglyphs, conditioned on non-homoglpyh input text images. Practical changes to current SOTA were required to facilitate the generation of more varied homoglyph text-based images. We also demonstrate a workflow of how PhishGAN together with a Homoglyph Identifier (HI) model can be used to identify the domain the homoglyph was trying to imitate. Furthermore, we demonstrate how PhishGAN's ability to generate datasets on the fly facilitate the quick adaptation of cybersecurity systems to detect new threats as they emerge.



### Insights from the Future for Continual Learning
- **Arxiv ID**: http://arxiv.org/abs/2006.13748v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.13748v1)
- **Published**: 2020-06-24 14:05:45+00:00
- **Updated**: 2020-06-24 14:05:45+00:00
- **Authors**: Arthur Douillard, Eduardo Valle, Charles Ollion, Thomas Robert, Matthieu Cord
- **Comment**: None
- **Journal**: None
- **Summary**: Continual learning aims to learn tasks sequentially, with (often severe) constraints on the storage of old learning samples, without suffering from catastrophic forgetting. In this work, we propose prescient continual learning, a novel experimental setting, to incorporate existing information about the classes, prior to any training data. Usually, each task in a traditional continual learning setting evaluates the model on present and past classes, the latter with a limited number of training samples. Our setting adds future classes, with no training samples at all. We introduce Ghost Model, a representation-learning-based model for continual learning using ideas from zero-shot learning. A generative model of the representation space in concert with a careful adjustment of the losses allows us to exploit insights from future classes to constraint the spatial arrangement of the past and current classes. Quantitative results on the AwA2 and aP\&Y datasets and detailed visualizations showcase the interest of this new setting and the method we propose to address it.



### OvA-INN: Continual Learning with Invertible Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2006.13772v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.13772v1)
- **Published**: 2020-06-24 14:40:05+00:00
- **Updated**: 2020-06-24 14:40:05+00:00
- **Authors**: G. Hocquet, O. Bichler, D. Querlioz
- **Comment**: to be published in IJCNN 2020
- **Journal**: None
- **Summary**: In the field of Continual Learning, the objective is to learn several tasks one after the other without access to the data from previous tasks. Several solutions have been proposed to tackle this problem but they usually assume that the user knows which of the tasks to perform at test time on a particular sample, or rely on small samples from previous data and most of them suffer of a substantial drop in accuracy when updated with batches of only one class at a time. In this article, we propose a new method, OvA-INN, which is able to learn one class at a time and without storing any of the previous data. To achieve this, for each class, we train a specific Invertible Neural Network to extract the relevant features to compute the likelihood on this class. At test time, we can predict the class of a sample by identifying the network which predicted the highest likelihood. With this method, we show that we can take advantage of pretrained models by stacking an Invertible Network on top of a feature extractor. This way, we are able to outperform state-of-the-art approaches that rely on features learning for the Continual Learning of MNIST and CIFAR-100 datasets. In our experiments, we reach 72% accuracy on CIFAR-100 after training our model one class at a time.



### Neural Splines: Fitting 3D Surfaces with Infinitely-Wide Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2006.13782v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2006.13782v3)
- **Published**: 2020-06-24 14:54:59+00:00
- **Updated**: 2021-05-27 13:56:24+00:00
- **Authors**: Francis Williams, Matthew Trager, Joan Bruna, Denis Zorin
- **Comment**: None
- **Journal**: None
- **Summary**: We present Neural Splines, a technique for 3D surface reconstruction that is based on random feature kernels arising from infinitely-wide shallow ReLU networks. Our method achieves state-of-the-art results, outperforming recent neural network-based techniques and widely used Poisson Surface Reconstruction (which, as we demonstrate, can also be viewed as a type of kernel method). Because our approach is based on a simple kernel formulation, it is easy to analyze and can be accelerated by general techniques designed for kernel-based learning. We provide explicit analytical expressions for our kernel and argue that our formulation can be seen as a generalization of cubic spline interpolation to higher dimensions. In particular, the RKHS norm associated with Neural Splines biases toward smooth interpolants.



### Post-DAE: Anatomically Plausible Segmentation via Post-Processing with Denoising Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/2006.13791v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.13791v1)
- **Published**: 2020-06-24 15:05:03+00:00
- **Updated**: 2020-06-24 15:05:03+00:00
- **Authors**: Agostina J Larrazabal, César Martínez, Ben Glocker, Enzo Ferrante
- **Comment**: Accepted for publication in IEEE Transactions on Medical Imaging
  (IEEE TMI)
- **Journal**: IEEE Transactions on Medical Imaging (IEEE TMI), 2020
- **Summary**: We introduce Post-DAE, a post-processing method based on denoising autoencoders (DAE) to improve the anatomical plausibility of arbitrary biomedical image segmentation algorithms. Some of the most popular segmentation methods (e.g. based on convolutional neural networks or random forest classifiers) incorporate additional post-processing steps to ensure that the resulting masks fulfill expected connectivity constraints. These methods operate under the hypothesis that contiguous pixels with similar aspect should belong to the same class. Even if valid in general, this assumption does not consider more complex priors like topological restrictions or convexity, which cannot be easily incorporated into these methods. Post-DAE leverages the latest developments in manifold learning via denoising autoencoders. First, we learn a compact and non-linear embedding that represents the space of anatomically plausible segmentations. Then, given a segmentation mask obtained with an arbitrary method, we reconstruct its anatomically plausible version by projecting it onto the learnt manifold. The proposed method is trained using unpaired segmentation mask, what makes it independent of intensity information and image modality. We performed experiments in binary and multi-label segmentation of chest X-ray and cardiac magnetic resonance images. We show how erroneous and noisy segmentation masks can be improved using Post-DAE. With almost no additional computation cost, our method brings erroneous segmentations back to a feasible space.



### A Novel Approach for Correcting Multiple Discrete Rigid In-Plane Motions Artefacts in MRI Scans
- **Arxiv ID**: http://arxiv.org/abs/2006.13804v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.13804v2)
- **Published**: 2020-06-24 15:25:11+00:00
- **Updated**: 2020-06-29 17:54:13+00:00
- **Authors**: Michael Rotman, Rafi Brada, Israel Beniaminy, Sangtae Ahn, Christopher J. Hardy, Lior Wolf
- **Comment**: None
- **Journal**: None
- **Summary**: Motion artefacts created by patient motion during an MRI scan occur frequently in practice, often rendering the scans clinically unusable and requiring a re-scan. While many methods have been employed to ameliorate the effects of patient motion, these often fall short in practice. In this paper we propose a novel method for removing motion artefacts using a deep neural network with two input branches that discriminates between patient poses using the motion's timing. The first branch receives a subset of the $k$-space data collected during a single patient pose, and the second branch receives the remaining part of the collected $k$-space data. The proposed method can be applied to artefacts generated by multiple movements of the patient. Furthermore, it can be used to correct motion for the case where $k$-space has been under-sampled, to shorten the scan time, as is common when using methods such as parallel imaging or compressed sensing. Experimental results on both simulated and real MRI data show the efficacy of our approach.



### X-ModalNet: A Semi-Supervised Deep Cross-Modal Network for Classification of Remote Sensing Data
- **Arxiv ID**: http://arxiv.org/abs/2006.13806v2
- **DOI**: 10.1016/j.isprsjprs.2020.06.014
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.13806v2)
- **Published**: 2020-06-24 15:29:41+00:00
- **Updated**: 2020-07-11 18:26:47+00:00
- **Authors**: Danfeng Hong, Naoto Yokoya, Gui-Song Xia, Jocelyn Chanussot, Xiao Xiang Zhu
- **Comment**: None
- **Journal**: ISPRS Journal of Photogrammetry and Remote Sensing,2020,167:12-23
- **Summary**: This paper addresses the problem of semi-supervised transfer learning with limited cross-modality data in remote sensing. A large amount of multi-modal earth observation images, such as multispectral imagery (MSI) or synthetic aperture radar (SAR) data, are openly available on a global scale, enabling parsing global urban scenes through remote sensing imagery. However, their ability in identifying materials (pixel-wise classification) remains limited, due to the noisy collection environment and poor discriminative information as well as limited number of well-annotated training images. To this end, we propose a novel cross-modal deep-learning framework, called X-ModalNet, with three well-designed modules: self-adversarial module, interactive learning module, and label propagation module, by learning to transfer more discriminative information from a small-scale hyperspectral image (HSI) into the classification task using a large-scale MSI or SAR data. Significantly, X-ModalNet generalizes well, owing to propagating labels on an updatable graph constructed by high-level features on the top of the network, yielding semi-supervised cross-modality learning. We evaluate X-ModalNet on two multi-modal remote sensing datasets (HSI-MSI and HSI-SAR) and achieve a significant improvement in comparison with several state-of-the-art methods.



### Interpretable Deep Models for Cardiac Resynchronisation Therapy Response Prediction
- **Arxiv ID**: http://arxiv.org/abs/2006.13811v2
- **DOI**: 10.1007/978-3-030-59710-8_28
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.13811v2)
- **Published**: 2020-06-24 15:35:47+00:00
- **Updated**: 2020-07-09 10:59:55+00:00
- **Authors**: Esther Puyol-Antón, Chen Chen, James R. Clough, Bram Ruijsink, Baldeep S. Sidhu, Justin Gould, Bradley Porter, Mark Elliott, Vishal Mehta, Daniel Rueckert, Christopher A. Rinaldi, Andrew P. King
- **Comment**: MICCAI 2020 conference
- **Journal**: None
- **Summary**: Advances in deep learning (DL) have resulted in impressive accuracy in some medical image classification tasks, but often deep models lack interpretability. The ability of these models to explain their decisions is important for fostering clinical trust and facilitating clinical translation. Furthermore, for many problems in medicine there is a wealth of existing clinical knowledge to draw upon, which may be useful in generating explanations, but it is not obvious how this knowledge can be encoded into DL models - most models are learnt either from scratch or using transfer learning from a different domain. In this paper we address both of these issues. We propose a novel DL framework for image-based classification based on a variational autoencoder (VAE). The framework allows prediction of the output of interest from the latent space of the autoencoder, as well as visualisation (in the image domain) of the effects of crossing the decision boundary, thus enhancing the interpretability of the classifier. Our key contribution is that the VAE disentangles the latent space based on `explanations' drawn from existing clinical knowledge. The framework can predict outputs as well as explanations for these outputs, and also raises the possibility of discovering new biomarkers that are separate (or disentangled) from the existing knowledge. We demonstrate our framework on the problem of predicting response of patients with cardiomyopathy to cardiac resynchronization therapy (CRT) from cine cardiac magnetic resonance images. The sensitivity and specificity of the proposed model on the task of CRT response prediction are 88.43% and 84.39% respectively, and we showcase the potential of our model in enhancing understanding of the factors contributing to CRT response.



### DeepTracking-Net: 3D Tracking with Unsupervised Learning of Continuous Flow
- **Arxiv ID**: http://arxiv.org/abs/2006.13848v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.13848v1)
- **Published**: 2020-06-24 16:20:48+00:00
- **Updated**: 2020-06-24 16:20:48+00:00
- **Authors**: Shuaihang Yuan, Xiang Li, Yi Fang
- **Comment**: 16 pages, 6 figures
- **Journal**: None
- **Summary**: This paper deals with the problem of 3D tracking, i.e., to find dense correspondences in a sequence of time-varying 3D shapes. Despite deep learning approaches have achieved promising performance for pairwise dense 3D shapes matching, it is a great challenge to generalize those approaches for the tracking of 3D time-varying geometries. In this paper, we aim at handling the problem of 3D tracking, which provides the tracking of the consecutive frames of 3D shapes. We propose a novel unsupervised 3D shape registration framework named DeepTracking-Net, which uses the deep neural networks (DNNs) as auxiliary functions to produce spatially and temporally continuous displacement fields for 3D tracking of objects in a temporal order. Our key novelty is that we present a novel temporal-aware correspondence descriptor (TCD) that captures spatio-temporal essence from consecutive 3D point cloud frames. Specifically, our DeepTracking-Net starts with optimizing a randomly initialized latent TCD. The TCD is then decoded to regress a continuous flow (i.e. a displacement vector field) which assigns a motion vector to every point of time-varying 3D shapes. Our DeepTracking-Net jointly optimizes TCDs and DNNs' weights towards the minimization of an unsupervised alignment loss. Experiments on both simulated and real data sets demonstrate that our unsupervised DeepTracking-Net outperforms the current supervised state-of-the-art method. In addition, we prepare a new synthetic 3D data, named SynMotions, to the 3D tracking and recognition community.



### Movement Tracking by Optical Flow Assisted Inertial Navigation
- **Arxiv ID**: http://arxiv.org/abs/2006.13856v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.13856v1)
- **Published**: 2020-06-24 16:36:13+00:00
- **Updated**: 2020-06-24 16:36:13+00:00
- **Authors**: Lassi Meronen, William J. Wilkinson, Arno Solin
- **Comment**: None
- **Journal**: None
- **Summary**: Robust and accurate six degree-of-freedom tracking on portable devices remains a challenging problem, especially on small hand-held devices such as smartphones. For improved robustness and accuracy, complementary movement information from an IMU and a camera is often fused. Conventional visual-inertial methods fuse information from IMUs with a sparse cloud of feature points tracked by the device camera. We consider a visually dense approach, where the IMU data is fused with the dense optical flow field estimated from the camera data. Learning-based methods applied to the full image frames can leverage visual cues and global consistency of the flow field to improve the flow estimates. We show how a learning-based optical flow model can be combined with conventional inertial navigation, and how ideas from probabilistic deep learning can aid the robustness of the measurement updates. The practical applicability is demonstrated on real-world data acquired by an iPad in a challenging low-texture environment.



### Feedback Graph Attention Convolutional Network for Medical Image Enhancement
- **Arxiv ID**: http://arxiv.org/abs/2006.13863v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.13863v2)
- **Published**: 2020-06-24 16:46:05+00:00
- **Updated**: 2021-04-13 19:25:12+00:00
- **Authors**: Xiaobin Hu, Yanyang Yan, Wenqi Ren, Hongwei Li, Yu Zhao, Amirhossein Bayat, Bjoern Menze
- **Comment**: The description of the experiments is not accurate and complete, and
  some details of equations and expressions should be corrected
- **Journal**: None
- **Summary**: Artifacts, blur and noise are the common distortions degrading MRI images during the acquisition process, and deep neural networks have been demonstrated to help in improving image quality. To well exploit global structural information and texture details, we propose a novel biomedical image enhancement network, named Feedback Graph Attention Convolutional Network (FB-GACN). As a key innovation, we consider the global structure of an image by building a graph network from image sub-regions that we consider to be node features, linking them non-locally according to their similarity. The proposed model consists of three main parts: 1) The parallel graph similarity branch and content branch, where the graph similarity branch aims at exploiting the similarity and symmetry across different image sub-regions in low-resolution feature space and provides additional priors for the content branch to enhance texture details. 2) A feedback mechanism with a recurrent structure to refine low-level representations with high-level information and generate powerful high-level texture details by handling the feedback connections. 3) A reconstruction to remove the artifacts and recover super-resolution images by using the estimated sub-region correlation priors obtained from the graph similarity branch. We evaluate our method on two image enhancement tasks: i) cross-protocol super resolution of diffusion MRI; ii) artifact removal of FLAIR MR images. Experimental results demonstrate that the proposed algorithm outperforms the state-of-the-art methods.



### Automatic Estimation of Self-Reported Pain by Interpretable Representations of Motion Dynamics
- **Arxiv ID**: http://arxiv.org/abs/2006.13882v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.13882v1)
- **Published**: 2020-06-24 17:08:16+00:00
- **Updated**: 2020-06-24 17:08:16+00:00
- **Authors**: Benjamin Szczapa, Mohamed Daoudi, Stefano Berretti, Pietro Pala, Alberto Del Bimbo, Zakia Hammal
- **Comment**: accepted at ICPR 2020 Conference
- **Journal**: None
- **Summary**: We propose an automatic method for pain intensity measurement from video. For each video, pain intensity was measured using the dynamics of facial movement using 66 facial points. Gram matrices formulation was used for facial points trajectory representations on the Riemannian manifold of symmetric positive semi-definite matrices of fixed rank. Curve fitting and temporal alignment were then used to smooth the extracted trajectories. A Support Vector Regression model was then trained to encode the extracted trajectories into ten pain intensity levels consistent with the Visual Analogue Scale for pain intensity measurement. The proposed approach was evaluated using the UNBC McMaster Shoulder Pain Archive and was compared to the state-of-the-art on the same data. Using both 5-fold cross-validation and leave-one-subject-out cross-validation, our results are competitive with respect to state-of-the-art methods.



### Learning Tumor Growth via Follow-Up Volume Prediction for Lung Nodules
- **Arxiv ID**: http://arxiv.org/abs/2006.13890v2
- **DOI**: 10.1007/978-3-030-59725-2_49
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.13890v2)
- **Published**: 2020-06-24 17:18:46+00:00
- **Updated**: 2020-10-09 04:27:42+00:00
- **Authors**: Yamin Li, Jiancheng Yang, Yi Xu, Jingwei Xu, Xiaodan Ye, Guangyu Tao, Xueqian Xie, Guixue Liu
- **Comment**: MICCAI 2020
- **Journal**: None
- **Summary**: Follow-up serves an important role in the management of pulmonary nodules for lung cancer. Imaging diagnostic guidelines with expert consensus have been made to help radiologists make clinical decision for each patient. However, tumor growth is such a complicated process that it is difficult to stratify high-risk nodules from low-risk ones based on morphologic characteristics. On the other hand, recent deep learning studies using convolutional neural networks (CNNs) to predict the malignancy score of nodules, only provides clinicians with black-box predictions. To this end, we propose a unified framework, named Nodule Follow-Up Prediction Network (NoFoNet), which predicts the growth of pulmonary nodules with high-quality visual appearances and accurate quantitative results, given any time interval from baseline observations. It is achieved by predicting future displacement field of each voxel with a WarpNet. A TextureNet is further developed to refine textural details of WarpNet outputs. We also introduce techniques including Temporal Encoding Module and Warp Segmentation Loss to encourage time-aware and shape-aware representation learning. We build an in-house follow-up dataset from two medical centers to validate the effectiveness of the proposed method. NoFoNet significantly outperforms direct prediction by a U-Net in terms of visual quality; more importantly, it demonstrates accurate differentiating performance between high- and low-risk nodules. Our promising results suggest the potentials in computer aided intervention for lung nodule management.



### Automated Chest CT Image Segmentation of COVID-19 Lung Infection based on 3D U-Net
- **Arxiv ID**: http://arxiv.org/abs/2007.04774v1
- **DOI**: 10.1016/j.imu.2021.100681.
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.04774v1)
- **Published**: 2020-06-24 17:29:26+00:00
- **Updated**: 2020-06-24 17:29:26+00:00
- **Authors**: Dominik Müller, Iñaki Soto Rey, Frank Kramer
- **Comment**: Code repository: https://github.com/frankkramer-lab/covid19.MIScnn
- **Journal**: Robust chest CT image segmentation of COVID-19 lung infection
  based on limited data. Informatics in Medicine Unlocked. Volume 25. 2021.
  https://www.sciencedirect.com/science/article/pii/S2352914821001660
- **Summary**: The coronavirus disease 2019 (COVID-19) affects billions of lives around the world and has a significant impact on public healthcare. Due to rising skepticism towards the sensitivity of RT-PCR as screening method, medical imaging like computed tomography offers great potential as alternative. For this reason, automated image segmentation is highly desired as clinical decision support for quantitative assessment and disease monitoring. However, publicly available COVID-19 imaging data is limited which leads to overfitting of traditional approaches. To address this problem, we propose an innovative automated segmentation pipeline for COVID-19 infected regions, which is able to handle small datasets by utilization as variant databases. Our method focuses on on-the-fly generation of unique and random image patches for training by performing several preprocessing methods and exploiting extensive data augmentation. For further reduction of the overfitting risk, we implemented a standard 3D U-Net architecture instead of new or computational complex neural network architectures. Through a 5-fold cross-validation on 20 CT scans of COVID-19 patients, we were able to develop a highly accurate as well as robust segmentation model for lungs and COVID-19 infected regions without overfitting on the limited data. Our method achieved Dice similarity coefficients of 0.956 for lungs and 0.761 for infection. We demonstrated that the proposed method outperforms related approaches, advances the state-of-the-art for COVID-19 segmentation and improves medical image analysis with limited data. The code and model are available under the following link: https://github.com/frankkramer-lab/covid19.MIScnn



### Modelling the Statistics of Cyclic Activities by Trajectory Analysis on the Manifold of Positive-Semi-Definite Matrices
- **Arxiv ID**: http://arxiv.org/abs/2006.13895v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.13895v1)
- **Published**: 2020-06-24 17:29:43+00:00
- **Updated**: 2020-06-24 17:29:43+00:00
- **Authors**: Ettore Maria Celozzi, Luca Ciabini, Luca Cultrera, Pietro Pala, Stefano Berretti, Mohamed Daoudi, Alberto Del Bimbo
- **Comment**: accepted at 15th IEEE International Conference on Automatic Face and
  Gesture Recognition 2020
- **Journal**: None
- **Summary**: In this paper, a model is presented to extract statistical summaries to characterize the repetition of a cyclic body action, for instance a gym exercise, for the purpose of checking the compliance of the observed action to a template one and highlighting the parts of the action that are not correctly executed (if any). The proposed system relies on a Riemannian metric to compute the distance between two poses in such a way that the geometry of the manifold where the pose descriptors lie is preserved; a model to detect the begin and end of each cycle; a model to temporally align the poses of different cycles so as to accurately estimate the \emph{cross-sectional} mean and variance of poses across different cycles. The proposed model is demonstrated using gym videos taken from the Internet.



### Feature-Dependent Cross-Connections in Multi-Path Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2006.13904v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.13904v2)
- **Published**: 2020-06-24 17:38:03+00:00
- **Updated**: 2021-01-01 16:49:47+00:00
- **Authors**: Dumindu Tissera, Kasun Vithanage, Rukshan Wijesinghe, Kumara Kahatapitiya, Subha Fernando, Ranga Rodrigo
- **Comment**: International Conference on Pattern Recognition (ICPR) 2020
- **Journal**: None
- **Summary**: Learning a particular task from a dataset, samples in which originate from diverse contexts, is challenging, and usually addressed by deepening or widening standard neural networks. As opposed to conventional network widening, multi-path architectures restrict the quadratic increment of complexity to a linear scale. However, existing multi-column/path networks or model ensembling methods do not consider any feature-dependent allocation of parallel resources, and therefore, tend to learn redundant features. Given a layer in a multi-path network, if we restrict each path to learn a context-specific set of features and introduce a mechanism to intelligently allocate incoming feature maps to such paths, each path can specialize in a certain context, reducing the redundancy and improving the quality of extracted features. This eventually leads to better-optimized usage of parallel resources. To do this, we propose inserting feature-dependent cross-connections between parallel sets of feature maps in successive layers. The weighting coefficients of these cross-connections are computed from the input features of the particular layer. Our multi-path networks show improved image recognition accuracy at a similar complexity compared to conventional and state-of-the-art methods for deepening, widening and adaptive feature extracting, in both small and large scale datasets.



### 3DMotion-Net: Learning Continuous Flow Function for 3D Motion Prediction
- **Arxiv ID**: http://arxiv.org/abs/2006.13906v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.13906v1)
- **Published**: 2020-06-24 17:39:19+00:00
- **Updated**: 2020-06-24 17:39:19+00:00
- **Authors**: Shuaihang Yuan, Xiang Li, Anthony Tzes, Yi Fang
- **Comment**: 8 pages, 7 figures
- **Journal**: None
- **Summary**: In this paper, we deal with the problem to predict the future 3D motions of 3D object scans from previous two consecutive frames. Previous methods mostly focus on sparse motion prediction in the form of skeletons. While in this paper we focus on predicting dense 3D motions in the from of 3D point clouds. To approach this problem, we propose a self-supervised approach that leverages the power of the deep neural network to learn a continuous flow function of 3D point clouds that can predict temporally consistent future motions and naturally bring out the correspondences among consecutive point clouds at the same time. More specifically, in our approach, to eliminate the unsolved and challenging process of defining a discrete point convolution on 3D point cloud sequences to encode spatial and temporal information, we introduce a learnable latent code to represent the temporal-aware shape descriptor which is optimized during model training. Moreover, a temporally consistent motion Morpher is proposed to learn a continuous flow field which deforms a 3D scan from the current frame to the next frame. We perform extensive experiments on D-FAUST, SCAPE and TOSCA benchmark data sets and the results demonstrate that our approach is capable of handling temporally inconsistent input and produces consistent future 3D motion while requiring no ground truth supervision.



### Improving task-specific representation via 1M unlabelled images without any extra knowledge
- **Arxiv ID**: http://arxiv.org/abs/2006.13919v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.13919v1)
- **Published**: 2020-06-24 17:49:05+00:00
- **Updated**: 2020-06-24 17:49:05+00:00
- **Authors**: Aayush Bansal
- **Comment**: Technical Report
- **Journal**: None
- **Summary**: We present a case-study to improve the task-specific representation by leveraging a million unlabelled images without any extra knowledge. We propose an exceedingly simple method of conditioning an existing representation on a diverse data distribution and observe that a model trained on diverse examples acts as a better initialization. We extensively study our findings for the task of surface normal estimation and semantic segmentation from a single image. We improve surface normal estimation on NYU-v2 depth dataset and semantic segmentation on PASCAL VOC by 4% over base model. We did not use any task-specific knowledge or auxiliary tasks, neither changed hyper-parameters nor made any modification in the underlying neural network architecture.



### Diffusion-Weighted Magnetic Resonance Brain Images Generation with Generative Adversarial Networks and Variational Autoencoders: A Comparison Study
- **Arxiv ID**: http://arxiv.org/abs/2006.13944v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.13944v1)
- **Published**: 2020-06-24 18:00:01+00:00
- **Updated**: 2020-06-24 18:00:01+00:00
- **Authors**: Alejandro Ungría Hirte, Moritz Platscher, Thomas Joyce, Jeremy J. Heit, Eric Tranvinh, Christian Federau
- **Comment**: 20 pages, 5 figures, 2 tables
- **Journal**: None
- **Summary**: We show that high quality, diverse and realistic-looking diffusion-weighted magnetic resonance images can be synthesized using deep generative models. Based on professional neuroradiologists' evaluations and diverse metrics with respect to quality and diversity of the generated synthetic brain images, we present two networks, the Introspective Variational Autoencoder and the Style-Based GAN, that qualify for data augmentation in the medical field, where information is saved in a dispatched and inhomogeneous way and access to it is in many aspects restricted.



### Bit Error Robustness for Energy-Efficient DNN Accelerators
- **Arxiv ID**: http://arxiv.org/abs/2006.13977v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AR, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.13977v3)
- **Published**: 2020-06-24 18:23:10+00:00
- **Updated**: 2021-04-09 15:24:12+00:00
- **Authors**: David Stutz, Nandhini Chandramoorthy, Matthias Hein, Bernt Schiele
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural network (DNN) accelerators received considerable attention in past years due to saved energy compared to mainstream hardware. Low-voltage operation of DNN accelerators allows to further reduce energy consumption significantly, however, causes bit-level failures in the memory storing the quantized DNN weights. In this paper, we show that a combination of robust fixed-point quantization, weight clipping, and random bit error training (RandBET) improves robustness against random bit errors in (quantized) DNN weights significantly. This leads to high energy savings from both low-voltage operation as well as low-precision quantization. Our approach generalizes across operating voltages and accelerators, as demonstrated on bit errors from profiled SRAM arrays. We also discuss why weight clipping alone is already a quite effective way to achieve robustness against bit errors. Moreover, we specifically discuss the involved trade-offs regarding accuracy, robustness and precision: Without losing more than 1% in accuracy compared to a normally trained 8-bit DNN, we can reduce energy consumption on CIFAR-10 by 20%. Higher energy savings of, e.g., 30%, are possible at the cost of 2.5% accuracy, even for 4-bit DNNs.



### Extended Labeled Faces in-the-Wild (ELFW): Augmenting Classes for Face Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2006.13980v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4.6
- **Links**: [PDF](http://arxiv.org/pdf/2006.13980v1)
- **Published**: 2020-06-24 18:26:17+00:00
- **Updated**: 2020-06-24 18:26:17+00:00
- **Authors**: Rafael Redondo, Jaume Gibert
- **Comment**: 14 pages, 12 figures
- **Journal**: None
- **Summary**: Existing face datasets often lack sufficient representation of occluding objects, which can hinder recognition, but also supply meaningful information to understand the visual context. In this work, we introduce Extended Labeled Faces in-the-Wild (ELFW), a dataset supplementing with additional face-related categories -- and also additional faces -- the originally released semantic labels in the vastly used Labeled Faces in-the-Wild (LFW) dataset. Additionally, two object-based data augmentation techniques are deployed to synthetically enrich under-represented categories which, in benchmarking experiments, reveal that not only segmenting the augmented categories improves, but also the remaining ones benefit.



### MCAL: Minimum Cost Human-Machine Active Labeling
- **Arxiv ID**: http://arxiv.org/abs/2006.13999v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.13999v3)
- **Published**: 2020-06-24 19:01:05+00:00
- **Updated**: 2023-02-27 01:24:00+00:00
- **Authors**: Hang Qiu, Krishna Chintalapudi, Ramesh Govindan
- **Comment**: ICLR 2023
- **Journal**: None
- **Summary**: Today, ground-truth generation uses data sets annotated by cloud-based annotation services. These services rely on human annotation, which can be prohibitively expensive. In this paper, we consider the problem of hybrid human-machine labeling, which trains a classifier to accurately auto-label part of the data set. However, training the classifier can be expensive too. We propose an iterative approach that minimizes total overall cost by, at each step, jointly determining which samples to label using humans and which to label using the trained classifier. We validate our approach on well known public data sets such as Fashion-MNIST, CIFAR-10, CIFAR-100, and ImageNet. In some cases, our approach has 6x lower overall cost relative to human labeling the entire data set, and is always cheaper than the cheapest competing strategy.



### Road obstacles positional and dynamic features extraction combining object detection, stereo disparity maps and optical flow data
- **Arxiv ID**: http://arxiv.org/abs/2006.14011v1
- **DOI**: 10.1007/s00138-020-01126-w
- **Categories**: **cs.CV**, I.5; I.5.4; I.4; I.4.7; I.4.8; I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/2006.14011v1)
- **Published**: 2020-06-24 19:29:06+00:00
- **Updated**: 2020-06-24 19:29:06+00:00
- **Authors**: Thiago Rateke, Aldo von Wangenheim
- **Comment**: 11 pages
- **Journal**: Machine Vision and Applications, 2020
- **Summary**: One of the most relevant tasks in an intelligent vehicle navigation system is the detection of obstacles. It is important that a visual perception system for navigation purposes identifies obstacles, and it is also important that this system can extract essential information that may influence the vehicle's behavior, whether it will be generating an alert for a human driver or guide an autonomous vehicle in order to be able to make its driving decisions. In this paper we present an approach for the identification of obstacles and extraction of class, position, depth and motion information from these objects that employs data gained exclusively from passive vision. We performed our experiments on two different data-sets and the results obtained shown a good efficacy from the use of depth and motion patterns to assess the obstacles' potential threat status.



### Compositional Explanations of Neurons
- **Arxiv ID**: http://arxiv.org/abs/2006.14032v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CL, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.14032v2)
- **Published**: 2020-06-24 20:37:05+00:00
- **Updated**: 2021-02-02 23:46:51+00:00
- **Authors**: Jesse Mu, Jacob Andreas
- **Comment**: NeurIPS 2020
- **Journal**: None
- **Summary**: We describe a procedure for explaining neurons in deep representations by identifying compositional logical concepts that closely approximate neuron behavior. Compared to prior work that uses atomic labels as explanations, analyzing neurons compositionally allows us to more precisely and expressively characterize their behavior. We use this procedure to answer several questions on interpretability in models for vision and natural language processing. First, we examine the kinds of abstractions learned by neurons. In image classification, we find that many neurons learn highly abstract but semantically coherent visual concepts, while other polysemantic neurons detect multiple unrelated features; in natural language inference (NLI), neurons learn shallow lexical heuristics from dataset biases. Second, we see whether compositional explanations give us insight into model performance: vision neurons that detect human-interpretable concepts are positively correlated with task performance, while NLI neurons that fire for shallow heuristics are negatively correlated with task performance. Finally, we show how compositional explanations provide an accessible way for end users to produce simple "copy-paste" adversarial examples that change model behavior in predictable ways.



### Blacklight: Scalable Defense for Neural Networks against Query-Based Black-Box Attacks
- **Arxiv ID**: http://arxiv.org/abs/2006.14042v3
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.14042v3)
- **Published**: 2020-06-24 20:52:24+00:00
- **Updated**: 2022-06-09 05:11:53+00:00
- **Authors**: Huiying Li, Shawn Shan, Emily Wenger, Jiayun Zhang, Haitao Zheng, Ben Y. Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning systems are known to be vulnerable to adversarial examples. In particular, query-based black-box attacks do not require knowledge of the deep learning model, but can compute adversarial examples over the network by submitting queries and inspecting returns. Recent work largely improves the efficiency of those attacks, demonstrating their practicality on today's ML-as-a-service platforms.   We propose Blacklight, a new defense against query-based black-box adversarial attacks. The fundamental insight driving our design is that, to compute adversarial examples, these attacks perform iterative optimization over the network, producing image queries highly similar in the input space. Blacklight detects query-based black-box attacks by detecting highly similar queries, using an efficient similarity engine operating on probabilistic content fingerprints. We evaluate Blacklight against eight state-of-the-art attacks, across a variety of models and image classification tasks. Blacklight identifies them all, often after only a handful of queries. By rejecting all detected queries, Blacklight prevents any attack to complete, even when attackers persist to submit queries after account ban or query rejection. Blacklight is also robust against several powerful countermeasures, including an optimal black-box attack that approximates white-box attacks in efficiency. Finally, we illustrate how Blacklight generalizes to other domains like text classification.



### Time for a Background Check! Uncovering the impact of Background Features on Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2006.14077v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.14077v1)
- **Published**: 2020-06-24 22:17:50+00:00
- **Updated**: 2020-06-24 22:17:50+00:00
- **Authors**: Vikash Sehwag, Rajvardhan Oak, Mung Chiang, Prateek Mittal
- **Comment**: 6 pages, 5 figures
- **Journal**: None
- **Summary**: With increasing expressive power, deep neural networks have significantly improved the state-of-the-art on image classification datasets, such as ImageNet. In this paper, we investigate to what extent the increasing performance of deep neural networks is impacted by background features? In particular, we focus on background invariance, i.e., accuracy unaffected by switching background features and background influence, i.e., predictive power of background features itself when foreground is masked. We perform experiments with 32 different neural networks ranging from small-size networks to large-scale networks trained with up to one Billion images. Our investigations reveal that increasing expressive power of DNNs leads to higher influence of background features, while simultaneously, increases their ability to make the correct prediction when background features are removed or replaced with a randomly selected texture-based background.



### The flag manifold as a tool for analyzing and comparing data sets
- **Arxiv ID**: http://arxiv.org/abs/2006.14086v1
- **DOI**: None
- **Categories**: **cs.CV**, math.OC, 65F45, 62H35, 68T10
- **Links**: [PDF](http://arxiv.org/pdf/2006.14086v1)
- **Published**: 2020-06-24 22:29:02+00:00
- **Updated**: 2020-06-24 22:29:02+00:00
- **Authors**: Xiaofeng Ma, Michael Kirby, Chris Peterson
- **Comment**: 15 pages, 8 figures
- **Journal**: None
- **Summary**: The shape and orientation of data clouds reflect variability in observations that can confound pattern recognition systems. Subspace methods, utilizing Grassmann manifolds, have been a great aid in dealing with such variability. However, this usefulness begins to falter when the data cloud contains sufficiently many outliers corresponding to stray elements from another class or when the number of data points is larger than the number of features. We illustrate how nested subspace methods, utilizing flag manifolds, can help to deal with such additional confounding factors. Flag manifolds, which are parameter spaces for nested subspaces, are a natural geometric generalization of Grassmann manifolds. To make practical comparisons on a flag manifold, algorithms are proposed for determining the distances between points $[A], [B]$ on a flag manifold, where $A$ and $B$ are arbitrary orthogonal matrix representatives for $[A]$ and $[B]$, and for determining the initial direction of these minimal length geodesics. The approach is illustrated in the context of (hyper) spectral imagery showing the impact of ambient dimension, sample dimension, and flag structure.



### Neural Architecture Design for GPU-Efficient Networks
- **Arxiv ID**: http://arxiv.org/abs/2006.14090v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.14090v4)
- **Published**: 2020-06-24 22:42:18+00:00
- **Updated**: 2020-08-11 22:54:26+00:00
- **Authors**: Ming Lin, Hesen Chen, Xiuyu Sun, Qi Qian, Hao Li, Rong Jin
- **Comment**: update training setting
- **Journal**: None
- **Summary**: Many mission-critical systems are based on GPU for inference. It requires not only high recognition accuracy but also low latency in responding time. Although many studies are devoted to optimizing the structure of deep models for efficient inference, most of them do not leverage the architecture of \textbf{modern GPU} for fast inference, leading to suboptimal performance. To address this issue, we propose a general principle for designing GPU-efficient networks based on extensive empirical studies. This design principle enables us to search for GPU-efficient network structures effectively by a simple and lightweight method as opposed to most Neural Architecture Search (NAS) methods that are complicated and computationally expensive. Based on the proposed framework, we design a family of GPU-Efficient Networks, or GENets in short. We did extensive evaluations on multiple GPU platforms and inference engines. While achieving $\geq 81.3\%$ top-1 accuracy on ImageNet, GENet is up to $6.4$ times faster than EfficienNet on GPU. It also outperforms most state-of-the-art models that are more efficient than EfficientNet in high precision regimes. Our source code and pre-trained models are available from \url{https://github.com/idstcv/GPU-Efficient-Networks}.



### Block-matching in FPGA
- **Arxiv ID**: http://arxiv.org/abs/2006.14105v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.AR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.14105v1)
- **Published**: 2020-06-24 23:53:28+00:00
- **Updated**: 2020-06-24 23:53:28+00:00
- **Authors**: Rafael Pizarro Solar, Michal Pleskowicz
- **Comment**: 19 pages, 15 figures, paper submitted in "CS413 - Computational
  Photography" at EPFL, for project repository see
  $\href{https://github.com/UlisesLuzius/ImageProcessingPipeline/}{\text{link}}$
- **Journal**: None
- **Summary**: Block-matching and 3D filtering (BM3D) is an image denoising algorithm that works in two similar steps. Both of these steps need to perform grouping by block-matching. We implement the block-matching in an FPGA, leveraging its ability to perform parallel computations. Our goal is to enable other researchers to use our solution in the future for real-time video denoising in video cameras that use FPGAs (such as the AXIOM Beta).



### Kinematic-Structure-Preserved Representation for Unsupervised 3D Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2006.14107v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.14107v1)
- **Published**: 2020-06-24 23:56:33+00:00
- **Updated**: 2020-06-24 23:56:33+00:00
- **Authors**: Jogendra Nath Kundu, Siddharth Seth, Rahul M V, Mugalodi Rakesh, R. Venkatesh Babu, Anirban Chakraborty
- **Comment**: AAAI 2020 (Oral)
- **Journal**: None
- **Summary**: Estimation of 3D human pose from monocular image has gained considerable attention, as a key step to several human-centric applications. However, generalizability of human pose estimation models developed using supervision on large-scale in-studio datasets remains questionable, as these models often perform unsatisfactorily on unseen in-the-wild environments. Though weakly-supervised models have been proposed to address this shortcoming, performance of such models relies on availability of paired supervision on some related tasks, such as 2D pose or multi-view image pairs. In contrast, we propose a novel kinematic-structure-preserved unsupervised 3D pose estimation framework, which is not restrained by any paired or unpaired weak supervisions. Our pose estimation framework relies on a minimal set of prior knowledge that defines the underlying kinematic 3D structure, such as skeletal joint connectivity information with bone-length ratios in a fixed canonical scale. The proposed model employs three consecutive differentiable transformations named as forward-kinematics, camera-projection and spatial-map transformation. This design not only acts as a suitable bottleneck stimulating effective pose disentanglement but also yields interpretable latent pose representations avoiding training of an explicit latent embedding to pose mapper. Furthermore, devoid of unstable adversarial setup, we re-utilize the decoder to formalize an energy-based loss, which enables us to learn from in-the-wild videos, beyond laboratory settings. Comprehensive experiments demonstrate our state-of-the-art unsupervised and weakly-supervised pose estimation performance on both Human3.6M and MPI-INF-3DHP datasets. Qualitative results on unseen environments further establish our superior generalization ability.



