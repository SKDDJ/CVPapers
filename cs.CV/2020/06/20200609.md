# Arxiv Papers in cs.CV on 2020-06-09
### Implicit Class-Conditioned Domain Alignment for Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2006.04996v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML, 68T07
- **Links**: [PDF](http://arxiv.org/pdf/2006.04996v1)
- **Published**: 2020-06-09 00:20:21+00:00
- **Updated**: 2020-06-09 00:20:21+00:00
- **Authors**: Xiang Jiang, Qicheng Lao, Stan Matwin, Mohammad Havaei
- **Comment**: Accepted at ICML2020. For code, see
  https://github.com/xiangdal/implicit_alignment
- **Journal**: None
- **Summary**: We present an approach for unsupervised domain adaptation---with a strong focus on practical considerations of within-domain class imbalance and between-domain class distribution shift---from a class-conditioned domain alignment perspective. Current methods for class-conditioned domain alignment aim to explicitly minimize a loss function based on pseudo-label estimations of the target domain. However, these methods suffer from pseudo-label bias in the form of error accumulation. We propose a method that removes the need for explicit optimization of model parameters from pseudo-labels directly. Instead, we present a sampling-based implicit alignment approach, where the sample selection procedure is implicitly guided by the pseudo-labels. Theoretical analysis reveals the existence of a domain-discriminator shortcut in misaligned classes, which is addressed by the proposed implicit alignment approach to facilitate domain-adversarial learning. Empirical results and ablation studies confirm the effectiveness of the proposed approach, especially in the presence of within-domain class imbalance and between-domain class distribution shift.



### Machine Learning Automatically Detects COVID-19 using Chest CTs in a Large Multicenter Cohort
- **Arxiv ID**: http://arxiv.org/abs/2006.04998v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.04998v3)
- **Published**: 2020-06-09 00:40:35+00:00
- **Updated**: 2020-10-10 00:53:14+00:00
- **Authors**: Eduardo Jose Mortani Barbosa Jr., Bogdan Georgescu, Shikha Chaganti, Gorka Bastarrika Aleman, Jordi Broncano Cabrero, Guillaume Chabin, Thomas Flohr, Philippe Grenier, Sasa Grbic, Nakul Gupta, François Mellot, Savvas Nicolaou, Thomas Re, Pina Sanelli, Alexander W. Sauter, Youngjin Yoo, Valentin Ziebandt, Dorin Comaniciu
- **Comment**: None
- **Journal**: None
- **Summary**: Objectives: To investigate machine-learning classifiers and interpretable models using chest CT for detection of COVID-19 and differentiation from other pneumonias, ILD and normal CTs.   Methods: Our retrospective multi-institutional study obtained 2096 chest CTs from 16 institutions (including 1077 COVID-19 patients). Training/testing cohorts included 927/100 COVID-19, 388/33 ILD, 189/33 other pneumonias, and 559/34 normal (no pathologies) CTs. A metric-based approach for classification of COVID-19 used interpretable features, relying on logistic regression and random forests. A deep learning-based classifier differentiated COVID-19 via 3D features extracted directly from CT attenuation and probability distribution of airspace opacities.   Results: Most discriminative features of COVID-19 are percentage of airspace opacity and peripheral and basal predominant opacities, concordant with the typical characterization of COVID-19 in the literature. Unsupervised hierarchical clustering compares feature distribution across COVID-19 and control cohorts. The metrics-based classifier achieved AUC=0.83, sensitivity=0.74, and specificity=0.79 of versus respectively 0.93, 0.90, and 0.83 for the DL-based classifier. Most of ambiguity comes from non-COVID-19 pneumonia with manifestations that overlap with COVID-19, as well as mild COVID-19 cases. Non-COVID-19 classification performance is 91% for ILD, 64% for other pneumonias and 94% for no pathologies, which demonstrates the robustness of our method against different compositions of control groups.   Conclusions: Our new method accurately discriminates COVID-19 from other types of pneumonia, ILD, and no pathologies CTs, using quantitative imaging features derived from chest CT, while balancing interpretability of results and classification performance, and therefore may be useful to facilitate diagnosis of COVID-19.



### RGB-D-E: Event Camera Calibration for Fast 6-DOF Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/2006.05011v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.05011v2)
- **Published**: 2020-06-09 01:55:48+00:00
- **Updated**: 2020-08-05 20:41:29+00:00
- **Authors**: Etienne Dubeau, Mathieu Garon, Benoit Debaque, Raoul de Charette, Jean-François Lalonde
- **Comment**: 9 pages, 9 figures
- **Journal**: None
- **Summary**: Augmented reality devices require multiple sensors to perform various tasks such as localization and tracking. Currently, popular cameras are mostly frame-based (e.g. RGB and Depth) which impose a high data bandwidth and power usage. With the necessity for low power and more responsive augmented reality systems, using solely frame-based sensors imposes limits to the various algorithms that needs high frequency data from the environement. As such, event-based sensors have become increasingly popular due to their low power, bandwidth and latency, as well as their very high frequency data acquisition capabilities. In this paper, we propose, for the first time, to use an event-based camera to increase the speed of 3D object tracking in 6 degrees of freedom. This application requires handling very high object speed to convey compelling AR experiences. To this end, we propose a new system which combines a recent RGB-D sensor (Kinect Azure) with an event camera (DAVIS346). We develop a deep learning approach, which combines an existing RGB-D network along with a novel event-based network in a cascade fashion, and demonstrate that our approach significantly improves the robustness of a state-of-the-art frame-based 6-DOF object tracker using our RGB-D-E pipeline.



### Can Synthetic Data Improve Object Detection Results for Remote Sensing Images?
- **Arxiv ID**: http://arxiv.org/abs/2006.05015v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.05015v1)
- **Published**: 2020-06-09 02:23:22+00:00
- **Updated**: 2020-06-09 02:23:22+00:00
- **Authors**: Weixing Liu, Jun Liu, Bin Luo
- **Comment**: 5 pages, 5 figures
- **Journal**: None
- **Summary**: Deep learning approaches require enough training samples to perform well, but it is a challenge to collect enough real training data and label them manually. In this letter, we propose the use of realistic synthetic data with a wide distribution to improve the performance of remote sensing image aircraft detection. Specifically, to increase the variability of synthetic data, we randomly set the parameters during rendering, such as the size of the instance and the class of background images. In order to make the synthetic images more realistic, we then refine the synthetic images at the pixel level using CycleGAN with real unlabeled images. We also fine-tune the model with a small amount of real data, to obtain a higher accuracy. Experiments on NWPU VHR-10, UCAS-AOD and DIOR datasets demonstrate that the proposed method can be applied for augmenting insufficient real data.



### Deep learning to estimate the physical proportion of infected region of lung for COVID-19 pneumonia with CT image set
- **Arxiv ID**: http://arxiv.org/abs/2006.05018v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.05018v1)
- **Published**: 2020-06-09 02:38:40+00:00
- **Updated**: 2020-06-09 02:38:40+00:00
- **Authors**: Wei Wu, Yu Shi, Xukun Li, Yukun Zhou, Peng Du, Shuangzhi Lv, Tingbo Liang, Jifang Sheng
- **Comment**: None
- **Journal**: None
- **Summary**: Utilizing computed tomography (CT) images to quickly estimate the severity of cases with COVID-19 is one of the most straightforward and efficacious methods. Two tasks were studied in this present paper. One was to segment the mask of intact lung in case of pneumonia. Another was to generate the masks of regions infected by COVID-19. The masks of these two parts of images then were converted to corresponding volumes to calculate the physical proportion of infected region of lung. A total of 129 CT image set were herein collected and studied. The intrinsic Hounsfiled value of CT images was firstly utilized to generate the initial dirty version of labeled masks both for intact lung and infected regions. Then, the samples were carefully adjusted and improved by two professional radiologists to generate the final training set and test benchmark. Two deep learning models were evaluated: UNet and 2.5D UNet. For the segment of infected regions, a deep learning based classifier was followed to remove unrelated blur-edged regions that were wrongly segmented out such as air tube and blood vessel tissue etc. For the segmented masks of intact lung and infected regions, the best method could achieve 0.972 and 0.757 measure in mean Dice similarity coefficient on our test benchmark. As the overall proportion of infected region of lung, the final result showed 0.961 (Pearson's correlation coefficient) and 11.7% (mean absolute percent error). The instant proportion of infected regions of lung could be used as a visual evidence to assist clinical physician to determine the severity of the case. Furthermore, a quantified report of infected regions can help predict the prognosis for COVID-19 cases which were scanned periodically within the treatment cycle.



### High Tissue Contrast MRI Synthesis Using Multi-Stage Attention-GAN for Glioma Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2006.05030v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.05030v1)
- **Published**: 2020-06-09 03:21:30+00:00
- **Updated**: 2020-06-09 03:21:30+00:00
- **Authors**: Mohammad Hamghalam, Baiying Lei, Tianfu Wang
- **Comment**: Will be published in Thirty-Fourth AAAI Conference on Artificial
  Intelligence (AAAI-2020)
- **Journal**: None
- **Summary**: Magnetic resonance imaging (MRI) provides varying tissue contrast images of internal organs based on a strong magnetic field. Despite the non-invasive advantage of MRI in frequent imaging, the low contrast MR images in the target area make tissue segmentation a challenging problem. This paper demonstrates the potential benefits of image-to-image translation techniques to generate synthetic high tissue contrast (HTC) images. Notably, we adopt a new cycle generative adversarial network (CycleGAN) with an attention mechanism to increase the contrast within underlying tissues. The attention block, as well as training on HTC images, guides our model to converge on certain tissues. To increase the resolution of HTC images, we employ multi-stage architecture to focus on one particular tissue as a foreground and filter out the irrelevant background in each stage. This multi-stage structure also alleviates the common artifacts of the synthetic images by decreasing the gap between source and target domains. We show the application of our method for synthesizing HTC images on brain MR scans, including glioma tumor. We also employ HTC MR images in both the end-to-end and two-stage segmentation structure to confirm the effectiveness of these images. The experiments over three competitive segmentation baselines on BraTS 2018 dataset indicate that incorporating the synthetic HTC images in the multi-modal segmentation framework improves the average Dice scores 0.8%, 0.6%, and 0.5% on the whole tumor, tumor core, and enhancing tumor, respectively, while eliminating one real MRI sequence from the segmentation procedure.



### Single Image Deraining via Scale-space Invariant Attention Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2006.05049v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.05049v2)
- **Published**: 2020-06-09 04:59:26+00:00
- **Updated**: 2020-06-11 01:35:10+00:00
- **Authors**: Bo Pang, Deming Zhai, Junjun Jiang, Xianming Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Image enhancement from degradation of rainy artifacts plays a critical role in outdoor visual computing systems. In this paper, we tackle the notion of scale that deals with visual changes in appearance of rain steaks with respect to the camera. Specifically, we revisit multi-scale representation by scale-space theory, and propose to represent the multi-scale correlation in convolutional feature domain, which is more compact and robust than that in pixel domain. Moreover, to improve the modeling ability of the network, we do not treat the extracted multi-scale features equally, but design a novel scale-space invariant attention mechanism to help the network focus on parts of the features. In this way, we summarize the most activated presence of feature maps as the salient features. Extensive experiments results on synthetic and real rainy scenes demonstrate the superior performance of our scheme over the state-of-the-arts.



### Deeply Shared Filter Bases for Parameter-Efficient Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2006.05066v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.05066v4)
- **Published**: 2020-06-09 06:09:42+00:00
- **Updated**: 2021-11-21 09:53:04+00:00
- **Authors**: Woochul Kang, Daeyeon Kim
- **Comment**: None
- **Journal**: None
- **Summary**: Modern convolutional neural networks (CNNs) have massive identical convolution blocks, and, hence, recursive sharing of parameters across these blocks has been proposed to reduce the amount of parameters. However, naive sharing of parameters poses many challenges such as limited representational power and the vanishing/exploding gradients problem of recursively shared parameters. In this paper, we present a recursive convolution block design and training method, in which a recursively shareable part, or a filter basis, is separated and learned while effectively avoiding the vanishing/exploding gradients problem during training. We show that the unwieldy vanishing/exploding gradients problem can be controlled by enforcing the elements of the filter basis orthonormal, and empirically demonstrate that the proposed orthogonality regularization improves the flow of gradients during training. Experimental results on image classification and object detection show that our approach, unlike previous parameter-sharing approaches, does not trade performance to save parameters and consistently outperforms overparameterized counterpart networks. This superior performance demonstrates that the proposed recursive convolution block design and the orthogonality regularization not only prevent performance degradation, but also consistently improve the representation capability while a significant amount of parameters are recursively shared.



### Detection of Makeup Presentation Attacks based on Deep Face Representations
- **Arxiv ID**: http://arxiv.org/abs/2006.05074v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.05074v2)
- **Published**: 2020-06-09 06:53:58+00:00
- **Updated**: 2021-01-19 11:19:14+00:00
- **Authors**: Christian Rathgeb, Pawel Drozdowski, Christoph Busch
- **Comment**: published at 25th International Conference on Pattern Recognition
  (ICPR'2020)
- **Journal**: None
- **Summary**: Facial cosmetics have the ability to substantially alter the facial appearance, which can negatively affect the decisions of a face recognition. In addition, it was recently shown that the application of makeup can be abused to launch so-called makeup presentation attacks. In such attacks, the attacker might apply heavy makeup in order to achieve the facial appearance of a target subject for the purpose of impersonation. In this work, we assess the vulnerability of a COTS face recognition system to makeup presentation attacks employing the publicly available Makeup Induced Face Spoofing (MIFS) database. It is shown that makeup presentation attacks might seriously impact the security of the face recognition system. Further, we propose an attack detection scheme which distinguishes makeup presentation attacks from genuine authentication attempts by analysing differences in deep face representations obtained from potential makeup presentation attacks and corresponding target face images. The proposed detection system employs a machine learning-based classifier, which is trained with synthetically generated makeup presentation attacks utilizing a generative adversarial network for facial makeup transfer in conjunction with image warping. Experimental evaluations conducted using the MIFS database reveal a detection equal error rate of 0.7% for the task of separating genuine authentication attempts from makeup presentation attacks.



### SEKD: Self-Evolving Keypoint Detection and Description
- **Arxiv ID**: http://arxiv.org/abs/2006.05077v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.05077v1)
- **Published**: 2020-06-09 06:56:50+00:00
- **Updated**: 2020-06-09 06:56:50+00:00
- **Authors**: Yafei Song, Ling Cai, Jia Li, Yonghong Tian, Mingyang Li
- **Comment**: None
- **Journal**: None
- **Summary**: Researchers have attempted utilizing deep neural network (DNN) to learn novel local features from images inspired by its recent successes on a variety of vision tasks. However, existing DNN-based algorithms have not achieved such remarkable progress that could be partly attributed to insufficient utilization of the interactive characters between local feature detector and descriptor. To alleviate these difficulties, we emphasize two desired properties, i.e., repeatability and reliability, to simultaneously summarize the inherent and interactive characters of local feature detector and descriptor. Guided by these properties, a self-supervised framework, namely self-evolving keypoint detection and description (SEKD), is proposed to learn an advanced local feature model from unlabeled natural images. Additionally, to have performance guarantees, novel training strategies have also been dedicatedly designed to minimize the gap between the learned feature and its properties. We benchmark the proposed method on homography estimation, relative pose estimation, and structure-from-motion tasks. Extensive experimental results demonstrate that the proposed method outperforms popular hand-crafted and DNN-based methods by remarkable margins. Ablation studies also verify the effectiveness of each critical training strategy. We will release our code along with the trained model publicly.



### PNL: Efficient Long-Range Dependencies Extraction with Pyramid Non-Local Module for Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2006.05091v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.05091v1)
- **Published**: 2020-06-09 07:40:23+00:00
- **Updated**: 2020-06-09 07:40:23+00:00
- **Authors**: Yuecong Xu, Haozhi Cao, Jianfei Yang, Kezhi Mao, Jianxiong Yin, Simon See
- **Comment**: Single column, 26 pages, 6 figures
- **Journal**: None
- **Summary**: Long-range spatiotemporal dependencies capturing plays an essential role in improving video features for action recognition. The non-local block inspired by the non-local means is designed to address this challenge and have shown excellent performance. However, the non-local block brings significant increase in computation cost to the original network. It also lacks the ability to model regional correlation in videos. To address the above limitations, we propose Pyramid Non-Local (PNL) module, which extends the non-local block by incorporating regional correlation at multiple scales through a pyramid structured module. This extension upscales the effectiveness of non-local operation by attending to the interaction between different regions. Empirical results prove the effectiveness and efficiency of our PNL module, which achieves state-of-the-art performance of 83.09% on the Mini-Kinetics dataset, with decreased computation cost compared to the non-local block.



### Towards an Intrinsic Definition of Robustness for a Classifier
- **Arxiv ID**: http://arxiv.org/abs/2006.05095v2
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.05095v2)
- **Published**: 2020-06-09 07:47:21+00:00
- **Updated**: 2020-06-11 12:40:07+00:00
- **Authors**: Théo Giraudon, Vincent Gripon, Matthias Löwe, Franck Vermet
- **Comment**: 13 pages
- **Journal**: None
- **Summary**: The robustness of classifiers has become a question of paramount importance in the past few years. Indeed, it has been shown that state-of-the-art deep learning architectures can easily be fooled with imperceptible changes to their inputs. Therefore, finding good measures of robustness of a trained classifier is a key issue in the field. In this paper, we point out that averaging the radius of robustness of samples in a validation set is a statistically weak measure. We propose instead to weight the importance of samples depending on their difficulty. We motivate the proposed score by a theoretical case study using logistic regression, where we show that the proposed score is independent of the choice of the samples it is evaluated upon. We also empirically demonstrate the ability of the proposed score to measure robustness of classifiers with little dependence on the choice of samples in more complex settings, including deep convolutional neural networks and real datasets.



### GAP++: Learning to generate target-conditioned adversarial examples
- **Arxiv ID**: http://arxiv.org/abs/2006.05097v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.05097v1)
- **Published**: 2020-06-09 07:49:49+00:00
- **Updated**: 2020-06-09 07:49:49+00:00
- **Authors**: Xiaofeng Mao, Yuefeng Chen, Yuhong Li, Yuan He, Hui Xue
- **Comment**: Accepted to IJCAI 2019 AIBS Workshop
- **Journal**: None
- **Summary**: Adversarial examples are perturbed inputs which can cause a serious threat for machine learning models. Finding these perturbations is such a hard task that we can only use the iterative methods to traverse. For computational efficiency, recent works use adversarial generative networks to model the distribution of both the universal or image-dependent perturbations directly. However, these methods generate perturbations only rely on input images. In this work, we propose a more general-purpose framework which infers target-conditioned perturbations dependent on both input image and target label. Different from previous single-target attack models, our model can conduct target-conditioned attacks by learning the relations of attack target and the semantics in image. Using extensive experiments on the datasets of MNIST and CIFAR10, we show that our method achieves superior performance with single target attack models and obtains high fooling rates with small perturbation norms.



### Roses Are Red, Violets Are Blue... but Should Vqa Expect Them To?
- **Arxiv ID**: http://arxiv.org/abs/2006.05121v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.05121v3)
- **Published**: 2020-06-09 08:50:39+00:00
- **Updated**: 2021-04-07 14:13:35+00:00
- **Authors**: Corentin Kervadec, Grigory Antipov, Moez Baccouche, Christian Wolf
- **Comment**: None
- **Journal**: None
- **Summary**: Models for Visual Question Answering (VQA) are notorious for their tendency to rely on dataset biases, as the large and unbalanced diversity of questions and concepts involved and tends to prevent models from learning to reason, leading them to perform educated guesses instead. In this paper, we claim that the standard evaluation metric, which consists in measuring the overall in-domain accuracy, is misleading. Since questions and concepts are unbalanced, this tends to favor models which exploit subtle training set statistics. Alternatively, naively introducing artificial distribution shifts between train and test splits is also not completely satisfying. First, the shifts do not reflect real-world tendencies, resulting in unsuitable models; second, since the shifts are handcrafted, trained models are specifically designed for this particular setting, and do not generalize to other configurations. We propose the GQA-OOD benchmark designed to overcome these concerns: we measure and compare accuracy over both rare and frequent question-answer pairs, and argue that the former is better suited to the evaluation of reasoning abilities, which we experimentally validate with models trained to more or less exploit biases. In a large-scale study involving 7 VQA models and 3 bias reduction techniques, we also experimentally demonstrate that these models fail to address questions involving infrequent concepts and provide recommendations for future directions of research.



### Orientation Attentive Robotic Grasp Synthesis with Augmented Grasp Map Representation
- **Arxiv ID**: http://arxiv.org/abs/2006.05123v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.05123v2)
- **Published**: 2020-06-09 08:54:54+00:00
- **Updated**: 2021-02-02 10:26:55+00:00
- **Authors**: Georgia Chalvatzaki, Nikolaos Gkanatsios, Petros Maragos, Jan Peters
- **Comment**: 7 pages, 4 figures, 5 tables
- **Journal**: None
- **Summary**: Inherent morphological characteristics in objects may offer a wide range of plausible grasping orientations that obfuscates the visual learning of robotic grasping. Existing grasp generation approaches are cursed to construct discontinuous grasp maps by aggregating annotations for drastically different orientations per grasping point. Moreover, current methods generate grasp candidates across a single direction in the robot's viewpoint, ignoring its feasibility constraints. In this paper, we propose a novel augmented grasp map representation, suitable for pixel-wise synthesis, that locally disentangles grasping orientations by partitioning the angle space into multiple bins. Furthermore, we introduce the ORientation AtteNtive Grasp synthEsis (ORANGE) framework, that jointly addresses classification into orientation bins and angle-value regression. The bin-wise orientation maps further serve as an attention mechanism for areas with higher graspability, i.e. probability of being an actual grasp point. We report new state-of-the-art 94.71% performance on Jacquard, with a simple U-Net using only depth images, outperforming even multi-modal approaches. Subsequent qualitative results with a real bi-manual robot validate ORANGE's effectiveness in generating grasps for multiple orientations, hence allowing planning grasps that are feasible.



### Over-crowdedness Alert! Forecasting the Future Crowd Distribution
- **Arxiv ID**: http://arxiv.org/abs/2006.05127v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.05127v1)
- **Published**: 2020-06-09 08:59:54+00:00
- **Updated**: 2020-06-09 08:59:54+00:00
- **Authors**: Yuzhen Niu, Weifeng Shi, Wenxi Liu, Shengfeng He, Jia Pan, Antoni B. Chan
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, vision-based crowd analysis has been studied extensively due to its practical applications in real world. In this paper, we formulate a novel crowd analysis problem, in which we aim to predict the crowd distribution in the near future given sequential frames of a crowd video without any identity annotations. Studying this research problem will benefit applications concerned with forecasting crowd dynamics. To solve this problem, we propose a global-residual two-stream recurrent network, which leverages the consecutive crowd video frames as inputs and their corresponding density maps as auxiliary information to predict the future crowd distribution. Moreover, to strengthen the capability of our network, we synthesize scene-specific crowd density maps using simulated data for pretraining. Finally, we demonstrate that our framework is able to predict the crowd distribution for different crowd scenarios and we delve into applications including predicting future crowd count, forecasting high-density region, etc.



### A Survey on Generative Adversarial Networks: Variants, Applications, and Training
- **Arxiv ID**: http://arxiv.org/abs/2006.05132v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.05132v1)
- **Published**: 2020-06-09 09:04:41+00:00
- **Updated**: 2020-06-09 09:04:41+00:00
- **Authors**: Abdul Jabbar, Xi Li, Bourahla Omar
- **Comment**: None
- **Journal**: None
- **Summary**: The Generative Models have gained considerable attention in the field of unsupervised learning via a new and practical framework called Generative Adversarial Networks (GAN) due to its outstanding data generation capability. Many models of GAN have proposed, and several practical applications emerged in various domains of computer vision and machine learning. Despite GAN's excellent success, there are still obstacles to stable training. The problems are due to Nash-equilibrium, internal covariate shift, mode collapse, vanishing gradient, and lack of proper evaluation metrics. Therefore, stable training is a crucial issue in different applications for the success of GAN. Herein, we survey several training solutions proposed by different researchers to stabilize GAN training. We survey, (I) the original GAN model and its modified classical versions, (II) detail analysis of various GAN applications in different domains, (III) detail study about the various GAN training obstacles as well as training solutions. Finally, we discuss several new issues as well as research outlines to the topic.



### Smooth Proxy-Anchor Loss for Noisy Metric Learning
- **Arxiv ID**: http://arxiv.org/abs/2006.05142v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.05142v1)
- **Published**: 2020-06-09 09:33:04+00:00
- **Updated**: 2020-06-09 09:33:04+00:00
- **Authors**: Carlos Roig, David Varas, Issey Masuda, Juan Carlos Riveiro, Elisenda Bou-Balust
- **Comment**: The 4th Workshop on Visual Understanding by Learning from Web Data
  (CVPR 2020)
- **Journal**: The 4th Workshop on Visual Understanding by Learning from Web Data
  (CVPR 2020)
- **Summary**: Many industrial applications use Metric Learning as a way to circumvent scalability issues when designing systems with a high number of classes. Because of this, this field of research is attracting a lot of interest from the academic and non-academic communities. Such industrial applications require large-scale datasets, which are usually generated with web data and, as a result, often contain a high number of noisy labels. While Metric Learning systems are sensitive to noisy labels, this is usually not tackled in the literature, that relies on manually annotated datasets.   In this work, we propose a Metric Learning method that is able to overcome the presence of noisy labels using our novel Smooth Proxy-Anchor Loss. We also present an architecture that uses the aforementioned loss with a two-phase learning procedure. First, we train a confidence module that computes sample class confidences. Second, these confidences are used to weight the influence of each sample for the training of the embeddings. This results in a system that is able to provide robust sample embeddings.   We compare the performance of the described method with current state-of-the-art Metric Learning losses (proxy-based and pair-based), when trained with a dataset containing noisy labels. The results showcase an improvement of 2.63 and 3.29 in Recall@1 with respect to MultiSimilarity and Proxy-Anchor Loss respectively, proving that our method outperforms the state-of-the-art of Metric Learning in noisy labeling conditions.



### Physically constrained short-term vehicle trajectory forecasting with naive semantic maps
- **Arxiv ID**: http://arxiv.org/abs/2006.05159v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2006.05159v1)
- **Published**: 2020-06-09 09:52:44+00:00
- **Updated**: 2020-06-09 09:52:44+00:00
- **Authors**: Albert Dulian, John C. Murray
- **Comment**: None
- **Journal**: None
- **Summary**: Urban environments manifest a high level of complexity, and therefore it is of vital importance for safety systems embedded within autonomous vehicles (AVs) to be able to accurately predict the short-term future motion of nearby agents. This problem can be further understood as generating a sequence of future coordinates for a given agent based on its past motion data e.g. position, velocity, acceleration etc, and whilst current approaches demonstrate plausible results they have a propensity to neglect a scene's physical constrains. In this paper we propose the model based on a combination of the CNN and LSTM encoder-decoder architecture that learns to extract a relevant road features from semantic maps as well as general motion of agents and uses this learned representation to predict their short-term future trajectories. We train and validate the model on the publicly available dataset that provides data from urban areas, allowing us to examine it in challenging and uncertain scenarios. We show that our model is not only capable of anticipating future motion whilst taking into consideration road boundaries, but can also effectively and precisely predict trajectories for a longer time horizon than initially trained for.



### Reconstruction and Quantification of 3D Iris Surface for Angle-Closure Glaucoma Detection in Anterior Segment OCT
- **Arxiv ID**: http://arxiv.org/abs/2006.05179v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.05179v1)
- **Published**: 2020-06-09 10:56:50+00:00
- **Updated**: 2020-06-09 10:56:50+00:00
- **Authors**: Jinkui Hao, Huazhu Fu, Yanwu Xu, Yan Hu, Fei Li, Xiulan Zhang, Jiang Liu, Yitian Zhao
- **Comment**: has been accepted by MICCAI 2020
- **Journal**: None
- **Summary**: Precise characterization and analysis of iris shape from Anterior Segment OCT (AS-OCT) are of great importance in facilitating diagnosis of angle-closure-related diseases. Existing methods focus solely on analyzing structural properties identified from the 2D slice, while accurate characterization of morphological changes of iris shape in 3D AS-OCT may be able to reveal in addition the risk of disease progression. In this paper, we propose a novel framework for reconstruction and quantification of 3D iris surface from AS-OCT imagery. We consider it to be the first work to detect angle-closure glaucoma by means of 3D representation. An iris segmentation network with wavelet refinement block (WRB) is first proposed to generate the initial shape of the iris from single AS-OCT slice. The 3D iris surface is then reconstructed using a guided optimization method with Poisson-disk sampling. Finally, a set of surface-based features are extracted, which are used in detecting of angle-closure glaucoma. Experimental results demonstrate that our method is highly effective in iris segmentation and surface reconstruction. Moreover, we show that 3D-based representation achieves better performance in angle-closure glaucoma detection than does 2D-based feature.



### Breaking the Limits of Remote Sensing by Simulation and Deep Learning for Flood and Debris Flow Mapping
- **Arxiv ID**: http://arxiv.org/abs/2006.05180v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.05180v1)
- **Published**: 2020-06-09 10:59:15+00:00
- **Updated**: 2020-06-09 10:59:15+00:00
- **Authors**: Naoto Yokoya, Kazuki Yamanoi, Wei He, Gerald Baier, Bruno Adriano, Hiroyuki Miura, Satoru Oishi
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a framework that estimates inundation depth (maximum water level) and debris-flow-induced topographic deformation from remote sensing imagery by integrating deep learning and numerical simulation. A water and debris flow simulator generates training data for various artificial disaster scenarios. We show that regression models based on Attention U-Net and LinkNet architectures trained on such synthetic data can predict the maximum water level and topographic deformation from a remote sensing-derived change detection map and a digital elevation model. The proposed framework has an inpainting capability, thus mitigating the false negatives that are inevitable in remote sensing image analysis. Our framework breaks the limits of remote sensing and enables rapid estimation of inundation depth and topographic deformation, essential information for emergency response, including rescue and relief activities. We conduct experiments with both synthetic and real data for two disaster events that caused simultaneous flooding and debris flows and demonstrate the effectiveness of our approach quantitatively and qualitatively.



### Automated Design Space Exploration for optimised Deployment of DNN on Arm Cortex-A CPUs
- **Arxiv ID**: http://arxiv.org/abs/2006.05181v2
- **DOI**: 10.1109/TCAD.2020.3046568
- **Categories**: **cs.LG**, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.05181v2)
- **Published**: 2020-06-09 11:00:06+00:00
- **Updated**: 2020-12-15 19:30:11+00:00
- **Authors**: Miguel de Prado, Andrew Mundy, Rabia Saeed, Maurizio Denna, Nuria Pazos, Luca Benini
- **Comment**: None
- **Journal**: None
- **Summary**: The spread of deep learning on embedded devices has prompted the development of numerous methods to optimise the deployment of deep neural networks (DNN). Works have mainly focused on: i) efficient DNN architectures, ii) network optimisation techniques such as pruning and quantisation, iii) optimised algorithms to speed up the execution of the most computational intensive layers and, iv) dedicated hardware to accelerate the data flow and computation. However, there is a lack of research on cross-level optimisation as the space of approaches becomes too large to test and obtain a globally optimised solution. Thus, leading to suboptimal deployment in terms of latency, accuracy, and memory. In this work, we first detail and analyse the methods to improve the deployment of DNNs across the different levels of software optimisation. Building on this knowledge, we present an automated exploration framework to ease the deployment of DNNs. The framework relies on a Reinforcement Learning search that, combined with a deep learning inference framework, automatically explores the design space and learns an optimised solution that speeds up the performance and reduces the memory on embedded CPU platforms. Thus, we present a set of results for state-of-the-art DNNs on a range of Arm Cortex-A CPU platforms achieving up to 4x improvement in performance and over 2x reduction in memory with negligible loss in accuracy with respect to the BLAS floating-point implementation.



### A Note on Deepfake Detection with Low-Resources
- **Arxiv ID**: http://arxiv.org/abs/2006.05183v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.05183v1)
- **Published**: 2020-06-09 11:07:08+00:00
- **Updated**: 2020-06-09 11:07:08+00:00
- **Authors**: Piotr Kawa, Piotr Syga
- **Comment**: None
- **Journal**: None
- **Summary**: Deepfakes are videos that include changes, quite often substituting face of a portrayed individual with a different face using neural networks. Even though the technology gained its popularity as a carrier of jokes and parodies it raises a serious threat to ones security - via biometric impersonation or besmearing. In this paper we present two methods that allow detecting Deepfakes for a user without significant computational power. In particular, we enhance MesoNet by replacing the original activation functions allowing a nearly 1% improvement as well as increasing the consistency of the results. Moreover, we introduced and verified a new activation function - Pish that at the cost of slight time overhead allows even higher consistency.   Additionally, we present a preliminary results of Deepfake detection method based on Local Feature Descriptors (LFD), that allows setting up the system even faster and without resorting to GPU computation. Our method achieved Equal Error Rate of 0.28, with both accuracy and recall exceeding 0.7.



### Stereo RGB and Deeper LIDAR Based Network for 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2006.05187v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.05187v1)
- **Published**: 2020-06-09 11:19:24+00:00
- **Updated**: 2020-06-09 11:19:24+00:00
- **Authors**: Qingdong He, Zhengning Wang, Hao Zeng, Yijun Liu, Shuaicheng Liu, Bing Zeng
- **Comment**: None
- **Journal**: None
- **Summary**: 3D object detection has become an emerging task in autonomous driving scenarios. Previous works process 3D point clouds using either projection-based or voxel-based models. However, both approaches contain some drawbacks. The voxel-based methods lack semantic information, while the projection-based methods suffer from numerous spatial information loss when projected to different views. In this paper, we propose the Stereo RGB and Deeper LIDAR (SRDL) framework which can utilize semantic and spatial information simultaneously such that the performance of network for 3D object detection can be improved naturally. Specifically, the network generates candidate boxes from stereo pairs and combines different region-wise features using a deep fusion scheme. The stereo strategy offers more information for prediction compared with prior works. Then, several local and global feature extractors are stacked in the segmentation module to capture richer deep semantic geometric features from point clouds. After aligning the interior points with fused features, the proposed network refines the prediction in a more accurate manner and encodes the whole box in a novel compact method. The decent experimental results on the challenging KITTI detection benchmark demonstrate the effectiveness of utilizing both stereo images and point clouds for 3D object detection.



### Multi-spectral Facial Landmark Detection
- **Arxiv ID**: http://arxiv.org/abs/2006.05196v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.05196v1)
- **Published**: 2020-06-09 11:43:46+00:00
- **Updated**: 2020-06-09 11:43:46+00:00
- **Authors**: Jin Keong, Xingbo Dong, Zhe Jin, Khawla Mallat, Jean-Luc Dugelay
- **Comment**: None
- **Journal**: None
- **Summary**: Thermal face image analysis is favorable for certain circumstances. For example, illumination-sensitive applications, like nighttime surveillance; and privacy-preserving demanded access control. However, the inadequate study on thermal face image analysis calls for attention in responding to the industry requirements. Detecting facial landmark points are important for many face analysis tasks, such as face recognition, 3D face reconstruction, and face expression recognition. In this paper, we propose a robust neural network enabled facial landmark detection, namely Deep Multi-Spectral Learning (DMSL). Briefly, DMSL consists of two sub-models, i.e. face boundary detection, and landmark coordinates detection. Such an architecture demonstrates the capability of detecting the facial landmarks on both visible and thermal images. Particularly, the proposed DMSL model is robust in facial landmark detection where the face is partially occluded, or facing different directions. The experiment conducted on Eurecom's visible and thermal paired database shows the superior performance of DMSL over the state-of-the-art for thermal facial landmark detection. In addition to that, we have annotated a thermal face dataset with their respective facial landmark for the purpose of experimentation.



### Neural Network Activation Quantization with Bitwise Information Bottlenecks
- **Arxiv ID**: http://arxiv.org/abs/2006.05210v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.05210v1)
- **Published**: 2020-06-09 12:10:04+00:00
- **Updated**: 2020-06-09 12:10:04+00:00
- **Authors**: Xichuan Zhou, Kui Liu, Cong Shi, Haijun Liu, Ji Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Recent researches on information bottleneck shed new light on the continuous attempts to open the black box of neural signal encoding. Inspired by the problem of lossy signal compression for wireless communication, this paper presents a Bitwise Information Bottleneck approach for quantizing and encoding neural network activations. Based on the rate-distortion theory, the Bitwise Information Bottleneck attempts to determine the most significant bits in activation representation by assigning and approximating the sparse coefficient associated with each bit. Given the constraint of a limited average code rate, the information bottleneck minimizes the rate-distortion for optimal activation quantization in a flexible layer-by-layer manner. Experiments over ImageNet and other datasets show that, by minimizing the quantization rate-distortion of each layer, the neural network with information bottlenecks achieves the state-of-the-art accuracy with low-precision activation. Meanwhile, by reducing the code rate, the proposed method can improve the memory and computational efficiency by over six times compared with the deep neural network with standard single-precision representation. Codes will be available on GitHub when the paper is accepted \url{https://github.com/BitBottleneck/PublicCode}.



### Super-resolution Variational Auto-Encoders
- **Arxiv ID**: http://arxiv.org/abs/2006.05218v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.05218v2)
- **Published**: 2020-06-09 12:32:16+00:00
- **Updated**: 2020-06-30 13:06:43+00:00
- **Authors**: Ioannis Gatopoulos, Maarten Stol, Jakub M. Tomczak
- **Comment**: 13 pages, 11 figures, 3 tables. Code available at:
  https://github.com/ioangatop/srVAE
- **Journal**: None
- **Summary**: The framework of variational autoencoders (VAEs) provides a principled method for jointly learning latent-variable models and corresponding inference models. However, the main drawback of this approach is the blurriness of the generated images. Some studies link this effect to the objective function, namely, the (negative) log-likelihood. Here, we propose to enhance VAEs by adding a random variable that is a downscaled version of the original image and still use the log-likelihood function as the learning objective. Further, by providing the downscaled image as an input to the decoder, it can be used in a manner similar to the super-resolution. We present empirically that the proposed approach performs comparably to VAEs in terms of the negative log-likelihood, but it obtains a better FID score in data synthesis.



### Rethinking Localization Map: Towards Accurate Object Perception with Self-Enhancement Maps
- **Arxiv ID**: http://arxiv.org/abs/2006.05220v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.05220v2)
- **Published**: 2020-06-09 12:35:55+00:00
- **Updated**: 2020-06-13 04:13:23+00:00
- **Authors**: Xiaolin Zhang, Yunchao Wei, Yi Yang, Fei Wu
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, remarkable progress has been made in weakly supervised object localization (WSOL) to promote object localization maps. The common practice of evaluating these maps applies an indirect and coarse way, i.e., obtaining tight bounding boxes which can cover high-activation regions and calculating intersection-over-union (IoU) scores between the predicted and ground-truth boxes. This measurement can evaluate the ability of localization maps to some extent, but we argue that the maps should be measured directly and delicately, i.e., comparing the maps with the ground-truth object masks pixel-wisely. To fulfill the direct evaluation, we annotate pixel-level object masks on the ILSVRC validation set. We propose to use IoU-Threshold curves for evaluating the real quality of localization maps. Beyond the amended evaluation metric and annotated object masks, this work also introduces a novel self-enhancement method to harvest accurate object localization maps and object boundaries with only category labels as supervision. We propose a two-stage approach to generate the localization maps by simply comparing the similarity of point-wise features between the high-activation and the rest pixels. Based on the predicted localization maps, we explore to estimate object boundaries on a very large dataset. A hard-negative suppression loss is proposed for obtaining fine boundaries. We conduct extensive experiments on the ILSVRC and CUB benchmarks. In particular, the proposed Self-Enhancement Maps achieve the state-of-the-art localization accuracy of 54.88% on ILSVRC. The code and the annotated masks are released at https://github.com/xiaomengyc/SEM.



### Deep Learning Based Single Sample Per Person Face Recognition: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2006.11395v2
- **DOI**: 10.1007/s10462-022-10240-2
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.11395v2)
- **Published**: 2020-06-09 13:24:27+00:00
- **Updated**: 2022-08-10 05:52:57+00:00
- **Authors**: Fan Liu, Delong Chen, Fei Wang, Zewen Li, Feng Xu
- **Comment**: Published in Artificial Intelligence Review
- **Journal**: None
- **Summary**: Face recognition has long been an active research area in the field of artificial intelligence, particularly since the rise of deep learning in recent years. In some practical situations, each identity has only a single sample available for training. Face recognition under this situation is referred to as single sample face recognition and poses significant challenges to the effective training of deep models. Therefore, in recent years, researchers have attempted to unleash more potential of deep learning and improve the model recognition performance in the single sample situation. While several comprehensive surveys have been conducted on traditional single sample face recognition approaches, emerging deep learning based methods are rarely involved in these reviews. Accordingly, we focus on the deep learning-based methods in this paper, classifying them into virtual sample methods and generic learning methods. In the former category, virtual images or virtual features are generated to benefit the training of the deep model. In the latter one, additional multi-sample generic sets are used. There are three types of generic learning methods: combining traditional methods and deep features, improving the loss function, and improving network structure, all of which are covered in our analysis. Moreover, we review face datasets that have been commonly used for evaluating single sample face recognition models and go on to compare the results of different types of models. Additionally, we discuss problems with existing single sample face recognition methods, including identity information preservation in virtual sample methods, domain adaption in generic learning methods. Furthermore, we regard developing unsupervised methods is a promising future direction, and point out that the semantic gap as an important issue that needs to be further considered.



### A Review of Automated Diagnosis of COVID-19 Based on Scanning Images
- **Arxiv ID**: http://arxiv.org/abs/2006.05245v3
- **DOI**: 10.1145/3449301.3449778
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.05245v3)
- **Published**: 2020-06-09 13:29:15+00:00
- **Updated**: 2021-07-24 05:18:17+00:00
- **Authors**: Delong Chen, Shunhui Ji, Fan Liu, Zewen Li, Xinyu Zhou
- **Comment**: In ICRAI 2020: 2020 6th International Conference on Robotics and
  Artificial Intelligence
- **Journal**: None
- **Summary**: The pandemic of COVID-19 has caused millions of infections, which has led to a great loss all over the world, socially and economically. Due to the false-negative rate and the time-consuming of the conventional Reverse Transcription Polymerase Chain Reaction (RT-PCR) tests, diagnosing based on X-ray images and Computed Tomography (CT) images has been widely adopted. Therefore, researchers of the computer vision area have developed many automatic diagnosing models based on machine learning or deep learning to assist the radiologists and improve the diagnosing accuracy. In this paper, we present a review of these recently emerging automatic diagnosing models. 70 models proposed from February 14, 2020, to July 21, 2020, are involved. We analyzed the models from the perspective of preprocessing, feature extraction, classification, and evaluation. Based on the limitation of existing models, we pointed out that domain adaption in transfer learning and interpretability promotion would be the possible future directions.



### What takes the brain so long: Object recognition at the level of minimal images develops for up to seconds of presentation time
- **Arxiv ID**: http://arxiv.org/abs/2006.05249v1
- **DOI**: None
- **Categories**: **q-bio.NC**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.05249v1)
- **Published**: 2020-06-09 13:33:04+00:00
- **Updated**: 2020-06-09 13:33:04+00:00
- **Authors**: Hanna Benoni, Daniel Harari, Shimon Ullman
- **Comment**: 7 pages, 2 figures, 1 table
- **Journal**: None
- **Summary**: Rich empirical evidence has shown that visual object recognition in the brain is fast and effortless, with relevant brain signals reported to start as early as 80 ms. Here we study the time trajectory of the recognition process at the level of minimal recognizable images (termed MIRC). These are images that can be recognized reliably, but in which a minute change of the image (reduction by either size or resolution) has a drastic effect on recognition. Subjects were assigned to one of nine exposure conditions: 200, 500, 1000, 2000 ms with or without masking, as well as unlimited time. The subjects were not limited in time to respond after presentation. The results show that in the masked conditions, recognition rates develop gradually over an extended period, e.g. average of 18% for 200 ms exposure and 45% for 500 ms, increasing significantly with longer exposure even above 2 secs. When presented for unlimited time (until response), MIRC recognition rates were equivalent to the rates of full-object images presented for 50 ms followed by masking. What takes the brain so long to recognize such images? We discuss why processes involving eye-movements, perceptual decision-making and pattern completion are unlikely explanations. Alternatively, we hypothesize that MIRC recognition requires an extended top-down process complementing the feed-forward phase.



### ComboNet: Combined 2D & 3D Architecture for Aorta Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2006.05325v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.05325v1)
- **Published**: 2020-06-09 15:02:55+00:00
- **Updated**: 2020-06-09 15:02:55+00:00
- **Authors**: Orhan Akal, Zhigang Peng, Gerardo Hermosillo Valadez
- **Comment**: 9 pages, 3 figures, 3 tables
- **Journal**: None
- **Summary**: 3D segmentation with deep learning if trained with full resolution is the ideal way of achieving the best accuracy. Unlike in 2D, 3D segmentation generally does not have sparse outliers, prevents leakage to surrounding soft tissues, at the very least it is generally more consistent than 2D segmentation. However, GPU memory is generally the bottleneck for such an application. Thus, most of the 3D segmentation applications handle sub-sampled input instead of full resolution, which comes with the cost of losing precision at the boundary. In order to maintain precision at the boundary and prevent sparse outliers and leakage, we designed ComboNet. ComboNet is designed in an end to end fashion with three sub-network structures. The first two are parallel: 2D UNet with full resolution and 3D UNet with four times sub-sampled input. The last stage is the concatenation of 2D and 3D outputs along with a full-resolution input image which is followed by two convolution layers either with 2D or 3D convolutions. With ComboNet we have achieved $92.1\%$ dice accuracy for aorta segmentation. With Combonet, we have observed up to $2.3\%$ improvement of dice accuracy as opposed to 2D UNet with the full-resolution input image.



### mEBAL: A Multimodal Database for Eye Blink Detection and Attention Level Estimation
- **Arxiv ID**: http://arxiv.org/abs/2006.05327v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2006.05327v2)
- **Published**: 2020-06-09 15:05:08+00:00
- **Updated**: 2020-10-22 11:11:33+00:00
- **Authors**: Roberto Daza, Aythami Morales, Julian Fierrez, Ruben Tolosana
- **Comment**: None
- **Journal**: None
- **Summary**: This work presents mEBAL, a multimodal database for eye blink detection and attention level estimation. The eye blink frequency is related to the cognitive activity and automatic detectors of eye blinks have been proposed for many tasks including attention level estimation, analysis of neuro-degenerative diseases, deception recognition, drive fatigue detection, or face anti-spoofing. However, most existing databases and algorithms in this area are limited to experiments involving only a few hundred samples and individual sensors like face cameras. The proposed mEBAL improves previous databases in terms of acquisition sensors and samples. In particular, three different sensors are simultaneously considered: Near Infrared (NIR) and RGB cameras to capture the face gestures and an Electroencephalography (EEG) band to capture the cognitive activity of the user and blinking events. Regarding the size of mEBAL, it comprises 6,000 samples and the corresponding attention level from 38 different students while conducting a number of e-learning tasks of varying difficulty. In addition to presenting mEBAL, we also include preliminary experiments on: i) eye blink detection using Convolutional Neural Networks (CNN) with the facial images, and ii) attention level estimation of the students based on their eye blink frequency.



### On Data Augmentation for GAN Training
- **Arxiv ID**: http://arxiv.org/abs/2006.05338v3
- **DOI**: 10.1109/TIP.2021.3049346
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.05338v3)
- **Published**: 2020-06-09 15:19:26+00:00
- **Updated**: 2020-12-31 08:34:10+00:00
- **Authors**: Ngoc-Trung Tran, Viet-Hung Tran, Ngoc-Bao Nguyen, Trung-Kien Nguyen, Ngai-Man Cheung
- **Comment**: Accepted in IEEE Transactions on Image Processing
- **Journal**: None
- **Summary**: Recent successes in Generative Adversarial Networks (GAN) have affirmed the importance of using more data in GAN training. Yet it is expensive to collect data in many domains such as medical applications. Data Augmentation (DA) has been applied in these applications. In this work, we first argue that the classical DA approach could mislead the generator to learn the distribution of the augmented data, which could be different from that of the original data. We then propose a principled framework, termed Data Augmentation Optimized for GAN (DAG), to enable the use of augmented data in GAN training to improve the learning of the original distribution. We provide theoretical analysis to show that using our proposed DAG aligns with the original GAN in minimizing the Jensen-Shannon (JS) divergence between the original distribution and model distribution. Importantly, the proposed DAG effectively leverages the augmented data to improve the learning of discriminator and generator. We conduct experiments to apply DAG to different GAN models: unconditional GAN, conditional GAN, self-supervised GAN and CycleGAN using datasets of natural images and medical images. The results show that DAG achieves consistent and considerable improvements across these models. Furthermore, when DAG is used in some GAN models, the system establishes state-of-the-art Frechet Inception Distance (FID) scores. Our code is available.



### MeshWalker: Deep Mesh Understanding by Random Walks
- **Arxiv ID**: http://arxiv.org/abs/2006.05353v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CG, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.05353v3)
- **Published**: 2020-06-09 15:35:41+00:00
- **Updated**: 2020-12-10 15:39:51+00:00
- **Authors**: Alon Lahav, Ayellet Tal
- **Comment**: None
- **Journal**: None
- **Summary**: Most attempts to represent 3D shapes for deep learning have focused on volumetric grids, multi-view images and point clouds. In this paper we look at the most popular representation of 3D shapes in computer graphics - a triangular mesh - and ask how it can be utilized within deep learning. The few attempts to answer this question propose to adapt convolutions & pooling to suit Convolutional Neural Networks (CNNs). This paper proposes a very different approach, termed MeshWalker, to learn the shape directly from a given mesh. The key idea is to represent the mesh by random walks along the surface, which "explore" the mesh's geometry and topology. Each walk is organized as a list of vertices, which in some manner imposes regularity on the mesh. The walk is fed into a Recurrent Neural Network (RNN) that "remembers" the history of the walk. We show that our approach achieves state-of-the-art results for two fundamental shape analysis tasks: shape classification and semantic segmentation. Furthermore, even a very small number of examples suffices for learning. This is highly important, since large datasets of meshes are difficult to acquire.



### A Hybrid Framework for Matching Printing Design Files to Product Photos
- **Arxiv ID**: http://arxiv.org/abs/2006.05355v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.05355v1)
- **Published**: 2020-06-09 15:39:14+00:00
- **Updated**: 2020-06-09 15:39:14+00:00
- **Authors**: Alper Kaplan, Erdem Akagunduz
- **Comment**: None
- **Journal**: published in Balkan Journal of Electrical and Computer
  Engineering, Volume 8 - Issue 2 - Apr 30, 2020
- **Summary**: We propose a real-time image matching framework, which is hybrid in the sense that it uses both hand-crafted features and deep features obtained from a well-tuned deep convolutional network. The matching problem, which we concentrate on, is specific to a certain application, that is, printing design to product photo matching. Printing designs are any kind of template image files, created using a design tool, thus are perfect image signals. However, photographs of a printed product suffer many unwanted effects, such as uncontrolled shooting angle, uncontrolled illumination, occlusions, printing deficiencies in color, camera noise, optic blur, et cetera. For this purpose, we create an image set that includes printing design and corresponding product photo pairs with collaboration of an actual printing facility. Using this image set, we benchmark various hand-crafted and deep features for matching performance and propose a framework in which deep learning is utilized with highest contribution, but without disabling real-time operation using an ordinary desktop computer.



### Open-Narrow-Synechiae Anterior Chamber Angle Classification in AS-OCT Sequences
- **Arxiv ID**: http://arxiv.org/abs/2006.05367v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.05367v1)
- **Published**: 2020-06-09 16:00:00+00:00
- **Updated**: 2020-06-09 16:00:00+00:00
- **Authors**: Huaying Hao, Huazhu Fu, Yanwu Xu, Jianlong Yang, Fei Li, Xiulan Zhang, Jiang Liu, Yitian Zhao
- **Comment**: Accepted to MICCAI 2020
- **Journal**: None
- **Summary**: Anterior chamber angle (ACA) classification is a key step in the diagnosis of angle-closure glaucoma in Anterior Segment Optical Coherence Tomography (AS-OCT). Existing automated analysis methods focus on a binary classification system (i.e., open angle or angle-closure) in a 2D AS-OCT slice. However, clinical diagnosis requires a more discriminating ACA three-class system (i.e., open, narrow, or synechiae angles) for the benefit of clinicians who seek better to understand the progression of the spectrum of angle-closure glaucoma types. To address this, we propose a novel sequence multi-scale aggregation deep network (SMA-Net) for open-narrow-synechiae ACA classification based on an AS-OCT sequence. In our method, a Multi-Scale Discriminative Aggregation (MSDA) block is utilized to learn the multi-scale representations at slice level, while a ConvLSTM is introduced to study the temporal dynamics of these representations at sequence level. Finally, a multi-level loss function is used to combine the slice-based and sequence-based losses. The proposed method is evaluated across two AS-OCT datasets. The experimental results show that the proposed method outperforms existing state-of-the-art methods in applicability, effectiveness, and accuracy. We believe this work to be the first attempt to classify ACAs into open, narrow, or synechia types grading using AS-OCT sequences.



### A t-distribution based operator for enhancing out of distribution robustness of neural network classifiers
- **Arxiv ID**: http://arxiv.org/abs/2006.05389v3
- **DOI**: 10.1109/LSP.2020.3001843
- **Categories**: **eess.SP**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.05389v3)
- **Published**: 2020-06-09 16:39:07+00:00
- **Updated**: 2020-10-09 12:15:02+00:00
- **Authors**: Niccolò Antonello, Philip N. Garner
- **Comment**: 5 pages, 5 figures, to be published in IEEE Signal Processing
  Letters, reproducible code https://github.com/idiap/tsoftmax
- **Journal**: None
- **Summary**: Neural Network (NN) classifiers can assign extreme probabilities to samples that have not appeared during training (out-of-distribution samples) resulting in erroneous and unreliable predictions. One of the causes for this unwanted behaviour lies in the use of the standard softmax operator which pushes the posterior probabilities to be either zero or unity hence failing to model uncertainty. The statistical derivation of the softmax operator relies on the assumption that the distributions of the latent variables for a given class are Gaussian with known variance. However, it is possible to use different assumptions in the same derivation and attain from other families of distributions as well. This allows derivation of novel operators with more favourable properties. Here, a novel operator is proposed that is derived using $t$-distributions which are capable of providing a better description of uncertainty. It is shown that classifiers that adopt this novel operator can be more robust to out of distribution samples, often outperforming NNs that use the standard softmax operator. These enhancements can be reached with minimal changes to the NN architecture.



### Deep Visual Reasoning: Learning to Predict Action Sequences for Task and Motion Planning from an Initial Scene Image
- **Arxiv ID**: http://arxiv.org/abs/2006.05398v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.05398v1)
- **Published**: 2020-06-09 16:52:02+00:00
- **Updated**: 2020-06-09 16:52:02+00:00
- **Authors**: Danny Driess, Jung-Su Ha, Marc Toussaint
- **Comment**: Robotics: Science and Systems (R:SS) 2020
- **Journal**: None
- **Summary**: In this paper, we propose a deep convolutional recurrent neural network that predicts action sequences for task and motion planning (TAMP) from an initial scene image. Typical TAMP problems are formalized by combining reasoning on a symbolic, discrete level (e.g. first-order logic) with continuous motion planning such as nonlinear trajectory optimization. Due to the great combinatorial complexity of possible discrete action sequences, a large number of optimization/motion planning problems have to be solved to find a solution, which limits the scalability of these approaches.   To circumvent this combinatorial complexity, we develop a neural network which, based on an initial image of the scene, directly predicts promising discrete action sequences such that ideally only one motion planning problem has to be solved to find a solution to the overall TAMP problem. A key aspect is that our method generalizes to scenes with many and varying number of objects, although being trained on only two objects at a time. This is possible by encoding the objects of the scene in images as input to the neural network, instead of a fixed feature vector. Results show runtime improvements of several magnitudes. Video: https://youtu.be/i8yyEbbvoEk



### SALD: Sign Agnostic Learning with Derivatives
- **Arxiv ID**: http://arxiv.org/abs/2006.05400v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.05400v2)
- **Published**: 2020-06-09 16:54:57+00:00
- **Updated**: 2020-10-03 17:24:48+00:00
- **Authors**: Matan Atzmon, Yaron Lipman
- **Comment**: None
- **Journal**: None
- **Summary**: Learning 3D geometry directly from raw data, such as point clouds, triangle soups, or unoriented meshes is still a challenging task that feeds many downstream computer vision and graphics applications.   In this paper, we introduce SALD: a method for learning implicit neural representations of shapes directly from raw data. We generalize sign agnostic learning (SAL) to include derivatives: given an unsigned distance function to the input raw data, we advocate a novel sign agnostic regression loss, incorporating both pointwise values and gradients of the unsigned distance function. Optimizing this loss leads to a signed implicit function solution, the zero level set of which is a high quality and valid manifold approximation to the input 3D data. The motivation behind SALD is that incorporating derivatives in a regression loss leads to a lower sample complexity, and consequently better fitting. In addition, we prove that SAL enjoys a minimal length property in 2D, favoring minimal length solutions. More importantly, we are able to show that this property still holds for SALD, i.e., with derivatives included.   We demonstrate the efficacy of SALD for shape space learning on two challenging datasets: ShapeNet that contains inconsistent orientation and non-manifold meshes, and D-Faust that contains raw 3D scans (triangle soups). On both these datasets, we present state-of-the-art results.



### D-VPnet: A Network for Real-time Dominant Vanishing Point Detection in Natural Scenes
- **Arxiv ID**: http://arxiv.org/abs/2006.05407v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, I.4.7
- **Links**: [PDF](http://arxiv.org/pdf/2006.05407v1)
- **Published**: 2020-06-09 17:12:27+00:00
- **Updated**: 2020-06-09 17:12:27+00:00
- **Authors**: Yin-Bo Liu, Ming Zeng, Qing-Hao Meng
- **Comment**: 18 pages, 6 figures, under review
- **Journal**: None
- **Summary**: As an important part of linear perspective, vanishing points (VPs) provide useful clues for mapping objects from 2D photos to 3D space. Existing methods are mainly focused on extracting structural features such as lines or contours and then clustering these features to detect VPs. However, these techniques suffer from ambiguous information due to the large number of line segments and contours detected in outdoor environments. In this paper, we present a new convolutional neural network (CNN) to detect dominant VPs in natural scenes, i.e., the Dominant Vanishing Point detection Network (D-VPnet). The key component of our method is the feature line-segment proposal unit (FLPU), which can be directly utilized to predict the location of the dominant VP. Moreover, the model also uses the two main parallel lines as an assistant to determine the position of the dominant VP. The proposed method was tested using a public dataset and a Parallel Line based Vanishing Point (PLVP) dataset. The experimental results suggest that the detection accuracy of our approach outperforms those of state-of-the-art methods under various conditions in real-time, achieving rates of 115fps.



### Neuroevolution in Deep Neural Networks: Current Trends and Future Challenges
- **Arxiv ID**: http://arxiv.org/abs/2006.05415v1
- **DOI**: 10.1109/TAI.2021.3067574
- **Categories**: **cs.NE**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.05415v1)
- **Published**: 2020-06-09 17:28:25+00:00
- **Updated**: 2020-06-09 17:28:25+00:00
- **Authors**: Edgar Galván, Peter Mooney
- **Comment**: 20 pages (double column), 2 figures, 3 tables, 157 references
- **Journal**: None
- **Summary**: A variety of methods have been applied to the architectural configuration and learning or training of artificial deep neural networks (DNN). These methods play a crucial role in the success or failure of the DNN for most problems and applications. Evolutionary Algorithms (EAs) are gaining momentum as a computationally feasible method for the automated optimisation and training of DNNs. Neuroevolution is a term which describes these processes of automated configuration and training of DNNs using EAs. While many works exist in the literature, no comprehensive surveys currently exist focusing exclusively on the strengths and limitations of using neuroevolution approaches in DNNs. Prolonged absence of such surveys can lead to a disjointed and fragmented field preventing DNNs researchers potentially adopting neuroevolutionary methods in their own research, resulting in lost opportunities for improving performance and wider application within real-world deep learning problems. This paper presents a comprehensive survey, discussion and evaluation of the state-of-the-art works on using EAs for architectural configuration and training of DNNs. Based on this survey, the paper highlights the most pertinent current issues and challenges in neuroevolution and identifies multiple promising future research directions.



### Dialog Policy Learning for Joint Clarification and Active Learning Queries
- **Arxiv ID**: http://arxiv.org/abs/2006.05456v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.05456v3)
- **Published**: 2020-06-09 18:53:21+00:00
- **Updated**: 2020-12-14 03:31:36+00:00
- **Authors**: Aishwarya Padmakumar, Raymond J. Mooney
- **Comment**: AAAI 2020 Camera Ready
- **Journal**: Proceedings of 2021 AAAI Conference on Artificial Intelligence
  (AAAI-2021)
- **Summary**: Intelligent systems need to be able to recover from mistakes, resolve uncertainty, and adapt to novel concepts not seen during training. Dialog interaction can enable this by the use of clarifications for correction and resolving uncertainty, and active learning queries to learn new concepts encountered during operation. Prior work on dialog systems has either focused on exclusively learning how to perform clarification/ information seeking, or to perform active learning. In this work, we train a hierarchical dialog policy to jointly perform both clarification and active learning in the context of an interactive language-based image retrieval task motivated by an online shopping application, and demonstrate that jointly learning dialog policies for clarification and active learning is more effective than the use of static dialog policies for one or both of these functions.



### Tamil Vowel Recognition With Augmented MNIST-like Data Set
- **Arxiv ID**: http://arxiv.org/abs/2006.08367v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2006.08367v2)
- **Published**: 2020-06-09 19:17:30+00:00
- **Updated**: 2020-06-16 19:20:09+00:00
- **Authors**: Muthiah Annamalai
- **Comment**: 8 pages, 3 figures, 4 tables
- **Journal**: None
- **Summary**: We report generation of a MNIST [4] compatible data set [1] for Tamil vowels to enable building a classification DNN or other such ML/AI deep learning [2] models for Tamil OCR/Handwriting applications. We report the capability of the 60,000 grayscale, 28x28 pixel dataset to build a 92% accuracy (training) and 82% cross-validation 4-layer CNN, with 100,000+ parameters, in TensorFlow. We also report a top-1 classification accuracy of 70% and top-2 classification accuracy of 92% on handwritten vowels showing, for the same network.



### Pruning neural networks without any data by iteratively conserving synaptic flow
- **Arxiv ID**: http://arxiv.org/abs/2006.05467v3
- **DOI**: None
- **Categories**: **cs.LG**, cond-mat.dis-nn, cs.CV, q-bio.NC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.05467v3)
- **Published**: 2020-06-09 19:21:57+00:00
- **Updated**: 2020-11-19 03:54:34+00:00
- **Authors**: Hidenori Tanaka, Daniel Kunin, Daniel L. K. Yamins, Surya Ganguli
- **Comment**: NeurIPS 2020, 18 pages, 10 figures
- **Journal**: Advances in Neural Information Processing Systems 2020
- **Summary**: Pruning the parameters of deep neural networks has generated intense interest due to potential savings in time, memory and energy both during training and at test time. Recent works have identified, through an expensive sequence of training and pruning cycles, the existence of winning lottery tickets or sparse trainable subnetworks at initialization. This raises a foundational question: can we identify highly sparse trainable subnetworks at initialization, without ever training, or indeed without ever looking at the data? We provide an affirmative answer to this question through theory driven algorithm design. We first mathematically formulate and experimentally verify a conservation law that explains why existing gradient-based pruning algorithms at initialization suffer from layer-collapse, the premature pruning of an entire layer rendering a network untrainable. This theory also elucidates how layer-collapse can be entirely avoided, motivating a novel pruning algorithm Iterative Synaptic Flow Pruning (SynFlow). This algorithm can be interpreted as preserving the total flow of synaptic strengths through the network at initialization subject to a sparsity constraint. Notably, this algorithm makes no reference to the training data and consistently competes with or outperforms existing state-of-the-art pruning algorithms at initialization over a range of models (VGG and ResNet), datasets (CIFAR-10/100 and Tiny ImageNet), and sparsity constraints (up to 99.99 percent). Thus our data-agnostic pruning algorithm challenges the existing paradigm that, at initialization, data must be used to quantify which synapses are important.



### Standardised convolutional filtering for radiomics
- **Arxiv ID**: http://arxiv.org/abs/2006.05470v8
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, 68U10 (Primary) 68T45 (Secondary), I.4.7; J.3
- **Links**: [PDF](http://arxiv.org/pdf/2006.05470v8)
- **Published**: 2020-06-09 19:29:49+00:00
- **Updated**: 2023-05-23 07:29:04+00:00
- **Authors**: Adrien Depeursinge, Vincent Andrearczyk, Philip Whybra, Joost van Griethuysen, Henning Müller, Roger Schaer, Martin Vallières, Alex Zwanenburg
- **Comment**: 87 pages. For additional information see https://theibsi.github.io/
- **Journal**: None
- **Summary**: The Image Biomarker Standardisation Initiative (IBSI) aims to improve reproducibility of radiomics studies by standardising the computational process of extracting image biomarkers (features) from images. We have previously established reference values for 169 commonly used features, created a standard radiomics image processing scheme, and developed reporting guidelines for radiomic studies. However, several aspects are not standardised.   Here we present a complete version of a reference manual on the use of convolutional filters in radiomics and quantitative image analysis. Filters, such as wavelets or Laplacian of Gaussian filters, play an important part in emphasising specific image characteristics such as edges and blobs. Features derived from filter response maps were found to be poorly reproducible. This reference manual provides definitions for convolutional filters, parameters that should be reported, reference feature values, and tests to verify software compliance with the reference standard.



### DcardNet: Diabetic Retinopathy Classification at Multiple Levels Based on Structural and Angiographic Optical Coherence Tomography
- **Arxiv ID**: http://arxiv.org/abs/2006.05480v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.05480v2)
- **Published**: 2020-06-09 19:44:10+00:00
- **Updated**: 2020-09-24 22:03:58+00:00
- **Authors**: Pengxiao Zang, Liqin Gao, Tristan T. Hormel, Jie Wang, Qisheng You, Thomas S. Hwang, Yali Jia
- **Comment**: Accepted for publication by IEEE Transactions on Biomedical
  Engineering
- **Journal**: None
- **Summary**: Objective: Optical coherence tomography (OCT) and its angiography (OCTA) have several advantages for the early detection and diagnosis of diabetic retinopathy (DR). However, automated, complete DR classification frameworks based on both OCT and OCTA data have not been proposed. In this study, a convolutional neural network (CNN) based method is proposed to fulfill a DR classification framework using en face OCT and OCTA. Methods: A densely and continuously connected neural network with adaptive rate dropout (DcardNet) is designed for the DR classification. In addition, adaptive label smoothing was proposed and used to suppress overfitting. Three separate classification levels are generated for each case based on the International Clinical Diabetic Retinopathy scale. At the highest level the network classifies scans as referable or non-referable for DR. The second level classifies the eye as non-DR, non-proliferative DR (NPDR), or proliferative DR (PDR). The last level classifies the case as no DR, mild and moderate NPDR, severe NPDR, and PDR. Results: We used 10-fold cross-validation with 10% of the data to assess the networks performance. The overall classification accuracies of the three levels were 95.7%, 85.0%, and 71.0% respectively. Conclusion/Significance: A reliable, sensitive and specific automated classification framework for referral to an ophthalmologist can be a key technology for reducing vision loss related to DR.



### Off-the-shelf sensor vs. experimental radar -- How much resolution is necessary in automotive radar classification?
- **Arxiv ID**: http://arxiv.org/abs/2006.05485v1
- **DOI**: 10.23919/FUSION45008.2020.9190338
- **Categories**: **cs.CV**, cs.LG, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2006.05485v1)
- **Published**: 2020-06-09 19:51:34+00:00
- **Updated**: 2020-06-09 19:51:34+00:00
- **Authors**: Nicolas Scheiner, Ole Schumann, Florian Kraus, Nils Appenrodt, Jürgen Dickmann, Bernhard Sick
- **Comment**: Accepted @ 23rd International Conference on Information Fusion
  (FUSION)
- **Journal**: 2020 IEEE 23rd International Conference on Information Fusion
  (FUSION)
- **Summary**: Radar-based road user detection is an important topic in the context of autonomous driving applications. The resolution of conventional automotive radar sensors results in a sparse data representation which is tough to refine during subsequent signal processing. On the other hand, a new sensor generation is waiting in the wings for its application in this challenging field. In this article, two sensors of different radar generations are evaluated against each other. The evaluation criterion is the performance on moving road user object detection and classification tasks. To this end, two data sets originating from an off-the-shelf radar and a high resolution next generation radar are compared. Special attention is given on how the two data sets are assembled in order to make them comparable. The utilized object detector consists of a clustering algorithm, a feature extraction module, and a recurrent neural network ensemble for classification. For the assessment, all components are evaluated both individually and, for the first time, as a whole. This allows for indicating where overall performance improvements have their origin in the pipeline. Furthermore, the generalization capabilities of both data sets are evaluated and important comparison metrics for automotive radar object detection are discussed. Results show clear benefits of the next generation radar. Interestingly, those benefits do not actually occur due to better performance at the classification stage, but rather because of the vast improvements at the clustering stage.



### Can artificial intelligence (AI) be used to accurately detect tuberculosis (TB) from chest X-rays? An evaluation of five AI products for TB screening and triaging in a high TB burden setting
- **Arxiv ID**: http://arxiv.org/abs/2006.05509v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, q-bio.QM, 92B20, I.2.1
- **Links**: [PDF](http://arxiv.org/pdf/2006.05509v3)
- **Published**: 2020-06-09 21:06:46+00:00
- **Updated**: 2021-05-28 15:28:07+00:00
- **Authors**: Zhi Zhen Qin, Shahriar Ahmed, Mohammad Shahnewaz Sarker, Kishor Paul, Ahammad Shafiq Sikder Adel, Tasneem Naheyan, Rachael Barrett, Sayera Banu, Jacob Creswell
- **Comment**: 43 pages, 3 Tables 3 Figures
- **Journal**: None
- **Summary**: Artificial intelligence (AI) products can be trained to recognize tuberculosis (TB)-related abnormalities on chest radiographs. Various AI products are available commercially, yet there is lack of evidence on how their performance compared with each other and with radiologists. We evaluated five AI software products for screening and triaging TB using a large dataset that had not been used to train any commercial AI products. Individuals (>=15 years old) presenting to three TB screening centers in Dhaka, Bangladesh, were recruited consecutively. All CXR were read independently by a group of three Bangladeshi registered radiologists and five commercial AI products: CAD4TB (v7), InferReadDR (v2), Lunit INSIGHT CXR (v4.9.0), JF CXR-1 (v2), and qXR (v3). All five AI products significantly outperformed the Bangladeshi radiologists. The areas under the receiver operating characteristic curve are qXR: 90.81% (95% CI:90.33-91.29%), CAD4TB: 90.34% (95% CI:89.81-90.87), Lunit INSIGHT CXR: 88.61% (95% CI:88.03%-89.20%), InferReadDR: 84.90% (95% CI: 84.27-85.54%) and JF CXR-1: 84.89% (95% CI:84.26-85.53%). Only qXR met the TPP with 74.3% specificity at 90% sensitivity. Five AI algorithms can reduce the number of Xpert tests required by 50%, while maintaining a sensitivity above 90%. All AI algorithms performed worse among the older age and people with prior TB history. AI products can be highly accurate and useful screening and triage tools for TB detection in high burden regions and outperform human readers.



### A Deep Learning-Based Method for Automatic Segmentation of Proximal Femur from Quantitative Computed Tomography Images
- **Arxiv ID**: http://arxiv.org/abs/2006.05513v3
- **DOI**: None
- **Categories**: **physics.med-ph**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.05513v3)
- **Published**: 2020-06-09 21:16:47+00:00
- **Updated**: 2020-07-01 13:04:52+00:00
- **Authors**: Chen Zhao, Joyce H. Keyak, Jinshan Tang, Tadashi S. Kaneko, Sundeep Khosla, Shreyasee Amin, Elizabeth J. Atkinson, Lan-Juan Zhao, Michael J. Serou, Chaoyang Zhang, Hui Shen, Hong-Wen Deng, Weihua Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: Purpose: Proximal femur image analyses based on quantitative computed tomography (QCT) provide a method to quantify the bone density and evaluate osteoporosis and risk of fracture. We aim to develop a deep-learning-based method for automatic proximal femur segmentation. Methods and Materials: We developed a 3D image segmentation method based on V-Net, an end-to-end fully convolutional neural network (CNN), to extract the proximal femur QCT images automatically. The proposed V-net methodology adopts a compound loss function, which includes a Dice loss and a L2 regularizer. We performed experiments to evaluate the effectiveness of the proposed segmentation method. In the experiments, a QCT dataset which included 397 QCT subjects was used. For the QCT image of each subject, the ground truth for the proximal femur was delineated by a well-trained scientist. During the experiments for the entire cohort then for male and female subjects separately, 90% of the subjects were used in 10-fold cross-validation for training and internal validation, and to select the optimal parameters of the proposed models; the rest of the subjects were used to evaluate the performance of models. Results: Visual comparison demonstrated high agreement between the model prediction and ground truth contours of the proximal femur portion of the QCT images. In the entire cohort, the proposed model achieved a Dice score of 0.9815, a sensitivity of 0.9852 and a specificity of 0.9992. In addition, an R2 score of 0.9956 (p<0.001) was obtained when comparing the volumes measured by our model prediction with the ground truth. Conclusion: This method shows a great promise for clinical application to QCT and QCT-based finite element analysis of the proximal femur for evaluating osteoporosis and hip fracture risk.



### MVLidarNet: Real-Time Multi-Class Scene Understanding for Autonomous Driving Using Multiple Views
- **Arxiv ID**: http://arxiv.org/abs/2006.05518v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO, I.2.6; I.4.6; I.5.1
- **Links**: [PDF](http://arxiv.org/pdf/2006.05518v2)
- **Published**: 2020-06-09 21:28:17+00:00
- **Updated**: 2020-08-18 03:09:18+00:00
- **Authors**: Ke Chen, Ryan Oldja, Nikolai Smolyanskiy, Stan Birchfield, Alexander Popov, David Wehr, Ibrahim Eden, Joachim Pehserl
- **Comment**: IROS 2020 conference (submitted March 1st, 2020). For accompanying
  video, see https://youtu.be/2ck5_sToayc
- **Journal**: None
- **Summary**: Autonomous driving requires the inference of actionable information such as detecting and classifying objects, and determining the drivable space. To this end, we present Multi-View LidarNet (MVLidarNet), a two-stage deep neural network for multi-class object detection and drivable space segmentation using multiple views of a single LiDAR point cloud. The first stage processes the point cloud projected onto a perspective view in order to semantically segment the scene. The second stage then processes the point cloud (along with semantic labels from the first stage) projected onto a bird's eye view, to detect and classify objects. Both stages use an encoder-decoder architecture. We show that our multi-view, multi-stage, multi-class approach is able to detect and classify objects while simultaneously determining the drivable space using a single LiDAR scan as input, in challenging scenes with more than one hundred vehicles and pedestrians at a time. The system operates efficiently at 150 fps on an embedded GPU designed for a self-driving car, including a postprocessing step to maintain identities over time. We show results on both KITTI and a much larger internal dataset, thus demonstrating the method's ability to scale by an order of magnitude.



### Bombus Species Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2006.11374v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.11374v1)
- **Published**: 2020-06-09 21:28:32+00:00
- **Updated**: 2020-06-09 21:28:32+00:00
- **Authors**: Venkat Margapuri, George Lavezzi, Robert Stewart, Dan Wagner
- **Comment**: None
- **Journal**: None
- **Summary**: Entomologists, ecologists and others struggle to rapidly and accurately identify the species of bumble bees they encounter in their field work and research. The current process requires the bees to be mounted, then physically shipped to a taxonomic expert for proper categorization. We investigated whether an image classification system derived from transfer learning can do this task. We used Google Inception, Oxford VGG16 and VGG19 and Microsoft ResNet 50. We found Inception and VGG classifiers were able to make some progress at identifying bumble bee species from the available data, whereas ResNet was not. Individual classifiers achieved accuracies of up to 23% for single species identification and 44% top-3 labels, where a composite model performed better, 27% and 50%. We feel the performance was most hampered by our limited data set of 5,000-plus labeled images of 29 species, with individual species represented by 59 -315 images.



### Supervised Learning of Sparsity-Promoting Regularizers for Denoising
- **Arxiv ID**: http://arxiv.org/abs/2006.05521v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.05521v1)
- **Published**: 2020-06-09 21:38:05+00:00
- **Updated**: 2020-06-09 21:38:05+00:00
- **Authors**: Michael T. McCann, Saiprasad Ravishankar
- **Comment**: None
- **Journal**: None
- **Summary**: We present a method for supervised learning of sparsity-promoting regularizers for image denoising. Sparsity-promoting regularization is a key ingredient in solving modern image reconstruction problems; however, the operators underlying these regularizers are usually either designed by hand or learned from data in an unsupervised way. The recent success of supervised learning (mainly convolutional neural networks) in solving image reconstruction problems suggests that it could be a fruitful approach to designing regularizers. As a first experiment in this direction, we propose to denoise images using a variational formulation with a parametric, sparsity-promoting regularizer, where the parameters of the regularizer are learned to minimize the mean squared error of reconstructions on a training set of (ground truth image, measurement) pairs. Training involves solving a challenging bilievel optimization problem; we derive an expression for the gradient of the training loss using Karush-Kuhn-Tucker conditions and provide an accompanying gradient descent algorithm to minimize it. Our experiments on a simple synthetic, denoising problem show that the proposed method can learn an operator that outperforms well-known regularizers (total variation, DCT-sparsity, and unsupervised dictionary learning) and collaborative filtering. While the approach we present is specific to denoising, we believe that it can be adapted to the whole class of inverse problems with linear measurement models, giving it applicability to a wide range of image reconstruction problems.



### Dual-stream Maximum Self-attention Multi-instance Learning
- **Arxiv ID**: http://arxiv.org/abs/2006.05538v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, I.2.0
- **Links**: [PDF](http://arxiv.org/pdf/2006.05538v1)
- **Published**: 2020-06-09 22:44:58+00:00
- **Updated**: 2020-06-09 22:44:58+00:00
- **Authors**: Bin Li, Kevin W. Eliceiri
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-instance learning (MIL) is a form of weakly supervised learning where a single class label is assigned to a bag of instances while the instance-level labels are not available. Training classifiers to accurately determine the bag label and instance labels is a challenging but critical task in many practical scenarios, such as computational histopathology. Recently, MIL models fully parameterized by neural networks have become popular due to the high flexibility and superior performance. Most of these models rely on attention mechanisms that assign attention scores across the instance embeddings in a bag and produce the bag embedding using an aggregation operator. In this paper, we proposed a dual-stream maximum self-attention MIL model (DSMIL) parameterized by neural networks. The first stream deploys a simple MIL max-pooling while the top-activated instance embedding is determined and used to obtain self-attention scores across instance embeddings in the second stream. Different from most of the previous methods, the proposed model jointly learns an instance classifier and a bag classifier based on the same instance embeddings. The experiments results show that our method achieves superior performance compared to the best MIL methods and demonstrates state-of-the-art performance on benchmark MIL datasets.



### Resolution-Enhanced MRI-Guided Navigation of Spinal Cellular Injection Robot
- **Arxiv ID**: http://arxiv.org/abs/2006.05544v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.05544v1)
- **Published**: 2020-06-09 23:07:55+00:00
- **Updated**: 2020-06-09 23:07:55+00:00
- **Authors**: Daniel Enrique Martinez, Waiman Meinhold, John Oshinski, Ai-Ping Hu, Jun Ueda
- **Comment**: 6 pages, 10 figures, 3 tables, conference
- **Journal**: None
- **Summary**: This paper presents a method of navigating a surgical robot beyond the resolution of magnetic resonance imaging (MRI) by using a resolution enhancement technique enabled by high-precision piezoelectric actuation. The surgical robot was specifically designed for injecting stem cells into the spinal cord. This particular therapy can be performed in a shorter time by using a MRI-compatible robotic platform than by using a manual needle positioning platform. Imaging resolution of fiducial markers attached to the needle guide tubing was enhanced by reconstructing a high-resolution image from multiple images with sub-pixel movements of the robot. The parallel-plane direct-drive needle positioning mechanism positioned the needle guide with a high spatial precision that is two orders of magnitude higher than typical MRI resolution up to 1 mm. Reconstructed resolution enhanced images were used to navigate the robot precisely that would not have been possible by using standard MRI. Experiments were conducted to verify the effectiveness of the proposed enhanced-resolution image-guided intervention.



### 3D Point Cloud Feature Explanations Using Gradient-Based Methods
- **Arxiv ID**: http://arxiv.org/abs/2006.05548v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.05548v1)
- **Published**: 2020-06-09 23:17:24+00:00
- **Updated**: 2020-06-09 23:17:24+00:00
- **Authors**: Ananya Gupta, Simon Watson, Hujun Yin
- **Comment**: Accepted for IJCNN 2020
- **Journal**: None
- **Summary**: Explainability is an important factor to drive user trust in the use of neural networks for tasks with material impact. However, most of the work done in this area focuses on image analysis and does not take into account 3D data. We extend the saliency methods that have been shown to work on image data to deal with 3D data. We analyse the features in point clouds and voxel spaces and show that edges and corners in 3D data are deemed as important features while planar surfaces are deemed less important. The approach is model-agnostic and can provide useful information about learnt features. Driven by the insight that 3D data is inherently sparse, we visualise the features learnt by a voxel-based classification network and show that these features are also sparse and can be pruned relatively easily, leading to more efficient neural networks. Our results show that the Voxception-ResNet model can be pruned down to 5\% of its parameters with negligible loss in accuracy.



### Tree Annotations in LiDAR Data Using Point Densities and Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2006.05560v1
- **DOI**: 10.1109/TGRS.2019.2942201
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.05560v1)
- **Published**: 2020-06-09 23:50:40+00:00
- **Updated**: 2020-06-09 23:50:40+00:00
- **Authors**: Ananya Gupta, Jonathan Byrne, David Moloney, Simon Watson, Hujun Yin
- **Comment**: None
- **Journal**: IEEE Transactions on Geoscience and Remote Sensing, vol. 58, no.
  2, pp. 971-981, Feb. 2020
- **Summary**: LiDAR provides highly accurate 3D point clouds. However, data needs to be manually labelled in order to provide subsequent useful information. Manual annotation of such data is time consuming, tedious and error prone, and hence in this paper we present three automatic methods for annotating trees in LiDAR data. The first method requires high density point clouds and uses certain LiDAR data attributes for the purpose of tree identification, achieving almost 90% accuracy. The second method uses a voxel-based 3D Convolutional Neural Network on low density LiDAR datasets and is able to identify most large trees accurately but struggles with smaller ones due to the voxelisation process. The third method is a scaled version of the PointNet++ method and works directly on outdoor point clouds and achieves an F_score of 82.1% on the ISPRS benchmark dataset, comparable to the state-of-the-art methods but with increased efficiency.



