# Arxiv Papers in cs.CV on 2020-06-08
### Multi-step Estimation for Gradient-based Meta-learning
- **Arxiv ID**: http://arxiv.org/abs/2006.04298v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.04298v1)
- **Published**: 2020-06-08 00:37:01+00:00
- **Updated**: 2020-06-08 00:37:01+00:00
- **Authors**: Jin-Hwa Kim, Junyoung Park, Yongseok Choi
- **Comment**: 17 pages, 5 figures
- **Journal**: None
- **Summary**: Gradient-based meta-learning approaches have been successful in few-shot learning, transfer learning, and a wide range of other domains. Despite its efficacy and simplicity, the burden of calculating the Hessian matrix with large memory footprints is the critical challenge in large-scale applications. To tackle this issue, we propose a simple yet straightforward method to reduce the cost by reusing the same gradient in a window of inner steps. We describe the dynamics of the multi-step estimation in the Lagrangian formalism and discuss how to reduce evaluating second-order derivatives estimating the dynamics. To validate our method, we experiment on meta-transfer learning and few-shot learning tasks for multiple settings. The experiment on meta-transfer emphasizes the applicability of training meta-networks, where other approximations are limited. For few-shot learning, we evaluate time and memory complexities compared with popular baselines. We show that our method significantly reduces training time and memory usage, maintaining competitive accuracies, or even outperforming in some cases.



### Text Detection and Recognition in the Wild: A Review
- **Arxiv ID**: http://arxiv.org/abs/2006.04305v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.04305v2)
- **Published**: 2020-06-08 01:08:04+00:00
- **Updated**: 2020-06-30 22:23:08+00:00
- **Authors**: Zobeir Raisi, Mohamed A. Naiel, Paul Fieguth, Steven Wardell, John Zelek
- **Comment**: None
- **Journal**: None
- **Summary**: Detection and recognition of text in natural images are two main problems in the field of computer vision that have a wide variety of applications in analysis of sports videos, autonomous driving, industrial automation, to name a few. They face common challenging problems that are factors in how text is represented and affected by several environmental conditions. The current state-of-the-art scene text detection and/or recognition methods have exploited the witnessed advancement in deep learning architectures and reported a superior accuracy on benchmark datasets when tackling multi-resolution and multi-oriented text. However, there are still several remaining challenges affecting text in the wild images that cause existing methods to underperform due to there models are not able to generalize to unseen data and the insufficient labeled data. Thus, unlike previous surveys in this field, the objectives of this survey are as follows: first, offering the reader not only a review on the recent advancement in scene text detection and recognition, but also presenting the results of conducting extensive experiments using a unified evaluation framework that assesses pre-trained models of the selected methods on challenging cases, and applies the same evaluation criteria on these techniques. Second, identifying several existing challenges for detecting or recognizing text in the wild images, namely, in-plane-rotation, multi-oriented and multi-resolution text, perspective distortion, illumination reflection, partial occlusion, complex fonts, and special characters. Finally, the paper also presents insight into the potential research directions in this field to address some of the mentioned challenges that are still encountering scene text detection and recognition techniques.



### Are We Hungry for 3D LiDAR Data for Semantic Segmentation? A Survey and Experimental Study
- **Arxiv ID**: http://arxiv.org/abs/2006.04307v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.04307v2)
- **Published**: 2020-06-08 01:20:59+00:00
- **Updated**: 2020-11-30 12:58:20+00:00
- **Authors**: Biao Gao, Yancheng Pan, Chengkun Li, Sibo Geng, Huijing Zhao
- **Comment**: 22 pages, 15 figures
- **Journal**: None
- **Summary**: 3D semantic segmentation is a fundamental task for robotic and autonomous driving applications. Recent works have been focused on using deep learning techniques, whereas developing fine-annotated 3D LiDAR datasets is extremely labor intensive and requires professional skills. The performance limitation caused by insufficient datasets is called data hunger problem. This research provides a comprehensive survey and experimental study on the question: are we hungry for 3D LiDAR data for semantic segmentation? The studies are conducted at three levels. First, a broad review to the main 3D LiDAR datasets is conducted, followed by a statistical analysis on three representative datasets to gain an in-depth view on the datasets' size and diversity, which are the critical factors in learning deep models. Second, a systematic review to the state-of-the-art 3D semantic segmentation is conducted, followed by experiments and cross examinations of three representative deep learning methods to find out how the size and diversity of the datasets affect deep models' performance. Finally, a systematic survey to the existing efforts to solve the data hunger problem is conducted on both methodological and dataset's viewpoints, followed by an insightful discussion of remaining problems and open questions To the best of our knowledge, this is the first work to analyze the data hunger problem for 3D semantic segmentation using deep learning techniques that are addressed in the literature review, statistical analysis, and cross-dataset and cross-algorithm experiments. We share findings and discussions, which may lead to potential topics in future works.



### Counterfactual VQA: A Cause-Effect Look at Language Bias
- **Arxiv ID**: http://arxiv.org/abs/2006.04315v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2006.04315v4)
- **Published**: 2020-06-08 01:49:27+00:00
- **Updated**: 2021-04-01 16:15:36+00:00
- **Authors**: Yulei Niu, Kaihua Tang, Hanwang Zhang, Zhiwu Lu, Xian-Sheng Hua, Ji-Rong Wen
- **Comment**: Accepted by CVPR 2021
- **Journal**: None
- **Summary**: VQA models may tend to rely on language bias as a shortcut and thus fail to sufficiently learn the multi-modal knowledge from both vision and language. Recent debiasing methods proposed to exclude the language prior during inference. However, they fail to disentangle the "good" language context and "bad" language bias from the whole. In this paper, we investigate how to mitigate language bias in VQA. Motivated by causal effects, we proposed a novel counterfactual inference framework, which enables us to capture the language bias as the direct causal effect of questions on answers and reduce the language bias by subtracting the direct language effect from the total causal effect. Experiments demonstrate that our proposed counterfactual inference framework 1) is general to various VQA backbones and fusion strategies, 2) achieves competitive performance on the language-bias sensitive VQA-CP dataset while performs robustly on the balanced VQA v2 dataset without any augmented data. The code is available at https://github.com/yuleiniu/cfvqa.



### Ensemble Model with Batch Spectral Regularization and Data Blending for Cross-Domain Few-Shot Learning with Unlabeled Data
- **Arxiv ID**: http://arxiv.org/abs/2006.04323v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.04323v2)
- **Published**: 2020-06-08 02:27:34+00:00
- **Updated**: 2020-06-09 07:52:51+00:00
- **Authors**: Zhen Zhao, Bingyu Liu, Yuhong Guo, Jieping Ye
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present our proposed ensemble model with batch spectral regularization and data blending mechanisms for the Track 2 problem of the cross-domain few-shot learning (CD-FSL) challenge. We build a multi-branch ensemble framework by using diverse feature transformation matrices, while deploying batch spectral feature regularization on each branch to improve the model's transferability. Moreover, we propose a data blending method to exploit the unlabeled data and augment the sparse support set in the target domain. Our proposed model demonstrates effective performance on the CD-FSL benchmark tasks.



### Fully Convolutional Mesh Autoencoder using Efficient Spatially Varying Kernels
- **Arxiv ID**: http://arxiv.org/abs/2006.04325v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.04325v2)
- **Published**: 2020-06-08 02:30:13+00:00
- **Updated**: 2020-10-21 06:16:23+00:00
- **Authors**: Yi Zhou, Chenglei Wu, Zimo Li, Chen Cao, Yuting Ye, Jason Saragih, Hao Li, Yaser Sheikh
- **Comment**: 12 pages
- **Journal**: None
- **Summary**: Learning latent representations of registered meshes is useful for many 3D tasks. Techniques have recently shifted to neural mesh autoencoders. Although they demonstrate higher precision than traditional methods, they remain unable to capture fine-grained deformations. Furthermore, these methods can only be applied to a template-specific surface mesh, and is not applicable to more general meshes, like tetrahedrons and non-manifold meshes. While more general graph convolution methods can be employed, they lack performance in reconstruction precision and require higher memory usage. In this paper, we propose a non-template-specific fully convolutional mesh autoencoder for arbitrary registered mesh data. It is enabled by our novel convolution and (un)pooling operators learned with globally shared weights and locally varying coefficients which can efficiently capture the spatially varying contents presented by irregular mesh connections. Our model outperforms state-of-the-art methods on reconstruction accuracy. In addition, the latent codes of our network are fully localized thanks to the fully convolutional structure, and thus have much higher interpolation capability than many traditional 3D mesh generation models.



### A Transductive Multi-Head Model for Cross-Domain Few-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2006.11384v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.11384v1)
- **Published**: 2020-06-08 02:39:59+00:00
- **Updated**: 2020-06-08 02:39:59+00:00
- **Authors**: Jianan Jiang, Zhenpeng Li, Yuhong Guo, Jieping Ye
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present a new method, Transductive Multi-Head Few-Shot learning (TMHFS), to address the Cross-Domain Few-Shot Learning (CD-FSL) challenge. The TMHFS method extends the Meta-Confidence Transduction (MCT) and Dense Feature-Matching Networks (DFMN) method [2] by introducing a new prediction head, i.e, an instance-wise global classification network based on semantic information, after the common feature embedding network. We train the embedding network with the multiple heads, i.e,, the MCT loss, the DFMN loss and the semantic classifier loss, simultaneously in the source domain. For the few-shot learning in the target domain, we first perform fine-tuning on the embedding network with only the semantic global classifier and the support instances, and then use the MCT part to predict labels of the query set with the fine-tuned embedding network. Moreover, we further exploit data augmentation techniques during the fine-tuning and test stages to improve the prediction performance. The experimental results demonstrate that the proposed methods greatly outperform the strong baseline, fine-tuning, on four different target domains.



### Deep Neural Network Based Real-time Kiwi Fruit Flower Detection in an Orchard Environment
- **Arxiv ID**: http://arxiv.org/abs/2006.04343v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2006.04343v1)
- **Published**: 2020-06-08 03:53:54+00:00
- **Updated**: 2020-06-08 03:53:54+00:00
- **Authors**: JongYoon Lim, Ho Seok Ahn, Mahla Nejati, Jamie Bell, Henry Williams, Bruce A. MacDonald
- **Comment**: ACRA(Australian Robotics and Automation Association) 2019
- **Journal**: None
- **Summary**: In this paper, we present a novel approach to kiwi fruit flower detection using Deep Neural Networks (DNNs) to build an accurate, fast, and robust autonomous pollination robot system. Recent work in deep neural networks has shown outstanding performance on object detection tasks in many areas. Inspired this, we aim for exploiting DNNs for kiwi fruit flower detection and present intensive experiments and their analysis on two state-of-the-art object detectors; Faster R-CNN and Single Shot Detector (SSD) Net, and feature extractors; Inception Net V2 and NAS Net with real-world orchard datasets. We also compare those approaches to find an optimal model which is suitable for a real-time agricultural pollination robot system in terms of accuracy and processing speed. We perform experiments with dataset collected from different seasons and locations (spatio-temporal consistency) in order to demonstrate the performance of the generalized model. The proposed system demonstrates promising results of 0.919, 0.874, and 0.889 for precision, recall, and F1-score respectively on our real-world dataset, and the performance satisfies the requirement for deploying the system onto an autonomous pollination robotics system.



### Sparsifying and Down-scaling Networks to Increase Robustness to Distortions
- **Arxiv ID**: http://arxiv.org/abs/2006.11389v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.11389v1)
- **Published**: 2020-06-08 03:58:27+00:00
- **Updated**: 2020-06-08 03:58:27+00:00
- **Authors**: Sergey Tarasenko
- **Comment**: 6 pages, 6 figures
- **Journal**: None
- **Summary**: It has been shown that perfectly trained networks exhibit drastic reduction in performance when presented with distorted images. Streaming Network (STNet) is a novel architecture capable of robust classification of the distorted images while been trained on undistorted images. The distortion robustness is enabled by means of sparse input and isolated parallel streams with decoupled weights. Recent results prove STNet is robust to 20 types of noise and distortions. STNet exhibits state-of-the-art performance for classification of low light images, while being of much smaller size when other networks. In this paper, we construct STNets by using scaled versions (number of filters in each layer is reduced by factor of n) of popular networks like VGG16, ResNet50 and MobileNetV2 as parallel streams. These new STNets are tested on several datasets. Our results indicate that more efficient (less FLOPS), new STNets exhibit higher or equal accuracy in comparison with original networks. Considering a diversity of datasets and networks used for tests, we conclude that a new type of STNets is an efficient tool for robust classification of distorted images.



### Fast Synthetic LiDAR Rendering via Spherical UV Unwrapping of Equirectangular Z-Buffer Images
- **Arxiv ID**: http://arxiv.org/abs/2006.04345v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.04345v1)
- **Published**: 2020-06-08 04:07:57+00:00
- **Updated**: 2020-06-08 04:07:57+00:00
- **Authors**: Mohammed Hossny, Khaled Saleh, Mohammed Attia, Ahmed Abobakr, Julie Iskander
- **Comment**: This version has been removed by arXiv administrators as the
  submitter did not have the right to agree to the license at the time of
  submission
- **Journal**: None
- **Summary**: LiDAR data is becoming increasingly essential with the rise of autonomous vehicles. Its ability to provide 360deg horizontal field of view of point cloud, equips self-driving vehicles with enhanced situational awareness capabilities. While synthetic LiDAR data generation pipelines provide a good solution to advance the machine learning research on LiDAR, they do suffer from a major shortcoming, which is rendering time. Physically accurate LiDAR simulators (e.g. Blensor) are computationally expensive with an average rendering time of 14-60 seconds per frame for urban scenes. This is often compensated for via using 3D models with simplified polygon topology (low poly assets) as is the case of CARLA (Dosovitskiy et al., 2017). However, this comes at the price of having coarse grained unrealistic LiDAR point clouds. In this paper, we present a novel method to simulate LiDAR point cloud with faster rendering time of 1 sec per frame. The proposed method relies on spherical UV unwrapping of Equirectangular Z-Buffer images. We chose Blensor (Gschwandtner et al., 2011) as the baseline method to compare the point clouds generated using the proposed method. The reported error for complex urban landscapes is 4.28cm for a scanning range between 2-120 meters with Velodyne HDL64-E2 parameters. The proposed method reported a total time per frame to 3.2 +/- 0.31 seconds per frame. In contrast, the BlenSor baseline method reported 16.2 +/- 1.82 seconds.



### Associate-3Ddet: Perceptual-to-Conceptual Association for 3D Point Cloud Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2006.04356v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.04356v1)
- **Published**: 2020-06-08 05:15:06+00:00
- **Updated**: 2020-06-08 05:15:06+00:00
- **Authors**: Liang Du, Xiaoqing Ye, Xiao Tan, Jianfeng Feng, Zhenbo Xu, Errui Ding, Shilei Wen
- **Comment**: 8 pages, 5 figures, CVPR 2020
- **Journal**: None
- **Summary**: Object detection from 3D point clouds remains a challenging task, though recent studies pushed the envelope with the deep learning techniques. Owing to the severe spatial occlusion and inherent variance of point density with the distance to sensors, appearance of a same object varies a lot in point cloud data. Designing robust feature representation against such appearance changes is hence the key issue in a 3D object detection method. In this paper, we innovatively propose a domain adaptation like approach to enhance the robustness of the feature representation. More specifically, we bridge the gap between the perceptual domain where the feature comes from a real scene and the conceptual domain where the feature is extracted from an augmented scene consisting of non-occlusion point cloud rich of detailed information. This domain adaptation approach mimics the functionality of the human brain when proceeding object perception. Extensive experiments demonstrate that our simple yet effective approach fundamentally boosts the performance of 3D point cloud object detection and achieves the state-of-the-art results.



### Neural Sparse Representation for Image Restoration
- **Arxiv ID**: http://arxiv.org/abs/2006.04357v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.04357v1)
- **Published**: 2020-06-08 05:15:17+00:00
- **Updated**: 2020-06-08 05:15:17+00:00
- **Authors**: Yuchen Fan, Jiahui Yu, Yiqun Mei, Yulun Zhang, Yun Fu, Ding Liu, Thomas S. Huang
- **Comment**: None
- **Journal**: None
- **Summary**: Inspired by the robustness and efficiency of sparse representation in sparse coding based image restoration models, we investigate the sparsity of neurons in deep networks. Our method structurally enforces sparsity constraints upon hidden neurons. The sparsity constraints are favorable for gradient-based learning algorithms and attachable to convolution layers in various networks. Sparsity in neurons enables computation saving by only operating on non-zero components without hurting accuracy. Meanwhile, our method can magnify representation dimensionality and model capacity with negligible additional computation cost. Experiments show that sparse representation is crucial in deep neural networks for multiple image restoration tasks, including image super-resolution, image denoising, and image compression artifacts removal. Code is available at https://github.com/ychfan/nsr



### Photoacoustic Microscopy with Sparse Data Enabled by Convolutional Neural Networks for Fast Imaging
- **Arxiv ID**: http://arxiv.org/abs/2006.04368v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.04368v1)
- **Published**: 2020-06-08 05:49:32+00:00
- **Updated**: 2020-06-08 05:49:32+00:00
- **Authors**: Jiasheng Zhou, Da He, Xiaoyu Shang, Zhendong Guo, Sung-liang Chen, Jiajia Luo
- **Comment**: 13 pages (including 2 pages of supplementary materials)
- **Journal**: None
- **Summary**: Photoacoustic microscopy (PAM) has been a promising biomedical imaging technology in recent years. However, the point-by-point scanning mechanism results in low-speed imaging, which limits the application of PAM. Reducing sampling density can naturally shorten image acquisition time, which is at the cost of image quality. In this work, we propose a method using convolutional neural networks (CNNs) to improve the quality of sparse PAM images, thereby speeding up image acquisition while keeping good image quality. The CNN model utilizes both squeeze-and-excitation blocks and residual blocks to achieve the enhancement, which is a mapping from a 1/4 or 1/16 low-sampling sparse PAM image to a latent fully-sampled image. The perceptual loss function is applied to keep the fidelity of images. The model is mainly trained and validated on PAM images of leaf veins. The experiments show the effectiveness of our proposed method, which significantly outperforms existing methods quantitatively and qualitatively. Our model is also tested using in vivo PAM images of blood vessels of mouse ears and eyes. The results show that the model can enhance the image quality of the sparse PAM image of blood vessels from several aspects, which may help fast PAM and facilitate its clinical applications.



### Semantics-Driven Unsupervised Learning for Monocular Depth and Ego-Motion Estimation
- **Arxiv ID**: http://arxiv.org/abs/2006.04371v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.04371v1)
- **Published**: 2020-06-08 05:55:07+00:00
- **Updated**: 2020-06-08 05:55:07+00:00
- **Authors**: Xiaobin Wei, Jianjiang Feng, Jie Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a semantics-driven unsupervised learning approach for monocular depth and ego-motion estimation from videos in this paper. Recent unsupervised learning methods employ photometric errors between synthetic view and actual image as a supervision signal for training. In our method, we exploit semantic segmentation information to mitigate the effects of dynamic objects and occlusions in the scene, and to improve depth prediction performance by considering the correlation between depth and semantics. To avoid costly labeling process, we use noisy semantic segmentation results obtained by a pre-trained semantic segmentation network. In addition, we minimize the position error between the corresponding points of adjacent frames to utilize 3D spatial information. Experimental results on the KITTI dataset show that our method achieves good performance in both depth and ego-motion estimation tasks.



### Learning the Compositional Visual Coherence for Complementary Recommendations
- **Arxiv ID**: http://arxiv.org/abs/2006.04380v1
- **DOI**: None
- **Categories**: **cs.IR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.04380v1)
- **Published**: 2020-06-08 06:57:18+00:00
- **Updated**: 2020-06-08 06:57:18+00:00
- **Authors**: Zhi Li, Bo Wu, Qi Liu, Likang Wu, Hongke Zhao, Tao Mei
- **Comment**: Early version accepted by IJCAI2020
- **Journal**: None
- **Summary**: Complementary recommendations, which aim at providing users product suggestions that are supplementary and compatible with their obtained items, have become a hot topic in both academia and industry in recent years. %However, it is challenging due to its complexity and subjectivity. Existing work mainly focused on modeling the co-purchased relations between two items, but the compositional associations of item collections are largely unexplored. Actually, when a user chooses the complementary items for the purchased products, it is intuitive that she will consider the visual semantic coherence (such as color collocations, texture compatibilities) in addition to global impressions. Towards this end, in this paper, we propose a novel Content Attentive Neural Network (CANN) to model the comprehensive compositional coherence on both global contents and semantic contents. Specifically, we first propose a \textit{Global Coherence Learning} (GCL) module based on multi-heads attention to model the global compositional coherence. Then, we generate the semantic-focal representations from different semantic regions and design a \textit{Focal Coherence Learning} (FCL) module to learn the focal compositional coherence from different semantic-focal representations. Finally, we optimize the CANN in a novel compositional optimization strategy. Extensive experiments on the large-scale real-world data clearly demonstrate the effectiveness of CANN compared with several state-of-the-art methods.



### Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for Dense Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2006.04388v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.04388v1)
- **Published**: 2020-06-08 07:24:33+00:00
- **Updated**: 2020-06-08 07:24:33+00:00
- **Authors**: Xiang Li, Wenhai Wang, Lijun Wu, Shuo Chen, Xiaolin Hu, Jun Li, Jinhui Tang, Jian Yang
- **Comment**: 14 pages, 14 figures
- **Journal**: None
- **Summary**: One-stage detector basically formulates object detection as dense classification and localization. The classification is usually optimized by Focal Loss and the box location is commonly learned under Dirac delta distribution. A recent trend for one-stage detectors is to introduce an individual prediction branch to estimate the quality of localization, where the predicted quality facilitates the classification to improve detection performance. This paper delves into the representations of the above three fundamental elements: quality estimation, classification and localization. Two problems are discovered in existing practices, including (1) the inconsistent usage of the quality estimation and classification between training and inference and (2) the inflexible Dirac delta distribution for localization when there is ambiguity and uncertainty in complex scenes. To address the problems, we design new representations for these elements. Specifically, we merge the quality estimation into the class prediction vector to form a joint representation of localization quality and classification, and use a vector to represent arbitrary distribution of box locations. The improved representations eliminate the inconsistency risk and accurately depict the flexible distribution in real data, but contain continuous labels, which is beyond the scope of Focal Loss. We then propose Generalized Focal Loss (GFL) that generalizes Focal Loss from its discrete form to the continuous version for successful optimization. On COCO test-dev, GFL achieves 45.0\% AP using ResNet-101 backbone, surpassing state-of-the-art SAPD (43.5\%) and ATSS (43.6\%) with higher or comparable inference speed, under the same backbone and training settings. Notably, our best model can achieve a single-model single-scale AP of 48.2\%, at 10 FPS on a single 2080Ti GPU. Code and models are available at https://github.com/implus/GFocal.



### Cross-Domain Segmentation with Adversarial Loss and Covariate Shift for Biomedical Imaging
- **Arxiv ID**: http://arxiv.org/abs/2006.04390v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.04390v1)
- **Published**: 2020-06-08 07:35:55+00:00
- **Updated**: 2020-06-08 07:35:55+00:00
- **Authors**: Bora Baydar, Savas Ozkan, A. Emre Kavur, N. Sinem Gezer, M. Alper Selver, Gozde Bozdagi Akar
- **Comment**: None
- **Journal**: None
- **Summary**: Despite the widespread use of deep learning methods for semantic segmentation of images that are acquired from a single source, clinicians often use multi-domain data for a detailed analysis. For instance, CT and MRI have advantages over each other in terms of imaging quality, artifacts, and output characteristics that lead to differential diagnosis. The capacity of current segmentation techniques is only allow to work for an individual domain due to their differences. However, the models that are capable of working on all modalities are essentially needed for a complete solution. Furthermore, robustness is drastically affected by the number of samples in the training step, especially for deep learning models. Hence, there is a necessity that all available data regardless of data domain should be used for reliable methods. For this purpose, this manuscript aims to implement a novel model that can learn robust representations from cross-domain data by encapsulating distinct and shared patterns from different modalities. Precisely, covariate shift property is retained with structural modification and adversarial loss where sparse and rich representations are obtained. Hence, a single parameter set is used to perform cross-domain segmentation task. The superiority of the proposed method is that no information related to modalities are provided in either training or inference phase. The tests on CT and MRI liver data acquired in routine clinical workflows show that the proposed model outperforms all other baseline with a large margin. Experiments are also conducted on Covid-19 dataset that it consists of CT data where significant intra-class visual differences are observed. Similarly, the proposed method achieves the best performance.



### Passive Batch Injection Training Technique: Boosting Network Performance by Injecting Mini-Batches from a different Data Distribution
- **Arxiv ID**: http://arxiv.org/abs/2006.04406v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.04406v1)
- **Published**: 2020-06-08 08:17:32+00:00
- **Updated**: 2020-06-08 08:17:32+00:00
- **Authors**: Pravendra Singh, Pratik Mazumder, Vinay P. Namboodiri
- **Comment**: Accepted in IJCNN 2020
- **Journal**: None
- **Summary**: This work presents a novel training technique for deep neural networks that makes use of additional data from a distribution that is different from that of the original input data. This technique aims to reduce overfitting and improve the generalization performance of the network. Our proposed technique, namely Passive Batch Injection Training Technique (PBITT), even reduces the level of overfitting in networks that already use the standard techniques for reducing overfitting such as $L_2$ regularization and batch normalization, resulting in significant accuracy improvements. Passive Batch Injection Training Technique (PBITT) introduces a few passive mini-batches into the training process that contain data from a distribution that is different from the input data distribution. This technique does not increase the number of parameters in the final model and also does not increase the inference (test) time but still improves the performance of deep CNNs. To the best of our knowledge, this is the first work that makes use of different data distribution to aid the training of convolutional neural networks (CNNs). We thoroughly evaluate the proposed approach on standard architectures: VGG, ResNet, and WideResNet, and on several popular datasets: CIFAR-10, CIFAR-100, SVHN, and ImageNet. We observe consistent accuracy improvement by using the proposed technique. We also show experimentally that the model trained by our technique generalizes well to other tasks such as object detection on the MS-COCO dataset using Faster R-CNN. We present extensive ablations to validate the proposed approach. Our approach improves the accuracy of VGG-16 by a significant margin of 2.1% over the CIFAR-100 dataset.



### Training Deep Spiking Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2006.04436v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.04436v1)
- **Published**: 2020-06-08 09:47:05+00:00
- **Updated**: 2020-06-08 09:47:05+00:00
- **Authors**: Eimantas Ledinauskas, Julius Ruseckas, Alfonsas Juršėnas, Giedrius Buračas
- **Comment**: None
- **Journal**: None
- **Summary**: Computation using brain-inspired spiking neural networks (SNNs) with neuromorphic hardware may offer orders of magnitude higher energy efficiency compared to the current analog neural networks (ANNs). Unfortunately, training SNNs with the same number of layers as state of the art ANNs remains a challenge. To our knowledge the only method which is successful in this regard is supervised training of ANN and then converting it to SNN. In this work we directly train deep SNNs using backpropagation with surrogate gradient and find that due to implicitly recurrent nature of feed forward SNN's the exploding or vanishing gradient problem severely hinders their training. We show that this problem can be solved by tuning the surrogate gradient function. We also propose using batch normalization from ANN literature on input currents of SNN neurons. Using these improvements we show that is is possible to train SNN with ResNet50 architecture on CIFAR100 and Imagenette object recognition datasets. The trained SNN falls behind in accuracy compared to analogous ANN but requires several orders of magnitude less inference time steps (as low as 10) to reach good accuracy compared to SNNs obtained by conversion from ANN which require on the order of 1000 time steps.



### On Universalized Adversarial and Invariant Perturbations
- **Arxiv ID**: http://arxiv.org/abs/2006.04449v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.04449v1)
- **Published**: 2020-06-08 10:08:20+00:00
- **Updated**: 2020-06-08 10:08:20+00:00
- **Authors**: Sandesh Kamath, Amit Deshpande, K V Subrahmanyam
- **Comment**: Some part of this work was presented in ICML 2018 Workshop on
  "Towards learning with limited labels: Equivariance, Invariance,and Beyond"
  as "Understanding Adversarial Robustness of Symmetric Networks"
- **Journal**: None
- **Summary**: Convolutional neural networks or standard CNNs (StdCNNs) are translation-equivariant models that achieve translation invariance when trained on data augmented with sufficient translations. Recent work on equivariant models for a given group of transformations (e.g., rotations) has lead to group-equivariant convolutional neural networks (GCNNs). GCNNs trained on data augmented with sufficient rotations achieve rotation invariance. Recent work by authors arXiv:2002.11318 studies a trade-off between invariance and robustness to adversarial attacks. In another related work arXiv:2005.08632, given any model and any input-dependent attack that satisfies a certain spectral property, the authors propose a universalization technique called SVD-Universal to produce a universal adversarial perturbation by looking at very few test examples. In this paper, we study the effectiveness of SVD-Universal on GCNNs as they gain rotation invariance through higher degree of training augmentation. We empirically observe that as GCNNs gain rotation invariance through training augmented with larger rotations, the fooling rate of SVD-Universal gets better. To understand this phenomenon, we introduce universal invariant directions and study their relation to the universal adversarial direction produced by SVD-Universal.



### Novel Adaptive Binary Search Strategy-First Hybrid Pyramid- and Clustering-Based CNN Filter Pruning Method without Parameters Setting
- **Arxiv ID**: http://arxiv.org/abs/2006.04451v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.04451v2)
- **Published**: 2020-06-08 10:09:43+00:00
- **Updated**: 2021-04-30 07:33:00+00:00
- **Authors**: Kuo-Liang Chung, Yu-Lun Chang, Bo-Wei Tsai
- **Comment**: None
- **Journal**: None
- **Summary**: Pruning redundant filters in CNN models has received growing attention. In this paper, we propose an adaptive binary search-first hybrid pyramid- and clustering-based (ABSHPC-based) method for pruning filters automatically. In our method, for each convolutional layer, initially a hybrid pyramid data structure is constructed to store the hierarchical information of each filter. Given a tolerant accuracy loss, without parameters setting, we begin from the last convolutional layer to the first layer; for each considered layer with less or equal pruning rate relative to its previous layer, our ABSHPC-based process is applied to optimally partition all filters to clusters, where each cluster is thus represented by the filter with the median root mean of the hybrid pyramid, leading to maximal removal of redundant filters. Based on the practical dataset and the CNN models, with higher accuracy, the thorough experimental results demonstrated the significant parameters and floating-point operations reduction merits of the proposed filter pruning method relative to the state-of-the-art methods.



### Continual Representation Learning for Biometric Identification
- **Arxiv ID**: http://arxiv.org/abs/2006.04455v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.04455v2)
- **Published**: 2020-06-08 10:18:06+00:00
- **Updated**: 2020-06-28 19:54:51+00:00
- **Authors**: Bo Zhao, Shixiang Tang, Dapeng Chen, Hakan Bilen, Rui Zhao
- **Comment**: None
- **Journal**: Proceedings of the IEEE/CVF Winter Conference on Applications of
  Computer Vision (WACV), 2021, pp. 1198-1208
- **Summary**: With the explosion of digital data in recent years, continuously learning new tasks from a stream of data without forgetting previously acquired knowledge has become increasingly important. In this paper, we propose a new continual learning (CL) setting, namely ``continual representation learning'', which focuses on learning better representation in a continuous way. We also provide two large-scale multi-step benchmarks for biometric identification, where the visual appearance of different classes are highly relevant. In contrast to requiring the model to recognize more learned classes, we aim to learn feature representation that can be better generalized to not only previously unseen images but also unseen classes/identities. For the new setting, we propose a novel approach that performs the knowledge distillation over a large number of identities by applying the neighbourhood selection and consistency relaxation strategies to improve scalability and flexibility of the continual learning model. We demonstrate that existing CL methods can improve the representation in the new setting, and our method achieves better results than the competitors.



### Deep hierarchical pooling design for cross-granularity action recognition
- **Arxiv ID**: http://arxiv.org/abs/2006.04473v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.04473v1)
- **Published**: 2020-06-08 11:03:54+00:00
- **Updated**: 2020-06-08 11:03:54+00:00
- **Authors**: Ahmed Mazari, Hichem Sahbi
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we introduce a novel hierarchical aggregation design that captures different levels of temporal granularity in action recognition. Our design principle is coarse-to-fine and achieved using a tree-structured network; as we traverse this network top-down, pooling operations are getting less invariant but timely more resolute and well localized. Learning the combination of operations in this network -- which best fits a given ground-truth -- is obtained by solving a constrained minimization problem whose solution corresponds to the distribution of weights that capture the contribution of each level (and thereby temporal granularity) in the global hierarchical pooling process. Besides being principled and well grounded, the proposed hierarchical pooling is also video-length agnostic and resilient to misalignments in actions. Extensive experiments conducted on the challenging UCF-101 database corroborate these statements.



### Action Recognition with Deep Multiple Aggregation Networks
- **Arxiv ID**: http://arxiv.org/abs/2006.04489v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.04489v1)
- **Published**: 2020-06-08 11:37:38+00:00
- **Updated**: 2020-06-08 11:37:38+00:00
- **Authors**: Ahmed Mazari, Hichem Sahbi
- **Comment**: None
- **Journal**: None
- **Summary**: Most of the current action recognition algorithms are based on deep networks which stack multiple convolutional, pooling and fully connected layers. While convolutional and fully connected operations have been widely studied in the literature, the design of pooling operations that handle action recognition, with different sources of temporal granularity in action categories, has comparatively received less attention, and existing solutions rely mainly on max or averaging operations. The latter are clearly powerless to fully exhibit the actual temporal granularity of action categories and thereby constitute a bottleneck in classification performances. In this paper, we introduce a novel hierarchical pooling design that captures different levels of temporal granularity in action recognition. Our design principle is coarse-to-fine and achieved using a tree-structured network; as we traverse this network top-down, pooling operations are getting less invariant but timely more resolute and well localized. Learning the combination of operations in this network -- which best fits a given ground-truth -- is obtained by solving a constrained minimization problem whose solution corresponds to the distribution of weights that capture the contribution of each level (and thereby temporal granularity) in the global hierarchical pooling process. Besides being principled and well grounded, the proposed hierarchical pooling is also video-length and resolution agnostic. Extensive experiments conducted on the challenging UCF-101, HMDB-51 and JHMDB-21 databases corroborate all these statements.



### More Information Supervised Probabilistic Deep Face Embedding Learning
- **Arxiv ID**: http://arxiv.org/abs/2006.04518v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.04518v2)
- **Published**: 2020-06-08 12:33:32+00:00
- **Updated**: 2020-06-11 02:25:56+00:00
- **Authors**: Ying Huang, Shangfeng Qiu, Wenwei Zhang, Xianghui Luo, Jinzhuo Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Researches using margin based comparison loss demonstrate the effectiveness of penalizing the distance between face feature and their corresponding class centers. Despite their popularity and excellent performance, they do not explicitly encourage the generic embedding learning for an open set recognition problem. In this paper, we analyse margin based softmax loss in probability view. With this perspective, we propose two general principles: 1) monotonic decreasing and 2) margin probability penalty, for designing new margin loss functions. Unlike methods optimized with single comparison metric, we provide a new perspective to treat open set face recognition as a problem of information transmission. And the generalization capability for face embedding is gained with more clean information. An auto-encoder architecture called Linear-Auto-TS-Encoder(LATSE) is proposed to corroborate this finding. Extensive experiments on several benchmarks demonstrate that LATSE help face embedding to gain more generalization capability and it boosted the single model performance with open training dataset to more than $99\%$ on MegaFace test.



### Learning 3D-3D Correspondences for One-shot Partial-to-partial Registration
- **Arxiv ID**: http://arxiv.org/abs/2006.04523v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.04523v2)
- **Published**: 2020-06-08 12:35:47+00:00
- **Updated**: 2020-06-16 15:57:22+00:00
- **Authors**: Zheng Dang, Fei Wang, Mathieu Salzmann
- **Comment**: 11 pages
- **Journal**: None
- **Summary**: While 3D-3D registration is traditionally tacked by optimization-based methods, recent work has shown that learning-based techniques could achieve faster and more robust results. In this context, however, only PRNet can handle the partial-to-partial registration scenario. Unfortunately, this is achieved at the cost of relying on an iterative procedure, with a complex network architecture. Here, we show that learning-based partial-to-partial registration can be achieved in a one-shot manner, jointly reducing network complexity and increasing registration accuracy. To this end, we propose an Optimal Transport layer able to account for occluded points thanks to the use of outlier bins. The resulting OPRNet framework outperforms the state of the art on standard benchmarks, demonstrating better robustness and generalization ability than existing techniques.



### FibeR-CNN: Expanding Mask R-CNN to Improve Image-Based Fiber Analysis
- **Arxiv ID**: http://arxiv.org/abs/2006.04552v2
- **DOI**: 10.1016/j.powtec.2020.08.034
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.04552v2)
- **Published**: 2020-06-08 13:03:09+00:00
- **Updated**: 2020-10-19 08:45:14+00:00
- **Authors**: Max Frei, Frank Einar Kruis
- **Comment**: 21 pages, 31 figures, 5 tables, 1 algorithm
- **Journal**: Powder Technology, vol. 377, pp. 974-991
- **Summary**: Fiber-shaped materials (e.g. carbon nano tubes) are of great relevance, due to their unique properties but also the health risk they can impose. Unfortunately, image-based analysis of fibers still involves manual annotation, which is a time-consuming and costly process. We therefore propose the use of region-based convolutional neural networks (R-CNNs) to automate this task. Mask R-CNN, the most widely used R-CNN for semantic segmentation tasks, is prone to errors when it comes to the analysis of fiber-shaped objects. Hence, a new architecture - FibeR-CNN - is introduced and validated. FibeR-CNN combines two established R-CNN architectures (Mask and Keypoint R-CNN) and adds additional network heads for the prediction of fiber widths and lengths. As a result, FibeR-CNN is able to surpass the mean average precision of Mask R-CNN by 33 % (11 percentage points) on a novel test data set of fiber images.



### Parameter-Efficient Person Re-identification in the 3D Space
- **Arxiv ID**: http://arxiv.org/abs/2006.04569v3
- **DOI**: 10.1109/TNNLS.2022.3214834
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.04569v3)
- **Published**: 2020-06-08 13:20:33+00:00
- **Updated**: 2021-07-31 02:45:33+00:00
- **Authors**: Zhedong Zheng, Nenggan Zheng, Yi Yang
- **Comment**: The code is available at https://github.com/layumi/person-reid-3d
- **Journal**: IEEE Transactions on Neural Networks and Learning Systems (2022)
- **Summary**: People live in a 3D world. However, existing works on person re-identification (re-id) mostly consider the semantic representation learning in a 2D space, intrinsically limiting the understanding of people. In this work, we address this limitation by exploring the prior knowledge of the 3D body structure. Specifically, we project 2D images to a 3D space and introduce a novel parameter-efficient Omni-scale Graph Network (OG-Net) to learn the pedestrian representation directly from 3D point clouds. OG-Net effectively exploits the local information provided by sparse 3D points and takes advantage of the structure and appearance information in a coherent manner. With the help of 3D geometry information, we can learn a new type of deep re-id feature free from noisy variants, such as scale and viewpoint. To our knowledge, we are among the first attempts to conduct person re-identification in the 3D space. We demonstrate through extensive experiments that the proposed method (1) eases the matching difficulty in the traditional 2D space, (2) exploits the complementary information of 2D appearance and 3D structure, (3) achieves competitive results with limited parameters on four large-scale person re-id datasets, and (4) has good scalability to unseen datasets. Our code, models and generated 3D human data are publicly available at https://github.com/layumi/person-reid-3d .



### BS-Net: learning COVID-19 pneumonia severity on a large Chest X-Ray dataset
- **Arxiv ID**: http://arxiv.org/abs/2006.04603v3
- **DOI**: 10.1016/j.media.2021.102046
- **Categories**: **eess.IV**, cs.CV, cs.LG, 68T45, I.2.10; I.5; I.4; J.3
- **Links**: [PDF](http://arxiv.org/pdf/2006.04603v3)
- **Published**: 2020-06-08 13:55:58+00:00
- **Updated**: 2021-04-03 08:44:53+00:00
- **Authors**: Alberto Signoroni, Mattia Savardi, Sergio Benini, Nicola Adami, Riccardo Leonardi, Paolo Gibellini, Filippo Vaccher, Marco Ravanelli, Andrea Borghesi, Roberto Maroldi, Davide Farina
- **Comment**: 28 pages, 11 figures, preprint of accepted paper to Medical Image
  Analysis, Project page with Code and Dataset Available at
  https://brixia.github.io/
- **Journal**: None
- **Summary**: In this work we design an end-to-end deep learning architecture for predicting, on Chest X-rays images (CXR), a multi-regional score conveying the degree of lung compromise in COVID-19 patients. Such semi-quantitative scoring system, namely Brixia~score, is applied in serial monitoring of such patients, showing significant prognostic value, in one of the hospitals that experienced one of the highest pandemic peaks in Italy. To solve such a challenging visual task, we adopt a weakly supervised learning strategy structured to handle different tasks (segmentation, spatial alignment, and score estimation) trained with a "from-the-part-to-the-whole" procedure involving different datasets. In particular, we exploit a clinical dataset of almost 5,000 CXR annotated images collected in the same hospital. Our BS-Net demonstrates self-attentive behavior and a high degree of accuracy in all processing stages. Through inter-rater agreement tests and a gold standard comparison, we show that our solution outperforms single human annotators in rating accuracy and consistency, thus supporting the possibility of using this tool in contexts of computer-assisted monitoring. Highly resolved (super-pixel level) explainability maps are also generated, with an original technique, to visually help the understanding of the network activity on the lung areas. We also consider other scores proposed in literature and provide a comparison with a recently proposed non-specific approach. We eventually test the performance robustness of our model on an assorted public COVID-19 dataset, for which we also provide Brixia~score annotations, observing good direct generalization and fine-tuning capabilities that highlight the portability of BS-Net in other clinical settings. The CXR dataset along with the source code and the trained model are publicly released for research purposes.



### SoftFlow: Probabilistic Framework for Normalizing Flow on Manifolds
- **Arxiv ID**: http://arxiv.org/abs/2006.04604v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.04604v4)
- **Published**: 2020-06-08 13:56:07+00:00
- **Updated**: 2020-11-15 11:18:29+00:00
- **Authors**: Hyeongju Kim, Hyeonseung Lee, Woo Hyun Kang, Joun Yeop Lee, Nam Soo Kim
- **Comment**: 17 pages, 15figures
- **Journal**: None
- **Summary**: Flow-based generative models are composed of invertible transformations between two random variables of the same dimension. Therefore, flow-based models cannot be adequately trained if the dimension of the data distribution does not match that of the underlying target distribution. In this paper, we propose SoftFlow, a probabilistic framework for training normalizing flows on manifolds. To sidestep the dimension mismatch problem, SoftFlow estimates a conditional distribution of the perturbed input data instead of learning the data distribution directly. We experimentally show that SoftFlow can capture the innate structure of the manifold data and generate high-quality samples unlike the conventional flow-based models. Furthermore, we apply the proposed framework to 3D point clouds to alleviate the difficulty of forming thin structures for flow-based models. The proposed model for 3D point clouds, namely SoftPointFlow, can estimate the distribution of various shapes more accurately and achieves state-of-the-art performance in point cloud generation.



### Neural Architecture Search without Training
- **Arxiv ID**: http://arxiv.org/abs/2006.04647v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.04647v3)
- **Published**: 2020-06-08 14:53:56+00:00
- **Updated**: 2021-06-11 14:31:02+00:00
- **Authors**: Joseph Mellor, Jack Turner, Amos Storkey, Elliot J. Crowley
- **Comment**: Accepted at ICML 2021 for a long presentation
- **Journal**: None
- **Summary**: The time and effort involved in hand-designing deep neural networks is immense. This has prompted the development of Neural Architecture Search (NAS) techniques to automate this design. However, NAS algorithms tend to be slow and expensive; they need to train vast numbers of candidate networks to inform the search process. This could be alleviated if we could partially predict a network's trained accuracy from its initial state. In this work, we examine the overlap of activations between datapoints in untrained networks and motivate how this can give a measure which is usefully indicative of a network's trained performance. We incorporate this measure into a simple algorithm that allows us to search for powerful networks without any training in a matter of seconds on a single GPU, and verify its effectiveness on NAS-Bench-101, NAS-Bench-201, NATS-Bench, and Network Design Spaces. Our approach can be readily combined with more expensive search methods; we examine a simple adaptation of regularised evolutionary search. Code for reproducing our experiments is available at https://github.com/BayesWatch/nas-without-training.



### Graph-based Visual-Semantic Entanglement Network for Zero-shot Image Recognition
- **Arxiv ID**: http://arxiv.org/abs/2006.04648v2
- **DOI**: 10.1109/TMM.2021.3082292
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.04648v2)
- **Published**: 2020-06-08 14:54:08+00:00
- **Updated**: 2021-06-12 01:21:22+00:00
- **Authors**: Yang Hu, Guihua Wen, Adriane Chapman, Pei Yang, Mingnan Luo, Yingxue Xu, Dan Dai, Wendy Hall
- **Comment**: 15 pages, 11 figures, on IEEE Transactions on Multimedia
- **Journal**: [J]. IEEE Transactions on Multimedia, 2021
- **Summary**: Zero-shot learning uses semantic attributes to connect the search space of unseen objects. In recent years, although the deep convolutional network brings powerful visual modeling capabilities to the ZSL task, its visual features have severe pattern inertia and lack of representation of semantic relationships, which leads to severe bias and ambiguity. In response to this, we propose the Graph-based Visual-Semantic Entanglement Network to conduct graph modeling of visual features, which is mapped to semantic attributes by using a knowledge graph, it contains several novel designs: 1. it establishes a multi-path entangled network with the convolutional neural network (CNN) and the graph convolutional network (GCN), which input the visual features from CNN to GCN to model the implicit semantic relations, then GCN feedback the graph modeled information to CNN features; 2. it uses attribute word vectors as the target for the graph semantic modeling of GCN, which forms a self-consistent regression for graph modeling and supervise GCN to learn more personalized attribute relations; 3. it fuses and supplements the hierarchical visual-semantic features refined by graph modeling into visual embedding. Our method outperforms state-of-the-art approaches on multiple representative ZSL datasets: AwA2, CUB, and SUN by promoting the semantic linkage modelling of visual features.



### Unstructured Road Vanishing Point Detection Using the Convolutional Neural Network and Heatmap Regression
- **Arxiv ID**: http://arxiv.org/abs/2006.04691v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/2006.04691v1)
- **Published**: 2020-06-08 15:44:37+00:00
- **Updated**: 2020-06-08 15:44:37+00:00
- **Authors**: Yin-Bo Liu, Ming Zeng, Qing-Hao Meng
- **Comment**: 8 pages, 6 figures, under review
- **Journal**: None
- **Summary**: Unstructured road vanishing point (VP) detection is a challenging problem, especially in the field of autonomous driving. In this paper, we proposed a novel solution combining the convolutional neural network (CNN) and heatmap regression to detect unstructured road VP. The proposed algorithm firstly adopts a lightweight backbone, i.e., depthwise convolution modified HRNet, to extract hierarchical features of the unstructured road image. Then, three advanced strategies, i.e., multi-scale supervised learning, heatmap super-resolution, and coordinate regression techniques are utilized to achieve fast and high-precision unstructured road VP detection. The empirical results on Kong's dataset show that our proposed approach enjoys the highest detection accuracy compared with state-of-the-art methods under various conditions in real-time, achieving the highest speed of 33 fps.



### Multimodal Future Localization and Emergence Prediction for Objects in Egocentric View with a Reachability Prior
- **Arxiv ID**: http://arxiv.org/abs/2006.04700v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.04700v1)
- **Published**: 2020-06-08 15:57:26+00:00
- **Updated**: 2020-06-08 15:57:26+00:00
- **Authors**: Osama Makansi, Özgün Cicek, Kevin Buchicchio, Thomas Brox
- **Comment**: In CVPR 2020
- **Journal**: None
- **Summary**: In this paper, we investigate the problem of anticipating future dynamics, particularly the future location of other vehicles and pedestrians, in the view of a moving vehicle. We approach two fundamental challenges: (1) the partial visibility due to the egocentric view with a single RGB camera and considerable field-of-view change due to the egomotion of the vehicle; (2) the multimodality of the distribution of future states. In contrast to many previous works, we do not assume structural knowledge from maps. We rather estimate a reachability prior for certain classes of objects from the semantic map of the present image and propagate it into the future using the planned egomotion. Experiments show that the reachability prior combined with multi-hypotheses learning improves multimodal prediction of the future location of tracked objects and, for the first time, the emergence of new objects. We also demonstrate promising zero-shot transfer to unseen datasets. Source code is available at $\href{https://github.com/lmb-freiburg/FLN-EPN-RPN}{\text{this https URL.}}$



### ResKD: Residual-Guided Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/2006.04719v4
- **DOI**: 10.1109/TIP.2021.3066051
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.04719v4)
- **Published**: 2020-06-08 16:18:45+00:00
- **Updated**: 2021-03-09 03:35:20+00:00
- **Authors**: Xuewei Li, Songyuan Li, Bourahla Omar, Fei Wu, Xi Li
- **Comment**: The first two authors (Xuewei Li and Songyuan Li) contribute equally.
  Accepted to IEEE TRANSACTIONS ON IMAGE PROCESSING (TIP)
- **Journal**: None
- **Summary**: Knowledge distillation, aimed at transferring the knowledge from a heavy teacher network to a lightweight student network, has emerged as a promising technique for compressing neural networks. However, due to the capacity gap between the heavy teacher and the lightweight student, there still exists a significant performance gap between them. In this paper, we see knowledge distillation in a fresh light, using the knowledge gap, or the residual, between a teacher and a student as guidance to train a much more lightweight student, called a res-student. We combine the student and the res-student into a new student, where the res-student rectifies the errors of the former student. Such a residual-guided process can be repeated until the user strikes the balance between accuracy and cost. At inference time, we propose a sample-adaptive strategy to decide which res-students are not necessary for each sample, which can save computational cost. Experimental results show that we achieve competitive performance with 18.04$\%$, 23.14$\%$, 53.59$\%$, and 56.86$\%$ of the teachers' computational costs on the CIFAR-10, CIFAR-100, Tiny-ImageNet, and ImageNet datasets. Finally, we do thorough theoretical and empirical analysis for our method.



### Biomechanics-informed Neural Networks for Myocardial Motion Tracking in MRI
- **Arxiv ID**: http://arxiv.org/abs/2006.04725v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.04725v3)
- **Published**: 2020-06-08 16:29:13+00:00
- **Updated**: 2020-07-08 09:42:11+00:00
- **Authors**: Chen Qin, Shuo Wang, Chen Chen, Huaqi Qiu, Wenjia Bai, Daniel Rueckert
- **Comment**: The paper is early accepted by MICCAI 2020
- **Journal**: None
- **Summary**: Image registration is an ill-posed inverse problem which often requires regularisation on the solution space. In contrast to most of the current approaches which impose explicit regularisation terms such as smoothness, in this paper we propose a novel method that can implicitly learn biomechanics-informed regularisation. Such an approach can incorporate application-specific prior knowledge into deep learning based registration. Particularly, the proposed biomechanics-informed regularisation leverages a variational autoencoder (VAE) to learn a manifold for biomechanically plausible deformations and to implicitly capture their underlying properties via reconstructing biomechanical simulations. The learnt VAE regulariser then can be coupled with any deep learning based registration network to regularise the solution space to be biomechanically plausible. The proposed method is validated in the context of myocardial motion tracking on 2D stacks of cardiac MRI data from two different datasets. The results show that it can achieve better performance against other competing methods in terms of motion tracking accuracy and has the ability to learn biomechanical properties such as incompressibility and strains. The method has also been shown to have better generalisability to unseen domains compared with commonly used L2 regularisation schemes.



### Unsupervised Transfer Learning with Self-Supervised Remedy
- **Arxiv ID**: http://arxiv.org/abs/2006.04737v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.04737v1)
- **Published**: 2020-06-08 16:42:17+00:00
- **Updated**: 2020-06-08 16:42:17+00:00
- **Authors**: Jiabo Huang, Shaogang Gong
- **Comment**: None
- **Journal**: None
- **Summary**: Generalising deep networks to novel domains without manual labels is challenging to deep learning. This problem is intrinsically difficult due to unpredictable changing nature of imagery data distributions in novel domains. Pre-learned knowledge does not transfer well without making strong assumptions about the learned and the novel domains. Different methods have been studied to address the underlying problem based on different assumptions, e.g. from domain adaptation to zero-shot and few-shot learning. In this work, we address this problem by transfer clustering that aims to learn a discriminative latent space of the unlabelled target data in a novel domain by knowledge transfer from labelled related domains. Specifically, we want to leverage relative (pairwise) imagery information, which is freely available and intrinsic to a target domain, to model the target domain image distribution characteristics as well as the prior-knowledge learned from related labelled domains to enable more discriminative clustering of unlabelled target data. Our method mitigates nontransferrable prior-knowledge by self-supervision, benefiting from both transfer and self-supervised learning. Extensive experiments on four datasets for image clustering tasks reveal the superiority of our model over the state-of-the-art transfer clustering techniques. We further demonstrate its competitive transferability on four zero-shot learning benchmarks.



### Motion Prediction using Trajectory Sets and Self-Driving Domain Knowledge
- **Arxiv ID**: http://arxiv.org/abs/2006.04767v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.RO, stat.ML, 68T07 (Primary) 68T40, 68T45 (Secondary), I.2.6; I.2.9; I.2.10; I.5
- **Links**: [PDF](http://arxiv.org/pdf/2006.04767v2)
- **Published**: 2020-06-08 17:37:15+00:00
- **Updated**: 2021-01-13 20:41:54+00:00
- **Authors**: Freddy A. Boulton, Elena Corina Grigore, Eric M. Wolff
- **Comment**: None
- **Journal**: None
- **Summary**: Predicting the future motion of vehicles has been studied using various techniques, including stochastic policies, generative models, and regression. Recent work has shown that classification over a trajectory set, which approximates possible motions, achieves state-of-the-art performance and avoids issues like mode collapse. However, map information and the physical relationships between nearby trajectories is not fully exploited in this formulation. We build on classification-based approaches to motion prediction by adding an auxiliary loss that penalizes off-road predictions. This auxiliary loss can easily be pretrained using only map information (e.g., off-road area), which significantly improves performance on small datasets. We also investigate weighted cross-entropy losses to capture spatial-temporal relationships among trajectories. Our final contribution is a detailed comparison of classification and ordinal regression on two public self-driving datasets.



### Novel Perception Algorithmic Framework For Object Identification and Tracking In Autonomous Navigation
- **Arxiv ID**: http://arxiv.org/abs/2006.04859v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2006.04859v1)
- **Published**: 2020-06-08 18:21:40+00:00
- **Updated**: 2020-06-08 18:21:40+00:00
- **Authors**: Suryansh Saxena, Isaac K Isukapati
- **Comment**: None
- **Journal**: None
- **Summary**: This paper introduces a novel perception framework that has the ability to identify and track objects in autonomous vehicle's field of view. The proposed algorithms don't require any training for achieving this goal. The framework makes use of ego-vehicle's pose estimation and a KD-Tree-based segmentation algorithm to generate object clusters. In turn, using a VFH technique, the geometry of each identified object cluster is translated into a multi-modal PDF and a motion model is initiated with every new object cluster for the purpose of robust spatio-temporal tracking. The methodology further uses statistical properties of high-dimensional probability density functions and Bayesian motion model estimates to identify and track objects from frame to frame. The effectiveness of the methodology is tested on a KITTI dataset. The results show that the median tracking accuracy is around 91% with an end-to-end computational time of 153 milliseconds



### DoubleU-Net: A Deep Convolutional Neural Network for Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2006.04868v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.04868v2)
- **Published**: 2020-06-08 18:38:24+00:00
- **Updated**: 2020-06-27 15:40:40+00:00
- **Authors**: Debesh Jha, Michael A. Riegler, Dag Johansen, Pål Halvorsen, Håvard D. Johansen
- **Comment**: None
- **Journal**: None
- **Summary**: Semantic image segmentation is the process of labeling each pixel of an image with its corresponding class. An encoder-decoder based approach, like U-Net and its variants, is a popular strategy for solving medical image segmentation tasks. To improve the performance of U-Net on various segmentation tasks, we propose a novel architecture called DoubleU-Net, which is a combination of two U-Net architectures stacked on top of each other. The first U-Net uses a pre-trained VGG-19 as the encoder, which has already learned features from ImageNet and can be transferred to another task easily. To capture more semantic information efficiently, we added another U-Net at the bottom. We also adopt Atrous Spatial Pyramid Pooling (ASPP) to capture contextual information within the network. We have evaluated DoubleU-Net using four medical segmentation datasets, covering various imaging modalities such as colonoscopy, dermoscopy, and microscopy. Experiments on the MICCAI 2015 segmentation challenge, the CVC-ClinicDB, the 2018 Data Science Bowl challenge, and the Lesion boundary segmentation datasets demonstrate that the DoubleU-Net outperforms U-Net and the baseline models. Moreover, DoubleU-Net produces more accurate segmentation masks, especially in the case of the CVC-ClinicDB and MICCAI 2015 segmentation challenge datasets, which have challenging images such as smaller and flat polyps. These results show the improvement over the existing U-Net model. The encouraging results, produced on various medical image segmentation datasets, show that DoubleU-Net can be used as a strong baseline for both medical image segmentation and cross-dataset evaluation testing to measure the generalizability of Deep Learning (DL) models.



### Skinning a Parameterization of Three-Dimensional Space for Neural Network Cloth
- **Arxiv ID**: http://arxiv.org/abs/2006.04874v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.04874v1)
- **Published**: 2020-06-08 18:53:03+00:00
- **Updated**: 2020-06-08 18:53:03+00:00
- **Authors**: Jane Wu, Zhenglin Geng, Hui Zhou, Ronald Fedkiw
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel learning framework for cloth deformation by embedding virtual cloth into a tetrahedral mesh that parametrizes the volumetric region of air surrounding the underlying body. In order to maintain this volumetric parameterization during character animation, the tetrahedral mesh is constrained to follow the body surface as it deforms. We embed the cloth mesh vertices into this parameterization of three-dimensional space in order to automatically capture much of the nonlinear deformation due to both joint rotations and collisions. We then train a convolutional neural network to recover ground truth deformation by learning cloth embedding offsets for each skeletal pose. Our experiments show significant improvement over learning cloth offsets from body surface parameterizations, both quantitatively and visually, with prior state of the art having a mean error five standard deviations higher than ours. Moreover, our results demonstrate the efficacy of a general learning paradigm where high-frequency details can be embedded into low-frequency parameterizations.



### KiU-Net: Towards Accurate Segmentation of Biomedical Images using Over-complete Representations
- **Arxiv ID**: http://arxiv.org/abs/2006.04878v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.04878v2)
- **Published**: 2020-06-08 18:59:24+00:00
- **Updated**: 2020-07-08 21:20:48+00:00
- **Authors**: Jeya Maria Jose, Vishwanath Sindagi, Ilker Hacihaliloglu, Vishal M. Patel
- **Comment**: Accepted at MICCAI 2020
- **Journal**: None
- **Summary**: Due to its excellent performance, U-Net is the most widely used backbone architecture for biomedical image segmentation in the recent years. However, in our studies, we observe that there is a considerable performance drop in the case of detecting smaller anatomical landmarks with blurred noisy boundaries. We analyze this issue in detail, and address it by proposing an over-complete architecture (Ki-Net) which involves projecting the data onto higher dimensions (in the spatial sense). This network, when augmented with U-Net, results in significant improvements in the case of segmenting small anatomical landmarks and blurred noisy boundaries while obtaining better overall performance. Furthermore, the proposed network has additional benefits like faster convergence and fewer number of parameters. We evaluate the proposed method on the task of brain anatomy segmentation from 2D Ultrasound (US) of preterm neonates, and achieve an improvement of around 4% in terms of the DICE accuracy and Jaccard index as compared to the standard-U-Net, while outperforming the recent best methods by 2%. Code: https://github.com/jeya-maria-jose/KiU-Net-pytorch .



### Probabilistic Semantic Mapping for Urban Autonomous Driving Applications
- **Arxiv ID**: http://arxiv.org/abs/2006.04894v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.04894v2)
- **Published**: 2020-06-08 19:29:09+00:00
- **Updated**: 2020-09-11 17:29:49+00:00
- **Authors**: David Paz, Hengyuan Zhang, Qinru Li, Hao Xiang, Henrik Christensen
- **Comment**: 6 pages, 7 figures, IROS 2020
- **Journal**: None
- **Summary**: Recent advancements in statistical learning and computational abilities have enabled autonomous vehicle technology to develop at a much faster rate. While many of the architectures previously introduced are capable of operating under highly dynamic environments, many of these are constrained to smaller-scale deployments, require constant maintenance due to the associated scalability cost with high-definition (HD) maps, and involve tedious manual labeling. As an attempt to tackle this problem, we propose to fuse image and pre-built point cloud map information to perform automatic and accurate labeling of static landmarks such as roads, sidewalks, crosswalks, and lanes. The method performs semantic segmentation on 2D images, associates the semantic labels with point cloud maps to accurately localize them in the world, and leverages the confusion matrix formulation to construct a probabilistic semantic map in bird's eye view from semantic point clouds. Experiments from data collected in an urban environment show that this model is able to predict most road features and can be extended for automatically incorporating road features into HD maps with potential future work directions.



### Reposing Humans by Warping 3D Features
- **Arxiv ID**: http://arxiv.org/abs/2006.04898v1
- **DOI**: 10.1109/CVPRW50498.2020.00530
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.04898v1)
- **Published**: 2020-06-08 19:31:02+00:00
- **Updated**: 2020-06-08 19:31:02+00:00
- **Authors**: Markus Knoche, István Sárándi, Bastian Leibe
- **Comment**: Accepted at CVPR 2020 Workshop on Human-Centric Image/Video Synthesis
- **Journal**: Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition (CVPR) Workshops, 2020, pp. 1044-1045
- **Summary**: We address the problem of reposing an image of a human into any desired novel pose. This conditional image-generation task requires reasoning about the 3D structure of the human, including self-occluded body parts. Most prior works are either based on 2D representations or require fitting and manipulating an explicit 3D body mesh. Based on the recent success in deep learning-based volumetric representations, we propose to implicitly learn a dense feature volume from human images, which lends itself to simple and intuitive manipulation through explicit geometric warping. Once the latent feature volume is warped according to the desired pose change, the volume is mapped back to RGB space by a convolutional decoder. Our state-of-the-art results on the DeepFashion and the iPER benchmarks indicate that dense volumetric human representations are worth investigating in more detail.



### What Matters in Unsupervised Optical Flow
- **Arxiv ID**: http://arxiv.org/abs/2006.04902v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.04902v2)
- **Published**: 2020-06-08 19:36:26+00:00
- **Updated**: 2020-08-14 13:39:34+00:00
- **Authors**: Rico Jonschkowski, Austin Stone, Jonathan T. Barron, Ariel Gordon, Kurt Konolige, Anelia Angelova
- **Comment**: Accepted at ECCV 2020 (Oral). Source code is available at
  https://github.com/google-research/google-research/tree/master/uflow
- **Journal**: None
- **Summary**: We systematically compare and analyze a set of key components in unsupervised optical flow to identify which photometric loss, occlusion handling, and smoothness regularization is most effective. Alongside this investigation we construct a number of novel improvements to unsupervised flow models, such as cost volume normalization, stopping the gradient at the occlusion mask, encouraging smoothness before upsampling the flow field, and continual self-supervision with image resizing. By combining the results of our investigation with our improved model components, we are able to present a new unsupervised flow technique that significantly outperforms the previous unsupervised state-of-the-art and performs on par with supervised FlowNet2 on the KITTI 2015 dataset, while also being significantly simpler than related approaches.



### A Self-supervised Approach for Adversarial Robustness
- **Arxiv ID**: http://arxiv.org/abs/2006.04924v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.04924v1)
- **Published**: 2020-06-08 20:42:39+00:00
- **Updated**: 2020-06-08 20:42:39+00:00
- **Authors**: Muzammal Naseer, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, Fatih Porikli
- **Comment**: CVPR-2020 (Oral). Code this http
  https://github.com/Muzammal-Naseer/NRP}
- **Journal**: None
- **Summary**: Adversarial examples can cause catastrophic mistakes in Deep Neural Network (DNNs) based vision systems e.g., for classification, segmentation and object detection. The vulnerability of DNNs against such attacks can prove a major roadblock towards their real-world deployment. Transferability of adversarial examples demand generalizable defenses that can provide cross-task protection. Adversarial training that enhances robustness by modifying target model's parameters lacks such generalizability. On the other hand, different input processing based defenses fall short in the face of continuously evolving attacks. In this paper, we take the first step to combine the benefits of both approaches and propose a self-supervised adversarial training mechanism in the input space. By design, our defense is a generalizable approach and provides significant robustness against the \textbf{unseen} adversarial attacks (\eg by reducing the success rate of translation-invariant \textbf{ensemble} attack from 82.6\% to 31.9\% in comparison to previous state-of-the-art). It can be deployed as a plug-and-play solution to protect a variety of vision systems, as we demonstrate for the case of classification, segmentation and detection. Code is available at: {\small\url{https://github.com/Muzammal-Naseer/NRP}}.



### Pixel-Wise Motion Deblurring of Thermal Videos
- **Arxiv ID**: http://arxiv.org/abs/2006.04973v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2006.04973v1)
- **Published**: 2020-06-08 22:35:12+00:00
- **Updated**: 2020-06-08 22:35:12+00:00
- **Authors**: Manikandasriram Srinivasan Ramanagopal, Zixu Zhang, Ram Vasudevan, Matthew Johnson-Roberson
- **Comment**: 10 pages, 8 figures, Accepted to Robotics: Science and Systems 2020
- **Journal**: None
- **Summary**: Uncooled microbolometers can enable robots to see in the absence of visible illumination by imaging the "heat" radiated from the scene. Despite this ability to see in the dark, these sensors suffer from significant motion blur. This has limited their application on robotic systems. As described in this paper, this motion blur arises due to the thermal inertia of each pixel. This has meant that traditional motion deblurring techniques, which rely on identifying an appropriate spatial blur kernel to perform spatial deconvolution, are unable to reliably perform motion deblurring on thermal camera images. To address this problem, this paper formulates reversing the effect of thermal inertia at a single pixel as a Least Absolute Shrinkage and Selection Operator (LASSO) problem which we can solve rapidly using a quadratic programming solver. By leveraging sparsity and a high frame rate, this pixel-wise LASSO formulation is able to recover motion deblurred frames of thermal videos without using any spatial information. To compare its quality against state-of-the-art visible camera based deblurring methods, this paper evaluated the performance of a family of pre-trained object detectors on a set of images restored by different deblurring algorithms. All evaluated object detectors performed systematically better on images restored by the proposed algorithm rather than any other tested, state-of-the-art methods.



### Object Segmentation Without Labels with Large-Scale Generative Models
- **Arxiv ID**: http://arxiv.org/abs/2006.04988v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.04988v2)
- **Published**: 2020-06-08 23:30:43+00:00
- **Updated**: 2021-06-11 09:49:40+00:00
- **Authors**: Andrey Voynov, Stanislav Morozov, Artem Babenko
- **Comment**: None
- **Journal**: None
- **Summary**: The recent rise of unsupervised and self-supervised learning has dramatically reduced the dependency on labeled data, providing effective image representations for transfer to downstream vision tasks. Furthermore, recent works employed these representations in a fully unsupervised setup for image classification, reducing the need for human labels on the fine-tuning stage as well. This work demonstrates that large-scale unsupervised models can also perform a more challenging object segmentation task, requiring neither pixel-level nor image-level labeling. Namely, we show that recent unsupervised GANs allow to differentiate between foreground/background pixels, providing high-quality saliency masks. By extensive comparison on standard benchmarks, we outperform existing unsupervised alternatives for object segmentation, achieving new state-of-the-art.



### Beyond Triplet Loss: Meta Prototypical N-tuple Loss for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/2006.04991v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.04991v2)
- **Published**: 2020-06-08 23:34:08+00:00
- **Updated**: 2021-09-24 08:55:05+00:00
- **Authors**: Zhizheng Zhang, Cuiling Lan, Wenjun Zeng, Zhibo Chen, Shih-Fu Chang
- **Comment**: Accepted by IEEE Transactions on Multimedia
- **Journal**: None
- **Summary**: Person Re-identification (ReID) aims at matching a person of interest across images. In convolutional neural network (CNN) based approaches, loss design plays a vital role in pulling closer features of the same identity and pushing far apart features of different identities. In recent years, triplet loss achieves superior performance and is predominant in ReID. However, triplet loss considers only three instances of two classes in per-query optimization (with an anchor sample as query) and it is actually equivalent to a two-class classification. There is a lack of loss design which enables the joint optimization of multiple instances (of multiple classes) within per-query optimization for person ReID. In this paper, we introduce a multi-class classification loss, i.e., N-tuple loss, to jointly consider multiple (N) instances for per-query optimization. This in fact aligns better with the ReID test/inference process, which conducts the ranking/comparisons among multiple instances. Furthermore, for more efficient multi-class classification, we propose a new meta prototypical N-tuple loss. With the multi-class classification incorporated, our model achieves the state-of-the-art performance on the benchmark person ReID datasets.



