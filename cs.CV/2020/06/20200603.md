# Arxiv Papers in cs.CV on 2020-06-03
### PILArNet: Public Dataset for Particle Imaging Liquid Argon Detectors in High Energy Physics
- **Arxiv ID**: http://arxiv.org/abs/2006.01993v1
- **DOI**: None
- **Categories**: **physics.ins-det**, cs.CV, cs.LG, hep-ex
- **Links**: [PDF](http://arxiv.org/pdf/2006.01993v1)
- **Published**: 2020-06-03 00:36:04+00:00
- **Updated**: 2020-06-03 00:36:04+00:00
- **Authors**: Corey Adams, Kazuhiro Terao, Taritree Wongjirad
- **Comment**: None
- **Journal**: None
- **Summary**: Rapid advancement of machine learning solutions has often coincided with the production of a test public data set. Such datasets reduce the largest barrier to entry for tackling a problem -- procuring data -- while also providing a benchmark to compare different solutions. Furthermore, large datasets have been used to train high-performing feature finders which are then used in new approaches to problems beyond that initially defined. In order to encourage the rapid development in the analysis of data collected using liquid argon time projection chambers, a class of particle detectors used in high energy physics experiments, we have produced the PILArNet, first 2D and 3D open dataset to be used for a couple of key analysis tasks. The initial dataset presented in this paper contains 300,000 samples simulated and recorded in three different volume sizes. The dataset is stored efficiently in sparse 2D and 3D matrix format with auxiliary information about simulated particles in the volume, and is made available for public research use. In this paper we describe the dataset, tasks, and the method used to procure the sample.



### MultiXNet: Multiclass Multistage Multimodal Motion Prediction
- **Arxiv ID**: http://arxiv.org/abs/2006.02000v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.02000v4)
- **Published**: 2020-06-03 01:01:48+00:00
- **Updated**: 2021-05-24 04:31:50+00:00
- **Authors**: Nemanja Djuric, Henggang Cui, Zhaoen Su, Shangxuan Wu, Huahua Wang, Fang-Chieh Chou, Luisa San Martin, Song Feng, Rui Hu, Yang Xu, Alyssa Dayan, Sidney Zhang, Brian C. Becker, Gregory P. Meyer, Carlos Vallespi-Gonzalez, Carl K. Wellington
- **Comment**: Accepted for publication at IEEE Intelligent Vehicles Symposium (IV)
  2021
- **Journal**: None
- **Summary**: One of the critical pieces of the self-driving puzzle is understanding the surroundings of a self-driving vehicle (SDV) and predicting how these surroundings will change in the near future. To address this task we propose MultiXNet, an end-to-end approach for detection and motion prediction based directly on lidar sensor data. This approach builds on prior work by handling multiple classes of traffic actors, adding a jointly trained second-stage trajectory refinement step, and producing a multimodal probability distribution over future actor motion that includes both multiple discrete traffic behaviors and calibrated continuous position uncertainties. The method was evaluated on large-scale, real-world data collected by a fleet of SDVs in several cities, with the results indicating that it outperforms existing state-of-the-art approaches.



### Open-Set Recognition with Gaussian Mixture Variational Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/2006.02003v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.02003v1)
- **Published**: 2020-06-03 01:15:19+00:00
- **Updated**: 2020-06-03 01:15:19+00:00
- **Authors**: Alexander Cao, Yuan Luo, Diego Klabjan
- **Comment**: 12 pages including 8 figures and 4 tables, plus 6 pages of
  supplementary material
- **Journal**: None
- **Summary**: In inference, open-set classification is to either classify a sample into a known class from training or reject it as an unknown class. Existing deep open-set classifiers train explicit closed-set classifiers, in some cases disjointly utilizing reconstruction, which we find dilutes the latent representation's ability to distinguish unknown classes. In contrast, we train our model to cooperatively learn reconstruction and perform class-based clustering in the latent space. With this, our Gaussian mixture variational autoencoder (GMVAE) achieves more accurate and robust open-set classification results, with an average F1 improvement of 29.5%, through extensive experiments aided by analytical results.



### Image Classification in the Dark using Quanta Image Sensors
- **Arxiv ID**: http://arxiv.org/abs/2006.02026v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.02026v3)
- **Published**: 2020-06-03 03:39:07+00:00
- **Updated**: 2020-07-16 20:22:46+00:00
- **Authors**: Abhiram Gnanasambandam, Stanley H. Chan
- **Comment**: Published in the 16th European Conference on Computer Vision (ECCV)
  2020
- **Journal**: None
- **Summary**: State-of-the-art image classifiers are trained and tested using well-illuminated images. These images are typically captured by CMOS image sensors with at least tens of photons per pixel. However, in dark environments when the photon flux is low, image classification becomes difficult because the measured signal is suppressed by noise. In this paper, we present a new low-light image classification solution using Quanta Image Sensors (QIS). QIS are a new type of image sensors that possess photon counting ability without compromising on pixel size and spatial resolution. Numerous studies over the past decade have demonstrated the feasibility of QIS for low-light imaging, but their usage for image classification has not been studied. This paper fills the gap by presenting a student-teacher learning scheme which allows us to classify the noisy QIS raw data. We show that with student-teacher learning, we are able to achieve image classification at a photon level of one photon per pixel or lower. Experimental results verify the effectiveness of the proposed method compared to existing solutions.



### Nested Scale Editing for Conditional Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2006.02038v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.02038v1)
- **Published**: 2020-06-03 04:29:21+00:00
- **Updated**: 2020-06-03 04:29:21+00:00
- **Authors**: Lingzhi Zhang, Jiancong Wang, Yinshuang Xu, Jie Min, Tarmily Wen, James C. Gee, Jianbo Shi
- **Comment**: None
- **Journal**: None
- **Summary**: We propose an image synthesis approach that provides stratified navigation in the latent code space. With a tiny amount of partial or very low-resolution image, our approach can consistently out-perform state-of-the-art counterparts in terms of generating the closest sampled image to the ground truth. We achieve this through scale-independent editing while expanding scale-specific diversity. Scale-independence is achieved with a nested scale disentanglement loss. Scale-specific diversity is created by incorporating a progressive diversification constraint. We introduce semantic persistency across the scales by sharing common latent codes. Together they provide better control of the image synthesis process. We evaluate the effectiveness of our proposed approach through various tasks, including image outpainting, image superresolution, and cross-domain image translation.



### FBNetV3: Joint Architecture-Recipe Search using Predictor Pretraining
- **Arxiv ID**: http://arxiv.org/abs/2006.02049v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2006.02049v3)
- **Published**: 2020-06-03 05:20:21+00:00
- **Updated**: 2021-03-30 14:54:08+00:00
- **Authors**: Xiaoliang Dai, Alvin Wan, Peizhao Zhang, Bichen Wu, Zijian He, Zhen Wei, Kan Chen, Yuandong Tian, Matthew Yu, Peter Vajda, Joseph E. Gonzalez
- **Comment**: None
- **Journal**: None
- **Summary**: Neural Architecture Search (NAS) yields state-of-the-art neural networks that outperform their best manually-designed counterparts. However, previous NAS methods search for architectures under one set of training hyper-parameters (i.e., a training recipe), overlooking superior architecture-recipe combinations. To address this, we present Neural Architecture-Recipe Search (NARS) to search both (a) architectures and (b) their corresponding training recipes, simultaneously. NARS utilizes an accuracy predictor that scores architecture and training recipes jointly, guiding both sample selection and ranking. Furthermore, to compensate for the enlarged search space, we leverage "free" architecture statistics (e.g., FLOP count) to pretrain the predictor, significantly improving its sample efficiency and prediction reliability. After training the predictor via constrained iterative optimization, we run fast evolutionary searches in just CPU minutes to generate architecture-recipe pairs for a variety of resource constraints, called FBNetV3. FBNetV3 makes up a family of state-of-the-art compact neural networks that outperform both automatically and manually-designed competitors. For example, FBNetV3 matches both EfficientNet and ResNeSt accuracy on ImageNet with up to 2.0x and 7.1x fewer FLOPs, respectively. Furthermore, FBNetV3 yields significant performance gains for downstream object detection tasks, improving mAP despite 18% fewer FLOPs and 34% fewer parameters than EfficientNet-based equivalents.



### Reference-guided Face Component Editing
- **Arxiv ID**: http://arxiv.org/abs/2006.02051v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.02051v2)
- **Published**: 2020-06-03 05:34:54+00:00
- **Updated**: 2020-07-14 13:37:59+00:00
- **Authors**: Qiyao Deng, Jie Cao, Yunfan Liu, Zhenhua Chai, Qi Li, Zhenan Sun
- **Comment**: None
- **Journal**: None
- **Summary**: Face portrait editing has achieved great progress in recent years. However, previous methods either 1) operate on pre-defined face attributes, lacking the flexibility of controlling shapes of high-level semantic facial components (e.g., eyes, nose, mouth), or 2) take manually edited mask or sketch as an intermediate representation for observable changes, but such additional input usually requires extra efforts to obtain. To break the limitations (e.g. shape, mask or sketch) of the existing methods, we propose a novel framework termed r-FACE (Reference-guided FAce Component Editing) for diverse and controllable face component editing with geometric changes. Specifically, r-FACE takes an image inpainting model as the backbone, utilizing reference images as conditions for controlling the shape of face components. In order to encourage the framework to concentrate on the target face components, an example-guided attention module is designed to fuse attention features and the target face component features extracted from the reference image. Through extensive experimental validation and comparisons, we verify the effectiveness of the proposed framework.



### PLG-IN: Pluggable Geometric Consistency Loss with Wasserstein Distance in Monocular Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/2006.02068v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2006.02068v2)
- **Published**: 2020-06-03 06:50:42+00:00
- **Updated**: 2020-08-06 02:55:13+00:00
- **Authors**: Noriaki Hirose, Satoshi Koide, Keisuke Kawano, Ruho Kondo
- **Comment**: 13 pages, 8 figures, 3 tables
- **Journal**: None
- **Summary**: We propose a novel objective for penalizing geometric inconsistencies to improve the depth and pose estimation performance of monocular camera images. Our objective is designed using the Wasserstein distance between two point clouds, estimated from images with different camera poses. The Wasserstein distance can impose a soft and symmetric coupling between two point clouds, which suitably maintains geometric constraints and results in a differentiable objective. By adding our objective to the those of other state-of-the-art methods, we can effectively penalize geometric inconsistencies and obtain highly accurate depth and pose estimations. Our proposed method is evaluated using the KITTI dataset.



### GFPNet: A Deep Network for Learning Shape Completion in Generic Fitted Primitives
- **Arxiv ID**: http://arxiv.org/abs/2006.02098v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2006.02098v1)
- **Published**: 2020-06-03 08:29:27+00:00
- **Updated**: 2020-06-03 08:29:27+00:00
- **Authors**: Tiberiu Cocias, Alexandru Razvant, Sorin Grigorescu
- **Comment**: 8 pages, 14 figures, IEEE Robotics and Automation Letters. Preprint
  Version. Accepted May, 2020
- **Journal**: None
- **Summary**: In this paper, we propose an object reconstruction apparatus that uses the so-called Generic Primitives (GP) to complete shapes. A GP is a 3D point cloud depicting a generalized shape of a class of objects. To reconstruct the objects in a scene we first fit a GP onto each occluded object to obtain an initial raw structure. Secondly, we use a model-based deformation technique to fold the surface of the GP over the occluded object. The deformation model is encoded within the layers of a Deep Neural Network (DNN), coined GFPNet. The objective of the network is to transfer the particularities of the object from the scene to the raw volume represented by the GP. We show that GFPNet competes with state of the art shape completion methods by providing performance results on the ModelNet and KITTI benchmarking datasets.



### Automatic Setting of DNN Hyper-Parameters by Mixing Bayesian Optimization and Tuning Rules
- **Arxiv ID**: http://arxiv.org/abs/2006.02105v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.02105v1)
- **Published**: 2020-06-03 08:53:48+00:00
- **Updated**: 2020-06-03 08:53:48+00:00
- **Authors**: Michele Fraccaroli, Evelina Lamma, Fabrizio Riguzzi
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning techniques play an increasingly important role in industrial and research environments due to their outstanding results. However, the large number of hyper-parameters to be set may lead to errors if they are set manually. The state-of-the-art hyper-parameters tuning methods are grid search, random search, and Bayesian Optimization. The first two methods are expensive because they try, respectively, all possible combinations and random combinations of hyper-parameters. Bayesian Optimization, instead, builds a surrogate model of the objective function, quantifies the uncertainty in the surrogate using Gaussian Process Regression and uses an acquisition function to decide where to sample the new set of hyper-parameters. This work faces the field of Hyper-Parameters Optimization (HPO). The aim is to improve Bayesian Optimization applied to Deep Neural Networks. For this goal, we build a new algorithm for evaluating and analyzing the results of the network on the training and validation sets and use a set of tuning rules to add new hyper-parameters and/or to reduce the hyper-parameter search space to select a better combination.



### Self-Supervised Localisation between Range Sensors and Overhead Imagery
- **Arxiv ID**: http://arxiv.org/abs/2006.02108v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.02108v2)
- **Published**: 2020-06-03 08:58:54+00:00
- **Updated**: 2020-09-23 12:49:46+00:00
- **Authors**: Tim Y. Tang, Daniele De Martini, Shangzhe Wu, Paul Newman
- **Comment**: Robotics: Science and Systems (RSS) 2020
- **Journal**: None
- **Summary**: Publicly available satellite imagery can be an ubiquitous, cheap, and powerful tool for vehicle localisation when a prior sensor map is unavailable. However, satellite images are not directly comparable to data from ground range sensors because of their starkly different modalities. We present a learned metric localisation method that not only handles the modality difference, but is cheap to train, learning in a self-supervised fashion without metrically accurate ground truth. By evaluating across multiple real-world datasets, we demonstrate the robustness and versatility of our method for various sensor configurations. We pay particular attention to the use of millimetre wave radar, which, owing to its complex interaction with the scene and its immunity to weather and lighting, makes for a compelling and valuable use case.



### From Real to Synthetic and Back: Synthesizing Training Data for Multi-Person Scene Understanding
- **Arxiv ID**: http://arxiv.org/abs/2006.02110v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.02110v1)
- **Published**: 2020-06-03 09:02:06+00:00
- **Updated**: 2020-06-03 09:02:06+00:00
- **Authors**: Igor Kviatkovsky, Nadav Bhonker, Gerard Medioni
- **Comment**: None
- **Journal**: None
- **Summary**: We present a method for synthesizing naturally looking images of multiple people interacting in a specific scenario. These images benefit from the advantages of synthetic data: being fully controllable and fully annotated with any type of standard or custom-defined ground truth. To reduce the synthetic-to-real domain gap, we introduce a pipeline consisting of the following steps: 1) we render scenes in a context modeled after the real world, 2) we train a human parsing model on the synthetic images, 3) we use the model to estimate segmentation maps for real images, 4) we train a conditional generative adversarial network (cGAN) to learn the inverse mapping -- from a segmentation map to a real image, and 5) given new synthetic segmentation maps, we use the cGAN to generate realistic images. An illustration of our pipeline is presented in Figure 2. We use the generated data to train a multi-task model on the challenging tasks of UV mapping and dense depth estimation. We demonstrate the value of the data generation and the trained model, both quantitatively and qualitatively on the CMU Panoptic Dataset.



### Multiple Generative Adversarial Networks Analysis for Predicting Photographers' Retouching
- **Arxiv ID**: http://arxiv.org/abs/2006.02921v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.02921v1)
- **Published**: 2020-06-03 10:10:01+00:00
- **Updated**: 2020-06-03 10:10:01+00:00
- **Authors**: Marc Bickel, Samuel Dubuis, Sébastien Gachoud
- **Comment**: 15 pages, 34 figures https://github.com/MarcBickel/CS-413
- **Journal**: None
- **Summary**: Anyone can take a photo, but not everybody has the ability to retouch their pictures and obtain result close to professional. Since it is not possible to ask experts to retouch thousands of pictures, we thought about teaching a piece of software how to reproduce the work of those said experts. This study aims to explore the possibility to use deep learning methods and more specifically, generative adversarial networks (GANs), to mimic artists' retouching and find which one of the studied models provides the best results.



### Interpolation-based semi-supervised learning for object detection
- **Arxiv ID**: http://arxiv.org/abs/2006.02158v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.02158v2)
- **Published**: 2020-06-03 10:53:44+00:00
- **Updated**: 2020-12-29 22:41:50+00:00
- **Authors**: Jisoo Jeong, Vikas Verma, Minsung Hyun, Juho Kannala, Nojun Kwak
- **Comment**: None
- **Journal**: None
- **Summary**: Despite the data labeling cost for the object detection tasks being substantially more than that of the classification tasks, semi-supervised learning methods for object detection have not been studied much. In this paper, we propose an Interpolation-based Semi-supervised learning method for object Detection (ISD), which considers and solves the problems caused by applying conventional Interpolation Regularization (IR) directly to object detection. We divide the output of the model into two types according to the objectness scores of both original patches that are mixed in IR. Then, we apply a separate loss suitable for each type in an unsupervised manner. The proposed losses dramatically improve the performance of semi-supervised learning as well as supervised learning. In the supervised learning setting, our method improves the baseline methods by a significant margin. In the semi-supervised learning setting, our algorithm improves the performance on a benchmark dataset (PASCAL VOC and MSCOCO) in a benchmark architecture (SSD).



### Multi-Temporal Scene Classification and Scene Change Detection with Correlation based Fusion
- **Arxiv ID**: http://arxiv.org/abs/2006.02176v1
- **DOI**: 10.1109/TIP.2020.3039328
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.02176v1)
- **Published**: 2020-06-03 11:24:31+00:00
- **Updated**: 2020-06-03 11:24:31+00:00
- **Authors**: Lixiang Ru, Bo Du, Chen Wu
- **Comment**: submitted
- **Journal**: None
- **Summary**: Classifying multi-temporal scene land-use categories and detecting their semantic scene-level changes for imagery covering urban regions could straightly reflect the land-use transitions. Existing methods for scene change detection rarely focus on the temporal correlation of bi-temporal features, and are mainly evaluated on small scale scene change detection datasets. In this work, we proposed a CorrFusion module that fuses the highly correlated components in bi-temporal feature embeddings. We firstly extracts the deep representations of the bi-temporal inputs with deep convolutional networks. Then the extracted features will be projected into a lower dimension space to computed the instance-level correlation. The cross-temporal fusion will be performed based on the computed correlation in CorrFusion module. The final scene classification are obtained with softmax activation layers. In the objective function, we introduced a new formulation for calculating the temporal correlation. The detailed derivation of backpropagation gradients for the proposed module is also given in this paper. Besides, we presented a much larger scale scene change detection dataset and conducted experiments on this dataset. The experimental results demonstrated that our proposed CorrFusion module could remarkably improve the multi-temporal scene classification and scene change detection results.



### Low-light Image Enhancement Using the Cell Vibration Model
- **Arxiv ID**: http://arxiv.org/abs/2006.02271v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.02271v2)
- **Published**: 2020-06-03 13:39:10+00:00
- **Updated**: 2022-05-15 03:57:00+00:00
- **Authors**: Xiaozhou Lei, Zixiang Fei, Wenju Zhou, Huiyu Zhou, Minrui Fei
- **Comment**: This paper has been accepted by IEEE Transactions on Multimedia (IEEE
  TMM) on May 12, 2022. The accepted version can be downloaded in arXiv
- **Journal**: None
- **Summary**: Low light very likely leads to the degradation of an image's quality and even causes visual task failures. Existing image enhancement technologies are prone to overenhancement, color distortion or time consumption, and their adaptability is fairly limited. Therefore, we propose a new single low-light image lightness enhancement method. First, an energy model is presented based on the analysis of membrane vibrations induced by photon stimulations. Then, based on the unique mathematical properties of the energy model and combined with the gamma correction model, a new global lightness enhancement model is proposed. Furthermore, a special relationship between image lightness and gamma intensity is found. Finally, a local fusion strategy, including segmentation, filtering and fusion, is proposed to optimize the local details of the global lightness enhancement images. Experimental results show that the proposed algorithm is superior to nine state-of-the-art methods in avoiding color distortion, restoring the textures of dark areas, reproducing natural colors and reducing time cost. The image source and code will be released at https://github.com/leixiaozhou/CDEFmethod.



### Deep Learning Methods for Real-time Detection and Analysis of Wagner Ulcer Classification System
- **Arxiv ID**: http://arxiv.org/abs/2006.02322v3
- **DOI**: None
- **Categories**: **cs.CV**, I.4.9; J.3
- **Links**: [PDF](http://arxiv.org/pdf/2006.02322v3)
- **Published**: 2020-06-03 15:08:39+00:00
- **Updated**: 2023-05-02 02:54:40+00:00
- **Authors**: Aifu Han, Yongze Zhang, Ajuan Li, Changjin Li, Fengying Zhao, Qiujie Dong, Qin Liu, Yanting Liu, Ximei Shen, Sunjie Yan, Shengzong Zhou
- **Comment**: 11 pages with 11 figures
- **Journal**: None
- **Summary**: At present, the ubiquity method to diagnose the severity of diabetic feet (DF) depends on professional podiatrists. However, in most cases, professional podiatrists have a heavy workload, especially in underdeveloped and developing countries and regions, and there are often insufficient podiatrists to meet the rapidly growing treatment needs of DF patients. It is necessary to develop a medical system that assists in diagnosing DF in order to reduce part of the workload for podiatrists and to provide timely relevant information to patients with DF. In this paper, we have developed a system that can classify and locate Wagner ulcers of diabetic foot in real-time. First, we proposed a dataset of 2688 diabetic feet with annotations. Then, in order to enable the system to detect diabetic foot ulcers in real time and accurately, this paper is based on the YOLOv3 algorithm coupled with image fusion, label smoothing, and variant learning rate mode technologies to improve the robustness and predictive accuracy of the original algorithm. Finally, the refinements on YOLOv3 was used as the optimal algorithm in this paper to deploy into Android smartphone to predict the classes and localization of the diabetic foot with real-time. The experimental results validate that the improved YOLOv3 algorithm achieves a mAP of 91.95%, and meets the needs of real-time detection and analysis of diabetic foot Wagner Ulcer on mobile devices, such as smart phones. This work has the potential to lead to a paradigm shift for clinical treatment of the DF in the future, to provide an effective healthcare solution for DF tissue analysis and healing status.



### Learning Multi-Modal Nonlinear Embeddings: Performance Bounds and an Algorithm
- **Arxiv ID**: http://arxiv.org/abs/2006.02330v2
- **DOI**: 10.1109/TIP.2021.3071688
- **Categories**: **cs.LG**, cs.CV, cs.IR, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.02330v2)
- **Published**: 2020-06-03 15:22:16+00:00
- **Updated**: 2020-12-24 22:01:04+00:00
- **Authors**: Semih Kaya, Elif Vural
- **Comment**: None
- **Journal**: None
- **Summary**: While many approaches exist in the literature to learn low-dimensional representations for data collections in multiple modalities, the generalizability of multi-modal nonlinear embeddings to previously unseen data is a rather overlooked subject. In this work, we first present a theoretical analysis of learning multi-modal nonlinear embeddings in a supervised setting. Our performance bounds indicate that for successful generalization in multi-modal classification and retrieval problems, the regularity of the interpolation functions extending the embedding to the whole data space is as important as the between-class separation and cross-modal alignment criteria. We then propose a multi-modal nonlinear representation learning algorithm that is motivated by these theoretical findings, where the embeddings of the training samples are optimized jointly with the Lipschitz regularity of the interpolators. Experimental comparison to recent multi-modal and single-modal learning algorithms suggests that the proposed method yields promising performance in multi-modal image classification and cross-modal image-text retrieval applications.



### Scene relighting with illumination estimation in the latent space on an encoder-decoder scheme
- **Arxiv ID**: http://arxiv.org/abs/2006.02333v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.02333v1)
- **Published**: 2020-06-03 15:25:11+00:00
- **Updated**: 2020-06-03 15:25:11+00:00
- **Authors**: Alexandre Pierre Dherse, Martin Nicolas Everaert, Jakub Jan Gwizdała
- **Comment**: Report for the CS-413 project at EPFL, Switzerland
- **Journal**: None
- **Summary**: The image relighting task of transferring illumination conditions between two images offers an interesting and difficult challenge with potential applications in photography, cinematography and computer graphics. In this report we present methods that we tried to achieve that goal. Our models are trained on a rendered dataset of artificial locations with varied scene content, light source location and color temperature. With this dataset, we used a network with illumination estimation component aiming to infer and replace light conditions in the latent space representation of the concerned scenes.



### DetectoRS: Detecting Objects with Recursive Feature Pyramid and Switchable Atrous Convolution
- **Arxiv ID**: http://arxiv.org/abs/2006.02334v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.02334v2)
- **Published**: 2020-06-03 15:28:16+00:00
- **Updated**: 2020-11-30 16:06:38+00:00
- **Authors**: Siyuan Qiao, Liang-Chieh Chen, Alan Yuille
- **Comment**: None
- **Journal**: None
- **Summary**: Many modern object detectors demonstrate outstanding performances by using the mechanism of looking and thinking twice. In this paper, we explore this mechanism in the backbone design for object detection. At the macro level, we propose Recursive Feature Pyramid, which incorporates extra feedback connections from Feature Pyramid Networks into the bottom-up backbone layers. At the micro level, we propose Switchable Atrous Convolution, which convolves the features with different atrous rates and gathers the results using switch functions. Combining them results in DetectoRS, which significantly improves the performances of object detection. On COCO test-dev, DetectoRS achieves state-of-the-art 55.7% box AP for object detection, 48.5% mask AP for instance segmentation, and 50.0% PQ for panoptic segmentation. The code is made publicly available.



### Flexible Bayesian Modelling for Nonlinear Image Registration
- **Arxiv ID**: http://arxiv.org/abs/2006.02338v1
- **DOI**: 10.1007/978-3-030-59716-0_25
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.02338v1)
- **Published**: 2020-06-03 15:33:14+00:00
- **Updated**: 2020-06-03 15:33:14+00:00
- **Authors**: Mikael Brudfors, Yaël Balbastre, Guillaume Flandin, Parashkev Nachev, John Ashburner
- **Comment**: Accepted for MICCAI 2020
- **Journal**: None
- **Summary**: We describe a diffeomorphic registration algorithm that allows groups of images to be accurately aligned to a common space, which we intend to incorporate into the SPM software. The idea is to perform inference in a probabilistic graphical model that accounts for variability in both shape and appearance. The resulting framework is general and entirely unsupervised. The model is evaluated at inter-subject registration of 3D human brain scans. Here, the main modeling assumption is that individual anatomies can be generated by deforming a latent 'average' brain. The method is agnostic to imaging modality and can be applied with no prior processing. We evaluate the algorithm using freely available, manually labelled datasets. In this validation we achieve state-of-the-art results, within reasonable runtimes, against previous state-of-the-art widely used, inter-subject registration algorithms. On the unprocessed dataset, the increase in overlap score is over 17%. These results demonstrate the benefits of using informative computational anatomy frameworks for nonlinear registration.



### Assessing Intelligence in Artificial Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2006.02909v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.02909v1)
- **Published**: 2020-06-03 16:45:42+00:00
- **Updated**: 2020-06-03 16:45:42+00:00
- **Authors**: Nicholas J. Schaub, Nathan Hotaling
- **Comment**: None
- **Journal**: None
- **Summary**: The purpose of this work was to develop of metrics to assess network architectures that balance neural network size and task performance. To this end, the concept of neural efficiency is introduced to measure neural layer utilization, and a second metric called artificial intelligence quotient (aIQ) was created to balance neural network performance and neural network efficiency. To study aIQ and neural efficiency, two simple neural networks were trained on MNIST: a fully connected network (LeNet-300-100) and a convolutional neural network (LeNet-5). The LeNet-5 network with the highest aIQ was 2.32% less accurate but contained 30,912 times fewer parameters than the highest accuracy network. Both batch normalization and dropout layers were found to increase neural efficiency. Finally, high aIQ networks are shown to be memorization and overtraining resistant, capable of learning proper digit classification with an accuracy of 92.51% even when 75% of the class labels are randomized. These results demonstrate the utility of aIQ and neural efficiency as metrics for balancing network performance and size.



### The Neural Tangent Link Between CNN Denoisers and Non-Local Filters
- **Arxiv ID**: http://arxiv.org/abs/2006.02379v4
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, eess.SP, 68T07
- **Links**: [PDF](http://arxiv.org/pdf/2006.02379v4)
- **Published**: 2020-06-03 16:50:54+00:00
- **Updated**: 2020-11-16 23:06:53+00:00
- **Authors**: Julián Tachella, Junqi Tang, Mike Davies
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) are now a well-established tool for solving computational imaging problems. Modern CNN-based algorithms obtain state-of-the-art performance in diverse image restoration problems. Furthermore, it has been recently shown that, despite being highly overparameterized, networks trained with a single corrupted image can still perform as well as fully trained networks. We introduce a formal link between such networks through their neural tangent kernel (NTK), and well-known non-local filtering techniques, such as non-local means or BM3D. The filtering function associated with a given network architecture can be obtained in closed form without need to train the network, being fully characterized by the random initialization of the network weights. While the NTK theory accurately predicts the filter associated with networks trained using standard gradient descent, our analysis shows that it falls short to explain the behaviour of networks trained using the popular Adam optimizer. The latter achieves a larger change of weights in hidden layers, adapting the non-local filtering function during training. We evaluate our findings via extensive image denoising experiments.



### Self-supervised Training of Graph Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/2006.02380v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.02380v1)
- **Published**: 2020-06-03 16:53:37+00:00
- **Updated**: 2020-06-03 16:53:37+00:00
- **Authors**: Qikui Zhu, Bo Du, Pingkun Yan
- **Comment**: None
- **Journal**: None
- **Summary**: Graph Convolutional Networks (GCNs) have been successfully applied to analyze non-grid data, where the classical convolutional neural networks (CNNs) cannot be directly used. One similarity shared by GCNs and CNNs is the requirement of massive amount of labeled data for network training. In addition, GCNs need the adjacency matrix as input to define the relationship between those non-grid data, which leads to all of data including training, validation and test data typically forms only one graph structures data for training. Furthermore, the adjacency matrix is usually pre-defined and stationary, which makes the data augmentation strategies cannot be employed on the constructed graph structures data to augment the amount of training data. To further improve the learning capacity and model performance under the limited training data, in this paper, we propose two types of self-supervised learning strategies to exploit available information from the input graph structure data itself. Our proposed self-supervised learning strategies are examined on two representative GCN models with three public citation network datasets - Citeseer, Cora and Pubmed. The experimental results demonstrate the generalization ability as well as the portability of our proposed strategies, which can significantly improve the performance of GCNs with the power of self-supervised learning in improving feature learning.



### Visual Summarization of Lecture Video Segments for Enhanced Navigation
- **Arxiv ID**: http://arxiv.org/abs/2006.02434v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2006.02434v1)
- **Published**: 2020-06-03 16:53:54+00:00
- **Updated**: 2020-06-03 16:53:54+00:00
- **Authors**: Mohammad Rajiur Rahman, Jaspal Subhlok, Shishir Shah
- **Comment**: None
- **Journal**: None
- **Summary**: Lecture videos are an increasingly important learning resource for higher education. However, the challenge of quickly finding the content of interest in a lecture video is an important limitation of this format. This paper introduces visual summarization of lecture video segments to enhance navigation. A lecture video is divided into segments based on the frame-to-frame similarity of content. The user navigates the lecture video content by viewing a single frame visual and textual summary of each segment. The paper presents a novel methodology to generate the visual summary of a lecture video segment by computing similarities between images extracted from the segment and employing a graph-based algorithm to identify the subset of most representative images. The results from this research are integrated into a real-world lecture video management portal called Videopoints. To collect ground truth for evaluation, a survey was conducted where multiple users manually provided visual summaries for 40 lecture video segments. The users also stated whether any images were not selected for the summary because they were similar to other selected images. The graph based algorithm for identifying summary images achieves 78% precision and 72% F1-measure with frequently selected images as the ground truth, and 94% precision and 72% F1-measure with the union of all user selected images as the ground truth. For 98% of algorithm selected visual summary images, at least one user also selected that image for their summary or considered it similar to another image they selected. Over 65% of automatically generated summaries were rated as good or very good by the users on a 4-point scale from poor to very good. Overall, the results establish that the methodology introduced in this paper produces good quality visual summaries that are practically useful for lecture video navigation.



### DGSAC: Density Guided Sampling and Consensus
- **Arxiv ID**: http://arxiv.org/abs/2006.02413v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.02413v1)
- **Published**: 2020-06-03 17:42:53+00:00
- **Updated**: 2020-06-03 17:42:53+00:00
- **Authors**: Lokender Tiwari, Saket Anand
- **Comment**: Working article
- **Journal**: None
- **Summary**: Robust multiple model fitting plays a crucial role in many computer vision applications. Unlike single model fitting problems, the multi-model fitting has additional challenges. The unknown number of models and the inlier noise scale are the two most important of them, which are in general provided by the user using ground-truth or some other auxiliary information. Mode seeking/ clustering-based approaches crucially depend on the quality of model hypotheses generated. While preference analysis based guided sampling approaches have shown remarkable performance, they operate in a time budget framework, and the user provides the time as a reasonable guess. In this paper, we deviate from the mode seeking and time budget framework. We propose a concept called Kernel Residual Density (KRD) and apply it to various components of a multiple-model fitting pipeline. The Kernel Residual Density act as a key differentiator between inliers and outliers. We use KRD to guide and automatically stop the sampling process. The sampling process stops after generating a set of hypotheses that can explain all the data points. An explanation score is maintained for each data point, which is updated on-the-fly. We propose two model selection algorithms, an optimal quadratic program based, and a greedy. Unlike mode seeking approaches, our model selection algorithms seek to find one representative hypothesis for each genuine structure present in the data. We evaluate our method (dubbed as DGSAC) on a wide variety of tasks like planar segmentation, motion segmentation, vanishing point estimation, plane fitting to 3D point cloud, line, and circle fitting, which shows the effectiveness of our method and its unified nature.



### CircleNet: Anchor-free Detection with Circle Representation
- **Arxiv ID**: http://arxiv.org/abs/2006.02474v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.02474v1)
- **Published**: 2020-06-03 18:31:51+00:00
- **Updated**: 2020-06-03 18:31:51+00:00
- **Authors**: Haichun Yang, Ruining Deng, Yuzhe Lu, Zheyu Zhu, Ye Chen, Joseph T. Roland, Le Lu, Bennett A. Landman, Agnes B. Fogo, Yuankai Huo
- **Comment**: None
- **Journal**: None
- **Summary**: Object detection networks are powerful in computer vision, but not necessarily optimized for biomedical object detection. In this work, we propose CircleNet, a simple anchor-free detection method with circle representation for detection of the ball-shaped glomerulus. Different from the traditional bounding box based detection method, the bounding circle (1) reduces the degrees of freedom of detection representation, (2) is naturally rotation invariant, (3) and optimized for ball-shaped objects. The key innovation to enable this representation is the anchor-free framework with the circle detection head. We evaluate CircleNet in the context of detection of glomerulus. CircleNet increases average precision of the glomerulus detection from 0.598 to 0.647. Another key advantage is that CircleNet achieves better rotation consistency compared with bounding box representations.



### M2P2: Multimodal Persuasion Prediction using Adaptive Fusion
- **Arxiv ID**: http://arxiv.org/abs/2006.11405v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG, cs.MM, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2006.11405v2)
- **Published**: 2020-06-03 18:47:24+00:00
- **Updated**: 2021-12-11 20:50:15+00:00
- **Authors**: Chongyang Bai, Haipeng Chen, Srijan Kumar, Jure Leskovec, V. S. Subrahmanian
- **Comment**: published in IEEE Trans. on Multimedia 2021
- **Journal**: None
- **Summary**: Identifying persuasive speakers in an adversarial environment is a critical task. In a national election, politicians would like to have persuasive speakers campaign on their behalf. When a company faces adverse publicity, they would like to engage persuasive advocates for their position in the presence of adversaries who are critical of them. Debates represent a common platform for these forms of adversarial persuasion. This paper solves two problems: the Debate Outcome Prediction (DOP) problem predicts who wins a debate while the Intensity of Persuasion Prediction (IPP) problem predicts the change in the number of votes before and after a speaker speaks. Though DOP has been previously studied, we are the first to study IPP. Past studies on DOP fail to leverage two important aspects of multimodal data: 1) multiple modalities are often semantically aligned, and 2) different modalities may provide diverse information for prediction. Our M2P2 (Multimodal Persuasion Prediction) framework is the first to use multimodal (acoustic, visual, language) data to solve the IPP problem. To leverage the alignment of different modalities while maintaining the diversity of the cues they provide, M2P2 devises a novel adaptive fusion learning framework which fuses embeddings obtained from two modules -- an alignment module that extracts shared information between modalities and a heterogeneity module that learns the weights of different modalities with guidance from three separately trained unimodal reference models. We test M2P2 on the popular IQ2US dataset designed for DOP. We also introduce a new dataset called QPS (from Qipashuo, a popular Chinese debate TV show ) for IPP. M2P2 significantly outperforms 4 recent baselines on both datasets.



### Phasic dopamine release identification using ensemble of AlexNet
- **Arxiv ID**: http://arxiv.org/abs/2006.02536v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.02536v1)
- **Published**: 2020-06-03 21:13:05+00:00
- **Updated**: 2020-06-03 21:13:05+00:00
- **Authors**: Luca Patarnello, Marco Celin, Loris Nanni
- **Comment**: None
- **Journal**: None
- **Summary**: Dopamine (DA) is an organic chemical that influences several parts of behaviour and physical functions. Fast-scan cyclic voltammetry (FSCV) is a technique used for in vivo phasic dopamine release measurements. The analysis of such measurements, though, requires notable effort. In this paper, we present the use of convolutional neural networks (CNNs) for the identification of phasic dopamine releases.



### Automated segmentation of retinal fluid volumes from structural and angiographic optical coherence tomography using deep learning
- **Arxiv ID**: http://arxiv.org/abs/2006.02569v1
- **DOI**: 10.1167/tvst.9.2.54
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.02569v1)
- **Published**: 2020-06-03 22:55:47+00:00
- **Updated**: 2020-06-03 22:55:47+00:00
- **Authors**: Yukun Guo, Tristan T. Hormel, Honglian Xiong, Jie Wang, Thomas S. Hwang, Yali Jia
- **Comment**: None
- **Journal**: None
- **Summary**: Purpose: We proposed a deep convolutional neural network (CNN), named Retinal Fluid Segmentation Network (ReF-Net) to segment volumetric retinal fluid on optical coherence tomography (OCT) volume. Methods: 3 x 3-mm OCT scans were acquired on one eye by a 70-kHz OCT commercial AngioVue system (RTVue-XR; Optovue, Inc.) from 51 participants in a clinical diabetic retinopathy (DR) study (45 with retinal edema and 6 healthy controls). A CNN with U-Net-like architecture was constructed to detect and segment the retinal fluid. Cross-sectional OCT and angiography (OCTA) scans were used for training and testing ReF-Net. The effect of including OCTA data for retinal fluid segmentation was investigated in this study. Volumetric retinal fluid can be constructed using the output of ReF-Net. Area-under-Receiver-Operating-Characteristic-curve (AROC), intersection-over-union (IoU), and F1-score were calculated to evaluate the performance of ReF-Net. Results: ReF-Net shows high accuracy (F1 = 0.864 +/- 0.084) in retinal fluid segmentation. The performance can be further improved (F1 = 0.892 +/- 0.038) by including information from both OCTA and structural OCT. ReF-Net also shows strong robustness to shadow artifacts. Volumetric retinal fluid can provide more comprehensive information than the 2D area, whether cross-sectional or en face projections. Conclusions: A deep-learning-based method can accurately segment retinal fluid volumetrically on OCT/OCTA scans with strong robustness to shadow artifacts. OCTA data can improve retinal fluid segmentation. Volumetric representations of retinal fluid are superior to 2D projections. Translational Relevance: Using a deep learning method to segment retinal fluid volumetrically has the potential to improve the diagnostic accuracy of diabetic macular edema by OCT systems.



### Exploration of Interpretability Techniques for Deep COVID-19 Classification using Chest X-ray Images
- **Arxiv ID**: http://arxiv.org/abs/2006.02570v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.02570v3)
- **Published**: 2020-06-03 22:55:53+00:00
- **Updated**: 2022-10-15 18:12:44+00:00
- **Authors**: Soumick Chatterjee, Fatima Saad, Chompunuch Sarasaen, Suhita Ghosh, Valerie Krug, Rupali Khatun, Rahul Mishra, Nirja Desai, Petia Radeva, Georg Rose, Sebastian Stober, Oliver Speck, Andreas Nürnberger
- **Comment**: None
- **Journal**: None
- **Summary**: The outbreak of COVID-19 has shocked the entire world with its fairly rapid spread and has challenged different sectors. One of the most effective ways to limit its spread is the early and accurate diagnosis of infected patients. Medical imaging such as X-ray and Computed Tomography (CT) combined with the potential of Artificial Intelligence (AI) plays an essential role in supporting the medical staff in the diagnosis process. Thereby, five different deep learning models (ResNet18, ResNet34, InceptionV3, InceptionResNetV2, and DenseNet161) and their Ensemble have been used in this paper to classify COVID-19, pneumoni{\ae} and healthy subjects using Chest X-Ray images. Multi-label classification was performed to predict multiple pathologies for each patient, if present. Foremost, the interpretability of each of the networks was thoroughly studied using local interpretability methods - occlusion, saliency, input X gradient, guided backpropagation, integrated gradients, and DeepLIFT, and using a global technique - neuron activation profiles. The mean Micro-F1 score of the models for COVID-19 classifications ranges from 0.66 to 0.875, and is 0.89 for the Ensemble of the network models. The qualitative results depicted the ResNets to be the most interpretable models. This research demonstrates the importance of using interpretability methods to compare different models before making the decision regarding the best-performing model.



### DFR-TSD: A Deep Learning Based Framework for Robust Traffic Sign Detection Under Challenging Weather Conditions
- **Arxiv ID**: http://arxiv.org/abs/2006.02578v1
- **DOI**: 10.13140/RG.2.2.18341.86249
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.02578v1)
- **Published**: 2020-06-03 23:12:26+00:00
- **Updated**: 2020-06-03 23:12:26+00:00
- **Authors**: Sabbir Ahmed, Uday Kamal, Md. Kamrul Hasan
- **Comment**: None
- **Journal**: None
- **Summary**: Robust traffic sign detection and recognition (TSDR) is of paramount importance for the successful realization of autonomous vehicle technology. The importance of this task has led to a vast amount of research efforts and many promising methods have been proposed in the existing literature. However, the SOTA (SOTA) methods have been evaluated on clean and challenge-free datasets and overlooked the performance deterioration associated with different challenging conditions (CCs) that obscure the traffic images captured in the wild. In this paper, we look at the TSDR problem under CCs and focus on the performance degradation associated with them. To overcome this, we propose a Convolutional Neural Network (CNN) based TSDR framework with prior enhancement. Our modular approach consists of a CNN-based challenge classifier, Enhance-Net, an encoder-decoder CNN architecture for image enhancement, and two separate CNN architectures for sign-detection and classification. We propose a novel training pipeline for Enhance-Net that focuses on the enhancement of the traffic sign regions (instead of the whole image) in the challenging images subject to their accurate detection. We used CURE-TSD dataset consisting of traffic videos captured under different CCs to evaluate the efficacy of our approach. We experimentally show that our method obtains an overall precision and recall of 91.1% and 70.71% that is 7.58% and 35.90% improvement in precision and recall, respectively, compared to the current benchmark. Furthermore, we compare our approach with SOTA object detection networks, Faster-RCNN and R-FCN, and show that our approach outperforms them by a large margin.



