# Arxiv Papers in cs.CV on 2020-06-11
### Dance Revolution: Long-Term Dance Generation with Music via Curriculum Learning
- **Arxiv ID**: http://arxiv.org/abs/2006.06119v7
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2006.06119v7)
- **Published**: 2020-06-11 00:08:25+00:00
- **Updated**: 2021-03-14 07:56:13+00:00
- **Authors**: Ruozi Huang, Huang Hu, Wei Wu, Kei Sawada, Mi Zhang, Daxin Jiang
- **Comment**: Accepted by ICLR 2021
- **Journal**: None
- **Summary**: Dancing to music is one of human's innate abilities since ancient times. In machine learning research, however, synthesizing dance movements from music is a challenging problem. Recently, researchers synthesize human motion sequences through autoregressive models like recurrent neural network (RNN). Such an approach often generates short sequences due to an accumulation of prediction errors that are fed back into the neural network. This problem becomes even more severe in the long motion sequence generation. Besides, the consistency between dance and music in terms of style, rhythm and beat is yet to be taken into account during modeling. In this paper, we formalize the music-conditioned dance generation as a sequence-to-sequence learning problem and devise a novel seq2seq architecture to efficiently process long sequences of music features and capture the fine-grained correspondence between music and dance. Furthermore, we propose a novel curriculum learning strategy to alleviate error accumulation of autoregressive models in long motion sequence generation, which gently changes the training process from a fully guided teacher-forcing scheme using the previous ground-truth movements, towards a less guided autoregressive scheme mostly using the generated movements instead. Extensive experiments show that our approach significantly outperforms the existing state-of-the-arts on automatic metrics and human evaluation. We also make a demo video to demonstrate the superior performance of our proposed approach at https://www.youtube.com/watch?v=lmE20MEheZ8.



### Kalman Filter Based Multiple Person Head Tracking
- **Arxiv ID**: http://arxiv.org/abs/2006.06134v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.06134v1)
- **Published**: 2020-06-11 00:54:45+00:00
- **Updated**: 2020-06-11 00:54:45+00:00
- **Authors**: Mohib Ullah, Maqsood Mahmud, Habib Ullah, Kashif Ahmad, Ali Shariq Imran, Faouzi Alaya Cheikh
- **Comment**: 5 pages, 2 figures
- **Journal**: None
- **Summary**: For multi-target tracking, target representation plays a crucial rule in performance. State-of-the-art approaches rely on the deep learning-based visual representation that gives an optimal performance at the cost of high computational complexity. In this paper, we come up with a simple yet effective target representation for human tracking. Our inspiration comes from the fact that the human body goes through severe deformation and inter/intra occlusion over the passage of time. So, instead of tracking the whole body part, a relative rigid organ tracking is selected for tracking the human over an extended period of time. Hence, we followed the tracking-by-detection paradigm and generated the target hypothesis of only the spatial locations of heads in every frame. After the localization of head location, a Kalman filter with a constant velocity motion model is instantiated for each target that follows the temporal evolution of the targets in the scene. For associating the targets in the consecutive frames, combinatorial optimization is used that associates the corresponding targets in a greedy fashion. Qualitative results are evaluated on four challenging video surveillance dataset and promising results has been achieved.



### A Tailored Convolutional Neural Network for Nonlinear Manifold Learning of Computational Physics Data using Unstructured Spatial Discretizations
- **Arxiv ID**: http://arxiv.org/abs/2006.06154v3
- **DOI**: 10.1137/20M1344263
- **Categories**: **physics.comp-ph**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.06154v3)
- **Published**: 2020-06-11 02:19:34+00:00
- **Updated**: 2021-03-25 23:47:43+00:00
- **Authors**: John Tencer, Kevin Potter
- **Comment**: Preprint
- **Journal**: None
- **Summary**: We propose a nonlinear manifold learning technique based on deep convolutional autoencoders that is appropriate for model order reduction of physical systems in complex geometries. Convolutional neural networks have proven to be highly advantageous for compressing data arising from systems demonstrating a slow-decaying Kolmogorov n-width. However, these networks are restricted to data on structured meshes. Unstructured meshes are often required for performing analyses of real systems with complex geometry. Our custom graph convolution operators based on the available differential operators for a given spatial discretization effectively extend the application space of deep convolutional autoencoders to systems with arbitrarily complex geometry that are typically discretized using unstructured meshes. We propose sets of convolution operators based on the spatial derivative operators for the underlying spatial discretization, making the method particularly well suited to data arising from the solution of partial differential equations. We demonstrate the method using examples from heat transfer and fluid mechanics and show better than an order of magnitude improvement in accuracy over linear methods.



### Image Deconvolution via Noise-Tolerant Self-Supervised Inversion
- **Arxiv ID**: http://arxiv.org/abs/2006.06156v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, physics.ins-det
- **Links**: [PDF](http://arxiv.org/pdf/2006.06156v1)
- **Published**: 2020-06-11 02:27:23+00:00
- **Updated**: 2020-06-11 02:27:23+00:00
- **Authors**: Hirofumi Kobayashi, Ahmet Can Solak, Joshua Batson, Loic A. Royer
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a general framework for solving inverse problems in the presence of noise that requires no signal prior, no noise estimate, and no clean training data. We only require that the forward model be available and that the noise be statistically independent across measurement dimensions. We build upon the theory of $\mathcal{J}$-invariant functions (Batson & Royer 2019, arXiv:1901.11365) and show how self-supervised denoising \emph{\`a la} Noise2Self is a special case of learning a noise-tolerant pseudo-inverse of the identity. We demonstrate our approach by showing how a convolutional neural network can be taught in a self-supervised manner to deconvolve images and surpass in image quality classical inversion schemes such as Lucy-Richardson deconvolution.



### 0-MMS: Zero-Shot Multi-Motion Segmentation With A Monocular Event Camera
- **Arxiv ID**: http://arxiv.org/abs/2006.06158v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.06158v2)
- **Published**: 2020-06-11 02:34:29+00:00
- **Updated**: 2020-11-07 01:58:31+00:00
- **Authors**: Chethan M. Parameshwara, Nitin J. Sanket, Chahat Deep Singh, Cornelia Ferm√ºller, Yiannis Aloimonos
- **Comment**: 7 pages, 6 figures, 4 tables, Under review ICRA 2021
- **Journal**: None
- **Summary**: Segmentation of moving objects in dynamic scenes is a key process in scene understanding for navigation tasks. Classical cameras suffer from motion blur in such scenarios rendering them effete. On the contrary, event cameras, because of their high temporal resolution and lack of motion blur, are tailor-made for this problem. We present an approach for monocular multi-motion segmentation, which combines bottom-up feature tracking and top-down motion compensation into a unified pipeline, which is the first of its kind to our knowledge. Using the events within a time-interval, our method segments the scene into multiple motions by splitting and merging. We further speed up our method by using the concept of motion propagation and cluster keyslices.   The approach was successfully evaluated on both challenging real-world and synthetic scenarios from the EV-IMO, EED, and MOD datasets and outperformed the state-of-the-art detection rate by 12\%, achieving a new state-of-the-art average detection rate of 81.06%, 94.2% and 82.35% on the aforementioned datasets. To enable further research and systematic evaluation of multi-motion segmentation, we present and open-source a new dataset/benchmark called MOD++, which includes challenging sequences and extensive data stratification in-terms of camera and object motion, velocity magnitudes, direction, and rotational speeds.



### Telling Left from Right: Learning Spatial Correspondence of Sight and Sound
- **Arxiv ID**: http://arxiv.org/abs/2006.06175v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.SD, eess.AS, 68T45, I.4.0
- **Links**: [PDF](http://arxiv.org/pdf/2006.06175v2)
- **Published**: 2020-06-11 04:00:24+00:00
- **Updated**: 2020-06-12 03:12:16+00:00
- **Authors**: Karren Yang, Bryan Russell, Justin Salamon
- **Comment**: CVPR 2020
- **Journal**: None
- **Summary**: Self-supervised audio-visual learning aims to capture useful representations of video by leveraging correspondences between visual and audio inputs. Existing approaches have focused primarily on matching semantic information between the sensory streams. We propose a novel self-supervised task to leverage an orthogonal principle: matching spatial information in the audio stream to the positions of sound sources in the visual stream. Our approach is simple yet effective. We train a model to determine whether the left and right audio channels have been flipped, forcing it to reason about spatial localization across the visual and audio streams. To train and evaluate our method, we introduce a large-scale video dataset, YouTube-ASMR-300K, with spatial audio comprising over 900 hours of footage. We demonstrate that understanding spatial correspondence enables models to perform better on three audio-visual tasks, achieving quantitative gains over supervised and self-supervised baselines that do not leverage spatial audio cues. We also show how to extend our self-supervised approach to 360 degree videos with ambisonic audio.



### COVID-19-CT-CXR: a freely accessible and weakly labeled chest X-ray and CT image collection on COVID-19 from biomedical literature
- **Arxiv ID**: http://arxiv.org/abs/2006.06177v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.06177v2)
- **Published**: 2020-06-11 04:00:56+00:00
- **Updated**: 2020-10-22 03:03:55+00:00
- **Authors**: Yifan Peng, Yu-Xing Tang, Sungwon Lee, Yingying Zhu, Ronald M. Summers, Zhiyong Lu
- **Comment**: Accepted by IEEE Transactions on Big Data
- **Journal**: None
- **Summary**: The latest threat to global health is the COVID-19 outbreak. Although there exist large datasets of chest X-rays (CXR) and computed tomography (CT) scans, few COVID-19 image collections are currently available due to patient privacy. At the same time, there is a rapid growth of COVID-19-relevant articles in the biomedical literature. Here, we present COVID-19-CT-CXR, a public database of COVID-19 CXR and CT images, which are automatically extracted from COVID-19-relevant articles from the PubMed Central Open Access (PMC-OA) Subset. We extracted figures, associated captions, and relevant figure descriptions in the article and separated compound figures into subfigures. We also designed a deep-learning model to distinguish them from other figure types and to classify them accordingly. The final database includes 1,327 CT and 263 CXR images (as of May 9, 2020) with their relevant text. To demonstrate the utility of COVID-19-CT-CXR, we conducted four case studies. (1) We show that COVID-19-CT-CXR, when used as additional training data, is able to contribute to improved DL performance for the classification of COVID-19 and non-COVID-19 CT. (2) We collected CT images of influenza and trained a DL baseline to distinguish a diagnosis of COVID-19, influenza, or normal or other types of diseases on CT. (3) We trained an unsupervised one-class classifier from non-COVID-19 CXR and performed anomaly detection to detect COVID-19 CXR. (4) From text-mined captions and figure descriptions, we compared clinical symptoms and clinical findings of COVID-19 vs. those of influenza to demonstrate the disease differences in the scientific publications. We believe that our work is complementary to existing resources and hope that it will contribute to medical image analysis of the COVID-19 pandemic. The dataset, code, and DL models are publicly available at https://github.com/ncbi-nlp/COVID-19-CT-CXR.



### JIT-Masker: Efficient Online Distillation for Background Matting
- **Arxiv ID**: http://arxiv.org/abs/2006.06185v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.06185v1)
- **Published**: 2020-06-11 04:28:09+00:00
- **Updated**: 2020-06-11 04:28:09+00:00
- **Authors**: Jo Chuang, Qian Dong
- **Comment**: None
- **Journal**: None
- **Summary**: We design a real-time portrait matting pipeline for everyday use, particularly for "virtual backgrounds" in video conferences. Existing segmentation and matting methods prioritize accuracy and quality over throughput and efficiency, and our pipeline enables trading off a controllable amount of accuracy for better throughput by leveraging online distillation on the input video stream. We construct our own dataset of simulated video calls in various scenarios, and show that our approach delivers a 5x speedup over a saliency detection based pipeline in a non-GPU accelerated setting while delivering higher quality results. We demonstrate that an online distillation approach can feasibly work as part of a general, consumer level product as a "virtual background" tool. Our public implementation is at https://github.com/josephch405/jit-masker.



### Large-Scale Adversarial Training for Vision-and-Language Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2006.06195v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.06195v2)
- **Published**: 2020-06-11 05:14:35+00:00
- **Updated**: 2020-10-22 18:12:53+00:00
- **Authors**: Zhe Gan, Yen-Chun Chen, Linjie Li, Chen Zhu, Yu Cheng, Jingjing Liu
- **Comment**: NeurIPS 2020 Spotlight paper
- **Journal**: None
- **Summary**: We present VILLA, the first known effort on large-scale adversarial training for vision-and-language (V+L) representation learning. VILLA consists of two training stages: (i) task-agnostic adversarial pre-training; followed by (ii) task-specific adversarial finetuning. Instead of adding adversarial perturbations on image pixels and textual tokens, we propose to perform adversarial training in the embedding space of each modality. To enable large-scale training, we adopt the "free" adversarial training strategy, and combine it with KL-divergence-based regularization to promote higher invariance in the embedding space. We apply VILLA to current best-performing V+L models, and achieve new state of the art on a wide range of tasks, including Visual Question Answering, Visual Commonsense Reasoning, Image-Text Retrieval, Referring Expression Comprehension, Visual Entailment, and NLVR2.



### An Edge Information and Mask Shrinking Based Image Inpainting Approach
- **Arxiv ID**: http://arxiv.org/abs/2006.06196v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.06196v1)
- **Published**: 2020-06-11 05:15:52+00:00
- **Updated**: 2020-06-11 05:15:52+00:00
- **Authors**: Huali Xu, Xiangdong Su, Meng Wang, Xiang Hao, Guanglai Gao
- **Comment**: Accepted by ICME2020
- **Journal**: None
- **Summary**: In the image inpainting task, the ability to repair both high-frequency and low-frequency information in the missing regions has a substantial influence on the quality of the restored image. However, existing inpainting methods usually fail to consider both high-frequency and low-frequency information simultaneously. To solve this problem, this paper proposes edge information and mask shrinking based image inpainting approach, which consists of two models. The first model is an edge generation model used to generate complete edge information from the damaged image, and the second model is an image completion model used to fix the missing regions with the generated edge information and the valid contents of the damaged image. The mask shrinking strategy is employed in the image completion model to track the areas to be repaired. The proposed approach is evaluated qualitatively and quantitatively on the dataset Places2. The result shows our approach outperforms state-of-the-art methods.



### Unsupervised Learning of 3D Point Set Registration
- **Arxiv ID**: http://arxiv.org/abs/2006.06200v1
- **DOI**: None
- **Categories**: **cs.CG**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.06200v1)
- **Published**: 2020-06-11 05:21:38+00:00
- **Updated**: 2020-06-11 05:21:38+00:00
- **Authors**: Lingjing Wang, Xiang Li, Yi Fang
- **Comment**: None
- **Journal**: None
- **Summary**: Point cloud registration is the process of aligning a pair of point sets via searching for a geometric transformation. Recent works leverage the power of deep learning for registering a pair of point sets. However, unfortunately, deep learning models often require a large number of ground truth labels for training. Moreover, for a pair of source and target point sets, existing deep learning mechanisms require explicitly designed encoders to extract both deep spatial features from unstructured point clouds and their spatial correlation representation, which is further fed to a decoder to regress the desired geometric transformation for point set alignment. To further enhance deep learning models for point set registration, this paper proposes Deep-3DAligner, a novel unsupervised registration framework based on a newly introduced deep Spatial Correlation Representation (SCR) feature. The SCR feature describes the geometric essence of the spatial correlation between source and target point sets in an encoding-free manner. More specifically, our method starts with optimizing a randomly initialized latent SCR feature, which is then decoded to a geometric transformation (i.e., rotation and translation) to align source and target point sets. Our Deep-3DAligner jointly updates the SCR feature and weights of the transformation decoder towards the minimization of an unsupervised alignment loss. We conducted experiments on the ModelNet40 datasets to validate the performance of our unsupervised Deep-3DAligner for point set registration. The results demonstrated that, even without ground truth and any assumption of a direct correspondence between source and target point sets for training, our proposed approach achieved comparative performance compared to most recent supervised state-of-the-art approaches.



### Fall Detector Adapted to Nursing Home Needs through an Optical-Flow based CNN
- **Arxiv ID**: http://arxiv.org/abs/2006.06201v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.06201v1)
- **Published**: 2020-06-11 05:23:12+00:00
- **Updated**: 2020-06-11 05:23:12+00:00
- **Authors**: Alexy Carlier, Paul Peyramaure, Ketty Favre, Muriel Pressigout
- **Comment**: None
- **Journal**: 42nd Annual International Conference of the IEEE Engineering in
  Medicine and Biology Society - EMBC2020, Jul 2020, Montreal, Canada
- **Summary**: Fall detection in specialized homes for the elderly is challenging. Vision-based fall detection solutions have a significant advantage over sensor-based ones as they do not instrument the resident who can suffer from mental diseases. This work is part of a project intended to deploy fall detection solutions in nursing homes. The proposed solution, based on Deep Learning, is built on a Convolutional Neural Network (CNN) trained to maximize a sensitivity-based metric. This work presents the requirements from the medical side and how it impacts the tuning of a CNN. Results highlight the importance of the temporal aspect of a fall. Therefore, a custom metric adapted to this use case and an implementation of a decision-making process are proposed in order to best meet the medical teams requirements. Clinical relevance This work presents a fall detection solution enabled to detect 86.2% of falls while producing only 11.6% of false alarms in average on the considered databases.



### Visualizing and Understanding Vision System
- **Arxiv ID**: http://arxiv.org/abs/2006.11413v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2006.11413v1)
- **Published**: 2020-06-11 07:08:49+00:00
- **Updated**: 2020-06-11 07:08:49+00:00
- **Authors**: Feng Qi, Guanjun Jiang
- **Comment**: None
- **Journal**: None
- **Summary**: How the human vision system addresses the object identity-preserving recognition problem is largely unknown. Here, we use a vision recognition-reconstruction network (RRN) to investigate the development, recognition, learning and forgetting mechanisms, and achieve similar characteristics to electrophysiological measurements in monkeys. First, in network development study, the RRN also experiences critical developmental stages characterized by specificities in neuron types, synapse and activation patterns, and visual task performance from the early stage of coarse salience map recognition to mature stage of fine structure recognition. In digit recognition study, we witness that the RRN could maintain object invariance representation under various viewing conditions by coordinated adjustment of responses of population neurons. And such concerted population responses contained untangled object identity and properties information that could be accurately extracted via high-level cortices or even a simple weighted summation decoder. In the learning and forgetting study, novel structure recognition is implemented by adjusting entire synapses in low magnitude while pattern specificities of original synaptic connectivity are preserved, which guaranteed a learning process without disrupting the existing functionalities. This work benefits the understanding of the human visual processing mechanism and the development of human-like machine intelligence.



### CLEval: Character-Level Evaluation for Text Detection and Recognition Tasks
- **Arxiv ID**: http://arxiv.org/abs/2006.06244v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.06244v1)
- **Published**: 2020-06-11 08:12:39+00:00
- **Updated**: 2020-06-11 08:12:39+00:00
- **Authors**: Youngmin Baek, Daehyun Nam, Sungrae Park, Junyeop Lee, Seung Shin, Jeonghun Baek, Chae Young Lee, Hwalsuk Lee
- **Comment**: 12 pages, 8 figures
- **Journal**: None
- **Summary**: Despite the recent success of text detection and recognition methods, existing evaluation metrics fail to provide a fair and reliable comparison among those methods. In addition, there exists no end-to-end evaluation metric that takes characteristics of OCR tasks into account. Previous end-to-end metric contains cascaded errors from the binary scoring process applied in both detection and recognition tasks. Ignoring partially correct results raises a gap between quantitative and qualitative analysis, and prevents fine-grained assessment. Based on the fact that character is a key element of text, we hereby propose a Character-Level Evaluation metric (CLEval). In CLEval, the \textit{instance matching} process handles split and merge detection cases, and the \textit{scoring process} conducts character-level evaluation. By aggregating character-level scores, the CLEval metric provides a fine-grained evaluation of end-to-end results composed of the detection and recognition as well as individual evaluations for each module from the end-performance perspective. We believe that our metrics can play a key role in developing and analyzing state-of-the-art text detection and recognition methods. The evaluation code is publicly available at https://github.com/clovaai/CLEval.



### Privacy-Aware Activity Classification from First Person Office Videos
- **Arxiv ID**: http://arxiv.org/abs/2006.06246v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.06246v1)
- **Published**: 2020-06-11 08:13:15+00:00
- **Updated**: 2020-06-11 08:13:15+00:00
- **Authors**: Partho Ghosh, Md. Abrar Istiak, Nayeeb Rashid, Ahsan Habib Akash, Ridwan Abrar, Ankan Ghosh Dastider, Asif Shahriyar Sushmit, Taufiq Hasan
- **Comment**: None
- **Journal**: None
- **Summary**: In the advent of wearable body-cameras, human activity classification from First-Person Videos (FPV) has become a topic of increasing importance for various applications, including in life-logging, law-enforcement, sports, workplace, and healthcare. One of the challenging aspects of FPV is its exposure to potentially sensitive objects within the user's field of view. In this work, we developed a privacy-aware activity classification system focusing on office videos. We utilized a Mask-RCNN with an Inception-ResNet hybrid as a feature extractor for detecting, and then blurring out sensitive objects (e.g., digital screens, human face, paper) from the videos. For activity classification, we incorporate an ensemble of Recurrent Neural Networks (RNNs) with ResNet, ResNext, and DenseNet based feature extractors. The proposed system was trained and evaluated on the FPV office video dataset that includes 18-classes made available through the IEEE Video and Image Processing (VIP) Cup 2019 competition. On the original unprotected FPVs, the proposed activity classifier ensemble reached an accuracy of 85.078% with precision, recall, and F1 scores of 0.88, 0.85 & 0.86, respectively. On privacy protected videos, the performances were slightly degraded, with accuracy, precision, recall, and F1 scores at 73.68%, 0.79, 0.75, and 0.74, respectively. The presented system won the 3rd prize in the IEEE VIP Cup 2019 competition.



### W-net: Simultaneous segmentation of multi-anatomical retinal structures using a multi-task deep neural network
- **Arxiv ID**: http://arxiv.org/abs/2006.06277v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.06277v1)
- **Published**: 2020-06-11 09:33:33+00:00
- **Updated**: 2020-06-11 09:33:33+00:00
- **Authors**: Hongwei Zhao, Chengtao Peng, Lei Liu, Bin Li
- **Comment**: None
- **Journal**: None
- **Summary**: Segmentation of multiple anatomical structures is of great importance in medical image analysis. In this study, we proposed a $\mathcal{W}$-net to simultaneously segment both the optic disc (OD) and the exudates in retinal images based on the multi-task learning (MTL) scheme. We introduced a class-balanced loss and a multi-task weighted loss to alleviate the imbalanced problem and to improve the robustness and generalization property of the $\mathcal{W}$-net. We demonstrated the effectiveness of our approach by applying five-fold cross-validation experiments on two public datasets e\_ophtha\_EX and DiaRetDb1. We achieved F1-score of 94.76\% and 95.73\% for OD segmentation, and 92.80\% and 94.14\% for exudates segmentation. To further prove the generalization property of the proposed method, we applied the trained model on the DRIONS-DB dataset for OD segmentation and on the MESSIDOR dataset for exudate segmentation. Our results demonstrated that by choosing the optimal weights of each task, the MTL based $\mathcal{W}$-net outperformed separate models trained individually on each task. Code and pre-trained models will be available at: \url{https://github.com/FundusResearch/MTL_for_OD_and_exudates.git}.



### DSU-net: Dense SegU-net for automatic head-and-neck tumor segmentation in MR images
- **Arxiv ID**: http://arxiv.org/abs/2006.06278v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.06278v3)
- **Published**: 2020-06-11 09:33:41+00:00
- **Updated**: 2020-12-20 03:15:56+00:00
- **Authors**: Pin Tang, Chen Zu, Mei Hong, Rui Yan, Xingchen Peng, Jianghong Xiao, Xi Wu, Jiliu Zhou, Luping Zhou, Yan Wang
- **Comment**: This research needs to be advanced in the future
- **Journal**: None
- **Summary**: Precise and accurate segmentation of the most common head-and-neck tumor, nasopharyngeal carcinoma (NPC), in MRI sheds light on treatment and regulatory decisions making. However, the large variations in the lesion size and shape of NPC, boundary ambiguity, as well as the limited available annotated samples conspire NPC segmentation in MRI towards a challenging task. In this paper, we propose a Dense SegU-net (DSU-net) framework for automatic NPC segmentation in MRI. Our contribution is threefold. First, different from the traditional decoder in U-net using upconvolution for upsamling, we argue that the restoration from low resolution features to high resolution output should be capable of preserving information significant for precise boundary localization. Hence, we use unpooling to unsample and propose SegU-net. Second, to combat the potential vanishing-gradient problem, we introduce dense blocks which can facilitate feature propagation and reuse. Third, using only cross entropy (CE) as loss function may bring about troubles such as miss-prediction, therefore we propose to use a loss function comprised of both CE loss and Dice loss to train the network. Quantitative and qualitative comparisons are carried out extensively on in-house datasets, the experimental results show that our proposed architecture outperforms the existing state-of-the-art segmentation networks.



### Fast Coherent Point Drift
- **Arxiv ID**: http://arxiv.org/abs/2006.06281v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.06281v1)
- **Published**: 2020-06-11 09:35:23+00:00
- **Updated**: 2020-06-11 09:35:23+00:00
- **Authors**: Xiang-Wei Feng, Da-Zheng Feng, Yun Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: Nonrigid point set registration is widely applied in the tasks of computer vision and pattern recognition. Coherent point drift (CPD) is a classical method for nonrigid point set registration. However, to solve spatial transformation functions, CPD has to compute inversion of a M*M matrix per iteration with time complexity O(M3). By introducing a simple corresponding constraint, we develop a fast implementation of CPD. The most advantage of our method is to avoid matrix-inverse operation. Before the iteration begins, our method requires to take eigenvalue decomposition of a M*M matrix once. After iteration begins, our method only needs to update a diagonal matrix with linear computational complexity, and perform matrix multiplication operation with time complexity approximately O(M2) in each iteration. Besides, our method can be further accelerated by the low-rank matrix approximation. Experimental results in 3D point cloud data show that our method can significantly reduce computation burden of the registration process, and keep comparable performance with CPD on accuracy.



### RTEX: A novel methodology for Ranking, Tagging, and Explanatory diagnostic captioning of radiography exams
- **Arxiv ID**: http://arxiv.org/abs/2006.06316v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.06316v1)
- **Published**: 2020-06-11 10:29:44+00:00
- **Updated**: 2020-06-11 10:29:44+00:00
- **Authors**: Vasiliki Kougia, John Pavlopoulos, Panagiotis Papapetrou, Max Gordon
- **Comment**: None
- **Journal**: None
- **Summary**: This paper introduces RTEx, a novel methodology for a) ranking radiography exams based on their probability to contain an abnormality, b) generating abnormality tags for abnormal exams, and c) providing a diagnostic explanation in natural language for each abnormal exam. The task of ranking radiography exams is an important first step for practitioners who want to identify and prioritize those radiography exams that are more likely to contain abnormalities, for example, to avoid mistakes due to tiredness or to manage heavy workload (e.g., during a pandemic). We used two publicly available datasets to assess our methodology and demonstrate that for the task of ranking it outperforms its competitors in terms of NDCG@k. For each abnormal radiography exam RTEx generates a set of abnormality tags alongside an explanatory diagnostic text to explain the tags and guide the medical expert. Our tagging component outperforms two strong competitor methods in terms of F1. Moreover, the diagnostic captioning component of RTEx, which exploits the already extracted tags to constrain the captioning process, outperforms all competitors with respect to clinical precision and recall.



### Hypernetwork-Based Augmentation
- **Arxiv ID**: http://arxiv.org/abs/2006.06320v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.06320v2)
- **Published**: 2020-06-11 10:36:39+00:00
- **Updated**: 2021-10-07 02:56:46+00:00
- **Authors**: Chih-Yang Chen, Che-Han Chang
- **Comment**: None
- **Journal**: None
- **Summary**: Data augmentation is an effective technique to improve the generalization of deep neural networks. Recently, AutoAugment proposed a well-designed search space and a search algorithm that automatically finds augmentation policies in a data-driven manner. However, AutoAugment is computationally intensive. In this paper, we propose an efficient gradient-based search algorithm, called Hypernetwork-Based Augmentation (HBA), which simultaneously learns model parameters and augmentation hyperparameters in a single training. Our HBA uses a hypernetwork to approximate a population-based training algorithm, which enables us to tune augmentation hyperparameters by gradient descent. Besides, we introduce a weight sharing strategy that simplifies our hypernetwork architecture and speeds up our search algorithm. We conduct experiments on CIFAR-10, CIFAR-100, SVHN, and ImageNet. Our results show that HBA is competitive to the state-of-the-art methods in terms of both search speed and accuracy.



### A Deep Learning Framework for Recognizing both Static and Dynamic Gestures
- **Arxiv ID**: http://arxiv.org/abs/2006.06321v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2006.06321v2)
- **Published**: 2020-06-11 10:39:02+00:00
- **Updated**: 2021-03-17 10:31:16+00:00
- **Authors**: Osama Mazhar, Sofiane Ramdani, Andrea Cherubini
- **Comment**: 19 pages - Accepted in MDPI Sensors: Sensors and Robotics
- **Journal**: None
- **Summary**: Intuitive user interfaces are indispensable to interact with the human centric smart environments. In this paper, we propose a unified framework that recognizes both static and dynamic gestures, using simple RGB vision (without depth sensing). This feature makes it suitable for inexpensive human-robot interaction in social or industrial settings. We employ a pose-driven spatial attention strategy, which guides our proposed Static and Dynamic gestures Network - StaDNet. From the image of the human upper body, we estimate his/her depth, along with the region-of-interest around his/her hands. The Convolutional Neural Network in StaDNet is fine-tuned on a background-substituted hand gestures dataset. It is utilized to detect 10 static gestures for each hand as well as to obtain the hand image-embeddings. These are subsequently fused with the augmented pose vector and then passed to the stacked Long Short-Term Memory blocks. Thus, human-centred frame-wise information from the augmented pose vector and from the left/right hands image-embeddings are aggregated in time to predict the dynamic gestures of the performing person. In a number of experiments, we show that the proposed approach surpasses the state-of-the-art results on the large-scale Chalearn 2016 dataset. Moreover, we transfer the knowledge learned through the proposed methodology to the Praxis gestures dataset, and the obtained results also outscore the state-of-the-art on this dataset.



### CoMIR: Contrastive Multimodal Image Representation for Registration
- **Arxiv ID**: http://arxiv.org/abs/2006.06325v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.06325v2)
- **Published**: 2020-06-11 10:51:33+00:00
- **Updated**: 2020-10-23 08:54:38+00:00
- **Authors**: Nicolas Pielawski, Elisabeth Wetzer, Johan √ñfverstedt, Jiahao Lu, Carolina W√§hlby, Joakim Lindblad, Nata≈°a Sladoje
- **Comment**: 21 pages, 11 figures
- **Journal**: NeurIPS 2020
- **Summary**: We propose contrastive coding to learn shared, dense image representations, referred to as CoMIRs (Contrastive Multimodal Image Representations). CoMIRs enable the registration of multimodal images where existing registration methods often fail due to a lack of sufficiently similar image structures. CoMIRs reduce the multimodal registration problem to a monomodal one, in which general intensity-based, as well as feature-based, registration algorithms can be applied. The method involves training one neural network per modality on aligned images, using a contrastive loss based on noise-contrastive estimation (InfoNCE). Unlike other contrastive coding methods, used for, e.g., classification, our approach generates image-like representations that contain the information shared between modalities. We introduce a novel, hyperparameter-free modification to InfoNCE, to enforce rotational equivariance of the learnt representations, a property essential to the registration task. We assess the extent of achieved rotational equivariance and the stability of the representations with respect to weight initialization, training set, and hyperparameter settings, on a remote sensing dataset of RGB and near-infrared images. We evaluate the learnt representations through registration of a biomedical dataset of bright-field and second-harmonic generation microscopy images; two modalities with very little apparent correlation. The proposed approach based on CoMIRs significantly outperforms registration of representations created by GAN-based image-to-image translation, as well as a state-of-the-art, application-specific method which takes additional knowledge about the data into account. Code is available at: https://github.com/MIDA-group/CoMIR.



### Adversarial Attack Vulnerability of Medical Image Analysis Systems: Unexplored Factors
- **Arxiv ID**: http://arxiv.org/abs/2006.06356v3
- **DOI**: 10.1016/j.media.2021.102141
- **Categories**: **cs.CR**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.06356v3)
- **Published**: 2020-06-11 12:19:39+00:00
- **Updated**: 2021-06-17 12:50:56+00:00
- **Authors**: Gerda Bortsova, Cristina Gonz√°lez-Gonzalo, Suzanne C. Wetstein, Florian Dubost, Ioannis Katramados, Laurens Hogeweg, Bart Liefers, Bram van Ginneken, Josien P. W. Pluim, Mitko Veta, Clara I. S√°nchez, Marleen de Bruijne
- **Comment**: First three authors contributed equally
- **Journal**: Medical Image Analysis. Available online 18 Jun 2021
- **Summary**: Adversarial attacks are considered a potentially serious security threat for machine learning systems. Medical image analysis (MedIA) systems have recently been argued to be vulnerable to adversarial attacks due to strong financial incentives and the associated technological infrastructure.   In this paper, we study previously unexplored factors affecting adversarial attack vulnerability of deep learning MedIA systems in three medical domains: ophthalmology, radiology, and pathology. We focus on adversarial black-box settings, in which the attacker does not have full access to the target model and usually uses another model, commonly referred to as surrogate model, to craft adversarial examples. We consider this to be the most realistic scenario for MedIA systems.   Firstly, we study the effect of weight initialization (ImageNet vs. random) on the transferability of adversarial attacks from the surrogate model to the target model. Secondly, we study the influence of differences in development data between target and surrogate models. We further study the interaction of weight initialization and data differences with differences in model architecture. All experiments were done with a perturbation degree tuned to ensure maximal transferability at minimal visual perceptibility of the attacks.   Our experiments show that pre-training may dramatically increase the transferability of adversarial examples, even when the target and surrogate's architectures are different: the larger the performance gain using pre-training, the larger the transferability. Differences in the development data between target and surrogate models considerably decrease the performance of the attack; this decrease is further amplified by difference in the model architecture. We believe these factors should be considered when developing security-critical MedIA systems planned to be deployed in clinical practice.



### TensorFlow with user friendly Graphical Framework for object detection API
- **Arxiv ID**: http://arxiv.org/abs/2006.06385v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.06385v1)
- **Published**: 2020-06-11 13:00:02+00:00
- **Updated**: 2020-06-11 13:00:02+00:00
- **Authors**: Heemoon Yoon, Sang-Hee Lee, Mira Park
- **Comment**: "The code of TF-GraF for TensorFlow object detection API is opened at
  https://github.com/boguss1225/ObjectDetectionGUI"
- **Journal**: None
- **Summary**: TensorFlow is an open-source framework for deep learning dataflow and contains application programming interfaces (APIs) of voice analysis, natural language process, and computer vision. Especially, TensorFlow object detection API in computer vision field has been widely applied to technologies of agriculture, engineering, and medicine but barriers to entry of the framework usage is still high through command-line interface (CLI) and code for amateurs and beginners of information technology (IT) field. Therefore, this is aim to develop an user friendly Graphical Framework for object detection API on TensorFlow which is called TensorFlow Graphical Framework (TF-GraF). The TF-GraF provides independent virtual environments according to user accounts in server-side, additionally, execution of data preprocessing, training, and evaluation without CLI in client-side. Furthermore, hyperparameter setting, real-time observation of training process, object visualization of test images, and metrics evaluations of test data can also be operated via TF-GraF. Especially, TF-GraF supports flexible model selection of SSD, Faster-RCNN, RFCN, and Mask-RCNN including convolutional neural networks (inceptions and ResNets) through GUI environment. Consequently, TF-GraF allows anyone, even without any previous knowledge of deep learning frameworks, to design, train and deploy machine intelligence models without coding. Since TF-GraF takes care of setting and configuration, it allows anyone to use deep learning technology for their project without spending time to install complex software and environment.



### Interpreting CNN for Low Complexity Learned Sub-pixel Motion Compensation in Video Coding
- **Arxiv ID**: http://arxiv.org/abs/2006.06392v1
- **DOI**: 10.1109/ICIP40778.2020.9191193
- **Categories**: **eess.IV**, cs.CC, cs.CV, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2006.06392v1)
- **Published**: 2020-06-11 13:10:20+00:00
- **Updated**: 2020-06-11 13:10:20+00:00
- **Authors**: Luka Murn, Saverio Blasi, Alan F. Smeaton, Noel E. O'Connor, Marta Mrak
- **Comment**: 27th IEEE International Conference on Image Processing, 25-28 Oct
  2020, Abu Dhabi, United Arab Emirates
- **Journal**: 2020 IEEE International Conference on Image Processing (ICIP),
  2020, pp. 798-802
- **Summary**: Deep learning has shown great potential in image and video compression tasks. However, it brings bit savings at the cost of significant increases in coding complexity, which limits its potential for implementation within practical applications. In this paper, a novel neural network-based tool is presented which improves the interpolation of reference samples needed for fractional precision motion compensation. Contrary to previous efforts, the proposed approach focuses on complexity reduction achieved by interpreting the interpolation filters learned by the networks. When the approach is implemented in the Versatile Video Coding (VVC) test model, up to 4.5% BD-rate saving for individual sequences is achieved compared with the baseline VVC, while the complexity of learned interpolation is significantly reduced compared to the application of full neural network.



### Convolutional neural networks compression with low rank and sparse tensor decompositions
- **Arxiv ID**: http://arxiv.org/abs/2006.06443v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.06443v1)
- **Published**: 2020-06-11 13:53:18+00:00
- **Updated**: 2020-06-11 13:53:18+00:00
- **Authors**: Pavel Kaloshin
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional neural networks show outstanding results in a variety of computer vision tasks. However, a neural network architecture design usually faces a trade-off between model performance and computational/memory complexity. For some real-world applications, it is crucial to develop models, which can be fast and light enough to run on edge systems and mobile devices. However, many modern architectures that demonstrate good performance don't satisfy inference time and storage limitation requirements. Thus, arises a problem of neural network compression to obtain a smaller and faster model, which is on par with the initial one.   In this work, we consider a neural network compression method based on tensor decompositions. Namely, we propose to approximate the convolutional layer weight with a tensor, which can be represented as a sum of low-rank and sparse components. The motivation for such approximation is based on the assumption that low-rank and sparse terms allow eliminating two different types of redundancy and thus yield a better compression rate. An efficient CPU implementation for the proposed method has been developed. Our algorithm has demonstrated up to 3.5x CPU layer speedup and 11x layer size reduction when compressing Resnet50 architecture for the image classification task.



### Morphing Attack Detection -- Database, Evaluation Platform and Benchmarking
- **Arxiv ID**: http://arxiv.org/abs/2006.06458v3
- **DOI**: 10.1109/TIFS.2020.3035252
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.06458v3)
- **Published**: 2020-06-11 14:11:09+00:00
- **Updated**: 2020-09-28 16:35:11+00:00
- **Authors**: Kiran Raja, Matteo Ferrara, Annalisa Franco, Luuk Spreeuwers, Illias Batskos, Florens de Wit Marta Gomez-Barrero, Ulrich Scherhag, Daniel Fischer, Sushma Venkatesh, Jag Mohan Singh, Guoqiang Li, Lo√Øc Bergeron, Sergey Isadskiy, Raghavendra Ramachandra, Christian Rathgeb, Dinusha Frings, Uwe Seidel, Fons Knopjes, Raymond Veldhuis, Davide Maltoni, Christoph Busch
- **Comment**: This paper is a pre-print. The article is accepted for publication in
  IEEE Transactions on Information Forensics and Security (TIFS)
- **Journal**: 10.1109/TIFS.2020.3035252
- **Summary**: Morphing attacks have posed a severe threat to Face Recognition System (FRS). Despite the number of advancements reported in recent works, we note serious open issues such as independent benchmarking, generalizability challenges and considerations to age, gender, ethnicity that are inadequately addressed. Morphing Attack Detection (MAD) algorithms often are prone to generalization challenges as they are database dependent. The existing databases, mostly of semi-public nature, lack in diversity in terms of ethnicity, various morphing process and post-processing pipelines. Further, they do not reflect a realistic operational scenario for Automated Border Control (ABC) and do not provide a basis to test MAD on unseen data, in order to benchmark the robustness of algorithms. In this work, we present a new sequestered dataset for facilitating the advancements of MAD where the algorithms can be tested on unseen data in an effort to better generalize. The newly constructed dataset consists of facial images from 150 subjects from various ethnicities, age-groups and both genders. In order to challenge the existing MAD algorithms, the morphed images are with careful subject pre-selection created from the contributing images, and further post-processed to remove morphing artifacts. The images are also printed and scanned to remove all digital cues and to simulate a realistic challenge for MAD algorithms. Further, we present a new online evaluation platform to test algorithms on sequestered data. With the platform we can benchmark the morph detection performance and study the generalization ability. This work also presents a detailed analysis on various subsets of sequestered data and outlines open challenges for future directions in MAD research.



### Let's Face It: Probabilistic Multi-modal Interlocutor-aware Generation of Facial Gestures in Dyadic Settings
- **Arxiv ID**: http://arxiv.org/abs/2006.09888v2
- **DOI**: 10.1145/3383652.3423911
- **Categories**: **cs.CV**, cs.HC, cs.LG, cs.SD, eess.AS, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.09888v2)
- **Published**: 2020-06-11 14:11:51+00:00
- **Updated**: 2020-10-22 21:22:20+00:00
- **Authors**: Patrik Jonell, Taras Kucherenko, Gustav Eje Henter, Jonas Beskow
- **Comment**: Best Paper Award. 8 pages, 4 figures, IVA '20: Proceedings of the
  20th ACM International Conference on Intelligent Virtual Agent
- **Journal**: None
- **Summary**: To enable more natural face-to-face interactions, conversational agents need to adapt their behavior to their interlocutors. One key aspect of this is generation of appropriate non-verbal behavior for the agent, for example facial gestures, here defined as facial expressions and head movements. Most existing gesture-generating systems do not utilize multi-modal cues from the interlocutor when synthesizing non-verbal behavior. Those that do, typically use deterministic methods that risk producing repetitive and non-vivid motions. In this paper, we introduce a probabilistic method to synthesize interlocutor-aware facial gestures - represented by highly expressive FLAME parameters - in dyadic conversations. Our contributions are: a) a method for feature extraction from multi-party video and speech recordings, resulting in a representation that allows for independent control and manipulation of expression and speech articulation in a 3D avatar; b) an extension to MoGlow, a recent motion-synthesis method based on normalizing flows, to also take multi-modal signals from the interlocutor as input and subsequently output interlocutor-aware facial gestures; and c) a subjective evaluation assessing the use and relative importance of the input modalities. The results show that the model successfully leverages the input from the interlocutor to generate more appropriate behavior. Videos, data, and code available at: https://jonepatr.github.io/lets_face_it.



### Minimum Potential Energy of Point Cloud for Robust Global Registration
- **Arxiv ID**: http://arxiv.org/abs/2006.06460v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.06460v2)
- **Published**: 2020-06-11 14:13:40+00:00
- **Updated**: 2020-06-12 02:41:13+00:00
- **Authors**: Zijie Wu, Yaonan Wang, Qing Zhu, Jianxu Mao, Haotian Wu, Mingtao Feng, Ajmal Mian
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a novel minimum gravitational potential energy (MPE)-based algorithm for global point set registration. The feature descriptors extraction algorithms have emerged as the standard approach to align point sets in the past few decades. However, the alignment can be challenging to take effect when the point set suffers from raw point data problems such as noises (Gaussian and Uniformly). Different from the most existing point set registration methods which usually extract the descriptors to find correspondences between point sets, our proposed MPE alignment method is able to handle large scale raw data offset without depending on traditional descriptors extraction, whether for the local or global registration methods. We decompose the solution into a global optimal convex approximation and the fast descent process to a local minimum. For the approximation step, the proposed minimum potential energy (MPE) approach consists of two main steps. Firstly, according to the construction of the force traction operator, we could simply compute the position of the potential energy minimum; Secondly, with respect to the finding of the MPE point, we propose a new theory that employs the two flags to observe the status of the registration procedure. The method of fast descent process to the minimum that we employed is the iterative closest point algorithm; it can achieve the global minimum. We demonstrate the performance of the proposed algorithm on synthetic data as well as on real data. The proposed method outperforms the other global methods in terms of both efficiency, accuracy and noise resistance.



### Protecting Against Image Translation Deepfakes by Leaking Universal Perturbations from Black-Box Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2006.06493v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2006.06493v1)
- **Published**: 2020-06-11 15:02:27+00:00
- **Updated**: 2020-06-11 15:02:27+00:00
- **Authors**: Nataniel Ruiz, Sarah Adel Bargal, Stan Sclaroff
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we develop efficient disruptions of black-box image translation deepfake generation systems. We are the first to demonstrate black-box deepfake generation disruption by presenting image translation formulations of attacks initially proposed for classification models. Nevertheless, a naive adaptation of classification black-box attacks results in a prohibitive number of queries for image translation systems in the real-world. We present a frustratingly simple yet highly effective algorithm Leaking Universal Perturbations (LUP), that significantly reduces the number of queries needed to attack an image. LUP consists of two phases: (1) a short leaking phase where we attack the network using traditional black-box attacks and gather information on successful attacks on a small dataset and (2) and an exploitation phase where we leverage said information to subsequently attack the network with improved efficiency. Our attack reduces the total number of queries necessary to attack GANimation and StarGAN by 30%.



### Quantification of groundnut leaf defects using image processing algorithms
- **Arxiv ID**: http://arxiv.org/abs/2006.09887v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.09887v1)
- **Published**: 2020-06-11 15:07:12+00:00
- **Updated**: 2020-06-11 15:07:12+00:00
- **Authors**: Asharf, Balasubramanian E, Sankarasrinivasan S
- **Comment**: None
- **Journal**: None
- **Summary**: Identification, classification, and quantification of crop defects are of paramount of interest to the farmers for preventive measures and decrease the yield loss through necessary remedial actions. Due to the vast agricultural field, manual inspection of crops is tedious and time-consuming. UAV based data collection, observation, identification, and quantification of defected leaves area are considered to be an effective solution. The present work attempts to estimate the percentage of affected groundnut leaves area across four regions of Andharapradesh using image processing techniques. The proposed method involves colour space transformation combined with thresholding technique to perform the segmentation. The calibration measures are performed during acquisition with respect to UAV capturing distance, angle and other relevant camera parameters. Finally, our method can estimate the consolidated leaves and defected area. The image analysis results across these four regions reveal that around 14 - 28% of leaves area is affected across the groundnut field and thereby yield will be diminished correspondingly. Hence, it is recommended to spray the pesticides on the affected regions alone across the field to improve the plant growth and thereby yield will be increased.



### Rethinking the Truly Unsupervised Image-to-Image Translation
- **Arxiv ID**: http://arxiv.org/abs/2006.06500v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.06500v2)
- **Published**: 2020-06-11 15:15:12+00:00
- **Updated**: 2021-08-20 03:36:26+00:00
- **Authors**: Kyungjune Baek, Yunjey Choi, Youngjung Uh, Jaejun Yoo, Hyunjung Shim
- **Comment**: Accepted to ICCV 2021
- **Journal**: None
- **Summary**: Every recent image-to-image translation model inherently requires either image-level (i.e. input-output pairs) or set-level (i.e. domain labels) supervision. However, even set-level supervision can be a severe bottleneck for data collection in practice. In this paper, we tackle image-to-image translation in a fully unsupervised setting, i.e., neither paired images nor domain labels. To this end, we propose a truly unsupervised image-to-image translation model (TUNIT) that simultaneously learns to separate image domains and translates input images into the estimated domains. Experimental results show that our model achieves comparable or even better performance than the set-level supervised model trained with full labels, generalizes well on various datasets, and is robust against the choice of hyperparameters (e.g. the preset number of pseudo domains). Furthermore, TUNIT can be easily extended to semi-supervised learning with a few labeled data.



### Attentive WaveBlock: Complementarity-enhanced Mutual Networks for Unsupervised Domain Adaptation in Person Re-identification and Beyond
- **Arxiv ID**: http://arxiv.org/abs/2006.06525v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.06525v3)
- **Published**: 2020-06-11 15:40:40+00:00
- **Updated**: 2021-12-26 15:58:45+00:00
- **Authors**: Wenhao Wang, Fang Zhao, Shengcai Liao, Ling Shao
- **Comment**: Accept to Transaction on Image Processing
- **Journal**: None
- **Summary**: Unsupervised domain adaptation (UDA) for person re-identification is challenging because of the huge gap between the source and target domain. A typical self-training method is to use pseudo-labels generated by clustering algorithms to iteratively optimize the model on the target domain. However, a drawback to this is that noisy pseudo-labels generally cause trouble in learning. To address this problem, a mutual learning method by dual networks has been developed to produce reliable soft labels. However, as the two neural networks gradually converge, their complementarity is weakened and they likely become biased towards the same kind of noise. This paper proposes a novel light-weight module, the Attentive WaveBlock (AWB), which can be integrated into the dual networks of mutual learning to enhance the complementarity and further depress noise in the pseudo-labels. Specifically, we first introduce a parameter-free module, the WaveBlock, which creates a difference between features learned by two networks by waving blocks of feature maps differently. Then, an attention mechanism is leveraged to enlarge the difference created and discover more complementary features. Furthermore, two kinds of combination strategies, i.e. pre-attention and post-attention, are explored. Experiments demonstrate that the proposed method achieves state-of-the-art performance with significant improvements on multiple UDA person re-identification tasks. We also prove the generality of the proposed method by applying it to vehicle re-identification and image classification tasks. Our codes and models are available at https://github.com/WangWenhao0716/Attentive-WaveBlock.



### A Primer on Large Intelligent Surface (LIS) for Wireless Sensing in an Industrial Setting
- **Arxiv ID**: http://arxiv.org/abs/2006.06563v3
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.06563v3)
- **Published**: 2020-06-11 16:15:50+00:00
- **Updated**: 2020-11-16 15:21:42+00:00
- **Authors**: Cristian J. Vaca-Rubio, Pablo Ramirez-Espinosa, Robin Jess Williams, Kimmo Kansanen, Zheng-Hua Tan, Elisabeth de Carvalho, Petar Popovski
- **Comment**: None
- **Journal**: None
- **Summary**: One of the beyond-5G developments that is often highlighted is the integration of wireless communication and radio sensing. This paper addresses the potential of communication-sensing integration of Large Intelligent Surfaces (LIS) in an exemplary Industry 4.0 scenario. Besides the potential for high throughput and efficient multiplexing of wireless links, an LIS can offer a high-resolution rendering of the propagation environment. This is because, in an indoor setting, it can be placed in proximity to the sensed phenomena, while the high resolution is offered by densely spaced tiny antennas deployed over a large area. By treating an LIS as a radio image of the environment, we develop sensing techniques that leverage the usage of computer vision combined with machine learning. We test these methods for a scenario where we need to detect whether an industrial robot deviates from a predefined route. The results show that the LIS-based sensing offers high precision and has a high application potential in indoor industrial environments.



### Exploring Category-Agnostic Clusters for Open-Set Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2006.06567v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.06567v1)
- **Published**: 2020-06-11 16:19:02+00:00
- **Updated**: 2020-06-11 16:19:02+00:00
- **Authors**: Yingwei Pan, Ting Yao, Yehao Li, Chong-Wah Ngo, Tao Mei
- **Comment**: CVPR 2020
- **Journal**: None
- **Summary**: Unsupervised domain adaptation has received significant attention in recent years. Most of existing works tackle the closed-set scenario, assuming that the source and target domains share the exactly same categories. In practice, nevertheless, a target domain often contains samples of classes unseen in source domain (i.e., unknown class). The extension of domain adaptation from closed-set to such open-set situation is not trivial since the target samples in unknown class are not expected to align with the source. In this paper, we address this problem by augmenting the state-of-the-art domain adaptation technique, Self-Ensembling, with category-agnostic clusters in target domain. Specifically, we present Self-Ensembling with Category-agnostic Clusters (SE-CC) -- a novel architecture that steers domain adaptation with the additional guidance of category-agnostic clusters that are specific to target domain. These clustering information provides domain-specific visual cues, facilitating the generalization of Self-Ensembling for both closed-set and open-set scenarios. Technically, clustering is firstly performed over all the unlabeled target samples to obtain the category-agnostic clusters, which reveal the underlying data space structure peculiar to target domain. A clustering branch is capitalized on to ensure that the learnt representation preserves such underlying structure by matching the estimated assignment distribution over clusters to the inherent cluster distribution for each target sample. Furthermore, SE-CC enhances the learnt representation with mutual information maximization. Extensive experiments are conducted on Office and VisDA datasets for both open-set and closed-set domain adaptation, and superior results are reported when comparing to the state-of-the-art approaches.



### Learning a Unified Sample Weighting Network for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2006.06568v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.06568v2)
- **Published**: 2020-06-11 16:19:16+00:00
- **Updated**: 2020-06-14 05:30:43+00:00
- **Authors**: Qi Cai, Yingwei Pan, Yu Wang, Jingen Liu, Ting Yao, Tao Mei
- **Comment**: CVPR 2020; The source code and model are publicly available at:
  \url{https://github.com/caiqi/sample-weighting-network}
- **Journal**: None
- **Summary**: Region sampling or weighting is significantly important to the success of modern region-based object detectors. Unlike some previous works, which only focus on "hard" samples when optimizing the objective function, we argue that sample weighting should be data-dependent and task-dependent. The importance of a sample for the objective function optimization is determined by its uncertainties to both object classification and bounding box regression tasks. To this end, we devise a general loss function to cover most region-based object detectors with various sampling strategies, and then based on it we propose a unified sample weighting network to predict a sample's task weights. Our framework is simple yet effective. It leverages the samples' uncertainty distributions on classification loss, regression loss, IoU, and probability score, to predict sample weights. Our approach has several advantages: (i). It jointly learns sample weights for both classification and regression tasks, which differentiates it from most previous work. (ii). It is a data-driven process, so it avoids some manual parameter tuning. (iii). It can be effortlessly plugged into most object detectors and achieves noticeable performance improvements without affecting their inference time. Our approach has been thoroughly evaluated with recent object detection frameworks and it can consistently boost the detection accuracy. Code has been made available at \url{https://github.com/caiqi/sample-weighting-network}.



### Transferring and Regularizing Prediction for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2006.06570v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.06570v1)
- **Published**: 2020-06-11 16:19:41+00:00
- **Updated**: 2020-06-11 16:19:41+00:00
- **Authors**: Yiheng Zhang, Zhaofan Qiu, Ting Yao, Chong-Wah Ngo, Dong Liu, Tao Mei
- **Comment**: CVPR 2020
- **Journal**: None
- **Summary**: Semantic segmentation often requires a large set of images with pixel-level annotations. In the view of extremely expensive expert labeling, recent research has shown that the models trained on photo-realistic synthetic data (e.g., computer games) with computer-generated annotations can be adapted to real images. Despite this progress, without constraining the prediction on real images, the models will easily overfit on synthetic data due to severe domain mismatch. In this paper, we novelly exploit the intrinsic properties of semantic segmentation to alleviate such problem for model transfer. Specifically, we present a Regularizer of Prediction Transfer (RPT) that imposes the intrinsic properties as constraints to regularize model transfer in an unsupervised fashion. These constraints include patch-level, cluster-level and context-level semantic prediction consistencies at different levels of image formation. As the transfer is label-free and data-driven, the robustness of prediction is addressed by selectively involving a subset of image regions for model regularization. Extensive experiments are conducted to verify the proposal of RPT on the transfer of models trained on GTA5 and SYNTHIA (synthetic data) to Cityscapes dataset (urban street scenes). RPT shows consistent improvements when injecting the constraints on several neural networks for semantic segmentation. More remarkably, when integrating RPT into the adversarial-based segmentation framework, we report to-date the best results: mIoU of 53.2%/51.7% when transferring from GTA5/SYNTHIA to Cityscapes, respectively.



### Spectral Image Segmentation with Global Appearance Modeling
- **Arxiv ID**: http://arxiv.org/abs/2006.06573v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, I.4; I.5
- **Links**: [PDF](http://arxiv.org/pdf/2006.06573v2)
- **Published**: 2020-06-11 16:21:54+00:00
- **Updated**: 2022-10-06 22:14:05+00:00
- **Authors**: Jeova F. S. Rocha Neto, Pedro F. Felzenszwalb
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a new spectral method for image segmentation that incorporates long range relationships for global appearance modeling. The approach combines two different graphs, one is a sparse graph that captures spatial relationships between nearby pixels and another is a dense graph that captures pairwise similarity between all pairs of pixels. We extend the spectral method for Normalized Cuts to this setting by combining the transition matrices of Markov chains associated with each graph. We also derive an efficient method for sparsifying the dense graph of appearance relationships. This leads to a practical algorithm for segmenting high-resolution images. The resulting method can segment challenging images without any filtering or pre-processing.



### What makes instance discrimination good for transfer learning?
- **Arxiv ID**: http://arxiv.org/abs/2006.06606v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.06606v2)
- **Published**: 2020-06-11 16:55:07+00:00
- **Updated**: 2021-01-19 15:45:44+00:00
- **Authors**: Nanxuan Zhao, Zhirong Wu, Rynson W. H. Lau, Stephen Lin
- **Comment**: Accepted by ICLR 2021
- **Journal**: None
- **Summary**: Contrastive visual pretraining based on the instance discrimination pretext task has made significant progress. Notably, recent work on unsupervised pretraining has shown to surpass the supervised counterpart for finetuning downstream applications such as object detection and segmentation. It comes as a surprise that image annotations would be better left unused for transfer learning. In this work, we investigate the following problems: What makes instance discrimination pretraining good for transfer learning? What knowledge is actually learned and transferred from these models? From this understanding of instance discrimination, how can we better exploit human annotation labels for pretraining? Our findings are threefold. First, what truly matters for the transfer is low-level and mid-level representations, not high-level representations. Second, the intra-category invariance enforced by the traditional supervised model weakens transferability by increasing task misalignment. Finally, supervised pretraining can be strengthened by following an exemplar-based approach without explicit constraints among the instances within the same category.



### Training Generative Adversarial Networks with Limited Data
- **Arxiv ID**: http://arxiv.org/abs/2006.06676v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.06676v2)
- **Published**: 2020-06-11 17:06:34+00:00
- **Updated**: 2020-10-07 17:09:24+00:00
- **Authors**: Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, Timo Aila
- **Comment**: None
- **Journal**: None
- **Summary**: Training generative adversarial networks (GAN) using too little data typically leads to discriminator overfitting, causing training to diverge. We propose an adaptive discriminator augmentation mechanism that significantly stabilizes training in limited data regimes. The approach does not require changes to loss functions or network architectures, and is applicable both when training from scratch and when fine-tuning an existing GAN on another dataset. We demonstrate, on several datasets, that good results are now possible using only a few thousand training images, often matching StyleGAN2 results with an order of magnitude fewer images. We expect this to open up new application domains for GANs. We also find that the widely used CIFAR-10 is, in fact, a limited data benchmark, and improve the record FID from 5.59 to 2.42.



### Improving Deep Metric Learning with Virtual Classes and Examples Mining
- **Arxiv ID**: http://arxiv.org/abs/2006.06611v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.06611v1)
- **Published**: 2020-06-11 17:09:43+00:00
- **Updated**: 2020-06-11 17:09:43+00:00
- **Authors**: Pierre Jacob, David Picard, Aymeric Histace, Edouard Klein
- **Comment**: None
- **Journal**: None
- **Summary**: In deep metric learning, the training procedure relies on sampling informative tuples. However, as the training procedure progresses, it becomes nearly impossible to sample relevant hard negative examples without proper mining strategies or generation-based methods. Recent work on hard negative generation have shown great promises to solve the mining problem. However, this generation process is difficult to tune and often leads to incorrectly labelled examples. To tackle this issue, we introduce MIRAGE, a generation-based method that relies on virtual classes entirely composed of generated examples that act as buffer areas between the training classes. We empirically show that virtual classes significantly improve the results on popular datasets (Cub-200-2011, Cars-196 and Stanford Online Products) compared to other generation methods.



### MatchGAN: A Self-Supervised Semi-Supervised Conditional Generative Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/2006.06614v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.06614v2)
- **Published**: 2020-06-11 17:14:55+00:00
- **Updated**: 2020-10-08 18:57:44+00:00
- **Authors**: Jiaze Sun, Binod Bhattarai, Tae-Kyun Kim
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel self-supervised learning approach for conditional generative adversarial networks (GANs) under a semi-supervised setting. Unlike prior self-supervised approaches which often involve geometric augmentations on the image space such as predicting rotation angles, our pretext task leverages the label space. We perform augmentation by randomly sampling sensible labels from the label space of the few labelled examples available and assigning them as target labels to the abundant unlabelled examples from the same distribution as that of the labelled ones. The images are then translated and grouped into positive and negative pairs by their target labels, acting as training examples for our pretext task which involves optimising an auxiliary match loss on the discriminator's side. We tested our method on two challenging benchmarks, CelebA and RaFD, and evaluated the results using standard metrics including Fr\'{e}chet Inception Distance, Inception Score, and Attribute Classification Rate. Extensive empirical evaluation demonstrates the effectiveness of our proposed method over competitive baselines and existing arts. In particular, our method surpasses the baseline with only 20% of the labelled examples used to train the baseline.



### SLIC-UAV: A Method for monitoring recovery in tropical restoration projects through identification of signature species using UAVs
- **Arxiv ID**: http://arxiv.org/abs/2006.06624v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.06624v1)
- **Published**: 2020-06-11 17:22:56+00:00
- **Updated**: 2020-06-11 17:22:56+00:00
- **Authors**: Jonathan Williams, Carola-Bibiane Sch√∂nlieb, Tom Swinfield, Bambang Irawan, Eva Achmad, Muhammad Zudhi, Habibi, Elva Gemita, David A. Coomes
- **Comment**: None
- **Journal**: None
- **Summary**: Logged forests cover four million square kilometres of the tropics and restoring these forests is essential if we are to avoid the worst impacts of climate change, yet monitoring recovery is challenging. Tracking the abundance of visually identifiable, early-successional species enables successional status and thereby restoration progress to be evaluated. Here we present a new pipeline, SLIC-UAV, for processing Unmanned Aerial Vehicle (UAV) imagery to map early-successional species in tropical forests. The pipeline is novel because it comprises: (a) a time-efficient approach for labelling crowns from UAV imagery; (b) machine learning of species based on spectral and textural features within individual tree crowns, and (c) automatic segmentation of orthomosaiced UAV imagery into 'superpixels', using Simple Linear Iterative Clustering (SLIC). Creating superpixels reduces the dataset's dimensionality and focuses prediction onto clusters of pixels, greatly improving accuracy. To demonstrate SLIC-UAV, support vector machines and random forests were used to predict the species of hand-labelled crowns in a restoration concession in Indonesia. Random forests were most accurate at discriminating species for whole crowns, with accuracy ranging from 79.3% when mapping five common species, to 90.5% when mapping the three most visually-distinctive species. In contrast, support vector machines proved better for labelling automatically segmented superpixels, with accuracy ranging from 74.3% to 91.7% for the same species. Models were extended to map species across 100 hectares of forest. The study demonstrates the power of SLIC-UAV for mapping characteristic early-successional tree species as an indicator of successional stage within tropical forest restoration areas. Continued effort is needed to develop easy-to-implement and low-cost technology to improve the affordability of project management.



### Diagnosis and Analysis of Celiac Disease and Environmental Enteropathy on Biopsy Images using Deep Learning Approaches
- **Arxiv ID**: http://arxiv.org/abs/2006.06627v1
- **DOI**: 10.18130/v3-837s-3a79
- **Categories**: **cs.LG**, cs.CV, eess.IV, q-bio.TO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.06627v1)
- **Published**: 2020-06-11 17:25:29+00:00
- **Updated**: 2020-06-11 17:25:29+00:00
- **Authors**: Kamran Kowsari
- **Comment**: PhD dissertation, Univ Virginia (May 2020)
- **Journal**: None
- **Summary**: Celiac Disease (CD) and Environmental Enteropathy (EE) are common causes of malnutrition and adversely impact normal childhood development. Both conditions require a tissue biopsy for diagnosis and a major challenge of interpreting clinical biopsy images to differentiate between these gastrointestinal diseases is striking histopathologic overlap between them. In the current study, we propose four diagnosis techniques for these diseases and address their limitations and advantages. First, the diagnosis between CD, EE, and Normal biopsies is considered, but the main challenge with this diagnosis technique is the staining problem. The dataset used in this research is collected from different centers with different staining standards. To solve this problem, we use color balancing in order to train our model with a varying range of colors. Random Multimodel Deep Learning (RMDL) architecture has been used as another approach to mitigate the effects of the staining problem. RMDL combines different architectures and structures of deep learning and the final output of the model is based on the majority vote. CD is a chronic autoimmune disease that affects the small intestine genetically predisposed children and adults. Typically, CD rapidly progress from Marsh I to IIIa. Marsh III is sub-divided into IIIa (partial villus atrophy), Marsh IIIb (subtotal villous atrophy), and Marsh IIIc (total villus atrophy) to explain the spectrum of villus atrophy along with crypt hypertrophy and increased intraepithelial lymphocytes. In the second part of this study, we proposed two ways for diagnosing different stages of CD. Finally, in the third part of this study, these two steps are combined as Hierarchical Medical Image Classification (HMIC) to have a model to diagnose the disease data hierarchically.



### Real-Time Video Inference on Edge Devices via Adaptive Model Streaming
- **Arxiv ID**: http://arxiv.org/abs/2006.06628v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NI, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.06628v2)
- **Published**: 2020-06-11 17:25:44+00:00
- **Updated**: 2021-04-05 23:29:53+00:00
- **Authors**: Mehrdad Khani, Pouya Hamadanian, Arash Nasr-Esfahany, Mohammad Alizadeh
- **Comment**: None
- **Journal**: None
- **Summary**: Real-time video inference on edge devices like mobile phones and drones is challenging due to the high computation cost of Deep Neural Networks. We present Adaptive Model Streaming (AMS), a new approach to improving performance of efficient lightweight models for video inference on edge devices. AMS uses a remote server to continually train and adapt a small model running on the edge device, boosting its performance on the live video using online knowledge distillation from a large, state-of-the-art model. We discuss the challenges of over-the-network model adaptation for video inference, and present several techniques to reduce communication cost of this approach: avoiding excessive overfitting, updating a small fraction of important model parameters, and adaptive sampling of training frames at edge devices. On the task of video semantic segmentation, our experimental results show 0.4--17.8 percent mean Intersection-over-Union improvement compared to a pre-trained model across several video datasets. Our prototype can perform video segmentation at 30 frames-per-second with 40 milliseconds camera-to-label latency on a Samsung Galaxy S10+ mobile phone, using less than 300 Kbps uplink and downlink bandwidth on the device.



### Privacy-Preserving Image Features via Adversarial Affine Subspace Embeddings
- **Arxiv ID**: http://arxiv.org/abs/2006.06634v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.06634v3)
- **Published**: 2020-06-11 17:29:48+00:00
- **Updated**: 2021-03-30 09:46:40+00:00
- **Authors**: Mihai Dusmanu, Johannes L. Sch√∂nberger, Sudipta N. Sinha, Marc Pollefeys
- **Comment**: Accepted at CVPR 2021. 16 pages, 10 figures, 4 tables
- **Journal**: None
- **Summary**: Many computer vision systems require users to upload image features to the cloud for processing and storage. These features can be exploited to recover sensitive information about the scene or subjects, e.g., by reconstructing the appearance of the original image. To address this privacy concern, we propose a new privacy-preserving feature representation. The core idea of our work is to drop constraints from each feature descriptor by embedding it within an affine subspace containing the original feature as well as adversarial feature samples. Feature matching on the privacy-preserving representation is enabled based on the notion of subspace-to-subspace distance. We experimentally demonstrate the effectiveness of our method and its high practical relevance for the applications of visual localization and mapping as well as face authentication. Compared to the original features, our approach makes it significantly more difficult for an adversary to recover private information.



### Exploring Weaknesses of VQA Models through Attribution Driven Insights
- **Arxiv ID**: http://arxiv.org/abs/2006.06637v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2006.06637v2)
- **Published**: 2020-06-11 17:30:07+00:00
- **Updated**: 2020-06-16 12:01:03+00:00
- **Authors**: Shaunak Halbe
- **Comment**: Second Grand-Challenge and Workshop on Multimodal Language, ACL 2020
- **Journal**: None
- **Summary**: Deep Neural Networks have been successfully used for the task of Visual Question Answering for the past few years owing to the availability of relevant large scale datasets. However these datasets are created in artificial settings and rarely reflect the real world scenario. Recent research effectively applies these VQA models for answering visual questions for the blind. Despite achieving high accuracy these models appear to be susceptible to variation in input questions.We analyze popular VQA models through the lens of attribution (input's influence on predictions) to gain valuable insights. Further, We use these insights to craft adversarial attacks which inflict significant damage to these systems with negligible change in meaning of the input questions. We believe this will enhance development of systems more robust to the possible variations in inputs when deployed to assist the visually impaired.



### Interpretable Visualizations with Differentiating Embedding Networks
- **Arxiv ID**: http://arxiv.org/abs/2006.06640v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.06640v1)
- **Published**: 2020-06-11 17:30:44+00:00
- **Updated**: 2020-06-11 17:30:44+00:00
- **Authors**: Isaac Robinson
- **Comment**: 10 pages, 4 figures, under review
- **Journal**: None
- **Summary**: We present a visualization algorithm based on a novel unsupervised Siamese neural network training regime and loss function, called Differentiating Embedding Networks (DEN). The Siamese neural network finds differentiating or similar features between specific pairs of samples in a dataset, and uses these features to embed the dataset in a lower dimensional space where it can be visualized. Unlike existing visualization algorithms such as UMAP or $t$-SNE, DEN is parametric, meaning it can be interpreted by techniques such as SHAP. To interpret DEN, we create an end-to-end parametric clustering algorithm on top of the visualization, and then leverage SHAP scores to determine which features in the sample space are important for understanding the structures shown in the visualization based on the clusters found. We compare DEN visualizations with existing techniques on a variety of datasets, including image and scRNA-seq data. We then show that our clustering algorithm performs similarly to the state of the art despite not having prior knowledge of the number of clusters, and sets a new state of the art on FashionMNIST. Finally, we demonstrate finding differentiating features of a dataset. Code available at https://github.com/isaacrob/DEN



### Closed Loop Neural-Symbolic Learning via Integrating Neural Perception, Grammar Parsing, and Symbolic Reasoning
- **Arxiv ID**: http://arxiv.org/abs/2006.06649v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.06649v2)
- **Published**: 2020-06-11 17:42:49+00:00
- **Updated**: 2020-07-27 22:17:10+00:00
- **Authors**: Qing Li, Siyuan Huang, Yining Hong, Yixin Chen, Ying Nian Wu, Song-Chun Zhu
- **Comment**: ICML 2020. Project page: https://liqing-ustc.github.io/NGS
- **Journal**: None
- **Summary**: The goal of neural-symbolic computation is to integrate the connectionist and symbolist paradigms. Prior methods learn the neural-symbolic models using reinforcement learning (RL) approaches, which ignore the error propagation in the symbolic reasoning module and thus converge slowly with sparse rewards. In this paper, we address these issues and close the loop of neural-symbolic learning by (1) introducing the \textbf{grammar} model as a \textit{symbolic prior} to bridge neural perception and symbolic reasoning, and (2) proposing a novel \textbf{back-search} algorithm which mimics the top-down human-like learning procedure to propagate the error through the symbolic reasoning module efficiently. We further interpret the proposed learning framework as maximum likelihood estimation using Markov chain Monte Carlo sampling and the back-search algorithm as a Metropolis-Hastings sampler. The experiments are conducted on two weakly-supervised neural-symbolic tasks: (1) handwritten formula recognition on the newly introduced HWF dataset; (2) visual question answering on the CLEVR dataset. The results show that our approach significantly outperforms the RL methods in terms of performance, converging speed, and data efficiency. Our code and data are released at \url{https://liqing-ustc.github.io/NGS}.



### Robust Multi-object Matching via Iterative Reweighting of the Graph Connection Laplacian
- **Arxiv ID**: http://arxiv.org/abs/2006.06658v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.NA, math.NA, math.PR, stat.ML, 90C26, 90C10, 90C17, 68Q87, 65C20, G.1.6; I.4.0
- **Links**: [PDF](http://arxiv.org/pdf/2006.06658v2)
- **Published**: 2020-06-11 17:53:01+00:00
- **Updated**: 2020-10-24 20:54:12+00:00
- **Authors**: Yunpeng Shi, Shaohan Li, Gilad Lerman
- **Comment**: None
- **Journal**: Advances in Neural Information Processing Systems (NeurIPS) 33,
  15243--15253 (2020)
- **Summary**: We propose an efficient and robust iterative solution to the multi-object matching problem. We first clarify serious limitations of current methods as well as the inappropriateness of the standard iteratively reweighted least squares procedure. In view of these limitations, we suggest a novel and more reliable iterative reweighting strategy that incorporates information from higher-order neighborhoods by exploiting the graph connection Laplacian. We demonstrate the superior performance of our procedure over state-of-the-art methods using both synthetic and real datasets.



### Quasi-Dense Similarity Learning for Multiple Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/2006.06664v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.06664v4)
- **Published**: 2020-06-11 17:57:12+00:00
- **Updated**: 2021-09-08 02:56:35+00:00
- **Authors**: Jiangmiao Pang, Linlu Qiu, Xia Li, Haofeng Chen, Qi Li, Trevor Darrell, Fisher Yu
- **Comment**: CVPR 2021 oral paper; Multiple object tracking on large-scale
  datasets
- **Journal**: None
- **Summary**: Similarity learning has been recognized as a crucial step for object tracking. However, existing multiple object tracking methods only use sparse ground truth matching as the training objective, while ignoring the majority of the informative regions on the images. In this paper, we present Quasi-Dense Similarity Learning, which densely samples hundreds of region proposals on a pair of images for contrastive learning. We can directly combine this similarity learning with existing detection methods to build Quasi-Dense Tracking (QDTrack) without turning to displacement regression or motion priors. We also find that the resulting distinctive feature space admits a simple nearest neighbor search at the inference time. Despite its simplicity, QDTrack outperforms all existing methods on MOT, BDD100K, Waymo, and TAO tracking benchmarks. It achieves 68.7 MOTA at 20.3 FPS on MOT17 without using external training data. Compared to methods with similar detectors, it boosts almost 10 points of MOTA and significantly decreases the number of ID switches on BDD100K and Waymo datasets. Our code and trained models are available at http://vis.xyz/pub/qdtrack.



### VirTex: Learning Visual Representations from Textual Annotations
- **Arxiv ID**: http://arxiv.org/abs/2006.06666v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2006.06666v3)
- **Published**: 2020-06-11 17:58:48+00:00
- **Updated**: 2021-09-25 23:45:16+00:00
- **Authors**: Karan Desai, Justin Johnson
- **Comment**: CVPR 2021. [v3: better captioning results on fixing beam search bug]
- **Journal**: None
- **Summary**: The de-facto approach to many vision tasks is to start from pretrained visual representations, typically learned via supervised training on ImageNet. Recent methods have explored unsupervised pretraining to scale to vast quantities of unlabeled images. In contrast, we aim to learn high-quality visual representations from fewer images. To this end, we revisit supervised pretraining, and seek data-efficient alternatives to classification-based pretraining. We propose VirTex -- a pretraining approach using semantically dense captions to learn visual representations. We train convolutional networks from scratch on COCO Captions, and transfer them to downstream recognition tasks including image classification, object detection, and instance segmentation. On all tasks, VirTex yields features that match or exceed those learned on ImageNet -- supervised or unsupervised -- despite using up to ten times fewer images.



### Disentangled Non-Local Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2006.06668v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.06668v2)
- **Published**: 2020-06-11 17:59:22+00:00
- **Updated**: 2020-09-08 14:12:09+00:00
- **Authors**: Minghao Yin, Zhuliang Yao, Yue Cao, Xiu Li, Zheng Zhang, Stephen Lin, Han Hu
- **Comment**: None
- **Journal**: None
- **Summary**: The non-local block is a popular module for strengthening the context modeling ability of a regular convolutional neural network. This paper first studies the non-local block in depth, where we find that its attention computation can be split into two terms, a whitened pairwise term accounting for the relationship between two pixels and a unary term representing the saliency of every pixel. We also observe that the two terms trained alone tend to model different visual clues, e.g. the whitened pairwise term learns within-region relationships while the unary term learns salient boundaries. However, the two terms are tightly coupled in the non-local block, which hinders the learning of each. Based on these findings, we present the disentangled non-local block, where the two terms are decoupled to facilitate learning for both terms. We demonstrate the effectiveness of the decoupled design on various tasks, such as semantic segmentation on Cityscapes, ADE20K and PASCAL Context, object detection on COCO, and action recognition on Kinetics.



### Understanding Human Hands in Contact at Internet Scale
- **Arxiv ID**: http://arxiv.org/abs/2006.06669v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.06669v1)
- **Published**: 2020-06-11 17:59:30+00:00
- **Updated**: 2020-06-11 17:59:30+00:00
- **Authors**: Dandan Shan, Jiaqi Geng, Michelle Shu, David F. Fouhey
- **Comment**: To appear at CVPR 2020 (Oral). Project and dataset webpage:
  http://fouheylab.eecs.umich.edu/~dandans/projects/100DOH/
- **Journal**: None
- **Summary**: Hands are the central means by which humans manipulate their world and being able to reliably extract hand state information from Internet videos of humans engaged in their hands has the potential to pave the way to systems that can learn from petabytes of video data. This paper proposes steps towards this by inferring a rich representation of hands engaged in interaction method that includes: hand location, side, contact state, and a box around the object in contact. To support this effort, we gather a large-scale dataset of hands in contact with objects consisting of 131 days of footage as well as a 100K annotated hand-contact video frame dataset. The learned model on this dataset can serve as a foundation for hand-contact understanding in videos. We quantitatively evaluate it both on its own and in service of predicting and learning from 3D meshes of human hands.



### Data Driven Prediction Architecture for Autonomous Driving and its Application on Apollo Platform
- **Arxiv ID**: http://arxiv.org/abs/2006.06715v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.SE
- **Links**: [PDF](http://arxiv.org/pdf/2006.06715v1)
- **Published**: 2020-06-11 18:16:12+00:00
- **Updated**: 2020-06-11 18:16:12+00:00
- **Authors**: Kecheng Xu, Xiangquan Xiao, Jinghao Miao, Qi Luo
- **Comment**: Accepted by the 31st IEEE Intelligent Vehicles Symposium (2020)
- **Journal**: None
- **Summary**: Autonomous Driving vehicles (ADV) are on road with large scales. For safe and efficient operations, ADVs must be able to predict the future states and iterative with road entities in complex, real-world driving scenarios. How to migrate a well-trained prediction model from one geo-fenced area to another is essential in scaling the ADV operation and is difficult most of the time since the terrains, traffic rules, entities distributions, driving/walking patterns would be largely different in different geo-fenced operation areas. In this paper, we introduce a highly automated learning-based prediction model pipeline, which has been deployed on Baidu Apollo self-driving platform, to support different prediction learning sub-modules' data annotation, feature extraction, model training/tuning and deployment. This pipeline is completely automatic without any human intervention and shows an up to 400\% efficiency increase in parameter tuning, when deployed at scale in different scenarios across nations.



### Gaze estimation problem tackled through synthetic images
- **Arxiv ID**: http://arxiv.org/abs/2006.06740v1
- **DOI**: 10.1145/3379156.3391368
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.06740v1)
- **Published**: 2020-06-11 18:53:51+00:00
- **Updated**: 2020-06-11 18:53:51+00:00
- **Authors**: Gonzalo Garde, Andoni Larumbe-Bergera, Beno√Æt Bossavit, Rafael Cabeza, Sonia Porta, Arantxa Villanueva
- **Comment**: https://dl.acm.org/doi/abs/10.1145/3379156.3391368
- **Journal**: ETRA '20 Short Papers: ACM Symposium on Eye Tracking Research and
  Applications; June 2020; Article No.: 16; Pages 1 to 5
- **Summary**: In this paper, we evaluate a synthetic framework to be used in the field of gaze estimation employing deep learning techniques. The lack of sufficient annotated data could be overcome by the utilization of a synthetic evaluation framework as far as it resembles the behavior of a real scenario. In this work, we use U2Eyes synthetic environment employing I2Head datataset as real benchmark for comparison based on alternative training and testing strategies. The results obtained show comparable average behavior between both frameworks although significantly more robust and stable performance is retrieved by the synthetic images. Additionally, the potential of synthetically pretrained models in order to be applied in user's specific calibration strategies is shown with outstanding performances.



### Deep Convolutional Likelihood Particle Filter for Visual Tracking
- **Arxiv ID**: http://arxiv.org/abs/2006.06746v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.06746v1)
- **Published**: 2020-06-11 19:02:27+00:00
- **Updated**: 2020-06-11 19:02:27+00:00
- **Authors**: Reza Jalil Mozhdehi, Henry Medeiros
- **Comment**: Accepted in Transactions on Computational Science & Computational
  Intelligence, 11 pages, 7 figures
- **Journal**: None
- **Summary**: We propose a novel particle filter for convolutional-correlation visual trackers. Our method uses correlation response maps to estimate likelihood distributions and employs these likelihoods as proposal densities to sample particles. Likelihood distributions are more reliable than proposal densities based on target transition distributions because correlation response maps provide additional information regarding the target's location. Additionally, our particle filter searches for multiple modes in the likelihood distribution, which improves performance in target occlusion scenarios while decreasing computational costs by more efficiently sampling particles. In other challenging scenarios such as those involving motion blur, where only one mode is present but a larger search area may be necessary, our particle filter allows for the variance of the likelihood distribution to increase. We tested our algorithm on the Visual Tracker Benchmark v1.1 (OTB100) and our experimental results demonstrate that our framework outperforms state-of-the-art methods.



### An Unsupervised Information-Theoretic Perceptual Quality Metric
- **Arxiv ID**: http://arxiv.org/abs/2006.06752v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.06752v3)
- **Published**: 2020-06-11 19:11:28+00:00
- **Updated**: 2021-01-10 19:28:57+00:00
- **Authors**: Sangnie Bhardwaj, Ian Fischer, Johannes Ball√©, Troy Chinen
- **Comment**: 19 pages, 10 figures. Presented at NeurIPS 2020. Code available at
  https://github.com/google-research/perceptual-quality
- **Journal**: None
- **Summary**: Tractable models of human perception have proved to be challenging to build. Hand-designed models such as MS-SSIM remain popular predictors of human image quality judgements due to their simplicity and speed. Recent modern deep learning approaches can perform better, but they rely on supervised data which can be costly to gather: large sets of class labels such as ImageNet, image quality ratings, or both. We combine recent advances in information-theoretic objective functions with a computational architecture informed by the physiology of the human visual system and unsupervised training on pairs of video frames, yielding our Perceptual Information Metric (PIM). We show that PIM is competitive with supervised metrics on the recent and challenging BAPPS image quality assessment dataset and outperforms them in predicting the ranking of image compression methods in CLIC 2020. We also perform qualitative experiments using the ImageNet-C dataset, and establish that PIM is robust with respect to architectural details.



### PRGFlow: Benchmarking SWAP-Aware Unified Deep Visual Inertial Odometry
- **Arxiv ID**: http://arxiv.org/abs/2006.06753v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2006.06753v1)
- **Published**: 2020-06-11 19:12:54+00:00
- **Updated**: 2020-06-11 19:12:54+00:00
- **Authors**: Nitin J. Sanket, Chahat Deep Singh, Cornelia Ferm√ºller, Yiannis Aloimonos
- **Comment**: 16 pages, 13 figures, 10 tables. Under review T-RO
- **Journal**: None
- **Summary**: Odometry on aerial robots has to be of low latency and high robustness whilst also respecting the Size, Weight, Area and Power (SWAP) constraints as demanded by the size of the robot. A combination of visual sensors coupled with Inertial Measurement Units (IMUs) has proven to be the best combination to obtain robust and low latency odometry on resource-constrained aerial robots. Recently, deep learning approaches for Visual Inertial fusion have gained momentum due to their high accuracy and robustness. However, the remarkable advantages of these techniques are their inherent scalability (adaptation to different sized aerial robots) and unification (same method works on different sized aerial robots) by utilizing compression methods and hardware acceleration, which have been lacking from previous approaches.   To this end, we present a deep learning approach for visual translation estimation and loosely fuse it with an Inertial sensor for full 6DoF odometry estimation. We also present a detailed benchmark comparing different architectures, loss functions and compression methods to enable scalability. We evaluate our network on the MSCOCO dataset and evaluate the VI fusion on multiple real-flight trajectories.



### On Improving Temporal Consistency for Online Face Liveness Detection
- **Arxiv ID**: http://arxiv.org/abs/2006.06756v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.06756v1)
- **Published**: 2020-06-11 19:19:47+00:00
- **Updated**: 2020-06-11 19:19:47+00:00
- **Authors**: Xiang Xu, Yuanjun Xiong, Wei Xia
- **Comment**: technical report
- **Journal**: None
- **Summary**: In this paper, we focus on improving the online face liveness detection system to enhance the security of the downstream face recognition system. Most of the existing frame-based methods are suffering from the prediction inconsistency across time. To address the issue, a simple yet effective solution based on temporal consistency is proposed. Specifically, in the training stage, to integrate the temporal consistency constraint, a temporal self-supervision loss and a class consistency loss are proposed in addition to the softmax cross-entropy loss. In the deployment stage, a training-free non-parametric uncertainty estimation module is developed to smooth the predictions adaptively. Beyond the common evaluation approach, a video segment-based evaluation is proposed to accommodate more practical scenarios. Extensive experiments demonstrated that our solution is more robust against several presentation attacks in various scenarios, and significantly outperformed the state-of-the-art on multiple public datasets by at least 40% in terms of ACER. Besides, with much less computational complexity (33% fewer FLOPs), it provides great potential for low-latency online applications.



### One Ring to Rule Them All: Certifiably Robust Geometric Perception with Outliers
- **Arxiv ID**: http://arxiv.org/abs/2006.06769v2
- **DOI**: None
- **Categories**: **math.OC**, cs.CV, cs.RO, 68T40, 74Pxx, 46N10, 65D19, I.2.9; G.1.6; I.4.5
- **Links**: [PDF](http://arxiv.org/pdf/2006.06769v2)
- **Published**: 2020-06-11 19:46:42+00:00
- **Updated**: 2020-10-19 14:27:57+00:00
- **Authors**: Heng Yang, Luca Carlone
- **Comment**: NeurIPS 2020. 9 pages main results, 34 pages total. Code available at
  https://github.com/MIT-SPARK/CertifiablyRobustPerception
- **Journal**: 34th Conference on Neural Information Processing Systems (NeurIPS
  2020)
- **Summary**: We propose the first general and practical framework to design certifiable algorithms for robust geometric perception in the presence of a large amount of outliers. We investigate the use of a truncated least squares (TLS) cost function, which is known to be robust to outliers, but leads to hard, nonconvex, and nonsmooth optimization problems. Our first contribution is to show that -for a broad class of geometric perception problems- TLS estimation can be reformulated as an optimization over the ring of polynomials and Lasserre's hierarchy of convex moment relaxations is empirically tight at the minimum relaxation order (i.e., certifiably obtains the global minimum of the nonconvex TLS problem). Our second contribution is to exploit the structural sparsity of the objective and constraint polynomials and leverage basis reduction to significantly reduce the size of the semidefinite program (SDP) resulting from the moment relaxation, without compromising its tightness. Our third contribution is to develop scalable dual optimality certifiers from the lens of sums-of-squares (SOS) relaxation, that can compute the suboptimality gap and possibly certify global optimality of any candidate solution (e.g., returned by fast heuristics such as RANSAC or graduated non-convexity). Our dual certifiers leverage Douglas-Rachford Splitting to solve a convex feasibility SDP. Numerical experiments across different perception problems, including single rotation averaging, shape alignment, 3D point cloud and mesh registration, and high-integrity satellite pose estimation, demonstrate the tightness of our relaxations, the correctness of the certification, and the scalability of the proposed dual certifiers to large problems, beyond the reach of current SDP solvers.



### On Improving the Generalization of Face Recognition in the Presence of Occlusions
- **Arxiv ID**: http://arxiv.org/abs/2006.06787v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.06787v1)
- **Published**: 2020-06-11 20:17:23+00:00
- **Updated**: 2020-06-11 20:17:23+00:00
- **Authors**: Xiang Xu, Nikolaos Sarafianos, Ioannis A. Kakadiaris
- **Comment**: Technical Report
- **Journal**: None
- **Summary**: In this paper, we address a key limitation of existing 2D face recognition methods: robustness to occlusions. To accomplish this task, we systematically analyzed the impact of facial attributes on the performance of a state-of-the-art face recognition method and through extensive experimentation, quantitatively analyzed the performance degradation under different types of occlusion. Our proposed Occlusion-aware face REcOgnition (OREO) approach learned discriminative facial templates despite the presence of such occlusions. First, an attention mechanism was proposed that extracted local identity-related region. The local features were then aggregated with the global representations to form a single template. Second, a simple, yet effective, training strategy was introduced to balance the non-occluded and occluded facial images. Extensive experiments demonstrated that OREO improved the generalization ability of face recognition under occlusions by (10.17%) in a single-image-based setting and outperformed the baseline by approximately (2%) in terms of rank-1 accuracy in an image-set-based scenario.



### Multigrid-in-Channels Architectures for Wide Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2006.06799v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.06799v2)
- **Published**: 2020-06-11 20:28:36+00:00
- **Updated**: 2020-11-19 18:30:01+00:00
- **Authors**: Jonathan Ephrath, Lars Ruthotto, Eran Treister
- **Comment**: This paper has been withdrawn by the authors. This paper has been
  superseded by arXiv:2011.09128
- **Journal**: None
- **Summary**: We present a multigrid approach that combats the quadratic growth of the number of parameters with respect to the number of channels in standard convolutional neural networks (CNNs). It has been shown that there is a redundancy in standard CNNs, as networks with much sparser convolution operators can yield similar performance to full networks. The sparsity patterns that lead to such behavior, however, are typically random, hampering hardware efficiency. In this work, we present a multigrid-in-channels approach for building CNN architectures that achieves full coupling of the channels, and whose number of parameters is linearly proportional to the width of the network. To this end, we replace each convolution layer in a generic CNN with a multilevel layer consisting of structured (i.e., grouped) convolutions. Our examples from supervised image classification show that applying this strategy to residual networks and MobileNetV2 considerably reduces the number of parameters without negatively affecting accuracy. Therefore, we can widen networks without dramatically increasing the number of parameters or operations.



### Automated Identification of Thoracic Pathology from Chest Radiographs with Enhanced Training Pipeline
- **Arxiv ID**: http://arxiv.org/abs/2006.06805v1
- **DOI**: 10.1117/12.2512600
- **Categories**: **eess.IV**, cs.CV, cs.LG, I.5.4; I.5.2; I.2.0
- **Links**: [PDF](http://arxiv.org/pdf/2006.06805v1)
- **Published**: 2020-06-11 20:43:09+00:00
- **Updated**: 2020-06-11 20:43:09+00:00
- **Authors**: Adora M. DSouza, Anas Z. Abidin, Axel Wism√ºller
- **Comment**: 6 pages, 1 figure, 2 tables
- **Journal**: Proc. SPIE 10950, Medical Imaging 2019: Computer-Aided Diagnosis,
  vol. 10950, p. 109503F, (2019)
- **Summary**: Chest x-rays are the most common radiology studies for diagnosing lung and heart disease. Hence, a system for automated pre-reporting of pathologic findings on chest x-rays would greatly enhance radiologists' productivity. To this end, we investigate a deep-learning framework with novel training schemes for classification of different thoracic pathology labels from chest x-rays. We use the currently largest publicly available annotated dataset ChestX-ray14 of 112,120 chest radiographs of 30,805 patients. Each image was annotated with either a 'NoFinding' class, or one or more of 14 thoracic pathology labels. Subjects can have multiple pathologies, resulting in a multi-class, multi-label problem. We encoded labels as binary vectors using k-hot encoding. We study the ResNet34 architecture, pre-trained on ImageNet, where two key modifications were incorporated into the training framework: (1) Stochastic gradient descent with momentum and with restarts using cosine annealing, (2) Variable image sizes for fine-tuning to prevent overfitting. Additionally, we use a heuristic algorithm to select a good learning rate. Learning with restarts was used to avoid local minima. Area Under receiver operating characteristics Curve (AUC) was used to quantitatively evaluate diagnostic quality. Our results are comparable to, or outperform the best results of current state-of-the-art methods with AUCs as follows: Atelectasis:0.81, Cardiomegaly:0.91, Consolidation:0.81, Edema:0.92, Effusion:0.89, Emphysema: 0.92, Fibrosis:0.81, Hernia:0.84, Infiltration:0.73, Mass:0.85, Nodule:0.76, Pleural Thickening:0.81, Pneumonia:0.77, Pneumothorax:0.89 and NoFinding:0.79. Our results suggest that, in addition to using sophisticated network architectures, a good learning rate, scheduler and a robust optimizer can boost performance.



### Auto-Encoding for Shared Cross Domain Feature Representation and Image-to-Image Translation
- **Arxiv ID**: http://arxiv.org/abs/2006.11404v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.11404v1)
- **Published**: 2020-06-11 21:38:23+00:00
- **Updated**: 2020-06-11 21:38:23+00:00
- **Authors**: Safalya Pal
- **Comment**: None
- **Journal**: None
- **Summary**: Image-to-image translation is a subset of computer vision and pattern recognition problems where our goal is to learn a mapping between input images of domain $\mathbf{X}_1$ and output images of domain $\mathbf{X}_2$. Current methods use neural networks with an encoder-decoder structure to learn a mapping $G:\mathbf{X}_1 \to\mathbf{X}_2$ such that the distribution of images from $\mathbf{X}_2$ and $G(\mathbf{X}_1)$ are identical, where $G(\mathbf{X}_1) = d_G (f_G (\mathbf{X}_1))$ and $f_G (\cdot)$ is referred as the encoder and $d_G(\cdot)$ is referred to as the decoder. Currently, such methods which also compute an inverse mapping $F:\mathbf{X}_2 \to \mathbf{X}_1$ use a separate encoder-decoder pair $d_F (f_F (\mathbf{X}_2))$ or at least a separate decoder $d_F (\cdot)$ to do so. Here we introduce a method to perform cross domain image-to-image translation across multiple domains using a single encoder-decoder architecture. We use an auto-encoder network which given an input image $\mathbf{X}_1$, first computes a latent domain encoding $Z_d = f_d (\mathbf{X}_1)$ and a latent content encoding $Z_c = f_c (\mathbf{X}_1)$, where the domain encoding $Z_d$ and content encoding $Z_c$ are independent. And then a decoder network $g(Z_d,Z_c)$ creates a reconstruction of the original image $\mathbf{\widehat{X}}_1=g(Z_d,Z_c )\approx \mathbf{X}_1$. Ideally, the domain encoding $Z_d$ contains no information regarding the content of the image and the content encoding $Z_c$ contains no information regarding the domain of the image. We use this property of the encodings to find the mapping across domains $G: X\to Y$ by simply changing the domain encoding $Z_d$ of the decoder's input. $G(\mathbf{X}_1 )=d(f_d (\mathbf{x}_2^i ),f_c (\mathbf{X}_1))$ where $\mathbf{x}_2^i$ is the $i^{th}$ observation of $\mathbf{X}_2$.



### SegNBDT: Visual Decision Rules for Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2006.06868v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.06868v1)
- **Published**: 2020-06-11 23:10:02+00:00
- **Updated**: 2020-06-11 23:10:02+00:00
- **Authors**: Alvin Wan, Daniel Ho, Younjin Song, Henk Tillman, Sarah Adel Bargal, Joseph E. Gonzalez
- **Comment**: 8 pages, 8 figures
- **Journal**: None
- **Summary**: The black-box nature of neural networks limits model decision interpretability, in particular for high-dimensional inputs in computer vision and for dense pixel prediction tasks like segmentation. To address this, prior work combines neural networks with decision trees. However, such models (1) perform poorly when compared to state-of-the-art segmentation models or (2) fail to produce decision rules with spatially-grounded semantic meaning. In this work, we build a hybrid neural-network and decision-tree model for segmentation that (1) attains neural network segmentation accuracy and (2) provides semi-automatically constructed visual decision rules such as "Is there a window?". We obtain semantic visual meaning by extending saliency methods to segmentation and attain accuracy by leveraging insights from neural-backed decision trees, a deep learning analog of decision trees for image classification. Our model SegNBDT attains accuracy within ~2-4% of the state-of-the-art HRNetV2 segmentation model while also retaining explainability; we achieve state-of-the-art performance for explainable models on three benchmark datasets -- Pascal-Context (49.12%), Cityscapes (79.01%), and Look Into Person (51.64%). Furthermore, user studies suggest visual decision rules are more interpretable, particularly for incorrect predictions. Code and pretrained models can be found at https://github.com/daniel-ho/SegNBDT.



### Feudal Steering: Hierarchical Learning for Steering Angle Prediction
- **Arxiv ID**: http://arxiv.org/abs/2006.06869v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.06869v1)
- **Published**: 2020-06-11 23:17:55+00:00
- **Updated**: 2020-06-11 23:17:55+00:00
- **Authors**: Faith Johnson, Kristin Dana
- **Comment**: InThe IEEE/CVFConference on Computer Vision and Pattern
  Recognition(CVPR) Workshops, June 2020
- **Journal**: None
- **Summary**: We consider the challenge of automated steering angle prediction for self driving cars using egocentric road images. In this work, we explore the use of feudal networks, used in hierarchical reinforcement learning (HRL), to devise a vehicle agent to predict steering angles from first person, dash-cam images of the Udacity driving dataset. Our method, Feudal Steering, is inspired by recent work in HRL consisting of a manager network and a worker network that operate on different temporal scales and have different goals. The manager works at a temporal scale that is relatively coarse compared to the worker and has a higher level, task-oriented goal space. Using feudal learning to divide the task into manager and worker sub-networks provides more accurate and robust prediction. Temporal abstraction in driving allows more complex primitives than the steering angle at a single time instance. Composite actions comprise a subroutine or skill that can be re-used throughout the driving sequence. The associated subroutine id is the manager network's goal, so that the manager seeks to succeed at the high level task (e.g. a sharp right turn, a slight right turn, moving straight in traffic, or moving straight unencumbered by traffic). The steering angle at a particular time instance is the worker network output which is regulated by the manager's high level task. We demonstrate state-of-the art steering angle prediction results on the Udacity dataset.



### Reintroducing Straight-Through Estimators as Principled Methods for Stochastic Binary Networks
- **Arxiv ID**: http://arxiv.org/abs/2006.06880v4
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2006.06880v4)
- **Published**: 2020-06-11 23:58:18+00:00
- **Updated**: 2021-10-19 14:45:41+00:00
- **Authors**: Alexander Shekhovtsov, Viktor Yanush
- **Comment**: 33 pages, DAGM 2021 version (presented, to be published)
- **Journal**: None
- **Summary**: Training neural networks with binary weights and activations is a challenging problem due to the lack of gradients and difficulty of optimization over discrete weights. Many successful experimental results have been achieved with empirical straight-through (ST) approaches, proposing a variety of ad-hoc rules for propagating gradients through non-differentiable activations and updating discrete weights. At the same time, ST methods can be truly derived as estimators in the stochastic binary network (SBN) model with Bernoulli weights. We advance these derivations to a more complete and systematic study. We analyze properties, estimation accuracy, obtain different forms of correct ST estimators for activations and weights, explain existing empirical approaches and their shortcomings, explain how latent weights arise from the mirror descent method when optimizing over probabilities. This allows to reintroduce ST methods, long known empirically, as sound approximations, apply them with clarity and develop further improvements.



### Rethinking Pre-training and Self-training
- **Arxiv ID**: http://arxiv.org/abs/2006.06882v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.06882v2)
- **Published**: 2020-06-11 23:59:16+00:00
- **Updated**: 2020-11-15 19:41:27+00:00
- **Authors**: Barret Zoph, Golnaz Ghiasi, Tsung-Yi Lin, Yin Cui, Hanxiao Liu, Ekin D. Cubuk, Quoc V. Le
- **Comment**: Accepted for publication at the Thirty-fourth Conference on Neural
  Information Processing Systems (NeurIPS 2020)
- **Journal**: None
- **Summary**: Pre-training is a dominant paradigm in computer vision. For example, supervised ImageNet pre-training is commonly used to initialize the backbones of object detection and segmentation models. He et al., however, show a surprising result that ImageNet pre-training has limited impact on COCO object detection. Here we investigate self-training as another method to utilize additional data on the same setup and contrast it against ImageNet pre-training. Our study reveals the generality and flexibility of self-training with three additional insights: 1) stronger data augmentation and more labeled data further diminish the value of pre-training, 2) unlike pre-training, self-training is always helpful when using stronger data augmentation, in both low-data and high-data regimes, and 3) in the case that pre-training is helpful, self-training improves upon pre-training. For example, on the COCO object detection dataset, pre-training benefits when we use one fifth of the labeled data, and hurts accuracy when we use all labeled data. Self-training, on the other hand, shows positive improvements from +1.3 to +3.4AP across all dataset sizes. In other words, self-training works well exactly on the same setup that pre-training does not work (using ImageNet to help COCO). On the PASCAL segmentation dataset, which is a much smaller dataset than COCO, though pre-training does help significantly, self-training improves upon the pre-trained model. On COCO object detection, we achieve 54.3AP, an improvement of +1.5AP over the strongest SpineNet model. On PASCAL segmentation, we achieve 90.5 mIOU, an improvement of +1.5% mIOU over the previous state-of-the-art result by DeepLabv3+.



