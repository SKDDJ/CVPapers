# Arxiv Papers in cs.CV on 2020-06-21
### Semi-Supervised Object Detection with Sparsely Annotated Dataset
- **Arxiv ID**: http://arxiv.org/abs/2006.11692v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.11692v1)
- **Published**: 2020-06-21 02:26:48+00:00
- **Updated**: 2020-06-21 02:26:48+00:00
- **Authors**: Jihun Yoon, Seungbum Hong, Sanha Jeong, Min-Kook Choi
- **Comment**: Challenge Winner in Epic-Kitchens 2020 Object Detection Challenge
  (EPIC@CVPR 2020)
- **Journal**: None
- **Summary**: In training object detector based on convolutional neural networks, selection of effective positive examples for training is an important factor. However, when training an anchor-based detectors with sparse annotations on an image, effort to find effective positive examples can hinder training performance. When using the anchor-based training for the ground truth bounding box to collect positive examples under given IoU, it is often possible to include objects from other classes in the current training class, or objects that are needed to be trained can only be sampled as negative examples. We used two approaches to solve this problem: 1) the use of an anchorless object detector and 2) a semi-supervised learning-based object detection using a single object tracker. The proposed technique performs single object tracking by using the sparsely annotated bounding box as an anchor in the temporal domain for successive frames. From the tracking results, dense annotations for training images were generated in an automated manner and used for training the object detector. We applied the proposed single object tracking-based semi-supervised learning to the Epic-Kitchens dataset. As a result, we were able to achieve \textbf{runner-up} performance in the Unseen section while achieving the first place in the Seen section of the Epic-Kitchens 2020 object detection challenge under IoU > 0.5 evaluation



### Dense-Captioning Events in Videos: SYSU Submission to ActivityNet Challenge 2020
- **Arxiv ID**: http://arxiv.org/abs/2006.11693v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.11693v2)
- **Published**: 2020-06-21 02:38:59+00:00
- **Updated**: 2020-08-12 03:44:21+00:00
- **Authors**: Teng Wang, Huicheng Zheng, Mingjing Yu
- **Comment**: Second-place solution to TASK 2 (Dense video captioning) in
  ActivityNet Challenge 2020. Code is available at
  https://github.com/ttengwang/dense-video-captioning-pytorch
- **Journal**: None
- **Summary**: This technical report presents a brief description of our submission to the dense video captioning task of ActivityNet Challenge 2020. Our approach follows a two-stage pipeline: first, we extract a set of temporal event proposals; then we propose a multi-event captioning model to capture the event-level temporal relationships and effectively fuse the multi-modal information. Our approach achieves a 9.28 METEOR score on the test set.



### Fast and Accurate: Structure Coherence Component for Face Alignment
- **Arxiv ID**: http://arxiv.org/abs/2006.11697v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.11697v1)
- **Published**: 2020-06-21 02:52:29+00:00
- **Updated**: 2020-06-21 02:52:29+00:00
- **Authors**: Beier Zhu, Chunze Lin, Quan Wang, Renjie Liao, Chen Qian
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a fast and accurate coordinate regression method for face alignment. Unlike most existing facial landmark regression methods which usually employ fully connected layers to convert feature maps into landmark coordinate, we present a structure coherence component to explicitly take the relation among facial landmarks into account. Due to the geometric structure of human face, structure coherence between different facial parts provides important cues for effectively localizing facial landmarks. However, the dense connection in the fully connected layers overuses such coherence, making the important cues unable to be distinguished from all connections. Instead, our structure coherence component leverages a dynamic sparse graph structure to passing features among the most related landmarks. Furthermore, we propose a novel objective function, named Soft Wing loss, to improve the accuracy. Extensive experiments on three popular benchmarks, including WFLW, COFW and 300W, demonstrate the effectiveness of the proposed method, achieving state-of-the-art performance with fast speed. Our approach is especially robust to challenging cases resulting in impressively low failure rate (0% and 2.88%) in COFW and WFLW datasets.



### A Universal Representation Transformer Layer for Few-Shot Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2006.11702v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.11702v4)
- **Published**: 2020-06-21 03:08:00+00:00
- **Updated**: 2020-09-02 22:35:10+00:00
- **Authors**: Lu Liu, William Hamilton, Guodong Long, Jing Jiang, Hugo Larochelle
- **Comment**: None
- **Journal**: None
- **Summary**: Few-shot classification aims to recognize unseen classes when presented with only a small number of samples. We consider the problem of multi-domain few-shot image classification, where unseen classes and examples come from diverse data sources. This problem has seen growing interest and has inspired the development of benchmarks such as Meta-Dataset. A key challenge in this multi-domain setting is to effectively integrate the feature representations from the diverse set of training domains. Here, we propose a Universal Representation Transformer (URT) layer, that meta-learns to leverage universal features for few-shot classification by dynamically re-weighting and composing the most appropriate domain-specific representations. In experiments, we show that URT sets a new state-of-the-art result on Meta-Dataset. Specifically, it achieves top-performance on the highest number of data sources compared to competing methods. We analyze variants of URT and present a visualization of the attention score heatmaps that sheds light on how the model performs cross-domain generalization. Our code is available at https://github.com/liulu112601/URT.



### Exploiting Contextual Information with Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2006.11706v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.11706v2)
- **Published**: 2020-06-21 03:40:30+00:00
- **Updated**: 2020-06-27 18:00:07+00:00
- **Authors**: Ismail Elezi
- **Comment**: Ph.D. thesis
- **Journal**: None
- **Summary**: Context matters! Nevertheless, there has not been much research in exploiting contextual information in deep neural networks. For most part, the entire usage of contextual information has been limited to recurrent neural networks. Attention models and capsule networks are two recent ways of introducing contextual information in non-recurrent models, however both of these algorithms have been developed after this work has started.   In this thesis, we show that contextual information can be exploited in 2 fundamentally different ways: implicitly and explicitly. In the DeepScore project, where the usage of context is very important for the recognition of many tiny objects, we show that by carefully crafting convolutional architectures, we can achieve state-of-the-art results, while also being able to implicitly correctly distinguish between objects which are virtually identical, but have different meanings based on their surrounding. In parallel, we show that by explicitly designing algorithms (motivated from graph theory and game theory) that take into considerations the entire structure of the dataset, we can achieve state-of-the-art results in different topics like semi-supervised learning and similarity learning.   To the best of our knowledge, we are the first to integrate graph-theoretical modules, carefully crafted for the problem of similarity learning and that are designed to consider contextual information, not only outperforming the other models, but also gaining a speed improvement while using a smaller number of parameters.



### Mapping Low-Resolution Images To Multiple High-Resolution Images Using Non-Adversarial Mapping
- **Arxiv ID**: http://arxiv.org/abs/2006.11708v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.11708v2)
- **Published**: 2020-06-21 04:14:24+00:00
- **Updated**: 2020-06-30 18:14:13+00:00
- **Authors**: Vasileios Lioutas
- **Comment**: Paper completed in April 2019
- **Journal**: None
- **Summary**: Several methods have recently been proposed for the Single Image Super-Resolution (SISR) problem. The current methods assume that a single low-resolution image can only yield a single high-resolution image. In addition, all of these methods use low-resolution images that were artificially generated through simple bilinear down-sampling. We argue that, first and foremost, the problem of SISR is an one-to-many mapping problem between the low-resolution and all possible candidate high-resolution images and we address the challenging task of learning how to realistically degrade and down-sample high-resolution images. To circumvent this problem, we propose SR-NAM which utilizes the Non-Adversarial Mapping (NAM) technique. Furthermore, we propose a degradation model that learns how to transform high-resolution images to low-resolution images that resemble realistically taken low-resolution photos. Finally, some qualitative results for the proposed method along with the weaknesses of SR-NAM are included.



### Off-Policy Self-Critical Training for Transformer in Visual Paragraph Generation
- **Arxiv ID**: http://arxiv.org/abs/2006.11714v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.11714v1)
- **Published**: 2020-06-21 05:10:17+00:00
- **Updated**: 2020-06-21 05:10:17+00:00
- **Authors**: Shiyang Yan, Yang Hua, Neil M. Robertson
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, several approaches have been proposed to solve language generation problems. Transformer is currently state-of-the-art seq-to-seq model in language generation. Reinforcement Learning (RL) is useful in solving exposure bias and the optimisation on non-differentiable metrics in seq-to-seq language learning. However, Transformer is hard to combine with RL as the costly computing resource is required for sampling. We tackle this problem by proposing an off-policy RL learning algorithm where a behaviour policy represented by GRUs performs the sampling. We reduce the high variance of importance sampling (IS) by applying the truncated relative importance sampling (TRIS) technique and Kullback-Leibler (KL)-control concept. TRIS is a simple yet effective technique, and there is a theoretical proof that KL-control helps to reduce the variance of IS. We formulate this off-policy RL based on self-critical sequence training. Specifically, we use a Transformer-based captioning model as the target policy and use an image-guided language auto-encoder as the behaviour policy to explore the environment. The proposed algorithm achieves state-of-the-art performance on the visual paragraph generation and improved results on image captioning.



### Learning compact generalizable neural representations supporting perceptual grouping
- **Arxiv ID**: http://arxiv.org/abs/2006.11716v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.11716v1)
- **Published**: 2020-06-21 05:46:01+00:00
- **Updated**: 2020-06-21 05:46:01+00:00
- **Authors**: Vijay Veerabadran, Virginia R. de Sa
- **Comment**: None
- **Journal**: None
- **Summary**: Work at the intersection of vision science and deep learning is starting to explore the efficacy of deep convolutional networks (DCNs) and recurrent networks in solving perceptual grouping problems that underlie primate visual recognition and segmentation. Here, we extend this line of work to investigate the compactness and generalizability of DCN solutions to learning low-level perceptual grouping routines involving contour integration. We introduce V1Net, a bio-inspired recurrent unit that incorporates lateral connections ubiquitous in cortical circuitry. Feedforward convolutional layers in DCNs can be substituted with V1Net modules to enhance their contextual visual processing support for perceptual grouping. We compare the learning efficiency and accuracy of V1Net-DCNs to that of 14 carefully selected feedforward and recurrent neural architectures (including state-of-the-art DCNs) on MarkedLong -- a synthetic forced-choice contour integration dataset of 800,000 images we introduce here -- and the previously published Pathfinder contour integration benchmarks. We gauged solution generalizability by measuring the transfer learning performance of our candidate models trained on MarkedLong that were fine-tuned to learn PathFinder. Our results demonstrate that a compact 3-layer V1Net-DCN matches or outperforms the test accuracy and sample efficiency of all tested comparison models which contain between 5x and 1000x more trainable parameters; we also note that V1Net-DCN learns the most compact generalizable solution to MarkedLong. A visualization of the temporal dynamics of a V1Net-DCN elucidates its usage of interpretable grouping computations to solve MarkedLong. The compact and rich representations of V1Net-DCN also make it a promising candidate to build on-device machine vision algorithms as well as help better understand biological cortical circuitry.



### Pose Trainer: Correcting Exercise Posture using Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2006.11718v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.11718v1)
- **Published**: 2020-06-21 05:51:37+00:00
- **Updated**: 2020-06-21 05:51:37+00:00
- **Authors**: Steven Chen, Richard R. Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Fitness exercises are very beneficial to personal health and fitness; however, they can also be ineffective and potentially dangerous if performed incorrectly by the user. Exercise mistakes are made when the user does not use the proper form, or pose. In our work, we introduce Pose Trainer, an application that detects the user's exercise pose and provides personalized, detailed recommendations on how the user can improve their form. Pose Trainer uses the state of the art in pose estimation to detect a user's pose, then evaluates the vector geometry of the pose through an exercise to provide useful feedback. We record a dataset of over 100 exercise videos of correct and incorrect form, based on personal training guidelines, and build geometric-heuristic and machine learning algorithms for evaluation. Pose Trainer works on four common exercises and supports any Windows or Linux computer with a GPU.



### Motion Representation Using Residual Frames with 3D CNN
- **Arxiv ID**: http://arxiv.org/abs/2006.13017v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.13017v1)
- **Published**: 2020-06-21 07:35:41+00:00
- **Updated**: 2020-06-21 07:35:41+00:00
- **Authors**: Li Tao, Xueting Wang, Toshihiko Yamasaki
- **Comment**: Accepted in IEEE ICIP 2020. arXiv admin note: substantial text
  overlap with arXiv:2001.05661
- **Journal**: None
- **Summary**: Recently, 3D convolutional networks (3D ConvNets) yield good performance in action recognition. However, optical flow stream is still needed to ensure better performance, the cost of which is very high. In this paper, we propose a fast but effective way to extract motion features from videos utilizing residual frames as the input data in 3D ConvNets. By replacing traditional stacked RGB frames with residual ones, 35.6% and 26.6% points improvements over top-1 accuracy can be obtained on the UCF101 and HMDB51 datasets when ResNet-18 models are trained from scratch. And we achieved the state-of-the-art results in this training mode. Analysis shows that better motion features can be extracted using residual frames compared to RGB counterpart. By combining with a simple appearance path, our proposal can be even better than some methods using optical flow streams.



### Kiwifruit detection in challenging conditions
- **Arxiv ID**: http://arxiv.org/abs/2006.11729v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2006.11729v1)
- **Published**: 2020-06-21 07:35:57+00:00
- **Updated**: 2020-06-21 07:35:57+00:00
- **Authors**: Mahla Nejati, Nicky Penhall, Henry Williams, Jamie Bell, JongYoon Lim, Ho Seok Ahn, Bruce MacDonald
- **Comment**: Accepted to the Australasian conference on robotics and automation
  (ACRA 2019)
- **Journal**: None
- **Summary**: Accurate and reliable kiwifruit detection is one of the biggest challenges in developing a selective fruit harvesting robot. The vision system of an orchard robot faces difficulties such as dynamic lighting conditions and fruit occlusions. This paper presents a semantic segmentation approach with two novel image prepossessing techniques designed to detect kiwifruit under the harsh lighting conditions found in the canopy. The performance of the presented system is evaluated on a 3D real-world image set of kiwifruit under different lighting conditions (typical, glare, and overexposed). Alone the semantic segmentation approach achieves an F1_score of 0.82 on the typical lighting image set, but struggles with harsh lighting with an F1_score of 0.13. Utilising the prepossessing techniques the vision system under harsh lighting improves to an F1_score 0.42. To address the fruit occlusion challenge, the overall approach was found to be capable of detecting 87.0% of non-occluded and 30.0% of occluded kiwifruit across all lighting conditions.



### Rotation-Equivariant Neural Networks for Privacy Protection
- **Arxiv ID**: http://arxiv.org/abs/2006.13016v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.13016v1)
- **Published**: 2020-06-21 08:00:14+00:00
- **Updated**: 2020-06-21 08:00:14+00:00
- **Authors**: Hao Zhang, Yiting Chen, Haotian Ma, Xu Cheng, Qihan Ren, Liyao Xiang, Jie Shi, Quanshi Zhang
- **Comment**: arXiv admin note: text overlap with arXiv:2003.08365
- **Journal**: None
- **Summary**: In order to prevent leaking input information from intermediate-layer features, this paper proposes a method to revise the traditional neural network into the rotation-equivariant neural network (RENN). Compared to the traditional neural network, the RENN uses d-ary vectors/tensors as features, in which each element is a d-ary number. These d-ary features can be rotated (analogous to the rotation of a d-dimensional vector) with a random angle as the encryption process. Input information is hidden in this target phase of d-ary features for attribute obfuscation. Even if attackers have obtained network parameters and intermediate-layer features, they cannot extract input information without knowing the target phase. Hence, the input privacy can be effectively protected by the RENN. Besides, the output accuracy of RENNs only degrades mildly compared to traditional neural networks, and the computational cost is significantly less than the homomorphic encryption.



### Efficient Integer-Arithmetic-Only Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2006.11735v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.11735v1)
- **Published**: 2020-06-21 08:23:03+00:00
- **Updated**: 2020-06-21 08:23:03+00:00
- **Authors**: Hengrui Zhao, Dong Liu, Houqiang Li
- **Comment**: 8 pages, 4 figures
- **Journal**: None
- **Summary**: Integer-arithmetic-only networks have been demonstrated effective to reduce computational cost and to ensure cross-platform consistency. However, previous works usually report a decline in the inference accuracy when converting well-trained floating-point-number (FPN) networks into integer networks. We analyze this phonomenon and find that the decline is due to activation quantization. Specifically, when we replace conventional ReLU with Bounded ReLU, how to set the bound for each neuron is a key problem. Considering the tradeoff between activation quantization error and network learning ability, we set an empirical rule to tune the bound of each Bounded ReLU. We also design a mechanism to handle the cases of feature map addition and feature map concatenation. Based on the proposed method, our trained 8-bit integer ResNet outperforms the 8-bit networks of Google's TensorFlow and NVIDIA's TensorRT for image recognition. We also experiment on VDSR for image super-resolution and on VRCNN for compression artifact reduction, both of which serve for regression tasks that natively require high inference accuracy. Our integer networks achieve equivalent performance as the corresponding FPN networks, but have only 1/4 memory cost and run 2x faster on modern GPUs. Our code and models can be found at github.com/HengRuiZ/brelu.



### Achieving Better Kinship Recognition Through Better Baseline
- **Arxiv ID**: http://arxiv.org/abs/2006.11739v1
- **DOI**: 10.1109/FG47880.2020.00137
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.11739v1)
- **Published**: 2020-06-21 08:40:53+00:00
- **Updated**: 2020-06-21 08:40:53+00:00
- **Authors**: Andrei Shadrikov
- **Comment**: Accepted for the 4th Recognizing Families In the Wild Workshop
- **Journal**: None
- **Summary**: Recognizing blood relations using face images can be seen as an application of face recognition systems with additional restrictions. These restrictions proved to be difficult to deal with, however, recent advancements in face verification show that there is still much to gain using more data and novel ideas. As a result face recognition is a great source domain from which we can transfer the knowledge to get better performance in kinship recognition as a source domain. We present a new baseline for an automatic kinship recognition task and relatives search based on RetinaFace[1] for face registration and ArcFace[2] face verification model. With the approach described above as the foundation, we constructed a pipeline that achieved state-of-the-art performance on two tracks in the recent Recognizing Families In the Wild Data Challenge.



### Weak Supervision and Referring Attention for Temporal-Textual Association Learning
- **Arxiv ID**: http://arxiv.org/abs/2006.11747v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.11747v2)
- **Published**: 2020-06-21 09:25:28+00:00
- **Updated**: 2020-06-27 08:19:35+00:00
- **Authors**: Zhiyuan Fang, Shu Kong, Zhe Wang, Charless Fowlkes, Yezhou Yang
- **Comment**: 12 pages, 6 figures
- **Journal**: None
- **Summary**: A system capturing the association between video frames and textual queries offer great potential for better video analysis. However, training such a system in a fully supervised way inevitably demands a meticulously curated video dataset with temporal-textual annotations. Therefore we provide a Weak-Supervised alternative with our proposed Referring Attention mechanism to learn temporal-textual association (dubbed WSRA). The weak supervision is simply a textual expression (e.g., short phrases or sentences) at video level, indicating this video contains relevant frames. The referring attention is our designed mechanism acting as a scoring function for grounding the given queries over frames temporally. It consists of multiple novel losses and sampling strategies for better training. The principle in our designed mechanism is to fully exploit 1) the weak supervision by considering informative and discriminative cues from intra-video segments anchored with the textual query, 2) multiple queries compared to the single video, and 3) cross-video visual similarities. We validate our WSRA through extensive experiments for temporally grounding by languages, demonstrating that it outperforms the state-of-the-art weakly-supervised methods notably.



### FNA++: Fast Network Adaptation via Parameter Remapping and Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/2006.12986v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.12986v2)
- **Published**: 2020-06-21 10:03:34+00:00
- **Updated**: 2020-12-16 03:57:51+00:00
- **Authors**: Jiemin Fang, Yuzhu Sun, Qian Zhang, Kangjian Peng, Yuan Li, Wenyu Liu, Xinggang Wang
- **Comment**: Accepted by TPAMI (extension of arXiv:2001.02525)
- **Journal**: None
- **Summary**: Deep neural networks achieve remarkable performance in many computer vision tasks. Most state-of-the-art (SOTA) semantic segmentation and object detection approaches reuse neural network architectures designed for image classification as the backbone, commonly pre-trained on ImageNet. However, performance gains can be achieved by designing network architectures specifically for detection and segmentation, as shown by recent neural architecture search (NAS) research for detection and segmentation. One major challenge though is that ImageNet pre-training of the search space representation (a.k.a. super network) or the searched networks incurs huge computational cost. In this paper, we propose a Fast Network Adaptation (FNA++) method, which can adapt both the architecture and parameters of a seed network (e.g. an ImageNet pre-trained network) to become a network with different depths, widths, or kernel sizes via a parameter remapping technique, making it possible to use NAS for segmentation and detection tasks a lot more efficiently. In our experiments, we apply FNA++ on MobileNetV2 to obtain new networks for semantic segmentation, object detection, and human pose estimation that clearly outperform existing networks designed both manually and by NAS. We also implement FNA++ on ResNets and NAS networks, which demonstrates a great generalization ability. The total computation cost of FNA++ is significantly less than SOTA segmentation and detection NAS approaches: 1737x less than DPC, 6.8x less than Auto-DeepLab, and 8.0x less than DetNAS. A series of ablation studies are performed to demonstrate the effectiveness, and detailed analysis is provided for more insights into the working mechanism. Codes are available at https://github.com/JaminFong/FNA.



### Measuring Performance of Generative Adversarial Networks on Devanagari Script
- **Arxiv ID**: http://arxiv.org/abs/2007.06710v1
- **DOI**: 10.5120/ijca2020920393
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.06710v1)
- **Published**: 2020-06-21 10:20:51+00:00
- **Updated**: 2020-06-21 10:20:51+00:00
- **Authors**: Amogh G. Warkhandkar, Baasit Sharief, Omkar B. Bhambure
- **Comment**: 5 pages, 5 figures
- **Journal**: International Journal of Computer Applications, Volume 176 -
  No.33, June 2020, Pages 5-9
- **Summary**: The working of neural networks following the adversarial philosophy to create a generative model is a fascinating field. Multiple papers have already explored the architectural aspect and proposed systems with potentially good results however, very few papers are available which implement it on a real-world example. Traditionally, people use the famous MNIST dataset as a Hello, World! example for implementing Generative Adversarial Networks (GAN). Instead of going the standard route of using handwritten digits, this paper uses the Devanagari script which has a more complex structure. As there is no conventional way of judging how well the generative models perform, three additional classifiers were built to judge the output of the GAN model. The following paper is an explanation of what this implementation has achieved.



### Methodology for Building Synthetic Datasets with Virtual Humans
- **Arxiv ID**: http://arxiv.org/abs/2006.11757v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.11757v1)
- **Published**: 2020-06-21 10:29:36+00:00
- **Updated**: 2020-06-21 10:29:36+00:00
- **Authors**: Shubhajit Basak, Hossein Javidnia, Faisal Khan, Rachel McDonnell, Michael Schukat
- **Comment**: Conference - ISSC 2020
- **Journal**: None
- **Summary**: Recent advances in deep learning methods have increased the performance of face detection and recognition systems. The accuracy of these models relies on the range of variation provided in the training data. Creating a dataset that represents all variations of real-world faces is not feasible as the control over the quality of the data decreases with the size of the dataset. Repeatability of data is another challenge as it is not possible to exactly recreate 'real-world' acquisition conditions outside of the laboratory. In this work, we explore a framework to synthetically generate facial data to be used as part of a toolchain to generate very large facial datasets with a high degree of control over facial and environmental variations. Such large datasets can be used for improved, targeted training of deep neural networks. In particular, we make use of a 3D morphable face model for the rendering of multiple 2D images across a dataset of 100 synthetic identities, providing full control over image variations such as pose, illumination, and background.



### Patch Based Classification of Remote Sensing Data: A Comparison of 2D-CNN, SVM and NN Classifiers
- **Arxiv ID**: http://arxiv.org/abs/2006.11767v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.11767v1)
- **Published**: 2020-06-21 11:07:37+00:00
- **Updated**: 2020-06-21 11:07:37+00:00
- **Authors**: Mahesh Pal, Akshay, Himanshu Rohilla, B. Charan Teja
- **Comment**: 8 pages, 2 Figures
- **Journal**: None
- **Summary**: Pixel based algorithms including back propagation neural networks (NN) and support vector machines (SVM) have been widely used for remotely sensed image classifications. Within last few years, deep learning based image classifier like convolution neural networks (2D-CNN) are becoming popular alternatives to these classifiers. In this paper, we compare performance of patch based SVM and NN with that of a deep learning algorithms comprising of 2D-CNN and fully connected layers. Similar to CNN which utilise image patches to derive features for further classification, we propose to use patches as an input in place of individual pixel with both SVM and NN classifiers. Two datasets, one multispectral and other hyperspectral data was used to compare the performance of different classifiers. Results with both datasets suggest the effectiveness of patch based SVM and NN classifiers in comparison to state of art 2D-CNN classifier.



### Flood severity mapping from Volunteered Geographic Information by interpreting water level from images containing people: a case study of Hurricane Harvey
- **Arxiv ID**: http://arxiv.org/abs/2006.11802v2
- **DOI**: 10.1016/j.isprsjprs.2020.09.011
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.11802v2)
- **Published**: 2020-06-21 13:57:42+00:00
- **Updated**: 2020-09-30 16:11:42+00:00
- **Authors**: Yu Feng, Claus Brenner, Monika Sester
- **Comment**: None
- **Journal**: None
- **Summary**: With increasing urbanization, in recent years there has been a growing interest and need in monitoring and analyzing urban flood events. Social media, as a new data source, can provide real-time information for flood monitoring. The social media posts with locations are often referred to as Volunteered Geographic Information (VGI), which can reveal the spatial pattern of such events. Since more images are shared on social media than ever before, recent research focused on the extraction of flood-related posts by analyzing images in addition to texts. Apart from merely classifying posts as flood relevant or not, more detailed information, e.g. the flood severity, can also be extracted based on image interpretation. However, it has been less tackled and has not yet been applied for flood severity mapping.   In this paper, we propose a novel three-step process to extract and map flood severity information. First, flood relevant images are retrieved with the help of pre-trained convolutional neural networks as feature extractors. Second, the images containing people are further classified into four severity levels by observing the relationship between body parts and their partial inundation, i.e. images are classified according to the water level with respect to different body parts, namely ankle, knee, hip, and chest. Lastly, locations of the Tweets are used for generating a map of estimated flood extent and severity. This process was applied to an image dataset collected during Hurricane Harvey in 2017, as a proof of concept. The results show that VGI can be used as a supplement to remote sensing observations for flood extent mapping and is beneficial, especially for urban areas, where the infrastructure is often occluding water. Based on the extracted water level information, an integrated overview of flood severity can be provided for the early stages of emergency response.



### Improving Image Captioning with Better Use of Captions
- **Arxiv ID**: http://arxiv.org/abs/2006.11807v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2006.11807v1)
- **Published**: 2020-06-21 14:10:47+00:00
- **Updated**: 2020-06-21 14:10:47+00:00
- **Authors**: Zhan Shi, Xu Zhou, Xipeng Qiu, Xiaodan Zhu
- **Comment**: ACL 2020
- **Journal**: None
- **Summary**: Image captioning is a multimodal problem that has drawn extensive attention in both the natural language processing and computer vision community. In this paper, we present a novel image captioning architecture to better explore semantics available in captions and leverage that to enhance both image representation and caption generation. Our models first construct caption-guided visual relationship graphs that introduce beneficial inductive bias using weakly supervised multi-instance learning. The representation is then enhanced with neighbouring and contextual nodes with their textual and visual features. During generation, the model further incorporates visual relationships using multi-task learning for jointly predicting word and object/predicate tag sequences. We perform extensive experiments on the MSCOCO dataset, showing that the proposed framework significantly outperforms the baselines, resulting in the state-of-the-art performance under a wide range of evaluation metrics.



### Deep Image Orientation Angle Detection
- **Arxiv ID**: http://arxiv.org/abs/2007.06709v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.06709v1)
- **Published**: 2020-06-21 14:24:18+00:00
- **Updated**: 2020-06-21 14:24:18+00:00
- **Authors**: Subhadip Maji, Smarajit Bose
- **Comment**: None
- **Journal**: None
- **Summary**: Estimating and rectifying the orientation angle of any image is a pretty challenging task. Initial work used the hand engineering features for this purpose, where after the invention of deep learning using convolution-based neural network showed significant improvement in this problem. However, this paper shows that the combination of CNN and a custom loss function specially designed for angles lead to a state-of-the-art results. This includes the estimation of the orientation angle of any image or document at any degree (0 to 360 degree),



### Sequential Feature Filtering Classifier
- **Arxiv ID**: http://arxiv.org/abs/2006.11808v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.11808v1)
- **Published**: 2020-06-21 14:31:45+00:00
- **Updated**: 2020-06-21 14:31:45+00:00
- **Authors**: Minseok Seo, Jaemin Lee, Jongchan Park, Dong-Geol Choi
- **Comment**: None
- **Journal**: None
- **Summary**: We propose Sequential Feature Filtering Classifier (FFC), a simple but effective classifier for convolutional neural networks (CNNs). With sequential LayerNorm and ReLU, FFC zeroes out low-activation units and preserves high-activation units. The sequential feature filtering process generates multiple features, which are fed into a shared classifier for multiple outputs. FFC can be applied to any CNNs with a classifier, and significantly improves performances with negligible overhead. We extensively validate the efficacy of FFC on various tasks: ImageNet-1K classification, MS COCO detection, Cityscapes segmentation, and HMDB51 action recognition. Moreover, we empirically show that FFC can further improve performances upon other techniques, including attention modules and augmentation techniques. The code and models will be publicly available.



### Subspace Clustering for Action Recognition with Covariance Representations and Temporal Pruning
- **Arxiv ID**: http://arxiv.org/abs/2006.11812v1
- **DOI**: 10.1109/ICPR48806.2021.9412060
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.11812v1)
- **Published**: 2020-06-21 14:44:03+00:00
- **Updated**: 2020-06-21 14:44:03+00:00
- **Authors**: Giancarlo Paoletti, Jacopo Cavazza, Cigdem Beyan, Alessio Del Bue
- **Comment**: None
- **Journal**: 25th International Conference on Pattern Recognition (ICPR) 2020
- **Summary**: This paper tackles the problem of human action recognition, defined as classifying which action is displayed in a trimmed sequence, from skeletal data. Albeit state-of-the-art approaches designed for this application are all supervised, in this paper we pursue a more challenging direction: Solving the problem with unsupervised learning. To this end, we propose a novel subspace clustering method, which exploits covariance matrix to enhance the action's discriminability and a timestamp pruning approach that allow us to better handle the temporal dimension of the data. Through a broad experimental validation, we show that our computational pipeline surpasses existing unsupervised approaches but also can result in favorable performances as compared to supervised methods.



### TreeRNN: Topology-Preserving Deep GraphEmbedding and Learning
- **Arxiv ID**: http://arxiv.org/abs/2006.11825v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.11825v2)
- **Published**: 2020-06-21 15:22:24+00:00
- **Updated**: 2020-07-22 13:45:36+00:00
- **Authors**: Yecheng Lyu, Ming Li, Xinming Huang, Ulkuhan Guler, Patrick Schaumont, Ziming Zhang
- **Comment**: 7 pages
- **Journal**: None
- **Summary**: General graphs are difficult for learning due to their irregular structures. Existing works employ message passing along graph edges to extract local patterns using customized graph kernels, but few of them are effective for the integration of such local patterns into global features. In contrast, in this paper we study the methods to transfer the graphs into trees so that explicit orders are learned to direct the feature integration from local to global. To this end, we apply the breadth first search (BFS) to construct trees from the graphs, which adds direction to the graph edges from the center node to the peripheral nodes. In addition, we proposed a novel projection scheme that transfer the trees to image representations, which is suitable for conventional convolution neural networks (CNNs) and recurrent neural networks (RNNs). To best learn the patterns from the graph-tree-images, we propose TreeRNN, a 2D RNN architecture that recurrently integrates the image pixels by rows and columns to help classify the graph categories. We evaluate the proposed method on several graph classification datasets, and manage to demonstrate comparable accuracy with the state-of-the-art on MUTAG, PTC-MR and NCI1 datasets.



### Quanta Burst Photography
- **Arxiv ID**: http://arxiv.org/abs/2006.11840v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.11840v1)
- **Published**: 2020-06-21 16:20:29+00:00
- **Updated**: 2020-06-21 16:20:29+00:00
- **Authors**: Sizhuo Ma, Shantanu Gupta, Arin C. Ulku, Claudio Bruschini, Edoardo Charbon, Mohit Gupta
- **Comment**: A version with better-quality images can be found on the project
  webpage: http://wisionlab.cs.wisc.edu/project/quanta-burst-photography/
- **Journal**: None
- **Summary**: Single-photon avalanche diodes (SPADs) are an emerging sensor technology capable of detecting individual incident photons, and capturing their time-of-arrival with high timing precision. While these sensors were limited to single-pixel or low-resolution devices in the past, recently, large (up to 1 MPixel) SPAD arrays have been developed. These single-photon cameras (SPCs) are capable of capturing high-speed sequences of binary single-photon images with no read noise. We present quanta burst photography, a computational photography technique that leverages SPCs as passive imaging devices for photography in challenging conditions, including ultra low-light and fast motion. Inspired by recent success of conventional burst photography, we design algorithms that align and merge binary sequences captured by SPCs into intensity images with minimal motion blur and artifacts, high signal-to-noise ratio (SNR), and high dynamic range. We theoretically analyze the SNR and dynamic range of quanta burst photography, and identify the imaging regimes where it provides significant benefits. We demonstrate, via a recently developed SPAD array, that the proposed method is able to generate high-quality images for scenes with challenging lighting, complex geometries, high dynamic range and moving objects. With the ongoing development of SPAD arrays, we envision quanta burst photography finding applications in both consumer and scientific photography.



### Unsupervised Learning of Deep-Learned Features from Breast Cancer Images
- **Arxiv ID**: http://arxiv.org/abs/2006.11843v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2006.11843v1)
- **Published**: 2020-06-21 16:38:36+00:00
- **Updated**: 2020-06-21 16:38:36+00:00
- **Authors**: Sanghoon Lee, Colton Farley, Simon Shim, Yanjun Zhao, Wookjin Choi, Wook-Sung Yoo
- **Comment**: 7 pages for IEEE BIBE
- **Journal**: None
- **Summary**: Detecting cancer manually in whole slide images requires significant time and effort on the laborious process. Recent advances in whole slide image analysis have stimulated the growth and development of machine learning-based approaches that improve the efficiency and effectiveness in the diagnosis of cancer diseases. In this paper, we propose an unsupervised learning approach for detecting cancer in breast invasive carcinoma (BRCA) whole slide images. The proposed method is fully automated and does not require human involvement during the unsupervised learning procedure. We demonstrate the effectiveness of the proposed approach for cancer detection in BRCA and show how the machine can choose the most appropriate clusters during the unsupervised learning procedure. Moreover, we present a prototype application that enables users to select relevant groups mapping all regions related to the groups in whole slide images.



### Perspective Texture Synthesis Based on Improved Energy Optimization
- **Arxiv ID**: http://arxiv.org/abs/2006.11851v1
- **DOI**: 10.1371/journal.pone.0110622
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2006.11851v1)
- **Published**: 2020-06-21 17:12:58+00:00
- **Updated**: 2020-06-21 17:12:58+00:00
- **Authors**: Syed Muhammad Arsalan Bashir, Farhan Ali Khan Ghouri
- **Comment**: Published in PLOS One
- **Journal**: PLoS ONE 9 (10), e110622 (2014)
- **Summary**: Perspective texture synthesis has great significance in many fields like video editing, scene capturing etc., due to its ability to read and control global feature information. In this paper, we present a novel example-based, specifically energy optimization-based algorithm, to synthesize perspective textures. Energy optimization technique is a pixel-based approach, so it is time-consuming. We improve it from two aspects with the purpose of achieving faster synthesis and high quality. Firstly, we change this pixel-based technique by replacing the pixel computation with a little patch. Secondly, we present a novel technique to accelerate searching nearest neighborhoods in energy optimization. Using k- means clustering technique to build a search tree to accelerate the search. Hence, we make use of principal component analysis (PCA) technique to reduce dimensions of input vectors. The high quality results prove that our approach is feasible. Besides, our proposed algorithm needs shorter time relative to other similar methods.



### Generalized Zero and Few-Shot Transfer for Facial Forgery Detection
- **Arxiv ID**: http://arxiv.org/abs/2006.11863v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.11863v1)
- **Published**: 2020-06-21 18:10:52+00:00
- **Updated**: 2020-06-21 18:10:52+00:00
- **Authors**: Shivangi Aneja, Matthias Nießner
- **Comment**: Project page: https://shivangi-aneja.github.io/ddt/
- **Journal**: None
- **Summary**: We propose Deep Distribution Transfer(DDT), a new transfer learning approach to address the problem of zero and few-shot transfer in the context of facial forgery detection. We examine how well a model (pre-)trained with one forgery creation method generalizes towards a previously unseen manipulation technique or different dataset. To facilitate this transfer, we introduce a new mixture model-based loss formulation that learns a multi-modal distribution, with modes corresponding to class categories of the underlying data of the source forgery method. Our core idea is to first pre-train an encoder neural network, which maps each mode of this distribution to the respective class labels, i.e., real or fake images in the source domain by minimizing wasserstein distance between them. In order to transfer this model to a new domain, we associate a few target samples with one of the previously trained modes. In addition, we propose a spatial mixup augmentation strategy that further helps generalization across domains. We find this learning strategy to be surprisingly effective at domain transfer compared to a traditional classification or even state-of-the-art domain adaptation/few-shot learning methods. For instance, compared to the best baseline, our method improves the classification accuracy by 4.88% for zero-shot and by 8.38% for the few-shot case transferred from the FaceForensics++ to Dessa dataset.



### Rotation Invariant Deep CBIR
- **Arxiv ID**: http://arxiv.org/abs/2006.13046v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.13046v1)
- **Published**: 2020-06-21 21:09:31+00:00
- **Updated**: 2020-06-21 21:09:31+00:00
- **Authors**: Subhadip Maji, Smarajit Bose
- **Comment**: arXiv admin note: text overlap with arXiv:2002.07877
- **Journal**: None
- **Summary**: Introduction of Convolutional Neural Networks has improved results on almost every image-based problem and Content-Based Image Retrieval is not an exception. But the CNN features, being rotation invariant, creates problems to build a rotation-invariant CBIR system. Though rotation-invariant features can be hand-engineered, the retrieval accuracy is very low because by hand engineering only low-level features can be created, unlike deep learning models that create high-level features along with low-level features. This paper shows a novel method to build a rotational invariant CBIR system by introducing a deep learning orientation angle detection model along with the CBIR feature extraction model. This paper also highlights that this rotation invariant deep CBIR can retrieve images from a large dataset in real-time.



### Quickest Intruder Detection for Multiple User Active Authentication
- **Arxiv ID**: http://arxiv.org/abs/2006.11921v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.11921v1)
- **Published**: 2020-06-21 21:59:01+00:00
- **Updated**: 2020-06-21 21:59:01+00:00
- **Authors**: Pramuditha Perera, Julian Fierrez, Vishal M. Patel
- **Comment**: Accepted for publication in ICIP 2020
- **Journal**: None
- **Summary**: In this paper, we investigate how to detect intruders with low latency for Active Authentication (AA) systems with multiple-users. We extend the Quickest Change Detection (QCD) framework to the multiple-user case and formulate the Multiple-user Quickest Intruder Detection (MQID) algorithm. Furthermore, we extend the algorithm to the data-efficient scenario where intruder detection is carried out with fewer observation samples. We evaluate the effectiveness of the proposed method on two publicly available AA datasets on the face modality.



### Fully Automated 3D Segmentation of MR-Imaged Calf Muscle Compartments: Neighborhood Relationship Enhanced Fully Convolutional Network
- **Arxiv ID**: http://arxiv.org/abs/2006.11930v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.11930v2)
- **Published**: 2020-06-21 22:53:58+00:00
- **Updated**: 2020-12-21 22:15:25+00:00
- **Authors**: Zhihui Guo, Honghai Zhang, Zhi Chen, Ellen van der Plas, Laurie Gutmann, Daniel Thedens, Peggy Nopoulos, Milan Sonka
- **Comment**: To be appeared in journal Computerized Medical Imaging and Graphics
- **Journal**: None
- **Summary**: Automated segmentation of individual calf muscle compartments from 3D magnetic resonance (MR) images is essential for developing quantitative biomarkers for muscular disease progression and its prediction. Achieving clinically acceptable results is a challenging task due to large variations in muscle shape and MR appearance. Although deep convolutional neural networks (DCNNs) achieved improved accuracy in various image segmentation tasks, certain problems such as utilizing long-range information and incorporating high-level constraints remain unsolved. We present a novel fully convolutional network (FCN), called FilterNet, that utilizes contextual information in a large neighborhood and embeds edge-aware constraints for individual calf muscle compartment segmentations. An encoder-decoder architecture with flexible backbone blocks is used to systematically enlarge convolution receptive field and preserve information at all resolutions. Edge positions derived from the FCN output muscle probability maps are explicitly regularized using kernel-based edge detection in an end-to-end optimization framework. Our FilterNet was evaluated on 40 T1-weighted MR images of 10 healthy and 30 diseased subjects by 4-fold cross-validation. Mean DICE coefficients of 88.00%--91.29% and mean absolute surface positioning errors of 1.04--1.66 mm were achieved for the five 3D muscle compartments.



### Lyric Video Analysis Using Text Detection and Tracking
- **Arxiv ID**: http://arxiv.org/abs/2006.11933v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.11933v1)
- **Published**: 2020-06-21 22:57:09+00:00
- **Updated**: 2020-06-21 22:57:09+00:00
- **Authors**: Shota Sakaguchi, Jun Kato, Masataka Goto, Seiichi Uchida
- **Comment**: 15 pages, 8 figures, DAS 2020
- **Journal**: None
- **Summary**: We attempt to recognize and track lyric words in lyric videos. Lyric video is a music video showing the lyric words of a song. The main characteristic of lyric videos is that the lyric words are shown at frames synchronously with the music. The difficulty of recognizing and tracking the lyric words is that (1) the words are often decorated and geometrically distorted and (2) the words move arbitrarily and drastically in the video frame. The purpose of this paper is to analyze the motion of the lyric words in lyric videos, as the first step of automatic lyric video generation. In order to analyze the motion of lyric words, we first apply a state-of-the-art scene text detector and recognizer to each video frame. Then, lyric-frame matching is performed to establish the optimal correspondence between lyric words and the frames. After fixing the motion trajectories of individual lyric words from correspondence, we analyze the trajectories of the lyric words by k-medoids clustering and dynamic time warping (DTW).



