# Arxiv Papers in cs.CV on 2020-06-10
### A gaze driven fast-forward method for first-person videos
- **Arxiv ID**: http://arxiv.org/abs/2006.05569v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.05569v1)
- **Published**: 2020-06-10 00:08:42+00:00
- **Updated**: 2020-06-10 00:08:42+00:00
- **Authors**: Alan Carvalho Neves, Michel Melo Silva, Mario Fernando Montenegro Campos, Erickson Rangel Nascimento
- **Comment**: Accepted for presentation at EPIC@CVPR2020 workshop
- **Journal**: None
- **Summary**: The growing data sharing and life-logging cultures are driving an unprecedented increase in the amount of unedited First-Person Videos. In this paper, we address the problem of accessing relevant information in First-Person Videos by creating an accelerated version of the input video and emphasizing the important moments to the recorder. Our method is based on an attention model driven by gaze and visual scene analysis that provides a semantic score of each frame of the input video. We performed several experimental evaluations on publicly available First-Person Videos datasets. The results show that our methodology can fast-forward videos emphasizing moments when the recorder visually interact with scene components while not including monotonous clips.



### Deep Learning-based Aerial Image Segmentation with Open Data for Disaster Impact Assessment
- **Arxiv ID**: http://arxiv.org/abs/2006.05575v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.05575v1)
- **Published**: 2020-06-10 00:19:58+00:00
- **Updated**: 2020-06-10 00:19:58+00:00
- **Authors**: Ananya Gupta, Simon Watson, Hujun Yin
- **Comment**: Accepted in Neurocomputing, 2020
- **Journal**: None
- **Summary**: Satellite images are an extremely valuable resource in the aftermath of natural disasters such as hurricanes and tsunamis where they can be used for risk assessment and disaster management. In order to provide timely and actionable information for disaster response, in this paper a framework utilising segmentation neural networks is proposed to identify impacted areas and accessible roads in post-disaster scenarios. The effectiveness of pretraining with ImageNet on the task of aerial image segmentation has been analysed and performances of popular segmentation models compared. Experimental results show that pretraining on ImageNet usually improves the segmentation performance for a number of models. Open data available from OpenStreetMap (OSM) is used for training, forgoing the need for time-consuming manual annotation. The method also makes use of graph theory to update road network data available from OSM and to detect the changes caused by a natural disaster. Extensive experiments on data from the 2018 tsunami that struck Palu, Indonesia show the effectiveness of the proposed framework. ENetSeparable, with 30% fewer parameters compared to ENet, achieved comparable segmentation results to that of the state-of-the-art networks.



### Syn2Real Transfer Learning for Image Deraining using Gaussian Processes
- **Arxiv ID**: http://arxiv.org/abs/2006.05580v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.05580v1)
- **Published**: 2020-06-10 00:33:18+00:00
- **Updated**: 2020-06-10 00:33:18+00:00
- **Authors**: Rajeev Yasarla, Vishwanath A. Sindagi, Vishal M. Patel
- **Comment**: Accepted at CVPR 2020
- **Journal**: None
- **Summary**: Recent CNN-based methods for image deraining have achieved excellent performance in terms of reconstruction error as well as visual quality. However, these methods are limited in the sense that they can be trained only on fully labeled data. Due to various challenges in obtaining real world fully-labeled image deraining datasets, existing methods are trained only on synthetically generated data and hence, generalize poorly to real-world images. The use of real-world data in training image deraining networks is relatively less explored in the literature. We propose a Gaussian Process-based semi-supervised learning framework which enables the network in learning to derain using synthetic dataset while generalizing better using unlabeled real-world images. Through extensive experiments and ablations on several challenging datasets (such as Rain800, Rain200H and DDN-SIRR), we show that the proposed method, when trained on limited labeled data, achieves on-par performance with fully-labeled training. Additionally, we demonstrate that using unlabeled real-world images in the proposed GP-based framework results in superior performance as compared to existing methods. Code is available at: https://github.com/rajeevyasarla/Syn2Real



### Dual-level Semantic Transfer Deep Hashing for Efficient Social Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2006.05586v1
- **DOI**: None
- **Categories**: **cs.IR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.05586v1)
- **Published**: 2020-06-10 01:03:09+00:00
- **Updated**: 2020-06-10 01:03:09+00:00
- **Authors**: Lei Zhu, Hui Cui, Zhiyong Cheng, Jingjing Li, Zheng Zhang
- **Comment**: Accepted by IEEE TCSVT
- **Journal**: None
- **Summary**: Social network stores and disseminates a tremendous amount of user shared images. Deep hashing is an efficient indexing technique to support large-scale social image retrieval, due to its deep representation capability, fast retrieval speed and low storage cost. Particularly, unsupervised deep hashing has well scalability as it does not require any manually labelled data for training. However, owing to the lacking of label guidance, existing methods suffer from severe semantic shortage when optimizing a large amount of deep neural network parameters. Differently, in this paper, we propose a Dual-level Semantic Transfer Deep Hashing (DSTDH) method to alleviate this problem with a unified deep hash learning framework. Our model targets at learning the semantically enhanced deep hash codes by specially exploiting the user-generated tags associated with the social images. Specifically, we design a complementary dual-level semantic transfer mechanism to efficiently discover the potential semantics of tags and seamlessly transfer them into binary hash codes. On the one hand, instance-level semantics are directly preserved into hash codes from the associated tags with adverse noise removing. Besides, an image-concept hypergraph is constructed for indirectly transferring the latent high-order semantic correlations of images and tags into hash codes. Moreover, the hash codes are obtained simultaneously with the deep representation learning by the discrete hash optimization strategy. Extensive experiments on two public social image retrieval datasets validate the superior performance of our method compared with state-of-the-art hashing methods. The source codes of our method can be obtained at https://github.com/research2020-1/DSTDH



### CNN-Based Semantic Change Detection in Satellite Imagery
- **Arxiv ID**: http://arxiv.org/abs/2006.05589v1
- **DOI**: 10.1007/978-3-030-30493-5_61
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.05589v1)
- **Published**: 2020-06-10 01:06:03+00:00
- **Updated**: 2020-06-10 01:06:03+00:00
- **Authors**: Ananya Gupta, Elisabeth Welburn, Simon Watson, Hujun Yin
- **Comment**: None
- **Journal**: Proceedings of International Conference on Artificial Neural
  Networks , 2019. pg-669-684
- **Summary**: Timely disaster risk management requires accurate road maps and prompt damage assessment. Currently, this is done by volunteers manually marking satellite imagery of affected areas but this process is slow and often error-prone. Segmentation algorithms can be applied to satellite images to detect road networks. However, existing methods are unsuitable for disaster-struck areas as they make assumptions about the road network topology which may no longer be valid in these scenarios. Herein, we propose a CNN-based framework for identifying accessible roads in post-disaster imagery by detecting changes from pre-disaster imagery. Graph theory is combined with the CNN output for detecting semantic changes in road networks with OpenStreetMap data. Our results are validated with data of a tsunami-affected region in Palu, Indonesia acquired from DigitalGlobe.



### Condensing Two-stage Detection with Automatic Object Key Part Discovery
- **Arxiv ID**: http://arxiv.org/abs/2006.05597v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.05597v3)
- **Published**: 2020-06-10 01:20:47+00:00
- **Updated**: 2020-08-13 01:08:50+00:00
- **Authors**: Zhe Chen, Jing Zhang, Dacheng Tao
- **Comment**: None
- **Journal**: None
- **Summary**: Modern two-stage object detectors generally require excessively large models for their detection heads to achieve high accuracy. To address this problem, we propose that the model parameters of two-stage detection heads can be condensed and reduced by concentrating on object key parts. To this end, we first introduce an automatic object key part discovery task to make neural networks discover representative sub-parts in each foreground object. With these discovered key parts, we then decompose the object appearance modeling into a key part modeling process and a global modeling process for detection. Key part modeling encodes fine and detailed features from the discovered key parts, and global modeling encodes rough and holistic object characteristics. In practice, such decomposition allows us to significantly abridge model parameters without sacrificing much detection accuracy. Experiments on popular datasets illustrate that our proposed technique consistently maintains original performance while waiving around 50% of the model parameters of common two-stage detection heads, with the performance only deteriorating by 1.5% when waiving around 96% of the original model parameters. Codes are released on: https://github.com/zhechen/Condensing2stageDetection.



### Deep Learning for Change Detection in Remote Sensing Images: Comprehensive Review and Meta-Analysis
- **Arxiv ID**: http://arxiv.org/abs/2006.05612v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.05612v1)
- **Published**: 2020-06-10 02:14:08+00:00
- **Updated**: 2020-06-10 02:14:08+00:00
- **Authors**: Lazhar Khelifi, Max Mignotte
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning (DL) algorithms are considered as a methodology of choice for remote-sensing image analysis over the past few years. Due to its effective applications, deep learning has also been introduced for automatic change detection and achieved great success. The present study attempts to provide a comprehensive review and a meta-analysis of the recent progress in this subfield. Specifically, we first introduce the fundamentals of deep learning methods which arefrequently adopted for change detection. Secondly, we present the details of the meta-analysis conducted to examine the status of change detection DL studies. Then, we focus on deep learning-based change detection methodologies for remote sensing images by giving a general overview of the existing methods. Specifically, these deep learning-based methods were classified into three groups; fully supervised learning-based methods, fully unsupervised learning-based methods and transfer learning-based techniques. As a result of these investigations, promising new directions were identified for future research. This study will contribute in several ways to our understanding of deep learning for change detection and will provide a basis for further research.



### A survey on deep hashing for image retrieval
- **Arxiv ID**: http://arxiv.org/abs/2006.05627v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.05627v1)
- **Published**: 2020-06-10 03:01:59+00:00
- **Updated**: 2020-06-10 03:01:59+00:00
- **Authors**: Xiaopeng Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Hashing has been widely used in approximate nearest search for large-scale database retrieval for its computation and storage efficiency. Deep hashing, which devises convolutional neural network architecture to exploit and extract the semantic information or feature of images, has received increasing attention recently. In this survey, several deep supervised hashing methods for image retrieval are evaluated and I conclude three main different directions for deep supervised hashing methods. Several comments are made at the end. Moreover, to break through the bottleneck of the existing hashing methods, I propose a Shadow Recurrent Hashing(SRH) method as a try. Specifically, I devise a CNN architecture to extract the semantic features of images and design a loss function to encourage similar images projected close. To this end, I propose a concept: shadow of the CNN output. During optimization process, the CNN output and its shadow are guiding each other so as to achieve the optimal solution as much as possible. Several experiments on dataset CIFAR-10 show the satisfying performance of SRH.



### Scalable Backdoor Detection in Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2006.05646v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.05646v1)
- **Published**: 2020-06-10 04:12:53+00:00
- **Updated**: 2020-06-10 04:12:53+00:00
- **Authors**: Haripriya Harikumar, Vuong Le, Santu Rana, Sourangshu Bhattacharya, Sunil Gupta, Svetha Venkatesh
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, it has been shown that deep learning models are vulnerable to Trojan attacks, where an attacker can install a backdoor during training time to make the resultant model misidentify samples contaminated with a small trigger patch. Current backdoor detection methods fail to achieve good detection performance and are computationally expensive. In this paper, we propose a novel trigger reverse-engineering based approach whose computational complexity does not scale with the number of labels, and is based on a measure that is both interpretable and universal across different network and patch types. In experiments, we observe that our method achieves a perfect score in separating Trojaned models from pure models, which is an improvement over the current state-of-the art method.



### Agrupamento de Pixels para o Reconhecimento de Faces
- **Arxiv ID**: http://arxiv.org/abs/2006.05652v1
- **DOI**: None
- **Categories**: **cs.CV**, 62H30, I.2.10; I.5.3; I.4.2; I.4.10
- **Links**: [PDF](http://arxiv.org/pdf/2006.05652v1)
- **Published**: 2020-06-10 04:42:07+00:00
- **Updated**: 2020-06-10 04:42:07+00:00
- **Authors**: Tiago Buarque Assunção de Carvalho
- **Comment**: 21 pages, in Portuguese, 5 figures, book chapter, recortado
  (adapatado) da tese
- **Journal**: None
- **Summary**: This research starts with the observation that face recognition can suffer a low impact from significant image shrinkage. To explain this fact, we proposed the Pixel Clustering methodology. It defines regions in the image in which its pixels are very similar to each other. We extract features from each region. We used three face databases in the experiments. We noticed that 512 is the maximum number of features needed for high accuracy image recognition. The proposed method is also robust, even if only it uses a few classes from the training set.



### H3DNet: 3D Object Detection Using Hybrid Geometric Primitives
- **Arxiv ID**: http://arxiv.org/abs/2006.05682v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.05682v3)
- **Published**: 2020-06-10 06:44:53+00:00
- **Updated**: 2020-07-23 19:16:39+00:00
- **Authors**: Zaiwei Zhang, Bo Sun, Haitao Yang, Qixing Huang
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce H3DNet, which takes a colorless 3D point cloud as input and outputs a collection of oriented object bounding boxes (or BB) and their semantic labels. The critical idea of H3DNet is to predict a hybrid set of geometric primitives, i.e., BB centers, BB face centers, and BB edge centers. We show how to convert the predicted geometric primitives into object proposals by defining a distance function between an object and the geometric primitives. This distance function enables continuous optimization of object proposals, and its local minimums provide high-fidelity object proposals. H3DNet then utilizes a matching and refinement module to classify object proposals into detected objects and fine-tune the geometric parameters of the detected objects. The hybrid set of geometric primitives not only provides more accurate signals for object detection than using a single type of geometric primitives, but it also provides an overcomplete set of constraints on the resulting 3D layout. Therefore, H3DNet can tolerate outliers in predicted geometric primitives. Our model achieves state-of-the-art 3D detection results on two large datasets with real 3D scans, ScanNet and SUN RGB-D.



### TubeTK: Adopting Tubes to Track Multi-Object in a One-Step Training Model
- **Arxiv ID**: http://arxiv.org/abs/2006.05683v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.05683v1)
- **Published**: 2020-06-10 06:45:05+00:00
- **Updated**: 2020-06-10 06:45:05+00:00
- **Authors**: Bo Pang, Yizhuo Li, Yifan Zhang, Muchen Li, Cewu Lu
- **Comment**: CVPR-2020 oral paper
- **Journal**: None
- **Summary**: Multi-object tracking is a fundamental vision problem that has been studied for a long time. As deep learning brings excellent performances to object detection algorithms, Tracking by Detection (TBD) has become the mainstream tracking framework. Despite the success of TBD, this two-step method is too complicated to train in an end-to-end manner and induces many challenges as well, such as insufficient exploration of video spatial-temporal information, vulnerability when facing object occlusion, and excessive reliance on detection results. To address these challenges, we propose a concise end-to-end model TubeTK which only needs one step training by introducing the ``bounding-tube" to indicate temporal-spatial locations of objects in a short video clip. TubeTK provides a novel direction of multi-object tracking, and we demonstrate its potential to solve the above challenges without bells and whistles. We analyze the performance of TubeTK on several MOT benchmarks and provide empirical evidence to show that TubeTK has the ability to overcome occlusions to some extent without any ancillary technologies like Re-ID. Compared with other methods that adopt private detection results, our one-stage end-to-end model achieves state-of-the-art performances even if it adopts no ready-made detection results. We hope that the proposed TubeTK model can serve as a simple but strong alternative for video-based MOT task. The code and models are available at https://github.com/BoPang1996/TubeTK.



### Combining the band-limited parameterization and Semi-Lagrangian Runge--Kutta integration for efficient PDE-constrained LDDMM
- **Arxiv ID**: http://arxiv.org/abs/2006.06823v1
- **DOI**: None
- **Categories**: **math.NA**, cs.CV, cs.NA
- **Links**: [PDF](http://arxiv.org/pdf/2006.06823v1)
- **Published**: 2020-06-10 07:12:18+00:00
- **Updated**: 2020-06-10 07:12:18+00:00
- **Authors**: Monica Hernandez
- **Comment**: None
- **Journal**: None
- **Summary**: The family of PDE-constrained LDDMM methods is emerging as a particularly interesting approach for physically meaningful diffeomorphic transformations. The original combination of Gauss--Newton--Krylov optimization and Runge--Kutta integration, shows excellent numerical accuracy and fast convergence rate. However, its most significant limitation is the huge computational complexity, hindering its extensive use in Computational Anatomy applied studies. This limitation has been treated independently by the problem formulation in the space of band-limited vector fields and Semi-Lagrangian integration. The purpose of this work is to combine both in three variants of band-limited PDE-constrained LDDMM for further increasing their computational efficiency. The accuracy of the resulting methods is evaluated extensively. For all the variants, the proposed combined approach shows a significant increment of the computational efficiency. In addition, the variant based on the deformation state equation is positioned consistently as the best performing method across all the evaluation frameworks in terms of accuracy and efficiency.



### Rendering Natural Camera Bokeh Effect with Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2006.05698v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.05698v1)
- **Published**: 2020-06-10 07:28:06+00:00
- **Updated**: 2020-06-10 07:28:06+00:00
- **Authors**: Andrey Ignatov, Jagruti Patel, Radu Timofte
- **Comment**: None
- **Journal**: None
- **Summary**: Bokeh is an important artistic effect used to highlight the main object of interest on the photo by blurring all out-of-focus areas. While DSLR and system camera lenses can render this effect naturally, mobile cameras are unable to produce shallow depth-of-field photos due to a very small aperture diameter of their optics. Unlike the current solutions simulating bokeh by applying Gaussian blur to image background, in this paper we propose to learn a realistic shallow focus technique directly from the photos produced by DSLR cameras. For this, we present a large-scale bokeh dataset consisting of 5K shallow / wide depth-of-field image pairs captured using the Canon 7D DSLR with 50mm f/1.8 lenses. We use these images to train a deep learning model to reproduce a natural bokeh effect based on a single narrow-aperture image. The experimental results show that the proposed approach is able to render a plausible non-uniform bokeh even in case of complex input data with multiple objects. The dataset, pre-trained models and codes used in this paper are available on the project website.



### Delta Descriptors: Change-Based Place Representation for Robust Visual Localization
- **Arxiv ID**: http://arxiv.org/abs/2006.05700v2
- **DOI**: 10.1109/LRA.2020.3005627
- **Categories**: **cs.CV**, cs.IR, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2006.05700v2)
- **Published**: 2020-06-10 07:37:29+00:00
- **Updated**: 2020-07-30 07:24:52+00:00
- **Authors**: Sourav Garg, Ben Harwood, Gaurangi Anand, Michael Milford
- **Comment**: 8 pages and 7 figures. Published in 2020 IEEE Robotics and Automation
  Letters (RA-L)
- **Journal**: None
- **Summary**: Visual place recognition is challenging because there are so many factors that can cause the appearance of a place to change, from day-night cycles to seasonal change to atmospheric conditions. In recent years a large range of approaches have been developed to address this challenge including deep-learnt image descriptors, domain translation, and sequential filtering, all with shortcomings including generality and velocity-sensitivity. In this paper we propose a novel descriptor derived from tracking changes in any learned global descriptor over time, dubbed Delta Descriptors. Delta Descriptors mitigate the offsets induced in the original descriptor matching space in an unsupervised manner by considering temporal differences across places observed along a route. Like all other approaches, Delta Descriptors have a shortcoming - volatility on a frame to frame basis - which can be overcome by combining them with sequential filtering methods. Using two benchmark datasets, we first demonstrate the high performance of Delta Descriptors in isolation, before showing new state-of-the-art performance when combined with sequence-based matching. We also present results demonstrating the approach working with four different underlying descriptor types, and two other beneficial properties of Delta Descriptors in comparison to existing techniques: their increased inherent robustness to variations in camera motion and a reduced rate of performance degradation as dimensional reduction is applied. Source code is made available at https://github.com/oravus/DeltaDescriptors.



### Unique Faces Recognition in Videos
- **Arxiv ID**: http://arxiv.org/abs/2006.05713v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.05713v1)
- **Published**: 2020-06-10 08:08:26+00:00
- **Updated**: 2020-06-10 08:08:26+00:00
- **Authors**: Jiahao Huo, Terence L van Zyl
- **Comment**: Paper was accepted into Fusion 2020 conference but will only be
  published after the virtual conference in July 2020. 7 pages long
- **Journal**: None
- **Summary**: This paper tackles face recognition in videos employing metric learning methods and similarity ranking models. The paper compares the use of the Siamese network with contrastive loss and Triplet Network with triplet loss implementing the following architectures: Google/Inception architecture, 3D Convolutional Network (C3D), and a 2-D Long short-term memory (LSTM) Recurrent Neural Network. We make use of still images and sequences from videos for training the networks and compare the performances implementing the above architectures. The dataset used was the YouTube Face Database designed for investigating the problem of face recognition in videos. The contribution of this paper is two-fold: to begin, the experiments have established 3-D Convolutional networks and 2-D LSTMs with the contrastive loss on image sequences do not outperform Google/Inception architecture with contrastive loss in top $n$ rank face retrievals with still images. However, the 3-D Convolution networks and 2-D LSTM with triplet Loss outperform the Google/Inception with triplet loss in top $n$ rank face retrievals on the dataset; second, a Support Vector Machine (SVM) was used in conjunction with the CNNs' learned feature representations for facial identification. The results show that feature representation learned with triplet loss is significantly better for n-shot facial identification compared to contrastive loss. The most useful feature representations for facial identification are from the 2-D LSTM with triplet loss. The experiments show that learning spatio-temporal features from video sequences is beneficial for facial recognition in videos.



### Real-time single image depth perception in the wild with handheld devices
- **Arxiv ID**: http://arxiv.org/abs/2006.05724v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2006.05724v1)
- **Published**: 2020-06-10 08:30:20+00:00
- **Updated**: 2020-06-10 08:30:20+00:00
- **Authors**: Filippo Aleotti, Giulio Zaccaroni, Luca Bartolomei, Matteo Poggi, Fabio Tosi, Stefano Mattoccia
- **Comment**: 11 pages, 9 figures
- **Journal**: None
- **Summary**: Depth perception is paramount to tackle real-world problems, ranging from autonomous driving to consumer applications. For the latter, depth estimation from a single image represents the most versatile solution, since a standard camera is available on almost any handheld device. Nonetheless, two main issues limit its practical deployment: i) the low reliability when deployed in-the-wild and ii) the demanding resource requirements to achieve real-time performance, often not compatible with such devices. Therefore, in this paper, we deeply investigate these issues showing how they are both addressable adopting appropriate network design and training strategies -- also outlining how to map the resulting networks on handheld devices to achieve real-time performance. Our thorough evaluation highlights the ability of such fast networks to generalize well to new environments, a crucial feature required to tackle the extremely varied contexts faced in real applications. Indeed, to further support this evidence, we report experimental results concerning real-time depth-aware augmented reality and image blurring with smartphones in-the-wild.



### Estimating semantic structure for the VQA answer space
- **Arxiv ID**: http://arxiv.org/abs/2006.05726v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2006.05726v2)
- **Published**: 2020-06-10 08:32:56+00:00
- **Updated**: 2021-04-08 10:33:21+00:00
- **Authors**: Corentin Kervadec, Grigory Antipov, Moez Baccouche, Christian Wolf
- **Comment**: [WARNING] We want to notice the reader that additional experiments
  (not in the paper) have shown that using a `random' semantic space performs
  as much as the proposed semantic loss. This additional result question the
  effectiveness of our method
- **Journal**: None
- **Summary**: Since its appearance, Visual Question Answering (VQA, i.e. answering a question posed over an image), has always been treated as a classification problem over a set of predefined answers. Despite its convenience, this classification approach poorly reflects the semantics of the problem limiting the answering to a choice between independent proposals, without taking into account the similarity between them (e.g. equally penalizing for answering cat or German shepherd instead of dog). We address this issue by proposing (1) two measures of proximity between VQA classes, and (2) a corresponding loss which takes into account the estimated proximity. This significantly improves the generalization of VQA models by reducing their language bias. In particular, we show that our approach is completely model-agnostic since it allows consistent improvements with three different VQA models. Finally, by combining our method with a language bias reduction approach, we report SOTA-level performance on the challenging VQAv2-CP dataset.



### Diagnosing Rarity in Human-Object Interaction Detection
- **Arxiv ID**: http://arxiv.org/abs/2006.05728v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.05728v1)
- **Published**: 2020-06-10 08:35:29+00:00
- **Updated**: 2020-06-10 08:35:29+00:00
- **Authors**: Mert Kilickaya, Arnold Smeulders
- **Comment**: Accepted at CVPR'20 Workshop on Learning from Limited Labels
- **Journal**: None
- **Summary**: Human-object interaction (HOI) detection is a core task in computer vision. The goal is to localize all human-object pairs and recognize their interactions. An interaction defined by a <verb, noun> tuple leads to a long-tailed visual recognition challenge since many combinations are rarely represented. The performance of the proposed models is limited especially for the tail categories, but little has been done to understand the reason. To that end, in this paper, we propose to diagnose rarity in HOI detection. We propose a three-step strategy, namely Detection, Identification and Recognition where we carefully analyse the limiting factors by studying state-of-the-art models. Our findings indicate that detection and identification steps are altered by the interaction signals like occlusion and relative location, as a result limiting the recognition accuracy.



### Object Detection in the DCT Domain: is Luminance the Solution?
- **Arxiv ID**: http://arxiv.org/abs/2006.05732v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.05732v3)
- **Published**: 2020-06-10 08:43:40+00:00
- **Updated**: 2021-07-14 08:09:24+00:00
- **Authors**: Benjamin Deguerre, Clement Chatelain, Gilles Gasso
- **Comment**: ICPR 2020
- **Journal**: None
- **Summary**: Object detection in images has reached unprecedented performances. The state-of-the-art methods rely on deep architectures that extract salient features and predict bounding boxes enclosing the objects of interest. These methods essentially run on RGB images. However, the RGB images are often compressed by the acquisition devices for storage purpose and transfer efficiency. Hence, their decompression is required for object detectors. To gain in efficiency, this paper proposes to take advantage of the compressed representation of images to carry out object detection usable in constrained resources conditions.   Specifically, we focus on JPEG images and propose a thorough analysis of detection architectures newly designed in regard of the peculiarities of the JPEG norm. This leads to a $\times 1.7$ speed up in comparison with a standard RGB-based architecture, while only reducing the detection performance by 5.5%. Additionally, our empirical findings demonstrate that only part of the compressed JPEG information, namely the luminance component, may be required to match detection accuracy of the full input methods.



### 3D Human Mesh Regression with Dense Correspondence
- **Arxiv ID**: http://arxiv.org/abs/2006.05734v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.05734v2)
- **Published**: 2020-06-10 08:50:53+00:00
- **Updated**: 2021-06-06 13:00:47+00:00
- **Authors**: Wang Zeng, Wanli Ouyang, Ping Luo, Wentao Liu, Xiaogang Wang
- **Comment**: To appear at CVPR 2020
- **Journal**: None
- **Summary**: Estimating 3D mesh of the human body from a single 2D image is an important task with many applications such as augmented reality and Human-Robot interaction. However, prior works reconstructed 3D mesh from global image feature extracted by using convolutional neural network (CNN), where the dense correspondences between the mesh surface and the image pixels are missing, leading to suboptimal solution. This paper proposes a model-free 3D human mesh estimation framework, named DecoMR, which explicitly establishes the dense correspondence between the mesh and the local image features in the UV space (i.e. a 2D space used for texture mapping of 3D mesh). DecoMR first predicts pixel-to-surface dense correspondence map (i.e., IUV image), with which we transfer local features from the image space to the UV space. Then the transferred local image features are processed in the UV space to regress a location map, which is well aligned with transferred features. Finally we reconstruct 3D human mesh from the regressed location map with a predefined mapping function. We also observe that the existing discontinuous UV map are unfriendly to the learning of network. Therefore, we propose a novel UV map that maintains most of the neighboring relations on the original mesh surface. Experiments demonstrate that our proposed local feature alignment and continuous UV map outperforms existing 3D mesh based methods on multiple public benchmarks. Code will be made available at https://github.com/zengwang430521/DecoMR



### Applying Deep-Learning-Based Computer Vision to Wireless Communications: Methodologies, Opportunities, and Challenges
- **Arxiv ID**: http://arxiv.org/abs/2006.05782v4
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.05782v4)
- **Published**: 2020-06-10 11:37:49+00:00
- **Updated**: 2020-12-02 12:25:26+00:00
- **Authors**: Yu Tian, Gaofeng Pan, Mohamed-Slim Alouini
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning (DL) has seen great success in the computer vision (CV) field, and related techniques have been used in security, healthcare, remote sensing, and many other fields. As a parallel development, visual data has become universal in daily life, easily generated by ubiquitous low-cost cameras. Therefore, exploring DL-based CV may yield useful information about objects, such as their number, locations, distribution, motion, etc. Intuitively, DL-based CV can also facilitate and improve the designs of wireless communications, especially in dynamic network scenarios. However, so far, such work is rare in the literature. The primary purpose of this article, then, is to introduce ideas about applying DL-based CV in wireless communications to bring some novel degrees of freedom to both theoretical research and engineering applications. To illustrate how DL-based CV can be applied in wireless communications, an example of using a DL-based CV with a millimeter-wave (mmWave) system is given to realize optimal mmWave multiple-input and multiple-output (MIMO) beamforming in mobile scenarios. In this example, we propose a framework to predict future beam indices from previously observed beam indices and images of street views using ResNet, 3-dimensional ResNext, and a long short-term memory network. The experimental results show that our frameworks achieve much higher accuracy than the baseline method, and that visual data can significantly improve the performance of the MIMO beamforming system. Finally, we discuss the opportunities and challenges of applying DL-based CV in wireless communications.



### Image Enhancement and Object Recognition for Night Vision Surveillance
- **Arxiv ID**: http://arxiv.org/abs/2006.05787v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2006.05787v1)
- **Published**: 2020-06-10 11:57:56+00:00
- **Updated**: 2020-06-10 11:57:56+00:00
- **Authors**: Aashish Bhandari, Aayush Kafle, Pranjal Dhakal, Prateek Raj Joshi, Dinesh Baniya Kshatri
- **Comment**: International Conference on Recent Trends in Computational
  Engineering and Technologies, 2018
- **Journal**: None
- **Summary**: Object recognition is a critical part of any surveillance system. It is the matter of utmost concern to identify intruders and foreign objects in the area where surveillance is done. The performance of surveillance system using the traditional camera in daylight is vastly superior as compared to night. The main problem for surveillance during the night is the objects captured by traditional cameras have low contrast against the background because of the absence of ambient light in the visible spectrum. Due to that reason, the image is taken in low light condition using an Infrared Camera and the image is enhanced to obtain an image with higher contrast using different enhancing algorithms based on the spatial domain. The enhanced image is then sent to the classification process. The classification is done by using convolutional neural network followed by a fully connected layer of neurons. The accuracy of classification after implementing different enhancement algorithms is compared in this paper.



### Fully-automated deep learning slice-based muscle estimation from CT images for sarcopenia assessment
- **Arxiv ID**: http://arxiv.org/abs/2006.06432v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.06432v1)
- **Published**: 2020-06-10 12:05:55+00:00
- **Updated**: 2020-06-10 12:05:55+00:00
- **Authors**: Fahdi Kanavati, Shah Islam, Zohaib Arain, Eric O. Aboagye, Andrea Rockall
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1811.09244
- **Journal**: None
- **Summary**: Objective: To demonstrate the effectiveness of using a deep learning-based approach for a fully automated slice-based measurement of muscle mass for assessing sarcopenia on CT scans of the abdomen without any case exclusion criteria.   Materials and Methods: This retrospective study was conducted using a collection of public and privately available CT images (n = 1070). The method consisted of two stages: slice detection from a CT volume and single-slice CT segmentation. Both stages used Fully Convolutional Neural Networks (FCNN) and were based on a UNet-like architecture. Input data consisted of CT volumes with a variety of fields of view. The output consisted of a segmented muscle mass on a CT slice at the level of L3 vertebra. The muscle mass is segmented into erector spinae, psoas, and rectus abdominus muscle groups. The output was tested against manual ground-truth segmentation by an expert annotator.   Results: 3-fold cross validation was used to evaluate the proposed method. The slice detection cross validation error was 1.41+-5.02 (in slices). The segmentation cross validation Dice overlaps were 0.97+-0.02, 0.95+-0.04, 0.94+-0.04 for erector spinae, psoas, and rectus abdominus, respectively, and 0.96+-0.02 for the combined muscle mass.   Conclusion: A deep learning approach to detect CT slices and segment muscle mass to perform slice-based analysis of sarcopenia is an effective and promising approach. The use of FCNN to accurately and efficiently detect a slice in CT volumes with a variety of fields of view, occlusions, and slice thicknesses was demonstrated.



### Embedding Task Knowledge into 3D Neural Networks via Self-supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2006.05798v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.05798v1)
- **Published**: 2020-06-10 12:37:39+00:00
- **Updated**: 2020-06-10 12:37:39+00:00
- **Authors**: Jiuwen Zhu, Yuexiang Li, Yifan Hu, S. Kevin Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning highly relies on the amount of annotated data. However, annotating medical images is extremely laborious and expensive. To this end, self-supervised learning (SSL), as a potential solution for deficient annotated data, attracts increasing attentions from the community. However, SSL approaches often design a proxy task that is not necessarily related to target task. In this paper, we propose a novel SSL approach for 3D medical image classification, namely Task-related Contrastive Prediction Coding (TCPC), which embeds task knowledge into training 3D neural networks. The proposed TCPC first locates the initial candidate lesions via supervoxel estimation using simple linear iterative clustering. Then, we extract features from the sub-volume cropped around potential lesion areas, and construct a calibrated contrastive predictive coding scheme for self-supervised learning. Extensive experiments are conducted on public and private datasets. The experimental results demonstrate the effectiveness of embedding lesion-related prior-knowledge into neural networks for 3D medical image classification.



### To Regularize or Not To Regularize? The Bias Variance Trade-off in Regularized AEs
- **Arxiv ID**: http://arxiv.org/abs/2006.05838v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.05838v2)
- **Published**: 2020-06-10 14:00:14+00:00
- **Updated**: 2020-09-19 10:56:48+00:00
- **Authors**: Arnab Kumar Mondal, Himanshu Asnani, Parag Singla, Prathosh AP
- **Comment**: None
- **Journal**: None
- **Summary**: Regularized Auto-Encoders (RAEs) form a rich class of neural generative models. They effectively model the joint-distribution between the data and the latent space using an Encoder-Decoder combination, with regularization imposed in terms of a prior over the latent space. Despite their advantages, such as stability in training, the performance of AE based models has not reached the superior standards as that of the other generative models such as Generative Adversarial Networks (GANs). Motivated by this, we examine the effect of the latent prior on the generation quality of deterministic AE models in this paper. Specifically, we consider the class of RAEs with deterministic Encoder-Decoder pairs, Wasserstein Auto-Encoders (WAE), and show that having a fixed prior distribution, \textit{a priori}, oblivious to the dimensionality of the `true' latent space, will lead to the infeasibility of the optimization problem considered. Further, we show that, in the finite data regime, despite knowing the correct latent dimensionality, there exists a bias-variance trade-off with any arbitrary prior imposition. As a remedy to both the issues mentioned above, we introduce an additional state space in the form of flexibly learnable latent priors, in the optimization objective of the WAEs. We implicitly learn the distribution of the latent prior jointly with the AE training, which not only makes the learning objective feasible but also facilitates operation on different points of the bias-variance curve. We show the efficacy of our model, called FlexAE, through several experiments on multiple datasets, and demonstrate that it is the new state-of-the-art for the AE based generative models.



### Searching Learning Strategy with Reinforcement Learning for 3D Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2006.05847v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.05847v1)
- **Published**: 2020-06-10 14:24:06+00:00
- **Updated**: 2020-06-10 14:24:06+00:00
- **Authors**: Dong Yang, Holger Roth, Ziyue Xu, Fausto Milletari, Ling Zhang, Daguang Xu
- **Comment**: 9 pages, 1 figures
- **Journal**: Published at MICCAI 2019
- **Summary**: Deep neural network (DNN) based approaches have been widely investigated and deployed in medical image analysis. For example, fully convolutional neural networks (FCN) achieve the state-of-the-art performance in several applications of 2D/3D medical image segmentation. Even the baseline neural network models (U-Net, V-Net, etc.) have been proven to be very effective and efficient when the training process is set up properly. Nevertheless, to fully exploit the potentials of neural networks, we propose an automated searching approach for the optimal training strategy with reinforcement learning. The proposed approach can be utilized for tuning hyper-parameters, and selecting necessary data augmentation with certain probabilities. The proposed approach is validated on several tasks of 3D medical image segmentation. The performance of the baseline model is boosted after searching, and it can achieve comparable accuracy to other manually-tuned state-of-the-art segmentation approaches.



### Geometry-Aware Segmentation of Remote Sensing Images via Implicit Height Estimation
- **Arxiv ID**: http://arxiv.org/abs/2006.05848v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.05848v2)
- **Published**: 2020-06-10 14:24:10+00:00
- **Updated**: 2020-09-22 01:48:22+00:00
- **Authors**: Xiang Li, Lingjing Wang, Yi Fang
- **Comment**: 13 pages, 10 figures
- **Journal**: None
- **Summary**: Recent studies have shown the benefits of using additional elevation data (e.g., DSM) for enhancing the performance of the semantic segmentation of aerial images. However, previous methods mostly adopt 3D elevation information as additional inputs. While in many real-world applications, one does not have the corresponding DSM information at hand and the spatial resolution of acquired DSM images usually do not match the aerial images. To alleviate this data constraint and also take advantage of 3D elevation information, in this paper, we introduce a geometry-aware segmentation model that achieves accurate semantic labeling of aerial images via joint height estimation. Instead of using a single-stream encoder-decoder network for semantic labeling, we design a separate decoder branch to predict the height map and use the DSM images as side supervision to train this newly designed decoder branch. In this way, our model does not require DSM as model input and still benefits from the helpful 3D geometric information during training. Moreover, we develop a new geometry-aware convolution module that fuses the 3D geometric features from the height decoder branch and the 2D contextual features from the semantic segmentation branch. The fused feature embeddings can produce geometry-aware segmentation maps with enhanced performance. Our model is trained with DSM images as side supervision, while in the inference stage, it does not require DSM data and directly predicts the semantic labels in an end-to-end fashion. Experiments on ISPRS Vaihingen and Potsdam datasets demonstrate the effectiveness of the proposed method for the semantic segmentation of aerial images. The proposed model achieves remarkable performance on both datasets without using any hand-crafted features or post-processing.



### A systematic review on the role of artificial intelligence in sonographic diagnosis of thyroid cancer: Past, present and future
- **Arxiv ID**: http://arxiv.org/abs/2006.05861v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2006.05861v1)
- **Published**: 2020-06-10 14:38:05+00:00
- **Updated**: 2020-06-10 14:38:05+00:00
- **Authors**: Fatemeh Abdolali, Atefeh Shahroudnejad, Abhilash Rakkunedeth Hareendranathan, Jacob L Jaremko, Michelle Noga, Kumaradevan Punithakumar
- **Comment**: None
- **Journal**: None
- **Summary**: Thyroid cancer is common worldwide, with a rapid increase in prevalence across North America in recent years. While most patients present with palpable nodules through physical examination, a large number of small and medium-sized nodules are detected by ultrasound examination. Suspicious nodules are then sent for biopsy through fine needle aspiration. Since biopsies are invasive and sometimes inconclusive, various research groups have tried to develop computer-aided diagnosis systems. Earlier approaches along these lines relied on clinically relevant features that were manually identified by radiologists. With the recent success of artificial intelligence (AI), various new methods are being developed to identify these features in thyroid ultrasound automatically. In this paper, we present a systematic review of state-of-the-art on AI application in sonographic diagnosis of thyroid cancer. This review follows a methodology-based classification of the different techniques available for thyroid cancer diagnosis. With more than 50 papers included in this review, we reflect on the trends and challenges of the field of sonographic diagnosis of thyroid malignancies and potential of computer-aided diagnosis to increase the impact of ultrasound applications on the future of thyroid cancer diagnosis. Machine learning will continue to play a fundamental role in the development of future thyroid cancer diagnosis frameworks.



### WasteNet: Waste Classification at the Edge for Smart Bins
- **Arxiv ID**: http://arxiv.org/abs/2006.05873v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/2006.05873v1)
- **Published**: 2020-06-10 14:57:58+00:00
- **Updated**: 2020-06-10 14:57:58+00:00
- **Authors**: Gary White, Christian Cabrera, Andrei Palade, Fan Li, Siobhan Clarke
- **Comment**: 8 pages, 9 figures
- **Journal**: None
- **Summary**: Smart Bins have become popular in smart cities and campuses around the world. These bins have a compaction mechanism that increases the bins' capacity as well as automated real-time collection notifications. In this paper, we propose WasteNet, a waste classification model based on convolutional neural networks that can be deployed on a low power device at the edge of the network, such as a Jetson Nano. The problem of segregating waste is a big challenge for many countries around the world. Automated waste classification at the edge allows for fast intelligent decisions in smart bins without needing access to the cloud. Waste is classified into six categories: paper, cardboard, glass, metal, plastic and other. Our model achieves a 97\% prediction accuracy on the test dataset. This level of classification accuracy will help to alleviate some common smart bin problems, such as recycling contamination, where different types of waste become mixed with recycling waste causing the bin to be contaminated. It also makes the bins more user friendly as citizens do not have to worry about disposing their rubbish in the correct bin as the smart bin will be able to make the decision for them.



### Speech Fusion to Face: Bridging the Gap Between Human's Vocal Characteristics and Facial Imaging
- **Arxiv ID**: http://arxiv.org/abs/2006.05888v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.05888v1)
- **Published**: 2020-06-10 15:19:31+00:00
- **Updated**: 2020-06-10 15:19:31+00:00
- **Authors**: Yeqi Bai, Tao Ma, Lipo Wang, Zhenjie Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: While deep learning technologies are now capable of generating realistic images confusing humans, the research efforts are turning to the synthesis of images for more concrete and application-specific purposes. Facial image generation based on vocal characteristics from speech is one of such important yet challenging tasks. It is the key enabler to influential use cases of image generation, especially for business in public security and entertainment. Existing solutions to the problem of speech2face renders limited image quality and fails to preserve facial similarity due to the lack of quality dataset for training and appropriate integration of vocal features. In this paper, we investigate these key technical challenges and propose Speech Fusion to Face, or SF2F in short, attempting to address the issue of facial image quality and the poor connection between vocal feature domain and modern image generation models. By adopting new strategies on data model and training, we demonstrate dramatic performance boost over state-of-the-art solution, by doubling the recall of individual identity, and lifting the quality score from 15 to 19 based on the mutual information score with VGGFace classifier.



### DisCont: Self-Supervised Visual Attribute Disentanglement using Context Vectors
- **Arxiv ID**: http://arxiv.org/abs/2006.05895v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.05895v2)
- **Published**: 2020-06-10 15:29:20+00:00
- **Updated**: 2020-06-29 23:23:12+00:00
- **Authors**: Sarthak Bhagat, Vishaal Udandarao, Shagun Uppal
- **Comment**: Published at the 37th International Conference on Machine Learning
  (ICML 2020) Workshop on ML Interpretability for Scientific Discovery
- **Journal**: None
- **Summary**: Disentangling the underlying feature attributes within an image with no prior supervision is a challenging task. Models that can disentangle attributes well provide greater interpretability and control. In this paper, we propose a self-supervised framework DisCont to disentangle multiple attributes by exploiting the structural inductive biases within images. Motivated by the recent surge in contrastive learning paradigms, our model bridges the gap between self-supervised contrastive learning algorithms and unsupervised disentanglement. We evaluate the efficacy of our approach, both qualitatively and quantitatively, on four benchmark datasets.



### Deep Learning with Attention Mechanism for Predicting Driver Intention at Intersection
- **Arxiv ID**: http://arxiv.org/abs/2006.05918v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.HC, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2006.05918v1)
- **Published**: 2020-06-10 16:12:00+00:00
- **Updated**: 2020-06-10 16:12:00+00:00
- **Authors**: Abenezer Girma, Seifemichael Amsalu, Abrham Workineh, Mubbashar Khan, Abdollah Homaifar
- **Comment**: IEEE Intelligent Vehicles Symposium 2020 (IEEE IV 2020)
- **Journal**: None
- **Summary**: In this paper, a driver's intention prediction near a road intersection is proposed. Our approach uses a deep bidirectional Long Short-Term Memory (LSTM) with an attention mechanism model based on a hybrid-state system (HSS) framework. As intersection is considered to be as one of the major source of road accidents, predicting a driver's intention at an intersection is very crucial. Our method uses a sequence to sequence modeling with an attention mechanism to effectively exploit temporal information out of the time-series vehicular data including velocity and yaw-rate. The model then predicts ahead of time whether the target vehicle/driver will go straight, stop, or take right or left turn. The performance of the proposed approach is evaluated on a naturalistic driving dataset and results show that our method achieves high accuracy as well as outperforms other methods. The proposed solution is promising to be applied in advanced driver assistance systems (ADAS) and as part of active safety system of autonomous vehicles.



### Separable Four Points Fundamental Matrix
- **Arxiv ID**: http://arxiv.org/abs/2006.05926v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.05926v2)
- **Published**: 2020-06-10 16:21:17+00:00
- **Updated**: 2020-09-29 22:56:12+00:00
- **Authors**: Gil Ben-Artzi
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel approach for RANSAC-based computation of the fundamental matrix based on epipolar homography decomposition. We analyze the geometrical meaning of the decomposition-based representation and show that it directly induces a consecutive sampling strategy of two independent sets of correspondences. We show that our method guarantees a minimal number of evaluated hypotheses with respect to current minimal approaches, on the condition that there are four correspondences on an image line. We validate our approach on real-world image pairs, providing fast and accurate results.



### Recent Advances in 3D Object and Hand Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2006.05927v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.05927v1)
- **Published**: 2020-06-10 16:25:28+00:00
- **Updated**: 2020-06-10 16:25:28+00:00
- **Authors**: Vincent Lepetit
- **Comment**: None
- **Journal**: None
- **Summary**: 3D object and hand pose estimation have huge potentials for Augmented Reality, to enable tangible interfaces, natural interfaces, and blurring the boundaries between the real and virtual worlds. In this chapter, we present the recent developments for 3D object and hand pose estimation using cameras, and discuss their abilities and limitations and the possible future development of the field.



### Dataset Condensation with Gradient Matching
- **Arxiv ID**: http://arxiv.org/abs/2006.05929v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.05929v3)
- **Published**: 2020-06-10 16:30:52+00:00
- **Updated**: 2021-03-08 13:31:22+00:00
- **Authors**: Bo Zhao, Konda Reddy Mopuri, Hakan Bilen
- **Comment**: None
- **Journal**: International Conference on Learning Representations 2021
- **Summary**: As the state-of-the-art machine learning methods in many fields rely on larger datasets, storing datasets and training models on them become significantly more expensive. This paper proposes a training set synthesis technique for data-efficient learning, called Dataset Condensation, that learns to condense large dataset into a small set of informative synthetic samples for training deep neural networks from scratch. We formulate this goal as a gradient matching problem between the gradients of deep neural network weights that are trained on the original and our synthetic data. We rigorously evaluate its performance in several computer vision benchmarks and demonstrate that it significantly outperforms the state-of-the-art methods. Finally we explore the use of our method in continual learning and neural architecture search and report promising gains when limited memory and computations are available.



### Simple and effective localized attribute representations for zero-shot learning
- **Arxiv ID**: http://arxiv.org/abs/2006.05938v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.05938v3)
- **Published**: 2020-06-10 16:46:12+00:00
- **Updated**: 2021-03-09 09:44:15+00:00
- **Authors**: Shiqi Yang, Kai Wang, Luis Herranz, Joost van de Weijer
- **Comment**: A journal version of the paper is arXiv:arXiv:2103.04704
- **Journal**: None
- **Summary**: Zero-shot learning (ZSL) aims to discriminate images from unseen classes by exploiting relations to seen classes via their semantic descriptions. Some recent papers have shown the importance of localized features together with fine-tuning the feature extractor to obtain discriminative and transferable features. However, these methods require complex attention or part detection modules to perform explicit localization in the visual space. In contrast, in this paper we propose localizing representations in the semantic/attribute space, with a simple but effective pipeline where localization is implicit. Focusing on attribute representations, we show that our method obtains state-of-the-art performance on CUB and SUN datasets, and also achieves competitive results on AWA2 dataset, outperforming generally more complex methods with explicit localization in the visual space. Our method can be implemented easily, which can be used as a new baseline for zero shot-learning. In addition, our localized representations are highly interpretable as attribute-specific heatmaps.



### MultiResolution Attention Extractor for Small Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2006.05941v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.05941v1)
- **Published**: 2020-06-10 16:47:56+00:00
- **Updated**: 2020-06-10 16:47:56+00:00
- **Authors**: Fan Zhang, Licheng Jiao, Lingling Li, Fang Liu, Xu Liu
- **Comment**: 11 pages, 5 figures
- **Journal**: None
- **Summary**: Small objects are difficult to detect because of their low resolution and small size. The existing small object detection methods mainly focus on data preprocessing or narrowing the differences between large and small objects. Inspired by human vision "attention" mechanism, we exploit two feature extraction methods to mine the most useful information of small objects. Both methods are based on multiresolution feature extraction. We initially design and explore the soft attention method, but we find that its convergence speed is slow. Then we present the second method, an attention-based feature interaction method, called a MultiResolution Attention Extractor (MRAE), showing significant improvement as a generic feature extractor in small object detection. After each building block in the vanilla feature extractor, we append a small network to generate attention weights followed by a weighted-sum operation to get the final attention maps. Our attention-based feature extractor is 2.0 times the AP of the "hard" attention counterpart (plain architecture) on the COCO small object detection benchmark, proving that MRAE can capture useful location and contextual information through adaptive learning.



### Stochastic Segmentation Networks: Modelling Spatially Correlated Aleatoric Uncertainty
- **Arxiv ID**: http://arxiv.org/abs/2006.06015v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.06015v2)
- **Published**: 2020-06-10 18:06:41+00:00
- **Updated**: 2020-12-22 16:28:58+00:00
- **Authors**: Miguel Monteiro, Loïc Le Folgoc, Daniel Coelho de Castro, Nick Pawlowski, Bernardo Marques, Konstantinos Kamnitsas, Mark van der Wilk, Ben Glocker
- **Comment**: Published at Neurips2020. 17 pages, 11 figures, 2 tables
- **Journal**: None
- **Summary**: In image segmentation, there is often more than one plausible solution for a given input. In medical imaging, for example, experts will often disagree about the exact location of object boundaries. Estimating this inherent uncertainty and predicting multiple plausible hypotheses is of great interest in many applications, yet this ability is lacking in most current deep learning methods. In this paper, we introduce stochastic segmentation networks (SSNs), an efficient probabilistic method for modelling aleatoric uncertainty with any image segmentation network architecture. In contrast to approaches that produce pixel-wise estimates, SSNs model joint distributions over entire label maps and thus can generate multiple spatially coherent hypotheses for a single image. By using a low-rank multivariate normal distribution over the logit space to model the probability of the label map given the image, we obtain a spatially consistent probability distribution that can be efficiently computed by a neural network without any changes to the underlying architecture. We tested our method on the segmentation of real-world medical data, including lung nodules in 2D CT and brain tumours in 3D multimodal MRI scans. SSNs outperform state-of-the-art for modelling correlated uncertainty in ambiguous images while being much simpler, more flexible, and more efficient.



### Revisiting visual-inertial structure from motion for odometry and SLAM initialization
- **Arxiv ID**: http://arxiv.org/abs/2006.06017v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2006.06017v2)
- **Published**: 2020-06-10 18:08:22+00:00
- **Updated**: 2021-01-28 17:58:46+00:00
- **Authors**: Georgios Evangelidis, Branislav Micusik
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, an efficient closed-form solution for the state initialization in visual-inertial odometry (VIO) and simultaneous localization and mapping (SLAM) is presented. Unlike the state-of-the-art, we do not derive linear equations from triangulating pairs of point observations. Instead, we build on a direct triangulation of the unknown $3D$ point paired with each of its observations. We show and validate the high impact of such a simple difference. The resulting linear system has a simpler structure and the solution through analytic elimination only requires solving a $6\times 6$ linear system (or $9 \times 9$ when accelerometer bias is included). In addition, all the observations of every scene point are jointly related, thereby leading to a less biased and more robust solution. The proposed formulation attains up to $50$ percent decreased velocity and point reconstruction error compared to the standard closed-form solver, while it is $4\times$ faster for a $7$-frame set. Apart from the inherent efficiency, fewer iterations are needed by any further non-linear refinement thanks to better parameter initialization. In this context, we provide the analytic Jacobians for a non-linear optimizer that optionally refines the initial parameters. The superior performance of the proposed solver is established by quantitative comparisons with the state-of-the-art solver.



### Towards Robust Fine-grained Recognition by Maximal Separation of Discriminative Features
- **Arxiv ID**: http://arxiv.org/abs/2006.06028v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.06028v1)
- **Published**: 2020-06-10 18:34:45+00:00
- **Updated**: 2020-06-10 18:34:45+00:00
- **Authors**: Krishna Kanth Nakka, Mathieu Salzmann
- **Comment**: None
- **Journal**: None
- **Summary**: Adversarial attacks have been widely studied for general classification tasks, but remain unexplored in the context of fine-grained recognition, where the inter-class similarities facilitate the attacker's task. In this paper, we identify the proximity of the latent representations of different classes in fine-grained recognition networks as a key factor to the success of adversarial attacks. We therefore introduce an attention-based regularization mechanism that maximally separates the discriminative latent features of different classes while minimizing the contribution of the non-discriminative regions to the final class prediction. As evidenced by our experiments, this allows us to significantly improve robustness to adversarial attacks, to the point of matching or even surpassing that of adversarial training, but without requiring access to adversarial samples.



### Map3D: Registration Based Multi-Object Tracking on 3D Serial Whole Slide Images
- **Arxiv ID**: http://arxiv.org/abs/2006.06038v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.06038v2)
- **Published**: 2020-06-10 19:31:02+00:00
- **Updated**: 2021-03-25 19:28:44+00:00
- **Authors**: Ruining Deng, Haichun Yang, Aadarsh Jha, Yuzhe Lu, Peng Chu, Agnes B. Fogo, Yuankai Huo
- **Comment**: Accepted by IEEE Transactions on Medical Imaging
- **Journal**: None
- **Summary**: There has been a long pursuit for precise and reproducible glomerular quantification on renal pathology to leverage both research and practice. When digitizing the biopsy tissue samples using whole slide imaging (WSI), a set of serial sections from the same tissue can be acquired as a stack of images, similar to frames in a video. In radiology, the stack of images (e.g., computed tomography) are naturally used to provide 3D context for organs, tissues, and tumors. In pathology, it is appealing to do a similar 3D assessment. However, the 3D identification and association of large-scale glomeruli on renal pathology is challenging due to large tissue deformation, missing tissues, and artifacts from WSI. In this paper, we propose a novel Multi-object Association for Pathology in 3D (Map3D) method for automatically identifying and associating large-scale cross-sections of 3D objects from routine serial sectioning and WSI. The innovations of the Map3D method are three-fold: (1) the large-scale glomerular association is formed as a new multi-object tracking (MOT) perspective; (2) the quality-aware whole series registration is proposed to not only provide affinity estimation but also offer automatic kidney-wise quality assurance (QA) for registration; (3) a dual-path association method is proposed to tackle the large deformation, missing tissues, and artifacts during tracking. To the best of our knowledge, the Map3D method is the first approach that enables automatic and large-scale glomerular association across 3D serial sectioning using WSI. Our proposed method Map3D achieved MOTA= 44.6, which is 12.1% higher than the non deep learning benchmarks.



### Joint Training of Variational Auto-Encoder and Latent Energy-Based Model
- **Arxiv ID**: http://arxiv.org/abs/2006.06059v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.06059v1)
- **Published**: 2020-06-10 20:32:25+00:00
- **Updated**: 2020-06-10 20:32:25+00:00
- **Authors**: Tian Han, Erik Nijkamp, Linqi Zhou, Bo Pang, Song-Chun Zhu, Ying Nian Wu
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a joint training method to learn both the variational auto-encoder (VAE) and the latent energy-based model (EBM). The joint training of VAE and latent EBM are based on an objective function that consists of three Kullback-Leibler divergences between three joint distributions on the latent vector and the image, and the objective function is of an elegant symmetric and anti-symmetric form of divergence triangle that seamlessly integrates variational and adversarial learning. In this joint training scheme, the latent EBM serves as a critic of the generator model, while the generator model and the inference model in VAE serve as the approximate synthesis sampler and inference sampler of the latent EBM. Our experiments show that the joint training greatly improves the synthesis quality of the VAE. It also enables learning of an energy function that is capable of detecting out of sample examples for anomaly detection.



### Deterministic Gaussian Averaged Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2006.06061v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2006.06061v1)
- **Published**: 2020-06-10 20:53:31+00:00
- **Updated**: 2020-06-10 20:53:31+00:00
- **Authors**: Ryan Campbell, Chris Finlay, Adam M Oberman
- **Comment**: None
- **Journal**: None
- **Summary**: We present a deterministic method to compute the Gaussian average of neural networks used in regression and classification. Our method is based on an equivalence between training with a particular regularized loss, and the expected values of Gaussian averages. We use this equivalence to certify models which perform well on clean data but are not robust to adversarial perturbations. In terms of certified accuracy and adversarial robustness, our method is comparable to known stochastic methods such as randomized smoothing, but requires only a single model evaluation during inference.



### Fully Unsupervised Diversity Denoising with Convolutional Variational Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/2006.06072v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2006.06072v2)
- **Published**: 2020-06-10 21:28:13+00:00
- **Updated**: 2021-03-01 12:28:08+00:00
- **Authors**: Mangal Prakash, Alexander Krull, Florian Jug
- **Comment**: 44 pages including supplement
- **Journal**: None
- **Summary**: Deep Learning based methods have emerged as the indisputable leaders for virtually all image restoration tasks. Especially in the domain of microscopy images, various content-aware image restoration (CARE) approaches are now used to improve the interpretability of acquired data. Naturally, there are limitations to what can be restored in corrupted images, and like for all inverse problems, many potential solutions exist, and one of them must be chosen. Here, we propose DivNoising, a denoising approach based on fully convolutional variational autoencoders (VAEs), overcoming the problem of having to choose a single solution by predicting a whole distribution of denoised images. First we introduce a principled way of formulating the unsupervised denoising problem within the VAE framework by explicitly incorporating imaging noise models into the decoder. Our approach is fully unsupervised, only requiring noisy images and a suitable description of the imaging noise distribution. We show that such a noise model can either be measured, bootstrapped from noisy data, or co-learned during training. If desired, consensus predictions can be inferred from a set of DivNoising predictions, leading to competitive results with other unsupervised methods and, on occasion, even with the supervised state-of-the-art. DivNoising samples from the posterior enable a plethora of useful applications. We are (i) showing denoising results for 13 datasets, (ii) discussing how optical character recognition (OCR) applications can benefit from diverse predictions, and are (iii) demonstrating how instance cell segmentation improves when using diverse DivNoising predictions.



### Autonomous Driving with Deep Learning: A Survey of State-of-Art Technologies
- **Arxiv ID**: http://arxiv.org/abs/2006.06091v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2006.06091v3)
- **Published**: 2020-06-10 22:21:57+00:00
- **Updated**: 2020-07-04 04:38:43+00:00
- **Authors**: Yu Huang, Yue Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Since DARPA Grand Challenges (rural) in 2004/05 and Urban Challenges in 2007, autonomous driving has been the most active field of AI applications. Almost at the same time, deep learning has made breakthrough by several pioneers, three of them (also called fathers of deep learning), Hinton, Bengio and LeCun, won ACM Turin Award in 2019. This is a survey of autonomous driving technologies with deep learning methods. We investigate the major fields of self-driving systems, such as perception, mapping and localization, prediction, planning and control, simulation, V2X and safety etc. Due to the limited space, we focus the analysis on several key areas, i.e. 2D and 3D object detection in perception, depth estimation from cameras, multiple sensor fusion on the data, feature and task level respectively, behavior modelling and prediction of vehicle driving and pedestrian trajectories.



### Continual Learning for Affective Computing
- **Arxiv ID**: http://arxiv.org/abs/2006.06113v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2006.06113v2)
- **Published**: 2020-06-10 23:36:06+00:00
- **Updated**: 2020-11-12 06:42:26+00:00
- **Authors**: Nikhil Churamani
- **Comment**: Accepted at the Doctoral Consortium for the IEEE International
  Conference on Automatic Face and Gesture Recognition (FG), 2020
- **Journal**: None
- **Summary**: Real-world application requires affect perception models to be sensitive to individual differences in expression. As each user is different and expresses differently, these models need to personalise towards each individual to adequately capture their expressions and thus, model their affective state. Despite high performance on benchmarks, current approaches fall short in such adaptation. In this work, we propose the use of Continual Learning (CL) for affective computing as a paradigm for developing personalised affect perception.



