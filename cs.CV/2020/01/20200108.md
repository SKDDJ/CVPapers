# Arxiv Papers in cs.CV on 2020-01-08
### Perception and Navigation in Autonomous Systems in the Era of Learning: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2001.02319v4
- **DOI**: 10.1109/TNNLS.2022.3167688
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.02319v4)
- **Published**: 2020-01-08 00:28:12+00:00
- **Updated**: 2022-04-30 15:35:50+00:00
- **Authors**: Yang Tang, Chaoqiang Zhao, Jianrui Wang, Chongzhen Zhang, Qiyu Sun, Weixing Zheng, Wenli Du, Feng Qian, Juergen Kurths
- **Comment**: This paper has been accepted by IEEE TNNLS
- **Journal**: None
- **Summary**: Autonomous systems possess the features of inferring their own state, understanding their surroundings, and performing autonomous navigation. With the applications of learning systems, like deep learning and reinforcement learning, the visual-based self-state estimation, environment perception and navigation capabilities of autonomous systems have been efficiently addressed, and many new learning-based algorithms have surfaced with respect to autonomous visual perception and navigation. In this review, we focus on the applications of learning-based monocular approaches in ego-motion perception, environment perception and navigation in autonomous systems, which is different from previous reviews that discussed traditional methods. First, we delineate the shortcomings of existing classical visual simultaneous localization and mapping (vSLAM) solutions, which demonstrate the necessity to integrate deep learning techniques. Second, we review the visual-based environmental perception and understanding methods based on deep learning, including deep learning-based monocular depth estimation, monocular ego-motion prediction, image enhancement, object detection, semantic segmentation, and their combinations with traditional vSLAM frameworks. Then, we focus on the visual navigation based on learning systems, mainly including reinforcement learning and deep reinforcement learning. Finally, we examine several challenges and promising directions discussed and concluded in related research of learning systems in the era of computer science and robotics.



### VisionNet: A Drivable-space-based Interactive Motion Prediction Network for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2001.02354v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2001.02354v1)
- **Published**: 2020-01-08 03:29:53+00:00
- **Updated**: 2020-01-08 03:29:53+00:00
- **Authors**: Yanliang Zhu, Deheng Qian, Dongchun Ren, Huaxia Xia
- **Comment**: None
- **Journal**: None
- **Summary**: The comprehension of environmental traffic situation largely ensures the driving safety of autonomous vehicles. Recently, the mission has been investigated by plenty of researches, while it is hard to be well addressed due to the limitation of collective influence in complex scenarios. These approaches model the interactions through the spatial relations between the target obstacle and its neighbors. However, they oversimplify the challenge since the training stage of the interactions lacks effective supervision. As a result, these models are far from promising. More intuitively, we transform the problem into calculating the interaction-aware drivable spaces and propose the CNN-based VisionNet for trajectory prediction. The VisionNet accepts a sequence of motion states, i.e., location, velocity, and acceleration, to estimate the future drivable spaces. The reified interactions significantly increase the interpretation ability of the VisionNet and refine the prediction. To further advance the performance, we propose an interactive loss to guide the generation of the drivable spaces. Experiments on multiple public datasets demonstrate the effectiveness of the proposed VisionNet.



### Weakly Supervised Visual Semantic Parsing
- **Arxiv ID**: http://arxiv.org/abs/2001.02359v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.02359v2)
- **Published**: 2020-01-08 03:46:13+00:00
- **Updated**: 2020-03-31 18:54:06+00:00
- **Authors**: Alireza Zareian, Svebor Karaman, Shih-Fu Chang
- **Comment**: To be presented at CVPR 2020 (oral paper)
- **Journal**: None
- **Summary**: Scene Graph Generation (SGG) aims to extract entities, predicates and their semantic structure from images, enabling deep understanding of visual content, with many applications such as visual reasoning and image retrieval. Nevertheless, existing SGG methods require millions of manually annotated bounding boxes for training, and are computationally inefficient, as they exhaustively process all pairs of object proposals to detect predicates. In this paper, we address those two limitations by first proposing a generalized formulation of SGG, namely Visual Semantic Parsing, which disentangles entity and predicate recognition, and enables sub-quadratic performance. Then we propose the Visual Semantic Parsing Network, VSPNet, based on a dynamic, attention-based, bipartite message passing framework that jointly infers graph nodes and edges through an iterative process. Additionally, we propose the first graph-based weakly supervised learning framework, based on a novel graph alignment algorithm, which enables training without bounding box annotations. Through extensive experiments, we show that VSPNet outperforms weakly supervised baselines significantly and approaches fully supervised performance, while being several times faster. We publicly release the source code of our method.



### Learning to Move with Affordance Maps
- **Arxiv ID**: http://arxiv.org/abs/2001.02364v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2001.02364v2)
- **Published**: 2020-01-08 04:05:11+00:00
- **Updated**: 2020-02-14 19:01:26+00:00
- **Authors**: William Qi, Ravi Teja Mullapudi, Saurabh Gupta, Deva Ramanan
- **Comment**: Published at ICLR 2020. For code, see https://github.com/wqi/A2L
- **Journal**: None
- **Summary**: The ability to autonomously explore and navigate a physical space is a fundamental requirement for virtually any mobile autonomous agent, from household robotic vacuums to autonomous vehicles. Traditional SLAM-based approaches for exploration and navigation largely focus on leveraging scene geometry, but fail to model dynamic objects (such as other agents) or semantic constraints (such as wet floors or doorways). Learning-based RL agents are an attractive alternative because they can incorporate both semantic and geometric information, but are notoriously sample inefficient, difficult to generalize to novel settings, and are difficult to interpret. In this paper, we combine the best of both worlds with a modular approach that learns a spatial representation of a scene that is trained to be effective when coupled with traditional geometric planners. Specifically, we design an agent that learns to predict a spatial affordance map that elucidates what parts of a scene are navigable through active self-supervised experience gathering. In contrast to most simulation environments that assume a static world, we evaluate our approach in the VizDoom simulator, using large-scale randomly-generated maps containing a variety of dynamic actors and hazards. We show that learned affordance maps can be used to augment traditional approaches for both exploration and navigation, providing significant improvements in performance.



### What can robotics research learn from computer vision research?
- **Arxiv ID**: http://arxiv.org/abs/2001.02366v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2001.02366v2)
- **Published**: 2020-01-08 04:32:10+00:00
- **Updated**: 2020-06-12 01:04:07+00:00
- **Authors**: Peter Corke, Feras Dayoub, David Hall, John Skinner, Niko SÃ¼nderhauf
- **Comment**: 15 pages, to appear in the proceeding of the International Symposium
  on Robotics Research (ISRR) 2019
- **Journal**: None
- **Summary**: The computer vision and robotics research communities are each strong. However progress in computer vision has become turbo-charged in recent years due to big data, GPU computing, novel learning algorithms and a very effective research methodology. By comparison, progress in robotics seems slower. It is true that robotics came later to exploring the potential of learning -- the advantages over the well-established body of knowledge in dynamics, kinematics, planning and control is still being debated, although reinforcement learning seems to offer real potential. However, the rapid development of computer vision compared to robotics cannot be only attributed to the former's adoption of deep learning. In this paper, we argue that the gains in computer vision are due to research methodology -- evaluation under strict constraints versus experiments; bold numbers versus videos.



### Learning to Zoom-in via Learning to Zoom-out: Real-world Super-resolution by Generating and Adapting Degradation
- **Arxiv ID**: http://arxiv.org/abs/2001.02381v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2001.02381v1)
- **Published**: 2020-01-08 05:17:02+00:00
- **Updated**: 2020-01-08 05:17:02+00:00
- **Authors**: Dong Gong, Wei Sun, Qinfeng Shi, Anton van den Hengel, Yanning Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Most learning-based super-resolution (SR) methods aim to recover high-resolution (HR) image from a given low-resolution (LR) image via learning on LR-HR image pairs. The SR methods learned on synthetic data do not perform well in real-world, due to the domain gap between the artificially synthesized and real LR images. Some efforts are thus taken to capture real-world image pairs. The captured LR-HR image pairs usually suffer from unavoidable misalignment, which hampers the performance of end-to-end learning, however. Here, focusing on the real-world SR, we ask a different question: since misalignment is unavoidable, can we propose a method that does not need LR-HR image pairing and alignment at all and utilize real images as they are? Hence we propose a framework to learn SR from an arbitrary set of unpaired LR and HR images and see how far a step can go in such a realistic and "unsupervised" setting. To do so, we firstly train a degradation generation network to generate realistic LR images and, more importantly, to capture their distribution (i.e., learning to zoom out). Instead of assuming the domain gap has been eliminated, we minimize the discrepancy between the generated data and real data while learning a degradation adaptive SR network (i.e., learning to zoom in). The proposed unpaired method achieves state-of-the-art SR results on real-world images, even in the datasets that favor the paired-learning methods more.



### Hypergraph Spectral Analysis and Processing in 3D Point Cloud
- **Arxiv ID**: http://arxiv.org/abs/2001.02384v1
- **DOI**: 10.1109/TIP.2020.3042088
- **Categories**: **eess.SP**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2001.02384v1)
- **Published**: 2020-01-08 05:30:16+00:00
- **Updated**: 2020-01-08 05:30:16+00:00
- **Authors**: Songyang Zhang, Shuguang Cui, Zhi Ding
- **Comment**: None
- **Journal**: None
- **Summary**: Along with increasingly popular virtual reality applications, the three-dimensional (3D) point cloud has become a fundamental data structure to characterize 3D objects and surroundings. To process 3D point clouds efficiently, a suitable model for the underlying structure and outlier noises is always critical. In this work, we propose a hypergraph-based new point cloud model that is amenable to efficient analysis and processing. We introduce tensor-based methods to estimate hypergraph spectrum components and frequency coefficients of point clouds in both ideal and noisy settings. We establish an analytical connection between hypergraph frequencies and structural features. We further evaluate the efficacy of hypergraph spectrum estimation in two common point cloud applications of sampling and denoising for which also we elaborate specific hypergraph filter design and spectral properties. The empirical performance demonstrates the strength of hypergraph signal processing as a tool in 3D point clouds and the underlying properties.



### A context based deep learning approach for unbalanced medical image segmentation
- **Arxiv ID**: http://arxiv.org/abs/2001.02387v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2001.02387v1)
- **Published**: 2020-01-08 05:46:02+00:00
- **Updated**: 2020-01-08 05:46:02+00:00
- **Authors**: Balamurali Murugesan, Kaushik Sarveswaran, Vijaya Raghavan S, Sharath M Shankaranarayana, Keerthi Ram, Mohanasankar Sivaprakasam
- **Comment**: Accepted in ISBI 2020
- **Journal**: None
- **Summary**: Automated medical image segmentation is an important step in many medical procedures. Recently, deep learning networks have been widely used for various medical image segmentation tasks, with U-Net and generative adversarial nets (GANs) being some of the commonly used ones. Foreground-background class imbalance is a common occurrence in medical images, and U-Net has difficulty in handling class imbalance because of its cross entropy (CE) objective function. Similarly, GAN also suffers from class imbalance because the discriminator looks at the entire image to classify it as real or fake. Since the discriminator is essentially a deep learning classifier, it is incapable of correctly identifying minor changes in small structures. To address these issues, we propose a novel context based CE loss function for U-Net, and a novel architecture Seg-GLGAN. The context based CE is a linear combination of CE obtained over the entire image and its region of interest (ROI). In Seg-GLGAN, we introduce a novel context discriminator to which the entire image and its ROI are fed as input, thus enforcing local context. We conduct extensive experiments using two challenging unbalanced datasets: PROMISE12 and ACDC. We observe that segmentation results obtained from our methods give better segmentation metrics as compared to various baseline methods.



### Training Progressively Binarizing Deep Networks Using FPGAs
- **Arxiv ID**: http://arxiv.org/abs/2001.02390v1
- **DOI**: 10.1109/ISCAS45731.2020.9181099
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2001.02390v1)
- **Published**: 2020-01-08 06:01:13+00:00
- **Updated**: 2020-01-08 06:01:13+00:00
- **Authors**: Corey Lammie, Wei Xiang, Mostafa Rahimi Azghadi
- **Comment**: Accepted at 2020 IEEE International Symposium on Circuits and Systems
  (ISCAS)
- **Journal**: 2020 IEEE International Symposium on Circuits and Systems (ISCAS)
- **Summary**: While hardware implementations of inference routines for Binarized Neural Networks (BNNs) are plentiful, current realizations of efficient BNN hardware training accelerators, suitable for Internet of Things (IoT) edge devices, leave much to be desired. Conventional BNN hardware training accelerators perform forward and backward propagations with parameters adopting binary representations, and optimization using parameters adopting floating or fixed-point real-valued representations--requiring two distinct sets of network parameters. In this paper, we propose a hardware-friendly training method that, contrary to conventional methods, progressively binarizes a singular set of fixed-point network parameters, yielding notable reductions in power and resource utilizations. We use the Intel FPGA SDK for OpenCL development environment to train our progressively binarizing DNNs on an OpenVINO FPGA. We benchmark our training approach on both GPUs and FPGAs using CIFAR-10 and compare it to conventional BNNs.



### Convolutional Networks with Dense Connectivity
- **Arxiv ID**: http://arxiv.org/abs/2001.02394v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2001.02394v1)
- **Published**: 2020-01-08 06:54:53+00:00
- **Updated**: 2020-01-08 06:54:53+00:00
- **Authors**: Gao Huang, Zhuang Liu, Geoff Pleiss, Laurens van der Maaten, Kilian Q. Weinberger
- **Comment**: Journal(PAMI) version of DenseNet(CVPR'17)
- **Journal**: None
- **Summary**: Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion.Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, encourage feature reuse and substantially improve parameter efficiency. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less parameters and computation to achieve high performance.



### DC-WCNN: A deep cascade of wavelet based convolutional neural networks for MR Image Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2001.02397v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2001.02397v1)
- **Published**: 2020-01-08 07:04:22+00:00
- **Updated**: 2020-01-08 07:04:22+00:00
- **Authors**: Sriprabha Ramanarayanan, Balamurali Murugesan, Keerthi Ram, Mohanasankar Sivaprakasam
- **Comment**: Accepted in ISBI 2020
- **Journal**: None
- **Summary**: Several variants of Convolutional Neural Networks (CNN) have been developed for Magnetic Resonance (MR) image reconstruction. Among them, U-Net has shown to be the baseline architecture for MR image reconstruction. However, sub-sampling is performed by its pooling layers, causing information loss which in turn leads to blur and missing fine details in the reconstructed image. We propose a modification to the U-Net architecture to recover fine structures. The proposed network is a wavelet packet transform based encoder-decoder CNN with residual learning called CNN. The proposed WCNN has discrete wavelet transform instead of pooling and inverse wavelet transform instead of unpooling layers and residual connections. We also propose a deep cascaded framework (DC-WCNN) which consists of cascades of WCNN and k-space data fidelity units to achieve high quality MR reconstruction. Experimental results show that WCNN and DC-WCNN give promising results in terms of evaluation metrics and better recovery of fine details as compared to other methods.



### SPACE: Unsupervised Object-Oriented Scene Representation via Spatial Attention and Decomposition
- **Arxiv ID**: http://arxiv.org/abs/2001.02407v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2001.02407v3)
- **Published**: 2020-01-08 07:44:32+00:00
- **Updated**: 2020-03-15 20:21:38+00:00
- **Authors**: Zhixuan Lin, Yi-Fu Wu, Skand Vishwanath Peri, Weihao Sun, Gautam Singh, Fei Deng, Jindong Jiang, Sungjin Ahn
- **Comment**: Accepted in ICLR 2020
- **Journal**: None
- **Summary**: The ability to decompose complex multi-object scenes into meaningful abstractions like objects is fundamental to achieve higher-level cognition. Previous approaches for unsupervised object-oriented scene representation learning are either based on spatial-attention or scene-mixture approaches and limited in scalability which is a main obstacle towards modeling real-world scenes. In this paper, we propose a generative latent variable model, called SPACE, that provides a unified probabilistic modeling framework that combines the best of spatial-attention and scene-mixture approaches. SPACE can explicitly provide factorized object representations for foreground objects while also decomposing background segments of complex morphology. Previous models are good at either of these, but not both. SPACE also resolves the scalability problems of previous methods by incorporating parallel spatial-attention and thus is applicable to scenes with a large number of objects without performance degradations. We show through experiments on Atari and 3D-Rooms that SPACE achieves the above properties consistently in comparison to SPAIR, IODINE, and GENESIS. Results of our experiments can be found on our project website: https://sites.google.com/view/space-project-page



### Disentangling Multiple Features in Video Sequences using Gaussian Processes in Variational Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/2001.02408v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.02408v3)
- **Published**: 2020-01-08 08:08:01+00:00
- **Updated**: 2020-07-19 14:31:01+00:00
- **Authors**: Sarthak Bhagat, Shagun Uppal, Zhuyun Yin, Nengli Lim
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce MGP-VAE (Multi-disentangled-features Gaussian Processes Variational AutoEncoder), a variational autoencoder which uses Gaussian processes (GP) to model the latent space for the unsupervised learning of disentangled representations in video sequences. We improve upon previous work by establishing a framework by which multiple features, static or dynamic, can be disentangled. Specifically we use fractional Brownian motions (fBM) and Brownian bridges (BB) to enforce an inter-frame correlation structure in each independent channel, and show that varying this structure enables one to capture different factors of variation in the data. We demonstrate the quality of our representations with experiments on three publicly available datasets, and also quantify the improvement using a video prediction task. Moreover, we introduce a novel geodesic loss function which takes into account the curvature of the data manifold to improve learning. Our experiments show that the combination of the improved representations with the novel loss function enable MGP-VAE to outperform the baselines in video prediction.



### Spinal Metastases Segmentation in MR Imaging using Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2001.05834v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2001.05834v2)
- **Published**: 2020-01-08 10:59:31+00:00
- **Updated**: 2020-01-28 10:21:08+00:00
- **Authors**: Georg Hille, Johannes Steffen, Max DÃ¼nnwald, Mathias Becker, Sylvia Saalfeld, Klaus TÃ¶nnies
- **Comment**: 9 pages, 5 figures
- **Journal**: None
- **Summary**: This study's objective was to segment spinal metastases in diagnostic MR images using a deep learning-based approach. Segmentation of such lesions can present a pivotal step towards enhanced therapy planning and validation, as well as intervention support during minimally invasive and image-guided surgeries like radiofrequency ablations. For this purpose, we used a U-Net like architecture trained with 40 clinical cases including both, lytic and sclerotic lesion types and various MR sequences. Our proposed method was evaluated with regards to various factors influencing the segmentation quality, e.g. the used MR sequences and the input dimension. We quantitatively assessed our experiments using Dice coefficients, sensitivity and specificity rates. Compared to expertly annotated lesion segmentations, the experiments yielded promising results with average Dice scores up to 77.6% and mean sensitivity rates up to 78.9%. To our best knowledge, our proposed study is one of the first to tackle this particular issue, which limits direct comparability with related works. In respect to similar deep learning-based lesion segmentations, e.g. in liver MR images or spinal CT images, our experiments showed similar or in some respects superior segmentation quality. Overall, our automatic approach can provide almost expert-like segmentation accuracy in this challenging and ambitious task.



### Table Structure Extraction with Bi-directional Gated Recurrent Unit Networks
- **Arxiv ID**: http://arxiv.org/abs/2001.02501v1
- **DOI**: 10.1109/ICDAR.2019.00220
- **Categories**: **cs.CV**, cs.IT, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/2001.02501v1)
- **Published**: 2020-01-08 13:17:44+00:00
- **Updated**: 2020-01-08 13:17:44+00:00
- **Authors**: Saqib Ali Khan, Syed Muhammad Daniyal Khalid, Muhammad Ali Shahzad, Faisal Shafait
- **Comment**: Proceedings of the 15th International Conference on Document Analysis
  and Recognition (ICDAR) 2019, Sydney, Australia
- **Journal**: None
- **Summary**: Tables present summarized and structured information to the reader, which makes table structure extraction an important part of document understanding applications. However, table structure identification is a hard problem not only because of the large variation in the table layouts and styles, but also owing to the variations in the page layouts and the noise contamination levels. A lot of research has been done to identify table structure, most of which is based on applying heuristics with the aid of optical character recognition (OCR) to hand pick layout features of the tables. These methods fail to generalize well because of the variations in the table layouts and the errors generated by OCR. In this paper, we have proposed a robust deep learning based approach to extract rows and columns from a detected table in document images with a high precision. In the proposed solution, the table images are first pre-processed and then fed to a bi-directional Recurrent Neural Network with Gated Recurrent Units (GRU) followed by a fully-connected layer with soft max activation. The network scans the images from top-to-bottom as well as left-to-right and classifies each input as either a row-separator or a column-separator. We have benchmarked our system on publicly available UNLV as well as ICDAR 2013 datasets on which it outperformed the state-of-the-art table structure extraction systems by a significant margin.



### Deep OCT Angiography Image Generation for Motion Artifact Suppression
- **Arxiv ID**: http://arxiv.org/abs/2001.02512v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2001.02512v1)
- **Published**: 2020-01-08 13:31:51+00:00
- **Updated**: 2020-01-08 13:31:51+00:00
- **Authors**: Julian Hossbach, Lennart Husvogt, Martin F. Kraus, James G. Fujimoto, Andreas K. Maier
- **Comment**: Accepted at BVM 2020
- **Journal**: None
- **Summary**: Eye movements, blinking and other motion during the acquisition of optical coherence tomography (OCT) can lead to artifacts, when processed to OCT angiography (OCTA) images. Affected scans emerge as high intensity (white) or missing (black) regions, resulting in lost information. The aim of this research is to fill these gaps using a deep generative model for OCT to OCTA image translation relying on a single intact OCT scan. Therefore, a U-Net is trained to extract the angiographic information from OCT patches. At inference, a detection algorithm finds outlier OCTA scans based on their surroundings, which are then replaced by the trained network. We show that generative models can augment the missing scans. The augmented volumes could then be used for 3-D segmentation or increase the diagnostic value.



### Fast Neural Network Adaptation via Parameter Remapping and Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/2001.02525v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.02525v2)
- **Published**: 2020-01-08 13:45:15+00:00
- **Updated**: 2020-04-09 08:45:48+00:00
- **Authors**: Jiemin Fang, Yuzhu Sun, Kangjian Peng, Qian Zhang, Yuan Li, Wenyu Liu, Xinggang Wang
- **Comment**: Accepted by ICLR 2020
- **Journal**: None
- **Summary**: Deep neural networks achieve remarkable performance in many computer vision tasks. Most state-of-the-art (SOTA) semantic segmentation and object detection approaches reuse neural network architectures designed for image classification as the backbone, commonly pre-trained on ImageNet. However, performance gains can be achieved by designing network architectures specifically for detection and segmentation, as shown by recent neural architecture search (NAS) research for detection and segmentation. One major challenge though, is that ImageNet pre-training of the search space representation (a.k.a. super network) or the searched networks incurs huge computational cost. In this paper, we propose a Fast Neural Network Adaptation (FNA) method, which can adapt both the architecture and parameters of a seed network (e.g. a high performing manually designed backbone) to become a network with different depth, width, or kernels via a Parameter Remapping technique, making it possible to utilize NAS for detection/segmentation tasks a lot more efficiently. In our experiments, we conduct FNA on MobileNetV2 to obtain new networks for both segmentation and detection that clearly out-perform existing networks designed both manually and by NAS. The total computation cost of FNA is significantly less than SOTA segmentation/detection NAS approaches: 1737$\times$ less than DPC, 6.8$\times$ less than Auto-DeepLab and 7.4$\times$ less than DetNAS. The code is available at https://github.com/JaminFong/FNA.



### An Analysis of Object Representations in Deep Visual Trackers
- **Arxiv ID**: http://arxiv.org/abs/2001.02593v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.02593v1)
- **Published**: 2020-01-08 16:03:57+00:00
- **Updated**: 2020-01-08 16:03:57+00:00
- **Authors**: Ross Goroshin, Jonathan Tompson, Debidatta Dwibedi
- **Comment**: None
- **Journal**: None
- **Summary**: Fully convolutional deep correlation networks are integral components of state-of the-art approaches to single object visual tracking. It is commonly assumed that these networks perform tracking by detection by matching features of the object instance with features of the entire frame. Strong architectural priors and conditioning on the object representation is thought to encourage this tracking strategy. Despite these strong priors, we show that deep trackers often default to tracking by saliency detection - without relying on the object instance representation. Our analysis shows that despite being a useful prior, salience detection can prevent the emergence of more robust tracking strategies in deep networks. This leads us to introduce an auxiliary detection task that encourages more discriminative object representations that improve tracking performance.



### Deep Learning for Free-Hand Sketch: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2001.02600v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2001.02600v3)
- **Published**: 2020-01-08 16:23:56+00:00
- **Updated**: 2022-02-01 17:23:14+00:00
- **Authors**: Peng Xu, Timothy M. Hospedales, Qiyue Yin, Yi-Zhe Song, Tao Xiang, Liang Wang
- **Comment**: This paper is accepted by IEEE TPAMI
- **Journal**: None
- **Summary**: Free-hand sketches are highly illustrative, and have been widely used by humans to depict objects or stories from ancient times to the present. The recent prevalence of touchscreen devices has made sketch creation a much easier task than ever and consequently made sketch-oriented applications increasingly popular. The progress of deep learning has immensely benefited free-hand sketch research and applications. This paper presents a comprehensive survey of the deep learning techniques oriented at free-hand sketch data, and the applications that they enable. The main contents of this survey include: (i) A discussion of the intrinsic traits and unique challenges of free-hand sketch, to highlight the essential differences between sketch data and other data modalities, e.g., natural photos. (ii) A review of the developments of free-hand sketch research in the deep learning era, by surveying existing datasets, research topics, and the state-of-the-art methods through a detailed taxonomy and experimental evaluation. (iii) Promotion of future work via a discussion of bottlenecks, open problems, and potential research directions for the community.



### Do As I Do: Transferring Human Motion and Appearance between Monocular Videos with Spatial and Temporal Constraints
- **Arxiv ID**: http://arxiv.org/abs/2001.02606v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.02606v2)
- **Published**: 2020-01-08 16:39:16+00:00
- **Updated**: 2020-01-21 17:26:48+00:00
- **Authors**: Thiago L. Gomes, Renato Martins, JoÃ£o Ferreira, Erickson R. Nascimento
- **Comment**: 10 pages, 8 figures, to appear in Proceedings of the IEEE Winter
  Conference on Applications of Computer Vision (WACV) 2020
- **Journal**: None
- **Summary**: Creating plausible virtual actors from images of real actors remains one of the key challenges in computer vision and computer graphics. Marker-less human motion estimation and shape modeling from images in the wild bring this challenge to the fore. Although the recent advances on view synthesis and image-to-image translation, currently available formulations are limited to transfer solely style and do not take into account the character's motion and shape, which are by nature intermingled to produce plausible human forms. In this paper, we propose a unifying formulation for transferring appearance and retargeting human motion from monocular videos that regards all these aspects. Our method synthesizes new videos of people in a different context where they were initially recorded. Differently from recent appearance transferring methods, our approach takes into account body shape, appearance, and motion constraints. The evaluation is performed with several experiments using publicly available real videos containing hard conditions. Our method is able to transfer both human motion and appearance outperforming state-of-the-art methods, while preserving specific features of the motion that must be maintained (e.g., feet touching the floor, hands touching a particular object) and holding the best visual quality and appearance metrics such as Structural Similarity (SSIM) and Learned Perceptual Image Patch Similarity (LPIPS).



### Don't Forget The Past: Recurrent Depth Estimation from Monocular Video
- **Arxiv ID**: http://arxiv.org/abs/2001.02613v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2001.02613v2)
- **Published**: 2020-01-08 16:50:51+00:00
- **Updated**: 2020-07-28 10:27:13+00:00
- **Authors**: Vaishakh Patil, Wouter Van Gansbeke, Dengxin Dai, Luc Van Gool
- **Comment**: Please refer to our webpage for details
  https://www.trace.ethz.ch/publications/2020/rec_depth_estimation/
- **Journal**: None
- **Summary**: Autonomous cars need continuously updated depth information. Thus far, depth is mostly estimated independently for a single frame at a time, even if the method starts from video input. Our method produces a time series of depth maps, which makes it an ideal candidate for online learning approaches. In particular, we put three different types of depth estimation (supervised depth prediction, self-supervised depth prediction, and self-supervised depth completion) into a common framework. We integrate the corresponding networks with a ConvLSTM such that the spatiotemporal structures of depth across frames can be exploited to yield a more accurate depth estimation. Our method is flexible. It can be applied to monocular videos only or be combined with different types of sparse depth patterns. We carefully study the architecture of the recurrent network and its training strategy. We are first to successfully exploit recurrent networks for real-time self-supervised monocular depth estimation and completion. Extensive experiments show that our recurrent method outperforms its image-based counterpart consistently and significantly in both self-supervised scenarios. It also outperforms previous depth estimation methods of the three popular groups. Please refer to https://www.trace.ethz.ch/publications/2020/rec_depth_estimation/ for details.



### CONSAC: Robust Multi-Model Fitting by Conditional Sample Consensus
- **Arxiv ID**: http://arxiv.org/abs/2001.02643v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.02643v3)
- **Published**: 2020-01-08 17:37:01+00:00
- **Updated**: 2020-03-25 11:48:39+00:00
- **Authors**: Florian Kluger, Eric Brachmann, Hanno Ackermann, Carsten Rother, Michael Ying Yang, Bodo Rosenhahn
- **Comment**: CVPR 2020
- **Journal**: None
- **Summary**: We present a robust estimator for fitting multiple parametric models of the same form to noisy measurements. Applications include finding multiple vanishing points in man-made scenes, fitting planes to architectural imagery, or estimating multiple rigid motions within the same sequence. In contrast to previous works, which resorted to hand-crafted search strategies for multiple model detection, we learn the search strategy from data. A neural network conditioned on previously detected models guides a RANSAC estimator to different subsets of all measurements, thereby finding model instances one after another. We train our method supervised as well as self-supervised. For supervised training of the search strategy, we contribute a new dataset for vanishing point estimation. Leveraging this dataset, the proposed algorithm is superior with respect to other robust estimators as well as to designated vanishing point estimation algorithms. For self-supervised learning of the search, we evaluate the proposed algorithm on multi-homography estimation and demonstrate an accuracy that is superior to state-of-the-art methods.



### Nonparametric Continuous Sensor Registration
- **Arxiv ID**: http://arxiv.org/abs/2001.04286v4
- **DOI**: None
- **Categories**: **math.OC**, cs.CV, cs.LG, cs.RO, 53B21, 46C05, 46C07, 68T40, 68T45, 93C85, I.2.9; I.2.10; I.4.10; G.1.6; G.4
- **Links**: [PDF](http://arxiv.org/pdf/2001.04286v4)
- **Published**: 2020-01-08 17:40:54+00:00
- **Updated**: 2021-10-18 16:41:16+00:00
- **Authors**: William Clark, Maani Ghaffari, Anthony Bloch
- **Comment**: 50 pages. Accepted for Journal of Machine Learning Research. arXiv
  admin note: text overlap with arXiv:1904.02266
- **Journal**: None
- **Summary**: This paper develops a new mathematical framework that enables nonparametric joint semantic and geometric representation of continuous functions using data. The joint embedding is modeled by representing the processes in a reproducing kernel Hilbert space. The functions can be defined on arbitrary smooth manifolds where the action of a Lie group aligns them. The continuous functions allow the registration to be independent of a specific signal resolution. The framework is fully analytical with a closed-form derivation of the Riemannian gradient and Hessian. We study a more specialized but widely used case where the Lie group acts on functions isometrically. We solve the problem by maximizing the inner product between two functions defined over data, while the continuous action of the rigid body motion Lie group is captured through the integration of the flow in the corresponding Lie algebra. Low-dimensional cases are derived with numerical examples to show the generality of the proposed framework. The high-dimensional derivation for the special Euclidean group acting on the Euclidean space showcases the point cloud registration and bird's-eye view map registration abilities. An implementation of this framework for RGB-D cameras outperforms the state-of-the-art robust visual odometry and performs well in texture and structure-scarce environments.



### Distributionally Robust Deep Learning using Hardness Weighted Sampling
- **Arxiv ID**: http://arxiv.org/abs/2001.02658v4
- **DOI**: 10.59275/j.melba.2022-8b6a
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2001.02658v4)
- **Published**: 2020-01-08 18:02:56+00:00
- **Updated**: 2022-07-14 22:03:25+00:00
- **Authors**: Lucas Fidon, Michael Aertsen, Thomas Deprest, Doaa Emam, FrÃ©dÃ©ric Guffens, Nada Mufti, Esther Van Elslander, Ernst Schwartz, Michael Ebner, Daniela Prayer, Gregor Kasprian, Anna L. David, Andrew Melbourne, SÃ©bastien Ourselin, Jan Deprest, Georg Langs, Tom Vercauteren
- **Comment**: Accepted for publication at the Journal of Machine Learning for
  Biomedical Imaging (MELBA) https://www.melba-journal.org/papers/2022:019.html
- **Journal**: https://www.melba-journal.org/papers/2022:019.html
- **Summary**: Limiting failures of machine learning systems is of paramount importance for safety-critical applications. In order to improve the robustness of machine learning systems, Distributionally Robust Optimization (DRO) has been proposed as a generalization of Empirical Risk Minimization (ERM). However, its use in deep learning has been severely restricted due to the relative inefficiency of the optimizers available for DRO in comparison to the wide-spread variants of Stochastic Gradient Descent (SGD) optimizers for ERM. We propose SGD with hardness weighted sampling, a principled and efficient optimization method for DRO in machine learning that is particularly suited in the context of deep learning. Similar to a hard example mining strategy in practice, the proposed algorithm is straightforward to implement and computationally as efficient as SGD-based optimizers used for deep learning, requiring minimal overhead computation. In contrast to typical ad hoc hard mining approaches, we prove the convergence of our DRO algorithm for over-parameterized deep learning networks with ReLU activation and a finite number of layers and parameters. Our experiments on fetal brain 3D MRI segmentation and brain tumor segmentation in MRI demonstrate the feasibility and the usefulness of our approach. Using our hardness weighted sampling for training a state-of-the-art deep learning pipeline leads to improved robustness to anatomical variabilities in automatic fetal brain 3D MRI segmentation using deep learning and to improved robustness to the image protocol variations in brain tumor segmentation. Our code is available at https://github.com/LucasFidon/HardnessWeightedSampler.



### Learning Generative Models using Denoising Density Estimators
- **Arxiv ID**: http://arxiv.org/abs/2001.02728v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2001.02728v2)
- **Published**: 2020-01-08 20:30:40+00:00
- **Updated**: 2020-06-09 21:26:44+00:00
- **Authors**: Siavash A. Bigdeli, Geng Lin, Tiziano Portenier, L. Andrea Dunbar, Matthias Zwicker
- **Comment**: Code and models available at
  https://drive.google.com/file/d/1EzKRxnFG1Hd8g6Ggvt-jvKkgpDDwK2bY
- **Journal**: None
- **Summary**: Learning probabilistic models that can estimate the density of a given set of samples, and generate samples from that density, is one of the fundamental challenges in unsupervised machine learning. We introduce a new generative model based on denoising density estimators (DDEs), which are scalar functions parameterized by neural networks, that are efficiently trained to represent kernel density estimators of the data. Leveraging DDEs, our main contribution is a novel technique to obtain generative models by minimizing the KL-divergence directly. We prove that our algorithm for obtaining generative models is guaranteed to converge to the correct solution. Our approach does not require specific network architecture as in normalizing flows, nor use ordinary differential equation solvers as in continuous normalizing flows. Experimental results demonstrate substantial improvement in density estimation and competitive performance in generative model training.



### The Effect of Data Ordering in Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2001.05857v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2001.05857v1)
- **Published**: 2020-01-08 20:34:00+00:00
- **Updated**: 2020-01-08 20:34:00+00:00
- **Authors**: Ethem F. Can, Aysu Ezen-Can
- **Comment**: None
- **Journal**: Under consideration at Pattern Recognition Letters 2020
- **Summary**: The success stories from deep learning models increase every day spanning different tasks from image classification to natural language understanding. With the increasing popularity of these models, scientists spend more and more time finding the optimal parameters and best model architectures for their tasks. In this paper, we focus on the ingredient that feeds these machines: the data. We hypothesize that the data ordering affects how well a model performs. To that end, we conduct experiments on an image classification task using ImageNet dataset and show that some data orderings are better than others in terms of obtaining higher classification accuracies. Experimental results show that independent of model architecture, learning rate and batch size, ordering of the data significantly affects the outcome. We show these findings using different metrics: NDCG, accuracy @ 1 and accuracy @ 5. Our goal here is to show that not only parameters and model architectures but also the data ordering has a say in obtaining better results.



### Explainable Deep Convolutional Candlestick Learner
- **Arxiv ID**: http://arxiv.org/abs/2001.02767v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2001.02767v4)
- **Published**: 2020-01-08 22:11:13+00:00
- **Updated**: 2020-05-29 06:04:54+00:00
- **Authors**: Jun-Hao Chen, Samuel Yen-Chi Chen, Yun-Cheng Tsai, Chih-Shiang Shur
- **Comment**: Accepted by The 32nd International Conference on Software Engineering
  & Knowledge Engineering (SEKE 2020), KSIR Virtual Conference Cener,
  Pittsburgh, USA, July 9--July 19, 2020
- **Journal**: None
- **Summary**: Candlesticks are graphical representations of price movements for a given period. The traders can discovery the trend of the asset by looking at the candlestick patterns. Although deep convolutional neural networks have achieved great success for recognizing the candlestick patterns, their reasoning hides inside a black box. The traders cannot make sure what the model has learned. In this contribution, we provide a framework which is to explain the reasoning of the learned model determining the specific candlestick patterns of time series. Based on the local search adversarial attacks, we show that the learned model perceives the pattern of the candlesticks in a way similar to the human trader.



### Transferability of Adversarial Examples to Attack Cloud-based Image Classifier Service
- **Arxiv ID**: http://arxiv.org/abs/2001.03460v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2001.03460v3)
- **Published**: 2020-01-08 23:03:35+00:00
- **Updated**: 2020-01-20 02:07:37+00:00
- **Authors**: Dou Goodman
- **Comment**: Accepted by Defcon China 2019. arXiv admin note: substantial text
  overlap with arXiv:1906.07997
- **Journal**: None
- **Summary**: In recent years, Deep Learning(DL) techniques have been extensively deployed for computer vision tasks, particularly visual classification problems, where new algorithms reported to achieve or even surpass the human performance. While many recent works demonstrated that DL models are vulnerable to adversarial examples. Fortunately, generating adversarial examples usually requires white-box access to the victim model, and real-world cloud-based image classification services are more complex than white-box classifier,the architecture and parameters of DL models on cloud platforms cannot be obtained by the attacker. The attacker can only access the APIs opened by cloud platforms. Thus, keeping models in the cloud can usually give a (false) sense of security. In this paper, we mainly focus on studying the security of real-world cloud-based image classification services. Specifically, (1) We propose a novel attack method, Fast Featuremap Loss PGD (FFL-PGD) attack based on Substitution model, which achieves a high bypass rate with a very limited number of queries. Instead of millions of queries in previous studies, our method finds the adversarial examples using only two queries per image; and (2) we make the first attempt to conduct an extensive empirical study of black-box attacks against real-world cloud-based classification services. Through evaluations on four popular cloud platforms including Amazon, Google, Microsoft, Clarifai, we demonstrate that FFL-PGD attack has a success rate over 90\% among different classification services. (3) We discuss the possible defenses to address these security challenges in cloud-based classification services. Our defense technology is mainly divided into model training stage and image preprocessing stage.



