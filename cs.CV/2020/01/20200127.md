# Arxiv Papers in cs.CV on 2020-01-27
### aiTPR: Attribute Interaction-Tensor Product Representation for Image Caption
- **Arxiv ID**: http://arxiv.org/abs/2001.09545v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2001.09545v1)
- **Published**: 2020-01-27 00:19:41+00:00
- **Updated**: 2020-01-27 00:19:41+00:00
- **Authors**: Chiranjib Sur
- **Comment**: 11 pages
- **Journal**: None
- **Summary**: Region visual features enhance the generative capability of the machines based on features, however they lack proper interaction attentional perceptions and thus ends up with biased or uncorrelated sentences or pieces of misinformation. In this work, we propose Attribute Interaction-Tensor Product Representation (aiTPR) which is a convenient way of gathering more information through orthogonal combination and learning the interactions as physical entities (tensors) and improving the captions. Compared to previous works, where features are added up to undefined feature spaces, TPR helps in maintaining sanity in combinations and orthogonality helps in defining familiar spaces. We have introduced a new concept layer that defines the objects and also their interactions that can play a crucial role in determination of different descriptions. The interaction portions have contributed heavily for better caption quality and has out-performed different previous works on this domain and MSCOCO dataset. We introduced, for the first time, the notion of combining regional image features and abstracted interaction likelihood embedding for image captioning.



### Crowd Scene Analysis by Output Encoding
- **Arxiv ID**: http://arxiv.org/abs/2001.09556v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.09556v1)
- **Published**: 2020-01-27 01:34:08+00:00
- **Updated**: 2020-01-27 01:34:08+00:00
- **Authors**: Yao Xue, Siming Liu, Yonghui Li, Xueming Qian
- **Comment**: None
- **Journal**: None
- **Summary**: Crowd scene analysis receives growing attention due to its wide applications. Grasping the accurate crowd location (rather than merely crowd count) is important for spatially identifying high-risk regions in congested scenes. In this paper, we propose a Compressed Sensing based Output Encoding (CSOE) scheme, which casts detecting pixel coordinates of small objects into a task of signal regression in encoding signal space. CSOE helps to boost localization performance in circumstances where targets are highly crowded without huge scale variation. In addition, proper receptive field sizes are crucial for crowd analysis due to human size variations. We create Multiple Dilated Convolution Branches (MDCB) that offers a set of different receptive field sizes, to improve localization accuracy when objects sizes change drastically in an image. Also, we develop an Adaptive Receptive Field Weighting (ARFW) module, which further deals with scale variation issue by adaptively emphasizing informative channels that have proper receptive field size. Experiments demonstrate the effectiveness of the proposed method, which achieves state-of-the-art performance across four mainstream datasets, especially achieves excellent results in highly crowded scenes. More importantly, experiments support our insights that it is crucial to tackle target size variation issue in crowd analysis task, and casting crowd localization as regression in encoding signal space is quite effective for crowd analysis.



### Genetic Programming for Evolving a Front of Interpretable Models for Data Visualisation
- **Arxiv ID**: http://arxiv.org/abs/2001.09578v1
- **DOI**: 10.1109/TCYB.2020.2970198
- **Categories**: **cs.NE**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2001.09578v1)
- **Published**: 2020-01-27 04:03:19+00:00
- **Updated**: 2020-01-27 04:03:19+00:00
- **Authors**: Andrew Lensen, Bing Xue, Mengjie Zhang
- **Comment**: Accepted by IEEE Transactions on Cybernetics, 2020
- **Journal**: None
- **Summary**: Data visualisation is a key tool in data mining for understanding big datasets. Many visualisation methods have been proposed, including the well-regarded state-of-the-art method t-Distributed Stochastic Neighbour Embedding. However, the most powerful visualisation methods have a significant limitation: the manner in which they create their visualisation from the original features of the dataset is completely opaque. Many domains require an understanding of the data in terms of the original features; there is hence a need for powerful visualisation methods which use understandable models. In this work, we propose a genetic programming approach named GPtSNE for evolving interpretable mappings from a dataset to highquality visualisations. A multi-objective approach is designed that produces a variety of visualisations in a single run which give different trade-offs between visual quality and model complexity. Testing against baseline methods on a variety of datasets shows the clear potential of GP-tSNE to allow deeper insight into data than that provided by existing visualisation methods. We further highlight the benefits of a multi-objective approach through an in-depth analysis of a candidate front, which shows how multiple models can



### SafeNet: An Assistive Solution to Assess Incoming Threats for Premises
- **Arxiv ID**: http://arxiv.org/abs/2002.04405v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2002.04405v1)
- **Published**: 2020-01-27 04:33:02+00:00
- **Updated**: 2020-01-27 04:33:02+00:00
- **Authors**: Shahinur Alam, Md Sultan Mahmud, Mohammed Yeasin
- **Comment**: arXiv admin note: text overlap with arXiv:1904.01178
- **Journal**: None
- **Summary**: An assistive solution to assess incoming threats (e.g., robbery, burglary, gun violence) for homes will enhance the safety of the people with or without disabilities. This paper presents "SafeNet"- an integrated assistive system to generate context-oriented image descriptions to assess incoming threats. The key functionality of the system includes the detection and identification of human and generating image descriptions from the real-time video streams obtained from the cameras placed in strategic locations around the house. In this paper, we focus on developing a robust model called "SafeNet" to generate image descriptions. To interact with the system, we implemented a dialog enabled interface for creating a personalized profile from face images or videos of friends/families. To improve computational efficiency, we apply change detection to filter out frames that do not have any activity and use Faster-RCNN to detect the human presence and extract faces using Multitask Cascaded Convolutional Networks (MTCNN). Subsequently, we apply LBP/FaceNet to identify a person. SafeNet sends image descriptions to the users with an MMS containing a person's name if any match found or as "Unknown", scene image, facial description, and contextual information. SafeNet identifies friends/families/caregiver versus intruders/unknown with an average F-score 0.97 and generates image descriptions from 10 classes with an average F-measure 0.97.



### FakeLocator: Robust Localization of GAN-Based Face Manipulations
- **Arxiv ID**: http://arxiv.org/abs/2001.09598v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2001.09598v4)
- **Published**: 2020-01-27 06:15:01+00:00
- **Updated**: 2021-11-23 05:54:48+00:00
- **Authors**: Yihao Huang, Felix Juefei-Xu, Qing Guo, Yang Liu, Geguang Pu
- **Comment**: 16 pages, accepted to IEEE Transactions on Information Forensics and
  Security
- **Journal**: None
- **Summary**: Full face synthesis and partial face manipulation by virtue of the generative adversarial networks (GANs) and its variants have raised wide public concerns. In the multi-media forensics area, detecting and ultimately locating the image forgery has become an imperative task. In this work, we investigate the architecture of existing GAN-based face manipulation methods and observe that the imperfection of upsampling methods therewithin could be served as an important asset for GAN-synthesized fake image detection and forgery localization. Based on this basic observation, we have proposed a novel approach, termed FakeLocator, to obtain high localization accuracy, at full resolution, on manipulated facial images. To the best of our knowledge, this is the very first attempt to solve the GAN-based fake localization problem with a gray-scale fakeness map that preserves more information of fake regions. To improve the universality of FakeLocator across multifarious facial attributes, we introduce an attention mechanism to guide the training of the model. To improve the universality of FakeLocator across different DeepFake methods, we propose partial data augmentation and single sample clustering on the training images. Experimental results on popular FaceForensics++, DFFD datasets and seven different state-of-the-art GAN-based face generation methods have shown the effectiveness of our method. Compared with the baselines, our method performs better on various metrics. Moreover, the proposed method is robust against various real-world facial image degradations such as JPEG compression, low-resolution, noise, and blur.



### Practical Fast Gradient Sign Attack against Mammographic Image Classifier
- **Arxiv ID**: http://arxiv.org/abs/2001.09610v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2001.09610v1)
- **Published**: 2020-01-27 07:37:07+00:00
- **Updated**: 2020-01-27 07:37:07+00:00
- **Authors**: Ibrahim Yilmaz
- **Comment**: None
- **Journal**: None
- **Summary**: Artificial intelligence (AI) has been a topic of major research for many years. Especially, with the emergence of deep neural network (DNN), these studies have been tremendously successful. Today machines are capable of making faster, more accurate decision than human. Thanks to the great development of machine learning (ML) techniques, ML have been used many different fields such as education, medicine, malware detection, autonomous car etc. In spite of having this degree of interest and much successful research, ML models are still vulnerable to adversarial attacks. Attackers can manipulate clean data in order to fool the ML classifiers to achieve their desire target. For instance; a benign sample can be modified as a malicious sample or a malicious one can be altered as benign while this modification can not be recognized by human observer. This can lead to many financial losses, or serious injuries, even deaths. The motivation behind this paper is that we emphasize this issue and want to raise awareness. Therefore, the security gap of mammographic image classifier against adversarial attack is demonstrated. We use mamographic images to train our model then evaluate our model performance in terms of accuracy. Later on, we poison original dataset and generate adversarial samples that missclassified by the model. We then using structural similarity index (SSIM) analyze similarity between clean images and adversarial images. Finally, we show how successful we are to misuse by using different poisoning factors.



### Convolution Neural Network Architecture Learning for Remote Sensing Scene Classification
- **Arxiv ID**: http://arxiv.org/abs/2001.09614v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.09614v1)
- **Published**: 2020-01-27 07:42:46+00:00
- **Updated**: 2020-01-27 07:42:46+00:00
- **Authors**: Jie Chen, Haozhe Huang, Jian Peng, Jiawei Zhu, Li Chen, Wenbo Li, Binyu Sun, Haifeng Li
- **Comment**: 10 pages, 12 figures, 3 tables
- **Journal**: None
- **Summary**: Remote sensing image scene classification is a fundamental but challenging task in understanding remote sensing images. Recently, deep learning-based methods, especially convolutional neural network-based (CNN-based) methods have shown enormous potential to understand remote sensing images. CNN-based methods meet with success by utilizing features learned from data rather than features designed manually. The feature-learning procedure of CNN largely depends on the architecture of CNN. However, most of the architectures of CNN used for remote sensing scene classification are still designed by hand which demands a considerable amount of architecture engineering skills and domain knowledge, and it may not play CNN's maximum potential on a special dataset. In this paper, we proposed an automatically architecture learning procedure for remote sensing scene classification. We designed a parameters space in which every set of parameters represents a certain architecture of CNN (i.e., some parameters represent the type of operators used in the architecture such as convolution, pooling, no connection or identity, and the others represent the way how these operators connect). To discover the optimal set of parameters for a given dataset, we introduced a learning strategy which can allow efficient search in the architecture space by means of gradient descent. An architecture generator finally maps the set of parameters into the CNN used in our experiments.



### The Whole Is Greater Than the Sum of Its Nonrigid Parts
- **Arxiv ID**: http://arxiv.org/abs/2001.09650v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CG, cs.LG, I.4.5
- **Links**: [PDF](http://arxiv.org/pdf/2001.09650v1)
- **Published**: 2020-01-27 09:48:01+00:00
- **Updated**: 2020-01-27 09:48:01+00:00
- **Authors**: Oshri Halimi, Ido Imanuel, Or Litany, Giovanni Trappolini, Emanuele Rodolà, Leonidas Guibas, Ron Kimmel
- **Comment**: None
- **Journal**: None
- **Summary**: According to Aristotle, a philosopher in Ancient Greece, "the whole is greater than the sum of its parts". This observation was adopted to explain human perception by the Gestalt psychology school of thought in the twentieth century. Here, we claim that observing part of an object which was previously acquired as a whole, one could deal with both partial matching and shape completion in a holistic manner. More specifically, given the geometry of a full, articulated object in a given pose, as well as a partial scan of the same object in a different pose, we address the problem of matching the part to the whole while simultaneously reconstructing the new pose from its partial observation. Our approach is data-driven, and takes the form of a Siamese autoencoder without the requirement of a consistent vertex labeling at inference time; as such, it can be used on unorganized point clouds as well as on triangle meshes. We demonstrate the practical effectiveness of our model in the applications of single-view deformable shape completion and dense shape correspondence, both on synthetic and real-world geometric data, where we outperform prior work on these tasks by a large margin.



### Explaining with Counter Visual Attributes and Examples
- **Arxiv ID**: http://arxiv.org/abs/2001.09671v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.09671v1)
- **Published**: 2020-01-27 10:28:47+00:00
- **Updated**: 2020-01-27 10:28:47+00:00
- **Authors**: Sadaf Gulshad, Arnold Smeulders
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1910.07416,
  arXiv:1904.08279
- **Journal**: None
- **Summary**: In this paper, we aim to explain the decisions of neural networks by utilizing multimodal information. That is counter-intuitive attributes and counter visual examples which appear when perturbed samples are introduced. Different from previous work on interpreting decisions using saliency maps, text, or visual patches we propose to use attributes and counter-attributes, and examples and counter-examples as part of the visual explanations. When humans explain visual decisions they tend to do so by providing attributes and examples. Hence, inspired by the way of human explanations in this paper we provide attribute-based and example-based explanations. Moreover, humans also tend to explain their visual decisions by adding counter-attributes and counter-examples to explain what is not seen. We introduce directed perturbations in the examples to observe which attribute values change when classifying the examples into the counter classes. This delivers intuitive counter-attributes and counter-examples. Our experiments with both coarse and fine-grained datasets show that attributes provide discriminating and human-understandable intuitive and counter-intuitive explanations.



### A Robust Real-Time Computing-based Environment Sensing System for Intelligent Vehicle
- **Arxiv ID**: http://arxiv.org/abs/2001.09678v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.09678v1)
- **Published**: 2020-01-27 10:47:50+00:00
- **Updated**: 2020-01-27 10:47:50+00:00
- **Authors**: Qiwei Xie, Qian Long, Liming Zhang, Zhao Sun
- **Comment**: None
- **Journal**: None
- **Summary**: For intelligent vehicles, sensing the 3D environment is the first but crucial step. In this paper, we build a real-time advanced driver assistance system based on a low-power mobile platform. The system is a real-time multi-scheme integrated innovation system, which combines stereo matching algorithm with machine learning based obstacle detection approach and takes advantage of the distributed computing technology of a mobile platform with GPU and CPUs. First of all, a multi-scale fast MPV (Multi-Path-Viterbi) stereo matching algorithm is proposed, which can generate robust and accurate disparity map. Then a machine learning, which is based on fusion technology of monocular and binocular, is applied to detect the obstacles. We also advance an automatic fast calibration mechanism based on Zhang's calibration method. Finally, the distributed computing and reasonable data flow programming are applied to ensure the operational efficiency of the system. The experimental results show that the system can achieve robust and accurate real-time environment perception for intelligent vehicles, which can be directly used in the commercial real-time intelligent driving applications.



### Multi-Modal Domain Adaptation for Fine-Grained Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2001.09691v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.09691v2)
- **Published**: 2020-01-27 11:06:06+00:00
- **Updated**: 2020-03-19 16:16:01+00:00
- **Authors**: Jonathan Munro, Dima Damen
- **Comment**: Accepted to CVPR 2020 for an oral presentation
- **Journal**: None
- **Summary**: Fine-grained action recognition datasets exhibit environmental bias, where multiple video sequences are captured from a limited number of environments. Training a model in one environment and deploying in another results in a drop in performance due to an unavoidable domain shift. Unsupervised Domain Adaptation (UDA) approaches have frequently utilised adversarial training between the source and target domains. However, these approaches have not explored the multi-modal nature of video within each domain. In this work we exploit the correspondence of modalities as a self-supervised alignment approach for UDA in addition to adversarial alignment.   We test our approach on three kitchens from our large-scale dataset, EPIC-Kitchens, using two modalities commonly employed for action recognition: RGB and Optical Flow. We show that multi-modal self-supervision alone improves the performance over source-only training by 2.4% on average. We then combine adversarial training with multi-modal self-supervision, showing that our approach outperforms other UDA methods by 3%.



### Unconstrained Biometric Recognition: Summary of Recent SOCIA Lab. Research
- **Arxiv ID**: http://arxiv.org/abs/2001.09703v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.09703v2)
- **Published**: 2020-01-27 11:38:16+00:00
- **Updated**: 2020-01-29 10:08:48+00:00
- **Authors**: Varsha Balakrishnan
- **Comment**: 16 pages, 2 figures
- **Journal**: None
- **Summary**: The development of biometric recognition solutions able to work in visual surveillance conditions, i.e., in unconstrained data acquisition conditions and under covert protocols has been motivating growing efforts from the research community. Among the various laboratories, schools and research institutes concerned about this problem, the SOCIA: Soft Computing and Image Analysis Lab., of the University of Beira Interior, Portugal, has been among the most active in pursuing disruptive solutions for obtaining such extremely ambitious kind of automata. This report summarises the research works published by elements of the SOCIA Lab. in the last decade in the scope of biometric recognition in unconstrained conditions. The idea is that it can be used as basis for someone wishing to entering in this research topic.



### Handling noise in image deblurring via joint learning
- **Arxiv ID**: http://arxiv.org/abs/2001.09730v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.09730v1)
- **Published**: 2020-01-27 12:59:52+00:00
- **Updated**: 2020-01-27 12:59:52+00:00
- **Authors**: Si Miao, Yongxin Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: Currently, many blind deblurring methods assume blurred images are noise-free and perform unsatisfactorily on the blurry images with noise. Unfortunately, noise is quite common in real scenes. A straightforward solution is to denoise images before deblurring them. However, even state-of-the-art denoisers cannot guarantee to remove noise entirely. Slight residual noise in the denoised images could cause significant artifacts in the deblurring stage. To tackle this problem, we propose a cascaded framework consisting of a denoiser subnetwork and a deblurring subnetwork. In contrast to previous methods, we train the two subnetworks jointly. Joint learning reduces the effect of the residual noise after denoising on deblurring, hence improves the robustness of deblurring to heavy noise. Moreover, our method is also helpful for blur kernel estimation. Experiments on the CelebA dataset and the GOPRO dataset show that our method performs favorably against several state-of-the-art methods.



### DRMIME: Differentiable Mutual Information and Matrix Exponential for Multi-Resolution Image Registration
- **Arxiv ID**: http://arxiv.org/abs/2001.09865v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.09865v1)
- **Published**: 2020-01-27 15:38:46+00:00
- **Updated**: 2020-01-27 15:38:46+00:00
- **Authors**: Abhishek Nan, Matthew Tennant, Uriel Rubin, Nilanjan Ray
- **Comment**: Software: https://github.com/abnan/DRMIME
- **Journal**: None
- **Summary**: In this work, we present a novel unsupervised image registration algorithm. It is differentiable end-to-end and can be used for both multi-modal and mono-modal registration. This is done using mutual information (MI) as a metric. The novelty here is that rather than using traditional ways of approximating MI, we use a neural estimator called MINE and supplement it with matrix exponential for transformation matrix computation. This leads to improved results as compared to the standard algorithms available out-of-the-box in state-of-the-art image registration toolboxes.



### Black Box Explanation by Learning Image Exemplars in the Latent Feature Space
- **Arxiv ID**: http://arxiv.org/abs/2002.03746v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.03746v1)
- **Published**: 2020-01-27 15:42:14+00:00
- **Updated**: 2020-01-27 15:42:14+00:00
- **Authors**: Riccardo Guidotti, Anna Monreale, Stan Matwin, Dino Pedreschi
- **Comment**: None
- **Journal**: None
- **Summary**: We present an approach to explain the decisions of black box models for image classification. While using the black box to label images, our explanation method exploits the latent feature space learned through an adversarial autoencoder. The proposed method first generates exemplar images in the latent feature space and learns a decision tree classifier. Then, it selects and decodes exemplars respecting local decision rules. Finally, it visualizes them in a manner that shows to the user how the exemplars can be modified to either stay within their class, or to become counter-factuals by "morphing" into another class. Since we focus on black box decision systems for image classification, the explanation obtained from the exemplars also provides a saliency map highlighting the areas of the image that contribute to its classification, and areas of the image that push it into another class. We present the results of an experimental evaluation on three datasets and two black box models. Besides providing the most useful and interpretable explanations, we show that the proposed method outperforms existing explainers in terms of fidelity, relevance, coherence, and stability.



### Rotation, Translation, and Cropping for Zero-Shot Generalization
- **Arxiv ID**: http://arxiv.org/abs/2001.09908v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2001.09908v3)
- **Published**: 2020-01-27 16:56:05+00:00
- **Updated**: 2020-06-12 03:18:52+00:00
- **Authors**: Chang Ye, Ahmed Khalifa, Philip Bontrager, Julian Togelius
- **Comment**: IEEE Conference on Games 2020 Full Paper
- **Journal**: None
- **Summary**: Deep Reinforcement Learning (DRL) has shown impressive performance on domains with visual inputs, in particular various games. However, the agent is usually trained on a fixed environment, e.g. a fixed number of levels. A growing mass of evidence suggests that these trained models fail to generalize to even slight variations of the environments they were trained on. This paper advances the hypothesis that the lack of generalization is partly due to the input representation, and explores how rotation, cropping and translation could increase generality. We show that a cropped, translated and rotated observation can get better generalization on unseen levels of two-dimensional arcade games from the GVGAI framework. The generality of the agents is evaluated on both human-designed and procedurally generated levels.



### Depthwise-STFT based separable Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2001.09912v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2001.09912v1)
- **Published**: 2020-01-27 17:07:08+00:00
- **Updated**: 2020-01-27 17:07:08+00:00
- **Authors**: Sudhakar Kumawat, Shanmuganathan Raman
- **Comment**: Accepted at ICASSP 2020
- **Journal**: None
- **Summary**: In this paper, we propose a new convolutional layer called Depthwise-STFT Separable layer that can serve as an alternative to the standard depthwise separable convolutional layer. The construction of the proposed layer is inspired by the fact that the Fourier coefficients can accurately represent important features such as edges in an image. It utilizes the Fourier coefficients computed (channelwise) in the 2D local neighborhood (e.g., 3x3) of each position of the input map to obtain the feature maps. The Fourier coefficients are computed using 2D Short Term Fourier Transform (STFT) at multiple fixed low frequency points in the 2D local neighborhood at each position. These feature maps at different frequency points are then linearly combined using trainable pointwise (1x1) convolutions. We show that the proposed layer outperforms the standard depthwise separable layer-based models on the CIFAR-10 and CIFAR-100 image classification datasets with reduced space-time complexity.



### Near real-time map building with multi-class image set labelling and classification of road conditions using convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/2001.09947v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2001.09947v1)
- **Published**: 2020-01-27 18:07:40+00:00
- **Updated**: 2020-01-27 18:07:40+00:00
- **Authors**: Sheela Ramanna, Cenker Sengoz, Scott Kehler, Dat Pham
- **Comment**: 23 pages, 12 figures
- **Journal**: None
- **Summary**: Weather is an important factor affecting transportation and road safety. In this paper, we leverage state-of-the-art convolutional neural networks in labelling images taken by street and highway cameras located across across North America. Road camera snapshots were used in experiments with multiple deep learning frameworks to classify images by road condition. The training data for these experiments used images labelled as dry, wet, snow/ice, poor, and offline. The experiments tested different configurations of six convolutional neural networks (VGG-16, ResNet50, Xception, InceptionResNetV2, EfficientNet-B0 and EfficientNet-B4) to assess their suitability to this problem. The precision, accuracy, and recall were measured for each framework configuration. In addition, the training sets were varied both in overall size and by size of individual classes. The final training set included 47,000 images labelled using the five aforementioned classes. The EfficientNet-B4 framework was found to be most suitable to this problem, achieving validation accuracy of 90.6%, although EfficientNet-B0 achieved an accuracy of 90.3% with half the execution time. It was observed that VGG-16 with transfer learning proved to be very useful for data acquisition and pseudo-labelling with limited hardware resources, throughout this project. The EfficientNet-B4 framework was then placed into a real-time production environment, where images could be classified in real-time on an ongoing basis. The classified images were then used to construct a map showing real-time road conditions at various camera locations across North America. The choice of these frameworks and our analysis take into account unique requirements of real-time map building functions. A detailed analysis of the process of semi-automated dataset labelling using these frameworks is also presented in this paper.



### Mi YouTube es Su YouTube? Analyzing the Cultures using YouTube Thumbnails of Popular Videos
- **Arxiv ID**: http://arxiv.org/abs/2002.00842v1
- **DOI**: None
- **Categories**: **cs.SI**, cs.CV, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/2002.00842v1)
- **Published**: 2020-01-27 20:15:57+00:00
- **Updated**: 2020-01-27 20:15:57+00:00
- **Authors**: Songyang Zhang, Tolga Aktas, Jiebo Luo
- **Comment**: None
- **Journal**: None
- **Summary**: YouTube, a world-famous video sharing website, maintains a list of the top trending videos on the platform. Due to its huge amount of users, it enables researchers to understand people's preference by analyzing the trending videos. Trending videos vary from country to country. By analyzing such differences and changes, we can tell how users' preferences differ over locations. Previous work focuses on analyzing such culture preferences from videos' metadata, while the culture information hidden within the visual content has not been discovered. In this study, we explore culture preferences among countries using the thumbnails of YouTube trending videos. We first process the thumbnail images of the videos using object detectors. The collected object information is then used for various statistical analysis. In particular, we examine the data from three perspectives: geographical locations, video genres and users' reactions. Experimental results indicate that the users from similar cultures shares interests in watching similar videos on YouTube. Our study demonstrates that discovering the culture preference through the thumbnails can be an effective mechanism for video social media analysis.



### Breast mass segmentation based on ultrasonic entropy maps and attention gated U-Net
- **Arxiv ID**: http://arxiv.org/abs/2001.10061v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2001.10061v1)
- **Published**: 2020-01-27 20:17:10+00:00
- **Updated**: 2020-01-27 20:17:10+00:00
- **Authors**: Michal Byra, Piotr Jarosik, Katarzyna Dobruch-Sobczak, Ziemowit Klimonda, Hanna Piotrzkowska-Wroblewska, Jerzy Litniewski, Andrzej Nowicki
- **Comment**: 4 pages, 4 figures
- **Journal**: None
- **Summary**: We propose a novel deep learning based approach to breast mass segmentation in ultrasound (US) imaging. In comparison to commonly applied segmentation methods, which use US images, our approach is based on quantitative entropy parametric maps. To segment the breast masses we utilized an attention gated U-Net convolutional neural network. US images and entropy maps were generated based on raw US signals collected from 269 breast masses. The segmentation networks were developed separately using US image and entropy maps, and evaluated on a test set of 81 breast masses. The attention U-Net trained based on entropy maps achieved average Dice score of 0.60 (median 0.71), while for the model trained using US images we obtained average Dice score of 0.53 (median 0.59). Our work presents the feasibility of using quantitative US parametric maps for the breast mass segmentation. The obtained results suggest that US parametric maps, which provide the information about local tissue scattering properties, might be more suitable for the development of breast mass segmentation methods than regular US images.



### Towards Open-Set Semantic Segmentation of Aerial Images
- **Arxiv ID**: http://arxiv.org/abs/2001.10063v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.10063v1)
- **Published**: 2020-01-27 20:19:57+00:00
- **Updated**: 2020-01-27 20:19:57+00:00
- **Authors**: Caio C. V. da Silva, Keiller Nogueira, Hugo N. Oliveira, Jefersson A. dos Santos
- **Comment**: None
- **Journal**: None
- **Summary**: Classical and more recently deep computer vision methods are optimized for visible spectrum images, commonly encoded in grayscale or RGB colorspaces acquired from smartphones or cameras. A more uncommon source of images exploited in the remote sensing field are satellite and aerial images. However, the development of pattern recognition approaches for these data is relatively recent, mainly due to the limited availability of this type of images, as until recently they were used exclusively for military purposes. Access to aerial imagery, including spectral information, has been increasing mainly due to the low cost of drones, cheapening of imaging satellite launch costs, and novel public datasets. Usually remote sensing applications employ computer vision techniques strictly modeled for classification tasks in closed set scenarios. However, real-world tasks rarely fit into closed set contexts, frequently presenting previously unknown classes, characterizing them as open set scenarios. Focusing on this problem, this is the first paper to study and develop semantic segmentation techniques for open set scenarios applied to remote sensing images. The main contributions of this paper are: 1) a discussion of related works in open set semantic segmentation, showing evidence that these techniques can be adapted for open set remote sensing tasks; 2) the development and evaluation of a novel approach for open set semantic segmentation. Our method yielded competitive results when compared to closed set methods for the same dataset.



### ABCTracker: an easy-to-use, cloud-based application for tracking multiple objects
- **Arxiv ID**: http://arxiv.org/abs/2001.10072v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.10072v2)
- **Published**: 2020-01-27 20:39:56+00:00
- **Updated**: 2020-01-29 14:51:56+00:00
- **Authors**: Lance Rice, Samual Tate, David Farynyk, Joshua Sun, Greg Chism, Daniel Charbonneau, Thomas Fasciano, Anna Dornhaus, Min C. Shin
- **Comment**: 17 pages, 11 figures
- **Journal**: None
- **Summary**: Visual multi-object tracking has the potential to accelerate many forms of quantitative analyses, especially in research communities investigating the motion, behavior, or social interactions within groups of animals. Despite its potential for increasing analysis throughput, complications related to accessibility, adaptability, accuracy, or scalable application arise with existing tracking systems. Several iterations of prototyping and testing have led us to a multi-object tracking system -- ABCTracker -- that is: accessible in both system as well as technical knowledge requirements, easily adaptable to new videos, and capable of producing accurate tracking data through a mixture of automatic and semi-automatic tracking features.



### Deep NRSfM++: Towards Unsupervised 2D-3D Lifting in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2001.10090v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.10090v2)
- **Published**: 2020-01-27 21:14:07+00:00
- **Updated**: 2021-03-31 02:18:27+00:00
- **Authors**: Chaoyang Wang, Chen-Hsuan Lin, Simon Lucey
- **Comment**: None
- **Journal**: None
- **Summary**: The recovery of 3D shape and pose from 2D landmarks stemming from a large ensemble of images can be viewed as a non-rigid structure from motion (NRSfM) problem. Classical NRSfM approaches, however, are problematic as they rely on heuristic priors on the 3D structure (e.g. low rank) that do not scale well to large datasets. Learning-based methods are showing the potential to reconstruct a much broader set of 3D structures than classical methods -- dramatically expanding the importance of NRSfM to atemporal unsupervised 2D to 3D lifting. Hitherto, these learning approaches have not been able to effectively model perspective cameras or handle missing/occluded points -- limiting their applicability to in-the-wild datasets. In this paper, we present a generalized strategy for improving learning-based NRSfM methods to tackle the above issues. Our approach, Deep NRSfM++, achieves state-of-the-art performance across numerous large-scale benchmarks, outperforming both classical and learning-based 2D-3D lifting methods.



### Print Defect Mapping with Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2001.10111v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.10111v1)
- **Published**: 2020-01-27 22:40:09+00:00
- **Updated**: 2020-01-27 22:40:09+00:00
- **Authors**: Augusto C. Valente, Cristina Wada, Deangela Neves, Deangeli Neves, Fábio V. M. Perez, Guilherme A. S. Megeto, Marcos H. Cascone, Otavio Gomes, Qian Lin
- **Comment**: Accepted in WACV 2020. 8 pages + references
- **Journal**: None
- **Summary**: Efficient automated print defect mapping is valuable to the printing industry since such defects directly influence customer-perceived printer quality and manually mapping them is cost-ineffective. Conventional methods consist of complicated and hand-crafted feature engineering techniques, usually targeting only one type of defect. In this paper, we propose the first end-to-end framework to map print defects at pixel level, adopting an approach based on semantic segmentation. Our framework uses Convolutional Neural Networks, specifically DeepLab-v3+, and achieves promising results in the identification of defects in printed images. We use synthetic training data by simulating two types of print defects and a print-scan effect with image processing and computer graphic techniques. Compared with conventional methods, our framework is versatile, allowing two inference strategies, one being near real-time and providing coarser results, and the other focusing on offline processing with more fine-grained detection. Our model is evaluated on a dataset of real printed images.



### Canadian Adverse Driving Conditions Dataset
- **Arxiv ID**: http://arxiv.org/abs/2001.10117v3
- **DOI**: 10.1177/0278364920979368
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.10117v3)
- **Published**: 2020-01-27 23:21:38+00:00
- **Updated**: 2020-02-27 17:23:40+00:00
- **Authors**: Matthew Pitropov, Danson Garcia, Jason Rebello, Michael Smart, Carlos Wang, Krzysztof Czarnecki, Steven Waslander
- **Comment**: None
- **Journal**: None
- **Summary**: The Canadian Adverse Driving Conditions (CADC) dataset was collected with the Autonomoose autonomous vehicle platform, based on a modified Lincoln MKZ. The dataset, collected during winter within the Region of Waterloo, Canada, is the first autonomous vehicle dataset that focuses on adverse driving conditions specifically. It contains 7,000 frames collected through a variety of winter weather conditions of annotated data from 8 cameras (Ximea MQ013CG-E2), Lidar (VLP-32C) and a GNSS+INS system (Novatel OEM638). The sensors are time synchronized and calibrated with the intrinsic and extrinsic calibrations included in the dataset. Lidar frame annotations that represent ground truth for 3D object detection and tracking have been provided by Scale AI.



