# Arxiv Papers in cs.CV on 2020-01-29
### Assistive Relative Pose Estimation for On-orbit Assembly using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2001.10673v2
- **DOI**: 10.2514/6.2020-2096
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2001.10673v2)
- **Published**: 2020-01-29 02:53:52+00:00
- **Updated**: 2020-02-19 08:02:42+00:00
- **Authors**: Shubham Sonawani, Ryan Alimo, Renaud Detry, Daniel Jeong, Andrew Hess, Heni Ben Amor
- **Comment**: None
- **Journal**: AIAA-Scitech 2020
- **Summary**: Accurate real-time pose estimation of spacecraft or object in space is a key capability necessary for on-orbit spacecraft servicing and assembly tasks. Pose estimation of objects in space is more challenging than for objects on Earth due to space images containing widely varying illumination conditions, high contrast, and poor resolution in addition to power and mass constraints. In this paper, a convolutional neural network is leveraged to uniquely determine the translation and rotation of an object of interest relative to the camera. The main idea of using CNN model is to assist object tracker used in on space assembly tasks where only feature based method is always not sufficient. The simulation framework designed for assembly task is used to generate dataset for training the modified CNN models and, then results of different models are compared with measure of how accurately models are predicting the pose. Unlike many current approaches for spacecraft or object in space pose estimation, the model does not rely on hand-crafted object-specific features which makes this model more robust and easier to apply to other types of spacecraft. It is shown that the model performs comparable to the current feature-selection methods and can therefore be used in conjunction with them to provide more reliable estimates.



### PulseSatellite: A tool using human-AI feedback loops for satellite image analysis in humanitarian contexts
- **Arxiv ID**: http://arxiv.org/abs/2001.10685v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2001.10685v1)
- **Published**: 2020-01-29 04:09:51+00:00
- **Updated**: 2020-01-29 04:09:51+00:00
- **Authors**: Tomaz Logar, Joseph Bullock, Edoardo Nemni, Lars Bromley, John A. Quinn, Miguel Luengo-Oroz
- **Comment**: 2 pages, 2 figures
- **Journal**: Proceedings of the AAAI Conference on Artificial Intelligence, New
  York, United States, 2020
- **Summary**: Humanitarian response to natural disasters and conflicts can be assisted by satellite image analysis. In a humanitarian context, very specific satellite image analysis tasks must be done accurately and in a timely manner to provide operational support. We present PulseSatellite, a collaborative satellite image analysis tool which leverages neural network models that can be retrained on-the fly and adapted to specific humanitarian contexts and geographies. We present two case studies, in mapping shelters and floods respectively, that illustrate the capabilities of PulseSatellite.



### ImVoteNet: Boosting 3D Object Detection in Point Clouds with Image Votes
- **Arxiv ID**: http://arxiv.org/abs/2001.10692v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.10692v1)
- **Published**: 2020-01-29 05:09:28+00:00
- **Updated**: 2020-01-29 05:09:28+00:00
- **Authors**: Charles R. Qi, Xinlei Chen, Or Litany, Leonidas J. Guibas
- **Comment**: None
- **Journal**: None
- **Summary**: 3D object detection has seen quick progress thanks to advances in deep learning on point clouds. A few recent works have even shown state-of-the-art performance with just point clouds input (e.g. VoteNet). However, point cloud data have inherent limitations. They are sparse, lack color information and often suffer from sensor noise. Images, on the other hand, have high resolution and rich texture. Thus they can complement the 3D geometry provided by point clouds. Yet how to effectively use image information to assist point cloud based detection is still an open question. In this work, we build on top of VoteNet and propose a 3D detection architecture called ImVoteNet specialized for RGB-D scenes. ImVoteNet is based on fusing 2D votes in images and 3D votes in point clouds. Compared to prior work on multi-modal detection, we explicitly extract both geometric and semantic features from the 2D images. We leverage camera parameters to lift these features to 3D. To improve the synergy of 2D-3D feature fusion, we also propose a multi-tower training scheme. We validate our model on the challenging SUN RGB-D dataset, advancing state-of-the-art results by 5.7 mAP. We also provide rich ablation studies to analyze the contribution of each design choice.



### Early-detection and classification of live bacteria using time-lapse coherent imaging and deep learning
- **Arxiv ID**: http://arxiv.org/abs/2001.10695v1
- **DOI**: 10.1038/s41377-020-00358-9
- **Categories**: **physics.ins-det**, cs.CV, physics.app-ph
- **Links**: [PDF](http://arxiv.org/pdf/2001.10695v1)
- **Published**: 2020-01-29 05:39:23+00:00
- **Updated**: 2020-01-29 05:39:23+00:00
- **Authors**: Hongda Wang, Hatice Ceylan Koydemir, Yunzhe Qiu, Bijie Bai, Yibo Zhang, Yiyin Jin, Sabiha Tok, Enis Cagatay Yilmaz, Esin Gumustekin, Yair Rivenson, Aydogan Ozcan
- **Comment**: 24 pages, 6 figures
- **Journal**: Light: Science & Applications (2020)
- **Summary**: We present a computational live bacteria detection system that periodically captures coherent microscopy images of bacterial growth inside a 60 mm diameter agar-plate and analyzes these time-lapsed holograms using deep neural networks for rapid detection of bacterial growth and classification of the corresponding species. The performance of our system was demonstrated by rapid detection of Escherichia coli and total coliform bacteria (i.e., Klebsiella aerogenes and Klebsiella pneumoniae subsp. pneumoniae) in water samples. These results were confirmed against gold-standard culture-based results, shortening the detection time of bacterial growth by >12 h as compared to the Environmental Protection Agency (EPA)-approved analytical methods. Our experiments further confirmed that this method successfully detects 90% of bacterial colonies within 7-10 h (and >95% within 12 h) with a precision of 99.2-100%, and correctly identifies their species in 7.6-12 h with 80% accuracy. Using pre-incubation of samples in growth media, our system achieved a limit of detection (LOD) of ~1 colony forming unit (CFU)/L within 9 h of total test time. This computational bacteria detection and classification platform is highly cost-effective (~$0.6 per test) and high-throughput with a scanning speed of 24 cm2/min over the entire plate surface, making it highly suitable for integration with the existing analytical methods currently used for bacteria detection on agar plates. Powered by deep learning, this automated and cost-effective live bacteria detection platform can be transformative for a wide range of applications in microbiology by significantly reducing the detection time, also automating the identification of colonies, without labeling or the need for an expert.



### Depth Based Semantic Scene Completion with Position Importance Aware Loss
- **Arxiv ID**: http://arxiv.org/abs/2001.10709v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.10709v2)
- **Published**: 2020-01-29 07:05:52+00:00
- **Updated**: 2020-01-30 20:16:34+00:00
- **Authors**: Yu Liu, Jie Li, Xia Yuan, Chunxia Zhao, Roland Siegwart, Ian Reid, Cesar Cadena
- **Comment**: ICRA2020 In Conjuction With RAL
- **Journal**: None
- **Summary**: Semantic Scene Completion (SSC) refers to the task of inferring the 3D semantic segmentation of a scene while simultaneously completing the 3D shapes. We propose PALNet, a novel hybrid network for SSC based on single depth. PALNet utilizes a two-stream network to extract both 2D and 3D features from multi-stages using fine-grained depth information to efficiently captures the context, as well as the geometric cues of the scene. Current methods for SSC treat all parts of the scene equally causing unnecessary attention to the interior of objects. To address this problem, we propose Position Aware Loss(PA-Loss) which is position importance aware while training the network. Specifically, PA-Loss considers Local Geometric Anisotropy to determine the importance of different positions within the scene. It is beneficial for recovering key details like the boundaries of objects and the corners of the scene. Comprehensive experiments on two benchmark datasets demonstrate the effectiveness of the proposed method and its superior performance. Models and Video demo can be found at: https://github.com/UniLauX/PALNet.



### Pre-defined Sparsity for Low-Complexity Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2001.10710v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.PF
- **Links**: [PDF](http://arxiv.org/pdf/2001.10710v2)
- **Published**: 2020-01-29 07:10:56+00:00
- **Updated**: 2020-02-04 19:26:39+00:00
- **Authors**: Souvik Kundu, Mahdi Nazemi, Massoud Pedram, Keith M. Chugg, Peter A. Beerel
- **Comment**: 14 pages, 13 figures
- **Journal**: None
- **Summary**: The high energy cost of processing deep convolutional neural networks impedes their ubiquitous deployment in energy-constrained platforms such as embedded systems and IoT devices. This work introduces convolutional layers with pre-defined sparse 2D kernels that have support sets that repeat periodically within and across filters. Due to the efficient storage of our periodic sparse kernels, the parameter savings can translate into considerable improvements in energy efficiency due to reduced DRAM accesses, thus promising significant improvements in the trade-off between energy consumption and accuracy for both training and inference. To evaluate this approach, we performed experiments with two widely accepted datasets, CIFAR-10 and Tiny ImageNet in sparse variants of the ResNet18 and VGG16 architectures. Compared to baseline models, our proposed sparse variants require up to 82% fewer model parameters with 5.6times fewer FLOPs with negligible loss in accuracy for ResNet18 on CIFAR-10. For VGG16 trained on Tiny ImageNet, our approach requires 5.8times fewer FLOPs and up to 83.3% fewer model parameters with a drop in top-5 (top-1) accuracy of only 1.2% (2.1%). We also compared the performance of our proposed architectures with that of ShuffleNet andMobileNetV2. Using similar hyperparameters and FLOPs, our ResNet18 variants yield an average accuracy improvement of 2.8%.



### The Tensor Brain: Semantic Decoding for Perception and Memory
- **Arxiv ID**: http://arxiv.org/abs/2001.11027v3
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2001.11027v3)
- **Published**: 2020-01-29 07:48:01+00:00
- **Updated**: 2020-02-10 08:41:03+00:00
- **Authors**: Volker Tresp, Sahand Sharifzadeh, Dario Konopatzki, Yunpu Ma
- **Comment**: None
- **Journal**: None
- **Summary**: We analyse perception and memory, using mathematical models for knowledge graphs and tensors, to gain insights into the corresponding functionalities of the human mind. Our discussion is based on the concept of propositional sentences consisting of \textit{subject-predicate-object} (SPO) triples for expressing elementary facts. SPO sentences are the basis for most natural languages but might also be important for explicit perception and declarative memories, as well as intra-brain communication and the ability to argue and reason. A set of SPO sentences can be described as a knowledge graph, which can be transformed into an adjacency tensor. We introduce tensor models, where concepts have dual representations as indices and associated embeddings, two constructs we believe are essential for the understanding of implicit and explicit perception and memory in the brain. We argue that a biological realization of perception and memory imposes constraints on information processing. In particular, we propose that explicit perception and declarative memories require a semantic decoder, which, in a simple realization, is based on four layers: First, a sensory memory layer, as a buffer for sensory input, second, an index layer representing concepts, third, a memoryless representation layer for the broadcasting of information ---the "blackboard", or the "canvas" of the brain--- and fourth, a working memory layer as a processing center and data buffer. We discuss the operations of the four layers and relate them to the global workspace theory. In a Bayesian brain interpretation, semantic memory defines the prior for observable triple statements. We propose that ---in evolution and during development--- semantic memory, episodic memory, and natural language evolved as emergent properties in agents' process to gain a deeper understanding of sensory information.



### Patient Specific Biomechanics Are Clinically Significant In Accurate Computer Aided Surgical Image Guidance
- **Arxiv ID**: http://arxiv.org/abs/2001.10717v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2001.10717v1)
- **Published**: 2020-01-29 08:11:07+00:00
- **Updated**: 2020-01-29 08:11:07+00:00
- **Authors**: Michael Barrow, Alice Chao, Qizhi He, Sonia Ramamoorthy, Claude Sirlin, Ryan Kastner
- **Comment**: 7 pages, 8 figures
- **Journal**: None
- **Summary**: Augmented Reality is used in Image Guided surgery (AR IG) to fuse surgical landmarks from preoperative images into a video overlay. Physical simulation is essential to maintaining accurate position of the landmarks as surgery progresses and ensuring patient safety by avoiding accidental damage to vessels etc. In liver procedures, AR IG simulation accuracy is hampered by an inability to model stiffness variations unique to the patients disease. We introduce a novel method to account for patient specific stiffness variation based on Magnetic Resonance Elastography (MRE) data. To the best of our knowledge we are the first to demonstrate the use of in-vivo biomechanical data for AR IG landmark placement. In this early work, a comparative evaluation of our MRE data driven simulation and the traditional method shows clinically significant differences in accuracy during landmark placement and motivates further animal model trials.



### Virtual KITTI 2
- **Arxiv ID**: http://arxiv.org/abs/2001.10773v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2001.10773v1)
- **Published**: 2020-01-29 12:13:20+00:00
- **Updated**: 2020-01-29 12:13:20+00:00
- **Authors**: Yohann Cabon, Naila Murray, Martin Humenberger
- **Comment**: None
- **Journal**: None
- **Summary**: This paper introduces an updated version of the well-known Virtual KITTI dataset which consists of 5 sequence clones from the KITTI tracking benchmark. In addition, the dataset provides different variants of these sequences such as modified weather conditions (e.g. fog, rain) or modified camera configurations (e.g. rotated by 15 degrees). For each sequence, we provide multiple sets of images containing RGB, depth, class segmentation, instance segmentation, flow, and scene flow data. Camera parameters and poses as well as vehicle locations are available as well. In order to showcase some of the dataset's capabilities, we ran multiple relevant experiments using state-of-the-art algorithms from the field of autonomous driving. The dataset is available for download at https://europe.naverlabs.com/Research/Computer-Vision/Proxy-Virtual-Worlds.



### Robust Multimodal Image Registration Using Deep Recurrent Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/2002.03733v1
- **DOI**: 10.1007/978-3-030-20890-5_33
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.03733v1)
- **Published**: 2020-01-29 12:22:09+00:00
- **Updated**: 2020-01-29 12:22:09+00:00
- **Authors**: Shanhui Sun, Jing Hu, Mingqing Yao, Jinrong Hu, Xiaodong Yang, Qi Song, Xi Wu
- **Comment**: None
- **Journal**: Asian Conference on Computer Vision (ACCV). 2018. 511-526
- **Summary**: The crucial components of a conventional image registration method are the choice of the right feature representations and similarity measures. These two components, although elaborately designed, are somewhat handcrafted using human knowledge. To this end, these two components are tackled in an end-to-end manner via reinforcement learning in this work. Specifically, an artificial agent, which is composed of a combined policy and value network, is trained to adjust the moving image toward the right direction. We train this network using an asynchronous reinforcement learning algorithm, where a customized reward function is also leveraged to encourage robust image registration. This trained network is further incorporated with a lookahead inference to improve the registration capability. The advantage of this algorithm is fully demonstrated by our superior performance on clinical MR and CT image pairs to other state-of-the-art medical image registration methods.



### Comparison of scanned administrative document images
- **Arxiv ID**: http://arxiv.org/abs/2001.10785v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.10785v1)
- **Published**: 2020-01-29 12:51:06+00:00
- **Updated**: 2020-01-29 12:51:06+00:00
- **Authors**: Elena Andreeva, Vladimir V. Arlazarov, Oleg Slavin, Aleksey Mishev
- **Comment**: None
- **Journal**: None
- **Summary**: In this work the methods of comparison of digitized copies of administrative documents were considered. This problem arises, for example, when comparing two copies of documents signed by two parties in order to find possible modifications made by one party, in the banking sector at the conclusion of contracts in paper form. The proposed method of document image comparison is based on a combination of several ways of image comparison of words that are descriptors of text feature points. Testing was conducted on public Payslip Dataset (French). The results showed the high quality and the reliability of finding differences in two images that are versions of the same document.



### Under the Radar: Learning to Predict Robust Keypoints for Odometry Estimation and Metric Localisation in Radar
- **Arxiv ID**: http://arxiv.org/abs/2001.10789v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2001.10789v3)
- **Published**: 2020-01-29 12:59:09+00:00
- **Updated**: 2020-02-24 13:43:32+00:00
- **Authors**: Dan Barnes, Ingmar Posner
- **Comment**: Video summary: https://youtu.be/L-PO7nxWpJU
- **Journal**: None
- **Summary**: This paper presents a self-supervised framework for learning to detect robust keypoints for odometry estimation and metric localisation in radar. By embedding a differentiable point-based motion estimator inside our architecture, we learn keypoint locations, scores and descriptors from localisation error alone. This approach avoids imposing any assumption on what makes a robust keypoint and crucially allows them to be optimised for our application. Furthermore the architecture is sensor agnostic and can be applied to most modalities. We run experiments on 280km of real world driving from the Oxford Radar RobotCar Dataset and improve on the state-of-the-art in point-based radar odometry, reducing errors by up to 45% whilst running an order of magnitude faster, simultaneously solving metric loop closures. Combining these outputs, we provide a framework capable of full mapping and localisation with radar in urban environments.



### H-OWAN: Multi-distorted Image Restoration with Tensor 1x1 Convolution
- **Arxiv ID**: http://arxiv.org/abs/2001.10853v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.10853v1)
- **Published**: 2020-01-29 14:18:16+00:00
- **Updated**: 2020-01-29 14:18:16+00:00
- **Authors**: Zihao Huang, Chao Li, Feng Duan, Qibin Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: It is a challenging task to restore images from their variants with combined distortions. In the existing works, a promising strategy is to apply parallel "operations" to handle different types of distortion. However, in the feature fusion phase, a small number of operations would dominate the restoration result due to the features' heterogeneity by different operations. To this end, we introduce the tensor 1x1 convolutional layer by imposing high-order tensor (outer) product, by which we not only harmonize the heterogeneous features but also take additional non-linearity into account. To avoid the unacceptable kernel size resulted from the tensor product, we construct the kernels with tensor network decomposition, which is able to convert the exponential growth of the dimension to linear growth. Armed with the new layer, we propose High-order OWAN for multi-distorted image restoration. In the numerical experiments, the proposed net outperforms the previous state-of-the-art and shows promising performance even in more difficult tasks.



### Evaluating the Progress of Deep Learning for Visual Relational Concepts
- **Arxiv ID**: http://arxiv.org/abs/2001.10857v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.10857v3)
- **Published**: 2020-01-29 14:21:34+00:00
- **Updated**: 2021-09-13 15:19:39+00:00
- **Authors**: Sebastian Stabinger, Peer David, Justus Piater, Antonio Rodríguez-Sánchez
- **Comment**: Accepted for publication at Journal of Vision
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) have become the state of the art method for image classification in the last ten years. Despite the fact that they achieve superhuman classification accuracy on many popular datasets, they often perform much worse on more abstract image classification tasks. We will show that these difficult tasks are linked to relational concepts from cognitive psychology and that despite progress over the last few years, such relational reasoning tasks still remain difficult for current neural network architectures.   We will review deep learning research that is linked to relational concept learning, even if it was not originally presented from this angle. Reviewing the current literature, we will argue that some form of attention will be an important component of future systems to solve relational tasks.   In addition, we will point out the shortcomings of currently used datasets, and we will recommend steps to make future datasets more relevant for testing systems on relational reasoning.



### Unsupervised Anomaly Detection for X-Ray Images
- **Arxiv ID**: http://arxiv.org/abs/2001.10883v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2001.10883v2)
- **Published**: 2020-01-29 15:14:56+00:00
- **Updated**: 2020-11-04 12:26:37+00:00
- **Authors**: Diana Davletshina, Valentyn Melnychuk, Viet Tran, Hitansh Singla, Max Berrendorf, Evgeniy Faerman, Michael Fromm, Matthias Schubert
- **Comment**: None
- **Journal**: None
- **Summary**: Obtaining labels for medical (image) data requires scarce and expensive experts. Moreover, due to ambiguous symptoms, single images rarely suffice to correctly diagnose a medical condition. Instead, it often requires to take additional background information such as the patient's medical history or test results into account. Hence, instead of focusing on uninterpretable black-box systems delivering an uncertain final diagnosis in an end-to-end-fashion, we investigate how unsupervised methods trained on images without anomalies can be used to assist doctors in evaluating X-ray images of hands. Our method increases the efficiency of making a diagnosis and reduces the risk of missing important regions. Therefore, we adopt state-of-the-art approaches for unsupervised learning to detect anomalies and show how the outputs of these methods can be explained. To reduce the effect of noise, which often can be mistaken for an anomaly, we introduce a powerful preprocessing pipeline. We provide an extensive evaluation of different approaches and demonstrate empirically that even without labels it is possible to achieve satisfying results on a real-world dataset of X-ray images of hands. We also evaluate the importance of preprocessing and one of our main findings is that without it, most of our approaches perform not better than random. To foster reproducibility and accelerate research we make our code publicly available at https://github.com/Valentyn1997/xray



### On Learning Vehicle Detection in Satellite Video
- **Arxiv ID**: http://arxiv.org/abs/2001.10900v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2001.10900v1)
- **Published**: 2020-01-29 15:35:16+00:00
- **Updated**: 2020-01-29 15:35:16+00:00
- **Authors**: Roman Pflugfelder, Axel Weissenfeld, Julian Wagner
- **Comment**: accepted by Computer Vision Winter Workshop
  (https://cvww2020.vicos.si)
- **Journal**: None
- **Summary**: Vehicle detection in aerial and satellite images is still challenging due to their tiny appearance in pixels compared to the overall size of remote sensing imagery. Classical methods of object detection very often fail in this scenario due to violation of implicit assumptions made such as rich texture, small to moderate ratios between image size and object size. Satellite video is a very new modality which introduces temporal consistency as inductive bias. Approaches for vehicle detection in satellite video use either background subtraction, frame differencing or subspace methods showing moderate performance (0.26 - 0.82 $F_1$ score). This work proposes to apply recent work on deep learning for wide-area motion imagery (WAMI) on satellite video. We show in a first approach comparable results (0.84 $F_1$) on Planet's SkySat-1 LasVegas video with room for further improvement.



### Examining the Benefits of Capsule Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2001.10964v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2001.10964v1)
- **Published**: 2020-01-29 17:18:43+00:00
- **Updated**: 2020-01-29 17:18:43+00:00
- **Authors**: Arjun Punjabi, Jonas Schmid, Aggelos K. Katsaggelos
- **Comment**: None
- **Journal**: None
- **Summary**: Capsule networks are a recently developed class of neural networks that potentially address some of the deficiencies with traditional convolutional neural networks. By replacing the standard scalar activations with vectors, and by connecting the artificial neurons in a new way, capsule networks aim to be the next great development for computer vision applications. However, in order to determine whether these networks truly operate differently than traditional networks, one must look at the differences in the capsule features. To this end, we perform several analyses with the purpose of elucidating capsule features and determining whether they perform as described in the initial publication. First, we perform a deep visualization analysis to visually compare capsule features and convolutional neural network features. Then, we look at the ability for capsule features to encode information across the vector components and address what changes in the capsule architecture provides the most benefit. Finally, we look at how well the capsule features are able to encode instantiation parameters of class objects via visual transformations.



### Dynamic Error-bounded Lossy Compression (EBLC) to Reduce the Bandwidth Requirement for Real-time Vision-based Pedestrian Safety Applications
- **Arxiv ID**: http://arxiv.org/abs/2002.03742v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.03742v1)
- **Published**: 2020-01-29 17:21:51+00:00
- **Updated**: 2020-01-29 17:21:51+00:00
- **Authors**: Mizanur Rahman, Mhafuzul Islam, Jon C. Calhoun, Mashrur Chowdhury
- **Comment**: 10 pages, 8 figures, 2 tables
- **Journal**: None
- **Summary**: As camera quality improves and their deployment moves to areas with limited bandwidth, communication bottlenecks can impair real-time constraints of an ITS application, such as video-based real-time pedestrian detection. Video compression reduces the bandwidth requirement to transmit the video but degrades the video quality. As the quality level of the video decreases, it results in the corresponding decreases in the accuracy of the vision-based pedestrian detection model. Furthermore, environmental conditions (e.g., rain and darkness) alter the compression ratio and can make maintaining a high pedestrian detection accuracy more difficult. The objective of this study is to develop a real-time error-bounded lossy compression (EBLC) strategy to dynamically change the video compression level depending on different environmental conditions in order to maintain a high pedestrian detection accuracy. We conduct a case study to show the efficacy of our dynamic EBLC strategy for real-time vision-based pedestrian detection under adverse environmental conditions. Our strategy selects the error tolerances dynamically for lossy compression that can maintain a high detection accuracy across a representative set of environmental conditions. Analyses reveal that our strategy increases pedestrian detection accuracy up to 14% and reduces the communication bandwidth up to 14x for adverse environmental conditions compared to the same conditions but without our dynamic EBLC strategy. Our dynamic EBLC strategy is independent of detection models and environmental conditions allowing other detection models and environmental conditions to be easily incorporated in our strategy.



### Developing a gender classification approach in human face images using modified local binary patterns and tani-moto based nearest neighbor algorithm
- **Arxiv ID**: http://arxiv.org/abs/2001.10966v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.10966v1)
- **Published**: 2020-01-29 17:21:56+00:00
- **Updated**: 2020-01-29 17:21:56+00:00
- **Authors**: Shervan Fekri-Ershad
- **Comment**: 12 Pages, 5 Figures, 3 Tables, Main publisher is NADIA,
- **Journal**: International Journal of Signal Processing, Image Processing and
  Pattern Recognition, Vol. 12, No. 4 (2019), pp.1-12
- **Summary**: Human identification is a much attention problem in computer vision. Gender classification plays an important role in human identification as preprocess step. So far, various methods have been proposed to solve this problem. Absolutely, classification accuracy is the main challenge for researchers in gender classification. But, some challenges such as rotation, gray scale variations, pose, illumination changes may be occurred in smart phone image capturing. In this respect, a multi step approach is proposed in this paper to classify genders in human face images based on improved local binary patters (MLBP). LBP is a texture descriptor, which extract local contrast and local spatial structure information. Some issues such as noise sensitivity, rotation sensitivity and low discriminative features can be considered as disadvantages of the basic LBP. MLBP handle disadvantages using a new theory to categorize extracted binary patterns of basic LBP. The proposed approach includes two stages. First of all, a feature vector is extracted for human face images based on MLBP. Next, non linear classifiers can be used to classify gender. In this paper nearest neighborhood classifier is evaluated based on Tani-Moto metric as distance measure. In the result part, two databases, self-collected and ICPR are used as human face database. Results are compared by some state-ofthe-art algorithms in this literature that shows the high quality of the proposed approach in terms of accuracy rate. Some of other main advantages of the proposed approach are rotation invariant, low noise sensitivity, size invariant and low computational complexity. The proposed approach decreases the computational complexity of smartphone applications because of reducing the number of database comparisons. It can also improve performance of the synchronous applications in the smarphones because of memory and CPU usage reduction.



### Evaluating Robustness to Context-Sensitive Feature Perturbations of Different Granularities
- **Arxiv ID**: http://arxiv.org/abs/2001.11055v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2001.11055v3)
- **Published**: 2020-01-29 19:20:01+00:00
- **Updated**: 2020-10-23 13:58:39+00:00
- **Authors**: Isaac Dunn, Laura Hanu, Hadrien Pouget, Daniel Kroening, Tom Melham
- **Comment**: None
- **Journal**: None
- **Summary**: We cannot guarantee that training datasets are representative of the distribution of inputs that will be encountered during deployment. So we must have confidence that our models do not over-rely on this assumption. To this end, we introduce a new method that identifies context-sensitive feature perturbations (e.g. shape, location, texture, colour) to the inputs of image classifiers. We produce these changes by performing small adjustments to the activation values of different layers of a trained generative neural network. Perturbing at layers earlier in the generator causes changes to coarser-grained features; perturbations further on cause finer-grained changes. Unsurprisingly, we find that state-of-the-art classifiers are not robust to any such changes. More surprisingly, when it comes to coarse-grained feature changes, we find that adversarial training against pixel-space perturbations is not just unhelpful: it is counterproductive.



### Just Noticeable Difference for Machines to Generate Adversarial Images
- **Arxiv ID**: http://arxiv.org/abs/2001.11064v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2001.11064v1)
- **Published**: 2020-01-29 19:42:35+00:00
- **Updated**: 2020-01-29 19:42:35+00:00
- **Authors**: Adil Kaan Akan, Mehmet Ali Genc, Fatos T. Yarman Vural
- **Comment**: 5 pages, 4 figures, submitted to ICIP2020
- **Journal**: None
- **Summary**: One way of designing a robust machine learning algorithm is to generate authentic adversarial images which can trick the algorithms as much as possible. In this study, we propose a new method to generate adversarial images which are very similar to true images, yet, these images are discriminated from the original ones and are assigned into another category by the model. The proposed method is based on a popular concept of experimental psychology, called, Just Noticeable Difference. We define Just Noticeable Difference for a machine learning model and generate a least perceptible difference for adversarial images which can trick a model. The suggested model iteratively distorts a true image by gradient descent method until the machine learning algorithm outputs a false label. Deep Neural Networks are trained for object detection and classification tasks. The cost function includes regularization terms to generate just noticeably different adversarial images which can be detected by the model. The adversarial images generated in this study looks more natural compared to the output of state of the art adversarial image generators.



### 3D Aggregated Faster R-CNN for General Lesion Detection
- **Arxiv ID**: http://arxiv.org/abs/2001.11071v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.11071v1)
- **Published**: 2020-01-29 19:57:35+00:00
- **Updated**: 2020-01-29 19:57:35+00:00
- **Authors**: Ning Zhang, Yu Cao, Benyuan Liu, Yan Luo
- **Comment**: None
- **Journal**: None
- **Summary**: Lesions are damages and abnormalities in tissues of the human body. Many of them can later turn into fatal diseases such as cancers. Detecting lesions are of great importance for early diagnosis and timely treatment. To this end, Computed Tomography (CT) scans often serve as the screening tool, allowing us to leverage the modern object detection techniques to detect the lesions. However, lesions in CT scans are often small and sparse. The local area of lesions can be very confusing, leading the region based classifier branch of Faster R-CNN easily fail. Therefore, most of the existing state-of-the-art solutions train two types of heterogeneous networks (multi-phase) separately for the candidate generation and the False Positive Reduction (FPR) purposes. In this paper, we enforce an end-to-end 3D Aggregated Faster R-CNN solution by stacking an "aggregated classifier branch" on the backbone of RPN. This classifier branch is equipped with Feature Aggregation and Local Magnification Layers to enhance the classifier branch. We demonstrate our model can achieve the state of the art performance on both LUNA16 and DeepLesion dataset. Especially, we achieve the best single-model FROC performance on LUNA16 with the inference time being 4.2s per processed scan.



### stream-learn -- open-source Python library for difficult data stream batch analysis
- **Arxiv ID**: http://arxiv.org/abs/2001.11077v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2001.11077v1)
- **Published**: 2020-01-29 20:15:09+00:00
- **Updated**: 2020-01-29 20:15:09+00:00
- **Authors**: Paweł Ksieniewicz, Paweł Zyblewski
- **Comment**: None
- **Journal**: None
- **Summary**: stream-learn is a Python package compatible with scikit-learn and developed for the drifting and imbalanced data stream analysis. Its main component is a stream generator, which allows to produce a synthetic data stream that may incorporate each of the three main concept drift types (i.e. sudden, gradual and incremental drift) in their recurring or non-recurring versions. The package allows conducting experiments following established evaluation methodologies (i.e. Test-Then-Train and Prequential). In addition, estimators adapted for data stream classification have been implemented, including both simple classifiers and state-of-art chunk-based and online classifier ensembles. To improve computational efficiency, package utilises its own implementations of prediction metrics for imbalanced binary classification tasks.



### Urban2Vec: Incorporating Street View Imagery and POIs for Multi-Modal Urban Neighborhood Embedding
- **Arxiv ID**: http://arxiv.org/abs/2001.11101v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2001.11101v1)
- **Published**: 2020-01-29 21:30:53+00:00
- **Updated**: 2020-01-29 21:30:53+00:00
- **Authors**: Zhecheng Wang, Haoyuan Li, Ram Rajagopal
- **Comment**: To appear in Proceedings of the Thirty-Fourth AAAI Conference on
  Artificial Intelligence (AAAI-20)
- **Journal**: None
- **Summary**: Understanding intrinsic patterns and predicting spatiotemporal characteristics of cities require a comprehensive representation of urban neighborhoods. Existing works relied on either inter- or intra-region connectivities to generate neighborhood representations but failed to fully utilize the informative yet heterogeneous data within neighborhoods. In this work, we propose Urban2Vec, an unsupervised multi-modal framework which incorporates both street view imagery and point-of-interest (POI) data to learn neighborhood embeddings. Specifically, we use a convolutional neural network to extract visual features from street view images while preserving geospatial similarity. Furthermore, we model each POI as a bag-of-words containing its category, rating, and review information. Analog to document embedding in natural language processing, we establish the semantic similarity between neighborhood ("document") and the words from its surrounding POIs in the vector space. By jointly encoding visual, textual, and geospatial information into the neighborhood representation, Urban2Vec can achieve performances better than baseline models and comparable to fully-supervised methods in downstream prediction tasks. Extensive experiments on three U.S. metropolitan areas also demonstrate the model interpretability, generalization capability, and its value in neighborhood similarity analysis.



### Gun Source and Muzzle Head Detection
- **Arxiv ID**: http://arxiv.org/abs/2001.11120v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.11120v1)
- **Published**: 2020-01-29 22:41:56+00:00
- **Updated**: 2020-01-29 22:41:56+00:00
- **Authors**: Zhong Zhou, Isak Czeresnia Etinger, Florian Metze, Alexander Hauptmann, Alexander Waibel
- **Comment**: EI 2020
- **Journal**: Electronic Imaging 2020.8 (2020): 187-1
- **Summary**: There is a surging need across the world for protection against gun violence. There are three main areas that we have identified as challenging in research that tries to curb gun violence: temporal location of gunshots, gun type prediction and gun source (shooter) detection. Our task is gun source detection and muzzle head detection, where the muzzle head is the round opening of the firing end of the gun. We would like to locate the muzzle head of the gun in the video visually, and identify who has fired the shot. In our formulation, we turn the problem of muzzle head detection into two sub-problems of human object detection and gun smoke detection. Our assumption is that the muzzle head typically lies between the gun smoke caused by the shot and the shooter. We have interesting results both in bounding the shooter as well as detecting the gun smoke. In our experiments, we are successful in detecting the muzzle head by detecting the gun smoke and the shooter.



### Joint Visual-Temporal Embedding for Unsupervised Learning of Actions in Untrimmed Sequences
- **Arxiv ID**: http://arxiv.org/abs/2001.11122v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.11122v3)
- **Published**: 2020-01-29 22:51:06+00:00
- **Updated**: 2020-09-30 17:17:06+00:00
- **Authors**: Rosaura G. VidalMata, Walter J. Scheirer, Anna Kukleva, David Cox, Hilde Kuehne
- **Comment**: None
- **Journal**: None
- **Summary**: Understanding the structure of complex activities in untrimmed videos is a challenging task in the area of action recognition. One problem here is that this task usually requires a large amount of hand-annotated minute- or even hour-long video data, but annotating such data is very time consuming and can not easily be automated or scaled. To address this problem, this paper proposes an approach for the unsupervised learning of actions in untrimmed video sequences based on a joint visual-temporal embedding space. To this end, we combine a visual embedding based on a predictive U-Net architecture with a temporal continuous function. The resulting representation space allows detecting relevant action clusters based on their visual as well as their temporal appearance. The proposed method is evaluated on three standard benchmark datasets, Breakfast Actions, INRIA YouTube Instructional Videos, and 50 Salads. We show that the proposed approach is able to provide a meaningful visual and temporal embedding out of the visual cues present in contiguous video frames and is suitable for the task of unsupervised temporal segmentation of actions.



