# Arxiv Papers in cs.CV on 2020-01-30
### Adversarial Attacks on Convolutional Neural Networks in Facial Recognition Domain
- **Arxiv ID**: http://arxiv.org/abs/2001.11137v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV, stat.ML, I.5.1; I.5.4; I.5.5
- **Links**: [PDF](http://arxiv.org/pdf/2001.11137v3)
- **Published**: 2020-01-30 00:25:05+00:00
- **Updated**: 2021-02-08 07:43:45+00:00
- **Authors**: Yigit Alparslan, Ken Alparslan, Jeremy Keim-Shenk, Shweta Khade, Rachel Greenstadt
- **Comment**: 18 pages, 8 figures, fixed typos, replotted figures, restyled the
  plots and tables
- **Journal**: None
- **Summary**: Numerous recent studies have demonstrated how Deep Neural Network (DNN) classifiers can be fooled by adversarial examples, in which an attacker adds perturbations to an original sample, causing the classifier to misclassify the sample. Adversarial attacks that render DNNs vulnerable in real life represent a serious threat in autonomous vehicles, malware filters, or biometric authentication systems. In this paper, we apply Fast Gradient Sign Method to introduce perturbations to a facial image dataset and then test the output on a different classifier that we trained ourselves, to analyze transferability of this method. Next, we craft a variety of different black-box attack algorithms on a facial image dataset assuming minimal adversarial knowledge, to further assess the robustness of DNNs in facial recognition. While experimenting with different image distortion techniques, we focus on modifying single optimal pixels by a large amount, or modifying all pixels by a smaller amount, or combining these two attack approaches. While our single-pixel attacks achieved about a 15% average decrease in classifier confidence level for the actual class, the all-pixel attacks were more successful and achieved up to an 84% average decrease in confidence, along with an 81.6% misclassification rate, in the case of the attack that we tested with the highest levels of perturbation. Even with these high levels of perturbation, the face images remained identifiable to a human. Understanding how these noised and perturbed images baffle the classification algorithms can yield valuable advances in the training of DNNs against defense-aware adversarial attacks, as well as adaptive noise reduction techniques. We hope our research may help to advance the study of adversarial attacks on DNNs and defensive mechanisms to counteract them, particularly in the facial recognition domain.



### Learn to Predict Sets Using Feed-Forward Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2001.11845v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2001.11845v2)
- **Published**: 2020-01-30 01:52:07+00:00
- **Updated**: 2021-10-25 06:33:27+00:00
- **Authors**: Hamid Rezatofighi, Tianyu Zhu, Roman Kaskman, Farbod T. Motlagh, Qinfeng Shi, Anton Milan, Daniel Cremers, Laura Leal-Taix√©, Ian Reid
- **Comment**: Accepted in IEEE Transactions on Pattern Analysis and Machine
  Intelligence (TPAMI) 2022. arXiv admin note: substantial text overlap with
  arXiv:1805.00613
- **Journal**: None
- **Summary**: This paper addresses the task of set prediction using deep feed-forward neural networks. A set is a collection of elements which is invariant under permutation and the size of a set is not fixed in advance. Many real-world problems, such as image tagging and object detection, have outputs that are naturally expressed as sets of entities. This creates a challenge for traditional deep neural networks which naturally deal with structured outputs such as vectors, matrices or tensors. We present a novel approach for learning to predict sets with unknown permutation and cardinality using deep neural networks. In our formulation we define a likelihood for a set distribution represented by a) two discrete distributions defining the set cardinally and permutation variables, and b) a joint distribution over set elements with a fixed cardinality. Depending on the problem under consideration, we define different training models for set prediction using deep neural networks. We demonstrate the validity of our set formulations on relevant vision problems such as: 1) multi-label image classification where we outperform the other competing methods on the PASCAL VOC and MS COCO datasets, 2) object detection, for which our formulation outperforms popular state-of-the-art detectors, and 3) a complex CAPTCHA test, where we observe that, surprisingly, our set-based network acquired the ability of mimicking arithmetics without any rules being coded.



### Adversarial Incremental Learning
- **Arxiv ID**: http://arxiv.org/abs/2001.11152v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2001.11152v2)
- **Published**: 2020-01-30 02:25:35+00:00
- **Updated**: 2020-02-03 17:30:22+00:00
- **Authors**: Ankur Singh
- **Comment**: I want my draft to be withdrawn from arXiv. I don't want to make the
  idea public right now. I understand now that it can't be completely removed.
  So at least if it can't be completely removed you can direct users to the
  latest version which doesn't has the PDF and not the previous version
- **Journal**: None
- **Summary**: Although deep learning performs really well in a wide variety of tasks, it still suffers from catastrophic forgetting -- the tendency of neural networks to forget previously learned information upon learning new tasks where previous data is not available. Earlier methods of incremental learning tackle this problem by either using a part of the old dataset, by generating exemplars or by using memory networks. Although, these methods have shown good results but using exemplars or generating them, increases memory and computation requirements. To solve these problems we propose an adversarial discriminator based method that does not make use of old data at all while training on new tasks. We particularly tackle the class incremental learning problem in image classification, where data is provided in a class-based sequential manner. For this problem, the network is trained using an adversarial loss along with the traditional cross-entropy loss. The cross-entropy loss helps the network progressively learn new classes while the adversarial loss helps in preserving information about the existing classes. Using this approach, we are able to outperform other state-of-the-art methods on CIFAR-100, SVHN, and MNIST datasets.



### Unsupervised Pixel-level Road Defect Detection via Adversarial Image-to-Frequency Transform
- **Arxiv ID**: http://arxiv.org/abs/2001.11175v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2001.11175v2)
- **Published**: 2020-01-30 04:50:00+00:00
- **Updated**: 2020-02-03 04:27:32+00:00
- **Authors**: Jongmin Yu, Duyong Kim, Younkwan Lee, Moongu Jeon
- **Comment**: Submitted to IV2020
- **Journal**: None
- **Summary**: In the past few years, the performance of road defect detection has been remarkably improved thanks to advancements on various studies on computer vision and deep learning. Although a large-scale and well-annotated datasets enhance the performance of detecting road pavement defects to some extent, it is still challengeable to derive a model which can perform reliably for various road conditions in practice, because it is intractable to construct a dataset considering diverse road conditions and defect patterns. To end this, we propose an unsupervised approach to detecting road defects, using Adversarial Image-to-Frequency Transform (AIFT). AIFT adopts the unsupervised manner and adversarial learning in deriving the defect detection model, so AIFT does not need annotations for road pavement defects. We evaluate the efficiency of AIFT using GAPs384 dataset, Cracktree200 dataset, CRACK500 dataset, and CFD dataset. The experimental results demonstrate that the proposed approach detects various road detects, and it outperforms existing state-of-the-art approaches.



### Multiple Object Tracking by Flowing and Fusing
- **Arxiv ID**: http://arxiv.org/abs/2001.11180v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.11180v1)
- **Published**: 2020-01-30 05:17:22+00:00
- **Updated**: 2020-01-30 05:17:22+00:00
- **Authors**: Jimuyang Zhang, Sanping Zhou, Xin Chang, Fangbin Wan, Jinjun Wang, Yang Wu, Dong Huang
- **Comment**: None
- **Journal**: None
- **Summary**: Most of Multiple Object Tracking (MOT) approaches compute individual target features for two subtasks: estimating target-wise motions and conducting pair-wise Re-Identification (Re-ID). Because of the indefinite number of targets among video frames, both subtasks are very difficult to scale up efficiently in end-to-end Deep Neural Networks (DNNs). In this paper, we design an end-to-end DNN tracking approach, Flow-Fuse-Tracker (FFT), that addresses the above issues with two efficient techniques: target flowing and target fusing. Specifically, in target flowing, a FlowTracker DNN module learns the indefinite number of target-wise motions jointly from pixel-level optical flows. In target fusing, a FuseTracker DNN module refines and fuses targets proposed by FlowTracker and frame-wise object detection, instead of trusting either of the two inaccurate sources of target proposal. Because FlowTracker can explore complex target-wise motion patterns and FuseTracker can refine and fuse targets from FlowTracker and detectors, our approach can achieve the state-of-the-art results on several MOT benchmarks. As an online MOT approach, FFT produced the top MOTA of 46.3 on the 2DMOT15, 56.5 on the MOT16, and 56.5 on the MOT17 tracking benchmarks, surpassing all the online and offline methods in existing publications.



### 2018 Robotic Scene Segmentation Challenge
- **Arxiv ID**: http://arxiv.org/abs/2001.11190v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2001.11190v3)
- **Published**: 2020-01-30 06:37:07+00:00
- **Updated**: 2020-08-03 01:55:24+00:00
- **Authors**: Max Allan, Satoshi Kondo, Sebastian Bodenstedt, Stefan Leger, Rahim Kadkhodamohammadi, Imanol Luengo, Felix Fuentes, Evangello Flouty, Ahmed Mohammed, Marius Pedersen, Avinash Kori, Varghese Alex, Ganapathy Krishnamurthi, David Rauber, Robert Mendel, Christoph Palm, Sophia Bano, Guinther Saibro, Chi-Sheng Shih, Hsun-An Chiang, Juntang Zhuang, Junlin Yang, Vladimir Iglovikov, Anton Dobrenkii, Madhu Reddiboina, Anubhav Reddy, Xingtong Liu, Cong Gao, Mathias Unberath, Myeonghyeon Kim, Chanho Kim, Chaewon Kim, Hyejin Kim, Gyeongmin Lee, Ihsan Ullah, Miguel Luna, Sang Hyun Park, Mahdi Azizian, Danail Stoyanov, Lena Maier-Hein, Stefanie Speidel
- **Comment**: None
- **Journal**: None
- **Summary**: In 2015 we began a sub-challenge at the EndoVis workshop at MICCAI in Munich using endoscope images of ex-vivo tissue with automatically generated annotations from robot forward kinematics and instrument CAD models. However, the limited background variation and simple motion rendered the dataset uninformative in learning about which techniques would be suitable for segmentation in real surgery. In 2017, at the same workshop in Quebec we introduced the robotic instrument segmentation dataset with 10 teams participating in the challenge to perform binary, articulating parts and type segmentation of da Vinci instruments. This challenge included realistic instrument motion and more complex porcine tissue as background and was widely addressed with modifications on U-Nets and other popular CNN architectures. In 2018 we added to the complexity by introducing a set of anatomical objects and medical devices to the segmented classes. To avoid over-complicating the challenge, we continued with porcine data which is dramatically simpler than human tissue due to the lack of fatty tissue occluding many organs.



### Automatic marker-free registration of tree point-cloud data based on rotating projection
- **Arxiv ID**: http://arxiv.org/abs/2001.11192v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.11192v1)
- **Published**: 2020-01-30 06:53:59+00:00
- **Updated**: 2020-01-30 06:53:59+00:00
- **Authors**: Xiuxian Xu, Pei Wang, Xiaozheng Gan, Yaxin Li, Li Zhang, Qing Zhang, Mei Zhou, Yinghui Zhao, Xinwei Li
- **Comment**: 33 pages
- **Journal**: None
- **Summary**: Point-cloud data acquired using a terrestrial laser scanner (TLS) play an important role in digital forestry research. Multiple scans are generally used to overcome occlusion effects and obtain complete tree structural information. However, it is time-consuming and difficult to place artificial reflectors in a forest with complex terrain for marker-based registration, a process that reduces registration automation and efficiency. In this study, we propose an automatic coarse-to-fine method for the registration of point-cloud data from multiple scans of a single tree. In coarse registration, point clouds produced by each scan are projected onto a spherical surface to generate a series of two-dimensional (2D) images, which are used to estimate the initial positions of multiple scans. Corresponding feature-point pairs are then extracted from these series of 2D images. In fine registration, point-cloud data slicing and fitting methods are used to extract corresponding central stem and branch centers for use as tie points to calculate fine transformation parameters. To evaluate the accuracy of registration results, we propose a model of error evaluation via calculating the distances between center points from corresponding branches in adjacent scans. For accurate evaluation, we conducted experiments on two simulated trees and a real-world tree. Average registration errors of the proposed method were 0.26m around on simulated tree point clouds, and 0.05m around on real-world tree point cloud.



### The Direction-Aware, Learnable, Additive Kernels and the Adversarial Network for Deep Floor Plan Recognition
- **Arxiv ID**: http://arxiv.org/abs/2001.11194v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2001.11194v1)
- **Published**: 2020-01-30 07:10:39+00:00
- **Updated**: 2020-01-30 07:10:39+00:00
- **Authors**: Yuli Zhang, Yeyang He, Shaowen Zhu, Xinhan Di
- **Comment**: deep learning, floor plan, computer vision
- **Journal**: None
- **Summary**: This paper presents a new approach for the recognition of elements in floor plan layouts. Besides of elements with common shapes, we aim to recognize elements with irregular shapes such as circular rooms and inclined walls. Furthermore, the reduction of noise in the semantic segmentation of the floor plan is on demand. To this end, we propose direction-aware, learnable, additive kernels in the application of both the context module and common convolutional blocks. We apply them for high performance of elements with both common and irregular shapes. Besides, an adversarial network with two discriminators is proposed to further improve the accuracy of the elements and to reduce the noise of the semantic segmentation. Experimental results demonstrate the superiority and effectiveness of the proposed network over the state-of-the-art methods.



### A CNN With Multi-scale Convolution for Hyperspectral Image Classification using Target-Pixel-Orientation scheme
- **Arxiv ID**: http://arxiv.org/abs/2001.11198v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.11198v3)
- **Published**: 2020-01-30 07:45:07+00:00
- **Updated**: 2021-05-05 14:28:06+00:00
- **Authors**: Jayasree Saha, Yuvraj Khanna, Jayanta Mukherjee
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, CNN is a popular choice to handle the hyperspectral image classification challenges. In spite of having such large spectral information in Hyper-Spectral Image(s) (HSI), it creates a curse of dimensionality. Also, large spatial variability of spectral signature adds more difficulty in classification problem. Additionally, training a CNN in the end to end fashion with scarced training examples is another challenging and interesting problem. In this paper, a novel target-patch-orientation method is proposed to train a CNN based network. Also, we have introduced a hybrid of 3D-CNN and 2D-CNN based network architecture to implement band reduction and feature extraction methods, respectively. Experimental results show that our method outperforms the accuracies reported in the existing state of the art methods.



### Image Embedded Segmentation: Uniting Supervised and Unsupervised Objectives for Segmenting Histopathological Images
- **Arxiv ID**: http://arxiv.org/abs/2001.11202v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2001.11202v3)
- **Published**: 2020-01-30 08:09:38+00:00
- **Updated**: 2020-11-25 11:18:20+00:00
- **Authors**: C. T. Sari, C. Sokmensuer, C. Gunduz-Demir
- **Comment**: This work has been submitted for possible publication
- **Journal**: None
- **Summary**: This paper presents a new regularization method to train a fully convolutional network for semantic tissue segmentation in histopathological images. This method relies on the benefit of unsupervised learning, in the form of image reconstruction, for network training. To this end, it puts forward an idea of defining a new embedding that allows uniting the main supervised task of semantic segmentation and an auxiliary unsupervised task of image reconstruction into a single one and proposes to learn this united task by a single generative model. This embedding generates an output image by superimposing an input image on its segmentation map. Then, the method learns to translate the input image to this embedded output image using a conditional generative adversarial network, which is known as quite effective for image-to-image translations. This proposal is different than the existing approach that uses image reconstruction for the same regularization purpose. The existing approach considers segmentation and image reconstruction as two separate tasks in a multi-task network, defines their losses independently, and combines them in a joint loss function. However, the definition of such a function requires externally determining right contributions of the supervised and unsupervised losses that yield balanced learning between the segmentation and image reconstruction tasks. The proposed approach provides an easier solution to this problem by uniting these two tasks into a single one, which intrinsically combines their losses. We test our approach on three datasets of histopathological images. Our experiments demonstrate that it leads to better segmentation results in these datasets, compared to its counterparts.



### Weakly Supervised Instance Segmentation by Deep Community Learning
- **Arxiv ID**: http://arxiv.org/abs/2001.11207v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.11207v3)
- **Published**: 2020-01-30 08:35:42+00:00
- **Updated**: 2020-11-18 09:43:49+00:00
- **Authors**: Jaedong Hwang, Seohyun Kim, Jeany Son, Bohyung Han
- **Comment**: WACV 2021
- **Journal**: None
- **Summary**: We present a weakly supervised instance segmentation algorithm based on deep community learning with multiple tasks. This task is formulated as a combination of weakly supervised object detection and semantic segmentation, where individual objects of the same class are identified and segmented separately. We address this problem by designing a unified deep neural network architecture, which has a positive feedback loop of object detection with bounding box regression, instance mask generation, instance segmentation, and feature extraction. Each component of the network makes active interactions with others to improve accuracy, and the end-to-end trainability of our model makes our results more robust and reproducible. The proposed algorithm achieves state-of-the-art performance in the weakly supervised setting without any additional training such as Fast R-CNN and Mask R-CNN on the standard benchmark dataset. The implementation of our algorithm is available on the project webpage: https://cv.snu.ac.kr/research/WSIS_CL.



### Efficient Scene Text Detection with Textual Attention Tower
- **Arxiv ID**: http://arxiv.org/abs/2002.03741v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.03741v1)
- **Published**: 2020-01-30 09:50:08+00:00
- **Updated**: 2020-01-30 09:50:08+00:00
- **Authors**: Liang Zhang, Yufei Liu, Hang Xiao, Lu Yang, Guangming Zhu, Syed Afaq Shah, Mohammed Bennamoun, Peiyi Shen
- **Comment**: Accepted by ICASSP 2020
- **Journal**: None
- **Summary**: Scene text detection has received attention for years and achieved an impressive performance across various benchmarks. In this work, we propose an efficient and accurate approach to detect multioriented text in scene images. The proposed feature fusion mechanism allows us to use a shallower network to reduce the computational complexity. A self-attention mechanism is adopted to suppress false positive detections. Experiments on public benchmarks including ICDAR 2013, ICDAR 2015 and MSRA-TD500 show that our proposed approach can achieve better or comparable performances with fewer parameters and less computational cost.



### Fast Video Object Segmentation using the Global Context Module
- **Arxiv ID**: http://arxiv.org/abs/2001.11243v2
- **DOI**: None
- **Categories**: **cs.CV**, I.4.6
- **Links**: [PDF](http://arxiv.org/pdf/2001.11243v2)
- **Published**: 2020-01-30 10:22:27+00:00
- **Updated**: 2020-07-18 01:11:10+00:00
- **Authors**: Yu Li, Zhuoran Shen, Ying Shan
- **Comment**: To appear at ECCV 2020
- **Journal**: None
- **Summary**: We developed a real-time, high-quality semi-supervised video object segmentation algorithm. Its accuracy is on par with the most accurate, time-consuming online-learning model, while its speed is similar to the fastest template-matching method with sub-optimal accuracy. The core component of the model is a novel global context module that effectively summarizes and propagates information through the entire video. Compared to previous approaches that only use one frame or a few frames to guide the segmentation of the current frame, the global context module uses all past frames. Unlike the previous state-of-the-art space-time memory network that caches a memory at each spatio-temporal position, the global context module uses a fixed-size feature representation. Therefore, it uses constant memory regardless of the video length and costs substantially less memory and computation. With the novel module, our model achieves top performance on standard benchmarks at a real-time speed.



### Weakly Supervised Segmentation of Cracks on Solar Cells using Normalized Lp Norm
- **Arxiv ID**: http://arxiv.org/abs/2001.11248v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.11248v1)
- **Published**: 2020-01-30 10:51:25+00:00
- **Updated**: 2020-01-30 10:51:25+00:00
- **Authors**: Martin Mayr, Mathis Hoffmann, Andreas Maier, Vincent Christlein
- **Comment**: ICIP'2019
- **Journal**: None
- **Summary**: Photovoltaic is one of the most important renewable energy sources for dealing with world-wide steadily increasing energy consumption. This raises the demand for fast and scalable automatic quality management during production and operation. However, the detection and segmentation of cracks on electroluminescence (EL) images of mono- or polycrystalline solar modules is a challenging task. In this work, we propose a weakly supervised learning strategy that only uses image-level annotations to obtain a method that is capable of segmenting cracks on EL images of solar cells. We use a modified ResNet-50 to derive a segmentation from network activation maps. We use defect classification as a surrogate task to train the network. To this end, we apply normalized Lp normalization to aggregate the activation maps into single scores for classification. In addition, we provide a study how different parameterizations of the normalized Lp layer affect the segmentation performance. This approach shows promising results for the given task. However, we think that the method has the potential to solve other weakly supervised segmentation problems as well.



### Person Re-identification: Implicitly Defining the Receptive Fields of Deep Learning Classification Frameworks
- **Arxiv ID**: http://arxiv.org/abs/2001.11267v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2001.11267v4)
- **Published**: 2020-01-30 11:45:44+00:00
- **Updated**: 2020-07-02 19:59:52+00:00
- **Authors**: Ehsan Yaghoubi, Diana Borza, Aruna Kumar, Hugo Proen√ßa
- **Comment**: Submitted to PRL
- **Journal**: None
- **Summary**: The \emph{receptive fields} of deep learning classification models determine the regions of the input data that have the most significance for providing correct decisions. The primary way to learn such receptive fields is to train the models upon masked data, which helps the networks to ignore any unwanted regions, but has two major drawbacks: 1) it often yields edge-sensitive decision processes; and 2) augments the computational cost of the inference phase considerably. This paper describes a solution for implicitly driving the inference of the networks' receptive fields, by creating synthetic learning data composed of interchanged segments that should be \emph{apriori} important/irrelevant for the network decision. In practice, we use a segmentation module to distinguish between the foreground (important)/background (irrelevant) parts of each learning instance, and randomly swap segments between image pairs, while keeping the class label exclusively consistent with the label of the deemed important segments. This strategy typically drives the networks to early convergence and appropriate solutions, where the identity and clutter descriptions are not correlated. Moreover, this data augmentation solution has various interesting properties: 1) it is parameter-free; 2) it fully preserves the label information; and, 3) it is compatible with the typical data augmentation techniques. In the empirical validation, we considered the person re-identification problem and evaluated the effectiveness of the proposed solution in the well-known \emph{Richly Annotated Pedestrian} (RAP) dataset for two different settings (\emph{upper-body} and \emph{full-body}), observing highly competitive results over the state-of-the-art. Under a reproducible research paradigm, both the code and the empirical evaluation protocol are available at \url{https://github.com/Ehsan-Yaghoubi/reid-strong-baseline}.



### The Ladder Algorithm: Finding Repetitive Structures in Medical Images by Induction
- **Arxiv ID**: http://arxiv.org/abs/2001.11284v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.11284v2)
- **Published**: 2020-01-30 12:40:31+00:00
- **Updated**: 2020-04-08 09:24:25+00:00
- **Authors**: Rhydian Windsor, Amir Jamaludin
- **Comment**: 5 pages, 4 figures, IEEE International Symposium on Biomedical
  Imaging (ISBI) 2020. Presentation:
  https://www.youtube.com/watch?v=khlBqpNGRnE
- **Journal**: None
- **Summary**: In this paper we introduce the Ladder Algorithm; a novel recurrent algorithm to detect repetitive structures in natural images with high accuracy using little training data.   We then demonstrate the algorithm on the task of extracting vertebrae from whole spine magnetic resonance scans with only lumbar MR scans for training data. It is shown to achieve high perforamance with 99.8% precision and recall, exceeding current state of the art approaches for lumbar vertebrae detection in T1 and T2 weighted scans. It also generalises without retraining to whole spine images with minimal drop in accuracy, achieving 99.4% detection rate.



### A Deeper Look into Hybrid Images
- **Arxiv ID**: http://arxiv.org/abs/2001.11302v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.11302v2)
- **Published**: 2020-01-30 13:25:14+00:00
- **Updated**: 2020-02-10 15:54:20+00:00
- **Authors**: Jimut Bahan Pal
- **Comment**: A deeper analysis for creating Hybrid Images
- **Journal**: None
- **Summary**: $Hybrid$ $images$ was first introduced by Olivia et al., that produced static images with two interpretations such that the images changes as a function of viewing distance. Hybrid images are built by studying human processing of multiscale images and are motivated by masking studies in visual perception. The first introduction of hybrid images showed that two images can be blend together with a high pass filter and a low pass filter in such a way that when the blended image is viewed from a distance, the high pass filter fades away and the low pass filter becomes prominent. Our main aim here is to study and review the original paper by changing and tweaking certain parameters to see how they affect the quality of the blended image produced. We have used exhaustively different set of images and filters to see how they function and whether this can be used in a real time system or not.



### Black-Box Saliency Map Generation Using Bayesian Optimisation
- **Arxiv ID**: http://arxiv.org/abs/2001.11366v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2001.11366v1)
- **Published**: 2020-01-30 14:39:12+00:00
- **Updated**: 2020-01-30 14:39:12+00:00
- **Authors**: Mamuku Mokuwe, Michael Burke, Anna Sergeevna Bosman
- **Comment**: Submitted to IJCNN 2020
- **Journal**: None
- **Summary**: Saliency maps are often used in computer vision to provide intuitive interpretations of what input regions a model has used to produce a specific prediction. A number of approaches to saliency map generation are available, but most require access to model parameters. This work proposes an approach for saliency map generation for black-box models, where no access to model parameters is available, using a Bayesian optimisation sampling method. The approach aims to find the global salient image region responsible for a particular (black-box) model's prediction. This is achieved by a sampling-based approach to model perturbations that seeks to localise salient regions of an image to the black-box model. Results show that the proposed approach to saliency map generation outperforms grid-based perturbation approaches, and performs similarly to gradient-based approaches which require access to model parameters.



### ERA: A Dataset and Deep Learning Benchmark for Event Recognition in Aerial Videos
- **Arxiv ID**: http://arxiv.org/abs/2001.11394v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.11394v4)
- **Published**: 2020-01-30 15:25:54+00:00
- **Updated**: 2020-06-25 10:23:08+00:00
- **Authors**: Lichao Mou, Yuansheng Hua, Pu Jin, Xiao Xiang Zhu
- **Comment**: IEEE Geoscience and Remote Sensing Magazine. Project page:
  https://lcmou.github.io/ERA_Dataset/
- **Journal**: None
- **Summary**: Along with the increasing use of unmanned aerial vehicles (UAVs), large volumes of aerial videos have been produced. It is unrealistic for humans to screen such big data and understand their contents. Hence methodological research on the automatic understanding of UAV videos is of paramount importance. In this paper, we introduce a novel problem of event recognition in unconstrained aerial videos in the remote sensing community and present a large-scale, human-annotated dataset, named ERA (Event Recognition in Aerial videos), consisting of 2,864 videos each with a label from 25 different classes corresponding to an event unfolding 5 seconds. The ERA dataset is designed to have a significant intra-class variation and inter-class similarity and captures dynamic events in various circumstances and at dramatically various scales. Moreover, to offer a benchmark for this task, we extensively validate existing deep networks. We expect that the ERA dataset will facilitate further progress in automatic aerial video comprehension. The website is https://lcmou.github.io/ERA_Dataset/



### Semi-Automatic Generation of Tight Binary Masks and Non-Convex Isosurfaces for Quantitative Analysis of 3D Biological Samples
- **Arxiv ID**: http://arxiv.org/abs/2001.11469v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, q-bio.CB, q-bio.TO
- **Links**: [PDF](http://arxiv.org/pdf/2001.11469v1)
- **Published**: 2020-01-30 17:36:42+00:00
- **Updated**: 2020-01-30 17:36:42+00:00
- **Authors**: Sourabh Bhide, Ralf Mikut, Maria Leptin, Johannes Stegmaier
- **Comment**: 5 pages, 2 figures
- **Journal**: None
- **Summary**: Current in vivo microscopy allows us detailed spatiotemporal imaging (3D+t) of complete organisms and offers insights into their development on the cellular level. Even though the imaging speed and quality is steadily improving, fully-automated segmentation and analysis methods are often not accurate enough. This is particularly true while imaging large samples (100um - 1mm) and deep inside the specimen. Drosophila embryogenesis, widely used as a developmental paradigm, presents an example for such a challenge, especially where cell outlines need to imaged - a general challenge in other systems as well. To deal with the current bottleneck in analyzing quantitatively the 3D+t light-sheet microscopy images of Drosophila embryos, we developed a collection of semi-automatic open-source tools. The presented methods include a semi-automatic masking procedure, automatic projection of non-convex 3D isosurfaces to 2D representations as well as cell segmentation and tracking.



### Adversarial Code Learning for Image Generation
- **Arxiv ID**: http://arxiv.org/abs/2001.11539v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2001.11539v1)
- **Published**: 2020-01-30 19:52:46+00:00
- **Updated**: 2020-01-30 19:52:46+00:00
- **Authors**: Jiangbo Yuan, Bing Wu, Wanying Ding, Qing Ping, Zhendong Yu
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce the "adversarial code learning" (ACL) module that improves overall image generation performance to several types of deep models. Instead of performing a posterior distribution modeling in the pixel spaces of generators, ACLs aim to jointly learn a latent code with another image encoder/inference net, with a prior noise as its input. We conduct the learning in an adversarial learning process, which bears a close resemblance to the original GAN but again shifts the learning from image spaces to prior and latent code spaces. ACL is a portable module that brings up much more flexibility and possibilities in generative model designs. First, it allows flexibility to convert non-generative models like Autoencoders and standard classification models to decent generative models. Second, it enhances existing GANs' performance by generating meaningful codes and images from any part of the prior. We have incorporated our ACL module with the aforementioned frameworks and have performed experiments on synthetic, MNIST, CIFAR-10, and CelebA datasets. Our models have achieved significant improvements which demonstrated the generality for image generation tasks.



### HistomicsML2.0: Fast interactive machine learning for whole slide imaging data
- **Arxiv ID**: http://arxiv.org/abs/2001.11547v1
- **DOI**: None
- **Categories**: **q-bio.QM**, cs.CV, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2001.11547v1)
- **Published**: 2020-01-30 20:10:26+00:00
- **Updated**: 2020-01-30 20:10:26+00:00
- **Authors**: Sanghoon Lee, Mohamed Amgad, Deepak R. Chittajallu, Matt McCormick, Brian P Pollack, Habiba Elfandy, Hagar Hussein, David A Gutman, Lee AD Cooper
- **Comment**: None
- **Journal**: None
- **Summary**: Extracting quantitative phenotypic information from whole-slide images presents significant challenges for investigators who are not experienced in developing image analysis algorithms. We present new software that enables rapid learn-by-example training of machine learning classifiers for detection of histologic patterns in whole-slide imaging datasets. HistomicsML2.0 uses convolutional networks to be readily adaptable to a variety of applications, provides a web-based user interface, and is available as a software container to simplify deployment.



### Dual Convolutional LSTM Network for Referring Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2001.11561v1
- **DOI**: 10.1109/TMM.2020.2971171
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.11561v1)
- **Published**: 2020-01-30 20:40:18+00:00
- **Updated**: 2020-01-30 20:40:18+00:00
- **Authors**: Linwei Ye, Zhi Liu, Yang Wang
- **Comment**: 12 pages, accepted for publication in IEEE Transactions on Multimedia
- **Journal**: None
- **Summary**: We consider referring image segmentation. It is a problem at the intersection of computer vision and natural language understanding. Given an input image and a referring expression in the form of a natural language sentence, the goal is to segment the object of interest in the image referred by the linguistic query. To this end, we propose a dual convolutional LSTM (ConvLSTM) network to tackle this problem. Our model consists of an encoder network and a decoder network, where ConvLSTM is used in both encoder and decoder networks to capture spatial and sequential information. The encoder network extracts visual and linguistic features for each word in the expression sentence, and adopts an attention mechanism to focus on words that are more informative in the multimodal interaction. The decoder network integrates the features generated by the encoder network at multiple levels as its input and produces the final precise segmentation mask. Experimental results on four challenging datasets demonstrate that the proposed network achieves superior segmentation performance compared with other state-of-the-art methods.



### Unsupervised Gaze Prediction in Egocentric Videos by Energy-based Surprise Modeling
- **Arxiv ID**: http://arxiv.org/abs/2001.11580v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.11580v2)
- **Published**: 2020-01-30 21:52:38+00:00
- **Updated**: 2021-04-29 06:15:53+00:00
- **Authors**: Sathyanarayanan N. Aakur, Arunkumar Bagavathi
- **Comment**: To appear at VISAP2021. More details:
  https://saakur.github.io/Projects/GazePrediction/
- **Journal**: None
- **Summary**: Egocentric perception has grown rapidly with the advent of immersive computing devices. Human gaze prediction is an important problem in analyzing egocentric videos and has primarily been tackled through either saliency-based modeling or highly supervised learning. We quantitatively analyze the generalization capabilities of supervised, deep learning models on the egocentric gaze prediction task on unseen, out-of-domain data. We find that their performance is highly dependent on the training data and is restricted to the domains specified in the training annotations. In this work, we tackle the problem of jointly predicting human gaze points and temporal segmentation of egocentric videos without using any training data. We introduce an unsupervised computational model that draws inspiration from cognitive psychology models of event perception. We use Grenander's pattern theory formalism to represent spatial-temporal features and model surprise as a mechanism to predict gaze fixation points. Extensive evaluation on two publicly available datasets - GTEA and GTEA+ datasets-shows that the proposed model can significantly outperform all unsupervised baselines and some supervised gaze prediction baselines. Finally, we show that the model can also temporally segment egocentric videos with a performance comparable to more complex, fully supervised deep learning baselines.



### Ellipse R-CNN: Learning to Infer Elliptical Object from Clustering and Occlusion
- **Arxiv ID**: http://arxiv.org/abs/2001.11584v2
- **DOI**: 10.1109/TIP.2021.3050673
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2001.11584v2)
- **Published**: 2020-01-30 22:04:54+00:00
- **Updated**: 2020-11-14 21:05:09+00:00
- **Authors**: Wenbo Dong, Pravakar Roy, Cheng Peng, Volkan Isler
- **Comment**: 18 pages, 20 figures, 7 tables
- **Journal**: None
- **Summary**: Images of heavily occluded objects in cluttered scenes, such as fruit clusters in trees, are hard to segment. To further retrieve the 3D size and 6D pose of each individual object in such cases, bounding boxes are not reliable from multiple views since only a little portion of the object's geometry is captured. We introduce the first CNN-based ellipse detector, called Ellipse R-CNN, to represent and infer occluded objects as ellipses. We first propose a robust and compact ellipse regression based on the Mask R-CNN architecture for elliptical object detection. Our method can infer the parameters of multiple elliptical objects even they are occluded by other neighboring objects. For better occlusion handling, we exploit refined feature regions for the regression stage, and integrate the U-Net structure for learning different occlusion patterns to compute the final detection score. The correctness of ellipse regression is validated through experiments performed on synthetic data of clustered ellipses. We further quantitatively and qualitatively demonstrate that our approach outperforms the state-of-the-art model (i.e., Mask R-CNN followed by ellipse fitting) and its three variants on both synthetic and real datasets of occluded and clustered elliptical objects.



### Path Planning in Dynamic Environments using Generative RNNs and Monte Carlo Tree Search
- **Arxiv ID**: http://arxiv.org/abs/2001.11597v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2001.11597v1)
- **Published**: 2020-01-30 22:46:37+00:00
- **Updated**: 2020-01-30 22:46:37+00:00
- **Authors**: Stuart Eiffert, He Kong, Navid Pirmarzdashti, Salah Sukkarieh
- **Comment**: Accepted to IEEE International Conference on Robotics and Automation
  (ICRA) 2020, video available at https://youtu.be/vBPKiqtCYRU
- **Journal**: None
- **Summary**: State of the art methods for robotic path planning in dynamic environments, such as crowds or traffic, rely on hand crafted motion models for agents. These models often do not reflect interactions of agents in real world scenarios. To overcome this limitation, this paper proposes an integrated path planning framework using generative Recurrent Neural Networks within a Monte Carlo Tree Search (MCTS). This approach uses a learnt model of social response to predict crowd dynamics during planning across the action space. This extends our recent work using generative RNNs to learn the relationship between planned robotic actions and the likely response of a crowd. We show that the proposed framework can considerably improve motion prediction accuracy during interactions, allowing more effective path planning. The performance of our method is compared in simulation with existing methods for collision avoidance in a crowd of pedestrians, demonstrating the ability to control future states of nearby individuals. We also conduct preliminary real world tests to validate the effectiveness of our method.



### UAV Autonomous Localization using Macro-Features Matching with a CAD Model
- **Arxiv ID**: http://arxiv.org/abs/2001.11610v1
- **DOI**: 10.3390/s20030743
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.11610v1)
- **Published**: 2020-01-30 23:49:15+00:00
- **Updated**: 2020-01-30 23:49:15+00:00
- **Authors**: Akkas Haque, Ahmed Elsaharti, Tarek Elderini, Mohamed Atef Elsaharty, Jeremiah Neubert
- **Comment**: None
- **Journal**: Sensors 2020, 20, 743
- **Summary**: Research in the field of autonomous Unmanned Aerial Vehicles (UAVs) has significantly advanced in recent years, mainly due to their relevance in a large variety of commercial, industrial, and military applications. However, UAV navigation in GPS-denied environments continues to be a challenging problem that has been tackled in recent research through sensor-based approaches. This paper presents a novel offline, portable, real-time in-door UAV localization technique that relies on macro-feature detection and matching. The proposed system leverages the support of machine learning, traditional computer vision techniques, and pre-existing knowledge of the environment. The main contribution of this work is the real-time creation of a macro-feature description vector from the UAV captured images which are simultaneously matched with an offline pre-existing vector from a Computer-Aided Design (CAD) model. This results in a quick UAV localization within the CAD model. The effectiveness and accuracy of the proposed system were evaluated through simulations and experimental prototype implementation. Final results reveal the algorithm's low computational burden as well as its ease of deployment in GPS-denied environments.



### Search for Better Students to Learn Distilled Knowledge
- **Arxiv ID**: http://arxiv.org/abs/2001.11612v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2001.11612v1)
- **Published**: 2020-01-30 23:55:15+00:00
- **Updated**: 2020-01-30 23:55:15+00:00
- **Authors**: Jindong Gu, Volker Tresp
- **Comment**: None
- **Journal**: 24th European Conference on Artificial Intelligence (ECAI), 2020
- **Summary**: Knowledge Distillation, as a model compression technique, has received great attention. The knowledge of a well-performed teacher is distilled to a student with a small architecture. The architecture of the small student is often chosen to be similar to their teacher's, with fewer layers or fewer channels, or both. However, even with the same number of FLOPs or parameters, the students with different architecture can achieve different generalization ability. The configuration of a student architecture requires intensive network architecture engineering. In this work, instead of designing a good student architecture manually, we propose to search for the optimal student automatically. Based on L1-norm optimization, a subgraph from the teacher network topology graph is selected as a student, the goal of which is to minimize the KL-divergence between student's and teacher's outputs. We verify the proposal on CIFAR10 and CIFAR100 datasets. The empirical experiments show that the learned student architecture achieves better performance than ones specified manually. We also visualize and understand the architecture of the found student.



