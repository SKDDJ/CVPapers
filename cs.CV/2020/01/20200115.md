# Arxiv Papers in cs.CV on 2020-01-15
### Proposal Learning for Semi-Supervised Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2001.05086v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.05086v2)
- **Published**: 2020-01-15 00:06:59+00:00
- **Updated**: 2020-04-23 18:13:23+00:00
- **Authors**: Peng Tang, Chetan Ramaiah, Yan Wang, Ran Xu, Caiming Xiong
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we focus on semi-supervised object detection to boost performance of proposal-based object detectors (a.k.a. two-stage object detectors) by training on both labeled and unlabeled data. However, it is non-trivial to train object detectors on unlabeled data due to the unavailability of ground truth labels. To address this problem, we present a proposal learning approach to learn proposal features and predictions from both labeled and unlabeled data. The approach consists of a self-supervised proposal learning module and a consistency-based proposal learning module. In the self-supervised proposal learning module, we present a proposal location loss and a contrastive loss to learn context-aware and noise-robust proposal features respectively. In the consistency-based proposal learning module, we apply consistency losses to both bounding box classification and regression predictions of proposals to learn noise-robust proposal features and predictions. Our approach enjoys the following benefits: 1) encouraging more context information to delivered in the proposals learning procedure; 2) noisy proposal features and enforcing consistency to allow noise-robust object detection; 3) building a general and high-performance semi-supervised object detection framework, which can be easily adapted to proposal-based object detectors with different backbone architectures. Experiments are conducted on the COCO dataset with all available labeled and unlabeled data. Results demonstrate that our approach consistently improves the performance of fully-supervised baselines. In particular, after combining with data distillation, our approach improves AP by about 2.0% and 0.9% on average compared to fully-supervised baselines and data distillation baselines respectively.



### Lightweight 3D Human Pose Estimation Network Training Using Teacher-Student Learning
- **Arxiv ID**: http://arxiv.org/abs/2001.05097v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.05097v1)
- **Published**: 2020-01-15 01:31:01+00:00
- **Updated**: 2020-01-15 01:31:01+00:00
- **Authors**: Dong-Hyun Hwang, Suntae Kim, Nicolas Monet, Hideki Koike, Soonmin Bae
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: We present MoVNect, a lightweight deep neural network to capture 3D human pose using a single RGB camera. To improve the overall performance of the model, we apply the teacher-student learning method based knowledge distillation to 3D human pose estimation. Real-time post-processing makes the CNN output yield temporally stable 3D skeletal information, which can be used in applications directly. We implement a 3D avatar application running on mobile in real-time to demonstrate that our network achieves both high accuracy and fast inference time. Extensive evaluations show the advantages of our lightweight model with the proposed training method over previous 3D pose estimation methods on the Human3.6M dataset and mobile devices.



### Filter Grafting for Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2001.05868v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2001.05868v3)
- **Published**: 2020-01-15 03:18:57+00:00
- **Updated**: 2020-02-26 13:42:25+00:00
- **Authors**: Fanxu Meng, Hao Cheng, Ke Li, Zhixin Xu, Rongrong Ji, Xing Sun, Gaungming Lu
- **Comment**: Accepted by CVPR2020
- **Journal**: None
- **Summary**: This paper proposes a new learning paradigm called filter grafting, which aims to improve the representation capability of Deep Neural Networks (DNNs). The motivation is that DNNs have unimportant (invalid) filters (e.g., l1 norm close to 0). These filters limit the potential of DNNs since they are identified as having little effect on the network. While filter pruning removes these invalid filters for efficiency consideration, filter grafting re-activates them from an accuracy boosting perspective. The activation is processed by grafting external information (weights) into invalid filters. To better perform the grafting process, we develop an entropy-based criterion to measure the information of filters and an adaptive weighting strategy for balancing the grafted information among networks. After the grafting operation, the network has very few invalid filters compared with its untouched state, enpowering the model with more representation capacity. We also perform extensive experiments on the classification and recognition tasks to show the superiority of our method. For example, the grafted MobileNetV2 outperforms the non-grafted MobileNetV2 by about 7 percent on CIFAR-100 dataset. Code is available at https://github.com/fxmeng/filter-grafting.git.



### Learning multiview 3D point cloud registration
- **Arxiv ID**: http://arxiv.org/abs/2001.05119v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2001.05119v2)
- **Published**: 2020-01-15 03:42:14+00:00
- **Updated**: 2020-03-31 07:53:36+00:00
- **Authors**: Zan Gojcic, Caifa Zhou, Jan D. Wegner, Leonidas J. Guibas, Tolga Birdal
- **Comment**: CVPR2020 - Camera Ready
- **Journal**: None
- **Summary**: We present a novel, end-to-end learnable, multiview 3D point cloud registration algorithm. Registration of multiple scans typically follows a two-stage pipeline: the initial pairwise alignment and the globally consistent refinement. The former is often ambiguous due to the low overlap of neighboring point clouds, symmetries and repetitive scene parts. Therefore, the latter global refinement aims at establishing the cyclic consistency across multiple scans and helps in resolving the ambiguous cases. In this paper we propose, to the best of our knowledge, the first end-to-end algorithm for joint learning of both parts of this two-stage problem. Experimental evaluation on well accepted benchmark datasets shows that our approach outperforms the state-of-the-art by a significant margin, while being end-to-end trainable and computationally less costly. Moreover, we present detailed analysis and an ablation study that validate the novel components of our approach. The source code and pretrained models are publicly available under https://github.com/zgojcic/3D_multiview_reg.



### The Synthinel-1 dataset: a collection of high resolution synthetic overhead imagery for building segmentation
- **Arxiv ID**: http://arxiv.org/abs/2001.05130v1
- **DOI**: 10.1109/wacv45572.2020.9093339
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2001.05130v1)
- **Published**: 2020-01-15 04:30:45+00:00
- **Updated**: 2020-01-15 04:30:45+00:00
- **Authors**: Fanjie Kong, Bohao Huang, Kyle Bradbury, Jordan M. Malof
- **Comment**: Preprint of paper accepted for publication at Winter Conference on
  Applications of Computer Vision (WACV) 2020
- **Journal**: F. Kong, The Synthinel-1 dataset: a collection of high resolution
  synthetic overhead imagery for building segmentation, 2020, IEEE Winter
  Conference on Applications of Computer Vision (WACV)
- **Summary**: Recently deep learning - namely convolutional neural networks (CNNs) - have yielded impressive performance for the task of building segmentation on large overhead (e.g., satellite) imagery benchmarks. However, these benchmark datasets only capture a small fraction of the variability present in real-world overhead imagery, limiting the ability to properly train, or evaluate, models for real-world application. Unfortunately, developing a dataset that captures even a small fraction of real-world variability is typically infeasible due to the cost of imagery, and manual pixel-wise labeling of the imagery. In this work we develop an approach to rapidly and cheaply generate large and diverse virtual environments from which we can capture synthetic overhead imagery for training segmentation CNNs. Using this approach, generate and publicly-release a collection of synthetic overhead imagery - termed Synthinel-1 with full pixel-wise building labels. We use several benchmark dataset to demonstrate that Synthinel-1 is consistently beneficial when used to augment real-world training imagery, especially when CNNs are tested on novel geographic locations or conditions.



### CDGAN: Cyclic Discriminative Generative Adversarial Networks for Image-to-Image Transformation
- **Arxiv ID**: http://arxiv.org/abs/2001.05489v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2001.05489v2)
- **Published**: 2020-01-15 05:12:32+00:00
- **Updated**: 2021-11-27 02:09:41+00:00
- **Authors**: Kancharagunta Kishan Babu, Shiv Ram Dubey
- **Comment**: Journal of Visual Communication and Image Representation, 2022
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) have facilitated a new direction to tackle the image-to-image transformation problem. Different GANs use generator and discriminator networks with different losses in the objective function. Still there is a gap to fill in terms of both the quality of the generated images and close to the ground truth images. In this work, we introduce a new Image-to-Image Transformation network named Cyclic Discriminative Generative Adversarial Networks (CDGAN) that fills the above mentioned gaps. The proposed CDGAN generates high quality and more realistic images by incorporating the additional discriminator networks for cycled images in addition to the original architecture of the CycleGAN. The proposed CDGAN is tested over three image-to-image transformation datasets. The quantitative and qualitative results are analyzed and compared with the state-of-the-art methods. The proposed CDGAN method outperforms the state-of-the-art methods when compared over the three baseline Image-to-Image transformation datasets. The code is available at https://github.com/KishanKancharagunta/CDGAN.



### Driver Safety Development Real Time Driver Drowsiness Detection System Based on Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2001.05137v3
- **DOI**: 10.1007/s42979-020-00306-9
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2001.05137v3)
- **Published**: 2020-01-15 05:38:24+00:00
- **Updated**: 2021-05-28 04:30:09+00:00
- **Authors**: Maryam Hashemi, Alireza Mirrashid, Aliasghar Beheshti Shirazi
- **Comment**: Hashemi, M., Mirrashid, A. & Beheshti Shirazi, A. Driver Safety
  Development: Real-Time Driver Drowsiness Detection System Based on
  Convolutional Neural Network. SN COMPUT. SCI. 1, 289 (2020).
  https://doi.org/10.1007/s42979-020-00306-9
- **Journal**: None
- **Summary**: This paper focuses on the challenge of driver safety on the road and presents a novel system for driver drowsiness detection. In this system, to detect the falling sleep state of the driver as the sign of drowsiness, Convolutional Neural Networks (CNN) are used with regarding the two goals of real-time application, including high accuracy and fastness. Three networks introduced as a potential network for eye status classifcation in which one of them is a Fully Designed Neural Network (FD-NN) and others use Transfer Learning in VGG16 and VGG19 with extra designed layers (TL-VGG). Lack of an available and accurate eye dataset strongly feels in the area of eye closure detection. Therefore, a new comprehensive dataset proposed. The experimental results show the high accuracy and low computational complexity of the eye closure estimation and the ability of the proposed framework on drowsiness detection.



### Tethered Aerial Visual Assistance
- **Arxiv ID**: http://arxiv.org/abs/2001.06347v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2001.06347v1)
- **Published**: 2020-01-15 06:41:04+00:00
- **Updated**: 2020-01-15 06:41:04+00:00
- **Authors**: Xuesu Xiao, Jan Dufek, Robin R. Murphy
- **Comment**: Submitted to special issue of "Field and Service Robotics" of the
  Journal of Field Robotics (JFR). arXiv admin note: text overlap with
  arXiv:1904.00078
- **Journal**: None
- **Summary**: In this paper, an autonomous tethered Unmanned Aerial Vehicle (UAV) is developed into a visual assistant in a marsupial co-robots team, collaborating with a tele-operated Unmanned Ground Vehicle (UGV) for robot operations in unstructured or confined environments. These environments pose extreme challenges to the remote tele-operator due to the lack of sufficient situational awareness, mostly caused by the unstructuredness and confinement, stationary and limited field-of-view and lack of depth perception from the robot's onboard cameras. To overcome these problems, a secondary tele-operated robot is used in current practices, who acts as a visual assistant and provides external viewpoints to overcome the perceptual limitations of the primary robot's onboard sensors. However, a second tele-operated robot requires extra manpower and teamwork demand between primary and secondary operators. The manually chosen viewpoints tend to be subjective and sub-optimal. Considering these intricacies, we develop an autonomous tethered aerial visual assistant in place of the secondary tele-operated robot and operator, to reduce human robot ratio from 2:2 to 1:2. Using a fundamental viewpoint quality theory, a formal risk reasoning framework, and a newly developed tethered motion suite, our visual assistant is able to autonomously navigate to good-quality viewpoints in a risk-aware manner through unstructured or confined spaces with a tether. The developed marsupial co-robots team could improve tele-operation efficiency in nuclear operations, bomb squad, disaster robots, and other domains with novel tasks or highly occluded environments, by reducing manpower and teamwork demand, and achieving better visual assistance quality with trustworthy risk-aware motion.



### Relevance Prediction from Eye-movements Using Semi-interpretable Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2001.05152v1
- **DOI**: 10.1145/3343413.3377960
- **Categories**: **cs.HC**, cs.CV, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/2001.05152v1)
- **Published**: 2020-01-15 07:02:14+00:00
- **Updated**: 2020-01-15 07:02:14+00:00
- **Authors**: Nilavra Bhattacharya, Somnath Rakshit, Jacek Gwizdka, Paul Kogut
- **Comment**: None
- **Journal**: 2020 Conference on Human Information Interaction and Retrieval
  (CHIIR '20), March 14--18, 2020, Vancouver, BC, Canada
- **Summary**: We propose an image-classification method to predict the perceived-relevance of text documents from eye-movements. An eye-tracking study was conducted where participants read short news articles, and rated them as relevant or irrelevant for answering a trigger question. We encode participants' eye-movement scanpaths as images, and then train a convolutional neural network classifier using these scanpath images. The trained classifier is used to predict participants' perceived-relevance of news articles from the corresponding scanpath images. This method is content-independent, as the classifier does not require knowledge of the screen-content, or the user's information-task. Even with little data, the image classifier can predict perceived-relevance with up to 80% accuracy. When compared to similar eye-tracking studies from the literature, this scanpath image classification method outperforms previously reported metrics by appreciable margins. We also attempt to interpret how the image classifier differentiates between scanpaths on relevant and irrelevant documents.



### Extending Class Activation Mapping Using Gaussian Receptive Field
- **Arxiv ID**: http://arxiv.org/abs/2001.05153v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.05153v1)
- **Published**: 2020-01-15 07:04:07+00:00
- **Updated**: 2020-01-15 07:04:07+00:00
- **Authors**: Bum Jun Kim, Gyogwon Koo, Hyeyeon Choi, Sang Woo Kim
- **Comment**: 7 pages, 5 figures
- **Journal**: None
- **Summary**: This paper addresses the visualization task of deep learning models. To improve Class Activation Mapping (CAM) based visualization method, we offer two options. First, we propose Gaussian upsampling, an improved upsampling method that can reflect the characteristics of deep learning models. Second, we identify and modify unnatural terms in the mathematical derivation of the existing CAM studies. Based on two options, we propose Extended-CAM, an advanced CAM-based visualization method, which exhibits improved theoretical properties. Experimental results show that Extended-CAM provides more accurate visualization than the existing methods.



### OpenHI2 -- Open source histopathological image platform
- **Arxiv ID**: http://arxiv.org/abs/2001.05158v1
- **DOI**: None
- **Categories**: **q-bio.QM**, cs.CV, cs.NI, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2001.05158v1)
- **Published**: 2020-01-15 07:29:29+00:00
- **Updated**: 2020-01-15 07:29:29+00:00
- **Authors**: Pargorn Puttapirat, Haichuan Zhang, Jingyi Deng, Yuxin Dong, Jiangbo Shi, Hongyu He, Zeyu Gao, Chunbao Wang, Xiangrong Zhang, Chen Li
- **Comment**: Preprint version accepted to AIPath2019 workshop at BIBM2019. 6
  pages, 3 figures, 2 tables
- **Journal**: None
- **Summary**: Transition from conventional to digital pathology requires a new category of biomedical informatic infrastructure which could facilitate delicate pathological routine. Pathological diagnoses are sensitive to many external factors and is known to be subjective. Only systems that can meet strict requirements in pathology would be able to run along pathological routines and eventually digitized the study area, and the developed platform should comply with existing pathological routines and international standards. Currently, there are a number of available software tools which can perform histopathological tasks including virtual slide viewing, annotating, and basic image analysis, however, none of them can serve as a digital platform for pathology. Here we describe OpenHI2, an enhanced version Open Histopathological Image platform which is capable of supporting all basic pathological tasks and file formats; ready to be deployed in medical institutions on a standard server environment or cloud computing infrastructure. In this paper, we also describe the development decisions for the platform and propose solutions to overcome technical challenges so that OpenHI2 could be used as a platform for histopathological images. Further addition can be made to the platform since each component is modularized and fully documented. OpenHI2 is free, open-source, and available at https://gitlab.com/BioAI/OpenHI.



### Pose-Assisted Multi-Camera Collaboration for Active Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/2001.05161v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MA, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2001.05161v1)
- **Published**: 2020-01-15 07:49:49+00:00
- **Updated**: 2020-01-15 07:49:49+00:00
- **Authors**: Jing Li, Jing Xu, Fangwei Zhong, Xiangyu Kong, Yu Qiao, Yizhou Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Active Object Tracking (AOT) is crucial to many visionbased applications, e.g., mobile robot, intelligent surveillance. However, there are a number of challenges when deploying active tracking in complex scenarios, e.g., target is frequently occluded by obstacles. In this paper, we extend the single-camera AOT to a multi-camera setting, where cameras tracking a target in a collaborative fashion. To achieve effective collaboration among cameras, we propose a novel Pose-Assisted Multi-Camera Collaboration System, which enables a camera to cooperate with the others by sharing camera poses for active object tracking. In the system, each camera is equipped with two controllers and a switcher: The vision-based controller tracks targets based on observed images. The pose-based controller moves the camera in accordance to the poses of the other cameras. At each step, the switcher decides which action to take from the two controllers according to the visibility of the target. The experimental results demonstrate that our system outperforms all the baselines and is capable of generalizing to unseen environments. The code and demo videos are available on our website https://sites.google.com/view/pose-assistedcollaboration.



### Ensemble based discriminative models for Visual Dialog Challenge 2018
- **Arxiv ID**: http://arxiv.org/abs/2001.05865v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2001.05865v1)
- **Published**: 2020-01-15 08:20:54+00:00
- **Updated**: 2020-01-15 08:20:54+00:00
- **Authors**: Shubham Agarwal, Raghav Goyal
- **Comment**: Rankings: https://visualdialog.org/challenge/2018#winners
- **Journal**: None
- **Summary**: This manuscript describes our approach for the Visual Dialog Challenge 2018. We use an ensemble of three discriminative models with different encoders and decoders for our final submission. Our best performing model on 'test-std' split achieves the NDCG score of 55.46 and the MRR value of 63.77, securing third position in the challenge.



### Uncertainty-Aware Multi-Shot Knowledge Distillation for Image-Based Object Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2001.05197v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.05197v2)
- **Published**: 2020-01-15 09:39:05+00:00
- **Updated**: 2020-01-21 17:21:07+00:00
- **Authors**: Xin Jin, Cuiling Lan, Wenjun Zeng, Zhibo Chen
- **Comment**: Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI-20)
- **Journal**: None
- **Summary**: Object re-identification (re-id) aims to identify a specific object across times or camera views, with the person re-id and vehicle re-id as the most widely studied applications. Re-id is challenging because of the variations in viewpoints, (human) poses, and occlusions. Multi-shots of the same object can cover diverse viewpoints/poses and thus provide more comprehensive information. In this paper, we propose exploiting the multi-shots of the same identity to guide the feature learning of each individual image. Specifically, we design an Uncertainty-aware Multi-shot Teacher-Student (UMTS) Network. It consists of a teacher network (T-net) that learns the comprehensive features from multiple images of the same object, and a student network (S-net) that takes a single image as input. In particular, we take into account the data dependent heteroscedastic uncertainty for effectively transferring the knowledge from the T-net to S-net. To the best of our knowledge, we are the first to make use of multi-shots of an object in a teacher-student learning manner for effectively boosting the single image based re-id. We validate the effectiveness of our approach on the popular vehicle re-id and person re-id datasets. In inference, the S-net alone significantly outperforms the baselines and achieves the state-of-the-art performance.



### Evaluating image matching methods for book cover identification
- **Arxiv ID**: http://arxiv.org/abs/2001.05200v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2001.05200v1)
- **Published**: 2020-01-15 09:52:38+00:00
- **Updated**: 2020-01-15 09:52:38+00:00
- **Authors**: Rabie Hachemi, Ikram Achar, Biasi Wiga, Mahfoud Sidi Ali Mebarek
- **Comment**: None
- **Journal**: None
- **Summary**: Humans are capable of identifying a book only by looking at its cover, but how can computers do the same? In this paper, we explore different feature detectors and matching methods for book cover identification, and compare their performances in terms of both speed and accuracy. This will allow, for example, libraries to develop interactive services based on cover book picture. Only one single image of a cover book needs to be available through a database. Tests have been performed by taking into account different transformations of each book cover image. Encouraging results have been achieved.



### Everybody's Talkin': Let Me Talk as You Want
- **Arxiv ID**: http://arxiv.org/abs/2001.05201v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2001.05201v1)
- **Published**: 2020-01-15 09:54:23+00:00
- **Updated**: 2020-01-15 09:54:23+00:00
- **Authors**: Linsen Song, Wayne Wu, Chen Qian, Ran He, Chen Change Loy
- **Comment**: Technical report. Project page:
  https://wywu.github.io/projects/EBT/EBT.html
- **Journal**: None
- **Summary**: We present a method to edit a target portrait footage by taking a sequence of audio as input to synthesize a photo-realistic video. This method is unique because it is highly dynamic. It does not assume a person-specific rendering network yet capable of translating arbitrary source audio into arbitrary video output. Instead of learning a highly heterogeneous and nonlinear mapping from audio to the video directly, we first factorize each target video frame into orthogonal parameter spaces, i.e., expression, geometry, and pose, via monocular 3D face reconstruction. Next, a recurrent network is introduced to translate source audio into expression parameters that are primarily related to the audio content. The audio-translated expression parameters are then used to synthesize a photo-realistic human subject in each video frame, with the movement of the mouth regions precisely mapped to the source audio. The geometry and pose parameters of the target human portrait are retained, therefore preserving the context of the original video footage. Finally, we introduce a novel video rendering network and a dynamic programming method to construct a temporally coherent and photo-realistic video. Extensive experiments demonstrate the superiority of our method over existing approaches. Our method is end-to-end learnable and robust to voice variations in the source audio.



### Structured GANs
- **Arxiv ID**: http://arxiv.org/abs/2001.05216v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2001.05216v1)
- **Published**: 2020-01-15 10:25:39+00:00
- **Updated**: 2020-01-15 10:25:39+00:00
- **Authors**: Irad Peleg, Lior Wolf
- **Comment**: None
- **Journal**: None
- **Summary**: We present Generative Adversarial Networks (GANs), in which the symmetric property of the generated images is controlled. This is obtained through the generator network's architecture, while the training procedure and the loss remain the same. The symmetric GANs are applied to face image synthesis in order to generate novel faces with a varying amount of symmetry. We also present an unsupervised face rotation capability, which is based on the novel notion of one-shot fine tuning.



### Moving Objects Detection with a Moving Camera: A Comprehensive Review
- **Arxiv ID**: http://arxiv.org/abs/2001.05238v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.05238v1)
- **Published**: 2020-01-15 11:12:51+00:00
- **Updated**: 2020-01-15 11:12:51+00:00
- **Authors**: Marie-Neige Chapel, Thierry Bouwmans
- **Comment**: Submitted to Computer Science Review
- **Journal**: None
- **Summary**: During about 30 years, a lot of research teams have worked on the big challenge of detection of moving objects in various challenging environments. First applications concern static cameras but with the rise of the mobile sensors studies on moving cameras have emerged over time. In this survey, we propose to identify and categorize the different existing methods found in the literature. For this purpose, we propose to classify these methods according to the choose of the scene representation: one plane or several parts. Inside these two categories, the methods are grouped according to eight different approaches: panoramic background subtraction, dual cameras, motion compensation, subspace segmentation, motion segmentation, plane+parallax, multi planes and split image in blocks. A reminder of methods for static cameras is provided as well as the challenges with both static and moving cameras. Publicly available datasets and evaluation metrics are also surveyed in this paper.



### DeepSUM++: Non-local Deep Neural Network for Super-Resolution of Unregistered Multitemporal Images
- **Arxiv ID**: http://arxiv.org/abs/2001.06342v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2001.06342v1)
- **Published**: 2020-01-15 11:17:19+00:00
- **Updated**: 2020-01-15 11:17:19+00:00
- **Authors**: Andrea Bordone Molini, Diego Valsesia, Giulia Fracastoro, Enrico Magli
- **Comment**: arXiv admin note: text overlap with arXiv:1907.06490
- **Journal**: None
- **Summary**: Deep learning methods for super-resolution of a remote sensing scene from multiple unregistered low-resolution images have recently gained attention thanks to a challenge proposed by the European Space Agency. This paper presents an evolution of the winner of the challenge, showing how incorporating non-local information in a convolutional neural network allows to exploit self-similar patterns that provide enhanced regularization of the super-resolution problem. Experiments on the dataset of the challenge show improved performance over the state-of-the-art, which does not exploit non-local information.



### Single Image Dehazing Using Ranking Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2001.05246v1
- **DOI**: 10.1109/TMM.2017.2771472
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.05246v1)
- **Published**: 2020-01-15 11:25:08+00:00
- **Updated**: 2020-01-15 11:25:08+00:00
- **Authors**: Yafei Song, Jia Li, Xiaogang Wang, Xiaowu Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Single image dehazing, which aims to recover the clear image solely from an input hazy or foggy image, is a challenging ill-posed problem. Analysing existing approaches, the common key step is to estimate the haze density of each pixel. To this end, various approaches often heuristically designed haze-relevant features. Several recent works also automatically learn the features via directly exploiting Convolutional Neural Networks (CNN). However, it may be insufficient to fully capture the intrinsic attributes of hazy images. To obtain effective features for single image dehazing, this paper presents a novel Ranking Convolutional Neural Network (Ranking-CNN). In Ranking-CNN, a novel ranking layer is proposed to extend the structure of CNN so that the statistical and structural attributes of hazy images can be simultaneously captured. By training Ranking-CNN in a well-designed manner, powerful haze-relevant features can be automatically learned from massive hazy image patches. Based on these features, haze can be effectively removed by using a haze density prediction model trained through the random forest regression. Experimental results show that our approach outperforms several previous dehazing approaches on synthetic and real-world benchmark images. Comprehensive analyses are also conducted to interpret the proposed Ranking-CNN from both the theoretical and experimental aspects.



### Towards Deep Unsupervised SAR Despeckling with Blind-Spot Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2001.05264v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2001.05264v1)
- **Published**: 2020-01-15 12:21:12+00:00
- **Updated**: 2020-01-15 12:21:12+00:00
- **Authors**: Andrea Bordone Molini, Diego Valsesia, Giulia Fracastoro, Enrico Magli
- **Comment**: None
- **Journal**: None
- **Summary**: SAR despeckling is a problem of paramount importance in remote sensing, since it represents the first step of many scene analysis algorithms. Recently, deep learning techniques have outperformed classical model-based despeckling algorithms. However, such methods require clean ground truth images for training, thus resorting to synthetically speckled optical images since clean SAR images cannot be acquired. In this paper, inspired by recent works on blind-spot denoising networks, we propose a self-supervised Bayesian despeckling method. The proposed method is trained employing only noisy images and can therefore learn features of real SAR images rather than synthetic data. We show that the performance of the proposed network is very close to the supervised training approach on synthetic data and competitive on real data.



### Correcting Decalibration of Stereo Cameras in Self-Driving Vehicles
- **Arxiv ID**: http://arxiv.org/abs/2001.05267v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4
- **Links**: [PDF](http://arxiv.org/pdf/2001.05267v1)
- **Published**: 2020-01-15 12:28:01+00:00
- **Updated**: 2020-01-15 12:28:01+00:00
- **Authors**: Jon Muhovič, Janez Perš
- **Comment**: 8 pages, 9 figures
- **Journal**: None
- **Summary**: We address the problem of optical decalibration in mobile stereo camera setups, especially in context of autonomous vehicles. In real world conditions, an optical system is subject to various sources of anticipated and unanticipated mechanical stress (vibration, rough handling, collisions). Mechanical stress changes the geometry between the cameras that make up the stereo pair, and as a consequence, the pre-calculated epipolar geometry is no longer valid. Our method is based on optimization of camera geometry parameters and plugs directly into the output of the stereo matching algorithm. Therefore, it is able to recover calibration parameters on image pairs obtained from a decalibrated stereo system with minimal use of additional computing resources. The number of successfully recovered depth pixels is used as an objective function, which we aim to maximize. Our simulation confirms that the method can run constantly in parallel to stereo estimation and thus help keep the system calibrated in real time. Results confirm that the method is able to recalibrate all the parameters except for the baseline distance, which scales the absolute depth readings. However, that scaling factor could be uniquely determined using any kind of absolute range finding methods (e.g. a single beam time-of-flight sensor).



### Morton Filters for Superior Template Protection for Iris Recognition
- **Arxiv ID**: http://arxiv.org/abs/2001.05290v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/2001.05290v1)
- **Published**: 2020-01-15 13:15:33+00:00
- **Updated**: 2020-01-15 13:15:33+00:00
- **Authors**: Kiran B. Raja, R. Raghavendra, Sushma Venkatesh, Christoph Busch
- **Comment**: None
- **Journal**: None
- **Summary**: We address the fundamental performance issues of template protection (TP) for iris verification. We base our work on the popular Bloom-Filter templates protection & address the key challenges like sub-optimal performance and low unlinkability. Specifically, we focus on cases where Bloom-filter templates results in non-ideal performance due to presence of large degradations within iris images. Iris recognition is challenged with number of occluding factors such as presence of eye-lashes within captured image, occlusion due to eyelids, low quality iris images due to motion blur. All of such degrading factors result in obtaining non-reliable iris codes & thereby provide non-ideal biometric performance. These factors directly impact the protected templates derived from iris images when classical Bloom-filters are employed. To this end, we propose and extend our earlier ideas of Morton-filters for obtaining better and reliable templates for iris. Morton filter based TP for iris codes is based on leveraging the intra and inter-class distribution by exploiting low-rank iris codes to derive the stable bits across iris images for a particular subject and also analyzing the discriminable bits across various subjects. Such low-rank non-noisy iris codes enables realizing the template protection in a superior way which not only can be used in constrained setting, but also in relaxed iris imaging. We further extend the work to analyze the applicability to VIS iris images by employing a large scale public iris image database - UBIRIS(v1 & v2), captured in a unconstrained setting. Through a set of experiments, we demonstrate the applicability of proposed approach and vet the strengths and weakness. Yet another contribution of this work stems in assessing the security of the proposed approach where factors of Unlinkability is studied to indicate the antagonistic nature to relaxed iris imaging scenarios.



### Assessing Robustness of Deep learning Methods in Dermatological Workflow
- **Arxiv ID**: http://arxiv.org/abs/2001.05878v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2001.05878v2)
- **Published**: 2020-01-15 14:15:38+00:00
- **Updated**: 2020-03-17 13:51:35+00:00
- **Authors**: Sourav Mishra, Subhajit Chaudhury, Hideaki Imaizumi, Toshihiko Yamasaki
- **Comment**: Accepted in ACM CHIL 2020 Workshop (Oral and poster, without
  publication)
- **Journal**: None
- **Summary**: This paper aims to evaluate the suitability of current deep learning methods for clinical workflow especially by focusing on dermatology. Although deep learning methods have been attempted to get dermatologist level accuracy in several individual conditions, it has not been rigorously tested for common clinical complaints. Most projects involve data acquired in well-controlled laboratory conditions. This may not reflect regular clinical evaluation where corresponding image quality is not always ideal. We test the robustness of deep learning methods by simulating non-ideal characteristics on user submitted images of ten classes of diseases. Assessing via imitated conditions, we have found the overall accuracy to drop and individual predictions change significantly in many cases despite of robust training.



### A Method for Estimating Reflectance map and Material using Deep Learning with Synthetic Dataset
- **Arxiv ID**: http://arxiv.org/abs/2001.05372v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.05372v1)
- **Published**: 2020-01-15 15:25:08+00:00
- **Updated**: 2020-01-15 15:25:08+00:00
- **Authors**: Mingi Lim, Sung-eui Yoon
- **Comment**: 8 pages, 8 figures
- **Journal**: None
- **Summary**: The process of decomposing target images into their internal properties is a difficult task due to the inherent ill-posed nature of the problem. The lack of data required to train a network is a one of the reasons why the decomposing appearance task is difficult. In this paper, we propose a deep learning-based reflectance map prediction system for material estimation of target objects in the image, so as to alleviate the ill-posed problem that occurs in this image decomposition operation. We also propose a network architecture for Bidirectional Reflectance Distribution Function (BRDF) parameter estimation, environment map estimation. We also use synthetic data to solve the lack of data problems. We get out of the previously proposed Deep Learning-based network architecture for reflectance map, and we newly propose to use conditional Generative Adversarial Network (cGAN) structures for estimating the reflectance map, which enables better results in many applications. To improve the efficiency of learning in this structure, we newly utilized the loss function using the normal map of the target object.



### 3D Object Segmentation for Shelf Bin Picking by Humanoid with Deep Learning and Occupancy Voxel Grid Map
- **Arxiv ID**: http://arxiv.org/abs/2001.05406v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2001.05406v2)
- **Published**: 2020-01-15 16:20:46+00:00
- **Updated**: 2020-01-16 08:54:39+00:00
- **Authors**: Kentaro Wada, Masaki Murooka, Kei Okada, Masayuki Inaba
- **Comment**: 6 pages, 9 figures, IEEE-RAS International Conference on Humanoid
  Robots 2016
- **Journal**: None
- **Summary**: Picking objects in a narrow space such as shelf bins is an important task for humanoid to extract target object from environment. In those situations, however, there are many occlusions between the camera and objects, and this makes it difficult to segment the target object three dimensionally because of the lack of three dimentional sensor inputs. We address this problem with accumulating segmentation result with multiple camera angles, and generating voxel model of the target object. Our approach consists of two components: first is object probability prediction for input image with convolutional networks, and second is generating voxel grid map which is designed for object segmentation. We evaluated the method with the picking task experiment for target objects in narrow shelf bins. Our method generates dense 3D object segments even with occlusions, and the real robot successfuly picked target objects from the narrow space.



### Show, Recall, and Tell: Image Captioning with Recall Mechanism
- **Arxiv ID**: http://arxiv.org/abs/2001.05876v3
- **DOI**: 10.1609/aaai.v34i07.6898
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2001.05876v3)
- **Published**: 2020-01-15 16:32:51+00:00
- **Updated**: 2021-03-12 05:00:56+00:00
- **Authors**: Li Wang, Zechen Bai, Yonghua Zhang, Hongtao Lu
- **Comment**: Published in AAAI 2020
- **Journal**: None
- **Summary**: Generating natural and accurate descriptions in image cap-tioning has always been a challenge. In this paper, we pro-pose a novel recall mechanism to imitate the way human con-duct captioning. There are three parts in our recall mecha-nism : recall unit, semantic guide (SG) and recalled-wordslot (RWS). Recall unit is a text-retrieval module designedto retrieve recalled words for images. SG and RWS are de-signed for the best use of recalled words. SG branch cangenerate a recalled context, which can guide the process ofgenerating caption. RWS branch is responsible for copyingrecalled words to the caption. Inspired by pointing mecha-nism in text summarization, we adopt a soft switch to balancethe generated-word probabilities between SG and RWS. Inthe CIDEr optimization step, we also introduce an individualrecalled-word reward (WR) to boost training. Our proposedmethods (SG+RWS+WR) achieve BLEU-4 / CIDEr / SPICEscores of 36.6 / 116.9 / 21.3 with cross-entropy loss and 38.7 /129.1 / 22.4 with CIDEr optimization on MSCOCO Karpathytest split, which surpass the results of other state-of-the-artmethods.



### Deep Residual Flow for Out of Distribution Detection
- **Arxiv ID**: http://arxiv.org/abs/2001.05419v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2001.05419v3)
- **Published**: 2020-01-15 16:38:47+00:00
- **Updated**: 2020-07-19 17:44:12+00:00
- **Authors**: Ev Zisselman, Aviv Tamar
- **Comment**: None
- **Journal**: None
- **Summary**: The effective application of neural networks in the real-world relies on proficiently detecting out-of-distribution examples. Contemporary methods seek to model the distribution of feature activations in the training data for adequately distinguishing abnormalities, and the state-of-the-art method uses Gaussian distribution models. In this work, we present a novel approach that improves upon the state-of-the-art by leveraging an expressive density model based on normalizing flows. We introduce the residual flow, a novel flow architecture that learns the residual distribution from a base Gaussian distribution. Our model is general, and can be applied to any data that is approximately Gaussian. For out of distribution detection in image datasets, our approach provides a principled improvement over the state-of-the-art. Specifically, we demonstrate the effectiveness of our method in ResNet and DenseNet architectures trained on various image datasets. For example, on a ResNet trained on CIFAR-100 and evaluated on detection of out-of-distribution samples from the ImageNet dataset, holding the true positive rate (TPR) at $95\%$, we improve the true negative rate (TNR) from $56.7\%$ (current state-of-the-art) to $77.5\%$ (ours).



### Indoor Layout Estimation by 2D LiDAR and Camera Fusion
- **Arxiv ID**: http://arxiv.org/abs/2001.05422v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.05422v1)
- **Published**: 2020-01-15 16:43:35+00:00
- **Updated**: 2020-01-15 16:43:35+00:00
- **Authors**: Jieyu Li, Robert L Stevenson
- **Comment**: Fast track article for IS&T International Symposium on Electronic
  Imaging 2020: Computational Imaging XVIII
- **Journal**: None
- **Summary**: This paper presents an algorithm for indoor layout estimation and reconstruction through the fusion of a sequence of captured images and LiDAR data sets. In the proposed system, a movable platform collects both intensity images and 2D LiDAR information. Pose estimation and semantic segmentation is computed jointly by aligning the LiDAR points to line segments from the images. For indoor scenes with walls orthogonal to floor, the alignment problem is decoupled into top-down view projection and a 2D similarity transformation estimation and solved by the recursive random sample consensus (R-RANSAC) algorithm. Hypotheses can be generated, evaluated and optimized by integrating new scans as the platform moves throughout the environment. The proposed method avoids the need of extensive prior training or a cuboid layout assumption, which is more effective and practical compared to most previous indoor layout estimation methods. Multi-sensor fusion allows the capability of providing accurate depth estimation and high resolution visual information.



### UnOVOST: Unsupervised Offline Video Object Segmentation and Tracking
- **Arxiv ID**: http://arxiv.org/abs/2001.05425v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.05425v1)
- **Published**: 2020-01-15 16:49:31+00:00
- **Updated**: 2020-01-15 16:49:31+00:00
- **Authors**: Jonathon Luiten, Idil Esen Zulfikar, Bastian Leibe
- **Comment**: Accepted for publication at WACV 2020
- **Journal**: None
- **Summary**: We address Unsupervised Video Object Segmentation (UVOS), the task of automatically generating accurate pixel masks for salient objects in a video sequence and of tracking these objects consistently through time, without any input about which objects should be tracked. Towards solving this task, we present UnOVOST (Unsupervised Offline Video Object Segmentation and Tracking) as a simple and generic algorithm which is able to track and segment a large variety of objects. This algorithm builds up tracks in a number stages, first grouping segments into short tracklets that are spatio-temporally consistent, before merging these tracklets into long-term consistent object tracks based on their visual similarity. In order to achieve this we introduce a novel tracklet-based Forest Path Cutting data association algorithm which builds up a decision forest of track hypotheses before cutting this forest into paths that form long-term consistent object tracks. When evaluating our approach on the DAVIS 2017 Unsupervised dataset we obtain state-of-the-art performance with a mean J &F score of 67.9% on the val, 58% on the test-dev and 56.4% on the test-challenge benchmarks, obtaining first place in the DAVIS 2019 Unsupervised Video Object Segmentation Challenge. UnOVOST even performs competitively with many semi-supervised video object segmentation algorithms even though it is not given any input as to which objects should be tracked and segmented.



### A Two-Stream Meticulous Processing Network for Retinal Vessel Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2001.05829v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2001.05829v1)
- **Published**: 2020-01-15 17:06:10+00:00
- **Updated**: 2020-01-15 17:06:10+00:00
- **Authors**: Shaoming Zheng, Tianyang Zhang, Jiawei Zhuang, Hao Wang, Jiang Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Vessel segmentation in fundus is a key diagnostic capability in ophthalmology, and there are various challenges remained in this essential task. Early approaches indicate that it is often difficult to obtain desirable segmentation performance on thin vessels and boundary areas due to the imbalance of vessel pixels with different thickness levels. In this paper, we propose a novel two-stream Meticulous-Processing Network (MP-Net) for tackling this problem. To pay more attention to the thin vessels and boundary areas, we firstly propose an efficient hierarchical model automatically stratifies the ground-truth masks into different thickness levels. Then a novel two-stream adversarial network is introduced to use the stratification results with a balanced loss function and an integration operation to achieve a better performance, especially in thin vessels and boundary areas detecting. Our model is proved to outperform state-of-the-art methods on DRIVE, STARE, and CHASE_DB1 datasets.



### A Reference Architecture for Plausible Threat Image Projection (TIP) Within 3D X-ray Computed Tomography Volumes
- **Arxiv ID**: http://arxiv.org/abs/2001.05459v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2001.05459v1)
- **Published**: 2020-01-15 18:25:23+00:00
- **Updated**: 2020-01-15 18:25:23+00:00
- **Authors**: Qian Wang, Najla Megherbi, Toby P. Breckon
- **Comment**: Technical Report, Durham University
- **Journal**: None
- **Summary**: Threat Image Projection (TIP) is a technique used in X-ray security baggage screening systems that superimposes a threat object signature onto a benign X-ray baggage image in a plausible and realistic manner. It has been shown to be highly effective in evaluating the ongoing performance of human operators, improving their vigilance and performance on threat detection. However, with the increasing use of 3D Computed Tomography (CT) in aviation security for both hold and cabin baggage screening a significant challenge arises in extending TIP to 3D CT volumes due to the difficulty in 3D CT volume segmentation and the proper insertion location determination. In this paper, we present an approach for 3D TIP in CT volumes targeting realistic and plausible threat object insertion within 3D CT baggage images. The proposed approach consists of dual threat (source) and baggage (target) volume segmentation, particle swarm optimisation based insertion determination and metal artefact generation. In addition, we propose a TIP quality score metric to evaluate the quality of generated TIP volumes. Qualitative evaluations on real 3D CT baggage imagery show that our approach is able to generate realistic and plausible TIP which are indiscernible from real CT volumes and the TIP quality scores are consistent with human evaluations.



### EEV: A Large-Scale Dataset for Studying Evoked Expressions from Video
- **Arxiv ID**: http://arxiv.org/abs/2001.05488v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.05488v2)
- **Published**: 2020-01-15 18:59:51+00:00
- **Updated**: 2021-02-22 18:33:20+00:00
- **Authors**: Jennifer J. Sun, Ting Liu, Alan S. Cowen, Florian Schroff, Hartwig Adam, Gautam Prasad
- **Comment**: Data subset at https://github.com/google-research-datasets/eev
- **Journal**: None
- **Summary**: Videos can evoke a range of affective responses in viewers. The ability to predict evoked affect from a video, before viewers watch the video, can help in content creation and video recommendation. We introduce the Evoked Expressions from Videos (EEV) dataset, a large-scale dataset for studying viewer responses to videos. Each video is annotated at 6 Hz with 15 continuous evoked expression labels, corresponding to the facial expression of viewers who reacted to the video. We use an expression recognition model within our data collection framework to achieve scalability. In total, there are 36.7 million annotations of viewer facial reactions to 23,574 videos (1,700 hours). We use a publicly available video corpus to obtain a diverse set of video content. We establish baseline performance on the EEV dataset using an existing multimodal recurrent model. Transfer learning experiments show an improvement in performance on the LIRIS-ACCEDE video dataset when pre-trained on EEV. We hope that the size and diversity of the EEV dataset will encourage further explorations in video understanding and affective computing. A subset of EEV is released at https://github.com/google-research-datasets/eev.



### A "Network Pruning Network" Approach to Deep Model Compression
- **Arxiv ID**: http://arxiv.org/abs/2001.05545v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2001.05545v1)
- **Published**: 2020-01-15 20:38:23+00:00
- **Updated**: 2020-01-15 20:38:23+00:00
- **Authors**: Vinay Kumar Verma, Pravendra Singh, Vinay P. Namboodiri, Piyush Rai
- **Comment**: Accepted in WACV'20
- **Journal**: None
- **Summary**: We present a filter pruning approach for deep model compression, using a multitask network. Our approach is based on learning a a pruner network to prune a pre-trained target network. The pruner is essentially a multitask deep neural network with binary outputs that help identify the filters from each layer of the original network that do not have any significant contribution to the model and can therefore be pruned. The pruner network has the same architecture as the original network except that it has a multitask/multi-output last layer containing binary-valued outputs (one per filter), which indicate which filters have to be pruned. The pruner's goal is to minimize the number of filters from the original network by assigning zero weights to the corresponding output feature-maps. In contrast to most of the existing methods, instead of relying on iterative pruning, our approach can prune the network (original network) in one go and, moreover, does not require specifying the degree of pruning for each layer (and can learn it instead). The compressed model produced by our approach is generic and does not need any special hardware/software support. Moreover, augmenting with other methods such as knowledge distillation, quantization, and connection pruning can increase the degree of compression for the proposed approach. We show the efficacy of our proposed approach for classification and object detection tasks.



### Segmentation with Residual Attention U-Net and an Edge-Enhancement Approach Preserves Cell Shape Features
- **Arxiv ID**: http://arxiv.org/abs/2001.05548v1
- **DOI**: None
- **Categories**: **q-bio.QM**, cs.CV, eess.IV, I.4.6; I.4.7; I.5.1; I.5.2
- **Links**: [PDF](http://arxiv.org/pdf/2001.05548v1)
- **Published**: 2020-01-15 20:44:39+00:00
- **Updated**: 2020-01-15 20:44:39+00:00
- **Authors**: Nanyan Zhu, Chen Liu, Zakary S. Singer, Tal Danino, Andrew F. Laine, Jia Guo
- **Comment**: 7 pages, 4 figures, 1 table. Nanyan Zhu and Chen Liu share equal
  contribution and are listed as co-first authors
- **Journal**: None
- **Summary**: The ability to extrapolate gene expression dynamics in living single cells requires robust cell segmentation, and one of the challenges is the amorphous or irregularly shaped cell boundaries. To address this issue, we modified the U-Net architecture to segment cells in fluorescence widefield microscopy images and quantitatively evaluated its performance. We also proposed a novel loss function approach that emphasizes the segmentation accuracy on cell boundaries and encourages shape feature preservation. With a 97% sensitivity, 93% specificity, 91% Jaccard similarity, and 95% Dice coefficient, our proposed method called Residual Attention U-Net with edge-enhancement surpassed the state-of-the-art U-Net in segmentation performance as evaluated by the traditional metrics. More remarkably, the same proposed candidate also performed the best in terms of the preservation of valuable shape features, namely area, eccentricity, major axis length, solidity and orientation. These improvements on shape feature preservation can serve as useful assets for downstream cell tracking and quantification of changes in cell statistics or features over time.



### Supervised Segmentation of Retinal Vessel Structures Using ANN
- **Arxiv ID**: http://arxiv.org/abs/2001.05549v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2001.05549v1)
- **Published**: 2020-01-15 20:48:03+00:00
- **Updated**: 2020-01-15 20:48:03+00:00
- **Authors**: Esra Kaya, İsmail Sarıtaş, Ilker Ali Ozkan
- **Comment**: None
- **Journal**: None
- **Summary**: In this study, a supervised retina blood vessel segmentation process was performed on the green channel of the RGB image using artificial neural network (ANN). The green channel is preferred because the retinal vessel structures can be distinguished most clearly from the green channel of the RGB image. The study was performed using 20 images in the DRIVE data set which is one of the most common retina data sets known. The images went through some preprocessing stages like contrastlimited adaptive histogram equalization (CLAHE), color intensity adjustment, morphological operations and median and Gaussian filtering to obtain a good segmentation. Retinal vessel structures were highlighted with top-hat and bot-hat morphological operations and converted to binary image by using global thresholding. Then, the network was trained by the binary version of the images specified as training images in the dataset and the targets are the images segmented manually by a specialist. The average segmentation accuracy for 20 images was found as 0.9492.



### Substituting Gadolinium in Brain MRI Using DeepContrast
- **Arxiv ID**: http://arxiv.org/abs/2001.05551v1
- **DOI**: None
- **Categories**: **q-bio.QM**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2001.05551v1)
- **Published**: 2020-01-15 20:53:40+00:00
- **Updated**: 2020-01-15 20:53:40+00:00
- **Authors**: Haoran Sun, Xueqing Liu, Xinyang Feng, Chen Liu, Nanyan Zhu, Sabrina J. Gjerswold-Selleck, Hong-Jian Wei, Pavan S. Upadhyayula, Angeliki Mela, Cheng-Chia Wu, Peter D. Canoll, Andrew F. Laine, J. Thomas Vaughan, Scott A. Small, Jia Guo
- **Comment**: None
- **Journal**: The IEEE International Symposium on Biomedical Imaging (ISBI) 2020
- **Summary**: Cerebral blood volume (CBV) is a hemodynamic correlate of oxygen metabolism and reflects brain activity and function. High-resolution CBV maps can be generated using the steady-state gadolinium-enhanced MRI technique. Such a technique requires an intravenous injection of exogenous gadolinium based contrast agent (GBCA) and recent studies suggest that the GBCA can accumulate in the brain after frequent use. We hypothesize that endogenous sources of contrast might exist within the most conventional and commonly acquired structural MRI, potentially obviating the need for exogenous contrast. Here, we test this hypothesis by developing and optimizing a deep learning algorithm, which we call DeepContrast, in mice. We find that DeepContrast performs equally well as exogenous GBCA in mapping CBV of the normal brain tissue and enhancing glioblastoma. Together, these studies validate our hypothesis that a deep learning approach can potentially replace the need for GBCAs in brain MRI.



### Image Segmentation Using Deep Learning: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2001.05566v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2001.05566v5)
- **Published**: 2020-01-15 21:37:47+00:00
- **Updated**: 2020-11-15 04:51:11+00:00
- **Authors**: Shervin Minaee, Yuri Boykov, Fatih Porikli, Antonio Plaza, Nasser Kehtarnavaz, Demetri Terzopoulos
- **Comment**: None
- **Journal**: None
- **Summary**: Image segmentation is a key topic in image processing and computer vision with applications such as scene understanding, medical image analysis, robotic perception, video surveillance, augmented reality, and image compression, among many others. Various algorithms for image segmentation have been developed in the literature. Recently, due to the success of deep learning models in a wide range of vision applications, there has been a substantial amount of works aimed at developing image segmentation approaches using deep learning models. In this survey, we provide a comprehensive review of the literature at the time of this writing, covering a broad spectrum of pioneering works for semantic and instance-level segmentation, including fully convolutional pixel-labeling networks, encoder-decoder architectures, multi-scale and pyramid based approaches, recurrent networks, visual attention models, and generative models in adversarial settings. We investigate the similarity, strengths and challenges of these deep learning models, examine the most widely used datasets, report performances, and discuss promising future research directions in this area.



### VSEC-LDA: Boosting Topic Modeling with Embedded Vocabulary Selection
- **Arxiv ID**: http://arxiv.org/abs/2001.05578v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.05578v1)
- **Published**: 2020-01-15 22:16:24+00:00
- **Updated**: 2020-01-15 22:16:24+00:00
- **Authors**: Yuzhen Ding, Baoxin Li
- **Comment**: None
- **Journal**: None
- **Summary**: Topic modeling has found wide application in many problems where latent structures of the data are crucial for typical inference tasks. When applying a topic model, a relatively standard pre-processing step is to first build a vocabulary of frequent words. Such a general pre-processing step is often independent of the topic modeling stage, and thus there is no guarantee that the pre-generated vocabulary can support the inference of some optimal (or even meaningful) topic models appropriate for a given task, especially for computer vision applications involving "visual words". In this paper, we propose a new approach to topic modeling, termed Vocabulary-Selection-Embedded Correspondence-LDA (VSEC-LDA), which learns the latent model while simultaneously selecting most relevant words. The selection of words is driven by an entropy-based metric that measures the relative contribution of the words to the underlying model, and is done dynamically while the model is learned. We present three variants of VSEC-LDA and evaluate the proposed approach with experiments on both synthetic and real databases from different applications. The results demonstrate the effectiveness of built-in vocabulary selection and its importance in improving the performance of topic modeling.



