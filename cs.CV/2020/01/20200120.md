# Arxiv Papers in cs.CV on 2020-01-20
### SQuINTing at VQA Models: Introspecting VQA Models with Sub-Questions
- **Arxiv ID**: http://arxiv.org/abs/2001.06927v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2001.06927v2)
- **Published**: 2020-01-20 01:02:36+00:00
- **Updated**: 2020-06-16 17:54:16+00:00
- **Authors**: Ramprasaath R. Selvaraju, Purva Tendulkar, Devi Parikh, Eric Horvitz, Marco Ribeiro, Besmira Nushi, Ece Kamar
- **Comment**: Accepted to CVPR'20 as an Oral Presentation
- **Journal**: None
- **Summary**: Existing VQA datasets contain questions with varying levels of complexity. While the majority of questions in these datasets require perception for recognizing existence, properties, and spatial relationships of entities, a significant portion of questions pose challenges that correspond to reasoning tasks - tasks that can only be answered through a synthesis of perception and knowledge about the world, logic and / or reasoning. Analyzing performance across this distinction allows us to notice when existing VQA models have consistency issues; they answer the reasoning questions correctly but fail on associated low-level perception questions. For example, in Figure 1, models answer the complex reasoning question "Is the banana ripe enough to eat?" correctly, but fail on the associated perception question "Are the bananas mostly green or yellow?" indicating that the model likely answered the reasoning question correctly but for the wrong reason. We quantify the extent to which this phenomenon occurs by creating a new Reasoning split of the VQA dataset and collecting VQA-introspect, a new dataset1 which consists of 238K new perception questions which serve as sub questions corresponding to the set of perceptual tasks needed to effectively answer the complex reasoning questions in the Reasoning split. Our evaluation shows that state-of-the-art VQA models have comparable performance in answering perception and reasoning questions, but suffer from consistency problems. To address this shortcoming, we propose an approach called Sub-Question Importance-aware Network Tuning (SQuINT), which encourages the model to attend to the same parts of the image when answering the reasoning question and the perception sub question. We show that SQuINT improves model consistency by ~5%, also marginally improving performance on the Reasoning questions in VQA, while also displaying better attention maps.



### A Novel Image Dehazing and Assessment Method
- **Arxiv ID**: http://arxiv.org/abs/2001.06963v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.06963v1)
- **Published**: 2020-01-20 04:01:09+00:00
- **Updated**: 2020-01-20 04:01:09+00:00
- **Authors**: Saad Bin Sami, Abdul Muqeet, Humera Tariq
- **Comment**: Accepted in IBA-ICICT 2019
- **Journal**: None
- **Summary**: Images captured in hazy weather conditions often suffer from color contrast and color fidelity. This degradation is represented by transmission map which represents the amount of attenuation and airlight which represents the color of additive noise. In this paper, we have proposed a method to estimate the transmission map using haze levels instead of airlight color since there are some ambiguities in estimation of airlight. Qualitative and quantitative results of proposed method show competitiveness of the method given. In addition we have proposed two metrics which are based on statistics of natural outdoor images for assessment of haze removal algorithms.



### Impact of Data Quality on Deep Neural Network Training
- **Arxiv ID**: http://arxiv.org/abs/2002.03732v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2002.03732v1)
- **Published**: 2020-01-20 04:09:48+00:00
- **Updated**: 2020-01-20 04:09:48+00:00
- **Authors**: Subrata Goswami
- **Comment**: 5 pages, 4 tables
- **Journal**: None
- **Summary**: It is well known that data is critical for training neural networks. Lot have been written about quantities of data required to train networks well. However, there is not much publications on how data quality effects convergence of such networks. There is dearth of information on what is considered good data ( for the task ). This empirical experimental study explores some impacts of data quality. Specific results are shown in the paper how simple changes can have impact on Mean Average Precision (mAP).



### G2MF-WA: Geometric Multi-Model Fitting with Weakly Annotated Data
- **Arxiv ID**: http://arxiv.org/abs/2001.06965v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.06965v1)
- **Published**: 2020-01-20 04:22:01+00:00
- **Updated**: 2020-01-20 04:22:01+00:00
- **Authors**: Chao Zhang, Xuequan Lu, Katsuya Hotta, Xi Yang
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we attempt to address the problem of geometric multi-model fitting with resorting to a few weakly annotated (WA) data points, which has been sparsely studied so far. In weak annotating, most of the manual annotations are supposed to be correct yet inevitably mixed with incorrect ones. The WA data can be naturally obtained in an interactive way for specific tasks, for example, in the case of homography estimation, one can easily annotate points on the same plane/object with a single label by observing the image. Motivated by this, we propose a novel method to make full use of the WA data to boost the multi-model fitting performance. Specifically, a graph for model proposal sampling is first constructed using the WA data, given the prior that the WA data annotated with the same weak label has a high probability of being assigned to the same model. By incorporating this prior knowledge into the calculation of edge probabilities, vertices (i.e., data points) lie on/near the latent model are likely to connect together and further form a subset/cluster for effective proposals generation. With the proposals generated, the $\alpha$-expansion is adopted for labeling, and our method in return updates the proposals. This works in an iterative way. Extensive experiments validate our method and show that the proposed method produces noticeably better results than state-of-the-art techniques in most cases.



### A hybrid algorithm for disparity calculation from sparse disparity estimates based on stereo vision
- **Arxiv ID**: http://arxiv.org/abs/2001.06967v1
- **DOI**: 10.1109/SPCOM.2014.6983949
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.06967v1)
- **Published**: 2020-01-20 04:33:28+00:00
- **Updated**: 2020-01-20 04:33:28+00:00
- **Authors**: Subhayan Mukherjee, Ram Mohana Reddy Guddeti
- **Comment**: 2014 SPCOM
- **Journal**: None
- **Summary**: In this paper, we have proposed a novel method for stereo disparity estimation by combining the existing methods of block based and region based stereo matching. Our method can generate dense disparity maps from disparity measurements of only 18% pixels of either the left or the right image of a stereo image pair. It works by segmenting the lightness values of image pixels using a fast implementation of K-Means clustering. It then refines those segment boundaries by morphological filtering and connected components analysis, thus removing a lot of redundant boundary pixels. This is followed by determining the boundaries' disparities by the SAD cost function. Lastly, we reconstruct the entire disparity map of the scene from the boundaries' disparities through disparity propagation along the scan lines and disparity prediction of regions of uncertainty by considering disparities of the neighboring regions. Experimental results on the Middlebury stereo vision dataset demonstrate that the proposed method outperforms traditional disparity determination methods like SAD and NCC by up to 30% and achieves an improvement of 2.6% when compared to a recent approach based on absolute difference (AD) cost function for disparity calculations [1].



### FD-GAN: Generative Adversarial Networks with Fusion-discriminator for Single Image Dehazing
- **Arxiv ID**: http://arxiv.org/abs/2001.06968v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.06968v2)
- **Published**: 2020-01-20 04:36:11+00:00
- **Updated**: 2021-03-24 08:27:44+00:00
- **Authors**: Yu Dong, Yihao Liu, He Zhang, Shifeng Chen, Yu Qiao
- **Comment**: Accepted by AAAI2020 (with supplementary files)
- **Journal**: None
- **Summary**: Recently, convolutional neural networks (CNNs) have achieved great improvements in single image dehazing and attained much attention in research. Most existing learning-based dehazing methods are not fully end-to-end, which still follow the traditional dehazing procedure: first estimate the medium transmission and the atmospheric light, then recover the haze-free image based on the atmospheric scattering model. However, in practice, due to lack of priors and constraints, it is hard to precisely estimate these intermediate parameters. Inaccurate estimation further degrades the performance of dehazing, resulting in artifacts, color distortion and insufficient haze removal. To address this, we propose a fully end-to-end Generative Adversarial Networks with Fusion-discriminator (FD-GAN) for image dehazing. With the proposed Fusion-discriminator which takes frequency information as additional priors, our model can generator more natural and realistic dehazed images with less color distortion and fewer artifacts. Moreover, we synthesize a large-scale training dataset including various indoor and outdoor hazy images to boost the performance and we reveal that for learning-based dehazing methods, the performance is strictly influenced by the training data. Experiments have shown that our method reaches state-of-the-art performance on both public synthetic datasets and real-world images with more visually pleasing dehazed results.



### Real-Time Object Detection and Recognition on Low-Compute Humanoid Robots using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2002.03735v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.03735v1)
- **Published**: 2020-01-20 05:24:58+00:00
- **Updated**: 2020-01-20 05:24:58+00:00
- **Authors**: Sayantan Chatterjee, Faheem H. Zunjani, Souvik Sen, Gora C. Nandi
- **Comment**: 7 pages
- **Journal**: None
- **Summary**: We envision that in the near future, humanoid robots would share home space and assist us in our daily and routine activities through object manipulations. One of the fundamental technologies that need to be developed for robots is to enable them to detect objects and recognize them for effective manipulations and take real-time decisions involving those objects. In this paper, we describe a novel architecture that enables multiple low-compute NAO robots to perform real-time detection, recognition and localization of objects in its camera view and take programmable actions based on the detected objects. The proposed algorithm for object detection and localization is an empirical modification of YOLOv3, based on indoor experiments in multiple scenarios, with a smaller weight size and lesser computational requirements. Quantization of the weights and re-adjusting filter sizes and layer arrangements for convolutions improved the inference time for low-resolution images from the robot s camera feed. YOLOv3 was chosen after a comparative study of bounding box algorithms was performed with an objective to choose one that strikes the perfect balance among information retention, low inference time and high accuracy for real-time object detection and localization. The architecture also comprises of an effective end-to-end pipeline to feed the real-time frames from the camera feed to the neural net and use its results for guiding the robot with customizable actions corresponding to the detected class labels.



### Adaptive Dithering Using Curved Markov-Gaussian Noise in the Quantized Domain for Mapping SDR to HDR Image
- **Arxiv ID**: http://arxiv.org/abs/2001.06983v1
- **DOI**: 10.1007/978-3-030-04375-9_17
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2001.06983v1)
- **Published**: 2020-01-20 05:30:16+00:00
- **Updated**: 2020-01-20 05:30:16+00:00
- **Authors**: Subhayan Mukherjee, Guan-Ming Su, Irene Cheng
- **Comment**: 2018 International Conference on Smart Multimedia
- **Journal**: None
- **Summary**: High Dynamic Range (HDR) imaging is gaining increased attention due to its realistic content, for not only regular displays but also smartphones. Before sufficient HDR content is distributed, HDR visualization still relies mostly on converting Standard Dynamic Range (SDR) content. SDR images are often quantized, or bit depth reduced, before SDR-to-HDR conversion, e.g. for video transmission. Quantization can easily lead to banding artefacts. In some computing and/or memory I/O limited environment, the traditional solution using spatial neighborhood information is not feasible. Our method includes noise generation (offline) and noise injection (online), and operates on pixels of the quantized image. We vary the magnitude and structure of the noise pattern adaptively based on the luma of the quantized pixel and the slope of the inverse-tone mapping function. Subjective user evaluations confirm the superior performance of our technique.



### Deep Frequent Spatial Temporal Learning for Face Anti-Spoofing
- **Arxiv ID**: http://arxiv.org/abs/2002.03723v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2002.03723v1)
- **Published**: 2020-01-20 06:02:45+00:00
- **Updated**: 2020-01-20 06:02:45+00:00
- **Authors**: Ying Huang, Wenwei Zhang, Jinzhuo Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Face anti-spoofing is crucial for the security of face recognition system, by avoiding invaded with presentation attack. Previous works have shown the effectiveness of using depth and temporal supervision for this task. However, depth supervision is often considered only in a single frame, and temporal supervision is explored by utilizing certain signals which is not robust to the change of scenes. In this work, motivated by two stream ConvNets, we propose a novel two stream FreqSaptialTemporalNet for face anti-spoofing which simultaneously takes advantage of frequent, spatial and temporal information. Compared with existing methods which mine spoofing cues in multi-frame RGB image, we make multi-frame spectrum image as one input stream for the discriminative deep neural network, encouraging the primary difference between live and fake video to be automatically unearthed. Extensive experiments show promising improvement results using the proposed architecture. Meanwhile, we proposed a concise method to obtain a large amount of spoofing training data by utilizing a frequent augmentation pipeline, which contributes detail visualization between live and fake images as well as data insufficiency issue when training large networks.



### An Efficient Framework for Automated Screening of Clinically Significant Macular Edema
- **Arxiv ID**: http://arxiv.org/abs/2001.07002v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2001.07002v1)
- **Published**: 2020-01-20 07:34:13+00:00
- **Updated**: 2020-01-20 07:34:13+00:00
- **Authors**: Renoh Johnson Chalakkal, Faizal Hafiz, Waleed Abdulla, Akshya Swain
- **Comment**: None
- **Journal**: None
- **Summary**: The present study proposes a new approach to automated screening of Clinically Significant Macular Edema (CSME) and addresses two major challenges associated with such screenings, i.e., exudate segmentation and imbalanced datasets. The proposed approach replaces the conventional exudate segmentation based feature extraction by combining a pre-trained deep neural network with meta-heuristic feature selection. A feature space over-sampling technique is being used to overcome the effects of skewed datasets and the screening is accomplished by a k-NN based classifier. The role of each data-processing step (e.g., class balancing, feature selection) and the effects of limiting the region-of-interest to fovea on the classification performance are critically analyzed. Finally, the selection and implication of operating point on Receiver Operating Characteristic curve are discussed. The results of this study convincingly demonstrate that by following these fundamental practices of machine learning, a basic k-NN based classifier could effectively accomplish the CSME screening.



### Deep Image Clustering with Tensor Kernels and Unsupervised Companion Objectives
- **Arxiv ID**: http://arxiv.org/abs/2001.07026v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2001.07026v2)
- **Published**: 2020-01-20 09:07:59+00:00
- **Updated**: 2020-11-23 15:12:18+00:00
- **Authors**: Daniel J. Trosten, Michael C. Kampffmeyer, Robert Jenssen
- **Comment**: Work in progress
- **Journal**: None
- **Summary**: In this paper we develop a new model for deep image clustering, using convolutional neural networks and tensor kernels. The proposed Deep Tensor Kernel Clustering (DTKC) consists of a convolutional neural network (CNN), which is trained to reflect a common cluster structure at the output of its intermediate layers. Encouraging a consistent cluster structure throughout the network has the potential to guide it towards meaningful clusters, even though these clusters might appear to be nonlinear in the input space. The cluster structure is enforced through the idea of unsupervised companion objectives, where separate loss functions are attached to layers in the network. These unsupervised companion objectives are constructed based on a proposed generalization of the Cauchy-Schwarz (CS) divergence, from vectors to tensors of arbitrary rank. Generalizing the CS divergence to tensor-valued data is a crucial step, due to the tensorial nature of the intermediate representations in the CNN. Several experiments are conducted to thoroughly assess the performance of the proposed DTKC model. The results indicate that the model outperforms, or performs comparable to, a wide range of baseline algorithms. We also empirically demonstrate that our model does not suffer from objective function mismatch, which can be a problematic artifact in autoencoder-based clustering models.



### Plane Pair Matching for Efficient 3D View Registration
- **Arxiv ID**: http://arxiv.org/abs/2001.07058v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.07058v1)
- **Published**: 2020-01-20 11:15:26+00:00
- **Updated**: 2020-01-20 11:15:26+00:00
- **Authors**: Adrien Kaiser, José Alonso Ybanez Zepeda, Tamy Boubekeur
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel method to estimate the motion matrix between overlapping pairs of 3D views in the context of indoor scenes. We use the Manhattan world assumption to introduce lightweight geometric constraints under the form of planes into the problem, which reduces complexity by taking into account the structure of the scene. In particular, we define a stochastic framework to categorize planes as vertical or horizontal and parallel or non-parallel. We leverage this classification to match pairs of planes in overlapping views with point-of-view agnostic structural metrics. We propose to split the motion computation using the classification and estimate separately the rotation and translation of the sensor, using a quadric minimizer. We validate our approach on a toy example and present quantitative experiments on a public RGB-D dataset, comparing against recent state-of-the-art methods. Our evaluation shows that planar constraints only add low computational overhead while improving results in precision when applied after a prior coarse estimate. We conclude by giving hints towards extensions and improvements of current results.



### Accuracy vs. Complexity: A Trade-off in Visual Question Answering Models
- **Arxiv ID**: http://arxiv.org/abs/2001.07059v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CC
- **Links**: [PDF](http://arxiv.org/pdf/2001.07059v1)
- **Published**: 2020-01-20 11:27:21+00:00
- **Updated**: 2020-01-20 11:27:21+00:00
- **Authors**: Moshiur R. Farazi, Salman H. Khan, Nick Barnes
- **Comment**: None
- **Journal**: None
- **Summary**: Visual Question Answering (VQA) has emerged as a Visual Turing Test to validate the reasoning ability of AI agents. The pivot to existing VQA models is the joint embedding that is learned by combining the visual features from an image and the semantic features from a given question. Consequently, a large body of literature has focused on developing complex joint embedding strategies coupled with visual attention mechanisms to effectively capture the interplay between these two modalities. However, modelling the visual and semantic features in a high dimensional (joint embedding) space is computationally expensive, and more complex models often result in trivial improvements in the VQA accuracy. In this work, we systematically study the trade-off between the model complexity and the performance on the VQA task. VQA models have a diverse architecture comprising of pre-processing, feature extraction, multimodal fusion, attention and final classification stages. We specifically focus on the effect of "multi-modal fusion" in VQA models that is typically the most expensive step in a VQA pipeline. Our thorough experimental evaluation leads us to two proposals, one optimized for minimal complexity and the other one optimized for state-of-the-art VQA performance.



### Multiplication fusion of sparse and collaborative-competitive representation for image classification
- **Arxiv ID**: http://arxiv.org/abs/2001.07090v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.07090v1)
- **Published**: 2020-01-20 12:55:55+00:00
- **Updated**: 2020-01-20 12:55:55+00:00
- **Authors**: Zi-Qi Li, Jun Sun, Xiao-Jun Wu, He-Feng Yin
- **Comment**: submitted to International Journal of Machine Learning and
  Cybernetics
- **Journal**: None
- **Summary**: Representation based classification methods have become a hot research topic during the past few years, and the two most prominent approaches are sparse representation based classification (SRC) and collaborative representation based classification (CRC). CRC reveals that it is the collaborative representation rather than the sparsity that makes SRC successful. Nevertheless, the dense representation of CRC may not be discriminative which will degrade its performance for classification tasks. To alleviate this problem to some extent, we propose a new method called sparse and collaborative-competitive representation based classification (SCCRC) for image classification. Firstly, the coefficients of the test sample are obtained by SRC and CCRC, respectively. Then the fused coefficient is derived by multiplying the coefficients of SRC and CCRC. Finally, the test sample is designated to the class that has the minimum residual. Experimental results on several benchmark databases demonstrate the efficacy of our proposed SCCRC. The source code of SCCRC is accessible at https://github.com/li-zi-qi/SCCRC.



### Convolutional Neural Networks as a Model of the Visual System: Past, Present, and Future
- **Arxiv ID**: http://arxiv.org/abs/2001.07092v2
- **DOI**: 10.1162/jocn_a_01544
- **Categories**: **q-bio.NC**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2001.07092v2)
- **Published**: 2020-01-20 13:04:37+00:00
- **Updated**: 2020-02-10 11:37:16+00:00
- **Authors**: Grace W. Lindsay
- **Comment**: Review Article to be published in Journal of Cognitive Neuroscience,
  18 pages, 5 figures plus 8 pages of references
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) were inspired by early findings in the study of biological vision. They have since become successful tools in computer vision and state-of-the-art models of both neural activity and behavior on visual tasks. This review highlights what, in the context of CNNs, it means to be a good model in computational neuroscience and the various ways models can provide insight. Specifically, it covers the origins of CNNs and the methods by which we validate them as models of biological vision. It then goes on to elaborate on what we can learn about biological vision by understanding and experimenting on CNNs and discusses emerging opportunities for the use of CNNS in vision research beyond basic object recognition.



### BARNet: Bilinear Attention Network with Adaptive Receptive Fields for Surgical Instrument Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2001.07093v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2001.07093v4)
- **Published**: 2020-01-20 13:05:08+00:00
- **Updated**: 2020-05-22 03:12:14+00:00
- **Authors**: Zhen-Liang Ni, Gui-Bin Bian, Guan-An Wang, Xiao-Hu Zhou, Zeng-Guang Hou, Xiao-Liang Xie, Zhen Li, Yu-Han Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Surgical instrument segmentation is extremely important for computer-assisted surgery. Different from common object segmentation, it is more challenging due to the large illumination and scale variation caused by the special surgical scenes. In this paper, we propose a novel bilinear attention network with adaptive receptive field to solve these two challenges. For the illumination variation, the bilinear attention module can capture second-order statistics to encode global contexts and semantic dependencies between local pixels. With them, semantic features in challenging areas can be inferred from their neighbors and the distinction of various semantics can be boosted. For the scale variation, our adaptive receptive field module aggregates multi-scale features and automatically fuses them with different weights. Specifically, it encodes the semantic relationship between channels to emphasize feature maps with appropriate scales, changing the receptive field of subsequent convolutions. The proposed network achieves the best performance 97.47% mean IOU on Cata7 and comes first place on EndoVis 2017 by 10.10% IOU overtaking second-ranking method.



### Active and Incremental Learning with Weak Supervision
- **Arxiv ID**: http://arxiv.org/abs/2001.07100v1
- **DOI**: 10.1007/s13218-020-00631-4
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.07100v1)
- **Published**: 2020-01-20 13:21:14+00:00
- **Updated**: 2020-01-20 13:21:14+00:00
- **Authors**: Clemens-Alexander Brust, Christoph Käding, Joachim Denzler
- **Comment**: Accepted for publication in KI - K\"unstliche Intelligenz
- **Journal**: None
- **Summary**: Large amounts of labeled training data are one of the main contributors to the great success that deep models have achieved in the past. Label acquisition for tasks other than benchmarks can pose a challenge due to requirements of both funding and expertise. By selecting unlabeled examples that are promising in terms of model improvement and only asking for respective labels, active learning can increase the efficiency of the labeling process in terms of time and cost.   In this work, we describe combinations of an incremental learning scheme and methods of active learning. These allow for continuous exploration of newly observed unlabeled data. We describe selection criteria based on model uncertainty as well as expected model output change (EMOC). An object detection task is evaluated in a continuous exploration context on the PASCAL VOC dataset. We also validate a weakly supervised system based on active and incremental learning in a real-world biodiversity application where images from camera traps are analyzed. Labeling only 32 images by accepting or rejecting proposals generated by our method yields an increase in accuracy from 25.4% to 42.6%.



### Spectral Pyramid Graph Attention Network for Hyperspectral Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2001.07108v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.07108v1)
- **Published**: 2020-01-20 13:49:43+00:00
- **Updated**: 2020-01-20 13:49:43+00:00
- **Authors**: Tinghuai Wang, Guangming Wang, Kuan Eeik Tan, Donghui Tan
- **Comment**: 7 pages, 6 figures, 4 tables
- **Journal**: None
- **Summary**: Convolutional neural networks (CNN) have made significant advances in hyperspectral image (HSI) classification. However, standard convolutional kernel neglects the intrinsic connections between data points, resulting in poor region delineation and small spurious predictions. Furthermore, HSIs have a unique continuous data distribution along the high dimensional spectrum domain - much remains to be addressed in characterizing the spectral contexts considering the prohibitively high dimensionality and improving reasoning capability in light of the limited amount of labelled data. This paper presents a novel architecture which explicitly addresses these two issues. Specifically, we design an architecture to encode the multiple spectral contextual information in the form of spectral pyramid of multiple embedding spaces. In each spectral embedding space, we propose graph attention mechanism to explicitly perform interpretable reasoning in the spatial domain based on the connection in spectral feature space. Experiments on three HSI datasets demonstrate that the proposed architecture can significantly improve the classification accuracy compared with the existing methods.



### DeepFL-IQA: Weak Supervision for Deep IQA Feature Learning
- **Arxiv ID**: http://arxiv.org/abs/2001.08113v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2001.08113v1)
- **Published**: 2020-01-20 15:13:26+00:00
- **Updated**: 2020-01-20 15:13:26+00:00
- **Authors**: Hanhe Lin, Vlad Hosu, Dietmar Saupe
- **Comment**: dataset url: http://database.mmsp-kn.de
- **Journal**: None
- **Summary**: Multi-level deep-features have been driving state-of-the-art methods for aesthetics and image quality assessment (IQA). However, most IQA benchmarks are comprised of artificially distorted images, for which features derived from ImageNet under-perform. We propose a new IQA dataset and a weakly supervised feature learning approach to train features more suitable for IQA of artificially distorted images. The dataset, KADIS-700k, is far more extensive than similar works, consisting of 140,000 pristine images, 25 distortions types, totaling 700k distorted versions. Our weakly supervised feature learning is designed as a multi-task learning type training, using eleven existing full-reference IQA metrics as proxies for differential mean opinion scores. We also introduce a benchmark database, KADID-10k, of artificially degraded images, each subjectively annotated by 30 crowd workers. We make use of our derived image feature vectors for (no-reference) image quality assessment by training and testing a shallow regression network on this database and five other benchmark IQA databases. Our method, termed DeepFL-IQA, performs better than other feature-based no-reference IQA methods and also better than all tested full-reference IQA methods on KADID-10k. For the other five benchmark IQA databases, DeepFL-IQA matches the performance of the best existing end-to-end deep learning-based methods on average.



### A deep network for sinogram and CT image reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2001.07150v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2001.07150v1)
- **Published**: 2020-01-20 15:50:16+00:00
- **Updated**: 2020-01-20 15:50:16+00:00
- **Authors**: Wei Wang, Xiang-Gen Xia, Chuanjiang He, Zemin Ren, Jian Lu, Tianfu Wang, Baiying Lei
- **Comment**: None
- **Journal**: None
- **Summary**: A CT image can be well reconstructed when the sampling rate of the sinogram satisfies the Nyquist criteria and the sampled signal is noise-free. However, in practice, the sinogram is usually contaminated by noise, which degrades the quality of a reconstructed CT image. In this paper, we design a deep network for sinogram and CT image reconstruction. The network consists of two cascaded blocks that are linked by a filter backprojection (FBP) layer, where the former block is responsible for denoising and completing the sinograms while the latter is used to removing the noise and artifacts of the CT images. Experimental results show that the reconstructed CT images by our methods have the highest PSNR and SSIM in average compared to state of the art methods.



### An Image Enhancing Pattern-based Sparsity for Real-time Inference on Mobile Devices
- **Arxiv ID**: http://arxiv.org/abs/2001.07710v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2001.07710v3)
- **Published**: 2020-01-20 16:17:36+00:00
- **Updated**: 2020-07-05 01:22:19+00:00
- **Authors**: Xiaolong Ma, Wei Niu, Tianyun Zhang, Sijia Liu, Sheng Lin, Hongjia Li, Xiang Chen, Jian Tang, Kaisheng Ma, Bin Ren, Yanzhi Wang
- **Comment**: Paper accepted in the 16th European Conference on Computer Vision
  (ECCV 2020)
- **Journal**: None
- **Summary**: Weight pruning has been widely acknowledged as a straightforward and effective method to eliminate redundancy in Deep Neural Networks (DNN), thereby achieving acceleration on various platforms. However, most of the pruning techniques are essentially trade-offs between model accuracy and regularity which lead to impaired inference accuracy and limited on-device acceleration performance. To solve the problem, we introduce a new sparsity dimension, namely pattern-based sparsity that comprises pattern and connectivity sparsity, and becoming both highly accurate and hardware friendly. With carefully designed patterns, the proposed pruning unprecedentedly and consistently achieves accuracy enhancement and better feature extraction ability on different DNN structures and datasets, and our pattern-aware pruning framework also achieves pattern library extraction, pattern selection, pattern and connectivity pruning and weight training simultaneously. Our approach on the new pattern-based sparsity naturally fits into compiler optimization for highly efficient DNN execution on mobile platforms. To the best of our knowledge, it is the first time that mobile devices achieve real-time inference for the large-scale DNN models thanks to the unique spatial property of pattern-based sparsity and the help of the code generation capability of compilers.



### The benefits of synthetic data for action categorization
- **Arxiv ID**: http://arxiv.org/abs/2001.11091v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2001.11091v1)
- **Published**: 2020-01-20 17:23:02+00:00
- **Updated**: 2020-01-20 17:23:02+00:00
- **Authors**: Mohamad Ballout, Mohammad Tuqan, Daniel Asmar, Elie Shammas, George Sakr
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we study the value of using synthetically produced videos as training data for neural networks used for action categorization. Motivated by the fact that texture and background of a video play little to no significant roles in optical flow, we generated simplified texture-less and background-less videos and utilized the synthetic data to train a Temporal Segment Network (TSN). The results demonstrated that augmenting TSN with simplified synthetic data improved the original network accuracy (68.5%), achieving 71.8% on HMDB-51 when adding 4,000 videos and 72.4% when adding 8,000 videos. Also, training using simplified synthetic videos alone on 25 classes of UCF-101 achieved 30.71% when trained on 2500 videos and 52.7% when trained on 5000 videos. Finally, results showed that when reducing the number of real videos of UCF-25 to 10% and combining them with synthetic videos, the accuracy drops to only 85.41%, compared to a drop to 77.4% when no synthetic data is added.



### Learning Deformable Registration of Medical Images with Anatomical Constraints
- **Arxiv ID**: http://arxiv.org/abs/2001.07183v2
- **DOI**: 10.1016/j.neunet.2020.01.023
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2001.07183v2)
- **Published**: 2020-01-20 17:44:11+00:00
- **Updated**: 2020-01-22 13:25:16+00:00
- **Authors**: Lucas Mansilla, Diego H. Milone, Enzo Ferrante
- **Comment**: Accepted for publication in Neural Networks (Elsevier). Source code
  and resulting segmentation masks for the NIH Chest-XRay14 dataset with
  estimated quality index available at
  https://github.com/lucasmansilla/ACRN_Chest_X-ray_IA
- **Journal**: None
- **Summary**: Deformable image registration is a fundamental problem in the field of medical image analysis. During the last years, we have witnessed the advent of deep learning-based image registration methods which achieve state-of-the-art performance, and drastically reduce the required computational time. However, little work has been done regarding how can we encourage our models to produce not only accurate, but also anatomically plausible results, which is still an open question in the field. In this work, we argue that incorporating anatomical priors in the form of global constraints into the learning process of these models, will further improve their performance and boost the realism of the warped images after registration. We learn global non-linear representations of image anatomy using segmentation masks, and employ them to constraint the registration process. The proposed AC-RegNet architecture is evaluated in the context of chest X-ray image registration using three different datasets, where the high anatomical variability makes the task extremely challenging. Our experiments show that the proposed anatomically constrained registration model produces more realistic and accurate results than state-of-the-art methods, demonstrating the potential of this approach.



### Recommending Themes for Ad Creative Design via Visual-Linguistic Representations
- **Arxiv ID**: http://arxiv.org/abs/2001.07194v2
- **DOI**: 10.1145/3366423.3380001
- **Categories**: **cs.CL**, cs.CV, cs.IR, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2001.07194v2)
- **Published**: 2020-01-20 18:04:10+00:00
- **Updated**: 2020-02-27 23:05:46+00:00
- **Authors**: Yichao Zhou, Shaunak Mishra, Manisha Verma, Narayan Bhamidipati, Wei Wang
- **Comment**: 7 pages, 8 figures, 2 tables, accepted by The Web Conference 2020
- **Journal**: None
- **Summary**: There is a perennial need in the online advertising industry to refresh ad creatives, i.e., images and text used for enticing online users towards a brand. Such refreshes are required to reduce the likelihood of ad fatigue among online users, and to incorporate insights from other successful campaigns in related product categories. Given a brand, to come up with themes for a new ad is a painstaking and time consuming process for creative strategists. Strategists typically draw inspiration from the images and text used for past ad campaigns, as well as world knowledge on the brands. To automatically infer ad themes via such multimodal sources of information in past ad campaigns, we propose a theme (keyphrase) recommender system for ad creative strategists. The theme recommender is based on aggregating results from a visual question answering (VQA) task, which ingests the following: (i) ad images, (ii) text associated with the ads as well as Wikipedia pages on the brands in the ads, and (iii) questions around the ad. We leverage transformer based cross-modality encoders to train visual-linguistic representations for our VQA task. We study two formulations for the VQA task along the lines of classification and ranking; via experiments on a public dataset, we show that cross-modal representations lead to significantly better classification accuracy and ranking precision-recall metrics. Cross-modal representations show better performance compared to separate image and text representations. In addition, the use of multimodal information shows a significant lift over using only textual or visual information.



### Autocamera Calibration for traffic surveillance cameras with wide angle lenses
- **Arxiv ID**: http://arxiv.org/abs/2001.07243v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2001.07243v1)
- **Published**: 2020-01-20 20:37:02+00:00
- **Updated**: 2020-01-20 20:37:02+00:00
- **Authors**: Aman Gajendra Jain, Nicolas Saunier
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a method for automatic calibration of a traffic surveillance camera with wide-angle lenses. Video footage of a few minutes is sufficient for the entire calibration process to take place. This method takes in the height of the camera from the ground plane as the only user input to overcome the scale ambiguity. The calibration is performed in two stages, 1. Intrinsic Calibration 2. Extrinsic Calibration. Intrinsic calibration is achieved by assuming an equidistant fisheye distortion and an ideal camera model. Extrinsic calibration is accomplished by estimating the two vanishing points, on the ground plane, from the motion of vehicles at perpendicular intersections. The first stage of intrinsic calibration is also valid for thermal cameras. Experiments have been conducted to demonstrate the effectiveness of this approach on visible as well as thermal cameras.   Index Terms: fish-eye, calibration, thermal camera, intelligent transportation systems, vanishing points



### UR2KiD: Unifying Retrieval, Keypoint Detection, and Keypoint Description without Local Correspondence Supervision
- **Arxiv ID**: http://arxiv.org/abs/2001.07252v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2001.07252v1)
- **Published**: 2020-01-20 21:01:38+00:00
- **Updated**: 2020-01-20 21:01:38+00:00
- **Authors**: Tsun-Yi Yang, Duy-Kien Nguyen, Huub Heijnen, Vassileios Balntas
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we explore how three related tasks, namely keypoint detection, description, and image retrieval can be jointly tackled using a single unified framework, which is trained without the need of training data with point to point correspondences. By leveraging diverse information from sequential layers of a standard ResNet-based architecture, we are able to extract keypoints and descriptors that encode local information using generic techniques such as local activation norms, channel grouping and dropping, and self-distillation. Subsequently, global information for image retrieval is encoded in an end-to-end pipeline, based on pooling of the aforementioned local responses. In contrast to previous methods in local matching, our method does not depend on pointwise/pixelwise correspondences, and requires no such supervision at all i.e. no depth-maps from an SfM model nor manually created synthetic affine transformations. We illustrate that this simple and direct paradigm, is able to achieve very competitive results against the state-of-the-art methods in various challenging benchmark conditions such as viewpoint changes, scale changes, and day-night shifting localization.



### Recovering Geometric Information with Learned Texture Perturbations
- **Arxiv ID**: http://arxiv.org/abs/2001.07253v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.07253v1)
- **Published**: 2020-01-20 21:15:13+00:00
- **Updated**: 2020-01-20 21:15:13+00:00
- **Authors**: Jane Wu, Yongxu Jin, Zhenglin Geng, Hui Zhou, Ronald Fedkiw
- **Comment**: None
- **Journal**: None
- **Summary**: Regularization is used to avoid overfitting when training a neural network; unfortunately, this reduces the attainable level of detail hindering the ability to capture high-frequency information present in the training data. Even though various approaches may be used to re-introduce high-frequency detail, it typically does not match the training data and is often not time coherent. In the case of network inferred cloth, these sentiments manifest themselves via either a lack of detailed wrinkles or unnaturally appearing and/or time incoherent surrogate wrinkles. Thus, we propose a general strategy whereby high-frequency information is procedurally embedded into low-frequency data so that when the latter is smeared out by the network the former still retains its high-frequency detail. We illustrate this approach by learning texture coordinates which when smeared do not in turn smear out the high-frequency detail in the texture itself but merely smoothly distort it. Notably, we prescribe perturbed texture coordinates that are subsequently used to correct the over-smoothed appearance of inferred cloth, and correcting the appearance from multiple camera views naturally recovers lost geometric information.



### Digital synthesis of histological stains using micro-structured and multiplexed virtual staining of label-free tissue
- **Arxiv ID**: http://arxiv.org/abs/2001.07267v1
- **DOI**: 10.1038/s41377-020-0315-y
- **Categories**: **eess.IV**, cs.CV, cs.LG, physics.med-ph, q-bio.QM, 68T01, 68T05, 68U10, 62M45, 78M32, 92C50, 92C55, 94A08, I.2; I.2.1; I.2.6; I.2.10; I.3; I.3.3; I.4.3; I.4.4; I.4.9; J.3
- **Links**: [PDF](http://arxiv.org/pdf/2001.07267v1)
- **Published**: 2020-01-20 22:14:06+00:00
- **Updated**: 2020-01-20 22:14:06+00:00
- **Authors**: Yijie Zhang, Kevin de Haan, Yair Rivenson, Jingxi Li, Apostolos Delis, Aydogan Ozcan
- **Comment**: 19 pages, 5 figures, 2 tables
- **Journal**: Light: Science & Applications (2020)
- **Summary**: Histological staining is a vital step used to diagnose various diseases and has been used for more than a century to provide contrast to tissue sections, rendering the tissue constituents visible for microscopic analysis by medical experts. However, this process is time-consuming, labor-intensive, expensive and destructive to the specimen. Recently, the ability to virtually-stain unlabeled tissue sections, entirely avoiding the histochemical staining step, has been demonstrated using tissue-stain specific deep neural networks. Here, we present a new deep learning-based framework which generates virtually-stained images using label-free tissue, where different stains are merged following a micro-structure map defined by the user. This approach uses a single deep neural network that receives two different sources of information at its input: (1) autofluorescence images of the label-free tissue sample, and (2) a digital staining matrix which represents the desired microscopic map of different stains to be virtually generated at the same tissue section. This digital staining matrix is also used to virtually blend existing stains, digitally synthesizing new histological stains. We trained and blindly tested this virtual-staining network using unlabeled kidney tissue sections to generate micro-structured combinations of Hematoxylin and Eosin (H&E), Jones silver stain, and Masson's Trichrome stain. Using a single network, this approach multiplexes virtual staining of label-free tissue with multiple types of stains and paves the way for synthesizing new digital histological stains that can be created on the same tissue cross-section, which is currently not feasible with standard histochemical staining methods.



