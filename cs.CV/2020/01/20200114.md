# Arxiv Papers in cs.CV on 2020-01-14
### Distortion Agnostic Deep Watermarking
- **Arxiv ID**: http://arxiv.org/abs/2001.04580v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2001.04580v1)
- **Published**: 2020-01-14 01:04:59+00:00
- **Updated**: 2020-01-14 01:04:59+00:00
- **Authors**: Xiyang Luo, Ruohan Zhan, Huiwen Chang, Feng Yang, Peyman Milanfar
- **Comment**: None
- **Journal**: None
- **Summary**: Watermarking is the process of embedding information into an image that can survive under distortions, while requiring the encoded image to have little or no perceptual difference from the original image. Recently, deep learning-based methods achieved impressive results in both visual quality and message payload under a wide variety of image distortions. However, these methods all require differentiable models for the image distortions at training time, and may generalize poorly to unknown distortions. This is undesirable since the types of distortions applied to watermarked images are usually unknown and non-differentiable. In this paper, we propose a new framework for distortion-agnostic watermarking, where the image distortion is not explicitly modeled during training. Instead, the robustness of our system comes from two sources: adversarial training and channel coding. Compared to training on a fixed set of distortions and noise levels, our method achieves comparable or better results on distortions available during training, and better performance on unknown distortions.



### EGO-TOPO: Environment Affordances from Egocentric Video
- **Arxiv ID**: http://arxiv.org/abs/2001.04583v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.04583v2)
- **Published**: 2020-01-14 01:20:39+00:00
- **Updated**: 2020-03-27 20:30:19+00:00
- **Authors**: Tushar Nagarajan, Yanghao Li, Christoph Feichtenhofer, Kristen Grauman
- **Comment**: Published in CVPR 2020, project page:
  http://vision.cs.utexas.edu/projects/ego-topo/
- **Journal**: None
- **Summary**: First-person video naturally brings the use of a physical environment to the forefront, since it shows the camera wearer interacting fluidly in a space based on his intentions. However, current methods largely separate the observed actions from the persistent space itself. We introduce a model for environment affordances that is learned directly from egocentric video. The main idea is to gain a human-centric model of a physical space (such as a kitchen) that captures (1) the primary spatial zones of interaction and (2) the likely activities they support. Our approach decomposes a space into a topological map derived from first-person activity, organizing an ego-video into a series of visits to the different zones. Further, we show how to link zones across multiple related environments (e.g., from videos of multiple kitchens) to obtain a consolidated representation of environment functionality. On EPIC-Kitchens and EGTEA+, we demonstrate our approach for learning scene affordances and anticipating future actions in long-form video.



### Actions as Moving Points
- **Arxiv ID**: http://arxiv.org/abs/2001.04608v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.04608v3)
- **Published**: 2020-01-14 03:29:44+00:00
- **Updated**: 2020-08-22 14:45:35+00:00
- **Authors**: Yixuan Li, Zixu Wang, Limin Wang, Gangshan Wu
- **Comment**: ECCV2020 camera ready version
- **Journal**: None
- **Summary**: The existing action tubelet detectors often depend on heuristic anchor design and placement, which might be computationally expensive and sub-optimal for precise localization. In this paper, we present a conceptually simple, computationally efficient, and more precise action tubelet detection framework, termed as MovingCenter Detector (MOC-detector), by treating an action instance as a trajectory of moving points. Based on the insight that movement information could simplify and assist action tubelet detection, our MOC-detector is composed of three crucial head branches: (1) Center Branch for instance center detection and action recognition, (2) Movement Branch for movement estimation at adjacent frames to form trajectories of moving points, (3) Box Branch for spatial extent detection by directly regressing bounding box size at each estimated center. These three branches work together to generate the tubelet detection results, which could be further linked to yield video-level tubes with a matching strategy. Our MOC-detector outperforms the existing state-of-the-art methods for both metrics of frame-mAP and video-mAP on the JHMDB and UCF101-24 datasets. The performance gap is more evident for higher video IoU, demonstrating that our MOC-detector is particularly effective for more precise action detection. We provide the code at https://github.com/MCG-NJU/MOC-Detector.



### Spatial-Spectral Residual Network for Hyperspectral Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2001.04609v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2001.04609v1)
- **Published**: 2020-01-14 03:34:55+00:00
- **Updated**: 2020-01-14 03:34:55+00:00
- **Authors**: Qi Wang, Qiang Li, Xuelong Li
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning-based hyperspectral image super-resolution (SR) methods have achieved great success recently. However, most existing models can not effectively explore spatial information and spectral information between bands simultaneously, obtaining relatively low performance. To address this issue, in this paper, we propose a novel spectral-spatial residual network for hyperspectral image super-resolution (SSRNet). Our method can effectively explore spatial-spectral information by using 3D convolution instead of 2D convolution, which enables the network to better extract potential information. Furthermore, we design a spectral-spatial residual module (SSRM) to adaptively learn more effective features from all the hierarchical features in units through local feature fusion, significantly improving the performance of the algorithm. In each unit, we employ spatial and temporal separable 3D convolution to extract spatial and spectral information, which not only reduces unaffordable memory usage and high computational cost, but also makes the network easier to train. Extensive evaluations and comparisons on three benchmark datasets demonstrate that the proposed approach achieves superior performance in comparison to existing state-of-the-art methods.



### Cross-dataset Training for Class Increasing Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2001.04621v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.04621v1)
- **Published**: 2020-01-14 04:40:47+00:00
- **Updated**: 2020-01-14 04:40:47+00:00
- **Authors**: Yongqiang Yao, Yan Wang, Yu Guo, Jiaojiao Lin, Hongwei Qin, Junjie Yan
- **Comment**: 10 pages, 6 figures
- **Journal**: None
- **Summary**: We present a conceptually simple, flexible and general framework for cross-dataset training in object detection. Given two or more already labeled datasets that target for different object classes, cross-dataset training aims to detect the union of the different classes, so that we do not have to label all the classes for all the datasets. By cross-dataset training, existing datasets can be utilized to detect the merged object classes with a single model. Further more, in industrial applications, the object classes usually increase on demand. So when adding new classes, it is quite time-consuming if we label the new classes on all the existing datasets. While using cross-dataset training, we only need to label the new classes on the new dataset. We experiment on PASCAL VOC, COCO, WIDER FACE and WIDER Pedestrian with both solo and cross-dataset settings. Results show that our cross-dataset pipeline can achieve similar impressive performance simultaneously on these datasets compared with training independently.



### Asymmetric Correlation Quantization Hashing for Cross-modal Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2001.04625v1
- **DOI**: None
- **Categories**: **cs.IR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2001.04625v1)
- **Published**: 2020-01-14 04:53:30+00:00
- **Updated**: 2020-01-14 04:53:30+00:00
- **Authors**: Lu Wang, Jie Yang
- **Comment**: 12 pages
- **Journal**: None
- **Summary**: Due to the superiority in similarity computation and database storage for large-scale multiple modalities data, cross-modal hashing methods have attracted extensive attention in similarity retrieval across the heterogeneous modalities. However, there are still some limitations to be further taken into account: (1) most current CMH methods transform real-valued data points into discrete compact binary codes under the binary constraints, limiting the capability of representation for original data on account of abundant loss of information and producing suboptimal hash codes; (2) the discrete binary constraint learning model is hard to solve, where the retrieval performance may greatly reduce by relaxing the binary constraints for large quantization error; (3) handling the learning problem of CMH in a symmetric framework, leading to difficult and complex optimization objective. To address above challenges, in this paper, a novel Asymmetric Correlation Quantization Hashing (ACQH) method is proposed. Specifically, ACQH learns the projection matrixs of heterogeneous modalities data points for transforming query into a low-dimensional real-valued vector in latent semantic space and constructs the stacked compositional quantization embedding in a coarse-to-fine manner for indicating database points by a series of learnt real-valued codeword in the codebook with the help of pointwise label information regression simultaneously. Besides, the unified hash codes across modalities can be directly obtained by the discrete iterative optimization framework devised in the paper. Comprehensive experiments on diverse three benchmark datasets have shown the effectiveness and rationality of ACQH.



### Self-supervising Action Recognition by Statistical Moment and Subspace Descriptors
- **Arxiv ID**: http://arxiv.org/abs/2001.04627v2
- **DOI**: 10.1145/3474085.3475572
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.04627v2)
- **Published**: 2020-01-14 05:03:54+00:00
- **Updated**: 2021-08-05 15:25:12+00:00
- **Authors**: Lei Wang, Piotr Koniusz
- **Comment**: ACM MM'21
- **Journal**: None
- **Summary**: In this paper, we build on a concept of self-supervision by taking RGB frames as input to learn to predict both action concepts and auxiliary descriptors e.g., object descriptors. So-called hallucination streams are trained to predict auxiliary cues, simultaneously fed into classification layers, and then hallucinated at the testing stage to aid network. We design and hallucinate two descriptors, one leveraging four popular object detectors applied to training videos, and the other leveraging image- and video-level saliency detectors. The first descriptor encodes the detector- and ImageNet-wise class prediction scores, confidence scores, and spatial locations of bounding boxes and frame indexes to capture the spatio-temporal distribution of features per video. Another descriptor encodes spatio-angular gradient distributions of saliency maps and intensity patterns. Inspired by the characteristic function of the probability distribution, we capture four statistical moments on the above intermediate descriptors. As numbers of coefficients in the mean, covariance, coskewness and cokurtotsis grow linearly, quadratically, cubically and quartically w.r.t. the dimension of feature vectors, we describe the covariance matrix by its leading n' eigenvectors (so-called subspace) and we capture skewness/kurtosis rather than costly coskewness/cokurtosis. We obtain state of the art on five popular datasets such as Charades and EPIC-Kitchens.



### Seeing the World in a Bag of Chips
- **Arxiv ID**: http://arxiv.org/abs/2001.04642v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.04642v2)
- **Published**: 2020-01-14 06:44:44+00:00
- **Updated**: 2020-06-15 16:15:06+00:00
- **Authors**: Jeong Joon Park, Aleksander Holynski, Steve Seitz
- **Comment**: CVPR 2020
- **Journal**: None
- **Summary**: We address the dual problems of novel view synthesis and environment reconstruction from hand-held RGBD sensors. Our contributions include 1) modeling highly specular objects, 2) modeling inter-reflections and Fresnel effects, and 3) enabling surface light field reconstruction with the same input needed to reconstruct shape alone. In cases where scene surface has a strong mirror-like material component, we generate highly detailed environment images, revealing room composition, objects, people, buildings, and trees visible through windows. Our approach yields state of the art view synthesis techniques, operates on low dynamic range imagery, and is robust to geometric and calibration errors.



### Structured Consistency Loss for semi-supervised semantic segmentation
- **Arxiv ID**: http://arxiv.org/abs/2001.04647v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.04647v2)
- **Published**: 2020-01-14 07:08:45+00:00
- **Updated**: 2021-11-22 04:22:49+00:00
- **Authors**: Jongmok Kim, Jooyoung Jang, Hyunwoo Park, SeongAh Jeong
- **Comment**: 10 pages, 4 figures
- **Journal**: None
- **Summary**: The consistency loss has played a key role in solving problems in recent studies on semi-supervised learning. Yet extant studies with the consistency loss are limited to its application to classification tasks; extant studies on semi-supervised semantic segmentation rely on pixel-wise classification, which does not reflect the structured nature of characteristics in prediction. We propose a structured consistency loss to address this limitation of extant studies. Structured consistency loss promotes consistency in inter-pixel similarity between teacher and student networks. Specifically, collaboration with CutMix optimizes the efficient performance of semi-supervised semantic segmentation with structured consistency loss by reducing computational burden dramatically. The superiority of proposed method is verified with the Cityscapes; The Cityscapes benchmark results with validation and with test data are 81.9 mIoU and 83.84 mIoU respectively. This ranks the first place on the pixel-level semantic labeling task of Cityscapes benchmark suite. To the best of our knowledge, we are the first to present the superiority of state-of-the-art semi-supervised learning in semantic segmentation.



### Effects of annotation granularity in deep learning models for histopathological images
- **Arxiv ID**: http://arxiv.org/abs/2001.04663v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2001.04663v1)
- **Published**: 2020-01-14 08:39:51+00:00
- **Updated**: 2020-01-14 08:39:51+00:00
- **Authors**: Jiangbo Shi, Zeyu Gao, Haichuan Zhang, Pargorn Puttapirat, Chunbao Wang, Xiangrong Zhang, Chen Li
- **Comment**: Accepted by AIPath2019 Workshop in BIBM2019. 7 pages, 4 figures, 4
  tables
- **Journal**: None
- **Summary**: Pathological is crucial to cancer diagnosis. Usually, Pathologists draw their conclusion based on observed cell and tissue structure on histology slides. Rapid development in machine learning, especially deep learning have established robust and accurate classifiers. They are being used to analyze histopathological slides and assist pathologists in diagnosis. Most machine learning systems rely heavily on annotated data sets to gain experiences and knowledge to correctly and accurately perform various tasks such as classification and segmentation. This work investigates different granularity of annotations in histopathological data set including image-wise, bounding box, ellipse-wise, and pixel-wise to verify the influence of annotation in pathological slide on deep learning models. We design corresponding experiments to test classification and segmentation performance of deep learning models based on annotations with different annotation granularity. In classification, state-of-the-art deep learning-based classifiers perform better when trained by pixel-wise annotation dataset. On average, precision, recall and F1-score improves by 7.87%, 8.83% and 7.85% respectively. Thus, it is suggested that finer granularity annotations are better utilized by deep learning algorithms in classification tasks. Similarly, semantic segmentation algorithms can achieve 8.33% better segmentation accuracy when trained by pixel-wise annotations. Our study shows not only that finer-grained annotation can improve the performance of deep learning models, but also help extracts more accurate phenotypic information from histopathological slides. Intelligence systems trained on granular annotations may help pathologists inspecting certain regions for better diagnosis. The compartmentalized prediction approach similar to this work may contribute to phenotype and genotype association studies.



### Face Attribute Invertion
- **Arxiv ID**: http://arxiv.org/abs/2001.04665v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.04665v1)
- **Published**: 2020-01-14 08:41:52+00:00
- **Updated**: 2020-01-14 08:41:52+00:00
- **Authors**: X G Tu, Y Luo, H S Zhang, W J Ai, Z Ma, M Xie
- **Comment**: 8 pages, 3 figures
- **Journal**: None
- **Summary**: Manipulating human facial images between two domains is an important and interesting problem. Most of the existing methods address this issue by applying two generators or one generator with extra conditional inputs. In this paper, we proposed a novel self-perception method based on GANs for automatical face attribute inverse. The proposed method takes face images as inputs and employs only one single generator without being conditioned on other inputs. Profiting from the multi-loss strategy and modified U-net structure, our model is quite stable in training and capable of preserving finer details of the original face images.



### Unsupervised Domain Adaptation for Mobile Semantic Segmentation based on Cycle Consistency and Feature Alignment
- **Arxiv ID**: http://arxiv.org/abs/2001.04692v2
- **DOI**: 10.1016/j.imavis.2020.103889
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2001.04692v2)
- **Published**: 2020-01-14 10:12:20+00:00
- **Updated**: 2020-03-12 10:22:43+00:00
- **Authors**: Marco Toldo, Umberto Michieli, Gianluca Agresti, Pietro Zanuttigh
- **Comment**: 11 pages, 3 figures, 3 tables
- **Journal**: Image and Vision Computing, Volume 95, March 2020
- **Summary**: The supervised training of deep networks for semantic segmentation requires a huge amount of labeled real world data. To solve this issue, a commonly exploited workaround is to use synthetic data for training, but deep networks show a critical performance drop when analyzing data with slightly different statistical properties with respect to the training set. In this work, we propose a novel Unsupervised Domain Adaptation (UDA) strategy to address the domain shift issue between real world and synthetic representations. An adversarial model, based on the cycle consistency framework, performs the mapping between the synthetic and real domain. The data is then fed to a MobileNet-v2 architecture that performs the semantic segmentation task. An additional couple of discriminators, working at the feature level of the MobileNet-v2, allows to better align the features of the two domain distributions and to further improve the performance. Finally, the consistency of the semantic maps is exploited. After an initial supervised training on synthetic data, the whole UDA architecture is trained end-to-end considering all its components at once. Experimental results show how the proposed strategy is able to obtain impressive performance in adapting a segmentation network trained on synthetic data to real world scenarios. The usage of the lightweight MobileNet-v2 architecture allows its deployment on devices with limited computational resources as the ones employed in autonomous vehicles.



### Real-Time Lane ID Estimation Using Recurrent Neural Networks With Dual Convention
- **Arxiv ID**: http://arxiv.org/abs/2001.04708v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.04708v1)
- **Published**: 2020-01-14 10:52:30+00:00
- **Updated**: 2020-01-14 10:52:30+00:00
- **Authors**: Ibrahim Halfaoui, Fahd Bouzaraa, Onay Urfalioglu, Li Minzhen
- **Comment**: None
- **Journal**: None
- **Summary**: Acquiring information about the road lane structure is a crucial step for autonomous navigation. To this end, several approaches tackle this task from different perspectives such as lane marking detection or semantic lane segmentation. However, to the best of our knowledge, there is yet no purely vision based end-to-end solution to answer the precise question: How to estimate the relative number or "ID" of the current driven lane within a multi-lane road or a highway? In this work, we propose a real-time, vision-only (i.e. monocular camera) solution to the problem based on a dual left-right convention. We interpret this task as a classification problem by limiting the maximum number of lane candidates to eight. Our approach is designed to meet low-complexity specifications and limited runtime requirements. It harnesses the temporal dimension inherent to the input sequences to improve upon high-complexity state-of-the-art models. We achieve more than 95% accuracy on a challenging test set with extreme conditions and different routes.



### Edge Preserving CNN SAR Despeckling Algorithm
- **Arxiv ID**: http://arxiv.org/abs/2001.04716v3
- **DOI**: 10.1109/LAGIRS48042.2020.9165559
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2001.04716v3)
- **Published**: 2020-01-14 11:26:43+00:00
- **Updated**: 2020-07-10 13:04:13+00:00
- **Authors**: Sergio Vitale, Giampaolo Ferraioli, Vito Pascazio
- **Comment**: Accepted to LAGIRS 2020
- **Journal**: 2020 IEEE Latin American GRSS & ISPRS Remote Sensing Conference
  (LAGIRS)
- **Summary**: SAR despeckling is a key tool for Earth Observation. Interpretation of SAR images are impaired by speckle, a multiplicative noise related to interference of backscattering from the illuminated scene towards the sensor. Reducing the noise is a crucial task for the understanding of the scene. Based on the results of our previous solution KL-DNN, in this work we define a new cost function for training a convolutional neural network for despeckling. The aim is to control the edge preservation and to better filter manmade structures and urban areas that are very challenging for KL-DNN. The results show a very good improvement on the not homogeneous areas keeping the good results in the homogeneous ones. Result on both simulated and real data are shown in the paper.



### Fine-grained Image Classification and Retrieval by Combining Visual and Locally Pooled Textual Features
- **Arxiv ID**: http://arxiv.org/abs/2001.04732v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.04732v1)
- **Published**: 2020-01-14 12:06:12+00:00
- **Updated**: 2020-01-14 12:06:12+00:00
- **Authors**: Andres Mafla, Sounak Dey, Ali Furkan Biten, Lluis Gomez, Dimosthenis Karatzas
- **Comment**: Winter Conference on Applications of Computer Vision (WACV 2020)
  Accepted paper
- **Journal**: None
- **Summary**: Text contained in an image carries high-level semantics that can be exploited to achieve richer image understanding. In particular, the mere presence of text provides strong guiding content that should be employed to tackle a diversity of computer vision tasks such as image retrieval, fine-grained classification, and visual question answering. In this paper, we address the problem of fine-grained classification and image retrieval by leveraging textual information along with visual cues to comprehend the existing intrinsic relation between the two modalities. The novelty of the proposed model consists of the usage of a PHOC descriptor to construct a bag of textual words along with a Fisher Vector Encoding that captures the morphology of text. This approach provides a stronger multimodal representation for this task and as our experiments demonstrate, it achieves state-of-the-art results on two different tasks, fine-grained classification and image retrieval.



### NODIS: Neural Ordinary Differential Scene Understanding
- **Arxiv ID**: http://arxiv.org/abs/2001.04735v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.04735v3)
- **Published**: 2020-01-14 12:17:18+00:00
- **Updated**: 2020-07-18 20:41:19+00:00
- **Authors**: Cong Yuren, Hanno Ackermann, Wentong Liao, Michael Ying Yang, Bodo Rosenhahn
- **Comment**: None
- **Journal**: None
- **Summary**: Semantic image understanding is a challenging topic in computer vision. It requires to detect all objects in an image, but also to identify all the relations between them. Detected objects, their labels and the discovered relations can be used to construct a scene graph which provides an abstract semantic interpretation of an image. In previous works, relations were identified by solving an assignment problem formulated as Mixed-Integer Linear Programs. In this work, we interpret that formulation as Ordinary Differential Equation (ODE). The proposed architecture performs scene graph inference by solving a neural variant of an ODE by end-to-end learning. It achieves state-of-the-art results on all three benchmark tasks: scene graph generation (SGGen), classification (SGCls) and visual relationship detection (PredCls) on Visual Genome benchmark.



### Deep Image Compression using Decoder Side Information
- **Arxiv ID**: http://arxiv.org/abs/2001.04753v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2001.04753v2)
- **Published**: 2020-01-14 12:55:27+00:00
- **Updated**: 2020-07-29 15:13:40+00:00
- **Authors**: Sharon Ayzik, Shai Avidan
- **Comment**: None
- **Journal**: None
- **Summary**: We present a Deep Image Compression neural network that relies on side information, which is only available to the decoder. We base our algorithm on the assumption that the image available to the encoder and the image available to the decoder are correlated, and we let the network learn these correlations in the training phase.   Then, at run time, the encoder side encodes the input image without knowing anything about the decoder side image and sends it to the decoder. The decoder then uses the encoded input image and the side information image to reconstruct the original image.   This problem is known as Distributed Source Coding in Information Theory, and we discuss several use cases for this technology. We compare our algorithm to several image compression algorithms and show that adding decoder-only side information does indeed improve results. Our code is publicly available at https://github.com/ayziksha/DSIN.



### The problems with using STNs to align CNN feature maps
- **Arxiv ID**: http://arxiv.org/abs/2001.05858v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.05858v1)
- **Published**: 2020-01-14 12:59:56+00:00
- **Updated**: 2020-01-14 12:59:56+00:00
- **Authors**: Lukas Finnveden, Ylva Jansson, Tony Lindeberg
- **Comment**: Accepted to Northern Lights Deep Learning Workshop 2020, Troms{\o}, 2
  pages, 3 figures
- **Journal**: None
- **Summary**: Spatial transformer networks (STNs) were designed to enable CNNs to learn invariance to image transformations. STNs were originally proposed to transform CNN feature maps as well as input images. This enables the use of more complex features when predicting transformation parameters. However, since STNs perform a purely spatial transformation, they do not, in the general case, have the ability to align the feature maps of a transformed image and its original. We present a theoretical argument for this and investigate the practical implications, showing that this inability is coupled with decreased classification accuracy. We advocate taking advantage of more complex features in deeper layers by instead sharing parameters between the classification and the localisation network.



### Deep Audio-Visual Learning: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2001.04758v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.04758v1)
- **Published**: 2020-01-14 13:11:21+00:00
- **Updated**: 2020-01-14 13:11:21+00:00
- **Authors**: Hao Zhu, Mandi Luo, Rui Wang, Aihua Zheng, Ran He
- **Comment**: None
- **Journal**: None
- **Summary**: Audio-visual learning, aimed at exploiting the relationship between audio and visual modalities, has drawn considerable attention since deep learning started to be used successfully. Researchers tend to leverage these two modalities either to improve the performance of previously considered single-modality tasks or to address new challenging problems. In this paper, we provide a comprehensive survey of recent audio-visual learning development. We divide the current audio-visual learning tasks into four different subfields: audio-visual separation and localization, audio-visual correspondence learning, audio-visual generation, and audio-visual representation learning. State-of-the-art methods as well as the remaining challenges of each subfield are further discussed. Finally, we summarize the commonly used datasets and performance metrics.



### Learned Multi-View Texture Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2001.04775v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.04775v1)
- **Published**: 2020-01-14 13:49:22+00:00
- **Updated**: 2020-01-14 13:49:22+00:00
- **Authors**: Audrey Richard, Ian Cherabier, Martin R. Oswald, Vagia Tsiminaki, Marc Pollefeys, Konrad Schindler
- **Comment**: 11 pages, 5 figures, 2019 International Conference on 3D Vision (3DV)
- **Journal**: None
- **Summary**: We present a super-resolution method capable of creating a high-resolution texture map for a virtual 3D object from a set of lower-resolution images of that object. Our architecture unifies the concepts of (i) multi-view super-resolution based on the redundancy of overlapping views and (ii) single-view super-resolution based on a learned prior of high-resolution (HR) image structure. The principle of multi-view super-resolution is to invert the image formation process and recover the latent HR texture from multiple lower-resolution projections. We map that inverse problem into a block of suitably designed neural network layers, and combine it with a standard encoder-decoder network for learned single-image super-resolution. Wiring the image formation model into the network avoids having to learn perspective mapping from textures to images, and elegantly handles a varying number of input views. Experiments demonstrate that the combination of multi-view observations and learned prior yields improved texture maps.



### Neural Architecture Search for Deep Image Prior
- **Arxiv ID**: http://arxiv.org/abs/2001.04776v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.04776v1)
- **Published**: 2020-01-14 13:51:32+00:00
- **Updated**: 2020-01-14 13:51:32+00:00
- **Authors**: Kary Ho, Andrew Gilbert, Hailin Jin, John Collomosse
- **Comment**: None
- **Journal**: None
- **Summary**: We present a neural architecture search (NAS) technique to enhance the performance of unsupervised image de-noising, in-painting and super-resolution under the recently proposed Deep Image Prior (DIP). We show that evolutionary search can automatically optimize the encoder-decoder (E-D) structure and meta-parameters of the DIP network, which serves as a content-specific prior to regularize these single image restoration tasks. Our binary representation encodes the design space for an asymmetric E-D network that typically converges to yield a content-specific DIP within 10-20 generations using a population size of 500. The optimized architectures consistently improve upon the visual quality of classical DIP for a diverse range of photographic and artistic content.



### Towards detection and classification of microscopic foraminifera using transfer learning
- **Arxiv ID**: http://arxiv.org/abs/2001.04782v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2001.04782v1)
- **Published**: 2020-01-14 13:57:08+00:00
- **Updated**: 2020-01-14 13:57:08+00:00
- **Authors**: Thomas Haugland Johansen, Steffen Aagaard Sørensen
- **Comment**: 6 pages, 5 figures. To be published in proceedings of Northern Lights
  Deep Learning Workshop 2020
- **Journal**: None
- **Summary**: Foraminifera are single-celled marine organisms, which may have a planktic or benthic lifestyle. During their life cycle they construct shells consisting of one or more chambers, and these shells remain as fossils in marine sediments. Classifying and counting these fossils have become an important tool in e.g. oceanography and climatology. Currently the process of identifying and counting microfossils is performed manually using a microscope and is very time consuming. Developing methods to automate this process is therefore considered important across a range of research fields. The first steps towards developing a deep learning model that can detect and classify microscopic foraminifera are proposed. The proposed model is based on a VGG16 model that has been pretrained on the ImageNet dataset, and adapted to the foraminifera task using transfer learning. Additionally, a novel image dataset consisting of microscopic foraminifera and sediments from the Barents Sea region is introduced.



### Improving Semantic Analysis on Point Clouds via Auxiliary Supervision of Local Geometric Priors
- **Arxiv ID**: http://arxiv.org/abs/2001.04803v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.04803v2)
- **Published**: 2020-01-14 14:33:31+00:00
- **Updated**: 2020-09-17 03:11:53+00:00
- **Authors**: Lulu Tang, Ke Chen, Chaozheng Wu, Yu Hong, Kui Jia, Zhixin Yang
- **Comment**: 11 pages, 8 figures, 9 tables; accepted by the IEEE Transactions on
  Cybernetics
- **Journal**: None
- **Summary**: Existing deep learning algorithms for point cloud analysis mainly concern discovering semantic patterns from global configuration of local geometries in a supervised learning manner. However, very few explore geometric properties revealing local surface manifolds embedded in 3D Euclidean space to discriminate semantic classes or object parts as additional supervision signals. This paper is the first attempt to propose a unique multi-task geometric learning network to improve semantic analysis by auxiliary geometric learning with local shape properties, which can be either generated via physical computation from point clouds themselves as self-supervision signals or provided as privileged information. Owing to explicitly encoding local shape manifolds in favor of semantic analysis, the proposed geometric self-supervised and privileged learning algorithms can achieve superior performance to their backbone baselines and other state-of-the-art methods, which are verified in the experiments on the popular benchmarks.



### Knowledge Representations in Technical Systems -- A Taxonomy
- **Arxiv ID**: http://arxiv.org/abs/2001.04835v2
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2001.04835v2)
- **Published**: 2020-01-14 15:00:09+00:00
- **Updated**: 2020-01-15 07:07:10+00:00
- **Authors**: Kristina Scharei, Florian Heidecker, Maarten Bieshaar
- **Comment**: 21 pages, 7 figures
- **Journal**: None
- **Summary**: The recent usage of technical systems in human-centric environments leads to the question, how to teach technical systems, e.g., robots, to understand, learn, and perform tasks desired by the human. Therefore, an accurate representation of knowledge is essential for the system to work as expected. This article mainly gives insight into different knowledge representation techniques and their categorization into various problem domains in artificial intelligence. Additionally, applications of presented knowledge representations are introduced in everyday robotics tasks. By means of the provided taxonomy, the search for a proper knowledge representation technique regarding a specific problem should be facilitated.



### Diabetic Retinopathy detection by retinal image recognizing
- **Arxiv ID**: http://arxiv.org/abs/2001.05835v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2001.05835v1)
- **Published**: 2020-01-14 16:36:59+00:00
- **Updated**: 2020-01-14 16:36:59+00:00
- **Authors**: Gilberto Luis De Conto Junior
- **Comment**: 10 source codes, 12 figures, 48 pages
- **Journal**: None
- **Summary**: Many people are affected by diabetes around the world. This disease may have type 1 and 2. Diabetes brings with it several complications including diabetic retinopathy, which is a disease that if not treated correctly can lead to irreversible damage in the patient's vision. The earlier it is detected, the better the chances that the patient will not lose vision. Methods of automating manual procedures are currently in evidence and the diagnostic process for retinopathy is manual with the physician analyzing the patient's retina on the monitor. The practice of image recognition can aid this detection by recognizing Diabetic Retinopathy patterns and comparing it with the patient's retina in diagnosis. This method can also assist in the act of telemedicine, in which people without access to the exam can benefit from the diagnosis provided by the application. The application development took place through convolutional neural networks, which do digital image processing analyzing each image pixel. The use of VGG-16 as a pre-trained model to the application basis was very useful and the final model accuracy was 82%.



### SimEx: Express Prediction of Inter-dataset Similarity by a Fleet of Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/2001.04893v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2001.04893v1)
- **Published**: 2020-01-14 16:52:50+00:00
- **Updated**: 2020-01-14 16:52:50+00:00
- **Authors**: Inseok Hwang, Jinho Lee, Frank Liu, Minsik Cho
- **Comment**: 12 pages
- **Journal**: None
- **Summary**: Knowing the similarity between sets of data has a number of positive implications in training an effective model, such as assisting an informed selection out of known datasets favorable to model transfer or data augmentation problems with an unknown dataset. Common practices to estimate the similarity between data include comparing in the original sample space, comparing in the embedding space from a model performing a certain task, or fine-tuning a pretrained model with different datasets and evaluating the performance changes therefrom. However, these practices would suffer from shallow comparisons, task-specific biases, or extensive time and computations required to perform comparisons. We present SimEx, a new method for early prediction of inter-dataset similarity using a set of pretrained autoencoders each of which is dedicated to reconstructing a specific part of known data. Specifically, our method takes unknown data samples as input to those pretrained autoencoders, and evaluate the difference between the reconstructed output samples against their original input samples. Our intuition is that, the more similarity exists between the unknown data samples and the part of known data that an autoencoder was trained with, the better chances there could be that this autoencoder makes use of its trained knowledge, reconstructing output samples closer to the originals. We demonstrate that our method achieves more than 10x speed-up in predicting inter-dataset similarity compared to common similarity-estimating practices. We also demonstrate that the inter-dataset similarity estimated by our method is well-correlated with common practices and outperforms the baselines approaches of comparing at sample- or embedding-spaces, without newly training anything at the comparison time.



### Convolutional Mean: A Simple Convolutional Neural Network for Illuminant Estimation
- **Arxiv ID**: http://arxiv.org/abs/2001.04911v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.04911v1)
- **Published**: 2020-01-14 17:11:17+00:00
- **Updated**: 2020-01-14 17:11:17+00:00
- **Authors**: Han Gong
- **Comment**: Accepted by BMVC 2019
- **Journal**: None
- **Summary**: We present Convolutional Mean (CM) - a simple and fast convolutional neural network for illuminant estimation. Our proposed method only requires a small neural network model (1.1K parameters) and a 48 x 32 thumbnail input image. Our unoptimized Python implementation takes 1 ms/image, which is arguably 3-3750x faster than the current leading solutions with similar accuracy. Using two public datasets, we show that our proposed light-weight method offers accuracy comparable to the current leading methods' (which consist of thousands/millions of parameters) across several measures.



### Unsupervised Domain Adaptation in Person re-ID via k-Reciprocal Clustering and Large-Scale Heterogeneous Environment Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2001.04928v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.04928v1)
- **Published**: 2020-01-14 17:43:52+00:00
- **Updated**: 2020-01-14 17:43:52+00:00
- **Authors**: Devinder Kumar, Parthipan Siva, Paul Marchwica, Alexander Wong
- **Comment**: None
- **Journal**: None
- **Summary**: An ongoing major challenge in computer vision is the task of person re-identification, where the goal is to match individuals across different, non-overlapping camera views. While recent success has been achieved via supervised learning using deep neural networks, such methods have limited widespread adoption due to the need for large-scale, customized data annotation. As such, there has been a recent focus on unsupervised learning approaches to mitigate the data annotation issue; however, current approaches in literature have limited performance compared to supervised learning approaches as well as limited applicability for adoption in new environments. In this paper, we address the aforementioned challenges faced in person re-identification for real-world, practical scenarios by introducing a novel, unsupervised domain adaptation approach for person re-identification. This is accomplished through the introduction of: i) k-reciprocal tracklet Clustering for Unsupervised Domain Adaptation (ktCUDA) (for pseudo-label generation on target domain), and ii) Synthesized Heterogeneous RE-id Domain (SHRED) composed of large-scale heterogeneous independent source environments (for improving robustness and adaptability to a wide diversity of target environments). Experimental results across four different image and video benchmark datasets show that the proposed ktCUDA and SHRED approach achieves an average improvement of +5.7 mAP in re-identification performance when compared to existing state-of-the-art methods, as well as demonstrate better adaptability to different types of environments.



### ImagineNet: Restyling Apps Using Neural Style Transfer
- **Arxiv ID**: http://arxiv.org/abs/2001.04932v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2001.04932v2)
- **Published**: 2020-01-14 17:47:50+00:00
- **Updated**: 2020-03-04 20:23:40+00:00
- **Authors**: Michael H. Fischer, Richard R. Yang, Monica S. Lam
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents ImagineNet, a tool that uses a novel neural style transfer model to enable end-users and app developers to restyle GUIs using an image of their choice. Former neural style transfer techniques are inadequate for this application because they produce GUIs that are illegible and hence nonfunctional. We propose a neural solution by adding a new loss term to the original formulation, which minimizes the squared error in the uncentered cross-covariance of features from different levels in a CNN between the style and output images. ImagineNet retains the details of GUIs, while transferring the colors and textures of the art. We presented GUIs restyled with ImagineNet as well as other style transfer techniques to 50 evaluators and all preferred those of ImagineNet. We show how ImagineNet can be used to restyle (1) the graphical assets of an app, (2) an app with user-supplied content, and (3) an app with dynamically generated GUIs.



### Neural Human Video Rendering by Learning Dynamic Textures and Rendering-to-Video Translation
- **Arxiv ID**: http://arxiv.org/abs/2001.04947v3
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2001.04947v3)
- **Published**: 2020-01-14 18:06:27+00:00
- **Updated**: 2021-07-05 21:08:52+00:00
- **Authors**: Lingjie Liu, Weipeng Xu, Marc Habermann, Michael Zollhoefer, Florian Bernard, Hyeongwoo Kim, Wenping Wang, Christian Theobalt
- **Comment**: None
- **Journal**: None
- **Summary**: Synthesizing realistic videos of humans using neural networks has been a popular alternative to the conventional graphics-based rendering pipeline due to its high efficiency. Existing works typically formulate this as an image-to-image translation problem in 2D screen space, which leads to artifacts such as over-smoothing, missing body parts, and temporal instability of fine-scale detail, such as pose-dependent wrinkles in the clothing. In this paper, we propose a novel human video synthesis method that approaches these limiting factors by explicitly disentangling the learning of time-coherent fine-scale details from the embedding of the human in 2D screen space. More specifically, our method relies on the combination of two convolutional neural networks (CNNs). Given the pose information, the first CNN predicts a dynamic texture map that contains time-coherent high-frequency details, and the second CNN conditions the generation of the final video on the temporally coherent output of the first CNN. We demonstrate several applications of our approach, such as human reenactment and novel view synthesis from monocular video, where we show significant improvement over the state of the art both qualitatively and quantitatively.



### Unifying Training and Inference for Panoptic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2001.04982v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.04982v2)
- **Published**: 2020-01-14 18:58:24+00:00
- **Updated**: 2020-05-26 18:44:52+00:00
- **Authors**: Qizhu Li, Xiaojuan Qi, Philip H. S. Torr
- **Comment**: CVPR 2020
- **Journal**: None
- **Summary**: We present an end-to-end network to bridge the gap between training and inference pipeline for panoptic segmentation, a task that seeks to partition an image into semantic regions for "stuff" and object instances for "things". In contrast to recent works, our network exploits a parametrised, yet lightweight panoptic segmentation submodule, powered by an end-to-end learnt dense instance affinity, to capture the probability that any pair of pixels belong to the same instance. This panoptic submodule gives rise to a novel propagation mechanism for panoptic logits and enables the network to output a coherent panoptic segmentation map for both "stuff" and "thing" classes, without any post-processing. Reaping the benefits of end-to-end training, our full system sets new records on the popular street scene dataset, Cityscapes, achieving 61.4 PQ with a ResNet-50 backbone using only the fine annotations. On the challenging COCO dataset, our ResNet-50-based network also delivers state-of-the-art accuracy of 43.4 PQ. Moreover, our network flexibly works with and without object mask cues, performing competitively under both settings, which is of interest for applications with computation budgets.



### Total Deep Variation for Linear Inverse Problems
- **Arxiv ID**: http://arxiv.org/abs/2001.05005v2
- **DOI**: None
- **Categories**: **math.OC**, cs.CV, cs.LG, 68T45, 93A30, 34H05, 49K15, 65L05
- **Links**: [PDF](http://arxiv.org/pdf/2001.05005v2)
- **Published**: 2020-01-14 19:01:50+00:00
- **Updated**: 2020-02-17 19:39:23+00:00
- **Authors**: Erich Kobler, Alexander Effland, Karl Kunisch, Thomas Pock
- **Comment**: 21 pages, 10 figures
- **Journal**: None
- **Summary**: Diverse inverse problems in imaging can be cast as variational problems composed of a task-specific data fidelity term and a regularization term. In this paper, we propose a novel learnable general-purpose regularizer exploiting recent architectural design patterns from deep learning. We cast the learning problem as a discrete sampled optimal control problem, for which we derive the adjoint state equations and an optimality condition. By exploiting the variational structure of our approach, we perform a sensitivity analysis with respect to the learned parameters obtained from different training datasets. Moreover, we carry out a nonlinear eigenfunction analysis, which reveals interesting properties of the learned regularizer. We show state-of-the-art performance for classical image restoration and medical image reconstruction problems.



### A smile I could recognise in a thousand: Automatic identification of identity from dental radiography
- **Arxiv ID**: http://arxiv.org/abs/2001.05006v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.05006v1)
- **Published**: 2020-01-14 19:02:06+00:00
- **Updated**: 2020-01-14 19:02:06+00:00
- **Authors**: Oscar de Felice, Gustavo de Felice
- **Comment**: 10 pages, 12 figures
- **Journal**: None
- **Summary**: In this paper, we present a method to automatically compare multiple radiographs in order to find the identity of a patient out of the dental features. The method is based on the matching of image features, previously extracted by computer vision algorithms for image descriptor recognition. The principal application (being also our motivation to study the problem) of such a method would be in victim identification in mass disasters.



### Emerging Disentanglement in Auto-Encoder Based Unsupervised Image Content Transfer
- **Arxiv ID**: http://arxiv.org/abs/2001.05017v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2001.05017v1)
- **Published**: 2020-01-14 19:36:47+00:00
- **Updated**: 2020-01-14 19:36:47+00:00
- **Authors**: Ori Press, Tomer Galanti, Sagie Benaim, Lior Wolf
- **Comment**: None
- **Journal**: ICLR 2019
- **Summary**: We study the problem of learning to map, in an unsupervised way, between domains A and B, such that the samples b in B contain all the information that exists in samples a in A and some additional information. For example, ignoring occlusions, B can be people with glasses, A people without, and the glasses, would be the added information. When mapping a sample a from the first domain to the other domain, the missing information is replicated from an independent reference sample b in B. Thus, in the above example, we can create, for every person without glasses a version with the glasses observed in any face image.   Our solution employs a single two-pathway encoder and a single decoder for both domains. The common part of the two domains and the separate part are encoded as two vectors, and the separate part is fixed at zero for domain A. The loss terms are minimal and involve reconstruction losses for the two domains and a domain confusion term. Our analysis shows that under mild assumptions, this architecture, which is much simpler than the literature guided-translation methods, is enough to ensure disentanglement between the two domains. We present convincing results in a few visual domains, such as no-glasses to glasses, adding facial hair based on a reference image, etc.



### Machine Learning Pipeline for Segmentation and Defect Identification from High Resolution Transmission Electron Microscopy Data
- **Arxiv ID**: http://arxiv.org/abs/2001.05022v2
- **DOI**: 10.1017/S1431927621000386
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2001.05022v2)
- **Published**: 2020-01-14 19:49:30+00:00
- **Updated**: 2021-02-23 23:30:56+00:00
- **Authors**: C. K. Groschner, Christina Choi, M. C. Scott
- **Comment**: 10 pages, 5 figures
- **Journal**: None
- **Summary**: In the field of transmission electron microscopy, data interpretation often lags behind acquisition methods, as image processing methods often have to be manually tailored to individual datasets. Machine learning offers a promising approach for fast, accurate analysis of electron microscopy data. Here, we demonstrate a flexible two step pipeline for analysis of high resolution transmission electron microscopy data, which uses a U-Net for segmentation followed by a random forest for detection of stacking faults. Our trained U-Net is able to segment nanoparticle regions from amorphous background with a Dice coefficient of 0.8 and significantly outperforms traditional image segmentation methods. Using these segmented regions, we are then able to classify whether nanoparticles contain a visible stacking fault with 86% accuracy. We provide this adaptable pipeline as an open source tool for the community. The combined output of the segmentation network and classifier offer a way to determine statistical distributions of features of interest, such as size, shape and defect presence, enabling detection of correlations between these features.



### Unifying Deep Local and Global Features for Image Search
- **Arxiv ID**: http://arxiv.org/abs/2001.05027v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.05027v4)
- **Published**: 2020-01-14 19:59:51+00:00
- **Updated**: 2020-09-15 18:21:56+00:00
- **Authors**: Bingyi Cao, Andre Araujo, Jack Sim
- **Comment**: ECCV'20 paper
- **Journal**: None
- **Summary**: Image retrieval is the problem of searching an image database for items that are similar to a query image. To address this task, two main types of image representations have been studied: global and local image features. In this work, our key contribution is to unify global and local features into a single deep model, enabling accurate retrieval with efficient feature extraction. We refer to the new model as DELG, standing for DEep Local and Global features. We leverage lessons from recent feature learning work and propose a model that combines generalized mean pooling for global features and attentive selection for local features. The entire network can be learned end-to-end by carefully balancing the gradient flow between two heads -- requiring only image-level labels. We also introduce an autoencoder-based dimensionality reduction technique for local features, which is integrated into the model, improving training efficiency and matching performance. Comprehensive experiments show that our model achieves state-of-the-art image retrieval on the Revisited Oxford and Paris datasets, and state-of-the-art single-model instance-level recognition on the Google Landmarks dataset v2. Code and models are available at https://github.com/tensorflow/models/tree/master/research/delf .



### Single Image Depth Estimation Trained via Depth from Defocus Cues
- **Arxiv ID**: http://arxiv.org/abs/2001.05036v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.05036v1)
- **Published**: 2020-01-14 20:22:54+00:00
- **Updated**: 2020-01-14 20:22:54+00:00
- **Authors**: Shir Gur, Lior Wolf
- **Comment**: None
- **Journal**: None
- **Summary**: Estimating depth from a single RGB images is a fundamental task in computer vision, which is most directly solved using supervised deep learning. In the field of unsupervised learning of depth from a single RGB image, depth is not given explicitly. Existing work in the field receives either a stereo pair, a monocular video, or multiple views, and, using losses that are based on structure-from-motion, trains a depth estimation network. In this work, we rely, instead of different views, on depth from focus cues. Learning is based on a novel Point Spread Function convolutional layer, which applies location specific kernels that arise from the Circle-Of-Confusion in each image location. We evaluate our method on data derived from five common datasets for depth estimation and lightfield images, and present results that are on par with supervised methods on KITTI and Make3D datasets and outperform unsupervised learning approaches. Since the phenomenon of depth from defocus is not dataset specific, we hypothesize that learning based on it would overfit less to the specific content in each dataset. Our experiments show that this is indeed the case, and an estimator learned on one dataset using our method provides better results on other datasets, than the directly supervised methods.



### DeepFactors: Real-Time Probabilistic Dense Monocular SLAM
- **Arxiv ID**: http://arxiv.org/abs/2001.05049v1
- **DOI**: 10.1109/LRA.2020.2965415
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.05049v1)
- **Published**: 2020-01-14 21:08:51+00:00
- **Updated**: 2020-01-14 21:08:51+00:00
- **Authors**: Jan Czarnowski, Tristan Laidlow, Ronald Clark, Andrew J. Davison
- **Comment**: RA-L
- **Journal**: None
- **Summary**: The ability to estimate rich geometry and camera motion from monocular imagery is fundamental to future interactive robotics and augmented reality applications. Different approaches have been proposed that vary in scene geometry representation (sparse landmarks, dense maps), the consistency metric used for optimising the multi-view problem, and the use of learned priors. We present a SLAM system that unifies these methods in a probabilistic framework while still maintaining real-time performance. This is achieved through the use of a learned compact depth map representation and reformulating three different types of errors: photometric, reprojection and geometric, which we make use of within standard factor graph software. We evaluate our system on trajectory estimation and depth reconstruction on real-world sequences and present various examples of estimated dense geometry.



### Hippocampus Segmentation on Epilepsy and Alzheimer's Disease Studies with Multiple Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2001.05058v2
- **DOI**: 10.1016/j.heliyon.2021.e06226
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2001.05058v2)
- **Published**: 2020-01-14 21:57:46+00:00
- **Updated**: 2021-02-10 21:23:29+00:00
- **Authors**: Diedre Carmo, Bruna Silva, Clarissa Yasuda, Letícia Rittner, Roberto Lotufo
- **Comment**: Code is available at https://github.com/dscarmo/e2dhipseg Published
  in Heliyon:
  https://www.sciencedirect.com/science/article/pii/S2405844021003315
- **Journal**: Heliyon, Volume 7, Issue 2, 2021
- **Summary**: Hippocampus segmentation on magnetic resonance imaging is of key importance for the diagnosis, treatment decision and investigation of neuropsychiatric disorders. Automatic segmentation is an active research field, with many recent models using deep learning. Most current state-of-the art hippocampus segmentation methods train their methods on healthy or Alzheimer's disease patients from public datasets. This raises the question whether these methods are capable of recognizing the hippocampus on a different domain, that of epilepsy patients with hippocampus resection. In this paper we present a state-of-the-art, open source, ready-to-use, deep learning based hippocampus segmentation method. It uses an extended 2D multi-orientation approach, with automatic pre-processing and orientation alignment. The methodology was developed and validated using HarP, a public Alzheimer's disease hippocampus segmentation dataset. We test this methodology alongside other recent deep learning methods, in two domains: The HarP test set and an in-house epilepsy dataset, containing hippocampus resections, named HCUnicamp. We show that our method, while trained only in HarP, surpasses others from the literature in both the HarP test set and HCUnicamp in Dice. Additionally, Results from training and testing in HCUnicamp volumes are also reported separately, alongside comparisons between training and testing in epilepsy and Alzheimer's data and vice versa. Although current state-of-the-art methods, including our own, achieve upwards of 0.9 Dice in HarP, all tested methods, including our own, produced false positives in HCUnicamp resection regions, showing that there is still room for improvement for hippocampus segmentation methods when resection is involved.



### Recognizing Video Events with Varying Rhythms
- **Arxiv ID**: http://arxiv.org/abs/2001.05060v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.05060v1)
- **Published**: 2020-01-14 22:06:48+00:00
- **Updated**: 2020-01-14 22:06:48+00:00
- **Authors**: Yikang Li, Tianshu Yu, Baoxin Li
- **Comment**: None
- **Journal**: None
- **Summary**: Recognizing Video events in long, complex videos with multiple sub-activities has received persistent attention recently. This task is more challenging than traditional action recognition with short, relatively homogeneous video clips. In this paper, we investigate the problem of recognizing long and complex events with varying action rhythms, which has not been considered in the literature but is a practical challenge. Our work is inspired in part by how humans identify events with varying rhythms: quickly catching frames contributing most to a specific event. We propose a two-stage \emph{end-to-end} framework, in which the first stage selects the most significant frames while the second stage recognizes the event using the selected frames. Our model needs only \emph{event-level labels} in the training stage, and thus is more practical when the sub-activity labels are missing or difficult to obtain. The results of extensive experiments show that our model can achieve significant improvement in event recognition from long videos while maintaining high accuracy even if the test videos suffer from severe rhythm changes. This demonstrates the potential of our method for real-world video-based applications, where test and training videos can differ drastically in rhythms of sub-activities.



### A Sample Selection Approach for Universal Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2001.05071v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.05071v1)
- **Published**: 2020-01-14 22:28:43+00:00
- **Updated**: 2020-01-14 22:28:43+00:00
- **Authors**: Omri Lifshitz, Lior Wolf
- **Comment**: None
- **Journal**: None
- **Summary**: We study the problem of unsupervised domain adaption in the universal scenario, in which only some of the classes are shared between the source and target domains. We present a scoring scheme that is effective in identifying the samples of the shared classes. The score is used to select which samples in the target domain to pseudo-label during training. Another loss term encourages diversity of labels within each batch. Taken together, our method is shown to outperform, by a sizable margin, the current state of the art on the literature benchmarks.



### Microvascular Dynamics from 4D Microscopy Using Temporal Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2001.05076v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2001.05076v1)
- **Published**: 2020-01-14 22:55:03+00:00
- **Updated**: 2020-01-14 22:55:03+00:00
- **Authors**: Shir Gur, Lior Wolf, Lior Golgher, Pablo Blinder
- **Comment**: None
- **Journal**: None
- **Summary**: Recently developed methods for rapid continuous volumetric two-photon microscopy facilitate the observation of neuronal activity in hundreds of individual neurons and changes in blood flow in adjacent blood vessels across a large volume of living brain at unprecedented spatio-temporal resolution. However, the high imaging rate necessitates fully automated image analysis, whereas tissue turbidity and photo-toxicity limitations lead to extremely sparse and noisy imagery. In this work, we extend a recently proposed deep learning volumetric blood vessel segmentation network, such that it supports temporal analysis. With this technology, we are able to track changes in cerebral blood volume over time and identify spontaneous arterial dilations that propagate towards the pial surface. This new capability is a promising step towards characterizing the hemodynamic response function upon which functional magnetic resonance imaging (fMRI) is based.



### Automated Anonymisation of Visual and Audio Data in Classroom Studies
- **Arxiv ID**: http://arxiv.org/abs/2001.05080v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.05080v1)
- **Published**: 2020-01-14 23:29:05+00:00
- **Updated**: 2020-01-14 23:29:05+00:00
- **Authors**: Ömer Sümer, Peter Gerjets, Ulrich Trautwein, Enkelejda Kasneci
- **Comment**: The Workshops of the Thirty-Fourth AAAI Conference on Artificial
  Intelligence
- **Journal**: None
- **Summary**: Understanding students' and teachers' verbal and non-verbal behaviours during instruction may help infer valuable information regarding the quality of teaching. In education research, there have been many studies that aim to measure students' attentional focus on learning-related tasks: Based on audio-visual recordings and manual or automated ratings of behaviours of teachers and students. Student data is, however, highly sensitive. Therefore, ensuring high standards of data protection and privacy has the utmost importance in current practices. For example, in the context of teaching management studies, data collection is carried out with the consent of pupils, parents, teachers and school administrations. Nevertheless, there may often be students whose data cannot be used for research purposes. Excluding these students from the classroom is an unnatural intrusion into the organisation of the classroom. A possible solution would be to request permission to record the audio-visual recordings of all students (including those who do not voluntarily participate in the study) and to anonymise their data. Yet, the manual anonymisation of audio-visual data is very demanding. In this study, we examine the use of artificial intelligence methods to automatically anonymise the visual and audio data of a particular person.



### Runtime Deep Model Multiplexing for Reduced Latency and Energy Consumption Inference
- **Arxiv ID**: http://arxiv.org/abs/2001.05870v2
- **DOI**: None
- **Categories**: **cs.DC**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2001.05870v2)
- **Published**: 2020-01-14 23:49:51+00:00
- **Updated**: 2020-09-17 17:07:31+00:00
- **Authors**: Amir Erfan Eshratifar, Massoud Pedram
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a learning algorithm to design a light-weight neural multiplexer that given the input and computational resource requirements, calls the model that will consume the minimum compute resources for a successful inference. Mobile devices can use the proposed algorithm to offload the hard inputs to the cloud while inferring the easy ones locally. Besides, in the large scale cloud-based intelligent applications, instead of replicating the most-accurate model, a range of small and large models can be multiplexed from depending on the input's complexity which will save the cloud's computational resources. The input complexity or hardness is determined by the number of models that can predict the correct label. For example, if no model can predict the label correctly, then the input is considered as the hardest. The proposed algorithm allows the mobile device to detect the inputs that can be processed locally and the ones that require a larger model and should be sent a cloud server. Therefore, the mobile user benefits from not only the local processing but also from an accurate model hosted on a cloud server. Our experimental results show that the proposed algorithm improves mobile's model accuracy by 8.52% which is because of those inputs that are properly selected and offloaded to the cloud server. In addition, it saves the cloud providers' compute resources by a factor of 2.85x as small models are chosen for easier inputs.



