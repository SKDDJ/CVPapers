# Arxiv Papers in cs.CV on 2020-01-16
### Detection and Tracking Meet Drones Challenge
- **Arxiv ID**: http://arxiv.org/abs/2001.06303v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.06303v3)
- **Published**: 2020-01-16 00:11:56+00:00
- **Updated**: 2021-10-04 03:37:37+00:00
- **Authors**: Pengfei Zhu, Longyin Wen, Dawei Du, Xiao Bian, Heng Fan, Qinghua Hu, Haibin Ling
- **Comment**: Accepted to TPAMI; arXiv admin note: text overlap with
  arXiv:1804.07437
- **Journal**: None
- **Summary**: Drones, or general UAVs, equipped with cameras have been fast deployed with a wide range of applications, including agriculture, aerial photography, and surveillance. Consequently, automatic understanding of visual data collected from drones becomes highly demanding, bringing computer vision and drones more and more closely. To promote and track the developments of object detection and tracking algorithms, we have organized three challenge workshops in conjunction with ECCV 2018, ICCV 2019 and ECCV 2020, attracting more than 100 teams around the world. We provide a large-scale drone captured dataset, VisDrone, which includes four tracks, i.e., (1) image object detection, (2) video object detection, (3) single object tracking, and (4) multi-object tracking. In this paper, we first present a thorough review of object detection and tracking datasets and benchmarks, and discuss the challenges of collecting large-scale drone-based object detection and tracking datasets with fully manual annotations. After that, we describe our VisDrone dataset, which is captured over various urban/suburban areas of 14 different cities across China from North to South. Being the largest such dataset ever published, VisDrone enables extensive evaluation and investigation of visual analysis algorithms for the drone platform. We provide a detailed analysis of the current state of the field of large-scale object detection and tracking on drones, and conclude the challenge as well as propose future directions. We expect the benchmark largely boost the research and development in video analysis on drone platforms. All the datasets and experimental results can be downloaded from https://github.com/VisDrone/VisDrone-Dataset.



### Synergetic Reconstruction from 2D Pose and 3D Motion for Wide-Space Multi-Person Video Motion Capture in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2001.05613v2
- **DOI**: 10.1016/j.imavis.2020.104028
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2001.05613v2)
- **Published**: 2020-01-16 02:14:59+00:00
- **Updated**: 2020-10-14 04:08:05+00:00
- **Authors**: Takuya Ohashi, Yosuke Ikegami, Yoshihiko Nakamura
- **Comment**: None
- **Journal**: Image and Vision Computing, Volume 104, 2020
- **Summary**: Although many studies have investigated markerless motion capture, the technology has not been applied to real sports or concerts. In this paper, we propose a markerless motion capture method with spatiotemporal accuracy and smoothness from multiple cameras in wide-space and multi-person environments. The proposed method predicts each person's 3D pose and determines the bounding box of multi-camera images small enough. This prediction and spatiotemporal filtering based on human skeletal model enables 3D reconstruction of the person and demonstrates high-accuracy. The accurate 3D reconstruction is then used to predict the bounding box of each camera image in the next frame. This is feedback from the 3D motion to 2D pose, and provides a synergetic effect on the overall performance of video motion capture. We evaluated the proposed method using various datasets and a real sports field. The experimental results demonstrate that the mean per joint position error (MPJPE) is 31.5 mm and the percentage of correct parts (PCP) is 99.5% for five people dynamically moving while satisfying the range of motion (RoM). Video demonstration, datasets, and additional materials are posted on our project page.



### Delving Deeper into the Decoder for Video Captioning
- **Arxiv ID**: http://arxiv.org/abs/2001.05614v3
- **DOI**: 10.3233/FAIA200204
- **Categories**: **cs.CV**, cs.CL, 68T45, 68T50, I.2.10; I.2.7
- **Links**: [PDF](http://arxiv.org/pdf/2001.05614v3)
- **Published**: 2020-01-16 02:18:27+00:00
- **Updated**: 2020-02-15 01:31:21+00:00
- **Authors**: Haoran Chen, Jianmin Li, Xiaolin Hu
- **Comment**: 8 pages, 3 figures, European Conference on Artificial Intelligence.
  ECAI 2020
- **Journal**: None
- **Summary**: Video captioning is an advanced multi-modal task which aims to describe a video clip using a natural language sentence. The encoder-decoder framework is the most popular paradigm for this task in recent years. However, there exist some problems in the decoder of a video captioning model. We make a thorough investigation into the decoder and adopt three techniques to improve the performance of the model. First of all, a combination of variational dropout and layer normalization is embedded into a recurrent unit to alleviate the problem of overfitting. Secondly, a new online method is proposed to evaluate the performance of a model on a validation set so as to select the best checkpoint for testing. Finally, a new training strategy called professional learning is proposed which uses the strengths of a captioning model and bypasses its weaknesses. It is demonstrated in the experiments on Microsoft Research Video Description Corpus (MSVD) and MSR-Video to Text (MSR-VTT) datasets that our model has achieved the best results evaluated by BLEU, CIDEr, METEOR and ROUGE-L metrics with significant gains of up to 18% on MSVD and 3.5% on MSR-VTT compared with the previous state-of-the-art models.



### Curriculum Labeling: Revisiting Pseudo-Labeling for Semi-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2001.06001v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2001.06001v2)
- **Published**: 2020-01-16 03:24:27+00:00
- **Updated**: 2020-12-10 16:14:35+00:00
- **Authors**: Paola Cascante-Bonilla, Fuwen Tan, Yanjun Qi, Vicente Ordonez
- **Comment**: In the 35th AAAI Conference on Artificial Intelligence. AAAI 2021
- **Journal**: None
- **Summary**: In this paper we revisit the idea of pseudo-labeling in the context of semi-supervised learning where a learning algorithm has access to a small set of labeled samples and a large set of unlabeled samples. Pseudo-labeling works by applying pseudo-labels to samples in the unlabeled set by using a model trained on the combination of the labeled samples and any previously pseudo-labeled samples, and iteratively repeating this process in a self-training cycle. Current methods seem to have abandoned this approach in favor of consistency regularization methods that train models under a combination of different styles of self-supervised losses on the unlabeled samples and standard supervised losses on the labeled samples. We empirically demonstrate that pseudo-labeling can in fact be competitive with the state-of-the-art, while being more resilient to out-of-distribution samples in the unlabeled set. We identify two key factors that allow pseudo-labeling to achieve such remarkable results (1) applying curriculum learning principles and (2) avoiding concept drift by restarting model parameters before each self-training cycle. We obtain 94.91% accuracy on CIFAR-10 using only 4,000 labeled samples, and 68.87% top-1 accuracy on Imagenet-ILSVRC using only 10% of the labeled samples. The code is available at https://github.com/uvavision/Curriculum-Labeling



### Self-supervised visual feature learning with curriculum
- **Arxiv ID**: http://arxiv.org/abs/2001.05634v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.05634v1)
- **Published**: 2020-01-16 03:28:58+00:00
- **Updated**: 2020-01-16 03:28:58+00:00
- **Authors**: Vishal Keshav, Fabien Delattre
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: Self-supervised learning techniques have shown their abilities to learn meaningful feature representation. This is made possible by training a model on pretext tasks that only requires to find correlations between inputs or parts of inputs. However, such pretext tasks need to be carefully hand selected to avoid low level signals that could make those pretext tasks trivial. Moreover, removing those shortcuts often leads to the loss of some semantically valuable information. We show that it directly impacts the speed of learning of the downstream task. In this paper we took inspiration from curriculum learning to progressively remove low level signals and show that it significantly increase the speed of convergence of the downstream task.



### Multimodal Story Generation on Plural Images
- **Arxiv ID**: http://arxiv.org/abs/2001.10980v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2001.10980v2)
- **Published**: 2020-01-16 03:39:00+00:00
- **Updated**: 2021-06-05 10:44:37+00:00
- **Authors**: Jing Jiang
- **Comment**: This is an undergraduate project report. Completed Dec. 2019 at the
  Cooper Union
- **Journal**: None
- **Summary**: Traditionally, text generation models take in a sequence of text as input, and iteratively generate the next most probable word using pre-trained parameters. In this work, we propose the architecture to use images instead of text as the input of the text generation model, called StoryGen. In the architecture, we design a Relational Text Data Generator algorithm that relates different features from multiple images. The output samples from the model demonstrate the ability to generate meaningful paragraphs of text containing the extracted features from the input images. This is an undergraduate project report. Completed Dec. 2019 at the Cooper Union.



### PDANet: Pyramid Density-aware Attention Net for Accurate Crowd Counting
- **Arxiv ID**: http://arxiv.org/abs/2001.05643v10
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2001.05643v10)
- **Published**: 2020-01-16 04:26:05+00:00
- **Updated**: 2020-04-29 03:04:05+00:00
- **Authors**: Saeed Amirgholipour, Xiangjian He, Wenjing Jia, Dadong Wang, Lei Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Crowd counting, i.e., estimating the number of people in a crowded area, has attracted much interest in the research community. Although many attempts have been reported, crowd counting remains an open real-world problem due to the vast scale variations in crowd density within the interested area, and severe occlusion among the crowd. In this paper, we propose a novel Pyramid Density-Aware Attention-based network, abbreviated as PDANet, that leverages the attention, pyramid scale feature and two branch decoder modules for density-aware crowd counting. The PDANet utilizes these modules to extract different scale features, focus on the relevant information, and suppress the misleading ones. We also address the variation of crowdedness levels among different images with an exclusive Density-Aware Decoder (DAD). For this purpose, a classifier evaluates the density level of the input features and then passes them to the corresponding high and low crowded DAD modules. Finally, we generate an overall density map by considering the summation of low and high crowded density maps as spatial attention. Meanwhile, we employ two losses to create a precise density map for the input scene. Extensive evaluations conducted on the challenging benchmark datasets well demonstrate the superior performance of the proposed PDANet in terms of the accuracy of counting and generated density maps over the well-known state of the arts.



### Control of the Final-Phase of Closed-Loop Visual Grasping using Image-Based Visual Servoing
- **Arxiv ID**: http://arxiv.org/abs/2001.05650v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2001.05650v2)
- **Published**: 2020-01-16 05:07:01+00:00
- **Updated**: 2020-02-28 03:14:44+00:00
- **Authors**: Jesse Haviland, Feras Dayoub, Peter Corke
- **Comment**: Under review for RA-L and IROS 2020
- **Journal**: None
- **Summary**: This paper considers the final approach phase of visual-closed-loop grasping where the RGB-D camera is no longer able to provide valid depth information. Many current robotic grasping controllers are not closed-loop and therefore fail for moving objects. Closed-loop grasp controllers based on RGB-D imagery can track a moving object, but fail when the sensor's minimum object distance is violated just before grasping. To overcome this we propose the use of image-based visual servoing (IBVS) to guide the robot to the object-relative grasp pose using camera RGB information. IBVS robustly moves the camera to a goal pose defined implicitly in terms of an image-plane feature configuration. In this work, the goal image feature coordinates are predicted from RGB-D data to enable RGB-only tracking once depth data becomes unavailable -- this enables more reliable grasping of previously unseen moving objects. Experimental results are provided.



### Combining Progressive Rethinking and Collaborative Learning: A Deep Framework for In-Loop Filtering
- **Arxiv ID**: http://arxiv.org/abs/2001.05651v3
- **DOI**: 10.1109/TIP.2021.3068638
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2001.05651v3)
- **Published**: 2020-01-16 05:14:34+00:00
- **Updated**: 2021-03-31 09:33:27+00:00
- **Authors**: Dezhao Wang, Sifeng Xia, Wenhan Yang, Jiaying Liu
- **Comment**: Accepted for publication in IEEE Transactions on Image Processing
  (TIP). Website available at https://dezhao-wang.github.io/PRN-v2/
- **Journal**: None
- **Summary**: In this paper, we aim to address issues of (1) joint spatial-temporal modeling and (2) side information injection for deep-learning based in-loop filter. For (1), we design a deep network with both progressive rethinking and collaborative learning mechanisms to improve quality of the reconstructed intra-frames and inter-frames, respectively. For intra coding, a Progressive Rethinking Network (PRN) is designed to simulate the human decision mechanism for effective spatial modeling. Our designed block introduces an additional inter-block connection to bypass a high-dimensional informative feature before the bottleneck module across blocks to review the complete past memorized experiences and rethinks progressively. For inter coding, the current reconstructed frame interacts with reference frames (peak quality frame and the nearest adjacent frame) collaboratively at the feature level. For (2), we extract both intra-frame and inter-frame side information for better context modeling. A coarse-to-fine partition map based on HEVC partition trees is built as the intra-frame side information. Furthermore, the warped features of the reference frames are offered as the inter-frame side information. Our PRN with intra-frame side information provides 9.0% BD-rate reduction on average compared to HEVC baseline under All-intra (AI) configuration. While under Low-Delay B (LDB), Low-Delay P (LDP) and Random Access (RA) configuration, our PRN with inter-frame side information provides 9.0%, 10.6% and 8.0% BD-rate reduction on average respectively. Our project webpage is https://dezhao-wang.github.io/PRN-v2/.



### LE-HGR: A Lightweight and Efficient RGB-based Online Gesture Recognition Network for Embedded AR Devices
- **Arxiv ID**: http://arxiv.org/abs/2001.05654v1
- **DOI**: 10.1109/ISMAR-Adjunct.2019.00-30
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2001.05654v1)
- **Published**: 2020-01-16 05:23:24+00:00
- **Updated**: 2020-01-16 05:23:24+00:00
- **Authors**: Hongwei Xie, Jiafang Wang, Baitao Shao, Jian Gu, Mingyang Li
- **Comment**: Published in: 2019 IEEE International Symposium on Mixed and
  Augmented Reality Adjunct (ISMAR-Adjunct)
- **Journal**: None
- **Summary**: Online hand gesture recognition (HGR) techniques are essential in augmented reality (AR) applications for enabling natural human-to-computer interaction and communication. In recent years, the consumer market for low-cost AR devices has been rapidly growing, while the technology maturity in this domain is still limited. Those devices are typical of low prices, limited memory, and resource-constrained computational units, which makes online HGR a challenging problem. To tackle this problem, we propose a lightweight and computationally efficient HGR framework, namely LE-HGR, to enable real-time gesture recognition on embedded devices with low computing power. We also show that the proposed method is of high accuracy and robustness, which is able to reach high-end performance in a variety of complicated interaction environments. To achieve our goal, we first propose a cascaded multi-task convolutional neural network (CNN) to simultaneously predict probabilities of hand detection and regress hand keypoint locations online. We show that, with the proposed cascaded architecture design, false-positive estimates can be largely eliminated. Additionally, an associated mapping approach is introduced to track the hand trace via the predicted locations, which addresses the interference of multi-handedness. Subsequently, we propose a trace sequence neural network (TraceSeqNN) to recognize the hand gesture by exploiting the motion features of the tracked trace. Finally, we provide a variety of experimental results to show that the proposed framework is able to achieve state-of-the-art accuracy with significantly reduced computational cost, which are the key properties for enabling real-time applications in low-cost commercial devices such as mobile devices and AR/VR headsets.



### Rethinking Motion Representation: Residual Frames with 3D ConvNets for Better Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2001.05661v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.05661v1)
- **Published**: 2020-01-16 05:49:13+00:00
- **Updated**: 2020-01-16 05:49:13+00:00
- **Authors**: Li Tao, Xueting Wang, Toshihiko Yamasaki
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, 3D convolutional networks yield good performance in action recognition. However, optical flow stream is still needed to ensure better performance, the cost of which is very high. In this paper, we propose a fast but effective way to extract motion features from videos utilizing residual frames as the input data in 3D ConvNets. By replacing traditional stacked RGB frames with residual ones, 20.5% and 12.5% points improvements over top-1 accuracy can be achieved on the UCF101 and HMDB51 datasets when trained from scratch. Because residual frames contain little information of object appearance, we further use a 2D convolutional network to extract appearance features and combine them with the results from residual frames to form a two-path solution. In three benchmark datasets, our two-path solution achieved better or comparable performances than those using additional optical flow methods, especially outperformed the state-of-the-art models on Mini-kinetics dataset. Further analysis indicates that better motion features can be extracted using residual frames with 3D ConvNets, and our residual-frame-input path is a good supplement for existing RGB-frame-input models.



### Predicting the Physical Dynamics of Unseen 3D Objects
- **Arxiv ID**: http://arxiv.org/abs/2001.06291v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2001.06291v1)
- **Published**: 2020-01-16 06:27:59+00:00
- **Updated**: 2020-01-16 06:27:59+00:00
- **Authors**: Davis Rempe, Srinath Sridhar, He Wang, Leonidas J. Guibas
- **Comment**: In Proceedings of Winter Conference on Applications of Computer
  Vision (WACV) 2020. arXiv admin note: text overlap with arXiv:1901.00466
- **Journal**: None
- **Summary**: Machines that can predict the effect of physical interactions on the dynamics of previously unseen object instances are important for creating better robots and interactive virtual worlds. In this work, we focus on predicting the dynamics of 3D objects on a plane that have just been subjected to an impulsive force. In particular, we predict the changes in state - 3D position, rotation, velocities, and stability. Different from previous work, our approach can generalize dynamics predictions to object shapes and initial conditions that were unseen during training. Our method takes the 3D object's shape as a point cloud and its initial linear and angular velocities as input. We extract shape features and use a recurrent neural network to predict the full change in state at each time step. Our model can support training with data from both a physics engine or the real world. Experiments show that we can accurately predict the changes in state for unseen object geometries and initial conditions.



### Probabilistic 3D Multi-Object Tracking for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2001.05673v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2001.05673v1)
- **Published**: 2020-01-16 06:38:02+00:00
- **Updated**: 2020-01-16 06:38:02+00:00
- **Authors**: Hsu-kuang Chiu, Antonio Prioletti, Jie Li, Jeannette Bohg
- **Comment**: None
- **Journal**: None
- **Summary**: 3D multi-object tracking is a key module in autonomous driving applications that provides a reliable dynamic representation of the world to the planning module. In this paper, we present our on-line tracking method, which made the first place in the NuScenes Tracking Challenge, held at the AI Driving Olympics Workshop at NeurIPS 2019. Our method estimates the object states by adopting a Kalman Filter. We initialize the state covariance as well as the process and observation noise covariance with statistics from the training set. We also use the stochastic information from the Kalman Filter in the data association step by measuring the Mahalanobis distance between the predicted object states and current object detections. Our experimental results on the NuScenes validation and test set show that our method outperforms the AB3DMOT baseline method by a large margin in the Average Multi-Object Tracking Accuracy (AMOTA) metric.



### Learning Spatiotemporal Features via Video and Text Pair Discrimination
- **Arxiv ID**: http://arxiv.org/abs/2001.05691v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.05691v3)
- **Published**: 2020-01-16 08:28:57+00:00
- **Updated**: 2021-01-28 01:43:34+00:00
- **Authors**: Tianhao Li, Limin Wang
- **Comment**: Technical Report
- **Journal**: None
- **Summary**: Current video representations heavily rely on learning from manually annotated video datasets which are time-consuming and expensive to acquire. We observe videos are naturally accompanied by abundant text information such as YouTube titles and Instagram captions. In this paper, we leverage this visual-textual connection to learn spatiotemporal features in an efficient weakly-supervised manner. We present a general cross-modal pair discrimination (CPD) framework to capture this correlation between a video and its associated text. Specifically, we adopt noise-contrastive estimation to tackle the computational issue imposed by the huge amount of pair instance classes and design a practical curriculum learning strategy. We train our CPD models on both standard video dataset (Kinetics-210k) and uncurated web video dataset (Instagram-300k) to demonstrate its effectiveness. Without further fine-tuning, the learnt models obtain competitive results for action classification on Kinetics under the linear classification protocol. Moreover, our visual model provides an effective initialization to fine-tune on downstream tasks, which yields a remarkable performance gain for action recognition on UCF101 and HMDB51, compared with the existing state-of-the-art self-supervised training methods. In addition, our CPD model yields a new state of the art for zero-shot action recognition on UCF101 by directly utilizing the learnt visual-textual embeddings. The code will be made available at https://github.com/MCG-NJU/CPD-Video.



### A Markerless Deep Learning-based 6 Degrees of Freedom PoseEstimation for with Mobile Robots using RGB Data
- **Arxiv ID**: http://arxiv.org/abs/2001.05703v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2001.05703v1)
- **Published**: 2020-01-16 09:13:31+00:00
- **Updated**: 2020-01-16 09:13:31+00:00
- **Authors**: Linh Kästner, Daniel Dimitrov, Jens Lambrecht
- **Comment**: 6 pages,5 figures. arXiv admin note: text overlap with
  arXiv:1912.12101
- **Journal**: None
- **Summary**: Augmented Reality has been subject to various integration efforts within industries due to its ability to enhance human machine interaction and understanding. Neural networks have achieved remarkable results in areas of computer vision, which bear great potential to assist and facilitate an enhanced Augmented Reality experience. However, most neural networks are computationally intensive and demand huge processing power thus, are not suitable for deployment on Augmented Reality devices. In this work we propose a method to deploy state of the art neural networks for real time 3D object localization on augmented reality devices. As a result, we provide a more automated method of calibrating the AR devices with mobile robotic systems. To accelerate the calibration process and enhance user experience, we focus on fast 2D detection approaches which are extracting the 3D pose of the object fast and accurately by using only 2D input. The results are implemented into an Augmented Reality application for intuitive robot control and sensor data visualization. For the 6D annotation of 2D images, we developed an annotation tool, which is, to our knowledge, the first open source tool to be available. We achieve feasible results which are generally applicable to any AR device thus making this work promising for further research in combining high demanding neural networks with Internet of Things devices.



### A lightweight target detection algorithm based on Mobilenet Convolution
- **Arxiv ID**: http://arxiv.org/abs/2002.03729v4
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2002.03729v4)
- **Published**: 2020-01-16 09:38:50+00:00
- **Updated**: 2023-08-09 10:01:08+00:00
- **Authors**: Nina Kuchuk, Shengquan Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Target detection algorithm based on deep learning needs high computer GPU configuration, even need to use high performance deep learning workstation, this not only makes the cost increase, also greatly limits the realizability of the ground, this paper introduces a kind of lightweight algorithm for target detection under the condition of the balance accuracy and computational efficiency, MobileNet as Backbone performs parameter The processing speed is 30fps on the RTX2060 card for images with the CNN separator layer. The processing speed is 30fps on the RTX2060 card for images with a resolution of 320*320.



### Adaptive Direction-Guided Structure Tensor Total Variation
- **Arxiv ID**: http://arxiv.org/abs/2001.05717v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2001.05717v1)
- **Published**: 2020-01-16 09:49:29+00:00
- **Updated**: 2020-01-16 09:49:29+00:00
- **Authors**: Ezgi Demircan-Tureyen, Mustafa E. Kamasak
- **Comment**: 13 pages, 6 figures, article
- **Journal**: None
- **Summary**: Direction-guided structure tensor total variation (DSTV) is a recently proposed regularization term that aims at increasing the sensitivity of the structure tensor total variation (STV) to the changes towards a predetermined direction. Despite of the plausible results obtained on the uni-directional images, the DSTV model is not applicable to the multi-directional images of real-world. In this study, we build a two-stage framework that brings adaptivity to DSTV. We design an alternative to STV, which encodes the first-order information within a local neighborhood under the guidance of spatially varying directional descriptors (i.e., orientation and the dose of anisotropy). In order to estimate those descriptors, we propose an efficient preprocessor that captures the local geometry based on the structure tensor. Through the extensive experiments, we demonstrate how beneficial the involvement of the directional information in STV is, by comparing the proposed method with the state-of-the-art analysis-based denoising models, both in terms of restoration quality and computational efficiency.



### SketchDesc: Learning Local Sketch Descriptors for Multi-view Correspondence
- **Arxiv ID**: http://arxiv.org/abs/2001.05744v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.05744v3)
- **Published**: 2020-01-16 11:31:21+00:00
- **Updated**: 2020-08-10 23:18:16+00:00
- **Authors**: Deng Yu, Lei Li, Youyi Zheng, Manfred Lau, Yi-Zhe Song, Chiew-Lan Tai, Hongbo Fu
- **Comment**: Accepted by IEEE Transactions on Circuits and Systems for Video
  Technology
- **Journal**: None
- **Summary**: In this paper, we study the problem of multi-view sketch correspondence, where we take as input multiple freehand sketches with different views of the same object and predict as output the semantic correspondence among the sketches. This problem is challenging since the visual features of corresponding points at different views can be very different. To this end, we take a deep learning approach and learn a novel local sketch descriptor from data. We contribute a training dataset by generating the pixel-level correspondence for the multi-view line drawings synthesized from 3D shapes. To handle the sparsity and ambiguity of sketches, we design a novel multi-branch neural network that integrates a patch-based representation and a multi-scale strategy to learn the pixel-level correspondence among multi-view sketches. We demonstrate the effectiveness of our proposed approach with extensive experiments on hand-drawn sketches and multi-view line drawings rendered from multiple 3D shape datasets.



### A Technology-aided Multi-modal Training Approach to Assist Abdominal Palpation Training and its Assessment in Medical Education
- **Arxiv ID**: http://arxiv.org/abs/2001.05745v1
- **DOI**: 10.1016/j.ijhcs.2020.102394
- **Categories**: **cs.HC**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2001.05745v1)
- **Published**: 2020-01-16 11:31:46+00:00
- **Updated**: 2020-01-16 11:31:46+00:00
- **Authors**: A. Asadipour, K. Debattista, V. Patel, A. Chalmers
- **Comment**: In Press
- **Journal**: None
- **Summary**: Computer-assisted multimodal training is an effective way of learning complex motor skills in various applications. In particular disciplines (eg. healthcare) incompetency in performing dexterous hands-on examinations (clinical palpation) may result in misdiagnosis of symptoms, serious injuries or even death. Furthermore, a high quality clinical examination can help to exclude significant pathology, and reduce time and cost of diagnosis by eliminating the need for unnecessary medical imaging. Medical palpation is used regularly as an effective preliminary diagnosis method all around the world but years of training are required currently to achieve competency. This paper focuses on a multimodal palpation training system to teach and improve clinical examination skills in relation to the abdomen. It is our aim to shorten significantly the palpation training duration by increasing the frequency of rehearsals as well as providing essential augmented feedback on how to perform various abdominal palpation techniques which has been captured and modelled from medical experts. Twenty three first year medical students divided into a control group (n=8), a semi-visually trained group (n=8), and a fully visually trained group (n=7) were invited to perform three palpation tasks (superficial, deep and liver). The medical students performances were assessed using both computer-based and human-based methods where a positive correlation was shown between the generated scores, r=.62, p(one-tailed)<.05. The visually-trained group significantly outperformed the control group in which abstract visualisation of applied forces and their palmar locations were provided to the students during each palpation examination (p<.05). Moreover, a positive trend was observed between groups when visual feedback was presented, J=132, z=2.62, r=0.55.



### Probabilistic 3D Multilabel Real-time Mapping for Multi-object Manipulation
- **Arxiv ID**: http://arxiv.org/abs/2001.05752v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2001.05752v1)
- **Published**: 2020-01-16 12:00:20+00:00
- **Updated**: 2020-01-16 12:00:20+00:00
- **Authors**: Kentaro Wada, Kei Okada, Masayuki Inaba
- **Comment**: 8 pages, 8 figures, IEEE/RSJ International Conference on Intelligent
  Robots and Systems (IROS) 2017
- **Journal**: None
- **Summary**: Probabilistic 3D map has been applied to object segmentation with multiple camera viewpoints, however, conventional methods lack of real-time efficiency and functionality of multilabel object mapping. In this paper, we propose a method to generate three-dimensional map with multilabel occupancy in real-time. Extending our previous work in which only target label occupancy is mapped, we achieve multilabel object segmentation in a single looking around action. We evaluate our method by testing segmentation accuracy with 39 different objects, and applying it to a manipulation task of multiple objects in the experiments. Our mapping-based method outperforms the conventional projection-based method by 40 - 96\% relative (12.6 mean $IU_{3d}$), and robot successfully recognizes (86.9\%) and manipulates multiple objects (60.7\%) in an environment with heavy occlusions.



### ScaIL: Classifier Weights Scaling for Class Incremental Learning
- **Arxiv ID**: http://arxiv.org/abs/2001.05755v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.05755v1)
- **Published**: 2020-01-16 12:10:45+00:00
- **Updated**: 2020-01-16 12:10:45+00:00
- **Authors**: Eden Belouadah, Adrian Popescu
- **Comment**: 8 pages, 4 figures, 2 tables, accepted in WACV2020
- **Journal**: None
- **Summary**: Incremental learning is useful if an AI agent needs to integrate data from a stream. The problem is non trivial if the agent runs on a limited computational budget and has a bounded memory of past data. In a deep learning approach, the constant computational budget requires the use of a fixed architecture for all incremental states. The bounded memory generates data imbalance in favor of new classes and a prediction bias toward them appears. This bias is commonly countered by introducing a data balancing step in addition to the basic network training. We depart from this approach and propose simple but efficient scaling of past class classifier weights to make them more comparable to those of new classes. Scaling exploits incremental state level statistics and is applied to the classifiers learned in the initial state of classes in order to profit from all their available data. We also question the utility of the widely used distillation loss component of incremental learning algorithms by comparing it to vanilla fine tuning in presence of a bounded memory. Evaluation is done against competitive baselines using four public datasets. Results show that the classifier weights scaling and the removal of the distillation are both beneficial.



### Fabricated Pictures Detection with Graph Matching
- **Arxiv ID**: http://arxiv.org/abs/2002.03720v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/2002.03720v1)
- **Published**: 2020-01-16 12:29:16+00:00
- **Updated**: 2020-01-16 12:29:16+00:00
- **Authors**: Binrui Shen, Qiang Niu, Shengxin Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: Fabricating experimental pictures in research work is a serious academic misconduct, which should better be detected in the reviewing process. However, due to large number of submissions, the detection whether a picture is fabricated or reused is laborious for reviewers, and sometimes is indistinct with human eyes. A tool for detecting similarity between images may help to alleviate this problem. Some methods based on local feature points matching work for most of the time, while these methods may result in mess of matchings due to ignorance of global relationship between features. We present a framework to detect similar, or perhaps fabricated, pictures with the graph matching techniques. A new iterative method is proposed, and experiments show that such a graph matching technique is better than the methods based only on local features for some cases.



### A Little Fog for a Large Turn
- **Arxiv ID**: http://arxiv.org/abs/2001.05873v1
- **DOI**: 10.1109/WACV45572.2020.9093549
- **Categories**: **cs.LG**, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2001.05873v1)
- **Published**: 2020-01-16 15:09:48+00:00
- **Updated**: 2020-01-16 15:09:48+00:00
- **Authors**: Harshitha Machiraju, Vineeth N Balasubramanian
- **Comment**: Accepted to WACV 2020
- **Journal**: None
- **Summary**: Small, carefully crafted perturbations called adversarial perturbations can easily fool neural networks. However, these perturbations are largely additive and not naturally found. We turn our attention to the field of Autonomous navigation wherein adverse weather conditions such as fog have a drastic effect on the predictions of these systems. These weather conditions are capable of acting like natural adversaries that can help in testing models. To this end, we introduce a general notion of adversarial perturbations, which can be created using generative models and provide a methodology inspired by Cycle-Consistent Generative Adversarial Networks to generate adversarial weather conditions for a given image. Our formulation and results show that these images provide a suitable testbed for steering models used in Autonomous navigation models. Our work also presents a more natural and general definition of Adversarial perturbations based on Perceptual Similarity.



### MixPath: A Unified Approach for One-shot Neural Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/2001.05887v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2001.05887v4)
- **Published**: 2020-01-16 15:24:26+00:00
- **Updated**: 2023-07-19 12:58:18+00:00
- **Authors**: Xiangxiang Chu, Shun Lu, Xudong Li, Bo Zhang
- **Comment**: ICCV2023
- **Journal**: None
- **Summary**: Blending multiple convolutional kernels is proved advantageous in neural architecture design. However, current two-stage neural architecture search methods are mainly limited to single-path search spaces. How to efficiently search models of multi-path structures remains a difficult problem. In this paper, we are motivated to train a one-shot multi-path supernet to accurately evaluate the candidate architectures. Specifically, we discover that in the studied search spaces, feature vectors summed from multiple paths are nearly multiples of those from a single path. Such disparity perturbs the supernet training and its ranking ability. Therefore, we propose a novel mechanism called Shadow Batch Normalization (SBN) to regularize the disparate feature statistics. Extensive experiments prove that SBNs are capable of stabilizing the optimization and improving ranking performance. We call our unified multi-path one-shot approach as MixPath, which generates a series of models that achieve state-of-the-art results on ImageNet.



### Continual Learning for Domain Adaptation in Chest X-ray Classification
- **Arxiv ID**: http://arxiv.org/abs/2001.05922v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2001.05922v1)
- **Published**: 2020-01-16 16:20:43+00:00
- **Updated**: 2020-01-16 16:20:43+00:00
- **Authors**: Matthias Lenga, Heinrich Schulz, Axel Saalbach
- **Comment**: None
- **Journal**: Proceedings of the Third Conference on Medical Imaging with Deep
  Learning, PMLR 121:413-423, 2020
- **Summary**: Over the last years, Deep Learning has been successfully applied to a broad range of medical applications. Especially in the context of chest X-ray classification, results have been reported which are on par, or even superior to experienced radiologists. Despite this success in controlled experimental environments, it has been noted that the ability of Deep Learning models to generalize to data from a new domain (with potentially different tasks) is often limited. In order to address this challenge, we investigate techniques from the field of Continual Learning (CL) including Joint Training (JT), Elastic Weight Consolidation (EWC) and Learning Without Forgetting (LWF). Using the ChestX-ray14 and the MIMIC-CXR datasets, we demonstrate empirically that these methods provide promising options to improve the performance of Deep Learning models on a target domain and to mitigate effectively catastrophic forgetting for the source domain. To this end, the best overall performance was obtained using JT, while for LWF competitive results could be achieved - even without accessing data from the source domain.



### Road Network and Travel Time Extraction from Multiple Look Angles with SpaceNet Data
- **Arxiv ID**: http://arxiv.org/abs/2001.05923v2
- **DOI**: 10.1109/IGARSS39084.2020.9324091
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.05923v2)
- **Published**: 2020-01-16 16:23:59+00:00
- **Updated**: 2021-03-02 16:48:50+00:00
- **Authors**: Adam Van Etten, Jacob Shermeyer, Daniel Hogan, Nicholas Weir, Ryan Lewis
- **Comment**: 4 pages, 5 figures. To appear at the 2020 IEEE International
  Geoscience and Remote Sensing Symposium
- **Journal**: None
- **Summary**: Identification of road networks and optimal routes directly from remote sensing is of critical importance to a broad array of humanitarian and commercial applications. Yet while identification of road pixels has been attempted before, estimation of route travel times from overhead imagery remains a novel problem, particularly for off-nadir overhead imagery. To this end, we extract road networks with travel time estimates from the SpaceNet MVOI dataset. Utilizing the CRESIv2 framework, we demonstrate the ability to extract road networks in various observation angles and quantify performance at 27 unique nadir angles with the graph-theoretic APLS_length and APLS_time metrics. A minimal gap of 0.03 between APLS_length and APLS_time scores indicates that our approach yields speed limits and travel times with very high fidelity. We also explore the utility of incorporating all available angles during model training, and find a peak score of APLS_time = 0.56. The combined model exhibits greatly improved robustness over angle-specific models, despite the very different appearance of road networks at extremely oblique off-nadir angles versus images captured from directly overhead.



### MeliusNet: Can Binary Neural Networks Achieve MobileNet-level Accuracy?
- **Arxiv ID**: http://arxiv.org/abs/2001.05936v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2001.05936v2)
- **Published**: 2020-01-16 16:56:10+00:00
- **Updated**: 2020-03-24 11:52:06+00:00
- **Authors**: Joseph Bethge, Christian Bartz, Haojin Yang, Ying Chen, Christoph Meinel
- **Comment**: None
- **Journal**: None
- **Summary**: Binary Neural Networks (BNNs) are neural networks which use binary weights and activations instead of the typical 32-bit floating point values. They have reduced model sizes and allow for efficient inference on mobile or embedded devices with limited power and computational resources. However, the binarization of weights and activations leads to feature maps of lower quality and lower capacity and thus a drop in accuracy compared to traditional networks. Previous work has increased the number of channels or used multiple binary bases to alleviate these problems. In this paper, we instead present an architectural approach: MeliusNet. It consists of alternating a DenseBlock, which increases the feature capacity, and our proposed ImprovementBlock, which increases the feature quality. Experiments on the ImageNet dataset demonstrate the superior performance of our MeliusNet over a variety of popular binary architectures with regards to both computation savings and accuracy. Furthermore, with our method we trained BNN models, which for the first time can match the accuracy of the popular compact network MobileNet-v1 in terms of model size, number of operations and accuracy. Our code is published online at https://github.com/hpi-xnor/BMXNet-v2



### Contextual Sense Making by Fusing Scene Classification, Detections, and Events in Full Motion Video
- **Arxiv ID**: http://arxiv.org/abs/2001.05979v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.05979v1)
- **Published**: 2020-01-16 18:26:34+00:00
- **Updated**: 2020-01-16 18:26:34+00:00
- **Authors**: Marc Bosch, Joseph Nassar, Benjamin Ortiz, Brendan Lammers, David Lindenbaum, John Wahl, Robert Mangum, Margaret Smith
- **Comment**: None
- **Journal**: None
- **Summary**: With the proliferation of imaging sensors, the volume of multi-modal imagery far exceeds the ability of human analysts to adequately consume and exploit it. Full motion video (FMV) possesses the extra challenge of containing large amounts of redundant temporal data. We aim to address the needs of human analysts to consume and exploit data given aerial FMV. We have investigated and designed a system capable of detecting events and activities of interest that deviate from the baseline patterns of observation given FMV feeds. We have divided the problem into three tasks: (1) Context awareness, (2) object cataloging, and (3) event detection. The goal of context awareness is to constraint the problem of visual search and detection in video data. A custom image classifier categorizes the scene with one or multiple labels to identify the operating context and environment. This step helps reducing the semantic search space of downstream tasks in order to increase their accuracy. The second step is object cataloging, where an ensemble of object detectors locates and labels any known objects found in the scene (people, vehicles, boats, planes, buildings, etc.). Finally, context information and detections are sent to the event detection engine to monitor for certain behaviors. A series of analytics monitor the scene by tracking object counts, and object interactions. If these object interactions are not declared to be commonly observed in the current scene, the system will report, geolocate, and log the event. Events of interest include identifying a gathering of people as a meeting and/or a crowd, alerting when there are boats on a beach unloading cargo, increased count of people entering a building, people getting in and/or out of vehicles of interest, etc. We have applied our methods on data from different sensors at different resolutions in a variety of geographical areas.



### A Common Operating Picture Framework Leveraging Data Fusion and Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2001.05982v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.05982v2)
- **Published**: 2020-01-16 18:32:19+00:00
- **Updated**: 2020-06-04 15:13:47+00:00
- **Authors**: Benjamin Ortiz, David Lindenbaum, Joseph Nassar, Brendan Lammers, John Wahl, Robert Mangum, Margaret Smith, Marc Bosch
- **Comment**: None
- **Journal**: None
- **Summary**: Organizations are starting to realize of the combined power of data and data-driven algorithmic models to gain insights, situational awareness, and advance their mission. A common challenge to gaining insights is connecting inherently different datasets. These datasets (e.g. geocoded features, video streams, raw text, social network data, etc.) per separate they provide very narrow answers; however collectively they can provide new capabilities. In this work, we present a data fusion framework for accelerating solutions for Processing, Exploitation, and Dissemination (PED). Our platform is a collection of services that extract information from several data sources (per separate) by leveraging deep learning and other means of processing. This information is fused by a set of analytical engines that perform data correlations, searches, and other modeling operations to combine information from the disparate data sources. As a result, events of interest are detected, geolocated, logged, and presented into a common operating picture. This common operating picture allows the user to visualize in real time all the data sources, per separate and their collective cooperation. In addition, forensic activities have been implemented and made available through the framework. Users can review archived results and compare them to the most recent snapshot of the operational environment. In our first iteration we have focused on visual data (FMV, WAMI, CCTV/PTZ-Cameras, open source video, etc.) and AIS data streams (satellite and terrestrial sources). As a proof-of-concept, in our experiments we show how FMV detections can be combined with vessel tracking signals from AIS sources to confirm identity, tip-and-cue aerial reconnaissance, and monitor vessel activity in an area.



### A simple way to make neural networks robust against diverse image corruptions
- **Arxiv ID**: http://arxiv.org/abs/2001.06057v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2001.06057v5)
- **Published**: 2020-01-16 20:10:25+00:00
- **Updated**: 2020-07-22 12:25:10+00:00
- **Authors**: Evgenia Rusak, Lukas Schott, Roland S. Zimmermann, Julian Bitterwolf, Oliver Bringmann, Matthias Bethge, Wieland Brendel
- **Comment**: Oral presentation at the European Conference for Computer Vision
  (ECCV 2020)
- **Journal**: None
- **Summary**: The human visual system is remarkably robust against a wide range of naturally occurring variations and corruptions like rain or snow. In contrast, the performance of modern image recognition models strongly degrades when evaluated on previously unseen corruptions. Here, we demonstrate that a simple but properly tuned training with additive Gaussian and Speckle noise generalizes surprisingly well to unseen corruptions, easily reaching the previous state of the art on the corruption benchmark ImageNet-C (with ResNet50) and on MNIST-C. We build on top of these strong baseline results and show that an adversarial training of the recognition model against uncorrelated worst-case noise distributions leads to an additional increase in performance. This regularization can be combined with previously proposed defense methods for further improvement.



### Estimating and abstracting the 3D structure of bones using neural networks on X-ray (2D) images
- **Arxiv ID**: http://arxiv.org/abs/2001.11499v1
- **DOI**: 10.1038/s42003-020-1057-3
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2001.11499v1)
- **Published**: 2020-01-16 20:41:17+00:00
- **Updated**: 2020-01-16 20:41:17+00:00
- **Authors**: Jana Čavojská, Julian Petrasch, Nicolas J. Lehmann, Agnès Voisard, Peter Böttcher
- **Comment**: 13 pages, 5 figures, 1 table, submitted to Communications Biology
- **Journal**: Communications biology, 2020, 3(1), pp.1-13
- **Summary**: In this paper, we present a deep-learning based method for estimating the 3D structure of a bone from a pair of 2D X-ray images. Our triplet loss-trained neural network selects the most closely matching 3D bone shape from a predefined set of shapes. Our predictions have an average root mean square (RMS) distance of 1.08 mm between the predicted and true shapes, making it more accurate than the average error achieved by eight other examined 3D bone reconstruction approaches. The prediction process that we use is fully automated and unlike many competing approaches, it does not rely on any previous knowledge about bone geometry. Additionally, our neural network can determine the identity of a bone based only on its X-ray image. It computes a low-dimensional representation ("embedding") of each 2D X-ray image and henceforth compares different X-ray images based only on their embeddings. An embedding holds enough information to uniquely identify the bone CT belonging to the input X-ray image with a 100% accuracy and can therefore serve as a kind of fingerprint for that bone. Possible applications include faster, image content-based bone database searches for forensic purposes.



### Tracking of Micro Unmanned Aerial Vehicles: A Comparative Study
- **Arxiv ID**: http://arxiv.org/abs/2001.06066v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2001.06066v1)
- **Published**: 2020-01-16 20:46:08+00:00
- **Updated**: 2020-01-16 20:46:08+00:00
- **Authors**: Fatih Gökçe
- **Comment**: In proceedings of the International Conference on Artificial
  Intelligence and Applied Mathematics in Engineering (ICAIAME 2019), 13 pages,
  9 Figures
- **Journal**: F. G\"ok\c{c}e. Tracking of Micro Unmanned Aerial Vehicles: A
  Comparative Study. In Proceedings of the International Conference on
  Artificial Intelligence and Applied Mathematics in Engineering, Antalya,
  Turkey, 20-22 Apr. 2019, pp.374-386
- **Summary**: Micro unmanned aerial vehicles (mUAV) became very common in recent years. As a result of their widespread usage, when they are flown by hobbyists illegally, crucial risks are imposed and such mUAVs need to be sensed by security systems. Furthermore, the sensing of mUAVs are essential for also swarm robotics research where the individuals in a flock of robots require systems to sense and localize each other for coordinated operation. In order to obtain such systems, there are studies to detect mUAVs utilizing different sensing mediums, such as vision, infrared and sound signals, and small-scale radars. However, there are still challenges that awaits to be handled in this field such as integrating tracking approaches to the vision-based detection systems to enhance accuracy and computational complexity. For this reason, in this study, we combine various tracking approaches to a vision-based mUAV detection system available in the literature, in order to evaluate different tracking approaches in terms of accuracy and as well as investigate the effect of such integration to the computational cost.



### Code-Bridged Classifier (CBC): A Low or Negative Overhead Defense for Making a CNN Classifier Robust Against Adversarial Attacks
- **Arxiv ID**: http://arxiv.org/abs/2001.06099v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2001.06099v1)
- **Published**: 2020-01-16 22:16:58+00:00
- **Updated**: 2020-01-16 22:16:58+00:00
- **Authors**: Farnaz Behnia, Ali Mirzaeian, Mohammad Sabokrou, Sai Manoj, Tinoosh Mohsenin, Khaled N. Khasawneh, Liang Zhao, Houman Homayoun, Avesta Sasan
- **Comment**: 6 pages, Accepted and to appear in ISQED 2020
- **Journal**: None
- **Summary**: In this paper, we propose Code-Bridged Classifier (CBC), a framework for making a Convolutional Neural Network (CNNs) robust against adversarial attacks without increasing or even by decreasing the overall models' computational complexity. More specifically, we propose a stacked encoder-convolutional model, in which the input image is first encoded by the encoder module of a denoising auto-encoder, and then the resulting latent representation (without being decoded) is fed to a reduced complexity CNN for image classification. We illustrate that this network not only is more robust to adversarial examples but also has a significantly lower computational complexity when compared to the prior art defenses.



### An adversarial learning framework for preserving users' anonymity in face-based emotion recognition
- **Arxiv ID**: http://arxiv.org/abs/2001.06103v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2001.06103v1)
- **Published**: 2020-01-16 22:45:52+00:00
- **Updated**: 2020-01-16 22:45:52+00:00
- **Authors**: Vansh Narula, Zhangyang, Wang, Theodora Chaspari
- **Comment**: None
- **Journal**: None
- **Summary**: Image and video-capturing technologies have permeated our every-day life. Such technologies can continuously monitor individuals' expressions in real-life settings, affording us new insights into their emotional states and transitions, thus paving the way to novel well-being and healthcare applications. Yet, due to the strong privacy concerns, the use of such technologies is met with strong skepticism, since current face-based emotion recognition systems relying on deep learning techniques tend to preserve substantial information related to the identity of the user, apart from the emotion-specific information. This paper proposes an adversarial learning framework which relies on a convolutional neural network (CNN) architecture trained through an iterative procedure for minimizing identity-specific information and maximizing emotion-dependent information. The proposed approach is evaluated through emotion classification and face identification metrics, and is compared against two CNNs, one trained solely for emotion recognition and the other trained solely for face identification. Experiments are performed using the Yale Face Dataset and Japanese Female Facial Expression Database. Results indicate that the proposed approach can learn a convolutional transformation for preserving emotion recognition accuracy and degrading face identity recognition, providing a foundation toward privacy-aware emotion recognition technologies.



