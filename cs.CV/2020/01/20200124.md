# Arxiv Papers in cs.CV on 2020-01-24
### Plant Stem Segmentation Using Fast Ground Truth Generation
- **Arxiv ID**: http://arxiv.org/abs/2001.08854v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.08854v1)
- **Published**: 2020-01-24 00:22:14+00:00
- **Updated**: 2020-01-24 00:22:14+00:00
- **Authors**: Changye Yang, Sriram Baireddy, Yuhao Chen, Enyu Cai, Denise Caldwell, Valérian Méline, Anjali S. Iyer-Pascuzzi, Edward J. Delp
- **Comment**: None
- **Journal**: None
- **Summary**: Accurately phenotyping plant wilting is important for understanding responses to environmental stress. Analysis of the shape of plants can potentially be used to accurately quantify the degree of wilting. Plant shape analysis can be enhanced by locating the stem, which serves as a consistent reference point during wilting. In this paper, we show that deep learning methods can accurately segment tomato plant stems. We also propose a control-point-based ground truth method that drastically reduces the resources needed to create a training dataset for a deep learning approach. Experimental results show the viability of both our proposed ground truth approach and deep learning based stem segmentation.



### Stochastic Optimization of Plain Convolutional Neural Networks with Simple methods
- **Arxiv ID**: http://arxiv.org/abs/2001.08856v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2001.08856v1)
- **Published**: 2020-01-24 01:20:59+00:00
- **Updated**: 2020-01-24 01:20:59+00:00
- **Authors**: Yahia Assiri
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1708.04552 by
  other authors
- **Journal**: 15th International Conference on Machine Learning and Data Mining,
  MLDM 2019, vol.II, New York, NY, USA, July 20-25, 2019, ibai-publishing, ISSN
  1864-9734 ISBN 978-3-942952-63-7, pages(833-844)
- **Summary**: Convolutional neural networks have been achieving the best possible accuracies in many visual pattern classification problems. However, due to the model capacity required to capture such representations, they are often oversensitive to overfitting and therefore require proper regularization to generalize well. In this paper, we present a combination of regularization techniques which work together to get better performance, we built plain CNNs, and then we used data augmentation, dropout and customized early stopping function, we tested and evaluated these techniques by applying models on five famous datasets, MNIST, CIFAR10, CIFAR100, SVHN, STL10, and we achieved three state-of-the-art-of (MNIST, SVHN, STL10) and very high-Accuracy on the other two datasets.



### Nonparametric Structure Regularization Machine for 2D Hand Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2001.08869v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2001.08869v1)
- **Published**: 2020-01-24 03:27:32+00:00
- **Updated**: 2020-01-24 03:27:32+00:00
- **Authors**: Yifei Chen, Haoyu Ma, Deying Kong, Xiangyi Yan, Jianbao Wu, Wei Fan, Xiaohui Xie
- **Comment**: The paper has be accepted and will be presented at 2020 IEEE Winter
  Conference on Applications of Computer Vision (WACV). The code is freely
  available at https://github.com/HowieMa/NSRMhand
- **Journal**: None
- **Summary**: Hand pose estimation is more challenging than body pose estimation due to severe articulation, self-occlusion and high dexterity of the hand. Current approaches often rely on a popular body pose algorithm, such as the Convolutional Pose Machine (CPM), to learn 2D keypoint features. These algorithms cannot adequately address the unique challenges of hand pose estimation, because they are trained solely based on keypoint positions without seeking to explicitly model structural relationship between them. We propose a novel Nonparametric Structure Regularization Machine (NSRM) for 2D hand pose estimation, adopting a cascade multi-task architecture to learn hand structure and keypoint representations jointly. The structure learning is guided by synthetic hand mask representations, which are directly computed from keypoint positions, and is further strengthened by a novel probabilistic representation of hand limbs and an anatomically inspired composition strategy of mask synthesis. We conduct extensive studies on two public datasets - OneHand 10k and CMU Panoptic Hand. Experimental results demonstrate that explicitly enforcing structure learning consistently improves pose estimation accuracy of CPM baseline models, by 1.17% on the first dataset and 4.01% on the second one. The implementation and experiment code is freely available online. Our proposal of incorporating structural learning to hand pose estimation requires no additional training information, and can be a generic add-on module to other pose estimation models.



### Progressive Local Filter Pruning for Image Retrieval Acceleration
- **Arxiv ID**: http://arxiv.org/abs/2001.08878v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.08878v1)
- **Published**: 2020-01-24 04:28:44+00:00
- **Updated**: 2020-01-24 04:28:44+00:00
- **Authors**: Xiaodong Wang, Zhedong Zheng, Yang He, Fei Yan, Zhiqiang Zeng, Yi Yang
- **Comment**: 7 pages, 5 figures
- **Journal**: None
- **Summary**: This paper focuses on network pruning for image retrieval acceleration. Prevailing image retrieval works target at the discriminative feature learning, while little attention is paid to how to accelerate the model inference, which should be taken into consideration in real-world practice. The challenge of pruning image retrieval models is that the middle-level feature should be preserved as much as possible. Such different requirements of the retrieval and classification model make the traditional pruning methods not that suitable for our task. To solve the problem, we propose a new Progressive Local Filter Pruning (PLFP) method for image retrieval acceleration. Specifically, layer by layer, we analyze the local geometric properties of each filter and select the one that can be replaced by the neighbors. Then we progressively prune the filter by gradually changing the filter weights. In this way, the representation ability of the model is preserved. To verify this, we evaluate our method on two widely-used image retrieval datasets,i.e., Oxford5k and Paris6K, and one person re-identification dataset,i.e., Market-1501. The proposed method arrives with superior performance to the conventional pruning methods, suggesting the effectiveness of the proposed method for image retrieval.



### Character-independent font identification
- **Arxiv ID**: http://arxiv.org/abs/2001.08893v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.08893v1)
- **Published**: 2020-01-24 05:59:53+00:00
- **Updated**: 2020-01-24 05:59:53+00:00
- **Authors**: Daichi Haraguchi, Shota Harada, Brian Kenji Iwana, Yuto Shinahara, Seiichi Uchida
- **Comment**: submitted DAS 2020
- **Journal**: None
- **Summary**: There are a countless number of fonts with various shapes and styles. In addition, there are many fonts that only have subtle differences in features. Due to this, font identification is a difficult task. In this paper, we propose a method of determining if any two characters are from the same font or not. This is difficult due to the difference between fonts typically being smaller than the difference between alphabet classes. Additionally, the proposed method can be used with fonts regardless of whether they exist in the training or not. In order to accomplish this, we use a Convolutional Neural Network (CNN) trained with various font image pairs. In the experiment, the network is trained on image pairs of various fonts. We then evaluate the model on a different set of fonts that are unseen by the network. The evaluation is performed with an accuracy of 92.27%. Moreover, we analyzed the relationship between character classes and font identification accuracy.



### Small, Accurate, and Fast Vehicle Re-ID on the Edge: the SAFR Approach
- **Arxiv ID**: http://arxiv.org/abs/2001.08895v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2001.08895v1)
- **Published**: 2020-01-24 06:07:17+00:00
- **Updated**: 2020-01-24 06:07:17+00:00
- **Authors**: Abhijit Suprem, Calton Pu, Joao Eduardo Ferreira
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a Small, Accurate, and Fast Re-ID (SAFR) design for flexible vehicle re-id under a variety of compute environments such as cloud, mobile, edge, or embedded devices by only changing the re-id model backbone. Through best-fit design choices, feature extraction, training tricks, global attention, and local attention, we create a reid model design that optimizes multi-dimensionally along model size, speed, & accuracy for deployment under various memory and compute constraints. We present several variations of our flexible SAFR model: SAFR-Large for cloud-type environments with large compute resources, SAFR-Small for mobile devices with some compute constraints, and SAFR-Micro for edge devices with severe memory & compute constraints.   SAFR-Large delivers state-of-the-art results with mAP 81.34 on the VeRi-776 vehicle re-id dataset (15% better than related work). SAFR-Small trades a 5.2% drop in performance (mAP 77.14 on VeRi-776) for over 60% model compression and 150% speedup. SAFR-Micro, at only 6MB and 130MFLOPS, trades 6.8% drop in accuracy (mAP 75.80 on VeRi-776) for 95% compression and 33x speedup compared to SAFR-Large.



### 6D Object Pose Regression via Supervised Learning on Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2001.08942v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.08942v1)
- **Published**: 2020-01-24 10:29:54+00:00
- **Updated**: 2020-01-24 10:29:54+00:00
- **Authors**: Ge Gao, Mikko Lauri, Yulong Wang, Xiaolin Hu, Jianwei Zhang, Simone Frintrop
- **Comment**: None
- **Journal**: None
- **Summary**: This paper addresses the task of estimating the 6 degrees of freedom pose of a known 3D object from depth information represented by a point cloud. Deep features learned by convolutional neural networks from color information have been the dominant features to be used for inferring object poses, while depth information receives much less attention. However, depth information contains rich geometric information of the object shape, which is important for inferring the object pose. We use depth information represented by point clouds as the input to both deep networks and geometry-based pose refinement and use separate networks for rotation and translation regression. We argue that the axis-angle representation is a suitable rotation representation for deep learning, and use a geodesic loss function for rotation regression. Ablation studies show that these design choices outperform alternatives such as the quaternion representation and L2 loss, or regressing translation and rotation with the same network. Our simple yet effective approach clearly outperforms state-of-the-art methods on the YCB-video dataset. The implementation and trained model are avaliable at: https://github.com/GeeeG/CloudPose.



### An Explicit Local and Global Representation Disentanglement Framework with Applications in Deep Clustering and Unsupervised Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2001.08957v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2001.08957v2)
- **Published**: 2020-01-24 12:09:20+00:00
- **Updated**: 2020-02-24 10:11:49+00:00
- **Authors**: Rujikorn Charakorn, Yuttapong Thawornwattana, Sirawaj Itthipuripat, Nick Pawlowski, Poramate Manoonpong, Nat Dilokthanakul
- **Comment**: 13 pages and 9 figures
- **Journal**: None
- **Summary**: Visual data can be understood at different levels of granularity, where global features correspond to semantic-level information and local features correspond to texture patterns. In this work, we propose a framework, called SPLIT, which allows us to disentangle local and global information into two separate sets of latent variables within the variational autoencoder (VAE) framework. Our framework adds generative assumption to the VAE by requiring a subset of the latent variables to generate an auxiliary set of observable data. This additional generative assumption primes the latent variables to local information and encourages the other latent variables to represent global information. We examine three different flavours of VAEs with different generative assumptions. We show that the framework can effectively disentangle local and global information within these models leads to improved representation, with better clustering and unsupervised object detection benchmarks. Finally, we establish connections between SPLIT and recent research in cognitive neuroscience regarding the disentanglement in human visual perception. The code for our experiments is at https://github.com/51616/split-vae .



### Unsupervised Learning Methods for Visual Place Recognition in Discretely and Continuously Changing Environments
- **Arxiv ID**: http://arxiv.org/abs/2001.08960v1
- **DOI**: 10.1109/ICRA40945.2020.9197044
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2001.08960v1)
- **Published**: 2020-01-24 12:23:15+00:00
- **Updated**: 2020-01-24 12:23:15+00:00
- **Authors**: Stefan Schubert, Peer Neubert, Peter Protzel
- **Comment**: Accepted for publication at International Conference on Robotics and
  Automation (ICRA) 2020. This is the submitted version
- **Journal**: None
- **Summary**: Visual place recognition in changing environments is the problem of finding matchings between two sets of observations, a query set and a reference set, despite severe appearance changes. Recently, image comparison using CNN-based descriptors showed very promising results. However, existing experiments from the literature typically assume a single distinctive condition within each set (e.g., reference: day, query: night). We demonstrate that as soon as the conditions change within one set (e.g., reference: day, query: traversal daytime-dusk-night-dawn), different places under the same condition can suddenly look more similar than same places under different conditions and state-of-the-art approaches like CNN-based descriptors fail. This paper discusses this practically very important problem of in-sequence condition changes and defines a hierarchy of problem setups from (1) no in-sequence changes, (2) discrete in-sequence changes, to (3) continuous in-sequence changes. We will experimentally evaluate the effect of these changes on two state-of-the-art CNN-descriptors. Our experiments emphasize the importance of statistical standardization of descriptors and shows its limitations in case of continuous changes. To address this practically most relevant setup, we investigate and experimentally evaluate the application of unsupervised learning methods using two available PCA-based approaches and propose a novel clustering-based extension of the statistical normalization.



### SOLAR: Second-Order Loss and Attention for Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2001.08972v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2001.08972v5)
- **Published**: 2020-01-24 12:54:23+00:00
- **Updated**: 2020-08-04 17:36:45+00:00
- **Authors**: Tony Ng, Vassileios Balntas, Yurun Tian, Krystian Mikolajczyk
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: Recent works in deep-learning have shown that second-order information is beneficial in many computer-vision tasks. Second-order information can be enforced both in the spatial context and the abstract feature dimensions. In this work, we explore two second-order components. One is focused on second-order spatial information to increase the performance of image descriptors, both local and global. It is used to re-weight feature maps, and thus emphasise salient image locations that are subsequently used for description. The second component is concerned with a second-order similarity (SOS) loss, that we extend to global descriptors for image retrieval, and is used to enhance the triplet loss with hard-negative mining. We validate our approach on two different tasks and datasets for image retrieval and image matching. The results show that our two second-order components complement each other, bringing significant performance improvements in both tasks and lead to state-of-the-art results across the public benchmarks. Code available at: http://github.com/tonyngjichun/SOLAR



### Discrete graphical models -- an optimization perspective
- **Arxiv ID**: http://arxiv.org/abs/2001.09017v1
- **DOI**: 10.1561/0600000084
- **Categories**: **math.OC**, cs.CV, cs.DM
- **Links**: [PDF](http://arxiv.org/pdf/2001.09017v1)
- **Published**: 2020-01-24 14:01:50+00:00
- **Updated**: 2020-01-24 14:01:50+00:00
- **Authors**: Bogdan Savchynskyy
- **Comment**: 270 pages
- **Journal**: Foundations and Trends in Computer Graphics and Vision: Vol. 11:
  No. 3-4, pp 160-429 (2019)
- **Summary**: This monograph is about discrete energy minimization for discrete graphical models. It considers graphical models, or, more precisely, maximum a posteriori inference for graphical models, purely as a combinatorial optimization problem. Modeling, applications, probabilistic interpretations and many other aspects are either ignored here or find their place in examples and remarks only. It covers the integer linear programming formulation of the problem as well as its linear programming, Lagrange and Lagrange decomposition-based relaxations. In particular, it provides a detailed analysis of the polynomially solvable acyclic and submodular problems, along with the corresponding exact optimization methods. Major approximate methods, such as message passing and graph cut techniques are also described and analyzed comprehensively. The monograph can be useful for undergraduate and graduate students studying optimization or graphical models, as well as for experts in optimization who want to have a look into graphical models. To make the monograph suitable for both categories of readers we explicitly separate the mathematical optimization background chapters from those specific to graphical models.



### PDE-based Group Equivariant Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2001.09046v6
- **DOI**: 10.1007/s10851-022-01114-x
- **Categories**: **cs.LG**, cs.CV, math.DG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2001.09046v6)
- **Published**: 2020-01-24 15:00:46+00:00
- **Updated**: 2022-05-30 19:05:29+00:00
- **Authors**: Bart Smets, Jim Portegies, Erik Bekkers, Remco Duits
- **Comment**: 27 pages, 18 figures. v2 changes: - mentioned KerCNNs - added section
  Generalization of G-CNNs - clarification that the experiments utilized
  automatic differentiation and SGD. v3 changes: - streamlined theoretical
  framework - formulation and proof Thm.1 & 2 - expanded experiments. v4
  changes: typos in Prop.5 and (20) v5/6 changes: minor revision
- **Journal**: None
- **Summary**: We present a PDE-based framework that generalizes Group equivariant Convolutional Neural Networks (G-CNNs). In this framework, a network layer is seen as a set of PDE-solvers where geometrically meaningful PDE-coefficients become the layer's trainable weights. Formulating our PDEs on homogeneous spaces allows these networks to be designed with built-in symmetries such as rotation in addition to the standard translation equivariance of CNNs.   Having all the desired symmetries included in the design obviates the need to include them by means of costly techniques such as data augmentation. We will discuss our PDE-based G-CNNs (PDE-G-CNNs) in a general homogeneous space setting while also going into the specifics of our primary case of interest: roto-translation equivariance.   We solve the PDE of interest by a combination of linear group convolutions and non-linear morphological group convolutions with analytic kernel approximations that we underpin with formal theorems. Our kernel approximations allow for fast GPU-implementation of the PDE-solvers, we release our implementation with this article in the form of the LieTorch extension to PyTorch, available at https://gitlab.com/bsmetsjr/lietorch . Just like for linear convolution a morphological convolution is specified by a kernel that we train in our PDE-G-CNNs. In PDE-G-CNNs we do not use non-linearities such as max/min-pooling and ReLUs as they are already subsumed by morphological convolutions.   We present a set of experiments to demonstrate the strength of the proposed PDE-G-CNNs in increasing the performance of deep learning based imaging applications with far fewer parameters than traditional CNNs.



### Kernel of CycleGAN as a Principle homogeneous space
- **Arxiv ID**: http://arxiv.org/abs/2001.09061v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2001.09061v1)
- **Published**: 2020-01-24 15:47:12+00:00
- **Updated**: 2020-01-24 15:47:12+00:00
- **Authors**: Nikita Moriakov, Jonas Adler, Jonas Teuwen
- **Comment**: Accepted at ICLR 2020
- **Journal**: None
- **Summary**: Unpaired image-to-image translation has attracted significant interest due to the invention of CycleGAN, a method which utilizes a combination of adversarial and cycle consistency losses to avoid the need for paired data. It is known that the CycleGAN problem might admit multiple solutions, and our goal in this paper is to analyze the space of exact solutions and to give perturbation bounds for approximate solutions. We show theoretically that the exact solution space is invariant with respect to automorphisms of the underlying probability spaces, and, furthermore, that the group of automorphisms acts freely and transitively on the space of exact solutions. We examine the case of zero `pure' CycleGAN loss first in its generality, and, subsequently, expand our analysis to approximate solutions for `extended' CycleGAN loss where identity loss term is included. In order to demonstrate that these results are applicable, we show that under mild conditions nontrivial smooth automorphisms exist. Furthermore, we provide empirical evidence that neural networks can learn these automorphisms with unexpected and unwanted results. We conclude that finding optimal solutions to the CycleGAN loss does not necessarily lead to the envisioned result in image-to-image translation tasks and that underlying hidden symmetries can render the result utterly useless.



### Deep Non-Line-of-Sight Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2001.09067v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2001.09067v2)
- **Published**: 2020-01-24 16:05:50+00:00
- **Updated**: 2020-01-29 12:42:53+00:00
- **Authors**: Javier Grau Chopite, Matthias B. Hullin, Michael Wand, Julian Iseringhausen
- **Comment**: Minor editorial update (typos, figure captions)
- **Journal**: None
- **Summary**: The recent years have seen a surge of interest in methods for imaging beyond the direct line of sight. The most prominent techniques rely on time-resolved optical impulse responses, obtained by illuminating a diffuse wall with an ultrashort light pulse and observing multi-bounce indirect reflections with an ultrafast time-resolved imager. Reconstruction of geometry from such data, however, is a complex non-linear inverse problem that comes with substantial computational demands. In this paper, we employ convolutional feed-forward networks for solving the reconstruction problem efficiently while maintaining good reconstruction quality. Specifically, we devise a tailored autoencoder architecture, trained end-to-end, that maps transient images directly to a depth map representation. Training is done using an efficient transient renderer for diffuse three-bounce indirect light transport that enables the quick generation of large amounts of training data for the network. We examine the performance of our method on a variety of synthetic and experimental datasets and its dependency on the choice of training data and augmentation strategies, as well as architectural features. We demonstrate that our feed-forward network, even though it is trained solely on synthetic data, generalizes to measured data from SPAD sensors and is able to obtain results that are competitive with model-based reconstruction methods.



### SceneEncoder: Scene-Aware Semantic Segmentation of Point Clouds with A Learnable Scene Descriptor
- **Arxiv ID**: http://arxiv.org/abs/2001.09087v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.09087v1)
- **Published**: 2020-01-24 16:53:30+00:00
- **Updated**: 2020-01-24 16:53:30+00:00
- **Authors**: Jiachen Xu, Jingyu Gong, Jie Zhou, Xin Tan, Yuan Xie, Lizhuang Ma
- **Comment**: None
- **Journal**: None
- **Summary**: Besides local features, global information plays an essential role in semantic segmentation, while recent works usually fail to explicitly extract the meaningful global information and make full use of it. In this paper, we propose a SceneEncoder module to impose a scene-aware guidance to enhance the effect of global information. The module predicts a scene descriptor, which learns to represent the categories of objects existing in the scene and directly guides the point-level semantic segmentation through filtering out categories not belonging to this scene. Additionally, to alleviate segmentation noise in local region, we design a region similarity loss to propagate distinguishing features to their own neighboring points with the same label, leading to the enhancement of the distinguishing ability of point-wise features. We integrate our methods into several prevailing networks and conduct extensive experiments on benchmark datasets ScanNet and ShapeNet. Results show that our methods greatly improve the performance of baselines and achieve state-of-the-art performance.



### TVR: A Large-Scale Dataset for Video-Subtitle Moment Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2001.09099v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/2001.09099v2)
- **Published**: 2020-01-24 17:09:39+00:00
- **Updated**: 2020-08-18 15:12:14+00:00
- **Authors**: Jie Lei, Licheng Yu, Tamara L. Berg, Mohit Bansal
- **Comment**: ECCV 2020 (extended version, with TVC dataset+models; 35 pages)
- **Journal**: None
- **Summary**: We introduce TV show Retrieval (TVR), a new multimodal retrieval dataset. TVR requires systems to understand both videos and their associated subtitle (dialogue) texts, making it more realistic. The dataset contains 109K queries collected on 21.8K videos from 6 TV shows of diverse genres, where each query is associated with a tight temporal window. The queries are also labeled with query types that indicate whether each of them is more related to video or subtitle or both, allowing for in-depth analysis of the dataset and the methods that built on top of it. Strict qualification and post-annotation verification tests are applied to ensure the quality of the collected data. Further, we present several baselines and a novel Cross-modal Moment Localization (XML ) network for multimodal moment retrieval tasks. The proposed XML model uses a late fusion design with a novel Convolutional Start-End detector (ConvSE), surpassing baselines by a large margin and with better efficiency, providing a strong starting point for future work. We have also collected additional descriptions for each annotated moment in TVR to form a new multimodal captioning dataset with 262K captions, named TV show Caption (TVC). Both datasets are publicly available. TVR: https://tvr.cs.unc.edu, TVC: https://tvr.cs.unc.edu/tvc.html.



### No Routing Needed Between Capsules
- **Arxiv ID**: http://arxiv.org/abs/2001.09136v6
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2001.09136v6)
- **Published**: 2020-01-24 18:37:41+00:00
- **Updated**: 2021-06-17 20:14:13+00:00
- **Authors**: Adam Byerly, Tatiana Kalganova, Ian Dear
- **Comment**: 13 pages, 7 figures, 9 tables
- **Journal**: None
- **Summary**: Most capsule network designs rely on traditional matrix multiplication between capsule layers and computationally expensive routing mechanisms to deal with the capsule dimensional entanglement that the matrix multiplication introduces. By using Homogeneous Vector Capsules (HVCs), which use element-wise multiplication rather than matrix multiplication, the dimensions of the capsules remain unentangled. In this work, we study HVCs as applied to the highly structured MNIST dataset in order to produce a direct comparison to the capsule research direction of Geoffrey Hinton, et al. In our study, we show that a simple convolutional neural network using HVCs performs as well as the prior best performing capsule network on MNIST using 5.5x fewer parameters, 4x fewer training epochs, no reconstruction sub-network, and requiring no routing mechanism. The addition of multiple classification branches to the network establishes a new state of the art for the MNIST dataset with an accuracy of 99.87% for an ensemble of these models, as well as establishing a new state of the art for a single model (99.83% accurate).



### RatLesNetv2: A Fully Convolutional Network for Rodent Brain Lesion Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2001.09138v4
- **DOI**: 10.3389/fnins.2020.610239
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2001.09138v4)
- **Published**: 2020-01-24 18:40:39+00:00
- **Updated**: 2020-12-30 09:05:42+00:00
- **Authors**: Juan Miguel Valverde, Artem Shatillo, Riccardo de Feo, Olli Gröhn, Alejandra Sierra, Jussi Tohka
- **Comment**: Added new changes
- **Journal**: None
- **Summary**: We present a fully convolutional neural network (ConvNet), named RatLesNetv2, for segmenting lesions in rodent magnetic resonance (MR) brain images. RatLesNetv2 architecture resembles an autoencoder and it incorporates residual blocks that facilitate its optimization. RatLesNetv2 is trained end to end on three-dimensional images and it requires no preprocessing. We evaluated RatLesNetv2 on an exceptionally large dataset composed of 916 T2-weighted rat brain MRI scans of 671 rats at nine different lesion stages that were used to study focal cerebral ischemia for drug development. In addition, we compared its performance with three other ConvNets specifically designed for medical image segmentation. RatLesNetv2 obtained similar to higher Dice coefficient values than the other ConvNets and it produced much more realistic and compact segmentations with notably fewer holes and lower Hausdorff distance. The Dice scores of RatLesNetv2 segmentations also exceeded inter-rater agreement of manual segmentations. In conclusion, RatLesNetv2 could be used for automated lesion segmentation, reducing human workload and improving reproducibility. RatLesNetv2 is publicly available at https://github.com/jmlipman/RatLesNetv2.



### Weakly Supervised Lesion Co-segmentation on CT Scans
- **Arxiv ID**: http://arxiv.org/abs/2001.09174v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.09174v1)
- **Published**: 2020-01-24 19:39:33+00:00
- **Updated**: 2020-01-24 19:39:33+00:00
- **Authors**: Vatsal Agarwal, Youbao Tang, Jing Xiao, Ronald M. Summers
- **Comment**: None
- **Journal**: None
- **Summary**: Lesion segmentation in medical imaging serves as an effective tool for assessing tumor sizes and monitoring changes in growth. However, not only is manual lesion segmentation time-consuming, but it is also expensive and requires expert radiologist knowledge. Therefore many hospitals rely on a loose substitute called response evaluation criteria in solid tumors (RECIST). Although these annotations are far from precise, they are widely used throughout hospitals and are found in their picture archiving and communication systems (PACS). Therefore, these annotations have the potential to serve as a robust yet challenging means of weak supervision for training full lesion segmentation models. In this work, we propose a weakly-supervised co-segmentation model that first generates pseudo-masks from the RECIST slices and uses these as training labels for an attention-based convolutional neural network capable of segmenting common lesions from a pair of CT scans. To validate and test the model, we utilize the DeepLesion dataset, an extensive CT-scan lesion dataset that contains 32,735 PACS bookmarked images. Extensive experimental results demonstrate the efficacy of our co-segmentation approach for lesion segmentation with a mean Dice coefficient of 90.3%.



### End-to-End Vision-Based Adaptive Cruise Control (ACC) Using Deep Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/2001.09181v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2001.09181v1)
- **Published**: 2020-01-24 20:02:50+00:00
- **Updated**: 2020-01-24 20:02:50+00:00
- **Authors**: Zhensong Wei, Yu Jiang, Xishun Liao, Xuewei Qi, Ziran Wang, Guoyuan Wu, Peng Hao, Matthew Barth
- **Comment**: This manuscript was presented at 99th Transportation Research Board
  Annual Meeting in Washington D.C., Jan 2020
- **Journal**: None
- **Summary**: This paper presented a deep reinforcement learning method named Double Deep Q-networks to design an end-to-end vision-based adaptive cruise control (ACC) system. A simulation environment of a highway scene was set up in Unity, which is a game engine that provided both physical models of vehicles and feature data for training and testing. Well-designed reward functions associated with the following distance and throttle/brake force were implemented in the reinforcement learning model for both internal combustion engine (ICE) vehicles and electric vehicles (EV) to perform adaptive cruise control. The gap statistics and total energy consumption are evaluated for different vehicle types to explore the relationship between reward functions and powertrain characteristics. Compared with the traditional radar-based ACC systems or human-in-the-loop simulation, the proposed vision-based ACC system can generate either a better gap regulated trajectory or a smoother speed trajectory depending on the preset reward function. The proposed system can be well adaptive to different speed trajectories of the preceding vehicle and operated in real-time.



### Learning a distance function with a Siamese network to localize anomalies in videos
- **Arxiv ID**: http://arxiv.org/abs/2001.09189v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.09189v1)
- **Published**: 2020-01-24 20:47:05+00:00
- **Updated**: 2020-01-24 20:47:05+00:00
- **Authors**: Bharathkumar Ramachandra, Michael J. Jones, Ranga Raju Vatsavai
- **Comment**: accepted to WACV 2020
- **Journal**: None
- **Summary**: This work introduces a new approach to localize anomalies in surveillance video. The main novelty is the idea of using a Siamese convolutional neural network (CNN) to learn a distance function between a pair of video patches (spatio-temporal regions of video). The learned distance function, which is not specific to the target video, is used to measure the distance between each video patch in the testing video and the video patches found in normal training video. If a testing video patch is not similar to any normal video patch then it must be anomalous. We compare our approach to previously published algorithms using 4 evaluation measures and 3 challenging target benchmark datasets. Experiments show that our approach either surpasses or performs comparably to current state-of-the-art methods.



### VerSe: A Vertebrae Labelling and Segmentation Benchmark for Multi-detector CT Images
- **Arxiv ID**: http://arxiv.org/abs/2001.09193v6
- **DOI**: 10.1016/j.media.2021.102166
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2001.09193v6)
- **Published**: 2020-01-24 21:09:18+00:00
- **Updated**: 2022-04-05 08:17:55+00:00
- **Authors**: Anjany Sekuboyina, Malek E. Husseini, Amirhossein Bayat, Maximilian Löffler, Hans Liebl, Hongwei Li, Giles Tetteh, Jan Kukačka, Christian Payer, Darko Štern, Martin Urschler, Maodong Chen, Dalong Cheng, Nikolas Lessmann, Yujin Hu, Tianfu Wang, Dong Yang, Daguang Xu, Felix Ambellan, Tamaz Amiranashvili, Moritz Ehlke, Hans Lamecker, Sebastian Lehnert, Marilia Lirio, Nicolás Pérez de Olaguer, Heiko Ramm, Manish Sahu, Alexander Tack, Stefan Zachow, Tao Jiang, Xinjun Ma, Christoph Angerman, Xin Wang, Kevin Brown, Alexandre Kirszenberg, Élodie Puybareau, Di Chen, Yiwei Bai, Brandon H. Rapazzo, Timyoas Yeah, Amber Zhang, Shangliang Xu, Feng Hou, Zhiqiang He, Chan Zeng, Zheng Xiangshang, Xu Liming, Tucker J. Netherton, Raymond P. Mumme, Laurence E. Court, Zixun Huang, Chenhang He, Li-Wen Wang, Sai Ho Ling, Lê Duy Huynh, Nicolas Boutry, Roman Jakubicek, Jiri Chmelik, Supriti Mulay, Mohanasankar Sivaprakasam, Johannes C. Paetzold, Suprosanna Shit, Ivan Ezhov, Benedikt Wiestler, Ben Glocker, Alexander Valentinitsch, Markus Rempfler, Björn H. Menze, Jan S. Kirschke
- **Comment**: Challenge report for the VerSe 2019 and 2020. Published in Medical
  Image Analysis (DOI: https://doi.org/10.1016/j.media.2021.102166)
- **Journal**: Medical Image Analysis, Volume 73, October 2021, 102166
- **Summary**: Vertebral labelling and segmentation are two fundamental tasks in an automated spine processing pipeline. Reliable and accurate processing of spine images is expected to benefit clinical decision-support systems for diagnosis, surgery planning, and population-based analysis on spine and bone health. However, designing automated algorithms for spine processing is challenging predominantly due to considerable variations in anatomy and acquisition protocols and due to a severe shortage of publicly available data. Addressing these limitations, the Large Scale Vertebrae Segmentation Challenge (VerSe) was organised in conjunction with the International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI) in 2019 and 2020, with a call for algorithms towards labelling and segmentation of vertebrae. Two datasets containing a total of 374 multi-detector CT scans from 355 patients were prepared and 4505 vertebrae have individually been annotated at voxel-level by a human-machine hybrid algorithm (https://osf.io/nqjyw/, https://osf.io/t98fz/). A total of 25 algorithms were benchmarked on these datasets. In this work, we present the the results of this evaluation and further investigate the performance-variation at vertebra-level, scan-level, and at different fields-of-view. We also evaluate the generalisability of the approaches to an implicit domain shift in data by evaluating the top performing algorithms of one challenge iteration on data from the other iteration. The principal takeaway from VerSe: the performance of an algorithm in labelling and segmenting a spine scan hinges on its ability to correctly identify vertebrae in cases of rare anatomical variations. The content and code concerning VerSe can be accessed at: https://github.com/anjany/verse.



### Modular network for high accuracy object detection
- **Arxiv ID**: http://arxiv.org/abs/2001.09203v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE, eess.IV, I.2.10; I.2.10; I.2.1; I.4.6; I.4.7; I.4.8; I.4.9; I.5.2; I.5.5
- **Links**: [PDF](http://arxiv.org/pdf/2001.09203v3)
- **Published**: 2020-01-24 21:38:22+00:00
- **Updated**: 2020-09-13 19:03:31+00:00
- **Authors**: Erez Yahalomi
- **Comment**: Revised version
- **Journal**: None
- **Summary**: We present a novel modular object detection convolutional neural network that significantly improves the accuracy of object detection. The network consists of two stages in a hierarchical structure. The first stage is a network that detects general classes. The second stage consists of separate networks to refine the classification and localization of each of the general classes objects. Compared to a state of the art object detection networks the classification error in the modular network is improved by approximately 3-5 times, from 12% to 2.5 %-4.5%. This network is easy to implement and has a 0.94 mAP. The network architecture can be a platform to improve the accuracy of widespread state of the art object detection networks and other kinds of deep learning networks. We show that a deep learning network initialized by transfer learning becomes more accurate as the number of classes it later trained to detect becomes smaller.



### Temporal Pulses Driven Spiking Neural Network for Fast Object Recognition in Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2001.09220v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2001.09220v1)
- **Published**: 2020-01-24 22:58:55+00:00
- **Updated**: 2020-01-24 22:58:55+00:00
- **Authors**: Wei Wang, Shibo Zhou, Jingxi Li, Xiaohua Li, Junsong Yuan, Zhanpeng Jin
- **Comment**: None
- **Journal**: None
- **Summary**: Accurate real-time object recognition from sensory data has long been a crucial and challenging task for autonomous driving. Even though deep neural networks (DNNs) have been successfully applied in this area, most existing methods still heavily rely on the pre-processing of the pulse signals derived from LiDAR sensors, and therefore introduce additional computational overhead and considerable latency. In this paper, we propose an approach to address the object recognition problem directly with raw temporal pulses utilizing the spiking neural network (SNN). Being evaluated on various datasets (including Sim LiDAR, KITTI and DVS-barrel) derived from LiDAR and dynamic vision sensor (DVS), our proposed method has shown comparable performance as the state-of-the-art methods, while achieving remarkable time efficiency. It highlights the SNN's great potentials in autonomous driving and related applications. To the best of our knowledge, this is the first attempt to use SNN to directly perform object recognition on raw temporal pulses.



