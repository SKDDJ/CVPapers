# Arxiv Papers in cs.CV on 2020-01-02
### Video Saliency Prediction Using Enhanced Spatiotemporal Alignment Network
- **Arxiv ID**: http://arxiv.org/abs/2001.00292v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.00292v1)
- **Published**: 2020-01-02 02:05:35+00:00
- **Updated**: 2020-01-02 02:05:35+00:00
- **Authors**: Jin Chen, Huihui Song, Kaihua Zhang, Bo Liu, Qingshan Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Due to a variety of motions across different frames, it is highly challenging to learn an effective spatiotemporal representation for accurate video saliency prediction (VSP). To address this issue, we develop an effective spatiotemporal feature alignment network tailored to VSP, mainly including two key sub-networks: a multi-scale deformable convolutional alignment network (MDAN) and a bidirectional convolutional Long Short-Term Memory (Bi-ConvLSTM) network. The MDAN learns to align the features of the neighboring frames to the reference one in a coarse-to-fine manner, which can well handle various motions. Specifically, the MDAN owns a pyramidal feature hierarchy structure that first leverages deformable convolution (Dconv) to align the lower-resolution features across frames, and then aggregates the aligned features to align the higher-resolution features, progressively enhancing the features from top to bottom. The output of MDAN is then fed into the Bi-ConvLSTM for further enhancement, which captures the useful long-time temporal information along forward and backward timing directions to effectively guide attention orientation shift prediction under complex scene transformation. Finally, the enhanced features are decoded to generate the predicted saliency map. The proposed model is trained end-to-end without any intricate post processing. Extensive evaluations on four VSP benchmark datasets demonstrate that the proposed method achieves favorable performance against state-of-the-art methods. The source codes and all the results will be released.



### Video Cloze Procedure for Self-Supervised Spatio-Temporal Learning
- **Arxiv ID**: http://arxiv.org/abs/2001.00294v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.00294v1)
- **Published**: 2020-01-02 02:15:24+00:00
- **Updated**: 2020-01-02 02:15:24+00:00
- **Authors**: Dezhao Luo, Chang Liu, Yu Zhou, Dongbao Yang, Can Ma, Qixiang Ye, Weiping Wang
- **Comment**: AAAI2020(Oral)
- **Journal**: None
- **Summary**: We propose a novel self-supervised method, referred to as Video Cloze Procedure (VCP), to learn rich spatial-temporal representations. VCP first generates "blanks" by withholding video clips and then creates "options" by applying spatio-temporal operations on the withheld clips. Finally, it fills the blanks with "options" and learns representations by predicting the categories of operations applied on the clips. VCP can act as either a proxy task or a target task in self-supervised learning. As a proxy task, it converts rich self-supervised representations into video clip operations (options), which enhances the flexibility and reduces the complexity of representation learning. As a target task, it can assess learned representation models in a uniform and interpretable manner. With VCP, we train spatial-temporal representation models (3D-CNNs) and apply such models on action recognition and video retrieval tasks. Experiments on commonly used benchmarks show that the trained models outperform the state-of-the-art self-supervised models with significant margins.



### BlendMask: Top-Down Meets Bottom-Up for Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2001.00309v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.00309v3)
- **Published**: 2020-01-02 03:30:17+00:00
- **Updated**: 2020-04-26 10:27:02+00:00
- **Authors**: Hao Chen, Kunyang Sun, Zhi Tian, Chunhua Shen, Yongming Huang, Youliang Yan
- **Comment**: Accepted to Proc. IEEE Conf. Computer Vision and Pattern Recognition
  2020. Fixed typos
- **Journal**: None
- **Summary**: Instance segmentation is one of the fundamental vision tasks. Recently, fully convolutional instance segmentation methods have drawn much attention as they are often simpler and more efficient than two-stage approaches like Mask R-CNN. To date, almost all such approaches fall behind the two-stage Mask R-CNN method in mask precision when models have similar computation complexity, leaving great room for improvement.   In this work, we achieve improved mask prediction by effectively combining instance-level information with semantic information with lower-level fine-granularity. Our main contribution is a blender module which draws inspiration from both top-down and bottom-up instance segmentation approaches. The proposed BlendMask can effectively predict dense per-pixel position-sensitive instance features with very few channels, and learn attention maps for each instance with merely one convolution layer, thus being fast in inference. BlendMask can be easily incorporated with the state-of-the-art one-stage detection frameworks and outperforms Mask R-CNN under the same training schedule while being 20% faster. A light-weight version of BlendMask achieves $ 34.2% $ mAP at 25 FPS evaluated on a single 1080Ti GPU card. Because of its simplicity and efficacy, we hope that our BlendMask could serve as a simple yet strong baseline for a wide range of instance-wise prediction tasks.   Code is available at https://git.io/AdelaiDet



### NAS-Bench-201: Extending the Scope of Reproducible Neural Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/2001.00326v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.00326v2)
- **Published**: 2020-01-02 05:28:26+00:00
- **Updated**: 2020-01-15 12:38:55+00:00
- **Authors**: Xuanyi Dong, Yi Yang
- **Comment**: Published at ICLR 2020 as a spotlight paper; 16 pages; 10 figures; 7
  tables; Code is available at https://github.com/D-X-Y/AutoDL-Projects
- **Journal**: None
- **Summary**: Neural architecture search (NAS) has achieved breakthrough success in a great number of applications in the past few years. It could be time to take a step back and analyze the good and bad aspects in the field of NAS. A variety of algorithms search architectures under different search space. These searched architectures are trained using different setups, e.g., hyper-parameters, data augmentation, regularization. This raises a comparability problem when comparing the performance of various NAS algorithms. NAS-Bench-101 has shown success to alleviate this problem. In this work, we propose an extension to NAS-Bench-101: NAS-Bench-201 with a different search space, results on multiple datasets, and more diagnostic information. NAS-Bench-201 has a fixed search space and provides a unified benchmark for almost any up-to-date NAS algorithms. The design of our search space is inspired from the one used in the most popular cell-based searching algorithms, where a cell is represented as a DAG. Each edge here is associated with an operation selected from a predefined operation set. For it to be applicable for all NAS algorithms, the search space defined in NAS-Bench-201 includes all possible architectures generated by 4 nodes and 5 associated operation options, which results in 15,625 candidates in total. The training log and the performance for each architecture candidate are provided for three datasets. This allows researchers to avoid unnecessary repetitive training for selected candidate and focus solely on the search algorithm itself. The training time saved for every candidate also largely improves the efficiency of many methods. We provide additional diagnostic information such as fine-grained loss and accuracy, which can give inspirations to new designs of NAS algorithms. In further support, we have analyzed it from many aspects and benchmarked 10 recent NAS algorithms.



### Graph-FCN for image semantic segmentation
- **Arxiv ID**: http://arxiv.org/abs/2001.00335v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.00335v1)
- **Published**: 2020-01-02 06:05:29+00:00
- **Updated**: 2020-01-02 06:05:29+00:00
- **Authors**: Yi Lu, Yaran Chen, Dongbin Zhao, Jianxin Chen
- **Comment**: None
- **Journal**: Advances in Neural Networks, ISNN 2019. Lecture Notes in Computer
  Science, vol 11554, pp. 97-105, Springer, Cham
- **Summary**: Semantic segmentation with deep learning has achieved great progress in classifying the pixels in the image. However, the local location information is usually ignored in the high-level feature extraction by the deep learning, which is important for image semantic segmentation. To avoid this problem, we propose a graph model initialized by a fully convolutional network (FCN) named Graph-FCN for image semantic segmentation. Firstly, the image grid data is extended to graph structure data by a convolutional network, which transforms the semantic segmentation problem into a graph node classification problem. Then we apply graph convolutional network to solve this graph node classification problem. As far as we know, it is the first time that we apply the graph convolutional network in image semantic segmentation. Our method achieves competitive performance in mean intersection over union (mIOU) on the VOC dataset(about 1.34% improvement), compared to the original FCN model.



### A$^3$DSegNet: Anatomy-aware artifact disentanglement and segmentation network for unpaired segmentation, artifact reduction, and modality translation
- **Arxiv ID**: http://arxiv.org/abs/2001.00339v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2001.00339v3)
- **Published**: 2020-01-02 06:37:09+00:00
- **Updated**: 2021-03-09 12:49:56+00:00
- **Authors**: Yuanyuan Lyu, Haofu Liao, Heqin Zhu, S. Kevin Zhou
- **Comment**: Accepted by IPMI 2021
- **Journal**: None
- **Summary**: Spinal surgery planning necessitates automatic segmentation of vertebrae in cone-beam computed tomography (CBCT), an intraoperative imaging modality that is widely used in intervention. However, CBCT images are of low-quality and artifact-laden due to noise, poor tissue contrast, and the presence of metallic objects, causing vertebra segmentation, even manually, a demanding task. In contrast, there exists a wealth of artifact-free, high quality CT images with vertebra annotations. This motivates us to build a CBCT vertebra segmentation model using unpaired CT images with annotations. To overcome the domain and artifact gaps between CBCT and CT, it is a must to address the three heterogeneous tasks of vertebra segmentation, artifact reduction and modality translation all together. To this, we propose a novel anatomy-aware artifact disentanglement and segmentation network (A$^3$DSegNet) that intensively leverages knowledge sharing of these three tasks to promote learning. Specifically, it takes a random pair of CBCT and CT images as the input and manipulates the synthesis and segmentation via different decoding combinations from the disentangled latent layers. Then, by proposing various forms of consistency among the synthesized images and among segmented vertebrae, the learning is achieved without paired (i.e., anatomically identical) data. Finally, we stack 2D slices together and build 3D networks on top to obtain final 3D segmentation result. Extensive experiments on a large number of clinical CBCT (21,364) and CT (17,089) images show that the proposed A$^3$DSegNet performs significantly better than state-of-the-art competing methods trained independently for each task and, remarkably, it achieves an average Dice coefficient of 0.926 for unpaired 3D CBCT vertebra segmentation.



### Encoding Metal Mask Projection for Metal Artifact Reduction in Computed Tomography
- **Arxiv ID**: http://arxiv.org/abs/2001.00340v3
- **DOI**: 10.1007/978-3-030-59713-9_15
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2001.00340v3)
- **Published**: 2020-01-02 06:39:06+00:00
- **Updated**: 2020-07-19 14:31:56+00:00
- **Authors**: Yuanyuan Lyu, Wei-An Lin, Haofu Liao, Jingjing Lu, S. Kevin Zhou
- **Comment**: accepted by MICCAI 2020
- **Journal**: None
- **Summary**: Metal artifact reduction (MAR) in computed tomography (CT) is a notoriously challenging task because the artifacts are structured and non-local in the image domain. However, they are inherently local in the sinogram domain. Thus, one possible approach to MAR is to exploit the latter characteristic by learning to reduce artifacts in the sinogram. However, if we directly treat the metal-affected regions in sinogram as missing and replace them with the surrogate data generated by a neural network, the artifact-reduced CT images tend to be over-smoothed and distorted since fine-grained details within the metal-affected regions are completely ignored. In this work, we provide analytical investigation to the issue and propose to address the problem by (1) retaining the metal-affected regions in sinogram and (2) replacing the binarized metal trace with the metal mask projection such that the geometry information of metal implants is encoded. Extensive experiments on simulated datasets and expert evaluations on clinical images demonstrate that our novel network yields anatomically more precise artifact-reduced images than the state-of-the-art approaches, especially when metallic objects are large.



### First image then video: A two-stage network for spatiotemporal video denoising
- **Arxiv ID**: http://arxiv.org/abs/2001.00346v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.00346v2)
- **Published**: 2020-01-02 07:21:39+00:00
- **Updated**: 2020-01-22 03:36:15+00:00
- **Authors**: Ce Wang, S. Kevin Zhou, Zhiwei Cheng
- **Comment**: None
- **Journal**: None
- **Summary**: Video denoising is to remove noise from noise-corrupted data, thus recovering true signals via spatiotemporal processing. Existing approaches for spatiotemporal video denoising tend to suffer from motion blur artifacts, that is, the boundary of a moving object tends to appear blurry especially when the object undergoes a fast motion, causing optical flow calculation to break down. In this paper, we address this challenge by designing a first-image-then-video two-stage denoising neural network, consisting of an image denoising module for spatially reducing intra-frame noise followed by a regular spatiotemporal video denoising module. The intuition is simple yet powerful and effective: the first stage of image denoising effectively reduces the noise level and, therefore, allows the second stage of spatiotemporal denoising for better modeling and learning everywhere, including along the moving object boundaries. This two-stage network, when trained in an end-to-end fashion, yields the state-of-the-art performances on the video denoising benchmark Vimeo90K dataset in terms of both denoising quality and computation. It also enables an unsupervised approach that achieves comparable performance to existing supervised approaches.



### Kernelized Support Tensor Train Machines
- **Arxiv ID**: http://arxiv.org/abs/2001.00360v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2001.00360v1)
- **Published**: 2020-01-02 08:40:15+00:00
- **Updated**: 2020-01-02 08:40:15+00:00
- **Authors**: Cong Chen, Kim Batselier, Wenjian Yu, Ngai Wong
- **Comment**: None
- **Journal**: None
- **Summary**: Tensor, a multi-dimensional data structure, has been exploited recently in the machine learning community. Traditional machine learning approaches are vector- or matrix-based, and cannot handle tensorial data directly. In this paper, we propose a tensor train (TT)-based kernel technique for the first time, and apply it to the conventional support vector machine (SVM) for image classification. Specifically, we propose a kernelized support tensor train machine that accepts tensorial input and preserves the intrinsic kernel property. The main contributions are threefold. First, we propose a TT-based feature mapping procedure that maintains the TT structure in the feature space. Second, we demonstrate two ways to construct the TT-based kernel function while considering consistency with the TT inner product and preservation of information. Third, we show that it is possible to apply different kernel functions on different data modes. In principle, our method tensorizes the standard SVM on its input structure and kernel mapping scheme. Extensive experiments are performed on real-world tensor data, which demonstrates the superiority of the proposed scheme under few-sample high-dimensional inputs.



### Butterfly Detection and Classification Based on Integrated YOLO Algorithm
- **Arxiv ID**: http://arxiv.org/abs/2001.00361v2
- **DOI**: 10.1007/978-981-15-3308-2_55
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.00361v2)
- **Published**: 2020-01-02 08:52:18+00:00
- **Updated**: 2020-04-26 02:50:28+00:00
- **Authors**: Bohan Liang, Shangxi Wu, Kaiyuan Xu, Jingyu Hao
- **Comment**: 13th ICGEC 2019: Qingdao, China
- **Journal**: Genetic and Evolutionary Computing. ICGEC 2019. Advances in
  Intelligent Systems and Computing, vol 1107. Springer, Singapore
- **Summary**: Insects are abundant species on the earth, and the task of identification and identification of insects is complex and arduous. How to apply artificial intelligence technology and digital image processing methods to automatic identification of insect species is a hot issue in current research. In this paper, the problem of automatic detection and classification recognition of butterfly photographs is studied, and a method of bio-labeling suitable for butterfly classification is proposed. On the basis of YOLO algorithm, by synthesizing the results of YOLO models with different training mechanisms, a butterfly automatic detection and classification recognition algorithm based on YOLO algorithm is proposed. It greatly improves the generalization ability of YOLO algorithm and makes it have better ability to solve small sample problems. The experimental results show that the proposed annotation method and integrated YOLO algorithm have high accuracy and recognition rate in butterfly automatic detection and recognition.



### Restricting the Flow: Information Bottlenecks for Attribution
- **Arxiv ID**: http://arxiv.org/abs/2001.00396v4
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2001.00396v4)
- **Published**: 2020-01-02 11:24:35+00:00
- **Updated**: 2020-05-25 14:21:37+00:00
- **Authors**: Karl Schulz, Leon Sixt, Federico Tombari, Tim Landgraf
- **Comment**: 18 pages, 12 figures, accepted at ICLR 2020 (Oral)
- **Journal**: None
- **Summary**: Attribution methods provide insights into the decision-making of machine learning models like artificial neural networks. For a given input sample, they assign a relevance score to each individual input variable, such as the pixels of an image. In this work we adapt the information bottleneck concept for attribution. By adding noise to intermediate feature maps we restrict the flow of information and can quantify (in bits) how much information image regions provide. We compare our method against ten baselines using three different metrics on VGG-16 and ResNet-50, and find that our methods outperform all baselines in five out of six settings. The method's information-theoretic foundation provides an absolute frame of reference for attribution values (bits) and a guarantee that regions scored close to zero are not necessary for the network's decision. For reviews: https://openreview.net/forum?id=S1xWh1rYwB For code: https://github.com/BioroboticsLab/IBA



### Kalman Filtering and Expectation Maximization for Multitemporal Spectral Unmixing
- **Arxiv ID**: http://arxiv.org/abs/2001.00425v2
- **DOI**: 10.1109/LGRS.2020.3025781
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2001.00425v2)
- **Published**: 2020-01-02 13:12:46+00:00
- **Updated**: 2020-08-12 21:52:20+00:00
- **Authors**: Ricardo Augusto Borsoi, Tales Imbiriba, Pau Closas, José Carlos Moreira Bermudez, Cédric Richard
- **Comment**: None
- **Journal**: None
- **Summary**: The recent evolution of hyperspectral imaging technology and the proliferation of new emerging applications presses for the processing of multiple temporal hyperspectral images. In this work, we propose a novel spectral unmixing (SU) strategy using physically motivated parametric endmember representations to account for temporal spectral variability. By representing the multitemporal mixing process using a state-space formulation, we are able to exploit the Bayesian filtering machinery to estimate the endmember variability coefficients. Moreover, by assuming that the temporal variability of the abundances is small over short intervals, an efficient implementation of the expectation maximization (EM) algorithm is employed to estimate the abundances and the other model parameters. Simulation results indicate that the proposed strategy outperforms state-of-the-art multitemporal SU algorithms.



### Using CNNs For Users Segmentation In Video See-Through Augmented Virtuality
- **Arxiv ID**: http://arxiv.org/abs/2001.00487v1
- **DOI**: 10.1109/AIVR46125.2019.00048
- **Categories**: **cs.CV**, I.2.10, I.3.7, I.2.10; I.3.7
- **Links**: [PDF](http://arxiv.org/pdf/2001.00487v1)
- **Published**: 2020-01-02 15:22:36+00:00
- **Updated**: 2020-01-02 15:22:36+00:00
- **Authors**: Pierre-Olivier Pigny, Lionel Dominjon
- **Comment**: 6 pages, 6 figures. Published in the 2nd International Conference on
  Artificial Intelligence & Virtual Reality (IEEE AIVR 2019)
- **Journal**: Proceedings of 2019 IEEE International Conference on Artificial
  Intelligence and Virtual Reality (AIVR), pp. 229-234, San Diego, US, December
  2019
- **Summary**: In this paper, we present preliminary results on the use of deep learning techniques to integrate the users self-body and other participants into a head-mounted video see-through augmented virtuality scenario. It has been previously shown that seeing users bodies in such simulations may improve the feeling of both self and social presence in the virtual environment, as well as user performance. We propose to use a convolutional neural network for real time semantic segmentation of users bodies in the stereoscopic RGB video streams acquired from the perspective of the user. We describe design issues as well as implementation details of the system and demonstrate the feasibility of using such neural networks for merging users bodies in an augmented virtuality simulation.



### Lightweight Residual Densely Connected Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2001.00526v2
- **DOI**: 10.1007/s11042-020-09223-8
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2001.00526v2)
- **Published**: 2020-01-02 17:15:32+00:00
- **Updated**: 2020-06-08 14:18:58+00:00
- **Authors**: Fahimeh Fooladgar, Shohreh Kasaei
- **Comment**: None
- **Journal**: None
- **Summary**: Extremely efficient convolutional neural network architectures are one of the most important requirements for limited-resource devices (such as embedded and mobile devices). The computing power and memory size are two important constraints of these devices. Recently, some architectures have been proposed to overcome these limitations by considering specific hardware-software equipment. In this paper, the lightweight residual densely connected blocks are proposed to guaranty the deep supervision, efficient gradient flow, and feature reuse abilities of convolutional neural network. The proposed method decreases the cost of training and inference processes without using any special hardware-software equipment by just reducing the number of parameters and computational operations while achieving a feasible accuracy. Extensive experimental results demonstrate that the proposed architecture is more efficient than the AlexNet and VGGNet in terms of model size, required parameters, and even accuracy. The proposed model has been evaluated on the ImageNet, MNIST, Fashion MNIST, SVHN, CIFAR-10, and CIFAR-100. It achieves state-of-the-art results on Fashion MNIST dataset and reasonable results on the others. The obtained results show the superiority of the proposed method to efficient models such as the SqueezNet. It is also comparable with state-of-the-art efficient models such as CondenseNet and ShuffleNet.



### Physically Plausible Spectral Reconstruction from RGB Images
- **Arxiv ID**: http://arxiv.org/abs/2001.00558v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2001.00558v1)
- **Published**: 2020-01-02 18:46:26+00:00
- **Updated**: 2020-01-02 18:46:26+00:00
- **Authors**: Yi-Tun Lin, Graham D. Finlayson
- **Comment**: None
- **Journal**: None
- **Summary**: Recently Convolutional Neural Networks (CNN) have been used to reconstruct hyperspectral information from RGB images. Moreover, this spectral reconstruction problem (SR) can often be solved with good (low) error. However, these methods are not physically plausible: that is when the recovered spectra are reintegrated with the underlying camera sensitivities, the resulting predicted RGB is not the same as the actual RGB, and sometimes this discrepancy can be large. The problem is further compounded by exposure change. Indeed, most learning-based SR models train for a fixed exposure setting and we show that this can result in poor performance when exposure varies.   In this paper we show how CNN learning can be extended so that physical plausibility is enforced and the problem resulting from changing exposures is mitigated. Our SR solution improves the state-of-the-art spectral recovery performance under varying exposure conditions while simultaneously ensuring physical plausibility (the recovered spectra reintegrate to the input RGBs exactly).



### PrivacyNet: Semi-Adversarial Networks for Multi-attribute Face Privacy
- **Arxiv ID**: http://arxiv.org/abs/2001.00561v3
- **DOI**: 10.1109/TIP.2020.3024026
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2001.00561v3)
- **Published**: 2020-01-02 18:53:31+00:00
- **Updated**: 2021-03-14 00:13:02+00:00
- **Authors**: Vahid Mirjalili, Sebastian Raschka, Arun Ross
- **Comment**: 13 pages, 9 figures
- **Journal**: IEEE Transactions on Image Processing, 2020
- **Summary**: Recent research has established the possibility of deducing soft-biometric attributes such as age, gender and race from an individual's face image with high accuracy. However, this raises privacy concerns, especially when face images collected for biometric recognition purposes are used for attribute analysis without the person's consent. To address this problem, we develop a technique for imparting soft biometric privacy to face images via an image perturbation methodology. The image perturbation is undertaken using a GAN-based Semi-Adversarial Network (SAN) - referred to as PrivacyNet - that modifies an input face image such that it can be used by a face matcher for matching purposes but cannot be reliably used by an attribute classifier. Further, PrivacyNet allows a person to choose specific attributes that have to be obfuscated in the input face images (e.g., age and race), while allowing for other types of attributes to be extracted (e.g., gender). Extensive experiments using multiple face matchers, multiple age/gender/race classifiers, and multiple face datasets demonstrate the generalizability of the proposed multi-attribute privacy enhancing method across multiple face and attribute classifiers.



### A Machine Learning Imaging Core using Separable FIR-IIR Filters
- **Arxiv ID**: http://arxiv.org/abs/2001.00630v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2001.00630v1)
- **Published**: 2020-01-02 21:24:26+00:00
- **Updated**: 2020-01-02 21:24:26+00:00
- **Authors**: Masayoshi Asama, Leo F. Isikdogan, Sushma Rao, Bhavin V. Nayak, Gilad Michael
- **Comment**: None
- **Journal**: None
- **Summary**: We propose fixed-function neural network hardware that is designed to perform pixel-to-pixel image transformations in a highly efficient way. We use a fully trainable, fixed-topology neural network to build a model that can perform a wide variety of image processing tasks. Our model uses compressed skip lines and hybrid FIR-IIR blocks to reduce the latency and hardware footprint. Our proposed Machine Learning Imaging Core, dubbed MagIC, uses a silicon area of ~3mm^2 (in TSMC 16nm), which is orders of magnitude smaller than a comparable pixel-wise dense prediction model. MagIC requires no DDR bandwidth, no SRAM, and practically no external memory. Each MagIC core consumes 56mW (215 mW max power) at 500MHz and achieves an energy-efficient throughput of 23TOPS/W/mm^2. MagIC can be used as a multi-purpose image processing block in an imaging pipeline, approximating compute-heavy image processing applications, such as image deblurring, denoising, and colorization, within the power and silicon area limits of mobile devices.



### From Kinematics To Dynamics: Estimating Center of Pressure and Base of Support from Video Frames of Human Motion
- **Arxiv ID**: http://arxiv.org/abs/2001.00657v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.00657v1)
- **Published**: 2020-01-02 22:41:00+00:00
- **Updated**: 2020-01-02 22:41:00+00:00
- **Authors**: Jesse Scott, Christopher Funk, Bharadwaj Ravichandran, John H. Challis, Robert T. Collins, Yanxi Liu
- **Comment**: None
- **Journal**: None
- **Summary**: To gain an understanding of the relation between a given human pose image and the corresponding physical foot pressure of the human subject, we propose and validate two end-to-end deep learning architectures, PressNet and PressNet-Simple, to regress foot pressure heatmaps (dynamics) from 2D human pose (kinematics) derived from a video frame. A unique video and foot pressure data set of 813,050 synchronized pairs, composed of 5-minute long choreographed Taiji movement sequences of 6 subjects, is collected and used for leaving-one-subject-out cross validation. Our initial experimental results demonstrate reliable and repeatable foot pressure prediction from a single image, setting the first baseline for such a complex cross modality mapping problem in computer vision. Furthermore, we compute and quantitatively validate the Center of Pressure (CoP) and Base of Support (BoS) from predicted foot pressure distribution, obtaining key components in pose stability analysis from images with potential applications in kinesiology, medicine, sports and robotics.



### Synthetic vascular structure generation for unsupervised pre-training in CTA segmentation tasks
- **Arxiv ID**: http://arxiv.org/abs/2001.00666v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2001.00666v1)
- **Published**: 2020-01-02 23:21:22+00:00
- **Updated**: 2020-01-02 23:21:22+00:00
- **Authors**: Nil Stolt Ansó
- **Comment**: None
- **Journal**: None
- **Summary**: Large enough computed tomography (CT) data sets to train supervised deep models are often hard to come by. One contributing issue is the amount of manual labor that goes into creating ground truth labels, specially for volumetric data. In this research, we train a U-net architecture at a vessel segmentation task that can be used to provide insights when treating stroke patients. We create a computational model that generates synthetic vascular structures which can be blended into unlabeled CT scans of the head. This unsupervised approached to labelling is used to pre-train deep segmentation models, which are later fine-tuned on real examples to achieve an increase in accuracy compared to models trained exclusively on a hand-labeled data set.



### DeepFocus: a Few-Shot Microscope Slide Auto-Focus using a Sample Invariant CNN-based Sharpness Function
- **Arxiv ID**: http://arxiv.org/abs/2001.00667v1
- **DOI**: 10.1109/ISBI45749.2020.9098331
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2001.00667v1)
- **Published**: 2020-01-02 23:29:11+00:00
- **Updated**: 2020-01-02 23:29:11+00:00
- **Authors**: Adrian Shajkofci, Michael Liebling
- **Comment**: Submitted to IEEE ISBI 2020
- **Journal**: 2020 IEEE 17th International Symposium on Biomedical Imaging
  (ISBI)
- **Summary**: Autofocus (AF) methods are extensively used in biomicroscopy, for example to acquire timelapses, where the imaged objects tend to drift out of focus. AD algorithms determine an optimal distance by which to move the sample back into the focal plane. Current hardware-based methods require modifying the microscope and image-based algorithms either rely on many images to converge to the sharpest position or need training data and models specific to each instrument and imaging configuration. Here we propose DeepFocus, an AF method we implemented as a Micro-Manager plugin, and characterize its Convolutional neural network-based sharpness function, which we observed to be depth co-variant and sample-invariant. Sample invariance allows our AF algorithm to converge to an optimal axial position within as few as three iterations using a model trained once for use with a wide range of optical microscopes and a single instrument-dependent calibration stack acquisition of a flat (but arbitrary) textured object. From experiments carried out both on synthetic and experimental data, we observed an average precision, given 3 measured images, of 0.30 +- 0.16 micrometers with a 10x, NA 0.3 objective. We foresee that this performance and low image number will help limit photodamage during acquisitions with light-sensitive samples.



