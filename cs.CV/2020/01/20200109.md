# Arxiv Papers in cs.CV on 2020-01-09
### Neural Data Server: A Large-Scale Search Engine for Transfer Learning Data
- **Arxiv ID**: http://arxiv.org/abs/2001.02799v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2001.02799v3)
- **Published**: 2020-01-09 01:21:30+00:00
- **Updated**: 2020-04-01 00:43:03+00:00
- **Authors**: Xi Yan, David Acuna, Sanja Fidler
- **Comment**: None
- **Journal**: None
- **Summary**: Transfer learning has proven to be a successful technique to train deep learning models in the domains where little training data is available. The dominant approach is to pretrain a model on a large generic dataset such as ImageNet and finetune its weights on the target domain. However, in the new era of an ever-increasing number of massive datasets, selecting the relevant data for pretraining is a critical issue. We introduce Neural Data Server (NDS), a large-scale search engine for finding the most useful transfer learning data to the target domain. NDS consists of a dataserver which indexes several large popular image datasets, and aims to recommend data to a client, an end-user with a target application with its own small labeled dataset. The dataserver represents large datasets with a much more compact mixture-of-experts model, and employs it to perform data search in a series of dataserver-client transactions at a low computational cost. We show the effectiveness of NDS in various transfer learning scenarios, demonstrating state-of-the-art performance on several target datasets and tasks such as image classification, object detection and instance segmentation. Neural Data Server is available as a web-service at http://aidemos.cs.toronto.edu/nds/.



### Learning landmark guided embeddings for animal re-identification
- **Arxiv ID**: http://arxiv.org/abs/2001.02801v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.02801v1)
- **Published**: 2020-01-09 01:31:00+00:00
- **Updated**: 2020-01-09 01:31:00+00:00
- **Authors**: Olga Moskvyak, Frederic Maire, Feras Dayoub, Mahsa Baktashmotlagh
- **Comment**: 7 pages, 7 figures
- **Journal**: None
- **Summary**: Re-identification of individual animals in images can be ambiguous due to subtle variations in body markings between different individuals and no constraints on the poses of animals in the wild. Person re-identification is a similar task and it has been approached with a deep convolutional neural network (CNN) that learns discriminative embeddings for images of people. However, learning discriminative features for an individual animal is more challenging than for a person's appearance due to the relatively small size of ecological datasets compared to labelled datasets of person's identities. We propose to improve embedding learning by exploiting body landmarks information explicitly. Body landmarks are provided to the input of a CNN as confidence heatmaps that can be obtained from a separate body landmark predictor. The model is encouraged to use heatmaps by learning an auxiliary task of reconstructing input heatmaps. Body landmarks guide a feature extraction network to learn the representation of a distinctive pattern and its position on the body. We evaluate the proposed method on a large synthetic dataset and a small real dataset. Our method outperforms the same model without body landmarks input by 26% and 18% on the synthetic and the real datasets respectively. The method is robust to noise in input coordinates and can tolerate an error in coordinates up to 10% of the image size.



### An Internal Covariate Shift Bounding Algorithm for Deep Neural Networks by Unitizing Layers' Outputs
- **Arxiv ID**: http://arxiv.org/abs/2001.02814v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2001.02814v1)
- **Published**: 2020-01-09 02:35:58+00:00
- **Updated**: 2020-01-09 02:35:58+00:00
- **Authors**: You Huang, Yuanlong Yu
- **Comment**: 19 pages, 3 figures
- **Journal**: None
- **Summary**: Batch Normalization (BN) techniques have been proposed to reduce the so-called Internal Covariate Shift (ICS) by attempting to keep the distributions of layer outputs unchanged. Experiments have shown their effectiveness on training deep neural networks. However, since only the first two moments are controlled in these BN techniques, it seems that a weak constraint is imposed on layer distributions and furthermore whether such constraint can reduce ICS is unknown. Thus this paper proposes a measure for ICS by using the Earth Mover (EM) distance and then derives the upper and lower bounds for the measure to provide a theoretical analysis of BN. The upper bound has shown that BN techniques can control ICS only for the outputs with low dimensions and small noise whereas their control is NOT effective in other cases. This paper also proves that such control is just a bounding of ICS rather than a reduction of ICS. Meanwhile, the analysis shows that the high-order moments and noise, which BN cannot control, have great impact on the lower bound. Based on such analysis, this paper furthermore proposes an algorithm that unitizes the outputs with an adjustable parameter to further bound ICS in order to cope with the problems of BN. The upper bound for the proposed unitization is noise-free and only dominated by the parameter. Thus, the parameter can be trained to tune the bound and further to control ICS. Besides, the unitization is embedded into the framework of BN to reduce the information loss. The experiments show that this proposed algorithm outperforms existing BN techniques on CIFAR-10, CIFAR-100 and ImageNet datasets.



### Multi-Scale Weight Sharing Network for Image Recognition
- **Arxiv ID**: http://arxiv.org/abs/2001.02816v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.02816v1)
- **Published**: 2020-01-09 02:42:00+00:00
- **Updated**: 2020-01-09 02:42:00+00:00
- **Authors**: Shubhra Aich, Ian Stavness, Yasuhiro Taniguchi, Masaki Yamazaki
- **Comment**: Accepted in Pattern Recognition Letters, Elsevier
- **Journal**: None
- **Summary**: In this paper, we explore the idea of weight sharing over multiple scales in convolutional networks. Inspired by traditional computer vision approaches, we share the weights of convolution kernels over different scales in the same layers of the network. Although multi-scale feature aggregation and sharing inside convolutional networks are common in practice, none of the previous works address the issue of convolutional weight sharing. We evaluate our weight sharing scheme on two heterogeneous image recognition datasets - ImageNet (object recognition) and Places365-Standard (scene classification). With approximately 25% fewer parameters, our shared-weight ResNet model provides similar performance compared to baseline ResNets. Shared-weight models are further validated via transfer learning experiments on four additional image recognition datasets - Caltech256 and Stanford 40 Actions (object-centric) and SUN397 and MIT Inddor67 (scene-centric). Experimental results demonstrate significant redundancy in the vanilla implementations of the deeper networks, and also indicate that a shift towards increasing the receptive field per parameter may improve future convolutional network architectures.



### A novel tree-structured point cloud dataset for skeletonization algorithm evaluation
- **Arxiv ID**: http://arxiv.org/abs/2001.02823v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.02823v1)
- **Published**: 2020-01-09 03:35:57+00:00
- **Updated**: 2020-01-09 03:35:57+00:00
- **Authors**: Yan Lin, Ji Liu, Jianlin Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: Curve skeleton extraction from unorganized point cloud is a fundamental task of computer vision and three-dimensional data preprocessing and visualization. A great amount of work has been done to extract skeleton from point cloud. but the lack of standard datasets of point cloud with ground truth skeleton makes it difficult to evaluate these algorithms. In this paper, we construct a brand new tree-structured point cloud dataset, including ground truth skeletons, and point cloud models. In addition, four types of point cloud are built on clean point cloud: point clouds with noise, point clouds with missing data, point clouds with different density, and point clouds with uneven density distribution. We first use tree editor to build the tree skeleton and corresponding mesh model. Since the implicit surface is sufficiently expressive to retain the edges and details of the complex branches model, we use the implicit surface to model the triangular mesh. With the implicit surface, virtual scanner is applied to the sampling of point cloud. Finally, considering the challenges in skeleton extraction, we introduce different methods to build four different types of point cloud models. This dataset can be used as standard dataset for skeleton extraction algorithms. And the evaluation between skeleton extraction algorithms can be performed by comparing the ground truth skeleton with the extracted skeleton.



### An inexact matching approach for the comparison of plane curves with general elastic metrics
- **Arxiv ID**: http://arxiv.org/abs/2001.02858v1
- **DOI**: 10.1109/IEEECONF44664.2019.9049031
- **Categories**: **cs.CG**, cs.CV, math.DG
- **Links**: [PDF](http://arxiv.org/pdf/2001.02858v1)
- **Published**: 2020-01-09 06:45:50+00:00
- **Updated**: 2020-01-09 06:45:50+00:00
- **Authors**: Yashil Sukurdeep, Martin Bauer, Nicolas Charon
- **Comment**: 5 pages, 5 figures
- **Journal**: None
- **Summary**: This paper introduces a new mathematical formulation and numerical approach for the computation of distances and geodesics between immersed planar curves. Our approach combines the general simplifying transform for first-order elastic metrics that was recently introduced by Kurtek and Needham, together with a relaxation of the matching constraint using parametrization-invariant fidelity metrics. The main advantages of this formulation are that it leads to a simple optimization problem for discretized curves, and that it provides a flexible approach to deal with noisy, inconsistent or corrupted data. These benefits are illustrated via a few preliminary numerical results.



### Semi-supervised Learning via Conditional Rotation Angle Estimation
- **Arxiv ID**: http://arxiv.org/abs/2001.02865v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.02865v1)
- **Published**: 2020-01-09 07:06:20+00:00
- **Updated**: 2020-01-09 07:06:20+00:00
- **Authors**: Hai-Ming Xu, Lingqiao Liu, Dong Gong
- **Comment**: None
- **Journal**: None
- **Summary**: Self-supervised learning (SlfSL), aiming at learning feature representations through ingeniously designed pretext tasks without human annotation, has achieved compelling progress in the past few years. Very recently, SlfSL has also been identified as a promising solution for semi-supervised learning (SemSL) since it offers a new paradigm to utilize unlabeled data. This work further explores this direction by proposing to couple SlfSL with SemSL. Our insight is that the prediction target in SemSL can be modeled as the latent factor in the predictor for the SlfSL target. Marginalizing over the latent factor naturally derives a new formulation which marries the prediction targets of these two learning processes. By implementing this idea through a simple-but-effective SlfSL approach -- rotation angle prediction, we create a new SemSL approach called Conditional Rotation Angle Estimation (CRAE). Specifically, CRAE is featured by adopting a module which predicts the image rotation angle conditioned on the candidate image class. Through experimental evaluation, we show that CRAE achieves superior performance over the other existing ways of combining SlfSL and SemSL. To further boost CRAE, we propose two extensions to strengthen the coupling between SemSL target and SlfSL target in basic CRAE. We show that this leads to an improved CRAE method which can achieve the state-of-the-art SemSL performance.



### Hybrid Multiple Attention Network for Semantic Segmentation in Aerial Images
- **Arxiv ID**: http://arxiv.org/abs/2001.02870v3
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2001.02870v3)
- **Published**: 2020-01-09 07:47:51+00:00
- **Updated**: 2020-09-15 02:17:37+00:00
- **Authors**: Ruigang Niu, Xian Sun, Yu Tian, Wenhui Diao, Kaiqiang Chen, Kun Fu
- **Comment**: None
- **Journal**: None
- **Summary**: Semantic segmentation in very high resolution (VHR) aerial images is one of the most challenging tasks in remote sensing image understanding. Most of the current approaches are based on deep convolutional neural networks (DCNNs). However, standard convolution with local receptive fields fails in modeling global dependencies. Prior researches have indicated that attention-based methods can capture long-range dependencies and further reconstruct the feature maps for better representation. Nevertheless, limited by the mere perspective of spacial and channel attention and huge computation complexity of self-attention mechanism, it is unlikely to model the effective semantic interdependencies between each pixel-pair of remote sensing data of complex spectra. In this work, we propose a novel attention-based framework named Hybrid Multiple Attention Network (HMANet) to adaptively capture global correlations from the perspective of space, channel and category in a more effective and efficient manner. Concretely, a class augmented attention (CAA) module embedded with a class channel attention (CCA) module can be used to compute category-based correlation and recalibrate the class-level information. Additionally, we introduce a simple yet effective region shuffle attention (RSA) module to reduce feature redundant and improve the efficiency of self-attention mechanism via region-wise representations. Extensive experimental results on the ISPRS Vaihingen and Potsdam benchmark demonstrate the effectiveness and efficiency of our HMANet over other state-of-the-art methods.



### Deep Plastic Surgery: Robust and Controllable Image Editing with Human-Drawn Sketches
- **Arxiv ID**: http://arxiv.org/abs/2001.02890v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.02890v1)
- **Published**: 2020-01-09 08:57:50+00:00
- **Updated**: 2020-01-09 08:57:50+00:00
- **Authors**: Shuai Yang, Zhangyang Wang, Jiaying Liu, Zongming Guo
- **Comment**: None
- **Journal**: None
- **Summary**: Sketch-based image editing aims to synthesize and modify photos based on the structural information provided by the human-drawn sketches. Since sketches are difficult to collect, previous methods mainly use edge maps instead of sketches to train models (referred to as edge-based models). However, sketches display great structural discrepancy with edge maps, thus failing edge-based models. Moreover, sketches often demonstrate huge variety among different users, demanding even higher generalizability and robustness for the editing model to work. In this paper, we propose Deep Plastic Surgery, a novel, robust and controllable image editing framework that allows users to interactively edit images using hand-drawn sketch inputs. We present a sketch refinement strategy, as inspired by the coarse-to-fine drawing process of the artists, which we show can help our model well adapt to casual and varied sketches without the need for real sketch training data. Our model further provides a refinement level control parameter that enables users to flexibly define how "reliable" the input sketch should be considered for the final output, balancing between sketch faithfulness and output verisimilitude (as the two goals might contradict if the input sketch is drawn poorly). To achieve the multi-level refinement, we introduce a style-based module for level conditioning, which allows adaptive feature representations for different levels in a singe network. Extensive experimental results demonstrate the superiority of our approach in improving the visual quality and user controllablity of image editing over the state-of-the-art methods.



### Self-Supervised Fast Adaptation for Denoising via Meta-Learning
- **Arxiv ID**: http://arxiv.org/abs/2001.02899v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.02899v1)
- **Published**: 2020-01-09 09:40:53+00:00
- **Updated**: 2020-01-09 09:40:53+00:00
- **Authors**: Seunghwan Lee, Donghyeon Cho, Jiwon Kim, Tae Hyun Kim
- **Comment**: None
- **Journal**: None
- **Summary**: Under certain statistical assumptions of noise, recent self-supervised approaches for denoising have been introduced to learn network parameters without true clean images, and these methods can restore an image by exploiting information available from the given input (i.e., internal statistics) at test time. However, self-supervised methods are not yet combined with conventional supervised denoising methods which train the denoising networks with a large number of external training samples. Thus, we propose a new denoising approach that can greatly outperform the state-of-the-art supervised denoising methods by adapting their network parameters to the given input through selfsupervision without changing the networks architectures. Moreover, we propose a meta-learning algorithm to enable quick adaptation of parameters to the specific input at test time. We demonstrate that the proposed method can be easily employed with state-of-the-art denoising networks without additional parameters, and achieve state-of-the-art performance on numerous benchmark datasets.



### Fast Adaptation to Super-Resolution Networks via Meta-Learning
- **Arxiv ID**: http://arxiv.org/abs/2001.02905v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.02905v3)
- **Published**: 2020-01-09 09:59:02+00:00
- **Updated**: 2020-08-25 09:24:50+00:00
- **Authors**: Seobin Park, Jinsu Yoo, Donghyeon Cho, Jiwon Kim, Tae Hyun Kim
- **Comment**: None
- **Journal**: None
- **Summary**: Conventional supervised super-resolution (SR) approaches are trained with massive external SR datasets but fail to exploit desirable properties of the given test image. On the other hand, self-supervised SR approaches utilize the internal information within a test image but suffer from computational complexity in run-time. In this work, we observe the opportunity for further improvement of the performance of SISR without changing the architecture of conventional SR networks by practically exploiting additional information given from the input image. In the training stage, we train the network via meta-learning; thus, the network can quickly adapt to any input image at test time. Then, in the test stage, parameters of this meta-learned network are rapidly fine-tuned with only a few iterations by only using the given low-resolution image. The adaptation at the test time takes full advantage of patch-recurrence property observed in natural images. Our method effectively handles unknown SR kernels and can be applied to any existing model. We demonstrate that the proposed model-agnostic approach consistently improves the performance of conventional SR networks on various benchmark SR datasets.



### Towards Coding for Human and Machine Vision: A Scalable Image Coding Approach
- **Arxiv ID**: http://arxiv.org/abs/2001.02915v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.02915v2)
- **Published**: 2020-01-09 10:37:17+00:00
- **Updated**: 2020-01-10 03:18:56+00:00
- **Authors**: Yueyu Hu, Shuai Yang, Wenhan Yang, Ling-Yu Duan, Jiaying Liu
- **Comment**: Project page: https://williamyang1991.github.io/projects/VCM-Face/
- **Journal**: None
- **Summary**: The past decades have witnessed the rapid development of image and video coding techniques in the era of big data. However, the signal fidelity-driven coding pipeline design limits the capability of the existing image/video coding frameworks to fulfill the needs of both machine and human vision. In this paper, we come up with a novel image coding framework by leveraging both the compressive and the generative models, to support machine vision and human perception tasks jointly. Given an input image, the feature analysis is first applied, and then the generative model is employed to perform image reconstruction with features and additional reference pixels, in which compact edge maps are extracted in this work to connect both kinds of vision in a scalable way. The compact edge map serves as the basic layer for machine vision tasks, and the reference pixels act as a sort of enhanced layer to guarantee signal fidelity for human vision. By introducing advanced generative models, we train a flexible network to reconstruct images from compact feature representations and the reference pixels. Experimental results demonstrate the superiority of our framework in both human visual quality and facial landmark detection, which provide useful evidence on the emerging standardization efforts on MPEG VCM (Video Coding for Machine).



### Domain Independent Unsupervised Learning to grasp the Novel Objects
- **Arxiv ID**: http://arxiv.org/abs/2001.05856v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2001.05856v1)
- **Published**: 2020-01-09 12:05:37+00:00
- **Updated**: 2020-01-09 12:05:37+00:00
- **Authors**: Siddhartha Vibhu Pharswan, Mohit Vohra, Ashish Kumar, Laxmidhar Behera
- **Comment**: Paper has been accepted for publication in IROS2019
- **Journal**: None
- **Summary**: One of the main challenges in the vision-based grasping is the selection of feasible grasp regions while interacting with novel objects. Recent approaches exploit the power of the convolutional neural network (CNN) to achieve accurate grasping at the cost of high computational power and time. In this paper, we present a novel unsupervised learning based algorithm for the selection of feasible grasp regions. Unsupervised learning infers the pattern in data-set without any external labels. We apply k-means clustering on the image plane to identify the grasp regions, followed by an axis assignment method. We define a novel concept of Grasp Decide Index (GDI) to select the best grasp pose in image plane. We have conducted several experiments in clutter or isolated environment on standard objects of Amazon Robotics Challenge 2017 and Amazon Picking Challenge 2016. We compare the results with prior learning based approaches to validate the robustness and adaptive nature of our algorithm for a variety of novel objects in different domains.



### Generative Pseudo-label Refinement for Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2001.02950v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.02950v1)
- **Published**: 2020-01-09 12:46:55+00:00
- **Updated**: 2020-01-09 12:46:55+00:00
- **Authors**: Pietro Morerio, Riccardo Volpi, Ruggero Ragonesi, Vittorio Murino
- **Comment**: None
- **Journal**: None
- **Summary**: We investigate and characterize the inherent resilience of conditional Generative Adversarial Networks (cGANs) against noise in their conditioning labels, and exploit this fact in the context of Unsupervised Domain Adaptation (UDA). In UDA, a classifier trained on the labelled source set can be used to infer pseudo-labels on the unlabelled target set. However, this will result in a significant amount of misclassified examples (due to the well-known domain shift issue), which can be interpreted as noise injection in the ground-truth labels for the target set. We show that cGANs are, to some extent, robust against such "shift noise". Indeed, cGANs trained with noisy pseudo-labels, are able to filter such noise and generate cleaner target samples. We exploit this finding in an iterative procedure where a generative model and a classifier are jointly trained: in turn, the generator allows to sample cleaner data from the target distribution, and the classifier allows to associate better labels to target samples, progressively refining target pseudo-labels. Results on common benchmarks show that our method performs better or comparably with the unsupervised domain adaptation state of the art.



### Objects detection for remote sensing images based on polar coordinates
- **Arxiv ID**: http://arxiv.org/abs/2001.02988v7
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.02988v7)
- **Published**: 2020-01-09 14:02:51+00:00
- **Updated**: 2020-09-21 02:31:48+00:00
- **Authors**: Lin Zhou, Haoran Wei, Hao Li, Wenzhe Zhao, Yi Zhang, Yue Zhang
- **Comment**: The paper needs a lot of revision. Some problem are not well
  described. However, this paper has spread out. I think the impact of an
  imperfect first draft is not good, so we want to withdraw and revise
- **Journal**: None
- **Summary**: Arbitrary-oriented object detection is an important task in the field of remote sensing object detection. Existing studies have shown that the polar coordinate system has obvious advantages in dealing with the problem of rotating object modeling, that is, using fewer parameters to achieve more accurate rotating object detection. However, present state-of-the-art detectors based on deep learning are all modeled in Cartesian coordinates. In this article, we introduce the polar coordinate system to the deep learning detector for the first time, and propose an anchor free Polar Remote Sensing Object Detector (P-RSDet), which can achieve competitive detection accuracy via uses simpler object representation model and less regression parameters. In P-RSDet method, arbitrary-oriented object detection can be achieved by predicting the center point and regressing one polar radius and two polar angles. Besides, in order to express the geometric constraint relationship between the polar radius and the polar angle, a Polar Ring Area Loss function is proposed to improve the prediction accuracy of the corner position. Experiments on DOTA, UCAS-AOD and NWPU VHR-10 datasets show that our P-RSDet achieves state-of-the-art performances with simpler model and less regression parameters.



### Spherical Image Generation from a Single Normal Field of View Image by Considering Scene Symmetry
- **Arxiv ID**: http://arxiv.org/abs/2001.02993v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2001.02993v1)
- **Published**: 2020-01-09 14:09:12+00:00
- **Updated**: 2020-01-09 14:09:12+00:00
- **Authors**: Takayuki Hara, Tatsuya Harada
- **Comment**: 15 pages
- **Journal**: None
- **Summary**: Spherical images taken in all directions (360 degrees) allow representing the surroundings of the subject and the space itself, providing an immersive experience to the viewers. Generating a spherical image from a single normal-field-of-view (NFOV) image is convenient and considerably expands the usage scenarios because there is no need to use a specific panoramic camera or take images from multiple directions; however, it is still a challenging and unsolved problem. The primary challenge is controlling the high degree of freedom involved in generating a wide area that includes the all directions of the desired plausible spherical image. On the other hand, scene symmetry is a basic property of the global structure of the spherical images, such as rotation symmetry, plane symmetry and asymmetry. We propose a method to generate spherical image from a single NFOV image, and control the degree of freedom of the generated regions using scene symmetry. We incorporate scene-symmetry parameters as latent variables into conditional variational autoencoders, following which we learn the conditional probability of spherical images for NFOV images and scene symmetry. Furthermore, the probability density functions are represented using neural networks, and scene symmetry is implemented using both circular shift and flip of the hidden variables. Our experiments show that the proposed method can generate various plausible spherical images, controlled from symmetric to asymmetric.



### An Emerging Coding Paradigm VCM: A Scalable Coding Approach Beyond Feature and Signal
- **Arxiv ID**: http://arxiv.org/abs/2001.03004v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2001.03004v1)
- **Published**: 2020-01-09 14:18:18+00:00
- **Updated**: 2020-01-09 14:18:18+00:00
- **Authors**: Sifeng Xia, Kunchangtai Liang, Wenhan Yang, Ling-Yu Duan, Jiaying Liu
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we study a new problem arising from the emerging MPEG standardization effort Video Coding for Machine (VCM), which aims to bridge the gap between visual feature compression and classical video coding. VCM is committed to address the requirement of compact signal representation for both machine and human vision in a more or less scalable way. To this end, we make endeavors in leveraging the strength of predictive and generative models to support advanced compression techniques for both machine and human vision tasks simultaneously, in which visual features serve as a bridge to connect signal-level and task-level compact representations in a scalable manner. Specifically, we employ a conditional deep generation network to reconstruct video frames with the guidance of learned motion pattern. By learning to extract sparse motion pattern via a predictive model, the network elegantly leverages the feature representation to generate the appearance of to-be-coded frames via a generative model, relying on the appearance of the coded key frames. Meanwhile, the sparse motion pattern is compact and highly effective for high-level vision tasks, e.g. action recognition. Experimental results demonstrate that our method yields much better reconstruction quality compared with the traditional video codecs (0.0063 gain in SSIM), as well as state-of-the-art action recognition performance over highly compressed videos (9.4% gain in recognition accuracy), which showcases a promising paradigm of coding signal for both human and machine vision.



### DeeperForensics-1.0: A Large-Scale Dataset for Real-World Face Forgery Detection
- **Arxiv ID**: http://arxiv.org/abs/2001.03024v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2001.03024v2)
- **Published**: 2020-01-09 14:37:17+00:00
- **Updated**: 2020-12-11 11:24:04+00:00
- **Authors**: Liming Jiang, Ren Li, Wayne Wu, Chen Qian, Chen Change Loy
- **Comment**: CVPR 2020. Project page:
  https://liming-jiang.com/projects/DrF1/DrF1.html
- **Journal**: None
- **Summary**: We present our on-going effort of constructing a large-scale benchmark for face forgery detection. The first version of this benchmark, DeeperForensics-1.0, represents the largest face forgery detection dataset by far, with 60,000 videos constituted by a total of 17.6 million frames, 10 times larger than existing datasets of the same kind. Extensive real-world perturbations are applied to obtain a more challenging benchmark of larger scale and higher diversity. All source videos in DeeperForensics-1.0 are carefully collected, and fake videos are generated by a newly proposed end-to-end face swapping framework. The quality of generated videos outperforms those in existing datasets, validated by user studies. The benchmark features a hidden test set, which contains manipulated videos achieving high deceptive scores in human evaluations. We further contribute a comprehensive study that evaluates five representative detection baselines and make a thorough analysis of different settings.



### Virtual to Real adaptation of Pedestrian Detectors
- **Arxiv ID**: http://arxiv.org/abs/2001.03032v3
- **DOI**: 10.3390/s20185250
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2001.03032v3)
- **Published**: 2020-01-09 14:50:11+00:00
- **Updated**: 2020-09-19 14:14:19+00:00
- **Authors**: Luca Ciampi, Nicola Messina, Fabrizio Falchi, Claudio Gennaro, Giuseppe Amato
- **Comment**: None
- **Journal**: Sensors 20.18 (2020): 5250
- **Summary**: Pedestrian detection through Computer Vision is a building block for a multitude of applications. Recently, there was an increasing interest in Convolutional Neural Network-based architectures for the execution of such a task. One of these supervised networks' critical goals is to generalize the knowledge learned during the training phase to new scenarios with different characteristics. A suitably labeled dataset is essential to achieve this purpose. The main problem is that manually annotating a dataset usually requires a lot of human effort, and it is costly. To this end, we introduce ViPeD (Virtual Pedestrian Dataset), a new synthetically generated set of images collected with the highly photo-realistic graphical engine of the video game GTA V - Grand Theft Auto V, where annotations are automatically acquired. However, when training solely on the synthetic dataset, the model experiences a Synthetic2Real Domain Shift leading to a performance drop when applied to real-world images. To mitigate this gap, we propose two different Domain Adaptation techniques suitable for the pedestrian detection task, but possibly applicable to general object detection. Experiments show that the network trained with ViPeD can generalize over unseen real-world scenarios better than the detector trained over real-world data, exploiting the variety of our synthetic dataset. Furthermore, we demonstrate that with our Domain Adaptation techniques, we can reduce the Synthetic2Real Domain Shift, making closer the two domains and obtaining a performance improvement when testing the network over the real-world images. The code, the models, and the dataset are made freely available at https://ciampluca.github.io/viped/



### STAViS: Spatio-Temporal AudioVisual Saliency Network
- **Arxiv ID**: http://arxiv.org/abs/2001.03063v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.03063v2)
- **Published**: 2020-01-09 15:34:04+00:00
- **Updated**: 2020-06-14 18:45:08+00:00
- **Authors**: Antigoni Tsiami, Petros Koutras, Petros Maragos
- **Comment**: CVPR 2020. Project page: https://github.com/atsiami/STAViS
- **Journal**: None
- **Summary**: We introduce STAViS, a spatio-temporal audiovisual saliency network that combines spatio-temporal visual and auditory information in order to efficiently address the problem of saliency estimation in videos. Our approach employs a single network that combines visual saliency and auditory features and learns to appropriately localize sound sources and to fuse the two saliencies in order to obtain a final saliency map. The network has been designed, trained end-to-end, and evaluated on six different databases that contain audiovisual eye-tracking data of a large variety of videos. We compare our method against 8 different state-of-the-art visual saliency models. Evaluation results across databases indicate that our STAViS model outperforms our visual only variant as well as the other state-of-the-art models in the majority of cases. Also, the consistently good performance it achieves for all databases indicates that it is appropriate for estimating saliency "in-the-wild". The code is available at https://github.com/atsiami/STAViS.



### Investigating the Impact of Inclusion in Face Recognition Training Data on Individual Face Identification
- **Arxiv ID**: http://arxiv.org/abs/2001.03071v2
- **DOI**: None
- **Categories**: **cs.CY**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2001.03071v2)
- **Published**: 2020-01-09 15:50:28+00:00
- **Updated**: 2020-01-10 21:17:59+00:00
- **Authors**: Chris Dulhanty, Alexander Wong
- **Comment**: 2020 AAAI/ACM Conference on AI, Ethics, and Society (AIES 20) | V2
  updated latex character rendering issues
- **Journal**: None
- **Summary**: Modern face recognition systems leverage datasets containing images of hundreds of thousands of specific individuals' faces to train deep convolutional neural networks to learn an embedding space that maps an arbitrary individual's face to a vector representation of their identity. The performance of a face recognition system in face verification (1:1) and face identification (1:N) tasks is directly related to the ability of an embedding space to discriminate between identities. Recently, there has been significant public scrutiny into the source and privacy implications of large-scale face recognition training datasets such as MS-Celeb-1M and MegaFace, as many people are uncomfortable with their face being used to train dual-use technologies that can enable mass surveillance. However, the impact of an individual's inclusion in training data on a derived system's ability to recognize them has not previously been studied. In this work, we audit ArcFace, a state-of-the-art, open source face recognition system, in a large-scale face identification experiment with more than one million distractor images. We find a Rank-1 face identification accuracy of 79.71% for individuals present in the model's training data and an accuracy of 75.73% for those not present. This modest difference in accuracy demonstrates that face recognition systems using deep learning work better for individuals they are trained on, which has serious privacy implications when one considers all major open source face recognition training datasets do not obtain informed consent from individuals during their collection.



### Compression of descriptor models for mobile applications
- **Arxiv ID**: http://arxiv.org/abs/2001.03102v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.03102v3)
- **Published**: 2020-01-09 17:00:21+00:00
- **Updated**: 2021-02-05 10:41:09+00:00
- **Authors**: Roy Miles, Krystian Mikolajczyk
- **Comment**: ICASSP 2021
- **Journal**: None
- **Summary**: Deep neural networks have demonstrated state-of-the-art performance for feature-based image matching through the advent of new large and diverse datasets. However, there has been little work on evaluating the computational cost, model size, and matching accuracy tradeoffs for these models. This paper explicitly addresses these practical metrics by considering the state-of-the-art HardNet model. We observe a significant redundancy in the learned weights, which we exploit through the use of depthwise separable layers and an efficient Tucker decomposition. We demonstrate that a combination of these methods is very effective, but still sacrifices the top-end accuracy. To resolve this, we propose the Convolution-Depthwise-Pointwise(CDP) layer, which provides a means of interpolating between the standard and depthwise separable convolutions. With this proposed layer, we can achieve an 8 times reduction in the number of parameters on the HardNet model, 13 times reduction in the computational complexity, while sacrificing less than 1% on the overall accuracy across theHPatchesbenchmarks. To further demonstrate the generalisation of this approach, we apply it to the state-of-the-art SuperPoint model, where we can significantly reduce the number of parameters and floating-point operations, with minimal degradation in the matching accuracy.



### Don't Judge an Object by Its Context: Learning to Overcome Contextual Bias
- **Arxiv ID**: http://arxiv.org/abs/2001.03152v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2001.03152v2)
- **Published**: 2020-01-09 18:31:55+00:00
- **Updated**: 2020-05-05 23:20:53+00:00
- **Authors**: Krishna Kumar Singh, Dhruv Mahajan, Kristen Grauman, Yong Jae Lee, Matt Feiszli, Deepti Ghadiyaram
- **Comment**: CVPR 2020
- **Journal**: None
- **Summary**: Existing models often leverage co-occurrences between objects and their context to improve recognition accuracy. However, strongly relying on context risks a model's generalizability, especially when typical co-occurrence patterns are absent. This work focuses on addressing such contextual biases to improve the robustness of the learnt feature representations. Our goal is to accurately recognize a category in the absence of its context, without compromising on performance when it co-occurs with context. Our key idea is to decorrelate feature representations of a category from its co-occurring context. We achieve this by learning a feature subspace that explicitly represents categories occurring in the absence of context along side a joint feature subspace that represents both categories and context. Our very simple yet effective method is extensible to two multi-label tasks -- object and attribute classification. On 4 challenging datasets, we demonstrate the effectiveness of our method in reducing contextual bias.



### CrDoCo: Pixel-level Domain Transfer with Cross-Domain Consistency
- **Arxiv ID**: http://arxiv.org/abs/2001.03182v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.03182v1)
- **Published**: 2020-01-09 19:00:35+00:00
- **Updated**: 2020-01-09 19:00:35+00:00
- **Authors**: Yun-Chun Chen, Yen-Yu Lin, Ming-Hsuan Yang, Jia-Bin Huang
- **Comment**: Code: https://github.com/YunChunChen/CrDoCo-pytorch Project:
  https://yunchunchen.github.io/CrDoCo/
- **Journal**: None
- **Summary**: Unsupervised domain adaptation algorithms aim to transfer the knowledge learned from one domain to another (e.g., synthetic to real images). The adapted representations often do not capture pixel-level domain shifts that are crucial for dense prediction tasks (e.g., semantic segmentation). In this paper, we present a novel pixel-wise adversarial domain adaptation algorithm. By leveraging image-to-image translation methods for data augmentation, our key insight is that while the translated images between domains may differ in styles, their predictions for the task should be consistent. We exploit this property and introduce a cross-domain consistency loss that enforces our adapted model to produce consistent predictions. Through extensive experimental results, we show that our method compares favorably against the state-of-the-art on a wide variety of unsupervised domain adaptation tasks.



### Vertebra-Focused Landmark Detection for Scoliosis Assessment
- **Arxiv ID**: http://arxiv.org/abs/2001.03187v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2001.03187v1)
- **Published**: 2020-01-09 19:17:41+00:00
- **Updated**: 2020-01-09 19:17:41+00:00
- **Authors**: Jingru Yi, Pengxiang Wu, Qiaoying Huang, Hui Qu, Dimitris N. Metaxas
- **Comment**: Accepted to ISBI2020
- **Journal**: None
- **Summary**: Adolescent idiopathic scoliosis (AIS) is a lifetime disease that arises in children. Accurate estimation of Cobb angles of the scoliosis is essential for clinicians to make diagnosis and treatment decisions. The Cobb angles are measured according to the vertebrae landmarks. Existing regression-based methods for the vertebra landmark detection typically suffer from large dense mapping parameters and inaccurate landmark localization. The segmentation-based methods tend to predict connected or corrupted vertebra masks. In this paper, we propose a novel vertebra-focused landmark detection method. Our model first localizes the vertebra centers, based on which it then traces the four corner landmarks of the vertebra through the learned corner offset. In this way, our method is able to keep the order of the landmarks. The comparison results demonstrate the merits of our method in both Cobb angle measurement and landmark detection on low-contrast and ambiguous X-ray images. Code is available at: \url{https://github.com/yijingru/Vertebra-Landmark-Detection}.



### MatrixNets: A New Scale and Aspect Ratio Aware Architecture for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2001.03194v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.03194v1)
- **Published**: 2020-01-09 19:32:22+00:00
- **Updated**: 2020-01-09 19:32:22+00:00
- **Authors**: Abdullah Rashwan, Rishav Agarwal, Agastya Kalra, Pascal Poupart
- **Comment**: This is the full paper for arXiv:1908.04646 with more applications,
  experiments, and ablation study
- **Journal**: None
- **Summary**: We present MatrixNets (xNets), a new deep architecture for object detection. xNets map objects with similar sizes and aspect ratios into many specialized layers, allowing xNets to provide a scale and aspect ratio aware architecture. We leverage xNets to enhance single-stage object detection frameworks. First, we apply xNets on anchor-based object detection, for which we predict object centers and regress the top-left and bottom-right corners. Second, we use MatrixNets for corner-based object detection by predicting top-left and bottom-right corners. Each corner predicts the center location of the object. We also enhance corner-based detection by replacing the embedding layer with center regression. Our final architecture achieves mAP of 47.8 on MS COCO, which is higher than its CornerNet counterpart by +5.6 mAP while also closing the gap between single-stage and two-stage detectors. The code is available at https://github.com/arashwan/matrixnet.



### Camera-Based Adaptive Trajectory Guidance via Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2001.03205v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2001.03205v1)
- **Published**: 2020-01-09 20:05:25+00:00
- **Updated**: 2020-01-09 20:05:25+00:00
- **Authors**: Aditya Rajguru, Christopher Collander, William J. Beksi
- **Comment**: To be published in the 2020 6th International Conference on
  Mechatronics and Robotics Engineering
- **Journal**: None
- **Summary**: In this paper, we introduce a novel method to capture visual trajectories for navigating an indoor robot in dynamic settings using streaming image data. First, an image processing pipeline is proposed to accurately segment trajectories from noisy backgrounds. Next, the captured trajectories are used to design, train, and compare two neural network architectures for predicting acceleration and steering commands for a line following robot over a continuous space in real time. Lastly, experimental results demonstrate the performance of the neural networks versus human teleoperation of the robot and the viability of the system in environments with occlusions and/or low-light conditions.



### RSL-Net: Localising in Satellite Images From a Radar on the Ground
- **Arxiv ID**: http://arxiv.org/abs/2001.03233v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2001.03233v2)
- **Published**: 2020-01-09 21:53:24+00:00
- **Updated**: 2020-02-06 19:45:16+00:00
- **Authors**: Tim Y. Tang, Daniele De Martini, Dan Barnes, Paul Newman
- **Comment**: Accepted to IEEE Robotics and Automation Letters (RA-L)
- **Journal**: None
- **Summary**: This paper is about localising a vehicle in an overhead image using FMCW radar mounted on a ground vehicle. FMCW radar offers extraordinary promise and efficacy for vehicle localisation. It is impervious to all weather types and lighting conditions. However the complexity of the interactions between millimetre radar wave and the physical environment makes it a challenging domain. Infrastructure-free large-scale radar-based localisation is in its infancy. Typically here a map is built and suitable techniques, compatible with the nature of sensor, are brought to bear. In this work we eschew the need for a radar-based map; instead we simply use an overhead image -- a resource readily available everywhere. This paper introduces a method that not only naturally deals with the complexity of the signal type but does so in the context of cross modal processing.



### Adaptive Control of Embedding Strength in Image Watermarking using Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2001.03251v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2001.03251v1)
- **Published**: 2020-01-09 23:08:34+00:00
- **Updated**: 2020-01-09 23:08:34+00:00
- **Authors**: Mahnoosh Bagheri, Majid Mohrekesh, Nader Karimi, Shadrokh Samavi
- **Comment**: 4 pages 5 figures
- **Journal**: None
- **Summary**: Digital image watermarking has been widely used in different applications such as copyright protection of digital media, such as audio, image, and video files. Two opposing criteria of robustness and transparency are the goals of watermarking methods. In this paper, we propose a framework for determining the appropriate embedding strength factor. The framework can use most DWT and DCT based blind watermarking approaches. We use Mask R-CNN on the COCO dataset to find a good strength factor for each sub-block. Experiments show that this method is robust against different attacks and has good transparency.



### A Deep Neural Networks Approach for Pixel-Level Runway Pavement Crack Segmentation Using Drone-Captured Images
- **Arxiv ID**: http://arxiv.org/abs/2001.03257v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2001.03257v1)
- **Published**: 2020-01-09 23:30:50+00:00
- **Updated**: 2020-01-09 23:30:50+00:00
- **Authors**: Liming Jiang, Yuanchang Xie, Tianzhu Ren
- **Comment**: 13 pages, 5 figures
- **Journal**: None
- **Summary**: Pavement conditions are a critical aspect of asset management and directly affect safety. This study introduces a deep neural network method called U-Net for pavement crack segmentation based on drone-captured images to reduce the cost and time needed for airport runway inspection. The proposed approach can also be used for highway pavement conditions assessment during off-peak periods when there are few vehicles on the road. In this study, runway pavement images are collected using drone at various heights from the Fitchburg Municipal Airport (FMA) in Massachusetts to evaluate their quality and applicability for crack segmentation, from which an optimal height is determined. Drone images captured at the optimal height are then used to evaluate the crack segmentation performance of the U-Net model. Deep learning methods typically require a huge set of annotated training datasets for model development, which can be a major obstacle for their applications. An online annotated pavement image dataset is used together with the FMA data to train the U-Net model. The results show that U-Net performs well on the FMA testing data even with limited FMA training images, suggesting that it has good generalization ability and great potential to be used for both airport runways and highway pavements.



### Deep Learning Enabled Uncorrelated Space Observation Association
- **Arxiv ID**: http://arxiv.org/abs/2001.05855v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2001.05855v1)
- **Published**: 2020-01-09 23:33:11+00:00
- **Updated**: 2020-01-09 23:33:11+00:00
- **Authors**: Jacob J Decoto, David RC Dayton
- **Comment**: Approved for public release by Department of Defense Prepublication
  Office, Ref: 20-S-0428
- **Journal**: None
- **Summary**: Uncorrelated optical space observation association represents a classic needle in a haystack problem. The objective being to find small groups of observations that are likely of the same resident space objects (RSOs) from amongst the much larger population of all uncorrelated observations. These observations being potentially widely disparate both temporally and with respect to the observing sensor position. By training on a large representative data set this paper shows that a deep learning enabled learned model with no encoded knowledge of physics or orbital mechanics can learn a model for identifying observations of common objects. When presented with balanced input sets of 50% matching observation pairs the learned model was able to correctly identify if the observation pairs were of the same RSO 83.1% of the time. The resulting learned model is then used in conjunction with a search algorithm on an unbalanced demonstration set of 1,000 disparate simulated uncorrelated observations and is shown to be able to successfully identify true three observation sets representing 111 out of 142 objects in the population. With most objects being identified in multiple three observation triplets. This is accomplished while only exploring 0.06% of the search space of 1.66e8 possible unique triplet combinations.



### FASTER: Fast and Safe Trajectory Planner for Navigation in Unknown Environments
- **Arxiv ID**: http://arxiv.org/abs/2001.04420v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2001.04420v2)
- **Published**: 2020-01-09 23:50:58+00:00
- **Updated**: 2021-08-30 18:21:30+00:00
- **Authors**: Jesus Tordesillas, Brett T. Lopez, Michael Everett, Jonathan P. How
- **Comment**: This paper has been accepted for publication in IEEE Transactions on
  Robotics. arXiv admin note: text overlap with arXiv:1903.03558
- **Journal**: None
- **Summary**: Planning high-speed trajectories for UAVs in unknown environments requires algorithmic techniques that enable fast reaction times to guarantee safety as more information about the environment becomes available. The standard approaches that ensure safety by enforcing a "stop" condition in the free-known space can severely limit the speed of the vehicle, especially in situations where much of the world is unknown. Moreover, the ad-hoc time and interval allocation scheme usually imposed on the trajectory also leads to conservative and slower trajectories. This work proposes FASTER (Fast and Safe Trajectory Planner) to ensure safety without sacrificing speed. FASTER obtains high-speed trajectories by enabling the local planner to optimize in both the free-known and unknown spaces. Safety is ensured by always having a safe back-up trajectory in the free-known space. The MIQP formulation proposed also allows the solver to choose the trajectory interval allocation. FASTER is tested extensively in simulation and in real hardware, showing flights in unknown cluttered environments with velocities up to 7.8m/s, and experiments at the maximum speed of a skid-steer ground robot (2m/s).



