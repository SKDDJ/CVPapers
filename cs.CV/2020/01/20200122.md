# Arxiv Papers in cs.CV on 2020-01-22
### LRF-Net: Learning Local Reference Frames for 3D Local Shape Description and Matching
- **Arxiv ID**: http://arxiv.org/abs/2001.07832v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2001.07832v2)
- **Published**: 2020-01-22 00:52:48+00:00
- **Updated**: 2020-05-03 00:13:41+00:00
- **Authors**: Angfan Zhu, Jiaqi Yang, Weiyue Zhao, Zhiguo Cao
- **Comment**: 28 pages, 14 figures
- **Journal**: None
- **Summary**: The local reference frame (LRF) acts as a critical role in 3D local shape description and matching. However, most of existing LRFs are hand-crafted and suffer from limited repeatability and robustness. This paper presents the first attempt to learn an LRF via a Siamese network that needs weak supervision only. In particular, we argue that each neighboring point in the local surface gives a unique contribution to LRF construction and measure such contributions via learned weights. Extensive analysis and comparative experiments on three public datasets addressing different application scenarios have demonstrated that LRF-Net is more repeatable and robust than several state-of-the-art LRF methods (LRF-Net is only trained on one dataset). In addition, LRF-Net can significantly boost the local shape description and 6-DoF pose estimation performance when matching 3D point clouds.



### A versatile anomaly detection method for medical images with a flow-based generative model in semi-supervision setting
- **Arxiv ID**: http://arxiv.org/abs/2001.07847v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2001.07847v3)
- **Published**: 2020-01-22 02:01:57+00:00
- **Updated**: 2020-10-20 07:09:03+00:00
- **Authors**: H. Shibata, S. Hanaoka, Y. Nomura, T. Nakao, I. Sato, D. Sato, N. Hayashi, O. Abe
- **Comment**: None
- **Journal**: None
- **Summary**: Oversight in medical images is a crucial problem, and timely reporting of medical images is desired. Therefore, an all-purpose anomaly detection method that can detect virtually all types of lesions/diseases in a given image is strongly desired. However, few commercially available and versatile anomaly detection methods for medical images have been provided so far. Recently, anomaly detection methods built upon deep learning methods have been rapidly growing in popularity, and these methods seem to provide reasonable solutions to the problem. However, the workload to label the images necessary for training in deep learning remains heavy. In this study, we present an anomaly detection method based on two trained flow-based generative models. With this method, the posterior probability can be computed as a normality metric for any given image. The training of the generative models requires two sets of images: a set containing only normal images and another set containing both normal and abnormal images without any labels. In the latter set, each sample does not have to be labeled as normal or abnormal; therefore, any mixture of images (e.g., all cases in a hospital) can be used as the dataset without cumbersome manual labeling. The method was validated with two types of medical images: chest X-ray radiographs (CXRs) and brain computed tomographies (BCTs). The areas under the receiver operating characteristic curves for logarithm posterior probabilities of CXRs (0.868 for pneumonia-like opacities) and BCTs (0.904 for infarction) were comparable to those in previous studies with other anomaly detection methods. This result showed the versatility of our method.



### M^2 Deep-ID: A Novel Model for Multi-View Face Identification Using Convolutional Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2001.07871v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.07871v1)
- **Published**: 2020-01-22 04:13:18+00:00
- **Updated**: 2020-01-22 04:13:18+00:00
- **Authors**: Sara Shahsavarani, Morteza Analoui, Reza Shoja Ghiass
- **Comment**: None
- **Journal**: None
- **Summary**: Despite significant advances in Deep Face Recognition (DFR) systems, introducing new DFRs under specific constraints such as varying pose still remains a big challenge. Most particularly, due to the 3D nature of a human head, facial appearance of the same subject introduces a high intra-class variability when projected to the camera image plane. In this paper, we propose a new multi-view Deep Face Recognition (MVDFR) system to address the mentioned challenge. In this context, multiple 2D images of each subject under different views are fed into the proposed deep neural network with a unique design to re-express the facial features in a single and more compact face descriptor, which in turn, produces a more informative and abstract way for face identification using convolutional neural networks. To extend the functionality of our proposed system to multi-view facial images, the golden standard Deep-ID model is modified in our proposed model. The experimental results indicate that our proposed method yields a 99.8% accuracy, while the state-of-the-art method achieves a 97% accuracy. We also gathered the Iran University of Science and Technology (IUST) face database with 6552 images of 504 subjects to accomplish our experiments.



### Curvature Regularized Surface Reconstruction from Point Cloud
- **Arxiv ID**: http://arxiv.org/abs/2001.07884v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, 65D18
- **Links**: [PDF](http://arxiv.org/pdf/2001.07884v2)
- **Published**: 2020-01-22 05:34:40+00:00
- **Updated**: 2020-09-10 03:22:10+00:00
- **Authors**: Yuchen He, Sung Ha Kang, Hao Liu
- **Comment**: 22 pages, 15 figures
- **Journal**: None
- **Summary**: We propose a variational functional and fast algorithms to reconstruct implicit surface from point cloud data with a curvature constraint. The minimizing functional balances the distance function from the point cloud and the mean curvature term. Only the point location is used, without any local normal or curvature estimation at each point. With the added curvature constraint, the computation becomes particularly challenging. To enhance the computational efficiency, we solve the problem by a novel operator splitting scheme. It replaces the original high-order PDEs by a decoupled PDE system, which is solved by a semi-implicit method. We also discuss approach using an augmented Lagrangian method. The proposed method shows robustness against noise, and recovers concave features and sharp corners better compared to models without curvature constraint. Numerical experiments in two and three dimensional data sets, noisy and sparse data are presented to validate the model.



### Partially-Shared Variational Auto-encoders for Unsupervised Domain Adaptation with Target Shift
- **Arxiv ID**: http://arxiv.org/abs/2001.07895v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.07895v3)
- **Published**: 2020-01-22 06:41:31+00:00
- **Updated**: 2020-01-25 05:41:25+00:00
- **Authors**: Ryuhei Takahashi, Atsushi Hashimoto, Motoharu Sonogashira, Masaaki Iiyama
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a novel approach for unsupervised domain adaptation (UDA) with target shift. Target shift is a problem of mismatch in label distribution between source and target domains. Typically it appears as class-imbalance in target domain. In practice, this is an important problem in UDA; as we do not know labels in target domain datasets, we do not know whether or not its distribution is identical to that in the source domain dataset. Many traditional approaches achieve UDA with distribution matching by minimizing mean maximum discrepancy or adversarial training; however these approaches implicitly assume a coincidence in the distributions and do not work under situations with target shift. Some recent UDA approaches focus on class boundary and some of them are robust to target shift, but they are only applicable to classification and not to regression.   To overcome the target shift problem in UDA, the proposed method, partially shared variational autoencoders (PS-VAEs), uses pair-wise feature alignment instead of feature distribution matching. PS-VAEs inter-convert domain of each sample by a CycleGAN-based architecture while preserving its label-related content. To evaluate the performance of PS-VAEs, we carried out two experiments: UDA with class-unbalanced digits datasets (classification), and UDA from synthesized data to real observation in human-pose-estimation (regression). The proposed method presented its robustness against the class-imbalance in the classification task, and outperformed the other methods in the regression task with a large margin.



### Dynamic multi-object Gaussian process models: A framework for data-driven functional modelling of human joints
- **Arxiv ID**: http://arxiv.org/abs/2001.07904v1
- **DOI**: 10.1007/978-3-030-59719-1_73
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.07904v1)
- **Published**: 2020-01-22 07:57:36+00:00
- **Updated**: 2020-01-22 07:57:36+00:00
- **Authors**: Jean-Rassaire Fouefack, Bhushan Borotikar, Tania S. Douglas, Val√©rie Burdin, Tinashe E. M. Mutsvangwa
- **Comment**: 15 pages, 14 figures
- **Journal**: None
- **Summary**: Statistical shape models (SSMs) are state-of-the-art medical image analysis tools for extracting and explaining features across a set of biological structures. However, a principled and robust way to combine shape and pose features has been illusive due to three main issues: 1) Non-homogeneity of the data (data with linear and non-linear natural variation across features), 2) non-optimal representation of the $3D$ motion (rigid transformation representations that are not proportional to the kinetic energy that move an object from one position to the other), and 3) artificial discretization of the models. In this paper, we propose a new framework for dynamic multi-object statistical modelling framework for the analysis of human joints in a continuous domain. Specifically, we propose to normalise shape and dynamic spatial features in the same linearized statistical space permitting the use of linear statistics; we adopt an optimal 3D motion representation for more accurate rigid transformation comparisons; and we provide a 3D shape and pose prediction protocol using a Markov chain Monte Carlo sampling-based fitting. The framework affords an efficient generative dynamic multi-object modelling platform for biological joints. We validate the framework using a controlled synthetic data. Finally, the framework is applied to an analysis of the human shoulder joint to compare its performance with standard SSM approaches in prediction of shape while adding the advantage of determining relative pose between bones in a complex. Excellent validity is observed and the shoulder joint shape-pose prediction results suggest that the novel framework may have utility for a range of medical image analysis applications. Furthermore, the framework is generic and can be extended to n$>$2 objects, making it suitable for clinical and diagnostic methods for the management of joint disorders.



### AutoFCL: Automatically Tuning Fully Connected Layers for Handling Small Dataset
- **Arxiv ID**: http://arxiv.org/abs/2001.11951v4
- **DOI**: 10.1007/s00521-020-05549-4
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2001.11951v4)
- **Published**: 2020-01-22 08:39:00+00:00
- **Updated**: 2021-01-28 17:05:06+00:00
- **Authors**: S. H. Shabbeer Basha, Sravan Kumar Vinakota, Shiv Ram Dubey, Viswanath Pulabaigari, Snehasis Mukherjee
- **Comment**: This paper is published in Neural Computing & Applications Journal
- **Journal**: None
- **Summary**: Deep Convolutional Neural Networks (CNN) have evolved as popular machine learning models for image classification during the past few years, due to their ability to learn the problem-specific features directly from the input images. The success of deep learning models solicits architecture engineering rather than hand-engineering the features. However, designing state-of-the-art CNN for a given task remains a non-trivial and challenging task, especially when training data size is less. To address this phenomena, transfer learning has been used as a popularly adopted technique. While transferring the learned knowledge from one task to another, fine-tuning with the target-dependent Fully Connected (FC) layers generally produces better results over the target task. In this paper, the proposed AutoFCL model attempts to learn the structure of FC layers of a CNN automatically using Bayesian optimization. To evaluate the performance of the proposed AutoFCL, we utilize five pre-trained CNN models such as VGG-16, ResNet, DenseNet, MobileNet, and NASNetMobile. The experiments are conducted on three benchmark datasets, namely CalTech-101, Oxford-102 Flowers, and UC Merced Land Use datasets. Fine-tuning the newly learned (target-dependent) FC layers leads to state-of-the-art performance, according to the experiments carried out in this research. The proposed AutoFCL method outperforms the existing methods over CalTech-101 and Oxford-102 Flowers datasets by achieving the accuracy of 94.38% and 98.89%, respectively. However, our method achieves comparable performance on the UC Merced Land Use dataset with 96.83% accuracy. The source codes of this research are available at https://github.com/shabbeersh/AutoFCL.



### Active Perception with A Monocular Camera for Multiscopic Vision
- **Arxiv ID**: http://arxiv.org/abs/2001.08212v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.08212v1)
- **Published**: 2020-01-22 08:46:45+00:00
- **Updated**: 2020-01-22 08:46:45+00:00
- **Authors**: Weihao Yuan, Rui Fan, Michael Yu Wang, Qifeng Chen
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: We design a multiscopic vision system that utilizes a low-cost monocular RGB camera to acquire accurate depth estimation for robotic applications. Unlike multi-view stereo with images captured at unconstrained camera poses, the proposed system actively controls a robot arm with a mounted camera to capture a sequence of images in horizontally or vertically aligned positions with the same parallax. In this system, we combine the cost volumes for stereo matching between the reference image and the surrounding images to form a fused cost volume that is robust to outliers. Experiments on the Middlebury dataset and real robot experiments show that our obtained disparity maps are more accurate than two-frame stereo matching: the average absolute error is reduced by 50.2% in our experiments.



### Optimized Generic Feature Learning for Few-shot Classification across Domains
- **Arxiv ID**: http://arxiv.org/abs/2001.07926v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.07926v1)
- **Published**: 2020-01-22 09:31:39+00:00
- **Updated**: 2020-01-22 09:31:39+00:00
- **Authors**: Tonmoy Saikia, Thomas Brox, Cordelia Schmid
- **Comment**: None
- **Journal**: None
- **Summary**: To learn models or features that generalize across tasks and domains is one of the grand goals of machine learning. In this paper, we propose to use cross-domain, cross-task data as validation objective for hyper-parameter optimization (HPO) to improve on this goal. Given a rich enough search space, optimization of hyper-parameters learn features that maximize validation performance and, due to the objective, generalize across tasks and domains. We demonstrate the effectiveness of this strategy on few-shot image classification within and across domains. The learned features outperform all previous few-shot and meta-learning approaches.



### A Fixation-based 360¬∞ Benchmark Dataset for Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2001.07960v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.07960v2)
- **Published**: 2020-01-22 11:16:39+00:00
- **Updated**: 2020-05-19 15:26:21+00:00
- **Authors**: Yi Zhang, Lu Zhang, Wassim Hamidouche, Olivier Deforges
- **Comment**: 5 pages, 5 figures, accepted by ICIP2020
- **Journal**: None
- **Summary**: Fixation prediction (FP) in panoramic contents has been widely investigated along with the booming trend of virtual reality (VR) applications. However, another issue within the field of visual saliency, salient object detection (SOD), has been seldom explored in 360{\deg} (or omnidirectional) images due to the lack of datasets representative of real scenes with pixel-level annotations. Toward this end, we collect 107 equirectangular panoramas with challenging scenes and multiple object classes. Based on the consistency between FP and explicit saliency judgements, we further manually annotate 1,165 salient objects over the collected images with precise masks under the guidance of real human eye fixation maps. Six state-of-the-art SOD models are then benchmarked on the proposed fixation-based 360{\deg} image dataset (F-360iSOD), by applying a multiple cubic projection-based fine-tuning method. Experimental results show a limitation of the current methods when used for SOD in panoramic images, which indicates the proposed dataset is challenging. Key issues for 360{\deg} SOD is also discussed. The proposed dataset is available at https://github.com/PanoAsh/F-360iSOD.



### ImageBERT: Cross-modal Pre-training with Large-scale Weak-supervised Image-Text Data
- **Arxiv ID**: http://arxiv.org/abs/2001.07966v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.07966v2)
- **Published**: 2020-01-22 11:35:58+00:00
- **Updated**: 2020-01-23 08:03:27+00:00
- **Authors**: Di Qi, Lin Su, Jia Song, Edward Cui, Taroon Bharti, Arun Sacheti
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we introduce a new vision-language pre-trained model -- ImageBERT -- for image-text joint embedding. Our model is a Transformer-based model, which takes different modalities as input and models the relationship between them. The model is pre-trained on four tasks simultaneously: Masked Language Modeling (MLM), Masked Object Classification (MOC), Masked Region Feature Regression (MRFR), and Image Text Matching (ITM). To further enhance the pre-training quality, we have collected a Large-scale weAk-supervised Image-Text (LAIT) dataset from Web. We first pre-train the model on this dataset, then conduct a second stage pre-training on Conceptual Captions and SBU Captions. Our experiments show that multi-stage pre-training strategy outperforms single-stage pre-training. We also fine-tune and evaluate our pre-trained ImageBERT model on image retrieval and text retrieval tasks, and achieve new state-of-the-art results on both MSCOCO and Flickr30k datasets.



### Safety Concerns and Mitigation Approaches Regarding the Use of Deep Learning in Safety-Critical Perception Tasks
- **Arxiv ID**: http://arxiv.org/abs/2001.08001v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2001.08001v1)
- **Published**: 2020-01-22 13:22:59+00:00
- **Updated**: 2020-01-22 13:22:59+00:00
- **Authors**: Oliver Willers, Sebastian Sudholt, Shervin Raafatnia, Stephanie Abrecht
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning methods are widely regarded as indispensable when it comes to designing perception pipelines for autonomous agents such as robots, drones or automated vehicles. The main reasons, however, for deep learning not being used for autonomous agents at large scale already are safety concerns. Deep learning approaches typically exhibit a black-box behavior which makes it hard for them to be evaluated with respect to safety-critical aspects. While there have been some work on safety in deep learning, most papers typically focus on high-level safety concerns. In this work, we seek to dive into the safety concerns of deep learning methods and present a concise enumeration on a deeply technical level. Additionally, we present extensive discussions on possible mitigation methods and give an outlook regarding what mitigation methods are still missing in order to facilitate an argumentation for the safety of a deep learning method.



### ResDepth: Learned Residual Stereo Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2001.08026v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.08026v3)
- **Published**: 2020-01-22 14:12:43+00:00
- **Updated**: 2021-06-18 16:06:20+00:00
- **Authors**: Corinne Stucker, Konrad Schindler
- **Comment**: updated supplementary material
- **Journal**: Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition (CVPR) Workshops, 2020, pp. 707-716
- **Summary**: We propose an embarrassingly simple but very effective scheme for high-quality dense stereo reconstruction: (i) generate an approximate reconstruction with your favourite stereo matcher; (ii) rewarp the input images with that approximate model; (iii) with the initial reconstruction and the warped images as input, train a deep network to enhance the reconstruction by regressing a residual correction; and (iv) if desired, iterate the refinement with the new, improved reconstruction. The strategy to only learn the residual greatly simplifies the learning problem. A standard Unet without bells and whistles is enough to reconstruct even small surface details, like dormers and roof substructures in satellite images. We also investigate residual reconstruction with less information and find that even a single image is enough to greatly improve an approximate reconstruction. Our full model reduces the mean absolute error of state-of-the-art stereo reconstruction systems by >50%, both in our target domain of satellite stereo and on stereo pairs from the ETH3D benchmark.



### ManyModalQA: Modality Disambiguation and QA over Diverse Inputs
- **Arxiv ID**: http://arxiv.org/abs/2001.08034v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2001.08034v1)
- **Published**: 2020-01-22 14:39:28+00:00
- **Updated**: 2020-01-22 14:39:28+00:00
- **Authors**: Darryl Hannan, Akshay Jain, Mohit Bansal
- **Comment**: AAAI 2020 (10 pages)
- **Journal**: None
- **Summary**: We present a new multimodal question answering challenge, ManyModalQA, in which an agent must answer a question by considering three distinct modalities: text, images, and tables. We collect our data by scraping Wikipedia and then utilize crowdsourcing to collect question-answer pairs. Our questions are ambiguous, in that the modality that contains the answer is not easily determined based solely upon the question. To demonstrate this ambiguity, we construct a modality selector (or disambiguator) network, and this model gets substantially lower accuracy on our challenge set, compared to existing datasets, indicating that our questions are more ambiguous. By analyzing this model, we investigate which words in the question are indicative of the modality. Next, we construct a simple baseline ManyModalQA model, which, based on the prediction from the modality selector, fires a corresponding pre-trained state-of-the-art unimodal QA model. We focus on providing the community with a new manymodal evaluation set and only provide a fine-tuning set, with the expectation that existing datasets and approaches will be transferred for most of the training, to encourage low-resource generalization without large, monolithic training sets for each new task. There is a significant gap between our baseline models and human performance; therefore, we hope that this challenge encourages research in end-to-end modality disambiguation and multimodal QA models, as well as transfer learning. Code and data available at: https://github.com/hannandarryl/ManyModalQA



### Attention! A Lightweight 2D Hand Pose Estimation Approach
- **Arxiv ID**: http://arxiv.org/abs/2001.08047v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2001.08047v2)
- **Published**: 2020-01-22 15:05:00+00:00
- **Updated**: 2020-05-31 01:24:50+00:00
- **Authors**: Nicholas Santavas, Ioannis Kansizoglou, Loukas Bampis, Evangelos Karakasis, Antonios Gasteratos
- **Comment**: updated version with ablation studies
- **Journal**: None
- **Summary**: Vision based human pose estimation is an non-invasive technology for Human-Computer Interaction (HCI). Direct use of the hand as an input device provides an attractive interaction method, with no need for specialized sensing equipment, such as exoskeletons, gloves etc, but a camera. Traditionally, HCI is employed in various applications spreading in areas including manufacturing, surgery, entertainment industry and architecture, to mention a few. Deployment of vision based human pose estimation algorithms can give a breath of innovation to these applications. In this letter, we present a novel Convolutional Neural Network architecture, reinforced with a Self-Attention module that it can be deployed on an embedded system, due to its lightweight nature, with just 1.9 Million parameters. The source code and qualitative results are publicly available.



### Depthwise Non-local Module for Fast Salient Object Detection Using a Single Thread
- **Arxiv ID**: http://arxiv.org/abs/2001.08057v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.08057v1)
- **Published**: 2020-01-22 15:23:48+00:00
- **Updated**: 2020-01-22 15:23:48+00:00
- **Authors**: Haofeng Li, Guanbin Li, Binbin Yang, Guanqi Chen, Liang Lin, Yizhou Yu
- **Comment**: Accepted as a regular paper in the IEEE Transactions on Cybernetics
- **Journal**: None
- **Summary**: Recently deep convolutional neural networks have achieved significant success in salient object detection. However, existing state-of-the-art methods require high-end GPUs to achieve real-time performance, which makes them hard to adapt to low-cost or portable devices. Although generic network architectures have been proposed to speed up inference on mobile devices, they are tailored to the task of image classification or semantic segmentation, and struggle to capture intra-channel and inter-channel correlations that are essential for contrast modeling in salient object detection. Motivated by the above observations, we design a new deep learning algorithm for fast salient object detection. The proposed algorithm for the first time achieves competitive accuracy and high inference efficiency simultaneously with a single CPU thread. Specifically, we propose a novel depthwise non-local moudule (DNL), which implicitly models contrast via harvesting intra-channel and inter-channel correlations in a self-attention manner. In addition, we introduce a depthwise non-local network architecture that incorporates both depthwise non-local modules and inverted residual blocks. Experimental results show that our proposed network attains very competitive accuracy on a wide range of salient object detection datasets while achieving state-of-the-art efficiency among all existing deep learning based algorithms.



### UniPose: Unified Human Pose Estimation in Single Images and Videos
- **Arxiv ID**: http://arxiv.org/abs/2001.08095v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.08095v1)
- **Published**: 2020-01-22 15:59:42+00:00
- **Updated**: 2020-01-22 15:59:42+00:00
- **Authors**: Bruno Artacho, Andreas Savakis
- **Comment**: None
- **Journal**: None
- **Summary**: We propose UniPose, a unified framework for human pose estimation, based on our "Waterfall" Atrous Spatial Pooling architecture, that achieves state-of-art-results on several pose estimation metrics. Current pose estimation methods utilizing standard CNN architectures heavily rely on statistical postprocessing or predefined anchor poses for joint localization. UniPose incorporates contextual segmentation and joint localization to estimate the human pose in a single stage, with high accuracy, without relying on statistical postprocessing methods. The Waterfall module in UniPose leverages the efficiency of progressive filtering in the cascade architecture, while maintaining multi-scale fields-of-view comparable to spatial pyramid configurations. Additionally, our method is extended to UniPose-LSTM for multi-frame processing and achieves state-of-the-art results for temporal pose estimation in Video. Our results on multiple datasets demonstrate that UniPose, with a ResNet backbone and Waterfall module, is a robust and efficient architecture for pose estimation obtaining state-of-the-art results in single person pose detection for both single images and videos.



### Learning to Correct 3D Reconstructions from Multiple Views
- **Arxiv ID**: http://arxiv.org/abs/2001.08098v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2001.08098v1)
- **Published**: 2020-01-22 16:02:23+00:00
- **Updated**: 2020-01-22 16:02:23+00:00
- **Authors**: ≈ûtefan SƒÉftescu, Paul Newman
- **Comment**: None
- **Journal**: None
- **Summary**: This paper is about reducing the cost of building good large-scale 3D reconstructions post-hoc. We render 2D views of an existing reconstruction and train a convolutional neural network (CNN) that refines inverse-depth to match a higher-quality reconstruction. Since the views that we correct are rendered from the same reconstruction, they share the same geometry, so overlapping views complement each other. We take advantage of that in two ways. Firstly, we impose a loss during training which guides predictions on neighbouring views to have the same geometry and has been shown to improve performance. Secondly, in contrast to previous work, which corrects each view independently, we also make predictions on sets of neighbouring views jointly. This is achieved by warping feature maps between views and thus bypassing memory-intensive 3D computation. We make the observation that features in the feature maps are viewpoint-dependent, and propose a method for transforming features with dynamic filters generated by a multi-layer perceptron from the relative poses between views. In our experiments we show that this last step is necessary for successfully fusing feature maps between views.



### Are Accelerometers for Activity Recognition a Dead-end?
- **Arxiv ID**: http://arxiv.org/abs/2001.08111v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.08111v2)
- **Published**: 2020-01-22 16:12:54+00:00
- **Updated**: 2020-01-30 10:51:07+00:00
- **Authors**: Catherine Tong, Shyam A. Tailor, Nicholas D. Lane
- **Comment**: None
- **Journal**: None
- **Summary**: Accelerometer-based (and by extension other inertial sensors) research for Human Activity Recognition (HAR) is a dead-end. This sensor does not offer enough information for us to progress in the core domain of HAR - to recognize everyday activities from sensor data. Despite continued and prolonged efforts in improving feature engineering and machine learning models, the activities that we can recognize reliably have only expanded slightly and many of the same flaws of early models are still present today. Instead of relying on acceleration data, we should instead consider modalities with much richer information - a logical choice are images. With the rapid advance in image sensing hardware and modelling techniques, we believe that a widespread adoption of image sensors will open many opportunities for accurate and robust inference across a wide spectrum of human activities.   In this paper, we make the case for imagers in place of accelerometers as the default sensor for human activity recognition. Our review of past works has led to the observation that progress in HAR had stalled, caused by our reliance on accelerometers. We further argue for the suitability of images for activity recognition by illustrating their richness of information and the marked progress in computer vision. Through a feasibility analysis, we find that deploying imagers and CNNs on device poses no substantial burden on modern mobile hardware. Overall, our work highlights the need to move away from accelerometers and calls for further exploration of using imagers for activity recognition.



### Optimizing Generative Adversarial Networks for Image Super Resolution via Latent Space Regularization
- **Arxiv ID**: http://arxiv.org/abs/2001.08126v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2001.08126v2)
- **Published**: 2020-01-22 16:27:20+00:00
- **Updated**: 2021-01-09 04:40:13+00:00
- **Authors**: Sheng Zhong, Shifu Zhou
- **Comment**: 11 pages, 5 figures
- **Journal**: None
- **Summary**: Natural images can be regarded as residing in a manifold that is embedded in a higher dimensional Euclidean space. Generative Adversarial Networks (GANs) try to learn the distribution of the real images in the manifold to generate samples that look real. But the results of existing methods still exhibit many unpleasant artifacts and distortions even for the cases where the desired ground truth target images are available for supervised learning such as in single image super resolution (SISR). We probe for ways to alleviate these problems for supervised GANs in this paper. We explicitly apply the Lipschitz Continuity Condition (LCC) to regularize the GAN. An encoding network that maps the image space to a new optimal latent space is derived from the LCC, and it is used to augment the GAN as a coupling component. The LCC is also converted to new regularization terms in the generator loss function to enforce local invariance. The GAN is optimized together with the encoding network in an attempt to make the generator converge to a more ideal and disentangled mapping that can generate samples more faithful to the target images. When the proposed models are applied to the single image super resolution problem, the results outperform the state of the art.



### Zero-Shot Activity Recognition with Videos
- **Arxiv ID**: http://arxiv.org/abs/2002.02265v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.02265v1)
- **Published**: 2020-01-22 16:33:10+00:00
- **Updated**: 2020-01-22 16:33:10+00:00
- **Authors**: Evin Pinar Ornek
- **Comment**: This is a research report done during master's studies
- **Journal**: None
- **Summary**: In this paper, we examined the zero-shot activity recognition task with the usage of videos. We introduce an auto-encoder based model to construct a multimodal joint embedding space between the visual and textual manifolds. On the visual side, we used activity videos and a state-of-the-art 3D convolutional action recognition network to extract the features. On the textual side, we worked with GloVe word embeddings. The zero-shot recognition results are evaluated by top-n accuracy. Then, the manifold learning ability is measured by mean Nearest Neighbor Overlap. In the end, we provide an extensive discussion over the results and the future directions.



### Pruning CNN's with linear filter ensembles
- **Arxiv ID**: http://arxiv.org/abs/2001.08142v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2001.08142v2)
- **Published**: 2020-01-22 16:52:06+00:00
- **Updated**: 2020-03-03 09:25:32+00:00
- **Authors**: Csan√°d S√°ndor, Szabolcs P√°vel, Lehel Csat√≥
- **Comment**: accepted to ECAI2020
- **Journal**: None
- **Summary**: Despite the promising results of convolutional neural networks (CNNs), their application on devices with limited resources is still a big challenge; this is mainly due to the huge memory and computation requirements of the CNN. To counter the limitation imposed by the network size, we use pruning to reduce the network size and -- implicitly -- the number of floating point operations (FLOPs). Contrary to the filter norm method -- used in ``conventional`` network pruning -- based on the assumption that a smaller norm implies ``less importance'' to its associated component, we develop a novel filter importance norm that is based on the change in the empirical loss caused by the presence or removal of a component from the network architecture.   Since there are too many individual possibilities for filter configuration, we repeatedly sample from these architectural components and measure the system performance in the respective state of components being active or disabled. The result is a collection of filter ensembles -- filter masks -- and associated performance values. We rank the filters based on a linear and additive model and remove the least important ones such that the drop in network accuracy is minimal. We evaluate our method on a fully connected network, as well as on the ResNet architecture trained on the CIFAR-10 dataset. Using our pruning method, we managed to remove $60\%$ of the parameters and $64\%$ of the FLOPs from the ResNet with an accuracy drop of less than $0.6\%$.



### Toward a Controllable Disentanglement Network
- **Arxiv ID**: http://arxiv.org/abs/2001.08572v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2001.08572v3)
- **Published**: 2020-01-22 16:54:07+00:00
- **Updated**: 2020-06-20 04:02:22+00:00
- **Authors**: Zengjie Song, Oluwasanmi Koyejo, Jiangshe Zhang
- **Comment**: Improved version of arXiv:1912.11675
- **Journal**: None
- **Summary**: This paper addresses two crucial problems of learning disentangled image representations, namely controlling the degree of disentanglement during image editing, and balancing the disentanglement strength and the reconstruction quality. To encourage disentanglement, we devise a distance covariance based decorrelation regularization. Further, for the reconstruction step, our model leverages a soft target representation combined with the latent image code. By exploring the real-valued space of the soft target representation, we are able to synthesize novel images with the designated properties. To improve the perceptual quality of images generated by autoencoder (AE)-based models, we extend the encoder-decoder architecture with the generative adversarial network (GAN) by collapsing the AE decoder and the GAN generator into one. We also design a classification based protocol to quantitatively evaluate the disentanglement strength of our model. Experimental results showcase the benefits of the proposed model.



### Causality based Feature Fusion for Brain Neuro-Developmental Analysis
- **Arxiv ID**: http://arxiv.org/abs/2001.08173v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/2001.08173v1)
- **Published**: 2020-01-22 17:38:42+00:00
- **Updated**: 2020-01-22 17:38:42+00:00
- **Authors**: Peyman Hosseinzadeh Kassani, Li Xiao, Gemeng Zhang, Julia M. Stephen, Tony W. Wilson, Vince D. Calhoun, Yu Ping Wang
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Human brain development is a complex and dynamic process that is affected by several factors such as genetics, sex hormones, and environmental changes. A number of recent studies on brain development have examined functional connectivity (FC) defined by the temporal correlation between time series of different brain regions. We propose to add the directional flow of information during brain maturation. To do so, we extract effective connectivity (EC) through Granger causality (GC) for two different groups of subjects, i.e., children and young adults. The motivation is that the inclusion of causal interaction may further discriminate brain connections between two age groups and help to discover new connections between brain regions. The contributions of this study are threefold. First, there has been a lack of attention to EC-based feature extraction in the context of brain development. To this end, we propose a new kernel-based GC (KGC) method to learn nonlinearity of complex brain network, where a reduced Sine hyperbolic polynomial (RSP) neural network was used as our proposed learner. Second, we used causality values as the weight for the directional connectivity between brain regions. Our findings indicated that the strength of connections was significantly higher in young adults relative to children. In addition, our new EC-based feature outperformed FC-based analysis from Philadelphia neurocohort (PNC) study with better discrimination of the different age groups. Moreover, the fusion of these two sets of features (FC + EC) improved brain age prediction accuracy by more than 4%, indicating that they should be used together for brain development studies.



### Deep learning-based prediction of response to HER2-targeted neoadjuvant chemotherapy from pre-treatment dynamic breast MRI: A multi-institutional validation study
- **Arxiv ID**: http://arxiv.org/abs/2001.08570v1
- **DOI**: None
- **Categories**: **q-bio.QM**, cs.CV, cs.LG, eess.IV, stat.AP, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2001.08570v1)
- **Published**: 2020-01-22 17:54:24+00:00
- **Updated**: 2020-01-22 17:54:24+00:00
- **Authors**: Nathaniel Braman, Mohammed El Adoui, Manasa Vulchi, Paulette Turk, Maryam Etesami, Pingfu Fu, Kaustav Bera, Stylianos Drisis, Vinay Varadan, Donna Plecha, Mohammed Benjelloun, Jame Abraham, Anant Madabhushi
- **Comment**: Braman and El Adoui contributed equally to this work. 33 pages, 3
  figures in main text
- **Journal**: None
- **Summary**: Predicting response to neoadjuvant therapy is a vexing challenge in breast cancer. In this study, we evaluate the ability of deep learning to predict response to HER2-targeted neo-adjuvant chemotherapy (NAC) from pre-treatment dynamic contrast-enhanced (DCE) MRI acquired prior to treatment. In a retrospective study encompassing DCE-MRI data from a total of 157 HER2+ breast cancer patients from 5 institutions, we developed and validated a deep learning approach for predicting pathological complete response (pCR) to HER2-targeted NAC prior to treatment. 100 patients who received HER2-targeted neoadjuvant chemotherapy at a single institution were used to train (n=85) and tune (n=15) a convolutional neural network (CNN) to predict pCR. A multi-input CNN leveraging both pre-contrast and late post-contrast DCE-MRI acquisitions was identified to achieve optimal response prediction within the validation set (AUC=0.93). This model was then tested on two independent testing cohorts with pre-treatment DCE-MRI data. It achieved strong performance in a 28 patient testing set from a second institution (AUC=0.85, 95% CI 0.67-1.0, p=.0008) and a 29 patient multicenter trial including data from 3 additional institutions (AUC=0.77, 95% CI 0.58-0.97, p=0.006). Deep learning-based response prediction model was found to exceed a multivariable model incorporating predictive clinical variables (AUC < .65 in testing cohorts) and a model of semi-quantitative DCE-MRI pharmacokinetic measurements (AUC < .60 in testing cohorts). The results presented in this work across multiple sites suggest that with further validation deep learning could provide an effective and reliable tool to guide targeted therapy in breast cancer, thus reducing overtreatment among HER2+ patients.



### Discovering Salient Anatomical Landmarks by Predicting Human Gaze
- **Arxiv ID**: http://arxiv.org/abs/2001.08188v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2001.08188v1)
- **Published**: 2020-01-22 18:17:14+00:00
- **Updated**: 2020-01-22 18:17:14+00:00
- **Authors**: Richard Droste, Pierre Chatelain, Lior Drukker, Harshita Sharma, Aris T. Papageorghiou, J. Alison Noble
- **Comment**: Accepted at IEEE International Symposium on Biomedical Imaging 2020
  (ISBI 2020)
- **Journal**: None
- **Summary**: Anatomical landmarks are a crucial prerequisite for many medical imaging tasks. Usually, the set of landmarks for a given task is predefined by experts. The landmark locations for a given image are then annotated manually or via machine learning methods trained on manual annotations. In this paper, in contrast, we present a method to automatically discover and localize anatomical landmarks in medical images. Specifically, we consider landmarks that attract the visual attention of humans, which we term visually salient landmarks. We illustrate the method for fetal neurosonographic images. First, full-length clinical fetal ultrasound scans are recorded with live sonographer gaze-tracking. Next, a convolutional neural network (CNN) is trained to predict the gaze point distribution (saliency map) of the sonographers on scan video frames. The CNN is then used to predict saliency maps of unseen fetal neurosonographic images, and the landmarks are extracted as the local maxima of these saliency maps. Finally, the landmarks are matched across images by clustering the landmark CNN features. We show that the discovered landmarks can be used within affine image registration, with average landmark alignment errors between 4.1% and 10.9% of the fetal head long axis length.



### Automatic phantom test pattern classification through transfer learning with deep neural networks
- **Arxiv ID**: http://arxiv.org/abs/2001.08189v1
- **DOI**: 10.1117/12.2549366
- **Categories**: **cs.CV**, cs.LG, cs.NE, eess.IV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2001.08189v1)
- **Published**: 2020-01-22 18:17:41+00:00
- **Updated**: 2020-01-22 18:17:41+00:00
- **Authors**: Rafael B. Fricks, Justin Solomon, Ehsan Samei
- **Comment**: None
- **Journal**: None
- **Summary**: Imaging phantoms are test patterns used to measure image quality in computer tomography (CT) systems. A new phantom platform (Mercury Phantom, Gammex) provides test patterns for estimating the task transfer function (TTF) or noise power spectrum (NPF) and simulates different patient sizes. Determining which image slices are suitable for analysis currently requires manual annotation of these patterns by an expert, as subtle defects may make an image unsuitable for measurement. We propose a method of automatically classifying these test patterns in a series of phantom images using deep learning techniques. By adapting a convolutional neural network based on the VGG19 architecture with weights trained on ImageNet, we use transfer learning to produce a classifier for this domain. The classifier is trained and evaluated with over 3,500 phantom images acquired at a university medical center. Input channels for color images are successfully adapted to convey contextual information for phantom images. A series of ablation studies are employed to verify design aspects of the classifier and evaluate its performance under varying training conditions. Our solution makes extensive use of image augmentation to produce a classifier that accurately classifies typical phantom images with 98% accuracy, while maintaining as much as 86% accuracy when the phantom is improperly imaged.



### RDAnet: A Deep Learning Based Approach for Synthetic Aperture Radar Image Formation
- **Arxiv ID**: http://arxiv.org/abs/2001.08202v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2001.08202v2)
- **Published**: 2020-01-22 18:44:40+00:00
- **Updated**: 2021-02-01 15:55:07+00:00
- **Authors**: Andrew Rittenbach, John Paul Walters
- **Comment**: 9 pages, 6 figures
- **Journal**: None
- **Summary**: Synthetic Aperture Radar (SAR) imaging systems operate by emitting radar signals from a moving object, such as a satellite, towards the target of interest. Reflected radar echoes are received and later used by image formation algorithms to form a SAR image. There is great interest in using SAR images in computer vision tasks such as classification or automatic target recognition. Today, however, SAR applications consist of multiple operations: image formation followed by image processing. In this work, we train a deep neural network that performs both the image formation and image processing tasks, integrating the SAR processing pipeline. Results show that our integrated pipeline can output accurately classified SAR imagery with image quality comparable to those formed using a traditional algorithm. We believe that this work is the first demonstration of an integrated neural network based SAR processing pipeline using real data.



### PENet: Object Detection using Points Estimation in Aerial Images
- **Arxiv ID**: http://arxiv.org/abs/2001.08247v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2001.08247v1)
- **Published**: 2020-01-22 19:43:17+00:00
- **Updated**: 2020-01-22 19:43:17+00:00
- **Authors**: Ziyang Tang, Xiang Liu, Guangyu Shen, Baijian Yang
- **Comment**: 7 pages, 5 figures
- **Journal**: None
- **Summary**: Aerial imagery has been increasingly adopted in mission-critical tasks, such as traffic surveillance, smart cities, and disaster assistance. However, identifying objects from aerial images faces the following challenges: 1) objects of interests are often too small and too dense relative to the images; 2) objects of interests are often in different relative sizes; and 3) the number of objects in each category is imbalanced. A novel network structure, Points Estimated Network (PENet), is proposed in this work to answer these challenges. PENet uses a Mask Resampling Module (MRM) to augment the imbalanced datasets, a coarse anchor-free detector (CPEN) to effectively predict the center points of the small object clusters, and a fine anchor-free detector FPEN to locate the precise positions of the small objects. An adaptive merge algorithm Non-maximum Merge (NMM) is implemented in CPEN to address the issue of detecting dense small objects, and a hierarchical loss is defined in FPEN to further improve the classification accuracy. Our extensive experiments on aerial datasets visDrone and UAVDT showed that PENet achieved higher precision results than existing state-of-the-art approaches. Our best model achieved 8.7% improvement on visDrone and 20.3% on UAVDT.



### How Much Position Information Do Convolutional Neural Networks Encode?
- **Arxiv ID**: http://arxiv.org/abs/2001.08248v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2001.08248v1)
- **Published**: 2020-01-22 19:44:43+00:00
- **Updated**: 2020-01-22 19:44:43+00:00
- **Authors**: Md Amirul Islam, Sen Jia, Neil D. B. Bruce
- **Comment**: Accepted to ICLR 2020
- **Journal**: None
- **Summary**: In contrast to fully connected networks, Convolutional Neural Networks (CNNs) achieve efficiency by learning weights associated with local filters with a finite spatial extent. An implication of this is that a filter may know what it is looking at, but not where it is positioned in the image. Information concerning absolute position is inherently useful, and it is reasonable to assume that deep CNNs may implicitly learn to encode this information if there is a means to do so. In this paper, we test this hypothesis revealing the surprising degree of absolute position information that is encoded in commonly used neural networks. A comprehensive set of experiments show the validity of this hypothesis and shed light on how and where this information is represented while offering clues to where positional information is derived from in deep CNNs.



### Using a Generative Adversarial Network for CT Normalization and its Impact on Radiomic Features
- **Arxiv ID**: http://arxiv.org/abs/2001.08741v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2001.08741v1)
- **Published**: 2020-01-22 23:41:29+00:00
- **Updated**: 2020-01-22 23:41:29+00:00
- **Authors**: Leihao Wei, Yannan Lin, William Hsu
- **Comment**: ISBI 2020
- **Journal**: None
- **Summary**: Computer-Aided-Diagnosis (CADx) systems assist radiologists with identifying and classifying potentially malignant pulmonary nodules on chest CT scans using morphology and texture-based (radiomic) features. However, radiomic features are sensitive to differences in acquisitions due to variations in dose levels and slice thickness. This study investigates the feasibility of generating a normalized scan from heterogeneous CT scans as input. We obtained projection data from 40 low-dose chest CT scans, simulating acquisitions at 10%, 25% and 50% dose and reconstructing the scans at 1.0mm and 2.0mm slice thickness. A 3D generative adversarial network (GAN) was used to simultaneously normalize reduced dose, thick slice (2.0mm) images to normal dose (100%), thinner slice (1.0mm) images. We evaluated the normalized image quality using peak signal-to-noise ratio (PSNR), structural similarity index (SSIM) and Learned Perceptual Image Patch Similarity (LPIPS). Our GAN improved perceptual similarity by 35%, compared to a baseline CNN method. Our analysis also shows that the GAN-based approach led to a significantly smaller error (p-value < 0.05) in nine studied radiomic features. These results indicated that GANs could be used to normalize heterogeneous CT images and reduce the variability in radiomic feature values.



### Learning to adapt class-specific features across domains for semantic segmentation
- **Arxiv ID**: http://arxiv.org/abs/2001.08311v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.08311v1)
- **Published**: 2020-01-22 23:51:30+00:00
- **Updated**: 2020-01-22 23:51:30+00:00
- **Authors**: Mikel Menta, Adriana Romero, Joost van de Weijer
- **Comment**: Master thesis dissertation for the Master in Computer Vision
  (Barcelona). 11 pages main article and 3 pages appendices. Code available
- **Journal**: None
- **Summary**: Recent advances in unsupervised domain adaptation have shown the effectiveness of adversarial training to adapt features across domains, endowing neural networks with the capability of being tested on a target domain without requiring any training annotations in this domain. The great majority of existing domain adaptation models rely on image translation networks, which often contain a huge amount of domain-specific parameters. Additionally, the feature adaptation step often happens globally, at a coarse level, hindering its applicability to tasks such as semantic segmentation, where details are of crucial importance to provide sharp results. In this thesis, we present a novel architecture, which learns to adapt features across domains by taking into account per class information. To that aim, we design a conditional pixel-wise discriminator network, whose output is conditioned on the segmentation masks. Moreover, following recent advances in image translation, we adopt the recently introduced StarGAN architecture as image translation backbone, since it is able to perform translations across multiple domains by means of a single generator network. Preliminary results on a segmentation task designed to assess the effectiveness of the proposed approach highlight the potential of the model, improving upon strong baselines and alternative designs.



