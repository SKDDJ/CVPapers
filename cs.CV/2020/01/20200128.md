# Arxiv Papers in cs.CV on 2020-01-28
### Hierarchical Multi-Process Fusion for Visual Place Recognition
- **Arxiv ID**: http://arxiv.org/abs/2002.03895v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2002.03895v1)
- **Published**: 2020-01-28 00:34:39+00:00
- **Updated**: 2020-01-28 00:34:39+00:00
- **Authors**: Stephen Hausler, Michael Milford
- **Comment**: Pre-print version of article which will be presented at ICRA 2020
- **Journal**: None
- **Summary**: Combining multiple complementary techniques together has long been regarded as a way to improve performance. In visual localization, multi-sensor fusion, multi-process fusion of a single sensing modality, and even combinations of different localization techniques have been shown to result in improved performance. However, merely fusing together different localization techniques does not account for the varying performance characteristics of different localization techniques. In this paper we present a novel, hierarchical localization system that explicitly benefits from three varying characteristics of localization techniques: the distribution of their localization hypotheses, their appearance- and viewpoint-invariant properties, and the resulting differences in where in an environment each system works well and fails. We show how two techniques deployed hierarchically work better than in parallel fusion, how combining two different techniques works better than two levels of a single technique, even when the single technique has superior individual performance, and develop two and three-tier hierarchical structures that progressively improve localization performance. Finally, we develop a stacked hierarchical framework where localization hypotheses from techniques with complementary characteristics are concatenated at each layer, significantly improving retention of the correct hypothesis through to the final localization stage. Using two challenging datasets, we show the proposed system outperforming state-of-the-art techniques.



### Medical Image Segmentation via Unsupervised Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2001.10155v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2001.10155v4)
- **Published**: 2020-01-28 03:56:42+00:00
- **Updated**: 2020-07-06 16:09:09+00:00
- **Authors**: Junyu Chen, Eric C. Frey
- **Comment**: In Medical Imaging with Deep Learning (2020)
- **Journal**: None
- **Summary**: For the majority of the learning-based segmentation methods, a large quantity of high-quality training data is required. In this paper, we present a novel learning-based segmentation model that could be trained semi- or un- supervised. Specifically, in the unsupervised setting, we parameterize the Active contour without edges (ACWE) framework via a convolutional neural network (ConvNet), and optimize the parameters of the ConvNet using a self-supervised method. In another setting (semi-supervised), the auxiliary segmentation ground truth is used during training. We show that the method provides fast and high-quality bone segmentation in the context of single-photon emission computed tomography (SPECT) image.



### An Internal Clock Based Space-time Neural Network for Motion Speed Recognition
- **Arxiv ID**: http://arxiv.org/abs/2001.10159v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2001.10159v1)
- **Published**: 2020-01-28 04:07:33+00:00
- **Updated**: 2020-01-28 04:07:33+00:00
- **Authors**: Junwen Luo, Jiaoyan Chen
- **Comment**: To appear in Neuro-inspired Computational Elements Workshop (NICE
  20). March 26-28,2020 Heidelberg, Germany, 8 pages
- **Journal**: None
- **Summary**: In this work we present a novel internal clock based space-time neural network for motion speed recognition. The developed system has a spike train encoder, a Spiking Neural Network (SNN) with internal clocking behaviors, a pattern transformation block and a Network Dynamic Dependent Plasticity (NDDP) learning block. The core principle is that the developed SNN will automatically tune its network pattern frequency (internal clock frequency) to recognize human motions in a speed domain. We employed both cartoons and real-world videos as training benchmarks, results demonstrate that our system can not only recognize motions with considerable speed differences (e.g. run, walk, jump, wonder(think) and standstill), but also motions with subtle speed gaps such as run and fast walk. The inference accuracy can be up to 83.3% (cartoon videos) and 75% (real-world videos). Meanwhile, the system only requires six video datasets in the learning stage and with up to 42 training trials. Hardware performance estimation indicates that the training time is 0.84-4.35s and power consumption is 33.26-201mW (based on an ARM Cortex M4 processor). Therefore, our system takes unique learning advantages of the requirement of the small dataset, quick learning and low power performance, which shows great potentials for edge or scalable AI-based applications.



### Robust Method for Semantic Segmentation of Whole-Slide Blood Cell Microscopic Image
- **Arxiv ID**: http://arxiv.org/abs/2001.10188v1
- **DOI**: 10.1155/2020/4015323
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2001.10188v1)
- **Published**: 2020-01-28 06:32:21+00:00
- **Updated**: 2020-01-28 06:32:21+00:00
- **Authors**: Muhammad Shahzad, Arif Iqbal Umar, Muazzam A. Khan, Syed Hamad Shirazi, Zakir Khan, Waqas Yousaf
- **Comment**: 13 pages, 13 figures
- **Journal**: Volume 2020, Article ID 4015323, 13 pages
- **Summary**: Previous works on segmentation of SEM (scanning electron microscope) blood cell image ignore the semantic segmentation approach of whole-slide blood cell segmentation. In the proposed work, we address the problem of whole-slide blood cell segmentation using the semantic segmentation approach. We design a novel convolutional encoder-decoder framework along with VGG-16 as the pixel-level feature extraction model. -e proposed framework comprises 3 main steps: First, all the original images along with manually generated ground truth masks of each blood cell type are passed through the preprocessing stage. In the preprocessing stage, pixel-level labeling, RGB to grayscale conversion of masked image and pixel fusing, and unity mask generation are performed. After that, VGG16 is loaded into the system, which acts as a pretrained pixel-level feature extraction model. In the third step, the training process is initiated on the proposed model. We have evaluated our network performance on three evaluation metrics. We obtained outstanding results with respect to classwise, as well as global and mean accuracies. Our system achieved classwise accuracies of 97.45%, 93.34%, and 85.11% for RBCs, WBCs, and platelets, respectively, while global and mean accuracies remain 97.18% and 91.96%, respectively.



### DFKI Cabin Simulator: A Test Platform for Visual In-Cabin Monitoring Functions
- **Arxiv ID**: http://arxiv.org/abs/2002.03749v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, cs.LG, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2002.03749v2)
- **Published**: 2020-01-28 07:15:50+00:00
- **Updated**: 2020-02-11 12:27:42+00:00
- **Authors**: Hartmut Feld, Bruno Mirbach, Jigyasa Katrolia, Mohamed Selim, Oliver Wasenmüller, Didier Stricker
- **Comment**: corrected typos and bad reference
- **Journal**: None
- **Summary**: We present a test platform for visual in-cabin scene analysis and occupant monitoring functions. The test platform is based on a driving simulator developed at the DFKI, consisting of a realistic in-cabin mock-up and a wide-angle projection system for a realistic driving experience. The platform has been equipped with a wide-angle 2D/3D camera system monitoring the entire interior of the vehicle mock-up of the simulator. It is also supplemented with a ground truth reference sensor system that allows to track and record the occupant's body movements synchronously with the 2D and 3D video streams of the camera. Thus, the resulting test platform will serve as a basis to validate numerous in-cabin monitoring functions, which are important for the realization of novel human-vehicle interfaces, advanced driver assistant systems, and automated driving. Among the considered functions are occupant presence detection, size and 3D-pose estimation and driver intention recognition. In addition, our platform will be the basis for the creation of large-scale in-cabin benchmark datasets.



### Learning to Catch Piglets in Flight
- **Arxiv ID**: http://arxiv.org/abs/2001.10220v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2001.10220v1)
- **Published**: 2020-01-28 09:13:17+00:00
- **Updated**: 2020-01-28 09:13:17+00:00
- **Authors**: Ozan Çatal, Lawrence De Mol, Tim Verbelen, Bart Dhoedt
- **Comment**: Fast Neural Perception and Learning for Intelligent Vehicles and
  Robotics workshop at IROS 2019
- **Journal**: None
- **Summary**: Catching objects in-flight is an outstanding challenge in robotics. In this paper, we present a closed-loop control system fusing data from two sensor modalities: an RGB-D camera and a radar. To develop and test our method, we start with an easy to identify object: a stuffed Piglet. We implement and compare two approaches to detect and track the object, and to predict the interception point. A baseline model uses colour filtering for locating the thrown object in the environment, while the interception point is predicted using a least squares regression over the physical ballistic trajectory equations. A deep learning based method uses artificial neural networks for both object detection and interception point prediction. We show that we are able to successfully catch Piglet in 80% of the cases with our deep learning approach.



### BioTouchPass2: Touchscreen Password Biometrics Using Time-Aligned Recurrent Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2001.10223v1
- **DOI**: 10.1109/TIFS.2020.2973832
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2001.10223v1)
- **Published**: 2020-01-28 09:25:06+00:00
- **Updated**: 2020-01-28 09:25:06+00:00
- **Authors**: Ruben Tolosana, Ruben Vera-Rodriguez, Julian Fierrez, Javier Ortega-Garcia
- **Comment**: None
- **Journal**: IEEE Transactions on Information Forensics and Security, 2020
- **Summary**: Passwords are still used on a daily basis for all kind of applications. However, they are not secure enough by themselves in many cases. This work enhances password scenarios through two-factor authentication asking the users to draw each character of the password instead of typing them as usual. The main contributions of this study are as follows: i) We present the novel MobileTouchDB public database, acquired in an unsupervised mobile scenario with no restrictions in terms of position, posture, and devices. This database contains more than 64K on-line character samples performed by 217 users, with 94 different smartphone models, and up to 6 acquisition sessions. ii) We perform a complete analysis of the proposed approach considering both traditional authentication systems such as Dynamic Time Warping (DTW) and novel approaches based on Recurrent Neural Networks (RNNs). In addition, we present a novel approach named Time-Aligned Recurrent Neural Networks (TA-RNNs). This approach combines the potential of DTW and RNNs to train more robust systems against attacks.   A complete analysis of the proposed approach is carried out using both MobileTouchDB and e-BioDigitDB databases. Our proposed TA-RNN system outperforms the state of the art, achieving a final 2.38% Equal Error Rate, using just a 4-digit password and one training sample per character. These results encourage the deployment of our proposed approach in comparison with traditional typed-based password systems where the attack would have 100% success rate under the same impostor scenario.



### Controlling generative models with continuous factors of variations
- **Arxiv ID**: http://arxiv.org/abs/2001.10238v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2001.10238v1)
- **Published**: 2020-01-28 10:04:04+00:00
- **Updated**: 2020-01-28 10:04:04+00:00
- **Authors**: Antoine Plumerault, Hervé Le Borgne, Céline Hudelot
- **Comment**: Accepted as a poster presentation at the International Conference for
  Learning Representations (ICLR), 2020
- **Journal**: None
- **Summary**: Recent deep generative models are able to provide photo-realistic images as well as visual or textual content embeddings useful to address various tasks of computer vision and natural language processing. Their usefulness is nevertheless often limited by the lack of control over the generative process or the poor understanding of the learned representation. To overcome these major issues, very recent work has shown the interest of studying the semantics of the latent space of generative models. In this paper, we propose to advance on the interpretability of the latent space of generative models by introducing a new method to find meaningful directions in the latent space of any generative model along which we can move to control precisely specific properties of the generated image like the position or scale of the object in the image. Our method does not require human annotations and is particularly well suited for the search of directions encoding simple transformations of the generated image, such as translation, zoom or color variations. We demonstrate the effectiveness of our method qualitatively and quantitatively, both for GANs and variational auto-encoders.



### Spatial-Adaptive Network for Single Image Denoising
- **Arxiv ID**: http://arxiv.org/abs/2001.10291v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2001.10291v2)
- **Published**: 2020-01-28 12:24:17+00:00
- **Updated**: 2020-07-14 02:30:55+00:00
- **Authors**: Meng Chang, Qi Li, Huajun Feng, Zhihai Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Previous works have shown that convolutional neural networks can achieve good performance in image denoising tasks. However, limited by the local rigid convolutional operation, these methods lead to oversmoothing artifacts. A deeper network structure could alleviate these problems, but more computational overhead is needed. In this paper, we propose a novel spatial-adaptive denoising network (SADNet) for efficient single image blind noise removal. To adapt to changes in spatial textures and edges, we design a residual spatial-adaptive block. Deformable convolution is introduced to sample the spatially correlated features for weighting. An encoder-decoder structure with a context block is introduced to capture multiscale information. With noise removal from the coarse to fine, a high-quality noisefree image can be obtained. We apply our method to both synthetic and real noisy image datasets. The experimental results demonstrate that our method can surpass the state-of-the-art denoising methods both quantitatively and visually.



### f-BRS: Rethinking Backpropagating Refinement for Interactive Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2001.10331v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.10331v3)
- **Published**: 2020-01-28 14:10:46+00:00
- **Updated**: 2020-08-25 11:52:26+00:00
- **Authors**: Konstantin Sofiiuk, Ilia Petrov, Olga Barinova, Anton Konushin
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks have become a mainstream approach to interactive segmentation. As we show in our experiments, while for some images a trained network provides accurate segmentation result with just a few clicks, for some unknown objects it cannot achieve satisfactory result even with a large amount of user input. Recently proposed backpropagating refinement (BRS) scheme introduces an optimization problem for interactive segmentation that results in significantly better performance for the hard cases. At the same time, BRS requires running forward and backward pass through a deep network several times that leads to significantly increased computational budget per click compared to other methods. We propose f-BRS (feature backpropagating refinement scheme) that solves an optimization problem with respect to auxiliary variables instead of the network inputs, and requires running forward and backward pass just for a small part of a network. Experiments on GrabCut, Berkeley, DAVIS and SBD datasets set new state-of-the-art at an order of magnitude lower time per click compared to original BRS. The code and trained models are available at https://github.com/saic-vul/fbrs_interactive_segmentation .



### Multi-Source Deep Domain Adaptation for Quality Control in Retail Food Packaging
- **Arxiv ID**: http://arxiv.org/abs/2001.10335v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2001.10335v1)
- **Published**: 2020-01-28 14:16:58+00:00
- **Updated**: 2020-01-28 14:16:58+00:00
- **Authors**: Mamatha Thota, Stefanos Kollias, Mark Swainson, Georgios Leontidis
- **Comment**: 8 pages, 3 figures, 7 tables
- **Journal**: None
- **Summary**: Retail food packaging contains information which informs choice and can be vital to consumer health, including product name, ingredients list, nutritional information, allergens, preparation guidelines, pack weight, storage and shelf life information (use-by / best before dates). The presence and accuracy of such information is critical to ensure a detailed understanding of the product and to reduce the potential for health risks. Consequently, erroneous or illegible labeling has the potential to be highly detrimental to consumers and many other stakeholders in the supply chain. In this paper, a multi-source deep learning-based domain adaptation system is proposed and tested to identify and verify the presence and legibility of use-by date information from food packaging photos taken as part of the validation process as the products pass along the food production line. This was achieved by improving the generalization of the techniques via making use of multi-source datasets in order to extract domain-invariant representations for all domains and aligning distribution of all pairs of source and target domains in a common feature space, along with the class boundaries. The proposed system performed very well in the conducted experiments, for automating the verification process and reducing labeling errors that could otherwise threaten public health and contravene legal requirements for food packaging information and accuracy. Comprehensive experiments on our food packaging datasets demonstrate that the proposed multi-source deep domain adaptation method significantly improves the classification accuracy and therefore has great potential for application and beneficial impact in food manufacturing control systems.



### CSNNs: Unsupervised, Backpropagation-free Convolutional Neural Networks for Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2001.10388v2
- **DOI**: 10.1109/ICMLA.2019.00265
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2001.10388v2)
- **Published**: 2020-01-28 14:57:39+00:00
- **Updated**: 2020-01-29 10:47:08+00:00
- **Authors**: Bonifaz Stuhr, Jürgen Brauer
- **Comment**: 18 pages,18 figures, Author's extended version of the paper. Final
  version presented at 18th IEEE International Conference on Machine Learning
  and Applications (ICMLA). Boca Raton, Florida / USA. 2019
- **Journal**: None
- **Summary**: This work combines Convolutional Neural Networks (CNNs), clustering via Self-Organizing Maps (SOMs) and Hebbian Learning to propose the building blocks of Convolutional Self-Organizing Neural Networks (CSNNs), which learn representations in an unsupervised and Backpropagation-free manner. Our approach replaces the learning of traditional convolutional layers from CNNs with the competitive learning procedure of SOMs and simultaneously learns local masks between those layers with separate Hebbian-like learning rules to overcome the problem of disentangling factors of variation when filters are learned through clustering. We investigate the learned representation by designing two simple models with our building blocks, achieving comparable performance to many methods which use Backpropagation, while we reach comparable performance on Cifar10 and give baseline performances on Cifar100, Tiny ImageNet and a small subset of ImageNet for Backpropagation-free methods.



### OPFython: A Python-Inspired Optimum-Path Forest Classifier
- **Arxiv ID**: http://arxiv.org/abs/2001.10420v3
- **DOI**: 10.1016/j.simpa.2021.100113
- **Categories**: **cs.LG**, cs.CV, stat.ML, 68T01, I.2.0; I.5.0
- **Links**: [PDF](http://arxiv.org/pdf/2001.10420v3)
- **Published**: 2020-01-28 15:46:19+00:00
- **Updated**: 2021-07-30 20:15:52+00:00
- **Authors**: Gustavo Henrique de Rosa, João Paulo Papa, Alexandre Xavier Falcão
- **Comment**: 14 pages, 11 figures
- **Journal**: Software Impacts, 1 (2021), Article 100113
- **Summary**: Machine learning techniques have been paramount throughout the last years, being applied in a wide range of tasks, such as classification, object recognition, person identification, and image segmentation. Nevertheless, conventional classification algorithms, e.g., Logistic Regression, Decision Trees, and Bayesian classifiers, might lack complexity and diversity, not suitable when dealing with real-world data. A recent graph-inspired classifier, known as the Optimum-Path Forest, has proven to be a state-of-the-art technique, comparable to Support Vector Machines and even surpassing it in some tasks. This paper proposes a Python-based Optimum-Path Forest framework, denoted as OPFython, where all of its functions and classes are based upon the original C language implementation. Additionally, as OPFython is a Python-based library, it provides a more friendly environment and a faster prototyping workspace than the C language.



### NAS-Bench-1Shot1: Benchmarking and Dissecting One-shot Neural Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/2001.10422v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2001.10422v2)
- **Published**: 2020-01-28 15:50:22+00:00
- **Updated**: 2020-04-12 22:48:28+00:00
- **Authors**: Arber Zela, Julien Siems, Frank Hutter
- **Comment**: In: International Conference on Learning Representations (ICLR 2020);
  19 pages, 17 figures
- **Journal**: None
- **Summary**: One-shot neural architecture search (NAS) has played a crucial role in making NAS methods computationally feasible in practice. Nevertheless, there is still a lack of understanding on how these weight-sharing algorithms exactly work due to the many factors controlling the dynamics of the process. In order to allow a scientific study of these components, we introduce a general framework for one-shot NAS that can be instantiated to many recently-introduced variants and introduce a general benchmarking framework that draws on the recent large-scale tabular benchmark NAS-Bench-101 for cheap anytime evaluations of one-shot NAS methods. To showcase the framework, we compare several state-of-the-art one-shot NAS methods, examine how sensitive they are to their hyperparameters and how they can be improved by tuning their hyperparameters, and compare their performance to that of blackbox optimizers for NAS-Bench-101.



### A Class of Linear Programs Solvable by Coordinate-Wise Minimization
- **Arxiv ID**: http://arxiv.org/abs/2001.10467v5
- **DOI**: None
- **Categories**: **math.OC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2001.10467v5)
- **Published**: 2020-01-28 17:14:47+00:00
- **Updated**: 2020-09-14 10:47:55+00:00
- **Authors**: Tomáš Dlask, Tomáš Werner
- **Comment**: The final authenticated version is available online at
  https://doi.org/10.1007/978-3-030-53552-0_8
- **Journal**: None
- **Summary**: Coordinate-wise minimization is a simple popular method for large-scale optimization. Unfortunately, for general (non-differentiable) convex problems it may not find global minima. We present a class of linear programs that coordinate-wise minimization solves exactly. We show that dual LP relaxations of several well-known combinatorial optimization problems are in this class and the method finds a global minimum with sufficient accuracy in reasonable runtimes. Moreover, for extensions of these problems that no longer are in this class the method yields reasonably good suboptima. Though the presented LP relaxations can be solved by more efficient methods (such as max-flow), our results are theoretically non-trivial and can lead to new large-scale optimization algorithms in the future.



### MGCN: Descriptor Learning using Multiscale GCNs
- **Arxiv ID**: http://arxiv.org/abs/2001.10472v3
- **DOI**: 10.1145/3386569.3392443
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2001.10472v3)
- **Published**: 2020-01-28 17:25:14+00:00
- **Updated**: 2020-08-07 10:18:28+00:00
- **Authors**: Yiqun Wang, Jing Ren, Dong-Ming Yan, Jianwei Guo, Xiaopeng Zhang, Peter Wonka
- **Comment**: Accepted to SIGGRAPH 2020. (15 pages, 15 figures, 12 tables,
  low-resolution version)
- **Journal**: ACM Transactions on Graphics (Proceedings of SIGGRAPH), Vol. 39,
  No. 4, Article 122. Publication date: July 2020
- **Summary**: We propose a novel framework for computing descriptors for characterizing points on three-dimensional surfaces. First, we present a new non-learned feature that uses graph wavelets to decompose the Dirichlet energy on a surface. We call this new feature wavelet energy decomposition signature (WEDS). Second, we propose a new multiscale graph convolutional network (MGCN) to transform a non-learned feature to a more discriminative descriptor. Our results show that the new descriptor WEDS is more discriminative than the current state-of-the-art non-learned descriptors and that the combination of WEDS and MGCN is better than the state-of-the-art learned descriptors. An important design criterion for our descriptor is the robustness to different surface discretizations including triangulations with varying numbers of vertices. Our results demonstrate that previous graph convolutional networks significantly overfit to a particular resolution or even a particular triangulation, but MGCN generalizes well to different surface discretizations. In addition, MGCN is compatible with previous descriptors and it can also be used to improve the performance of other descriptors, such as the heat kernel signature, the wave kernel signature, or the local point signature.



### Lossless Compression of Mosaic Images with Convolutional Neural Network Prediction
- **Arxiv ID**: http://arxiv.org/abs/2001.10484v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2001.10484v1)
- **Published**: 2020-01-28 17:41:31+00:00
- **Updated**: 2020-01-28 17:41:31+00:00
- **Authors**: Seyed Mehdi Ayyoubzadeh, Xiaolin Wu
- **Comment**: None
- **Journal**: None
- **Summary**: We present a CNN-based predictive lossless compression scheme for raw color mosaic images of digital cameras. This specialized application problem was previously understudied but it is now becoming increasingly important, because modern CNN methods for image restoration tasks (e.g., superresolution, low lighting enhancement, deblurring), must operate on original raw mosaic images to obtain the best possible results. The key innovation of this paper is a high-order nonlinear CNN predictor of spatial-spectral mosaic patterns. The deep learning prediction can model highly complex sample dependencies in spatial-spectral mosaic images more accurately and hence remove statistical redundancies more thoroughly than existing image predictors. Experiments show that the proposed CNN predictor achieves unprecedented lossless compression performance on camera raw images.



### Accurately identifying vertebral levels in large datasets
- **Arxiv ID**: http://arxiv.org/abs/2001.10503v1
- **DOI**: 10.1117/12.2551247
- **Categories**: **eess.IV**, cs.CV, I.5.1; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2001.10503v1)
- **Published**: 2020-01-28 18:15:02+00:00
- **Updated**: 2020-01-28 18:15:02+00:00
- **Authors**: Daniel C. Elton, Veit Sandfort, Perry J. Pickhardt, Ronald M. Summers
- **Comment**: Accepted for publication in Proceedings of SPIE 2020: Medical Imaging
- **Journal**: None
- **Summary**: The vertebral levels of the spine provide a useful coordinate system when making measurements of plaque, muscle, fat, and bone mineral density. Correctly classifying vertebral levels with high accuracy is challenging due to the similar appearance of each vertebra, the curvature of the spine, and the possibility of anomalies such as fractured vertebrae, implants, lumbarization of the sacrum, and sacralization of L5. The goal of this work is to develop a system that can accurately and robustly identify the L1 level in large heterogeneous datasets. The first approach we study is using a 3D U-Net to segment the L1 vertebra directly using the entire scan volume to provide context. We also tested models for two class segmentation of L1 and T12 and a three class segmentation of L1, T12 and the rib attached to T12. By increasing the number of training examples to 249 scans using pseudo-segmentations from an in-house segmentation tool we were able to achieve 98% accuracy with respect to identifying the L1 vertebra, with an average error of 4.5 mm in the craniocaudal level. We next developed an algorithm which performs iterative instance segmentation and classification of the entire spine with a 3D U-Net. We found the instance based approach was able to yield better segmentations of nearly the entire spine, but had lower classification accuracy for L1.



### Segmentation and Recovery of Superquadric Models using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2001.10504v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.10504v1)
- **Published**: 2020-01-28 18:17:48+00:00
- **Updated**: 2020-01-28 18:17:48+00:00
- **Authors**: Jaka Šircelj, Tim Oblak, Klemen Grm, Uroš Petković, Aleš Jaklič, Peter Peer, Vitomir Štruc, Franc Solina
- **Comment**: 8 pages, in Computer Vision Winter Workshop, 2020
- **Journal**: None
- **Summary**: In this paper we address the problem of representing 3D visual data with parameterized volumetric shape primitives. Specifically, we present a (two-stage) approach built around convolutional neural networks (CNNs) capable of segmenting complex depth scenes into the simpler geometric structures that can be represented with superquadric models. In the first stage, our approach uses a Mask RCNN model to identify superquadric-like structures in depth scenes and then fits superquadric models to the segmented structures using a specially designed CNN regressor. Using our approach we are able to describe complex structures with a small number of interpretable parameters. We evaluated the proposed approach on synthetic as well as real-world depth data and show that our solution does not only result in competitive performance in comparison to the state-of-the-art, but is able to decompose scenes into a number of superquadric models at a fraction of the time required by competing approaches. We make all data and models used in the paper available from https://lmi.fe.uni-lj.si/en/research/resources/sq-seg.



### Identifying Mislabeled Data using the Area Under the Margin Ranking
- **Arxiv ID**: http://arxiv.org/abs/2001.10528v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2001.10528v4)
- **Published**: 2020-01-28 18:59:03+00:00
- **Updated**: 2020-12-23 14:01:54+00:00
- **Authors**: Geoff Pleiss, Tianyi Zhang, Ethan R. Elenberg, Kilian Q. Weinberger
- **Comment**: NeurIPS 2020
- **Journal**: None
- **Summary**: Not all data in a typical training set help with generalization; some samples can be overly ambiguous or outrightly mislabeled. This paper introduces a new method to identify such samples and mitigate their impact when training neural networks. At the heart of our algorithm is the Area Under the Margin (AUM) statistic, which exploits differences in the training dynamics of clean and mislabeled samples. A simple procedure - adding an extra class populated with purposefully mislabeled threshold samples - learns a AUM upper bound that isolates mislabeled data. This approach consistently improves upon prior work on synthetic and real-world datasets. On the WebVision50 classification task our method removes 17% of training data, yielding a 1.6% (absolute) improvement in test error. On CIFAR100 removing 13% of the data leads to a 1.2% drop in error.



### A Review on Object Pose Recovery: from 3D Bounding Box Detectors to Full 6D Pose Estimators
- **Arxiv ID**: http://arxiv.org/abs/2001.10609v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.10609v2)
- **Published**: 2020-01-28 22:05:09+00:00
- **Updated**: 2020-04-19 11:59:38+00:00
- **Authors**: Caner Sahin, Guillermo Garcia-Hernando, Juil Sock, Tae-Kyun Kim
- **Comment**: Accepted to the journal of Image and Vision Computing (IVC). arXiv
  admin note: text overlap with arXiv:1903.04229
- **Journal**: None
- **Summary**: Object pose recovery has gained increasing attention in the computer vision field as it has become an important problem in rapidly evolving technological areas related to autonomous driving, robotics, and augmented reality. Existing review-related studies have addressed the problem at visual level in 2D, going through the methods which produce 2D bounding boxes of objects of interest in RGB images. The 2D search space is enlarged either using the geometry information available in the 3D space along with RGB (Mono/Stereo) images, or utilizing depth data from LIDAR sensors and/or RGB-D cameras. 3D bounding box detectors, producing category-level amodal 3D bounding boxes, are evaluated on gravity aligned images, while full 6D object pose estimators are mostly tested at instance-level on the images where the alignment constraint is removed. Recently, 6D object pose estimation is tackled at the level of categories. In this paper, we present the first comprehensive and most recent review of the methods on object pose recovery, from 3D bounding box detectors to full 6D pose estimators. The methods mathematically model the problem as a classification, regression, classification & regression, template matching, and point-pair feature matching task. Based on this, a mathematical-model-based categorization of the methods is established. Datasets used for evaluating the methods are investigated with respect to the challenges, and evaluation metrics are studied. Quantitative results of experiments in the literature are analyzed to show which category of methods best performs across what types of challenges. The analyses are further extended comparing two methods, which are our own implementations, so that the outcomes from the public results are further solidified. Current position of the field is summarized regarding object pose recovery, and possible research directions are identified.



### Deep Learning in Multi-organ Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2001.10619v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2001.10619v1)
- **Published**: 2020-01-28 22:11:44+00:00
- **Updated**: 2020-01-28 22:11:44+00:00
- **Authors**: Yang Lei, Yabo Fu, Tonghe Wang, Richard L. J. Qiu, Walter J. Curran, Tian Liu, Xiaofeng Yang
- **Comment**: 37 pages, 2 figures, 8 tables
- **Journal**: None
- **Summary**: This paper presents a review of deep learning (DL) in multi-organ segmentation. We summarized the latest DL-based methods for medical image segmentation and applications. These methods were classified into six categories according to their network design. For each category, we listed the surveyed works, highlighted important contributions and identified specific challenges. Following the detailed review of each category, we briefly discussed its achievements, shortcomings and future potentials. We provided a comprehensive comparison among DL-based methods for thoracic and head & neck multiorgan segmentation using benchmark datasets, including the 2017 AAPM Thoracic Auto-segmentation Challenge datasets and 2015 MICCAI Head Neck Auto-Segmentation Challenge datasets.



