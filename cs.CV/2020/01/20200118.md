# Arxiv Papers in cs.CV on 2020-01-18
### Media Forensics and DeepFakes: an overview
- **Arxiv ID**: http://arxiv.org/abs/2001.06564v1
- **DOI**: 10.1109/JSTSP.2020.3002101
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.06564v1)
- **Published**: 2020-01-18 00:13:32+00:00
- **Updated**: 2020-01-18 00:13:32+00:00
- **Authors**: Luisa Verdoliva
- **Comment**: None
- **Journal**: None
- **Summary**: With the rapid progress of recent years, techniques that generate and manipulate multimedia content can now guarantee a very advanced level of realism. The boundary between real and synthetic media has become very thin. On the one hand, this opens the door to a series of exciting applications in different fields such as creative arts, advertising, film production, video games. On the other hand, it poses enormous security threats. Software packages freely available on the web allow any individual, without special skills, to create very realistic fake images and videos. So-called deepfakes can be used to manipulate public opinion during elections, commit fraud, discredit or blackmail people. Potential abuses are limited only by human imagination. Therefore, there is an urgent need for automated tools capable of detecting false multimedia content and avoiding the spread of dangerous false information. This review paper aims to present an analysis of the methods for visual media integrity verification, that is, the detection of manipulated images and videos. Special emphasis will be placed on the emerging phenomenon of deepfakes and, from the point of view of the forensic analyst, on modern data-driven forensic methods. The analysis will help to highlight the limits of current forensic tools, the most relevant issues, the upcoming challenges, and suggest future directions for research.



### Harmonic Convolutional Networks based on Discrete Cosine Transform
- **Arxiv ID**: http://arxiv.org/abs/2001.06570v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2001.06570v3)
- **Published**: 2020-01-18 01:13:20+00:00
- **Updated**: 2022-04-09 18:27:21+00:00
- **Authors**: Matej Ulicny, Vladimir A. Krylov, Rozenn Dahyot
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1812.03205
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) learn filters in order to capture local correlation patterns in feature space. We propose to learn these filters as combinations of preset spectral filters defined by the Discrete Cosine Transform (DCT). Our proposed DCT-based harmonic blocks replace conventional convolutional layers to produce partially or fully harmonic versions of new or existing CNN architectures. Using DCT energy compaction properties, we demonstrate how the harmonic networks can be efficiently compressed by truncating high-frequency information in harmonic blocks thanks to the redundancies in the spectral domain. We report extensive experimental validation demonstrating benefits of the introduction of harmonic blocks into state-of-the-art CNN models in image classification, object detection and semantic segmentation applications.



### A GAN-based Tunable Image Compression System
- **Arxiv ID**: http://arxiv.org/abs/2001.06580v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2001.06580v1)
- **Published**: 2020-01-18 02:40:09+00:00
- **Updated**: 2020-01-18 02:40:09+00:00
- **Authors**: Lirong Wu, Kejie Huang, Haibin Shen
- **Comment**: None
- **Journal**: None
- **Summary**: The method of importance map has been widely adopted in DNN-based lossy image compression to achieve bit allocation according to the importance of image contents. However, insufficient allocation of bits in non-important regions often leads to severe distortion at low bpp (bits per pixel), which hampers the development of efficient content-weighted image compression systems. This paper rethinks content-based compression by using Generative Adversarial Network (GAN) to reconstruct the non-important regions. Moreover, multiscale pyramid decomposition is applied to both the encoder and the discriminator to achieve global compression of high-resolution images. A tunable compression scheme is also proposed in this paper to compress an image to any specific compression ratio without retraining the model. The experimental results show that our proposed method improves MS-SSIM by more than 10.3% compared to the recently reported GAN-based method to achieve the same low bpp (0.05) on the Kodak dataset.



### A Foreground-background Parallel Compression with Residual Encoding for Surveillance Video
- **Arxiv ID**: http://arxiv.org/abs/2001.06590v3
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2001.06590v3)
- **Published**: 2020-01-18 03:35:00+00:00
- **Updated**: 2020-09-28 08:49:06+00:00
- **Authors**: Lirong Wu, Kejie Huang, Haibin Shen, Lianli Gao
- **Comment**: None
- **Journal**: None
- **Summary**: The data storage has been one of the bottlenecks in surveillance systems. The conventional video compression algorithms such as H.264 and H.265 do not fully utilize the low information density characteristic of the surveillance video. In this paper, we propose a video compression method that extracts and compresses the foreground and background of the video separately. The compression ratio is greatly improved by sharing background information among multiple adjacent frames through an adaptive background updating and interpolation module. Besides, we present two different schemes to compress the foreground and compare their performance in the ablation study to show the importance of temporal information for video compression. In the decoding end, a coarse-to-fine two-stage module is applied to achieve the composition of the foreground and background and the enhancements of frame quality. Furthermore, an adaptive sampling method for surveillance cameras is proposed, and we have shown its effects through software simulation. The experimental results show that our proposed method requires 69.5% less bpp (bits per pixel) than the conventional algorithm H.265 to achieve the same PSNR (36 dB) on the HECV dataset.



### Deep Metric Structured Learning For Facial Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/2001.06612v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.06612v2)
- **Published**: 2020-01-18 06:23:18+00:00
- **Updated**: 2022-01-06 03:31:32+00:00
- **Authors**: Pedro D. Marrero Fernandez, Tsang Ing Ren, Tsang Ing Jyh, Fidel A. Guerrero Pe√±a, Alexandre Cunha
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a deep metric learning model to create embedded sub-spaces with a well defined structure. A new loss function that imposes Gaussian structures on the output space is introduced to create these sub-spaces thus shaping the distribution of the data. Having a mixture of Gaussians solution space is advantageous given its simplified and well established structure. It allows fast discovering of classes within classes and the identification of mean representatives at the centroids of individual classes. We also propose a new semi-supervised method to create sub-classes. We illustrate our methods on the facial expression recognition problem and validate results on the FER+, AffectNet, Extended Cohn-Kanade (CK+), BU-3DFE, and JAFFE datasets. We experimentally demonstrate that the learned embedding can be successfully used for various applications including expression retrieval and emotion recognition.



### Accelerating the Registration of Image Sequences by Spatio-temporal Multilevel Strategies
- **Arxiv ID**: http://arxiv.org/abs/2001.06613v1
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2001.06613v1)
- **Published**: 2020-01-18 06:46:16+00:00
- **Updated**: 2020-01-18 06:46:16+00:00
- **Authors**: Hari Om Aggrawal, Jan Modersitzki
- **Comment**: Accepted at ISBI 2020
- **Journal**: None
- **Summary**: Multilevel strategies are an integral part of many image registration algorithms. These strategies are very well-known for avoiding undesirable local minima, providing an outstanding initial guess, and reducing overall computation time. State-of-the-art multilevel strategies build a hierarchy of discretization in the spatial dimensions. In this paper, we present a spatio-temporal strategy, where we introduce a hierarchical discretization in the temporal dimension at each spatial level. This strategy is suitable for a motion estimation problem where the motion is assumed smooth over time. Our strategy exploits the temporal smoothness among image frames by following a predictor-corrector approach. The strategy predicts the motion by a novel interpolation method and later corrects it by registration. The prediction step provides a good initial guess for the correction step, hence reduces the overall computational time for registration. The acceleration is achieved by a factor of 2.5 on average, over the state-of-the-art multilevel methods on three examined optical coherence tomography datasets.



### OIAD: One-for-all Image Anomaly Detection with Disentanglement Learning
- **Arxiv ID**: http://arxiv.org/abs/2001.06640v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.DB, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2001.06640v2)
- **Published**: 2020-01-18 09:57:37+00:00
- **Updated**: 2020-03-26 09:00:14+00:00
- **Authors**: Shuo Wang, Tianle Chen, Shangyu Chen, Carsten Rudolph, Surya Nepal, Marthie Grobler
- **Comment**: arXiv admin note: text overlap with arXiv:1802.05983,
  arXiv:1909.02755, arXiv:1804.03599 by other authors
- **Journal**: None
- **Summary**: Anomaly detection aims to recognize samples with anomalous and unusual patterns with respect to a set of normal data. This is significant for numerous domain applications, such as industrial inspection, medical imaging, and security enforcement. There are two key research challenges associated with existing anomaly detection approaches: (1) many approaches perform well on low-dimensional problems however the performance on high-dimensional instances, such as images, is limited; (2) many approaches often rely on traditional supervised approaches and manual engineering of features, while the topic has not been fully explored yet using modern deep learning approaches, even when the well-label samples are limited. In this paper, we propose a One-for-all Image Anomaly Detection system (OIAD) based on disentangled learning using only clean samples. Our key insight is that the impact of small perturbation on the latent representation can be bounded for normal samples while anomaly images are usually outside such bounded intervals, referred to as structure consistency. We implement this idea and evaluate its performance for anomaly detection. Our experiments with three datasets show that OIAD can detect over $90\%$ of anomalies while maintaining a low false alarm rate. It can also detect suspicious samples from samples labeled as clean, coincided with what humans would deem unusual.



### Stacked Adversarial Network for Zero-Shot Sketch based Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2001.06657v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2001.06657v1)
- **Published**: 2020-01-18 12:18:28+00:00
- **Updated**: 2020-01-18 12:18:28+00:00
- **Authors**: Anubha Pandey, Ashish Mishra, Vinay Kumar Verma, Anurag Mittal, Hema A. Murthy
- **Comment**: Accepted in WACV'2020
- **Journal**: None
- **Summary**: Conventional approaches to Sketch-Based Image Retrieval (SBIR) assume that the data of all the classes are available during training. The assumption may not always be practical since the data of a few classes may be unavailable, or the classes may not appear at the time of training. Zero-Shot Sketch-Based Image Retrieval (ZS-SBIR) relaxes this constraint and allows the algorithm to handle previously unseen classes during the test. This paper proposes a generative approach based on the Stacked Adversarial Network (SAN) and the advantage of Siamese Network (SN) for ZS-SBIR. While SAN generates a high-quality sample, SN learns a better distance metric compared to that of the nearest neighbor search. The capability of the generative model to synthesize image features based on the sketch reduces the SBIR problem to that of an image-to-image retrieval problem. We evaluate the efficacy of our proposed approach on TU-Berlin, and Sketchy database in both standard ZSL and generalized ZSL setting. The proposed method yields a significant improvement in standard ZSL as well as in a more challenging generalized ZSL setting (GZSL) for SBIR.



### Text-to-Image Generation with Attention Based Recurrent Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2001.06658v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.06658v1)
- **Published**: 2020-01-18 12:19:19+00:00
- **Updated**: 2020-01-18 12:19:19+00:00
- **Authors**: Tehseen Zia, Shahan Arif, Shakeeb Murtaza, Mirza Ahsan Ullah
- **Comment**: None
- **Journal**: None
- **Summary**: Conditional image modeling based on textual descriptions is a relatively new domain in unsupervised learning. Previous approaches use a latent variable model and generative adversarial networks. While the formers are approximated by using variational auto-encoders and rely on the intractable inference that can hamper their performance, the latter is unstable to train due to Nash equilibrium based objective function. We develop a tractable and stable caption-based image generation model. The model uses an attention-based encoder to learn word-to-pixel dependencies. A conditional autoregressive based decoder is used for learning pixel-to-pixel dependencies and generating images. Experimentations are performed on Microsoft COCO, and MNIST-with-captions datasets and performance is evaluated by using the Structural Similarity Index. Results show that the proposed model performs better than contemporary approaches and generate better quality images. Keywords: Generative image modeling, autoregressive image modeling, caption-based image generation, neural attention, recurrent neural networks.



### Multi-View Photometric Stereo: A Robust Solution and Benchmark Dataset for Spatially Varying Isotropic Materials
- **Arxiv ID**: http://arxiv.org/abs/2001.06659v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.06659v1)
- **Published**: 2020-01-18 12:26:22+00:00
- **Updated**: 2020-01-18 12:26:22+00:00
- **Authors**: Min Li, Zhenglong Zhou, Zhe Wu, Boxin Shi, Changyu Diao, Ping Tan
- **Comment**: None
- **Journal**: None
- **Summary**: We present a method to capture both 3D shape and spatially varying reflectance with a multi-view photometric stereo (MVPS) technique that works for general isotropic materials. Our algorithm is suitable for perspective cameras and nearby point light sources. Our data capture setup is simple, which consists of only a digital camera, some LED lights, and an optional automatic turntable. From a single viewpoint, we use a set of photometric stereo images to identify surface points with the same distance to the camera. We collect this information from multiple viewpoints and combine it with structure-from-motion to obtain a precise reconstruction of the complete 3D shape. The spatially varying isotropic bidirectional reflectance distribution function (BRDF) is captured by simultaneously inferring a set of basis BRDFs and their mixing weights at each surface point. In experiments, we demonstrate our algorithm with two different setups: a studio setup for highest precision and a desktop setup for best usability. According to our experiments, under the studio setting, the captured shapes are accurate to 0.5 millimeters and the captured reflectance has a relative root-mean-square error (RMSE) of 9%. We also quantitatively evaluate state-of-the-art MVPS on a newly collected benchmark dataset, which is publicly available for inspiring future research.



### A Transfer Learning Approach to Cross-Modal Object Recognition: From Visual Observation to Robotic Haptic Exploration
- **Arxiv ID**: http://arxiv.org/abs/2001.06673v1
- **DOI**: 10.1109/TRO.2019.2914772
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2001.06673v1)
- **Published**: 2020-01-18 14:47:02+00:00
- **Updated**: 2020-01-18 14:47:02+00:00
- **Authors**: Pietro Falco, Shuang Lu, Ciro Natale, Salvatore Pirozzi, Dongheui Lee
- **Comment**: None
- **Journal**: IEEE Transactions on Robotics ( Volume: 35 , Issue: 4 , Aug. 2019
  )
- **Summary**: In this work, we introduce the problem of cross-modal visuo-tactile object recognition with robotic active exploration. With this term, we mean that the robot observes a set of objects with visual perception and, later on, it is able to recognize such objects only with tactile exploration, without having touched any object before. Using a machine learning terminology, in our application we have a visual training set and a tactile test set, or vice versa. To tackle this problem, we propose an approach constituted by four steps: finding a visuo-tactile common representation, defining a suitable set of features, transferring the features across the domains, and classifying the objects. We show the results of our approach using a set of 15 objects, collecting 40 visual examples and five tactile examples for each object. The proposed approach achieves an accuracy of 94.7%, which is comparable with the accuracy of the monomodal case, i.e., when using visual data both as training set and test set. Moreover, it performs well compared to the human ability, which we have roughly estimated carrying out an experiment with ten participants.



### Evolutionary Neural Architecture Search for Retinal Vessel Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2001.06678v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2001.06678v3)
- **Published**: 2020-01-18 15:07:26+00:00
- **Updated**: 2020-03-18 05:58:25+00:00
- **Authors**: Zhun Fan, Jiahong Wei, Guijie Zhu, Jiajie Mo, Wenji Li
- **Comment**: None
- **Journal**: None
- **Summary**: The accurate retinal vessel segmentation (RVS) is of great significance to assist doctors in the diagnosis of ophthalmology diseases and other systemic diseases. Manually designing a valid neural network architecture for retinal vessel segmentation requires high expertise and a large workload. In order to improve the performance of vessel segmentation and reduce the workload of manually designing neural network, we propose novel approach which applies neural architecture search (NAS) to optimize an encoder-decoder architecture for retinal vessel segmentation. A modified evolutionary algorithm is used to evolve the architectures of encoder-decoder framework with limited computing resources. The evolved model obtained by the proposed approach achieves top performance among all compared methods on the three datasets, namely DRIVE, STARE and CHASE_DB1, but with much fewer parameters. Moreover, the results of cross-training show that the evolved model is with considerable scalability, which indicates a great potential for clinical disease diagnosis.



### Tree-Structured Policy based Progressive Reinforcement Learning for Temporally Language Grounding in Video
- **Arxiv ID**: http://arxiv.org/abs/2001.06680v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.06680v1)
- **Published**: 2020-01-18 15:08:04+00:00
- **Updated**: 2020-01-18 15:08:04+00:00
- **Authors**: Jie Wu, Guanbin Li, Si Liu, Liang Lin
- **Comment**: To appear in AAAI2020
- **Journal**: None
- **Summary**: Temporally language grounding in untrimmed videos is a newly-raised task in video understanding. Most of the existing methods suffer from inferior efficiency, lacking interpretability, and deviating from the human perception mechanism. Inspired by human's coarse-to-fine decision-making paradigm, we formulate a novel Tree-Structured Policy based Progressive Reinforcement Learning (TSP-PRL) framework to sequentially regulate the temporal boundary by an iterative refinement process. The semantic concepts are explicitly represented as the branches in the policy, which contributes to efficiently decomposing complex policies into an interpretable primitive action. Progressive reinforcement learning provides correct credit assignment via two task-oriented rewards that encourage mutual promotion within the tree-structured policy. We extensively evaluate TSP-PRL on the Charades-STA and ActivityNet datasets, and experimental results show that TSP-PRL achieves competitive performance over existing state-of-the-art methods.



### NETNet: Neighbor Erasing and Transferring Network for Better Single Shot Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2001.06690v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.06690v1)
- **Published**: 2020-01-18 15:21:29+00:00
- **Updated**: 2020-01-18 15:21:29+00:00
- **Authors**: Yazhao Li, Yanwei Pang, Jianbing Shen, Jiale Cao, Ling Shao
- **Comment**: 10 pages, 8 figures
- **Journal**: None
- **Summary**: Due to the advantages of real-time detection and improved performance, single-shot detectors have gained great attention recently. To solve the complex scale variations, single-shot detectors make scale-aware predictions based on multiple pyramid layers. However, the features in the pyramid are not scale-aware enough, which limits the detection performance. Two common problems in single-shot detectors caused by object scale variations can be observed: (1) small objects are easily missed; (2) the salient part of a large object is sometimes detected as an object. With this observation, we propose a new Neighbor Erasing and Transferring (NET) mechanism to reconfigure the pyramid features and explore scale-aware features. In NET, a Neighbor Erasing Module (NEM) is designed to erase the salient features of large objects and emphasize the features of small objects in shallow layers. A Neighbor Transferring Module (NTM) is introduced to transfer the erased features and highlight large objects in deep layers. With this mechanism, a single-shot network called NETNet is constructed for scale-aware object detection. In addition, we propose to aggregate nearest neighboring pyramid features to enhance our NET. NETNet achieves 38.5% AP at a speed of 27 FPS and 32.0% AP at a speed of 55 FPS on MS COCO dataset. As a result, NETNet achieves a better trade-off for real-time and accurate object detection.



