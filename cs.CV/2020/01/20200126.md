# Arxiv Papers in cs.CV on 2020-01-26
### Scene Text Recognition With Finer Grid Rectification
- **Arxiv ID**: http://arxiv.org/abs/2001.09389v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.09389v1)
- **Published**: 2020-01-26 02:40:11+00:00
- **Updated**: 2020-01-26 02:40:11+00:00
- **Authors**: Gang Wang
- **Comment**: 6pages,3 figures
- **Journal**: None
- **Summary**: Scene Text Recognition is a challenging problem because of irregular styles and various distortions. This paper proposed an end-to-end trainable model consists of a finer rectification module and a bidirectional attentional recognition network(Firbarn). The rectification module adopts finer grid to rectify the distorted input image and the bidirectional decoder contains only one decoding layer instead of two separated one. Firbarn can be trained in a weak supervised way, only requiring the scene text images and the corresponding word labels. With the flexible rectification and the novel bidirectional decoder, the results of extensive evaluation on the standard benchmarks show Firbarn outperforms previous works, especially on irregular datasets.



### An Overview of Two Age Synthesis and Estimation Techniques
- **Arxiv ID**: http://arxiv.org/abs/2002.03750v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.03750v1)
- **Published**: 2020-01-26 06:07:49+00:00
- **Updated**: 2020-01-26 06:07:49+00:00
- **Authors**: Milad Taleby Ahvanooey, Qianmu Li
- **Comment**: 16 pages, 5 figures. International Congress on Engineering Science
  and Sustainable Urban Development Denmark, Copenhagen, September 2018
- **Journal**: None
- **Summary**: Age estimation is a technique for predicting human ages from digital facial images, which analyzes a person's face image and estimates his/her age based on the year measure. Nowadays, intelligent age estimation and age synthesis have become particularly prevalent research topics in computer vision and face verification systems. Age synthesis is defined to render a facial image aesthetically with rejuvenating and natural aging effects on the person's face. Age estimation is defined to label a facial image automatically with the age group (year range) or the exact age (year) of the person's face. In this case study, we overview the existing models, popular techniques, system performances, and technical challenges related to the facial image-based age synthesis and estimation topics. The main goal of this review is to provide an easy understanding and promising future directions with systematic discussions.



### Curriculum Audiovisual Learning
- **Arxiv ID**: http://arxiv.org/abs/2001.09414v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.09414v1)
- **Published**: 2020-01-26 07:08:47+00:00
- **Updated**: 2020-01-26 07:08:47+00:00
- **Authors**: Di Hu, Zheng Wang, Haoyi Xiong, Dong Wang, Feiping Nie, Dejing Dou
- **Comment**: None
- **Journal**: None
- **Summary**: Associating sound and its producer in complex audiovisual scene is a challenging task, especially when we are lack of annotated training data. In this paper, we present a flexible audiovisual model that introduces a soft-clustering module as the audio and visual content detector, and regards the pervasive property of audiovisual concurrency as the latent supervision for inferring the correlation among detected contents. To ease the difficulty of audiovisual learning, we propose a novel curriculum learning strategy that trains the model from simple to complex scene. We show that such ordered learning procedure rewards the model the merits of easy training and fast convergence. Meanwhile, our audiovisual model can also provide effective unimodal representation and cross-modal alignment performance. We further deploy the well-trained model into practical audiovisual sound localization and separation task. We show that our localization model significantly outperforms existing methods, based on which we show comparable performance in sound separation without referring external visual supervision. Our video demo can be found at https://youtu.be/kuClfGG0cFU.



### Deep Learning-based Image Compression with Trellis Coded Quantization
- **Arxiv ID**: http://arxiv.org/abs/2001.09417v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2001.09417v1)
- **Published**: 2020-01-26 08:00:04+00:00
- **Updated**: 2020-01-26 08:00:04+00:00
- **Authors**: Binglin Li, Mohammad Akbari, Jie Liang, Yang Wang
- **Comment**: Accepted in Data Compression Conference (DCC) 2020
- **Journal**: None
- **Summary**: Recently many works attempt to develop image compression models based on deep learning architectures, where the uniform scalar quantizer (SQ) is commonly applied to the feature maps between the encoder and decoder. In this paper, we propose to incorporate trellis coded quantizer (TCQ) into a deep learning based image compression framework. A soft-to-hard strategy is applied to allow for back propagation during training. We develop a simple image compression model that consists of three subnetworks (encoder, decoder and entropy estimation), and optimize all of the components in an end-to-end manner. We experiment on two high resolution image datasets and both show that our model can achieve superior performance at low bit rates. We also show the comparisons between TCQ and SQ based on our proposed baseline model and demonstrate the advantage of TCQ.



### EEG fingerprinting: subject specific signature based on the aperiodic component of power spectrum
- **Arxiv ID**: http://arxiv.org/abs/2001.09424v1
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/2001.09424v1)
- **Published**: 2020-01-26 09:04:26+00:00
- **Updated**: 2020-01-26 09:04:26+00:00
- **Authors**: Matteo Demuru, Matteo Fraschini
- **Comment**: 11 pages, 2 figures
- **Journal**: None
- **Summary**: During the last few years, there has been growing interest in the effects induced by individual variability on activation patterns and brain connectivity. The practical implications of individual variability is of basic relevance for both group level and subject level studies. The Electroencephalogram (EEG), still represents one of the most used recording techniques to investigate a wide range of brain related features. In this work, we aim to estimate the effect of individual variability on a set of very simple and easily interpretable features extracted from the EEG power spectra. In particular, in an identification scenario, we investigated how the aperiodic (1/f background) component of the EEG power spectra can accurately identify subjects from a large EEG dataset. The results of this study show that the aperiodic component of the EEG signal is characterized by strong subject-specific properties, that this feature is consistent across different experimental conditions (eyes-open and eyes-closed) and outperforms the canonically-defined frequency bands. These findings suggest that the simple features (slope and offset) extracted from the aperiodic component of the EEG signal are sensitive to individual traits and may help to characterize and make inferences at single subject level.



### SDOD:Real-time Segmenting and Detecting 3D Object by Depth
- **Arxiv ID**: http://arxiv.org/abs/2001.09425v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.09425v3)
- **Published**: 2020-01-26 09:06:18+00:00
- **Updated**: 2020-10-24 08:59:00+00:00
- **Authors**: Shengjie Li, Caiyi Xu, Jianping Xing, Yafei Ning, Yonghong Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Most existing instance segmentation methods only focus on improving performance and are not suitable for real-time scenes such as autonomous driving. This paper proposes a real-time framework that segmenting and detecting 3D objects by depth. The framework is composed of two parallel branches: one for instance segmentation and another for object detection. We discretize the objects' depth into depth categories and transform the instance segmentation task into a pixel-level classification task. The Mask branch predicts pixel-level depth categories, and the 3D branch indicates instance-level depth categories. We produce an instance mask by assigning pixels which have the same depth categories to each instance. In addition, to solve the imbalance between mask labels and 3D labels in the KITTI dataset, we introduce a coarse mask generated by the auto-annotation model to increase samples. Experiments on the challenging KITTI dataset show that our approach outperforms LklNet about 1.8 times on the speed of segmentation and 3D detection.



### Point-of-Care Diabetic Retinopathy Diagnosis: A Standalone Mobile Application Approach
- **Arxiv ID**: http://arxiv.org/abs/2002.04066v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2002.04066v1)
- **Published**: 2020-01-26 11:03:16+00:00
- **Updated**: 2020-01-26 11:03:16+00:00
- **Authors**: Misgina Tsighe Hagos
- **Comment**: Dissertation for a Masters of Technology in Data Science
- **Journal**: None
- **Summary**: Although deep learning research and applications have grown rapidly over the past decade, it has shown limitation in healthcare applications and its reachability to people in remote areas. One of the challenges of incorporating deep learning in medical data classification or prediction is the shortage of annotated training data in the healthcare industry. Medical data sharing privacy issues and limited patient population size can be stated as some of the reasons for training data insufficiency in healthcare. Methods to exploit deep learning applications in healthcare have been proposed and implemented in this dissertation.   Traditional diagnosis of diabetic retinopathy requires trained ophthalmologists and expensive imaging equipment to reach healthcare centres in order to provide facilities for treatment of preventable blindness. Diabetic people residing in remote areas with shortage of healthcare services and ophthalmologists usually fail to get periodical diagnosis of diabetic retinopathy thereby facing the probability of vision loss or impairment. Deep learning and mobile application development have been integrated in this dissertation to provide an easy to use point-of-care smartphone based diagnosis of diabetic retinopathy. In order to solve the challenge of shortage of healthcare centres and trained ophthalmologists, the standalone diagnostic service was built so as to be operated by a non-expert without an internet connection. This approach could be transferred to other areas of medical image classification.



### Explainable Artificial Intelligence and Machine Learning: A reality rooted perspective
- **Arxiv ID**: http://arxiv.org/abs/2001.09464v1
- **DOI**: 10.1002/widm.1368
- **Categories**: **cs.AI**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2001.09464v1)
- **Published**: 2020-01-26 15:09:45+00:00
- **Updated**: 2020-01-26 15:09:45+00:00
- **Authors**: Frank Emmert-Streib, Olli Yli-Harja, Matthias Dehmer
- **Comment**: None
- **Journal**: None
- **Summary**: We are used to the availability of big data generated in nearly all fields of science as a consequence of technological progress. However, the analysis of such data possess vast challenges. One of these relates to the explainability of artificial intelligence (AI) or machine learning methods. Currently, many of such methods are non-transparent with respect to their working mechanism and for this reason are called black box models, most notably deep learning methods. However, it has been realized that this constitutes severe problems for a number of fields including the health sciences and criminal justice and arguments have been brought forward in favor of an explainable AI. In this paper, we do not assume the usual perspective presenting explainable AI as it should be, but rather we provide a discussion what explainable AI can be. The difference is that we do not present wishful thinking but reality grounded properties in relation to a scientific theory beyond physics.



### Brain Metastasis Segmentation Network Trained with Robustness to Annotations with Multiple False Negatives
- **Arxiv ID**: http://arxiv.org/abs/2001.09501v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2001.09501v1)
- **Published**: 2020-01-26 19:23:07+00:00
- **Updated**: 2020-01-26 19:23:07+00:00
- **Authors**: Darvin Yi, Endre Grøvik, Michael Iv, Elizabeth Tong, Greg Zaharchuk, Daniel Rubin
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning has proven to be an essential tool for medical image analysis. However, the need for accurately labeled input data, often requiring time- and labor-intensive annotation by experts, is a major limitation to the use of deep learning. One solution to this challenge is to allow for use of coarse or noisy labels, which could permit more efficient and scalable labeling of images. In this work, we develop a lopsided loss function based on entropy regularization that assumes the existence of a nontrivial false negative rate in the target annotations. Starting with a carefully annotated brain metastasis lesion dataset, we simulate data with false negatives by (1) randomly censoring the annotated lesions and (2) systematically censoring the smallest lesions. The latter better models true physician error because smaller lesions are harder to notice than the larger ones. Even with a simulated false negative rate as high as 50%, applying our loss function to randomly censored data preserves maximum sensitivity at 97% of the baseline with uncensored training data, compared to just 10% for a standard loss function. For the size-based censorship, performance is restored from 17% with the current standard to 88% with our lopsided bootstrap loss. Our work will enable more efficient scaling of the image labeling process, in parallel with other approaches on creating more efficient user interfaces and tools for annotation.



### Unsupervised Disentanglement of Pose, Appearance and Background from Images and Videos
- **Arxiv ID**: http://arxiv.org/abs/2001.09518v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.09518v1)
- **Published**: 2020-01-26 20:59:47+00:00
- **Updated**: 2020-01-26 20:59:47+00:00
- **Authors**: Aysegul Dundar, Kevin J. Shih, Animesh Garg, Robert Pottorf, Andrew Tao, Bryan Catanzaro
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised landmark learning is the task of learning semantic keypoint-like representations without the use of expensive input keypoint-level annotations. A popular approach is to factorize an image into a pose and appearance data stream, then to reconstruct the image from the factorized components. The pose representation should capture a set of consistent and tightly localized landmarks in order to facilitate reconstruction of the input image. Ultimately, we wish for our learned landmarks to focus on the foreground object of interest. However, the reconstruction task of the entire image forces the model to allocate landmarks to model the background. This work explores the effects of factorizing the reconstruction task into separate foreground and background reconstructions, conditioning only the foreground reconstruction on the unsupervised landmarks. Our experiments demonstrate that the proposed factorization results in landmarks that are focused on the foreground object of interest. Furthermore, the rendered background quality is also improved, as the background rendering pipeline no longer requires the ill-suited landmarks to model its pose and appearance. We demonstrate this improvement in the context of the video-prediction task.



### Abdominal multi-organ segmentation with cascaded convolutional and adversarial deep networks
- **Arxiv ID**: http://arxiv.org/abs/2001.09521v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2001.09521v1)
- **Published**: 2020-01-26 21:28:04+00:00
- **Updated**: 2020-01-26 21:28:04+00:00
- **Authors**: Pierre-Henri Conze, Ali Emre Kavur, Emilie Cornec-Le Gall, Naciye Sinem Gezer, Yannick Le Meur, M. Alper Selver, François Rousseau
- **Comment**: None
- **Journal**: None
- **Summary**: Objective : Abdominal anatomy segmentation is crucial for numerous applications from computer-assisted diagnosis to image-guided surgery. In this context, we address fully-automated multi-organ segmentation from abdominal CT and MR images using deep learning. Methods: The proposed model extends standard conditional generative adversarial networks. Additionally to the discriminator which enforces the model to create realistic organ delineations, it embeds cascaded partially pre-trained convolutional encoder-decoders as generator. Encoder fine-tuning from a large amount of non-medical images alleviates data scarcity limitations. The network is trained end-to-end to benefit from simultaneous multi-level segmentation refinements using auto-context. Results : Employed for healthy liver, kidneys and spleen segmentation, our pipeline provides promising results by outperforming state-of-the-art encoder-decoder schemes. Followed for the Combined Healthy Abdominal Organ Segmentation (CHAOS) challenge organized in conjunction with the IEEE International Symposium on Biomedical Imaging 2019, it gave us the first rank for three competition categories: liver CT, liver MR and multi-organ MR segmentation. Conclusion : Combining cascaded convolutional and adversarial networks strengthens the ability of deep learning pipelines to automatically delineate multiple abdominal organs, with good generalization capability. Significance : The comprehensive evaluation provided suggests that better guidance could be achieved to help clinicians in abdominal image interpretation and clinical decision making.



### Markov-Chain Monte Carlo Approximation of the Ideal Observer using Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2001.09526v1
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2001.09526v1)
- **Published**: 2020-01-26 21:51:08+00:00
- **Updated**: 2020-01-26 21:51:08+00:00
- **Authors**: Weimin Zhou, Mark A. Anastasio
- **Comment**: SPIE Medical Imaging 2020
- **Journal**: None
- **Summary**: The Ideal Observer (IO) performance has been advocated when optimizing medical imaging systems for signal detection tasks. However, analytical computation of the IO test statistic is generally intractable. To approximate the IO test statistic, sampling-based methods that employ Markov-Chain Monte Carlo (MCMC) techniques have been developed. However, current applications of MCMC techniques have been limited to several object models such as a lumpy object model and a binary texture model, and it remains unclear how MCMC methods can be implemented with other more sophisticated object models. Deep learning methods that employ generative adversarial networks (GANs) hold great promise to learn stochastic object models (SOMs) from image data. In this study, we described a method to approximate the IO by applying MCMC techniques to SOMs learned by use of GANs. The proposed method can be employed with arbitrary object models that can be learned by use of GANs, thereby the domain of applicability of MCMC techniques for approximating the IO performance is extended. In this study, both signal-known-exactly (SKE) and signal-known-statistically (SKS) binary signal detection tasks are considered. The IO performance computed by the proposed method is compared to that computed by the conventional MCMC method. The advantages of the proposed method are discussed.



### Imperfect ImaGANation: Implications of GANs Exacerbating Biases on Facial Data Augmentation and Snapchat Selfie Lenses
- **Arxiv ID**: http://arxiv.org/abs/2001.09528v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2001.09528v3)
- **Published**: 2020-01-26 21:57:26+00:00
- **Updated**: 2021-06-16 02:13:46+00:00
- **Authors**: Niharika Jain, Alberto Olmo, Sailik Sengupta, Lydia Manikonda, Subbarao Kambhampati
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we show that popular Generative Adversarial Networks (GANs) exacerbate biases along the axes of gender and skin tone when given a skewed distribution of face-shots. While practitioners celebrate synthetic data generation using GANs as an economical way to augment data for training data-hungry machine learning models, it is unclear whether they recognize the perils of such techniques when applied to real world datasets biased along latent dimensions. Specifically, we show that (1) traditional GANs further skew the distribution of a dataset consisting of engineering faculty headshots, generating minority modes less often and of worse quality and (2) image-to-image translation (conditional) GANs also exacerbate biases by lightening skin color of non-white faces and transforming female facial features to be masculine when generating faces of engineering professors. Thus, our study is meant to serve as a cautionary tale.



### Using Simulated Data to Generate Images of Climate Change
- **Arxiv ID**: http://arxiv.org/abs/2001.09531v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2001.09531v1)
- **Published**: 2020-01-26 22:19:13+00:00
- **Updated**: 2020-01-26 22:19:13+00:00
- **Authors**: Gautier Cosne, Adrien Juraver, Mélisande Teng, Victor Schmidt, Vahe Vardanyan, Alexandra Luccioni, Yoshua Bengio
- **Comment**: Proceeding ML-IRL workshop at ICLR 2020
- **Journal**: None
- **Summary**: Generative adversarial networks (GANs) used in domain adaptation tasks have the ability to generate images that are both realistic and personalized, transforming an input image while maintaining its identifiable characteristics. However, they often require a large quantity of training data to produce high-quality images in a robust way, which limits their usability in cases when access to data is limited. In our paper, we explore the potential of using images from a simulated 3D environment to improve a domain adaptation task carried out by the MUNIT architecture, aiming to use the resulting images to raise awareness of the potential future impacts of climate change.



### Visualisation of Medical Image Fusion and Translation for Accurate Diagnosis of High Grade Gliomas
- **Arxiv ID**: http://arxiv.org/abs/2001.09535v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.09535v3)
- **Published**: 2020-01-26 22:49:14+00:00
- **Updated**: 2020-01-30 13:35:39+00:00
- **Authors**: Nishant Kumar, Nico Hoffmann, Matthias Kirsch, Stefan Gumhold
- **Comment**: 5 pages, 3 figures, IEEE International Symposium on Biomedical
  Imaging (IEEE ISBI 2020)
- **Journal**: None
- **Summary**: The medical image fusion combines two or more modalities into a single view while medical image translation synthesizes new images and assists in data augmentation. Together, these methods help in faster diagnosis of high grade malignant gliomas. However, they might be untrustworthy due to which neurosurgeons demand a robust visualisation tool to verify the reliability of the fusion and translation results before they make pre-operative surgical decisions. In this paper, we propose a novel approach to compute a confidence heat map between the source-target image pair by estimating the information transfer from the source to the target image using the joint probability distribution of the two images. We evaluate several fusion and translation methods using our visualisation procedure and showcase its robustness in enabling neurosurgeons to make finer clinical decisions.



### Weakly Supervised Few-shot Object Segmentation using Co-Attention with Visual and Semantic Embeddings
- **Arxiv ID**: http://arxiv.org/abs/2001.09540v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.09540v3)
- **Published**: 2020-01-26 23:56:26+00:00
- **Updated**: 2020-05-17 17:08:51+00:00
- **Authors**: Mennatullah Siam, Naren Doraiswamy, Boris N. Oreshkin, Hengshuai Yao, Martin Jagersand
- **Comment**: Accepted to IJCAI'20. The first three authors listed contributed
  equally
- **Journal**: None
- **Summary**: Significant progress has been made recently in developing few-shot object segmentation methods. Learning is shown to be successful in few-shot segmentation settings, using pixel-level, scribbles and bounding box supervision. This paper takes another approach, i.e., only requiring image-level label for few-shot object segmentation. We propose a novel multi-modal interaction module for few-shot object segmentation that utilizes a co-attention mechanism using both visual and word embedding. Our model using image-level labels achieves 4.8% improvement over previously proposed image-level few-shot object segmentation. It also outperforms state-of-the-art methods that use weak bounding box supervision on PASCAL-5i. Our results show that few-shot segmentation benefits from utilizing word embeddings, and that we are able to perform few-shot segmentation using stacked joint visual semantic processing with weak image-level labels. We further propose a novel setup, Temporal Object Segmentation for Few-shot Learning (TOSFL) for videos. TOSFL can be used on a variety of public video data such as Youtube-VOS, as demonstrated in both instance-level and category-level TOSFL experiments.



