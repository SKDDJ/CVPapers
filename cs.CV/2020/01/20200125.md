# Arxiv Papers in cs.CV on 2020-01-25
### PSC-Net: Learning Part Spatial Co-occurrence for Occluded Pedestrian Detection
- **Arxiv ID**: http://arxiv.org/abs/2001.09252v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.09252v2)
- **Published**: 2020-01-25 02:03:17+00:00
- **Updated**: 2020-03-10 13:19:15+00:00
- **Authors**: Jin Xie, Yanwei Pang, Hisham Cholakkal, Rao Muhammad Anwer, Fahad Shahbaz Khan, Ling Shao
- **Comment**: None
- **Journal**: None
- **Summary**: Detecting pedestrians, especially under heavy occlusions, is a challenging computer vision problem with numerous real-world applications. This paper introduces a novel approach, termed as PSC-Net, for occluded pedestrian detection. The proposed PSC-Net contains a dedicated module that is designed to explicitly capture both inter and intra-part co-occurrence information of different pedestrian body parts through a Graph Convolutional Network (GCN). Both inter and intra-part co-occurrence information contribute towards improving the feature representation for handling varying level of occlusions, ranging from partial to severe occlusions. Our PSC-Net exploits the topological structure of pedestrian and does not require part-based annotations or additional visible bounding-box (VBB) information to learn part spatial co-occurrence. Comprehensive experiments are performed on two challenging datasets: CityPersons and Caltech datasets. The proposed PSC-Net achieves state-of-the-art detection performance on both. On the heavy occluded (\textbf{HO}) set of CityPerosns test set, our PSC-Net obtains an absolute gain of 4.0\% in terms of log-average miss rate over the state-of-the-art with same backbone, input scale and without using additional VBB supervision. Further, PSC-Net improves the state-of-the-art from 37.9 to 34.8 in terms of log-average miss rate on Caltech (\textbf{HO}) test set.



### On the Role of Receptive Field in Unsupervised Sim-to-Real Image Translation
- **Arxiv ID**: http://arxiv.org/abs/2001.09257v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.09257v1)
- **Published**: 2020-01-25 03:02:12+00:00
- **Updated**: 2020-01-25 03:02:12+00:00
- **Authors**: Nikita Jaipuria, Shubh Gupta, Praveen Narayanan, Vidya N. Murali
- **Comment**: Machine Learning for Autonomous Driving Workshop at the 33rd
  Conference on Neural Information Processing Systems (NeurIPS 2019),
  Vancouver, Canada
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) are now widely used for photo-realistic image synthesis. In applications where a simulated image needs to be translated into a realistic image (sim-to-real), GANs trained on unpaired data from the two domains are susceptible to failure in semantic content retention as the image is translated from one domain to the other. This failure mode is more pronounced in cases where the real data lacks content diversity, resulting in a content \emph{mismatch} between the two domains - a situation often encountered in real-world deployment. In this paper, we investigate the role of the discriminator's receptive field in GANs for unsupervised image-to-image translation with mismatched data, and study its effect on semantic content retention. Experiments with the discriminator architecture of a state-of-the-art coupled Variational Auto-Encoder (VAE) - GAN model on diverse, mismatched datasets show that the discriminator receptive field is directly correlated with semantic content discrepancy of the generated image.



### Towards High Performance Low Complexity Calibration in Appearance Based Gaze Estimation
- **Arxiv ID**: http://arxiv.org/abs/2001.09284v2
- **DOI**: 10.1109/TPAMI.2022.3148386
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.09284v2)
- **Published**: 2020-01-25 09:30:06+00:00
- **Updated**: 2022-02-13 14:58:42+00:00
- **Authors**: Zhaokang Chen, Bertram E. Shi
- **Comment**: Accepted by IEEE Transactions on Pattern Analysis and Machine
  Intelligence (TPAMI)
- **Journal**: None
- **Summary**: Appearance-based gaze estimation from RGB images provides relatively unconstrained gaze tracking. We have previously proposed a gaze decomposition method that decomposes the gaze angle into the sum of a subject-independent gaze estimate from the image and a subject-dependent bias. This paper extends that work with a more complete characterization of the interplay between the complexity of the calibration dataset and estimation accuracy. We analyze the effect of the number of gaze targets, the number of images used per gaze target and the number of head positions in calibration data using a new NISLGaze dataset, which is well suited for analyzing these effects as it includes more diversity in head positions and orientations for each subject than other datasets. A better understanding of these factors enables low complexity high performance calibration. Our results indicate that using only a single gaze target and single head position is sufficient to achieve high quality calibration, outperforming state-of-the-art methods by more than 6.3%. One of the surprising findings is that the same estimator yields the best performance both with and without calibration. To better understand the reasons, we provide a new theoretical analysis that specifies the conditions under which this can be expected.



### Look Closer to Ground Better: Weakly-Supervised Temporal Grounding of Sentence in Video
- **Arxiv ID**: http://arxiv.org/abs/2001.09308v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.09308v1)
- **Published**: 2020-01-25 13:07:43+00:00
- **Updated**: 2020-01-25 13:07:43+00:00
- **Authors**: Zhenfang Chen, Lin Ma, Wenhan Luo, Peng Tang, Kwan-Yee K. Wong
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we study the problem of weakly-supervised temporal grounding of sentence in video. Specifically, given an untrimmed video and a query sentence, our goal is to localize a temporal segment in the video that semantically corresponds to the query sentence, with no reliance on any temporal annotation during training. We propose a two-stage model to tackle this problem in a coarse-to-fine manner. In the coarse stage, we first generate a set of fixed-length temporal proposals using multi-scale sliding windows, and match their visual features against the sentence features to identify the best-matched proposal as a coarse grounding result. In the fine stage, we perform a fine-grained matching between the visual features of the frames in the best-matched proposal and the sentence features to locate the precise frame boundary of the fine grounding result. Comprehensive experiments on the ActivityNet Captions dataset and the Charades-STA dataset demonstrate that our two-stage model achieves compelling performance.



### Domain Adaptive Medical Image Segmentation via Adversarial Learning of Disease-Specific Spatial Patterns
- **Arxiv ID**: http://arxiv.org/abs/2001.09313v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.09313v3)
- **Published**: 2020-01-25 13:48:02+00:00
- **Updated**: 2020-08-11 12:28:09+00:00
- **Authors**: Hongwei Li, Timo Loehr, Anjany Sekuboyina, Jianguo Zhang, Benedikt Wiestler, Bjoern Menze
- **Comment**: submitted to a journal and under review
- **Journal**: None
- **Summary**: In medical imaging, the heterogeneity of multi-centre data impedes the applicability of deep learning-based methods and results in significant performance degradation when applying models in an unseen data domain, e.g. a new centreor a new scanner. In this paper, we propose an unsupervised domain adaptation framework for boosting image segmentation performance across multiple domains without using any manual annotations from the new target domains, but by re-calibrating the networks on few images from the target domain. To achieve this, we enforce architectures to be adaptive to new data by rejecting improbable segmentation patterns and implicitly learning through semantic and boundary information, thus to capture disease-specific spatial patterns in an adversarial optimization. The adaptation process needs continuous monitoring, however, as we cannot assume the presence of ground-truth masks for the target domain, we propose two new metrics to monitor the adaptation process, and strategies to train the segmentation algorithm in a stable fashion. We build upon well-established 2D and 3D architectures and perform extensive experiments on three cross-centre brain lesion segmentation tasks, involving multicentre public and in-house datasets. We demonstrate that recalibrating the deep networks on a few unlabeled images from the target domain improves the segmentation accuracy significantly.



### Learning Canonical Shape Space for Category-Level 6D Object Pose and Size Estimation
- **Arxiv ID**: http://arxiv.org/abs/2001.09322v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.09322v3)
- **Published**: 2020-01-25 14:16:17+00:00
- **Updated**: 2021-11-21 08:38:17+00:00
- **Authors**: Dengsheng Chen, Jun Li, Zheng Wang, Kai Xu
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel approach to category-level 6D object pose and size estimation. To tackle intra-class shape variations, we learn canonical shape space (CASS), a unified representation for a large variety of instances of a certain object category. In particular, CASS is modeled as the latent space of a deep generative model of canonical 3D shapes with normalized pose. We train a variational auto-encoder (VAE) for generating 3D point clouds in the canonical space from an RGBD image. The VAE is trained in a cross-category fashion, exploiting the publicly available large 3D shape repositories. Since the 3D point cloud is generated in normalized pose (with actual size), the encoder of the VAE learns view-factorized RGBD embedding. It maps an RGBD image in arbitrary view into a pose-independent 3D shape representation. Object pose is then estimated via contrasting it with a pose-dependent feature of the input RGBD extracted with a separate deep neural networks. We integrate the learning of CASS and pose and size estimation into an end-to-end trainable network, achieving the state-of-the-art performance.



### An Empirical Study of Person Re-Identification with Attributes
- **Arxiv ID**: http://arxiv.org/abs/2002.03752v1
- **DOI**: 10.1109/RO-MAN46459.2019.8956459
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2002.03752v1)
- **Published**: 2020-01-25 22:18:51+00:00
- **Updated**: 2020-01-25 22:18:51+00:00
- **Authors**: Vikram Shree, Wei-Lun Chao, Mark Campbell
- **Comment**: Accepted by RO-MAN 2019, 28th IEEE International Conference on Robot
  and Human Interactive Communication (RO-MAN). IEEE, 2019
- **Journal**: None
- **Summary**: Person re-identification aims to identify a person from an image collection, given one image of that person as the query. There is, however, a plethora of real-life scenarios where we may not have a priori library of query images and therefore must rely on information from other modalities. In this paper, an attribute-based approach is proposed where the person of interest (POI) is described by a set of visual attributes, which are used to perform the search. We compare multiple algorithms and analyze how the quality of attributes impacts the performance. While prior work mostly relies on high precision attributes annotated by experts, we conduct a human-subject study and reveal that certain visual attributes could not be consistently described by human observers, making them less reliable in real applications. A key conclusion is that the performance achieved by non-expert attributes, instead of expert-annotated ones, is a more faithful indicator of the status quo of attribute-based approaches for person re-identification.



### Learning Preference-Based Similarities from Face Images using Siamese Multi-Task CNNs
- **Arxiv ID**: http://arxiv.org/abs/2001.09371v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.09371v1)
- **Published**: 2020-01-25 23:08:12+00:00
- **Updated**: 2020-01-25 23:08:12+00:00
- **Authors**: Nils Gessert, Alexander Schlaefer
- **Comment**: None
- **Journal**: None
- **Summary**: Online dating has become a common occurrence over the last few decades. A key challenge for online dating platforms is to determine suitable matches for their users. A lot of dating services rely on self-reported user traits and preferences for matching. At the same time, some services largely rely on user images and thus initial visual preference. Especially for the latter approach, previous research has attempted to capture users' visual preferences for automatic match recommendation. These approaches are mostly based on the assumption that physical attraction is the key factor for relationship formation and personal preferences, interests, and attitude are largely neglected. Deep learning approaches have shown that a variety of properties can be predicted from human faces to some degree, including age, health and even personality traits. Therefore, we investigate the feasibility of bridging image-based matching and matching with personal interests, preferences, and attitude. We approach the problem in a supervised manner by predicting similarity scores between two users based on images of their faces only. The ground-truth for the similarity matching scores is determined by a test that aims to capture users' preferences, interests, and attitude that are relevant for forming romantic relationships. The images are processed by a Siamese Multi-Task deep learning architecture. We find a statistically significant correlation between predicted and target similarity scores. Thus, our results indicate that learning similarities in terms of interests, preferences, and attitude from face images appears to be feasible to some degree.



### Weighted Average Precision: Adversarial Example Detection in the Visual Perception of Autonomous Vehicles
- **Arxiv ID**: http://arxiv.org/abs/2002.03751v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2002.03751v2)
- **Published**: 2020-01-25 23:59:34+00:00
- **Updated**: 2020-05-04 00:50:00+00:00
- **Authors**: Yilan Li, Senem Velipasalar
- **Comment**: None
- **Journal**: None
- **Summary**: Recent works have shown that neural networks are vulnerable to carefully crafted adversarial examples (AE). By adding small perturbations to input images, AEs are able to make the victim model predicts incorrect outputs. Several research work in adversarial machine learning started to focus on the detection of AEs in autonomous driving. However, the existing studies either use preliminary assumption on outputs of detections or ignore the tracking system in the perception pipeline. In this paper, we firstly propose a novel distance metric for practical autonomous driving object detection outputs. Then, we bridge the gap between the current AE detection research and the real-world autonomous systems by providing a temporal detection algorithm, which takes the impact of tracking system into consideration. We perform evaluation on Berkeley Deep Drive (BDD) and CityScapes datasets to show how our approach outperforms existing single-frame-mAP based AE detections by increasing 17.76% accuracy of performance.



