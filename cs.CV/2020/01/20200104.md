# Arxiv Papers in cs.CV on 2020-01-04
### Distributed Stochastic Algorithms for High-rate Streaming Principal Component Analysis
- **Arxiv ID**: http://arxiv.org/abs/2001.01017v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.SP, math.OC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2001.01017v1)
- **Published**: 2020-01-04 00:46:47+00:00
- **Updated**: 2020-01-04 00:46:47+00:00
- **Authors**: Haroon Raja, Waheed U. Bajwa
- **Comment**: 37 pages, 11 figures; preprint of a journal submission
- **Journal**: None
- **Summary**: This paper considers the problem of estimating the principal eigenvector of a covariance matrix from independent and identically distributed data samples in streaming settings. The streaming rate of data in many contemporary applications can be high enough that a single processor cannot finish an iteration of existing methods for eigenvector estimation before a new sample arrives. This paper formulates and analyzes a distributed variant of the classical Krasulina's method (D-Krasulina) that can keep up with the high streaming rate of data by distributing the computational load across multiple processing nodes. The analysis shows that---under appropriate conditions---D-Krasulina converges to the principal eigenvector in an order-wise optimal manner; i.e., after receiving $M$ samples across all nodes, its estimation error can be $O(1/M)$. In order to reduce the network communication overhead, the paper also develops and analyzes a mini-batch extension of D-Krasulina, which is termed DM-Krasulina. The analysis of DM-Krasulina shows that it can also achieve order-optimal estimation error rates under appropriate conditions, even when some samples have to be discarded within the network due to communication latency. Finally, experiments are performed over synthetic and real-world data to validate the convergence behaviors of D-Krasulina and DM-Krasulina in high-rate streaming settings.



### Painting Many Pasts: Synthesizing Time Lapse Videos of Paintings
- **Arxiv ID**: http://arxiv.org/abs/2001.01026v2
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2001.01026v2)
- **Published**: 2020-01-04 03:12:38+00:00
- **Updated**: 2020-04-25 22:20:46+00:00
- **Authors**: Amy Zhao, Guha Balakrishnan, Kathleen M. Lewis, Fr√©do Durand, John V. Guttag, Adrian V. Dalca
- **Comment**: 10 pages, CVPR 2020
- **Journal**: None
- **Summary**: We introduce a new video synthesis task: synthesizing time lapse videos depicting how a given painting might have been created. Artists paint using unique combinations of brushes, strokes, and colors. There are often many possible ways to create a given painting. Our goal is to learn to capture this rich range of possibilities.   Creating distributions of long-term videos is a challenge for learning-based video synthesis methods. We present a probabilistic model that, given a single image of a completed painting, recurrently synthesizes steps of the painting process. We implement this model as a convolutional neural network, and introduce a novel training scheme to enable learning from a limited dataset of painting time lapses. We demonstrate that this model can be used to sample many time steps, enabling long-term stochastic video synthesis. We evaluate our method on digital and watercolor paintings collected from video websites, and show that human raters find our synthetic videos to be similar to time lapse videos produced by real artists. Our code is available at https://xamyzhao.github.io/timecraft.



### Visual Semantic SLAM with Landmarks for Large-Scale Outdoor Environment
- **Arxiv ID**: http://arxiv.org/abs/2001.01028v1
- **DOI**: 10.1109/CCHI.2019.8901910
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2001.01028v1)
- **Published**: 2020-01-04 03:34:23+00:00
- **Updated**: 2020-01-04 03:34:23+00:00
- **Authors**: Zirui Zhao, Yijun Mao, Yan Ding, Pengju Ren, Nanning Zheng
- **Comment**: Accepted by 2019 China Symposium on Cognitive Computing and Hybrid
  Intelligence(CCHI'19)
- **Journal**: None
- **Summary**: Semantic SLAM is an important field in autonomous driving and intelligent agents, which can enable robots to achieve high-level navigation tasks, obtain simple cognition or reasoning ability and achieve language-based human-robot-interaction. In this paper, we built a system to creat a semantic 3D map by combining 3D point cloud from ORB SLAM with semantic segmentation information from Convolutional Neural Network model PSPNet-101 for large-scale environments. Besides, a new dataset for KITTI sequences has been built, which contains the GPS information and labels of landmarks from Google Map in related streets of the sequences. Moreover, we find a way to associate the real-world landmark with point cloud map and built a topological map based on semantic map.



### Grab: Fast and Accurate Sensor Processing for Cashier-Free Shopping
- **Arxiv ID**: http://arxiv.org/abs/2001.01033v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/2001.01033v1)
- **Published**: 2020-01-04 04:12:06+00:00
- **Updated**: 2020-01-04 04:12:06+00:00
- **Authors**: Xiaochen Liu, Yurong Jiang, Kyu-Han Kim, Ramesh Govindan
- **Comment**: None
- **Journal**: None
- **Summary**: Cashier-free shopping systems like Amazon Go improve shopping experience, but can require significant store redesign. In this paper, we propose Grab, a practical system that leverages existing infrastructure and devices to enable cashier-free shopping. Grab needs to accurately identify and track customers, and associate each shopper with items he or she retrieves from shelves. To do this, it uses a keypoint-based pose tracker as a building block for identification and tracking, develops robust feature-based face trackers, and algorithms for associating and tracking arm movements. It also uses a probabilistic framework to fuse readings from camera, weight and RFID sensors in order to accurately assess which shopper picks up which item. In experiments from a pilot deployment in a retail store, Grab can achieve over 90% precision and recall even when 40% of shopping actions are designed to confuse the system. Moreover, Grab has optimizations that help reduce investment in computing infrastructure four-fold.



### FrequentNet: A Novel Interpretable Deep Learning Model for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2001.01034v4
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2001.01034v4)
- **Published**: 2020-01-04 04:31:32+00:00
- **Updated**: 2021-08-12 03:41:36+00:00
- **Authors**: Yifei Li, Kuangyan Song, Yiming Sun, Liao Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: This paper has proposed a new baseline deep learning model of more benefits for image classification. Different from the convolutional neural network(CNN) practice where filters are trained by back propagation to represent different patterns of an image, we are inspired by a method called "PCANet" in "PCANet: A Simple Deep Learning Baseline for Image Classification?" to choose filter vectors from basis vectors in frequency domain like Fourier coefficients or wavelets without back propagation. Researchers have demonstrated that those basis in frequency domain can usually provide physical insights, which adds to the interpretability of the model by analyzing the frequencies selected. Besides, the training process will also be more time efficient, mathematically clear and interpretable compared with the "black-box" training process of CNN.



### Explain and Improve: LRP-Inference Fine-Tuning for Image Captioning Models
- **Arxiv ID**: http://arxiv.org/abs/2001.01037v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2001.01037v5)
- **Published**: 2020-01-04 05:15:11+00:00
- **Updated**: 2021-08-01 06:27:04+00:00
- **Authors**: Jiamei Sun, Sebastian Lapuschkin, Wojciech Samek, Alexander Binder
- **Comment**: None
- **Journal**: None
- **Summary**: This paper analyzes the predictions of image captioning models with attention mechanisms beyond visualizing the attention itself. We develop variants of layer-wise relevance propagation (LRP) and gradient-based explanation methods, tailored to image captioning models with attention mechanisms. We compare the interpretability of attention heatmaps systematically against the explanations provided by explanation methods such as LRP, Grad-CAM, and Guided Grad-CAM. We show that explanation methods provide simultaneously pixel-wise image explanations (supporting and opposing pixels of the input image) and linguistic explanations (supporting and opposing words of the preceding sequence) for each word in the predicted captions. We demonstrate with extensive experiments that explanation methods 1) can reveal additional evidence used by the model to make decisions compared to attention; 2) correlate to object locations with high precision; 3) are helpful to "debug" the model, e.g. by analyzing the reasons for hallucinated object words. With the observed properties of explanations, we further design an LRP-inference fine-tuning strategy that reduces the issue of object hallucination in image captioning models, and meanwhile, maintains the sentence fluency. We conduct experiments with two widely used attention mechanisms: the adaptive attention mechanism calculated with the additive attention and the multi-head attention mechanism calculated with the scaled dot product.



### Adversarial-Learned Loss for Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2001.01046v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2001.01046v1)
- **Published**: 2020-01-04 06:24:44+00:00
- **Updated**: 2020-01-04 06:24:44+00:00
- **Authors**: Minghao Chen, Shuai Zhao, Haifeng Liu, Deng Cai
- **Comment**: Published in 34th AAAI Conference on Artificial Intelligence, 2020
- **Journal**: None
- **Summary**: Recently, remarkable progress has been made in learning transferable representation across domains. Previous works in domain adaptation are majorly based on two techniques: domain-adversarial learning and self-training. However, domain-adversarial learning only aligns feature distributions between domains but does not consider whether the target features are discriminative. On the other hand, self-training utilizes the model predictions to enhance the discrimination of target features, but it is unable to explicitly align domain distributions. In order to combine the strengths of these two methods, we propose a novel method called Adversarial-Learned Loss for Domain Adaptation (ALDA). We first analyze the pseudo-label method, a typical self-training method. Nevertheless, there is a gap between pseudo-labels and the ground truth, which can cause incorrect training. Thus we introduce the confusion matrix, which is learned through an adversarial manner in ALDA, to reduce the gap and align the feature distributions. Finally, a new loss function is auto-constructed from the learned confusion matrix, which serves as the loss for unlabeled target samples. Our ALDA outperforms state-of-the-art approaches in four standard domain adaptation datasets. Our code is available at https://github.com/ZJULearning/ALDA.



### Discrimination-aware Network Pruning for Deep Model Compression
- **Arxiv ID**: http://arxiv.org/abs/2001.01050v2
- **DOI**: 10.1109/TPAMI.2021.3066410
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.01050v2)
- **Published**: 2020-01-04 07:07:41+00:00
- **Updated**: 2021-03-29 15:52:18+00:00
- **Authors**: Jing Liu, Bohan Zhuang, Zhuangwei Zhuang, Yong Guo, Junzhou Huang, Jinhui Zhu, Mingkui Tan
- **Comment**: 14 pages. Extended version of the NeurIPS paper arXiv:1810.11809
- **Journal**: None
- **Summary**: We study network pruning which aims to remove redundant channels/kernels and hence speed up the inference of deep networks. Existing pruning methods either train from scratch with sparsity constraints or minimize the reconstruction error between the feature maps of the pre-trained models and the compressed ones. Both strategies suffer from some limitations: the former kind is computationally expensive and difficult to converge, while the latter kind optimizes the reconstruction error but ignores the discriminative power of channels. In this paper, we propose a simple-yet-effective method called discrimination-aware channel pruning (DCP) to choose the channels that actually contribute to the discriminative power. Note that a channel often consists of a set of kernels. Besides the redundancy in channels, some kernels in a channel may also be redundant and fail to contribute to the discriminative power of the network, resulting in kernel level redundancy. To solve this, we propose a discrimination-aware kernel pruning (DKP) method to further compress deep networks by removing redundant kernels. To prevent DCP/DKP from selecting redundant channels/kernels, we propose a new adaptive stopping condition, which helps to automatically determine the number of selected channels/kernels and often results in more compact models with better performance. Extensive experiments on both image classification and face recognition demonstrate the effectiveness of our methods. For example, on ILSVRC-12, the resultant ResNet-50 model with 30% reduction of channels even outperforms the baseline model by 0.36% in terms of Top-1 accuracy. The pruned MobileNetV1 and MobileNetV2 achieve 1.93x and 1.42x inference acceleration on a mobile device, respectively, with negligible performance degradation. The source code and the pre-trained models are available at https://github.com/SCUT-AILab/DCP.



### Image Speckle Noise Denoising by a Multi-Layer Fusion Enhancement Method based on Block Matching and 3D Filtering
- **Arxiv ID**: http://arxiv.org/abs/2001.01055v1
- **DOI**: 10.1080/13682199.2019.1612589
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2001.01055v1)
- **Published**: 2020-01-04 08:17:52+00:00
- **Updated**: 2020-01-04 08:17:52+00:00
- **Authors**: Huang Shuo, Zhou Ping, Shi Hao, Sun Yu, Wan Suiren
- **Comment**: None
- **Journal**: The Imaging Science Journal, 67(4), 224-235 (2019)
- **Summary**: In order to improve speckle noise denoising of block matching 3d filtering (BM3D) method, an image frequency-domain multi-layer fusion enhancement method (MLFE-BM3D) based on nonsubsampled contourlet transform (NSCT) has been proposed. The method designs a NSCT hard threshold denoising enhancement to preprocess the image, then uses fusion enhancement in NSCT domain to fuse the preliminary estimation results of images before and after the NSCT hard threshold denoising, finally, BM3D denoising is carried out with the fused image to obtain the final denoising result. Experiments on natural images and medical ultrasound images show that MLFE-BM3D method can achieve better visual effects than BM3D method, the peak signal to noise ratio (PSNR) of the denoised image is increased by 0.5dB. The MLFE-BM3D method can improve the denoising effect of speckle noise in the texture region, and still maintain a good denoising effect in the smooth region of the image.



### Pixel-Semantic Revise of Position Learning A One-Stage Object Detector with A Shared Encoder-Decoder
- **Arxiv ID**: http://arxiv.org/abs/2001.01057v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2001.01057v2)
- **Published**: 2020-01-04 08:55:00+00:00
- **Updated**: 2020-09-29 02:28:34+00:00
- **Authors**: Qian Li, Nan Guo, Xiaochun Ye, Dongrui Fan, Zhimin Tang
- **Comment**: Accepted by ICONIP2020(International Conference on Neural Information
  Processing)
- **Journal**: None
- **Summary**: Recently, many methods have been proposed for object detection. They cannot detect objects by semantic features, adaptively. In this work, according to channel and spatial attention mechanisms, we mainly analyze that different methods detect objects adaptively. Some state-of-the-art detectors combine different feature pyramids with many mechanisms to enhance multi-level semantic information. However, they require more cost. This work addresses that by an anchor-free detector with shared encoder-decoder with attention mechanism, extracting shared features. We consider features of different levels from backbone (e.g., ResNet-50) as the basis features. Then, we feed the features into a simple module, followed by a detector header to detect objects. Meantime, we use the semantic features to revise geometric locations, and the detector is a pixel-semantic revising of position. More importantly, this work analyzes the impact of different pooling strategies (e.g., mean, maximum or minimum) on multi-scale objects, and finds the minimum pooling improve detection performance on small objects better. Compared with state-of-the-art MNC based on ResNet-101 for the standard MSCOCO 2014 baseline, our method improves detection AP of 3.8%.



### Self-Learning AI Framework for Skin Lesion Image Segmentation and Classification
- **Arxiv ID**: http://arxiv.org/abs/2001.05838v1
- **DOI**: 10.5121/ijcsit.2019.11604
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2001.05838v1)
- **Published**: 2020-01-04 09:31:11+00:00
- **Updated**: 2020-01-04 09:31:11+00:00
- **Authors**: Anandhanarayanan Kamalakannan, Shiva Shankar Ganesan, Govindaraj Rajamanickam
- **Comment**: 10 pages, 3 figures, 2 tables
- **Journal**: None
- **Summary**: Image segmentation and classification are the two main fundamental steps in pattern recognition. To perform medical image segmentation or classification with deep learning models, it requires training on large image dataset with annotation. The dermoscopy images (ISIC archive) considered for this work does not have ground truth information for lesion segmentation. Performing manual labelling on this dataset is time-consuming. To overcome this issue, self-learning annotation scheme was proposed in the two-stage deep learning algorithm. The two-stage deep learning algorithm consists of U-Net segmentation model with the annotation scheme and CNN classifier model. The annotation scheme uses a K-means clustering algorithm along with merging conditions to achieve initial labelling information for training the U-Net model. The classifier models namely ResNet-50 and LeNet-5 were trained and tested on the image dataset without segmentation for comparison and with the U-Net segmentation for implementing the proposed self-learning Artificial Intelligence (AI) framework. The classification results of the proposed AI framework achieved training accuracy of 93.8% and testing accuracy of 82.42% when compared with the two classifier models directly trained on the input images.



### DAF-NET: a saliency based weakly supervised method of dual attention fusion for fine-grained image classification
- **Arxiv ID**: http://arxiv.org/abs/2001.02219v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.02219v1)
- **Published**: 2020-01-04 12:59:48+00:00
- **Updated**: 2020-01-04 12:59:48+00:00
- **Authors**: ZiChao Dong, JiLong Wu, TingTing Ren, Yue Wang, MengYing Ge
- **Comment**: arXiv admin note: text overlap with arXiv:1809.00287 by other authors
- **Journal**: None
- **Summary**: Fine-grained image classification is a challenging problem, since the difficulty of finding discriminative features. To handle this circumstance, basically, there are two ways to go. One is use attention based method to focus on informative areas, while the other one aims to find high order between features. Further, for attention based method there are two directions, activation based and detection based, which are proved effective by scholars. However ,rare work focus on fusing two types of attention with high order feature. In this paper, we propose a novel DAF method which fuse two types of attention and use them to as PAF(part attention filter) in deep bilinear transformation module to mine the relationship between separate parts of an object. Briefly, our network constructed by a student net who attempt to output two attention maps and a teacher net uses these two maps as empirical information to refine the result. The experiment result shows that only student net could get 87.6% accuracy in CUB dataset while cooperating with teacher net could achieve 89.1% accuracy.



### Res3ATN -- Deep 3D Residual Attention Network for Hand Gesture Recognition in Videos
- **Arxiv ID**: http://arxiv.org/abs/2001.01083v1
- **DOI**: 10.1109/3DV.2019.00061
- **Categories**: **cs.CV**, cs.LG, eess.IV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2001.01083v1)
- **Published**: 2020-01-04 14:36:36+00:00
- **Updated**: 2020-01-04 14:36:36+00:00
- **Authors**: Naina Dhingra, Andreas Kunz
- **Comment**: 10 pages, 4 figures, International Conference on 3D Vision (3DV
  2019), Quebec City, Canada, September 16-19, 2019
- **Journal**: 2019 International Conference on 3D Vision (3DV), 491--501, 2019
- **Summary**: Hand gesture recognition is a strenuous task to solve in videos. In this paper, we use a 3D residual attention network which is trained end to end for hand gesture recognition. Based on the stacked multiple attention blocks, we build a 3D network which generates different features at each attention block. Our 3D attention based residual network (Res3ATN) can be built and extended to very deep layers. Using this network, an extensive analysis is performed on other 3D networks based on three publicly available datasets. The Res3ATN network performance is compared to C3D, ResNet-10, and ResNext-101 networks. We also study and evaluate our baseline network with different number of attention blocks. The comparison shows that the 3D residual attention network with 3 attention blocks is robust in attention learning and is able to classify the gestures with better accuracy, thus outperforming existing networks.



### RPR: Random Partition Relaxation for Training; Binary and Ternary Weight Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2001.01091v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.01091v1)
- **Published**: 2020-01-04 15:56:10+00:00
- **Updated**: 2020-01-04 15:56:10+00:00
- **Authors**: Lukas Cavigelli, Luca Benini
- **Comment**: None
- **Journal**: None
- **Summary**: We present Random Partition Relaxation (RPR), a method for strong quantization of neural networks weight to binary (+1/-1) and ternary (+1/0/-1) values. Starting from a pre-trained model, we quantize the weights and then relax random partitions of them to their continuous values for retraining before re-quantizing them and switching to another weight partition for further adaptation. We demonstrate binary and ternary-weight networks with accuracies beyond the state-of-the-art for GoogLeNet and competitive performance for ResNet-18 and ResNet-50 using an SGD-based training method that can easily be integrated into existing frameworks.



### Human Action Recognition and Assessment via Deep Neural Network Self-Organization
- **Arxiv ID**: http://arxiv.org/abs/2001.05837v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2001.05837v2)
- **Published**: 2020-01-04 15:58:06+00:00
- **Updated**: 2020-02-16 16:56:09+00:00
- **Authors**: German I. Parisi
- **Comment**: None
- **Journal**: None
- **Summary**: The robust recognition and assessment of human actions are crucial in human-robot interaction (HRI) domains. While state-of-the-art models of action perception show remarkable results in large-scale action datasets, they mostly lack the flexibility, robustness, and scalability needed to operate in natural HRI scenarios which require the continuous acquisition of sensory information as well as the classification or assessment of human body patterns in real time. In this chapter, I introduce a set of hierarchical models for the learning and recognition of actions from depth maps and RGB images through the use of neural network self-organization. A particularity of these models is the use of growing self-organizing networks that quickly adapt to non-stationary distributions and implement dedicated mechanisms for continual learning from temporally correlated input.



### Represented Value Function Approach for Large Scale Multi Agent Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/2001.01096v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.MA
- **Links**: [PDF](http://arxiv.org/pdf/2001.01096v2)
- **Published**: 2020-01-04 16:29:13+00:00
- **Updated**: 2020-01-10 01:57:34+00:00
- **Authors**: Weiya Ren
- **Comment**: 9 pages the code is published and the result is reproducible
- **Journal**: None
- **Summary**: In this paper, we consider the problem of large scale multi agent reinforcement learning. Firstly, we studied the representation problem of the pairwise value function to reduce the complexity of the interactions among agents. Secondly, we adopt a l2-norm trick to ensure the trivial term of the approximated value function is bounded. Thirdly, experimental results on battle game demonstrate the effectiveness of the proposed approach.



### COPD Classification in CT Images Using a 3D Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2001.01100v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2001.01100v1)
- **Published**: 2020-01-04 16:58:45+00:00
- **Updated**: 2020-01-04 16:58:45+00:00
- **Authors**: Jalil Ahmed, Sulaiman Vesal, Felix Durlak, Rainer Kaergel, Nishant Ravikumar, Martine Remy-Jardin, Andreas Maier
- **Comment**: None
- **Journal**: None
- **Summary**: Chronic obstructive pulmonary disease (COPD) is a lung disease that is not fully reversible and one of the leading causes of morbidity and mortality in the world. Early detection and diagnosis of COPD can increase the survival rate and reduce the risk of COPD progression in patients. Currently, the primary examination tool to diagnose COPD is spirometry. However, computed tomography (CT) is used for detecting symptoms and sub-type classification of COPD. Using different imaging modalities is a difficult and tedious task even for physicians and is subjective to inter-and intra-observer variations. Hence, developing meth-ods that can automatically classify COPD versus healthy patients is of great interest. In this paper, we propose a 3D deep learning approach to classify COPD and emphysema using volume-wise annotations only. We also demonstrate the impact of transfer learning on the classification of emphysema using knowledge transfer from a pre-trained COPD classification model.



### Biologically-Motivated Deep Learning Method using Hierarchical Competitive Learning
- **Arxiv ID**: http://arxiv.org/abs/2001.01121v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2001.01121v1)
- **Published**: 2020-01-04 20:07:36+00:00
- **Updated**: 2020-01-04 20:07:36+00:00
- **Authors**: Takashi Shinozaki
- **Comment**: Appeared at NIPS 2019 Workshop: Shared Visual Representations in
  Human and Machine Intelligence (SVRHM)
- **Journal**: None
- **Summary**: This study proposes a novel biologically-motivated learning method for deep convolutional neural networks (CNNs). The combination of CNNs and back propagation (BP) learning is the most powerful method in recent machine learning regimes. However, it requires large labeled data for training, and this requirement can occasionally become a barrier for real world applications. To address this problem and utilize unlabeled data, I propose to introduce unsupervised competitive learning which only requires forward propagating signals as a pre-training method for CNNs. The method was evaluated by image discrimination tasks using MNIST, CIFAR-10, and ImageNet datasets, and it achieved a state-of-the-art performance as a biologically-motivated method in the ImageNet experiment. The results suggested that the method enables higher-level learning representations solely from forward propagating signals without a backward error signal for the learning of convolutional layers. The proposed method could be useful for a variety of poorly labeled data, for example, time series or medical data.



### TCM-ICP: Transformation Compatibility Measure for Registering Multiple LIDAR Scans
- **Arxiv ID**: http://arxiv.org/abs/2001.01129v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, 68U05, I.3.5; I.4.8; I.5.3
- **Links**: [PDF](http://arxiv.org/pdf/2001.01129v2)
- **Published**: 2020-01-04 21:05:27+00:00
- **Updated**: 2020-01-31 17:20:47+00:00
- **Authors**: Aby Thomas, Adarsh Sunilkumar, Shankar Shylesh, Aby Abahai T., Subhasree Methirumangalath, Dong Chen, Jiju Peethambaran
- **Comment**: 9 pages, 8 figures, submitted to IEEE GRSL
- **Journal**: None
- **Summary**: Rigid registration of multi-view and multi-platform LiDAR scans is a fundamental problem in 3D mapping, robotic navigation, and large-scale urban modeling applications. Data acquisition with LiDAR sensors involves scanning multiple areas from different points of view, thus generating partially overlapping point clouds of the real world scenes. Traditionally, ICP (Iterative Closest Point) algorithm is used to register the acquired point clouds together to form a unique point cloud that captures the scanned real world scene. Conventional ICP faces local minima issues and often needs a coarse initial alignment to converge to the optimum. In this work, we present an algorithm for registering multiple, overlapping LiDAR scans. We introduce a geometric metric called Transformation Compatibility Measure (TCM) which aids in choosing the most similar point clouds for registration in each iteration of the algorithm. The LiDAR scan most similar to the reference LiDAR scan is then transformed using simplex technique. An optimization of the transformation using gradient descent and simulated annealing techniques are then applied to improve the resulting registration. We evaluate the proposed algorithm on four different real world scenes and experimental results shows that the registration performance of the proposed method is comparable or superior to the traditionally used registration methods. Further, the algorithm achieves superior registration results even when dealing with outliers.



