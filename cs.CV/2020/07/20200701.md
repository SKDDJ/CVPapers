# Arxiv Papers in cs.CV on 2020-07-01
### Online Domain Adaptation for Occupancy Mapping
- **Arxiv ID**: http://arxiv.org/abs/2007.00164v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO, 90C27, G.3
- **Links**: [PDF](http://arxiv.org/pdf/2007.00164v1)
- **Published**: 2020-07-01 00:46:51+00:00
- **Updated**: 2020-07-01 00:46:51+00:00
- **Authors**: Anthony Tompkins, Ransalu Senanayake, Fabio Ramos
- **Comment**: Robotics: Science and Systems (RSS) 2020 conference
- **Journal**: None
- **Summary**: Creating accurate spatial representations that take into account uncertainty is critical for autonomous robots to safely navigate in unstructured environments. Although recent LIDAR based mapping techniques can produce robust occupancy maps, learning the parameters of such models demand considerable computational time, discouraging them from being used in real-time and large-scale applications such as autonomous driving. Recognizing the fact that real-world structures exhibit similar geometric features across a variety of urban environments, in this paper, we argue that it is redundant to learn all geometry dependent parameters from scratch. Instead, we propose a theoretical framework building upon the theory of optimal transport to adapt model parameters to account for changes in the environment, significantly amortizing the training cost. Further, with the use of high-fidelity driving simulators and real-world datasets, we demonstrate how parameters of 2D and 3D occupancy maps can be automatically adapted to accord with local spatial changes. We validate various domain adaptation paradigms through a series of experiments, ranging from inter-domain feature transfer to simulation-to-real-world feature transfer. Experiments verified the possibility of estimating parameters with a negligible computational and memory cost, enabling large-scale probabilistic mapping in urban environments.



### Fused Text Recogniser and Deep Embeddings Improve Word Recognition and Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2007.00166v1
- **DOI**: 10.1007/978-3-030-57058-3_22
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.00166v1)
- **Published**: 2020-07-01 00:55:34+00:00
- **Updated**: 2020-07-01 00:55:34+00:00
- **Authors**: Siddhant Bansal, Praveen Krishnan, C. V. Jawahar
- **Comment**: 15 pages, 8 figures, Accepted in IAPR International Workshop on
  Document Analysis Systems (DAS) 2020, "Visit project page, at
  http://cvit.iiit.ac.in/research/projects/cvit-projects/fused-text-recogniser-and-deep-embeddings-improve-word-recognition-and-retrieval"
- **Journal**: None
- **Summary**: Recognition and retrieval of textual content from the large document collections have been a powerful use case for the document image analysis community. Often the word is the basic unit for recognition as well as retrieval. Systems that rely only on the text recogniser (OCR) output are not robust enough in many situations, especially when the word recognition rates are poor, as in the case of historic documents or digital libraries. An alternative has been word spotting based methods that retrieve/match words based on a holistic representation of the word. In this paper, we fuse the noisy output of text recogniser with a deep embeddings representation derived out of the entire word. We use average and max fusion for improving the ranked results in the case of retrieval. We validate our methods on a collection of Hindi documents. We improve word recognition rate by 1.4 and retrieval by 11.13 in the mAP.



### Low-light Image Restoration with Short- and Long-exposure Raw Pairs
- **Arxiv ID**: http://arxiv.org/abs/2007.00199v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.00199v2)
- **Published**: 2020-07-01 03:22:26+00:00
- **Updated**: 2021-02-28 07:42:04+00:00
- **Authors**: Meng Chang, Huajun Feng, Zhihai Xu, Qi Li
- **Comment**: None
- **Journal**: None
- **Summary**: Low-light imaging with handheld mobile devices is a challenging issue. Limited by the existing models and training data, most existing methods cannot be effectively applied in real scenarios. In this paper, we propose a new low-light image restoration method by using the complementary information of short- and long-exposure images. We first propose a novel data generation method to synthesize realistic short- and longexposure raw images by simulating the imaging pipeline in lowlight environment. Then, we design a new long-short-exposure fusion network (LSFNet) to deal with the problems of low-light image fusion, including high noise, motion blur, color distortion and misalignment. The proposed LSFNet takes pairs of shortand long-exposure raw images as input, and outputs a clear RGB image. Using our data generation method and the proposed LSFNet, we can recover the details and color of the original scene, and improve the low-light image quality effectively. Experiments demonstrate that our method can outperform the state-of-the art methods.



### Multimodal Text Style Transfer for Outdoor Vision-and-Language Navigation
- **Arxiv ID**: http://arxiv.org/abs/2007.00229v3
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.00229v3)
- **Published**: 2020-07-01 04:29:07+00:00
- **Updated**: 2021-02-04 04:48:23+00:00
- **Authors**: Wanrong Zhu, Xin Eric Wang, Tsu-Jui Fu, An Yan, Pradyumna Narayana, Kazoo Sone, Sugato Basu, William Yang Wang
- **Comment**: EACL 2021
- **Journal**: None
- **Summary**: One of the most challenging topics in Natural Language Processing (NLP) is visually-grounded language understanding and reasoning. Outdoor vision-and-language navigation (VLN) is such a task where an agent follows natural language instructions and navigates a real-life urban environment. Due to the lack of human-annotated instructions that illustrate intricate urban scenes, outdoor VLN remains a challenging task to solve. This paper introduces a Multimodal Text Style Transfer (MTST) learning approach and leverages external multimodal resources to mitigate data scarcity in outdoor navigation tasks. We first enrich the navigation data by transferring the style of the instructions generated by Google Maps API, then pre-train the navigator with the augmented external outdoor navigation dataset. Experimental results show that our MTST learning approach is model-agnostic, and our MTST approach significantly outperforms the baseline models on the outdoor VLN task, improving task completion rate by 8.7% relatively on the test set.



### Motion Prediction in Visual Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/2007.01120v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2007.01120v1)
- **Published**: 2020-07-01 04:29:41+00:00
- **Updated**: 2020-07-01 04:29:41+00:00
- **Authors**: Jianren Wang, Yihui He
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1904.03280
- **Journal**: None
- **Summary**: Visual object tracking (VOT) is an essential component for many applications, such as autonomous driving or assistive robotics. However, recent works tend to develop accurate systems based on more computationally expensive feature extractors for better instance matching. In contrast, this work addresses the importance of motion prediction in VOT. We use an off-the-shelf object detector to obtain instance bounding boxes. Then, a combination of camera motion decouple and Kalman filter is used for state estimation. Although our baseline system is a straightforward combination of standard methods, we obtain state-of-the-art results. Our method establishes new state-of-the-art performance on VOT (VOT-2016 and VOT-2018). Our proposed method improves the EAO on VOT-2016 from 0.472 of prior art to 0.505, from 0.410 to 0.431 on VOT-2018. To show the generalizability, we also test our method on video object segmentation (VOS: DAVIS-2016 and DAVIS-2017) and observe consistent improvement.



### BiO-Net: Learning Recurrent Bi-directional Connections for Encoder-Decoder Architecture
- **Arxiv ID**: http://arxiv.org/abs/2007.00243v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.00243v2)
- **Published**: 2020-07-01 05:07:49+00:00
- **Updated**: 2020-07-06 00:31:21+00:00
- **Authors**: Tiange Xiang, Chaoyi Zhang, Dongnan Liu, Yang Song, Heng Huang, Weidong Cai
- **Comment**: 10 pages, 4 figures, MICCAI2020
- **Journal**: None
- **Summary**: U-Net has become one of the state-of-the-art deep learning-based approaches for modern computer vision tasks such as semantic segmentation, super resolution, image denoising, and inpainting. Previous extensions of U-Net have focused mainly on the modification of its existing building blocks or the development of new functional modules for performance gains. As a result, these variants usually lead to an unneglectable increase in model complexity. To tackle this issue in such U-Net variants, in this paper, we present a novel Bi-directional O-shape network (BiO-Net) that reuses the building blocks in a recurrent manner without introducing any extra parameters. Our proposed bi-directional skip connections can be directly adopted into any encoder-decoder architecture to further enhance its capabilities in various task domains. We evaluated our method on various medical image analysis tasks and the results show that our BiO-Net significantly outperforms the vanilla U-Net as well as other state-of-the-art methods. Our code is available at https://github.com/tiangexiang/BiO-Net.



### Enhancing the Association in Multi-Object Tracking via Neighbor Graph
- **Arxiv ID**: http://arxiv.org/abs/2007.00265v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.00265v1)
- **Published**: 2020-07-01 06:21:31+00:00
- **Updated**: 2020-07-01 06:21:31+00:00
- **Authors**: Tianyi Liang, Long Lan, Zhigang Luo
- **Comment**: None
- **Journal**: None
- **Summary**: Most modern multi-object tracking (MOT) systems follow the tracking-by-detection paradigm. It first localizes the objects of interest, then extracting their individual appearance features to make data association. The individual features, however, are susceptible to the negative effects as occlusions, illumination variations and inaccurate detections, thus resulting in the mismatch in the association inference. In this work, we propose to handle this problem via making full use of the neighboring information. Our motivations derive from the observations that people tend to move in a group. As such, when an individual target's appearance is seriously changed, we can still identify it with the help of its neighbors. To this end, we first utilize the spatio-temporal relations produced by the tracking self to efficiently select suitable neighbors for the targets. Subsequently, we construct neighbor graph of the target and neighbors then employ the graph convolution networks (GCN) to learn the graph features. To the best of our knowledge, it is the first time to exploit neighbor cues via GCN in MOT. Finally, we test our approach on the MOT benchmarks and achieve state-of-the-art performance in online tracking.



### Robust Semantic Segmentation in Adverse Weather Conditions by means of Fast Video-Sequence Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2007.00290v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.00290v1)
- **Published**: 2020-07-01 07:29:35+00:00
- **Updated**: 2020-07-01 07:29:35+00:00
- **Authors**: Andreas Pfeuffer, Klaus Dietmayer
- **Comment**: None
- **Journal**: None
- **Summary**: Computer vision tasks such as semantic segmentation perform very well in good weather conditions, but if the weather turns bad, they have problems to achieve this performance in these conditions. One possibility to obtain more robust and reliable results in adverse weather conditions is to use video-segmentation approaches instead of commonly used single-image segmentation methods. Video-segmentation approaches capture temporal information of the previous video-frames in addition to current image information, and hence, they are more robust against disturbances, especially if they occur in only a few frames of the video-sequence. However, video-segmentation approaches, which are often based on recurrent neural networks, cannot be applied in real-time applications anymore, since their recurrent structures in the network are computational expensive. For instance, the inference time of the LSTM-ICNet, in which recurrent units are placed at proper positions in the single-segmentation approach ICNet, increases up to 61 percent compared to the basic ICNet. Hence, in this work, the LSTM-ICNet is sped up by modifying the recurrent units of the network so that it becomes real-time capable again. Experiments on different datasets and various weather conditions show that the inference time can be decreased by about 23 percent by these modifications, while they achieve similar performance than the LSTM-ICNet and outperform the single-segmentation approach enormously in adverse weather conditions.



### FlowControl: Optical Flow Based Visual Servoing
- **Arxiv ID**: http://arxiv.org/abs/2007.00291v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.00291v1)
- **Published**: 2020-07-01 07:32:00+00:00
- **Updated**: 2020-07-01 07:32:00+00:00
- **Authors**: Max Argus, Lukas Hermann, Jon Long, Thomas Brox
- **Comment**: None
- **Journal**: None
- **Summary**: One-shot imitation is the vision of robot programming from a single demonstration, rather than by tedious construction of computer code. We present a practical method for realizing one-shot imitation for manipulation tasks, exploiting modern learning-based optical flow to perform real-time visual servoing. Our approach, which we call FlowControl, continuously tracks a demonstration video, using a specified foreground mask to attend to an object of interest. Using RGB-D observations, FlowControl requires no 3D object models, and is easy to set up. FlowControl inherits great robustness to visual appearance from decades of work in optical flow. We exhibit FlowControl on a range of problems, including ones requiring very precise motions, and ones requiring the ability to generalize.



### Robustifying the Deployment of tinyML Models for Autonomous mini-vehicles
- **Arxiv ID**: http://arxiv.org/abs/2007.00302v2
- **DOI**: 10.1109/ISCAS51556.2021.9401154
- **Categories**: **cs.CV**, cs.LG, cs.RO, cs.SY, eess.SP, eess.SY
- **Links**: [PDF](http://arxiv.org/pdf/2007.00302v2)
- **Published**: 2020-07-01 07:54:26+00:00
- **Updated**: 2021-02-13 20:38:02+00:00
- **Authors**: Miguel de Prado, Manuele Rusci, Romain Donze, Alessandro Capotondi, Serge Monnerat, Luca Benini and, Nuria Pazos
- **Comment**: None
- **Journal**: None
- **Summary**: Standard-size autonomous navigation vehicles have rapidly improved thanks to the breakthroughs of deep learning. However, scaling autonomous driving to low-power systems deployed on dynamic environments poses several challenges that prevent their adoption. To address them, we propose a closed-loop learning flow for autonomous driving mini-vehicles that includes the target environment in-the-loop. We leverage a family of compact and high-throughput tinyCNNs to control the mini-vehicle, which learn in the target environment by imitating a computer vision algorithm, i.e., the expert. Thus, the tinyCNNs, having only access to an on-board fast-rate linear camera, gain robustness to lighting conditions and improve over time. Further, we leverage GAP8, a parallel ultra-low-power RISC-V SoC, to meet the inference requirements. When running the family of CNNs, our GAP8's solution outperforms any other implementation on the STM32L4 and NXP k64f (Cortex-M4), reducing the latency by over 13x and the energy consummation by 92%.



### Towards Explainable Graph Representations in Digital Pathology
- **Arxiv ID**: http://arxiv.org/abs/2007.00311v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.00311v1)
- **Published**: 2020-07-01 08:05:26+00:00
- **Updated**: 2020-07-01 08:05:26+00:00
- **Authors**: Guillaume Jaume, Pushpak Pati, Antonio Foncubierta-Rodriguez, Florinda Feroce, Giosue Scognamiglio, Anna Maria Anniciello, Jean-Philippe Thiran, Orcun Goksel, Maria Gabrani
- **Comment**: ICML'20 workshop on Computational Biology
- **Journal**: None
- **Summary**: Explainability of machine learning (ML) techniques in digital pathology (DP) is of great significance to facilitate their wide adoption in clinics. Recently, graph techniques encoding relevant biological entities have been employed to represent and assess DP images. Such paradigm shift from pixel-wise to entity-wise analysis provides more control over concept representation. In this paper, we introduce a post-hoc explainer to derive compact per-instance explanations emphasizing diagnostically important entities in the graph. Although we focus our analyses to cells and cellular interactions in breast cancer subtyping, the proposed explainer is generic enough to be extended to other topological representations in DP. Qualitative and quantitative analyses demonstrate the efficacy of the explainer in generating comprehensive and compact explanations.



### Future Urban Scenes Generation Through Vehicles Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2007.00323v3
- **DOI**: 10.1109/ICPR48806.2021.9412880
- **Categories**: **cs.CV**, cs.CG
- **Links**: [PDF](http://arxiv.org/pdf/2007.00323v3)
- **Published**: 2020-07-01 08:40:16+00:00
- **Updated**: 2021-10-22 07:54:00+00:00
- **Authors**: Alessandro Simoni, Luca Bergamini, Andrea Palazzi, Simone Calderara, Rita Cucchiara
- **Comment**: Accepted at ICPR2020
- **Journal**: None
- **Summary**: In this work we propose a deep learning pipeline to predict the visual future appearance of an urban scene. Despite recent advances, generating the entire scene in an end-to-end fashion is still far from being achieved. Instead, here we follow a two stages approach, where interpretable information is included in the loop and each actor is modelled independently. We leverage a per-object novel view synthesis paradigm; i.e. generating a synthetic representation of an object undergoing a geometrical roto-translation in the 3D space. Our model can be easily conditioned with constraints (e.g. input trajectories) provided by state-of-the-art tracking methods or by the user itself. This allows us to generate a set of diverse realistic futures starting from the same input in a multi-modal fashion. We visually and quantitatively show the superiority of this approach over traditional end-to-end scene-generation methods on CityFlow, a challenging real world dataset.



### NestFuse: An Infrared and Visible Image Fusion Architecture based on Nest Connection and Spatial/Channel Attention Models
- **Arxiv ID**: http://arxiv.org/abs/2007.00328v2
- **DOI**: 10.1109/TIM.2020.3005230
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.00328v2)
- **Published**: 2020-07-01 08:46:23+00:00
- **Updated**: 2020-07-11 06:31:34+00:00
- **Authors**: Hui Li, Xiao-Jun Wu, Tariq Durrani
- **Comment**: 12 pages, 13 figures, 6 tables. IEEE Transactions on Instrumentation
  and Measurement
- **Journal**: None
- **Summary**: In this paper we propose a novel method for infrared and visible image fusion where we develop nest connection-based network and spatial/channel attention models. The nest connection-based network can preserve significant amounts of information from input data in a multi-scale perspective. The approach comprises three key elements: encoder, fusion strategy and decoder respectively. In our proposed fusion strategy, spatial attention models and channel attention models are developed that describe the importance of each spatial position and of each channel with deep features. Firstly, the source images are fed into the encoder to extract multi-scale deep features. The novel fusion strategy is then developed to fuse these features for each scale. Finally, the fused image is reconstructed by the nest connection-based decoder. Experiments are performed on publicly available datasets. These exhibit that our proposed approach has better fusion performance than other state-of-the-art methods. This claim is justified through both subjective and objective evaluation. The code of our fusion method is available at https://github.com/hli1221/imagefusion-nestfuse



### Determining Sequence of Image Processing Technique (IPT) to Detect Adversarial Attacks
- **Arxiv ID**: http://arxiv.org/abs/2007.00337v2
- **DOI**: 10.1007/s42979-021-00773-8
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2007.00337v2)
- **Published**: 2020-07-01 08:59:14+00:00
- **Updated**: 2020-07-07 09:26:57+00:00
- **Authors**: Kishor Datta Gupta, Zahid Akhtar, Dipankar Dasgupta
- **Comment**: None
- **Journal**: SN COMPUT. SCI. 2, 383 (2021)
- **Summary**: Developing secure machine learning models from adversarial examples is challenging as various methods are continually being developed to generate adversarial attacks. In this work, we propose an evolutionary approach to automatically determine Image Processing Techniques Sequence (IPTS) for detecting malicious inputs. Accordingly, we first used a diverse set of attack methods including adaptive attack methods (on our defense) to generate adversarial samples from the clean dataset. A detection framework based on a genetic algorithm (GA) is developed to find the optimal IPTS, where the optimality is estimated by different fitness measures such as Euclidean distance, entropy loss, average histogram, local binary pattern and loss functions. The "image difference" between the original and processed images is used to extract the features, which are then fed to a classification scheme in order to determine whether the input sample is adversarial or clean. This paper described our methodology and performed experiments using multiple data-sets tested with several adversarial attacks. For each attack-type and dataset, it generates unique IPTS. A set of IPTS selected dynamically in testing time which works as a filter for the adversarial attack. Our empirical experiments exhibited promising results indicating the approach can efficiently be used as processing for any AI model.



### Adversarial Network with Multiple Classifiers for Open Set Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2007.00384v3
- **DOI**: 10.1109/TMM.2020.3016126
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.00384v3)
- **Published**: 2020-07-01 11:23:07+00:00
- **Updated**: 2020-08-07 10:20:22+00:00
- **Authors**: Tasfia Shermin, Guojun Lu, Shyh Wei Teng, Manzur Murshed, Ferdous Sohel
- **Comment**: Accepted in IEEE Transactions on Multimedia (in press), 2020
- **Journal**: IEEE Transactions on Multimedia, 2020 (CODE:
  https://github.com/tasfia/DAMC)
- **Summary**: Domain adaptation aims to transfer knowledge from a domain with adequate labeled samples to a domain with scarce labeled samples. Prior research has introduced various open set domain adaptation settings in the literature to extend the applications of domain adaptation methods in real-world scenarios. This paper focuses on the type of open set domain adaptation setting where the target domain has both private ('unknown classes') label space and the shared ('known classes') label space. However, the source domain only has the 'known classes' label space. Prevalent distribution-matching domain adaptation methods are inadequate in such a setting that demands adaptation from a smaller source domain to a larger and diverse target domain with more classes. For addressing this specific open set domain adaptation setting, prior research introduces a domain adversarial model that uses a fixed threshold for distinguishing known from unknown target samples and lacks at handling negative transfers. We extend their adversarial model and propose a novel adversarial domain adaptation model with multiple auxiliary classifiers. The proposed multi-classifier structure introduces a weighting module that evaluates distinctive domain characteristics for assigning the target samples with weights which are more representative to whether they are likely to belong to the known and unknown classes to encourage positive transfers during adversarial training and simultaneously reduces the domain gap between the shared classes of the source and target domains. A thorough experimental investigation shows that our proposed method outperforms existing domain adaptation methods on a number of domain adaptation datasets.



### The IKEA ASM Dataset: Understanding People Assembling Furniture through Actions, Objects and Pose
- **Arxiv ID**: http://arxiv.org/abs/2007.00394v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.00394v2)
- **Published**: 2020-07-01 11:34:46+00:00
- **Updated**: 2023-05-17 07:56:52+00:00
- **Authors**: Yizhak Ben-Shabat, Xin Yu, Fatemeh Sadat Saleh, Dylan Campbell, Cristian Rodriguez-Opazo, Hongdong Li, Stephen Gould
- **Comment**: None
- **Journal**: None
- **Summary**: The availability of a large labeled dataset is a key requirement for applying deep learning methods to solve various computer vision tasks. In the context of understanding human activities, existing public datasets, while large in size, are often limited to a single RGB camera and provide only per-frame or per-clip action annotations. To enable richer analysis and understanding of human activities, we introduce IKEA ASM -- a three million frame, multi-view, furniture assembly video dataset that includes depth, atomic actions, object segmentation, and human pose. Additionally, we benchmark prominent methods for video action recognition, object segmentation and human pose estimation tasks on this challenging dataset. The dataset enables the development of holistic methods, which integrate multi-modal and multi-view data to better perform on these tasks.



### DocVQA: A Dataset for VQA on Document Images
- **Arxiv ID**: http://arxiv.org/abs/2007.00398v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/2007.00398v3)
- **Published**: 2020-07-01 11:37:40+00:00
- **Updated**: 2021-01-05 05:39:39+00:00
- **Authors**: Minesh Mathew, Dimosthenis Karatzas, C. V. Jawahar
- **Comment**: accepted at WACV 2021
- **Journal**: None
- **Summary**: We present a new dataset for Visual Question Answering (VQA) on document images called DocVQA. The dataset consists of 50,000 questions defined on 12,000+ document images. Detailed analysis of the dataset in comparison with similar datasets for VQA and reading comprehension is presented. We report several baseline results by adopting existing VQA and reading comprehension models. Although the existing models perform reasonably well on certain types of questions, there is large performance gap compared to human performance (94.36% accuracy). The models need to improve specifically on questions where understanding structure of the document is crucial. The dataset, code and leaderboard are available at docvqa.org



### M3d-CAM: A PyTorch library to generate 3D data attention maps for medical deep learning
- **Arxiv ID**: http://arxiv.org/abs/2007.00453v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.00453v1)
- **Published**: 2020-07-01 12:55:57+00:00
- **Updated**: 2020-07-01 12:55:57+00:00
- **Authors**: Karol Gotkowski, Camila Gonzalez, Andreas Bucher, Anirban Mukhopadhyay
- **Comment**: None
- **Journal**: None
- **Summary**: M3d-CAM is an easy to use library for generating attention maps of CNN-based PyTorch models improving the interpretability of model predictions for humans. The attention maps can be generated with multiple methods like Guided Backpropagation, Grad-CAM, Guided Grad-CAM and Grad-CAM++. These attention maps visualize the regions in the input data that influenced the model prediction the most at a certain layer. Furthermore, M3d-CAM supports 2D and 3D data for the task of classification as well as for segmentation. A key feature is also that in most cases only a single line of code is required for generating attention maps for a model making M3d-CAM basically plug and play.



### Automatic Crack Detection on Road Pavements Using Encoder Decoder Architecture
- **Arxiv ID**: http://arxiv.org/abs/2007.00477v1
- **DOI**: 10.3390/ma13132960
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.00477v1)
- **Published**: 2020-07-01 13:32:23+00:00
- **Updated**: 2020-07-01 13:32:23+00:00
- **Authors**: Zhun Fan, Chong Li, Ying Chen, Jiahong Wei, Giuseppe Loprencipe, Xiaopeng Chen, Paola Di Mascio
- **Comment**: None
- **Journal**: None
- **Summary**: Inspired by the development of deep learning in computer vision and object detection, the proposed algorithm considers an encoder-decoder architecture with hierarchical feature learning and dilated convolution, named U-Hierarchical Dilated Network (U-HDN), to perform crack detection in an end-to-end method. Crack characteristics with multiple context information are automatically able to learn and perform end-to-end crack detection. Then, a multi-dilation module embedded in an encoder-decoder architecture is proposed. The crack features of multiple context sizes can be integrated into the multi-dilation module by dilation convolution with different dilatation rates, which can obtain much more cracks information. Finally, the hierarchical feature learning module is designed to obtain a multi-scale features from the high to low-level convolutional layers, which are integrated to predict pixel-wise crack detection. Some experiments on public crack databases using 118 images were performed and the results were compared with those obtained with other methods on the same images. The results show that the proposed U-HDN method achieves high performance because it can extract and fuse different context sizes and different levels of feature maps than other algorithms.



### Optimisation of a Siamese Neural Network for Real-Time Energy Efficient Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/2007.00491v1
- **DOI**: 10.1007/978-3-030-59006-2_14
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.00491v1)
- **Published**: 2020-07-01 13:49:56+00:00
- **Updated**: 2020-07-01 13:49:56+00:00
- **Authors**: Dominika Przewlocka, Mateusz Wasala, Hubert Szolc, Krzysztof Blachut, Tomasz Kryjak
- **Comment**: 12 pages, accepted for ICCVG 2020
- **Journal**: None
- **Summary**: In this paper the research on optimisation of visual object tracking using a Siamese neural network for embedded vision systems is presented. It was assumed that the solution shall operate in real-time, preferably for a high resolution video stream, with the lowest possible energy consumption. To meet these requirements, techniques such as the reduction of computational precision and pruning were considered. Brevitas, a tool dedicated for optimisation and quantisation of neural networks for FPGA implementation, was used. A number of training scenarios were tested with varying levels of optimisations - from integer uniform quantisation with 16 bits to ternary and binary networks. Next, the influence of these optimisations on the tracking performance was evaluated. It was possible to reduce the size of the convolutional filters up to 10 times in relation to the original network. The obtained results indicate that using quantisation can significantly reduce the memory and computational complexity of the proposed network while still enabling precise tracking, thus allow to use it in embedded vision systems. Moreover, quantisation of weights positively affects the network training by decreasing overfitting.



### Optimisation of the PointPillars network for 3D object detection in point clouds
- **Arxiv ID**: http://arxiv.org/abs/2007.00493v1
- **DOI**: 10.23919/SPA50552.2020.9241265
- **Categories**: **cs.CV**, eess.IV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2007.00493v1)
- **Published**: 2020-07-01 13:50:42+00:00
- **Updated**: 2020-07-01 13:50:42+00:00
- **Authors**: Joanna Stanisz, Konrad Lis, Tomasz Kryjak, Marek Gorgon
- **Comment**: 7 pages, 2 figures, submitted to SPA 2020 conference
- **Journal**: None
- **Summary**: In this paper we present our research on the optimisation of a deep neural network for 3D object detection in a point cloud. Techniques like quantisation and pruning available in the Brevitas and PyTorch tools were used. We performed the experiments for the PointPillars network, which offers a reasonable compromise between detection accuracy and calculation complexity. The aim of this work was to propose a variant of the network which we will ultimately implement in an FPGA device. This will allow for real-time LiDAR data processing with low energy consumption. The obtained results indicate that even a significant quantisation from 32-bit floating point to 2-bit integer in the main part of the algorithm, results in 5%-9% decrease of the detection accuracy, while allowing for almost a 16-fold reduction in size of the model.



### Learning unbiased zero-shot semantic segmentation networks via transductive transfer
- **Arxiv ID**: http://arxiv.org/abs/2007.00515v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.00515v1)
- **Published**: 2020-07-01 14:25:13+00:00
- **Updated**: 2020-07-01 14:25:13+00:00
- **Authors**: Haiyang Liu, Yichen Wang, Jiayi Zhao, Guowu Yang, Fengmao Lv
- **Comment**: None
- **Journal**: None
- **Summary**: Semantic segmentation, which aims to acquire a detailed understanding of images, is an essential issue in computer vision. However, in practical scenarios, new categories that are different from the categories in training usually appear. Since it is impractical to collect labeled data for all categories, how to conduct zero-shot learning in semantic segmentation establishes an important problem. Although the attribute embedding of categories can promote effective knowledge transfer across different categories, the prediction of segmentation network reveals obvious bias to seen categories. In this paper, we propose an easy-to-implement transductive approach to alleviate the prediction bias in zero-shot semantic segmentation. Our method assumes that both the source images with full pixel-level labels and unlabeled target images are available during training. To be specific, the source images are used to learn the relationship between visual images and semantic embeddings, while the target images are used to alleviate the prediction bias towards seen categories. We conduct comprehensive experiments on diverse split s of the PASCAL dataset. The experimental results clearly demonstrate the effectiveness of our method.



### A Characteristic Function-based Algorithm for Geodesic Active Contours
- **Arxiv ID**: http://arxiv.org/abs/2007.00525v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.NA, math.NA
- **Links**: [PDF](http://arxiv.org/pdf/2007.00525v2)
- **Published**: 2020-07-01 14:39:14+00:00
- **Updated**: 2021-05-07 14:35:09+00:00
- **Authors**: Jun Ma, Dong Wang, Xiao-Ping Wang, Xiaoping Yang
- **Comment**: 22 pages
- **Journal**: None
- **Summary**: Active contour models have been widely used in image segmentation, and the level set method (LSM) is the most popular approach for solving the models, via implicitly representing the contour by a level set function. However, the LSM suffers from high computational burden and numerical instability, requiring additional regularization terms or re-initialization techniques. In this paper, we use characteristic functions to implicitly represent the contours, propose a new representation to the geodesic active contours and derive an efficient algorithm termed as the iterative convolution-thresholding method (ICTM). Compared to the LSM, the ICTM is simpler and much more efficient. In addition, the ICTM enjoys most desired features of the level set-based methods. Extensive experiments, on 2D synthetic, 2D ultrasound, 3D CT, and 3D MR images for nodule, organ and lesion segmentation, demonstrate that the proposed method not only obtains comparable or even better segmentation results (compared to the LSM) but also achieves significant acceleration.



### Student-Teacher Curriculum Learning via Reinforcement Learning: Predicting Hospital Inpatient Admission Location
- **Arxiv ID**: http://arxiv.org/abs/2007.01135v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML, 00Bxx, I.5
- **Links**: [PDF](http://arxiv.org/pdf/2007.01135v1)
- **Published**: 2020-07-01 15:00:43+00:00
- **Updated**: 2020-07-01 15:00:43+00:00
- **Authors**: Rasheed el-Bouri, David Eyre, Peter Watkinson, Tingting Zhu, David Clifton
- **Comment**: 16 pages, 31 figures, In Proceedings of the 37th International
  Conference on Machine Learning
- **Journal**: In Proceedings of the 37th International Conference on Machine
  Learning, 2020
- **Summary**: Accurate and reliable prediction of hospital admission location is important due to resource-constraints and space availability in a clinical setting, particularly when dealing with patients who come from the emergency department. In this work we propose a student-teacher network via reinforcement learning to deal with this specific problem. A representation of the weights of the student network is treated as the state and is fed as an input to the teacher network. The teacher network's action is to select the most appropriate batch of data to train the student network on from a training set sorted according to entropy. By validating on three datasets, not only do we show that our approach outperforms state-of-the-art methods on tabular data and performs competitively on image recognition, but also that novel curricula are learned by the teacher network. We demonstrate experimentally that the teacher network can actively learn about the student network and guide it to achieve better performance than if trained alone.



### Rethinking Anticipation Tasks: Uncertainty-aware Anticipation of Sparse Surgical Instrument Usage for Context-aware Assistance
- **Arxiv ID**: http://arxiv.org/abs/2007.00548v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.00548v4)
- **Published**: 2020-07-01 15:17:56+00:00
- **Updated**: 2022-03-30 11:33:14+00:00
- **Authors**: Dominik Rivoir, Sebastian Bodenstedt, Isabel Funke, Felix von Bechtolsheim, Marius Distler, Jürgen Weitz, Stefanie Speidel
- **Comment**: Accepted at MICCAI 2020
- **Journal**: None
- **Summary**: Intra-operative anticipation of instrument usage is a necessary component for context-aware assistance in surgery, e.g. for instrument preparation or semi-automation of robotic tasks. However, the sparsity of instrument occurrences in long videos poses a challenge. Current approaches are limited as they assume knowledge on the timing of future actions or require dense temporal segmentations during training and inference. We propose a novel learning task for anticipation of instrument usage in laparoscopic videos that overcomes these limitations. During training, only sparse instrument annotations are required and inference is done solely on image data. We train a probabilistic model to address the uncertainty associated with future events. Our approach outperforms several baselines and is competitive to a variant using richer annotations. We demonstrate the model's ability to quantify task-relevant uncertainties. To the best of our knowledge, we are the first to propose a method for anticipating instruments in surgery.



### FVV Live: A real-time free-viewpoint video system with consumer electronics hardware
- **Arxiv ID**: http://arxiv.org/abs/2007.00558v1
- **DOI**: 10.1109/TMM.2021.3079711
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2007.00558v1)
- **Published**: 2020-07-01 15:40:28+00:00
- **Updated**: 2020-07-01 15:40:28+00:00
- **Authors**: Pablo Carballeira, Carlos Carmona, César Díaz, Daniel Berjón, Daniel Corregidor, Julián Cabrera, Francisco Morán, Carmen Doblado, Sergio Arnaldo, María del Mar Martín, Narciso García
- **Comment**: None
- **Journal**: None
- **Summary**: FVV Live is a novel end-to-end free-viewpoint video system, designed for low cost and real-time operation, based on off-the-shelf components. The system has been designed to yield high-quality free-viewpoint video using consumer-grade cameras and hardware, which enables low deployment costs and easy installation for immersive event-broadcasting or videoconferencing.   The paper describes the architecture of the system, including acquisition and encoding of multiview plus depth data in several capture servers and virtual view synthesis on an edge server. All the blocks of the system have been designed to overcome the limitations imposed by hardware and network, which impact directly on the accuracy of depth data and thus on the quality of virtual view synthesis. The design of FVV Live allows for an arbitrary number of cameras and capture servers, and the results presented in this paper correspond to an implementation with nine stereo-based depth cameras.   FVV Live presents low motion-to-photon and end-to-end delays, which enables seamless free-viewpoint navigation and bilateral immersive communications. Moreover, the visual quality of FVV Live has been assessed through subjective assessment with satisfactory results, and additional comparative tests show that it is preferred over state-of-the-art DIBR alternatives.



### HACT-Net: A Hierarchical Cell-to-Tissue Graph Neural Network for Histopathological Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2007.00584v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.00584v1)
- **Published**: 2020-07-01 16:22:48+00:00
- **Updated**: 2020-07-01 16:22:48+00:00
- **Authors**: Pushpak Pati, Guillaume Jaume, Lauren Alisha Fernandes, Antonio Foncubierta, Florinda Feroce, Anna Maria Anniciello, Giosue Scognamiglio, Nadia Brancati, Daniel Riccio, Maurizio Do Bonito, Giuseppe De Pietro, Gerardo Botti, Orcun Goksel, Jean-Philippe Thiran, Maria Frucci, Maria Gabrani
- **Comment**: None
- **Journal**: None
- **Summary**: Cancer diagnosis, prognosis, and therapeutic response prediction are heavily influenced by the relationship between the histopathological structures and the function of the tissue. Recent approaches acknowledging the structure-function relationship, have linked the structural and spatial patterns of cell organization in tissue via cell-graphs to tumor grades. Though cell organization is imperative, it is insufficient to entirely represent the histopathological structure. We propose a novel hierarchical cell-to-tissue-graph (HACT) representation to improve the structural depiction of the tissue. It consists of a low-level cell-graph, capturing cell morphology and interactions, a high-level tissue-graph, capturing morphology and spatial distribution of tissue parts, and cells-to-tissue hierarchies, encoding the relative spatial distribution of the cells with respect to the tissue distribution. Further, a hierarchical graph neural network (HACT-Net) is proposed to efficiently map the HACT representations to histopathological breast cancer subtypes. We assess the methodology on a large set of annotated tissue regions of interest from H\&E stained breast carcinoma whole-slides. Upon evaluation, the proposed method outperformed recent convolutional neural network and graph neural network approaches for breast cancer multi-class subtyping. The proposed entity-based topological analysis is more inline with the pathological diagnostic procedure of the tissue. It provides more command over the tissue modelling, therefore encourages the further inclusion of pathological priors into task-specific tissue representation.



### Lightweight Temporal Self-Attention for Classifying Satellite Image Time Series
- **Arxiv ID**: http://arxiv.org/abs/2007.00586v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.00586v3)
- **Published**: 2020-07-01 16:23:28+00:00
- **Updated**: 2020-07-08 13:36:34+00:00
- **Authors**: Vivien Sainte Fare Garnot, Loic Landrieu
- **Comment**: None
- **Journal**: None
- **Summary**: The increasing accessibility and precision of Earth observation satellite data offers considerable opportunities for industrial and state actors alike. This calls however for efficient methods able to process time-series on a global scale. Building on recent work employing multi-headed self-attention mechanisms to classify remote sensing time sequences, we propose a modification of the Temporal Attention Encoder. In our network, the channels of the temporal inputs are distributed among several compact attention heads operating in parallel. Each head extracts highly-specialized temporal features which are in turn concatenated into a single representation. Our approach outperforms other state-of-the-art time series classification algorithms on an open-access satellite image dataset, while using significantly fewer parameters and with a reduced computational complexity.



### A New Basis for Sparse Principal Component Analysis
- **Arxiv ID**: http://arxiv.org/abs/2007.00596v3
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG, stat.CO, stat.ME
- **Links**: [PDF](http://arxiv.org/pdf/2007.00596v3)
- **Published**: 2020-07-01 16:32:22+00:00
- **Updated**: 2023-08-04 03:12:41+00:00
- **Authors**: Fan Chen, Karl Rohe
- **Comment**: 50 pages, 10 figures
- **Journal**: None
- **Summary**: Previous versions of sparse principal component analysis (PCA) have presumed that the eigen-basis (a $p \times k$ matrix) is approximately sparse. We propose a method that presumes the $p \times k$ matrix becomes approximately sparse after a $k \times k$ rotation. The simplest version of the algorithm initializes with the leading $k$ principal components. Then, the principal components are rotated with an $k \times k$ orthogonal rotation to make them approximately sparse. Finally, soft-thresholding is applied to the rotated principal components. This approach differs from prior approaches because it uses an orthogonal rotation to approximate a sparse basis. One consequence is that a sparse component need not to be a leading eigenvector, but rather a mixture of them. In this way, we propose a new (rotated) basis for sparse PCA. In addition, our approach avoids "deflation" and multiple tuning parameters required for that. Our sparse PCA framework is versatile; for example, it extends naturally to a two-way analysis of a data matrix for simultaneous dimensionality reduction of rows and columns. We provide evidence showing that for the same level of sparsity, the proposed sparse PCA method is more stable and can explain more variance compared to alternative methods. Through three applications -- sparse coding of images, analysis of transcriptome sequencing data, and large-scale clustering of social networks, we demonstrate the modern usefulness of sparse PCA in exploring multivariate data.



### Exploiting the Logits: Joint Sign Language Recognition and Spell-Correction
- **Arxiv ID**: http://arxiv.org/abs/2007.00603v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.00603v1)
- **Published**: 2020-07-01 16:40:00+00:00
- **Updated**: 2020-07-01 16:40:00+00:00
- **Authors**: Christina Runkel, Stefan Dorenkamp, Hartmut Bauermeister, Michael Moeller
- **Comment**: First two authors contributed equally. Accepted at ICPR 2020
- **Journal**: None
- **Summary**: Machine learning techniques have excelled in the automatic semantic analysis of images, reaching human-level performances on challenging benchmarks. Yet, the semantic analysis of videos remains challenging due to the significantly higher dimensionality of the input data, respectively, the significantly higher need for annotated training examples. By studying the automatic recognition of German sign language videos, we demonstrate that on the relatively scarce training data of 2.800 videos, modern deep learning architectures for video analysis (such as ResNeXt) along with transfer learning on large gesture recognition tasks, can achieve about 75% character accuracy. Considering that this leaves us with a probability of under 25% that a 5 letter word is spelled correctly, spell-correction systems are crucial for producing readable outputs. The contribution of this paper is to propose a convolutional neural network for spell-correction that expects the softmax outputs of the character recognition network (instead of a misspelled word) as an input. We demonstrate that purely learning on softmax inputs in combination with scarce training data yields overfitting as the network learns the inputs by heart. In contrast, training the network on several variants of the logits of the classification output i.e. scaling by a constant factor, adding of random noise, mixing of softmax and hardmax inputs or purely training on hardmax inputs, leads to better generalization while benefitting from the significant information hidden in these outputs (that have 98% top-5 accuracy), yielding a readable text despite the comparably low character accuracy.



### A Multi-spectral Dataset for Evaluating Motion Estimation Systems
- **Arxiv ID**: http://arxiv.org/abs/2007.00622v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2007.00622v2)
- **Published**: 2020-07-01 17:11:02+00:00
- **Updated**: 2021-05-16 08:46:02+00:00
- **Authors**: Weichen Dai, Yu Zhang, Shenzhou Chen, Donglei Sun, Da Kong
- **Comment**: None
- **Journal**: None
- **Summary**: Visible images have been widely used for motion estimation. Thermal images, in contrast, are more challenging to be used in motion estimation since they typically have lower resolution, less texture, and more noise. In this paper, a novel dataset for evaluating the performance of multi-spectral motion estimation systems is presented. All the sequences are recorded from a handheld multi-spectral device. It consists of a standard visible-light camera, a long-wave infrared camera, an RGB-D camera, and an inertial measurement unit (IMU). The multi-spectral images, including both color and thermal images in full sensor resolution (640 x 480), are obtained from a standard and a long-wave infrared camera at 32Hz with hardware-synchronization. The depth images are captured by a Microsoft Kinect2 and can have benefits for learning cross-modalities stereo matching. For trajectory evaluation, accurate ground-truth camera poses obtained from a motion capture system are provided. In addition to the sequences with bright illumination, the dataset also contains dim, varying, and complex illumination scenes. The full dataset, including raw data and calibration data with detailed data format specifications, is publicly available.



### Causal Discovery in Physical Systems from Videos
- **Arxiv ID**: http://arxiv.org/abs/2007.00631v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.00631v3)
- **Published**: 2020-07-01 17:29:57+00:00
- **Updated**: 2020-11-29 20:47:06+00:00
- **Authors**: Yunzhu Li, Antonio Torralba, Animashree Anandkumar, Dieter Fox, Animesh Garg
- **Comment**: NeurIPS 2020. Project page: https://yunzhuli.github.io/V-CDN/
- **Journal**: None
- **Summary**: Causal discovery is at the core of human cognition. It enables us to reason about the environment and make counterfactual predictions about unseen scenarios that can vastly differ from our previous experiences. We consider the task of causal discovery from videos in an end-to-end fashion without supervision on the ground-truth graph structure. In particular, our goal is to discover the structural dependencies among environmental and object variables: inferring the type and strength of interactions that have a causal effect on the behavior of the dynamical system. Our model consists of (a) a perception module that extracts a semantically meaningful and temporally consistent keypoint representation from images, (b) an inference module for determining the graph distribution induced by the detected keypoints, and (c) a dynamics module that can predict the future by conditioning on the inferred graph. We assume access to different configurations and environmental conditions, i.e., data from unknown interventions on the underlying system; thus, we can hope to discover the correct underlying causal graph without explicit interventions. We evaluate our method in a planar multi-body interaction environment and scenarios involving fabrics of different shapes like shirts and pants. Experiments demonstrate that our model can correctly identify the interactions from a short sequence of images and make long-term future predictions. The causal structure assumed by the model also allows it to make counterfactual predictions and extrapolate to systems of unseen interaction graphs or graphs of various sizes.



### End-to-End JPEG Decoding and Artifacts Suppression Using Heterogeneous Residual Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2007.00639v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.00639v1)
- **Published**: 2020-07-01 17:44:00+00:00
- **Updated**: 2020-07-01 17:44:00+00:00
- **Authors**: Jun Niu
- **Comment**: None
- **Journal**: None
- **Summary**: Existing deep learning models separate JPEG artifacts suppression from the decoding protocol as independent task. In this work, we take one step forward to design a true end-to-end heterogeneous residual convolutional neural network (HR-CNN) with spectrum decomposition and heterogeneous reconstruction mechanism. Benefitting from the full CNN architecture and GPU acceleration, the proposed model considerably improves the reconstruction efficiency. Numerical experiments show that the overall reconstruction speed reaches to the same magnitude of the standard CPU JPEG decoding protocol, while both decoding and artifacts suppression are completed together. We formulate the JPEG artifacts suppression task as an interactive process of decoding and image detail reconstructions. A heterogeneous, fully convolutional, mechanism is proposed to particularly address the uncorrelated nature of different spectral channels. Directly starting from the JPEG code in k-space, the network first extracts the spectral samples channel by channel, and restores the spectral snapshots with expanded throughput. These intermediate snapshots are then heterogeneously decoded and merged into the pixel space image. A cascaded residual learning segment is designed to further enhance the image details. Experiments verify that the model achieves outstanding performance in JPEG artifacts suppression, while its full convolutional operations and elegant network structure offers higher computational efficiency for practical online usage compared with other deep learning models on this topic.



### Object Goal Navigation using Goal-Oriented Semantic Exploration
- **Arxiv ID**: http://arxiv.org/abs/2007.00643v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2007.00643v2)
- **Published**: 2020-07-01 17:52:32+00:00
- **Updated**: 2020-07-02 01:38:41+00:00
- **Authors**: Devendra Singh Chaplot, Dhiraj Gandhi, Abhinav Gupta, Ruslan Salakhutdinov
- **Comment**: Winner of the CVPR 2020 AI-Habitat Object Goal Navigation Challenge.
  See the project webpage at
  https://devendrachaplot.github.io/projects/semantic-exploration.html
- **Journal**: None
- **Summary**: This work studies the problem of object goal navigation which involves navigating to an instance of the given object category in unseen environments. End-to-end learning-based navigation methods struggle at this task as they are ineffective at exploration and long-term planning. We propose a modular system called, `Goal-Oriented Semantic Exploration' which builds an episodic semantic map and uses it to explore the environment efficiently based on the goal object category. Empirical results in visually realistic simulation environments show that the proposed model outperforms a wide range of baselines including end-to-end learning-based methods as well as modular map-based methods and led to the winning entry of the CVPR-2020 Habitat ObjectNav Challenge. Ablation analysis indicates that the proposed model learns semantic priors of the relative arrangement of objects in a scene, and uses them to explore efficiently. Domain-agnostic module design allow us to transfer our model to a mobile robot platform and achieve similar performance for object goal navigation in the real-world.



### Measuring Robustness to Natural Distribution Shifts in Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2007.00644v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.00644v2)
- **Published**: 2020-07-01 17:53:26+00:00
- **Updated**: 2020-09-14 09:55:13+00:00
- **Authors**: Rohan Taori, Achal Dave, Vaishaal Shankar, Nicholas Carlini, Benjamin Recht, Ludwig Schmidt
- **Comment**: None
- **Journal**: None
- **Summary**: We study how robust current ImageNet models are to distribution shifts arising from natural variations in datasets. Most research on robustness focuses on synthetic image perturbations (noise, simulated weather artifacts, adversarial examples, etc.), which leaves open how robustness on synthetic distribution shift relates to distribution shift arising in real data. Informed by an evaluation of 204 ImageNet models in 213 different test conditions, we find that there is often little to no transfer of robustness from current synthetic to natural distribution shift. Moreover, most current techniques provide no robustness to the natural distribution shifts in our testbed. The main exception is training on larger and more diverse datasets, which in multiple cases increases robustness, but is still far from closing the performance gaps. Our results indicate that distribution shifts arising in real data are currently an open research problem. We provide our testbed and data as a resource for future work at https://modestyachts.github.io/imagenet-testbed/ .



### Group Ensemble: Learning an Ensemble of ConvNets in a single ConvNet
- **Arxiv ID**: http://arxiv.org/abs/2007.00649v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.00649v1)
- **Published**: 2020-07-01 17:56:06+00:00
- **Updated**: 2020-07-01 17:56:06+00:00
- **Authors**: Hao Chen, Abhinav Shrivastava
- **Comment**: None
- **Journal**: None
- **Summary**: Ensemble learning is a general technique to improve accuracy in machine learning. However, the heavy computation of a ConvNets ensemble limits its usage in deep learning. In this paper, we present Group Ensemble Network (GENet), an architecture incorporating an ensemble of ConvNets in a single ConvNet. Through a shared-base and multi-head structure, GENet is divided into several groups to make explicit ensemble learning possible in a single ConvNet. Owing to group convolution and the shared-base, GENet can fully leverage the advantage of explicit ensemble learning while retaining the same computation as a single ConvNet. Additionally, we present Group Averaging, Group Wagging and Group Boosting as three different strategies to aggregate these ensemble members. Finally, GENet outperforms larger single networks, standard ensembles of smaller networks, and other recent state-of-the-art methods on CIFAR and ImageNet. Specifically, group ensemble reduces the top-1 error by 1.83% for ResNeXt-50 on ImageNet. We also demonstrate its effectiveness on action recognition and object detection tasks.



### Swapping Autoencoder for Deep Image Manipulation
- **Arxiv ID**: http://arxiv.org/abs/2007.00653v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.00653v2)
- **Published**: 2020-07-01 17:59:57+00:00
- **Updated**: 2020-12-14 09:41:33+00:00
- **Authors**: Taesung Park, Jun-Yan Zhu, Oliver Wang, Jingwan Lu, Eli Shechtman, Alexei A. Efros, Richard Zhang
- **Comment**: NeurIPS 2020. Please visit https://taesung.me/SwappingAutoencoder/
  for an introductory video. v2 mainly contains reorganization of the
  Introduction and Broader Impact section
- **Journal**: None
- **Summary**: Deep generative models have become increasingly effective at producing realistic images from randomly sampled seeds, but using such models for controllable manipulation of existing images remains challenging. We propose the Swapping Autoencoder, a deep model designed specifically for image manipulation, rather than random sampling. The key idea is to encode an image with two independent components and enforce that any swapped combination maps to a realistic image. In particular, we encourage the components to represent structure and texture, by enforcing one component to encode co-occurrent patch statistics across different parts of an image. As our method is trained with an encoder, finding the latent codes for a new input image becomes trivial, rather than cumbersome. As a result, it can be used to manipulate real input images in various ways, including texture swapping, local and global editing, and latent code vector arithmetic. Experiments on multiple datasets show that our model produces better results and is substantially more efficient compared to recent generative models.



### ConFoc: Content-Focus Protection Against Trojan Attacks on Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2007.00711v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.00711v1)
- **Published**: 2020-07-01 19:25:34+00:00
- **Updated**: 2020-07-01 19:25:34+00:00
- **Authors**: Miguel Villarreal-Vasquez, Bharat Bhargava
- **Comment**: 13 pages (excluding references), 7 figures, 7 tables
- **Journal**: None
- **Summary**: Deep Neural Networks (DNNs) have been applied successfully in computer vision. However, their wide adoption in image-related applications is threatened by their vulnerability to trojan attacks. These attacks insert some misbehavior at training using samples with a mark or trigger, which is exploited at inference or testing time. In this work, we analyze the composition of the features learned by DNNs at training. We identify that they, including those related to the inserted triggers, contain both content (semantic information) and style (texture information), which are recognized as a whole by DNNs at testing time. We then propose a novel defensive technique against trojan attacks, in which DNNs are taught to disregard the styles of inputs and focus on their content only to mitigate the effect of triggers during the classification. The generic applicability of the approach is demonstrated in the context of a traffic sign and a face recognition application. Each of them is exposed to a different attack with a variety of triggers. Results show that the method reduces the attack success rate significantly to values < 1% in all the tested attacks while keeping as well as improving the initial accuracy of the models when processing both benign and adversarial data.



### Adversarial Example Games
- **Arxiv ID**: http://arxiv.org/abs/2007.00720v6
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.00720v6)
- **Published**: 2020-07-01 19:47:23+00:00
- **Updated**: 2021-01-09 01:44:02+00:00
- **Authors**: Avishek Joey Bose, Gauthier Gidel, Hugo Berard, Andre Cianflone, Pascal Vincent, Simon Lacoste-Julien, William L. Hamilton
- **Comment**: Appears in: Advances in Neural Information Processing Systems 33
  (NeurIPS 2020)
- **Journal**: None
- **Summary**: The existence of adversarial examples capable of fooling trained neural network classifiers calls for a much better understanding of possible attacks to guide the development of safeguards against them. This includes attack methods in the challenging non-interactive blackbox setting, where adversarial attacks are generated without any access, including queries, to the target model. Prior attacks in this setting have relied mainly on algorithmic innovations derived from empirical observations (e.g., that momentum helps), lacking principled transferability guarantees. In this work, we provide a theoretical foundation for crafting transferable adversarial examples to entire hypothesis classes. We introduce Adversarial Example Games (AEG), a framework that models the crafting of adversarial examples as a min-max game between a generator of attacks and a classifier. AEG provides a new way to design adversarial examples by adversarially training a generator and a classifier from a given hypothesis class (e.g., architecture). We prove that this game has an equilibrium, and that the optimal generator is able to craft adversarial examples that can attack any classifier from the corresponding hypothesis class. We demonstrate the efficacy of AEG on the MNIST and CIFAR-10 datasets, outperforming prior state-of-the-art approaches with an average relative improvement of $29.9\%$ and $47.2\%$ against undefended and robust models (Table 2 & 3) respectively.



### Learning Geocentric Object Pose in Oblique Monocular Images
- **Arxiv ID**: http://arxiv.org/abs/2007.00729v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.00729v1)
- **Published**: 2020-07-01 20:06:19+00:00
- **Updated**: 2020-07-01 20:06:19+00:00
- **Authors**: Gordon Christie, Rodrigo Rene Rai Munoz Abujder, Kevin Foster, Shea Hagstrom, Gregory D. Hager, Myron Z. Brown
- **Comment**: CVPR 2020
- **Journal**: None
- **Summary**: An object's geocentric pose, defined as the height above ground and orientation with respect to gravity, is a powerful representation of real-world structure for object detection, segmentation, and localization tasks using RGBD images. For close-range vision tasks, height and orientation have been derived directly from stereo-computed depth and more recently from monocular depth predicted by deep networks. For long-range vision tasks such as Earth observation, depth cannot be reliably estimated with monocular images. Inspired by recent work in monocular height above ground prediction and optical flow prediction from static images, we develop an encoding of geocentric pose to address this challenge and train a deep network to compute the representation densely, supervised by publicly available airborne lidar. We exploit these attributes to rectify oblique images and remove observed object parallax to dramatically improve the accuracy of localization and to enable accurate alignment of multiple images taken from very different oblique viewpoints. We demonstrate the value of our approach by extending two large-scale public datasets for semantic segmentation in oblique satellite images. All of our data and code are publicly available.



### Virtual Testbed for Monocular Visual Navigation of Small Unmanned Aircraft Systems
- **Arxiv ID**: http://arxiv.org/abs/2007.00737v1
- **DOI**: 10.1177/1548512920954545
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2007.00737v1)
- **Published**: 2020-07-01 20:35:26+00:00
- **Updated**: 2020-07-01 20:35:26+00:00
- **Authors**: Kyung Kim, Robert C. Leishman, Scott L. Nykl
- **Comment**: None
- **Journal**: The Journal of Defense Modeling and Simulation: Applications,
  Methodology, Technology 2020
- **Summary**: Monocular visual navigation methods have seen significant advances in the last decade, recently producing several real-time solutions for autonomously navigating small unmanned aircraft systems without relying on GPS. This is critical for military operations which may involve environments where GPS signals are degraded or denied. However, testing and comparing visual navigation algorithms remains a challenge since visual data is expensive to gather. Conducting flight tests in a virtual environment is an attractive solution prior to committing to outdoor testing.   This work presents a virtual testbed for conducting simulated flight tests over real-world terrain and analyzing the real-time performance of visual navigation algorithms at 31 Hz. This tool was created to ultimately find a visual odometry algorithm appropriate for further GPS-denied navigation research on fixed-wing aircraft, even though all of the algorithms were designed for other modalities. This testbed was used to evaluate three current state-of-the-art, open-source monocular visual odometry algorithms on a fixed-wing platform: Direct Sparse Odometry, Semi-Direct Visual Odometry, and ORB-SLAM2 (with loop closures disabled).



### Deep learning-based holographic polarization microscopy
- **Arxiv ID**: http://arxiv.org/abs/2007.00741v1
- **DOI**: 10.1021/acsphotonics.0c01051
- **Categories**: **physics.optics**, cs.CV, eess.IV, physics.app-ph
- **Links**: [PDF](http://arxiv.org/pdf/2007.00741v1)
- **Published**: 2020-07-01 20:39:50+00:00
- **Updated**: 2020-07-01 20:39:50+00:00
- **Authors**: Tairan Liu, Kevin de Haan, Bijie Bai, Yair Rivenson, Yi Luo, Hongda Wang, David Karalli, Hongxiang Fu, Yibo Zhang, John FitzGerald, Aydogan Ozcan
- **Comment**: 20 pages, 8 figures
- **Journal**: ACS Photonics (2020)
- **Summary**: Polarized light microscopy provides high contrast to birefringent specimen and is widely used as a diagnostic tool in pathology. However, polarization microscopy systems typically operate by analyzing images collected from two or more light paths in different states of polarization, which lead to relatively complex optical designs, high system costs or experienced technicians being required. Here, we present a deep learning-based holographic polarization microscope that is capable of obtaining quantitative birefringence retardance and orientation information of specimen from a phase recovered hologram, while only requiring the addition of one polarizer/analyzer pair to an existing holographic imaging system. Using a deep neural network, the reconstructed holographic images from a single state of polarization can be transformed into images equivalent to those captured using a single-shot computational polarized light microscope (SCPLM). Our analysis shows that a trained deep neural network can extract the birefringence information using both the sample specific morphological features as well as the holographic amplitude and phase distribution. To demonstrate the efficacy of this method, we tested it by imaging various birefringent samples including e.g., monosodium urate (MSU) and triamcinolone acetonide (TCA) crystals. Our method achieves similar results to SCPLM both qualitatively and quantitatively, and due to its simpler optical design and significantly larger field-of-view, this method has the potential to expand the access to polarization microscopy and its use for medical diagnosis in resource limited settings.



### Weakly-Supervised Segmentation for Disease Localization in Chest X-Ray Images
- **Arxiv ID**: http://arxiv.org/abs/2007.00748v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.00748v1)
- **Published**: 2020-07-01 20:48:35+00:00
- **Updated**: 2020-07-01 20:48:35+00:00
- **Authors**: Ostap Viniavskyi, Mariia Dobko, Oles Dobosevych
- **Comment**: Accepted to AIME 2020
- **Journal**: None
- **Summary**: Deep Convolutional Neural Networks have proven effective in solving the task of semantic segmentation. However, their efficiency heavily relies on the pixel-level annotations that are expensive to get and often require domain expertise, especially in medical imaging. Weakly supervised semantic segmentation helps to overcome these issues and also provides explainable deep learning models. In this paper, we propose a novel approach to the semantic segmentation of medical chest X-ray images with only image-level class labels as supervision. We improve the disease localization accuracy by combining three approaches as consecutive steps. First, we generate pseudo segmentation labels of abnormal regions in the training images through a supervised classification model enhanced with a regularization procedure. The obtained activation maps are then post-processed and propagated into a second classification model-Inter-pixel Relation Network, which improves the boundaries between different object classes. Finally, the resulting pseudo-labels are used to train a proposed fully supervised segmentation model. We analyze the robustness of the presented method and test its performance on two distinct datasets: PASCAL VOC 2012 and SIIM-ACR Pneumothorax. We achieve significant results in the segmentation on both datasets using only image-level annotations. We show that this approach is applicable to chest X-rays for detecting an anomalous volume of air in the pleural space between the lung and the chest wall. Our code has been made publicly available.



### Rapid tissue oxygenation mapping from snapshot structured-light images with adversarial deep learning
- **Arxiv ID**: http://arxiv.org/abs/2007.00760v1
- **DOI**: 10.1117/1.JBO.25.11.112907
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.00760v1)
- **Published**: 2020-07-01 21:14:05+00:00
- **Updated**: 2020-07-01 21:14:05+00:00
- **Authors**: Mason T. Chen, Nicholas J. Durr
- **Comment**: None
- **Journal**: None
- **Summary**: Spatial frequency domain imaging (SFDI) is a powerful technique for mapping tissue oxygen saturation over a wide field of view. However, current SFDI methods either require a sequence of several images with different illumination patterns or, in the case of single snapshot optical properties (SSOP), introduce artifacts and sacrifice accuracy. To avoid this tradeoff, we introduce OxyGAN: a data-driven, content-aware method to estimate tissue oxygenation directly from single structured light images using end-to-end generative adversarial networks. Conventional SFDI is used to obtain ground truth tissue oxygenation maps for ex vivo human esophagi, in vivo hands and feet, and an in vivo pig colon sample under 659 nm and 851 nm sinusoidal illumination. We benchmark OxyGAN by comparing to SSOP and to a two-step hybrid technique that uses a previously-developed deep learning model to predict optical properties followed by a physical model to calculate tissue oxygenation. When tested on human feet, a cross-validated OxyGAN maps tissue oxygenation with an accuracy of 96.5%. When applied to sample types not included in the training set, such as human hands and pig colon, OxyGAN achieves a 93.0% accuracy, demonstrating robustness to various tissue types. On average, OxyGAN outperforms SSOP and a hybrid model in estimating tissue oxygenation by 24.9% and 24.7%, respectively. Lastly, we optimize OxyGAN inference so that oxygenation maps are computed ~10 times faster than previous work, enabling video-rate, 25Hz imaging. Due to its rapid acquisition and processing speed, OxyGAN has the potential to enable real-time, high-fidelity tissue oxygenation mapping that may be useful for many clinical applications.



### Self-supervised Deep Reconstruction of Mixed Strip-shredded Text Documents
- **Arxiv ID**: http://arxiv.org/abs/2007.00779v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.00779v1)
- **Published**: 2020-07-01 21:48:05+00:00
- **Updated**: 2020-07-01 21:48:05+00:00
- **Authors**: Thiago M. Paixão, Rodrigo F. Berriel, Maria C. S. Boeres, Alessandro L. Koerich, Claudine Badue, Alberto F. de Souza, Thiago Oliveira-Santos
- **Comment**: Accepted for publication in Pattern Recognition
- **Journal**: None
- **Summary**: The reconstruction of shredded documents consists of coherently arranging fragments of paper (shreds) to recover the original document(s). A great challenge in computational reconstruction is to properly evaluate the compatibility between the shreds. While traditional pixel-based approaches are not robust to real shredding, more sophisticated solutions compromise significantly time performance. The solution presented in this work extends our previous deep learning method for single-page reconstruction to a more realistic/complex scenario: the reconstruction of several mixed shredded documents at once. In our approach, the compatibility evaluation is modeled as a two-class (valid or invalid) pattern recognition problem. The model is trained in a self-supervised manner on samples extracted from simulated-shredded documents, which obviates manual annotation. Experimental results on three datasets -- including a new collection of 100 strip-shredded documents produced for this work -- have shown that the proposed method outperforms the competing ones on complex scenarios, achieving accuracy superior to 90%.



### Age-Oriented Face Synthesis with Conditional Discriminator Pool and Adversarial Triplet Loss
- **Arxiv ID**: http://arxiv.org/abs/2007.00792v2
- **DOI**: 10.1109/TIP.2021.3084106
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.00792v2)
- **Published**: 2020-07-01 22:18:21+00:00
- **Updated**: 2020-07-03 23:31:58+00:00
- **Authors**: Haoyi Wang, Victor Sanchez, Chang-Tsun Li
- **Comment**: None
- **Journal**: None
- **Summary**: The vanilla Generative Adversarial Networks (GAN) are commonly used to generate realistic images depicting aged and rejuvenated faces. However, the performance of such vanilla GANs in the age-oriented face synthesis task is often compromised by the mode collapse issue, which may result in the generation of faces with minimal variations and a poor synthesis accuracy. In addition, recent age-oriented face synthesis methods use the L1 or L2 constraint to preserve the identity information on synthesized faces, which implicitly limits the identity permanence capabilities when these constraints are associated with a trivial weighting factor. In this paper, we propose a method for the age-oriented face synthesis task that achieves a high synthesis accuracy with strong identity permanence capabilities. Specifically, to achieve a high synthesis accuracy, our method tackles the mode collapse issue with a novel Conditional Discriminator Pool (CDP), which consists of multiple discriminators, each targeting one particular age category. To achieve strong identity permanence capabilities, our method uses a novel Adversarial Triplet loss. This loss, which is based on the Triplet loss, adds a ranking operation to further pull the positive embedding towards the anchor embedding resulting in significantly reduced intra-class variances in the feature space. Through extensive experiments, we show that our proposed method outperforms state-of-the-art methods in terms of synthesis accuracy and identity permanence capabilities, qualitatively and quantitatively.



### Learning Surrogates via Deep Embedding
- **Arxiv ID**: http://arxiv.org/abs/2007.00799v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.00799v2)
- **Published**: 2020-07-01 22:55:41+00:00
- **Updated**: 2020-07-17 12:39:16+00:00
- **Authors**: Yash Patel, Tomas Hodan, Jiri Matas
- **Comment**: ECCV 2020 camera-ready version
- **Journal**: None
- **Summary**: This paper proposes a technique for training a neural network by minimizing a surrogate loss that approximates the target evaluation metric, which may be non-differentiable. The surrogate is learned via a deep embedding where the Euclidean distance between the prediction and the ground truth corresponds to the value of the evaluation metric. The effectiveness of the proposed technique is demonstrated in a post-tuning setup, where a trained model is tuned using the learned surrogate. Without a significant computational overhead and any bells and whistles, improvements are demonstrated on challenging and practical tasks of scene-text recognition and detection. In the recognition task, the model is tuned using a surrogate approximating the edit distance metric and achieves up to $39\%$ relative improvement in the total edit distance. In the detection task, the surrogate approximates the intersection over union metric for rotated bounding boxes and yields up to $4.25\%$ relative improvement in the $F_{1}$ score.



### TiledSoilingNet: Tile-level Soiling Detection on Automotive Surround-view Cameras Using Coverage Metric
- **Arxiv ID**: http://arxiv.org/abs/2007.00801v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2007.00801v1)
- **Published**: 2020-07-01 23:00:47+00:00
- **Updated**: 2020-07-01 23:00:47+00:00
- **Authors**: Arindam Das, Pavel Krizek, Ganesh Sistu, Fabian Burger, Sankaralingam Madasamy, Michal Uricar, Varun Ravi Kumar, Senthil Yogamani
- **Comment**: Accepted for Oral Presentation at IEEE Intelligent Transportation
  Systems Conference (ITSC) 2020
- **Journal**: None
- **Summary**: Automotive cameras, particularly surround-view cameras, tend to get soiled by mud, water, snow, etc. For higher levels of autonomous driving, it is necessary to have a soiling detection algorithm which will trigger an automatic cleaning system. Localized detection of soiling in an image is necessary to control the cleaning system. It is also necessary to enable partial functionality in unsoiled areas while reducing confidence in soiled areas. Although this can be solved using a semantic segmentation task, we explore a more efficient solution targeting deployment in low power embedded system. We propose a novel method to regress the area of each soiling type within a tile directly. We refer to this as coverage. The proposed approach is better than learning the dominant class in a tile as multiple soiling types occur within a tile commonly. It also has the advantage of dealing with coarse polygon annotation, which will cause the segmentation task. The proposed soiling coverage decoder is an order of magnitude faster than an equivalent segmentation decoder. We also integrated it into an object detection and semantic segmentation multi-task model using an asynchronous back-propagation algorithm. A portion of the dataset used will be released publicly as part of our WoodScape dataset to encourage further research.



### Query-Free Adversarial Transfer via Undertrained Surrogates
- **Arxiv ID**: http://arxiv.org/abs/2007.00806v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.00806v2)
- **Published**: 2020-07-01 23:12:22+00:00
- **Updated**: 2020-11-28 06:05:53+00:00
- **Authors**: Chris Miller, Soroush Vosoughi
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks are vulnerable to adversarial examples -- minor perturbations added to a model's input which cause the model to output an incorrect prediction. We introduce a new method for improving the efficacy of adversarial attacks in a black-box setting by undertraining the surrogate model which the attacks are generated on. Using two datasets and five model architectures, we show that this method transfers well across architectures and outperforms state-of-the-art methods by a wide margin. We interpret the effectiveness of our approach as a function of reduced surrogate model loss function curvature and increased universal gradient characteristics, and show that our approach reduces the presence of local loss maxima which hinder transferability. Our results suggest that finding strong single surrogate models is a highly effective and simple method for generating transferable adversarial attacks, and that this method represents a valuable route for future study in this field.



