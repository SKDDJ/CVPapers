# Arxiv Papers in cs.CV on 2020-07-16
### Negative Pseudo Labeling using Class Proportion for Semantic Segmentation in Pathology
- **Arxiv ID**: http://arxiv.org/abs/2007.08044v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.08044v1)
- **Published**: 2020-07-16 00:28:07+00:00
- **Updated**: 2020-07-16 00:28:07+00:00
- **Authors**: Hiroki Tokunaga, Brian Kenji Iwana, Yuki Teramoto, Akihiko Yoshizawa, Ryoma Bise
- **Comment**: 17 pages, 7 figures, Accepted in ECCV 2020
- **Journal**: None
- **Summary**: We propose a weakly-supervised cell tracking method that can train a convolutional neural network (CNN) by using only the annotation of "cell detection" (i.e., the coordinates of cell positions) without association information, in which cell positions can be easily obtained by nuclear staining. First, we train a co-detection CNN that detects cells in successive frames by using weak-labels. Our key assumption is that the co-detection CNN implicitly learns association in addition to detection. To obtain the association information, we propose a backward-and-forward propagation method that analyzes the correspondence of cell positions in the detection maps output of the co-detection CNN. Experiments demonstrated that the proposed method can match positions by analyzing the co-detection CNN. Even though the method uses only weak supervision, the performance of our method was almost the same as the state-of-the-art supervised method.



### Learning End-to-End Action Interaction by Paired-Embedding Data Augmentation
- **Arxiv ID**: http://arxiv.org/abs/2007.08071v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.08071v1)
- **Published**: 2020-07-16 01:54:16+00:00
- **Updated**: 2020-07-16 01:54:16+00:00
- **Authors**: Ziyang Song, Zejian Yuan, Chong Zhang, Wanchao Chi, Yonggen Ling, Shenghao Zhang
- **Comment**: 16 pages, 7 figures
- **Journal**: None
- **Summary**: In recognition-based action interaction, robots' responses to human actions are often pre-designed according to recognized categories and thus stiff. In this paper, we specify a new Interactive Action Translation (IAT) task which aims to learn end-to-end action interaction from unlabeled interactive pairs, removing explicit action recognition. To enable learning on small-scale data, we propose a Paired-Embedding (PE) method for effective and reliable data augmentation. Specifically, our method first utilizes paired relationships to cluster individual actions in an embedding space. Then two actions originally paired can be replaced with other actions in their respective neighborhood, assembling into new pairs. An Act2Act network based on conditional GAN follows to learn from augmented data. Besides, IAT-test and IAT-train scores are specifically proposed for evaluating methods on our task. Experimental results on two datasets show impressive effects and broad application prospects of our method.



### Unseen Object Instance Segmentation for Robotic Environments
- **Arxiv ID**: http://arxiv.org/abs/2007.08073v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2007.08073v2)
- **Published**: 2020-07-16 01:59:13+00:00
- **Updated**: 2021-10-14 02:56:33+00:00
- **Authors**: Christopher Xie, Yu Xiang, Arsalan Mousavian, Dieter Fox
- **Comment**: Journal version of arXiv:1907.13236
- **Journal**: None
- **Summary**: In order to function in unstructured environments, robots need the ability to recognize unseen objects. We take a step in this direction by tackling the problem of segmenting unseen object instances in tabletop environments. However, the type of large-scale real-world dataset required for this task typically does not exist for most robotic settings, which motivates the use of synthetic data. Our proposed method, UOIS-Net, separately leverages synthetic RGB and synthetic depth for unseen object instance segmentation. UOIS-Net is comprised of two stages: first, it operates only on depth to produce object instance center votes in 2D or 3D and assembles them into rough initial masks. Secondly, these initial masks are refined using RGB. Surprisingly, our framework is able to learn from synthetic RGB-D data where the RGB is non-photorealistic. To train our method, we introduce a large-scale synthetic dataset of random objects on tabletops. We show that our method can produce sharp and accurate segmentation masks, outperforming state-of-the-art methods on unseen object instance segmentation. We also show that our method can segment unseen objects for robot grasping.



### Suppress and Balance: A Simple Gated Network for Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2007.08074v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.08074v3)
- **Published**: 2020-07-16 02:00:53+00:00
- **Updated**: 2020-07-27 09:34:12+00:00
- **Authors**: Xiaoqi Zhao, Youwei Pang, Lihe Zhang, Huchuan Lu, Lei Zhang
- **Comment**: Accepted in ECCV2020(oral). Code:
  https://github.com/Xiaoqi-Zhao-DLUT/GateNet-RGB-Saliency
- **Journal**: None
- **Summary**: Most salient object detection approaches use U-Net or feature pyramid networks (FPN) as their basic structures. These methods ignore two key problems when the encoder exchanges information with the decoder: one is the lack of interference control between them, the other is without considering the disparity of the contributions of different encoder blocks. In this work, we propose a simple gated network (GateNet) to solve both issues at once. With the help of multilevel gate units, the valuable context information from the encoder can be optimally transmitted to the decoder. We design a novel gated dual branch structure to build the cooperation among different levels of features and improve the discriminability of the whole network. Through the dual branch design, more details of the saliency map can be further restored. In addition, we adopt the atrous spatial pyramid pooling based on the proposed "Fold" operation (Fold-ASPP) to accurately localize salient objects of various scales. Extensive experiments on five challenging datasets demonstrate that the proposed model performs favorably against most state-of-the-art methods under different evaluation metrics.



### Memory based fusion for multi-modal deep learning
- **Arxiv ID**: http://arxiv.org/abs/2007.08076v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.08076v3)
- **Published**: 2020-07-16 02:05:54+00:00
- **Updated**: 2020-10-23 05:22:34+00:00
- **Authors**: Darshana Priyasad, Tharindu Fernando, Simon Denman, Sridha Sridharan, Clinton Fookes
- **Comment**: Pre-print submitted to Information Fusion
- **Journal**: None
- **Summary**: The use of multi-modal data for deep machine learning has shown promise when compared to uni-modal approaches with fusion of multi-modal features resulting in improved performance in several applications. However, most state-of-the-art methods use naive fusion which processes feature streams independently, ignoring possible long-term dependencies within the data during fusion. In this paper, we present a novel Memory based Attentive Fusion layer, which fuses modes by incorporating both the current features and longterm dependencies in the data, thus allowing the model to understand the relative importance of modes over time. We introduce an explicit memory block within the fusion layer which stores features containing long-term dependencies of the fused data. The feature inputs from uni-modal encoders are fused through attentive composition and transformation followed by naive fusion of the resultant memory derived features with layer inputs. Following state-of-the-art methods, we have evaluated the performance and the generalizability of the proposed fusion approach on two different datasets with different modalities. In our experiments, we replace the naive fusion layer in benchmark networks with our proposed layer to enable a fair comparison. Experimental results indicate that the MBAF layer can generalise across different modalities and networks to enhance fusion and improve performance.



### EfficientHRNet: Efficient Scaling for Lightweight High-Resolution Multi-Person Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2007.08090v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2007.08090v2)
- **Published**: 2020-07-16 03:27:26+00:00
- **Updated**: 2020-12-30 17:43:31+00:00
- **Authors**: Christopher Neff, Aneri Sheth, Steven Furgurson, Hamed Tabkhi
- **Comment**: 11 pages (13 with references), 3 figures
- **Journal**: None
- **Summary**: There is an increasing demand for lightweight multi-person pose estimation for many emerging smart IoT applications. However, the existing algorithms tend to have large model sizes and intense computational requirements, making them ill-suited for real-time applications and deployment on resource-constrained hardware. Lightweight and real-time approaches are exceedingly rare and come at the cost of inferior accuracy. In this paper, we present EfficientHRNet, a family of lightweight multi-person human pose estimators that are able to perform in real-time on resource-constrained devices. By unifying recent advances in model scaling with high-resolution feature representations, EfficientHRNet creates highly accurate models while reducing computation enough to achieve real-time performance. The largest model is able to come within 4.4% accuracy of the current state-of-the-art, while having 1/3 the model size and 1/6 the computation, achieving 23 FPS on Nvidia Jetson Xavier. Compared to the top real-time approach, EfficientHRNet increases accuracy by 22% while achieving similar FPS with 1/3 the power. At every level, EfficientHRNet proves to be more computationally efficient than other bottom-up 2D human pose estimation approaches, while achieving highly competitive accuracy.



### TrashCan: A Semantically-Segmented Dataset towards Visual Detection of Marine Debris
- **Arxiv ID**: http://arxiv.org/abs/2007.08097v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2007.08097v1)
- **Published**: 2020-07-16 04:19:06+00:00
- **Updated**: 2020-07-16 04:19:06+00:00
- **Authors**: Jungseok Hong, Michael Fulton, Junaed Sattar
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents TrashCan, a large dataset comprised of images of underwater trash collected from a variety of sources, annotated both using bounding boxes and segmentation labels, for development of robust detectors of marine debris. The dataset has two versions, TrashCan-Material and TrashCan-Instance, corresponding to different object class configurations. The eventual goal is to develop efficient and accurate trash detection methods suitable for onboard robot deployment. Along with information about the construction and sourcing of the TrashCan dataset, we present initial results of instance segmentation from Mask R-CNN and object detection from Faster R-CNN. These do not represent the best possible detection results but provides an initial baseline for future work in instance segmentation and object detection on the TrashCan dataset.



### Probabilistic Anchor Assignment with IoU Prediction for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2007.08103v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.08103v2)
- **Published**: 2020-07-16 04:26:57+00:00
- **Updated**: 2020-09-05 07:44:24+00:00
- **Authors**: Kang Kim, Hee Seok Lee
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: In object detection, determining which anchors to assign as positive or negative samples, known as anchor assignment, has been revealed as a core procedure that can significantly affect a model's performance. In this paper we propose a novel anchor assignment strategy that adaptively separates anchors into positive and negative samples for a ground truth bounding box according to the model's learning status such that it is able to reason about the separation in a probabilistic manner. To do so we first calculate the scores of anchors conditioned on the model and fit a probability distribution to these scores. The model is then trained with anchors separated into positive and negative samples according to their probabilities. Moreover, we investigate the gap between the training and testing objectives and propose to predict the Intersection-over-Unions of detected boxes as a measure of localization quality to reduce the discrepancy. The combined score of classification and localization qualities serving as a box selection metric in non-maximum suppression well aligns with the proposed anchor assignment strategy and leads significant performance improvements. The proposed methods only add a single convolutional layer to RetinaNet baseline and does not require multiple anchors per location, so are efficient. Experimental results verify the effectiveness of the proposed methods. Especially, our models set new records for single-stage detectors on MS COCO test-dev dataset with various backbones. Code is available at https://github.com/kkhoot/PAA.



### Defocus Blur Detection via Depth Distillation
- **Arxiv ID**: http://arxiv.org/abs/2007.08113v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.08113v1)
- **Published**: 2020-07-16 04:58:09+00:00
- **Updated**: 2020-07-16 04:58:09+00:00
- **Authors**: Xiaodong Cun, Chi-Man Pun
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: Defocus Blur Detection(DBD) aims to separate in-focus and out-of-focus regions from a single image pixel-wisely. This task has been paid much attention since bokeh effects are widely used in digital cameras and smartphone photography. However, identifying obscure homogeneous regions and borderline transitions in partially defocus images is still challenging. To solve these problems, we introduce depth information into DBD for the first time. When the camera parameters are fixed, we argue that the accuracy of DBD is highly related to scene depth. Hence, we consider the depth information as the approximate soft label of DBD and propose a joint learning framework inspired by knowledge distillation. In detail, we learn the defocus blur from ground truth and the depth distilled from a well-trained depth estimation network at the same time. Thus, the sharp region will provide a strong prior for depth estimation while the blur detection also gains benefits from the distilled depth. Besides, we propose a novel decoder in the fully convolutional network(FCN) as our network structure. In each level of the decoder, we design the Selective Reception Field Block(SRFB) for merging multi-scale features efficiently and reuse the side outputs as Supervision-guided Attention Block(SAB). Unlike previous methods, the proposed decoder builds reception field pyramids and emphasizes salient regions simply and efficiently. Experiments show that our approach outperforms 11 other state-of-the-art methods on two popular datasets. Our method also runs at over 30 fps on a single GPU, which is 2x faster than previous works. The code is available at: https://github.com/vinthony/depth-distillation



### PerMO: Perceiving More at Once from a Single Image for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2007.08116v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.08116v1)
- **Published**: 2020-07-16 05:02:45+00:00
- **Updated**: 2020-07-16 05:02:45+00:00
- **Authors**: Feixiang Lu, Zongdai Liu, Xibin Song, Dingfu Zhou, Wei Li, Hui Miao, Miao Liao, Liangjun Zhang, Bin Zhou, Ruigang Yang, Dinesh Manocha
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel approach to detect, segment, and reconstruct complete textured 3D models of vehicles from a single image for autonomous driving. Our approach combines the strengths of deep learning and the elegance of traditional techniques from part-based deformable model representation to produce high-quality 3D models in the presence of severe occlusions. We present a new part-based deformable vehicle model that is used for instance segmentation and automatically generate a dataset that contains dense correspondences between 2D images and 3D models. We also present a novel end-to-end deep neural network to predict dense 2D/3D mapping and highlight its benefits. Based on the dense mapping, we are able to compute precise 6-DoF poses and 3D reconstruction results at almost interactive rates on a commodity GPU. We have integrated these algorithms with an autonomous driving system. In practice, our method outperforms the state-of-the-art methods for all major vehicle parsing tasks: 2D instance segmentation by 4.4 points (mAP), 6-DoF pose estimation by 9.11 points, and 3D detection by 1.37. Moreover, we have released all of the source code, dataset, and the trained model on Github.



### Layer-Wise Adaptive Updating for Few-Shot Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2007.08129v1
- **DOI**: 10.1109/LSP.2020.3036348
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.08129v1)
- **Published**: 2020-07-16 06:02:44+00:00
- **Updated**: 2020-07-16 06:02:44+00:00
- **Authors**: Yunxiao Qin, Weiguo Zhang, Zezheng Wang, Chenxu Zhao, Jingping Shi
- **Comment**: None
- **Journal**: None
- **Summary**: Few-shot image classification (FSIC), which requires a model to recognize new categories via learning from few images of these categories, has attracted lots of attention. Recently, meta-learning based methods have been shown as a promising direction for FSIC. Commonly, they train a meta-learner (meta-learning model) to learn easy fine-tuning weight, and when solving an FSIC task, the meta-learner efficiently fine-tunes itself to a task-specific model by updating itself on few images of the task. In this paper, we propose a novel meta-learning based layer-wise adaptive updating (LWAU) method for FSIC. LWAU is inspired by an interesting finding that compared with common deep models, the meta-learner pays much more attention to update its top layer when learning from few images. According to this finding, we assume that the meta-learner may greatly prefer updating its top layer to updating its bottom layers for better FSIC performance. Therefore, in LWAU, the meta-learner is trained to learn not only the easy fine-tuning model but also its favorite layer-wise adaptive updating rule to improve its learning efficiency. Extensive experiments show that with the layer-wise adaptive updating rule, the proposed LWAU: 1) outperforms existing few-shot classification methods with a clear margin; 2) learns from few images more efficiently by at least 5 times than existing meta-learners when solving FSIC.



### Interactive Video Object Segmentation Using Global and Local Transfer Modules
- **Arxiv ID**: http://arxiv.org/abs/2007.08139v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.08139v1)
- **Published**: 2020-07-16 06:49:07+00:00
- **Updated**: 2020-07-16 06:49:07+00:00
- **Authors**: Yuk Heo, Yeong Jun Koh, Chang-Su Kim
- **Comment**: None
- **Journal**: None
- **Summary**: An interactive video object segmentation algorithm, which takes scribble annotations on query objects as input, is proposed in this paper. We develop a deep neural network, which consists of the annotation network (A-Net) and the transfer network (T-Net). First, given user scribbles on a frame, A-Net yields a segmentation result based on the encoder-decoder architecture. Second, T-Net transfers the segmentation result bidirectionally to the other frames, by employing the global and local transfer modules. The global transfer module conveys the segmentation information in an annotated frame to a target frame, while the local transfer module propagates the segmentation information in a temporally adjacent frame to the target frame. By applying A-Net and T-Net alternately, a user can obtain desired segmentation results with minimal efforts. We train the entire network in two stages, by emulating user scribbles and employing an auxiliary loss. Experimental results demonstrate that the proposed interactive video object segmentation algorithm outperforms the state-of-the-art conventional algorithms. Codes and models are available at https://github.com/yuk6heo/IVOS-ATNet.



### Odyssey: Creation, Analysis and Detection of Trojan Models
- **Arxiv ID**: http://arxiv.org/abs/2007.08142v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.08142v2)
- **Published**: 2020-07-16 06:55:00+00:00
- **Updated**: 2020-12-08 08:09:51+00:00
- **Authors**: Marzieh Edraki, Nazmul Karim, Nazanin Rahnavard, Ajmal Mian, Mubarak Shah
- **Comment**: None
- **Journal**: None
- **Summary**: Along with the success of deep neural network (DNN) models, rise the threats to the integrity of these models. A recent threat is the Trojan attack where an attacker interferes with the training pipeline by inserting triggers into some of the training samples and trains the model to act maliciously only for samples that contain the trigger. Since the knowledge of triggers is privy to the attacker, detection of Trojan networks is challenging. Existing Trojan detectors make strong assumptions about the types of triggers and attacks. We propose a detector that is based on the analysis of the intrinsic DNN properties; that are affected due to the Trojaning process. For a comprehensive analysis, we develop Odysseus, the most diverse dataset to date with over 3,000 clean and Trojan models. Odysseus covers a large spectrum of attacks; generated by leveraging the versatility in trigger designs and source to target class mappings. Our analysis results show that Trojan attacks affect the classifier margin and shape of decision boundary around the manifold of clean data. Exploiting these two factors, we propose an efficient Trojan detector that operates without any knowledge of the attack and significantly outperforms existing methods. Through a comprehensive set of experiments we demonstrate the efficacy of the detector on cross model architectures, unseen Triggers and regularized models.



### Enhanced detection of fetal pose in 3D MRI by Deep Reinforcement Learning with physical structure priors on anatomy
- **Arxiv ID**: http://arxiv.org/abs/2007.08146v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.08146v1)
- **Published**: 2020-07-16 07:10:21+00:00
- **Updated**: 2020-07-16 07:10:21+00:00
- **Authors**: Molin Zhang, Junshen Xu, Esra Abaci Turk, P. Ellen Grant, Polina Golland, Elfar Adalsteinsson
- **Comment**: 10 pages, 3 figures, MICCAI 2020
- **Journal**: None
- **Summary**: Fetal MRI is heavily constrained by unpredictable and substantial fetal motion that causes image artifacts and limits the set of viable diagnostic image contrasts. Current mitigation of motion artifacts is predominantly performed by fast, single-shot MRI and retrospective motion correction. Estimation of fetal pose in real time during MRI stands to benefit prospective methods to detect and mitigate fetal motion artifacts where inferred fetal motion is combined with online slice prescription with low-latency decision making. Current developments of deep reinforcement learning (DRL), offer a novel approach for fetal landmarks detection. In this task 15 agents are deployed to detect 15 landmarks simultaneously by DRL. The optimization is challenging, and here we propose an improved DRL that incorporates priors on physical structure of the fetal body. First, we use graph communication layers to improve the communication among agents based on a graph where each node represents a fetal-body landmark. Further, additional reward based on the distance between agents and physical structures such as the fetal limbs is used to fully exploit physical structure. Evaluation of this method on a repository of 3-mm resolution in vivo data demonstrates a mean accuracy of landmark estimation within 10 mm of ground truth as 87.3%, and a mean error of 6.9 mm. The proposed DRL for fetal pose landmark search demonstrates a potential clinical utility for online detection of fetal motion that guides real-time mitigation of motion artifacts as well as health diagnosis during MRI of the pregnant mother.



### Comprehensive Facial Expression Synthesis using Human-Interpretable Language
- **Arxiv ID**: http://arxiv.org/abs/2007.08154v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.08154v1)
- **Published**: 2020-07-16 07:28:25+00:00
- **Updated**: 2020-07-16 07:28:25+00:00
- **Authors**: Joanna Hong, Jung Uk Kim, Sangmin Lee, Yong Man Ro
- **Comment**: ICIP 2020
- **Journal**: None
- **Summary**: Recent advances in facial expression synthesis have shown promising results using diverse expression representations including facial action units. Facial action units for an elaborate facial expression synthesis need to be intuitively represented for human comprehension, not a numeric categorization of facial action units. To address this issue, we utilize human-friendly approach: use of natural language where language helps human grasp conceptual contexts. In this paper, therefore, we propose a new facial expression synthesis model from language-based facial expression description. Our method can synthesize the facial image with detailed expressions. In addition, effectively embedding language features on facial features, our method can control individual word to handle each part of facial movement. Extensive qualitative and quantitative evaluations were conducted to verify the effectiveness of the natural language.



### Moving fast and slow: Analysis of representations and post-processing in speech-driven automatic gesture generation
- **Arxiv ID**: http://arxiv.org/abs/2007.09170v3
- **DOI**: 10.1080/10447318.2021.1883883
- **Categories**: **cs.CV**, cs.GR, cs.HC, cs.LG, I.2.7; I.2.6; I.3.7
- **Links**: [PDF](http://arxiv.org/pdf/2007.09170v3)
- **Published**: 2020-07-16 07:32:00+00:00
- **Updated**: 2021-01-28 12:49:17+00:00
- **Authors**: Taras Kucherenko, Dai Hasegawa, Naoshi Kaneko, Gustav Eje Henter, Hedvig Kjellström
- **Comment**: Extension of our IVA'19 paper. Accepted at the International Journal
  of Human-Computer Interaction. See more at
  https://svito-zar.github.io/audio2gestures/. arXiv admin note: substantial
  text overlap with arXiv:1903.03369
- **Journal**: Int. J. Hum. Comput.Interact.(2021)
- **Summary**: This paper presents a novel framework for speech-driven gesture production, applicable to virtual agents to enhance human-computer interaction. Specifically, we extend recent deep-learning-based, data-driven methods for speech-driven gesture generation by incorporating representation learning. Our model takes speech as input and produces gestures as output, in the form of a sequence of 3D coordinates. We provide an analysis of different representations for the input (speech) and the output (motion) of the network by both objective and subjective evaluations. We also analyse the importance of smoothing of the produced motion. Our results indicated that the proposed method improved on our baseline in terms of objective measures. For example, it better captured the motion dynamics and better matched the motion-speed distribution. Moreover, we performed user studies on two different datasets. The studies confirmed that our proposed method is perceived as more natural than the baseline, although the difference in the studies was eliminated by appropriate post-processing: hip-centering and smoothing. We conclude that it is important to take both motion representation and post-processing into account when designing an automatic gesture-production method.



### VIPriors Object Detection Challenge
- **Arxiv ID**: http://arxiv.org/abs/2007.08170v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.08170v1)
- **Published**: 2020-07-16 08:10:42+00:00
- **Updated**: 2020-07-16 08:10:42+00:00
- **Authors**: Zhipeng Luo, Lixuan Che
- **Comment**: None
- **Journal**: None
- **Summary**: This paper is a brief report to our submission to the VIPriors Object Detection Challenge. Object Detection has attracted many researchers' attention for its full application, but it is still a challenging task. In this paper, we study analysis the characteristics of the data, and an effective data enhancement method is proposed. We carefully choose the model which is more suitable for training from scratch. We benefit a lot from using softnms and model fusion skillfully.



### Efficient Full Image Interactive Segmentation by Leveraging Within-image Appearance Similarity
- **Arxiv ID**: http://arxiv.org/abs/2007.08173v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.08173v1)
- **Published**: 2020-07-16 08:21:59+00:00
- **Updated**: 2020-07-16 08:21:59+00:00
- **Authors**: Mykhaylo Andriluka, Stefano Pellegrini, Stefan Popov, Vittorio Ferrari
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a new approach to interactive full-image semantic segmentation which enables quickly collecting training data for new datasets with previously unseen semantic classes (A demo is available at https://youtu.be/yUk8D5gEX-o). We leverage a key observation: propagation from labeled to unlabeled pixels does not necessarily require class-specific knowledge, but can be done purely based on appearance similarity within an image. We build on this observation and propose an approach capable of jointly propagating pixel labels from multiple classes without having explicit class-specific appearance models. To enable long-range propagation, our approach first globally measures appearance similarity between labeled and unlabeled pixels across the entire image. Then it locally integrates per-pixel measurements which improves the accuracy at boundaries and removes noisy label switches in homogeneous regions. We also design an efficient manual annotation interface that extends the traditional polygon drawing tools with a suite of additional convenient features (and add automatic propagation to it). Experiments with human annotators on the COCO Panoptic Challenge dataset show that the combination of our better manual interface and our novel automatic propagation mechanism leads to reducing annotation time by more than factor of 2x compared to polygon drawing. We also test our method on the ADE-20k and Fashionista datasets without making any dataset-specific adaptation nor retraining our model, demonstrating that it can generalize to new datasets and visual classes.



### Challenge report:VIPriors Action Recognition Challenge
- **Arxiv ID**: http://arxiv.org/abs/2007.08180v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.08180v1)
- **Published**: 2020-07-16 08:40:31+00:00
- **Updated**: 2020-07-16 08:40:31+00:00
- **Authors**: Zhipeng Luo, Dawei Xu, Zhiguang Zhang
- **Comment**: ECCV2020,VIPriors Action Recognition Challenge
- **Journal**: None
- **Summary**: This paper is a brief report to our submission to the VIPriors Action Recognition Challenge. Action recognition has attracted many researchers attention for its full application, but it is still challenging. In this paper, we study previous methods and propose our method. In our method, we are primarily making improvements on the SlowFast Network and fusing with TSM to make further breakthroughs. Also, we use a fast but effective way to extract motion features from videos by using residual frames as input. Better motion features can be extracted using residual frames with SlowFast, and the residual-frame-input path is an excellent supplement for existing RGB-frame-input models. And better performance obtained by combining 3D convolution(SlowFast) with 2D convolution(TSM). The above experiments were all trained from scratch on UCF101.



### Training Interpretable Convolutional Neural Networks by Differentiating Class-specific Filters
- **Arxiv ID**: http://arxiv.org/abs/2007.08194v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.08194v3)
- **Published**: 2020-07-16 09:12:26+00:00
- **Updated**: 2021-07-01 10:40:06+00:00
- **Authors**: Haoyu Liang, Zhihao Ouyang, Yuyuan Zeng, Hang Su, Zihao He, Shu-Tao Xia, Jun Zhu, Bo Zhang
- **Comment**: European Conference on Computer Vision (ECCV), 2020
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) have been successfully used in a range of tasks. However, CNNs are often viewed as "black-box" and lack of interpretability. One main reason is due to the filter-class entanglement -- an intricate many-to-many correspondence between filters and classes. Most existing works attempt post-hoc interpretation on a pre-trained model, while neglecting to reduce the entanglement underlying the model. In contrast, we focus on alleviating filter-class entanglement during training. Inspired by cellular differentiation, we propose a novel strategy to train interpretable CNNs by encouraging class-specific filters, among which each filter responds to only one (or few) class. Concretely, we design a learnable sparse Class-Specific Gate (CSG) structure to assign each filter with one (or few) class in a flexible way. The gate allows a filter's activation to pass only when the input samples come from the specific class. Extensive experiments demonstrate the fabulous performance of our method in generating a sparse and highly class-related representation of the input, which leads to stronger interpretability. Moreover, comparing with the standard training strategy, our model displays benefits in applications like object localization and adversarial sample detection. Code link: https://github.com/hyliang96/CSGCNN.



### Learning from Noisy Labels with Deep Neural Networks: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2007.08199v7
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.08199v7)
- **Published**: 2020-07-16 09:23:13+00:00
- **Updated**: 2022-03-10 01:51:43+00:00
- **Authors**: Hwanjun Song, Minseok Kim, Dongmin Park, Yooju Shin, Jae-Gil Lee
- **Comment**: Final version published in TNNLS Journal (2022 March)
- **Journal**: None
- **Summary**: Deep learning has achieved remarkable success in numerous domains with help from large amounts of big data. However, the quality of data labels is a concern because of the lack of high-quality labels in many real-world scenarios. As noisy labels severely degrade the generalization performance of deep neural networks, learning from noisy labels (robust training) is becoming an important task in modern deep learning applications. In this survey, we first describe the problem of learning with label noise from a supervised learning perspective. Next, we provide a comprehensive review of 62 state-of-the-art robust training methods, all of which are categorized into five groups according to their methodological difference, followed by a systematic comparison of six properties used to evaluate their superiority. Subsequently, we perform an in-depth analysis of noise rate estimation and summarize the typically used evaluation methodology, including public noisy datasets and evaluation metrics. Finally, we present several promising research directions that can serve as a guideline for future studies. All the contents will be available at https://github.com/songhwanjun/Awesome-Noisy-Labels.



### SSN: Soft Shadow Network for Image Compositing
- **Arxiv ID**: http://arxiv.org/abs/2007.08211v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, eess.IV, 68T45, 68U05, I.3.3; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2007.08211v3)
- **Published**: 2020-07-16 09:36:39+00:00
- **Updated**: 2021-04-01 19:14:00+00:00
- **Authors**: Yichen Sheng, Jianming Zhang, Bedrich Benes
- **Comment**: 11 pages, 10 figures
- **Journal**: None
- **Summary**: We introduce an interactive Soft Shadow Network (SSN) to generates controllable soft shadows for image compositing. SSN takes a 2D object mask as input and thus is agnostic to image types such as painting and vector art. An environment light map is used to control the shadow's characteristics, such as angle and softness. SSN employs an Ambient Occlusion Prediction module to predict an intermediate ambient occlusion map, which can be further refined by the user to provides geometric cues to modulate the shadow generation. To train our model, we design an efficient pipeline to produce diverse soft shadow training data using 3D object models. In addition, we propose an inverse shadow map representation to improve model training. We demonstrate that our model produces realistic soft shadows in real-time. Our user studies show that the generated shadows are often indistinguishable from shadows calculated by a physics-based renderer and users can easily use SSN through an interactive application to generate specific shadow effects in minutes.



### Video-based Remote Physiological Measurement via Cross-verified Feature Disentangling
- **Arxiv ID**: http://arxiv.org/abs/2007.08213v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.08213v1)
- **Published**: 2020-07-16 09:39:17+00:00
- **Updated**: 2020-07-16 09:39:17+00:00
- **Authors**: Xuesong Niu, Zitong Yu, Hu Han, Xiaobai Li, Shiguang Shan, Guoying Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: Remote physiological measurements, e.g., remote photoplethysmography (rPPG) based heart rate (HR), heart rate variability (HRV) and respiration frequency (RF) measuring, are playing more and more important roles under the application scenarios where contact measurement is inconvenient or impossible. Since the amplitude of the physiological signals is very small, they can be easily affected by head movements, lighting conditions, and sensor diversities. To address these challenges, we propose a cross-verified feature disentangling strategy to disentangle the physiological features with non-physiological representations, and then use the distilled physiological features for robust multi-task physiological measurements. We first transform the input face videos into a multi-scale spatial-temporal map (MSTmap), which can suppress the irrelevant background and noise features while retaining most of the temporal characteristics of the periodic physiological signals. Then we take pairwise MSTmaps as inputs to an autoencoder architecture with two encoders (one for physiological signals and the other for non-physiological information) and use a cross-verified scheme to obtain physiological features disentangled with the non-physiological features. The disentangled features are finally used for the joint prediction of multiple physiological signals like average HR values and rPPG signals. Comprehensive experiments on different large-scale public datasets of multiple physiological measurement tasks as well as the cross-database testing demonstrate the robustness of our approach.



### DeepInit Phase Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2007.08214v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, eess.SP, physics.ins-det
- **Links**: [PDF](http://arxiv.org/pdf/2007.08214v1)
- **Published**: 2020-07-16 09:39:28+00:00
- **Updated**: 2020-07-16 09:39:28+00:00
- **Authors**: Martin Reiche, Peter Jung
- **Comment**: 9 pages, 12 figures
- **Journal**: None
- **Summary**: This paper shows how data-driven deep generative models can be utilized to solve challenging phase retrieval problems, in which one wants to reconstruct a signal from only few intensity measurements. Classical iterative algorithms are known to work well if initialized close to the optimum but otherwise suffer from non-convexity and often get stuck in local minima. We therefore propose DeepInit Phase Retrieval, which uses regularized gradient descent under a deep generative data prior to compute a trained initialization for a fast classical algorithm (e.g. the randomized Kaczmarz method). We empirically show that our hybrid approach is able to deliver very high reconstruction results at low sampling rates even when there is significant generator model error. Conceptually, learned initializations may therefore help to overcome the non-convexity of the problem by starting classical descent steps closer to the global optimum. Also, our idea demonstrates superior runtime performance over conventional gradient-based reconstruction methods. We evaluate our method for generic measurements and show empirically that it is also applicable to diffraction-type measurement models which are found in terahertz single-pixel phase retrieval.



### An Efficient Mixture of Deep and Machine Learning Models for COVID-19 and Tuberculosis Detection Using X-Ray Images in Resource Limited Settings
- **Arxiv ID**: http://arxiv.org/abs/2007.08223v1
- **DOI**: 10.1007/978-3-030-69744-0_6
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.08223v1)
- **Published**: 2020-07-16 09:49:49+00:00
- **Updated**: 2020-07-16 09:49:49+00:00
- **Authors**: Ali H. Al-Timemy, Rami N. Khushaba, Zahraa M. Mosa, Javier Escudero
- **Comment**: The final constructed dataset named COVID-19 five-class balanced
  dataset is available from:
  https://drive.google.com/drive/folders/1toMymyHTy0DR_fyE7hjO3LSBGWtVoPNf?usp=sharing
- **Journal**: Artificial Intelligence for COVID-19. Studies in Systems, Decision
  and Control, vol 358. 2021, Springer, Cham
- **Summary**: Clinicians in the frontline need to assess quickly whether a patient with symptoms indeed has COVID-19 or not. The difficulty of this task is exacerbated in low resource settings that may not have access to biotechnology tests. Furthermore, Tuberculosis (TB) remains a major health problem in several low- and middle-income countries and its common symptoms include fever, cough and tiredness, similarly to COVID-19. In order to help in the detection of COVID-19, we propose the extraction of deep features (DF) from chest X-ray images, a technology available in most hospitals, and their subsequent classification using machine learning methods that do not require large computational resources. We compiled a five-class dataset of X-ray chest images including a balanced number of COVID-19, viral pneumonia, bacterial pneumonia, TB, and healthy cases. We compared the performance of pipelines combining 14 individual state-of-the-art pre-trained deep networks for DF extraction with traditional machine learning classifiers. A pipeline consisting of ResNet-50 for DF computation and ensemble of subspace discriminant classifier was the best performer in the classification of the five classes, achieving a detection accuracy of 91.6+ 2.6% (accuracy + 95% Confidence Interval). Furthermore, the same pipeline achieved accuracies of 98.6+1.4% and 99.9+0.5% in simpler three-class and two-class classification problems focused on distinguishing COVID-19, TB and healthy cases; and COVID-19 and healthy images, respectively. The pipeline was computationally efficient requiring just 0.19 second to extract DF per X-ray image and 2 minutes for training a traditional classifier with more than 2000 images on a CPU machine. The results suggest the potential benefits of using our pipeline in the detection of COVID-19, particularly in resource-limited settings and it can run with limited computational resources.



### SAILenv: Learning in Virtual Visual Environments Made Simple
- **Arxiv ID**: http://arxiv.org/abs/2007.08224v2
- **DOI**: 10.1109/ICPR48806.2021.9412909
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.08224v2)
- **Published**: 2020-07-16 09:50:23+00:00
- **Updated**: 2020-07-20 15:42:02+00:00
- **Authors**: Enrico Meloni, Luca Pasqualini, Matteo Tiezzi, Marco Gori, Stefano Melacci
- **Comment**: 8 pages, 7 figures, submitted to ICPR 2020
- **Journal**: None
- **Summary**: Recently, researchers in Machine Learning algorithms, Computer Vision scientists, engineers and others, showed a growing interest in 3D simulators as a mean to artificially create experimental settings that are very close to those in the real world. However, most of the existing platforms to interface algorithms with 3D environments are often designed to setup navigation-related experiments, to study physical interactions, or to handle ad-hoc cases that are not thought to be customized, sometimes lacking a strong photorealistic appearance and an easy-to-use software interface. In this paper, we present a novel platform, SAILenv, that is specifically designed to be simple and customizable, and that allows researchers to experiment visual recognition in virtual 3D scenes. A few lines of code are needed to interface every algorithm with the virtual world, and non-3D-graphics experts can easily customize the 3D environment itself, exploiting a collection of photorealistic objects. Our framework yields pixel-level semantic and instance labeling, depth, and, to the best of our knowledge, it is the only one that provides motion-related information directly inherited from the 3D engine. The client-server communication operates at a low level, avoiding the overhead of HTTP-based data exchanges. We perform experiments using a state-of-the-art object detector trained on real-world images, showing that it is able to recognize the photorealistic 3D objects of our environment. The computational burden of the optical flow compares favourably with the estimation performed using modern GPU-based convolutional networks or more classic implementations. We believe that the scientific community will benefit from the easiness and high-quality of our framework to evaluate newly proposed algorithms in their own customized realistic conditions.



### U-Net Based Architecture for an Improved Multiresolution Segmentation in Medical Images
- **Arxiv ID**: http://arxiv.org/abs/2007.08238v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.08238v2)
- **Published**: 2020-07-16 10:19:01+00:00
- **Updated**: 2020-07-17 06:17:20+00:00
- **Authors**: Simindokht Jahangard, Mohammad Hossein Zangooei, Maysam Shahedi
- **Comment**: None
- **Journal**: None
- **Summary**: Purpose: Manual medical image segmentation is an exhausting and time-consuming task along with high inter-observer variability. In this study, our objective is to improve the multi-resolution image segmentation performance of U-Net architecture. Approach: We have proposed a fully convolutional neural network for image segmentation in a multi-resolution framework. We used U-Net as the base architecture and modified that to improve its image segmentation performance. In the proposed architecture (mrU-Net), the input image and its down-sampled versions were used as the network inputs. We added more convolution layers to extract features directly from the down-sampled images. We trained and tested the network on four different medical datasets, including skin lesion photos, lung computed tomography (CT) images (LUNA dataset), retina images (DRIVE dataset), and prostate magnetic resonance (MR) images (PROMISE12 dataset). We compared the performance of mrU-Net to U-Net under similar training and testing conditions. Results: Comparing the results to manual segmentation labels, mrU-Net achieved average Dice similarity coefficients of 70.6%, 97.9%, 73.6%, and 77.9% for the skin lesion, LUNA, DRIVE, and PROMISE12 segmentation, respectively. For the skin lesion, LUNA, and DRIVE datasets, mrU-Net outperformed U-Net with significantly higher accuracy and for the PROMISE12 dataset, both networks achieved similar accuracy. Furthermore, using mrU-Net led to a faster training rate on LUNA and DRIVE datasets when compared to U-Net. Conclusions: The striking feature of the proposed architecture is its higher capability in extracting image-derived features compared to U-Net. mrU-Net illustrated a faster training rate and slightly more accurate image segmentation compared to U-Net.



### Autoregressive Unsupervised Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2007.08247v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.08247v1)
- **Published**: 2020-07-16 10:47:40+00:00
- **Updated**: 2020-07-16 10:47:40+00:00
- **Authors**: Yassine Ouali, Céline Hudelot, Myriam Tami
- **Comment**: Accepted at ECCV 2020
- **Journal**: None
- **Summary**: In this work, we propose a new unsupervised image segmentation approach based on mutual information maximization between different constructed views of the inputs. Taking inspiration from autoregressive generative models that predict the current pixel from past pixels in a raster-scan ordering created with masked convolutions, we propose to use different orderings over the inputs using various forms of masked convolutions to construct different views of the data. For a given input, the model produces a pair of predictions with two valid orderings, and is then trained to maximize the mutual information between the two outputs. These outputs can either be low-dimensional features for representation learning or output clusters corresponding to semantic labels for clustering. While masked convolutions are used during training, in inference, no masking is applied and we fall back to the standard convolution where the model has access to the full input. The proposed method outperforms current state-of-the-art on unsupervised image segmentation. It is simple and easy to implement, and can be extended to other visual tasks and integrated seamlessly into existing unsupervised learning methods requiring different views of the data.



### Weighing Counts: Sequential Crowd Counting by Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/2007.08260v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.08260v1)
- **Published**: 2020-07-16 11:16:12+00:00
- **Updated**: 2020-07-16 11:16:12+00:00
- **Authors**: Liang Liu, Hao Lu, Hongwei Zou, Haipeng Xiong, Zhiguo Cao, Chunhua Shen
- **Comment**: Accepted to Proc. Eur. Conf. Computer Vision (ECCV) 2020
- **Journal**: None
- **Summary**: We formulate counting as a sequential decision problem and present a novel crowd counting model solvable by deep reinforcement learning. In contrast to existing counting models that directly output count values, we divide one-step estimation into a sequence of much easier and more tractable sub-decision problems. Such sequential decision nature corresponds exactly to a physical process in reality scale weighing. Inspired by scale weighing, we propose a novel 'counting scale' termed LibraNet where the count value is analogized by weight. By virtually placing a crowd image on one side of a scale, LibraNet (agent) sequentially learns to place appropriate weights on the other side to match the crowd count. At each step, LibraNet chooses one weight (action) from the weight box (the pre-defined action pool) according to the current crowd image features and weights placed on the scale pan (state). LibraNet is required to learn to balance the scale according to the feedback of the needle (Q values). We show that LibraNet exactly implements scale weighing by visualizing the decision process how LibraNet chooses actions. Extensive experiments demonstrate the effectiveness of our design choices and report state-of-the-art results on a few crowd counting benchmarks. We also demonstrate good cross-dataset generalization of LibraNet. Code and models are made available at: https://git.io/libranet



### Kernelized Memory Network for Video Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2007.08270v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.08270v1)
- **Published**: 2020-07-16 11:44:12+00:00
- **Updated**: 2020-07-16 11:44:12+00:00
- **Authors**: Hongje Seong, Junhyuk Hyun, Euntai Kim
- **Comment**: Accepted to ECCV 2020
- **Journal**: None
- **Summary**: Semi-supervised video object segmentation (VOS) is a task that involves predicting a target object in a video when the ground truth segmentation mask of the target object is given in the first frame. Recently, space-time memory networks (STM) have received significant attention as a promising solution for semi-supervised VOS. However, an important point is overlooked when applying STM to VOS. The solution (STM) is non-local, but the problem (VOS) is predominantly local. To solve the mismatch between STM and VOS, we propose a kernelized memory network (KMN). Before being trained on real videos, our KMN is pre-trained on static images, as in previous works. Unlike in previous works, we use the Hide-and-Seek strategy in pre-training to obtain the best possible results in handling occlusions and segment boundary extraction. The proposed KMN surpasses the state-of-the-art on standard benchmarks by a significant margin (+5% on DAVIS 2017 test-dev set). In addition, the runtime of KMN is 0.12 seconds per frame on the DAVIS 2016 validation set, and the KMN rarely requires extra computation, when compared with STM.



### Event Enhanced High-Quality Image Recovery
- **Arxiv ID**: http://arxiv.org/abs/2007.08336v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.08336v1)
- **Published**: 2020-07-16 13:51:15+00:00
- **Updated**: 2020-07-16 13:51:15+00:00
- **Authors**: Bishan Wang, Jingwei He, Lei Yu, Gui-Song Xia, Wen Yang
- **Comment**: None
- **Journal**: None
- **Summary**: With extremely high temporal resolution, event cameras have a large potential for robotics and computer vision. However, their asynchronous imaging mechanism often aggravates the measurement sensitivity to noises and brings a physical burden to increase the image spatial resolution. To recover high-quality intensity images, one should address both denoising and super-resolution problems for event cameras. Since events depict brightness changes, with the enhanced degeneration model by the events, the clear and sharp high-resolution latent images can be recovered from the noisy, blurry and low-resolution intensity observations. Exploiting the framework of sparse learning, the events and the low-resolution intensity observations can be jointly considered. Based on this, we propose an explainable network, an event-enhanced sparse learning network (eSL-Net), to recover the high-quality images from event cameras. After training with a synthetic dataset, the proposed eSL-Net can largely improve the performance of the state-of-the-art by 7-12 dB. Furthermore, without additional training process, the proposed eSL-Net can be easily extended to generate continuous frames with frame-rate as high as the events.



### Human Pose Estimation on Privacy-Preserving Low-Resolution Depth Images
- **Arxiv ID**: http://arxiv.org/abs/2007.08340v2
- **DOI**: 10.1007/978-3-030-32254-0_65
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.08340v2)
- **Published**: 2020-07-16 14:03:52+00:00
- **Updated**: 2021-08-20 10:48:22+00:00
- **Authors**: Vinkle Srivastav, Afshin Gangi, Nicolas Padoy
- **Comment**: Published at MICCAI-2019. Code is available at
  https://github.com/CAMMA-public/ORPose-depth
- **Journal**: Springer (2019) 583-591
- **Summary**: Human pose estimation (HPE) is a key building block for developing AI-based context-aware systems inside the operating room (OR). The 24/7 use of images coming from cameras mounted on the OR ceiling can however raise concerns for privacy, even in the case of depth images captured by RGB-D sensors. Being able to solely use low-resolution privacy-preserving images would address these concerns and help scale up the computer-assisted approaches that rely on such data to a larger number of ORs. In this paper, we introduce the problem of HPE on low-resolution depth images and propose an end-to-end solution that integrates a multi-scale super-resolution network with a 2D human pose estimation network. By exploiting intermediate feature-maps generated at different super-resolution, our approach achieves body pose results on low-resolution images (of size 64x48) that are on par with those of an approach trained and tested on full resolution images (of size 640x480).



### Natural Graph Networks
- **Arxiv ID**: http://arxiv.org/abs/2007.08349v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.08349v2)
- **Published**: 2020-07-16 14:19:06+00:00
- **Updated**: 2020-11-23 15:38:46+00:00
- **Authors**: Pim de Haan, Taco Cohen, Max Welling
- **Comment**: Published at NeurIPS 2020
- **Journal**: None
- **Summary**: A key requirement for graph neural networks is that they must process a graph in a way that does not depend on how the graph is described. Traditionally this has been taken to mean that a graph network must be equivariant to node permutations. Here we show that instead of equivariance, the more general concept of naturality is sufficient for a graph network to be well-defined, opening up a larger class of graph networks. We define global and local natural graph networks, the latter of which are as scalable as conventional message passing graph neural networks while being more flexible. We give one practical instantiation of a natural network on graphs which uses an equivariant message network parameterization, yielding good performance on several benchmarks.



### Self-supervision on Unlabelled OR Data for Multi-person 2D/3D Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2007.08354v2
- **DOI**: 10.1007/978-3-030-59710-8_74
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.08354v2)
- **Published**: 2020-07-16 14:28:22+00:00
- **Updated**: 2021-08-20 10:53:38+00:00
- **Authors**: Vinkle Srivastav, Afshin Gangi, Nicolas Padoy
- **Comment**: Published at MICCAI 2020. Code is available at
  https://github.com/CAMMA-public/ORPose-Color
- **Journal**: Springer (2020) LNCS, volume 12261
- **Summary**: 2D/3D human pose estimation is needed to develop novel intelligent tools for the operating room that can analyze and support the clinical activities. The lack of annotated data and the complexity of state-of-the-art pose estimation approaches limit, however, the deployment of such techniques inside the OR. In this work, we propose to use knowledge distillation in a teacher/student framework to harness the knowledge present in a large-scale non-annotated dataset and in an accurate but complex multi-stage teacher network to train a lightweight network for joint 2D/3D pose estimation. The teacher network also exploits the unlabeled data to generate both hard and soft labels useful in improving the student predictions. The easily deployable network trained using this effective self-supervision strategy performs on par with the teacher network on \emph{MVOR+}, an extension of the public MVOR dataset where all persons have been fully annotated, thus providing a viable solution for real-time 2D/3D human pose estimation in the OR.



### A high fidelity synthetic face framework for computer vision
- **Arxiv ID**: http://arxiv.org/abs/2007.08364v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.08364v1)
- **Published**: 2020-07-16 14:40:28+00:00
- **Updated**: 2020-07-16 14:40:28+00:00
- **Authors**: Tadas Baltrusaitis, Erroll Wood, Virginia Estellers, Charlie Hewitt, Sebastian Dziadzio, Marek Kowalski, Matthew Johnson, Thomas J. Cashman, Jamie Shotton
- **Comment**: None
- **Journal**: None
- **Summary**: Analysis of faces is one of the core applications of computer vision, with tasks ranging from landmark alignment, head pose estimation, expression recognition, and face recognition among others. However, building reliable methods requires time-consuming data collection and often even more time-consuming manual annotation, which can be unreliable. In our work we propose synthesizing such facial data, including ground truth annotations that would be almost impossible to acquire through manual annotation at the consistency and scale possible through use of synthetic data. We use a parametric face model together with hand crafted assets which enable us to generate training data with unprecedented quality and diversity (varying shape, texture, expression, pose, lighting, and hair).



### Self-Supervised Nuclei Segmentation in Histopathological Images Using Attention
- **Arxiv ID**: http://arxiv.org/abs/2007.08373v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.08373v1)
- **Published**: 2020-07-16 14:49:20+00:00
- **Updated**: 2020-07-16 14:49:20+00:00
- **Authors**: Mihir Sahasrabudhe, Stergios Christodoulidis, Roberto Salgado, Stefan Michiels, Sherene Loi, Fabrice André, Nikos Paragios, Maria Vakalopoulou
- **Comment**: 10 pages. Code available online at
  https://github.com/msahasrabudhe/miccai2020_self_sup_nuclei_seg
- **Journal**: None
- **Summary**: Segmentation and accurate localization of nuclei in histopathological images is a very challenging problem, with most existing approaches adopting a supervised strategy. These methods usually rely on manual annotations that require a lot of time and effort from medical experts. In this study, we present a self-supervised approach for segmentation of nuclei for whole slide histopathology images. Our method works on the assumption that the size and texture of nuclei can determine the magnification at which a patch is extracted. We show that the identification of the magnification level for tiles can generate a preliminary self-supervision signal to locate nuclei. We further show that by appropriately constraining our model it is possible to retrieve meaningful segmentation maps as an auxiliary output to the primary magnification identification task. Our experiments show that with standard post-processing, our method can outperform other unsupervised nuclei segmentation approaches and report similar performance with supervised ones on the publicly available MoNuSeg dataset. Our code and models are available online to facilitate further research.



### Neuro-Endo-Trainer-Online Assessment System (NET-OAS) for Neuro-Endoscopic Skills Training
- **Arxiv ID**: http://arxiv.org/abs/2007.08378v1
- **DOI**: 10.15439/2017F316
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.08378v1)
- **Published**: 2020-07-16 14:54:09+00:00
- **Updated**: 2020-07-16 14:54:09+00:00
- **Authors**: Vinkle Srivastav, Britty Baby, Ramandeep Singh, Prem Kalra, Ashish Suri
- **Comment**: Published at Federated Conference on Computer Science and Information
  Systems - FedCSIS 2017
- **Journal**: IEEE (2017)
- **Summary**: Neuro-endoscopy is a challenging minimally invasive neurosurgery that requires surgical skills to be acquired using training methods different from the existing apprenticeship model. There are various training systems developed for imparting fundamental technical skills in laparoscopy where as limited systems for neuro-endoscopy. Neuro-Endo-Trainer was a box-trainer developed for endo-nasal transsphenoidal surgical skills training with video based offline evaluation system. The objective of the current study was to develop a modified version (Neuro-Endo-Trainer-Online Assessment System (NET-OAS)) by providing a stand-alone system with online evaluation and real-time feedback. The validation study on a group of 15 novice participants shows the improvement in the technical skills for handling the neuro-endoscope and the tool while performing pick and place activity.



### MTP: Multi-Task Pruning for Efficient Semantic Segmentation Networks
- **Arxiv ID**: http://arxiv.org/abs/2007.08386v2
- **DOI**: 10.1109/ICME52920.2022.9859583
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.08386v2)
- **Published**: 2020-07-16 15:03:01+00:00
- **Updated**: 2022-03-15 06:54:53+00:00
- **Authors**: Xinghao Chen, Yiman Zhang, Yunhe Wang
- **Comment**: None
- **Journal**: None
- **Summary**: This paper focuses on channel pruning for semantic segmentation networks. Previous methods to compress and accelerate deep neural networks in the classification task cannot be straightforwardly applied to the semantic segmentation network that involves an implicit multi-task learning problem via pre-training. To identify the redundancy in segmentation networks, we present a multi-task channel pruning approach. The importance of each convolution filter \wrt the channel of an arbitrary layer will be simultaneously determined by the classification and segmentation tasks. In addition, we develop an alternative scheme for optimizing importance scores of filters in the entire network. Experimental results on several benchmarks illustrate the superiority of the proposed algorithm over the state-of-the-art pruning methods. Notably, we can obtain an about $2\times$ FLOPs reduction on DeepLabv3 with only an about $1\%$ mIoU drop on the PASCAL VOC 2012 dataset and an about $1.3\%$ mIoU drop on Cityscapes dataset, respectively.



### Controllable Image Synthesis via SegVAE
- **Arxiv ID**: http://arxiv.org/abs/2007.08397v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.08397v2)
- **Published**: 2020-07-16 15:18:53+00:00
- **Updated**: 2020-07-17 04:13:08+00:00
- **Authors**: Yen-Chi Cheng, Hsin-Ying Lee, Min Sun, Ming-Hsuan Yang
- **Comment**: ECCV 2020. Project page: https://yccyenchicheng.github.io/SegVAE/
  Code: https://github.com/yccyenchicheng/SegVAE
- **Journal**: None
- **Summary**: Flexible user controls are desirable for content creation and image editing. A semantic map is commonly used intermediate representation for conditional image generation. Compared to the operation on raw RGB pixels, the semantic map enables simpler user modification. In this work, we specifically target at generating semantic maps given a label-set consisting of desired categories. The proposed framework, SegVAE, synthesizes semantic maps in an iterative manner using conditional variational autoencoder. Quantitative and qualitative experiments demonstrate that the proposed model can generate realistic and diverse semantic maps. We also apply an off-the-shelf image-to-image translation model to generate realistic RGB images to better understand the quality of the synthesized semantic maps. Furthermore, we showcase several real-world image-editing applications including object removal, object insertion, and object replacement.



### Semi-Siamese Training for Shallow Face Learning
- **Arxiv ID**: http://arxiv.org/abs/2007.08398v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.08398v1)
- **Published**: 2020-07-16 15:20:04+00:00
- **Updated**: 2020-07-16 15:20:04+00:00
- **Authors**: Hang Du, Hailin Shi, Yuchi Liu, Jun Wang, Zhen Lei, Dan Zeng, Tao Mei
- **Comment**: ECCV 2020 Spotlight
- **Journal**: None
- **Summary**: Most existing public face datasets, such as MS-Celeb-1M and VGGFace2, provide abundant information in both breadth (large number of IDs) and depth (sufficient number of samples) for training. However, in many real-world scenarios of face recognition, the training dataset is limited in depth, i.e. only two face images are available for each ID. $\textit{We define this situation as Shallow Face Learning, and find it problematic with existing training methods.}$ Unlike deep face data, the shallow face data lacks intra-class diversity. As such, it can lead to collapse of feature dimension and consequently the learned network can easily suffer from degeneration and over-fitting in the collapsed dimension. In this paper, we aim to address the problem by introducing a novel training method named Semi-Siamese Training (SST). A pair of Semi-Siamese networks constitute the forward propagation structure, and the training loss is computed with an updating gallery queue, conducting effective optimization on shallow training data. Our method is developed without extra-dependency, thus can be flexibly integrated with the existing loss functions and network architectures. Extensive experiments on various benchmarks of face recognition show the proposed method significantly improves the training, not only in shallow face learning, but also for conventional deep face data.



### Learning to Restore a Single Face Image Degraded by Atmospheric Turbulence using CNNs
- **Arxiv ID**: http://arxiv.org/abs/2007.08404v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.08404v1)
- **Published**: 2020-07-16 15:25:08+00:00
- **Updated**: 2020-07-16 15:25:08+00:00
- **Authors**: Rajeev Yasarla, Vishal M Patel
- **Comment**: None
- **Journal**: None
- **Summary**: Atmospheric turbulence significantly affects imaging systems which use light that has propagated through long atmospheric paths. Images captured under such condition suffer from a combination of geometric deformation and space varying blur. We present a deep learning-based solution to the problem of restoring a turbulence-degraded face image where prior information regarding the amount of geometric distortion and blur at each location of the face image is first estimated using two separate networks. The estimated prior information is then used by a network called, Turbulence Distortion Removal Network (TDRN), to correct geometric distortion and reduce blur in the face image. Furthermore, a novel loss is proposed to train TDRN where first and second order image gradients are computed along with their confidence maps to mitigate the effect of turbulence degradation. Comprehensive experiments on synthetic and real face images show that this framework is capable of alleviating blur and geometric distortion caused by atmospheric turbulence, and significantly improves the visual quality. In addition, an ablation study is performed to demonstrate the improvements obtained by different modules in the proposed method.



### Improving rigid 3D calibration for robotic surgery
- **Arxiv ID**: http://arxiv.org/abs/2007.08427v1
- **DOI**: 10.1109/TMRB.2020.3033670
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.08427v1)
- **Published**: 2020-07-16 16:06:26+00:00
- **Updated**: 2020-07-16 16:06:26+00:00
- **Authors**: Andrea Roberti, Nicola Piccinelli, Daniele Meli, Riccardo Muradore, Paolo Fiorini
- **Comment**: Submitted to the special issue of IEEE Transactions on Medical
  Robotics and Bionics 2020
- **Journal**: None
- **Summary**: Autonomy is the frontier of research in robotic surgery and its aim is to improve the quality of surgical procedures in the next future. One fundamental requirement for autonomy is advanced perception capability through vision sensors. In this paper, we propose a novel calibration technique for a surgical scenario with da Vinci robot. Calibration of the camera and the robot is necessary for precise positioning of the tools in order to emulate the high performance surgeons. Our calibration technique is tailored for RGB-D camera. Different tests performed on relevant use cases for surgery prove that we significantly improve precision and accuracy with respect to the state of the art solutions for similar devices on a surgical-size setup. Moreover, our calibration method can be easily extended to standard surgical endoscope to prompt its use in real surgical scenario.



### On Adversarial Robustness: A Neural Architecture Search perspective
- **Arxiv ID**: http://arxiv.org/abs/2007.08428v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.08428v4)
- **Published**: 2020-07-16 16:07:10+00:00
- **Updated**: 2021-08-26 09:01:16+00:00
- **Authors**: Chaitanya Devaguptapu, Devansh Agarwal, Gaurav Mittal, Pulkit Gopalani, Vineeth N Balasubramanian
- **Comment**: Accepted at the Workshop on Adversarial Robustness in Real-World,
  ICCV-2021 (previous version accepted at four ICLR-21 Workshops)
- **Journal**: None
- **Summary**: Adversarial robustness of deep learning models has gained much traction in the last few years. Various attacks and defenses are proposed to improve the adversarial robustness of modern-day deep learning architectures. While all these approaches help improve the robustness, one promising direction for improving adversarial robustness is unexplored, i.e., the complex topology of the neural network architecture. In this work, we address the following question: Can the complex topology of a neural network give adversarial robustness without any form of adversarial training?. We answer this empirically by experimenting with different hand-crafted and NAS-based architectures. Our findings show that, for small-scale attacks, NAS-based architectures are more robust for small-scale datasets and simple tasks than hand-crafted architectures. However, as the size of the dataset or the complexity of task increases, hand-crafted architectures are more robust than NAS-based architectures. Our work is the first large-scale study to understand adversarial robustness purely from an architectural perspective. Our study shows that random sampling in the search space of DARTS (a popular NAS method) with simple ensembling can improve the robustness to PGD attack by nearly~12\%. We show that NAS, which is popular for achieving SoTA accuracy, can provide adversarial accuracy as a free add-on without any form of adversarial training. Our results show that leveraging the search space of NAS methods with methods like ensembles can be an excellent way to achieve adversarial robustness without any form of adversarial training. We also introduce a metric that can be used to calculate the trade-off between clean accuracy and adversarial robustness. Code and pre-trained models will be made available at \url{https://github.com/tdchaitanya/nas-robustness}



### Appearance-Preserving 3D Convolution for Video-based Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/2007.08434v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.08434v2)
- **Published**: 2020-07-16 16:21:34+00:00
- **Updated**: 2020-07-27 10:57:04+00:00
- **Authors**: Xinqian Gu, Hong Chang, Bingpeng Ma, Hongkai Zhang, Xilin Chen
- **Comment**: Accepted by ECCV2020 (Oral)
- **Journal**: None
- **Summary**: Due to the imperfect person detection results and posture changes, temporal appearance misalignment is unavoidable in video-based person re-identification (ReID). In this case, 3D convolution may destroy the appearance representation of person video clips, thus it is harmful to ReID. To address this problem, we propose AppearancePreserving 3D Convolution (AP3D), which is composed of two components: an Appearance-Preserving Module (APM) and a 3D convolution kernel. With APM aligning the adjacent feature maps in pixel level, the following 3D convolution can model temporal information on the premise of maintaining the appearance representation quality. It is easy to combine AP3D with existing 3D ConvNets by simply replacing the original 3D convolution kernels with AP3Ds. Extensive experiments demonstrate the effectiveness of AP3D for video-based ReID and the results on three widely used datasets surpass the state-of-the-arts. Code is available at: https://github.com/guxinqian/AP3D.



### Kronecker Attention Networks
- **Arxiv ID**: http://arxiv.org/abs/2007.08442v1
- **DOI**: 10.1145/3394486.3403065
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.08442v1)
- **Published**: 2020-07-16 16:26:02+00:00
- **Updated**: 2020-07-16 16:26:02+00:00
- **Authors**: Hongyang Gao, Zhengyang Wang, Shuiwang Ji
- **Comment**: 9 pages, KDD2020
- **Journal**: None
- **Summary**: Attention operators have been applied on both 1-D data like texts and higher-order data such as images and videos. Use of attention operators on high-order data requires flattening of the spatial or spatial-temporal dimensions into a vector, which is assumed to follow a multivariate normal distribution. This not only incurs excessive requirements on computational resources, but also fails to preserve structures in data. In this work, we propose to avoid flattening by assuming the data follow matrix-variate normal distributions. Based on this new view, we develop Kronecker attention operators (KAOs) that operate on high-order tensor data directly. More importantly, the proposed KAOs lead to dramatic reductions in computational resources. Experimental results show that our methods reduce the amount of required computational resources by a factor of hundreds, with larger factors for higher-dimensional and higher-order data. Results also show that networks with KAOs outperform models without attention, while achieving competitive performance as those with original attention operators.



### Towards Evaluating Driver Fatigue with Robust Deep Learning Models
- **Arxiv ID**: http://arxiv.org/abs/2007.08453v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.08453v4)
- **Published**: 2020-07-16 16:44:49+00:00
- **Updated**: 2021-02-08 06:35:21+00:00
- **Authors**: Ken Alparslan, Yigit Alparslan, Matthew Burlick
- **Comment**: 8 pages, 12 figures, fixed typos, converted referencing to BibLatex
  from plain text
- **Journal**: None
- **Summary**: In this paper, we explore different deep learning based approaches to detect driver fatigue. Drowsy driving results in approximately 72,000 crashes and 44,000 injuries every year in the US and detecting drowsiness and alerting the driver can save many lives. There have been many approaches to detect fatigue, of which eye closedness detection is one. We propose a framework to detect eye closedness in a captured camera frame as a gateway for detecting drowsiness. We explore two different datasets to detect eye closedness. We develop an eye model by using new Eye-blink dataset and a face model by using the Closed Eyes in the Wild (CEW). We also explore different techniques to make the models more robust by adding noise. We achieve 95.84% accuracy on our eye model and 80.01% accuracy on our face model. We also see that we can improve our accuracy on the face model by 6% via adversarial training and data augmentation. We hope that our work will be useful to the field of driver fatigue detection to avoid potential vehicle accidents related to drowsy driving.



### Shape Prior Deformation for Categorical 6D Object Pose and Size Estimation
- **Arxiv ID**: http://arxiv.org/abs/2007.08454v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.08454v1)
- **Published**: 2020-07-16 16:45:05+00:00
- **Updated**: 2020-07-16 16:45:05+00:00
- **Authors**: Meng Tian, Marcelo H Ang Jr, Gim Hee Lee
- **Comment**: Accepted at ECCV 2020
- **Journal**: None
- **Summary**: We present a novel learning approach to recover the 6D poses and sizes of unseen object instances from an RGB-D image. To handle the intra-class shape variation, we propose a deep network to reconstruct the 3D object model by explicitly modeling the deformation from a pre-learned categorical shape prior. Additionally, our network infers the dense correspondences between the depth observation of the object instance and the reconstructed 3D model to jointly estimate the 6D object pose and size. We design an autoencoder that trains on a collection of object models and compute the mean latent embedding for each category to learn the categorical shape priors. Extensive experiments on both synthetic and real-world datasets demonstrate that our approach significantly outperforms the state of the art. Our code is available at https://github.com/mentian/object-deformnet.



### Artificial Fingerprinting for Generative Models: Rooting Deepfake Attribution in Training Data
- **Arxiv ID**: http://arxiv.org/abs/2007.08457v7
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, cs.CY, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.08457v7)
- **Published**: 2020-07-16 16:49:55+00:00
- **Updated**: 2022-03-17 20:45:53+00:00
- **Authors**: Ning Yu, Vladislav Skripniuk, Sahar Abdelnabi, Mario Fritz
- **Comment**: Accepted to ICCV'21 as Oral
- **Journal**: None
- **Summary**: Photorealistic image generation has reached a new level of quality due to the breakthroughs of generative adversarial networks (GANs). Yet, the dark side of such deepfakes, the malicious use of generated media, raises concerns about visual misinformation. While existing research work on deepfake detection demonstrates high accuracy, it is subject to advances in generation techniques and adversarial iterations on detection countermeasure techniques. Thus, we seek a proactive and sustainable solution on deepfake detection, that is agnostic to the evolution of generative models, by introducing artificial fingerprints into the models.   Our approach is simple and effective. We first embed artificial fingerprints into training data, then validate a surprising discovery on the transferability of such fingerprints from training data to generative models, which in turn appears in the generated deepfakes. Experiments show that our fingerprinting solution (1) holds for a variety of cutting-edge generative models, (2) leads to a negligible side effect on generation quality, (3) stays robust against image-level and model-level perturbations, (4) stays hard to be detected by adversaries, and (5) converts deepfake detection and attribution into trivial tasks and outperforms the recent state-of-the-art baselines. Our solution closes the responsibility loop between publishing pre-trained generative model inventions and their possible misuses, which makes it independent of the current arms race. Code and models are available at https://github.com/ningyu1991/ArtificialGANFingerprints .



### openDD: A Large-Scale Roundabout Drone Dataset
- **Arxiv ID**: http://arxiv.org/abs/2007.08463v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.08463v1)
- **Published**: 2020-07-16 17:01:44+00:00
- **Updated**: 2020-07-16 17:01:44+00:00
- **Authors**: Antonia Breuer, Jan-Aike Termöhlen, Silviu Homoceanu, Tim Fingscheidt
- **Comment**: ITSC 2020 Conference Paper
- **Journal**: None
- **Summary**: Analyzing and predicting the traffic scene around the ego vehicle has been one of the key challenges in autonomous driving. Datasets including the trajectories of all road users present in a scene, as well as the underlying road topology are invaluable to analyze the behavior of the different traffic participants. The interaction between the various traffic participants is especially high in intersection types that are not regulated by traffic lights, the most common one being the roundabout.   We introduce the openDD dataset, including 84,774 accurately tracked trajectories and HD map data of seven different roundabouts. The openDD dataset is annotated using images taken by a drone in 501 separate flights, totalling in over 62 hours of trajectory data. As of today, openDD is by far the largest publicly available trajectory dataset recorded from a drone perspective, while comparable datasets span 17 hours at most.   The data is available, for both commercial and noncommercial use, at: http://www.l3pilot.eu/openDD.



### Certifiably Adversarially Robust Detection of Out-of-Distribution Data
- **Arxiv ID**: http://arxiv.org/abs/2007.08473v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.08473v3)
- **Published**: 2020-07-16 17:16:47+00:00
- **Updated**: 2021-03-10 15:55:00+00:00
- **Authors**: Julian Bitterwolf, Alexander Meinke, Matthias Hein
- **Comment**: Published and presented at NeurIPS 2020. Code available at
  https://gitlab.com/Bitterwolf/GOOD v3: added missing acknowledgement
- **Journal**: Advances in Neural Information Processing Systems 33 (NeurIPS
  2020)
- **Summary**: Deep neural networks are known to be overconfident when applied to out-of-distribution (OOD) inputs which clearly do not belong to any class. This is a problem in safety-critical applications since a reliable assessment of the uncertainty of a classifier is a key property, allowing the system to trigger human intervention or to transfer into a safe state. In this paper, we aim for certifiable worst case guarantees for OOD detection by enforcing not only low confidence at the OOD point but also in an $l_\infty$-ball around it. For this purpose, we use interval bound propagation (IBP) to upper bound the maximal confidence in the $l_\infty$-ball and minimize this upper bound during training time. We show that non-trivial bounds on the confidence for OOD data generalizing beyond the OOD dataset seen at training time are possible. Moreover, in contrast to certified adversarial robustness which typically comes with significant loss in prediction performance, certified guarantees for worst case OOD detection are possible without much loss in accuracy.



### 3D CNN-PCA: A Deep-Learning-Based Parameterization for Complex Geomodels
- **Arxiv ID**: http://arxiv.org/abs/2007.08478v1
- **DOI**: 10.1016/j.cageo.2020.104676
- **Categories**: **cs.CV**, cs.CE, cs.LG, physics.geo-ph
- **Links**: [PDF](http://arxiv.org/pdf/2007.08478v1)
- **Published**: 2020-07-16 17:25:14+00:00
- **Updated**: 2020-07-16 17:25:14+00:00
- **Authors**: Yimin Liu, Louis J. Durlofsky
- **Comment**: None
- **Journal**: None
- **Summary**: Geological parameterization enables the representation of geomodels in terms of a relatively small set of variables. Parameterization is therefore very useful in the context of data assimilation and uncertainty quantification. In this study, a deep-learning-based geological parameterization algorithm, CNN-PCA, is developed for complex 3D geomodels. CNN-PCA entails the use of convolutional neural networks as a post-processor for the low-dimensional principal component analysis representation of a geomodel. The 3D treatments presented here differ somewhat from those used in the 2D CNN-PCA procedure. Specifically, we introduce a new supervised-learning-based reconstruction loss, which is used in combination with style loss and hard data loss. The style loss uses features extracted from a 3D CNN pretrained for video classification. The 3D CNN-PCA algorithm is applied for the generation of conditional 3D realizations, defined on $60\times60\times40$ grids, for three geological scenarios (binary and bimodal channelized systems, and a three-facies channel-levee-mud system). CNN-PCA realizations are shown to exhibit geological features that are visually consistent with reference models generated using object-based methods. Statistics of flow responses ($\text{P}_{10}$, $\text{P}_{50}$, $\text{P}_{90}$ percentile results) for test sets of 3D CNN-PCA models are shown to be in consistent agreement with those from reference geomodels. Lastly, CNN-PCA is successfully applied for history matching with ESMDA for the bimodal channelized system.



### Co-Attention for Conditioned Image Matching
- **Arxiv ID**: http://arxiv.org/abs/2007.08480v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.08480v2)
- **Published**: 2020-07-16 17:32:00+00:00
- **Updated**: 2021-03-26 17:10:13+00:00
- **Authors**: Olivia Wiles, Sebastien Ehrhardt, Andrew Zisserman
- **Comment**: Accepted at CVPR 2021. Project page:
  https://www.robots.ox.ac.uk/~ow/coam.html. Formerly D2D: Learning to find
  good correspondences for image matching and manipulation
- **Journal**: None
- **Summary**: We propose a new approach to determine correspondences between image pairs in the wild under large changes in illumination, viewpoint, context, and material. While other approaches find correspondences between pairs of images by treating the images independently, we instead condition on both images to implicitly take account of the differences between them. To achieve this, we introduce (i) a spatial attention mechanism (a co-attention module, CoAM) for conditioning the learned features on both images, and (ii) a distinctiveness score used to choose the best matches at test time. CoAM can be added to standard architectures and trained using self-supervision or supervised data, and achieves a significant performance improvement under hard conditions, e.g. large viewpoint changes. We demonstrate that models using CoAM achieve state of the art or competitive results on a wide range of tasks: local matching, camera localization, 3D reconstruction, and image stylization.



### Complete & Label: A Domain Adaptation Approach to Semantic Segmentation of LiDAR Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2007.08488v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.08488v2)
- **Published**: 2020-07-16 17:42:05+00:00
- **Updated**: 2021-03-30 23:12:59+00:00
- **Authors**: Li Yi, Boqing Gong, Thomas Funkhouser
- **Comment**: None
- **Journal**: None
- **Summary**: We study an unsupervised domain adaptation problem for the semantic labeling of 3D point clouds, with a particular focus on domain discrepancies induced by different LiDAR sensors. Based on the observation that sparse 3D point clouds are sampled from 3D surfaces, we take a Complete and Label approach to recover the underlying surfaces before passing them to a segmentation network. Specifically, we design a Sparse Voxel Completion Network (SVCN) to complete the 3D surfaces of a sparse point cloud. Unlike semantic labels, to obtain training pairs for SVCN requires no manual labeling. We also introduce local adversarial learning to model the surface prior. The recovered 3D surfaces serve as a canonical domain, from which semantic labels can transfer across different LiDAR sensors. Experiments and ablation studies with our new benchmark for cross-domain semantic labeling of LiDAR data show that the proposed approach provides 8.2-36.6% better performance than previous domain adaptation methods.



### Do Adversarially Robust ImageNet Models Transfer Better?
- **Arxiv ID**: http://arxiv.org/abs/2007.08489v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.08489v2)
- **Published**: 2020-07-16 17:42:40+00:00
- **Updated**: 2020-12-08 01:56:10+00:00
- **Authors**: Hadi Salman, Andrew Ilyas, Logan Engstrom, Ashish Kapoor, Aleksander Madry
- **Comment**: NeurIPS 2020
- **Journal**: None
- **Summary**: Transfer learning is a widely-used paradigm in deep learning, where models pre-trained on standard datasets can be efficiently adapted to downstream tasks. Typically, better pre-trained models yield better transfer results, suggesting that initial accuracy is a key aspect of transfer learning performance. In this work, we identify another such aspect: we find that adversarially robust models, while less accurate, often perform better than their standard-trained counterparts when used for transfer learning. Specifically, we focus on adversarially robust ImageNet classifiers, and show that they yield improved accuracy on a standard suite of downstream classification tasks. Further analysis uncovers more differences between robust and standard models in the context of transfer learning. Our results are consistent with (and in fact, add to) recent hypotheses stating that robustness leads to improved feature representations. Our code and models are available at https://github.com/Microsoft/robust-models-transfer .



### Vehicle Detection of Multi-source Remote Sensing Data Using Active Fine-tuning Network
- **Arxiv ID**: http://arxiv.org/abs/2007.08494v1
- **DOI**: 10.1016/j.isprsjprs.2020.06.016
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.08494v1)
- **Published**: 2020-07-16 17:46:46+00:00
- **Updated**: 2020-07-16 17:46:46+00:00
- **Authors**: Xin Wu, Wei Li, Danfeng Hong, Jiaojiao Tian, Ran Tao, Qian Du
- **Comment**: None
- **Journal**: ISPRS Journal of Photogrammetry and Remote Sensing,167:39-53,2020
- **Summary**: Vehicle detection in remote sensing images has attracted increasing interest in recent years. However, its detection ability is limited due to lack of well-annotated samples, especially in densely crowded scenes. Furthermore, since a list of remotely sensed data sources is available, efficient exploitation of useful information from multi-source data for better vehicle detection is challenging. To solve the above issues, a multi-source active fine-tuning vehicle detection (Ms-AFt) framework is proposed, which integrates transfer learning, segmentation, and active classification into a unified framework for auto-labeling and detection. The proposed Ms-AFt employs a fine-tuning network to firstly generate a vehicle training set from an unlabeled dataset. To cope with the diversity of vehicle categories, a multi-source based segmentation branch is then designed to construct additional candidate object sets. The separation of high quality vehicles is realized by a designed attentive classifications network. Finally, all three branches are combined to achieve vehicle detection. Extensive experimental results conducted on two open ISPRS benchmark datasets, namely the Vaihingen village and Potsdam city datasets, demonstrate the superiority and effectiveness of the proposed Ms-AFt for vehicle detection. In addition, the generalization ability of Ms-AFt in dense remote sensing scenes is further verified on stereo aerial imagery of a large camping site.



### Accelerating 3D Deep Learning with PyTorch3D
- **Arxiv ID**: http://arxiv.org/abs/2007.08501v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.08501v1)
- **Published**: 2020-07-16 17:53:02+00:00
- **Updated**: 2020-07-16 17:53:02+00:00
- **Authors**: Nikhila Ravi, Jeremy Reizenstein, David Novotny, Taylor Gordon, Wan-Yen Lo, Justin Johnson, Georgia Gkioxari
- **Comment**: tech report
- **Journal**: None
- **Summary**: Deep learning has significantly improved 2D image recognition. Extending into 3D may advance many new applications including autonomous vehicles, virtual and augmented reality, authoring 3D content, and even improving 2D recognition. However despite growing interest, 3D deep learning remains relatively underexplored. We believe that some of this disparity is due to the engineering challenges involved in 3D deep learning, such as efficiently processing heterogeneous data and reframing graphics operations to be differentiable. We address these challenges by introducing PyTorch3D, a library of modular, efficient, and differentiable operators for 3D deep learning. It includes a fast, modular differentiable renderer for meshes and point clouds, enabling analysis-by-synthesis approaches. Compared with other differentiable renderers, PyTorch3D is more modular and efficient, allowing users to more easily extend it while also gracefully scaling to large meshes and images. We compare the PyTorch3D operators and renderer with other implementations and demonstrate significant speed and memory improvements. We also use PyTorch3D to improve the state-of-the-art for unsupervised 3D mesh and point cloud prediction from 2D images on ShapeNet. PyTorch3D is open-source and we hope it will help accelerate research in 3D deep learning.



### Implicit Mesh Reconstruction from Unannotated Image Collections
- **Arxiv ID**: http://arxiv.org/abs/2007.08504v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.08504v1)
- **Published**: 2020-07-16 17:55:20+00:00
- **Updated**: 2020-07-16 17:55:20+00:00
- **Authors**: Shubham Tulsiani, Nilesh Kulkarni, Abhinav Gupta
- **Comment**: Project page: https://shubhtuls.github.io/imr/
- **Journal**: None
- **Summary**: We present an approach to infer the 3D shape, texture, and camera pose for an object from a single RGB image, using only category-level image collections with foreground masks as supervision. We represent the shape as an image-conditioned implicit function that transforms the surface of a sphere to that of the predicted mesh, while additionally predicting the corresponding texture. To derive supervisory signal for learning, we enforce that: a) our predictions when rendered should explain the available image evidence, and b) the inferred 3D structure should be geometrically consistent with learned pixel to surface mappings. We empirically show that our approach improves over prior work that leverages similar supervision, and in fact performs competitively to methods that use stronger supervision. Finally, as our method enables learning with limited supervision, we qualitatively demonstrate its applicability over a set of about 30 object categories.



### FeatMatch: Feature-Based Augmentation for Semi-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2007.08505v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.08505v1)
- **Published**: 2020-07-16 17:55:31+00:00
- **Updated**: 2020-07-16 17:55:31+00:00
- **Authors**: Chia-Wen Kuo, Chih-Yao Ma, Jia-Bin Huang, Zsolt Kira
- **Comment**: Paper accepted in ECCV 2020. Project page:
  https://sites.google.com/view/chiawen-kuo/home/featmatch
- **Journal**: None
- **Summary**: Recent state-of-the-art semi-supervised learning (SSL) methods use a combination of image-based transformations and consistency regularization as core components. Such methods, however, are limited to simple transformations such as traditional data augmentation or convex combinations of two images. In this paper, we propose a novel learned feature-based refinement and augmentation method that produces a varied set of complex transformations. Importantly, these transformations also use information from both within-class and across-class prototypical representations that we extract through clustering. We use features already computed across iterations by storing them in a memory bank, obviating the need for significant extra computation. These transformations, combined with traditional image-based augmentation, are then used as part of the consistency-based regularization loss. We demonstrate that our method is comparable to current state of art for smaller datasets (CIFAR-10 and SVHN) while being able to scale up to larger datasets such as CIFAR-100 and mini-Imagenet where we achieve significant gains over the state of art (\textit{e.g.,} absolute 17.44\% gain on mini-ImageNet). We further test our method on DomainNet, demonstrating better robustness to out-of-domain unlabeled data, and perform rigorous ablations and analysis to validate the method.



### RepPoints V2: Verification Meets Regression for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2007.08508v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.08508v1)
- **Published**: 2020-07-16 17:57:08+00:00
- **Updated**: 2020-07-16 17:57:08+00:00
- **Authors**: Yihong Chen, Zheng Zhang, Yue Cao, Liwei Wang, Stephen Lin, Han Hu
- **Comment**: None
- **Journal**: None
- **Summary**: Verification and regression are two general methodologies for prediction in neural networks. Each has its own strengths: verification can be easier to infer accurately, and regression is more efficient and applicable to continuous target variables. Hence, it is often beneficial to carefully combine them to take advantage of their benefits. In this paper, we take this philosophy to improve state-of-the-art object detection, specifically by RepPoints. Though RepPoints provides high performance, we find that its heavy reliance on regression for object localization leaves room for improvement. We introduce verification tasks into the localization prediction of RepPoints, producing RepPoints v2, which provides consistent improvements of about 2.0 mAP over the original RepPoints on the COCO object detection benchmark using different backbones and training methods. RepPoints v2 also achieves 52.1 mAP on COCO \texttt{test-dev} by a single model. Moreover, we show that the proposed approach can more generally elevate other object detection frameworks as well as applications such as instance segmentation. The code is available at https://github.com/Scalsol/RepPointsV2.



### World-Consistent Video-to-Video Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2007.08509v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.08509v1)
- **Published**: 2020-07-16 17:58:13+00:00
- **Updated**: 2020-07-16 17:58:13+00:00
- **Authors**: Arun Mallya, Ting-Chun Wang, Karan Sapra, Ming-Yu Liu
- **Comment**: Published at the European Conference on Computer Vision, 2020
- **Journal**: None
- **Summary**: Video-to-video synthesis (vid2vid) aims for converting high-level semantic inputs to photorealistic videos. While existing vid2vid methods can achieve short-term temporal consistency, they fail to ensure the long-term one. This is because they lack knowledge of the 3D world being rendered and generate each frame only based on the past few frames. To address the limitation, we introduce a novel vid2vid framework that efficiently and effectively utilizes all past generated frames during rendering. This is achieved by condensing the 3D world rendered so far into a physically-grounded estimate of the current frame, which we call the guidance image. We further propose a novel neural network architecture to take advantage of the information stored in the guidance images. Extensive experimental results on several challenging datasets verify the effectiveness of our approach in achieving world consistency - the output video is consistent within the entire rendered 3D world.   https://nvlabs.github.io/wc-vid2vid/



### RetrieveGAN: Image Synthesis via Differentiable Patch Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2007.08513v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.08513v1)
- **Published**: 2020-07-16 17:59:04+00:00
- **Updated**: 2020-07-16 17:59:04+00:00
- **Authors**: Hung-Yu Tseng, Hsin-Ying Lee, Lu Jiang, Ming-Hsuan Yang, Weilong Yang
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: Image generation from scene description is a cornerstone technique for the controlled generation, which is beneficial to applications such as content creation and image editing. In this work, we aim to synthesize images from scene description with retrieved patches as reference. We propose a differentiable retrieval module. With the differentiable retrieval module, we can (1) make the entire pipeline end-to-end trainable, enabling the learning of better feature embedding for retrieval; (2) encourage the selection of mutually compatible patches with additional objective functions. We conduct extensive quantitative and qualitative experiments to demonstrate that the proposed method can generate realistic and diverse images, where the retrieved patches are reasonable and mutually compatible.



### Talking-head Generation with Rhythmic Head Motion
- **Arxiv ID**: http://arxiv.org/abs/2007.08547v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2007.08547v1)
- **Published**: 2020-07-16 18:13:40+00:00
- **Updated**: 2020-07-16 18:13:40+00:00
- **Authors**: Lele Chen, Guofeng Cui, Celong Liu, Zhong Li, Ziyi Kou, Yi Xu, Chenliang Xu
- **Comment**: None
- **Journal**: None
- **Summary**: When people deliver a speech, they naturally move heads, and this rhythmic head motion conveys prosodic information. However, generating a lip-synced video while moving head naturally is challenging. While remarkably successful, existing works either generate still talkingface videos or rely on landmark/video frames as sparse/dense mapping guidance to generate head movements, which leads to unrealistic or uncontrollable video synthesis. To overcome the limitations, we propose a 3D-aware generative network along with a hybrid embedding module and a non-linear composition module. Through modeling the head motion and facial expressions1 explicitly, manipulating 3D animation carefully, and embedding reference images dynamically, our approach achieves controllable, photo-realistic, and temporally coherent talking-head videos with natural head movements. Thoughtful experiments on several standard benchmarks demonstrate that our method achieves significantly better results than the state-of-the-art methods in both quantitative and qualitative comparisons. The code is available on https://github.com/ lelechen63/Talking-head-Generation-with-Rhythmic-Head-Motion.



### Smooth Deformation Field-based Mismatch Removal in Real-time
- **Arxiv ID**: http://arxiv.org/abs/2007.08553v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2007.08553v1)
- **Published**: 2020-07-16 18:20:25+00:00
- **Updated**: 2020-07-16 18:20:25+00:00
- **Authors**: Haoyin Zhou, Jagadeesan Jayender
- **Comment**: submitted for peer review since 10/2019
- **Journal**: None
- **Summary**: This paper studies the mismatch removal problem, which may serve as the subsequent step of feature matching. Non-rigid deformation makes it difficult to remove mismatches because no parametric transformation can be found. To solve this problem, we first propose an algorithm based on the re-weighting and 1-point RANSAC strategy (R1P-RNSC), which is a parametric method under a reasonable assumption that the non-rigid deformation can be approximately represented by multiple locally rigid transformations. R1P-RNSC is fast but suffers from a drawback that the local smoothing information cannot be taken into account. Then, we propose a non-parametric algorithm based on the expectation maximization algorithm and dual quaternion (EMDQ) representation to generate the smooth deformation field. The two algorithms compensate for the drawbacks of each other. Specifically, EMDQ needs good initial values provided by R1P-RNSC, and R1P-RNSC needs EMDQ for refinement. Experimental results with real-world data demonstrate that the combination of the two algorithms has the best accuracy compared to other state-of-the-art methods, which can handle up to 85% of outliers in real-time. The ability to generate dense deformation field from sparse matches with outliers in real-time makes the proposed algorithms have many potential applications, such as non-rigid registration and SLAM.



### A New Look at Ghost Normalization
- **Arxiv ID**: http://arxiv.org/abs/2007.08554v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.08554v1)
- **Published**: 2020-07-16 18:23:52+00:00
- **Updated**: 2020-07-16 18:23:52+00:00
- **Authors**: Neofytos Dimitriou, Ognjen Arandjelovic
- **Comment**: None
- **Journal**: None
- **Summary**: Batch normalization (BatchNorm) is an effective yet poorly understood technique for neural network optimization. It is often assumed that the degradation in BatchNorm performance to smaller batch sizes stems from it having to estimate layer statistics using smaller sample sizes. However, recently, Ghost normalization (GhostNorm), a variant of BatchNorm that explicitly uses smaller sample sizes for normalization, has been shown to improve upon BatchNorm in some datasets. Our contributions are: (i) we uncover a source of regularization that is unique to GhostNorm, and not simply an extension from BatchNorm, (ii) three types of GhostNorm implementations are described, two of which employ BatchNorm as the underlying normalization technique, (iii) by visualising the loss landscape of GhostNorm, we observe that GhostNorm consistently decreases the smoothness when compared to BatchNorm, (iv) we introduce Sequential Normalization (SeqNorm), and report superior performance over state-of-the-art methodologies on both CIFAR--10 and CIFAR--100 datasets.



### InfoFocus: 3D Object Detection for Autonomous Driving with Dynamic Information Modeling
- **Arxiv ID**: http://arxiv.org/abs/2007.08556v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.08556v1)
- **Published**: 2020-07-16 18:27:08+00:00
- **Updated**: 2020-07-16 18:27:08+00:00
- **Authors**: Jun Wang, Shiyi Lan, Mingfei Gao, Larry S. Davis
- **Comment**: None
- **Journal**: None
- **Summary**: Real-time 3D object detection is crucial for autonomous cars. Achieving promising performance with high efficiency, voxel-based approaches have received considerable attention. However, previous methods model the input space with features extracted from equally divided sub-regions without considering that point cloud is generally non-uniformly distributed over the space. To address this issue, we propose a novel 3D object detection framework with dynamic information modeling. The proposed framework is designed in a coarse-to-fine manner. Coarse predictions are generated in the first stage via a voxel-based region proposal network. We introduce InfoFocus, which improves the coarse detections by adaptively refining features guided by the information of point cloud density. Experiments are conducted on the large-scale nuScenes 3D detection benchmark. Results show that our framework achieves the state-of-the-art performance with 31 FPS and improves our baseline significantly by 9.0% mAP on the nuScenes test set.



### On Robustness and Transferability of Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2007.08558v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.08558v2)
- **Published**: 2020-07-16 18:39:04+00:00
- **Updated**: 2021-03-23 16:31:47+00:00
- **Authors**: Josip Djolonga, Jessica Yung, Michael Tschannen, Rob Romijnders, Lucas Beyer, Alexander Kolesnikov, Joan Puigcerver, Matthias Minderer, Alexander D'Amour, Dan Moldovan, Sylvain Gelly, Neil Houlsby, Xiaohua Zhai, Mario Lucic
- **Comment**: Accepted at CVPR 2021
- **Journal**: None
- **Summary**: Modern deep convolutional networks (CNNs) are often criticized for not generalizing under distributional shifts. However, several recent breakthroughs in transfer learning suggest that these networks can cope with severe distribution shifts and successfully adapt to new tasks from a few training examples. In this work we study the interplay between out-of-distribution and transfer performance of modern image classification CNNs for the first time and investigate the impact of the pre-training data size, the model scale, and the data preprocessing pipeline. We find that increasing both the training set and model sizes significantly improve the distributional shift robustness. Furthermore, we show that, perhaps surprisingly, simple changes in the preprocessing such as modifying the image resolution can significantly mitigate robustness issues in some cases. Finally, we outline the shortcomings of existing robustness evaluation datasets and introduce a synthetic dataset SI-Score we use for a systematic analysis across factors of variation common in visual data such as object size and position.



### SqueezeFacePoseNet: Lightweight Face Verification Across Different Poses for Mobile Platforms
- **Arxiv ID**: http://arxiv.org/abs/2007.08566v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.08566v2)
- **Published**: 2020-07-16 19:02:38+00:00
- **Updated**: 2020-11-16 14:57:36+00:00
- **Authors**: Fernando Alonso-Fernandez, Javier Barrachina, Kevin Hernandez-Diaz, Josef Bigun
- **Comment**: None
- **Journal**: Published at ICPR 2020/WMWB IAPR TC4 Workshop on Mobile and
  Wearable Biometrics. Presentation available at https://youtu.be/suJmO8IWp8k
- **Summary**: Virtual applications through mobile platforms are one of the most critical and ever-growing fields in AI, where ubiquitous and real-time person authentication has become critical after the breakthrough of all services provided via mobile devices. In this context, face verification technologies can provide reliable and robust user authentication, given the availability of cameras in these devices, as well as their widespread use in everyday applications. The rapid development of deep Convolutional Neural Networks has resulted in many accurate face verification architectures. However, their typical size (hundreds of megabytes) makes them infeasible to be incorporated in downloadable mobile applications where the entire file typically may not exceed 100 Mb. Accordingly, we address the challenge of developing a lightweight face recognition network of just a few megabytes that can operate with sufficient accuracy in comparison to much larger models. The network also should be able to operate under different poses, given the variability naturally observed in uncontrolled environments where mobile devices are typically used. In this paper, we adapt the lightweight SqueezeNet model, of just 4.4MB, to effectively provide cross-pose face recognition. After trained on the MS-Celeb-1M and VGGFace2 databases, our model achieves an EER of 1.23% on the difficult frontal vs. profile comparison, and0.54% on profile vs. profile images. Under less extreme variations involving frontal images in any of the enrolment/query images pair, EER is pushed down to<0.3%, and the FRR at FAR=0.1%to less than 1%. This makes our light model suitable for face recognition where at least acquisition of the enrolment image can be controlled. At the cost of a slight degradation in performance, we also test an even lighter model (of just 2.5MB) where regular convolutions are replaced with depth-wise separable convolutions.



### Real-time Dense Reconstruction of Tissue Surface from Stereo Optical Video
- **Arxiv ID**: http://arxiv.org/abs/2007.12623v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.12623v1)
- **Published**: 2020-07-16 19:14:05+00:00
- **Updated**: 2020-07-16 19:14:05+00:00
- **Authors**: Haoyin Zhou, Jagadeesan Jayender
- **Comment**: None
- **Journal**: IEEE transactions on medical imaging 39, no. 2 (2019): 400-412
- **Summary**: We propose an approach to reconstruct dense three-dimensional (3D) model of tissue surface from stereo optical videos in real-time, the basic idea of which is to first extract 3D information from video frames by using stereo matching, and then to mosaic the reconstructed 3D models. To handle the common low texture regions on tissue surfaces, we propose effective post-processing steps for the local stereo matching method to enlarge the radius of constraint, which include outliers removal, hole filling and smoothing. Since the tissue models obtained by stereo matching are limited to the field of view of the imaging modality, we propose a model mosaicking method by using a novel feature-based simultaneously localization and mapping (SLAM) method to align the models. Low texture regions and the varying illumination condition may lead to a large percentage of feature matching outliers. To solve this problem, we propose several algorithms to improve the robustness of SLAM, which mainly include (1) a histogram voting-based method to roughly select possible inliers from the feature matching results, (2) a novel 1-point RANSAC-based P$n$P algorithm called as DynamicR1PP$n$P to track the camera motion and (3) a GPU-based iterative closest points (ICP) and bundle adjustment (BA) method to refine the camera motion estimation results. Experimental results on ex- and in vivo data showed that the reconstructed 3D models have high resolution texture with an accuracy error of less than 2 mm. Most algorithms are highly parallelized for GPU computation, and the average runtime for processing one key frame is 76.3 ms on stereo images with 960x540 resolution.



### Camera Bias in a Fine Grained Classification Task
- **Arxiv ID**: http://arxiv.org/abs/2007.08574v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.08574v1)
- **Published**: 2020-07-16 19:18:49+00:00
- **Updated**: 2020-07-16 19:18:49+00:00
- **Authors**: Philip T. Jackson, Stephen Bonner, Ning Jia, Christopher Holder, Jon Stonehouse, Boguslaw Obara
- **Comment**: None
- **Journal**: None
- **Summary**: We show that correlations between the camera used to acquire an image and the class label of that image can be exploited by convolutional neural networks (CNN), resulting in a model that "cheats" at an image classification task by recognizing which camera took the image and inferring the class label from the camera. We show that models trained on a dataset with camera / label correlations do not generalize well to images in which those correlations are absent, nor to images from unencountered cameras. Furthermore, we investigate which visual features they are exploiting for camera recognition. Our experiments present evidence against the importance of global color statistics, lens deformation and chromatic aberration, and in favor of high frequency features, which may be introduced by image processing algorithms built into the cameras.



### Real-time Surface Deformation Recovery from Stereo Videos
- **Arxiv ID**: http://arxiv.org/abs/2007.08576v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.08576v1)
- **Published**: 2020-07-16 19:24:47+00:00
- **Updated**: 2020-07-16 19:24:47+00:00
- **Authors**: Haoyin Zhou, Jagadeesan Jayender
- **Comment**: In International Conference on Medical Image Computing and
  Computer-Assisted Intervention (MICCAI) (pp. 339-347). Springer, Cham
- **Journal**: None
- **Summary**: Tissue deformation during the surgery may significantly decrease the accuracy of surgical navigation systems. In this paper, we propose an approach to estimate the deformation of tissue surface from stereo videos in real-time, which is capable of handling occlusion, smooth surface and fast deformation. We first use a stereo matching method to extract depth information from stereo video frames and generate the tissue template, and then estimate the deformation of the obtained template by minimizing ICP, ORB feature matching and as-rigid-as-possible (ARAP) costs. The main novelties are twofold: (1) Due to non-rigid deformation, feature matching outliers are difficult to be removed by traditional RANSAC methods; therefore we propose a novel 1-point RANSAC and reweighting method to preselect matching inliers, which handles smooth surfaces and fast deformations. (2) We propose a novel ARAP cost function based on dense connections between the control points to achieve better smoothing performance with limited number of iterations. Algorithms are designed and implemented for GPU parallel computing. Experiments on ex- and in vivo data showed that this approach works at an update rate of 15Hz with an accuracy of less than 2.5 mm on a NVIDIA Titan X GPU.



### Re-weighting and 1-Point RANSAC-Based PnP Solution to Handle Outliers
- **Arxiv ID**: http://arxiv.org/abs/2007.08577v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.08577v1)
- **Published**: 2020-07-16 19:28:17+00:00
- **Updated**: 2020-07-16 19:28:17+00:00
- **Authors**: Haoyin Zhou, Tao Zhang, Jagadeesan Jayender
- **Comment**: https://github.com/haoyinzhou/PnP_Toolbox
- **Journal**: IEEE transactions on pattern analysis and machine intelligence 41,
  no. 12 (2018): 3022-3033
- **Summary**: The ability to handle outliers is essential for performing the perspective-n-point (PnP) approach in practical applications, but conventional RANSAC+P3P or P4P methods have high time complexities. We propose a fast PnP solution named R1PPnP to handle outliers by utilizing a soft re-weighting mechanism and the 1-point RANSAC scheme. We first present a PnP algorithm, which serves as the core of R1PPnP, for solving the PnP problem in outlier-free situations. The core algorithm is an optimal process minimizing an objective function conducted with a random control point. Then, to reduce the impact of outliers, we propose a reprojection error-based re-weighting method and integrate it into the core algorithm. Finally, we employ the 1-point RANSAC scheme to try different control points. Experiments with synthetic and real-world data demonstrate that R1PPnP is faster than RANSAC+P3P or P4P methods especially when the percentage of outliers is large, and is accurate. Besides, comparisons with outlier-free synthetic data show that R1PPnP is among the most accurate and fast PnP solutions, which usually serve as the final refinement step of RANSAC+P3P or P4P. Compared with REPPnP, which is the state-of-the-art PnP algorithm with an explicit outliers-handling mechanism, R1PPnP is slower but does not suffer from the percentage of outliers limitation as REPPnP.



### Advances in Deep Learning for Hyperspectral Image Analysis--Addressing Challenges Arising in Practical Imaging Scenarios
- **Arxiv ID**: http://arxiv.org/abs/2007.08592v1
- **DOI**: 10.1007/978-3-030-38617-7_5
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.08592v1)
- **Published**: 2020-07-16 19:51:02+00:00
- **Updated**: 2020-07-16 19:51:02+00:00
- **Authors**: Xiong Zhou, Saurabh Prasad
- **Comment**: Published as a chapter in Hyperspectral Image Analysis. Advances in
  Computer Vision and Pattern Recognition
- **Journal**: None
- **Summary**: Deep neural networks have proven to be very effective for computer vision tasks, such as image classification, object detection, and semantic segmentation -- these are primarily applied to color imagery and video. In recent years, there has been an emergence of deep learning algorithms being applied to hyperspectral and multispectral imagery for remote sensing and biomedicine tasks. These multi-channel images come with their own unique set of challenges that must be addressed for effective image analysis. Challenges include limited ground truth (annotation is expensive and extensive labeling is often not feasible), and high dimensional nature of the data (each pixel is represented by hundreds of spectral bands), despite being presented by a large amount of unlabeled data and the potential to leverage multiple sensors/sources that observe the same scene. In this chapter, we will review recent advances in the community that leverage deep learning for robust hyperspectral image analysis despite these unique challenges -- specifically, we will review unsupervised, semi-supervised and active learning approaches to image analysis, as well as transfer learning approaches for multi-source (e.g. multi-sensor, or multi-temporal) image analysis.



### Dynamic Low-light Imaging with Quanta Image Sensors
- **Arxiv ID**: http://arxiv.org/abs/2007.08614v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/2007.08614v1)
- **Published**: 2020-07-16 20:29:52+00:00
- **Updated**: 2020-07-16 20:29:52+00:00
- **Authors**: Yiheng Chi, Abhiram Gnanasambandam, Vladlen Koltun, Stanley H. Chan
- **Comment**: Published in the 16th European Conference on Computer Vision (ECCV)
  2020
- **Journal**: None
- **Summary**: Imaging in low light is difficult because the number of photons arriving at the sensor is low. Imaging dynamic scenes in low-light environments is even more difficult because as the scene moves, pixels in adjacent frames need to be aligned before they can be denoised. Conventional CMOS image sensors (CIS) are at a particular disadvantage in dynamic low-light settings because the exposure cannot be too short lest the read noise overwhelms the signal. We propose a solution using Quanta Image Sensors (QIS) and present a new image reconstruction algorithm. QIS are single-photon image sensors with photon counting capabilities. Studies over the past decade have confirmed the effectiveness of QIS for low-light imaging but reconstruction algorithms for dynamic scenes in low light remain an open problem. We fill the gap by proposing a student-teacher training protocol that transfers knowledge from a motion teacher and a denoising teacher to a student network. We show that dynamic scenes can be reconstructed from a burst of frames at a photon level of 1 photon per pixel per frame. Experimental results confirm the advantages of the proposed method compared to existing methods.



### Preserving Semantic Neighborhoods for Robust Cross-modal Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2007.08617v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.IR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.08617v1)
- **Published**: 2020-07-16 20:32:54+00:00
- **Updated**: 2020-07-16 20:32:54+00:00
- **Authors**: Christopher Thomas, Adriana Kovashka
- **Comment**: None
- **Journal**: ECCV 2020
- **Summary**: The abundance of multimodal data (e.g. social media posts) has inspired interest in cross-modal retrieval methods. Popular approaches rely on a variety of metric learning losses, which prescribe what the proximity of image and text should be, in the learned space. However, most prior methods have focused on the case where image and text convey redundant information; in contrast, real-world image-text pairs convey complementary information with little overlap. Further, images in news articles and media portray topics in a visually diverse fashion; thus, we need to take special care to ensure a meaningful image representation. We propose novel within-modality losses which encourage semantic coherency in both the text and image subspaces, which does not necessarily align with visual coherency. Our method ensures that not only are paired images and texts close, but the expected image-image and text-text relationships are also observed. Our approach improves the results of cross-modal retrieval on four datasets compared to five baselines.



### Accelerated Stochastic Gradient-free and Projection-free Methods
- **Arxiv ID**: http://arxiv.org/abs/2007.12625v2
- **DOI**: None
- **Categories**: **math.OC**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.12625v2)
- **Published**: 2020-07-16 20:50:15+00:00
- **Updated**: 2020-08-10 15:45:54+00:00
- **Authors**: Feihu Huang, Lue Tao, Songcan Chen
- **Comment**: Accepted to ICML 2020, 34 pages
- **Journal**: None
- **Summary**: In the paper, we propose a class of accelerated stochastic gradient-free and projection-free (a.k.a., zeroth-order Frank-Wolfe) methods to solve the constrained stochastic and finite-sum nonconvex optimization. Specifically, we propose an accelerated stochastic zeroth-order Frank-Wolfe (Acc-SZOFW) method based on the variance reduced technique of SPIDER/SpiderBoost and a novel momentum accelerated technique. Moreover, under some mild conditions, we prove that the Acc-SZOFW has the function query complexity of $O(d\sqrt{n}\epsilon^{-2})$ for finding an $\epsilon$-stationary point in the finite-sum problem, which improves the exiting best result by a factor of $O(\sqrt{n}\epsilon^{-2})$, and has the function query complexity of $O(d\epsilon^{-3})$ in the stochastic problem, which improves the exiting best result by a factor of $O(\epsilon^{-1})$. To relax the large batches required in the Acc-SZOFW, we further propose a novel accelerated stochastic zeroth-order Frank-Wolfe (Acc-SZOFW*) based on a new variance reduced technique of STORM, which still reaches the function query complexity of $O(d\epsilon^{-3})$ in the stochastic problem without relying on any large batches. In particular, we present an accelerated framework of the Frank-Wolfe methods based on the proposed momentum accelerated technique. The extensive experimental results on black-box adversarial attack and robust black-box classification demonstrate the efficiency of our algorithms.



### Explainable Deep Learning for Uncovering Actionable Scientific Insights for Materials Discovery and Design
- **Arxiv ID**: http://arxiv.org/abs/2007.08631v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.08631v1)
- **Published**: 2020-07-16 20:53:40+00:00
- **Updated**: 2020-07-16 20:53:40+00:00
- **Authors**: Shusen Liu, Bhavya Kailkhura, Jize Zhang, Anna M. Hiszpanski, Emily Robertson, Donald Loveland, T. Yong-Jin Han
- **Comment**: None
- **Journal**: None
- **Summary**: The scientific community has been increasingly interested in harnessing the power of deep learning to solve various domain challenges. However, despite the effectiveness in building predictive models, fundamental challenges exist in extracting actionable knowledge from deep neural networks due to their opaque nature. In this work, we propose techniques for exploring the behavior of deep learning models by injecting domain-specific actionable attributes as tunable "knobs" in the analysis pipeline. By incorporating the domain knowledge in a generative modeling framework, we are not only able to better understand the behavior of these black-box models, but also provide scientists with actionable insights that can potentially lead to fundamental discoveries.



### COV-ELM classifier: An Extreme Learning Machine based identification of COVID-19 using Chest X-Ray Images
- **Arxiv ID**: http://arxiv.org/abs/2007.08637v6
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.08637v6)
- **Published**: 2020-07-16 21:03:22+00:00
- **Updated**: 2021-09-28 05:31:29+00:00
- **Authors**: Sheetal Rajpal, Manoj Agarwal, Ankit Rajpal, Navin Lakhyani, Arpita Saggar, Naveen Kumar
- **Comment**: None
- **Journal**: None
- **Summary**: Coronaviruses constitute a family of viruses that gives rise to respiratory diseases. As COVID-19 is highly contagious, early diagnosis of COVID-19 is crucial for an effective treatment strategy. However, the RT-PCR test which is considered to be a gold standard in the diagnosis of COVID-19 suffers from a high false-negative rate. Chest X-ray (CXR) image analysis has emerged as a feasible and effective diagnostic technique towards this objective. In this work, we propose the COVID-19 classification problem as a three-class classification problem to distinguish between COVID-19, normal, and pneumonia classes. We propose a three-stage framework, named COV-ELM. Stage one deals with preprocessing and transformation while stage two deals with feature extraction. These extracted features are passed as an input to the ELM at the third stage, resulting in the identification of COVID-19. The choice of ELM in this work has been motivated by its faster convergence, better generalization capability, and shorter training time in comparison to the conventional gradient-based learning algorithms. As bigger and diverse datasets become available, ELM can be quickly retrained as compared to its gradient-based competitor models. The proposed model achieved a macro average F1-score of 0.95 and the overall sensitivity of ${0.94 \pm 0.02} at a 95% confidence interval. When compared to state-of-the-art machine learning algorithms, the COV-ELM is found to outperform its competitors in this three-class classification scenario. Further, LIME has been integrated with the proposed COV-ELM model to generate annotated CXR images. The annotations are based on the superpixels that have contributed to distinguish between the different classes. It was observed that the superpixels correspond to the regions of the human lungs that are clinically observed in COVID-19 and Pneumonia cases.



### SiamParseNet: Joint Body Parsing and Label Propagation in Infant Movement Videos
- **Arxiv ID**: http://arxiv.org/abs/2007.08646v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.08646v1)
- **Published**: 2020-07-16 21:14:25+00:00
- **Updated**: 2020-07-16 21:14:25+00:00
- **Authors**: Haomiao Ni, Yuan Xue, Qian Zhang, Xiaolei Huang
- **Comment**: MICCAI 2020
- **Journal**: None
- **Summary**: General movement assessment (GMA) of infant movement videos (IMVs) is an effective method for the early detection of cerebral palsy (CP) in infants. Automated body parsing is a crucial step towards computer-aided GMA, in which infant body parts are segmented and tracked over time for movement analysis. However, acquiring fully annotated data for video-based body parsing is particularly expensive due to the large number of frames in IMVs. In this paper, we propose a semi-supervised body parsing model, termed SiamParseNet (SPN), to jointly learn single frame body parsing and label propagation between frames in a semi-supervised fashion. The Siamese-structured SPN consists of a shared feature encoder, followed by two separate branches: one for intra-frame body parts segmentation, and one for inter-frame label propagation. The two branches are trained jointly, taking pairs of frames from the same videos as their input. An adaptive training process is proposed that alternates training modes between using input pairs of only labeled frames and using inputs of both labeled and unlabeled frames. During testing, we employ a multi-source inference mechanism, where the final result for a test frame is either obtained via the segmentation branch or via propagation from a nearby key frame. We conduct extensive experiments on a partially-labeled IMV dataset where SPN outperforms all prior arts, demonstrating the effectiveness of our proposed method.



### Least squares surface reconstruction on arbitrary domains
- **Arxiv ID**: http://arxiv.org/abs/2007.08661v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.08661v2)
- **Published**: 2020-07-16 21:33:39+00:00
- **Updated**: 2020-07-20 14:27:27+00:00
- **Authors**: Dizhong Zhu, William A P Smith
- **Comment**: None
- **Journal**: None
- **Summary**: Almost universally in computer vision, when surface derivatives are required, they are computed using only first order accurate finite difference approximations. We propose a new method for computing numerical derivatives based on 2D Savitzky-Golay filters and K-nearest neighbour kernels. The resulting derivative matrices can be used for least squares surface reconstruction over arbitrary (even disconnected) domains in the presence of large noise and allowing for higher order polynomial local surface approximations. They are useful for a range of tasks including normal-from-depth (i.e. surface differentiation), height-from-normals (i.e. surface integration) and shape-from-x. We show how to write both orthographic or perspective height-from-normals as a linear least squares problem using the same formulation and avoiding a nonlinear change of variables in the perspective case. We demonstrate improved performance relative to state-of-the-art across these tasks on both synthetic and real data and make available an open source implementation of our method.



### Super-Resolution Remote Imaging using Time Encoded Remote Apertures
- **Arxiv ID**: http://arxiv.org/abs/2007.08667v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.08667v1)
- **Published**: 2020-07-16 21:55:09+00:00
- **Updated**: 2020-07-16 21:55:09+00:00
- **Authors**: Ji Hyun Nam, Andreas Velten
- **Comment**: None
- **Journal**: None
- **Summary**: Imaging of scenes using light or other wave phenomena is subject to the diffraction limit. The spatial profile of a wave propagating between a scene and the imaging system is distorted by diffraction resulting in a loss of resolution that is proportional with traveled distance. We show here that it is possible to reconstruct sparse scenes from the temporal profile of the wave-front using only one spatial pixel or a spatial average. The temporal profile of the wave is not affected by diffraction yielding an imaging method that can in theory achieve wavelength scale resolution independent of distance from the scene.



### Deep Small Bowel Segmentation with Cylindrical Topological Constraints
- **Arxiv ID**: http://arxiv.org/abs/2007.08674v1
- **DOI**: 10.1007/978-3-030-59719-1_21
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.08674v1)
- **Published**: 2020-07-16 22:28:27+00:00
- **Updated**: 2020-07-16 22:28:27+00:00
- **Authors**: Seung Yeon Shin, Sungwon Lee, Daniel C. Elton, James L. Gulley, Ronald M. Summers
- **Comment**: Accepted to MICCAI 2020
- **Journal**: None
- **Summary**: We present a novel method for small bowel segmentation where a cylindrical topological constraint based on persistent homology is applied. To address the touching issue which could break the applied constraint, we propose to augment a network with an additional branch to predict an inner cylinder of the small bowel. Since the inner cylinder is free of the touching issue, a cylindrical shape constraint applied on this augmented branch guides the network to generate a topologically correct segmentation. For strict evaluation, we achieved an abdominal computed tomography dataset with dense segmentation ground-truths. The proposed method showed clear improvements in terms of four different metrics compared to the baseline method, and also showed the statistical significance from a paired t-test.



