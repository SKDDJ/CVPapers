# Arxiv Papers in cs.CV on 2020-07-22
### FLOT: Scene Flow on Point Clouds Guided by Optimal Transport
- **Arxiv ID**: http://arxiv.org/abs/2007.11142v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.11142v1)
- **Published**: 2020-07-22 00:15:30+00:00
- **Updated**: 2020-07-22 00:15:30+00:00
- **Authors**: Gilles Puy, Alexandre Boulch, Renaud Marlet
- **Comment**: Accepted at ECCV20
- **Journal**: None
- **Summary**: We propose and study a method called FLOT that estimates scene flow on point clouds. We start the design of FLOT by noticing that scene flow estimation on point clouds reduces to estimating a permutation matrix in a perfect world. Inspired by recent works on graph matching, we build a method to find these correspondences by borrowing tools from optimal transport. Then, we relax the transport constraints to take into account real-world imperfections. The transport cost between two points is given by the pairwise similarity between deep features extracted by a neural network trained under full supervision using synthetic datasets. Our main finding is that FLOT can perform as well as the best existing methods on synthetic and real-world datasets while requiring much less parameters and without using multiscale analysis. Our second finding is that, on the training datasets considered, most of the performance can be explained by the learned transport cost. This yields a simpler method, FLOT$_0$, which is obtained using a particular choice of optimal transport parameters and performs nearly as well as FLOT.



### Camera On-boarding for Person Re-identification using Hypothesis Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/2007.11149v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.11149v2)
- **Published**: 2020-07-22 00:43:29+00:00
- **Updated**: 2020-08-05 21:48:31+00:00
- **Authors**: Sk Miraj Ahmed, Aske R Lejb√∏lle, Rameswar Panda, Amit K. Roy-Chowdhury
- **Comment**: Accepted to CVPR 2020
- **Journal**: Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition (2020) 12144-12153
- **Summary**: Most of the existing approaches for person re-identification consider a static setting where the number of cameras in the network is fixed. An interesting direction, which has received little attention, is to explore the dynamic nature of a camera network, where one tries to adapt the existing re-identification models after on-boarding new cameras, with little additional effort. There have been a few recent methods proposed in person re-identification that attempt to address this problem by assuming the labeled data in the existing network is still available while adding new cameras. This is a strong assumption since there may exist some privacy issues for which one may not have access to those data. Rather, based on the fact that it is easy to store the learned re-identifications models, which mitigates any data privacy concern, we develop an efficient model adaptation approach using hypothesis transfer learning that aims to transfer the knowledge using only source models and limited labeled data, but without using any source camera data from the existing network. Our approach minimizes the effect of negative transfer by finding an optimal weighted combination of multiple source models for transferring the knowledge. Extensive experiments on four challenging benchmark datasets with a variable number of cameras well demonstrate the efficacy of our proposed approach over state-of-the-art methods.



### Rethinking CNN Models for Audio Classification
- **Arxiv ID**: http://arxiv.org/abs/2007.11154v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2007.11154v2)
- **Published**: 2020-07-22 01:31:44+00:00
- **Updated**: 2020-11-13 19:09:09+00:00
- **Authors**: Kamalesh Palanisamy, Dipika Singhania, Angela Yao
- **Comment**: 8 pages, 3 figures
- **Journal**: None
- **Summary**: In this paper, we show that ImageNet-Pretrained standard deep CNN models can be used as strong baseline networks for audio classification. Even though there is a significant difference between audio Spectrogram and standard ImageNet image samples, transfer learning assumptions still hold firmly. To understand what enables the ImageNet pretrained models to learn useful audio representations, we systematically study how much of pretrained weights is useful for learning spectrograms. We show (1) that for a given standard model using pretrained weights is better than using randomly initialized weights (2) qualitative results of what the CNNs learn from the spectrograms by visualizing the gradients. Besides, we show that even though we use the pretrained model weights for initialization, there is variance in performance in various output runs of the same model. This variance in performance is due to the random initialization of linear classification layer and random mini-batch orderings in multiple runs. This brings significant diversity to build stronger ensemble models with an overall improvement in accuracy. An ensemble of ImageNet pretrained DenseNet achieves 92.89% validation accuracy on the ESC-50 dataset and 87.42% validation accuracy on the UrbanSound8K dataset which is the current state-of-the-art on both of these datasets.



### A Computation-Efficient CNN System for High-Quality Brain Tumor Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2007.12066v3
- **DOI**: 10.1016/j.bspc.2021.103475
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.12066v3)
- **Published**: 2020-07-22 02:45:54+00:00
- **Updated**: 2021-08-13 21:33:04+00:00
- **Authors**: Yanming Sun, Chunyan Wang
- **Comment**: None
- **Journal**: None
- **Summary**: The work presented in this paper is to propose a reliable high-quality system of Convolutional Neural Network (CNN) for brain tumor segmentation with a low computation requirement. The system consists of a CNN for the main processing for the segmentation, a pre-CNN block for data reduction and post-CNN refinement block. The unique CNN consists of 7 convolution layers involving only 108 kernels and 20308 trainable parameters. It is custom-designed, following the proposed paradigm of ASCNN (application specific CNN), to perform mono-modality and cross-modality feature extraction, tumor localization and pixel classification. Each layer fits the task assigned to it, by means of (i) appropriate normalization applied to its input data, (ii) correct convolution modes for the assigned task, and (iii) suitable nonlinear transformation to optimize the convolution results. In this specific design context, the number of kernels in each of the 7 layers is made to be just-sufficient for its task, instead of exponentially growing over the layers, to increase information density and to reduce randomness in the processing. The proposed activation function Full-ReLU helps to halve the number of kernels in convolution layers of high-pass filtering without degrading processing quality. A large number of experiments with BRATS2018 dataset have been conducted to measure the processing quality and reproducibility of the proposed system. The results demonstrate that the system reproduces reliably almost the same output to the same input after retraining. The mean dice scores for enhancing tumor, whole tumor and tumor core are 77.2%, 89.2% and 76.3%, respectively. The simple structure and reliable high processing quality of the proposed system will facilitate its implementation and medical applications.



### MI^2GAN: Generative Adversarial Network for Medical Image Domain Adaptation using Mutual Information Constraint
- **Arxiv ID**: http://arxiv.org/abs/2007.11180v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.11180v2)
- **Published**: 2020-07-22 03:19:54+00:00
- **Updated**: 2020-07-30 07:57:03+00:00
- **Authors**: Xinpeng Xie, Jiawei Chen, Yuexiang Li, Linlin Shen, Kai Ma, Yefeng Zheng
- **Comment**: The first two authors contributed equally
- **Journal**: None
- **Summary**: Domain shift between medical images from multicentres is still an open question for the community, which degrades the generalization performance of deep learning models. Generative adversarial network (GAN), which synthesize plausible images, is one of the potential solutions to address the problem. However, the existing GAN-based approaches are prone to fail at preserving image-objects in image-to-image (I2I) translation, which reduces their practicality on domain adaptation tasks. In this paper, we propose a novel GAN (namely MI$^2$GAN) to maintain image-contents during cross-domain I2I translation. Particularly, we disentangle the content features from domain information for both the source and translated images, and then maximize the mutual information between the disentangled content features to preserve the image-objects. The proposed MI$^2$GAN is evaluated on two tasks---polyp segmentation using colonoscopic images and the segmentation of optic disc and cup in fundus images. The experimental results demonstrate that the proposed MI$^2$GAN can not only generate elegant translated images, but also significantly improve the generalization performance of widely used deep learning networks (e.g., U-Net).



### Instance-aware Self-supervised Learning for Nuclei Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2007.11186v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.11186v1)
- **Published**: 2020-07-22 03:37:14+00:00
- **Updated**: 2020-07-22 03:37:14+00:00
- **Authors**: Xinpeng Xie, Jiawei Chen, Yuexiang Li, Linlin Shen, Kai Ma, Yefeng Zheng
- **Comment**: MICCAI 2020; The first two authors contributed equally
- **Journal**: None
- **Summary**: Due to the wide existence and large morphological variances of nuclei, accurate nuclei instance segmentation is still one of the most challenging tasks in computational pathology. The annotating of nuclei instances, requiring experienced pathologists to manually draw the contours, is extremely laborious and expensive, which often results in the deficiency of annotated data. The deep learning based segmentation approaches, which highly rely on the quantity of training data, are difficult to fully demonstrate their capacity in this area. In this paper, we propose a novel self-supervised learning framework to deeply exploit the capacity of widely-used convolutional neural networks (CNNs) on the nuclei instance segmentation task. The proposed approach involves two sub-tasks (i.e., scale-wise triplet learning and count ranking), which enable neural networks to implicitly leverage the prior-knowledge of nuclei size and quantity, and accordingly mine the instance-aware feature representations from the raw data. Experimental results on the publicly available MoNuSeg dataset show that the proposed self-supervised learning approach can remarkably boost the segmentation accuracy of nuclei instance---a new state-of-the-art average Aggregated Jaccard Index (AJI) of 70.63%, is achieved by our self-supervised ResUNet-101. To our best knowledge, this is the first work focusing on the self-supervised learning for instance segmentation.



### Greenhouse Segmentation on High-Resolution Optical Satellite Imagery using Deep Learning Techniques
- **Arxiv ID**: http://arxiv.org/abs/2007.11222v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.11222v1)
- **Published**: 2020-07-22 06:12:57+00:00
- **Updated**: 2020-07-22 06:12:57+00:00
- **Authors**: Orkhan Baghirli, Imran Ibrahimli, Tarlan Mammadzada
- **Comment**: 12 pages, 14 Figures, 3 Tables, uses arxiv.sty
- **Journal**: None
- **Summary**: Greenhouse segmentation has pivotal importance for climate-smart agricultural land-use planning. Deep learning-based approaches provide state-of-the-art performance in natural image segmentation. However, semantic segmentation on high-resolution optical satellite imagery is a challenging task because of the complex environment. In this paper, a sound methodology is proposed for pixel-wise classification on images acquired by the Azersky (SPOT-7) optical satellite. In particular, customized variations of U-Net-like architectures are employed to identify greenhouses. Two models are proposed which uniquely incorporate dilated convolutions and skip connections, and the results are compared to that of the baseline U-Net model. The dataset used consists of pan-sharpened orthorectified Azersky images (red, green, blue,and near infrared channels) with 1.5-meter resolution and annotation masks, collected from 15 regions in Azerbaijan where the greenhouses are densely congested. The images cover the cumulative area of 1008 $km^2$ and annotation masks contain 47559 polygons in total. The $F_1, Kappa, AUC$, and $IOU$ scores are used for performance evaluation. It is observed that the use of the deconvolutional layers alone throughout the expansive path does not yield satisfactory results; therefore, they are either replaced or coupled with bilinear interpolation. All models benefit from the hard example mining (HEM) strategy. It is also reported that the best accuracy of $93.29\%$ ($F_1\,score$) is recorded when the weighted binary cross-entropy loss is coupled with the dice loss. Experimental results showed that both of the proposed models outperformed the baseline U-Net architecture such that the best model proposed scored $4.48\%$ higher in comparison to the baseline architecture.



### Edge-aware Graph Representation Learning and Reasoning for Face Parsing
- **Arxiv ID**: http://arxiv.org/abs/2007.11240v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.11240v1)
- **Published**: 2020-07-22 07:46:34+00:00
- **Updated**: 2020-07-22 07:46:34+00:00
- **Authors**: Gusi Te, Yinglu Liu, Wei Hu, Hailin Shi, Tao Mei
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: Face parsing infers a pixel-wise label to each facial component, which has drawn much attention recently. Previous methods have shown their efficiency in face parsing, which however overlook the correlation among different face regions. The correlation is a critical clue about the facial appearance, pose, expression etc., and should be taken into account for face parsing. To this end, we propose to model and reason the region-wise relations by learning graph representations, and leverage the edge information between regions for optimized abstraction. Specifically, we encode a facial image onto a global graph representation where a collection of pixels ("regions") with similar features are projected to each vertex. Our model learns and reasons over relations between the regions by propagating information across vertices on the graph. Furthermore, we incorporate the edge information to aggregate the pixel-wise features onto vertices, which emphasizes on the features around edges for fine segmentation along edges. The finally learned graph representation is projected back to pixel grids for parsing. Experiments demonstrate that our model outperforms state-of-the-art methods on the widely used Helen dataset, and also exhibits the superior performance on the large-scale CelebAMask-HQ and LaPa dataset. The code is available at https://github.com/tegusi/EAGRNet.



### Learnable Descent Algorithm for Nonsmooth Nonconvex Image Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2007.11245v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.11245v5)
- **Published**: 2020-07-22 07:59:07+00:00
- **Updated**: 2022-09-03 10:55:02+00:00
- **Authors**: Yunmei Chen, Hongcheng Liu, Xiaojing Ye, Qingchao Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a general learning based framework for solving nonsmooth and nonconvex image reconstruction problems. We model the regularization function as the composition of the $l_{2,1}$ norm and a smooth but nonconvex feature mapping parametrized as a deep convolutional neural network. We develop a provably convergent descent-type algorithm to solve the nonsmooth nonconvex minimization problem by leveraging the Nesterov's smoothing technique and the idea of residual learning, and learn the network parameters such that the outputs of the algorithm match the references in training data. Our method is versatile as one can employ various modern network structures into the regularization, and the resulting network inherits the guaranteed convergence of the algorithm. We also show that the proposed network is parameter-efficient and its performance compares favorably to the state-of-the-art methods in a variety of image reconstruction problems in practice.



### Fragments-Expert: A Graphical User Interface MATLAB Toolbox for Classification of File Fragments
- **Arxiv ID**: http://arxiv.org/abs/2007.11246v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.11246v1)
- **Published**: 2020-07-22 08:03:02+00:00
- **Updated**: 2020-07-22 08:03:02+00:00
- **Authors**: Mehdi Teimouri, Zahra Seyedghorban, Fatemeh Amirjani
- **Comment**: 47 Pages, 34 Figures, and 3 Tables
- **Journal**: None
- **Summary**: The classification of file fragments of various file formats is an essential task in various applications such as firewalls, intrusion detection systems, anti-viruses, web content filtering, and digital forensics. However, the community lacks a suitable software tool that can integrate major methods for feature extraction from file fragments and classification among various file formats. In this paper, we present Fragments-Expert that is a graphical user interface MATLAB toolbox for the classification of file fragments. It provides users with 22 categories of features extracted from file fragments. These features can be employed by 7 categories of machine learning algorithms for the task of classification among various file formats.



### DeepCLR: Correspondence-Less Architecture for Deep End-to-End Point Cloud Registration
- **Arxiv ID**: http://arxiv.org/abs/2007.11255v2
- **DOI**: 10.1109/ITSC45102.2020.9294279
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.11255v2)
- **Published**: 2020-07-22 08:20:57+00:00
- **Updated**: 2021-01-13 10:04:51+00:00
- **Authors**: Markus Horn, Nico Engel, Vasileios Belagiannis, Michael Buchholz, Klaus Dietmayer
- **Comment**: 7 pages, 5 figures, 4 tables
- **Journal**: 2020 IEEE 23rd International Conference on Intelligent
  Transportation Systems (ITSC)
- **Summary**: This work addresses the problem of point cloud registration using deep neural networks. We propose an approach to predict the alignment between two point clouds with overlapping data content, but displaced origins. Such point clouds originate, for example, from consecutive measurements of a LiDAR mounted on a moving platform. The main difficulty in deep registration of raw point clouds is the fusion of template and source point cloud. Our proposed architecture applies flow embedding to tackle this problem, which generates features that describe the motion of each template point. These features are then used to predict the alignment in an end-to-end fashion without extracting explicit point correspondences between both input clouds. We rely on the KITTI odometry and ModelNet40 datasets for evaluating our method on various point distributions. Our approach achieves state-of-the-art accuracy and the lowest run-time of the compared methods.



### Improving Monocular Depth Estimation by Leveraging Structural Awareness and Complementary Datasets
- **Arxiv ID**: http://arxiv.org/abs/2007.11256v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.11256v1)
- **Published**: 2020-07-22 08:21:02+00:00
- **Updated**: 2020-07-22 08:21:02+00:00
- **Authors**: Tian Chen, Shijie An, Yuan Zhang, Chongyang Ma, Huayan Wang, Xiaoyan Guo, Wen Zheng
- **Comment**: 14 pages, 8 figures
- **Journal**: None
- **Summary**: Monocular depth estimation plays a crucial role in 3D recognition and understanding. One key limitation of existing approaches lies in their lack of structural information exploitation, which leads to inaccurate spatial layout, discontinuous surface, and ambiguous boundaries. In this paper, we tackle this problem in three aspects. First, to exploit the spatial relationship of visual features, we propose a structure-aware neural network with spatial attention blocks. These blocks guide the network attention to global structures or local details across different feature layers. Second, we introduce a global focal relative loss for uniform point pairs to enhance spatial constraint in the prediction, and explicitly increase the penalty on errors in depth-wise discontinuous regions, which helps preserve the sharpness of estimation results. Finally, based on analysis of failure cases for prior methods, we collect a new Hard Case (HC) Depth dataset of challenging scenes, such as special lighting conditions, dynamic objects, and tilted camera angles. The new dataset is leveraged by an informed learning curriculum that mixes training examples incrementally to handle diverse data distributions. Experimental results show that our method outperforms state-of-the-art approaches by a large margin in terms of both prediction accuracy on NYUDv2 dataset and generalization performance on unseen datasets.



### Unsupervised Deep Representation Learning for Real-Time Tracking
- **Arxiv ID**: http://arxiv.org/abs/2007.11984v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.11984v1)
- **Published**: 2020-07-22 08:23:12+00:00
- **Updated**: 2020-07-22 08:23:12+00:00
- **Authors**: Ning Wang, Wengang Zhou, Yibing Song, Chao Ma, Wei Liu, Houqiang Li
- **Comment**: Journal version of the CVPR2019 paper "Unsupervised Deep Tracking".
  Accepted by IJCV. arXiv admin note: text overlap with arXiv:1904.01828
- **Journal**: None
- **Summary**: The advancement of visual tracking has continuously been brought by deep learning models. Typically, supervised learning is employed to train these models with expensive labeled data. In order to reduce the workload of manual annotations and learn to track arbitrary objects, we propose an unsupervised learning method for visual tracking. The motivation of our unsupervised learning is that a robust tracker should be effective in bidirectional tracking. Specifically, the tracker is able to forward localize a target object in successive frames and backtrace to its initial position in the first frame. Based on such a motivation, in the training process, we measure the consistency between forward and backward trajectories to learn a robust tracker from scratch merely using unlabeled videos. We build our framework on a Siamese correlation filter network, and propose a multi-frame validation scheme and a cost-sensitive loss to facilitate unsupervised learning. Without bells and whistles, the proposed unsupervised tracker achieves the baseline accuracy as classic fully supervised trackers while achieving a real-time speed. Furthermore, our unsupervised framework exhibits a potential in leveraging more unlabeled or weakly labeled data to further improve the tracking accuracy.



### Deep-VFX: Deep Action Recognition Driven VFX for Short Video
- **Arxiv ID**: http://arxiv.org/abs/2007.11257v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.11257v1)
- **Published**: 2020-07-22 08:27:52+00:00
- **Updated**: 2020-07-22 08:27:52+00:00
- **Authors**: Ao Luo, Ning Xie, Zhijia Tao, Feng Jiang
- **Comment**: None
- **Journal**: None
- **Summary**: Human motion is a key function to communicate information. In the application, short-form mobile video is so popular all over the world such as Tik Tok. The users would like to add more VFX so as to pursue creativity and personlity. Many special effects are added on the short video platform. These gives the users more possibility to show off these personality. The common and traditional way is to create the template of VFX. However, in order to synthesis the perfect, the users have to tedious attempt to grasp the timing and rhythm of new templates. It is not easy-to-use especially for the mobile app. This paper aims to change the VFX synthesis by motion driven instead of the traditional template matching. We propose the AI method to improve this VFX synthesis. In detail, in order to add the special effect on the human body. The skeleton extraction is essential in this system. We also propose a novel form of LSTM to find out the user's intention by action recognition. The experiment shows that our system enables to generate VFX for short video more easier and efficient.



### Adversarial Training Reduces Information and Improves Transferability
- **Arxiv ID**: http://arxiv.org/abs/2007.11259v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.11259v4)
- **Published**: 2020-07-22 08:30:16+00:00
- **Updated**: 2020-12-15 22:52:16+00:00
- **Authors**: Matteo Terzi, Alessandro Achille, Marco Maggipinto, Gian Antonio Susto
- **Comment**: None
- **Journal**: None
- **Summary**: Recent results show that features of adversarially trained networks for classification, in addition to being robust, enable desirable properties such as invertibility. The latter property may seem counter-intuitive as it is widely accepted by the community that classification models should only capture the minimal information (features) required for the task. Motivated by this discrepancy, we investigate the dual relationship between Adversarial Training and Information Theory. We show that the Adversarial Training can improve linear transferability to new tasks, from which arises a new trade-off between transferability of representations and accuracy on the source task. We validate our results employing robust networks trained on CIFAR-10, CIFAR-100 and ImageNet on several datasets. Moreover, we show that Adversarial Training reduces Fisher information of representations about the input and of the weights about the task, and we provide a theoretical argument which explains the invertibility of deterministic networks without violating the principle of minimality. Finally, we leverage our theoretical insights to remarkably improve the quality of reconstructed images through inversion.



### Deep Learning Based Segmentation of Various Brain Lesions for Radiosurgery
- **Arxiv ID**: http://arxiv.org/abs/2007.11784v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.11784v1)
- **Published**: 2020-07-22 09:35:04+00:00
- **Updated**: 2020-07-22 09:35:04+00:00
- **Authors**: Siang-Ruei Wu, Hao-Yun Chang, Florence T Su, Heng-Chun Liao, Wanju Tseng, Chun-Chih Liao, Feipei Lai, Feng-Ming Hsu, Furen Xiao
- **Comment**: None
- **Journal**: None
- **Summary**: Semantic segmentation of medical images with deep learning models is rapidly developed. In this study, we benchmarked state-of-the-art deep learning segmentation algorithms on our clinical stereotactic radiosurgery dataset, demonstrating the strengths and weaknesses of these algorithms in a fairly practical scenario. In particular, we compared the model performances with respect to their sampling method, model architecture, and the choice of loss functions, identifying the suitable settings for their applications and shedding light on the possible improvements.



### DeepSVG: A Hierarchical Generative Network for Vector Graphics Animation
- **Arxiv ID**: http://arxiv.org/abs/2007.11301v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.11301v3)
- **Published**: 2020-07-22 09:36:31+00:00
- **Updated**: 2020-10-22 14:31:42+00:00
- **Authors**: Alexandre Carlier, Martin Danelljan, Alexandre Alahi, Radu Timofte
- **Comment**: Accepted to NeurIPS 2020
- **Journal**: None
- **Summary**: Scalable Vector Graphics (SVG) are ubiquitous in modern 2D interfaces due to their ability to scale to different resolutions. However, despite the success of deep learning-based models applied to rasterized images, the problem of vector graphics representation learning and generation remains largely unexplored. In this work, we propose a novel hierarchical generative network, called DeepSVG, for complex SVG icons generation and interpolation. Our architecture effectively disentangles high-level shapes from the low-level commands that encode the shape itself. The network directly predicts a set of shapes in a non-autoregressive fashion. We introduce the task of complex SVG icons generation by releasing a new large-scale dataset along with an open-source library for SVG manipulation. We demonstrate that our network learns to accurately reconstruct diverse vector graphics, and can serve as a powerful animation tool by performing interpolations and other latent space operations. Our code is available at https://github.com/alexandre01/deepsvg.



### Multi-Spectral Facial Biometrics in Access Control
- **Arxiv ID**: http://arxiv.org/abs/2007.11318v1
- **DOI**: 10.1109/CIBIM.2014.7015450
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.11318v1)
- **Published**: 2020-07-22 10:16:05+00:00
- **Updated**: 2020-07-22 10:16:05+00:00
- **Authors**: K. Lai, S. Samoil, S. N. Yanushkevich
- **Comment**: None
- **Journal**: 2014 IEEE Symposium on Computational Intelligence in Biometrics
  and Identity Management (CIBIM), Orlando, FL, 2014, pp. 102-109
- **Summary**: This study demonstrates how facial biometrics, acquired using multi-spectral sensors, such as RGB, depth, and infrared, assist the data accumulation in the process of authorizing users of automated and semi-automated access systems. This data serves the purposes of person authentication, as well as facial temperature estimation. We utilize depth data taken using an inexpensive RGB-D sensor to find the head pose of a subject. This allows the selection of video frames containing a frontal-view head pose for face recognition and face temperature reading. Usage of the frontal-view frames improves the efficiency of face recognition while the corresponding synchronized IR video frames allow for more efficient temperature estimation for facial regions of interest. In addition, this study reports emerging applications of biometrics in biomedical and health care solutions. Including surveys of recent pilot projects, involving new sensors of biometric data and new applications of human physiological and behavioral biometrics. It also shows the new and promising horizons of using biometrics in natural and contactless control interfaces for surgical control, rehabilitation and accessibility.



### Real-Time Instrument Segmentation in Robotic Surgery using Auxiliary Supervised Deep Adversarial Learning
- **Arxiv ID**: http://arxiv.org/abs/2007.11319v2
- **DOI**: 10.1109/LRA.2019.2900854
- **Categories**: **cs.CV**, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.11319v2)
- **Published**: 2020-07-22 10:16:07+00:00
- **Updated**: 2020-09-30 09:07:50+00:00
- **Authors**: Mobarakol Islam, Daniel A. Atputharuban, Ravikiran Ramesh, Hongliang Ren
- **Comment**: Published in IEEE RAL
- **Journal**: None
- **Summary**: Robot-assisted surgery is an emerging technology which has undergone rapid growth with the development of robotics and imaging systems. Innovations in vision, haptics and accurate movements of robot arms have enabled surgeons to perform precise minimally invasive surgeries. Real-time semantic segmentation of the robotic instruments and tissues is a crucial step in robot-assisted surgery. Accurate and efficient segmentation of the surgical scene not only aids in the identification and tracking of instruments but also provided contextual information about the different tissues and instruments being operated with. For this purpose, we have developed a light-weight cascaded convolutional neural network (CNN) to segment the surgical instruments from high-resolution videos obtained from a commercial robotic system. We propose a multi-resolution feature fusion module (MFF) to fuse the feature maps of different dimensions and channels from the auxiliary and main branch. We also introduce a novel way of combining auxiliary loss and adversarial loss to regularize the segmentation model. Auxiliary loss helps the model to learn low-resolution features, and adversarial loss improves the segmentation prediction by learning higher order structural information. The model also consists of a light-weight spatial pyramid pooling (SPP) unit to aggregate rich contextual information in the intermediate stage. We show that our model surpasses existing algorithms for pixel-wise segmentation of surgical instruments in both prediction accuracy and segmentation time of high-resolution videos.



### Multi-Metric Evaluation of Thermal-to-Visual Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2007.11987v1
- **DOI**: 10.1109/EST.2019.8806202
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.11987v1)
- **Published**: 2020-07-22 10:18:34+00:00
- **Updated**: 2020-07-22 10:18:34+00:00
- **Authors**: Kenneth Lai, Svetlana N. Yanushkevich
- **Comment**: None
- **Journal**: 2019 Eighth International Conference on Emerging Security
  Technologies (EST), Colchester, United Kingdom, 2019, pp. 1-6
- **Summary**: In this paper, we aim to address the problem of heterogeneous or cross-spectral face recognition using machine learning to synthesize visual spectrum face from infrared images. The synthesis of visual-band face images allows for more optimal extraction of facial features to be used for face identification and/or verification. We explore the ability to use Generative Adversarial Networks (GANs) for face image synthesis, and examine the performance of these images using pre-trained Convolutional Neural Networks (CNNs). The features extracted using CNNs are applied in face identification and verification. We explore the performance in terms of acceptance rate when using various similarity measures for face verification.



### Risk Assessment in the Face-based Watchlist Screening in e-Border
- **Arxiv ID**: http://arxiv.org/abs/2007.11323v1
- **DOI**: 10.1109/BTAS.2017.8272677
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.11323v1)
- **Published**: 2020-07-22 10:20:22+00:00
- **Updated**: 2020-07-22 10:20:22+00:00
- **Authors**: Kenneth Lai, Svetlana N. Yanushkevich, Vlad Shmerko
- **Comment**: None
- **Journal**: 2017 IEEE International Joint Conference on Biometrics (IJCB),
  Denver, CO, 2017, pp. 16-21
- **Summary**: This paper concerns with facial-based watchlist technology as a component of automated border control machines deployed in e-borders. The key task of the watchlist technology is to mitigate effects of mis-identification and impersonation. To address this problem, we developed a novel cost-based model of traveler risk assessment and proved its efficiency via intensive experiments using large-scale facial databases. The results of this study are applicable to any biometric modality to be used in watchlist technology.



### Dog Identification using Soft Biometrics and Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2007.11986v1
- **DOI**: 10.1109/IJCNN.2019.8851971
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.11986v1)
- **Published**: 2020-07-22 10:22:46+00:00
- **Updated**: 2020-07-22 10:22:46+00:00
- **Authors**: Kenneth Lai, Xinyuan Tu, Svetlana Yanushkevich
- **Comment**: None
- **Journal**: 2019 International Joint Conference on Neural Networks (IJCNN),
  Budapest, Hungary, 2019, pp. 1-8
- **Summary**: This paper addresses the problem of biometric identification of animals, specifically dogs. We apply advanced machine learning models such as deep neural network on the photographs of pets in order to determine the pet identity. In this paper, we explore the possibility of using different types of "soft" biometrics, such as breed, height, or gender, in fusion with "hard" biometrics such as photographs of the pet's face. We apply the principle of transfer learning on different Convolutional Neural Networks, in order to create a network designed specifically for breed classification. The proposed network is able to achieve an accuracy of 90.80% and 91.29% when differentiating between the two dog breeds, for two different datasets. Without the use of "soft" biometrics, the identification rate of dogs is 78.09% but by using a decision network to incorporate "soft" biometrics, the identification rate can achieve an accuracy of 84.94%.



### CNN+RNN Depth and Skeleton based Dynamic Hand Gesture Recognition
- **Arxiv ID**: http://arxiv.org/abs/2007.11983v1
- **DOI**: 10.1109/ICPR.2018.8545718
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.11983v1)
- **Published**: 2020-07-22 10:25:19+00:00
- **Updated**: 2020-07-22 10:25:19+00:00
- **Authors**: Kenneth Lai, Svetlana N. Yanushkevich
- **Comment**: None
- **Journal**: 2018 24th International Conference on Pattern Recognition (ICPR),
  Beijing, 2018, pp. 3451-3456
- **Summary**: Human activity and gesture recognition is an important component of rapidly growing domain of ambient intelligence, in particular in assisting living and smart homes. In this paper, we propose to combine the power of two deep learning techniques, the convolutional neural networks (CNN) and the recurrent neural networks (RNN), for automated hand gesture recognition using both depth and skeleton data. Each of these types of data can be used separately to train neural networks to recognize hand gestures. While RNN were reported previously to perform well in recognition of sequences of movement for each skeleton joint given the skeleton information only, this study aims at utilizing depth data and apply CNN to extract important spatial information from the depth images. Together, the tandem CNN+RNN is capable of recognizing a sequence of gestures more accurately. As well, various types of fusion are studied to combine both the skeleton and depth information in order to extract temporal-spatial information. An overall accuracy of 85.46% is achieved on the dynamic hand gesture-14/28 dataset.



### Watchlist Risk Assessment using Multiparametric Cost and Relative Entropy
- **Arxiv ID**: http://arxiv.org/abs/2007.11328v1
- **DOI**: 10.1109/SSCI.2017.8285219
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.11328v1)
- **Published**: 2020-07-22 10:27:53+00:00
- **Updated**: 2020-07-22 10:27:53+00:00
- **Authors**: K. Lai, S. N. Yanushkevich
- **Comment**: None
- **Journal**: 2017 IEEE Symposium Series on Computational Intelligence (SSCI),
  Honolulu, HI, 2017, pp. 1-7
- **Summary**: This paper addresses the facial biometric-enabled watchlist technology in which risk detectors are mandatory mechanisms for early detection of threats, as well as for avoiding offense to innocent travelers. We propose a multiparametric cost assessment and relative entropy measures as risk detectors. We experimentally demonstrate the effects of mis-identification and impersonation under various watchlist screening scenarios and constraints. The key contributions of this paper are the novel techniques for design and analysis of the biometric-enabled watchlist and the supporting infrastructure, as well as measuring the impersonation impact on e-border performance.



### Multi-Task Curriculum Framework for Open-Set Semi-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2007.11330v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.11330v1)
- **Published**: 2020-07-22 10:33:55+00:00
- **Updated**: 2020-07-22 10:33:55+00:00
- **Authors**: Qing Yu, Daiki Ikami, Go Irie, Kiyoharu Aizawa
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: Semi-supervised learning (SSL) has been proposed to leverage unlabeled data for training powerful models when only limited labeled data is available. While existing SSL methods assume that samples in the labeled and unlabeled data share the classes of their samples, we address a more complex novel scenario named open-set SSL, where out-of-distribution (OOD) samples are contained in unlabeled data. Instead of training an OOD detector and SSL separately, we propose a multi-task curriculum learning framework. First, to detect the OOD samples in unlabeled data, we estimate the probability of the sample belonging to OOD. We use a joint optimization framework, which updates the network parameters and the OOD score alternately. Simultaneously, to achieve high performance on the classification of in-distribution (ID) data, we select ID samples in unlabeled data having small OOD scores, and use these data with labeled data for training the deep neural networks to classify ID samples in a semi-supervised manner. We conduct several experiments, and our method achieves state-of-the-art results by successfully eliminating the effect of OOD samples.



### Unsupervised Shape and Pose Disentanglement for 3D Meshes
- **Arxiv ID**: http://arxiv.org/abs/2007.11341v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.11341v1)
- **Published**: 2020-07-22 11:00:27+00:00
- **Updated**: 2020-07-22 11:00:27+00:00
- **Authors**: Keyang Zhou, Bharat Lal Bhatnagar, Gerard Pons-Moll
- **Comment**: None
- **Journal**: None
- **Summary**: Parametric models of humans, faces, hands and animals have been widely used for a range of tasks such as image-based reconstruction, shape correspondence estimation, and animation. Their key strength is the ability to factor surface variations into shape and pose dependent components. Learning such models requires lots of expert knowledge and hand-defined object-specific constraints, making the learning approach unscalable to novel objects. In this paper, we present a simple yet effective approach to learn disentangled shape and pose representations in an unsupervised setting. We use a combination of self-consistency and cross-consistency constraints to learn pose and shape space from registered meshes. We additionally incorporate as-rigid-as-possible deformation(ARAP) into the training loop to avoid degenerate solutions. We demonstrate the usefulness of learned representations through a number of tasks including pose transfer and shape retrieval. The experiments on datasets of 3D humans, faces, hands and animals demonstrate the generality of our approach. Code is made available at https://virtualhumans.mpi-inf.mpg.de/unsup_shape_pose/.



### DEAL: Deep Evidential Active Learning for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2007.11344v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.11344v2)
- **Published**: 2020-07-22 11:14:23+00:00
- **Updated**: 2020-10-27 07:35:51+00:00
- **Authors**: Patrick Hemmer, Niklas K√ºhl, Jakob Sch√∂ffer
- **Comment**: Extended version of the paper "DEAL: Deep Evidential Active Learning
  for Image Classification" accepted for publication at ICMLA 2020
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) have proven to be state-of-the-art models for supervised computer vision tasks, such as image classification. However, large labeled data sets are generally needed for the training and validation of such models. In many domains, unlabeled data is available but labeling is expensive, for instance when specific expert knowledge is required. Active Learning (AL) is one approach to mitigate the problem of limited labeled data. Through selecting the most informative and representative data instances for labeling, AL can contribute to more efficient learning of the model. Recent AL methods for CNNs propose different solutions for the selection of instances to be labeled. However, they do not perform consistently well and are often computationally expensive. In this paper, we propose a novel AL algorithm that efficiently learns from unlabeled data by capturing high prediction uncertainty. By replacing the softmax standard output of a CNN with the parameters of a Dirichlet density, the model learns to identify data instances that contribute efficiently to improving model performance during training. We demonstrate in several experiments with publicly available data that our method consistently outperforms other state-of-the-art AL approaches. It can be easily implemented and does not require extensive computational resources for training. Additionally, we are able to show the benefits of the approach on a real-world medical use case in the field of automated detection of visual signals for pneumonia on chest radiographs.



### Learning Directional Feature Maps for Cardiac MRI Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2007.11349v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.11349v1)
- **Published**: 2020-07-22 11:31:04+00:00
- **Updated**: 2020-07-22 11:31:04+00:00
- **Authors**: Feng Cheng, Cheng Chen, Yukang Wang, Heshui Shi, Yukun Cao, Dandan Tu, Changzheng Zhang, Yongchao Xu
- **Comment**: Accepted by MICCAI2020
- **Journal**: None
- **Summary**: Cardiac MRI segmentation plays a crucial role in clinical diagnosis for evaluating personalized cardiac performance parameters. Due to the indistinct boundaries and heterogeneous intensity distributions in the cardiac MRI, most existing methods still suffer from two aspects of challenges: inter-class indistinction and intra-class inconsistency. To tackle these two problems, we propose a novel method to exploit the directional feature maps, which can simultaneously strengthen the differences between classes and the similarities within classes. Specifically, we perform cardiac segmentation and learn a direction field pointing away from the nearest cardiac tissue boundary to each pixel via a direction field (DF) module. Based on the learned direction field, we then propose a feature rectification and fusion (FRF) module to improve the original segmentation features, and obtain the final segmentation. The proposed modules are simple yet effective and can be flexibly added to any existing segmentation network without excessively increasing time and space complexity. We evaluate the proposed method on the 2017 MICCAI Automated Cardiac Diagnosis Challenge (ACDC) dataset and a large-scale self-collected dataset, showing good segmentation performance and robust generalization ability of the proposed method.



### Leveraging Undiagnosed Data for Glaucoma Classification with Teacher-Student Learning
- **Arxiv ID**: http://arxiv.org/abs/2007.11355v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.11355v1)
- **Published**: 2020-07-22 12:05:26+00:00
- **Updated**: 2020-07-22 12:05:26+00:00
- **Authors**: Junde Wu, Shuang Yu, Wenting Chen, Kai Ma, Rao Fu, Hanruo Liu, Xiaoguang Di, Yefeng Zheng
- **Comment**: None
- **Journal**: MICCAI 2020
- **Summary**: Recently, deep learning has been adopted to the glaucoma classification task with performance comparable to that of human experts. However, a well trained deep learning model demands a large quantity of properly labeled data, which is relatively expensive since the accurate labeling of glaucoma requires years of specialist training. In order to alleviate this problem, we propose a glaucoma classification framework which takes advantage of not only the properly labeled images, but also undiagnosed images without glaucoma labels. To be more specific, the proposed framework is adapted from the teacher-student-learning paradigm. The teacher model encodes the wrapped information of undiagnosed images to a latent feature space, meanwhile the student model learns from the teacher through knowledge transfer to improve the glaucoma classification. For the model training procedure, we propose a novel training strategy that simulates the real-world teaching practice named as 'Learning To Teach with Knowledge Transfer (L2T-KT)', and establish a 'Quiz Pool' as the teacher's optimization target. Experiments show that the proposed framework is able to utilize the undiagnosed data effectively to improve the glaucoma prediction performance.



### Human-Centered Unsupervised Segmentation Fusion
- **Arxiv ID**: http://arxiv.org/abs/2007.11361v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.11361v1)
- **Published**: 2020-07-22 12:18:31+00:00
- **Updated**: 2020-07-22 12:18:31+00:00
- **Authors**: Gregor Koporec, Janez Per≈°
- **Comment**: Accepted to the IROS2019 Workshop: Benchmark and Dataset for
  Probabilistic Prediction of Interactive Human Behavior, 5 pages
- **Journal**: None
- **Summary**: Segmentation is generally an ill-posed problem since it results in multiple solutions and is, therefore, hard to define ground truth data to evaluate algorithms. The problem can be naively surpassed by using only one annotator per image, but such acquisition doesn't represent the cognitive perception of an image by the majority of people. Nowadays, it is not difficult to obtain multiple segmentations with crowdsourcing, so the only problem that stays is how to get one ground truth segmentation per image. There already exist numerous algorithmic solutions, but most methods are supervised or don't consider confidence per human segmentation. In this paper, we introduce a new segmentation fusion model that is based on K-Modes clustering. Results obtained from publicly available datasets with human ground truth segmentations clearly show that our model outperforms the state-of-the-art on human segmentations.



### Depthwise Spatio-Temporal STFT Convolutional Neural Networks for Human Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2007.11365v1
- **DOI**: 10.1109/TPAMI.2021.3076522
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.11365v1)
- **Published**: 2020-07-22 12:26:04+00:00
- **Updated**: 2020-07-22 12:26:04+00:00
- **Authors**: Sudhakar Kumawat, Manisha Verma, Yuta Nakashima, Shanmuganathan Raman
- **Comment**: Extended version of our CVPR 2019 work
- **Journal**: None
- **Summary**: Conventional 3D convolutional neural networks (CNNs) are computationally expensive, memory intensive, prone to overfitting, and most importantly, there is a need to improve their feature learning capabilities. To address these issues, we propose spatio-temporal short term Fourier transform (STFT) blocks, a new class of convolutional blocks that can serve as an alternative to the 3D convolutional layer and its variants in 3D CNNs. An STFT block consists of non-trainable convolution layers that capture spatially and/or temporally local Fourier information using a STFT kernel at multiple low frequency points, followed by a set of trainable linear weights for learning channel correlations. The STFT blocks significantly reduce the space-time complexity in 3D CNNs. In general, they use 3.5 to 4.5 times less parameters and 1.5 to 1.8 times less computational costs when compared to the state-of-the-art methods. Furthermore, their feature learning capabilities are significantly better than the conventional 3D convolutional layer and its variants. Our extensive evaluation on seven action recognition datasets, including Something-something v1 and v2, Jester, Diving-48, Kinetics-400, UCF 101, and HMDB 51, demonstrate that STFT blocks based 3D CNNs achieve on par or even better performance compared to the state-of-the-art methods.



### Feature based Sequential Classifier with Attention Mechanism
- **Arxiv ID**: http://arxiv.org/abs/2007.11392v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.11392v1)
- **Published**: 2020-07-22 12:54:30+00:00
- **Updated**: 2020-07-22 12:54:30+00:00
- **Authors**: Sudhir Sornapudi, R. Joe Stanley, William V. Stoecker, Rodney Long, Zhiyun Xue, Rosemary Zuna, Shelliane R. Frazier, Sameer Antani
- **Comment**: None
- **Journal**: None
- **Summary**: Cervical cancer is one of the deadliest cancers affecting women globally. Cervical intraepithelial neoplasia (CIN) assessment using histopathological examination of cervical biopsy slides is subject to interobserver variability. Automated processing of digitized histopathology slides has the potential for more accurate classification for CIN grades from normal to increasing grades of pre-malignancy: CIN1, CIN2 and CIN3. Cervix disease is generally understood to progress from the bottom (basement membrane) to the top of the epithelium. To model this relationship of disease severity to spatial distribution of abnormalities, we propose a network pipeline, DeepCIN, to analyze high-resolution epithelium images (manually extracted from whole-slide images) hierarchically by focusing on localized vertical regions and fusing this local information for determining Normal/CIN classification. The pipeline contains two classifier networks: 1) a cross-sectional, vertical segment-level sequence generator (two-stage encoder model) is trained using weak supervision to generate feature sequences from the vertical segments to preserve the bottom-to-top feature relationships in the epithelium image data; 2) an attention-based fusion network image-level classifier predicting the final CIN grade by merging vertical segment sequences. The model produces the CIN classification results and also determines the vertical segment contributions to CIN grade prediction. Experiments show that DeepCIN achieves pathologist-level CIN classification accuracy.



### Learning to predict metal deformations in hot-rolling processes
- **Arxiv ID**: http://arxiv.org/abs/2007.14471v1
- **DOI**: None
- **Categories**: **cs.CE**, cs.CV, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2007.14471v1)
- **Published**: 2020-07-22 13:33:44+00:00
- **Updated**: 2020-07-22 13:33:44+00:00
- **Authors**: R. Omar Chavez-Garcia, Emian Furger, Samuele Kronauer, Christian Brianza, Marco Scarf√≤, Luca Diviani, Alessandro Giusti
- **Comment**: Accepted for publication in the IEEE Robotics & Automation Letters
  (2020)
- **Journal**: None
- **Summary**: Hot-rolling is a metal forming process that produces a workpiece with a desired target cross-section from an input workpiece through a sequence of plastic deformations; each deformation is generated by a stand composed of opposing rolls with a specific geometry. In current practice, the rolling sequence (i.e., the sequence of stands and the geometry of their rolls) needed to achieve a given final cross-section is designed by experts based on previous experience, and iteratively refined in a costly trial-and-error process. Finite Element Method simulations are increasingly adopted to make this process more efficient and to test potential rolling sequences, achieving good accuracy at the cost of long simulation times, limiting the practical use of the approach. We propose a supervised learning approach to predict the deformation of a given workpiece by a set of rolls with a given geometry; the model is trained on a large dataset of procedurally-generated FEM simulations, which we publish as supplementary material. The resulting predictor is four orders of magnitude faster than simulations, and yields an average Jaccard Similarity Index of 0.972 (against ground truth from simulations) and 0.925 (against real-world measured deformations); we additionally report preliminary results on using the predictor for automatic planning of rolling sequences.



### Learning Disentangled Feature Representation for Hybrid-distorted Image Restoration
- **Arxiv ID**: http://arxiv.org/abs/2007.11430v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.11430v1)
- **Published**: 2020-07-22 13:43:40+00:00
- **Updated**: 2020-07-22 13:43:40+00:00
- **Authors**: Xin Li, Xin Jin, Jianxin Lin, Tao Yu, Sen Liu, Yaojun Wu, Wei Zhou, Zhibo Chen
- **Comment**: Accepted by ECCV2020
- **Journal**: None
- **Summary**: Hybrid-distorted image restoration (HD-IR) is dedicated to restore real distorted image that is degraded by multiple distortions. Existing HD-IR approaches usually ignore the inherent interference among hybrid distortions which compromises the restoration performance. To decompose such interference, we introduce the concept of Disentangled Feature Learning to achieve the feature-level divide-and-conquer of hybrid distortions. Specifically, we propose the feature disentanglement module (FDM) to distribute feature representations of different distortions into different channels by revising gain-control-based normalization. We also propose a feature aggregation module (FAM) with channel-wise attention to adaptively filter out the distortion representations and aggregate useful content information from different channels for the construction of raw image. The effectiveness of the proposed scheme is verified by visualizing the correlation matrix of features and channel responses of different distortions. Extensive experimental results also prove superior performance of our approach compared with the latest HD-IR schemes.



### Combining Implicit Function Learning and Parametric Models for 3D Human Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2007.11432v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.11432v2)
- **Published**: 2020-07-22 13:46:14+00:00
- **Updated**: 2021-11-26 05:28:05+00:00
- **Authors**: Bharat Lal Bhatnagar, Cristian Sminchisescu, Christian Theobalt, Gerard Pons-Moll
- **Comment**: Accepted at ECCV'20 (Oral)
- **Journal**: None
- **Summary**: Implicit functions represented as deep learning approximations are powerful for reconstructing 3D surfaces. However, they can only produce static surfaces that are not controllable, which provides limited ability to modify the resulting model by editing its pose or shape parameters. Nevertheless, such features are essential in building flexible models for both computer graphics and computer vision. In this work, we present methodology that combines detail-rich implicit functions and parametric representations in order to reconstruct 3D models of people that remain controllable and accurate even in the presence of clothing. Given sparse 3D point clouds sampled on the surface of a dressed person, we use an Implicit Part Network (IP-Net)to jointly predict the outer 3D surface of the dressed person, the and inner body surface, and the semantic correspondences to a parametric body model. We subsequently use correspondences to fit the body model to our inner surface and then non-rigidly deform it (under a parametric body + displacement model) to the outer surface in order to capture garment, face and hair detail. In quantitative and qualitative experiments with both full body data and hand scans we show that the proposed methodology generalizes, and is effective even given incomplete point clouds collected from single-view depth images. Our models and code can be downloaded from http://virtualhumans.mpi-inf.mpg.de/ipnet.



### Learning One Class Representations for Face Presentation Attack Detection using Multi-channel Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2007.11457v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.11457v1)
- **Published**: 2020-07-22 14:19:33+00:00
- **Updated**: 2020-07-22 14:19:33+00:00
- **Authors**: Anjith George, Sebastien Marcel
- **Comment**: 15 pages
- **Journal**: None
- **Summary**: Face recognition has evolved as a widely used biometric modality. However, its vulnerability against presentation attacks poses a significant security threat. Though presentation attack detection (PAD) methods try to address this issue, they often fail in generalizing to unseen attacks. In this work, we propose a new framework for PAD using a one-class classifier, where the representation used is learned with a Multi-Channel Convolutional Neural Network (MCCNN). A novel loss function is introduced, which forces the network to learn a compact embedding for bonafide class while being far from the representation of attacks. A one-class Gaussian Mixture Model is used on top of these embeddings for the PAD task. The proposed framework introduces a novel approach to learn a robust PAD system from bonafide and available (known) attack classes. This is particularly important as collecting bonafide data and simpler attacks are much easier than collecting a wide variety of expensive attacks. The proposed system is evaluated on the publicly available WMCA multi-channel face PAD database, which contains a wide variety of 2D and 3D attacks. Further, we have performed experiments with MLFP and SiW-M datasets using RGB channels only. Superior performance in unseen attack protocols shows the effectiveness of the proposed approach. Software, data, and protocols to reproduce the results are made available publicly.



### Perceptron Synthesis Network: Rethinking the Action Scale Variances in Videos
- **Arxiv ID**: http://arxiv.org/abs/2007.11460v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.11460v3)
- **Published**: 2020-07-22 14:22:29+00:00
- **Updated**: 2022-04-19 13:32:32+00:00
- **Authors**: Yuan Tian, Guangtao Zhai, Zhiyong Gao
- **Comment**: None
- **Journal**: None
- **Summary**: Video action recognition has been partially addressed by the CNNs stacking of fixed-size 3D kernels. However, these methods may under-perform for only capturing rigid spatial-temporal patterns in single-scale spaces, while neglecting the scale variances across different action primitives. To overcome this limitation, we propose to learn the optimal-scale kernels from the data. More specifically, an \textit{action perceptron synthesizer} is proposed to generate the kernels from a bag of fixed-size kernels that are interacted by dense routing paths. To guarantee the interaction richness and the information capacity of the paths, we design the novel \textit{optimized feature fusion layer}. This layer establishes a principled universal paradigm that suffices to cover most of the current feature fusion techniques (e.g., channel shuffling, and channel dropout) for the first time. By inserting the \textit{synthesizer}, our method can easily adapt the traditional 2D CNNs to the video understanding tasks such as action recognition with marginal additional computation cost. The proposed method is thoroughly evaluated over several challenging datasets (i.e., Somehting-to-Somthing, Kinetics and Diving48) that highly require temporal reasoning or appearance discriminating, achieving new state-of-the-art results. Particularly, our low-resolution model outperforms the recent strong baseline methods, i.e., TSM and GST, with less than 30\% of their computation cost.



### FedOCR: Communication-Efficient Federated Learning for Scene Text Recognition
- **Arxiv ID**: http://arxiv.org/abs/2007.11462v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.11462v2)
- **Published**: 2020-07-22 14:30:50+00:00
- **Updated**: 2022-02-07 15:44:15+00:00
- **Authors**: Wenqing Zhang, Yang Qiu, Song Bai, Rui Zhang, Xiaolin Wei, Xiang Bai
- **Comment**: None
- **Journal**: None
- **Summary**: While scene text recognition techniques have been widely used in commercial applications, data privacy has rarely been taken into account by this research community. Most existing algorithms have assumed a set of shared or centralized training data. However, in practice, data may be distributed on different local devices that can not be centralized to share due to the privacy restrictions. In this paper, we study how to make use of decentralized datasets for training a robust scene text recognizer while keeping them stay on local devices. To the best of our knowledge, we propose the first framework leveraging federated learning for scene text recognition, which is trained with decentralized datasets collaboratively. Hence we name it FedOCR. To make FedCOR fairly suitable to be deployed on end devices, we make two improvements including using lightweight models and hashing techniques. We argue that both are crucial for FedOCR in terms of the communication efficiency of federated learning. The simulations on decentralized datasets show that the proposed FedOCR achieves competitive results to the models that are trained with centralized data, with fewer communication costs and higher-level privacy-preserving.



### Wasserstein Routed Capsule Networks
- **Arxiv ID**: http://arxiv.org/abs/2007.11465v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML, I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2007.11465v1)
- **Published**: 2020-07-22 14:38:05+00:00
- **Updated**: 2020-07-22 14:38:05+00:00
- **Authors**: Alexander Fuchs, Franz Pernkopf
- **Comment**: 8 pages, 3 figures
- **Journal**: None
- **Summary**: Capsule networks offer interesting properties and provide an alternative to today's deep neural network architectures. However, recent approaches have failed to consistently achieve competitive results across different image datasets. We propose a new parameter efficient capsule architecture, that is able to tackle complex tasks by using neural networks trained with an approximate Wasserstein objective to dynamically select capsules throughout the entire architecture. This approach focuses on implementing a robust routing scheme, which can deliver improved results using little overhead. We perform several ablation studies verifying the proposed concepts and show that our network is able to substantially outperform other capsule approaches by over 1.2 % on CIFAR-10, using fewer parameters.



### Deep Models and Shortwave Infrared Information to Detect Face Presentation Attacks
- **Arxiv ID**: http://arxiv.org/abs/2007.11469v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.11469v1)
- **Published**: 2020-07-22 14:41:14+00:00
- **Updated**: 2020-07-22 14:41:14+00:00
- **Authors**: Guillaume Heusch, Anjith George, David Geissbuhler, Zohreh Mostaani, Sebastien Marcel
- **Comment**: None
- **Journal**: None
- **Summary**: This paper addresses the problem of face presentation attack detection using different image modalities. In particular, the usage of short wave infrared (SWIR) imaging is considered. Face presentation attack detection is performed using recent models based on Convolutional Neural Networks using only carefully selected SWIR image differences as input. Conducted experiments show superior performance over similar models acting on either color images or on a combination of different modalities (visible, NIR, thermal and depth), as well as on a SVM-based classifier acting on SWIR image differences. Experiments have been carried on a new public and freely available database, containing a wide variety of attacks. Video sequences have been recorded thanks to several sensors resulting in 14 different streams in the visible, NIR, SWIR and thermal spectra, as well as depth data. The best proposed approach is able to almost perfectly detect all impersonation attacks while ensuring low bonafide classification errors. On the other hand, obtained results show that obfuscation attacks are more difficult to detect. We hope that the proposed database will foster research on this challenging problem. Finally, all the code and instructions to reproduce presented experiments is made available to the research community.



### A Novel adaptive optimization of Dual-Tree Complex Wavelet Transform for Medical Image Fusion
- **Arxiv ID**: http://arxiv.org/abs/2007.13538v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.13538v1)
- **Published**: 2020-07-22 15:34:01+00:00
- **Updated**: 2020-07-22 15:34:01+00:00
- **Authors**: T. Deepika, G. Karpaga Kannan
- **Comment**: Conference on Computing Communication and Signal Processing. arXiv
  admin note: text overlap with arXiv:2007.11488
- **Journal**: None
- **Summary**: In recent years, many research achievements are made in the medical image fusion field. Fusion is basically extraction of best of inputs and conveying it to the output. Medical Image fusion means that several of various modality image information is comprehended together to form one image to express its information. The aim of image fusion is to integrate complementary and redundant information. In this paper, a multimodal image fusion algorithm based on the dual-tree complex wavelet transform (DT-CWT) and adaptive particle swarm optimization (APSO) is proposed. Fusion is achieved through the formation of a fused pyramid using the DTCWT coefficients from the decomposed pyramids of the source images. The coefficients are fused by the weighted average method based on pixels, and the weights are estimated by the APSO to gain optimal fused images. The fused image is obtained through conventional inverse dual-tree complex wavelet transform reconstruction process. Experiment results show that the proposed method based on adaptive particle swarm optimization algorithm is remarkably better than the method based on particle swarm optimization. The resulting fused images are compared visually and through benchmarks such as Entropy (E), Peak Signal to Noise Ratio, (PSNR), Root Mean Square Error (RMSE), Standard deviation (SD) and Structure Similarity Index Metric (SSIM) computations.



### CrossTransformers: spatially-aware few-shot transfer
- **Arxiv ID**: http://arxiv.org/abs/2007.11498v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.11498v5)
- **Published**: 2020-07-22 15:37:08+00:00
- **Updated**: 2021-02-17 18:05:48+00:00
- **Authors**: Carl Doersch, Ankush Gupta, Andrew Zisserman
- **Comment**: Published at NeurIPS 2020. Code/checkpoints:
  https://github.com/google-research/meta-dataset
- **Journal**: None
- **Summary**: Given new tasks with very little data$-$such as new classes in a classification problem or a domain shift in the input$-$performance of modern vision systems degrades remarkably quickly. In this work, we illustrate how the neural network representations which underpin modern vision systems are subject to supervision collapse, whereby they lose any information that is not necessary for performing the training task, including information that may be necessary for transfer to new tasks or domains. We then propose two methods to mitigate this problem. First, we employ self-supervised learning to encourage general-purpose features that transfer better. Second, we propose a novel Transformer based neural network architecture called CrossTransformers, which can take a small number of labeled images and an unlabeled query, find coarse spatial correspondence between the query and the labeled images, and then infer class membership by computing distances between spatially-corresponding features. The result is a classifier that is more robust to task and domain shift, which we demonstrate via state-of-the-art performance on Meta-Dataset, a recent dataset for evaluating transfer from ImageNet to many other vision datasets.



### A Novel Spatial-Spectral Framework for the Classification of Hyperspectral Satellite Imagery
- **Arxiv ID**: http://arxiv.org/abs/2008.02797v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.02797v1)
- **Published**: 2020-07-22 16:12:08+00:00
- **Updated**: 2020-07-22 16:12:08+00:00
- **Authors**: Shriya TP Gupta, Sanjay K Sahay
- **Comment**: 13 Pages, 15 Figures, EANN-2020
- **Journal**: Springer, INNS, Vol. 2, pp 227-239, 2020
- **Summary**: Hyper-spectral satellite imagery is now widely being used for accurate disaster prediction and terrain feature classification. However, in such classification tasks, most of the present approaches use only the spectral information contained in the images. Therefore, in this paper, we present a novel framework that takes into account both the spectral and spatial information contained in the data for land cover classification. For this purpose, we use the Gaussian Maximum Likelihood (GML) and Convolutional Neural Network methods for the pixel-wise spectral classification and then, using segmentation maps generated by the Watershed algorithm, we incorporate the spatial contextual information into our model with a modified majority vote technique. The experimental analyses on two benchmark datasets demonstrate that our proposed methodology performs better than the earlier approaches by achieving an accuracy of 99.52% and 98.31% on the Pavia University and the Indian Pines datasets respectively. Additionally, our GML based approach, a non-deep learning algorithm, shows comparable performance to the state-of-the-art deep learning techniques, which indicates the importance of the proposed approach for performing a computationally efficient classification of hyper-spectral imagery.



### Endo-Sim2Real: Consistency learning-based domain adaptation for instrument segmentation
- **Arxiv ID**: http://arxiv.org/abs/2007.11514v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.11514v1)
- **Published**: 2020-07-22 16:18:11+00:00
- **Updated**: 2020-07-22 16:18:11+00:00
- **Authors**: Manish Sahu, Ronja Str√∂msd√∂rfer, Anirban Mukhopadhyay, Stefan Zachow
- **Comment**: Accepted at MICCAI2020
- **Journal**: None
- **Summary**: Surgical tool segmentation in endoscopic videos is an important component of computer assisted interventions systems. Recent success of image-based solutions using fully-supervised deep learning approaches can be attributed to the collection of big labeled datasets. However, the annotation of a big dataset of real videos can be prohibitively expensive and time consuming. Computer simulations could alleviate the manual labeling problem, however, models trained on simulated data do not generalize to real data. This work proposes a consistency-based framework for joint learning of simulated and real (unlabeled) endoscopic data to bridge this performance generalization issue. Empirical results on two data sets (15 videos of the Cholec80 and EndoVis'15 dataset) highlight the effectiveness of the proposed \emph{Endo-Sim2Real} method for instrument segmentation. We compare the segmentation of the proposed approach with state-of-the-art solutions and show that our method improves segmentation both in terms of quality and quantity.



### Attend and Segment: Attention Guided Active Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2007.11548v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.11548v1)
- **Published**: 2020-07-22 17:09:13+00:00
- **Updated**: 2020-07-22 17:09:13+00:00
- **Authors**: Soroush Seifi, Tinne Tuytelaars
- **Comment**: None
- **Journal**: None
- **Summary**: In a dynamic environment, an agent with a limited field of view/resource cannot fully observe the scene before attempting to parse it. The deployment of common semantic segmentation architectures is not feasible in such settings. In this paper we propose a method to gradually segment a scene given a sequence of partial observations. The main idea is to refine an agent's understanding of the environment by attending the areas it is most uncertain about. Our method includes a self-supervised attention mechanism and a specialized architecture to maintain and exploit spatial memory maps for filling-in the unseen areas in the environment. The agent can select and attend an area while relying on the cues coming from the visited areas to hallucinate the other parts. We reach a mean pixel-wise accuracy of 78.1%, 80.9% and 76.5% on CityScapes, CamVid, and Kitti datasets by processing only 18% of the image pixels (10 retina-like glimpses). We perform an ablation study on the number of glimpses, input image size and effectiveness of retina-like glimpses. We compare our method to several baselines and show that the optimal results are achieved by having access to a very low resolution view of the scene at the first timestep.



### Neural Sparse Voxel Fields
- **Arxiv ID**: http://arxiv.org/abs/2007.11571v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.11571v2)
- **Published**: 2020-07-22 17:51:31+00:00
- **Updated**: 2021-01-06 21:04:24+00:00
- **Authors**: Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, Christian Theobalt
- **Comment**: 20 pages, in progress
- **Journal**: None
- **Summary**: Photo-realistic free-viewpoint rendering of real-world scenes using classical computer graphics techniques is challenging, because it requires the difficult step of capturing detailed appearance and geometry models. Recent studies have demonstrated promising results by learning scene representations that implicitly encode both geometry and appearance without 3D supervision. However, existing approaches in practice often show blurry renderings caused by the limited network capacity or the difficulty in finding accurate intersections of camera rays with the scene geometry. Synthesizing high-resolution imagery from these representations often requires time-consuming optical ray marching. In this work, we introduce Neural Sparse Voxel Fields (NSVF), a new neural scene representation for fast and high-quality free-viewpoint rendering. NSVF defines a set of voxel-bounded implicit fields organized in a sparse voxel octree to model local properties in each cell. We progressively learn the underlying voxel structures with a differentiable ray-marching operation from only a set of posed RGB images. With the sparse voxel octree structure, rendering novel views can be accelerated by skipping the voxels containing no relevant scene content. Our method is typically over 10 times faster than the state-of-the-art (namely, NeRF(Mildenhall et al., 2020)) at inference time while achieving higher quality results. Furthermore, by utilizing an explicit sparse voxel representation, our method can easily be applied to scene editing and scene composition. We also demonstrate several challenging tasks, including multi-scene learning, free-viewpoint rendering of a moving human, and large-scale scene rendering. Code and data are available at our website: https://github.com/facebookresearch/NSVF.



### Deep Variational Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2007.11576v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.11576v2)
- **Published**: 2020-07-22 17:57:49+00:00
- **Updated**: 2020-10-24 06:17:44+00:00
- **Authors**: Jialin Yuan, Chao Chen, Li Fuxin
- **Comment**: The work has been accepted by NeurIPS 2020
- **Journal**: None
- **Summary**: Instance Segmentation, which seeks to obtain both class and instance labels for each pixel in the input image, is a challenging task in computer vision. State-of-the-art algorithms often employ two separate stages, the first one generating object proposals and the second one recognizing and refining the boundaries. Further, proposals are usually based on detectors such as faster R-CNN which search for boxes in the entire image exhaustively. In this paper, we propose a novel algorithm that directly utilizes a fully convolutional network (FCN) to predict instance labels. Specifically, we propose a variational relaxation of instance segmentation as minimizing an optimization functional for a piecewise-constant segmentation problem, which can be used to train an FCN end-to-end. It extends the classical Mumford-Shah variational segmentation problem to be able to handle permutation-invariant labels in the ground truth of instance segmentation. Experiments on PASCAL VOC 2012, Semantic Boundaries dataset(SBD), and the MSCOCO 2017 dataset show that the proposed approach efficiently tackle the instance segmentation task. The source code and trained models will be released with the paper.



### SIZER: A Dataset and Model for Parsing 3D Clothing and Learning Size Sensitive 3D Clothing
- **Arxiv ID**: http://arxiv.org/abs/2007.11610v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.11610v1)
- **Published**: 2020-07-22 18:13:24+00:00
- **Updated**: 2020-07-22 18:13:24+00:00
- **Authors**: Garvita Tiwari, Bharat Lal Bhatnagar, Tony Tung, Gerard Pons-Moll
- **Comment**: European Conference on Computer Vision 2020
- **Journal**: None
- **Summary**: While models of 3D clothing learned from real data exist, no method can predict clothing deformation as a function of garment size. In this paper, we introduce SizerNet to predict 3D clothing conditioned on human body shape and garment size parameters, and ParserNet to infer garment meshes and shape under clothing with personal details in a single pass from an input mesh. SizerNet allows to estimate and visualize the dressing effect of a garment in various sizes, and ParserNet allows to edit clothing of an input mesh directly, removing the need for scan segmentation, which is a challenging problem in itself. To learn these models, we introduce the SIZER dataset of clothing size variation which includes $100$ different subjects wearing casual clothing items in various sizes, totaling to approximately 2000 scans. This dataset includes the scans, registrations to the SMPL model, scans segmented in clothing parts, garment category and size labels. Our experiments show better parsing accuracy and size prediction than baseline methods trained on SIZER. The code, model and dataset will be released for research purposes.



### TinyTL: Reduce Activations, Not Trainable Parameters for Efficient On-Device Learning
- **Arxiv ID**: http://arxiv.org/abs/2007.11622v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.11622v5)
- **Published**: 2020-07-22 18:39:53+00:00
- **Updated**: 2021-06-06 01:23:16+00:00
- **Authors**: Han Cai, Chuang Gan, Ligeng Zhu, Song Han
- **Comment**: NeurIPS 2020
- **Journal**: None
- **Summary**: On-device learning enables edge devices to continually adapt the AI models to new data, which requires a small memory footprint to fit the tight memory constraint of edge devices. Existing work solves this problem by reducing the number of trainable parameters. However, this doesn't directly translate to memory saving since the major bottleneck is the activations, not parameters. In this work, we present Tiny-Transfer-Learning (TinyTL) for memory-efficient on-device learning. TinyTL freezes the weights while only learns the bias modules, thus no need to store the intermediate activations. To maintain the adaptation capacity, we introduce a new memory-efficient bias module, the lite residual module, to refine the feature extractor by learning small residual feature maps adding only 3.8% memory overhead. Extensive experiments show that TinyTL significantly saves the memory (up to 6.5x) with little accuracy loss compared to fine-tuning the full network. Compared to fine-tuning the last layer, TinyTL provides significant accuracy improvements (up to 34.1%) with little memory overhead. Furthermore, combined with feature extractor adaptation, TinyTL provides 7.3-12.9x memory saving without sacrificing accuracy compared to fine-tuning the full Inception-V3.



### Subjective and Objective Quality Assessment of High Frame Rate Videos
- **Arxiv ID**: http://arxiv.org/abs/2007.11634v2
- **DOI**: 10.1109/ACCESS.2021.3100462
- **Categories**: **cs.MM**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.11634v2)
- **Published**: 2020-07-22 19:11:42+00:00
- **Updated**: 2021-09-27 03:58:25+00:00
- **Authors**: Pavan C. Madhusudana, Xiangxu Yu, Neil Birkbeck, Yilin Wang, Balu Adsumilli, Alan C. Bovik
- **Comment**: None
- **Journal**: IEEE Access. 9 (2021) 108069 - 108082
- **Summary**: High frame rate (HFR) videos are becoming increasingly common with the tremendous popularity of live, high-action streaming content such as sports. Although HFR contents are generally of very high quality, high bandwidth requirements make them challenging to deliver efficiently, while simultaneously maintaining their quality. To optimize trade-offs between bandwidth requirements and video quality, in terms of frame rate adaptation, it is imperative to understand the intricate relationship between frame rate and perceptual video quality. Towards advancing progression in this direction we designed a new subjective resource, called the LIVE-YouTube-HFR (LIVE-YT-HFR) dataset, which is comprised of 480 videos having 6 different frame rates, obtained from 16 diverse contents. In order to understand the combined effects of compression and frame rate adjustment, we also processed videos at 5 compression levels at each frame rate. To obtain subjective labels on the videos, we conducted a human study yielding 19,000 human quality ratings obtained from a pool of 85 human subjects. We also conducted a holistic evaluation of existing state-of-the-art Full and No-Reference video quality algorithms, and statistically benchmarked their performance on the new database. The LIVE-YT-HFR database has been made available online for public use and evaluation purposes, with hopes that it will help advance research in this exciting video technology direction. It may be obtained at \url{https://live.ece.utexas.edu/research/LIVE_YT_HFR/LIVE_YT_HFR/index.html}



### Attention based Multiple Instance Learning for Classification of Blood Cell Disorders
- **Arxiv ID**: http://arxiv.org/abs/2007.11641v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.11641v1)
- **Published**: 2020-07-22 19:29:40+00:00
- **Updated**: 2020-07-22 19:29:40+00:00
- **Authors**: Ario Sadafi, Asya Makhro, Anna Bogdanova, Nassir Navab, Tingying Peng, Shadi Albarqouni, Carsten Marr
- **Comment**: None
- **Journal**: None
- **Summary**: Red blood cells are highly deformable and present in various shapes. In blood cell disorders, only a subset of all cells is morphologically altered and relevant for the diagnosis. However, manually labeling of all cells is laborious, complicated and introduces inter-expert variability. We propose an attention based multiple instance learning method to classify blood samples of patients suffering from blood cell disorders. Cells are detected using an R-CNN architecture. With the features extracted for each cell, a multiple instance learning method classifies patient samples into one out of four blood cell disorders. The attention mechanism provides a measure of the contribution of each cell to the overall classification and significantly improves the network's classification accuracy as well as its interpretability for the medical expert.



### Darwin's Neural Network: AI-based Strategies for Rapid and Scalable Cell and Coronavirus Screening
- **Arxiv ID**: http://arxiv.org/abs/2007.11653v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, I.5.0
- **Links**: [PDF](http://arxiv.org/pdf/2007.11653v1)
- **Published**: 2020-07-22 20:11:06+00:00
- **Updated**: 2020-07-22 20:11:06+00:00
- **Authors**: Sang Won Lee, Yueh-Ting Chiu, Philip Brudnicki, Audrey M. Bischoff, Angus Jelinek, Jenny Zijun Wang, Danielle R. Bogdanowicz, Andrew F. Laine, Jia Guo, Helen H. Lu
- **Comment**: 19 pages, 7 figures
- **Journal**: None
- **Summary**: Recent advances in the interdisciplinary scientific field of machine perception, computer vision, and biomedical engineering underpin a collection of machine learning algorithms with a remarkable ability to decipher the contents of microscope and nanoscope images. Machine learning algorithms are transforming the interpretation and analysis of microscope and nanoscope imaging data through use in conjunction with biological imaging modalities. These advances are enabling researchers to carry out real-time experiments that were previously thought to be computationally impossible. Here we adapt the theory of survival of the fittest in the field of computer vision and machine perception to introduce a new framework of multi-class instance segmentation deep learning, Darwin's Neural Network (DNN), to carry out morphometric analysis and classification of COVID19 and MERS-CoV collected in vivo and of multiple mammalian cell types in vitro.



### Analogical Reasoning for Visually Grounded Language Acquisition
- **Arxiv ID**: http://arxiv.org/abs/2007.11668v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV, cs.RO, 68T07, 68T45, 68T50, 68T40, 68T27, I.2.10; I.2.6; I.2.7; I.2.9
- **Links**: [PDF](http://arxiv.org/pdf/2007.11668v1)
- **Published**: 2020-07-22 20:51:58+00:00
- **Updated**: 2020-07-22 20:51:58+00:00
- **Authors**: Bo Wu, Haoyu Qin, Alireza Zareian, Carl Vondrick, Shih-Fu Chang
- **Comment**: 12 pages
- **Journal**: None
- **Summary**: Children acquire language subconsciously by observing the surrounding world and listening to descriptions. They can discover the meaning of words even without explicit language knowledge, and generalize to novel compositions effortlessly. In this paper, we bring this ability to AI, by studying the task of Visually grounded Language Acquisition (VLA). We propose a multimodal transformer model augmented with a novel mechanism for analogical reasoning, which approximates novel compositions by learning semantic mapping and reasoning operations from previously seen compositions. Our proposed method, Analogical Reasoning Transformer Networks (ARTNet), is trained on raw multimedia data (video frames and transcripts), and after observing a set of compositions such as "washing apple" or "cutting carrot", it can generalize and recognize new compositions in new video frames, such as "washing carrot" or "cutting apple". To this end, ARTNet refers to relevant instances in the training data and uses their visual features and captions to establish analogies with the query image. Then it chooses the suitable verb and noun to create a new composition that describes the new image best. Extensive experiments on an instructional video dataset demonstrate that the proposed method achieves significantly better generalization capability and recognition accuracy compared to state-of-the-art transformer models.



### Contact and Human Dynamics from Monocular Video
- **Arxiv ID**: http://arxiv.org/abs/2007.11678v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.11678v2)
- **Published**: 2020-07-22 21:09:11+00:00
- **Updated**: 2020-07-24 04:02:14+00:00
- **Authors**: Davis Rempe, Leonidas J. Guibas, Aaron Hertzmann, Bryan Russell, Ruben Villegas, Jimei Yang
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: Existing deep models predict 2D and 3D kinematic poses from video that are approximately accurate, but contain visible errors that violate physical constraints, such as feet penetrating the ground and bodies leaning at extreme angles. In this paper, we present a physics-based method for inferring 3D human motion from video sequences that takes initial 2D and 3D pose estimates as input. We first estimate ground contact timings with a novel prediction network which is trained without hand-labeled data. A physics-based trajectory optimization then solves for a physically-plausible motion, based on the inputs. We show this process produces motions that are significantly more realistic than those from purely kinematic methods, substantially improving quantitative measures of both kinematic and dynamic plausibility. We demonstrate our method on character animation and pose estimation tasks on dynamic motions of dancing and sports with complex contact patterns.



### Cloud Transformers: A Universal Approach To Point Cloud Processing Tasks
- **Arxiv ID**: http://arxiv.org/abs/2007.11679v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.11679v4)
- **Published**: 2020-07-22 21:09:14+00:00
- **Updated**: 2021-10-03 20:09:41+00:00
- **Authors**: Kirill Mazur, Victor Lempitsky
- **Comment**: Accepted for ICCV 2021
- **Journal**: None
- **Summary**: We present a new versatile building block for deep point cloud processing architectures that is equally suited for diverse tasks. This building block combines the ideas of spatial transformers and multi-view convolutional networks with the efficiency of standard convolutional layers in two and three-dimensional dense grids. The new block operates via multiple parallel heads, whereas each head differentiably rasterizes feature representations of individual points into a low-dimensional space, and then uses dense convolution to propagate information across points. The results of the processing of individual heads are then combined together resulting in the update of point features. Using the new block, we build architectures for both discriminative (point cloud segmentation, point cloud classification) and generative (point cloud inpainting and image-based point cloud reconstruction) tasks. The resulting architectures achieve state-of-the-art performance for these tasks, demonstrating the versatility of the new block for point cloud processing.



### Multi-modality imaging with structure-promoting regularisers
- **Arxiv ID**: http://arxiv.org/abs/2007.11689v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.NA, math.NA
- **Links**: [PDF](http://arxiv.org/pdf/2007.11689v1)
- **Published**: 2020-07-22 21:26:37+00:00
- **Updated**: 2020-07-22 21:26:37+00:00
- **Authors**: Matthias J. Ehrhardt
- **Comment**: None
- **Journal**: None
- **Summary**: Imaging with multiple modalities or multiple channels is becoming increasingly important for our modern society. A key tool for understanding and early diagnosis of cancer and dementia is PET-MR, a combined positron emission tomography and magnetic resonance imaging scanner which can simultaneously acquire functional and anatomical data. Similarly in remote sensing, while hyperspectral sensors may allow to characterise and distinguish materials, digital cameras offer high spatial resolution to delineate objects. In both of these examples, the imaging modalities can be considered individually or jointly. In this chapter we discuss mathematical approaches which allow to combine information from several imaging modalities so that multi-modality imaging can be more than just the sum of its components.



### Integrating Image Captioning with Rule-based Entity Masking
- **Arxiv ID**: http://arxiv.org/abs/2007.11690v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.11690v1)
- **Published**: 2020-07-22 21:27:12+00:00
- **Updated**: 2020-07-22 21:27:12+00:00
- **Authors**: Aditya Mogadala, Xiaoyu Shen, Dietrich Klakow
- **Comment**: None
- **Journal**: None
- **Summary**: Given an image, generating its natural language description (i.e., caption) is a well studied problem. Approaches proposed to address this problem usually rely on image features that are difficult to interpret. Particularly, these image features are subdivided into global and local features, where global features are extracted from the global representation of the image, while local features are extracted from the objects detected locally in an image. Although, local features extract rich visual information from the image, existing models generate captions in a blackbox manner and humans have difficulty interpreting which local objects the caption is aimed to represent. Hence in this paper, we propose a novel framework for the image captioning with an explicit object (e.g., knowledge graph entity) selection process while still maintaining its end-to-end training ability. The model first explicitly selects which local entities to include in the caption according to a human-interpretable mask, then generate proper captions by attending to selected entities. Experiments conducted on the MSCOCO dataset demonstrate that our method achieves good performance in terms of the caption quality and diversity with a more interpretable generating process than previous counterparts.



### End-to-End Trainable Deep Active Contour Models for Automated Image Segmentation: Delineating Buildings in Aerial Imagery
- **Arxiv ID**: http://arxiv.org/abs/2007.11691v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.11691v1)
- **Published**: 2020-07-22 21:27:17+00:00
- **Updated**: 2020-07-22 21:27:17+00:00
- **Authors**: Ali Hatamizadeh, Debleena Sengupta, Demetri Terzopoulos
- **Comment**: Accepted to European Conference on Computer Vision (ECCV) 2020
- **Journal**: None
- **Summary**: The automated segmentation of buildings in remote sensing imagery is a challenging task that requires the accurate delineation of multiple building instances over typically large image areas. Manual methods are often laborious and current deep-learning-based approaches fail to delineate all building instances and do so with adequate accuracy. As a solution, we present Trainable Deep Active Contours (TDACs), an automatic image segmentation framework that intimately unites Convolutional Neural Networks (CNNs) and Active Contour Models (ACMs). The Eulerian energy functional of the ACM component includes per-pixel parameter maps that are predicted by the backbone CNN, which also initializes the ACM. Importantly, both the ACM and CNN components are fully implemented in TensorFlow and the entire TDAC architecture is end-to-end automatically differentiable and backpropagation trainable without user intervention. TDAC yields fast, accurate, and fully automatic simultaneous delineation of arbitrarily many buildings in the image. We validate the model on two publicly available aerial image datasets for building segmentation, and our results demonstrate that TDAC establishes a new state-of-the-art performance.



### Adversarial Attacks against Face Recognition: A Comprehensive Study
- **Arxiv ID**: http://arxiv.org/abs/2007.11709v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.11709v3)
- **Published**: 2020-07-22 22:46:00+00:00
- **Updated**: 2021-02-06 14:46:56+00:00
- **Authors**: Fatemeh Vakhshiteh, Ahmad Nickabadi, Raghavendra Ramachandra
- **Comment**: None
- **Journal**: None
- **Summary**: Face recognition (FR) systems have demonstrated outstanding verification performance, suggesting suitability for real-world applications ranging from photo tagging in social media to automated border control (ABC). In an advanced FR system with deep learning-based architecture, however, promoting the recognition efficiency alone is not sufficient, and the system should also withstand potential kinds of attacks designed to target its proficiency. Recent studies show that (deep) FR systems exhibit an intriguing vulnerability to imperceptible or perceptible but natural-looking adversarial input images that drive the model to incorrect output predictions. In this article, we present a comprehensive survey on adversarial attacks against FR systems and elaborate on the competence of new countermeasures against them. Further, we propose a taxonomy of existing attack and defense methods based on different criteria. We compare attack methods on the orientation and attributes and defense approaches on the category. Finally, we explore the challenges and potential research direction.



