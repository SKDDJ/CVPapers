# Arxiv Papers in cs.CV on 2020-07-25
### All-Optical Information Processing Capacity of Diffractive Surfaces
- **Arxiv ID**: http://arxiv.org/abs/2007.12813v2
- **DOI**: 10.1038/s41377-020-00439-9
- **Categories**: **eess.IV**, cs.CV, cs.NE, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/2007.12813v2)
- **Published**: 2020-07-25 00:40:46+00:00
- **Updated**: 2020-11-18 03:49:56+00:00
- **Authors**: Onur Kulce, Deniz Mengu, Yair Rivenson, Aydogan Ozcan
- **Comment**: 31 Pages, 6 Figures, 1 Table
- **Journal**: Light: Science & Applications (2021)
- **Summary**: Precise engineering of materials and surfaces has been at the heart of some of the recent advances in optics and photonics. These advances around the engineering of materials with new functionalities have also opened up exciting avenues for designing trainable surfaces that can perform computation and machine learning tasks through light-matter interaction and diffraction. Here, we analyze the information processing capacity of coherent optical networks formed by diffractive surfaces that are trained to perform an all-optical computational task between a given input and output field-of-view. We show that the dimensionality of the all-optical solution space covering the complex-valued transformations between the input and output fields-of-view is linearly proportional to the number of diffractive surfaces within the optical network, up to a limit that is dictated by the extent of the input and output fields-of-view. Deeper diffractive networks that are composed of larger numbers of trainable surfaces can cover a higher dimensional subspace of the complex-valued linear transformations between a larger input field-of-view and a larger output field-of-view, and exhibit depth advantages in terms of their statistical inference, learning and generalization capabilities for different image classification tasks, when compared with a single trainable diffractive surface. These analyses and conclusions are broadly applicable to various forms of diffractive surfaces, including e.g., plasmonic and/or dielectric-based metasurfaces and flat optics that can be used to form all-optical processors.



### Joint Featurewise Weighting and Lobal Structure Learning for Multi-view Subspace Clustering
- **Arxiv ID**: http://arxiv.org/abs/2007.12829v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.12829v1)
- **Published**: 2020-07-25 01:57:57+00:00
- **Updated**: 2020-07-25 01:57:57+00:00
- **Authors**: Shi-Xun Lina, Guo Zhongb, Ting Shu
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-view clustering integrates multiple feature sets, which reveal distinct aspects of the data and provide complementary information to each other, to improve the clustering performance. It remains challenging to effectively exploit complementary information across multiple views since the original data often contain noise and are highly redundant. Moreover, most existing multi-view clustering methods only aim to explore the consistency of all views while ignoring the local structure of each view. However, it is necessary to take the local structure of each view into consideration, because different views would present different geometric structures while admitting the same cluster structure. To address the above issues, we propose a novel multi-view subspace clustering method via simultaneously assigning weights for different features and capturing local information of data in view-specific self-representation feature spaces. Especially, a common cluster structure regularization is adopted to guarantee consistency among different views. An efficient algorithm based on an augmented Lagrangian multiplier is also developed to solve the associated optimization problem. Experiments conducted on several benchmark datasets demonstrate that the proposed method achieves state-of-the-art performance. We provide the Matlab code on https://github.com/Ekin102003/JFLMSC.



### A Self-Training Approach for Point-Supervised Object Detection and Counting in Crowds
- **Arxiv ID**: http://arxiv.org/abs/2007.12831v3
- **DOI**: 10.1109/TIP.2021.3055632
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.12831v3)
- **Published**: 2020-07-25 02:14:42+00:00
- **Updated**: 2021-02-18 07:00:06+00:00
- **Authors**: Yi Wang, Junhui Hou, Xinyu Hou, Lap-Pui Chau
- **Comment**: 12 pages. Accepted for Publication at IEEE Transactions on Image
  Processing
- **Journal**: None
- **Summary**: In this paper, we propose a novel self-training approach named Crowd-SDNet that enables a typical object detector trained only with point-level annotations (i.e., objects are labeled with points) to estimate both the center points and sizes of crowded objects. Specifically, during training, we utilize the available point annotations to supervise the estimation of the center points of objects directly. Based on a locally-uniform distribution assumption, we initialize pseudo object sizes from the point-level supervisory information, which are then leveraged to guide the regression of object sizes via a crowdedness-aware loss. Meanwhile, we propose a confidence and order-aware refinement scheme to continuously refine the initial pseudo object sizes such that the ability of the detector is increasingly boosted to detect and count objects in crowds simultaneously. Moreover, to address extremely crowded scenes, we propose an effective decoding method to improve the detector's representation ability. Experimental results on the WiderFace benchmark show that our approach significantly outperforms state-of-the-art point-supervised methods under both detection and counting tasks, i.e., our method improves the average precision by more than 10% and reduces the counting error by 31.2%. Besides, our method obtains the best results on the crowd counting and localization datasets (i.e., ShanghaiTech and NWPU-Crowd) and vehicle counting datasets (i.e., CARPK and PUCPR+) compared with state-of-the-art counting-by-detection methods. The code will be publicly available at https://github.com/WangyiNTU/Point-supervised-crowd-detection.



### Modal Uncertainty Estimation via Discrete Latent Representation
- **Arxiv ID**: http://arxiv.org/abs/2007.12858v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.12858v1)
- **Published**: 2020-07-25 05:29:34+00:00
- **Updated**: 2020-07-25 05:29:34+00:00
- **Authors**: Di Qiu, Lok Ming Lui
- **Comment**: None
- **Journal**: None
- **Summary**: Many important problems in the real world don't have unique solutions. It is thus important for machine learning models to be capable of proposing different plausible solutions with meaningful probability measures. In this work we introduce such a deep learning framework that learns the one-to-many mappings between the inputs and outputs, together with faithful uncertainty measures. We call our framework {\it modal uncertainty estimation} since we model the one-to-many mappings to be generated through a set of discrete latent variables, each representing a latent mode hypothesis that explains the corresponding type of input-output relationship. The discrete nature of the latent representations thus allows us to estimate for any input the conditional probability distribution of the outputs very effectively. Both the discrete latent space and its uncertainty estimation are jointly learned during training. We motivate our use of discrete latent space through the multi-modal posterior collapse problem in current conditional generative models, then develop the theoretical background, and extensively validate our method on both synthetic and realistic tasks. Our framework demonstrates significantly more accurate uncertainty estimation than the current state-of-the-art methods, and is informative and convenient for practical use.



### OpenRooms: An End-to-End Open Framework for Photorealistic Indoor Scene Datasets
- **Arxiv ID**: http://arxiv.org/abs/2007.12868v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.12868v3)
- **Published**: 2020-07-25 06:48:47+00:00
- **Updated**: 2021-09-27 05:29:08+00:00
- **Authors**: Zhengqin Li, Ting-Wei Yu, Shen Sang, Sarah Wang, Meng Song, Yuhan Liu, Yu-Ying Yeh, Rui Zhu, Nitesh Gundavarapu, Jia Shi, Sai Bi, Zexiang Xu, Hong-Xing Yu, Kalyan Sunkavalli, Miloš Hašan, Ravi Ramamoorthi, Manmohan Chandraker
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel framework for creating large-scale photorealistic datasets of indoor scenes, with ground truth geometry, material, lighting and semantics. Our goal is to make the dataset creation process widely accessible, transforming scans into photorealistic datasets with high-quality ground truth for appearance, layout, semantic labels, high quality spatially-varying BRDF and complex lighting, including direct, indirect and visibility components. This enables important applications in inverse rendering, scene understanding and robotics. We show that deep networks trained on the proposed dataset achieve competitive performance for shape, material and lighting estimation on real images, enabling photorealistic augmented reality applications, such as object insertion and material editing. We also show our semantic labels may be used for segmentation and multi-task learning. Finally, we demonstrate that our framework may also be integrated with physics engines, to create virtual robotics environments with unique ground truth such as friction coefficients and correspondence to real scenes. The dataset and all the tools to create such datasets will be made publicly available.



### Applying Semantic Segmentation to Autonomous Cars in the Snowy Environment
- **Arxiv ID**: http://arxiv.org/abs/2007.12869v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.12869v1)
- **Published**: 2020-07-25 07:07:23+00:00
- **Updated**: 2020-07-25 07:07:23+00:00
- **Authors**: Zhaoyu Pan, Takanori Emaru, Ankit Ravankar, Yukinori Kobayashi
- **Comment**: 4 pages, 5 Figures
- **Journal**: 36th Annual Conference of the Robot Society of Japan, Nagoya, 2018
- **Summary**: This paper mainly focuses on environment perception in snowy situations which forms the backbone of the autonomous driving technology. For the purpose, semantic segmentation is employed to classify the objects while the vehicle is driven autonomously. We train the Fully Convolutional Networks (FCN) on our own dataset and present the experimental results. Finally, the outcomes are analyzed to give a conclusion. It can be concluded that the database still needs to be optimized and a favorable algorithm should be proposed to get better results.



### MirrorNet: Bio-Inspired Camouflaged Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2007.12881v3
- **DOI**: 10.1109/ACCESS.2021.3064443
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.12881v3)
- **Published**: 2020-07-25 08:31:15+00:00
- **Updated**: 2021-03-11 04:39:35+00:00
- **Authors**: Jinnan Yan, Trung-Nghia Le, Khanh-Duy Nguyen, Minh-Triet Tran, Thanh-Toan Do, Tam V. Nguyen
- **Comment**: Accepted to IEEE Access
- **Journal**: None
- **Summary**: Camouflaged objects are generally difficult to be detected in their natural environment even for human beings. In this paper, we propose a novel bio-inspired network, named the MirrorNet, that leverages both instance segmentation and mirror stream for the camouflaged object segmentation. Differently from existing networks for segmentation, our proposed network possesses two segmentation streams: the main stream and the mirror stream corresponding with the original image and its flipped image, respectively. The output from the mirror stream is then fused into the main stream's result for the final camouflage map to boost up the segmentation accuracy. Extensive experiments conducted on the public CAMO dataset demonstrate the effectiveness of our proposed network. Our proposed method achieves 89% in accuracy, outperforming the state-of-the-arts.   Project Page: https://sites.google.com/view/ltnghia/research/camo



### Learning Disentangled Representations with Latent Variation Predictability
- **Arxiv ID**: http://arxiv.org/abs/2007.12885v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.12885v1)
- **Published**: 2020-07-25 08:54:26+00:00
- **Updated**: 2020-07-25 08:54:26+00:00
- **Authors**: Xinqi Zhu, Chang Xu, Dacheng Tao
- **Comment**: 14 pages, ECCV20
- **Journal**: None
- **Summary**: Latent traversal is a popular approach to visualize the disentangled latent representations. Given a bunch of variations in a single unit of the latent representation, it is expected that there is a change in a single factor of variation of the data while others are fixed. However, this impressive experimental observation is rarely explicitly encoded in the objective function of learning disentangled representations. This paper defines the variation predictability of latent disentangled representations. Given image pairs generated by latent codes varying in a single dimension, this varied dimension could be closely correlated with these image pairs if the representation is well disentangled. Within an adversarial generation process, we encourage variation predictability by maximizing the mutual information between latent variations and corresponding image pairs. We further develop an evaluation metric that does not rely on the ground-truth generative factors to measure the disentanglement of latent representations. The proposed variation predictability is a general constraint that is applicable to the VAE and GAN frameworks for boosting disentanglement of latent representations. Experiments show that the proposed variation predictability correlates well with existing ground-truth-required metrics and the proposed algorithm is effective for disentanglement learning.



### Approximated Bilinear Modules for Temporal Modeling
- **Arxiv ID**: http://arxiv.org/abs/2007.12887v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.12887v1)
- **Published**: 2020-07-25 09:07:35+00:00
- **Updated**: 2020-07-25 09:07:35+00:00
- **Authors**: Xinqi Zhu, Chang Xu, Langwen Hui, Cewu Lu, Dacheng Tao
- **Comment**: 8 pages, ICCV19
- **Journal**: None
- **Summary**: We consider two less-emphasized temporal properties of video: 1. Temporal cues are fine-grained; 2. Temporal modeling needs reasoning. To tackle both problems at once, we exploit approximated bilinear modules (ABMs) for temporal modeling. There are two main points making the modules effective: two-layer MLPs can be seen as a constraint approximation of bilinear operations, thus can be used to construct deep ABMs in existing CNNs while reusing pretrained parameters; frame features can be divided into static and dynamic parts because of visual repetition in adjacent frames, which enables temporal modeling to be more efficient. Multiple ABM variants and implementations are investigated, from high performance to high efficiency. Specifically, we show how two-layer subnets in CNNs can be converted to temporal bilinear modules by adding an auxiliary-branch. Besides, we introduce snippet sampling and shifting inference to boost sparse-frame video classification performance. Extensive ablation studies are conducted to show the effectiveness of proposed techniques. Our models can outperform most state-of-the-art methods on Something-Something v1 and v2 datasets without Kinetics pretraining, and are also competitive on other YouTube-like action recognition datasets. Our code is available on https://github.com/zhuxinqimac/abm-pytorch.



### 3D Neural Network for Lung Cancer Risk Prediction on CT Volumes
- **Arxiv ID**: http://arxiv.org/abs/2007.12898v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.12898v1)
- **Published**: 2020-07-25 10:01:22+00:00
- **Updated**: 2020-07-25 10:01:22+00:00
- **Authors**: Daniel Korat
- **Comment**: None
- **Journal**: None
- **Summary**: With an estimated 160,000 deaths in 2018, lung cancer is the most common cause of cancer death in the United States. Lung cancer CT screening has been shown to reduce mortality by up to 40% and is now included in US screening guidelines. Reducing the high error rates in lung cancer screening is imperative because of the high clinical and financial costs caused by diagnosis mistakes. Despite the use of standards for radiological diagnosis, persistent inter-grader variability and incomplete characterization of comprehensive imaging findings remain as limitations of current methods. These limitations suggest opportunities for more sophisticated systems to improve performance and inter-reader consistency. In this report, we reproduce a state-of-the-art deep learning algorithm for lung cancer risk prediction. Our model predicts malignancy probability and risk bucket classification from lung CT studies. This allows for risk categorization of patients being screened and suggests the most appropriate surveillance and management. Combining our solution high accuracy, consistency and fully automated nature, our approach may enable highly efficient screening procedures and accelerate the adoption of lung cancer screening.



### CNN Detection of GAN-Generated Face Images based on Cross-Band Co-occurrences Analysis
- **Arxiv ID**: http://arxiv.org/abs/2007.12909v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.12909v2)
- **Published**: 2020-07-25 10:55:04+00:00
- **Updated**: 2020-10-02 12:43:28+00:00
- **Authors**: Mauro Barni, Kassem Kallas, Ehsan Nowroozi, Benedetta Tondi
- **Comment**: (6 pages, 2 figures, 4 tables), (IEEE International Workshop on
  Information Forensics and Security - WIFS 2020, New York, USA)
- **Journal**: None
- **Summary**: Last-generation GAN models allow to generate synthetic images which are visually indistinguishable from natural ones, raising the need to develop tools to distinguish fake and natural images thus contributing to preserve the trustworthiness of digital images. While modern GAN models can generate very high-quality images with no visible spatial artifacts, reconstruction of consistent relationships among colour channels is expectedly more difficult. In this paper, we propose a method for distinguishing GAN-generated from natural images by exploiting inconsistencies among spectral bands, with specific focus on the generation of synthetic face images. Specifically, we use cross-band co-occurrence matrices, in addition to spatial co-occurrence matrices, as input to a CNN model, which is trained to distinguish between real and synthetic faces. The results of our experiments confirm the goodness of our approach which outperforms a similar detection technique based on intra-band spatial co-occurrences only. The performance gain is particularly significant with regard to robustness against post-processing, like geometric transformations, filtering and contrast manipulations.



### Tighter risk certificates for neural networks
- **Arxiv ID**: http://arxiv.org/abs/2007.12911v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.12911v3)
- **Published**: 2020-07-25 11:02:16+00:00
- **Updated**: 2021-09-22 14:27:19+00:00
- **Authors**: María Pérez-Ortiz, Omar Rivasplata, John Shawe-Taylor, Csaba Szepesvári
- **Comment**: New version includes: i) experiment showing the potential of the risk
  certificate for neural architecture search (Fig. 2); ii) experiments spanning
  uncertainty quantification and analysis of prior/posterior (Section 7.8);
  iii) an outline of the strengths of probabilistic neural networks trained by
  PBB (Section 7.9) and iv) a strengthened discussion on the connection to
  Bayesian learning
- **Journal**: Journal of Machine Learning Research, 2021
- **Summary**: This paper presents an empirical study regarding training probabilistic neural networks using training objectives derived from PAC-Bayes bounds. In the context of probabilistic neural networks, the output of training is a probability distribution over network weights. We present two training objectives, used here for the first time in connection with training neural networks. These two training objectives are derived from tight PAC-Bayes bounds. We also re-implement a previously used training objective based on a classical PAC-Bayes bound, to compare the properties of the predictors learned using the different training objectives. We compute risk certificates for the learnt predictors, based on part of the data used to learn the predictors. We further experiment with different types of priors on the weights (both data-free and data-dependent priors) and neural network architectures. Our experiments on MNIST and CIFAR-10 show that our training methods produce competitive test set errors and non-vacuous risk bounds with much tighter values than previous results in the literature, showing promise not only to guide the learning algorithm through bounding the risk but also for model selection. These observations suggest that the methods studied here might be good candidates for self-certified learning, in the sense of using the whole data set for learning a predictor and certifying its risk on any unseen data (from the same distribution as the training data) potentially without the need for holding out test data.



### Crowdsourced 3D Mapping: A Combined Multi-View Geometry and Self-Supervised Learning Approach
- **Arxiv ID**: http://arxiv.org/abs/2007.12918v1
- **DOI**: 10.1109/IROS45743.2020.9341243
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2007.12918v1)
- **Published**: 2020-07-25 12:10:16+00:00
- **Updated**: 2020-07-25 12:10:16+00:00
- **Authors**: Hemang Chawla, Matti Jukola, Terence Brouns, Elahe Arani, Bahram Zonooz
- **Comment**: Accepted at 2020 IEEE/RSJ International Conference on Intelligent
  Robots and Systems (IROS)
- **Journal**: None
- **Summary**: The ability to efficiently utilize crowdsourced visual data carries immense potential for the domains of large scale dynamic mapping and autonomous driving. However, state-of-the-art methods for crowdsourced 3D mapping assume prior knowledge of camera intrinsics. In this work, we propose a framework that estimates the 3D positions of semantically meaningful landmarks such as traffic signs without assuming known camera intrinsics, using only monocular color camera and GPS. We utilize multi-view geometry as well as deep learning based self-calibration, depth, and ego-motion estimation for traffic sign positioning, and show that combining their strengths is important for increasing the map coverage. To facilitate research on this task, we construct and make available a KITTI based 3D traffic sign ground truth positioning dataset. Using our proposed framework, we achieve an average single-journey relative and absolute positioning accuracy of 39cm and 1.26m respectively, on this dataset.



### Neural networks with late-phase weights
- **Arxiv ID**: http://arxiv.org/abs/2007.12927v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.12927v4)
- **Published**: 2020-07-25 13:23:37+00:00
- **Updated**: 2022-04-11 13:55:43+00:00
- **Authors**: Johannes von Oswald, Seijin Kobayashi, Alexander Meulemans, Christian Henning, Benjamin F. Grewe, João Sacramento
- **Comment**: 25 pages, 6 figures
- **Journal**: Published as a conference paper at ICLR 2021
- **Summary**: The largely successful method of training neural networks is to learn their weights using some variant of stochastic gradient descent (SGD). Here, we show that the solutions found by SGD can be further improved by ensembling a subset of the weights in late stages of learning. At the end of learning, we obtain back a single model by taking a spatial average in weight space. To avoid incurring increased computational costs, we investigate a family of low-dimensional late-phase weight models which interact multiplicatively with the remaining parameters. Our results show that augmenting standard models with late-phase weights improves generalization in established benchmarks such as CIFAR-10/100, ImageNet and enwik8. These findings are complemented with a theoretical analysis of a noisy quadratic problem which provides a simplified picture of the late phases of neural network learning.



### Towards 3D Visualization of Video from Frames
- **Arxiv ID**: http://arxiv.org/abs/2007.14465v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.14465v1)
- **Published**: 2020-07-25 13:37:42+00:00
- **Updated**: 2020-07-25 13:37:42+00:00
- **Authors**: Slimane Larabi
- **Comment**: paper not published
- **Journal**: None
- **Summary**: We explain theoretically how to reconstruct the 3D scene from successive frames in order to see the video in 3D. To do this, features, associated to moving rigid objects in 3D, are extracted in frames and matched. The vanishing point computed in frame corresponding to the direction of moving object is used for 3D positioning of the 3D structure of the moving object. First experiments are conducted and the obtained results are shown and publicly available. They demonstrate the feasibility of our method. We conclude this paper by future works in order to improve this method tacking into account non-rigid objects and the case of moving camera.



### Video Super Resolution Based on Deep Learning: A Comprehensive Survey
- **Arxiv ID**: http://arxiv.org/abs/2007.12928v3
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.12928v3)
- **Published**: 2020-07-25 13:39:54+00:00
- **Updated**: 2022-03-16 15:07:21+00:00
- **Authors**: Hongying Liu, Zhubo Ruan, Peng Zhao, Chao Dong, Fanhua Shang, Yuanyuan Liu, Linlin Yang, Radu Timofte
- **Comment**: 33 pages, 41 figures, accepted by Artificial Intelligence Review,
  2022
- **Journal**: None
- **Summary**: In recent years, deep learning has made great progress in many fields such as image recognition, natural language processing, speech recognition and video super-resolution. In this survey, we comprehensively investigate 33 state-of-the-art video super-resolution (VSR) methods based on deep learning. It is well known that the leverage of information within video frames is important for video super-resolution. Thus we propose a taxonomy and classify the methods into six sub-categories according to the ways of utilizing inter-frame information. Moreover, the architectures and implementation details of all the methods are depicted in detail. Finally, we summarize and compare the performance of the representative VSR method on some benchmark datasets. We also discuss some challenges, which need to be further addressed by researchers in the community of VSR. To the best of our knowledge, this work is the first systematic review on VSR tasks, and it is expected to make a contribution to the development of recent studies in this area and potentially deepen our understanding to the VSR techniques based on deep learning.



### A deep learning based multiscale approach to segment cancer area in liver whole slide image
- **Arxiv ID**: http://arxiv.org/abs/2007.12935v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.12935v1)
- **Published**: 2020-07-25 13:54:01+00:00
- **Updated**: 2020-07-25 13:54:01+00:00
- **Authors**: Yanbo Feng, Adel Hafiane, Hélène Laurent
- **Comment**: None
- **Journal**: None
- **Summary**: This paper addresses the problem of liver cancer segmentation in Whole Slide Image (WSI). We propose a multi-scale image processing method based on automatic end-to-end deep neural network algorithm for segmentation of cancer area. A seven-levels gaussian pyramid representation of the histopathological image was built to provide the texture information in different scales. In this work, several neural architectures were compared using the original image level for the training procedure. The proposed method is based on U-Net applied to seven levels of various resolutions (pyramidal subsumpling). The predictions in different levels are combined through a voting mechanism. The final segmentation result is generated at the original image level. Partial color normalization and weighted overlapping method were applied in preprocessing and prediction separately. The results show the effectiveness of the proposed multi-scales approach achieving better scores compared to the state-of-the-art.



### Gradient Regularized Contrastive Learning for Continual Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2007.12942v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.12942v1)
- **Published**: 2020-07-25 14:30:03+00:00
- **Updated**: 2020-07-25 14:30:03+00:00
- **Authors**: Peng Su, Shixiang Tang, Peng Gao, Di Qiu, Ni Zhao, Xiaogang Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Human beings can quickly adapt to environmental changes by leveraging learning experience. However, the poor ability of adapting to dynamic environments remains a major challenge for AI models. To better understand this issue, we study the problem of continual domain adaptation, where the model is presented with a labeled source domain and a sequence of unlabeled target domains. There are two major obstacles in this problem: domain shifts and catastrophic forgetting. In this work, we propose Gradient Regularized Contrastive Learning to solve the above obstacles. At the core of our method, gradient regularization plays two key roles: (1) enforces the gradient of contrastive loss not to increase the supervised training loss on the source domain, which maintains the discriminative power of learned features; (2) regularizes the gradient update on the new domain not to increase the classification loss on the old target domains, which enables the model to adapt to an in-coming target domain while preserving the performance of previously observed domains. Hence our method can jointly learn both semantically discriminative and domain-invariant features with labeled source domain and unlabeled target domains. The experiments on Digits, DomainNet and Office-Caltech benchmarks demonstrate the strong performance of our approach when compared to the state-of-the-art.



### MRGAN: Multi-Rooted 3D Shape Generation with Unsupervised Part Disentanglement
- **Arxiv ID**: http://arxiv.org/abs/2007.12944v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.12944v1)
- **Published**: 2020-07-25 14:41:51+00:00
- **Updated**: 2020-07-25 14:41:51+00:00
- **Authors**: Rinon Gal, Amit Bermano, Hao Zhang, Daniel Cohen-Or
- **Comment**: None
- **Journal**: None
- **Summary**: We present MRGAN, a multi-rooted adversarial network which generates part-disentangled 3D point-cloud shapes without part-based shape supervision. The network fuses multiple branches of tree-structured graph convolution layers which produce point clouds, with learnable constant inputs at the tree roots. Each branch learns to grow a different shape part, offering control over the shape generation at the part level. Our network encourages disentangled generation of semantic parts via two key ingredients: a root-mixing training strategy which helps decorrelate the different branches to facilitate disentanglement, and a set of loss terms designed with part disentanglement and shape semantics in mind. Of these, a novel convexity loss incentivizes the generation of parts that are more convex, as semantic parts tend to be. In addition, a root-dropping loss further ensures that each root seeds a single part, preventing the degeneration or over-growth of the point-producing branches. We evaluate the performance of our network on a number of 3D shape classes, and offer qualitative and quantitative comparisons to previous works and baseline approaches. We demonstrate the controllability offered by our part-disentangled generation through two applications for shape modeling: part mixing and individual part variation, without receiving segmented shapes as input.



### GP-Aligner: Unsupervised Non-rigid Groupwise Point Set Registration Based On Optimized Group Latent Descriptor
- **Arxiv ID**: http://arxiv.org/abs/2007.12979v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.12979v1)
- **Published**: 2020-07-25 17:09:53+00:00
- **Updated**: 2020-07-25 17:09:53+00:00
- **Authors**: Lingjing Wang, Xiang Li, Yi Fang
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a novel method named GP-Aligner to deal with the problem of non-rigid groupwise point set registration. Compared to previous non-learning approaches, our proposed method gains competitive advantages by leveraging the power of deep neural networks to effectively and efficiently learn to align a large number of highly deformed 3D shapes with superior performance. Unlike most learning-based methods that use an explicit feature encoding network to extract the per-shape features and their correlations, our model leverages a model-free learnable latent descriptor to characterize the group relationship. More specifically, for a given group we first define an optimizable Group Latent Descriptor (GLD) to characterize the gruopwise relationship among a group of point sets. Each GLD is randomly initialized from a Gaussian distribution and then concatenated with the coordinates of each point of the associated point sets in the group. A neural network-based decoder is further constructed to predict the coherent drifts as the desired transformation from input groups of shapes to aligned groups of shapes. During the optimization process, GP-Aligner jointly updates all GLDs and weight parameters of the decoder network towards the minimization of an unsupervised groupwise alignment loss. After optimization, for each group our model coherently drives each point set towards a middle, common position (shape) without specifying one as the target. GP-Aligner does not require large-scale training data for network training and it can directly align groups of point sets in a one-stage optimization process. GP-Aligner shows both accuracy and computational efficiency improvement in comparison with state-of-the-art methods for groupwise point set registration. Moreover, GP-Aligner is shown great efficiency in aligning a large number of groups of real-world 3D shapes.



### Robust and Generalizable Visual Representation Learning via Random Convolutions
- **Arxiv ID**: http://arxiv.org/abs/2007.13003v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.13003v3)
- **Published**: 2020-07-25 19:52:25+00:00
- **Updated**: 2021-05-03 16:12:15+00:00
- **Authors**: Zhenlin Xu, Deyi Liu, Junlin Yang, Colin Raffel, Marc Niethammer
- **Comment**: ICLR 2021. Code is available at
  https://github.com/wildphoton/RandConv
- **Journal**: None
- **Summary**: While successful for various computer vision tasks, deep neural networks have shown to be vulnerable to texture style shifts and small perturbations to which humans are robust. In this work, we show that the robustness of neural networks can be greatly improved through the use of random convolutions as data augmentation. Random convolutions are approximately shape-preserving and may distort local textures. Intuitively, randomized convolutions create an infinite number of new domains with similar global shapes but random local textures. Therefore, we explore using outputs of multi-scale random convolutions as new images or mixing them with the original images during training. When applying a network trained with our approach to unseen domains, our method consistently improves the performance on domain generalization benchmarks and is scalable to ImageNet. In particular, in the challenging scenario of generalizing to the sketch domain in PACS and to ImageNet-Sketch, our method outperforms state-of-art methods by a large margin. More interestingly, our method can benefit downstream tasks by providing a more robust pretrained visual representation.



### Jointly Optimizing Preprocessing and Inference for DNN-based Visual Analytics
- **Arxiv ID**: http://arxiv.org/abs/2007.13005v1
- **DOI**: None
- **Categories**: **cs.DB**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.13005v1)
- **Published**: 2020-07-25 20:26:05+00:00
- **Updated**: 2020-07-25 20:26:05+00:00
- **Authors**: Daniel Kang, Ankit Mathur, Teja Veeramacheneni, Peter Bailis, Matei Zaharia
- **Comment**: None
- **Journal**: None
- **Summary**: While deep neural networks (DNNs) are an increasingly popular way to query large corpora of data, their significant runtime remains an active area of research. As a result, researchers have proposed systems and optimizations to reduce these costs by allowing users to trade off accuracy and speed. In this work, we examine end-to-end DNN execution in visual analytics systems on modern accelerators. Through a novel measurement study, we show that the preprocessing of data (e.g., decoding, resizing) can be the bottleneck in many visual analytics systems on modern hardware.   To address the bottleneck of preprocessing, we introduce two optimizations for end-to-end visual analytics systems. First, we introduce novel methods of achieving accuracy and throughput trade-offs by using natively present, low-resolution visual data. Second, we develop a runtime engine for efficient visual DNN inference. This runtime engine a) efficiently pipelines preprocessing and DNN execution for inference, b) places preprocessing operations on the CPU or GPU in a hardware- and input-aware manner, and c) efficiently manages memory and threading for high throughput execution. We implement these optimizations in a novel system, Smol, and evaluate Smol on eight visual datasets. We show that its optimizations can achieve up to 5.9x end-to-end throughput improvements at a fixed accuracy over recent work in visual analytics.



### HATNet: An End-to-End Holistic Attention Network for Diagnosis of Breast Biopsy Images
- **Arxiv ID**: http://arxiv.org/abs/2007.13007v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.13007v1)
- **Published**: 2020-07-25 20:42:21+00:00
- **Updated**: 2020-07-25 20:42:21+00:00
- **Authors**: Sachin Mehta, Ximing Lu, Donald Weaver, Joann G. Elmore, Hannaneh Hajishirzi, Linda Shapiro
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Training end-to-end networks for classifying gigapixel size histopathological images is computationally intractable. Most approaches are patch-based and first learn local representations (patch-wise) before combining these local representations to produce image-level decisions. However, dividing large tissue structures into patches limits the context available to these networks, which may reduce their ability to learn representations from clinically relevant structures. In this paper, we introduce a novel attention-based network, the Holistic ATtention Network (HATNet) to classify breast biopsy images. We streamline the histopathological image classification pipeline and show how to learn representations from gigapixel size images end-to-end. HATNet extends the bag-of-words approach and uses self-attention to encode global information, allowing it to learn representations from clinically relevant tissue structures without any explicit supervision. It outperforms the previous best network Y-Net, which uses supervision in the form of tissue-level segmentation masks, by 8%. Importantly, our analysis reveals that HATNet learns representations from clinically relevant structures, and it matches the classification accuracy of human pathologists for this challenging test set. Our source code is available at \url{https://github.com/sacmehta/HATNet}



### Style is a Distribution of Features
- **Arxiv ID**: http://arxiv.org/abs/2007.13010v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.13010v1)
- **Published**: 2020-07-25 21:17:51+00:00
- **Updated**: 2020-07-25 21:17:51+00:00
- **Authors**: Eddie Huang, Sahil Gupta
- **Comment**: None
- **Journal**: None
- **Summary**: Neural style transfer (NST) is a powerful image generation technique that uses a convolutional neural network (CNN) to merge the content of one image with the style of another. Contemporary methods of NST use first or second order statistics of the CNN's features to achieve transfers with relatively little computational cost. However, these methods cannot fully extract the style from the CNN's features. We present a new algorithm for style transfer that fully extracts the style from the features by redefining the style loss as the Wasserstein distance between the distribution of features. Thus, we set a new standard in style transfer quality. In addition, we state two important interpretations of NST. The first is a re-emphasis from Li et al., which states that style is simply the distribution of features. The second states that NST is a type of generative adversarial network (GAN) problem.



