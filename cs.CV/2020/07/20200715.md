# Arxiv Papers in cs.CV on 2020-07-15
### Automatic Image Labelling at Pixel Level
- **Arxiv ID**: http://arxiv.org/abs/2007.07415v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.07415v2)
- **Published**: 2020-07-15 00:34:11+00:00
- **Updated**: 2020-07-20 03:17:32+00:00
- **Authors**: Xiang Zhang, Wei Zhang, Jinye Peng, Jianping Fan
- **Comment**: None
- **Journal**: None
- **Summary**: The performance of deep networks for semantic image segmentation largely depends on the availability of large-scale training images which are labelled at the pixel level. Typically, such pixel-level image labellings are obtained manually by a labour-intensive process. To alleviate the burden of manual image labelling, we propose an interesting learning approach to generate pixel-level image labellings automatically. A Guided Filter Network (GFN) is first developed to learn the segmentation knowledge from a source domain, and such GFN then transfers such segmentation knowledge to generate coarse object masks in the target domain. Such coarse object masks are treated as pseudo labels and they are further integrated to optimize/refine the GFN iteratively in the target domain. Our experiments on six image sets have demonstrated that our proposed approach can generate fine-grained object masks (i.e., pixel-level object labellings), whose quality is very comparable to the manually-labelled ones. Our proposed approach can also achieve better performance on semantic image segmentation than most existing weakly-supervised approaches.



### Comparing to Learn: Surpassing ImageNet Pretraining on Radiographs By Comparing Image Representations
- **Arxiv ID**: http://arxiv.org/abs/2007.07423v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.07423v3)
- **Published**: 2020-07-15 01:14:34+00:00
- **Updated**: 2020-07-22 03:00:56+00:00
- **Authors**: Hong-Yu Zhou, Shuang Yu, Cheng Bian, Yifan Hu, Kai Ma, Yefeng Zheng
- **Comment**: MICCAI 2020 early accept; Code and pretrained models available at
  http://github.com/funnyzhou/C2L_MICCAI2020
- **Journal**: None
- **Summary**: In deep learning era, pretrained models play an important role in medical image analysis, in which ImageNet pretraining has been widely adopted as the best way. However, it is undeniable that there exists an obvious domain gap between natural images and medical images. To bridge this gap, we propose a new pretraining method which learns from 700k radiographs given no manual annotations. We call our method as Comparing to Learn (C2L) because it learns robust features by comparing different image representations. To verify the effectiveness of C2L, we conduct comprehensive ablation studies and evaluate it on different tasks and datasets. The experimental results on radiographs show that C2L can outperform ImageNet pretraining and previous state-of-the-art approaches significantly. Code and models are available.



### COCO-FUNIT: Few-Shot Unsupervised Image Translation with a Content Conditioned Style Encoder
- **Arxiv ID**: http://arxiv.org/abs/2007.07431v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.07431v3)
- **Published**: 2020-07-15 02:01:14+00:00
- **Updated**: 2020-07-29 02:06:50+00:00
- **Authors**: Kuniaki Saito, Kate Saenko, Ming-Yu Liu
- **Comment**: The paper will be presented at the EUROPEAN Conference on Computer
  Vision (ECCV) 2020
- **Journal**: None
- **Summary**: Unsupervised image-to-image translation intends to learn a mapping of an image in a given domain to an analogous image in a different domain, without explicit supervision of the mapping. Few-shot unsupervised image-to-image translation further attempts to generalize the model to an unseen domain by leveraging example images of the unseen domain provided at inference time. While remarkably successful, existing few-shot image-to-image translation models find it difficult to preserve the structure of the input image while emulating the appearance of the unseen domain, which we refer to as the content loss problem. This is particularly severe when the poses of the objects in the input and example images are very different. To address the issue, we propose a new few-shot image translation model, COCO-FUNIT, which computes the style embedding of the example images conditioned on the input image and a new module called the constant style bias. Through extensive experimental validations with comparison to the state-of-the-art, our model shows effectiveness in addressing the content loss problem. For code and pretrained models, please check out https://nvlabs.github.io/COCO-FUNIT/ .



### AdvFlow: Inconspicuous Black-box Adversarial Attacks using Normalizing Flows
- **Arxiv ID**: http://arxiv.org/abs/2007.07435v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.07435v2)
- **Published**: 2020-07-15 02:13:49+00:00
- **Updated**: 2020-10-23 00:36:25+00:00
- **Authors**: Hadi M. Dolatabadi, Sarah Erfani, Christopher Leckie
- **Comment**: Accepted to the 34th Conference on Neural Information Processing
  Systems (NeurIPS 2020)
- **Journal**: None
- **Summary**: Deep learning classifiers are susceptible to well-crafted, imperceptible variations of their inputs, known as adversarial attacks. In this regard, the study of powerful attack models sheds light on the sources of vulnerability in these classifiers, hopefully leading to more robust ones. In this paper, we introduce AdvFlow: a novel black-box adversarial attack method on image classifiers that exploits the power of normalizing flows to model the density of adversarial examples around a given target image. We see that the proposed method generates adversaries that closely follow the clean data distribution, a property which makes their detection less likely. Also, our experimental results show competitive performance of the proposed approach with some of the existing attack methods on defended classifiers. The code is available at https://github.com/hmdolatabadi/AdvFlow.



### ContourRend: A Segmentation Method for Improving Contours by Rendering
- **Arxiv ID**: http://arxiv.org/abs/2007.07437v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.07437v1)
- **Published**: 2020-07-15 02:16:00+00:00
- **Updated**: 2020-07-15 02:16:00+00:00
- **Authors**: Junwen Chen, Yi Lu, Yaran Chen, Dongbin Zhao, Zhonghua Pang
- **Comment**: None
- **Journal**: None
- **Summary**: A good object segmentation should contain clear contours and complete regions. However, mask-based segmentation can not handle contour features well on a coarse prediction grid, thus causing problems of blurry edges. While contour-based segmentation provides contours directly, but misses contours' details. In order to obtain fine contours, we propose a segmentation method named ContourRend which adopts a contour renderer to refine segmentation contours. And we implement our method on a segmentation model based on graph convolutional network (GCN). For the single object segmentation task on cityscapes dataset, the GCN-based segmentation con-tour is used to generate a contour of a single object, then our contour renderer focuses on the pixels around the contour and predicts the category at high resolution. By rendering the contour result, our method reaches 72.41% mean intersection over union (IoU) and surpasses baseline Polygon-GCN by 1.22%.



### RGB-IR Cross-modality Person ReID based on Teacher-Student GAN Model
- **Arxiv ID**: http://arxiv.org/abs/2007.07452v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.07452v1)
- **Published**: 2020-07-15 02:58:46+00:00
- **Updated**: 2020-07-15 02:58:46+00:00
- **Authors**: Ziyue Zhang, Shuai Jiang, Congzhentao Huang, Yang Li, Richard Yi Da Xu
- **Comment**: 8 pages including 1 page reference
- **Journal**: None
- **Summary**: RGB-Infrared (RGB-IR) person re-identification (ReID) is a technology where the system can automatically identify the same person appearing at different parts of a video when light is unavailable. The critical challenge of this task is the cross-modality gap of features under different modalities. To solve this challenge, we proposed a Teacher-Student GAN model (TS-GAN) to adopt different domains and guide the ReID backbone to learn better ReID information. (1) In order to get corresponding RGB-IR image pairs, the RGB-IR Generative Adversarial Network (GAN) was used to generate IR images. (2) To kick-start the training of identities, a ReID Teacher module was trained under IR modality person images, which is then used to guide its Student counterpart in training. (3) Likewise, to better adapt different domain features and enhance model ReID performance, three Teacher-Student loss functions were used. Unlike other GAN based models, the proposed model only needs the backbone module at the test stage, making it more efficient and resource-saving. To showcase our model's capability, we did extensive experiments on the newly-released SYSU-MM01 RGB-IR Re-ID benchmark and achieved superior performance to the state-of-the-art with 49.8% Rank-1 and 47.4% mAP.



### Graph-Based Social Relation Reasoning
- **Arxiv ID**: http://arxiv.org/abs/2007.07453v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.07453v3)
- **Published**: 2020-07-15 03:01:11+00:00
- **Updated**: 2020-07-17 07:20:51+00:00
- **Authors**: Wanhua Li, Yueqi Duan, Jiwen Lu, Jianjiang Feng, Jie Zhou
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: Human beings are fundamentally sociable -- that we generally organize our social lives in terms of relations with other people. Understanding social relations from an image has great potential for intelligent systems such as social chatbots and personal assistants. In this paper, we propose a simpler, faster, and more accurate method named graph relational reasoning network (GR2N) for social relation recognition. Different from existing methods which process all social relations on an image independently, our method considers the paradigm of jointly inferring the relations by constructing a social relation graph. Furthermore, the proposed GR2N constructs several virtual relation graphs to explicitly grasp the strong logical constraints among different types of social relations. Experimental results illustrate that our method generates a reasonable and consistent social relation graph and improves the performance in both accuracy and efficiency.



### Reorganizing local image features with chaotic maps: an application to texture recognition
- **Arxiv ID**: http://arxiv.org/abs/2007.07456v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.07456v1)
- **Published**: 2020-07-15 03:15:01+00:00
- **Updated**: 2020-07-15 03:15:01+00:00
- **Authors**: Joao Florindo
- **Comment**: None
- **Journal**: None
- **Summary**: Despite the recent success of convolutional neural networks in texture recognition, model-based descriptors are still competitive, especially when we do not have access to large amounts of annotated data for training and the interpretation of the model is an important issue. Among the model-based approaches, fractal geometry has been one of the most popular, especially in biological applications. Nevertheless, fractals are part of a much broader family of models, which are the non-linear operators, studied in chaos theory. In this context, we propose here a chaos-based local descriptor for texture recognition. More specifically, we map the image into the three-dimensional Euclidean space, iterate a chaotic map over this three-dimensional structure and convert it back to the original image. From such chaos-transformed image at each iteration we collect local descriptors (here we use local binary patters) and those descriptors compose the feature representation of the texture. The performance of our method was verified on the classification of benchmark databases and in the identification of Brazilian plant species based on the texture of the leaf surface. The achieved results confirmed our expectation of a competitive performance, even when compared with some learning-based modern approaches in the literature.



### A cellular automata approach to local patterns for texture recognition
- **Arxiv ID**: http://arxiv.org/abs/2007.07462v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.07462v1)
- **Published**: 2020-07-15 03:25:51+00:00
- **Updated**: 2020-07-15 03:25:51+00:00
- **Authors**: Joao Florindo, Konradin Metze
- **Comment**: None
- **Journal**: None
- **Summary**: Texture recognition is one of the most important tasks in computer vision and, despite the recent success of learning-based approaches, there is still need for model-based solutions. This is especially the case when the amount of data available for training is not sufficiently large, a common situation in several applied areas, or when computational resources are limited. In this context, here we propose a method for texture descriptors that combines the representation power of complex objects by cellular automata with the known effectiveness of local descriptors in texture analysis. The method formulates a new transition function for the automaton inspired on local binary descriptors. It counterbalances the new state of each cell with the previous state, in this way introducing an idea of "controlled deterministic chaos". The descriptors are obtained from the distribution of cell states. The proposed descriptors are applied to the classification of texture images both on benchmark data sets and a real-world problem, i.e., that of identifying plant species based on the texture of their leaf surfaces. Our proposal outperforms other classical and state-of-the-art approaches, especially in the real-world problem, thus revealing its potential to be applied in numerous practical tasks involving texture recognition at some stage.



### How to trust unlabeled data? Instance Credibility Inference for Few-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2007.08461v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.08461v4)
- **Published**: 2020-07-15 03:38:09+00:00
- **Updated**: 2021-05-11 03:21:15+00:00
- **Authors**: Yikai Wang, Li Zhang, Yuan Yao, Yanwei Fu
- **Comment**: Journal extension of arXiv:2003.11853 that appears in CVPR 2020 Code
  and models are released at https://github.com/Yikai-Wang/ICI-FSL
- **Journal**: None
- **Summary**: Deep learning based models have excelled in many computer vision tasks and appear to surpass humans' performance. However, these models require an avalanche of expensive human labeled training data and many iterations to train their large number of parameters. This severely limits their scalability to the real-world long-tail distributed categories, some of which are with a large number of instances, but with only a few manually annotated. Learning from such extremely limited labeled examples is known as Few-shot learning (FSL). Different to prior arts that leverage meta-learning or data augmentation strategies to alleviate this extremely data-scarce problem, this paper presents a statistical approach, dubbed Instance Credibility Inference (ICI) to exploit the support of unlabeled instances for few-shot visual recognition. Typically, we repurpose the self-taught learning paradigm to predict pseudo-labels of unlabeled instances with an initial classifier trained from the few shot and then select the most confident ones to augment the training set to re-train the classifier. This is achieved by constructing a (Generalized) Linear Model (LM/GLM) with incidental parameters to model the mapping from (un-)labeled features to their (pseudo-)labels, in which the sparsity of the incidental parameters indicates the credibility of the corresponding pseudo-labeled instance. We rank the credibility of pseudo-labeled instances along the regularization path of their corresponding incidental parameters, and the most trustworthy pseudo-labeled examples are preserved as the augmented labeled instances. Theoretically, under mild conditions of restricted eigenvalue, irrepresentability, and large error, our approach is guaranteed to collect all the correctly-predicted instances from the noisy pseudo-labeled set.



### Tell me what this is: Few-Shot Incremental Object Learning by a Robot
- **Arxiv ID**: http://arxiv.org/abs/2008.00819v1
- **DOI**: 10.1109/IROS45743.2020.9341140
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2008.00819v1)
- **Published**: 2020-07-15 04:42:14+00:00
- **Updated**: 2020-07-15 04:42:14+00:00
- **Authors**: Ali Ayub, Alan R. Wagner
- **Comment**: Accepted at IEEE IROS 2020
- **Journal**: None
- **Summary**: For many applications, robots will need to be incrementally trained to recognize the specific objects needed for an application. This paper presents a practical system for incrementally training a robot to recognize different object categories using only a small set of visual examples provided by a human. The paper uses a recently developed state-of-the-art method for few-shot incremental learning of objects. After learning the object classes incrementally, the robot performs a table cleaning task organizing objects into categories specified by the human. We also demonstrate the system's ability to learn arrangements of objects and predict missing or incorrectly placed objects. Experimental evaluations demonstrate that our approach achieves nearly the same performance as a system trained with all examples at one time (batch training), which constitutes a theoretical upper bound.



### Explaining Deep Neural Networks using Unsupervised Clustering
- **Arxiv ID**: http://arxiv.org/abs/2007.07477v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.07477v2)
- **Published**: 2020-07-15 04:49:43+00:00
- **Updated**: 2020-07-16 00:50:15+00:00
- **Authors**: Yu-han Liu, Sercan O. Arik
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel method to explain trained deep neural networks (DNNs), by distilling them into surrogate models using unsupervised clustering. Our method can be applied flexibly to any subset of layers of a DNN architecture and can incorporate low-level and high-level information. On image datasets given pre-trained DNNs, we demonstrate the strength of our method in finding similar training samples, and shedding light on the concepts the DNNs base their decisions on. Via user studies, we show that our model can improve the user trust in model's prediction.



### Decoding CNN based Object Classifier Using Visualization
- **Arxiv ID**: http://arxiv.org/abs/2007.07482v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, I.4; I.2
- **Links**: [PDF](http://arxiv.org/pdf/2007.07482v1)
- **Published**: 2020-07-15 05:01:27+00:00
- **Updated**: 2020-07-15 05:01:27+00:00
- **Authors**: Abhishek Mukhopadhyay, Imon Mukherjee, Pradipta Biswas
- **Comment**: Accepted at ACM International conference on Automotive User Interface
  2020
- **Journal**: None
- **Summary**: This paper investigates how working of Convolutional Neural Network (CNN) can be explained through visualization in the context of machine perception of autonomous vehicles. We visualize what type of features are extracted in different convolution layers of CNN that helps to understand how CNN gradually increases spatial information in every layer. Thus, it concentrates on region of interests in every transformation. Visualizing heat map of activation helps us to understand how CNN classifies and localizes different objects in image. This study also helps us to reason behind low accuracy of a model helps to increase trust on object detection module.



### Monocular Retinal Depth Estimation and Joint Optic Disc and Cup Segmentation using Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2007.07502v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.07502v1)
- **Published**: 2020-07-15 06:21:46+00:00
- **Updated**: 2020-07-15 06:21:46+00:00
- **Authors**: Sharath M Shankaranarayana, Keerthi Ram, Kaushik Mitra, Mohanasankar Sivaprakasam
- **Comment**: None
- **Journal**: None
- **Summary**: One of the important parameters for the assessment of glaucoma is optic nerve head (ONH) evaluation, which usually involves depth estimation and subsequent optic disc and cup boundary extraction. Depth is usually obtained explicitly from imaging modalities like optical coherence tomography (OCT) and is very challenging to estimate depth from a single RGB image. To this end, we propose a novel method using adversarial network to predict depth map from a single image. The proposed depth estimation technique is trained and evaluated using individual retinal images from INSPIRE-stereo dataset. We obtain a very high average correlation coefficient of 0.92 upon five fold cross validation outperforming the state of the art. We then use the depth estimation process as a proxy task for joint optic disc and cup segmentation.



### Learning Visual Context by Comparison
- **Arxiv ID**: http://arxiv.org/abs/2007.07506v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.07506v1)
- **Published**: 2020-07-15 06:47:06+00:00
- **Updated**: 2020-07-15 06:47:06+00:00
- **Authors**: Minchul Kim, Jongchan Park, Seil Na, Chang Min Park, Donggeun Yoo
- **Comment**: ECCV 2020 spotlight paper
- **Journal**: None
- **Summary**: Finding diseases from an X-ray image is an important yet highly challenging task. Current methods for solving this task exploit various characteristics of the chest X-ray image, but one of the most important characteristics is still missing: the necessity of comparison between related regions in an image. In this paper, we present Attend-and-Compare Module (ACM) for capturing the difference between an object of interest and its corresponding context. We show that explicit difference modeling can be very helpful in tasks that require direct comparison between locations from afar. This module can be plugged into existing deep learning models. For evaluation, we apply our module to three chest X-ray recognition tasks and COCO object detection & segmentation tasks and observe consistent improvements across tasks. The code is available at https://github.com/mk-minchul/attend-and-compare.



### Channel-Level Variable Quantization Network for Deep Image Compression
- **Arxiv ID**: http://arxiv.org/abs/2007.12619v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.12619v1)
- **Published**: 2020-07-15 07:20:39+00:00
- **Updated**: 2020-07-15 07:20:39+00:00
- **Authors**: Zhisheng Zhong, Hiroaki Akutsu, Kiyoharu Aizawa
- **Comment**: Proceedings of International Joint Conference on Artificial
  Intelligence (IJCAI), 2020
- **Journal**: None
- **Summary**: Deep image compression systems mainly contain four components: encoder, quantizer, entropy model, and decoder. To optimize these four components, a joint rate-distortion framework was proposed, and many deep neural network-based methods achieved great success in image compression. However, almost all convolutional neural network-based methods treat channel-wise feature maps equally, reducing the flexibility in handling different types of information. In this paper, we propose a channel-level variable quantization network to dynamically allocate more bitrates for significant channels and withdraw bitrates for negligible channels. Specifically, we propose a variable quantization controller. It consists of two key components: the channel importance module, which can dynamically learn the importance of channels during training, and the splitting-merging module, which can allocate different bitrates for different channels. We also formulate the quantizer into a Gaussian mixture model manner. Quantitative and qualitative experiments verify the effectiveness of the proposed model and demonstrate that our method achieves superior performance and can produce much better visual reconstructions.



### Learning with Privileged Information for Efficient Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2007.07524v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.07524v1)
- **Published**: 2020-07-15 07:44:18+00:00
- **Updated**: 2020-07-15 07:44:18+00:00
- **Authors**: Wonkyung Lee, Junghyup Lee, Dohyung Kim, Bumsub Ham
- **Comment**: ECCV-2020
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) have allowed remarkable advances in single image super-resolution (SISR) over the last decade. Most SR methods based on CNNs have focused on achieving performance gains in terms of quality metrics, such as PSNR and SSIM, over classical approaches. They typically require a large amount of memory and computational units. FSRCNN, consisting of few numbers of convolutional layers, has shown promising results, while using an extremely small number of network parameters. We introduce in this paper a novel distillation framework, consisting of teacher and student networks, that allows to boost the performance of FSRCNN drastically. To this end, we propose to use ground-truth high-resolution (HR) images as privileged information. The encoder in the teacher learns the degradation process, subsampling of HR images, using an imitation loss. The student and the decoder in the teacher, having the same network architecture as FSRCNN, try to reconstruct HR images. Intermediate features in the decoder, affordable for the student to learn, are transferred to the student through feature distillation. Experimental results on standard benchmarks demonstrate the effectiveness and the generalization ability of our framework, which significantly boosts the performance of FSRCNN as well as other SR methods. Our code and model are available online: https://cvlab.yonsei.ac.kr/projects/PISR.



### Dive Deeper Into Box for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2007.14350v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.14350v1)
- **Published**: 2020-07-15 07:49:05+00:00
- **Updated**: 2020-07-15 07:49:05+00:00
- **Authors**: Ran Chen, Yong Liu, Mengdan Zhang, Shu Liu, Bei Yu, Yu-Wing Tai
- **Comment**: Accepted by ECCV 2020
- **Journal**: None
- **Summary**: Anchor free methods have defined the new frontier in state-of-the-art object detection researches where accurate bounding box estimation is the key to the success of these methods. However, even the bounding box has the highest confidence score, it is still far from perfect at localization. To this end, we propose a box reorganization method(DDBNet), which can dive deeper into the box for more accurate localization. At the first step, drifted boxes are filtered out because the contents in these boxes are inconsistent with target semantics. Next, the selected boxes are broken into boundaries, and the well-aligned boundaries are searched and grouped into a sort of optimal boxes toward tightening instances more precisely. Experimental results show that our method is effective which leads to state-of-the-art performance for object detection.



### Learning to Parse Wireframes in Images of Man-Made Environments
- **Arxiv ID**: http://arxiv.org/abs/2007.07527v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.07527v1)
- **Published**: 2020-07-15 07:54:18+00:00
- **Updated**: 2020-07-15 07:54:18+00:00
- **Authors**: Kun Huang, Yifan Wang, Zihan Zhou, Tianjiao Ding, Shenghua Gao, Yi Ma
- **Comment**: CVPR 2018
- **Journal**: IEEE Conference on Computer Vision and Pattern Recognition (2018)
  626-635
- **Summary**: In this paper, we propose a learning-based approach to the task of automatically extracting a "wireframe" representation for images of cluttered man-made environments. The wireframe (see Fig. 1) contains all salient straight lines and their junctions of the scene that encode efficiently and accurately large-scale geometry and object shapes. To this end, we have built a very large new dataset of over 5,000 images with wireframes thoroughly labelled by humans. We have proposed two convolutional neural networks that are suitable for extracting junctions and lines with large spatial support, respectively. The networks trained on our dataset have achieved significantly better performance than state-of-the-art methods for junction detection and line segment detection, respectively. We have conducted extensive experiments to evaluate quantitatively and qualitatively the wireframes obtained by our method, and have convincingly shown that effectively and efficiently parsing wireframes for images of man-made environments is a feasible goal within reach. Such wireframes could benefit many important visual tasks such as feature correspondence, 3D reconstruction, vision-based mapping, localization, and navigation. The data and source code are available at https://github.com/huangkuns/wireframe.



### RobustScanner: Dynamically Enhancing Positional Clues for Robust Text Recognition
- **Arxiv ID**: http://arxiv.org/abs/2007.07542v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.07542v2)
- **Published**: 2020-07-15 08:37:40+00:00
- **Updated**: 2020-07-17 07:16:45+00:00
- **Authors**: Xiaoyu Yue, Zhanghui Kuang, Chenhao Lin, Hongbin Sun, Wayne Zhang
- **Comment**: Accepted to ECCV 2020
- **Journal**: None
- **Summary**: The attention-based encoder-decoder framework has recently achieved impressive results for scene text recognition, and many variants have emerged with improvements in recognition quality. However, it performs poorly on contextless texts (e.g., random character sequences) which is unacceptable in most of real application scenarios. In this paper, we first deeply investigate the decoding process of the decoder. We empirically find that a representative character-level sequence decoder utilizes not only context information but also positional information. Contextual information, which the existing approaches heavily rely on, causes the problem of attention drift. To suppress such side-effect, we propose a novel position enhancement branch, and dynamically fuse its outputs with those of the decoder attention module for scene text recognition. Specifically, it contains a position aware module to enable the encoder to output feature vectors encoding their own spatial positions, and an attention module to estimate glimpses using the positional clue (i.e., the current decoding time step) only. The dynamic fusion is conducted for more robust feature via an element-wise gate mechanism. Theoretically, our proposed method, dubbed \emph{RobustScanner}, decodes individual characters with dynamic ratio between context and positional clues, and utilizes more positional ones when the decoding sequences with scarce context, and thus is robust and practical. Empirically, it has achieved new state-of-the-art results on popular regular and irregular text recognition benchmarks while without much performance drop on contextless benchmarks, validating its robustness in both contextual and contextless application scenarios.



### Evaluation of Neural Network Classification Systems on Document Stream
- **Arxiv ID**: http://arxiv.org/abs/2007.07547v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, I.7.1; J.1
- **Links**: [PDF](http://arxiv.org/pdf/2007.07547v1)
- **Published**: 2020-07-15 08:52:39+00:00
- **Updated**: 2020-07-15 08:52:39+00:00
- **Authors**: Joris Voerman, Aurelie Joseph, Mickael Coustaty, Vincent Poulain d Andecy, Jean-Marc Ogier
- **Comment**: 15 pages, 3 figures and submitted to DAS conferences 2020
- **Journal**: None
- **Summary**: One major drawback of state of the art Neural Networks (NN)-based approaches for document classification purposes is the large number of training samples required to obtain an efficient classification. The minimum required number is around one thousand annotated documents for each class. In many cases it is very difficult, if not impossible, to gather this number of samples in real industrial processes. In this paper, we analyse the efficiency of NN-based document classification systems in a sub-optimal training case, based on the situation of a company document stream. We evaluated three different approaches, one based on image content and two on textual content. The evaluation was divided into four parts: a reference case, to assess the performance of the system in the lab; two cases that each simulate a specific difficulty linked to document stream processing; and a realistic case that combined all of these difficulties. The realistic case highlighted the fact that there is a significant drop in the efficiency of NN-Based document classification systems. Although they remain efficient for well represented classes (with an over-fitting of the system for those classes), it is impossible for them to handle appropriately less well represented classes. NN-Based document classification systems need to be adapted to resolve these two problems before they can be considered for use in a company document stream.



### Partially Supervised Multi-Task Network for Single-View Dietary Assessment
- **Arxiv ID**: http://arxiv.org/abs/2008.00818v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.00818v1)
- **Published**: 2020-07-15 08:53:05+00:00
- **Updated**: 2020-07-15 08:53:05+00:00
- **Authors**: Ya Lu, Thomai Stathopoulou, Stavroula Mougiakakou
- **Comment**: None
- **Journal**: None
- **Summary**: Food volume estimation is an essential step in the pipeline of dietary assessment and demands the precise depth estimation of the food surface and table plane. Existing methods based on computer vision require either multi-image input or additional depth maps, reducing convenience of implementation and practical significance. Despite the recent advances in unsupervised depth estimation from a single image, the achieved performance in the case of large texture-less areas needs to be improved. In this paper, we propose a network architecture that jointly performs geometric understanding (i.e., depth prediction and 3D plane estimation) and semantic prediction on a single food image, enabling a robust and accurate food volume estimation regardless of the texture characteristics of the target plane. For the training of the network, only monocular videos with semantic ground truth are required, while the depth map and 3D plane ground truth are no longer needed. Experimental results on two separate food image databases demonstrate that our method performs robustly on texture-less scenarios and is superior to unsupervised networks and structure from motion based approaches, while it achieves comparable performance to fully-supervised methods.



### Learning Part Boundaries from 3D Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2007.07563v1
- **DOI**: 10.1111/cgf.14078
- **Categories**: **cs.CV**, cs.CG
- **Links**: [PDF](http://arxiv.org/pdf/2007.07563v1)
- **Published**: 2020-07-15 09:24:09+00:00
- **Updated**: 2020-07-15 09:24:09+00:00
- **Authors**: Marios Loizou, Melinos Averkiou, Evangelos Kalogerakis
- **Comment**: Appeared in Eurographics Symposium on Geometry Processing 2020
- **Journal**: None
- **Summary**: We present a method that detects boundaries of parts in 3D shapes represented as point clouds. Our method is based on a graph convolutional network architecture that outputs a probability for a point to lie in an area that separates two or more parts in a 3D shape. Our boundary detector is quite generic: it can be trained to localize boundaries of semantic parts or geometric primitives commonly used in 3D modeling. Our experiments demonstrate that our method can extract more accurate boundaries that are closer to ground-truth ones compared to alternatives. We also demonstrate an application of our network to fine-grained semantic shape segmentation, where we also show improvements in terms of part labeling performance.



### P2D: a self-supervised method for depth estimation from polarimetry
- **Arxiv ID**: http://arxiv.org/abs/2007.07567v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.07567v1)
- **Published**: 2020-07-15 09:32:53+00:00
- **Updated**: 2020-07-15 09:32:53+00:00
- **Authors**: Marc Blanchon, Désiré Sidibé, Olivier Morel, Ralph Seulin, Daniel Braun, Fabrice Meriaudeau
- **Comment**: 8 pages, submitted to ICPR2020 second round
- **Journal**: None
- **Summary**: Monocular depth estimation is a recurring subject in the field of computer vision. Its ability to describe scenes via a depth map while reducing the constraints related to the formulation of perspective geometry tends to favor its use. However, despite the constant improvement of algorithms, most methods exploit only colorimetric information. Consequently, robustness to events to which the modality is not sensitive to, like specularity or transparency, is neglected. In response to this phenomenon, we propose using polarimetry as an input for a self-supervised monodepth network. Therefore, we propose exploiting polarization cues to encourage accurate reconstruction of scenes. Furthermore, we include a term of polarimetric regularization to state-of-the-art method to take specific advantage of the data. Our method is evaluated both qualitatively and quantitatively demonstrating that the contribution of this new information as well as an enhanced loss function improves depth estimation results, especially for specular areas.



### CycAs: Self-supervised Cycle Association for Learning Re-identifiable Descriptions
- **Arxiv ID**: http://arxiv.org/abs/2007.07577v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.07577v1)
- **Published**: 2020-07-15 09:52:35+00:00
- **Updated**: 2020-07-15 09:52:35+00:00
- **Authors**: Zhongdao Wang, Jingwei Zhang, Liang Zheng, Yixuan Liu, Yifan Sun, Yali Li, Shengjin Wang
- **Comment**: Accepted to ECCV 2020
- **Journal**: None
- **Summary**: This paper proposes a self-supervised learning method for the person re-identification (re-ID) problem, where existing unsupervised methods usually rely on pseudo labels, such as those from video tracklets or clustering. A potential drawback of using pseudo labels is that errors may accumulate and it is challenging to estimate the number of pseudo IDs. We introduce a different unsupervised method that allows us to learn pedestrian embeddings from raw videos, without resorting to pseudo labels. The goal is to construct a self-supervised pretext task that matches the person re-ID objective. Inspired by the \emph{data association} concept in multi-object tracking, we propose the \textbf{Cyc}le \textbf{As}sociation (\textbf{CycAs}) task: after performing data association between a pair of video frames forward and then backward, a pedestrian instance is supposed to be associated to itself. To fulfill this goal, the model must learn a meaningful representation that can well describe correspondences between instances in frame pairs. We adapt the discrete association process to a differentiable form, such that end-to-end training becomes feasible. Experiments are conducted in two aspects: We first compare our method with existing unsupervised re-ID methods on seven benchmarks and demonstrate CycAs' superiority. Then, to further validate the practical value of CycAs in real-world applications, we perform training on self-collected videos and report promising performance on standard test sets.



### Augmented Bi-path Network for Few-shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2007.07614v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.07614v1)
- **Published**: 2020-07-15 11:13:38+00:00
- **Updated**: 2020-07-15 11:13:38+00:00
- **Authors**: Baoming Yan, Chen Zhou, Bo Zhao, Kan Guo, Jiang Yang, Xiaobo Li, Ming Zhang, Yizhou Wang
- **Comment**: None
- **Journal**: International Conference on Pattern Recognition 2020
- **Summary**: Few-shot Learning (FSL) which aims to learn from few labeled training data is becoming a popular research topic, due to the expensive labeling cost in many real-world applications. One kind of successful FSL method learns to compare the testing (query) image and training (support) image by simply concatenating the features of two images and feeding it into the neural network. However, with few labeled data in each class, the neural network has difficulty in learning or comparing the local features of two images. Such simple image-level comparison may cause serious mis-classification. To solve this problem, we propose Augmented Bi-path Network (ABNet) for learning to compare both global and local features on multi-scales. Specifically, the salient patches are extracted and embedded as the local features for every image. Then, the model learns to augment the features for better robustness. Finally, the model learns to compare global and local features separately, i.e., in two paths, before merging the similarities. Extensive experiments show that the proposed ABNet outperforms the state-of-the-art methods. Both quantitative and visual ablation studies are provided to verify that the proposed modules lead to more precise comparison results.



### SpaceNet: Make Free Space For Continual Learning
- **Arxiv ID**: http://arxiv.org/abs/2007.07617v3
- **DOI**: 10.1016/j.neucom.2021.01.078
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.07617v3)
- **Published**: 2020-07-15 11:21:31+00:00
- **Updated**: 2021-04-14 08:39:33+00:00
- **Authors**: Ghada Sokar, Decebal Constantin Mocanu, Mykola Pechenizkiy
- **Comment**: Published in Neurocomputing Journal
- **Journal**: Neurocomputing, 439: 1-11, 2021
- **Summary**: The continual learning (CL) paradigm aims to enable neural networks to learn tasks continually in a sequential fashion. The fundamental challenge in this learning paradigm is catastrophic forgetting previously learned tasks when the model is optimized for a new task, especially when their data is not accessible. Current architectural-based methods aim at alleviating the catastrophic forgetting problem but at the expense of expanding the capacity of the model. Regularization-based methods maintain a fixed model capacity; however, previous studies showed the huge performance degradation of these methods when the task identity is not available during inference (e.g. class incremental learning scenario). In this work, we propose a novel architectural-based method referred as SpaceNet for class incremental learning scenario where we utilize the available fixed capacity of the model intelligently. SpaceNet trains sparse deep neural networks from scratch in an adaptive way that compresses the sparse connections of each task in a compact number of neurons. The adaptive training of the sparse connections results in sparse representations that reduce the interference between the tasks. Experimental results show the robustness of our proposed method against catastrophic forgetting old tasks and the efficiency of SpaceNet in utilizing the available capacity of the model, leaving space for more tasks to be learned. In particular, when SpaceNet is tested on the well-known benchmarks for CL: split MNIST, split Fashion-MNIST, and CIFAR-10/100, it outperforms regularization-based methods by a big performance gap. Moreover, it achieves better performance than architectural-based methods without model expansion and achieved comparable results with rehearsal-based methods, while offering a huge memory reduction.



### Temporal Distinct Representation Learning for Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2007.07626v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.07626v1)
- **Published**: 2020-07-15 11:30:40+00:00
- **Updated**: 2020-07-15 11:30:40+00:00
- **Authors**: Junwu Weng, Donghao Luo, Yabiao Wang, Ying Tai, Chengjie Wang, Jilin Li, Feiyue Huang, Xudong Jiang, Junsong Yuan
- **Comment**: 16 pages, 4 figures, 7 tables
- **Journal**: None
- **Summary**: Motivated by the previous success of Two-Dimensional Convolutional Neural Network (2D CNN) on image recognition, researchers endeavor to leverage it to characterize videos. However, one limitation of applying 2D CNN to analyze videos is that different frames of a video share the same 2D CNN kernels, which may result in repeated and redundant information utilization, especially in the spatial semantics extraction process, hence neglecting the critical variations among frames. In this paper, we attempt to tackle this issue through two ways. 1) Design a sequential channel filtering mechanism, i.e., Progressive Enhancement Module (PEM), to excite the discriminative channels of features from different frames step by step, and thus avoid repeated information extraction. 2) Create a Temporal Diversity Loss (TD Loss) to force the kernels to concentrate on and capture the variations among frames rather than the image regions with similar appearance. Our method is evaluated on benchmark temporal reasoning datasets Something-Something V1 and V2, and it achieves visible improvements over the best competitor by 2.4% and 1.3%, respectively. Besides, performance improvements over the 2D-CNN-based state-of-the-arts on the large-scale dataset Kinetics are also witnessed.



### Fast and Robust Iterative Closest Point
- **Arxiv ID**: http://arxiv.org/abs/2007.07627v3
- **DOI**: 10.1109/TPAMI.2021.3054619
- **Categories**: **cs.CV**, cs.GR, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2007.07627v3)
- **Published**: 2020-07-15 11:32:53+00:00
- **Updated**: 2023-04-14 19:47:28+00:00
- **Authors**: Juyong Zhang, Yuxin Yao, Bailin Deng
- **Comment**: None
- **Journal**: IEEE Transactions on Pattern Analysis and Machine Intelligence,
  2021
- **Summary**: The Iterative Closest Point (ICP) algorithm and its variants are a fundamental technique for rigid registration between two point sets, with wide applications in different areas from robotics to 3D reconstruction. The main drawbacks for ICP are its slow convergence as well as its sensitivity to outliers, missing data, and partial overlaps. Recent work such as Sparse ICP achieves robustness via sparsity optimization at the cost of computational speed. In this paper, we propose a new method for robust registration with fast convergence. First, we show that the classical point-to-point ICP can be treated as a majorization-minimization (MM) algorithm, and propose an Anderson acceleration approach to speed up its convergence. In addition, we introduce a robust error metric based on the Welsch's function, which is minimized efficiently using the MM algorithm with Anderson acceleration. On challenging datasets with noises and partial overlaps, we achieve similar or better accuracy than Sparse ICP while being at least an order of magnitude faster. Finally, we extend the robust formulation to point-to-plane ICP, and solve the resulting problem using a similar Anderson-accelerated MM strategy. Our robust ICP methods improve the registration accuracy on benchmark datasets while being competitive in computational time.



### Visualizing Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/2007.07628v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.07628v1)
- **Published**: 2020-07-15 11:34:46+00:00
- **Updated**: 2020-07-15 11:34:46+00:00
- **Authors**: Róbert Szabó, Dániel Katona, Márton Csillag, Adrián Csiszárik, Dániel Varga
- **Comment**: 2020 ICML Workshop on Human Interpretability in Machine Learning (WHI
  2020)
- **Journal**: None
- **Summary**: We provide visualizations of individual neurons of a deep image recognition network during the temporal process of transfer learning. These visualizations qualitatively demonstrate various novel properties of the transfer learning process regarding the speed and characteristics of adaptation, neuron reuse, spatial scale of the represented image features, and behavior of transfer learning to small data. We publish the large-scale dataset that we have created for the purposes of this analysis.



### Learning Multiplicative Interactions with Bayesian Neural Networks for Visual-Inertial Odometry
- **Arxiv ID**: http://arxiv.org/abs/2007.07630v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.07630v1)
- **Published**: 2020-07-15 11:39:29+00:00
- **Updated**: 2020-07-15 11:39:29+00:00
- **Authors**: Kashmira Shinde, Jongseok Lee, Matthias Humt, Aydin Sezgin, Rudolph Triebel
- **Comment**: Published at Workshop on AI for Autonomous Driving (AIAD), the 37th
  International Conference on Machine Learning, Vienna, Austria, 2020
- **Journal**: None
- **Summary**: This paper presents an end-to-end multi-modal learning approach for monocular Visual-Inertial Odometry (VIO), which is specifically designed to exploit sensor complementarity in the light of sensor degradation scenarios. The proposed network makes use of a multi-head self-attention mechanism that learns multiplicative interactions between multiple streams of information. Another design feature of our approach is the incorporation of the model uncertainty using scalable Laplace Approximation. We evaluate the performance of the proposed approach by comparing it against the end-to-end state-of-the-art methods on the KITTI dataset and show that it achieves superior performance. Importantly, our work thereby provides an empirical evidence that learning multiplicative interactions can result in a powerful inductive bias for increased robustness to sensor failures.



### Learning to Learn with Variational Information Bottleneck for Domain Generalization
- **Arxiv ID**: http://arxiv.org/abs/2007.07645v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.07645v1)
- **Published**: 2020-07-15 12:05:52+00:00
- **Updated**: 2020-07-15 12:05:52+00:00
- **Authors**: Yingjun Du, Jun Xu, Huan Xiong, Qiang Qiu, Xiantong Zhen, Cees G. M. Snoek, Ling Shao
- **Comment**: 15 pages, 4 figures, ECCV2020
- **Journal**: None
- **Summary**: Domain generalization models learn to generalize to previously unseen domains, but suffer from prediction uncertainty and domain shift. In this paper, we address both problems. We introduce a probabilistic meta-learning model for domain generalization, in which classifier parameters shared across domains are modeled as distributions. This enables better handling of prediction uncertainty on unseen domains. To deal with domain shift, we learn domain-invariant representations by the proposed principle of meta variational information bottleneck, we call MetaVIB. MetaVIB is derived from novel variational bounds of mutual information, by leveraging the meta-learning setting of domain generalization. Through episodic training, MetaVIB learns to gradually narrow domain gaps to establish domain-invariant representations, while simultaneously maximizing prediction accuracy. We conduct experiments on three benchmarks for cross-domain visual recognition. Comprehensive ablation studies validate the benefits of MetaVIB for domain generalization. The comparison results demonstrate our method outperforms previous approaches consistently.



### Retinal Image Segmentation with a Structure-Texture Demixing Network
- **Arxiv ID**: http://arxiv.org/abs/2008.00817v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.00817v1)
- **Published**: 2020-07-15 12:19:03+00:00
- **Updated**: 2020-07-15 12:19:03+00:00
- **Authors**: Shihao Zhang, Huazhu Fu, Yanwu Xu, Yanxia Liu, Mingkui Tan
- **Comment**: Accepted to MICCAI 2020
- **Journal**: None
- **Summary**: Retinal image segmentation plays an important role in automatic disease diagnosis. This task is very challenging because the complex structure and texture information are mixed in a retinal image, and distinguishing the information is difficult. Existing methods handle texture and structure jointly, which may lead biased models toward recognizing textures and thus results in inferior segmentation performance. To address it, we propose a segmentation strategy that seeks to separate structure and texture components and significantly improve the performance. To this end, we design a structure-texture demixing network (STD-Net) that can process structures and textures differently and better. Extensive experiments on two retinal image segmentation tasks (i.e., blood vessel segmentation, optic disc and cup segmentation) demonstrate the effectiveness of the proposed method.



### End-to-end training of a two-stage neural network for defect detection
- **Arxiv ID**: http://arxiv.org/abs/2007.07676v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.07676v1)
- **Published**: 2020-07-15 13:42:26+00:00
- **Updated**: 2020-07-15 13:42:26+00:00
- **Authors**: Jakob Božič, Domen Tabernik, Danijel Skočaj
- **Comment**: None
- **Journal**: None
- **Summary**: Segmentation-based, two-stage neural network has shown excellent results in the surface defect detection, enabling the network to learn from a relatively small number of samples. In this work, we introduce end-to-end training of the two-stage network together with several extensions to the training process, which reduce the amount of training time and improve the results on the surface defect detection tasks. To enable end-to-end training we carefully balance the contributions of both the segmentation and the classification loss throughout the learning. We adjust the gradient flow from the classification into the segmentation network in order to prevent the unstable features from corrupting the learning. As an additional extension to the learning, we propose frequency-of-use sampling scheme of negative samples to address the issue of over- and under-sampling of images during the training, while we employ the distance transform algorithm on the region-based segmentation masks as weights for positive pixels, giving greater importance to areas with higher probability of presence of defect without requiring a detailed annotation. We demonstrate the performance of the end-to-end training scheme and the proposed extensions on three defect detection datasets - DAGM, KolektorSDD and Severstal Steel defect dataset - where we show state-of-the-art results. On the DAGM and the KolektorSDD we demonstrate 100\% detection rate, therefore completely solving the datasets. Additional ablation study performed on all three datasets quantitatively demonstrates the contribution to the overall result improvements for each of the proposed extensions.



### Relative Pose Estimation of Calibrated Cameras with Known $\mathrm{SE}(3)$ Invariants
- **Arxiv ID**: http://arxiv.org/abs/2007.07686v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.07686v1)
- **Published**: 2020-07-15 13:55:55+00:00
- **Updated**: 2020-07-15 13:55:55+00:00
- **Authors**: Bo Li, Evgeniy Martyushev, Gim Hee Lee
- **Comment**: None
- **Journal**: None
- **Summary**: The $\mathrm{SE}(3)$ invariants of a pose include its rotation angle and screw translation. In this paper, we present a complete comprehensive study of the relative pose estimation problem for a calibrated camera constrained by known $\mathrm{SE}(3)$ invariant, which involves 5 minimal problems in total. These problems reduces the minimal number of point pairs for relative pose estimation and improves the estimation efficiency and robustness. The $\mathrm{SE}(3)$ invariant constraints can come from extra sensor measurements or motion assumption. Different from conventional relative pose estimation with extra constraints, no extrinsic calibration is required to transform the constraints to the camera frame. This advantage comes from the invariance of $\mathrm{SE}(3)$ invariants cross different coordinate systems on a rigid body and makes the solvers more convenient and flexible in practical applications.   Besides proposing the concept of relative pose estimation constrained by $\mathrm{SE}(3)$ invariants, we present a comprehensive study of existing polynomial formulations for relative pose estimation and discover their relationship. Different formulations are carefully chosen for each proposed problems to achieve best efficiency. Experiments on synthetic and real data shows performance improvement compared to conventional relative pose estimation methods.



### Proof of Concept: Automatic Type Recognition
- **Arxiv ID**: http://arxiv.org/abs/2007.07690v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.07690v2)
- **Published**: 2020-07-15 13:58:27+00:00
- **Updated**: 2020-10-20 11:46:11+00:00
- **Authors**: Vincent Christlein, Nikolaus Weichselbaumer, Saskia Limbach, Mathias Seuret
- **Comment**: InfDH 2020
- **Journal**: None
- **Summary**: The type used to print an early modern book can give scholars valuable information about the time and place of its production as well as its producer. Recognizing such type is currently done manually using both the character shapes of `M' or `Qu' and the size of the total type to look it up in a large reference work. This is a reliable method, but it is also slow and requires specific skills. We investigate the performance of type classification and type retrieval using a newly created dataset consisting of easy and difficult types used in early printed books. For type classification, we rely on a deep Convolutional Neural Network (CNN) originally used for font-group classification while we use a common writer identification method for the retrieval case. We show that in both scenarios, easy types can be classified/retrieved with a high accuracy while difficult cases are indeed difficult.



### P$^{2}$Net: Patch-match and Plane-regularization for Unsupervised Indoor Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/2007.07696v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.07696v1)
- **Published**: 2020-07-15 14:10:43+00:00
- **Updated**: 2020-07-15 14:10:43+00:00
- **Authors**: Zehao Yu, Lei Jin, Shenghua Gao
- **Comment**: Accepted by ECCV2020
- **Journal**: None
- **Summary**: This paper tackles the unsupervised depth estimation task in indoor environments. The task is extremely challenging because of the vast areas of non-texture regions in these scenes. These areas could overwhelm the optimization process in the commonly used unsupervised depth estimation framework proposed for outdoor environments. However, even when those regions are masked out, the performance is still unsatisfactory. In this paper, we argue that the poor performance suffers from the non-discriminative point-based matching. To this end, we propose P$^2$Net. We first extract points with large local gradients and adopt patches centered at each point as its representation. Multiview consistency loss is then defined over patches. This operation significantly improves the robustness of the network training. Furthermore, because those textureless regions in indoor scenes (e.g., wall, floor, roof, \etc) usually correspond to planar regions, we propose to leverage superpixels as a plane prior. We enforce the predicted depth to be well fitted by a plane within each superpixel. Extensive experiments on NYUv2 and ScanNet show that our P$^2$Net outperforms existing approaches by a large margin. Code is available at \url{https://github.com/svip-lab/Indoor-SfMLearner}.



### Facial Recognition: A cross-national Survey on Public Acceptance, Privacy, and Discrimination
- **Arxiv ID**: http://arxiv.org/abs/2008.07275v1
- **DOI**: None
- **Categories**: **cs.CY**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2008.07275v1)
- **Published**: 2020-07-15 14:17:21+00:00
- **Updated**: 2020-07-15 14:17:21+00:00
- **Authors**: Léa Steinacker, Miriam Meckel, Genia Kostka, Damian Borth
- **Comment**: ICML 2020 - Law and Machine Learning Workshop, Vienna, Austria
- **Journal**: None
- **Summary**: With rapid advances in machine learning (ML), more of this technology is being deployed into the real world interacting with us and our environment. One of the most widely applied application of ML is facial recognition as it is running on millions of devices. While being useful for some people, others perceive it as a threat when used by public authorities. This discrepancy and the lack of policy increases the uncertainty in the ML community about the future direction of facial recognition research and development. In this paper we present results from a cross-national survey about public acceptance, privacy, and discrimination of the use of facial recognition technology (FRT) in the public. This study provides insights about the opinion towards FRT from China, Germany, the United Kingdom (UK), and the United States (US), which can serve as input for policy makers and legal regulators.



### Lunar Terrain Relative Navigation Using a Convolutional Neural Network for Visual Crater Detection
- **Arxiv ID**: http://arxiv.org/abs/2007.07702v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO, cs.SY, eess.IV, eess.SY
- **Links**: [PDF](http://arxiv.org/pdf/2007.07702v1)
- **Published**: 2020-07-15 14:19:27+00:00
- **Updated**: 2020-07-15 14:19:27+00:00
- **Authors**: Lena M. Downes, Ted J. Steiner, Jonathan P. How
- **Comment**: 6 pages, 4 figures. This work was accepted by the 2020 American
  Control Conference
- **Journal**: None
- **Summary**: Terrain relative navigation can improve the precision of a spacecraft's position estimate by detecting global features that act as supplementary measurements to correct for drift in the inertial navigation system. This paper presents a system that uses a convolutional neural network (CNN) and image processing methods to track the location of a simulated spacecraft with an extended Kalman filter (EKF). The CNN, called LunaNet, visually detects craters in the simulated camera frame and those detections are matched to known lunar craters in the region of the current estimated spacecraft position. These matched craters are treated as features that are tracked using the EKF. LunaNet enables more reliable position tracking over a simulated trajectory due to its greater robustness to changes in image brightness and more repeatable crater detections from frame to frame throughout a trajectory. LunaNet combined with an EKF produces a decrease of 60% in the average final position estimation error and a decrease of 25% in average final velocity estimation error compared to an EKF using an image processing-based crater detection method when tested on trajectories using images of standard brightness.



### PVSNet: Pixelwise Visibility-Aware Multi-View Stereo Network
- **Arxiv ID**: http://arxiv.org/abs/2007.07714v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.07714v1)
- **Published**: 2020-07-15 14:39:49+00:00
- **Updated**: 2020-07-15 14:39:49+00:00
- **Authors**: Qingshan Xu, Wenbing Tao
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, learning-based multi-view stereo methods have achieved promising results. However, they all overlook the visibility difference among different views, which leads to an indiscriminate multi-view similarity definition and greatly limits their performance on datasets with strong viewpoint variations. In this paper, a Pixelwise Visibility-aware multi-view Stereo Network (PVSNet) is proposed for robust dense 3D reconstruction. We present a pixelwise visibility network to learn the visibility information for different neighboring images before computing the multi-view similarity, and then construct an adaptive weighted cost volume with the visibility information. Moreover, we present an anti-noise training strategy that introduces disturbing views during model training to make the pixelwise visibility network more distinguishable to unrelated views, which is different with the existing learning methods that only use two best neighboring views for training. To the best of our knowledge, PVSNet is the first deep learning framework that is able to capture the visibility information of different neighboring views. In this way, our method can be generalized well to different types of datasets, especially the ETH3D high-res benchmark with strong viewpoint variations. Extensive experiments show that PVSNet achieves the state-of-the-art performance on different datasets.



### Focus-and-Expand: Training Guidance Through Gradual Manipulation of Input Features
- **Arxiv ID**: http://arxiv.org/abs/2007.07723v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.07723v1)
- **Published**: 2020-07-15 14:49:56+00:00
- **Updated**: 2020-07-15 14:49:56+00:00
- **Authors**: Moab Arar, Noa Fish, Dani Daniel, Evgeny Tenetov, Ariel Shamir, Amit Bermano
- **Comment**: None
- **Journal**: None
- **Summary**: We present a simple and intuitive Focus-and-eXpand (\fax) method to guide the training process of a neural network towards a specific solution. Optimizing a neural network is a highly non-convex problem. Typically, the space of solutions is large, with numerous possible local minima, where reaching a specific minimum depends on many factors. In many cases, however, a solution which considers specific aspects, or features, of the input is desired. For example, in the presence of bias, a solution that disregards the biased feature is a more robust and accurate one. Drawing inspiration from Parameter Continuation methods, we propose steering the training process to consider specific features in the input more than others, through gradual shifts in the input domain. \fax extracts a subset of features from each input data-point, and exposes the learner to these features first, Focusing the solution on them. Then, by using a blending/mixing parameter $\alpha$ it gradually eXpands the learning process to include all features of the input. This process encourages the consideration of the desired features more than others. Though not restricted to this field, we quantitatively evaluate the effectiveness of our approach on various Computer Vision tasks, and achieve state-of-the-art bias removal, improvements to an established augmentation method, and two examples of improvements to image classification tasks. Through these few examples we demonstrate the impact this approach potentially carries for a wide variety of problems, which stand to gain from understanding the solution landscape.



### Attention as Activation
- **Arxiv ID**: http://arxiv.org/abs/2007.07729v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.07729v2)
- **Published**: 2020-07-15 14:52:29+00:00
- **Updated**: 2020-08-02 09:40:56+00:00
- **Authors**: Yimian Dai, Stefan Oehmcke, Fabian Gieseke, Yiquan Wu, Kobus Barnard
- **Comment**: None
- **Journal**: None
- **Summary**: Activation functions and attention mechanisms are typically treated as having different purposes and have evolved differently. However, both concepts can be formulated as a non-linear gating function. Inspired by their similarity, we propose a novel type of activation units called attentional activation (ATAC) units as a unification of activation functions and attention mechanisms. In particular, we propose a local channel attention module for the simultaneous non-linear activation and element-wise feature refinement, which locally aggregates point-wise cross-channel feature contexts. By replacing the well-known rectified linear units by such ATAC units in convolutional networks, we can construct fully attentional networks that perform significantly better with a modest number of additional parameters. We conducted detailed ablation studies on the ATAC units using several host networks with varying network depths to empirically verify the effectiveness and efficiency of the units. Furthermore, we compared the performance of the ATAC units against existing activation functions as well as other attention mechanisms on the CIFAR-10, CIFAR-100, and ImageNet datasets. Our experimental results show that networks constructed with the proposed ATAC units generally yield performance gains over their competitors given a comparable number of parameters.



### Finding Non-Uniform Quantization Schemes using Multi-Task Gaussian Processes
- **Arxiv ID**: http://arxiv.org/abs/2007.07743v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.07743v2)
- **Published**: 2020-07-15 15:16:18+00:00
- **Updated**: 2020-07-20 09:46:24+00:00
- **Authors**: Marcelo Gennari do Nascimento, Theo W. Costain, Victor Adrian Prisacariu
- **Comment**: Accepted for publication at ECCV 2020. Code availiable at
  https://code.active.vision . Updated for typo
- **Journal**: None
- **Summary**: We propose a novel method for neural network quantization that casts the neural architecture search problem as one of hyperparameter search to find non-uniform bit distributions throughout the layers of a CNN. We perform the search assuming a Multi-Task Gaussian Processes prior, which splits the problem to multiple tasks, each corresponding to different number of training epochs, and explore the space by sampling those configurations that yield maximum information. We then show that with significantly lower precision in the last layers we achieve a minimal loss of accuracy with appreciable memory savings. We test our findings on the CIFAR10 and ImageNet datasets using the VGG, ResNet and GoogLeNet architectures.



### Two-Level Adversarial Visual-Semantic Coupling for Generalized Zero-shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2007.07757v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.07757v2)
- **Published**: 2020-07-15 15:34:09+00:00
- **Updated**: 2020-11-30 11:00:45+00:00
- **Authors**: Shivam Chandhok, Vineeth N Balasubramanian
- **Comment**: Under Submission
- **Journal**: None
- **Summary**: The performance of generative zero-shot methods mainly depends on the quality of generated features and how well the model facilitates knowledge transfer between visual and semantic domains. The quality of generated features is a direct consequence of the ability of the model to capture the several modes of the underlying data distribution. To address these issues, we propose a new two-level joint maximization idea to augment the generative network with an inference network during training which helps our model capture the several modes of the data and generate features that better represent the underlying data distribution. This provides strong cross-modal interaction for effective transfer of knowledge between visual and semantic domains. Furthermore, existing methods train the zero-shot classifier either on generate synthetic image features or latent embeddings produced by leveraging representation learning. In this work, we unify these paradigms into a single model which in addition to synthesizing image features, also utilizes the representation learning capabilities of the inference network to provide discriminative features for the final zero-shot recognition task. We evaluate our approach on four benchmark datasets i.e. CUB, FLO, AWA1 and AWA2 against several state-of-the-art methods, and show its performance. We also perform ablation studies to analyze and understand our method more carefully for the Generalized Zero-shot Learning task.



### Self-Supervised Representation Learning for Detection of ACL Tear Injury in Knee MR Videos
- **Arxiv ID**: http://arxiv.org/abs/2007.07761v3
- **DOI**: 10.1016/j.patrec.2022.01.008
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.07761v3)
- **Published**: 2020-07-15 15:35:47+00:00
- **Updated**: 2020-12-14 12:27:43+00:00
- **Authors**: Siladittya Manna, Saumik Bhattacharya, Umapada Pal
- **Comment**: None
- **Journal**: None
- **Summary**: The success of deep learning based models for computer vision applications requires large scale human annotated data which are often expensive to generate. Self-supervised learning, a subset of unsupervised learning, handles this problem by learning meaningful features from unlabeled image or video data. In this paper, we propose a self-supervised learning approach to learn transferable features from MR video clips by enforcing the model to learn anatomical features. The pretext task models are designed to predict the correct ordering of the jumbled image patches that the MR video frames are divided into. To the best of our knowledge, none of the supervised learning models performing injury classification task from MR video provide any explanation for the decisions made by the models and hence makes our work the first of its kind on MR video data. Experiments on the pretext task show that this proposed approach enables the model to learn spatial context invariant features which help for reliable and explainable performance in downstream tasks like classification of Anterior Cruciate Ligament tear injury from knee MRI. The efficiency of the novel Convolutional Neural Network proposed in this paper is reflected in the experimental results obtained in the downstream task.



### CANet: Context Aware Network for 3D Brain Glioma Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2007.07788v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.07788v3)
- **Published**: 2020-07-15 16:12:41+00:00
- **Updated**: 2021-03-22 10:03:12+00:00
- **Authors**: Zhihua Liu, Lei Tong, Long Chen, Feixiang Zhou, Zheheng Jiang, Qianni Zhang, Yinhai Wang, Caifeng Shan, Ling Li, Huiyu Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: Automated segmentation of brain glioma plays an active role in diagnosis decision, progression monitoring and surgery planning. Based on deep neural networks, previous studies have shown promising technologies for brain glioma segmentation. However, these approaches lack powerful strategies to incorporate contextual information of tumor cells and their surrounding, which has been proven as a fundamental cue to deal with local ambiguity. In this work, we propose a novel approach named Context-Aware Network (CANet) for brain glioma segmentation. CANet captures high dimensional and discriminative features with contexts from both the convolutional space and feature interaction graphs. We further propose context guided attentive conditional random fields which can selectively aggregate features. We evaluate our method using publicly accessible brain glioma segmentation datasets BRATS2017, BRATS2018 and BRATS2019. The experimental results show that the proposed algorithm has better or competitive performance against several State-of-The-Art approaches under different segmentation metrics on the training and validation sets.



### Data-Efficient Deep Learning Method for Image Classification Using Data Augmentation, Focal Cosine Loss, and Ensemble
- **Arxiv ID**: http://arxiv.org/abs/2007.07805v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.07805v1)
- **Published**: 2020-07-15 16:30:57+00:00
- **Updated**: 2020-07-15 16:30:57+00:00
- **Authors**: Byeongjo Kim, Chanran Kim, Jaehoon Lee, Jein Song, Gyoungsoo Park
- **Comment**: 7 pages, 2 figures, technical report of 1st Visual Inductive Priors
  for Data-Efficient Deep Learning Workshop Challenge in ECCV 2020
- **Journal**: None
- **Summary**: In general, sufficient data is essential for the better performance and generalization of deep-learning models. However, lots of limitations(cost, resources, etc.) of data collection leads to lack of enough data in most of the areas. In addition, various domains of each data sources and licenses also lead to difficulties in collection of sufficient data. This situation makes us hard to utilize not only the pre-trained model, but also the external knowledge. Therefore, it is important to leverage small dataset effectively for achieving the better performance. We applied some techniques in three aspects: data, loss function, and prediction to enable training from scratch with less data. With these methods, we obtain high accuracy by leveraging ImageNet data which consist of only 50 images per class. Furthermore, our model is ranked 4th in Visual Inductive Printers for Data-Effective Computer Vision Challenge.



### VidCEP: Complex Event Processing Framework to Detect Spatiotemporal Patterns in Video Streams
- **Arxiv ID**: http://arxiv.org/abs/2007.07817v1
- **DOI**: 10.1109/BigData47090.2019.9006018
- **Categories**: **cs.CV**, cs.DB, cs.MM, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.07817v1)
- **Published**: 2020-07-15 16:43:37+00:00
- **Updated**: 2020-07-15 16:43:37+00:00
- **Authors**: Piyush Yadav, Edward Curry
- **Comment**: 10 pages, 19 figures, Paper published in IEEE BigData 2019
- **Journal**: None
- **Summary**: Video data is highly expressive and has traditionally been very difficult for a machine to interpret. Querying event patterns from video streams is challenging due to its unstructured representation. Middleware systems such as Complex Event Processing (CEP) mine patterns from data streams and send notifications to users in a timely fashion. Current CEP systems have inherent limitations to query video streams due to their unstructured data model and lack of expressive query language. In this work, we focus on a CEP framework where users can define high-level expressive queries over videos to detect a range of spatiotemporal event patterns. In this context, we propose: i) VidCEP, an in-memory, on the fly, near real-time complex event matching framework for video streams. The system uses a graph-based event representation for video streams which enables the detection of high-level semantic concepts from video using cascades of Deep Neural Network models, ii) a Video Event Query language (VEQL) to express high-level user queries for video streams in CEP, iii) a complex event matcher to detect spatiotemporal video event patterns by matching expressive user queries over video data. The proposed approach detects spatiotemporal video event patterns with an F-score ranging from 0.66 to 0.89. VidCEP maintains near real-time performance with an average throughput of 70 frames per second for 5 parallel videos with sub-second matching latency.



### Few-shot Scene-adaptive Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/2007.07843v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.07843v1)
- **Published**: 2020-07-15 17:08:46+00:00
- **Updated**: 2020-07-15 17:08:46+00:00
- **Authors**: Yiwei Lu, Frank Yu, Mahesh Kumar Krishna Reddy, Yang Wang
- **Comment**: Accepted to ECCV 2020 as a spotlight paper
- **Journal**: None
- **Summary**: We address the problem of anomaly detection in videos. The goal is to identify unusual behaviours automatically by learning exclusively from normal videos. Most existing approaches are usually data-hungry and have limited generalization abilities. They usually need to be trained on a large number of videos from a target scene to achieve good results in that scene. In this paper, we propose a novel few-shot scene-adaptive anomaly detection problem to address the limitations of previous approaches. Our goal is to learn to detect anomalies in a previously unseen scene with only a few frames. A reliable solution for this new problem will have huge potential in real-world applications since it is expensive to collect a massive amount of data for each target scene. We propose a meta-learning based approach for solving this new problem; extensive experimental results demonstrate the effectiveness of our proposed method.



### Transformation Consistency Regularization- A Semi-Supervised Paradigm for Image-to-Image Translation
- **Arxiv ID**: http://arxiv.org/abs/2007.07867v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.07867v1)
- **Published**: 2020-07-15 17:41:35+00:00
- **Updated**: 2020-07-15 17:41:35+00:00
- **Authors**: Aamir Mustafa, Rafal K. Mantiuk
- **Comment**: Accepted at ECCV 2020
- **Journal**: None
- **Summary**: Scarcity of labeled data has motivated the development of semi-supervised learning methods, which learn from large portions of unlabeled data alongside a few labeled samples. Consistency Regularization between model's predictions under different input perturbations, particularly has shown to provide state-of-the art results in a semi-supervised framework. However, most of these method have been limited to classification and segmentation applications. We propose Transformation Consistency Regularization, which delves into a more challenging setting of image-to-image translation, which remains unexplored by semi-supervised algorithms. The method introduces a diverse set of geometric transformations and enforces the model's predictions for unlabeled data to be invariant to those transformations. We evaluate the efficacy of our algorithm on three different applications: image colorization, denoising and super-resolution. Our method is significantly data efficient, requiring only around 10 - 20% of labeled samples to achieve similar image reconstructions to its fully-supervised counterpart. Furthermore, we show the effectiveness of our method in video processing applications, where knowledge from a few frames can be leveraged to enhance the quality of the rest of the movie.



### Gradient-based Hyperparameter Optimization Over Long Horizons
- **Arxiv ID**: http://arxiv.org/abs/2007.07869v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.07869v2)
- **Published**: 2020-07-15 17:44:07+00:00
- **Updated**: 2021-09-30 15:51:36+00:00
- **Authors**: Paul Micaelli, Amos Storkey
- **Comment**: None
- **Journal**: None
- **Summary**: Gradient-based hyperparameter optimization has earned a widespread popularity in the context of few-shot meta-learning, but remains broadly impractical for tasks with long horizons (many gradient steps), due to memory scaling and gradient degradation issues. A common workaround is to learn hyperparameters online, but this introduces greediness which comes with a significant performance drop. We propose forward-mode differentiation with sharing (FDS), a simple and efficient algorithm which tackles memory scaling issues with forward-mode differentiation, and gradient degradation issues by sharing hyperparameters that are contiguous in time. We provide theoretical guarantees about the noise reduction properties of our algorithm, and demonstrate its efficiency empirically by differentiating through $\sim 10^4$ gradient steps of unrolled optimization. We consider large hyperparameter search ranges on CIFAR-10 where we significantly outperform greedy gradient-based alternatives, while achieving $\times 20$ speedups compared to the state-of-the-art black-box methods. Code is available at: \url{https://github.com/polo5/FDS}



### Adaptive L2 Regularization in Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2007.07875v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.07875v2)
- **Published**: 2020-07-15 17:50:34+00:00
- **Updated**: 2020-10-18 15:26:19+00:00
- **Authors**: Xingyang Ni, Liang Fang, Heikki Huttunen
- **Comment**: Accepted at ICPR 2020
- **Journal**: None
- **Summary**: We introduce an adaptive L2 regularization mechanism in the setting of person re-identification. In the literature, it is common practice to utilize hand-picked regularization factors which remain constant throughout the training procedure. Unlike existing approaches, the regularization factors in our proposed method are updated adaptively through backpropagation. This is achieved by incorporating trainable scalar variables as the regularization factors, which are further fed into a scaled hard sigmoid function. Extensive experiments on the Market-1501, DukeMTMC-reID and MSMT17 datasets validate the effectiveness of our framework. Most notably, we obtain state-of-the-art performance on MSMT17, which is the largest dataset for person re-identification. Source code is publicly available at https://github.com/nixingyang/AdaptiveL2Regularization.



### A Refined Deep Learning Architecture for Diabetic Foot Ulcers Detection
- **Arxiv ID**: http://arxiv.org/abs/2007.07922v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.07922v1)
- **Published**: 2020-07-15 18:06:53+00:00
- **Updated**: 2020-07-15 18:06:53+00:00
- **Authors**: Manu Goyal, Saeed Hassanpour
- **Comment**: 8 Pages and DFUC Challenge
- **Journal**: None
- **Summary**: Diabetic Foot Ulcers (DFU) that affect the lower extremities are a major complication of diabetes. Each year, more than 1 million diabetic patients undergo amputation due to failure to recognize DFU and get the proper treatment from clinicians. There is an urgent need to use a CAD system for the detection of DFU. In this paper, we propose using deep learning methods (EfficientDet Architectures) for the detection of DFU in the DFUC2020 challenge dataset, which consists of 4,500 DFU images. We further refined the EfficientDet architecture to avoid false negative and false positive predictions. The code for this method is available at https://github.com/Manugoyal12345/Yet-Another-EfficientDet-Pytorch.



### Image De-Quantization Using Generative Models as Priors
- **Arxiv ID**: http://arxiv.org/abs/2007.07923v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.07923v2)
- **Published**: 2020-07-15 18:09:00+00:00
- **Updated**: 2020-07-17 21:40:45+00:00
- **Authors**: Kalliopi Basioti, George V. Moustakides
- **Comment**: None
- **Journal**: None
- **Summary**: Image quantization is used in several applications aiming in reducing the number of available colors in an image and therefore its size. De-quantization is the task of reversing the quantization effect and recovering the original multi-chromatic level image. Existing techniques achieve de-quantization by imposing suitable constraints on the ideal image in order to make the recovery problem feasible since it is otherwise ill-posed. Our goal in this work is to develop a de-quantization mechanism through a rigorous mathematical analysis which is based on the classical statistical estimation theory. In this effort we incorporate generative modeling of the ideal image as a suitable prior information. The resulting technique is simple and capable of de-quantizing successfully images that have experienced severe quantization effects. Interestingly, our method can recover images even if the quantization process is not exactly known and contains unknown parameters.



### Tracking Passengers and Baggage Items using Multi-camera Systems at Security Checkpoints
- **Arxiv ID**: http://arxiv.org/abs/2007.07924v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.07924v2)
- **Published**: 2020-07-15 18:09:31+00:00
- **Updated**: 2022-02-22 17:48:37+00:00
- **Authors**: Abubakar Siddique, Henry Medeiros
- **Comment**: 14 pages, 11 figures
- **Journal**: None
- **Summary**: We introduce a novel tracking-by-detection framework to track multiple objects in overhead camera videos for airport checkpoint security scenarios where targets correspond to passengers and their baggage items. Our approach improves object detection by employing a test-time data augmentation procedure that provides multiple geometrically transformed images as inputs to a convolutional neural network. We cluster the multiple detections generated by the network using the mean-shift algorithm. The multiple hypothesis tracking algorithm then keeps track of the temporal identifiers of the targets based on the cluster centroids. Our method also incorporates a trajectory association mechanism to maintain the consistency of the temporal identifiers as passengers travel across camera views. Finally, we also introduce a simple distance-based matching mechanism to associate passengers with their luggage. An evaluation of detection, tracking, and association performances on videos obtained from multiple overhead cameras in a realistic airport checkpoint environment demonstrates the effectiveness of the proposed approach.



### Filter Style Transfer between Photos
- **Arxiv ID**: http://arxiv.org/abs/2007.07925v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.07925v1)
- **Published**: 2020-07-15 18:09:35+00:00
- **Updated**: 2020-07-15 18:09:35+00:00
- **Authors**: Jonghwa Yim, Jisung Yoo, Won-joon Do, Beomsu Kim, Jihwan Choe
- **Comment**: ECCV (Spotlight) 2020
- **Journal**: None
- **Summary**: Over the past few years, image-to-image style transfer has risen to the frontiers of neural image processing. While conventional methods were successful in various tasks such as color and texture transfer between images, none could effectively work with the custom filter effects that are applied by users through various platforms like Instagram. In this paper, we introduce a new concept of style transfer, Filter Style Transfer (FST). Unlike conventional style transfer, new technique FST can extract and transfer custom filter style from a filtered style image to a content image. FST first infers the original image from a filtered reference via image-to-image translation. Then it estimates filter parameters from the difference between them. To resolve the ill-posed nature of reconstructing the original image from the reference, we represent each pixel color of an image to class mean and deviation. Besides, to handle the intra-class color variation, we propose an uncertainty based weighted least square method for restoring an original image. To the best of our knowledge, FST is the first style transfer method that can transfer custom filter effects between FHD image under 2ms on a mobile device without any textual context loss.



### ClassMix: Segmentation-Based Data Augmentation for Semi-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2007.07936v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.07936v2)
- **Published**: 2020-07-15 18:21:17+00:00
- **Updated**: 2020-11-29 11:14:07+00:00
- **Authors**: Viktor Olsson, Wilhelm Tranheden, Juliano Pinto, Lennart Svensson
- **Comment**: This paper has been accepted to WACV2021
- **Journal**: None
- **Summary**: The state of the art in semantic segmentation is steadily increasing in performance, resulting in more precise and reliable segmentations in many different applications. However, progress is limited by the cost of generating labels for training, which sometimes requires hours of manual labor for a single image. Because of this, semi-supervised methods have been applied to this task, with varying degrees of success. A key challenge is that common augmentations used in semi-supervised classification are less effective for semantic segmentation. We propose a novel data augmentation mechanism called ClassMix, which generates augmentations by mixing unlabelled samples, by leveraging on the network's predictions for respecting object boundaries. We evaluate this augmentation technique on two common semi-supervised semantic segmentation benchmarks, showing that it attains state-of-the-art results. Lastly, we also provide extensive ablation studies comparing different design decisions and training regimes.



### The Notary in the Haystack -- Countering Class Imbalance in Document Processing with CNNs
- **Arxiv ID**: http://arxiv.org/abs/2007.07943v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.07943v1)
- **Published**: 2020-07-15 18:40:33+00:00
- **Updated**: 2020-07-15 18:40:33+00:00
- **Authors**: Martin Leipert, Georg Vogeler, Mathias Seuret, Andreas Maier, Vincent Christlein
- **Comment**: Accepted at DAS Workshop 2020
- **Journal**: None
- **Summary**: Notarial instruments are a category of documents. A notarial instrument can be distinguished from other documents by its notary sign, a prominent symbol in the certificate, which also allows to identify the document's issuer. Naturally, notarial instruments are underrepresented in regard to other documents. This makes a classification difficult because class imbalance in training data worsens the performance of Convolutional Neural Networks. In this work, we evaluate different countermeasures for this problem. They are applied to a binary classification and a segmentation task on a collection of medieval documents. In classification, notarial instruments are distinguished from other documents, while the notary sign is separated from the certificate in the segmentation task. We evaluate different techniques, such as data augmentation, under- and oversampling, as well as regularizing with focal loss. The combination of random minority oversampling and data augmentation leads to the best performance. In segmentation, we evaluate three loss-functions and their combinations, where only class-weighted dice loss was able to segment the notary sign sufficiently.



### CloudCast: A Satellite-Based Dataset and Baseline for Forecasting Clouds
- **Arxiv ID**: http://arxiv.org/abs/2007.07978v2
- **DOI**: 10.1109/JSTARS.2021.3062936
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.07978v2)
- **Published**: 2020-07-15 20:20:55+00:00
- **Updated**: 2021-06-16 09:08:24+00:00
- **Authors**: A. H. Nielsen, A. Iosifidis, H. Karstoft
- **Comment**: For the novel dataset, see
  https://vision.eng.au.dk/cloudcast-dataset/
- **Journal**: IEEE Journal of Selected Topics in Applied Earth Observations and
  Remote Sensing, vol. 14, pp. 3485-3494, 2021
- **Summary**: Forecasting the formation and development of clouds is a central element of modern weather forecasting systems. Incorrect clouds forecasts can lead to major uncertainty in the overall accuracy of weather forecasts due to their intrinsic role in the Earth's climate system. Few studies have tackled this challenging problem from a machine learning point-of-view due to a shortage of high-resolution datasets with many historical observations globally. In this paper, we present a novel satellite-based dataset called ``CloudCast''. It consists of 70,080 images with 10 different cloud types for multiple layers of the atmosphere annotated on a pixel level. The spatial resolution of the dataset is 928 x 1530 pixels (3x3 km per pixel) with 15-min intervals between frames for the period 2017-01-01 to 2018-12-31. All frames are centered and projected over Europe. To supplement the dataset, we conduct an evaluation study with current state-of-the-art video prediction methods such as convolutional long short-term memory networks, generative adversarial networks, and optical flow-based extrapolation methods. As the evaluation of video prediction is difficult in practice, we aim for a thorough evaluation in the spatial and temporal domain. Our benchmark models show promising results but with ample room for improvement. This is the first publicly available global-scale dataset with high-resolution cloud types on a high temporal granularity to the authors' best knowledge.



### Leveraging Category Information for Single-Frame Visual Sound Source Separation
- **Arxiv ID**: http://arxiv.org/abs/2007.07984v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.07984v2)
- **Published**: 2020-07-15 20:35:29+00:00
- **Updated**: 2021-04-16 14:30:19+00:00
- **Authors**: Lingyu Zhu, Esa Rahtu
- **Comment**: 6 pages. The code is available at
  https://github.com/ly-zhu/Leveraging-Category-Information-for-Single-Frame-Visual-Sound-Source-Separation
- **Journal**: None
- **Summary**: Visual sound source separation aims at identifying sound components from a given sound mixture with the presence of visual cues. Prior works have demonstrated impressive results, but with the expense of large multi-stage architectures and complex data representations (e.g. optical flow trajectories). In contrast, we study simple yet efficient models for visual sound separation using only a single video frame. Furthermore, our models are able to exploit the information of the sound source category in the separation process. To this end, we propose two models where we assume that i) the category labels are available at the training time, or ii) we know if the training sample pairs are from the same or different category. The experiments with the MUSIC dataset show that our model obtains comparable or better performance compared to several recent baseline methods. The code is available at https://github.com/ly-zhu/Leveraging-Category-Information-for-Single-Frame-Visual-Sound-Source-Separation



### Detecting Deepfake Videos: An Analysis of Three Techniques
- **Arxiv ID**: http://arxiv.org/abs/2007.08517v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.08517v1)
- **Published**: 2020-07-15 20:36:23+00:00
- **Updated**: 2020-07-15 20:36:23+00:00
- **Authors**: Armaan Pishori, Brittany Rollins, Nicolas van Houten, Nisha Chatwani, Omar Uraimov
- **Comment**: 11 pages, 8 figures, 2 tables
- **Journal**: None
- **Summary**: Recent advances in deepfake generating algorithms that produce manipulated media have had dangerous implications in privacy, security and mass communication. Efforts to combat this issue have risen in the form of competitions and funding for research to detect deepfakes. This paper presents three techniques and algorithms: convolutional LSTM, eye blink detection and grayscale histograms-pursued while participating in the Deepfake Detection Challenge. We assessed the current knowledge about deepfake videos, a more severe version of manipulated media, and previous methods used, and found relevance in the grayscale histogram technique over others. We discussed the implications of each method developed and provided further steps to improve the given findings.



### Boosting Weakly Supervised Object Detection with Progressive Knowledge Transfer
- **Arxiv ID**: http://arxiv.org/abs/2007.07986v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.07986v1)
- **Published**: 2020-07-15 20:38:25+00:00
- **Updated**: 2020-07-15 20:38:25+00:00
- **Authors**: Yuanyi Zhong, Jianfeng Wang, Jian Peng, Lei Zhang
- **Comment**: ECCV 2020. Code: https://github.com/mikuhatsune/wsod_transfer
- **Journal**: None
- **Summary**: In this paper, we propose an effective knowledge transfer framework to boost the weakly supervised object detection accuracy with the help of an external fully-annotated source dataset, whose categories may not overlap with the target domain. This setting is of great practical value due to the existence of many off-the-shelf detection datasets. To more effectively utilize the source dataset, we propose to iteratively transfer the knowledge from the source domain by a one-class universal detector and learn the target-domain detector. The box-level pseudo ground truths mined by the target-domain detector in each iteration effectively improve the one-class universal detector. Therefore, the knowledge in the source dataset is more thoroughly exploited and leveraged. Extensive experiments are conducted with Pascal VOC 2007 as the target weakly-annotated dataset and COCO/ImageNet as the source fully-annotated dataset. With the proposed solution, we achieved an mAP of $59.7\%$ detection performance on the VOC test set and an mAP of $60.2\%$ after retraining a fully supervised Faster RCNN with the mined pseudo ground truths. This is significantly better than any previously known results in related literature and sets a new state-of-the-art of weakly supervised object detection under the knowledge transfer setting. Code: \url{https://github.com/mikuhatsune/wsod_transfer}.



### Combining Task Predictors via Enhancing Joint Predictability
- **Arxiv ID**: http://arxiv.org/abs/2007.08012v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.08012v1)
- **Published**: 2020-07-15 21:58:39+00:00
- **Updated**: 2020-07-15 21:58:39+00:00
- **Authors**: Kwang In Kim, Christian Richardt, Hyung Jin Chang
- **Comment**: None
- **Journal**: None
- **Summary**: Predictor combination aims to improve a (target) predictor of a learning task based on the (reference) predictors of potentially relevant tasks, without having access to the internals of individual predictors. We present a new predictor combination algorithm that improves the target by i) measuring the relevance of references based on their capabilities in predicting the target, and ii) strengthening such estimated relevance. Unlike existing predictor combination approaches that only exploit pairwise relationships between the target and each reference, and thereby ignore potentially useful dependence among references, our algorithm jointly assesses the relevance of all references by adopting a Bayesian framework. This also offers a rigorous way to automatically select only relevant references. Based on experiments on seven real-world datasets from visual attribute ranking and multi-class classification scenarios, we demonstrate that our algorithm offers a significant performance gain and broadens the application range of existing predictor combination approaches.



### Attention-Based Query Expansion Learning
- **Arxiv ID**: http://arxiv.org/abs/2007.08019v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.08019v1)
- **Published**: 2020-07-15 22:15:55+00:00
- **Updated**: 2020-07-15 22:15:55+00:00
- **Authors**: Albert Gordo, Filip Radenovic, Tamara Berg
- **Comment**: Accepted for publication at ECCV2020
- **Journal**: None
- **Summary**: Query expansion is a technique widely used in image search consisting in combining highly ranked images from an original query into an expanded query that is then reissued, generally leading to increased recall and precision. An important aspect of query expansion is choosing an appropriate way to combine the images into a new query. Interestingly, despite the undeniable empirical success of query expansion, ad-hoc methods with different caveats have dominated the landscape, and not a lot of research has been done on learning how to do query expansion. In this paper we propose a more principled framework to query expansion, where one trains, in a discriminative manner, a model that learns how images should be aggregated to form the expanded query. Within this framework, we propose a model that leverages a self-attention mechanism to effectively learn how to transfer information between the different images before aggregating them. Our approach obtains higher accuracy than existing approaches on standard benchmarks. More importantly, our approach is the only one that consistently shows high accuracy under different regimes, overcoming caveats of existing methods.



### Predicting Clinical Outcomes in COVID-19 using Radiomics and Deep Learning on Chest Radiographs: A Multi-Institutional Study
- **Arxiv ID**: http://arxiv.org/abs/2007.08028v2
- **DOI**: None
- **Categories**: **q-bio.QM**, cs.CV, cs.LG, eess.IV, J.3; I.2.6
- **Links**: [PDF](http://arxiv.org/pdf/2007.08028v2)
- **Published**: 2020-07-15 22:48:11+00:00
- **Updated**: 2021-07-01 18:47:22+00:00
- **Authors**: Joseph Bae, Saarthak Kapse, Gagandeep Singh, Rishabh Gattu, Syed Ali, Neal Shah, Colin Marshall, Jonathan Pierce, Tej Phatak, Amit Gupta, Jeremy Green, Nikhil Madan, Prateek Prasanna
- **Comment**: Joseph Bae and Saarthak Kapse have contributed equally to this work
- **Journal**: None
- **Summary**: We predict mechanical ventilation requirement and mortality using computational modeling of chest radiographs (CXRs) for coronavirus disease 2019 (COVID-19) patients. This two-center, retrospective study analyzed 530 deidentified CXRs from 515 COVID-19 patients treated at Stony Brook University Hospital and Newark Beth Israel Medical Center between March and August 2020. DL and machine learning classifiers to predict mechanical ventilation requirement and mortality were trained and evaluated using patient CXRs. A novel radiomic embedding framework was also explored for outcome prediction. All results are compared against radiologist grading of CXRs (zone-wise expert severity scores). Radiomic and DL classification models had mAUCs of 0.78+/-0.02 and 0.81+/-0.04, compared with expert scores mAUCs of 0.75+/-0.02 and 0.79+/-0.05 for mechanical ventilation requirement and mortality prediction, respectively. Combined classifiers using both radiomics and expert severity scores resulted in mAUCs of 0.79+/-0.04 and 0.83+/-0.04 for each prediction task, demonstrating improvement over either artificial intelligence or radiologist interpretation alone. Our results also suggest instances where inclusion of radiomic features in DL improves model predictions, something that might be explored in other pathologies. The models proposed in this study and the prognostic information they provide might aid physician decision making and resource allocation during the COVID-19 pandemic.



### When and how CNNs generalize to out-of-distribution category-viewpoint combinations
- **Arxiv ID**: http://arxiv.org/abs/2007.08032v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.08032v3)
- **Published**: 2020-07-15 23:04:37+00:00
- **Updated**: 2021-11-17 12:29:25+00:00
- **Authors**: Spandan Madan, Timothy Henry, Jamell Dozier, Helen Ho, Nishchal Bhandari, Tomotake Sasaki, Frédo Durand, Hanspeter Pfister, Xavier Boix
- **Comment**: None
- **Journal**: None
- **Summary**: Object recognition and viewpoint estimation lie at the heart of visual understanding. Recent works suggest that convolutional neural networks (CNNs) fail to generalize to out-of-distribution (OOD) category-viewpoint combinations, ie. combinations not seen during training. In this paper, we investigate when and how such OOD generalization may be possible by evaluating CNNs trained to classify both object category and 3D viewpoint on OOD combinations, and identifying the neural mechanisms that facilitate such OOD generalization. We show that increasing the number of in-distribution combinations (ie. data diversity) substantially improves generalization to OOD combinations, even with the same amount of training data. We compare learning category and viewpoint in separate and shared network architectures, and observe starkly different trends on in-distribution and OOD combinations, ie. while shared networks are helpful in-distribution, separate networks significantly outperform shared ones at OOD combinations. Finally, we demonstrate that such OOD generalization is facilitated by the neural mechanism of specialization, ie. the emergence of two types of neurons -- neurons selective to category and invariant to viewpoint, and vice versa.



### Active Visual Information Gathering for Vision-Language Navigation
- **Arxiv ID**: http://arxiv.org/abs/2007.08037v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.08037v3)
- **Published**: 2020-07-15 23:54:20+00:00
- **Updated**: 2020-08-19 19:48:02+00:00
- **Authors**: Hanqing Wang, Wenguan Wang, Tianmin Shu, Wei Liang, Jianbing Shen
- **Comment**: ECCV2020 (changed with improved perfromance on Pre-Explore and Beam
  Search settings); website: https://github.com/HanqingWangAI/Active_VLN
- **Journal**: None
- **Summary**: Vision-language navigation (VLN) is the task of entailing an agent to carry out navigational instructions inside photo-realistic environments. One of the key challenges in VLN is how to conduct a robust navigation by mitigating the uncertainty caused by ambiguous instructions and insufficient observation of the environment. Agents trained by current approaches typically suffer from this and would consequently struggle to avoid random and inefficient actions at every step. In contrast, when humans face such a challenge, they can still maintain robust navigation by actively exploring the surroundings to gather more information and thus make more confident navigation decisions. This work draws inspiration from human navigation behavior and endows an agent with an active information gathering ability for a more intelligent vision-language navigation policy. To achieve this, we propose an end-to-end framework for learning an exploration policy that decides i) when and where to explore, ii) what information is worth gathering during exploration, and iii) how to adjust the navigation decision after the exploration. The experimental results show promising exploration strategies emerged from training, which leads to significant boost in navigation performance. On the R2R challenge leaderboard, our agent gets promising results all three VLN settings, i.e., single run, pre-exploration, and beam search.



