# Arxiv Papers in cs.CV on 2020-07-12
### Train Your Data Processor: Distribution-Aware and Error-Compensation Coordinate Decoding for Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2007.05887v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.05887v4)
- **Published**: 2020-07-12 02:17:29+00:00
- **Updated**: 2020-07-17 04:03:25+00:00
- **Authors**: Feiyu Yang, Zhan Song, Zhenzhong Xiao, Yu Chen, Zhe Pan, Min Zhang, Min Xue, Yaoyang Mo, Yao Zhang, Guoxiong Guan, Beibei Qian
- **Comment**: Improve the state-of-the-art of COCO keypoint detection challenge by
  1-2 AP. Project page: https://github.com/fyang235/DAEC
- **Journal**: None
- **Summary**: Recently, the leading performance of human pose estimation is dominated by heatmap based methods. While being a fundamental component of heatmap processing, heatmap decoding (i.e. transforming heatmaps to coordinates) receives only limited investigations, to our best knowledge. This work fills the gap by studying the heatmap decoding processing with a particular focus on the errors introduced throughout the prediction process. We found that the errors of heatmap based methods are surprisingly significant, which nevertheless was universally ignored before. In view of the discovered importance, we further reveal the intrinsic limitations of the previous widely used heatmap decoding methods and thereout propose a Distribution-Aware and Error-Compensation Coordinate Decoding (DAEC). Serving as a model-agnostic plug-in, DAEC learns its decoding strategy from training data and remarkably improves the performance of a variety of state-of-the-art human pose estimation models with negligible extra computation. Specifically, equipped with DAEC, the SimpleBaseline-ResNet152-256x192 and HRNet-W48-256x192 are significantly improved by 2.6 AP and 2.9 AP achieving 72.6 AP and 75.7 AP on COCO, respectively. Moreover, the HRNet-W32-256x256 and ResNet-152-256x256 frameworks enjoy even more dramatic promotions of 8.4% and 7.8% on MPII with PCKh0.1 metric. Extensive experiments performed on these two common benchmarks, demonstrates that DAEC exceeds its competitors by considerable margins, backing up the rationality and generality of our novel heatmap decoding idea. The project is available at https://github.com/fyang235/DAEC.



### PA-GAN: Progressive Attention Generative Adversarial Network for Facial Attribute Editing
- **Arxiv ID**: http://arxiv.org/abs/2007.05892v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.05892v1)
- **Published**: 2020-07-12 03:04:12+00:00
- **Updated**: 2020-07-12 03:04:12+00:00
- **Authors**: Zhenliang He, Meina Kan, Jichao Zhang, Shiguang Shan
- **Comment**: Code: https://github.com/LynnHo/PA-GAN-Tensorflow
- **Journal**: None
- **Summary**: Facial attribute editing aims to manipulate attributes on the human face, e.g., adding a mustache or changing the hair color. Existing approaches suffer from a serious compromise between correct attribute generation and preservation of the other information such as identity and background, because they edit the attributes in the imprecise area. To resolve this dilemma, we propose a progressive attention GAN (PA-GAN) for facial attribute editing. In our approach, the editing is progressively conducted from high to low feature level while being constrained inside a proper attribute area by an attention mask at each level. This manner prevents undesired modifications to the irrelevant regions from the beginning, and then the network can focus more on correctly generating the attributes within a proper boundary at each level. As a result, our approach achieves correct attribute editing with irrelevant details much better preserved compared with the state-of-the-arts. Codes are released at https://github.com/LynnHo/PA-GAN-Tensorflow.



### Framework for Passenger Seat Availability Using Face Detection in Passenger Bus
- **Arxiv ID**: http://arxiv.org/abs/2007.05906v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.05906v1)
- **Published**: 2020-07-12 04:31:28+00:00
- **Updated**: 2020-07-12 04:31:28+00:00
- **Authors**: Khawar Islam, Uzma Afzal
- **Comment**: None
- **Journal**: None
- **Summary**: Advancements in Intelligent Transportation System (IES) improve passenger traveling by providing information systems for bus arrival time and counting the number of passengers and buses in cities. Passengers still face bus waiting and seat unavailability issues which have adverse effects on traffic management and controlling authority. We propose a Face Detection based Framework (FDF) to determine passenger seat availability in a camera-equipped bus through face detection which is based on background subtraction to count empty, filled, and total seats. FDF has an integrated smartphone Passenger Application (PA) to identify the nearest bus stop. We evaluate FDF in a live test environment and results show that it gives 90% accuracy. We believe our results have the potential to address traffic management concerns and assist passengers to save their valuable time



### Two-Stream Deep Feature Modelling for Automated Video Endoscopy Data Analysis
- **Arxiv ID**: http://arxiv.org/abs/2007.05914v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.05914v1)
- **Published**: 2020-07-12 05:24:08+00:00
- **Updated**: 2020-07-12 05:24:08+00:00
- **Authors**: Harshala Gammulle, Simon Denman, Sridha Sridharan, Clinton Fookes
- **Comment**: Accepted for Publication at MICCAI 2020
- **Journal**: None
- **Summary**: Automating the analysis of imagery of the Gastrointestinal (GI) tract captured during endoscopy procedures has substantial potential benefits for patients, as it can provide diagnostic support to medical practitioners and reduce mistakes via human error. To further the development of such methods, we propose a two-stream model for endoscopic image analysis. Our model fuses two streams of deep feature inputs by mapping their inherent relations through a novel relational network model, to better model symptoms and classify the image. In contrast to handcrafted feature-based models, our proposed network is able to learn features automatically and outperforms existing state-of-the-art methods on two public datasets: KVASIR and Nerthus. Our extensive evaluations illustrate the importance of having two streams of inputs instead of a single stream and also demonstrates the merits of the proposed relational network architecture to combine those streams.



### Pose-aware Adversarial Domain Adaptation for Personalized Facial Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/2007.05932v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.05932v1)
- **Published**: 2020-07-12 07:58:31+00:00
- **Updated**: 2020-07-12 07:58:31+00:00
- **Authors**: Guang Liang, Shangfei Wang, Can Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Current facial expression recognition methods fail to simultaneously cope with pose and subject variations.   In this paper, we propose a novel unsupervised adversarial domain adaptation method which can alleviate both variations at the same time. Specially, our method consists of three learning strategies: adversarial domain adaptation learning, cross adversarial feature learning, and reconstruction learning. The first aims to learn pose- and expression-related feature representations in the source domain and adapt both feature distributions to that of the target domain by imposing adversarial learning. By using personalized adversarial domain adaptation, this learning strategy can alleviate subject variations and exploit information from the source domain to help learning in the target domain.   The second serves to perform feature disentanglement between pose- and expression-related feature representations by impulsing pose-related feature representations expression-undistinguished and the expression-related feature representations pose-undistinguished.   The last can further boost feature learning by applying face image reconstructions so that the learned expression-related feature representations are more pose- and identity-robust.   Experimental results on four benchmark datasets demonstrate the effectiveness of the proposed method.



### Adversarial Self-Supervised Learning for Semi-Supervised 3D Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2007.05934v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.05934v1)
- **Published**: 2020-07-12 08:01:06+00:00
- **Updated**: 2020-07-12 08:01:06+00:00
- **Authors**: Chenyang Si, Xuecheng Nie, Wei Wang, Liang Wang, Tieniu Tan, Jiashi Feng
- **Comment**: Accepted by ECCV2020
- **Journal**: None
- **Summary**: We consider the problem of semi-supervised 3D action recognition which has been rarely explored before. Its major challenge lies in how to effectively learn motion representations from unlabeled data. Self-supervised learning (SSL) has been proved very effective at learning representations from unlabeled data in the image domain. However, few effective self-supervised approaches exist for 3D action recognition, and directly applying SSL for semi-supervised learning suffers from misalignment of representations learned from SSL and supervised learning tasks. To address these issues, we present Adversarial Self-Supervised Learning (ASSL), a novel framework that tightly couples SSL and the semi-supervised scheme via neighbor relation exploration and adversarial learning. Specifically, we design an effective SSL scheme to improve the discrimination capability of learned representations for 3D action recognition, through exploring the data relations within a neighborhood. We further propose an adversarial regularization to align the feature distributions of labeled and unlabeled samples. To demonstrate effectiveness of the proposed ASSL in semi-supervised 3D action recognition, we conduct extensive experiments on NTU and N-UCLA datasets. The results confirm its advantageous performance over state-of-the-art semi-supervised methods in the few label regime for 3D action recognition.



### Label-free detection of Giardia lamblia cysts using a deep learning-enabled portable imaging flow cytometer
- **Arxiv ID**: http://arxiv.org/abs/2007.10795v1
- **DOI**: 10.1039/D0LC00708K
- **Categories**: **eess.IV**, cs.CV, physics.app-ph
- **Links**: [PDF](http://arxiv.org/pdf/2007.10795v1)
- **Published**: 2020-07-12 08:40:18+00:00
- **Updated**: 2020-07-12 08:40:18+00:00
- **Authors**: Zoltan Gorocs, David Baum, Fang Song, Kevin DeHaan, Hatice Ceylan Koydemir, Yunzhe Qiu, Zilin Cai, Thamira Skandakumar, Spencer Peterman, Miu Tamamitsu, Aydogan Ozcan
- **Comment**: 17 Pages, 5 Figures, 1 Table
- **Journal**: Lab on a Chip (2020)
- **Summary**: We report a field-portable and cost-effective imaging flow cytometer that uses deep learning to accurately detect Giardia lamblia cysts in water samples at a volumetric throughput of 100 mL/h. This flow cytometer uses lensfree color holographic imaging to capture and reconstruct phase and intensity images of microscopic objects in a continuously flowing sample, and automatically identifies Giardia Lamblia cysts in real-time without the use of any labels or fluorophores. The imaging flow cytometer is housed in an environmentally-sealed enclosure with dimensions of 19 cm x 19 cm x 16 cm and weighs 1.6 kg. We demonstrate that this portable imaging flow cytometer coupled to a laptop computer can detect and quantify, in real-time, low levels of Giardia contamination (e.g., <10 cysts per 50 mL) in both freshwater and seawater samples. The field-portable and label-free nature of this method has the potential to allow rapid and automated screening of drinking water supplies in resource limited settings in order to detect waterborne parasites and monitor the integrity of the filters used for water treatment.



### Fruit classification using deep feature maps in the presence of deceptive similar classes
- **Arxiv ID**: http://arxiv.org/abs/2007.05942v1
- **DOI**: 10.1109/IJCNN52387.2021.9533678
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.05942v1)
- **Published**: 2020-07-12 09:01:57+00:00
- **Updated**: 2020-07-12 09:01:57+00:00
- **Authors**: Mohit Dandekar, Narinder Singh Punn, Sanjay Kumar Sonbhadra, Sonali Agarwal
- **Comment**: None
- **Journal**: None
- **Summary**: Autonomous detection and classification of objects are admired area of research in many industrial applications. Though, humans can distinguish objects with high multi-granular similarities very easily; but for the machines, it is a very challenging task. The convolution neural networks (CNN) have illustrated efficient performance in multi-level representations of objects for classification. Conventionally, the existing deep learning models utilize the transformed features generated by the rearmost layer for training and testing. However, it is evident that this does not work well with multi-granular data, especially, in presence of deceptive similar classes (almost similar but different classes). The objective of the present research is to address the challenge of classification of deceptively similar multi-granular objects with an ensemble approach thfat utilizes activations from multiple layers of CNN (deep features). These multi-layer activations are further utilized to build multiple deep decision trees (known as Random forest) for classification of objects with similar appearance. The Fruits-360 dataset is utilized for evaluation of the proposed approach. With extensive trials it was observed that the proposed model outperformed over the conventional deep learning approaches.



### Dual Adversarial Network: Toward Real-world Noise Removal and Noise Generation
- **Arxiv ID**: http://arxiv.org/abs/2007.05946v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, I.4.4
- **Links**: [PDF](http://arxiv.org/pdf/2007.05946v1)
- **Published**: 2020-07-12 09:16:06+00:00
- **Updated**: 2020-07-12 09:16:06+00:00
- **Authors**: Zongsheng Yue, Qian Zhao, Lei Zhang, Deyu Meng
- **Comment**: Accepted by ECCV 2020
- **Journal**: None
- **Summary**: Real-world image noise removal is a long-standing yet very challenging task in computer vision. The success of deep neural network in denoising stimulates the research of noise generation, aiming at synthesizing more clean-noisy image pairs to facilitate the training of deep denoisers. In this work, we propose a novel unified framework to simultaneously deal with the noise removal and noise generation tasks. Instead of only inferring the posteriori distribution of the latent clean image conditioned on the observed noisy image in traditional MAP framework, our proposed method learns the joint distribution of the clean-noisy image pairs. Specifically, we approximate the joint distribution with two different factorized forms, which can be formulated as a denoiser mapping the noisy image to the clean one and a generator mapping the clean image to the noisy one. The learned joint distribution implicitly contains all the information between the noisy and clean images, avoiding the necessity of manually designing the image priors and noise assumptions as traditional. Besides, the performance of our denoiser can be further improved by augmenting the original training dataset with the learned generator. Moreover, we propose two metrics to assess the quality of the generated noisy image, for which, to the best of our knowledge, such metrics are firstly proposed along this research line. Extensive experiments have been conducted to demonstrate the superiority of our method over the state-of-the-arts both in the real noise removal and generation tasks. The training and testing code is available at https://github.com/zsyOAOA/DANet.



### Self-Supervised Drivable Area and Road Anomaly Segmentation using RGB-D Data for Robotic Wheelchairs
- **Arxiv ID**: http://arxiv.org/abs/2007.05950v1
- **DOI**: 10.1109/LRA.2019.2932874
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2007.05950v1)
- **Published**: 2020-07-12 10:12:46+00:00
- **Updated**: 2020-07-12 10:12:46+00:00
- **Authors**: Hengli Wang, Yuxiang Sun, Ming Liu
- **Comment**: Published in IEEE Robotics and Automation Letters (RA-L); 8 pages, 8
  figures and 3 tables
- **Journal**: None
- **Summary**: The segmentation of drivable areas and road anomalies are critical capabilities to achieve autonomous navigation for robotic wheelchairs. The recent progress of semantic segmentation using deep learning techniques has presented effective results. However, the acquisition of large-scale datasets with hand-labeled ground truth is time-consuming and labor-intensive, making the deep learning-based methods often hard to implement in practice. We contribute to the solution of this problem for the task of drivable area and road anomaly segmentation by proposing a self-supervised learning approach. We develop a pipeline that can automatically generate segmentation labels for drivable areas and road anomalies. Then, we train RGB-D data-based semantic segmentation neural networks and get predicted labels. Experimental results show that our proposed automatic labeling pipeline achieves an impressive speed-up compared to manual labeling. In addition, our proposed self-supervised approach exhibits more robust and accurate results than the state-of-the-art traditional algorithms as well as the state-of-the-art self-supervised algorithms.



### MeTRAbs: Metric-Scale Truncation-Robust Heatmaps for Absolute 3D Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2007.07227v2
- **DOI**: 10.1109/TBIOM.2020.3037257
- **Categories**: **cs.CV**, I.2.10; I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/2007.07227v2)
- **Published**: 2020-07-12 11:52:09+00:00
- **Updated**: 2020-11-14 19:32:45+00:00
- **Authors**: István Sárándi, Timm Linder, Kai O. Arras, Bastian Leibe
- **Comment**: See project page at https://vision.rwth-aachen.de/metrabs . Accepted
  for publication in the IEEE Transactions on Biometrics, Behavior, and
  Identity Science (TBIOM), Special Issue "Selected Best Works From Automated
  Face and Gesture Recognition 2020". Extended version of FG paper
  arXiv:2003.02953
- **Journal**: IEEE Transactions on Biometrics, Behavior, and Identity Science,
  vol. 3, no. 1, pp. 16-30, Jan. 2021
- **Summary**: Heatmap representations have formed the basis of human pose estimation systems for many years, and their extension to 3D has been a fruitful line of recent research. This includes 2.5D volumetric heatmaps, whose X and Y axes correspond to image space and Z to metric depth around the subject. To obtain metric-scale predictions, 2.5D methods need a separate post-processing step to resolve scale ambiguity. Further, they cannot localize body joints outside the image boundaries, leading to incomplete estimates for truncated images. To address these limitations, we propose metric-scale truncation-robust (MeTRo) volumetric heatmaps, whose dimensions are all defined in metric 3D space, instead of being aligned with image space. This reinterpretation of heatmap dimensions allows us to directly estimate complete, metric-scale poses without test-time knowledge of distance or relying on anthropometric heuristics, such as bone lengths. To further demonstrate the utility our representation, we present a differentiable combination of our 3D metric-scale heatmaps with 2D image-space ones to estimate absolute 3D pose (our MeTRAbs architecture). We find that supervision via absolute pose loss is crucial for accurate non-root-relative localization. Using a ResNet-50 backbone without further learned layers, we obtain state-of-the-art results on Human3.6M, MPI-INF-3DHP and MuPoTS-3D. Our code will be made publicly available to facilitate further research.



### Structured Weight Priors for Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2007.14235v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.14235v1)
- **Published**: 2020-07-12 13:05:51+00:00
- **Updated**: 2020-07-12 13:05:51+00:00
- **Authors**: Tim Pearce, Andrew Y. K. Foong, Alexandra Brintrup
- **Comment**: Presented at the ICML 2020 Workshop on Uncertainty and Robustness in
  Deep Learning
- **Journal**: None
- **Summary**: Selection of an architectural prior well suited to a task (e.g. convolutions for image data) is crucial to the success of deep neural networks (NNs). Conversely, the weight priors within these architectures are typically left vague, e.g.~independent Gaussian distributions, which has led to debate over the utility of Bayesian deep learning. This paper explores the benefits of adding structure to weight priors. It initially considers first-layer filters of a convolutional NN, designing a prior based on random Gabor filters. Second, it considers adding structure to the prior of final-layer weights by estimating how each hidden feature relates to each class. Empirical results suggest that these structured weight priors lead to more meaningful functional priors for image data. This contributes to the ongoing discussion on the importance of weight priors.



### IllumiNet: Transferring Illumination from Planar Surfaces to Virtual Objects in Augmented Reality
- **Arxiv ID**: http://arxiv.org/abs/2007.05981v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.05981v1)
- **Published**: 2020-07-12 13:11:14+00:00
- **Updated**: 2020-07-12 13:11:14+00:00
- **Authors**: Di Xu, Zhen Li, Yanning Zhang, Qi Cao
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents an illumination estimation method for virtual objects in real environment by learning. While previous works tackled this problem by reconstructing high dynamic range (HDR) environment maps or the corresponding spherical harmonics, we do not seek to recover the lighting environment of the entire scene. Given a single RGB image, our method directly infers the relit virtual object by transferring the illumination features extracted from planar surfaces in the scene to the desired geometries. Compared to previous works, our approach is more robust as it works in both indoor and outdoor environments with spatially-varying illumination. Experiments and evaluation results show that our approach outperforms the state-of-the-art quantitatively and qualitatively, achieving realistic augmented experience.



### Deep Network Interpolation for Accelerated Parallel MR Image Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2007.05993v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.05993v1)
- **Published**: 2020-07-12 13:58:07+00:00
- **Updated**: 2020-07-12 13:58:07+00:00
- **Authors**: Chen Qin, Jo Schlemper, Kerstin Hammernik, Jinming Duan, Ronald M Summers, Daniel Rueckert
- **Comment**: Presented at 2020 ISMRM Conference & Exhibition (Abstract #4958)
- **Journal**: None
- **Summary**: We present a deep network interpolation strategy for accelerated parallel MR image reconstruction. In particular, we examine the network interpolation in parameter space between a source model that is formulated in an unrolled scheme with L1 and SSIM losses and its counterpart that is trained with an adversarial loss. We show that by interpolating between the two different models of the same network structure, the new interpolated network can model a trade-off between perceptual quality and fidelity.



### Differentiable Programming for Hyperspectral Unmixing using a Physics-based Dispersion Model
- **Arxiv ID**: http://arxiv.org/abs/2007.05996v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, physics.ao-ph
- **Links**: [PDF](http://arxiv.org/pdf/2007.05996v1)
- **Published**: 2020-07-12 14:16:35+00:00
- **Updated**: 2020-07-12 14:16:35+00:00
- **Authors**: John Janiczek, Parth Thaker, Gautam Dasarathy, Christopher S. Edwards, Philip Christensen, Suren Jayasuriya
- **Comment**: 36 pages, 11 figures. Accepted to European Conference on Computer
  Vision (ECCV) 2020
- **Journal**: None
- **Summary**: Hyperspectral unmixing is an important remote sensing task with applications including material identification and analysis. Characteristic spectral features make many pure materials identifiable from their visible-to-infrared spectra, but quantifying their presence within a mixture is a challenging task due to nonlinearities and factors of variation. In this paper, spectral variation is considered from a physics-based approach and incorporated into an end-to-end spectral unmixing algorithm via differentiable programming. The dispersion model is introduced to simulate realistic spectral variation, and an efficient method to fit the parameters is presented. Then, this dispersion model is utilized as a generative model within an analysis-by-synthesis spectral unmixing algorithm. Further, a technique for inverse rendering using a convolutional neural network to predict parameters of the generative model is introduced to enhance performance and speed when training data is available. Results achieve state-of-the-art on both infrared and visible-to-near-infrared (VNIR) datasets, and show promise for the synergy between physics-based models and deep learning in hyperspectral unmixing in the future.



### Multi-Modality Information Fusion for Radiomics-based Neural Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/2007.06002v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.06002v1)
- **Published**: 2020-07-12 14:35:13+00:00
- **Updated**: 2020-07-12 14:35:13+00:00
- **Authors**: Yige Peng, Lei Bi, Michael Fulham, Dagan Feng, Jinman Kim
- **Comment**: Accepted by MICCAI 2020
- **Journal**: None
- **Summary**: 'Radiomics' is a method that extracts mineable quantitative features from radiographic images. These features can then be used to determine prognosis, for example, predicting the development of distant metastases (DM). Existing radiomics methods, however, require complex manual effort including the design of hand-crafted radiomic features and their extraction and selection. Recent radiomics methods, based on convolutional neural networks (CNNs), also require manual input in network architecture design and hyper-parameter tuning. Radiomic complexity is further compounded when there are multiple imaging modalities, for example, combined positron emission tomography - computed tomography (PET-CT) where there is functional information from PET and complementary anatomical localization information from computed tomography (CT). Existing multi-modality radiomics methods manually fuse the data that are extracted separately. Reliance on manual fusion often results in sub-optimal fusion because they are dependent on an 'expert's' understanding of medical images. In this study, we propose a multi-modality neural architecture search method (MM-NAS) to automatically derive optimal multi-modality image features for radiomics and thus negate the dependence on a manual process. We evaluated our MM-NAS on the ability to predict DM using a public PET-CT dataset of patients with soft-tissue sarcomas (STSs). Our results show that our MM-NAS had a higher prediction accuracy when compared to state-of-the-art radiomics methods.



### MeDaS: An open-source platform as service to help break the walls between medicine and informatics
- **Arxiv ID**: http://arxiv.org/abs/2007.06013v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.06013v2)
- **Published**: 2020-07-12 15:17:00+00:00
- **Updated**: 2020-07-14 01:59:08+00:00
- **Authors**: Liang Zhang, Johann Li, Ping Li, Xiaoyuan Lu, Peiyi Shen, Guangming Zhu, Syed Afaq Shah, Mohammed Bennarmoun, Kun Qian, Björn W. Schuller
- **Comment**: layout error fixed
- **Journal**: None
- **Summary**: In the past decade, deep learning (DL) has achieved unprecedented success in numerous fields including computer vision, natural language processing, and healthcare. In particular, DL is experiencing an increasing development in applications for advanced medical image analysis in terms of analysis, segmentation, classification, and furthermore. On the one hand, tremendous needs that leverage the power of DL for medical image analysis are arising from the research community of a medical, clinical, and informatics background to jointly share their expertise, knowledge, skills, and experience. On the other hand, barriers between disciplines are on the road for them often hampering a full and efficient collaboration. To this end, we propose our novel open-source platform, i.e., MeDaS -- the MeDical open-source platform as Service. To the best of our knowledge, MeDaS is the first open-source platform proving a collaborative and interactive service for researchers from a medical background easily using DL related toolkits, and at the same time for scientists or engineers from information sciences to understand the medical knowledge side. Based on a series of toolkits and utilities from the idea of RINV (Rapid Implementation aNd Verification), our proposed MeDaS platform can implement pre-processing, post-processing, augmentation, visualization, and other phases needed in medical image analysis. Five tasks including the subjects of lung, liver, brain, chest, and pathology, are validated and demonstrated to be efficiently realisable by using MeDaS.



### Probabilistic Jacobian-based Saliency Maps Attacks
- **Arxiv ID**: http://arxiv.org/abs/2007.06032v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.06032v4)
- **Published**: 2020-07-12 16:32:26+00:00
- **Updated**: 2020-12-10 11:19:11+00:00
- **Authors**: Théo Combey, António Loison, Maxime Faucher, Hatem Hajri
- **Comment**: Journal Machine Learning and Knowledge Extraction
- **Journal**: None
- **Summary**: Neural network classifiers (NNCs) are known to be vulnerable to malicious adversarial perturbations of inputs including those modifying a small fraction of the input features named sparse or $L_0$ attacks. Effective and fast $L_0$ attacks, such as the widely used Jacobian-based Saliency Map Attack (JSMA) are practical to fool NNCs but also to improve their robustness. In this paper, we show that penalising saliency maps of JSMA by the output probabilities and the input features of the NNC allows to obtain more powerful attack algorithms that better take into account each input's characteristics. This leads us to introduce improved versions of JSMA, named Weighted JSMA (WJSMA) and Taylor JSMA (TJSMA), and demonstrate through a variety of white-box and black-box experiments on three different datasets (MNIST, CIFAR-10 and GTSRB), that they are both significantly faster and more efficient than the original targeted and non-targeted versions of JSMA. Experiments also demonstrate, in some cases, very competitive results of our attacks in comparison with the Carlini-Wagner (CW) $L_0$ attack, while remaining, like JSMA, significantly faster (WJSMA and TJSMA are more than 50 times faster than CW $L_0$ on CIFAR-10). Therefore, our new attacks provide good trade-offs between JSMA and CW for $L_0$ real-time adversarial testing on datasets such as the ones previously cited. Codes are publicly available through the link https://github.com/probabilistic-jsmas/probabilistic-jsmas.



### Learning to associate detections for real-time multiple object tracking
- **Arxiv ID**: http://arxiv.org/abs/2007.06041v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.06041v1)
- **Published**: 2020-07-12 17:08:41+00:00
- **Updated**: 2020-07-12 17:08:41+00:00
- **Authors**: Michel Meneses, Leonardo Matos, Bruno Prado, André de Carvalho, Hendrik Macedo
- **Comment**: 8 pages, 6 figures
- **Journal**: None
- **Summary**: With the recent advances in the object detection research field, tracking-by-detection has become the leading paradigm adopted by multi-object tracking algorithms. By extracting different features from detected objects, those algorithms can estimate the objects' similarities and association patterns along successive frames. However, since similarity functions applied by tracking algorithms are handcrafted, it is difficult to employ them in new contexts. In this study, it is investigated the use of artificial neural networks to learning a similarity function that can be used among detections. During training, the networks were introduced to correct and incorrect association patterns, sampled from a pedestrian tracking data set. For such, different motion and appearance features combinations have been explored. Finally, a trained network has been inserted into a multiple-object tracking framework, which has been assessed on the MOT Challenge benchmark. Throughout the experiments, the proposed tracker matched the results obtained by state-of-the-art methods, it has run 58\% faster than a recent and similar method, used as baseline.



### Traffic Prediction Framework for OpenStreetMap using Deep Learning based Complex Event Processing and Open Traffic Cameras
- **Arxiv ID**: http://arxiv.org/abs/2008.00928v1
- **DOI**: 10.4230/LIPIcs.GIScience.2021.I.17
- **Categories**: **cs.CV**, cs.AI, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/2008.00928v1)
- **Published**: 2020-07-12 17:10:43+00:00
- **Updated**: 2020-07-12 17:10:43+00:00
- **Authors**: Piyush Yadav, Dipto Sarkar, Dhaval Salwala, Edward Curry
- **Comment**: 16 pages, 9 Figures, 3 Tables, Paper accepted in GIScience 2020 (now
  postponed to 2021)
- **Journal**: None
- **Summary**: Displaying near-real-time traffic information is a useful feature of digital navigation maps. However, most commercial providers rely on privacy-compromising measures such as deriving location information from cellphones to estimate traffic. The lack of an open-source traffic estimation method using open data platforms is a bottleneck for building sophisticated navigation services on top of OpenStreetMap (OSM). We propose a deep learning-based Complex Event Processing (CEP) method that relies on publicly available video camera streams for traffic estimation. The proposed framework performs near-real-time object detection and objects property extraction across camera clusters in parallel to derive multiple measures related to traffic with the results visualized on OpenStreetMap. The estimation of object properties (e.g. vehicle speed, count, direction) provides multidimensional data that can be leveraged to create metrics and visualization for congestion beyond commonly used density-based measures. Our approach couples both flow and count measures during interpolation by considering each vehicle as a sample point and their speed as weight. We demonstrate multidimensional traffic metrics (e.g. flow rate, congestion estimation) over OSM by processing 22 traffic cameras from London streets. The system achieves a near-real-time performance of 1.42 seconds median latency and an average F-score of 0.80.



### It Is Likely That Your Loss Should be a Likelihood
- **Arxiv ID**: http://arxiv.org/abs/2007.06059v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.06059v2)
- **Published**: 2020-07-12 18:25:17+00:00
- **Updated**: 2020-10-02 14:39:37+00:00
- **Authors**: Mark Hamilton, Evan Shelhamer, William T. Freeman
- **Comment**: None
- **Journal**: None
- **Summary**: Many common loss functions such as mean-squared-error, cross-entropy, and reconstruction loss are unnecessarily rigid. Under a probabilistic interpretation, these common losses correspond to distributions with fixed shapes and scales. We instead argue for optimizing full likelihoods that include parameters like the normal variance and softmax temperature. Joint optimization of these "likelihood parameters" with model parameters can adaptively tune the scales and shapes of losses in addition to the strength of regularization. We explore and systematically evaluate how to parameterize and apply likelihood parameters for robust modeling, outlier-detection, and re-calibration. Additionally, we propose adaptively tuning $L_2$ and $L_1$ weights by fitting the scale parameters of normal and Laplace priors and introduce more flexible element-wise regularizers.



### Exploiting Uncertainties from Ensemble Learners to Improve Decision-Making in Healthcare AI
- **Arxiv ID**: http://arxiv.org/abs/2007.06063v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.06063v1)
- **Published**: 2020-07-12 18:33:09+00:00
- **Updated**: 2020-07-12 18:33:09+00:00
- **Authors**: Yingshui Tan, Baihong Jin, Xiangyu Yue, Yuxin Chen, Alberto Sangiovanni Vincentelli
- **Comment**: Preprint of submission to NeurIPS 2020
- **Journal**: None
- **Summary**: Ensemble learning is widely applied in Machine Learning (ML) to improve model performance and to mitigate decision risks. In this approach, predictions from a diverse set of learners are combined to obtain a joint decision. Recently, various methods have been explored in literature for estimating decision uncertainties using ensemble learning; however, determining which metrics are a better fit for certain decision-making applications remains a challenging task. In this paper, we study the following key research question in the selection of uncertainty metrics: when does an uncertainty metric outperforms another? We answer this question via a rigorous analysis of two commonly used uncertainty metrics in ensemble learning, namely ensemble mean and ensemble variance. We show that, under mild assumptions on the ensemble learners, ensemble mean is preferable with respect to ensemble variance as an uncertainty metric for decision making. We empirically validate our assumptions and theoretical results via an extensive case study: the diagnosis of referable diabetic retinopathy.



### Visualizing Classification Structure of Large-Scale Classifiers
- **Arxiv ID**: http://arxiv.org/abs/2007.06068v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.06068v2)
- **Published**: 2020-07-12 18:55:31+00:00
- **Updated**: 2020-07-19 01:58:03+00:00
- **Authors**: Bilal Alsallakh, Zhixin Yan, Shabnam Ghaffarzadegan, Zeng Dai, Liu Ren
- **Comment**: 2020 ICML Workshop on Human Interpretability in Machine Learning (WHI
  2020)
- **Journal**: None
- **Summary**: We propose a measure to compute class similarity in large-scale classification based on prediction scores. Such measure has not been formally pro-posed in the literature. We show how visualizing the class similarity matrix can reveal hierarchical structures and relationships that govern the classes. Through examples with various classifiers, we demonstrate how such structures can help in analyzing the classification behavior and in inferring potential corner cases. The source code for one example is available as a notebook at https://github.com/bilalsal/blocks



### A Comparative Study on Polyp Classification using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2007.06071v1
- **DOI**: 10.1371/journal.pone.0236452
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.06071v1)
- **Published**: 2020-07-12 19:16:19+00:00
- **Updated**: 2020-07-12 19:16:19+00:00
- **Authors**: Krushi Patel, Kaidong Li, Ke Tao, Quan Wang, Ajay Bansal, Amit Rastogi, Guanghui Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Colorectal cancer is the third most common cancer diagnosed in both men and women in the United States. Most colorectal cancers start as a growth on the inner lining of the colon or rectum, called 'polyp'. Not all polyps are cancerous, but some can develop into cancer. Early detection and recognition of the type of polyps is critical to prevent cancer and change outcomes. However, visual classification of polyps is challenging due to varying illumination conditions of endoscopy, variant texture, appearance, and overlapping morphology between polyps. More importantly, evaluation of polyp patterns by gastroenterologists is subjective leading to a poor agreement among observers. Deep convolutional neural networks have proven very successful in object classification across various object categories. In this work, we compare the performance of the state-of-the-art general object classification models for polyp classification. We trained a total of six CNN models end-to-end using a dataset of 157 video sequences composed of two types of polyps: hyperplastic and adenomatous. Our results demonstrate that the state-of-the-art CNN models can successfully classify polyps with an accuracy comparable or better than reported among gastroenterologists. The results of this study can guide future research in polyp classification.



### Sparse Graph to Sequence Learning for Vision Conditioned Long Textual Sequence Generation
- **Arxiv ID**: http://arxiv.org/abs/2007.06077v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.06077v1)
- **Published**: 2020-07-12 19:54:32+00:00
- **Updated**: 2020-07-12 19:54:32+00:00
- **Authors**: Aditya Mogadala, Marius Mosbach, Dietrich Klakow
- **Comment**: International Conference on Machine Learning (ICML) 2020 Workshop
  (https://logicalreasoninggnn.github.io/)
- **Journal**: None
- **Summary**: Generating longer textual sequences when conditioned on the visual information is an interesting problem to explore. The challenge here proliferate over the standard vision conditioned sentence-level generation (e.g., image or video captioning) as it requires to produce a brief and coherent story describing the visual content. In this paper, we mask this Vision-to-Sequence as Graph-to-Sequence learning problem and approach it with the Transformer architecture. To be specific, we introduce Sparse Graph-to-Sequence Transformer (SGST) for encoding the graph and decoding a sequence. The encoder aims to directly encode graph-level semantics, while the decoder is used to generate longer sequences. Experiments conducted with the benchmark image paragraph dataset show that our proposed achieve 13.3% improvement on the CIDEr evaluation measure when comparing to the previous state-of-the-art approach.



### SkyScapes -- Fine-Grained Semantic Understanding of Aerial Scenes
- **Arxiv ID**: http://arxiv.org/abs/2007.06102v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.06102v1)
- **Published**: 2020-07-12 21:44:38+00:00
- **Updated**: 2020-07-12 21:44:38+00:00
- **Authors**: Seyed Majid Azimi, Corentin Henry, Lars Sommer, Arne Schumann, Eleonora Vig
- **Comment**: Accepted in IEEE ICCV19
- **Journal**: None
- **Summary**: Understanding the complex urban infrastructure with centimeter-level accuracy is essential for many applications from autonomous driving to mapping, infrastructure monitoring, and urban management. Aerial images provide valuable information over a large area instantaneously; nevertheless, no current dataset captures the complexity of aerial scenes at the level of granularity required by real-world applications. To address this, we introduce SkyScapes, an aerial image dataset with highly-accurate, fine-grained annotations for pixel-level semantic labeling. SkyScapes provides annotations for 31 semantic categories ranging from large structures, such as buildings, roads and vegetation, to fine details, such as 12 (sub-)categories of lane markings. We have defined two main tasks on this dataset: dense semantic segmentation and multi-class lane-marking prediction. We carry out extensive experiments to evaluate state-of-the-art segmentation methods on SkyScapes. Existing methods struggle to deal with the wide range of classes, object sizes, scales, and fine details present. We therefore propose a novel multi-task model, which incorporates semantic edge detection and is better tuned for feature extraction from a wide range of scales. This model achieves notable improvements over the baselines in region outlines and level of detail on both tasks.



### VINNAS: Variational Inference-based Neural Network Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/2007.06103v5
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.06103v5)
- **Published**: 2020-07-12 21:47:35+00:00
- **Updated**: 2021-01-14 21:26:57+00:00
- **Authors**: Martin Ferianc, Hongxiang Fan, Miguel Rodrigues
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: In recent years, neural architecture search (NAS) has received intensive scientific and industrial interest due to its capability of finding a neural architecture with high accuracy for various artificial intelligence tasks such as image classification or object detection. In particular, gradient-based NAS approaches have become one of the more popular approaches thanks to their computational efficiency during the search. However, these methods often experience a mode collapse, where the quality of the found architectures is poor due to the algorithm resorting to choosing a single operation type for the entire network, or stagnating at a local minima for various datasets or search spaces.   To address these defects, we present a differentiable variational inference-based NAS method for searching sparse convolutional neural networks. Our approach finds the optimal neural architecture by dropping out candidate operations in an over-parameterised supergraph using variational dropout with automatic relevance determination prior, which makes the algorithm gradually remove unnecessary operations and connections without risking mode collapse. The evaluation is conducted through searching two types of convolutional cells that shape the neural network for classifying different image datasets. Our method finds diverse network cells, while showing state-of-the-art accuracy with up to almost 2 times fewer non-zero parameters.



### EAGLE: Large-scale Vehicle Detection Dataset in Real-World Scenarios using Aerial Imagery
- **Arxiv ID**: http://arxiv.org/abs/2007.06124v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.06124v3)
- **Published**: 2020-07-12 23:00:30+00:00
- **Updated**: 2020-11-23 21:45:29+00:00
- **Authors**: Seyed Majid Azimi, Reza Bahmanyar, Corenin Henry, Franz Kurz
- **Comment**: Accepted in ICPR 2020
- **Journal**: None
- **Summary**: Multi-class vehicle detection from airborne imagery with orientation estimation is an important task in the near and remote vision domains with applications in traffic monitoring and disaster management. In the last decade, we have witnessed significant progress in object detection in ground imagery, but it is still in its infancy in airborne imagery, mostly due to the scarcity of diverse and large-scale datasets. Despite being a useful tool for different applications, current airborne datasets only partially reflect the challenges of real-world scenarios. To address this issue, we introduce EAGLE (oriEnted vehicle detection using Aerial imaGery in real-worLd scEnarios), a large-scale dataset for multi-class vehicle detection with object orientation information in aerial imagery. It features high-resolution aerial images composed of different real-world situations with a wide variety of camera sensor, resolution, flight altitude, weather, illumination, haze, shadow, time, city, country, occlusion, and camera angle. The annotation was done by airborne imagery experts with small- and large-vehicle classes. EAGLE contains 215,986 instances annotated with oriented bounding boxes defined by four points and orientation, making it by far the largest dataset to date in this task. It also supports researches on the haze and shadow removal as well as super-resolution and in-painting applications. We define three tasks: detection by (1) horizontal bounding boxes, (2) rotated bounding boxes, and (3) oriented bounding boxes. We carried out several experiments to evaluate several state-of-the-art methods in object detection on our dataset to form a baseline. Experiments show that the EAGLE dataset accurately reflects real-world situations and correspondingly challenging applications.



### DRWR: A Differentiable Renderer without Rendering for Unsupervised 3D Structure Learning from Silhouette Images
- **Arxiv ID**: http://arxiv.org/abs/2007.06127v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.06127v1)
- **Published**: 2020-07-12 23:13:06+00:00
- **Updated**: 2020-07-12 23:13:06+00:00
- **Authors**: Zhizhong Han, Chao Chen, Yu-Shen Liu, Matthias Zwicker
- **Comment**: Accepted at ICML2020
- **Journal**: None
- **Summary**: Differentiable renderers have been used successfully for unsupervised 3D structure learning from 2D images because they can bridge the gap between 3D and 2D. To optimize 3D shape parameters, current renderers rely on pixel-wise losses between rendered images of 3D reconstructions and ground truth images from corresponding viewpoints. Hence they require interpolation of the recovered 3D structure at each pixel, visibility handling, and optionally evaluating a shading model. In contrast, here we propose a Differentiable Renderer Without Rendering (DRWR) that omits these steps. DRWR only relies on a simple but effective loss that evaluates how well the projections of reconstructed 3D point clouds cover the ground truth object silhouette. Specifically, DRWR employs a smooth silhouette loss to pull the projection of each individual 3D point inside the object silhouette, and a structure-aware repulsion loss to push each pair of projections that fall inside the silhouette far away from each other. Although we omit surface interpolation, visibility handling, and shading, our results demonstrate that DRWR achieves state-of-the-art accuracies under widely used benchmarks, outperforming previous methods both qualitatively and quantitatively. In addition, our training times are significantly lower due to the simplicity of DRWR.



### Locality Guided Neural Networks for Explainable Artificial Intelligence
- **Arxiv ID**: http://arxiv.org/abs/2007.06131v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.06131v1)
- **Published**: 2020-07-12 23:45:51+00:00
- **Updated**: 2020-07-12 23:45:51+00:00
- **Authors**: Randy Tan, Naimul Khan, Ling Guan
- **Comment**: 8 pages, 3 figures, submitted to WCCI2020
- **Journal**: None
- **Summary**: In current deep network architectures, deeper layers in networks tend to contain hundreds of independent neurons which makes it hard for humans to understand how they interact with each other. By organizing the neurons by correlation, humans can observe how clusters of neighbouring neurons interact with each other. In this paper, we propose a novel algorithm for back propagation, called Locality Guided Neural Network(LGNN) for training networks that preserves locality between neighbouring neurons within each layer of a deep network. Heavily motivated by Self-Organizing Map (SOM), the goal is to enforce a local topology on each layer of a deep network such that neighbouring neurons are highly correlated with each other. This method contributes to the domain of Explainable Artificial Intelligence (XAI), which aims to alleviate the black-box nature of current AI methods and make them understandable by humans. Our method aims to achieve XAI in deep learning without changing the structure of current models nor requiring any post processing. This paper focuses on Convolutional Neural Networks (CNNs), but can theoretically be applied to any type of deep learning architecture. In our experiments, we train various VGG and Wide ResNet (WRN) networks for image classification on CIFAR100. In depth analyses presenting both qualitative and quantitative results demonstrate that our method is capable of enforcing a topology on each layer while achieving a small increase in classification accuracy



