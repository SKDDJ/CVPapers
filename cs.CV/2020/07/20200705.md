# Arxiv Papers in cs.CV on 2020-07-05
### An Integer Approximation Method for Discrete Sinusoidal Transforms
- **Arxiv ID**: http://arxiv.org/abs/2007.02232v1
- **DOI**: 10.1007/s00034-011-9318-5
- **Categories**: **eess.SP**, cs.CV, cs.MM, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.02232v1)
- **Published**: 2020-07-05 03:37:35+00:00
- **Updated**: 2020-07-05 03:37:35+00:00
- **Authors**: R. J. Cintra
- **Comment**: 13 pages, 5 figures, 8 tables
- **Journal**: Circuits, Systems, and Signal Processing, vol. 30, n. 6, 2011
- **Summary**: Approximate methods have been considered as a means to the evaluation of discrete transforms. In this work, we propose and analyze a class of integer transforms for the discrete Fourier, Hartley, and cosine transforms (DFT, DHT, and DCT), based on simple dyadic rational approximation methods. The introduced method is general, applicable to several block-lengths, whereas existing approaches are usually dedicated to specific transform sizes. The suggested approximate transforms enjoy low multiplicative complexity and the orthogonality property is achievable via matrix polar decomposition. We show that the obtained transforms are competitive with archived methods in literature. New 8-point square wave approximate transforms for the DFT, DHT, and DCT are also introduced as particular cases of the introduced methodology.



### A Systematic Evaluation of Object Detection Networks for Scientific Plots
- **Arxiv ID**: http://arxiv.org/abs/2007.02240v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.02240v2)
- **Published**: 2020-07-05 05:30:53+00:00
- **Updated**: 2020-12-19 07:37:10+00:00
- **Authors**: Pritha Ganguly, Nitesh Methani, Mitesh M. Khapra, Pratyush Kumar
- **Comment**: This work has been accepted and will be presented at AAAI 2021
- **Journal**: None
- **Summary**: Are existing object detection methods adequate for detecting text and visual elements in scientific plots which are arguably different than the objects found in natural images? To answer this question, we train and compare the accuracy of various SOTA object detection networks on the PlotQA dataset. At the standard IOU setting of 0.5, most networks perform well with mAP scores greater than 80% in detecting the relatively simple objects in plots. However, the performance drops drastically when evaluated at a stricter IOU of 0.9 with the best model giving a mAP of 35.70%. Note that such a stricter evaluation is essential when dealing with scientific plots where even minor localisation errors can lead to large errors in downstream numerical inferences. Given this poor performance, we propose minor modifications to existing models by combining ideas from different object detection networks. While this significantly improves the performance, there are still 2 main issues: (i) performance on text objects which are essential for reasoning is very poor, and (ii) inference time is unacceptably large considering the simplicity of plots. To solve this open problem, we make a series of contributions: (a) an efficient region proposal method based on Laplacian edge detectors, (b) a feature representation of region proposals that includes neighbouring information, (c) a linking component to join multiple region proposals for detecting longer textual objects, and (d) a custom loss function that combines a smooth L1-loss with an IOU-based loss. Combining these ideas, our final model is very accurate at extreme IOU values achieving a mAP of 93.44%@0.9 IOU. Simultaneously, our model is very efficient with an inference time 16x lesser than the current models, including one-stage detectors. With these contributions, we enable further exploration on the automated reasoning of plots.



### Blind Inverse Gamma Correction with Maximized Differential Entropy
- **Arxiv ID**: http://arxiv.org/abs/2007.02246v1
- **DOI**: 10.1016/j.sigpro.2021.108427
- **Categories**: **eess.IV**, cs.CV, I.4.1; I.4.3; I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/2007.02246v1)
- **Published**: 2020-07-05 06:15:01+00:00
- **Updated**: 2020-07-05 06:15:01+00:00
- **Authors**: Yong Lee, Shaohua Zhang, Miao Li, Xiaoyu He
- **Comment**: 12 pages, 8 figures
- **Journal**: Signal Processing
- **Summary**: Unwanted nonlinear gamma distortion frequently occurs in a great diversity of images during the procedures of image acquisition, processing, and/or display. And the gamma distortion often varies with capture setup change and luminance variation. Blind inverse gamma correction, which automatically determines a proper restoration gamma value from a given image, is of paramount importance to attenuate the distortion. For blind inverse gamma correction, an adaptive gamma transformation method (AGT-ME) is proposed directly from a maximized differential entropy model. And the corresponding optimization has a mathematical concise closed-form solution, resulting in efficient implementation and accurate gamma restoration of AGT-ME. Considering the human eye has a non-linear perception sensitivity, a modified version AGT-ME-VISUAL is also proposed to achieve better visual performance. Tested on variable datasets, AGT-ME could obtain an accurate estimation of a large range of gamma distortion (0.1 to 3.0), outperforming the state-of-the-art methods. Besides, the proposed AGT-ME and AGT-ME-VISUAL were applied to three typical applications, including automatic gamma adjustment, natural/medical image contrast enhancement, and fringe projection profilometry image restoration. Furthermore, the AGT-ME/ AGT-ME-VISUAL is general and can be seamlessly extended to the masked image, multi-channel (color or spectrum) image, or multi-frame video, and free of the arbitrary tuning parameter. Besides, the corresponding Python code (https://github.com/yongleex/AGT-ME) is also provided for interested users.



### CIDMP: Completely Interpretable Detection of Malaria Parasite in Red Blood Cells using Lower-dimensional Feature Space
- **Arxiv ID**: http://arxiv.org/abs/2007.02248v1
- **DOI**: 10.1109/IJCNN48605.2020.9206885
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.02248v1)
- **Published**: 2020-07-05 06:28:09+00:00
- **Updated**: 2020-07-05 06:28:09+00:00
- **Authors**: Anik Khan, Kishor Datta Gupta, Deepak Venugopal, Nirman Kumar
- **Comment**: Accepted in The 2020 International Joint Conference on Neural
  Networks (IJCNN 2020) At Glasgow (UK)
- **Journal**: None
- **Summary**: Predicting if red blood cells (RBC) are infected with the malaria parasite is an important problem in Pathology. Recently, supervised machine learning approaches have been used for this problem, and they have had reasonable success. In particular, state-of-the-art methods such as Convolutional Neural Networks automatically extract increasingly complex feature hierarchies from the image pixels. While such generalized automatic feature extraction methods have significantly reduced the burden of feature engineering in many domains, for niche tasks such as the one we consider in this paper, they result in two major problems. First, they use a very large number of features (that may or may not be relevant) and therefore training such models is computationally expensive. Further, more importantly, the large feature-space makes it very hard to interpret which features are truly important for predictions. Thus, a criticism of such methods is that learning algorithms pose opaque black boxes to its users, in this case, medical experts. The recommendation of such algorithms can be understood easily, but the reason for their recommendation is not clear. This is the problem of non-interpretability of the model, and the best-performing algorithms are usually the least interpretable. To address these issues, in this paper, we propose an approach to extract a very small number of aggregated features that are easy to interpret and compute, and empirically show that we obtain high prediction accuracy even with a significantly reduced feature-space.



### Stereo Visual Inertial Pose Estimation Based on Feedforward-Feedback Loops
- **Arxiv ID**: http://arxiv.org/abs/2007.02250v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.02250v2)
- **Published**: 2020-07-05 06:40:11+00:00
- **Updated**: 2020-07-13 09:07:02+00:00
- **Authors**: Shengyang Chen, Chih-Yung Wen, Yajing Zou, Wu Chen
- **Comment**: 14 pages, 14 figures, 2 tables
- **Journal**: None
- **Summary**: In this paper, we present a novel stereo visual inertial pose estimation method. Compared to the widely used filter-based or optimization-based approaches, the pose estimation process is modeled as a control system. Designed feedback or feedforward loops are introduced to achieve the stable control of the system, which include a gradient decreased feedback loop, a roll-pitch feed forward loop and a bias estimation feedback loop. This system, named FLVIS (Feedforward-feedback Loop-based Visual Inertial System), is evaluated on the popular EuRoc MAV dataset. FLVIS achieves high accuracy and robustness with respect to other state-of-the-art visual SLAM approaches. The system has also been implemented and tested on a UAV platform. The source code of this research is public to the research community.



### Spatial-Angular Attention Network for Light Field Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2007.02252v2
- **DOI**: 10.1109/TIP.2021.3122089
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.02252v2)
- **Published**: 2020-07-05 06:55:29+00:00
- **Updated**: 2021-10-14 01:35:24+00:00
- **Authors**: Gaochang Wu, Yingqian Wang, Yebin Liu, Lu Fang, Tianyou Chai
- **Comment**: 15 pages, 13 figures and 5 tables, Accepted by IEEE Transactions on
  Image Processing (IEEE TIP)
- **Journal**: IEEE Transactions on Image Processing, 2021
- **Summary**: Typical learning-based light field reconstruction methods demand in constructing a large receptive field by deepening the network to capture correspondences between input views. In this paper, we propose a spatial-angular attention network to perceive correspondences in the light field non-locally, and reconstruction high angular resolution light field in an end-to-end manner. Motivated by the non-local attention mechanism, a spatial-angular attention module specifically for the high-dimensional light field data is introduced to compute the responses from all the positions in the epipolar plane for each pixel in the light field, and generate an attention map that captures correspondences along the angular dimension. We then propose a multi-scale reconstruction structure to efficiently implement the non-local attention in the low spatial scale, while also preserving the high frequency components in the high spatial scales. Extensive experiments demonstrate the superior performance of the proposed spatial-angular attention network for reconstructing sparsely-sampled light fields with non-Lambertian effects.



### Experiments of Federated Learning for COVID-19 Chest X-ray Images
- **Arxiv ID**: http://arxiv.org/abs/2007.05592v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.05592v1)
- **Published**: 2020-07-05 08:25:37+00:00
- **Updated**: 2020-07-05 08:25:37+00:00
- **Authors**: Boyi Liu, Bingjie Yan, Yize Zhou, Yifan Yang, Yixian Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: AI plays an important role in COVID-19 identification. Computer vision and deep learning techniques can assist in determining COVID-19 infection with Chest X-ray Images. However, for the protection and respect of the privacy of patients, the hospital's specific medical-related data did not allow leakage and sharing without permission. Collecting such training data was a major challenge. To a certain extent, this has caused a lack of sufficient data samples when performing deep learning approaches to detect COVID-19. Federated Learning is an available way to address this issue. It can effectively address the issue of data silos and get a shared model without obtaining local data. In the work, we propose the use of federated learning for COVID-19 data training and deploy experiments to verify the effectiveness. And we also compare performances of four popular models (MobileNet, ResNet18, MoblieNet, and COVID-Net) with the federated learning framework and without the framework. This work aims to inspire more researches on federated learning about COVID-19.



### Deep Learning based Dimple Segmentation for Quantitative Fractography
- **Arxiv ID**: http://arxiv.org/abs/2007.02267v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.02267v3)
- **Published**: 2020-07-05 08:43:58+00:00
- **Updated**: 2020-10-01 10:33:09+00:00
- **Authors**: Ashish Sinha, K S Suresh
- **Comment**: Accepted as a poster only at IC-MSE 2021. In review for publication
  in a conference
- **Journal**: None
- **Summary**: In this work, we try to address the challenging problem of dimple detection and segmentation in Titanium alloys using machine learning methods, especially neural networks. The images i.e. fractographs are obtained using a Scanning Election Microscope (SEM). To determine the cause of fracture in metals we address the problem of segmentation of dimples in fractographs i.e. the fracture surface of metals using supervised machine learning methods. Determining the cause of fracture would help us in material property, mechanical property prediction and development of new fracture-resistant materials. This method would also help in correlating the topography of the fracture surface with the mechanical properties of the material. Our proposed novel model achieves the best performance as compared to other previous approaches. To the best of our knowledge, this is one the first work in fractography using fully convolutional neural networks with self-attention for supervised learning of dimple fractography, though it can be easily extended to account for brittle characteristics as well.



### Image Aesthetics Prediction Using Multiple Patches Preserving the Original Aspect Ratio of Contents
- **Arxiv ID**: http://arxiv.org/abs/2007.02268v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2007.02268v1)
- **Published**: 2020-07-05 08:49:23+00:00
- **Updated**: 2020-07-05 08:49:23+00:00
- **Authors**: Lijie Wang, Xueting Wang, Toshihiko Yamasaki
- **Comment**: None
- **Journal**: None
- **Summary**: The spread of social networking services has created an increasing demand for selecting, editing, and generating impressive images. This trend increases the importance of evaluating image aesthetics as a complementary function of automatic image processing. We propose a multi-patch method, named MPA-Net (Multi-Patch Aggregation Network), to predict image aesthetics scores by maintaining the original aspect ratios of contents in the images. Through an experiment involving the large-scale AVA dataset, which contains 250,000 images, we show that the effectiveness of the equal-interval multi-patch selection approach for aesthetics score prediction is significant compared to the single-patch prediction and random patch selection approaches. For this dataset, MPA-Net outperforms the neural image assessment algorithm, which was regarded as a baseline method. In particular, MPA-Net yields a 0.073 (11.5%) higher linear correlation coefficient (LCC) of aesthetics scores and a 0.088 (14.4%) higher Spearman's rank correlation coefficient (SRCC). MPA-Net also reduces the mean square error (MSE) by 0.0115 (4.18%) and achieves results for the LCC and SRCC that are comparable to those of the state-of-the-art continuous aesthetics score prediction methods. Most notably, MPA-Net yields a significant lower MSE especially for images with aspect ratios far from 1.0, indicating that MPA-Net is useful for a wide range of image aspect ratios. MPA-Net uses only images and does not require external information during the training nor prediction stages. Therefore, MPA-Net has great potential for applications aside from aesthetics score prediction such as other human subjectivity prediction.



### Rethinking Bottleneck Structure for Efficient Mobile Network Design
- **Arxiv ID**: http://arxiv.org/abs/2007.02269v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.02269v4)
- **Published**: 2020-07-05 08:55:26+00:00
- **Updated**: 2020-11-27 16:02:39+00:00
- **Authors**: Zhou Daquan, Qibin Hou, Yunpeng Chen, Jiashi Feng, Shuicheng Yan
- **Comment**: A journal version under review of the previous paper published as a
  ECCV20 conference paper, improved segmentation and detection results
- **Journal**: None
- **Summary**: The inverted residual block is dominating architecture design for mobile networks recently. It changes the classic residual bottleneck by introducing two design rules: learning inverted residuals and using linear bottlenecks. In this paper, we rethink the necessity of such design changes and find it may bring risks of information loss and gradient confusion. We thus propose to flip the structure and present a novel bottleneck design, called the sandglass block, that performs identity mapping and spatial transformation at higher dimensions and thus alleviates information loss and gradient confusion effectively. Extensive experiments demonstrate that, different from the common belief, such bottleneck structure is more beneficial than the inverted ones for mobile networks. In ImageNet classification, by simply replacing the inverted residual block with our sandglass block without increasing parameters and computation, the classification accuracy can be improved by more than 1.7% over MobileNetV2. On Pascal VOC 2007 test set, we observe that there is also 0.9% mAP improvement in object detection. We further verify the effectiveness of the sandglass block by adding it into the search space of neural architecture search method DARTS. With 25% parameter reduction, the classification accuracy is improved by 0.13% over previous DARTS models. Code can be found at: https://github.com/zhoudaquan/rethinking_bottleneck_design.



### Automatically Generating Codes from Graphical Screenshots Based on Deep Autocoder
- **Arxiv ID**: http://arxiv.org/abs/2007.02272v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.02272v1)
- **Published**: 2020-07-05 09:40:48+00:00
- **Updated**: 2020-07-05 09:40:48+00:00
- **Authors**: Xiaoling Huang, Feng Liao
- **Comment**: None
- **Journal**: None
- **Summary**: During software front-end development, the work to convert Graphical User Interface(GUI) image to the corresponding front-end code is an inevitable tedious work. There have been some attempts to make this work to be automatic. However, the GUI code generated by these models is not accurate due to the lack of attention mechanism guidance. To solve this problem, we propose PixCoder based on an artificially supervised attention mechanism. The approach is to train a neural network to predict the style sheets in the input GUI image and then output a vector. PixCoder generate the GUI code targeting specific platform according to the output vector. The experimental results have shown the accuracy of the GUI code generated by PixCoder is over 95%.



### Weakly Supervised Domain Adaptation for Built-up Region Segmentation in Aerial and Satellite Imagery
- **Arxiv ID**: http://arxiv.org/abs/2007.02277v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.02277v1)
- **Published**: 2020-07-05 10:05:01+00:00
- **Updated**: 2020-07-05 10:05:01+00:00
- **Authors**: Javed Iqbal, Mohsen Ali
- **Comment**: Accepted at ISPRS Journal of Photogrammetry and Remote Sensing
- **Journal**: None
- **Summary**: This paper proposes a novel domain adaptation algorithm to handle the challenges posed by the satellite and aerial imagery, and demonstrates its effectiveness on the built-up region segmentation problem. Built-up area estimation is an important component in understanding the human impact on the environment, the effect of public policy, and general urban population analysis. The diverse nature of aerial and satellite imagery and lack of labeled data covering this diversity makes machine learning algorithms difficult to generalize for such tasks, especially across multiple domains. On the other hand, due to the lack of strong spatial context and structure, in comparison to the ground imagery, the application of existing unsupervised domain adaptation methods results in the sub-optimal adaptation. We thoroughly study the limitations of existing domain adaptation methods and propose a weakly-supervised adaptation strategy where we assume image-level labels are available for the target domain. More specifically, we design a built-up area segmentation network (as encoder-decoder), with an image classification head added to guide the adaptation. The devised system is able to address the problem of visual differences in multiple satellite and aerial imagery datasets, ranging from high resolution (HR) to very high resolution (VHR). A realistic and challenging HR dataset is created by hand-tagging the 73.4 sq-km of Rwanda, capturing a variety of build-up structures over different terrain. The developed dataset is spatially rich compared to existing datasets and covers diverse built-up scenarios including built-up areas in forests and deserts, mud houses, tin, and colored rooftops. Extensive experiments are performed by adapting from the single-source domain, to segment out the target domain. We achieve high gains ranging 11.6%-52% in IoU over the existing state-of-the-art methods.



### TilinGNN: Learning to Tile with Self-Supervised Graph Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2007.02278v1
- **DOI**: 10.1145/3386569.3392380
- **Categories**: **cs.CV**, cs.CG, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.02278v1)
- **Published**: 2020-07-05 10:06:06+00:00
- **Updated**: 2020-07-05 10:06:06+00:00
- **Authors**: Hao Xu, Ka Hei Hui, Chi-Wing Fu, Hao Zhang
- **Comment**: SIGGRAPH 2020, Technical paper. ACM Trans. Graph., Vol. 39, No. 4,
  Article 129. Homapage:
  https://appsrv.cse.cuhk.edu.hk/~haoxu/projects/TilinGnn/index.html
- **Journal**: None
- **Summary**: We introduce the first neural optimization framework to solve a classical instance of the tiling problem. Namely, we seek a non-periodic tiling of an arbitrary 2D shape using one or more types of tiles: the tiles maximally fill the shape's interior without overlaps or holes. To start, we reformulate tiling as a graph problem by modeling candidate tile locations in the target shape as graph nodes and connectivity between tile locations as edges. Further, we build a graph convolutional neural network, coined TilinGNN, to progressively propagate and aggregate features over graph edges and predict tile placements. TilinGNN is trained by maximizing the tiling coverage on target shapes, while avoiding overlaps and holes between the tiles. Importantly, our network is self-supervised, as we articulate these criteria as loss terms defined on the network outputs, without the need of ground-truth tiling solutions. After training, the runtime of TilinGNN is roughly linear to the number of candidate tile locations, significantly outperforming traditional combinatorial search. We conducted various experiments on a variety of shapes to showcase the speed and versatility of TilinGNN. We also present comparisons to alternative methods and manual solutions, robustness analysis, and ablation studies to demonstrate the quality of our approach.



### Multi view stereo with semantic priors
- **Arxiv ID**: http://arxiv.org/abs/2007.02295v1
- **DOI**: 10.5194/isprs-archives-XLII-2-W15-1135-2019
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.02295v1)
- **Published**: 2020-07-05 11:30:29+00:00
- **Updated**: 2020-07-05 11:30:29+00:00
- **Authors**: Elisavet Konstantina Stathopoulou, Fabio Remondino
- **Comment**: None
- **Journal**: None
- **Summary**: Patch-based stereo is nowadays a commonly used image-based technique for dense 3D reconstruction in large scale multi-view applications. The typical steps of such a pipeline can be summarized in stereo pair selection, depth map computation, depth map refinement and, finally, fusion in order to generate a complete and accurate representation of the scene in 3D. In this study, we aim to support the standard dense 3D reconstruction of scenes as implemented in the open source library OpenMVS by using semantic priors. To this end, during the depth map fusion step, along with the depth consistency check between depth maps of neighbouring views referring to the same part of the 3D scene, we impose extra semantic constraints in order to remove possible errors and selectively obtain segmented point clouds per label, boosting automation towards this direction. I n order to reassure semantic coherence between neighbouring views, additional semantic criterions can be considered, aiming to elim inate mismatches of pixels belonging in different classes.



### Collaborative Unsupervised Domain Adaptation for Medical Image Diagnosis
- **Arxiv ID**: http://arxiv.org/abs/2007.07222v1
- **DOI**: 10.1109/TIP.2020.3006377
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.07222v1)
- **Published**: 2020-07-05 11:49:17+00:00
- **Updated**: 2020-07-05 11:49:17+00:00
- **Authors**: Yifan Zhang, Ying Wei, Qingyao Wu, Peilin Zhao, Shuaicheng Niu, Junzhou Huang, Mingkui Tan
- **Comment**: IEEE Transactions on Image Processing
- **Journal**: None
- **Summary**: Deep learning based medical image diagnosis has shown great potential in clinical medicine. However, it often suffers two major difficulties in real-world applications: 1) only limited labels are available for model training, due to expensive annotation costs over medical images; 2) labeled images may contain considerable label noise (e.g., mislabeling labels) due to diagnostic difficulties of diseases. To address these, we seek to exploit rich labeled data from relevant domains to help the learning in the target task via {Unsupervised Domain Adaptation} (UDA). Unlike most UDA methods that rely on clean labeled data or assume samples are equally transferable, we innovatively propose a Collaborative Unsupervised Domain Adaptation algorithm, which conducts transferability-aware adaptation and conquers label noise in a collaborative way. We theoretically analyze the generalization performance of the proposed method, and also empirically evaluate it on both medical and general images. Promising experimental results demonstrate the superiority and generalization of the proposed method.



### Radial Intersection Count Image: a Clutter Resistant 3D Shape Descriptor
- **Arxiv ID**: http://arxiv.org/abs/2007.02306v1
- **DOI**: 10.1016/j.cag.2020.07.007
- **Categories**: **cs.CV**, cs.CG
- **Links**: [PDF](http://arxiv.org/pdf/2007.02306v1)
- **Published**: 2020-07-05 12:11:38+00:00
- **Updated**: 2020-07-05 12:11:38+00:00
- **Authors**: Bart Iver van Blokland, Theoharis Theoharis
- **Comment**: 18 pages, 16 figures, to be published in Computers & Graphics
- **Journal**: Computers & Graphics, Volume 91, 2020, Pages 118-128
- **Summary**: A novel shape descriptor for cluttered scenes is presented, the Radial Intersection Count Image (RICI), and is shown to significantly outperform the classic Spin Image (SI) and 3D Shape Context (3DSC) in both uncluttered and, more significantly, cluttered scenes. It is also faster to compute and compare. The clutter resistance of the RICI is mainly due to the design of a novel distance function, capable of disregarding clutter to a great extent. As opposed to the SI and 3DSC, which both count point samples, the RICI uses intersection counts with the mesh surface, and is therefore noise-free. For efficient RICI construction, novel algorithms of general interest were developed. These include an efficient circle-triangle intersection algorithm and an algorithm for projecting a point into SI-like ($\alpha$, $\beta$) coordinates. The 'clutterbox experiment' is also introduced as a better way of evaluating descriptors' response to clutter. The SI, 3DSC, and RICI are evaluated in this framework and the advantage of the RICI is clearly demonstrated.



### Reflection Backdoor: A Natural Backdoor Attack on Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2007.02343v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.02343v2)
- **Published**: 2020-07-05 13:56:48+00:00
- **Updated**: 2020-07-13 13:46:10+00:00
- **Authors**: Yunfei Liu, Xingjun Ma, James Bailey, Feng Lu
- **Comment**: Accepted by ECCV-2020
- **Journal**: None
- **Summary**: Recent studies have shown that DNNs can be compromised by backdoor attacks crafted at training time. A backdoor attack installs a backdoor into the victim model by injecting a backdoor pattern into a small proportion of the training data. At test time, the victim model behaves normally on clean test data, yet consistently predicts a specific (likely incorrect) target class whenever the backdoor pattern is present in a test example. While existing backdoor attacks are effective, they are not stealthy. The modifications made on training data or labels are often suspicious and can be easily detected by simple data filtering or human inspection. In this paper, we present a new type of backdoor attack inspired by an important natural phenomenon: reflection. Using mathematical modeling of physical reflection models, we propose reflection backdoor (Refool) to plant reflections as backdoor into a victim model. We demonstrate on 3 computer vision tasks and 5 datasets that, Refool can attack state-of-the-art DNNs with high success rate, and is resistant to state-of-the-art backdoor defenses.



### HoughNet: Integrating near and long-range evidence for bottom-up object detection
- **Arxiv ID**: http://arxiv.org/abs/2007.02355v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.02355v3)
- **Published**: 2020-07-05 14:45:01+00:00
- **Updated**: 2020-07-24 07:12:13+00:00
- **Authors**: Nermin Samet, Samet Hicsonmez, Emre Akbas
- **Comment**: ECCV 2020 camera-ready version
- **Journal**: None
- **Summary**: This paper presents HoughNet, a one-stage, anchor-free, voting-based, bottom-up object detection method. Inspired by the Generalized Hough Transform, HoughNet determines the presence of an object at a certain location by the sum of the votes cast on that location. Votes are collected from both near and long-distance locations based on a log-polar vote field. Thanks to this voting mechanism, HoughNet is able to integrate both near and long-range, class-conditional evidence for visual recognition, thereby generalizing and enhancing current object detection methodology, which typically relies on only local evidence. On the COCO dataset, HoughNet's best model achieves 46.4 $AP$ (and 65.1 $AP_{50}$), performing on par with the state-of-the-art in bottom-up object detection and outperforming most major one-stage and two-stage methods. We further validate the effectiveness of our proposal in another task, namely, "labels to photo" image generation by integrating the voting module of HoughNet to two different GAN models and showing that the accuracy is significantly improved in both cases. Code is available at https://github.com/nerminsamet/houghnet.



### Self-supervised Depth Estimation to Regularise Semantic Segmentation in Knee Arthroscopy
- **Arxiv ID**: http://arxiv.org/abs/2007.02361v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.02361v1)
- **Published**: 2020-07-05 15:13:44+00:00
- **Updated**: 2020-07-05 15:13:44+00:00
- **Authors**: Fengbei Liu, Yaqub Jonmohamadi, Gabriel Maicas, Ajay K. Pandey, Gustavo Carneiro
- **Comment**: 10 pages, 6 figures
- **Journal**: MICAAI 2020
- **Summary**: Intra-operative automatic semantic segmentation of knee joint structures can assist surgeons during knee arthroscopy in terms of situational awareness. However, due to poor imaging conditions (e.g., low texture, overexposure, etc.), automatic semantic segmentation is a challenging scenario, which justifies the scarce literature on this topic. In this paper, we propose a novel self-supervised monocular depth estimation to regularise the training of the semantic segmentation in knee arthroscopy. To further regularise the depth estimation, we propose the use of clean training images captured by the stereo arthroscope of routine objects (presenting none of the poor imaging conditions and with rich texture information) to pre-train the model. We fine-tune such model to produce both the semantic segmentation and self-supervised monocular depth using stereo arthroscopic images taken from inside the knee. Using a data set containing 3868 arthroscopic images captured during cadaveric knee arthroscopy with semantic segmentation annotations, 2000 stereo image pairs of cadaveric knee arthroscopy, and 2150 stereo image pairs of routine objects, we show that our semantic segmentation regularised by self-supervised depth estimation produces a more accurate segmentation than a state-of-the-art semantic segmentation approach modeled exclusively with semantic segmentation annotation.



### Aligning Partially Overlapping Point Sets: an Inner Approximation Algorithm
- **Arxiv ID**: http://arxiv.org/abs/2007.02363v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.02363v1)
- **Published**: 2020-07-05 15:23:33+00:00
- **Updated**: 2020-07-05 15:23:33+00:00
- **Authors**: Wei Lian, WangMeng Zuo, Lei Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Aligning partially overlapping point sets where there is no prior information about the value of the transformation is a challenging problem in computer vision. To achieve this goal, we first reduce the objective of the robust point matching algorithm to a function of a low dimensional variable. The resulting function, however, is only concave over a finite region including the feasible region. To cope with this issue, we employ the inner approximation optimization algorithm which only operates within the region where the objective function is concave. Our algorithm does not need regularization on transformation, and thus can handle the situation where there is no prior information about the values of the transformations. Our method is also $\epsilon-$globally optimal and thus is guaranteed to be robust. Moreover, its most computationally expensive subroutine is a linear assignment problem which can be efficiently solved. Experimental results demonstrate the better robustness of the proposed method over state-of-the-art algorithms. Our method is also efficient when the number of transformation parameters is small.



### GanglionNet: Objectively Assess the Density and Distribution of Ganglion Cells With NABLA-N Network
- **Arxiv ID**: http://arxiv.org/abs/2007.02367v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.02367v1)
- **Published**: 2020-07-05 15:46:13+00:00
- **Updated**: 2020-07-05 15:46:13+00:00
- **Authors**: Md Zahangir Alom, Raj P. Kapur, TJ Browen, Vijayan K. Asari
- **Comment**: 8 pages, 8 figures, 2 Tables
- **Journal**: None
- **Summary**: Hirschsprungs disease (HD) is a birth defect which is diagnosed and managed by multiple medical specialties such as pediatric gastroenterology, surgery, radiology, and pathology. HD is characterized by absence of ganglion cells in the distal intestinal tract with a gradual normalization of ganglion cell numbers in adjacent upstream bowel, termed as the transition zone (TZ). Definitive surgical management to remove the abnormal bowel requires accurate assessment of ganglion cell density in histological sections from the TZ, which is difficult, time-consuming and prone to operator error. We present an automated method to detect and count immunostained ganglion cells using a new NABLA_N network based deep learning (DL) approach, called GanglionNet. The morphological image analysis methods are applied for refinement of the regions for counting of the cells and define ganglia regions (a set of ganglion cells) from the predicted masks. The proposed model is trained with single point annotated samples by the expert pathologist. The GanglionNet is tested on ten completely new High Power Field (HPF) images with dimension of 2560x1920 pixels and the outputs are compared against the manual counting results by the expert pathologist. The proposed method shows a robust 97.49% detection accuracy for ganglion cells, when compared to counts by the expert pathologist, which demonstrates the robustness of GanglionNet. The proposed DL based ganglion cell detection and counting method will simplify and standardize TZ diagnosis for HD patients.



### Detail Preserved Point Cloud Completion via Separated Feature Aggregation
- **Arxiv ID**: http://arxiv.org/abs/2007.02374v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CG
- **Links**: [PDF](http://arxiv.org/pdf/2007.02374v1)
- **Published**: 2020-07-05 16:11:55+00:00
- **Updated**: 2020-07-05 16:11:55+00:00
- **Authors**: Wenxiao Zhang, Qingan Yan, Chunxia Xiao
- **Comment**: To be appeared in ECCV 2020
- **Journal**: None
- **Summary**: Point cloud shape completion is a challenging problem in 3D vision and robotics. Existing learning-based frameworks leverage encoder-decoder architectures to recover the complete shape from a highly encoded global feature vector. Though the global feature can approximately represent the overall shape of 3D objects, it would lead to the loss of shape details during the completion process. In this work, instead of using a global feature to recover the whole complete surface, we explore the functionality of multi-level features and aggregate different features to represent the known part and the missing part separately. We propose two different feature aggregation strategies, named global \& local feature aggregation(GLFA) and residual feature aggregation(RFA), to express the two kinds of features and reconstruct coordinates from their combination. In addition, we also design a refinement component to prevent the generated point cloud from non-uniform distribution and outliers. Extensive experiments have been conducted on the ShapeNet dataset. Qualitative and quantitative evaluations demonstrate that our proposed network outperforms current state-of-the art methods especially on detail preservation.



### Auto-captions on GIF: A Large-scale Video-sentence Dataset for Vision-language Pre-training
- **Arxiv ID**: http://arxiv.org/abs/2007.02375v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2007.02375v1)
- **Published**: 2020-07-05 16:11:57+00:00
- **Updated**: 2020-07-05 16:11:57+00:00
- **Authors**: Yingwei Pan, Yehao Li, Jianjie Luo, Jun Xu, Ting Yao, Tao Mei
- **Comment**: The Auto-captions on GIF dataset is available at
  \url{http://www.auto-video-captions.top/2020/dataset}
- **Journal**: None
- **Summary**: In this work, we present Auto-captions on GIF, which is a new large-scale pre-training dataset for generic video understanding. All video-sentence pairs are created by automatically extracting and filtering video caption annotations from billions of web pages. Auto-captions on GIF dataset can be utilized to pre-train the generic feature representation or encoder-decoder structure for video captioning, and other downstream tasks (e.g., sentence localization in videos, video question answering, etc.) as well. We present a detailed analysis of Auto-captions on GIF dataset in comparison to existing video-sentence datasets. We also provide an evaluation of a Transformer-based encoder-decoder structure for vision-language pre-training, which is further adapted to video captioning downstream task and yields the compelling generalizability on MSR-VTT. The dataset is available at \url{http://www.auto-video-captions.top/2020/dataset}.



### MetaConcept: Learn to Abstract via Concept Graph for Weakly-Supervised Few-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2007.02379v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.02379v2)
- **Published**: 2020-07-05 16:29:09+00:00
- **Updated**: 2021-05-11 04:54:04+00:00
- **Authors**: Baoquan Zhang, Ka-Cheong Leung, Yunming Ye, Xutao Li
- **Comment**: Accepted by Pattern Recognition 2021
- **Journal**: None
- **Summary**: Meta-learning has been proved to be an effective framework to address few-shot learning problems. The key challenge is how to minimize the generalization error of base learner across tasks. In this paper, we explore the concept hierarchy knowledge by leveraging concept graph, and take the concept graph as explicit meta-knowledge for the base learner, instead of learning implicit meta-knowledge, so as to boost the classification performance of meta-learning on weakly-supervised few-shot learning problems. To this end, we propose a novel meta-learning framework, called MetaConcept, which learns to abstract concepts via the concept graph. Specifically, we firstly propose a novel regularization with multi-level conceptual abstraction to constrain a meta-learner to learn to abstract concepts via the concept graph (i.e. identifying the concepts from low to high levels). Then, we propose a meta concept inference network as the meta-learner for the base learner, aiming to quickly adapt to a novel task by the joint inference of the abstract concepts and a few annotated samples. We have conducted extensive experiments on two weakly-supervised few-shot learning benchmarks, namely, WS-ImageNet-Pure and WS-ImageNet-Mix. Our experimental results show that 1) the proposed MetaConcept outperforms state-of-the-art methods with an improvement of 2% to 6% in classification accuracy; 2) the proposed MetaConcept can be able to yield a good performance though merely training with weakly-labeled data sets.



### Simplicial Complex based Point Correspondence between Images warped onto Manifolds
- **Arxiv ID**: http://arxiv.org/abs/2007.02381v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.02381v3)
- **Published**: 2020-07-05 16:41:08+00:00
- **Updated**: 2020-07-29 15:12:41+00:00
- **Authors**: Charu Sharma, Manohar Kaul
- **Comment**: Accepted at ECCV 2020
- **Journal**: None
- **Summary**: Recent increase in the availability of warped images projected onto a manifold (e.g., omnidirectional spherical images), coupled with the success of higher-order assignment methods, has sparked an interest in the search for improved higher-order matching algorithms on warped images due to projection. Although currently, several existing methods "flatten" such 3D images to use planar graph / hypergraph matching methods, they still suffer from severe distortions and other undesired artifacts, which result in inaccurate matching. Alternatively, current planar methods cannot be trivially extended to effectively match points on images warped onto manifolds. Hence, matching on these warped images persists as a formidable challenge. In this paper, we pose the assignment problem as finding a bijective map between two graph induced simplicial complexes, which are higher-order analogues of graphs. We propose a constrained quadratic assignment problem (QAP) that matches each p-skeleton of the simplicial complexes, iterating from the highest to the lowest dimension. The accuracy and robustness of our approach are illustrated on both synthetic and real-world spherical / warped (projected) images with known ground-truth correspondences. We significantly outperform existing state-of-the-art spherical matching methods on a diverse set of datasets.



### Learning Color Compatibility in Fashion Outfits
- **Arxiv ID**: http://arxiv.org/abs/2007.02388v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.02388v1)
- **Published**: 2020-07-05 17:09:31+00:00
- **Updated**: 2020-07-05 17:09:31+00:00
- **Authors**: Heming Zhang, Xuewen Yang, Jianchao Tan, Chi-Hao Wu, Jue Wang, C. -C. Jay Kuo
- **Comment**: None
- **Journal**: None
- **Summary**: Color compatibility is important for evaluating the compatibility of a fashion outfit, yet it was neglected in previous studies. We bring this important problem to researchers' attention and present a compatibility learning framework as solution to various fashion tasks. The framework consists of a novel way to model outfit compatibility and an innovative learning scheme. Specifically, we model the outfits as graphs and propose a novel graph construction to better utilize the power of graph neural networks. Then we utilize both ground-truth labels and pseudo labels to train the compatibility model in a weakly-supervised manner.Extensive experimental results verify the importance of color compatibility alone with the effectiveness of our framework. With color information alone, our model's performance is already comparable to previous methods that use deep image features. Our full model combining the aforementioned contributions set the new state-of-the-art in fashion compatibility prediction.



### Deep Convolutional Neural Network for Identifying Seam-Carving Forgery
- **Arxiv ID**: http://arxiv.org/abs/2007.02393v2
- **DOI**: 10.1109/TCSVT.2020.3037662
- **Categories**: **cs.MM**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.02393v2)
- **Published**: 2020-07-05 17:20:51+00:00
- **Updated**: 2020-07-07 09:33:23+00:00
- **Authors**: Seung-Hun Nam, Wonhyuk Ahn, In-Jae Yu, Myung-Joon Kwon, Minseok Son, Heung-Kyu Lee
- **Comment**: None
- **Journal**: None
- **Summary**: Seam carving is a representative content-aware image retargeting approach to adjust the size of an image while preserving its visually prominent content. To maintain visually important content, seam-carving algorithms first calculate the connected path of pixels, referred to as the seam, according to a defined cost function and then adjust the size of an image by removing and duplicating repeatedly calculated seams. Seam carving is actively exploited to overcome diversity in the resolution of images between applications and devices; hence, detecting the distortion caused by seam carving has become important in image forensics. In this paper, we propose a convolutional neural network (CNN)-based approach to classifying seam-carving-based image retargeting for reduction and expansion. To attain the ability to learn low-level features, we designed a CNN architecture comprising five types of network blocks specialized for capturing subtle signals. An ensemble module is further adopted to both enhance performance and comprehensively analyze the features in the local areas of the given image. To validate the effectiveness of our work, extensive experiments based on various CNN-based baselines were conducted. Compared to the baselines, our work exhibits state-of-the-art performance in terms of three-class classification (original, seam inserted, and seam removed). In addition, our model with the ensemble module is robust for various unseen cases. The experimental results also demonstrate that our method can be applied to localize both seam-removed and seam-inserted areas.



### Meta-Semi: A Meta-learning Approach for Semi-supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2007.02394v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.02394v3)
- **Published**: 2020-07-05 17:31:14+00:00
- **Updated**: 2021-09-07 17:31:52+00:00
- **Authors**: Yulin Wang, Jiayi Guo, Shiji Song, Gao Huang
- **Comment**: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible
- **Journal**: None
- **Summary**: Deep learning based semi-supervised learning (SSL) algorithms have led to promising results in recent years. However, they tend to introduce multiple tunable hyper-parameters, making them less practical in real SSL scenarios where the labeled data is scarce for extensive hyper-parameter search. In this paper, we propose a novel meta-learning based SSL algorithm (Meta-Semi) that requires tuning only one additional hyper-parameter, compared with a standard supervised deep learning algorithm, to achieve competitive performance under various conditions of SSL. We start by defining a meta optimization problem that minimizes the loss on labeled data through dynamically reweighting the loss on unlabeled samples, which are associated with soft pseudo labels during training. As the meta problem is computationally intensive to solve directly, we propose an efficient algorithm to dynamically obtain the approximate solutions. We show theoretically that Meta-Semi converges to the stationary point of the loss function on labeled data under mild conditions. Empirically, Meta-Semi outperforms state-of-the-art SSL algorithms significantly on the challenging semi-supervised CIFAR-100 and STL-10 tasks, and achieves competitive performance on CIFAR-10 and SVHN.



### Attention-based Joint Detection of Object and Semantic Part
- **Arxiv ID**: http://arxiv.org/abs/2007.02419v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.02419v1)
- **Published**: 2020-07-05 18:54:10+00:00
- **Updated**: 2020-07-05 18:54:10+00:00
- **Authors**: Keval Morabia, Jatin Arora, Tara Vijaykumar
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we address the problem of joint detection of objects like dog and its semantic parts like face, leg, etc. Our model is created on top of two Faster-RCNN models that share their features to perform a novel Attention-based feature fusion of related Object and Part features to get enhanced representations of both. These representations are used for final classification and bounding box regression separately for both models. Our experiments on the PASCAL-Part 2010 dataset show that joint detection can simultaneously improve both object detection and part detection in terms of mean Average Precision (mAP) at IoU=0.5.



### Contextual-Relation Consistent Domain Adaptation for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2007.02424v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.02424v2)
- **Published**: 2020-07-05 19:00:46+00:00
- **Updated**: 2020-07-15 12:22:36+00:00
- **Authors**: Jiaxing Huang, Shijian Lu, Dayan Guan, Xiaobing Zhang
- **Comment**: Accepted to ECCV 2020
- **Journal**: None
- **Summary**: Recent advances in unsupervised domain adaptation for semantic segmentation have shown great potentials to relieve the demand of expensive per-pixel annotations. However, most existing works address the domain discrepancy by aligning the data distributions of two domains at a global image level whereas the local consistencies are largely neglected. This paper presents an innovative local contextual-relation consistent domain adaptation (CrCDA) technique that aims to achieve local-level consistencies during the global-level alignment. The idea is to take a closer look at region-wise feature representations and align them for local-level consistencies. Specifically, CrCDA learns and enforces the prototypical local contextual-relations explicitly in the feature space of a labelled source domain while transferring them to an unlabelled target domain via backpropagation-based adversarial learning. An adaptive entropy max-min adversarial learning scheme is designed to optimally align these hundreds of local contextual-relations across domain without requiring discriminator or extra computation overhead. The proposed CrCDA has been evaluated extensively over two challenging domain adaptive segmentation tasks (e.g., GTA5 to Cityscapes and SYNTHIA to Cityscapes), and experiments demonstrate its superior segmentation performance as compared with state-of-the-art methods.



### Estimation of Ground Contacts from Human Gait by a Wearable Inertial Measurement Unit using machine learning
- **Arxiv ID**: http://arxiv.org/abs/2007.02433v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.02433v2)
- **Published**: 2020-07-05 19:47:06+00:00
- **Updated**: 2020-07-08 18:05:41+00:00
- **Authors**: Muhammad Junaid Umer, Qaiser Riaz
- **Comment**: Not completely discussed with supervisor need some improvements in
  article to prepare a final draft
- **Journal**: None
- **Summary**: Robotics system for rehabilitation of movement disorders and motion assistance are gaining increased intention. In this scenario estimation of ground contact is an active area of research in robotics and healthcare. This article addresses the estimation and classification of right and left foot during the healthy human gait based on the IMU sensor data of chest and lower back. For this purpose we have collected an IMU data of 48 subjects by using two smartphones at chest and lower back of the human body and one smart watch at right ankle of the body. To show the robustness of our approach data was collected at six different surfaces (road tiles carpet grass concrete and soil). The recorded data of lower back and chest sensor was segmented into single steps on the basis of right ankle sensor data, then we computed a total of 408 features from time frequency and wavelet domain of each segmented step. For classification task we have trained two machine learning classifiers SVM and RF with 10 fold cross validation method. We performed classification experiments at individual surfaces, hard surfaces, soft surfaces and all surfaces, highest accuracy was achieved at individual surfaces with accuracy index of 98.88%. Furthermore, classification rate at hard soft and all surface are 95.60%, 94.38% and 95.05% respectively. The results shows that estimation of ground contact form normal human walk at different surfaces can be performed with high accuracy using 6D data of angular velocities and accelerations from chest and lower back location of the body.



### GRAF: Generative Radiance Fields for 3D-Aware Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2007.02442v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.02442v4)
- **Published**: 2020-07-05 20:37:39+00:00
- **Updated**: 2021-03-30 11:33:35+00:00
- **Authors**: Katja Schwarz, Yiyi Liao, Michael Niemeyer, Andreas Geiger
- **Comment**: None
- **Journal**: Advances in Neural Information Processing Systems, NeurIPS 2020
- **Summary**: While 2D generative adversarial networks have enabled high-resolution image synthesis, they largely lack an understanding of the 3D world and the image formation process. Thus, they do not provide precise control over camera viewpoint or object pose. To address this problem, several recent approaches leverage intermediate voxel-based representations in combination with differentiable rendering. However, existing methods either produce low image resolution or fall short in disentangling camera and scene properties, e.g., the object identity may vary with the viewpoint. In this paper, we propose a generative model for radiance fields which have recently proven successful for novel view synthesis of a single scene. In contrast to voxel-based representations, radiance fields are not confined to a coarse discretization of the 3D space, yet allow for disentangling camera and scene properties while degrading gracefully in the presence of reconstruction ambiguity. By introducing a multi-scale patch-based discriminator, we demonstrate synthesis of high-resolution images while training our model from unposed 2D images alone. We systematically analyze our approach on several challenging synthetic and real-world datasets. Our experiments reveal that radiance fields are a powerful representation for generative image synthesis, leading to 3D consistent models that render with high fidelity.



### Pseudo-Rehearsal for Continual Learning with Normalizing Flows
- **Arxiv ID**: http://arxiv.org/abs/2007.02443v4
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.02443v4)
- **Published**: 2020-07-05 20:43:52+00:00
- **Updated**: 2021-08-05 15:32:02+00:00
- **Authors**: Jary Pomponi, Simone Scardapane, Aurelio Uncini
- **Comment**: A preliminary unpublished version of this work was presented in the
  LifelongML workshop, at ICML 2020
- **Journal**: None
- **Summary**: Catastrophic forgetting (CF) happens whenever a neural network overwrites past knowledge while being trained on new tasks. Common techniques to handle CF include regularization of the weights (using, e.g., their importance on past tasks), and rehearsal strategies, where the network is constantly re-trained on past data. Generative models have also been applied for the latter, in order to have endless sources of data. In this paper, we propose a novel method that combines the strengths of regularization and generative-based rehearsal approaches. Our generative model consists of a normalizing flow (NF), a probabilistic and invertible neural network, trained on the internal embeddings of the network. By keeping a single NF conditioned on the task, we show that our memory overhead remains constant. In addition, exploiting the invertibility of the NF, we propose a simple approach to regularize the network's embeddings with respect to past tasks. We show that our method performs favorably with respect to state-of-the-art approaches in the literature, with bounded computational power and memory overheads.



### Anatomical Data Augmentation via Fluid-based Image Registration
- **Arxiv ID**: http://arxiv.org/abs/2007.02447v1
- **DOI**: None
- **Categories**: **cs.CV**, I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2007.02447v1)
- **Published**: 2020-07-05 21:06:00+00:00
- **Updated**: 2020-07-05 21:06:00+00:00
- **Authors**: Zhengyang Shen, Zhenlin Xu, Sahin Olut, Marc Niethammer
- **Comment**: MICCAI 2020
- **Journal**: None
- **Summary**: We introduce a fluid-based image augmentation method for medical image analysis. In contrast to existing methods, our framework generates anatomically meaningful images via interpolation from the geodesic subspace underlying given samples. Our approach consists of three steps: 1) given a source image and a set of target images, we construct a geodesic subspace using the Large Deformation Diffeomorphic Metric Mapping (LDDMM) model; 2) we sample transformations from the resulting geodesic subspace; 3) we obtain deformed images and segmentations via interpolation. Experiments on brain (LPBA) and knee (OAI) data illustrate the performance of our approach on two tasks: 1) data augmentation during training and testing for image segmentation; 2) one-shot learning for single atlas image segmentation. We demonstrate that our approach generates anatomically meaningful data and improves performance on these tasks over competing approaches. Code is available at https://github.com/uncbiag/easyreg.



### Self-Challenging Improves Cross-Domain Generalization
- **Arxiv ID**: http://arxiv.org/abs/2007.02454v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.02454v1)
- **Published**: 2020-07-05 21:42:26+00:00
- **Updated**: 2020-07-05 21:42:26+00:00
- **Authors**: Zeyi Huang, Haohan Wang, Eric P. Xing, Dong Huang
- **Comment**: to appear at ECCV2020 as an oral paper
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNN) conduct image classification by activating dominant features that correlated with labels. When the training and testing data are under similar distributions, their dominant features are similar, which usually facilitates decent performance on the testing data. The performance is nonetheless unmet when tested on samples from different distributions, leading to the challenges in cross-domain image classification. We introduce a simple training heuristic, Representation Self-Challenging (RSC), that significantly improves the generalization of CNN to the out-of-domain data. RSC iteratively challenges (discards) the dominant features activated on the training data, and forces the network to activate remaining features that correlates with labels. This process appears to activate feature representations applicable to out-of-domain data without prior knowledge of new domain and without learning extra network parameters. We present theoretical properties and conditions of RSC for improving cross-domain generalization. The experiments endorse the simple, effective and architecture-agnostic nature of our RSC method.



### Using Capsule Neural Network to predict Tuberculosis in lens-free microscopic images
- **Arxiv ID**: http://arxiv.org/abs/2007.02457v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.02457v1)
- **Published**: 2020-07-05 22:18:13+00:00
- **Updated**: 2020-07-05 22:18:13+00:00
- **Authors**: Dennis Núñez-Fernández, Lamberto Ballan, Gabriel Jiménez-Avalos, Jorge Coronel, Mirko Zimic
- **Comment**: HSYS Workshop at ICML 2020
- **Journal**: None
- **Summary**: Tuberculosis, caused by a bacteria called Mycobacterium tuberculosis, is one of the most serious public health problems worldwide. This work seeks to facilitate and automate the prediction of tuberculosis by the MODS method and using lens-free microscopy, which is easy to use by untrained personnel. We employ the CapsNet architecture in our collected dataset and show that it has a better accuracy than traditional CNN architectures.



