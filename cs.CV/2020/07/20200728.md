# Arxiv Papers in cs.CV on 2020-07-28
### Demystifying Contrastive Self-Supervised Learning: Invariances, Augmentations and Dataset Biases
- **Arxiv ID**: http://arxiv.org/abs/2007.13916v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.13916v2)
- **Published**: 2020-07-28 00:11:31+00:00
- **Updated**: 2020-07-29 05:38:11+00:00
- **Authors**: Senthil Purushwalkam, Abhinav Gupta
- **Comment**: None
- **Journal**: None
- **Summary**: Self-supervised representation learning approaches have recently surpassed their supervised learning counterparts on downstream tasks like object detection and image classification. Somewhat mysteriously the recent gains in performance come from training instance classification models, treating each image and it's augmented versions as samples of a single class. In this work, we first present quantitative experiments to demystify these gains. We demonstrate that approaches like MOCO and PIRL learn occlusion-invariant representations. However, they fail to capture viewpoint and category instance invariance which are crucial components for object recognition. Second, we demonstrate that these approaches obtain further gains from access to a clean object-centric training dataset like Imagenet. Finally, we propose an approach to leverage unstructured videos to learn representations that possess higher viewpoint invariance. Our results show that the learned representations outperform MOCOv2 trained on the same data in terms of invariances encoded and the performance on downstream image classification and semantic segmentation tasks.



### Variants of BERT, Random Forests and SVM approach for Multimodal Emotion-Target Sub-challenge
- **Arxiv ID**: http://arxiv.org/abs/2007.13928v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.13928v1)
- **Published**: 2020-07-28 01:15:50+00:00
- **Updated**: 2020-07-28 01:15:50+00:00
- **Authors**: Hoang Manh Hung, Hyung-Jeong Yang, Soo-Hyung Kim, Guee-Sang Lee
- **Comment**: 3 pages, 2 figures
- **Journal**: None
- **Summary**: Emotion recognition has become a major problem in computer vision in recent years that made a lot of effort by researchers to overcome the difficulties in this task. In the field of affective computing, emotion recognition has a wide range of applications, such as healthcare, robotics, human-computer interaction. Due to its practical importance for other tasks, many techniques and approaches have been investigated for different problems and various data sources. Nevertheless, comprehensive fusion of the audio-visual and language modalities to get the benefits from them is still a problem to solve. In this paper, we present and discuss our classification methodology for MuSe-Topic Sub-challenge, as well as the data and results. For the topic classification, we ensemble two language models which are ALBERT and RoBERTa to predict 10 classes of topics. Moreover, for the classification of valence and arousal, SVM and Random forests are employed in conjunction with feature selection to enhance the performance.



### EasierPath: An Open-source Tool for Human-in-the-loop Deep Learning of Renal Pathology
- **Arxiv ID**: http://arxiv.org/abs/2007.13952v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.13952v1)
- **Published**: 2020-07-28 02:21:11+00:00
- **Updated**: 2020-07-28 02:21:11+00:00
- **Authors**: Zheyu Zhu, Yuzhe Lu, Ruining Deng, Haichun Yang, Agnes B. Fogo, Yuankai Huo
- **Comment**: None
- **Journal**: None
- **Summary**: Considerable morphological phenotyping studies in nephrology have emerged in the past few years, aiming to discover hidden regularities between clinical and imaging phenotypes. Such studies have been largely enabled by deep learning based image analysis to extract sparsely located targeting objects (e.g., glomeruli) on high-resolution whole slide images (WSI). However, such methods need to be trained using labor-intensive high-quality annotations, ideally labeled by pathologists. Inspired by the recent "human-in-the-loop" strategy, we developed EasierPath, an open-source tool to integrate human physicians and deep learning algorithms for efficient large-scale pathological image quantification as a loop. Using EasierPath, physicians are able to (1) optimize the recall and precision of deep learning object detection outcomes adaptively, (2) seamlessly support deep learning outcomes refining using either our EasierPath or prevalent ImageScope software without changing physician's user habit, and (3) manage and phenotype each object with user-defined classes. As a user case of EasierPath, we present the procedure of curating large-scale glomeruli in an efficient human-in-the-loop fashion (with two loops). From the experiments, the EasierPath saved 57 % of the annotation efforts to curate 8,833 glomeruli during the second loop. Meanwhile, the average precision of glomerular detection was leveraged from 0.504 to 0.620. The EasierPath software has been released as open-source to enable the large-scale glomerular prototyping. The code can be found in https://github.com/yuankaihuo/EasierPath



### Efficient OCT Image Segmentation Using Neural Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/2007.14790v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.14790v1)
- **Published**: 2020-07-28 02:48:07+00:00
- **Updated**: 2020-07-28 02:48:07+00:00
- **Authors**: Saba Heidari Gheshlaghi, Omid Dehzangi, Ali Dabouei, Annahita Amireskandari, Ali Rezai, Nasser M Nasrabadi
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we propose a Neural Architecture Search (NAS) for retinal layer segmentation in Optical Coherence Tomography (OCT) scans. We incorporate the Unet architecture in the NAS framework as its backbone for the segmentation of the retinal layers in our collected and pre-processed OCT image dataset. At the pre-processing stage, we conduct super resolution and image processing techniques on the raw OCT scans to improve the quality of the raw images. For our search strategy, different primitive operations are suggested to find the down- & up-sampling cell blocks, and the binary gate method is applied to make the search strategy practical for the task in hand. We empirically evaluated our method on our in-house OCT dataset. The experimental results demonstrate that the self-adapting NAS-Unet architecture substantially outperformed the competitive human-designed architecture by achieving 95.4% in mean Intersection over Union metric and 78.7% in Dice similarity coefficient.



### KOVIS: Keypoint-based Visual Servoing with Zero-Shot Sim-to-Real Transfer for Robotics Manipulation
- **Arxiv ID**: http://arxiv.org/abs/2007.13960v1
- **DOI**: 10.1109/IROS45743.2020.9341370
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.13960v1)
- **Published**: 2020-07-28 02:53:28+00:00
- **Updated**: 2020-07-28 02:53:28+00:00
- **Authors**: En Yen Puang, Keng Peng Tee, Wei Jing
- **Comment**: Accepted by IROS 2020
- **Journal**: None
- **Summary**: We present KOVIS, a novel learning-based, calibration-free visual servoing method for fine robotic manipulation tasks with eye-in-hand stereo camera system. We train the deep neural network only in the simulated environment; and the trained model could be directly used for real-world visual servoing tasks. KOVIS consists of two networks. The first keypoint network learns the keypoint representation from the image using with an autoencoder. Then the visual servoing network learns the motion based on keypoints extracted from the camera image. The two networks are trained end-to-end in the simulated environment by self-supervised learning without manual data labeling. After training with data augmentation, domain randomization, and adversarial examples, we are able to achieve zero-shot sim-to-real transfer to real-world robotic manipulation tasks. We demonstrate the effectiveness of the proposed method in both simulated environment and real-world experiment with different robotic manipulation tasks, including grasping, peg-in-hole insertion with 4mm clearance, and M13 screw insertion. The demo video is available at http://youtu.be/gfBJBR2tDzA



### Weakly Supervised 3D Object Detection from Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2007.13970v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.13970v1)
- **Published**: 2020-07-28 03:30:11+00:00
- **Updated**: 2020-07-28 03:30:11+00:00
- **Authors**: Zengyi Qin, Jinglu Wang, Yan Lu
- **Comment**: Accepted by ACM MM 2020
- **Journal**: None
- **Summary**: A crucial task in scene understanding is 3D object detection, which aims to detect and localize the 3D bounding boxes of objects belonging to specific classes. Existing 3D object detectors heavily rely on annotated 3D bounding boxes during training, while these annotations could be expensive to obtain and only accessible in limited scenarios. Weakly supervised learning is a promising approach to reducing the annotation requirement, but existing weakly supervised object detectors are mostly for 2D detection rather than 3D. In this work, we propose VS3D, a framework for weakly supervised 3D object detection from point clouds without using any ground truth 3D bounding box for training. First, we introduce an unsupervised 3D proposal module that generates object proposals by leveraging normalized point cloud densities. Second, we present a cross-modal knowledge distillation strategy, where a convolutional neural network learns to predict the final results from the 3D object proposals by querying a teacher network pretrained on image datasets. Comprehensive experiments on the challenging KITTI dataset demonstrate the superior performance of our VS3D in diverse evaluation settings. The source code and pretrained models are publicly available at https://github.com/Zengyi-Qin/Weakly-Supervised-3D-Object-Detection.



### Accurate, Low-Latency Visual Perception for Autonomous Racing:Challenges, Mechanisms, and Practical Solutions
- **Arxiv ID**: http://arxiv.org/abs/2007.13971v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2007.13971v1)
- **Published**: 2020-07-28 03:33:41+00:00
- **Updated**: 2020-07-28 03:33:41+00:00
- **Authors**: Kieran Strobel, Sibo Zhu, Raphael Chang, Skanda Koppula
- **Comment**: None
- **Journal**: None
- **Summary**: Autonomous racing provides the opportunity to test safety-critical perception pipelines at their limit. This paper describes the practical challenges and solutions to applying state-of-the-art computer vision algorithms to build a low-latency, high-accuracy perception system for DUT18 Driverless (DUT18D), a 4WD electric race car with podium finishes at all Formula Driverless competitions for which it raced. The key components of DUT18D include YOLOv3-based object detection, pose estimation, and time synchronization on its dual stereovision/monovision camera setup. We highlight modifications required to adapt perception CNNs to racing domains, improvements to loss functions used for pose estimation, and methodologies for sub-microsecond camera synchronization among other improvements. We perform a thorough experimental evaluation of the system, demonstrating its accuracy and low-latency in real-world racing scenarios.



### Self-supervised Neural Audio-Visual Sound Source Localization via Probabilistic Spatial Modeling
- **Arxiv ID**: http://arxiv.org/abs/2007.13976v1
- **DOI**: None
- **Categories**: **cs.SD**, cs.CV, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2007.13976v1)
- **Published**: 2020-07-28 03:52:53+00:00
- **Updated**: 2020-07-28 03:52:53+00:00
- **Authors**: Yoshiki Masuyama, Yoshiaki Bando, Kohei Yatabe, Yoko Sasaki, Masaki Onishi, Yasuhiro Oikawa
- **Comment**: Accepted for publication in 2020 IEEE/RSJ International Conference on
  Intelligent Robots and Systems (IROS)
- **Journal**: None
- **Summary**: Detecting sound source objects within visual observation is important for autonomous robots to comprehend surrounding environments. Since sounding objects have a large variety with different appearances in our living environments, labeling all sounding objects is impossible in practice. This calls for self-supervised learning which does not require manual labeling. Most of conventional self-supervised learning uses monaural audio signals and images and cannot distinguish sound source objects having similar appearances due to poor spatial information in audio signals. To solve this problem, this paper presents a self-supervised training method using 360{\deg} images and multichannel audio signals. By incorporating with the spatial information in multichannel audio signals, our method trains deep neural networks (DNNs) to distinguish multiple sound source objects. Our system for localizing sound source objects in the image is composed of audio and visual DNNs. The visual DNN is trained to localize sound source candidates within an input image. The audio DNN verifies whether each candidate actually produces sound or not. These DNNs are jointly trained in a self-supervised manner based on a probabilistic spatial audio model. Experimental results with simulated data showed that the DNNs trained by our method localized multiple speakers. We also demonstrate that the visual DNN detected objects including talking visitors and specific exhibits from real data recorded in a science museum.



### Monocular Real-Time Volumetric Performance Capture
- **Arxiv ID**: http://arxiv.org/abs/2007.13988v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG, cs.PF
- **Links**: [PDF](http://arxiv.org/pdf/2007.13988v1)
- **Published**: 2020-07-28 04:45:13+00:00
- **Updated**: 2020-07-28 04:45:13+00:00
- **Authors**: Ruilong Li, Yuliang Xiu, Shunsuke Saito, Zeng Huang, Kyle Olszewski, Hao Li
- **Comment**: None
- **Journal**: None
- **Summary**: We present the first approach to volumetric performance capture and novel-view rendering at real-time speed from monocular video, eliminating the need for expensive multi-view systems or cumbersome pre-acquisition of a personalized template model. Our system reconstructs a fully textured 3D human from each frame by leveraging Pixel-Aligned Implicit Function (PIFu). While PIFu achieves high-resolution reconstruction in a memory-efficient manner, its computationally expensive inference prevents us from deploying such a system for real-time applications. To this end, we propose a novel hierarchical surface localization algorithm and a direct rendering method without explicitly extracting surface meshes. By culling unnecessary regions for evaluation in a coarse-to-fine manner, we successfully accelerate the reconstruction by two orders of magnitude from the baseline without compromising the quality. Furthermore, we introduce an Online Hard Example Mining (OHEM) technique that effectively suppresses failure modes due to the rare occurrence of challenging examples. We adaptively update the sampling probability of the training data based on the current reconstruction accuracy, which effectively alleviates reconstruction artifacts. Our experiments and evaluations demonstrate the robustness of our system to various challenging angles, illuminations, poses, and clothing styles. We also show that our approach compares favorably with the state-of-the-art monocular performance capture. Our proposed approach removes the need for multi-view studio settings and enables a consumer-accessible solution for volumetric capture.



### Quantum-soft QUBO Suppression for Accurate Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2007.13992v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.13992v1)
- **Published**: 2020-07-28 05:12:51+00:00
- **Updated**: 2020-07-28 05:12:51+00:00
- **Authors**: Junde Li, Swaroop Ghosh
- **Comment**: Accepted on ECCV 2020
- **Journal**: None
- **Summary**: Non-maximum suppression (NMS) has been adopted by default for removing redundant object detections for decades. It eliminates false positives by only keeping the image M with highest detection score and images whose overlap ratio with M is less than a predefined threshold. However, this greedy algorithm may not work well for object detection under occlusion scenario where true positives with lower detection scores are possibly suppressed. In this paper, we first map the task of removing redundant detections into Quadratic Unconstrained Binary Optimization (QUBO) framework that consists of detection score from each bounding box and overlap ratio between pair of bounding boxes. Next, we solve the QUBO problem using the proposed Quantum-soft QUBO Suppression (QSQS) algorithm for fast and accurate detection by exploiting quantum computing advantages. Experiments indicate that QSQS improves mean average precision from 74.20% to 75.11% for PASCAL VOC 2007. It consistently outperforms NMS and soft-NMS for Reasonable subset of benchmark pedestrian detection CityPersons.



### Robust Ego and Object 6-DoF Motion Estimation and Tracking
- **Arxiv ID**: http://arxiv.org/abs/2007.13993v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.13993v1)
- **Published**: 2020-07-28 05:12:56+00:00
- **Updated**: 2020-07-28 05:12:56+00:00
- **Authors**: Jun Zhang, Mina Henein, Robert Mahony, Viorela Ila
- **Comment**: 7 pages, 6 figures, 5 tables, accepted in the IEEE/RSJ International
  Conference on Intelligent Robots and Systems (IROS) 2020
- **Journal**: None
- **Summary**: The problem of tracking self-motion as well as motion of objects in the scene using information from a camera is known as multi-body visual odometry and is a challenging task. This paper proposes a robust solution to achieve accurate estimation and consistent track-ability for dynamic multi-body visual odometry. A compact and effective framework is proposed leveraging recent advances in semantic instance-level segmentation and accurate optical flow estimation. A novel formulation, jointly optimizing SE(3) motion and optical flow is introduced that improves the quality of the tracked points and the motion estimation accuracy. The proposed approach is evaluated on the virtual KITTI Dataset and tested on the real KITTI Dataset, demonstrating its applicability to autonomous driving applications. For the benefit of the community, we make the source code public.



### Change Detection Using Synthetic Aperture Radar Videos
- **Arxiv ID**: http://arxiv.org/abs/2007.14001v1
- **DOI**: 10.5121/csit.2020.101011
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.14001v1)
- **Published**: 2020-07-28 05:53:10+00:00
- **Updated**: 2020-07-28 05:53:10+00:00
- **Authors**: Hasara Maithree, Dilan Dinushka, Adeesha Wijayasiri
- **Comment**: None
- **Journal**: AIRCC Publishing Corporation, Volume 10, Number 10, July 2020
- **Summary**: Many researches have been carried out for change detection using temporal SAR images. In this paper an algorithm for change detection using SAR videos has been proposed. There are various challenges related to SAR videos such as high level of speckle noise, rotation of SAR image frames of the video around a particular axis due to the circular movement of airborne vehicle, non-uniform back scattering of SAR pulses. Hence conventional change detection algorithms used for optical videos and SAR temporal images cannot be directly utilized for SAR videos. We propose an algorithm which is a combination of optical flow calculation using Lucas Kanade (LK) method and blob detection. The developed method follows a four steps approach: image filtering and enhancement, applying LK method, blob analysis and combining LK method with blob analysis. The performance of the developed approach was tested on SAR videos available on Sandia National Laboratories website and SAR videos generated by a SAR simulator.



### Spectral Superresolution of Multispectral Imagery with Joint Sparse and Low-Rank Learning
- **Arxiv ID**: http://arxiv.org/abs/2007.14006v1
- **DOI**: 10.1109/TGRS.2020.3000684
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.14006v1)
- **Published**: 2020-07-28 06:08:44+00:00
- **Updated**: 2020-07-28 06:08:44+00:00
- **Authors**: Lianru Gao, Danfeng Hong, Jing Yao, Bing Zhang, Paolo Gamba, Jocelyn Chanussot
- **Comment**: None
- **Journal**: IEEE Transactions on Geoscience and Remote Sensing, 2020
- **Summary**: Extensive attention has been widely paid to enhance the spatial resolution of hyperspectral (HS) images with the aid of multispectral (MS) images in remote sensing. However, the ability in the fusion of HS and MS images remains to be improved, particularly in large-scale scenes, due to the limited acquisition of HS images. Alternatively, we super-resolve MS images in the spectral domain by the means of partially overlapped HS images, yielding a novel and promising topic: spectral superresolution (SSR) of MS imagery. This is challenging and less investigated task due to its high ill-posedness in inverse imaging. To this end, we develop a simple but effective method, called joint sparse and low-rank learning (J-SLoL), to spectrally enhance MS images by jointly learning low-rank HS-MS dictionary pairs from overlapped regions. J-SLoL infers and recovers the unknown hyperspectral signals over a larger coverage by sparse coding on the learned dictionary pair. Furthermore, we validate the SSR performance on three HS-MS datasets (two for classification and one for unmixing) in terms of reconstruction, classification, and unmixing by comparing with several existing state-of-the-art baselines, showing the effectiveness and superiority of the proposed J-SLoL algorithm. Furthermore, the codes and datasets will be available at: https://github.com/danfenghong/IEEE\_TGRS\_J-SLoL, contributing to the RS community.



### Coupled Convolutional Neural Network with Adaptive Response Function Learning for Unsupervised Hyperspectral Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2007.14007v1
- **DOI**: 10.1109/TGRS.2020.3006534
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.14007v1)
- **Published**: 2020-07-28 06:17:02+00:00
- **Updated**: 2020-07-28 06:17:02+00:00
- **Authors**: Ke Zheng, Lianru Gao, Wenzhi Liao, Danfeng Hong, Bing Zhang, Ximin Cui, Jocelyn Chanussot
- **Comment**: None
- **Journal**: IEEE Transactions on Geoscience and Remote Sensing,2020
- **Summary**: Due to the limitations of hyperspectral imaging systems, hyperspectral imagery (HSI) often suffers from poor spatial resolution, thus hampering many applications of the imagery. Hyperspectral super-resolution refers to fusing HSI and MSI to generate an image with both high spatial and high spectral resolutions. Recently, several new methods have been proposed to solve this fusion problem, and most of these methods assume that the prior information of the Point Spread Function (PSF) and Spectral Response Function (SRF) are known. However, in practice, this information is often limited or unavailable. In this work, an unsupervised deep learning-based fusion method - HyCoNet - that can solve the problems in HSI-MSI fusion without the prior PSF and SRF information is proposed. HyCoNet consists of three coupled autoencoder nets in which the HSI and MSI are unmixed into endmembers and abundances based on the linear unmixing model. Two special convolutional layers are designed to act as a bridge that coordinates with the three autoencoder nets, and the PSF and SRF parameters are learned adaptively in the two convolution layers during the training process. Furthermore, driven by the joint loss function, the proposed method is straightforward and easily implemented in an end-to-end training manner. The experiments performed in the study demonstrate that the proposed method performs well and produces robust results for different datasets and arbitrary PSFs and SRFs.



### Superpixel Based Graph Laplacian Regularization for Sparse Hyperspectral Unmixing
- **Arxiv ID**: http://arxiv.org/abs/2007.14033v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.14033v2)
- **Published**: 2020-07-28 07:30:50+00:00
- **Updated**: 2020-09-12 13:12:49+00:00
- **Authors**: Taner Ince
- **Comment**: 5 pages
- **Journal**: None
- **Summary**: An efficient spatial regularization method using superpixel segmentation and graph Laplacian regularization is proposed for sparse hyperspectral unmixing method. Since it is likely to find spectrally similar pixels in a homogeneous region, we use a superpixel segmentation algorithm to extract the homogeneous regions by considering the image boundaries. We first extract the homogeneous regions, which are called superpixels, then a weighted graph in each superpixel is constructed by selecting $K$-nearest pixels in each superpixel. Each node in the graph represents the spectrum of a pixel and edges connect the similar pixels inside the superpixel. The spatial similarity is investigated using graph Laplacian regularization. Sparsity regularization for abundance matrix is provided using a weighted sparsity promoting norm. Experimental results on simulated and real data sets show the superiority of the proposed algorithm over the well-known algorithms in the literature.



### Risk-Averse MPC via Visual-Inertial Input and Recurrent Networks for Online Collision Avoidance
- **Arxiv ID**: http://arxiv.org/abs/2007.14035v1
- **DOI**: 10.1109/IROS45743.2020.9341070
- **Categories**: **cs.RO**, cs.CV, cs.LG, cs.SY, eess.SY
- **Links**: [PDF](http://arxiv.org/pdf/2007.14035v1)
- **Published**: 2020-07-28 07:34:30+00:00
- **Updated**: 2020-07-28 07:34:30+00:00
- **Authors**: Alexander Schperberg, Kenny Chen, Stephanie Tsuei, Michael Jewett, Joshua Hooks, Stefano Soatto, Ankur Mehta, Dennis Hong
- **Comment**: Accepted to the 2020 IEEE/RSJ International Conference on Intelligent
  Robots and Systems (IROS), Las Vegas, USA. First two authors contributed
  equally. For supplementary video, see
  https://www.youtube.com/watch?v=td4K55Tj-U8
- **Journal**: None
- **Summary**: In this paper, we propose an online path planning architecture that extends the model predictive control (MPC) formulation to consider future location uncertainties for safer navigation through cluttered environments. Our algorithm combines an object detection pipeline with a recurrent neural network (RNN) which infers the covariance of state estimates through each step of our MPC's finite time horizon. The RNN model is trained on a dataset that comprises of robot and landmark poses generated from camera images and inertial measurement unit (IMU) readings via a state-of-the-art visual-inertial odometry framework. To detect and extract object locations for avoidance, we use a custom-trained convolutional neural network model in conjunction with a feature extractor to retrieve 3D centroid and radii boundaries of nearby obstacles. The robustness of our methods is validated on complex quadruped robot dynamics and can be generally applied to most robotic platforms, demonstrating autonomous behaviors that can plan fast and collision-free paths towards a goal point.



### Toward Zero-Shot Unsupervised Image-to-Image Translation
- **Arxiv ID**: http://arxiv.org/abs/2007.14050v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.14050v1)
- **Published**: 2020-07-28 08:13:18+00:00
- **Updated**: 2020-07-28 08:13:18+00:00
- **Authors**: Yuanqi Chen, Xiaoming Yu, Shan Liu, Ge Li
- **Comment**: None
- **Journal**: None
- **Summary**: Recent studies have shown remarkable success in unsupervised image-to-image translation. However, if there has no access to enough images in target classes, learning a mapping from source classes to the target classes always suffers from mode collapse, which limits the application of the existing methods. In this work, we propose a zero-shot unsupervised image-to-image translation framework to address this limitation, by associating categories with their side information like attributes. To generalize the translator to previous unseen classes, we introduce two strategies for exploiting the space spanned by the semantic attributes. Specifically, we propose to preserve semantic relations to the visual space and expand attribute space by utilizing attribute vectors of unseen classes, thus encourage the translator to explore the modes of unseen classes. Quantitative and qualitative results on different datasets demonstrate the effectiveness of our proposed approach. Moreover, we demonstrate that our framework can be applied to many tasks, such as zero-shot classification and fashion design.



### Real-Time Point Cloud Fusion of Multi-LiDAR Infrastructure Sensor Setups with Unknown Spatial Location and Orientation
- **Arxiv ID**: http://arxiv.org/abs/2008.00801v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2008.00801v1)
- **Published**: 2020-07-28 08:43:39+00:00
- **Updated**: 2020-07-28 08:43:39+00:00
- **Authors**: Laurent Kloeker, Christian Kotulla, Lutz Eckstein
- **Comment**: Accepted to be published as part of the 23rd IEEE International
  Conference on Intelligent Transportation Systems (ITSC), Rhodes, Greece,
  September 20-23, 2020
- **Journal**: None
- **Summary**: The use of infrastructure sensor technology for traffic detection has already been proven several times. However, extrinsic sensor calibration is still a challenge for the operator. While previous approaches are unable to calibrate the sensors without the use of reference objects in the sensor field of view (FOV), we present an algorithm that is completely detached from external assistance and runs fully automatically. Our method focuses on the high-precision fusion of LiDAR point clouds and is evaluated in simulation as well as on real measurements. We set the LiDARs in a continuous pendulum motion in order to simulate real-world operation as closely as possible and to increase the demands on the algorithm. However, it does not receive any information about the initial spatial location and orientation of the LiDARs throughout the entire measurement period. Experiments in simulation as well as with real measurements have shown that our algorithm performs a continuous point cloud registration of up to four 64-layer LiDARs in real-time. The averaged resulting translational error is within a few centimeters and the averaged error in rotation is below 0.15 degrees.



### Improving Generative Adversarial Networks with Local Coordinate Coding
- **Arxiv ID**: http://arxiv.org/abs/2008.00942v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2008.00942v1)
- **Published**: 2020-07-28 09:17:50+00:00
- **Updated**: 2020-07-28 09:17:50+00:00
- **Authors**: Jiezhang Cao, Yong Guo, Qingyao Wu, Chunhua Shen, Junzhou Huang, Mingkui Tan
- **Comment**: 20 pages, 5 figures
- **Journal**: None
- **Summary**: Generative adversarial networks (GANs) have shown remarkable success in generating realistic data from some predefined prior distribution (e.g., Gaussian noises). However, such prior distribution is often independent of real data and thus may lose semantic information (e.g., geometric structure or content in images) of data. In practice, the semantic information might be represented by some latent distribution learned from data. However, such latent distribution may incur difficulties in data sampling for GANs. In this paper, rather than sampling from the predefined prior distribution, we propose an LCCGAN model with local coordinate coding (LCC) to improve the performance of generating data. First, we propose an LCC sampling method in LCCGAN to sample meaningful points from the latent manifold. With the LCC sampling method, we can exploit the local information on the latent manifold and thus produce new data with promising quality. Second, we propose an improved version, namely LCCGAN++, by introducing a higher-order term in the generator approximation. This term is able to achieve better approximation and thus further improve the performance. More critically, we derive the generalization bound for both LCCGAN and LCCGAN++ and prove that a low-dimensional input is sufficient to achieve good generalization performance. Extensive experiments on four benchmark datasets demonstrate the superiority of the proposed method over existing GANs.



### A Deep Learning-based Detector for Brown Spot Disease in Passion Fruit Plant Leaves
- **Arxiv ID**: http://arxiv.org/abs/2007.14103v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.14103v2)
- **Published**: 2020-07-28 10:17:43+00:00
- **Updated**: 2020-07-29 08:51:21+00:00
- **Authors**: Andrew Katumba, Moses Bomera, Cosmas Mwikirize, Gorret Namulondo, Mary Gorret Ajero, Idd Ramathani, Olivia Nakayima, Grace Nakabonge, Dorothy Okello, Jonathan Serugunda
- **Comment**: None
- **Journal**: None
- **Summary**: Pests and diseases pose a key challenge to passion fruit farmers across Uganda and East Africa in general. They lead to loss of investment as yields reduce and losses increases. As the majority of the farmers, including passion fruit farmers, in the country are smallholder farmers from low-income households, they do not have the sufficient information and means to combat these challenges. While, passion fruits have the potential to improve the well-being of these farmers as they have a short maturity period and high market value , without the required knowledge about the health of their crops, farmers cannot intervene promptly to turn the situation around.   For this work, we have partnered with the Uganda National Crop Research Institute (NaCRRI) to develop a dataset of expertly labelled passion fruit plant leaves and fruits, both diseased and healthy. We have made use of their extension service to collect images from 5 districts in Uganda,   With the dataset in place, we are employing state-of-the-art techniques in machine learning, and specifically deep learning, techniques at scale for object detection and classification to correctly determine the health status of passion fruit plants and provide an accurate diagnosis for positive detections.This work focuses on two major diseases woodiness (viral) and brown spot (fungal) diseases.



### WaveFuse: A Unified Deep Framework for Image Fusion with Discrete Wavelet Transform
- **Arxiv ID**: http://arxiv.org/abs/2007.14110v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.IT, cs.LG, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/2007.14110v4)
- **Published**: 2020-07-28 10:30:47+00:00
- **Updated**: 2021-10-11 05:37:27+00:00
- **Authors**: Shaolei Liu, Manning Wang, Zhijian Song
- **Comment**: accepted at The 28th International Conference on Neural Information
  Processing(ICONIP2021)
- **Journal**: None
- **Summary**: We propose an unsupervised image fusion architecture for multiple application scenarios based on the combination of multi-scale discrete wavelet transform through regional energy and deep learning. To our best knowledge, this is the first time the conventional image fusion method has been combined with deep learning. The useful information of feature maps can be utilized adequately through multi-scale discrete wavelet transform in our proposed method.Compared with other state-of-the-art fusion method, the proposed algorithm exhibits better fusion performance in both subjective and objective evaluation. Moreover, it's worth mentioning that comparable fusion performance trained in COCO dataset can be obtained by training with a much smaller dataset with only hundreds of images chosen randomly from COCO. Hence, the training time is shortened substantially, leading to the improvement of the model's performance both in practicality and training efficiency.



### DeScarGAN: Disease-Specific Anomaly Detection with Weak Supervision
- **Arxiv ID**: http://arxiv.org/abs/2007.14118v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.14118v1)
- **Published**: 2020-07-28 10:50:38+00:00
- **Updated**: 2020-07-28 10:50:38+00:00
- **Authors**: Julia Wolleb, Robin Sandkühler, Philippe C. Cattin
- **Comment**: None
- **Journal**: None
- **Summary**: Anomaly detection and localization in medical images is a challenging task, especially when the anomaly exhibits a change of existing structures, e.g., brain atrophy or changes in the pleural space due to pleural effusions. In this work, we present a weakly supervised and detail-preserving method that is able to detect structural changes of existing anatomical structures. In contrast to standard anomaly detection methods, our method extracts information about the disease characteristics from two groups: a group of patients affected by the same disease and a healthy control group. Together with identity-preserving mechanisms, this enables our method to extract highly disease-specific characteristics for a more detailed detection of structural changes. We designed a specific synthetic data set to evaluate and compare our method against state-of-the-art anomaly detection methods. Finally, we show the performance of our method on chest X-ray images. Our method called DeScarGAN outperforms other anomaly detection methods on the synthetic data set and by visual inspection on the chest X-ray image data set.



### Reachable Sets of Classifiers and Regression Models: (Non-)Robustness Analysis and Robust Training
- **Arxiv ID**: http://arxiv.org/abs/2007.14120v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.14120v2)
- **Published**: 2020-07-28 10:58:06+00:00
- **Updated**: 2021-05-12 16:38:47+00:00
- **Authors**: Anna-Kathrin Kopetzki, Stephan Günnemann
- **Comment**: Published as a journal paper at ECML PKDD 2021
- **Journal**: None
- **Summary**: Neural networks achieve outstanding accuracy in classification and regression tasks. However, understanding their behavior still remains an open challenge that requires questions to be addressed on the robustness, explainability and reliability of predictions. We answer these questions by computing reachable sets of neural networks, i.e. sets of outputs resulting from continuous sets of inputs. We provide two efficient approaches that lead to over- and under-approximations of the reachable set. This principle is highly versatile, as we show. First, we use it to analyze and enhance the robustness properties of both classifiers and regression models. This is in contrast to existing works, which are mainly focused on classification. Specifically, we verify (non-)robustness, propose a robust training procedure, and show that our approach outperforms adversarial attacks as well as state-of-the-art methods of verifying classifiers for non-norm bound perturbations. Second, we provide techniques to distinguish between reliable and non-reliable predictions for unlabeled inputs, to quantify the influence of each feature on a prediction, and compute a feature ranking.



### Multi-camera Torso Pose Estimation using Graph Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2007.14126v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.14126v1)
- **Published**: 2020-07-28 11:14:02+00:00
- **Updated**: 2020-07-28 11:14:02+00:00
- **Authors**: Daniel Rodriguez-Criado, Pilar Bachiller, Pablo Bustos, George Vogiatzis, Luis J. Manso
- **Comment**: 6 pages, accepted in ROMAN 2020
- **Journal**: None
- **Summary**: Estimating the location and orientation of humans is an essential skill for service and assistive robots. To achieve a reliable estimation in a wide area such as an apartment, multiple RGBD cameras are frequently used. Firstly, these setups are relatively expensive. Secondly, they seldom perform an effective data fusion using the multiple camera sources at an early stage of the processing pipeline. Occlusions and partial views make this second point very relevant in these scenarios. The proposal presented in this paper makes use of graph neural networks to merge the information acquired from multiple camera sources, achieving a mean absolute error below 125 mm for the location and 10 degrees for the orientation using low-resolution RGB images. The experiments, conducted in an apartment with three cameras, benchmarked two different graph neural network implementations and a third architecture based on fully connected layers. The software used has been released as open-source in a public repository (https://github.com/vangiel/WheresTheFellow).



### Nonnegative Low Rank Tensor Approximation and its Application to Multi-dimensional Images
- **Arxiv ID**: http://arxiv.org/abs/2007.14137v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.NA, eess.IV, math.NA
- **Links**: [PDF](http://arxiv.org/pdf/2007.14137v2)
- **Published**: 2020-07-28 11:52:19+00:00
- **Updated**: 2021-09-26 13:46:21+00:00
- **Authors**: Tai-Xiang Jiang, Michael K. Ng, Junjun Pan, Guangjing Song
- **Comment**: None
- **Journal**: None
- **Summary**: The main aim of this paper is to develop a new algorithm for computing nonnegative low rank tensor approximation for nonnegative tensors that arise in many multi-dimensional imaging applications. Nonnegativity is one of the important property as each pixel value refers to nonzero light intensity in image data acquisition. Our approach is different from classical nonnegative tensor factorization (NTF) which requires each factorized matrix and/or tensor to be nonnegative. In this paper, we determine a nonnegative low Tucker rank tensor to approximate a given nonnegative tensor. We propose an alternating projections algorithm for computing such nonnegative low rank tensor approximation, which is referred to as NLRT. The convergence of the proposed manifold projection method is established. Experimental results for synthetic data and multi-dimensional images are presented to demonstrate the performance of NLRT is better than state-of-the-art NTF methods.



### Learning Modality Interaction for Temporal Sentence Localization and Event Captioning in Videos
- **Arxiv ID**: http://arxiv.org/abs/2007.14164v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.14164v1)
- **Published**: 2020-07-28 12:40:59+00:00
- **Updated**: 2020-07-28 12:40:59+00:00
- **Authors**: Shaoxiang Chen, Wenhao Jiang, Wei Liu, Yu-Gang Jiang
- **Comment**: None
- **Journal**: None
- **Summary**: Automatically generating sentences to describe events and temporally localizing sentences in a video are two important tasks that bridge language and videos. Recent techniques leverage the multimodal nature of videos by using off-the-shelf features to represent videos, but interactions between modalities are rarely explored. Inspired by the fact that there exist cross-modal interactions in the human brain, we propose a novel method for learning pairwise modality interactions in order to better exploit complementary information for each pair of modalities in videos and thus improve performances on both tasks. We model modality interaction in both the sequence and channel levels in a pairwise fashion, and the pairwise interaction also provides some explainability for the predictions of target tasks. We demonstrate the effectiveness of our method and validate specific design choices through extensive ablation studies. Our method turns out to achieve state-of-the-art performances on four standard benchmark datasets: MSVD and MSR-VTT (event captioning task), and Charades-STA and ActivityNet Captions (temporal sentence localization task).



### Generative networks as inverse problems with fractional wavelet scattering networks
- **Arxiv ID**: http://arxiv.org/abs/2007.14177v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.14177v1)
- **Published**: 2020-07-28 12:58:15+00:00
- **Updated**: 2020-07-28 12:58:15+00:00
- **Authors**: Jiasong Wu, Jing Zhang, Fuzhi Wu, Youyong Kong, Guanyu Yang, Lotfi Senhadji, Huazhong Shu
- **Comment**: 27 pages, 13 figures, 6 tables
- **Journal**: None
- **Summary**: Deep learning is a hot research topic in the field of machine learning methods and applications. Generative Adversarial Networks (GANs) and Variational Auto-Encoders (VAEs) provide impressive image generations from Gaussian white noise, but both of them are difficult to train since they need to train the generator (or encoder) and the discriminator (or decoder) simultaneously, which is easy to cause unstable training. In order to solve or alleviate the synchronous training difficult problems of GANs and VAEs, recently, researchers propose Generative Scattering Networks (GSNs), which use wavelet scattering networks (ScatNets) as the encoder to obtain the features (or ScatNet embeddings) and convolutional neural networks (CNNs) as the decoder to generate the image. The advantage of GSNs is the parameters of ScatNets are not needed to learn, and the disadvantage of GSNs is that the expression ability of ScatNets is slightly weaker than CNNs and the dimensional reduction method of Principal Component Analysis (PCA) is easy to lead overfitting in the training of GSNs, and therefore affect the generated quality in the testing process. In order to further improve the quality of generated images while keep the advantages of GSNs, this paper proposes Generative Fractional Scattering Networks (GFRSNs), which use more expressive fractional wavelet scattering networks (FrScatNets) instead of ScatNets as the encoder to obtain the features (or FrScatNet embeddings) and use the similar CNNs of GSNs as the decoder to generate the image. Additionally, this paper develops a new dimensional reduction method named Feature-Map Fusion (FMF) instead of PCA for better keeping the information of FrScatNets and the effect of image fusion on the quality of image generation is also discussed.



### Optimization of XNOR Convolution for Binary Convolutional Neural Networks on GPU
- **Arxiv ID**: http://arxiv.org/abs/2007.14178v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.DC
- **Links**: [PDF](http://arxiv.org/pdf/2007.14178v1)
- **Published**: 2020-07-28 13:01:17+00:00
- **Updated**: 2020-07-28 13:01:17+00:00
- **Authors**: Mete Can Kaya, Alperen İnci, Alptekin Temizel
- **Comment**: None
- **Journal**: None
- **Summary**: Binary convolutional networks have lower computational load and lower memory foot-print compared to their full-precision counterparts. So, they are a feasible alternative for the deployment of computer vision applications on limited capacity embedded devices. Once trained on less resource-constrained computational environments, they can be deployed for real-time inference on such devices. In this study, we propose an implementation of binary convolutional network inference on GPU by focusing on optimization of XNOR convolution. Experimental results show that using GPU can provide a speed-up of up to $42.61\times$ with a kernel size of $3\times3$. The implementation is publicly available at https://github.com/metcan/Binary-Convolutional-Neural-Network-Inference-on-GPU



### Handling confounding variables in statistical shape analysis -- application to cardiac remodelling
- **Arxiv ID**: http://arxiv.org/abs/2007.14239v1
- **DOI**: 10.1016/j.media.2020.101792
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.14239v1)
- **Published**: 2020-07-28 14:05:34+00:00
- **Updated**: 2020-07-28 14:05:34+00:00
- **Authors**: Gabriel Bernardino, Oualid Benkarim, María Sanz-de la Garza, Susanna Prat-Gonzàlez, Álvaro Sepulveda-Martinez, Fàtima Crispi, Marta Sitges, Mathieu De Craene, Bart Bijnens, Miguel Ángel González Ballester
- **Comment**: This paper has been acccepted for publication in Medical Image
  Analysis. Please find the final version with its supplementary materials at
  doi.org/10.1016/j.media.2020.101792. Shared under license CC-BY-NC-ND
- **Journal**: None
- **Summary**: Statistical shape analysis is a powerful tool to assess organ morphologies and find shape changes associated to a particular disease. However, imbalance in confounding factors, such as demographics might invalidate the analysis if not taken into consideration. Despite the methodological advances in the field, providing new methods that are able to capture complex and regional shape differences, the relationship between non-imaging information and shape variability has been overlooked. We present a linear statistical shape analysis framework that finds shape differences unassociated to a controlled set of confounding variables. It includes two confounding correction methods: confounding deflation and adjustment. We applied our framework to a cardiac magnetic resonance imaging dataset, consisting of the cardiac ventricles of 89 triathletes and 77 controls, to identify cardiac remodelling due to the practice of endurance exercise. To test robustness to confounders, subsets of this dataset were generated by randomly removing controls with low body mass index, thus introducing imbalance. The analysis of the whole dataset indicates an increase of ventricular volumes and myocardial mass in athletes, which is consistent with the clinical literature. However, when confounders are not taken into consideration no increase of myocardial mass is found. Using the downsampled datasets, we find that confounder adjustment methods are needed to find the real remodelling patterns in imbalanced datasets.



### Efficient Adaptation of Neural Network Filter for Video Compression
- **Arxiv ID**: http://arxiv.org/abs/2007.14267v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2007.14267v2)
- **Published**: 2020-07-28 14:24:28+00:00
- **Updated**: 2020-08-13 09:07:25+00:00
- **Authors**: Yat-Hong Lam, Alireza Zare, Francesco Cricri, Jani Lainema, Miska Hannuksela
- **Comment**: Accepted in ACM Multimedia 2020
- **Journal**: None
- **Summary**: We present an efficient finetuning methodology for neural-network filters which are applied as a postprocessing artifact-removal step in video coding pipelines. The fine-tuning is performed at encoder side to adapt the neural network to the specific content that is being encoded. In order to maximize the PSNR gain and minimize the bitrate overhead, we propose to finetune only the convolutional layers' biases. The proposed method achieves convergence much faster than conventional finetuning approaches, making it suitable for practical applications. The weight-update can be included into the video bitstream generated by the existing video codecs. We show that our method achieves up to 9.7% average BD-rate gain when compared to the state-of-art Versatile Video Coding (VVC) standard codec on 7 test sequences.



### Faster Mean-shift: GPU-accelerated clustering for cosine embedding-based cell segmentation and tracking
- **Arxiv ID**: http://arxiv.org/abs/2007.14283v2
- **DOI**: 10.1016/j.media.2021.102048
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.14283v2)
- **Published**: 2020-07-28 14:52:51+00:00
- **Updated**: 2021-04-20 02:37:04+00:00
- **Authors**: Mengyang Zhao, Aadarsh Jha, Quan Liu, Bryan A. Millis, Anita Mahadevan-Jansen, Le Lu, Bennett A. Landman, Matthew J. Tyskac, Yuankai Huo
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, single-stage embedding based deep learning algorithms gain increasing attention in cell segmentation and tracking. Compared with the traditional "segment-then-associate" two-stage approach, a single-stage algorithm not only simultaneously achieves consistent instance cell segmentation and tracking but also gains superior performance when distinguishing ambiguous pixels on boundaries and overlaps. However, the deployment of an embedding based algorithm is restricted by slow inference speed (e.g., around 1-2 mins per frame). In this study, we propose a novel Faster Mean-shift algorithm, which tackles the computational bottleneck of embedding based cell segmentation and tracking. Different from previous GPU-accelerated fast mean-shift algorithms, a new online seed optimization policy (OSOP) is introduced to adaptively determine the minimal number of seeds, accelerate computation, and save GPU memory. With both embedding simulation and empirical validation via the four cohorts from the ISBI cell tracking challenge, the proposed Faster Mean-shift algorithm achieved 7-10 times speedup compared to the state-of-the-art embedding based cell instance segmentation and tracking algorithm. Our Faster Mean-shift algorithm also achieved the highest computational speed compared to other GPU benchmarks with optimized memory consumption. The Faster Mean-shift is a plug-and-play model, which can be employed on other pixel embedding based clustering inference for medical image analysis. (Plug-and-play model is publicly available: https://github.com/masqm/Faster-Mean-Shift)



### Discrepancy Minimization in Domain Generalization with Generative Nearest Neighbors
- **Arxiv ID**: http://arxiv.org/abs/2007.14284v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.14284v1)
- **Published**: 2020-07-28 14:54:25+00:00
- **Updated**: 2020-07-28 14:54:25+00:00
- **Authors**: Prashant Pandey, Mrigank Raman, Sumanth Varambally, Prathosh AP
- **Comment**: None
- **Journal**: None
- **Summary**: Domain generalization (DG) deals with the problem of domain shift where a machine learning model trained on multiple-source domains fail to generalize well on a target domain with different statistics. Multiple approaches have been proposed to solve the problem of domain generalization by learning domain invariant representations across the source domains that fail to guarantee generalization on the shifted target domain. We propose a Generative Nearest Neighbor based Discrepancy Minimization (GNNDM) method which provides a theoretical guarantee that is upper bounded by the error in the labeling process of the target. We employ a Domain Discrepancy Minimization Network (DDMN) that learns domain agnostic features to produce a single source domain while preserving the class labels of the data points. Features extracted from this source domain are learned using a generative model whose latent space is used as a sampler to retrieve the nearest neighbors for the target data points. The proposed method does not require access to the domain labels (a more realistic scenario) as opposed to the existing approaches. Empirically, we show the efficacy of our method on two datasets: PACS and VLCS. Through extensive experimentation, we demonstrate the effectiveness of the proposed method that outperforms several state-of-the-art DG methods.



### Monochrome and Color Polarization Demosaicking Using Edge-Aware Residual Interpolation
- **Arxiv ID**: http://arxiv.org/abs/2007.14292v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.14292v1)
- **Published**: 2020-07-28 15:04:36+00:00
- **Updated**: 2020-07-28 15:04:36+00:00
- **Authors**: Miki Morimatsu, Yusuke Monno, Masayuki Tanaka, Masatoshi Okutomi
- **Comment**: Accepted in ICIP2020. Dataset and code are available at
  http://www.ok.sc.e.titech.ac.jp/res/PolarDem/index.html
- **Journal**: None
- **Summary**: A division-of-focal-plane or microgrid image polarimeter enables us to acquire a set of polarization images in one shot. Since the polarimeter consists of an image sensor equipped with a monochrome or color polarization filter array (MPFA or CPFA), the demosaicking process to interpolate missing pixel values plays a crucial role in obtaining high-quality polarization images. In this paper, we propose a novel MPFA demosaicking method based on edge-aware residual interpolation (EARI) and also extend it to CPFA demosaicking. The key of EARI is a new edge detector for generating an effective guide image used to interpolate the missing pixel values. We also present a newly constructed full color-polarization image dataset captured using a 3-CCD camera and a rotating polarizer. Using the dataset, we experimentally demonstrate that our EARI-based method outperforms existing methods in MPFA and CPFA demosaicking.



### On the Impact of Lossy Image and Video Compression on the Performance of Deep Convolutional Neural Network Architectures
- **Arxiv ID**: http://arxiv.org/abs/2007.14314v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.14314v1)
- **Published**: 2020-07-28 15:37:37+00:00
- **Updated**: 2020-07-28 15:37:37+00:00
- **Authors**: Matt Poyser, Amir Atapour-Abarghouei, Toby P. Breckon
- **Comment**: 8 pages, 21 figures, to be published in ICPR 2020 conference
- **Journal**: None
- **Summary**: Recent advances in generalized image understanding have seen a surge in the use of deep convolutional neural networks (CNN) across a broad range of image-based detection, classification and prediction tasks. Whilst the reported performance of these approaches is impressive, this study investigates the hitherto unapproached question of the impact of commonplace image and video compression techniques on the performance of such deep learning architectures. Focusing on the JPEG and H.264 (MPEG-4 AVC) as a representative proxy for contemporary lossy image/video compression techniques that are in common use within network-connected image/video devices and infrastructure, we examine the impact on performance across five discrete tasks: human pose estimation, semantic segmentation, object detection, action recognition, and monocular depth estimation. As such, within this study we include a variety of network architectures and domains spanning end-to-end convolution, encoder-decoder, region-based CNN (R-CNN), dual-stream, and generative adversarial networks (GAN). Our results show a non-linear and non-uniform relationship between network performance and the level of lossy compression applied. Notably, performance decreases significantly below a JPEG quality (quantization) level of 15% and a H.264 Constant Rate Factor (CRF) of 40. However, retraining said architectures on pre-compressed imagery conversely recovers network performance by up to 78.4% in some cases. Furthermore, there is a correlation between architectures employing an encoder-decoder pipeline and those that demonstrate resilience to lossy image compression. The characteristics of the relationship between input compression to output task performance can be used to inform design decisions within future image/video devices and infrastructure.



### Detection and Segmentation of Custom Objects using High Distraction Photorealistic Synthetic Data
- **Arxiv ID**: http://arxiv.org/abs/2007.14354v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.14354v2)
- **Published**: 2020-07-28 16:33:42+00:00
- **Updated**: 2021-05-23 07:21:05+00:00
- **Authors**: Roey Ron, Gil Elbaz
- **Comment**: None
- **Journal**: None
- **Summary**: We show a straightforward and useful methodology for performing instance segmentation using synthetic data. We apply this methodology on a basic case and derived insights through quantitative analysis. We created a new public dataset: The Expo Markers Dataset intended for detection and segmentation tasks. This dataset contains 5,000 synthetic photorealistic images with their corresponding pixel-perfect segmentation ground truth. The goal is to achieve high performance on manually-gathered and annotated real-world data of custom objects. We do that by creating 3D models of the target objects and other possible distraction objects and place them within a simulated environment. Expo Markers were chosen for this task, fitting our requirements of a custom object due to the exact texture, size and 3D shape. An additional advantage is the availability of this object in offices around the world for easy testing and validation of our results. We generate the data using a domain randomization technique that also simulates other photorealistic objects in the scene, known as distraction objects. These objects provide visual complexity, occlusions, and lighting challenges to help our model gain robustness in training. We are also releasing our manually-gathered datasets used for comparison and evaluation of our synthetic dataset. This white-paper provides strong evidence that photorealistic simulated data can be used in practical real world applications as a more scalable and flexible solution than manually-captured data. Code is available at the following address: https://github.com/DataGenResearchTeam/expo_markers



### Assessing Risks of Biases in Cognitive Decision Support Systems
- **Arxiv ID**: http://arxiv.org/abs/2007.14361v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/2007.14361v1)
- **Published**: 2020-07-28 16:53:45+00:00
- **Updated**: 2020-07-28 16:53:45+00:00
- **Authors**: Kenneth Lai, Helder C. R. Oliveira, Ming Hou, Svetlana N. Yanushkevich, Vlad Shmerko
- **Comment**: submitted to 28th European Signal Processing Conference (EUSIPCO
  2020)
- **Journal**: None
- **Summary**: Recognizing, assessing, countering, and mitigating the biases of different nature from heterogeneous sources is a critical problem in designing a cognitive Decision Support System (DSS). An example of such a system is a cognitive biometric-enabled security checkpoint. Biased algorithms affect the decision-making process in an unpredictable way, e.g. face recognition for different demographic groups may severely impact the risk assessment at a checkpoint. This paper addresses a challenging research question on how to manage an ensemble of biases? We provide performance projections of the DSS operational landscape in terms of biases. A probabilistic reasoning technique is used for assessment of the risk of such biases. We also provide a motivational experiment using face biometric component of the checkpoint system which highlights the discovery of an ensemble of biases and the techniques to assess their risks.



### RadarNet: Exploiting Radar for Robust Perception of Dynamic Objects
- **Arxiv ID**: http://arxiv.org/abs/2007.14366v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.14366v1)
- **Published**: 2020-07-28 17:15:02+00:00
- **Updated**: 2020-07-28 17:15:02+00:00
- **Authors**: Bin Yang, Runsheng Guo, Ming Liang, Sergio Casas, Raquel Urtasun
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: We tackle the problem of exploiting Radar for perception in the context of self-driving as Radar provides complementary information to other sensors such as LiDAR or cameras in the form of Doppler velocity. The main challenges of using Radar are the noise and measurement ambiguities which have been a struggle for existing simple input or output fusion methods. To better address this, we propose a new solution that exploits both LiDAR and Radar sensors for perception. Our approach, dubbed RadarNet, features a voxel-based early fusion and an attention-based late fusion, which learn from data to exploit both geometric and dynamic information of Radar data. RadarNet achieves state-of-the-art results on two large-scale real-world datasets in the tasks of object detection and velocity estimation. We further show that exploiting Radar improves the perception capabilities of detecting faraway objects and understanding the motion of dynamic objects.



### DSC IIT-ISM at SemEval-2020 Task 8: Bi-Fusion Techniques for Deep Meme Emotion Analysis
- **Arxiv ID**: http://arxiv.org/abs/2008.00825v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.00825v1)
- **Published**: 2020-07-28 17:23:35+00:00
- **Updated**: 2020-07-28 17:23:35+00:00
- **Authors**: Pradyumna Gupta, Himanshu Gupta, Aman Sinha
- **Comment**: None
- **Journal**: None
- **Summary**: Memes have become an ubiquitous social media entity and the processing and analysis of suchmultimodal data is currently an active area of research. This paper presents our work on theMemotion Analysis shared task of SemEval 2020, which involves the sentiment and humoranalysis of memes. We propose a system which uses different bimodal fusion techniques toleverage the inter-modal dependency for sentiment and humor classification tasks. Out of all ourexperiments, the best system improved the baseline with macro F1 scores of 0.357 on SentimentClassification (Task A), 0.510 on Humor Classification (Task B) and 0.312 on Scales of SemanticClasses (Task C).



### Flower: A Friendly Federated Learning Research Framework
- **Arxiv ID**: http://arxiv.org/abs/2007.14390v5
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.14390v5)
- **Published**: 2020-07-28 17:59:07+00:00
- **Updated**: 2022-03-05 20:30:32+00:00
- **Authors**: Daniel J. Beutel, Taner Topal, Akhil Mathur, Xinchi Qiu, Javier Fernandez-Marques, Yan Gao, Lorenzo Sani, Kwing Hei Li, Titouan Parcollet, Pedro Porto Buarque de Gusmão, Nicholas D. Lane
- **Comment**: Open-Source, mobile-friendly Federated Learning framework
- **Journal**: None
- **Summary**: Federated Learning (FL) has emerged as a promising technique for edge devices to collaboratively learn a shared prediction model, while keeping their training data on the device, thereby decoupling the ability to do machine learning from the need to store the data in the cloud. However, FL is difficult to implement realistically, both in terms of scale and systems heterogeneity. Although there are a number of research frameworks available to simulate FL algorithms, they do not support the study of scalable FL workloads on heterogeneous edge devices.   In this paper, we present Flower -- a comprehensive FL framework that distinguishes itself from existing platforms by offering new facilities to execute large-scale FL experiments and consider richly heterogeneous FL device scenarios. Our experiments show Flower can perform FL experiments up to 15M in client size using only a pair of high-end GPUs. Researchers can then seamlessly migrate experiments to real devices to examine other parts of the design space. We believe Flower provides the community with a critical new tool for FL study and development.



### AiR: Attention with Reasoning Capability
- **Arxiv ID**: http://arxiv.org/abs/2007.14419v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.14419v1)
- **Published**: 2020-07-28 18:09:45+00:00
- **Updated**: 2020-07-28 18:09:45+00:00
- **Authors**: Shi Chen, Ming Jiang, Jinhui Yang, Qi Zhao
- **Comment**: ECCV2020 (Oral), supplementary materials included
- **Journal**: None
- **Summary**: While attention has been an increasingly popular component in deep neural networks to both interpret and boost performance of models, little work has examined how attention progresses to accomplish a task and whether it is reasonable. In this work, we propose an Attention with Reasoning capability (AiR) framework that uses attention to understand and improve the process leading to task outcomes. We first define an evaluation metric based on a sequence of atomic reasoning operations, enabling quantitative measurement of attention that considers the reasoning process. We then collect human eye-tracking and answer correctness data, and analyze various machine and human attentions on their reasoning capability and how they impact task performance. Furthermore, we propose a supervision method to jointly and progressively optimize attention, reasoning, and task performance so that models learn to look at regions of interests by following a reasoning process. We demonstrate the effectiveness of the proposed framework in analyzing and modeling attention with better reasoning capability and task performance. The code and data are available at https://github.com/szzexpoi/AiR



### A Convolutional Neural Network for gaze preference detection: A potential tool for diagnostics of autism spectrum disorder in children
- **Arxiv ID**: http://arxiv.org/abs/2007.14432v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.14432v1)
- **Published**: 2020-07-28 18:47:21+00:00
- **Updated**: 2020-07-28 18:47:21+00:00
- **Authors**: Dennis Núñez Fernández, Franklin Barrientos Porras, Robert H. Gilman, Macarena Vittet Mondonedo, Patricia Sheen, Mirko Zimic
- **Comment**: Pre-printed version for submission in a journal
- **Journal**: None
- **Summary**: Early diagnosis of autism spectrum disorder (ASD) is known to improve the quality of life of affected individuals. However, diagnosis is often delayed even in wealthier countries including the US, largely due to the fact that gold standard diagnostic tools such as the Autism Diagnostic Observation Schedule (ADOS) and the Autism Diagnostic Interview-Revised (ADI-R) are time consuming and require expertise to administer. This trend is even more pronounced lower resources settings due to a lack of trained experts. As a result, alternative, less technical methods that leverage the unique ways in which children with ASD react to visual stimulation in a controlled environment have been developed to help facilitate early diagnosis. Previous studies have shown that, when exposed to a video that presents both social and abstract scenes side by side, a child with ASD will focus their attention towards the abstract images on the screen to a greater extent than a child without ASD. Such differential responses make it possible to implement an algorithm for the rapid diagnosis of ASD based on eye tracking against different visual stimuli. Here we propose a convolutional neural network (CNN) algorithm for gaze prediction using images extracted from a one-minute stimulus video. Our model achieved a high accuracy rate and robustness for prediction of gaze direction with independent persons and employing a different camera than the one used during testing. In addition to this, the proposed algorithm achieves a fast response time, providing a near real-time evaluation of ASD. Thereby, by applying the proposed method, we could significantly reduce the diagnosis time and facilitate the diagnosis of ASD in low resource regions.



### Cassandra: Detecting Trojaned Networks from Adversarial Perturbations
- **Arxiv ID**: http://arxiv.org/abs/2007.14433v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.14433v1)
- **Published**: 2020-07-28 19:00:40+00:00
- **Updated**: 2020-07-28 19:00:40+00:00
- **Authors**: Xiaoyu Zhang, Ajmal Mian, Rohit Gupta, Nazanin Rahnavard, Mubarak Shah
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks are being widely deployed for many critical tasks due to their high classification accuracy. In many cases, pre-trained models are sourced from vendors who may have disrupted the training pipeline to insert Trojan behaviors into the models. These malicious behaviors can be triggered at the adversary's will and hence, cause a serious threat to the widespread deployment of deep models. We propose a method to verify if a pre-trained model is Trojaned or benign. Our method captures fingerprints of neural networks in the form of adversarial perturbations learned from the network gradients. Inserting backdoors into a network alters its decision boundaries which are effectively encoded in their adversarial perturbations. We train a two stream network for Trojan detection from its global ($L_\infty$ and $L_2$ bounded) perturbations and the localized region of high energy within each perturbation. The former encodes decision boundaries of the network and latter encodes the unknown trigger shape. We also propose an anomaly detection method to identify the target class in a Trojaned network. Our methods are invariant to the trigger type, trigger size, training data and network architecture. We evaluate our methods on MNIST, NIST-Round0 and NIST-Round1 datasets, with up to 1,000 pre-trained models making this the largest study to date on Trojaned network detection, and achieve over 92\% detection accuracy to set the new state-of-the-art.



### Learning from Scale-Invariant Examples for Domain Adaptation in Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2007.14449v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.14449v1)
- **Published**: 2020-07-28 19:40:45+00:00
- **Updated**: 2020-07-28 19:40:45+00:00
- **Authors**: M. Naseer Subhani, Mohsen Ali
- **Comment**: Accepted to ECCV2020
- **Journal**: None
- **Summary**: Self-supervised learning approaches for unsupervised domain adaptation (UDA) of semantic segmentation models suffer from challenges of predicting and selecting reasonable good quality pseudo labels. In this paper, we propose a novel approach of exploiting scale-invariance property of the semantic segmentation model for self-supervised domain adaptation. Our algorithm is based on a reasonable assumption that, in general, regardless of the size of the object and stuff (given context) the semantic labeling should be unchanged. We show that this constraint is violated over the images of the target domain, and hence could be used to transfer labels in-between differently scaled patches. Specifically, we show that semantic segmentation model produces output with high entropy when presented with scaled-up patches of target domain, in comparison to when presented original size images. These scale-invariant examples are extracted from the most confident images of the target domain. Dynamic class specific entropy thresholding mechanism is presented to filter out unreliable pseudo-labels. Furthermore, we also incorporate the focal loss to tackle the problem of class imbalance in self-supervised learning. Extensive experiments have been performed, and results indicate that exploiting the scale-invariant labeling, we outperform existing self-supervised based state-of-the-art domain adaptation methods. Specifically, we achieve 1.3% and 3.8% of lead for GTA5 to Cityscapes and SYNTHIA to Cityscapes with VGG16-FCN8 baseline network.



### Extending LOUPE for K-space Under-sampling Pattern Optimization in Multi-coil MRI
- **Arxiv ID**: http://arxiv.org/abs/2007.14450v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.14450v1)
- **Published**: 2020-07-28 19:41:47+00:00
- **Updated**: 2020-07-28 19:41:47+00:00
- **Authors**: Jinwei Zhang, Hang Zhang, Alan Wang, Qihao Zhang, Mert Sabuncu, Pascal Spincemaille, Thanh D. Nguyen, Yi Wang
- **Comment**: None
- **Journal**: None
- **Summary**: The previously established LOUPE (Learning-based Optimization of the Under-sampling Pattern) framework for optimizing the k-space sampling pattern in MRI was extended in three folds: firstly, fully sampled multi-coil k-space data from the scanner, rather than simulated k-space data from magnitude MR images in LOUPE, was retrospectively under-sampled to optimize the under-sampling pattern of in-vivo k-space data; secondly, binary stochastic k-space sampling, rather than approximate stochastic k-space sampling of LOUPE during training, was applied together with a straight-through (ST) estimator to estimate the gradient of the threshold operation in a neural network; thirdly, modified unrolled optimization network, rather than modified U-Net in LOUPE, was used as the reconstruction network in order to reconstruct multi-coil data properly and reduce the dependency on training data. Experimental results show that when dealing with the in-vivo k-space data, unrolled optimization network with binary under-sampling block and ST estimator had better reconstruction performance compared to the ones with either U-Net reconstruction network or approximate sampling pattern optimization network, and once trained, the learned optimal sampling pattern worked better than the hand-crafted variable density sampling pattern when deployed with other conventional reconstruction methods.



### Enhancement of Retinal Fundus Images via Pixel Color Amplification
- **Arxiv ID**: http://arxiv.org/abs/2007.14456v1
- **DOI**: 10.1007/978-3-030-50516-5_26
- **Categories**: **eess.IV**, cs.CV, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2007.14456v1)
- **Published**: 2020-07-28 19:56:34+00:00
- **Updated**: 2020-07-28 19:56:34+00:00
- **Authors**: Alex Gaudio, Asim Smailagic, Aurélio Campilho
- **Comment**: Accepted to International Conference on Image Analysis and
  Recognition, ICIAR 2020 ; // Published at
  https://doi.org/10.1007/978-3-030-50516-5_26 ;// CODE, SLIDES, and an
  expanded/modified 20 page version https://github.com/adgaudio/ietk-ret
- **Journal**: None
- **Summary**: We propose a pixel color amplification theory and family of enhancement methods to facilitate segmentation tasks on retinal images. Our novel re-interpretation of the image distortion model underlying dehazing theory shows how three existing priors commonly used by the dehazing community and a novel fourth prior are related. We utilize the theory to develop a family of enhancement methods for retinal images, including novel methods for whole image brightening and darkening. We show a novel derivation of the Unsharp Masking algorithm. We evaluate the enhancement methods as a pre-processing step to a challenging multi-task segmentation problem and show large increases in performance on all tasks, with Dice score increases over a no-enhancement baseline by as much as 0.491. We provide evidence that our enhancement preprocessing is useful for unbalanced and difficult data. We show that the enhancements can perform class balancing by composing them together.



### Color-complexity enabled exhaustive color-dots identification and spatial patterns testing in images
- **Arxiv ID**: http://arxiv.org/abs/2007.14485v1
- **DOI**: 10.1371/journal.pone.0251258
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.14485v1)
- **Published**: 2020-07-28 21:06:12+00:00
- **Updated**: 2020-07-28 21:06:12+00:00
- **Authors**: Shuting Liao, Li-Yu Liu, Ting-An Chen, Kuang-Yu Chen, Fushing Hsieh
- **Comment**: 21 pages, 21 figures
- **Journal**: None
- **Summary**: Targeted color-dots with varying shapes and sizes in images are first exhaustively identified, and then their multiscale 2D geometric patterns are extracted for testing spatial uniformness in a progressive fashion. Based on color theory in physics, we develop a new color-identification algorithm relying on highly associative relations among the three color-coordinates: RGB or HSV. Such high associations critically imply low color-complexity of a color image, and renders potentials of exhaustive identification of targeted color-dots of all shapes and sizes. Via heterogeneous shaded regions and lighting conditions, our algorithm is shown being robust, practical and efficient comparing with the popular Contour and OpenCV approaches. Upon all identified color-pixels, we form color-dots as individually connected networks with shapes and sizes. We construct minimum spanning trees (MST) as spatial geometries of dot-collectives of various size-scales. Given a size-scale, the distribution of distances between immediate neighbors in the observed MST is extracted, so do many simulated MSTs under the spatial uniformness assumption. We devise a new algorithm for testing 2D spatial uniformness based on a Hierarchical clustering tree upon all involving MSTs. Our developments are illustrated on images obtained by mimicking chemical spraying via drone in Precision Agriculture.



### Unsupervised Learning of Particle Image Velocimetry
- **Arxiv ID**: http://arxiv.org/abs/2007.14487v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.14487v1)
- **Published**: 2020-07-28 21:08:37+00:00
- **Updated**: 2020-07-28 21:08:37+00:00
- **Authors**: Mingrui Zhang, Matthew D. Piggott
- **Comment**: None
- **Journal**: None
- **Summary**: Particle Image Velocimetry (PIV) is a classical flow estimation problem which is widely considered and utilised, especially as a diagnostic tool in experimental fluid dynamics and the remote sensing of environmental flows. Recently, the development of deep learning based methods has inspired new approaches to tackle the PIV problem. These supervised learning based methods are driven by large volumes of data with ground truth training information. However, it is difficult to collect reliable ground truth data in large-scale, real-world scenarios. Although synthetic datasets can be used as alternatives, the gap between the training set-ups and real-world scenarios limits applicability. We present here what we believe to be the first work which takes an unsupervised learning based approach to tackle PIV problems. The proposed approach is inspired by classic optical flow methods. Instead of using ground truth data, we make use of photometric loss between two consecutive image frames, consistency loss in bidirectional flow estimates and spatial smoothness loss to construct the total unsupervised loss function. The approach shows significant potential and advantages for fluid flow estimation. Results presented here demonstrate that our method outputs competitive results compared with classical PIV methods as well as supervised learning based methods for a broad PIV dataset, and even outperforms these existing approaches in some difficult flow cases. Codes and trained models are available at https://github.com/erizmr/UnLiteFlowNet-PIV.



### Families In Wild Multimedia: A Multimodal Database for Recognizing Kinship
- **Arxiv ID**: http://arxiv.org/abs/2007.14509v6
- **DOI**: 10.1109/TMM.2021.3103074
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.14509v6)
- **Published**: 2020-07-28 22:36:57+00:00
- **Updated**: 2021-10-01 20:16:01+00:00
- **Authors**: Joseph P. Robinson, Zaid Khan, Yu Yin, Ming Shao, Yun Fu
- **Comment**: None
- **Journal**: IEEE Transactions on Multimedia (2021)
- **Summary**: Kinship, a soft biometric detectable in media, is fundamental for a myriad of use-cases. Despite the difficulty of detecting kinship, annual data challenges using still-images have consistently improved performances and attracted new researchers. Now, systems reach performance levels unforeseeable a decade ago, closing in on performances acceptable to deploy in practice. Like other biometric tasks, we expect systems can receive help from other modalities. We hypothesize that adding modalities to FIW, which has only still-images, will improve performance. Thus, to narrow the gap between research and reality and enhance the power of kinship recognition systems, we extend FIW with multimedia (MM) data (i.e., video, audio, and text captions). Specifically, we introduce the first publicly available multi-task MM kinship dataset. To build FIW MM, we developed machinery to automatically collect, annotate, and prepare the data, requiring minimal human input and no financial cost. The proposed MM corpus allows the problem statements to be more realistic template-based protocols. We show significant improvements in all benchmarks with the added modalities. The results highlight edge cases to inspire future research with different areas of improvement. FIW MM supplies the data needed to increase the potential of automated systems to detect kinship in MM. It also allows experts from diverse fields to collaborate in novel ways.



### Decompose X-ray Images for Bone and Soft Tissue
- **Arxiv ID**: http://arxiv.org/abs/2007.14510v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.14510v1)
- **Published**: 2020-07-28 22:38:54+00:00
- **Updated**: 2020-07-28 22:38:54+00:00
- **Authors**: Yuanhao Gong
- **Comment**: None
- **Journal**: None
- **Summary**: Bones are always wrapped by soft tissues. As a result, bones in their X-ray images are obscured and become unclear. In this paper, we tackle this problem and propose a novel task to virtually decompose the soft tissue and bone by image processing algorithms. This task is fundamentally different from segmentation because the decomposed images share the same imaging domain. Our decomposition task is also fundamentally different from the conventional image enhancement. We propose a new mathematical model for such decomposition. Our model is ill-posed and thus it requires some priors. With proper assumptions, our model can be solved by solving a standard Laplace equation. The resulting bone image is theoretically guaranteed to have better contrast than the original input image. Therefore, the details of bones get enhanced and become clearer. Several numerical experiments confirm the effective and efficiency of our method. Our approach is important for clinical diagnosis, surgery planning, recognition, deep learning, etc.



### $S^3$Net: Semantic-Aware Self-supervised Depth Estimation with Monocular Videos and Synthetic Data
- **Arxiv ID**: http://arxiv.org/abs/2007.14511v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.14511v1)
- **Published**: 2020-07-28 22:40:54+00:00
- **Updated**: 2020-07-28 22:40:54+00:00
- **Authors**: Bin Cheng, Inderjot Singh Saggu, Raunak Shah, Gaurav Bansal, Dinesh Bharadia
- **Comment**: None
- **Journal**: None
- **Summary**: Solving depth estimation with monocular cameras enables the possibility of widespread use of cameras as low-cost depth estimation sensors in applications such as autonomous driving and robotics. However, learning such a scalable depth estimation model would require a lot of labeled data which is expensive to collect. There are two popular existing approaches which do not require annotated depth maps: (i) using labeled synthetic and unlabeled real data in an adversarial framework to predict more accurate depth, and (ii) unsupervised models which exploit geometric structure across space and time in monocular video frames. Ideally, we would like to leverage features provided by both approaches as they complement each other; however, existing methods do not adequately exploit these additive benefits. We present $S^3$Net, a self-supervised framework which combines these complementary features: we use synthetic and real-world images for training while exploiting geometric, temporal, as well as semantic constraints. Our novel consolidated architecture provides a new state-of-the-art in self-supervised depth estimation using monocular videos. We present a unique way to train this self-supervised framework, and achieve (i) more than $15\%$ improvement over previous synthetic supervised approaches that use domain adaptation and (ii) more than $10\%$ improvement over previous self-supervised approaches which exploit geometric constraints from the real data.



### Group Knowledge Transfer: Federated Learning of Large CNNs at the Edge
- **Arxiv ID**: http://arxiv.org/abs/2007.14513v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.14513v4)
- **Published**: 2020-07-28 22:48:52+00:00
- **Updated**: 2020-11-05 07:24:42+00:00
- **Authors**: Chaoyang He, Murali Annavaram, Salman Avestimehr
- **Comment**: This paper is accepted to NeurIPS 2020. We propose FedGKT, attempting
  to address one of the core problems of federated learning: training deep
  neural networks in resource-constrained edge devices
- **Journal**: None
- **Summary**: Scaling up the convolutional neural network (CNN) size (e.g., width, depth, etc.) is known to effectively improve model accuracy. However, the large model size impedes training on resource-constrained edge devices. For instance, federated learning (FL) may place undue burden on the compute capability of edge nodes, even though there is a strong practical need for FL due to its privacy and confidentiality properties. To address the resource-constrained reality of edge devices, we reformulate FL as a group knowledge transfer training algorithm, called FedGKT. FedGKT designs a variant of the alternating minimization approach to train small CNNs on edge nodes and periodically transfer their knowledge by knowledge distillation to a large server-side CNN. FedGKT consolidates several advantages into a single framework: reduced demand for edge computation, lower communication bandwidth for large CNNs, and asynchronous training, all while maintaining model accuracy comparable to FedAvg. We train CNNs designed based on ResNet-56 and ResNet-110 using three distinct datasets (CIFAR-10, CIFAR-100, and CINIC-10) and their non-I.I.D. variants. Our results show that FedGKT can obtain comparable or even slightly higher accuracy than FedAvg. More importantly, FedGKT makes edge training affordable. Compared to the edge training using FedAvg, FedGKT demands 9 to 17 times less computational power (FLOPs) on edge devices and requires 54 to 105 times fewer parameters in the edge CNN. Our source code is released at FedML (https://fedml.ai).



### A Deep Learning Framework for Generation and Analysis of Driving Scenario Trajectories
- **Arxiv ID**: http://arxiv.org/abs/2007.14524v2
- **DOI**: 10.1007/s42979-023-01714-3
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.14524v2)
- **Published**: 2020-07-28 23:33:05+00:00
- **Updated**: 2023-08-12 20:42:08+00:00
- **Authors**: Andreas Demetriou, Henrik Alfsvåg, Sadegh Rahrovani, Morteza Haghir Chehreghani
- **Comment**: None
- **Journal**: The work is published in SN Computer Science, 4, 251, 2023
- **Summary**: We propose a unified deep learning framework for the generation and analysis of driving scenario trajectories, and validate its effectiveness in a principled way. To model and generate scenarios of trajectories with different lengths, we develop two approaches. First, we adapt the Recurrent Conditional Generative Adversarial Networks (RC-GAN) by conditioning on the length of the trajectories. This provides us the flexibility to generate variable-length driving trajectories, a desirable feature for scenario test case generation in the verification of autonomous driving. Second, we develop an architecture based on Recurrent Autoencoder with GANs to obviate the variable length issue, wherein we train a GAN to learn/generate the latent representations of original trajectories. In this approach, we train an integrated feed-forward neural network to estimate the length of the trajectories to be able to bring them back from the latent space representation. In addition to trajectory generation, we employ the trained autoencoder as a feature extractor, for the purpose of clustering and anomaly detection, to obtain further insights into the collected scenario dataset. We experimentally investigate the performance of the proposed framework on real-world scenario trajectories obtained from in-field data collection.



