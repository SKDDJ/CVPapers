# Arxiv Papers in cs.CV on 2020-07-03
### Anatomy-Aware Siamese Network: Exploiting Semantic Asymmetry for Accurate Pelvic Fracture Detection in X-ray Images
- **Arxiv ID**: http://arxiv.org/abs/2007.01464v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.01464v3)
- **Published**: 2020-07-03 02:33:24+00:00
- **Updated**: 2020-07-23 14:30:38+00:00
- **Authors**: Haomin Chen, Yirui Wang, Kang Zheng, Weijian Li, Chi-Tung Cheng, Adam P. Harrison, Jing Xiao, Gregory D. Hager, Le Lu, Chien-Hung Liao, Shun Miao
- **Comment**: ECCV 2020 (camera-ready)
- **Journal**: None
- **Summary**: Visual cues of enforcing bilaterally symmetric anatomies as normal findings are widely used in clinical practice to disambiguate subtle abnormalities from medical images. So far, inadequate research attention has been received on effectively emulating this practice in CAD methods. In this work, we exploit semantic anatomical symmetry or asymmetry analysis in a complex CAD scenario, i.e., anterior pelvic fracture detection in trauma PXRs, where semantically pathological (refer to as fracture) and non-pathological (e.g., pose) asymmetries both occur. Visually subtle yet pathologically critical fracture sites can be missed even by experienced clinicians, when limited diagnosis time is permitted in emergency care. We propose a novel fracture detection framework that builds upon a Siamese network enhanced with a spatial transformer layer to holistically analyze symmetric image features. Image features are spatially formatted to encode bilaterally symmetric anatomies. A new contrastive feature learning component in our Siamese network is designed to optimize the deep image features being more salient corresponding to the underlying semantic asymmetries (caused by pelvic fracture occurrences). Our proposed method have been extensively evaluated on 2,359 PXRs from unique patients (the largest study to-date), and report an area under ROC curve score of 0.9771. This is the highest among state-of-the-art fracture detection methods, with improved clinical indications.



### Task-agnostic Temporally Consistent Facial Video Editing
- **Arxiv ID**: http://arxiv.org/abs/2007.01466v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.01466v1)
- **Published**: 2020-07-03 02:49:20+00:00
- **Updated**: 2020-07-03 02:49:20+00:00
- **Authors**: Meng Cao, Haozhi Huang, Hao Wang, Xuan Wang, Li Shen, Sheng Wang, Linchao Bao, Zhifeng Li, Jiebo Luo
- **Comment**: None
- **Journal**: None
- **Summary**: Recent research has witnessed the advances in facial image editing tasks. For video editing, however, previous methods either simply apply transformations frame by frame or utilize multiple frames in a concatenated or iterative fashion, which leads to noticeable visual flickers. In addition, these methods are confined to dealing with one specific task at a time without any extensibility. In this paper, we propose a task-agnostic temporally consistent facial video editing framework. Based on a 3D reconstruction model, our framework is designed to handle several editing tasks in a more unified and disentangled manner. The core design includes a dynamic training sample selection mechanism and a novel 3D temporal loss constraint that fully exploits both image and video datasets and enforces temporal consistency. Compared with the state-of-the-art facial image editing methods, our framework generates video portraits that are more photo-realistic and temporally smooth.



### ODE-CNN: Omnidirectional Depth Extension Networks
- **Arxiv ID**: http://arxiv.org/abs/2007.01475v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.01475v1)
- **Published**: 2020-07-03 03:14:09+00:00
- **Updated**: 2020-07-03 03:14:09+00:00
- **Authors**: Xinjing Cheng, Peng Wang, Yanqi Zhou, Chenye Guan, Ruigang Yang
- **Comment**: Accepted by ICRA 2020, 7 pages, 5 figures
- **Journal**: None
- **Summary**: Omnidirectional 360{\deg} camera proliferates rapidly for autonomous robots since it significantly enhances the perception ability by widening the field of view(FoV). However, corresponding 360{\deg} depth sensors, which are also critical for the perception system, are still difficult or expensive to have. In this paper, we propose a low-cost 3D sensing system that combines an omnidirectional camera with a calibrated projective depth camera, where the depth from the limited FoV can be automatically extended to the rest of the recorded omnidirectional image. To accurately recover the missing depths, we design an omnidirectional depth extension convolutional neural network(ODE-CNN), in which a spherical feature transform layer(SFTL) is embedded at the end of feature encoding layers, and a deformable convolutional spatial propagation network(D-CSPN) is appended at the end of feature decoding layers. The former resamples the neighborhood of each pixel in the omnidirectional coordination to the projective coordination, which reduces the difficulty of feature learning, and the later automatically finds a proper context to well align the structures in the estimated depths via CNN w.r.t. the reference image, which significantly improves the visual quality. Finally, we demonstrate the effectiveness of proposed ODE-CNN over the popular 360D dataset and show that ODE-CNN significantly outperforms (relatively 33% reduction in-depth error) other state-of-the-art (SoTA) methods.



### Interactive Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/2007.01476v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.01476v3)
- **Published**: 2020-07-03 03:22:04+00:00
- **Updated**: 2021-04-15 07:21:43+00:00
- **Authors**: Shipeng Fu, Zhen Li, Jun Xu, Ming-Ming Cheng, Zitao Liu, Xiaomin Yang
- **Comment**: This work (IAKD) and BERT-of-Theseus (see arXiv:2002.02925) are
  concurrent works. IAKD was first submitted to CVPR2020 and now is accepted in
  Neurocomputing. Thank the authors in BERT-of-Theseus for pointing out this
  issue
- **Journal**: None
- **Summary**: Knowledge distillation is a standard teacher-student learning framework to train a light-weight student network under the guidance of a well-trained large teacher network. As an effective teaching strategy, interactive teaching has been widely employed at school to motivate students, in which teachers not only provide knowledge but also give constructive feedback to students upon their responses, to improve their learning performance. In this work, we propose an InterActive Knowledge Distillation (IAKD) scheme to leverage the interactive teaching strategy for efficient knowledge distillation. In the distillation process, the interaction between teacher and student networks is implemented by a swapping-in operation: randomly replacing the blocks in the student network with the corresponding blocks in the teacher network. In the way, we directly involve the teacher's powerful feature transformation ability to largely boost the student's performance. Experiments with typical settings of teacher-student networks demonstrate that the student networks trained by our IAKD achieve better performance than those trained by conventional knowledge distillation methods on diverse image classification datasets.



### RSAC: Regularized Subspace Approximation Classifier for Lightweight Continuous Learning
- **Arxiv ID**: http://arxiv.org/abs/2007.01480v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.01480v1)
- **Published**: 2020-07-03 03:38:06+00:00
- **Updated**: 2020-07-03 03:38:06+00:00
- **Authors**: Chih-Hsing Ho, Shang-Ho, Tsai
- **Comment**: None
- **Journal**: None
- **Summary**: Continuous learning seeks to perform the learning on the data that arrives from time to time. While prior works have demonstrated several possible solutions, these approaches require excessive training time as well as memory usage. This is impractical for applications where time and storage are constrained, such as edge computing. In this work, a novel training algorithm, regularized subspace approximation classifier (RSAC), is proposed to achieve lightweight continuous learning. RSAC contains a feature reduction module and classifier module with regularization. Extensive experiments show that RSAC is more efficient than prior continuous learning works and outperforms these works on various experimental settings.



### Learning to Prune in Training via Dynamic Channel Propagation
- **Arxiv ID**: http://arxiv.org/abs/2007.01486v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.01486v1)
- **Published**: 2020-07-03 04:02:41+00:00
- **Updated**: 2020-07-03 04:02:41+00:00
- **Authors**: Shibo Shen, Rongpeng Li, Zhifeng Zhao, Honggang Zhang, Yugeng Zhou
- **Comment**: accepted by ICPR-2020
- **Journal**: None
- **Summary**: In this paper, we propose a novel network training mechanism called "dynamic channel propagation" to prune the neural networks during the training period. In particular, we pick up a specific group of channels in each convolutional layer to participate in the forward propagation in training time according to the significance level of channel, which is defined as channel utility. The utility values with respect to all selected channels are updated simultaneously with the error back-propagation process and will adaptively change. Furthermore, when the training ends, channels with high utility values are retained whereas those with low utility values are discarded. Hence, our proposed scheme trains and prunes neural networks simultaneously. We empirically evaluate our novel training scheme on various representative benchmark datasets and advanced convolutional neural network (CNN) architectures, including VGGNet and ResNet. The experiment results verify the superior performance and robust effectiveness of our approach.



### Self-Supervised GAN Compression
- **Arxiv ID**: http://arxiv.org/abs/2007.01491v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.01491v2)
- **Published**: 2020-07-03 04:18:54+00:00
- **Updated**: 2020-07-12 16:43:57+00:00
- **Authors**: Chong Yu, Jeff Pool
- **Comment**: The appendix for this paper is in the following repository
  https://gitlab.com/dxxz/Self-Supervised-GAN-Compression-Appendix
- **Journal**: NeurIPS 2020
- **Summary**: Deep learning's success has led to larger and larger models to handle more and more complex tasks; trained models can contain millions of parameters. These large models are compute- and memory-intensive, which makes it a challenge to deploy them with minimized latency, throughput, and storage requirements. Some model compression methods have been successfully applied to image classification and detection or language models, but there has been very little work compressing generative adversarial networks (GANs) performing complex tasks. In this paper, we show that a standard model compression technique, weight pruning, cannot be applied to GANs using existing methods. We then develop a self-supervised compression technique which uses the trained discriminator to supervise the training of a compressed generator. We show that this framework has a compelling performance to high degrees of sparsity, can be easily applied to new tasks and models, and enables meaningful comparisons between different pruning granularities.



### Few-Shot Semantic Segmentation Augmented with Image-Level Weak Annotations
- **Arxiv ID**: http://arxiv.org/abs/2007.01496v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.01496v2)
- **Published**: 2020-07-03 04:58:20+00:00
- **Updated**: 2021-06-18 17:55:54+00:00
- **Authors**: Shuo Lei, Xuchao Zhang, Jianfeng He, Fanglan Chen, Chang-Tien Lu
- **Comment**: Accpeted to ICME2021
- **Journal**: None
- **Summary**: Despite the great progress made by deep neural networks in the semantic segmentation task, traditional neural-networkbased methods typically suffer from a shortage of large amounts of pixel-level annotations. Recent progress in fewshot semantic segmentation tackles the issue by only a few pixel-level annotated examples. However, these few-shot approaches cannot easily be applied to multi-way or weak annotation settings. In this paper, we advance the few-shot segmentation paradigm towards a scenario where image-level annotations are available to help the training process of a few pixel-level annotations. Our key idea is to learn a better prototype representation of the class by fusing the knowledge from the image-level labeled data. Specifically, we propose a new framework, called PAIA, to learn the class prototype representation in a metric space by integrating image-level annotations. Furthermore, by considering the uncertainty of pseudo-masks, a distilled soft masked average pooling strategy is designed to handle distractions in image-level annotations. Extensive empirical results on two datasets show superior performance of PAIA.



### A Competence-aware Curriculum for Visual Concepts Learning via Question Answering
- **Arxiv ID**: http://arxiv.org/abs/2007.01499v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.01499v2)
- **Published**: 2020-07-03 05:08:09+00:00
- **Updated**: 2020-07-27 21:57:39+00:00
- **Authors**: Qing Li, Siyuan Huang, Yining Hong, Song-Chun Zhu
- **Comment**: ECCV 2020 (Oral) Camera Ready. Project page:
  https://liqing-ustc.github.io/CL-mIRT/
- **Journal**: None
- **Summary**: Humans can progressively learn visual concepts from easy to hard questions. To mimic this efficient learning ability, we propose a competence-aware curriculum for visual concept learning in a question-answering manner. Specifically, we design a neural-symbolic concept learner for learning the visual concepts and a multi-dimensional Item Response Theory (mIRT) model for guiding the learning process with an adaptive curriculum. The mIRT effectively estimates the concept difficulty and the model competence at each learning step from accumulated model responses. The estimated concept difficulty and model competence are further utilized to select the most profitable training samples. Experimental results on CLEVR show that with a competence-aware curriculum, the proposed method achieves state-of-the-art performances with superior data efficiency and convergence speed. Specifically, the proposed model only uses 40% of training data and converges three times faster compared with other state-of-the-art methods.



### Self-supervised Neural Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/2007.01500v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.01500v1)
- **Published**: 2020-07-03 05:09:30+00:00
- **Updated**: 2020-07-03 05:09:30+00:00
- **Authors**: Sapir Kaplan, Raja Giryes
- **Comment**: None
- **Journal**: None
- **Summary**: Neural Architecture Search (NAS) has been used recently to achieve improved performance in various tasks and most prominently in image classification. Yet, current search strategies rely on large labeled datasets, which limit their usage in the case where only a smaller fraction of the data is annotated. Self-supervised learning has shown great promise in training neural networks using unlabeled data. In this work, we propose a self-supervised neural architecture search (SSNAS) that allows finding novel network models without the need for labeled data. We show that such a search leads to comparable results to supervised training with a "fully labeled" NAS and that it can improve the performance of self-supervised learning. Moreover, we demonstrate the advantage of the proposed approach when the number of labels in the search is relatively small.



### A Similarity Inference Metric for RGB-Infrared Cross-Modality Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/2007.01504v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.01504v1)
- **Published**: 2020-07-03 05:28:13+00:00
- **Updated**: 2020-07-03 05:28:13+00:00
- **Authors**: Mengxi Jia, Yunpeng Zhai, Shijian Lu, Siwei Ma, Jian Zhang
- **Comment**: Accepted by IJCAI2020
- **Journal**: None
- **Summary**: RGB-Infrared (IR) cross-modality person re-identification (re-ID), which aims to search an IR image in RGB gallery or vice versa, is a challenging task due to the large discrepancy between IR and RGB modalities. Existing methods address this challenge typically by aligning feature distributions or image styles across modalities, whereas the very useful similarities among gallery samples of the same modality (i.e. intra-modality sample similarities) is largely neglected. This paper presents a novel similarity inference metric (SIM) that exploits the intra-modality sample similarities to circumvent the cross-modality discrepancy targeting optimal cross-modality image matching. SIM works by successive similarity graph reasoning and mutual nearest-neighbor reasoning that mine cross-modality sample similarities by leveraging intra-modality sample similarities from two different perspectives. Extensive experiments over two cross-modality re-ID datasets (SYSU-MM01 and RegDB) show that SIM achieves significant accuracy improvement but with little extra training as compared with the state-of-the-art.



### Three-dimensional Human Tracking of a Mobile Robot by Fusion of Tracking Results of Two Cameras
- **Arxiv ID**: http://arxiv.org/abs/2007.01514v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO, 68-04, I.4.0
- **Links**: [PDF](http://arxiv.org/pdf/2007.01514v1)
- **Published**: 2020-07-03 06:46:49+00:00
- **Updated**: 2020-07-03 06:46:49+00:00
- **Authors**: Shinya Matsubara, Akihiko Honda, Yonghoon Ji, Kazunori Umeda
- **Comment**: 4 pages, 11 figures
- **Journal**: None
- **Summary**: This paper proposes a process that uses two cameras to obtain three-dimensional (3D) information of a target object for human tracking. Results of human detection and tracking from two cameras are integrated to obtain the 3D information. OpenPose is used for human detection. In the case of a general processing a stereo camera, a range image of the entire scene is acquired as precisely as possible, and then the range image is processed. However, there are problems such as incorrect matching and computational cost for the calibration process. A new stereo vision framework is proposed to cope with the problems. The effectiveness of the proposed framework and the method is verified through target-tracking experiments.



### Domain Adaptation without Source Data
- **Arxiv ID**: http://arxiv.org/abs/2007.01524v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.01524v4)
- **Published**: 2020-07-03 07:21:30+00:00
- **Updated**: 2021-08-30 06:29:13+00:00
- **Authors**: Youngeun Kim, Donghyeon Cho, Kyeongtak Han, Priyadarshini Panda, Sungeun Hong
- **Comment**: 13 pages
- **Journal**: None
- **Summary**: Domain adaptation assumes that samples from source and target domains are freely accessible during a training phase. However, such an assumption is rarely plausible in the real-world and possibly causes data-privacy issues, especially when the label of the source domain can be a sensitive attribute as an identifier. To avoid accessing source data that may contain sensitive information, we introduce Source data-Free Domain Adaptation (SFDA). Our key idea is to leverage a pre-trained model from the source domain and progressively update the target model in a self-learning manner. We observe that target samples with lower self-entropy measured by the pre-trained source model are more likely to be classified correctly. From this, we select the reliable samples with the self-entropy criterion and define these as class prototypes. We then assign pseudo labels for every target sample based on the similarity score with class prototypes. Furthermore, to reduce the uncertainty from the pseudo labeling process, we propose set-to-set distance-based filtering which does not require any tunable hyperparameters. Finally, we train the target model with the filtered pseudo labels with regularization from the pre-trained source model. Surprisingly, without direct usage of labeled source samples, our PrDA outperforms conventional domain adaptation methods on benchmark datasets. Our code is publicly available at https://github.com/youngryan1993/SFDA-SourceFreeDA



### Multiple Expert Brainstorming for Domain Adaptive Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/2007.01546v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.01546v3)
- **Published**: 2020-07-03 08:16:19+00:00
- **Updated**: 2020-07-13 13:11:44+00:00
- **Authors**: Yunpeng Zhai, Qixiang Ye, Shijian Lu, Mengxi Jia, Rongrong Ji, Yonghong Tian
- **Comment**: Accepted by ECCV'20
- **Journal**: None
- **Summary**: Often the best performing deep neural models are ensembles of multiple base-level networks, nevertheless, ensemble learning with respect to domain adaptive person re-ID remains unexplored. In this paper, we propose a multiple expert brainstorming network (MEB-Net) for domain adaptive person re-ID, opening up a promising direction about model ensemble problem under unsupervised conditions. MEB-Net adopts a mutual learning strategy, where multiple networks with different architectures are pre-trained within a source domain as expert models equipped with specific features and knowledge, while the adaptation is then accomplished through brainstorming (mutual learning) among expert models. MEB-Net accommodates the heterogeneity of experts learned with different architectures and enhances discrimination capability of the adapted re-ID model, by introducing a regularization scheme about authority of experts. Extensive experiments on large-scale datasets (Market-1501 and DukeMTMC-reID) demonstrate the superior performance of MEB-Net over the state-of-the-arts.



### Multiple Instance-Based Video Anomaly Detection using Deep Temporal Encoding-Decoding
- **Arxiv ID**: http://arxiv.org/abs/2007.01548v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.01548v2)
- **Published**: 2020-07-03 08:22:42+00:00
- **Updated**: 2021-01-05 05:53:21+00:00
- **Authors**: Ammar Mansoor Kamoona, Amirali Khodadadian Gosta, Alireza Bab-Hadiashar, Reza Hoseinnezhad
- **Comment**: The paper is under review
- **Journal**: None
- **Summary**: In this paper, we propose a weakly supervised deep temporal encoding-decoding solution for anomaly detection in surveillance videos using multiple instance learning. The proposed approach uses both abnormal and normal video clips during the training phase which is developed in the multiple instance framework where we treat video as a bag and video clips as instances in the bag. Our main contribution lies in the proposed novel approach to consider temporal relations between video instances. We deal with video instances (clips) as a sequential visual data rather than independent instances. We employ a deep temporal and encoder network that is designed to capture spatial-temporal evolution of video instances over time. We also propose a new loss function that is smoother than similar loss functions recently presented in the computer vision literature, and therefore; enjoys faster convergence and improved tolerance to local minima during the training phase. The proposed temporal encoding-decoding approach with modified loss is benchmarked against the state-of-the-art in simulation studies. The results show that the proposed method performs similar to or better than the state-of-the-art solutions for anomaly detection in video surveillance applications.



### PointTrack++ for Effective Online Multi-Object Tracking and Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2007.01549v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.01549v1)
- **Published**: 2020-07-03 08:28:37+00:00
- **Updated**: 2020-07-03 08:28:37+00:00
- **Authors**: Zhenbo Xu, Wei Zhang, Xiao Tan, Wei Yang, Xiangbo Su, Yuchen Yuan, Hongwu Zhang, Shilei Wen, Errui Ding, Liusheng Huang
- **Comment**: CVPR2020 MOTS Challenge Winner. PointTrack++ ranks first on KITTI
  MOTS (http://www.cvlibs.net/datasets/kitti/eval_mots.php)
- **Journal**: None
- **Summary**: Multiple-object tracking and segmentation (MOTS) is a novel computer vision task that aims to jointly perform multiple object tracking (MOT) and instance segmentation. In this work, we present PointTrack++, an effective on-line framework for MOTS, which remarkably extends our recently proposed PointTrack framework. To begin with, PointTrack adopts an efficient one-stage framework for instance segmentation, and learns instance embeddings by converting compact image representations to un-ordered 2D point cloud. Compared with PointTrack, our proposed PointTrack++ offers three major improvements. Firstly, in the instance segmentation stage, we adopt a semantic segmentation decoder trained with focal loss to improve the instance selection quality. Secondly, to further boost the segmentation performance, we propose a data augmentation strategy by copy-and-paste instances into training images. Finally, we introduce a better training strategy in the instance association stage to improve the distinguishability of learned instance embeddings. The resulting framework achieves the state-of-the-art performance on the 5th BMTT MOTChallenge.



### Segment as Points for Efficient Online Multi-Object Tracking and Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2007.01550v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.01550v1)
- **Published**: 2020-07-03 08:29:35+00:00
- **Updated**: 2020-07-03 08:29:35+00:00
- **Authors**: Zhenbo Xu, Wei Zhang, Xiao Tan, Wei Yang, Huan Huang, Shilei Wen, Errui Ding, Liusheng Huang
- **Comment**: ECCV2020 ORAL (top 2%). Code already available at
  https://github.com/detectRecog/PointTrack. A highly effective method for
  learning features based on instance segments
- **Journal**: None
- **Summary**: Current multi-object tracking and segmentation (MOTS) methods follow the tracking-by-detection paradigm and adopt convolutions for feature extraction. However, as affected by the inherent receptive field, convolution based feature extraction inevitably mixes up the foreground features and the background features, resulting in ambiguities in the subsequent instance association. In this paper, we propose a highly effective method for learning instance embeddings based on segments by converting the compact image representation to un-ordered 2D point cloud representation. Our method generates a new tracking-by-points paradigm where discriminative instance embeddings are learned from randomly selected points rather than images. Furthermore, multiple informative data modalities are converted into point-wise representations to enrich point-wise features. The resulting online MOTS framework, named PointTrack, surpasses all the state-of-the-art methods including 3D tracking methods by large margins (5.4% higher MOTSA and 18 times faster over MOTSFusion) with the near real-time speed (22 FPS). Evaluations across three datasets demonstrate both the effectiveness and efficiency of our method. Moreover, based on the observation that current MOTS datasets lack crowded scenes, we build a more challenging MOTS dataset named APOLLO MOTS with higher instance density. Both APOLLO MOTS and our codes are publicly available at https://github.com/detectRecog/PointTrack.



### Surrogate-assisted Particle Swarm Optimisation for Evolving Variable-length Transferable Blocks for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2007.01556v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2007.01556v1)
- **Published**: 2020-07-03 08:48:21+00:00
- **Updated**: 2020-07-03 08:48:21+00:00
- **Authors**: Bin Wang, Bing Xue, Mengjie Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Deep convolutional neural networks have demonstrated promising performance on image classification tasks, but the manual design process becomes more and more complex due to the fast depth growth and the increasingly complex topologies of convolutional neural networks. As a result, neural architecture search has emerged to automatically design convolutional neural networks that outperform handcrafted counterparts. However, the computational cost is immense, e.g. 22,400 GPU-days and 2,000 GPU-days for two outstanding neural architecture search works named NAS and NASNet, respectively, which motivates this work. A new effective and efficient surrogate-assisted particle swarm optimisation algorithm is proposed to automatically evolve convolutional neural networks. This is achieved by proposing a novel surrogate model, a new method of creating a surrogate dataset and a new encoding strategy to encode variable-length blocks of convolutional neural networks, all of which are integrated into a particle swarm optimisation algorithm to form the proposed method. The proposed method shows its effectiveness by achieving competitive error rates of 3.49% on the CIFAR-10 dataset, 18.49% on the CIFAR-100 dataset, and 1.82% on the SVHN dataset. The convolutional neural network blocks are efficiently learned by the proposed method from CIFAR-10 within 3 GPU-days due to the acceleration achieved by the surrogate model and the surrogate dataset to avoid the training of 80.1% of convolutional neural network blocks represented by the particles. Without any further search, the evolved blocks from CIFAR-10 can be successfully transferred to CIFAR-100 and SVHN, which exhibits the transferability of the block learned by the proposed method.



### Domain Adaptive Object Detection via Asymmetric Tri-way Faster-RCNN
- **Arxiv ID**: http://arxiv.org/abs/2007.01571v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.01571v1)
- **Published**: 2020-07-03 09:30:18+00:00
- **Updated**: 2020-07-03 09:30:18+00:00
- **Authors**: Zhenwei He, Lei Zhang
- **Comment**: The paper is accepted in ECCV2020
- **Journal**: None
- **Summary**: Conventional object detection models inevitably encounter a performance drop as the domain disparity exists. Unsupervised domain adaptive object detection is proposed recently to reduce the disparity between domains, where the source domain is label-rich while the target domain is label-agnostic. The existing models follow a parameter shared siamese structure for adversarial domain alignment, which, however, easily leads to the collapse and out-of-control risk of the source domain and brings negative impact to feature adaption. The main reason is that the labeling unfairness (asymmetry) between source and target makes the parameter sharing mechanism unable to adapt. Therefore, in order to avoid the source domain collapse risk caused by parameter sharing, we propose an asymmetric tri-way Faster-RCNN (ATF) for domain adaptive object detection. Our ATF model has two distinct merits: 1) A ancillary net supervised by source label is deployed to learn ancillary target features and simultaneously preserve the discrimination of source domain, which enhances the structural discrimination (object classification vs. bounding box regression) of domain alignment. 2) The asymmetric structure consisting of a chief net and an independent ancillary net essentially overcomes the parameter sharing aroused source risk collapse. The adaption safety of the proposed ATF detector is guaranteed. Extensive experiments on a number of datasets, including Cityscapes, Foggy-cityscapes, KITTI, Sim10k, Pascal VOC, Clipart and Watercolor, demonstrate the SOTA performance of our method.



### Ground Truth Free Denoising by Optimal Transport
- **Arxiv ID**: http://arxiv.org/abs/2007.01575v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE, math.FA, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/2007.01575v1)
- **Published**: 2020-07-03 09:39:25+00:00
- **Updated**: 2020-07-03 09:39:25+00:00
- **Authors**: Sören Dittmer, Carola-Bibiane Schönlieb, Peter Maass
- **Comment**: None
- **Journal**: None
- **Summary**: We present a learned unsupervised denoising method for arbitrary types of data, which we explore on images and one-dimensional signals. The training is solely based on samples of noisy data and examples of noise, which -- critically -- do not need to come in pairs. We only need the assumption that the noise is independent and additive (although we describe how this can be extended). The method rests on a Wasserstein Generative Adversarial Network setting, which utilizes two critics and one generator.



### Deep image prior for 3D magnetic particle imaging: A quantitative comparison of regularization techniques on Open MPI dataset
- **Arxiv ID**: http://arxiv.org/abs/2007.01593v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, math.FA, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/2007.01593v1)
- **Published**: 2020-07-03 10:13:10+00:00
- **Updated**: 2020-07-03 10:13:10+00:00
- **Authors**: Sören Dittmer, Tobias Kluth, Mads Thorstein Roar Henriksen, Peter Maass
- **Comment**: None
- **Journal**: None
- **Summary**: Magnetic particle imaging (MPI) is an imaging modality exploiting the nonlinear magnetization behavior of (super-)paramagnetic nanoparticles to obtain a space- and often also time-dependent concentration of a tracer consisting of these nanoparticles. MPI has a continuously increasing number of potential medical applications. One prerequisite for successful performance in these applications is a proper solution to the image reconstruction problem. More classical methods from inverse problems theory, as well as novel approaches from the field of machine learning, have the potential to deliver high-quality reconstructions in MPI. We investigate a novel reconstruction approach based on a deep image prior, which builds on representing the solution by a deep neural network. Novel approaches, as well as variational and iterative regularization techniques, are compared quantitatively in terms of peak signal-to-noise ratios and structural similarity indices on the publicly available Open MPI dataset.



### LOL: Lidar-Only Odometry and Localization in 3D Point Cloud Maps
- **Arxiv ID**: http://arxiv.org/abs/2007.01595v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.01595v1)
- **Published**: 2020-07-03 10:20:53+00:00
- **Updated**: 2020-07-03 10:20:53+00:00
- **Authors**: David Rozenberszki, Andras Majdik
- **Comment**: Accepted paper for ICRA 2020, Github repository for implementation
  at: https://github.com/RozDavid/LOL
- **Journal**: None
- **Summary**: In this paper we deal with the problem of odometry and localization for Lidar-equipped vehicles driving in urban environments, where a premade target map exists to localize against. In our problem formulation, to correct the accumulated drift of the Lidar-only odometry we apply a place recognition method to detect geometrically similar locations between the online 3D point cloud and the a priori offline map. In the proposed system, we integrate a state-of-the-art Lidar-only odometry algorithm with a recently proposed 3D point segment matching method by complementing their advantages. Also, we propose additional enhancements in order to reduce the number of false matches between the online point cloud and the target map, and to refine the position estimation error whenever a good match is detected. We demonstrate the utility of the proposed LOL system on several Kitti datasets of different lengths and environments, where the relocalization accuracy and the precision of the vehicle's trajectory were significantly improved in every case, while still being able to maintain real-time performance.



### Weakly Supervised Temporal Action Localization with Segment-Level Labels
- **Arxiv ID**: http://arxiv.org/abs/2007.01598v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.01598v1)
- **Published**: 2020-07-03 10:32:19+00:00
- **Updated**: 2020-07-03 10:32:19+00:00
- **Authors**: Xinpeng Ding, Nannan Wang, Xinbo Gao, Jie Li, Xiaoyu Wang, Tongliang Liu
- **Comment**: 18 pages,7 figures
- **Journal**: None
- **Summary**: Temporal action localization presents a trade-off between test performance and annotation-time cost. Fully supervised methods achieve good performance with time-consuming boundary annotations. Weakly supervised methods with cheaper video-level category label annotations result in worse performance. In this paper, we introduce a new segment-level supervision setting: segments are labeled when annotators observe actions happening here. We incorporate this segment-level supervision along with a novel localization module in the training. Specifically, we devise a partial segment loss regarded as a loss sampling to learn integral action parts from labeled segments. Since the labeled segments are only parts of actions, the model tends to overfit along with the training process. To tackle this problem, we first obtain a similarity matrix from discriminative features guided by a sphere loss. Then, a propagation loss is devised based on the matrix to act as a regularization term, allowing implicit unlabeled segments propagation during training. Experiments validate that our method can outperform the video-level supervision methods with almost same the annotation time.



### Balanced Symmetric Cross Entropy for Large Scale Imbalanced and Noisy Data
- **Arxiv ID**: http://arxiv.org/abs/2007.01618v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.01618v1)
- **Published**: 2020-07-03 11:24:43+00:00
- **Updated**: 2020-07-03 11:24:43+00:00
- **Authors**: Feifei Huang, Jie Li, Xuelin Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: Deep convolution neural network has attracted many attentions in large-scale visual classification task, and achieves significant performance improvement compared to traditional visual analysis methods. In this paper, we explore many kinds of deep convolution neural network architectures for large-scale product recognition task, which is heavily class-imbalanced and noisy labeled data, making it more challenged. Extensive experiments show that PNASNet achieves best performance among a variety of convolutional architectures. Together with ensemble technology and negative learning loss for noisy labeled data, we further improve the model performance on online test data. Finally, our proposed method achieves 0.1515 mean top-1 error on online test data.



### Complex Network Construction for Interactive Image Segmentation using Particle Competition and Cooperation: A New Approach
- **Arxiv ID**: http://arxiv.org/abs/2007.01625v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.01625v1)
- **Published**: 2020-07-03 11:42:07+00:00
- **Updated**: 2020-07-03 11:42:07+00:00
- **Authors**: Jefferson Antonio Ribeiro Passerini, Fabricio Aparecido Breve
- **Comment**: The 20th International Conference on Computational Science and its
  Applications (ICCSA2020)
- **Journal**: None
- **Summary**: In the interactive image segmentation task, the Particle Competition and Cooperation (PCC) model is fed with a complex network, which is built from the input image. In the network construction phase, a weight vector is needed to define the importance of each element in the feature set, which consists of color and location information of the corresponding pixels, thus demanding a specialist's intervention. The present paper proposes the elimination of the weight vector through modifications in the network construction phase. The proposed model and the reference model, without the use of a weight vector, were compared using 151 images extracted from the Grabcut dataset, the PASCAL VOC dataset and the Alpha matting dataset. Each model was applied 30 times to each image to obtain an error average. These simulations resulted in an error rate of only 0.49\% when classifying pixels with the proposed model while the reference model had an error rate of 3.14\%. The proposed method also presented less error variation in the diversity of the evaluated images, when compared to the reference model.



### HDR-GAN: HDR Image Reconstruction from Multi-Exposed LDR Images with Large Motions
- **Arxiv ID**: http://arxiv.org/abs/2007.01628v1
- **DOI**: 10.1109/TIP.2021.3064433
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.01628v1)
- **Published**: 2020-07-03 11:42:35+00:00
- **Updated**: 2020-07-03 11:42:35+00:00
- **Authors**: Yuzhen Niu, Jianbin Wu, Wenxi Liu, Wenzhong Guo, Rynson W. H. Lau
- **Comment**: None
- **Journal**: None
- **Summary**: Synthesizing high dynamic range (HDR) images from multiple low-dynamic range (LDR) exposures in dynamic scenes is challenging. There are two major problems caused by the large motions of foreground objects. One is the severe misalignment among the LDR images. The other is the missing content due to the over-/under-saturated regions caused by the moving objects, which may not be easily compensated for by the multiple LDR exposures. Thus, it requires the HDR generation model to be able to properly fuse the LDR images and restore the missing details without introducing artifacts. To address these two problems, we propose in this paper a novel GAN-based model, HDR-GAN, for synthesizing HDR images from multi-exposed LDR images. To our best knowledge, this work is the first GAN-based approach for fusing multi-exposed LDR images for HDR reconstruction. By incorporating adversarial learning, our method is able to produce faithful information in the regions with missing content. In addition, we also propose a novel generator network, with a reference-based residual merging block for aligning large object motions in the feature domain, and a deep HDR supervision scheme for eliminating artifacts of the reconstructed HDR images. Experimental results demonstrate that our model achieves state-of-the-art reconstruction performance over the prior HDR methods on diverse scenes.



### Accurate Bounding-box Regression with Distance-IoU Loss for Visual Tracking
- **Arxiv ID**: http://arxiv.org/abs/2007.01864v4
- **DOI**: 10.1016/j.jvcir.2021.103428
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.01864v4)
- **Published**: 2020-07-03 11:57:54+00:00
- **Updated**: 2022-01-20 02:43:52+00:00
- **Authors**: Di Yuan, Xiu Shu, Nana Fan, Xiaojun Chang, Qiao Liu, Zhenyu He
- **Comment**: None
- **Journal**: None
- **Summary**: Most existing trackers are based on using a classifier and multi-scale estimation to estimate the target state. Consequently, and as expected, trackers have become more stable while tracking accuracy has stagnated. While trackers adopt a maximum overlap method based on an intersection-over-union (IoU) loss to mitigate this problem, there are defects in the IoU loss itself, that make it impossible to continue to optimize the objective function when a given bounding box is completely contained within/without another bounding box; this makes it very challenging to accurately estimate the target state. Accordingly, in this paper, we address the above-mentioned problem by proposing a novel tracking method based on a distance-IoU (DIoU) loss, such that the proposed tracker consists of target estimation and target classification. The target estimation part is trained to predict the DIoU score between the target ground-truth bounding-box and the estimated bounding-box. The DIoU loss can maintain the advantage provided by the IoU loss while minimizing the distance between the center points of two bounding boxes, thereby making the target estimation more accurate. Moreover, we introduce a classification part that is trained online and optimized with a Conjugate-Gradient-based strategy to guarantee real-time tracking speed. Comprehensive experimental results demonstrate that the proposed method achieves competitive tracking accuracy when compared to state-of-the-art trackers while with a real-time tracking speed.



### Improving auto-encoder novelty detection using channel attention and entropy minimization
- **Arxiv ID**: http://arxiv.org/abs/2007.01682v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.01682v2)
- **Published**: 2020-07-03 13:41:34+00:00
- **Updated**: 2021-05-10 05:36:11+00:00
- **Authors**: Miao Tian, Dongyan Guo, Ying Cui, Xiang Pan, Shengyong Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Novelty detection is a important research area which mainly solves the classification problem of inliers which usually consists of normal samples and outliers composed of abnormal samples. Auto-encoder is often used for novelty detection. However, the generalization ability of the auto-encoder may cause the undesirable reconstruction of abnormal elements and reduce the identification ability of the model. To solve the problem, we focus on the perspective of better reconstructing the normal samples as well as retaining the unique information of normal samples to improve the performance of auto-encoder for novelty detection. Firstly, we introduce attention mechanism into the task. Under the action of attention mechanism, auto-encoder can pay more attention to the representation of inlier samples through adversarial training. Secondly, we apply the information entropy into the latent layer to make it sparse and constrain the expression of diversity. Experimental results on three public datasets show that the proposed method achieves comparable performance compared with previous popular approaches.



### Synergistic saliency and depth prediction for RGB-D saliency detection
- **Arxiv ID**: http://arxiv.org/abs/2007.01711v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.01711v2)
- **Published**: 2020-07-03 14:24:41+00:00
- **Updated**: 2020-10-26 06:23:18+00:00
- **Authors**: Yue Wang, Yuke Li, James H. Elder, Huchuan Lu, Runmin Wu, Lu Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Depth information available from an RGB-D camera can be useful in segmenting salient objects when figure/ground cues from RGB channels are weak. This has motivated the development of several RGB-D saliency datasets and algorithms that use all four channels of the RGB-D data for both training and inference. Unfortunately, existing RGB-D saliency datasets are small, which may lead to overfitting and limited generalization for diverse scenarios. Here we propose a semi-supervised system for RGB-D saliency detection that can be trained on smaller RGB-D saliency datasets without saliency ground truth, while also make effective joint use of a large RGB saliency dataset with saliency ground truth together. To generalize our method on RGB-D saliency datasets, a novel prediction-guided cross-refinement module which jointly estimates both saliency and depth by mutual refinement between two respective tasks, and an adversarial learning approach are employed. Critically, our system does not require saliency ground-truth for the RGB-D datasets, which saves the massive human labor for hand labeling, and does not require the depth data for inference, allowing the method to be used for the much broader range of applications where only RGB data are available. Evaluation on seven RGB-D datasets demonstrates that even without saliency ground truth for RGB-D datasets and using only the RGB data of RGB-D datasets at inference, our semi-supervised system performs favorable against state-of-the-art fully-supervised RGB-D saliency detection methods that use saliency ground truth for RGB-D datasets at training and depth data at inference on two largest testing datasets. Our approach also achieves comparable results on other popular RGB-D saliency benchmarks.



### Perceptually Optimizing Deep Image Compression
- **Arxiv ID**: http://arxiv.org/abs/2007.02711v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.02711v2)
- **Published**: 2020-07-03 14:33:28+00:00
- **Updated**: 2020-07-09 15:04:06+00:00
- **Authors**: Li-Heng Chen, Christos G. Bampis, Zhi Li, Andrey Norkin, Alan C. Bovik
- **Comment**: 7 pages, 6 figures. arXiv admin note: substantial text overlap with
  arXiv:1910.08845
- **Journal**: None
- **Summary**: Mean squared error (MSE) and $\ell_p$ norms have largely dominated the measurement of loss in neural networks due to their simplicity and analytical properties. However, when used to assess visual information loss, these simple norms are not highly consistent with human perception. Here, we propose a different proxy approach to optimize image analysis networks against quantitative perceptual models. Specifically, we construct a proxy network, which mimics the perceptual model while serving as a loss layer of the network.We experimentally demonstrate how this optimization framework can be applied to train an end-to-end optimized image compression network. By building on top of a modern deep image compression models, we are able to demonstrate an averaged bitrate reduction of $28.7\%$ over MSE optimization, given a specified perceptual quality (VMAF) level.



### Deep Fence Estimation using Stereo Guidance and Adversarial Learning
- **Arxiv ID**: http://arxiv.org/abs/2007.01724v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4
- **Links**: [PDF](http://arxiv.org/pdf/2007.01724v1)
- **Published**: 2020-07-03 14:46:30+00:00
- **Updated**: 2020-07-03 14:46:30+00:00
- **Authors**: Paritosh Mittal, Shankar M Venkatesan, Viswanath Veera, Aloknath De
- **Comment**: It was previously submitted to IEEE ICIP 2020. A previous version was
  also submitted to BMVC 2019
- **Journal**: None
- **Summary**: People capture memorable images of events and exhibits that are often occluded by a wire mesh loosely termed as fence. Recent works in removing fence have limited performance due to the difficulty in initial fence segmentation. This work aims to accurately segment fence using a novel fence guidance mask (FM) generated from stereo image pair. This binary guidance mask contains deterministic cues about the structure of fence and is given as additional input to the deep fence estimation model. We also introduce a directional connectivity loss (DCL), which is used alongside adversarial loss to precisely detect thin wires. Experimental results obtained on real world scenarios demonstrate the superiority of proposed method over state-of-the-art techniques.



### Video Prediction via Example Guidance
- **Arxiv ID**: http://arxiv.org/abs/2007.01738v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.01738v1)
- **Published**: 2020-07-03 14:57:24+00:00
- **Updated**: 2020-07-03 14:57:24+00:00
- **Authors**: Jingwei Xu, Huazhe Xu, Bingbing Ni, Xiaokang Yang, Trevor Darrell
- **Comment**: Project Page: https://sites.google.com/view/vpeg-supp/home
- **Journal**: ICML 2020
- **Summary**: In video prediction tasks, one major challenge is to capture the multi-modal nature of future contents and dynamics. In this work, we propose a simple yet effective framework that can efficiently predict plausible future states. The key insight is that the potential distribution of a sequence could be approximated with analogous ones in a repertoire of training pool, namely, expert examples. By further incorporating a novel optimization scheme into the training procedure, plausible predictions can be sampled efficiently from distribution constructed from the retrieved examples. Meanwhile, our method could be seamlessly integrated with existing stochastic predictive models; significant enhancement is observed with comprehensive experiments in both quantitative and qualitative aspects. We also demonstrate the generalization ability to predict the motion of unseen class, i.e., without access to corresponding data during training phase.



### Learning to Discover Multi-Class Attentional Regions for Multi-Label Image Recognition
- **Arxiv ID**: http://arxiv.org/abs/2007.01755v3
- **DOI**: 10.1109/TIP.2021.3088605
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.01755v3)
- **Published**: 2020-07-03 15:22:46+00:00
- **Updated**: 2021-06-09 08:27:59+00:00
- **Authors**: Bin-Bin Gao, Hong-Yu Zhou
- **Comment**: 13 pages, Accepted by IEEE TIP (5-Jun-2021)
- **Journal**: None
- **Summary**: Multi-label image recognition is a practical and challenging task compared to single-label image classification. However, previous works may be suboptimal because of a great number of object proposals or complex attentional region generation modules. In this paper, we propose a simple but efficient two-stream framework to recognize multi-category objects from global image to local regions, similar to how human beings perceive objects. To bridge the gap between global and local streams, we propose a multi-class attentional region module which aims to make the number of attentional regions as small as possible and keep the diversity of these regions as high as possible. Our method can efficiently and effectively recognize multi-class objects with an affordable computation cost and a parameter-free region localization module. Over three benchmarks on multi-label image classification, we create new state-of-the-art results with a single model only using image semantics without label dependency. In addition, the effectiveness of the proposed method is extensively demonstrated under different factors such as global pooling strategy, input size and network architecture. Code has been made available at~\url{https://github.com/gaobb/MCAR}.



### Collaborative Learning for Faster StyleGAN Embedding
- **Arxiv ID**: http://arxiv.org/abs/2007.01758v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.01758v1)
- **Published**: 2020-07-03 15:27:37+00:00
- **Updated**: 2020-07-03 15:27:37+00:00
- **Authors**: Shanyan Guan, Ying Tai, Bingbing Ni, Feida Zhu, Feiyue Huang, Xiaokang Yang
- **Comment**: 10 pages, 11 figures
- **Journal**: None
- **Summary**: The latent code of the recent popular model StyleGAN has learned disentangled representations thanks to the multi-layer style-based generator. Embedding a given image back to the latent space of StyleGAN enables wide interesting semantic image editing applications. Although previous works are able to yield impressive inversion results based on an optimization framework, which however suffers from the efficiency issue. In this work, we propose a novel collaborative learning framework that consists of an efficient embedding network and an optimization-based iterator. On one hand, with the progress of training, the embedding network gives a reasonable latent code initialization for the iterator. On the other hand, the updated latent code from the iterator in turn supervises the embedding network. In the end, high-quality latent code can be obtained efficiently with a single forward pass through our embedding network. Extensive experiments demonstrate the effectiveness and efficiency of our work.



### Selecting Regions of Interest in Large Multi-Scale Images for Cancer Pathology
- **Arxiv ID**: http://arxiv.org/abs/2007.01866v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.01866v1)
- **Published**: 2020-07-03 15:27:41+00:00
- **Updated**: 2020-07-03 15:27:41+00:00
- **Authors**: Rui Aguiar, Jon Braatz
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: Recent breakthroughs in object detection and image classification using Convolutional Neural Networks (CNNs) are revolutionizing the state of the art in medical imaging, and microscopy in particular presents abundant opportunities for computer vision algorithms to assist medical professionals in diagnosis of diseases ranging from malaria to cancer. High resolution scans of microscopy slides called Whole Slide Images (WSIs) offer enough information for a cancer pathologist to come to a conclusion regarding cancer presence, subtype, and severity based on measurements of features within the slide image at multiple scales and resolutions. WSIs' extremely high resolutions and feature scales ranging from gross anatomical structures down to cell nuclei preclude the use of standard CNN models for object detection and classification, which have typically been designed for images with dimensions in the hundreds of pixels and with objects on the order of the size of the image itself. We explore parallel approaches based on Reinforcement Learning and Beam Search to learn to progressively zoom into the WSI to detect Regions of Interest (ROIs) in liver pathology slides containing one of two types of liver cancer, namely Hepatocellular Carcinoma (HCC) and Cholangiocarcinoma (CC). These ROIs can then be presented directly to the pathologist to aid in measurement and diagnosis or be used for automated classification of tumor subtype.



### Explainable Deep One-Class Classification
- **Arxiv ID**: http://arxiv.org/abs/2007.01760v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.01760v3)
- **Published**: 2020-07-03 15:29:06+00:00
- **Updated**: 2021-03-18 10:35:33+00:00
- **Authors**: Philipp Liznerski, Lukas Ruff, Robert A. Vandermeulen, Billy Joe Franks, Marius Kloft, Klaus-Robert Müller
- **Comment**: 25 pages, published as a conference paper at ICLR 2021
- **Journal**: None
- **Summary**: Deep one-class classification variants for anomaly detection learn a mapping that concentrates nominal samples in feature space causing anomalies to be mapped away. Because this transformation is highly non-linear, finding interpretations poses a significant challenge. In this paper we present an explainable deep one-class classification method, Fully Convolutional Data Description (FCDD), where the mapped samples are themselves also an explanation heatmap. FCDD yields competitive detection performance and provides reasonable explanations on common anomaly detection benchmarks with CIFAR-10 and ImageNet. On MVTec-AD, a recent manufacturing dataset offering ground-truth anomaly maps, FCDD sets a new state of the art in the unsupervised setting. Our method can incorporate ground-truth anomaly maps during training and using even a few of these (~5) improves performance significantly. Finally, using FCDD's explanations we demonstrate the vulnerability of deep one-class classification models to spurious image features such as image watermarks.



### End-to-end Interpretable Learning of Non-blind Image Deblurring
- **Arxiv ID**: http://arxiv.org/abs/2007.01769v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.01769v2)
- **Published**: 2020-07-03 15:45:01+00:00
- **Updated**: 2020-09-15 14:44:59+00:00
- **Authors**: Thomas Eboli, Jian Sun, Jean Ponce
- **Comment**: Accepted at ECCV2020 (poster)
- **Journal**: None
- **Summary**: Non-blind image deblurring is typically formulated as a linear least-squares problem regularized by natural priors on the corresponding sharp picture's gradients, which can be solved, for example, using a half-quadratic splitting method with Richardson fixed-point iterations for its least-squares updates and a proximal operator for the auxiliary variable updates. We propose to precondition the Richardson solver using approximate inverse filters of the (known) blur and natural image prior kernels. Using convolutions instead of a generic linear preconditioner allows extremely efficient parameter sharing across the image, and leads to significant gains in accuracy and/or speed compared to classical FFT and conjugate-gradient methods. More importantly, the proposed architecture is easily adapted to learning both the preconditioner and the proximal operator using CNN embeddings. This yields a simple and efficient algorithm for non-blind image deblurring which is fully interpretable, can be learned end to end, and whose accuracy matches or exceeds the state of the art, quite significantly, in the non-uniform case.



### Learning Expectation of Label Distribution for Facial Age and Attractiveness Estimation
- **Arxiv ID**: http://arxiv.org/abs/2007.01771v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.01771v2)
- **Published**: 2020-07-03 15:46:53+00:00
- **Updated**: 2021-12-31 11:00:57+00:00
- **Authors**: Bin-Bin Gao, Xin-Xin Liu, Hong-Yu Zhou, Jianxin Wu, Xin Geng
- **Comment**: submitted to Pattern Recognition
- **Journal**: None
- **Summary**: Facial attributes (\eg, age and attractiveness) estimation performance has been greatly improved by using convolutional neural networks. However, existing methods have an inconsistency between the training objectives and the evaluation metric, so they may be suboptimal. In addition, these methods always adopt image classification or face recognition models with a large amount of parameters, which carry expensive computation cost and storage overhead. In this paper, we firstly analyze the essential relationship between two state-of-the-art methods (Ranking-CNN and DLDL) and show that the Ranking method is in fact learning label distribution implicitly. This result thus firstly unifies two existing popular state-of-the-art methods into the DLDL framework. Second, in order to alleviate the inconsistency and reduce resource consumption, we design a lightweight network architecture and propose a unified framework which can jointly learn facial attribute distribution and regress attribute value. The effectiveness of our approach has been demonstrated on both facial age and attractiveness estimation tasks. Our method achieves new state-of-the-art results using the single model with 36$\times$ fewer parameters and 3$\times$ faster inference speed on facial age/attractiveness estimation. Moreover, our method can achieve comparable results as the state-of-the-art even though the number of parameters is further reduced to 0.9M (3.8MB disk storage).



### Visual Question Answering as a Multi-Task Problem
- **Arxiv ID**: http://arxiv.org/abs/2007.01780v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2007.01780v1)
- **Published**: 2020-07-03 16:07:13+00:00
- **Updated**: 2020-07-03 16:07:13+00:00
- **Authors**: Amelia Elizabeth Pollard, Jonathan L. Shapiro
- **Comment**: None
- **Journal**: None
- **Summary**: Visual Question Answering(VQA) is a highly complex problem set, relying on many sub-problems to produce reasonable answers. In this paper, we present the hypothesis that Visual Question Answering should be viewed as a multi-task problem, and provide evidence to support this hypothesis. We demonstrate this by reformatting two commonly used Visual Question Answering datasets, COCO-QA and DAQUAR, into a multi-task format and train these reformatted datasets on two baseline networks, with one designed specifically to eliminate other possible causes for performance changes as a result of the reformatting. Though the networks demonstrated in this paper do not achieve strongly competitive results, we find that the multi-task approach to Visual Question Answering results in increases in performance of 5-9% against the single-task formatting, and that the networks reach convergence much faster than in the single-task case. Finally we discuss possible reasons for the observed difference in performance, and perform additional experiments which rule out causes not associated with the learning of the dataset as a multi-task problem.



### Eliminating Catastrophic Interference with Biased Competition
- **Arxiv ID**: http://arxiv.org/abs/2007.02833v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.02833v1)
- **Published**: 2020-07-03 16:15:15+00:00
- **Updated**: 2020-07-03 16:15:15+00:00
- **Authors**: Amelia Elizabeth Pollard, Jonathan L. Shapiro
- **Comment**: None
- **Journal**: None
- **Summary**: We present here a model to take advantage of the multi-task nature of complex datasets by learning to separate tasks and subtasks in and end to end manner by biasing competitive interactions in the network. This method does not require additional labelling or reformatting of data in a dataset. We propose an alternate view to the monolithic one-task-fits-all learning of multi-task problems, and describe a model based on a theory of neuronal attention from neuroscience, proposed by Desimone. We create and exhibit a new toy dataset, based on the MNIST dataset, which we call MNIST-QA, for testing Visual Question Answering architectures in a low-dimensional environment while preserving the more difficult components of the Visual Question Answering task, and demonstrate the proposed network architecture on this new dataset, as well as on COCO-QA and DAQUAR-FULL. We then demonstrate that this model eliminates catastrophic interference between tasks on a newly created toy dataset and provides competitive results in the Visual Question Answering space. We provide further evidence that Visual Question Answering can be approached as a multi-task problem, and demonstrate that this new architecture based on the Biased Competition model is capable of learning to separate and learn the tasks in an end-to-end fashion without the need for task labels.



### Evaluating Uncertainty Estimation Methods on 3D Semantic Segmentation of Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2007.01787v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2007.01787v1)
- **Published**: 2020-07-03 16:22:34+00:00
- **Updated**: 2020-07-03 16:22:34+00:00
- **Authors**: Swaroop Bhandary K, Nico Hochgeschwender, Paul Plöger, Frank Kirchner, Matias Valdenegro-Toro
- **Comment**: 12 pages, 19 figures, ICML 2020 Workshop on Uncertainty and
  Robustness in Deep Learning
- **Journal**: None
- **Summary**: Deep learning models are extensively used in various safety critical applications. Hence these models along with being accurate need to be highly reliable. One way of achieving this is by quantifying uncertainty. Bayesian methods for UQ have been extensively studied for Deep Learning models applied on images but have been less explored for 3D modalities such as point clouds often used for Robots and Autonomous Systems. In this work, we evaluate three uncertainty quantification methods namely Deep Ensembles, MC-Dropout and MC-DropConnect on the DarkNet21Seg 3D semantic segmentation model and comprehensively analyze the impact of various parameters such as number of models in ensembles or forward passes, and drop probability values, on task performance and uncertainty estimate quality. We find that Deep Ensembles outperforms other methods in both performance and uncertainty metrics. Deep ensembles outperform other methods by a margin of 2.4% in terms of mIOU, 1.3% in terms of accuracy, while providing reliable uncertainty for decision making.



### Do Not Mask What You Do Not Need to Mask: a Parser-Free Virtual Try-On
- **Arxiv ID**: http://arxiv.org/abs/2007.02721v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.02721v2)
- **Published**: 2020-07-03 16:39:39+00:00
- **Updated**: 2020-07-29 16:04:59+00:00
- **Authors**: Thibaut Issenhuth, Jérémie Mary, Clément Calauzènes
- **Comment**: Accepted at ECCV 2020. arXiv admin note: text overlap with
  arXiv:1906.01347
- **Journal**: None
- **Summary**: The 2D virtual try-on task has recently attracted a great interest from the research community, for its direct potential applications in online shopping as well as for its inherent and non-addressed scientific challenges. This task requires fitting an in-shop cloth image on the image of a person, which is highly challenging because it involves cloth warping, image compositing, and synthesizing. Casting virtual try-on into a supervised task faces a difficulty: available datasets are composed of pairs of pictures (cloth, person wearing the cloth). Thus, we have no access to ground-truth when the cloth on the person changes. State-of-the-art models solve this by masking the cloth information on the person with both a human parser and a pose estimator. Then, image synthesis modules are trained to reconstruct the person image from the masked person image and the cloth image. This procedure has several caveats: firstly, human parsers are prone to errors; secondly, it is a costly pre-processing step, which also has to be applied at inference time; finally, it makes the task harder than it is since the mask covers information that should be kept such as hands or accessories. In this paper, we propose a novel student-teacher paradigm where the teacher is trained in the standard way (reconstruction) before guiding the student to focus on the initial task (changing the cloth). The student additionally learns from an adversarial loss, which pushes it to follow the distribution of the real images. Consequently, the student exploits information that is masked to the teacher. A student trained without the adversarial loss would not use this information. Also, getting rid of both human parser and pose estimator at inference time allows obtaining a real-time virtual try-on.



### Deep learning for scene recognition from visual data: a survey
- **Arxiv ID**: http://arxiv.org/abs/2007.01806v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.01806v1)
- **Published**: 2020-07-03 16:53:18+00:00
- **Updated**: 2020-07-03 16:53:18+00:00
- **Authors**: Alina Matei, Andreea Glavan, Estefania Talavera
- **Comment**: None
- **Journal**: None
- **Summary**: The use of deep learning techniques has exploded during the last few years, resulting in a direct contribution to the field of artificial intelligence. This work aims to be a review of the state-of-the-art in scene recognition with deep learning models from visual data. Scene recognition is still an emerging field in computer vision, which has been addressed from a single image and dynamic image perspective. We first give an overview of available datasets for image and video scene recognition. Later, we describe ensemble techniques introduced by research papers in the field. Finally, we give some remarks on our findings and discuss what we consider challenges in the field and future lines of research. This paper aims to be a future guide for model selection for the task of scene recognition.



### Continuously Indexed Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2007.01807v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.01807v2)
- **Published**: 2020-07-03 16:53:50+00:00
- **Updated**: 2020-08-30 02:31:43+00:00
- **Authors**: Hao Wang, Hao He, Dina Katabi
- **Comment**: Accepted at ICML 2020. Talk:
  https://www.youtube.com/watch?v=KtZPSCD-WhQ Code and Project Page:
  https://github.com/hehaodele/CIDA
- **Journal**: None
- **Summary**: Existing domain adaptation focuses on transferring knowledge between domains with categorical indices (e.g., between datasets A and B). However, many tasks involve continuously indexed domains. For example, in medical applications, one often needs to transfer disease analysis and prediction across patients of different ages, where age acts as a continuous domain index. Such tasks are challenging for prior domain adaptation methods since they ignore the underlying relation among domains. In this paper, we propose the first method for continuously indexed domain adaptation. Our approach combines traditional adversarial adaptation with a novel discriminator that models the encoding-conditioned domain index distribution. Our theoretical analysis demonstrates the value of leveraging the domain index to generate invariant features across a continuous range of domains. Our empirical results show that our approach outperforms the state-of-the-art domain adaption methods on both synthetic and real-world medical datasets.



### AVP-SLAM: Semantic Visual Mapping and Localization for Autonomous Vehicles in the Parking Lot
- **Arxiv ID**: http://arxiv.org/abs/2007.01813v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.01813v2)
- **Published**: 2020-07-03 17:02:46+00:00
- **Updated**: 2020-07-08 14:13:25+00:00
- **Authors**: Tong Qin, Tongqing Chen, Yilun Chen, Qing Su
- **Comment**: The IEEE/RSJ International Conference on Intelligent Robots and
  Systems, IROS 2020
- **Journal**: None
- **Summary**: Autonomous valet parking is a specific application for autonomous vehicles. In this task, vehicles need to navigate in narrow, crowded and GPS-denied parking lots. Accurate localization ability is of great importance. Traditional visual-based methods suffer from tracking lost due to texture-less regions, repeated structures, and appearance changes. In this paper, we exploit robust semantic features to build the map and localize vehicles in parking lots. Semantic features contain guide signs, parking lines, speed bumps, etc, which typically appear in parking lots. Compared with traditional features, these semantic features are long-term stable and robust to the perspective and illumination change. We adopt four surround-view cameras to increase the perception range. Assisting by an IMU (Inertial Measurement Unit) and wheel encoders, the proposed system generates a global visual semantic map. This map is further used to localize vehicles at the centimeter level. We analyze the accuracy and recall of our system and compare it against other methods in real experiments. Furthermore, we demonstrate the practicability of the proposed system by the autonomous parking application.



### Image-based Vehicle Re-identification Model with Adaptive Attention Modules and Metadata Re-ranking
- **Arxiv ID**: http://arxiv.org/abs/2007.01818v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.01818v1)
- **Published**: 2020-07-03 17:14:18+00:00
- **Updated**: 2020-07-03 17:14:18+00:00
- **Authors**: Quang Truong, Hy Dang, Zhankai Ye, Minh Nguyen, Bo Mei
- **Comment**: None
- **Journal**: None
- **Summary**: Vehicle Re-identification is a challenging task due to intra-class variability and inter-class similarity across non-overlapping cameras. To tackle these problems, recently proposed methods require additional annotation to extract more features for false positive image exclusion. In this paper, we propose a model powered by adaptive attention modules that requires fewer label annotations but still out-performs the previous models. We also include a re-ranking method that takes account of the importance of metadata feature embeddings in our paper. The proposed method is evaluated on CVPR AI City Challenge 2020 dataset and achieves mAP of 37.25% in Track 2.



### LOOC: Localize Overlapping Objects with Count Supervision
- **Arxiv ID**: http://arxiv.org/abs/2007.01837v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.01837v1)
- **Published**: 2020-07-03 17:44:13+00:00
- **Updated**: 2020-07-03 17:44:13+00:00
- **Authors**: Issam H. Laradji, Rafael Pardinas, Pau Rodriguez, David Vazquez
- **Comment**: None
- **Journal**: None
- **Summary**: Acquiring count annotations generally requires less human effort than point-level and bounding box annotations. Thus, we propose the novel problem setup of localizing objects in dense scenes under this weaker supervision. We propose LOOC, a method to Localize Overlapping Objects with Count supervision. We train LOOC by alternating between two stages. In the first stage, LOOC learns to generate pseudo point-level annotations in a semi-supervised manner. In the second stage, LOOC uses a fully-supervised localization method that trains on these pseudo labels. The localization method is used to progressively improve the quality of the pseudo labels. We conducted experiments on popular counting datasets. For localization, LOOC achieves a strong new baseline in the novel problem setup where only count supervision is available. For counting, LOOC outperforms current state-of-the-art methods that only use count as their supervision. Code is available at: https://github.com/ElementAI/looc.



### Swoosh! Rattle! Thump! -- Actions that Sound
- **Arxiv ID**: http://arxiv.org/abs/2007.01851v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.01851v1)
- **Published**: 2020-07-03 17:57:54+00:00
- **Updated**: 2020-07-03 17:57:54+00:00
- **Authors**: Dhiraj Gandhi, Abhinav Gupta, Lerrel Pinto
- **Comment**: To be presented at Robotics: Science and Systems 2020
- **Journal**: None
- **Summary**: Truly intelligent agents need to capture the interplay of all their senses to build a rich physical understanding of their world. In robotics, we have seen tremendous progress in using visual and tactile perception; however, we have often ignored a key sense: sound. This is primarily due to the lack of data that captures the interplay of action and sound. In this work, we perform the first large-scale study of the interactions between sound and robotic action. To do this, we create the largest available sound-action-vision dataset with 15,000 interactions on 60 objects using our robotic platform Tilt-Bot. By tilting objects and allowing them to crash into the walls of a robotic tray, we collect rich four-channel audio information. Using this data, we explore the synergies between sound and action and present three key insights. First, sound is indicative of fine-grained object class information, e.g., sound can differentiate a metal screwdriver from a metal wrench. Second, sound also contains information about the causal effects of an action, i.e. given the sound produced, we can predict what action was applied to the object. Finally, object representations derived from audio embeddings are indicative of implicit physical properties. We demonstrate that on previously unseen objects, audio embeddings generated through interactions can predict forward models 24% better than passive visual embeddings. Project videos and data are at https://dhiraj100892.github.io/swoosh/



### Egocentric Action Recognition by Video Attention and Temporal Context
- **Arxiv ID**: http://arxiv.org/abs/2007.01883v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.01883v1)
- **Published**: 2020-07-03 18:00:32+00:00
- **Updated**: 2020-07-03 18:00:32+00:00
- **Authors**: Juan-Manuel Perez-Rua, Antoine Toisoul, Brais Martinez, Victor Escorcia, Li Zhang, Xiatian Zhu, Tao Xiang
- **Comment**: EPIC-Kitchens challenges@CVPR 2020
- **Journal**: None
- **Summary**: We present the submission of Samsung AI Centre Cambridge to the CVPR2020 EPIC-Kitchens Action Recognition Challenge. In this challenge, action recognition is posed as the problem of simultaneously predicting a single `verb' and `noun' class label given an input trimmed video clip. That is, a `verb' and a `noun' together define a compositional `action' class. The challenging aspects of this real-life action recognition task include small fast moving objects, complex hand-object interactions, and occlusions. At the core of our submission is a recently-proposed spatial-temporal video attention model, called `W3' (`What-Where-When') attention~\cite{perez2020knowing}. We further introduce a simple yet effective contextual learning mechanism to model `action' class scores directly from long-term temporal behaviour based on the `verb' and `noun' prediction scores. Our solution achieves strong performance on the challenge metrics without using object-specific reasoning nor extra training data. In particular, our best solution with multimodal ensemble achieves the 2$^{nd}$ best position for `verb', and 3$^{rd}$ best for `noun' and `action' on the Seen Kitchens test set.



### A Few-Shot Sequential Approach for Object Counting
- **Arxiv ID**: http://arxiv.org/abs/2007.01899v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.01899v2)
- **Published**: 2020-07-03 18:23:39+00:00
- **Updated**: 2020-07-07 20:11:00+00:00
- **Authors**: Negin Sokhandan, Pegah Kamousi, Alejandro Posada, Eniola Alese, Negar Rostamzadeh
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we address the problem of few-shot multi-class object counting with point-level annotations. The proposed technique leverages a class agnostic attention mechanism that sequentially attends to objects in the image and extracts their relevant features. This process is employed on an adapted prototypical-based few-shot approach that uses the extracted features to classify each one either as one of the classes present in the support set images or as background. The proposed technique is trained on point-level annotations and uses a novel loss function that disentangles class-dependent and class-agnostic aspects of the model to help with the task of few-shot object counting. We present our results on a variety of object-counting/detection datasets, including FSOD and MS COCO. In addition, we introduce a new dataset that is specifically designed for weakly supervised multi-class object counting/detection and contains considerably different classes and distribution of number of classes/instances per image compared to the existing datasets. We demonstrate the robustness of our approach by testing our system on a totally different distribution of classes from what it has been trained on.



### Graph2Kernel Grid-LSTM: A Multi-Cued Model for Pedestrian Trajectory Prediction by Learning Adaptive Neighborhoods
- **Arxiv ID**: http://arxiv.org/abs/2007.01915v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.01915v2)
- **Published**: 2020-07-03 19:05:48+00:00
- **Updated**: 2020-07-08 08:48:41+00:00
- **Authors**: Sirin Haddad, Siew Kei Lam
- **Comment**: None
- **Journal**: None
- **Summary**: Pedestrian trajectory prediction is a prominent research track that has advanced towards modelling of crowd social and contextual interactions, with extensive usage of Long Short-Term Memory (LSTM) for temporal representation of walking trajectories.   Existing approaches use virtual neighborhoods as a fixed grid for pooling social states of pedestrians with tuning process that controls how social interactions are being captured. This entails performance customization to specific scenes but lowers the generalization capability of the approaches. In our work, we deploy \textit{Grid-LSTM}, a recent extension of LSTM, which operates over multidimensional feature inputs. We present a new perspective to interaction modeling by proposing that pedestrian neighborhoods can become adaptive in design. We use \textit{Grid-LSTM} as an encoder to learn about potential future neighborhoods and their influence on pedestrian motion given the visual and the spatial boundaries. Our model outperforms state-of-the-art approaches that collate resembling features over several publicly-tested surveillance videos. The experiment results clearly illustrate the generalization of our approach across datasets that varies in scene features and crowd dynamics.



### Knowledge Distillation Beyond Model Compression
- **Arxiv ID**: http://arxiv.org/abs/2007.01922v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.01922v1)
- **Published**: 2020-07-03 19:54:04+00:00
- **Updated**: 2020-07-03 19:54:04+00:00
- **Authors**: Fahad Sarfraz, Elahe Arani, Bahram Zonooz
- **Comment**: Accepted at ICPR 2020
- **Journal**: None
- **Summary**: Knowledge distillation (KD) is commonly deemed as an effective model compression technique in which a compact model (student) is trained under the supervision of a larger pretrained model or an ensemble of models (teacher). Various techniques have been proposed since the original formulation, which mimic different aspects of the teacher such as the representation space, decision boundary, or intra-data relationship. Some methods replace the one-way knowledge distillation from a static teacher with collaborative learning between a cohort of students. Despite the recent advances, a clear understanding of where knowledge resides in a deep neural network and an optimal method for capturing knowledge from teacher and transferring it to student remains an open question. In this study, we provide an extensive study on nine different KD methods which covers a broad spectrum of approaches to capture and transfer knowledge. We demonstrate the versatility of the KD framework on different datasets and network architectures under varying capacity gaps between the teacher and student. The study provides intuition for the effects of mimicking different aspects of the teacher and derives insights from the performance of the different distillation approaches to guide the design of more effective KD methods. Furthermore, our study shows the effectiveness of the KD framework in learning efficiently under varying severity levels of label noise and class imbalance, consistently providing generalization gains over standard training. We emphasize that the efficacy of KD goes much beyond a model compression technique and it should be considered as a general-purpose training paradigm which offers more robustness to common challenges in the real-world datasets compared to the standard training procedure.



### Feedback Neural Network based Super-resolution of DEM for generating high fidelity features
- **Arxiv ID**: http://arxiv.org/abs/2007.01940v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.01940v1)
- **Published**: 2020-07-03 21:10:19+00:00
- **Updated**: 2020-07-03 21:10:19+00:00
- **Authors**: Ashish Kubade, Avinash Sharma, K S Rajan
- **Comment**: Accepted for publication in IEEE IGARSS 2020 conference
- **Journal**: None
- **Summary**: High resolution Digital Elevation Models(DEMs) are an important requirement for many applications like modelling water flow, landslides, avalanches etc. Yet publicly available DEMs have low resolution for most parts of the world. Despite tremendous success in image super resolution task using deep learning solutions, there are very few works that have used these powerful systems on DEMs to generate HRDEMs. Motivated from feedback neural networks, we propose a novel neural network architecture that learns to add high frequency details iteratively to low resolution DEM, turning it into a high resolution DEM without compromising its fidelity. Our experiments confirm that without any additional modality such as aerial images(RGB), our network DSRFB achieves RMSEs of 0.59 to 1.27 across 4 different datasets.



### Multigrid for Bundle Adjustment
- **Arxiv ID**: http://arxiv.org/abs/2007.01941v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NA, math.NA
- **Links**: [PDF](http://arxiv.org/pdf/2007.01941v1)
- **Published**: 2020-07-03 21:14:35+00:00
- **Updated**: 2020-07-03 21:14:35+00:00
- **Authors**: Tristan Konolige, Jed Brown
- **Comment**: None
- **Journal**: None
- **Summary**: Bundle adjustment is an important global optimization step in many structure from motion pipelines. Performance is dependent on the speed of the linear solver used to compute steps towards the optimum. For large problems, the current state of the art scales superlinearly with the number of cameras in the problem. We investigate the conditioning of global bundle adjustment problems as the number of images increases in different regimes and fundamental consequences in terms of superlinear scaling of the current state of the art methods. We present an unsmoothed aggregation multigrid preconditioner that accurately represents the global modes that underlie poor scaling of existing methods and demonstrate solves of up to 13 times faster than the state of the art on large, challenging problem sets.



### Mining Cross-Image Semantics for Weakly Supervised Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2007.01947v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.01947v2)
- **Published**: 2020-07-03 21:53:46+00:00
- **Updated**: 2020-07-08 11:51:59+00:00
- **Authors**: Guolei Sun, Wenguan Wang, Jifeng Dai, Luc Van Gool
- **Comment**: Full version of ECCV2020 Oral, CVPR2020 LID workshop Best Paper and
  LID challenge Track1 winner; website: https://github.com/GuoleiSun/MCIS_wsss
- **Journal**: None
- **Summary**: This paper studies the problem of learning semantic segmentation from image-level supervision only. Current popular solutions leverage object localization maps from classifiers as supervision signals, and struggle to make the localization maps capture more complete object content. Rather than previous efforts that primarily focus on intra-image information, we address the value of cross-image semantic relations for comprehensive object pattern mining. To achieve this, two neural co-attentions are incorporated into the classifier to complimentarily capture cross-image semantic similarities and differences. In particular, given a pair of training images, one co-attention enforces the classifier to recognize the common semantics from co-attentive objects, while the other one, called contrastive co-attention, drives the classifier to identify the unshared semantics from the rest, uncommon objects. This helps the classifier discover more object patterns and better ground semantics in image regions. In addition to boosting object pattern learning, the co-attention can leverage context from other related images to improve localization map inference, hence eventually benefiting semantic segmentation learning. More essentially, our algorithm provides a unified framework that handles well different WSSS settings, i.e., learning WSSS with (1) precise image-level supervision only, (2) extra simple single-label data, and (3) extra noisy web data. It sets new state-of-the-arts on all these settings, demonstrating well its efficacy and generalizability. Moreover, our approach ranked 1st place in the Weakly-Supervised Semantic Segmentation Track of CVPR2020 Learning from Imperfect Data Challenge.



### Improving Weakly Supervised Visual Grounding by Contrastive Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/2007.01951v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.01951v2)
- **Published**: 2020-07-03 22:02:00+00:00
- **Updated**: 2021-04-25 05:11:11+00:00
- **Authors**: Liwei Wang, Jing Huang, Yin Li, Kun Xu, Zhengyuan Yang, Dong Yu
- **Comment**: None
- **Journal**: None
- **Summary**: Weakly supervised phrase grounding aims at learning region-phrase correspondences using only image-sentence pairs. A major challenge thus lies in the missing links between image regions and sentence phrases during training. To address this challenge, we leverage a generic object detector at training time, and propose a contrastive learning framework that accounts for both region-phrase and image-sentence matching. Our core innovation is the learning of a region-phrase score function, based on which an image-sentence score function is further constructed. Importantly, our region-phrase score function is learned by distilling from soft matching scores between the detected object names and candidate phrases within an image-sentence pair, while the image-sentence score function is supervised by ground-truth image-sentence pairs. The design of such score functions removes the need of object detection at test time, thereby significantly reducing the inference cost. Without bells and whistles, our approach achieves state-of-the-art results on visual phrase grounding, surpassing previous methods that require expensive object detectors at test time.



