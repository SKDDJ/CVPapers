# Arxiv Papers in cs.CV on 2020-07-17
### Anisotropic Mesh Adaptation for Image Segmentation Based on Mumford-Shah Functional
- **Arxiv ID**: http://arxiv.org/abs/2007.08696v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NA, math.NA
- **Links**: [PDF](http://arxiv.org/pdf/2007.08696v1)
- **Published**: 2020-07-17 00:00:31+00:00
- **Updated**: 2020-07-17 00:00:31+00:00
- **Authors**: Karrar Abbas, Xianping Li
- **Comment**: 17 pages, 9 figures
- **Journal**: None
- **Summary**: As the resolution of digital images increase significantly, the processing of images becomes more challenging in terms of accuracy and efficiency. In this paper, we consider image segmentation by solving a partial differentiation equation (PDE) model based on the Mumford-Shah functional. We develop a new algorithm by combining anisotropic mesh adaptation for image representation and finite element method for solving the PDE model. Comparing to traditional algorithms solved by finite difference method, our algorithm provides faster and better results without the need to resizing the images to lower quality. We also extend the algorithm to segment images with multiple regions.



### DACS: Domain Adaptation via Cross-domain Mixed Sampling
- **Arxiv ID**: http://arxiv.org/abs/2007.08702v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.08702v2)
- **Published**: 2020-07-17 00:43:11+00:00
- **Updated**: 2020-11-29 11:13:40+00:00
- **Authors**: Wilhelm Tranheden, Viktor Olsson, Juliano Pinto, Lennart Svensson
- **Comment**: This paper has been accepted to WACV2021
- **Journal**: None
- **Summary**: Semantic segmentation models based on convolutional neural networks have recently displayed remarkable performance for a multitude of applications. However, these models typically do not generalize well when applied on new domains, especially when going from synthetic to real data. In this paper we address the problem of unsupervised domain adaptation (UDA), which attempts to train on labelled data from one domain (source domain), and simultaneously learn from unlabelled data in the domain of interest (target domain). Existing methods have seen success by training on pseudo-labels for these unlabelled images. Multiple techniques have been proposed to mitigate low-quality pseudo-labels arising from the domain shift, with varying degrees of success. We propose DACS: Domain Adaptation via Cross-domain mixed Sampling, which mixes images from the two domains along with the corresponding labels and pseudo-labels. These mixed samples are then trained on, in addition to the labelled data itself. We demonstrate the effectiveness of our solution by achieving state-of-the-art results for GTA5 to Cityscapes, a common synthetic-to-real semantic segmentation benchmark for UDA.



### Visualizing the Finer Cluster Structure of Large-Scale and High-Dimensional Data
- **Arxiv ID**: http://arxiv.org/abs/2007.08711v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.08711v1)
- **Published**: 2020-07-17 01:36:45+00:00
- **Updated**: 2020-07-17 01:36:45+00:00
- **Authors**: Yu Liang, Arin Chaudhuri, Haoyu Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Dimension reduction and visualization of high-dimensional data have become very important research topics because of the rapid growth of large databases in data science. In this paper, we propose using a generalized sigmoid function to model the distance similarity in both high- and low-dimensional spaces. In particular, the parameter b is introduced to the generalized sigmoid function in low-dimensional space, so that we can adjust the heaviness of the function tail by changing the value of b. Using both simulated and real-world data sets, we show that our proposed method can generate visualization results comparable to those of uniform manifold approximation and projection (UMAP), which is a newly developed manifold learning technique with fast running speed, better global structure, and scalability to massive data sets. In addition, according to the purpose of the study and the data structure, we can decrease or increase the value of b to either reveal the finer cluster structure of the data or maintain the neighborhood continuity of the embedding for better visualization. Finally, we use domain knowledge to demonstrate that the finer subclusters revealed with small values of b are meaningful.



### Transfer Learning without Knowing: Reprogramming Black-box Machine Learning Models with Scarce Data and Limited Resources
- **Arxiv ID**: http://arxiv.org/abs/2007.08714v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.08714v2)
- **Published**: 2020-07-17 01:52:34+00:00
- **Updated**: 2020-07-29 12:12:30+00:00
- **Authors**: Yun-Yun Tsai, Pin-Yu Chen, Tsung-Yi Ho
- **Comment**: None
- **Journal**: None
- **Summary**: Current transfer learning methods are mainly based on finetuning a pretrained model with target-domain data. Motivated by the techniques from adversarial machine learning (ML) that are capable of manipulating the model prediction via data perturbations, in this paper we propose a novel approach, black-box adversarial reprogramming (BAR), that repurposes a well-trained black-box ML model (e.g., a prediction API or a proprietary software) for solving different ML tasks, especially in the scenario with scarce data and constrained resources. The rationale lies in exploiting high-performance but unknown ML models to gain learning capability for transfer learning. Using zeroth order optimization and multi-label mapping techniques, BAR can reprogram a black-box ML model solely based on its input-output responses without knowing the model architecture or changing any parameter. More importantly, in the limited medical data setting, on autism spectrum disorder classification, diabetic retinopathy detection, and melanoma detection tasks, BAR outperforms state-of-the-art methods and yields comparable performance to the vanilla adversarial reprogramming method requiring complete knowledge of the target ML model. BAR also outperforms baseline transfer learning approaches by a significant margin, demonstrating cost-effective means and new insights for transfer learning.



### Understanding and Diagnosing Vulnerability under Adversarial Attacks
- **Arxiv ID**: http://arxiv.org/abs/2007.08716v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.08716v1)
- **Published**: 2020-07-17 01:56:28+00:00
- **Updated**: 2020-07-17 01:56:28+00:00
- **Authors**: Haizhong Zheng, Ziqi Zhang, Honglak Lee, Atul Prakash
- **Comment**: None
- **Journal**: None
- **Summary**: Deep Neural Networks (DNNs) are known to be vulnerable to adversarial attacks. Currently, there is no clear insight into how slight perturbations cause such a large difference in classification results and how we can design a more robust model architecture. In this work, we propose a novel interpretability method, InterpretGAN, to generate explanations for features used for classification in latent variables. Interpreting the classification process of adversarial examples exposes how adversarial perturbations influence features layer by layer as well as which features are modified by perturbations. Moreover, we design the first diagnostic method to quantify the vulnerability contributed by each layer, which can be used to identify vulnerable parts of model architectures. The diagnostic results show that the layers introducing more information loss tend to be more vulnerable than other layers. Based on the findings, our evaluation results on MNIST and CIFAR10 datasets suggest that average pooling layers, with lower information loss, are more robust than max pooling layers for the network architectures studied in this paper.



### A Technical Report for VIPriors Image Classification Challenge
- **Arxiv ID**: http://arxiv.org/abs/2007.08722v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.08722v1)
- **Published**: 2020-07-17 02:30:09+00:00
- **Updated**: 2020-07-17 02:30:09+00:00
- **Authors**: Zhipeng Luo, Ge Li, Zhiguang Zhang
- **Comment**: ECCV2020,VIPriors Image Classification Challenge
- **Journal**: None
- **Summary**: Image classification has always been a hot and challenging task. This paper is a brief report to our submission to the VIPriors Image Classification Challenge. In this challenge, the difficulty is how to train the model from scratch without any pretrained weight. In our method, several strong backbones and multiple loss functions are used to learn more representative features. To improve the models' generalization and robustness, efficient image augmentation strategies are utilized, like autoaugment and cutmix. Finally, ensemble learning is used to increase the performance of the models. The final Top-1 accuracy of our team DeepBlueAI is 0.7015, ranking second in the leaderboard.



### End-to-end Deep Prototype and Exemplar Models for Predicting Human Behavior
- **Arxiv ID**: http://arxiv.org/abs/2007.08723v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.08723v1)
- **Published**: 2020-07-17 02:32:17+00:00
- **Updated**: 2020-07-17 02:32:17+00:00
- **Authors**: Pulkit Singh, Joshua C. Peterson, Ruairidh M. Battleday, Thomas L. Griffiths
- **Comment**: 7 pages, 4 figures, 2 tables. Accepted as a paper to the 42nd Annual
  Meeting of the Cognitive Science Society (CogSci 2020)
- **Journal**: None
- **Summary**: Traditional models of category learning in psychology focus on representation at the category level as opposed to the stimulus level, even though the two are likely to interact. The stimulus representations employed in such models are either hand-designed by the experimenter, inferred circuitously from human judgments, or borrowed from pretrained deep neural networks that are themselves competing models of category learning. In this work, we extend classic prototype and exemplar models to learn both stimulus and category representations jointly from raw input. This new class of models can be parameterized by deep neural networks (DNN) and trained end-to-end. Following their namesakes, we refer to them as Deep Prototype Models, Deep Exemplar Models, and Deep Gaussian Mixture Models. Compared to typical DNNs, we find that their cognitively inspired counterparts both provide better intrinsic fit to human behavior and improve ground-truth classification.



### Detecting Human-Object Interactions with Action Co-occurrence Priors
- **Arxiv ID**: http://arxiv.org/abs/2007.08728v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.08728v2)
- **Published**: 2020-07-17 02:47:45+00:00
- **Updated**: 2020-07-27 05:42:32+00:00
- **Authors**: Dong-Jin Kim, Xiao Sun, Jinsoo Choi, Stephen Lin, In So Kweon
- **Comment**: ECCV 2020. Source code :
  https://github.com/Dong-JinKim/ActionCooccurrencePriors/
- **Journal**: None
- **Summary**: A common problem in human-object interaction (HOI) detection task is that numerous HOI classes have only a small number of labeled examples, resulting in training sets with a long-tailed distribution. The lack of positive labels can lead to low classification accuracy for these classes. Towards addressing this issue, we observe that there exist natural correlations and anti-correlations among human-object interactions. In this paper, we model the correlations as action co-occurrence matrices and present techniques to learn these priors and leverage them for more effective training, especially in rare classes. The utility of our approach is demonstrated experimentally, where the performance of our approach exceeds the state-of-the-art methods on both of the two leading HOI detection benchmark datasets, HICO-Det and V-COCO.



### Adaptive Task Sampling for Meta-Learning
- **Arxiv ID**: http://arxiv.org/abs/2007.08735v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.08735v1)
- **Published**: 2020-07-17 03:15:53+00:00
- **Updated**: 2020-07-17 03:15:53+00:00
- **Authors**: Chenghao Liu, Zhihao Wang, Doyen Sahoo, Yuan Fang, Kun Zhang, Steven C. H. Hoi
- **Comment**: ECCV2020
- **Journal**: None
- **Summary**: Meta-learning methods have been extensively studied and applied in computer vision, especially for few-shot classification tasks. The key idea of meta-learning for few-shot classification is to mimic the few-shot situations faced at test time by randomly sampling classes in meta-training data to construct few-shot tasks for episodic training. While a rich line of work focuses solely on how to extract meta-knowledge across tasks, we exploit the complementary problem on how to generate informative tasks. We argue that the randomly sampled tasks could be sub-optimal and uninformative (e.g., the task of classifying "dog" from "laptop" is often trivial) to the meta-learner. In this paper, we propose an adaptive task sampling method to improve the generalization performance. Unlike instance based sampling, task based sampling is much more challenging due to the implicit definition of the task in each episode. Therefore, we accordingly propose a greedy class-pair based sampling method, which selects difficult tasks according to class-pair potentials. We evaluate our adaptive task sampling method on two few-shot classification benchmarks, and it achieves consistent improvements across different feature backbones, meta-learning algorithms and datasets.



### Learning to Match Distributions for Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2007.10791v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.10791v3)
- **Published**: 2020-07-17 03:26:13+00:00
- **Updated**: 2020-07-27 01:44:38+00:00
- **Authors**: Chaohui Yu, Jindong Wang, Chang Liu, Tao Qin, Renjun Xu, Wenjie Feng, Yiqiang Chen, Tie-Yan Liu
- **Comment**: Preprint. 20 Pages. Code available at
  https://github.com/jindongwang/transferlearning/tree/master/code/deep/Learning-to-Match
- **Journal**: None
- **Summary**: When the training and test data are from different distributions, domain adaptation is needed to reduce dataset bias to improve the model's generalization ability. Since it is difficult to directly match the cross-domain joint distributions, existing methods tend to reduce the marginal or conditional distribution divergence using predefined distances such as MMD and adversarial-based discrepancies. However, it remains challenging to determine which method is suitable for a given application since they are built with certain priors or bias. Thus they may fail to uncover the underlying relationship between transferable features and joint distributions. This paper proposes Learning to Match (L2M) to automatically learn the cross-domain distribution matching without relying on hand-crafted priors on the matching loss. Instead, L2M reduces the inductive bias by using a meta-network to learn the distribution matching loss in a data-driven way. L2M is a general framework that unifies task-independent and human-designed matching features. We design a novel optimization algorithm for this challenging objective with self-supervised label propagation. Experiments on public datasets substantiate the superiority of L2M over SOTA methods. Moreover, we apply L2M to transfer from pneumonia to COVID-19 chest X-ray images with remarkable performance. L2M can also be extended in other distribution matching applications where we show in a trial experiment that L2M generates more realistic and sharper MNIST samples.



### Channel-wise Autoregressive Entropy Models for Learned Image Compression
- **Arxiv ID**: http://arxiv.org/abs/2007.08739v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.IT, cs.LG, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/2007.08739v1)
- **Published**: 2020-07-17 03:33:53+00:00
- **Updated**: 2020-07-17 03:33:53+00:00
- **Authors**: David Minnen, Saurabh Singh
- **Comment**: Published at the IEEE International Conference on Image Processing
  (ICIP) 2020
- **Journal**: None
- **Summary**: In learning-based approaches to image compression, codecs are developed by optimizing a computational model to minimize a rate-distortion objective. Currently, the most effective learned image codecs take the form of an entropy-constrained autoencoder with an entropy model that uses both forward and backward adaptation. Forward adaptation makes use of side information and can be efficiently integrated into a deep neural network. In contrast, backward adaptation typically makes predictions based on the causal context of each symbol, which requires serial processing that prevents efficient GPU / TPU utilization. We introduce two enhancements, channel-conditioning and latent residual prediction, that lead to network architectures with better rate-distortion performance than existing context-adaptive models while minimizing serial processing. Empirically, we see an average rate savings of 6.7% on the Kodak image set and 11.4% on the Tecnick image set compared to a context-adaptive baseline model. At low bit rates, where the improvements are most effective, our model saves up to 18% over the baseline and outperforms hand-engineered codecs like BPG by up to 25%.



### Leveraging both Lesion Features and Procedural Bias in Neuroimaging: An Dual-Task Split dynamics of inverse scale space
- **Arxiv ID**: http://arxiv.org/abs/2007.08740v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.08740v1)
- **Published**: 2020-07-17 03:41:48+00:00
- **Updated**: 2020-07-17 03:41:48+00:00
- **Authors**: Xinwei Sun, Wenjing Han, Lingjing Hu, Yuan Yao, Yizhou Wang
- **Comment**: Thanks to Xinwei's girlfriend Yue Cao, for her love and support
- **Journal**: None
- **Summary**: The prediction and selection of lesion features are two important tasks in voxel-based neuroimage analysis. Existing multivariate learning models take two tasks equivalently and optimize simultaneously. However, in addition to lesion features, we observe that there is another type of feature, which is commonly introduced during the procedure of preprocessing steps, which can improve the prediction result. We call such a type of feature as procedural bias. Therefore, in this paper, we propose that the features/voxels in neuroimage data are consist of three orthogonal parts: lesion features, procedural bias, and null features. To stably select lesion features and leverage procedural bias into prediction, we propose an iterative algorithm (termed GSplit LBI) as a discretization of differential inclusion of inverse scale space, which is the combination of Variable Splitting scheme and Linearized Bregman Iteration (LBI). Specifically, with a variable the splitting term, two estimators are introduced and split apart, i.e. one is for feature selection (the sparse estimator) and the other is for prediction (the dense estimator). Implemented with Linearized Bregman Iteration (LBI), the solution path of both estimators can be returned with different sparsity levels on the sparse estimator for the selection of lesion features. Besides, the dense the estimator can additionally leverage procedural bias to further improve prediction results. To test the efficacy of our method, we conduct experiments on the simulated study and Alzheimer's Disease Neuroimaging Initiative (ADNI) database. The validity and the benefit of our model can be shown by the improvement of prediction results and the interpretability of visualized procedural bias and lesion features.



### Contextualizing Enhances Gradient Based Meta Learning
- **Arxiv ID**: http://arxiv.org/abs/2007.10143v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.10143v1)
- **Published**: 2020-07-17 04:01:56+00:00
- **Updated**: 2020-07-17 04:01:56+00:00
- **Authors**: Evan Vogelbaum, Rumen Dangovski, Li Jing, Marin Soljačić
- **Comment**: None
- **Journal**: None
- **Summary**: Meta learning methods have found success when applied to few shot classification problems, in which they quickly adapt to a small number of labeled examples. Prototypical representations, each representing a particular class, have been of particular importance in this setting, as they provide a compact form to convey information learned from the labeled examples. However, these prototypes are just one method of representing this information, and they are narrow in their scope and ability to classify unseen examples. We propose the implementation of contextualizers, which are generalizable prototypes that adapt to given examples and play a larger role in classification for gradient-based models. We demonstrate how to equip meta learning methods with contextualizers and show that their use can significantly boost performance on a range of few shot learning datasets. We also present figures of merit demonstrating the potential benefits of contextualizers, along with analysis of how models make use of them. Our approach is particularly apt for low-data environments where it is difficult to update parameters without overfitting. Our implementation and instructions to reproduce the experiments are available at https://github.com/naveace/proto-context.



### Backdoor Learning: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2007.08745v5
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.08745v5)
- **Published**: 2020-07-17 04:09:20+00:00
- **Updated**: 2022-02-16 06:39:39+00:00
- **Authors**: Yiming Li, Yong Jiang, Zhifeng Li, Shu-Tao Xia
- **Comment**: 17 pages. A curated list of backdoor learning resources in this paper
  is presented in the Github Repo
  (https://github.com/THUYimingLi/backdoor-learning-resources). We will try our
  best to continuously maintain this Github Repo
- **Journal**: None
- **Summary**: Backdoor attack intends to embed hidden backdoor into deep neural networks (DNNs), so that the attacked models perform well on benign samples, whereas their predictions will be maliciously changed if the hidden backdoor is activated by attacker-specified triggers. This threat could happen when the training process is not fully controlled, such as training on third-party datasets or adopting third-party models, which poses a new and realistic threat. Although backdoor learning is an emerging and rapidly growing research area, its systematic review, however, remains blank. In this paper, we present the first comprehensive survey of this realm. We summarize and categorize existing backdoor attacks and defenses based on their characteristics, and provide a unified framework for analyzing poisoning-based backdoor attacks. Besides, we also analyze the relation between backdoor attacks and relevant fields ($i.e.,$ adversarial attacks and data poisoning), and summarize widely adopted benchmark datasets. Finally, we briefly outline certain future research directions relying upon reviewed works. A curated list of backdoor-related resources is also available at \url{https://github.com/THUYimingLi/backdoor-learning-resources}.



### Knowledge-Based Video Question Answering with Unsupervised Scene Descriptions
- **Arxiv ID**: http://arxiv.org/abs/2007.08751v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2007.08751v1)
- **Published**: 2020-07-17 04:26:38+00:00
- **Updated**: 2020-07-17 04:26:38+00:00
- **Authors**: Noa Garcia, Yuta Nakashima
- **Comment**: None
- **Journal**: None
- **Summary**: To understand movies, humans constantly reason over the dialogues and actions shown in specific scenes and relate them to the overall storyline already seen. Inspired by this behaviour, we design ROLL, a model for knowledge-based video story question answering that leverages three crucial aspects of movie understanding: dialog comprehension, scene reasoning, and storyline recalling. In ROLL, each of these tasks is in charge of extracting rich and diverse information by 1) processing scene dialogues, 2) generating unsupervised video scene descriptions, and 3) obtaining external knowledge in a weakly supervised fashion. To answer a given question correctly, the information generated by each inspired-cognitive task is encoded via Transformers and fused through a modality weighting mechanism, which balances the information from the different sources. Exhaustive evaluation demonstrates the effectiveness of our approach, which yields a new state-of-the-art on two challenging video question answering datasets: KnowIT VQA and TVQA+.



### Proactive Network Maintenance using Fast, Accurate Anomaly Localization and Classification on 1-D Data Series
- **Arxiv ID**: http://arxiv.org/abs/2007.08752v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2007.08752v1)
- **Published**: 2020-07-17 04:27:20+00:00
- **Updated**: 2020-07-17 04:27:20+00:00
- **Authors**: Jingjie Zhu, Karthik Sundaresan, Jason Rupe
- **Comment**: This paper has been accepted by ICPHM 2020
- **Journal**: None
- **Summary**: Proactive network maintenance (PNM) is the concept of using data from a network to identify and locate network faults, many or all of which could worsen to become service failures. The separation between the network fault and the service failure affords early detection of problems in the network to allow PNM to take place. Consequently, PNM is a form of prognostics and health management (PHM).   The problem of localizing and classifying anomalies on 1-dimensional data series has been under research for years. We introduce a new algorithm that leverages Deep Convolutional Neural Networks to efficiently and accurately detect anomalies and events on data series, and it reaches 97.82% mean average precision (mAP) in our evaluation.



### Sketching Image Gist: Human-Mimetic Hierarchical Scene Graph Generation
- **Arxiv ID**: http://arxiv.org/abs/2007.08760v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.08760v1)
- **Published**: 2020-07-17 05:12:13+00:00
- **Updated**: 2020-07-17 05:12:13+00:00
- **Authors**: Wenbin Wang, Ruiping Wang, Shiguang Shan, Xilin Chen
- **Comment**: Accepted by ECCV 2020
- **Journal**: None
- **Summary**: Scene graph aims to faithfully reveal humans' perception of image content. When humans analyze a scene, they usually prefer to describe image gist first, namely major objects and key relations in a scene graph. This humans' inherent perceptive habit implies that there exists a hierarchical structure about humans' preference during the scene parsing procedure. Therefore, we argue that a desirable scene graph should be also hierarchically constructed, and introduce a new scheme for modeling scene graph. Concretely, a scene is represented by a human-mimetic Hierarchical Entity Tree (HET) consisting of a series of image regions. To generate a scene graph based on HET, we parse HET with a Hybrid Long Short-Term Memory (Hybrid-LSTM) which specifically encodes hierarchy and siblings context to capture the structured information embedded in HET. To further prioritize key relations in the scene graph, we devise a Relation Ranking Module (RRM) to dynamically adjust their rankings by learning to capture humans' subjective perceptive habits from objective entity saliency and size. Experiments indicate that our method not only achieves state-of-the-art performances for scene graph generation, but also is expert in mining image-specific relations which play a great role in serving downstream tasks.



### AE-Net: Autonomous Evolution Image Fusion Method Inspired by Human Cognitive Mechanism
- **Arxiv ID**: http://arxiv.org/abs/2007.08763v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.08763v1)
- **Published**: 2020-07-17 05:19:51+00:00
- **Updated**: 2020-07-17 05:19:51+00:00
- **Authors**: Aiqing Fang, Xinbo Zhao, Jiaqi Yang, Shihao Cao, Yanning Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: In order to solve the robustness and generality problems of the image fusion task,inspired by the human brain cognitive mechanism, we propose a robust and general image fusion method with autonomous evolution ability, and is therefore denoted with AE-Net. Through the collaborative optimization of multiple image fusion methods to simulate the cognitive process of human brain, unsupervised learning image fusion task can be transformed into semi-supervised image fusion task or supervised image fusion task, thus promoting the evolutionary ability of network model weight. Firstly, the relationship between human brain cognitive mechanism and image fusion task is analyzed and a physical model is established to simulate human brain cognitive mechanism. Secondly, we analyze existing image fusion methods and image fusion loss functions, select the image fusion method with complementary features to construct the algorithm module, establish the multi-loss joint evaluation function to obtain the optimal solution of algorithm module. The optimal solution of each image is used to guide the weight training of network model. Our image fusion method can effectively unify the cross-modal image fusion task and the same modal image fusion task, and effectively overcome the difference of data distribution between different datasets. Finally, extensive numerical results verify the effectiveness and superiority of our method on a variety of image fusion datasets, including multi-focus dataset, infrared and visi-ble dataset, medical image dataset and multi-exposure dataset. Comprehensive experiments demonstrate the superiority of our image fusion method in robustness and generality. In addition, experimental results also demonstate the effectiveness of human brain cognitive mechanism to improve the robustness and generality of image fusion.



### Spatial-Spectral Manifold Embedding of Hyperspectral Data
- **Arxiv ID**: http://arxiv.org/abs/2007.08767v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.08767v1)
- **Published**: 2020-07-17 05:40:27+00:00
- **Updated**: 2020-07-17 05:40:27+00:00
- **Authors**: Danfeng Hong, Jing Yao, Xin Wu, Jocelyn Chanussot, Xiao Xiang Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, hyperspectral imaging, also known as imaging spectroscopy, has been paid an increasing interest in geoscience and remote sensing community. Hyperspectral imagery is characterized by very rich spectral information, which enables us to recognize the materials of interest lying on the surface of the Earth more easier. We have to admit, however, that high spectral dimension inevitably brings some drawbacks, such as expensive data storage and transmission, information redundancy, etc. Therefore, to reduce the spectral dimensionality effectively and learn more discriminative spectral low-dimensional embedding, in this paper we propose a novel hyperspectral embedding approach by simultaneously considering spatial and spectral information, called spatial-spectral manifold embedding (SSME). Beyond the pixel-wise spectral embedding approaches, SSME models the spatial and spectral information jointly in a patch-based fashion. SSME not only learns the spectral embedding by using the adjacency matrix obtained by similarity measurement between spectral signatures, but also models the spatial neighbours of a target pixel in hyperspectral scene by sharing the same weights (or edges) in the process of learning embedding. Classification is explored as a potential strategy to quantitatively evaluate the performance of learned embedding representations. Classification is explored as a potential application for quantitatively evaluating the performance of these hyperspectral embedding algorithms. Extensive experiments conducted on the widely-used hyperspectral datasets demonstrate the superiority and effectiveness of the proposed SSME as compared to several state-of-the-art embedding methods.



### CASNet: Common Attribute Support Network for image instance and panoptic segmentation
- **Arxiv ID**: http://arxiv.org/abs/2008.00810v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.00810v1)
- **Published**: 2020-07-17 06:23:52+00:00
- **Updated**: 2020-07-17 06:23:52+00:00
- **Authors**: Xiaolong Liu, Yuqing Hou, Anbang Yao, Yurong Chen, Keqiang Li
- **Comment**: None
- **Journal**: None
- **Summary**: Instance segmentation and panoptic segmentation is being paid more and more attention in recent years. In comparison with bounding box based object detection and semantic segmentation, instance segmentation can provide more analytical results at pixel level. Given the insight that pixels belonging to one instance have one or more common attributes of current instance, we bring up an one-stage instance segmentation network named Common Attribute Support Network (CASNet), which realizes instance segmentation by predicting and clustering common attributes. CASNet is designed in the manner of fully convolutional and can implement training and inference from end to end. And CASNet manages predicting the instance without overlaps and holes, which problem exists in most of current instance segmentation algorithms. Furthermore, it can be easily extended to panoptic segmentation through minor modifications with little computation overhead. CASNet builds a bridge between semantic and instance segmentation from finding pixel class ID to obtaining class and instance ID by operations on common attribute. Through experiment for instance and panoptic segmentation, CASNet gets mAP 32.8% and PQ 59.0% on Cityscapes validation dataset by joint training, and mAP 36.3% and PQ 66.1% by separated training mode. For panoptic segmentation, CASNet gets state-of-the-art performance on the Cityscapes validation dataset.



### Progressive Multi-stage Feature Mix for Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2007.08779v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.08779v2)
- **Published**: 2020-07-17 06:59:39+00:00
- **Updated**: 2020-10-26 06:39:04+00:00
- **Authors**: Yan Zhang, Binyu He, Li Sun
- **Comment**: None
- **Journal**: None
- **Summary**: Image features from a small local region often give strong evidence in person re-identification task. However, CNN suffers from paying too much attention on the most salient local areas, thus ignoring other discriminative clues, e.g., hair, shoes or logos on clothes. %BDB proposes to randomly drop one block in a batch to enlarge the high response areas. Although BDB has achieved remarkable results, there still room for improvement. In this work, we propose a Progressive Multi-stage feature Mix network (PMM), which enables the model to find out the more precise and diverse features in a progressive manner. Specifically, 1. to enforce the model to look for different clues in the image, we adopt a multi-stage classifier and expect that the model is able to focus on a complementary region in each stage. 2. we propose an Attentive feature Hard-Mix (A-Hard-Mix) to replace the salient feature blocks by the negative example in the current batch, whose label is different from the current sample. 3. extensive experiments have been carried out on reID datasets such as the Market-1501, DukeMTMC-reID and CUHK03, showing that the proposed method can boost the re-identification performance significantly.



### Mixing Real and Synthetic Data to Enhance Neural Network Training -- A Review of Current Approaches
- **Arxiv ID**: http://arxiv.org/abs/2007.08781v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.08781v1)
- **Published**: 2020-07-17 07:12:31+00:00
- **Updated**: 2020-07-17 07:12:31+00:00
- **Authors**: Viktor Seib, Benjamin Lange, Stefan Wirtz
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks have gained tremendous importance in many computer vision tasks. However, their power comes at the cost of large amounts of annotated data required for supervised training. In this work we review and compare different techniques available in the literature to improve training results without acquiring additional annotated real-world data. This goal is mostly achieved by applying annotation-preserving transformations to existing data or by synthetically creating more data.



### Deep Learning Based Traffic Surveillance System For Missing and Suspicious Car Detection
- **Arxiv ID**: http://arxiv.org/abs/2007.08783v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.08783v1)
- **Published**: 2020-07-17 07:18:12+00:00
- **Updated**: 2020-07-17 07:18:12+00:00
- **Authors**: K. V. Kadambari, Vishnu Vardhan Nimmalapudi
- **Comment**: None
- **Journal**: None
- **Summary**: Vehicle theft is arguably one of the fastest-growing types of crime in India. In some of the urban areas, vehicle theft cases are believed to be around 100 each day. Identification of stolen vehicles in such precarious scenarios is not possible using traditional methods like manual checking and radio frequency identification(RFID) based technologies. This paper presents a deep learning based automatic traffic surveillance system for the detection of stolen/suspicious cars from the closed circuit television(CCTV) camera footage. It mainly comprises of four parts: Select-Detector, Image Quality Enhancer, Image Transformer, and Smart Recognizer. The Select-Detector is used for extracting the frames containing vehicles and to detect the license plates much efficiently with minimum time complexity. The quality of the license plates is then enhanced using Image Quality Enhancer which uses pix2pix generative adversarial network(GAN) for enhancing the license plates that are affected by temporal changes like low light, shadow, etc. Image Transformer is used to tackle the problem of inefficient recognition of license plates which are not horizontal(which are at an angle) by transforming the license plate to different levels of rotation and cropping. Smart Recognizer recognizes the license plate number using Tesseract optical character recognition(OCR) and corrects the wrongly recognized characters using Error-Detector. The effectiveness of the proposed approach is tested on the government's CCTV camera footage, which resulted in identifying the stolen/suspicious cars with an accuracy of 87%.



### Learning Posterior and Prior for Uncertainty Modeling in Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2007.08785v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.08785v1)
- **Published**: 2020-07-17 07:20:39+00:00
- **Updated**: 2020-07-17 07:20:39+00:00
- **Authors**: Yan Zhang, Zhilin Zheng, Binyu He, Li Sun
- **Comment**: None
- **Journal**: None
- **Summary**: Data uncertainty in practical person reID is ubiquitous, hence it requires not only learning the discriminative features, but also modeling the uncertainty based on the input. This paper proposes to learn the sample posterior and the class prior distribution in the latent space, so that not only representative features but also the uncertainty can be built by the model. The prior reflects the distribution of all data in the same class, and it is the trainable model parameters. While the posterior is the probability density of a single sample, so it is actually the feature defined on the input. We assume that both of them are in Gaussian form. To simultaneously model them, we put forward a distribution loss, which measures the KL divergence from the posterior to the priors in the manner of supervised learning. In addition, we assume that the posterior variance, which is essentially the uncertainty, is supposed to have the second-order characteristic. Therefore, a $\Sigma-$net is proposed to compute it by the high order representation from its input. Extensive experiments have been carried out on Market1501, DukeMTMC, MARS and noisy dataset as well.



### Cross-Identity Motion Transfer for Arbitrary Objects through Pose-Attentive Video Reassembling
- **Arxiv ID**: http://arxiv.org/abs/2007.08786v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.08786v1)
- **Published**: 2020-07-17 07:21:12+00:00
- **Updated**: 2020-07-17 07:21:12+00:00
- **Authors**: Subin Jeon, Seonghyeon Nam, Seoung Wug Oh, Seon Joo Kim
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: We propose an attention-based networks for transferring motions between arbitrary objects. Given a source image(s) and a driving video, our networks animate the subject in the source images according to the motion in the driving video. In our attention mechanism, dense similarities between the learned keypoints in the source and the driving images are computed in order to retrieve the appearance information from the source images. Taking a different approach from the well-studied warping based models, our attention-based model has several advantages. By reassembling non-locally searched pieces from the source contents, our approach can produce more realistic outputs. Furthermore, our system can make use of multiple observations of the source appearance (e.g. front and sides of faces) to make the results more accurate. To reduce the training-testing discrepancy of the self-supervised learning, a novel cross-identity training scheme is additionally introduced. With the training scheme, our networks is trained to transfer motions between different subjects, as in the real testing scenario. Experimental results validate that our method produces visually pleasing results in various object domains, showing better performances compared to previous works.



### An ensemble classifier for vibration-based quality monitoring
- **Arxiv ID**: http://arxiv.org/abs/2007.08789v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.08789v2)
- **Published**: 2020-07-17 07:23:56+00:00
- **Updated**: 2020-11-20 20:01:51+00:00
- **Authors**: Vahid Yaghoubi, Liangliang Cheng, Wim Van Paepegem, Mathias Kersemans
- **Comment**: None
- **Journal**: None
- **Summary**: Vibration-based quality monitoring of manufactured components often employs pattern recognition methods. Albeit developing several classification methods, they usually provide high accuracy for specific types of datasets, but not for general cases. In this paper, this issue has been addressed by developing a novel ensemble classifier based on the Dempster-Shafer theory of evidence. To deal with conflicting evidences, three remedies are proposed prior to combination: (i) selection of proper classifiers by evaluating the relevancy between the predicted and target outputs, (ii) devising an optimization method to minimize the distance between the predicted and target outputs, (iii) utilizing five different weighting factors, including a new one, to enhance the fusion performance. The effectiveness of the proposed framework is validated by its application to 15 UCI and KEEL machine learning datasets. It is then applied to two vibration-based datasets to detect defected samples: one synthetic dataset generated from the finite element model of a dogbone cylinder, and one real experimental dataset generated by collecting broadband vibrational response of polycrystalline Nickel alloy first-stage turbine blades. The investigation is made through statistical analysis in presence of different levels of noise-to-signal ratio. Comparing the results with those of four state-of-the-art fusion techniques reveals the good performance of the proposed ensemble method.



### Explanation-Guided Training for Cross-Domain Few-Shot Classification
- **Arxiv ID**: http://arxiv.org/abs/2007.08790v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.08790v2)
- **Published**: 2020-07-17 07:28:08+00:00
- **Updated**: 2020-12-09 09:53:24+00:00
- **Authors**: Jiamei Sun, Sebastian Lapuschkin, Wojciech Samek, Yunqing Zhao, Ngai-Man Cheung, Alexander Binder
- **Comment**: None
- **Journal**: Proceedings of the 25th International Conference on Pattern
  Recognition 2021
- **Summary**: Cross-domain few-shot classification task (CD-FSC) combines few-shot classification with the requirement to generalize across domains represented by datasets. This setup faces challenges originating from the limited labeled data in each class and, additionally, from the domain shift between training and test sets. In this paper, we introduce a novel training approach for existing FSC models. It leverages on the explanation scores, obtained from existing explanation methods when applied to the predictions of FSC models, computed for intermediate feature maps of the models. Firstly, we tailor the layer-wise relevance propagation (LRP) method to explain the predictions of FSC models. Secondly, we develop a model-agnostic explanation-guided training strategy that dynamically finds and emphasizes the features which are important for the predictions. Our contribution does not target a novel explanation method but lies in a novel application of explanations for the training phase. We show that explanation-guided training effectively improves the model generalization. We observe improved accuracy for three different FSC models: RelationNet, cross attention network, and a graph neural network-based formulation, on five few-shot learning datasets: miniImagenet, CUB, Cars, Places, and Plantae. The source code is available at https://github.com/SunJiamei/few-shot-lrp-guided



### Computing stable resultant-based minimal solvers by hiding a variable
- **Arxiv ID**: http://arxiv.org/abs/2007.10100v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.SC, I.4; I.1
- **Links**: [PDF](http://arxiv.org/pdf/2007.10100v1)
- **Published**: 2020-07-17 07:40:10+00:00
- **Updated**: 2020-07-17 07:40:10+00:00
- **Authors**: Snehal Bhayani, Zuzana Kukelova, Janne Heikkilä
- **Comment**: arXiv admin note: text overlap with arXiv:1912.10268
- **Journal**: None
- **Summary**: Many computer vision applications require robust and efficient estimation of camera geometry. The robust estimation is usually based on solving camera geometry problems from a minimal number of input data measurements, i.e., solving minimal problems, in a RANSAC-style framework. Minimal problems often result in complex systems of polynomial equations. The existing state-of-the-art methods for solving such systems are either based on Gr\"obner bases and the action matrix method, which have been extensively studied and optimized in the recent years or recently proposed approach based on a sparse resultant computation using an extra variable.   In this paper, we study an interesting alternative sparse resultant-based method for solving sparse systems of polynomial equations by hiding one variable. This approach results in a larger eigenvalue problem than the action matrix and extra variable sparse resultant-based methods; however, it does not need to compute an inverse or elimination of large matrices that may be numerically unstable. The proposed approach includes several improvements to the standard sparse resultant algorithms, which significantly improves the efficiency and stability of the hidden variable resultant-based solvers as we demonstrate on several interesting computer vision problems. We show that for the studied problems, our sparse resultant based approach leads to more stable solvers than the state-of-the-art Gr\"obner bases-based solvers as well as existing sparse resultant-based solvers, especially in close to critical configurations. Our new method can be fully automated and incorporated into existing tools for the automatic generation of efficient minimal solvers.



### Learning to Combine: Knowledge Aggregation for Multi-Source Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2007.08801v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.08801v3)
- **Published**: 2020-07-17 07:52:44+00:00
- **Updated**: 2020-07-28 15:12:38+00:00
- **Authors**: Hang Wang, Minghao Xu, Bingbing Ni, Wenjun Zhang
- **Comment**: Accepted by ECCV 2020. Code is available at
  \url{https://github.com/ChrisAllenMing/LtC-MSDA}
- **Journal**: None
- **Summary**: Transferring knowledges learned from multiple source domains to target domain is a more practical and challenging task than conventional single-source domain adaptation. Furthermore, the increase of modalities brings more difficulty in aligning feature distributions among multiple domains. To mitigate these problems, we propose a Learning to Combine for Multi-Source Domain Adaptation (LtC-MSDA) framework via exploring interactions among domains. In the nutshell, a knowledge graph is constructed on the prototypes of various domains to realize the information propagation among semantically adjacent representations. On such basis, a graph model is learned to predict query samples under the guidance of correlated prototypes. In addition, we design a Relation Alignment Loss (RAL) to facilitate the consistency of categories' relational interdependency and the compactness of features, which boosts features' intra-class invariance and inter-class separability. Comprehensive results on public benchmark datasets demonstrate that our approach outperforms existing methods with a remarkable margin. Our code is available at \url{https://github.com/ChrisAllenMing/LtC-MSDA}



### Learn to Propagate Reliably on Noisy Affinity Graphs
- **Arxiv ID**: http://arxiv.org/abs/2007.08802v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.08802v1)
- **Published**: 2020-07-17 07:55:59+00:00
- **Updated**: 2020-07-17 07:55:59+00:00
- **Authors**: Lei Yang, Qingqiu Huang, Huaiyi Huang, Linning Xu, Dahua Lin
- **Comment**: 14 pages, 7 figures, ECCV 2020
- **Journal**: None
- **Summary**: Recent works have shown that exploiting unlabeled data through label propagation can substantially reduce the labeling cost, which has been a critical issue in developing visual recognition models. Yet, how to propagate labels reliably, especially on a dataset with unknown outliers, remains an open question. Conventional methods such as linear diffusion lack the capability of handling complex graph structures and may perform poorly when the seeds are sparse. Latest methods based on graph neural networks would face difficulties on performance drop as they scale out to noisy graphs. To overcome these difficulties, we propose a new framework that allows labels to be propagated reliably on large-scale real-world data. This framework incorporates (1) a local graph neural network to predict accurately on varying local structures while maintaining high scalability, and (2) a confidence-based path scheduler that identifies outliers and moves forward the propagation frontier in a prudent way. Experiments on both ImageNet and Ms-Celeb-1M show that our confidence guided framework can significantly improve the overall accuracies of the propagated labels, especially when the graph is very noisy.



### SumGraph: Video Summarization via Recursive Graph Modeling
- **Arxiv ID**: http://arxiv.org/abs/2007.08809v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.08809v1)
- **Published**: 2020-07-17 08:11:30+00:00
- **Updated**: 2020-07-17 08:11:30+00:00
- **Authors**: Jungin Park, Jiyoung Lee, Ig-Jae Kim, Kwanghoon Sohn
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: The goal of video summarization is to select keyframes that are visually diverse and can represent a whole story of an input video. State-of-the-art approaches for video summarization have mostly regarded the task as a frame-wise keyframe selection problem by aggregating all frames with equal weight. However, to find informative parts of the video, it is necessary to consider how all the frames of the video are related to each other. To this end, we cast video summarization as a graph modeling problem. We propose recursive graph modeling networks for video summarization, termed SumGraph, to represent a relation graph, where frames are regarded as nodes and nodes are connected by semantic relationships among frames. Our networks accomplish this through a recursive approach to refine an initially estimated graph to correctly classify each node as a keyframe by reasoning the graph representation via graph convolutional networks. To leverage SumGraph in a more practical environment, we also present a way to adapt our graph modeling in an unsupervised fashion. With SumGraph, we achieved state-of-the-art performance on several benchmarks for video summarization in both supervised and unsupervised manners.



### Visual Relation Grounding in Videos
- **Arxiv ID**: http://arxiv.org/abs/2007.08814v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.08814v2)
- **Published**: 2020-07-17 08:20:39+00:00
- **Updated**: 2020-07-21 07:20:32+00:00
- **Authors**: Junbin Xiao, Xindi Shang, Xun Yang, Sheng Tang, Tat-Seng Chua
- **Comment**: ECCV2020 (spotlight)
- **Journal**: None
- **Summary**: In this paper, we explore a novel task named visual Relation Grounding in Videos (vRGV). The task aims at spatio-temporally localizing the given relations in the form of subject-predicate-object in the videos, so as to provide supportive visual facts for other high-level video-language tasks (e.g., video-language grounding and video question answering). The challenges in this task include but not limited to: (1) both the subject and object are required to be spatio-temporally localized to ground a query relation; (2) the temporal dynamic nature of visual relations in videos is difficult to capture; and (3) the grounding should be achieved without any direct supervision in space and time. To ground the relations, we tackle the challenges by collaboratively optimizing two sequences of regions over a constructed hierarchical spatio-temporal region graph through relation attending and reconstruction, in which we further propose a message passing mechanism by spatial attention shifting between visual entities. Experimental results demonstrate that our model can not only outperform baseline approaches significantly, but also produces visually meaningful facts to support visual grounding. (Code is available at https://github.com/doc-doc/vRGV).



### Revisiting Rubik's Cube: Self-supervised Learning with Volume-wise Transformation for 3D Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2007.08826v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.08826v1)
- **Published**: 2020-07-17 08:53:53+00:00
- **Updated**: 2020-07-17 08:53:53+00:00
- **Authors**: Xing Tao, Yuexiang Li, Wenhui Zhou, Kai Ma, Yefeng Zheng
- **Comment**: Accepted by MICCAI 2020
- **Journal**: None
- **Summary**: Deep learning highly relies on the quantity of annotated data. However, the annotations for 3D volumetric medical data require experienced physicians to spend hours or even days for investigation. Self-supervised learning is a potential solution to get rid of the strong requirement of training data by deeply exploiting raw data information. In this paper, we propose a novel self-supervised learning framework for volumetric medical images. Specifically, we propose a context restoration task, i.e., Rubik's cube++, to pre-train 3D neural networks. Different from the existing context-restoration-based approaches, we adopt a volume-wise transformation for context permutation, which encourages network to better exploit the inherent 3D anatomical information of organs. Compared to the strategy of training from scratch, fine-tuning from the Rubik's cube++ pre-trained weight can achieve better performance in various tasks such as pancreas segmentation and brain tissue segmentation. The experimental results show that our self-supervised learning method can significantly improve the accuracy of 3D deep learning networks on volumetric medical datasets without the use of extra data.



### Polarimetric Multi-View Inverse Rendering
- **Arxiv ID**: http://arxiv.org/abs/2007.08830v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.08830v1)
- **Published**: 2020-07-17 09:00:20+00:00
- **Updated**: 2020-07-17 09:00:20+00:00
- **Authors**: Jinyu Zhao, Yusuke Monno, Masatoshi Okutomi
- **Comment**: Paper accepted in ECCV 2020
- **Journal**: None
- **Summary**: A polarization camera has great potential for 3D reconstruction since the angle of polarization (AoP) of reflected light is related to an object's surface normal. In this paper, we propose a novel 3D reconstruction method called Polarimetric Multi-View Inverse Rendering (Polarimetric MVIR) that effectively exploits geometric, photometric, and polarimetric cues extracted from input multi-view color polarization images. We first estimate camera poses and an initial 3D model by geometric reconstruction with a standard structure-from-motion and multi-view stereo pipeline. We then refine the initial model by optimizing photometric and polarimetric rendering errors using multi-view RGB and AoP images, where we propose a novel polarimetric rendering cost function that enables us to effectively constrain each estimated surface vertex's normal while considering four possible ambiguous azimuth angles revealed from the AoP measurement. Experimental results using both synthetic and real data demonstrate that our Polarimetric MVIR can reconstruct a detailed 3D shape without assuming a specific polarized reflection depending on the material.



### BMBC:Bilateral Motion Estimation with Bilateral Cost Volume for Video Interpolation
- **Arxiv ID**: http://arxiv.org/abs/2007.12622v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.12622v1)
- **Published**: 2020-07-17 09:07:51+00:00
- **Updated**: 2020-07-17 09:07:51+00:00
- **Authors**: Junheum Park, Keunsoo Ko, Chul Lee, Chang-Su Kim
- **Comment**: Accepted to ECCV 2020
- **Journal**: None
- **Summary**: Video interpolation increases the temporal resolution of a video sequence by synthesizing intermediate frames between two consecutive frames. We propose a novel deep-learning-based video interpolation algorithm based on bilateral motion estimation. First, we develop the bilateral motion network with the bilateral cost volume to estimate bilateral motions accurately. Then, we approximate bi-directional motions to predict a different kind of bilateral motions. We then warp the two input frames using the estimated bilateral motions. Next, we develop the dynamic filter generation network to yield dynamic blending filters. Finally, we combine the warped frames using the dynamic blending filters to generate intermediate frames. Experimental results show that the proposed algorithm outperforms the state-of-the-art video interpolation algorithms on several benchmark datasets.



### The Effect of Top-Down Attention in Occluded Object Recognition
- **Arxiv ID**: http://arxiv.org/abs/2007.10232v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.10232v1)
- **Published**: 2020-07-17 09:09:41+00:00
- **Updated**: 2020-07-17 09:09:41+00:00
- **Authors**: Zahra Sadeghi
- **Comment**: 4 pages, 2 figures
- **Journal**: None
- **Summary**: This study is concerned with the top-down visual processing benefit in the task of occluded object recognition. To this end, a psychophysical experiment is designed and carried out which aimed at investigating the effect of consistency of contextual information on the recognition of objects which are partially occluded. The results demonstrate the facilitative impact of consistent contextual clues on the task of object recognition in presence of occlusion.



### Two-stream Fusion Model for Dynamic Hand Gesture Recognition using 3D-CNN and 2D-CNN Optical Flow guided Motion Template
- **Arxiv ID**: http://arxiv.org/abs/2007.08847v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.08847v1)
- **Published**: 2020-07-17 09:20:20+00:00
- **Updated**: 2020-07-17 09:20:20+00:00
- **Authors**: Debajit Sarma, V. Kavyasree, M. K. Bhuyan
- **Comment**: 7 pages, 6 figures, 2 tables. Keywords: Action and gesture
  recognition, Two-stream fusion model, Optical flow guided motion template
  (OFMT), 2D and 3D-CNN
- **Journal**: None
- **Summary**: The use of hand gestures can be a useful tool for many applications in the human-computer interaction community. In a broad range of areas hand gesture techniques can be applied specifically in sign language recognition, robotic surgery, etc. In the process of hand gesture recognition, proper detection, and tracking of the moving hand become challenging due to the varied shape and size of the hand. Here the objective is to track the movement of the hand irrespective of the shape, size, and color of the hand. And, for this, a motion template guided by optical flow (OFMT) is proposed. OFMT is a compact representation of the motion information of a gesture encoded into a single image. In the experimentation, different datasets using bare hand with an open palm, and folded palm wearing green-glove are used, and in both cases, we could generate the OFMT images with equal precision. Recently, deep network-based techniques have shown impressive improvements as compared to conventional hand-crafted feature-based techniques. Moreover, in the literature, it is seen that the use of different streams with informative input data helps to increase the performance in the recognition accuracy. This work basically proposes a two-stream fusion model for hand gesture recognition and a compact yet efficient motion template based on optical flow. Specifically, the two-stream network consists of two layers: a 3D convolutional neural network (C3D) that takes gesture videos as input and a 2D-CNN that takes OFMT images as input. C3D has shown its efficiency in capturing spatio-temporal information of a video. Whereas OFMT helps to eliminate irrelevant gestures providing additional motion information. Though each stream can work independently, they are combined with a fusion scheme to boost the recognition results. We have shown the efficiency of the proposed two-stream network on two databases.



### 2nd Place Solution to ECCV 2020 VIPriors Object Detection Challenge
- **Arxiv ID**: http://arxiv.org/abs/2007.08849v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.08849v1)
- **Published**: 2020-07-17 09:21:29+00:00
- **Updated**: 2020-07-17 09:21:29+00:00
- **Authors**: Yinzheng Gu, Yihan Pan, Shizhe Chen
- **Comment**: Technical report for the ECCV 2020 VIPriors Object Detection
  Challenge
- **Journal**: None
- **Summary**: In this report, we descibe our approach to the ECCV 2020 VIPriors Object Detection Challenge which took place from March to July in 2020. We show that by using state-of-the-art data augmentation strategies, model designs, and post-processing ensemble methods, it is possible to overcome the difficulty of data shortage and obtain competitive results. Notably, our overall detection system achieves 36.6$\%$ AP on the COCO 2017 validation set using only 10K training images without any pre-training or transfer learning weights ranking us 2nd place in the challenge.



### DVI: Depth Guided Video Inpainting for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2007.08854v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.08854v1)
- **Published**: 2020-07-17 09:29:53+00:00
- **Updated**: 2020-07-17 09:29:53+00:00
- **Authors**: Miao Liao, Feixiang Lu, Dingfu Zhou, Sibo Zhang, Wei Li, Ruigang Yang
- **Comment**: None
- **Journal**: None
- **Summary**: To get clear street-view and photo-realistic simulation in autonomous driving, we present an automatic video inpainting algorithm that can remove traffic agents from videos and synthesize missing regions with the guidance of depth/point cloud. By building a dense 3D map from stitched point clouds, frames within a video are geometrically correlated via this common 3D map. In order to fill a target inpainting area in a frame, it is straightforward to transform pixels from other frames into the current one with correct occlusion. Furthermore, we are able to fuse multiple videos through 3D point cloud registration, making it possible to inpaint a target video with multiple source videos. The motivation is to solve the long-time occlusion problem where an occluded area has never been visible in the entire video. To our knowledge, we are the first to fuse multiple videos for video inpainting. To verify the effectiveness of our approach, we build a large inpainting dataset in the real urban road environment with synchronized images and Lidar data including many challenge scenes, e.g., long time occlusion. The experimental results show that the proposed approach outperforms the state-of-the-art approaches for all the criteria, especially the RMSE (Root Mean Squared Error) has been reduced by about 13%.



### EPNet: Enhancing Point Features with Image Semantics for 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2007.08856v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.08856v1)
- **Published**: 2020-07-17 09:33:05+00:00
- **Updated**: 2020-07-17 09:33:05+00:00
- **Authors**: Tengteng Huang, Zhe Liu, Xiwu Chen, Xiang Bai
- **Comment**: Accepted by ECCV2020
- **Journal**: None
- **Summary**: In this paper, we aim at addressing two critical issues in the 3D detection task, including the exploitation of multiple sensors~(namely LiDAR point cloud and camera image), as well as the inconsistency between the localization and classification confidence. To this end, we propose a novel fusion module to enhance the point features with semantic image features in a point-wise manner without any image annotations. Besides, a consistency enforcing loss is employed to explicitly encourage the consistency of both the localization and classification confidence. We design an end-to-end learnable framework named EPNet to integrate these two components. Extensive experiments on the KITTI and SUN-RGBD datasets demonstrate the superiority of EPNet over the state-of-the-art methods. Codes and models are available at: \url{https://github.com/happinesslz/EPNet}.



### Impact of base dataset design on few-shot image classification
- **Arxiv ID**: http://arxiv.org/abs/2007.08872v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.08872v1)
- **Published**: 2020-07-17 09:58:50+00:00
- **Updated**: 2020-07-17 09:58:50+00:00
- **Authors**: Othman Sbai, Camille Couprie, Mathieu Aubry
- **Comment**: 23 pages, 11 figures, to appear in ECCV 2020
- **Journal**: None
- **Summary**: The quality and generality of deep image features is crucially determined by the data they have been trained on, but little is known about this often overlooked effect. In this paper, we systematically study the effect of variations in the training data by evaluating deep features trained on different image sets in a few-shot classification setting. The experimental protocol we define allows to explore key practical questions. What is the influence of the similarity between base and test classes? Given a fixed annotation budget, what is the optimal trade-off between the number of images per class and the number of classes? Given a fixed dataset, can features be improved by splitting or combining different classes? Should simple or diverse classes be annotated? In a wide range of experiments, we provide clear answers to these questions on the miniImageNet, ImageNet and CUB-200 benchmarks. We also show how the base dataset design can improve performance in few-shot classification more drastically than replacing a simple baseline by an advanced state of the art algorithm.



### Consensus-Aware Visual-Semantic Embedding for Image-Text Matching
- **Arxiv ID**: http://arxiv.org/abs/2007.08883v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.08883v2)
- **Published**: 2020-07-17 10:22:57+00:00
- **Updated**: 2021-02-01 12:35:10+00:00
- **Authors**: Haoran Wang, Ying Zhang, Zhong Ji, Yanwei Pang, Lin Ma
- **Comment**: Accepted by ECCV 2020, Code is publicly available at:
  https://github.com/BruceW91/CVSE
- **Journal**: None
- **Summary**: Image-text matching plays a central role in bridging vision and language. Most existing approaches only rely on the image-text instance pair to learn their representations, thereby exploiting their matching relationships and making the corresponding alignments. Such approaches only exploit the superficial associations contained in the instance pairwise data, with no consideration of any external commonsense knowledge, which may hinder their capabilities to reason the higher-level relationships between image and text. In this paper, we propose a Consensus-aware Visual-Semantic Embedding (CVSE) model to incorporate the consensus information, namely the commonsense knowledge shared between both modalities, into image-text matching. Specifically, the consensus information is exploited by computing the statistical co-occurrence correlations between the semantic concepts from the image captioning corpus and deploying the constructed concept correlation graph to yield the consensus-aware concept (CAC) representations. Afterwards, CVSE learns the associations and alignments between image and text based on the exploited consensus as well as the instance-level representations for both modalities. Extensive experiments conducted on two public datasets verify that the exploited consensus makes significant contributions to constructing more meaningful visual-semantic embeddings, with the superior performances over the state-of-the-art approaches on the bidirectional image and text retrieval task. Our code of this paper is available at: https://github.com/BruceW91/CVSE.



### Superpixel-Guided Label Softening for Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2007.08897v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.08897v1)
- **Published**: 2020-07-17 10:55:59+00:00
- **Updated**: 2020-07-17 10:55:59+00:00
- **Authors**: Hang Li, Dong Wei, Shilei Cao, Kai Ma, Liansheng Wang, Yefeng Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: Segmentation of objects of interest is one of the central tasks in medical image analysis, which is indispensable for quantitative analysis. When developing machine-learning based methods for automated segmentation, manual annotations are usually used as the ground truth toward which the models learn to mimic. While the bulky parts of the segmentation targets are relatively easy to label, the peripheral areas are often difficult to handle due to ambiguous boundaries and the partial volume effect, etc., and are likely to be labeled with uncertainty. This uncertainty in labeling may, in turn, result in unsatisfactory performance of the trained models. In this paper, we propose superpixel-based label softening to tackle the above issue. Generated by unsupervised over-segmentation, each superpixel is expected to represent a locally homogeneous area. If a superpixel intersects with the annotation boundary, we consider a high probability of uncertain labeling within this area. Driven by this intuition, we soften labels in this area based on signed distances to the annotation boundary and assign probability values within [0, 1] to them, in comparison with the original "hard", binary labels of either 0 or 1. The softened labels are then used to train the segmentation models together with the hard labels. Experimental results on a brain MRI dataset and an optical coherence tomography dataset demonstrate that this conceptually simple and implementation-wise easy method achieves overall superior segmentation performances to baseline and comparison methods for both 3D and 2D medical images.



### Identification of Tree Species in Japanese Forests based on Aerial Photography and Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2007.08907v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.08907v1)
- **Published**: 2020-07-17 11:26:38+00:00
- **Updated**: 2020-07-17 11:26:38+00:00
- **Authors**: Sarah Kentsch, Savvas Karatsiolis, Andreas Kamilaris, Luca Tomhave, Maximo Larry Lopez Caceres
- **Comment**: Proc. of EnviroInfo 2020, Nicosia, Cyprus, September 2020
- **Journal**: None
- **Summary**: Natural forests are complex ecosystems whose tree species distribution and their ecosystem functions are still not well understood. Sustainable management of these forests is of high importance because of their significant role in climate regulation, biodiversity, soil erosion and disaster prevention among many other ecosystem services they provide. In Japan particularly, natural forests are mainly located in steep mountains, hence the use of aerial imagery in combination with computer vision are important modern tools that can be applied to forest research. Thus, this study constitutes a preliminary research in this field, aiming at classifying tree species in Japanese mixed forests using UAV images and deep learning in two different mixed forest types: a black pine (Pinus thunbergii)-black locust (Robinia pseudoacacia) and a larch (Larix kaempferi)-oak (Quercus mongolica) mixed forest. Our results indicate that it is possible to identify black locust trees with 62.6 % True Positives (TP) and 98.1% True Negatives (TN), while lower precision was reached for larch trees (37.4% TP and 97.7% TN).



### Edge-Preserving Guided Semantic Segmentation for VIPriors Challenge
- **Arxiv ID**: http://arxiv.org/abs/2007.08919v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.08919v1)
- **Published**: 2020-07-17 11:49:10+00:00
- **Updated**: 2020-07-17 11:49:10+00:00
- **Authors**: Chih-Chung Hsu, Hsin-Ti Ma
- **Comment**: Technical report for VIPChallenge
- **Journal**: None
- **Summary**: Semantic segmentation is one of the most attractive research fields in computer vision. In the VIPriors challenge, only very limited numbers of training samples are allowed, leading to that the current state-of-the-art and deep learning-based semantic segmentation techniques are hard to train well. To overcome this shortcoming, therefore, we propose edge-preserving guidance to obtain the extra prior information, to avoid the overfitting under small-scale training dataset. First, a two-channeled convolutional layer is concatenated to the last layer of the conventional semantic segmentation network. Then, an edge map is calculated from the ground truth by Sobel operation and followed by concatenating a hard-thresholding operation to indicate whether the pixel is the edge or not. Then, the two-dimensional cross-entropy loss is adopted to calculate the loss between the predicted edge map and its ground truth, termed as an edge-preserving loss. In this way, the continuity of boundaries between different instances can be forced by the proposed edge-preserving loss. Experiments demonstrate that the proposed method can achieve excellent performance under small-scale training set, compared to state-of-the-art semantic segmentation techniques.



### Vision-based Estimation of MDS-UPDRS Gait Scores for Assessing Parkinson's Disease Motor Severity
- **Arxiv ID**: http://arxiv.org/abs/2007.08920v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.08920v1)
- **Published**: 2020-07-17 11:49:30+00:00
- **Updated**: 2020-07-17 11:49:30+00:00
- **Authors**: Mandy Lu, Kathleen Poston, Adolf Pfefferbaum, Edith V. Sullivan, Li Fei-Fei, Kilian M. Pohl, Juan Carlos Niebles, Ehsan Adeli
- **Comment**: Accepted as a conference paper at MICCAI (Medical Image Computing and
  Computer Assisted Intervention), Lima, Peru, October 2020. 11 pages, LaTeX
- **Journal**: None
- **Summary**: Parkinson's disease (PD) is a progressive neurological disorder primarily affecting motor function resulting in tremor at rest, rigidity, bradykinesia, and postural instability. The physical severity of PD impairments can be quantified through the Movement Disorder Society Unified Parkinson's Disease Rating Scale (MDS-UPDRS), a widely used clinical rating scale. Accurate and quantitative assessment of disease progression is critical to developing a treatment that slows or stops further advancement of the disease. Prior work has mainly focused on dopamine transport neuroimaging for diagnosis or costly and intrusive wearables evaluating motor impairments. For the first time, we propose a computer vision-based model that observes non-intrusive video recordings of individuals, extracts their 3D body skeletons, tracks them through time, and classifies the movements according to the MDS-UPDRS gait scores. Experimental results show that our proposed method performs significantly better than chance and competing methods with an F1-score of 0.83 and a balanced accuracy of 81%. This is the first benchmark for classifying PD patients based on MDS-UPDRS gait severity and could be an objective biomarker for disease severity. Our work demonstrates how computer-assisted technologies can be used to non-intrusively monitor patients and their motor impairments. The code is available at https://github.com/mlu355/PD-Motor-Severity-Estimation.



### Boundary-preserving Mask R-CNN
- **Arxiv ID**: http://arxiv.org/abs/2007.08921v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.08921v1)
- **Published**: 2020-07-17 11:54:02+00:00
- **Updated**: 2020-07-17 11:54:02+00:00
- **Authors**: Tianheng Cheng, Xinggang Wang, Lichao Huang, Wenyu Liu
- **Comment**: 17 pages, 8 figures. Accepted by ECCV 2020
- **Journal**: None
- **Summary**: Tremendous efforts have been made to improve mask localization accuracy in instance segmentation. Modern instance segmentation methods relying on fully convolutional networks perform pixel-wise classification, which ignores object boundaries and shapes, leading coarse and indistinct mask prediction results and imprecise localization. To remedy these problems, we propose a conceptually simple yet effective Boundary-preserving Mask R-CNN (BMask R-CNN) to leverage object boundary information to improve mask localization accuracy. BMask R-CNN contains a boundary-preserving mask head in which object boundary and mask are mutually learned via feature fusion blocks. As a result, the predicted masks are better aligned with object boundaries. Without bells and whistles, BMask R-CNN outperforms Mask R-CNN by a considerable margin on the COCO dataset; in the Cityscapes dataset, there are more accurate boundary groundtruths available, so that BMask R-CNN obtains remarkable improvements over Mask R-CNN. Besides, it is not surprising to observe that BMask R-CNN obtains more obvious improvement when the evaluation criterion requires better localization (e.g., AP$_{75}$) as shown in Fig.1. Code and models are available at \url{https://github.com/hustvl/BMaskR-CNN}.



### Can Learned Frame-Prediction Compete with Block-Motion Compensation for Video Coding?
- **Arxiv ID**: http://arxiv.org/abs/2007.08922v1
- **DOI**: 10.1007/s11760-020-01751-y
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.08922v1)
- **Published**: 2020-07-17 11:54:30+00:00
- **Updated**: 2020-07-17 11:54:30+00:00
- **Authors**: Serkan Sulun, A. Murat Tekalp
- **Comment**: Accepted for publication in Springer Journal of Signal, Image and
  Video Processing
- **Journal**: None
- **Summary**: Given recent advances in learned video prediction, we investigate whether a simple video codec using a pre-trained deep model for next frame prediction based on previously encoded/decoded frames without sending any motion side information can compete with standard video codecs based on block-motion compensation. Frame differences given learned frame predictions are encoded by a standard still-image (intra) codec. Experimental results show that the rate-distortion performance of the simple codec with symmetric complexity is on average better than that of x264 codec on 10 MPEG test videos, but does not yet reach the level of x265 codec. This result demonstrates the power of learned frame prediction (LFP), since unlike motion compensation, LFP does not use information from the current picture. The implications of training with L1, L2, or combined L2 and adversarial loss on prediction performance and compression efficiency are analyzed.



### Geometric Correspondence Fields: Learned Differentiable Rendering for 3D Pose Refinement in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2007.08939v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.08939v1)
- **Published**: 2020-07-17 12:34:38+00:00
- **Updated**: 2020-07-17 12:34:38+00:00
- **Authors**: Alexander Grabner, Yaming Wang, Peizhao Zhang, Peihong Guo, Tong Xiao, Peter Vajda, Peter M. Roth, Vincent Lepetit
- **Comment**: Accepted to European Conference on Computer Vision (ECCV) 2020
- **Journal**: None
- **Summary**: We present a novel 3D pose refinement approach based on differentiable rendering for objects of arbitrary categories in the wild. In contrast to previous methods, we make two main contributions: First, instead of comparing real-world images and synthetic renderings in the RGB or mask space, we compare them in a feature space optimized for 3D pose refinement. Second, we introduce a novel differentiable renderer that learns to approximate the rasterization backward pass from data instead of relying on a hand-crafted algorithm. For this purpose, we predict deep cross-domain correspondences between RGB images and 3D model renderings in the form of what we call geometric correspondence fields. These correspondence fields serve as pixel-level gradients which are analytically propagated backward through the rendering pipeline to perform a gradient-based optimization directly on the 3D pose. In this way, we precisely align 3D models to objects in RGB images which results in significantly improved 3D pose estimates. We evaluate our approach on the challenging Pix3D dataset and achieve up to 55% relative improvement compared to state-of-the-art refinement methods in multiple metrics.



### HDNet: Human Depth Estimation for Multi-Person Camera-Space Localization
- **Arxiv ID**: http://arxiv.org/abs/2007.08943v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.08943v1)
- **Published**: 2020-07-17 12:44:23+00:00
- **Updated**: 2020-07-17 12:44:23+00:00
- **Authors**: Jiahao Lin, Gim Hee Lee
- **Comment**: 16 pages, 5 figures. Accepted in ECCV 2020
- **Journal**: None
- **Summary**: Current works on multi-person 3D pose estimation mainly focus on the estimation of the 3D joint locations relative to the root joint and ignore the absolute locations of each pose. In this paper, we propose the Human Depth Estimation Network (HDNet), an end-to-end framework for absolute root joint localization in the camera coordinate space. Our HDNet first estimates the 2D human pose with heatmaps of the joints. These estimated heatmaps serve as attention masks for pooling features from image regions corresponding to the target person. A skeleton-based Graph Neural Network (GNN) is utilized to propagate features among joints. We formulate the target depth regression as a bin index estimation problem, which can be transformed with a soft-argmax operation from the classification output of our HDNet. We evaluate our HDNet on the root joint localization and root-relative 3D pose estimation tasks with two benchmark datasets, i.e., Human3.6M and MuPoTS-3D. The experimental results show that we outperform the previous state-of-the-art consistently under multiple evaluation metrics. Our source code is available at: https://github.com/jiahaoLjh/HumanDepth.



### Deep multi-metric learning for text-independent speaker verification
- **Arxiv ID**: http://arxiv.org/abs/2007.10479v1
- **DOI**: 10.1016/j.neucom.2020.06.045
- **Categories**: **eess.AS**, cs.CV, cs.SD
- **Links**: [PDF](http://arxiv.org/pdf/2007.10479v1)
- **Published**: 2020-07-17 13:19:44+00:00
- **Updated**: 2020-07-17 13:19:44+00:00
- **Authors**: Jiwei Xu, Xinggang Wang, Bin Feng, Wenyu Liu
- **Comment**: None
- **Journal**: Neurocomputing, Volume 410, 14 October 2020, Pages 394-400
- **Summary**: Text-independent speaker verification is an important artificial intelligence problem that has a wide spectrum of applications, such as criminal investigation, payment certification, and interest-based customer services. The purpose of text-independent speaker verification is to determine whether two given uncontrolled utterances originate from the same speaker or not. Extracting speech features for each speaker using deep neural networks is a promising direction to explore and a straightforward solution is to train the discriminative feature extraction network by using a metric learning loss function. However, a single loss function often has certain limitations. Thus, we use deep multi-metric learning to address the problem and introduce three different losses for this problem, i.e., triplet loss, n-pair loss and angular loss. The three loss functions work in a cooperative way to train a feature extraction network equipped with Residual connections and squeeze-and-excitation attention. We conduct experiments on the large-scale \texttt{VoxCeleb2} dataset, which contains over a million utterances from over $6,000$ speakers, and the proposed deep neural network obtains an equal error rate of $3.48\%$, which is a very competitive result. Codes for both training and testing and pretrained models are available at \url{https://github.com/GreatJiweix/DmmlTiSV}, which is the first publicly available code repository for large-scale text-independent speaker verification with performance on par with the state-of-the-art systems.



### Weakly-supervised Learning of Human Dynamics
- **Arxiv ID**: http://arxiv.org/abs/2007.08969v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.08969v2)
- **Published**: 2020-07-17 13:32:57+00:00
- **Updated**: 2021-04-23 11:10:00+00:00
- **Authors**: Petrissa Zell, Bodo Rosenhahn, Bastian Wandt
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a weakly-supervised learning framework for dynamics estimation from human motion. Although there are many solutions to capture pure human motion readily available, their data is not sufficient to analyze quality and efficiency of movements. Instead, the forces and moments driving human motion (the dynamics) need to be considered. Since recording dynamics is a laborious task that requires expensive sensors and complex, time-consuming optimization, dynamics data sets are small compared to human motion data sets and are rarely made public. The proposed approach takes advantage of easily obtainable motion data which enables weakly-supervised learning on small dynamics sets and weakly-supervised domain transfer. Our method includes novel neural network (NN) layers for forward and inverse dynamics during end-to-end training. On this basis, a cyclic loss between pure motion data can be minimized, i.e. no ground truth forces and moments are required during training. The proposed method achieves state-of-the-art results in terms of ground reaction force, ground reaction moment and joint torque regression and is able to maintain good performance on substantially reduced sets.



### LEED: Label-Free Expression Editing via Disentanglement
- **Arxiv ID**: http://arxiv.org/abs/2007.08971v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.08971v1)
- **Published**: 2020-07-17 13:36:15+00:00
- **Updated**: 2020-07-17 13:36:15+00:00
- **Authors**: Rongliang Wu, Shijian Lu
- **Comment**: Accepted to ECCV 2020
- **Journal**: None
- **Summary**: Recent studies on facial expression editing have obtained very promising progress. On the other hand, existing methods face the constraint of requiring a large amount of expression labels which are often expensive and time-consuming to collect. This paper presents an innovative label-free expression editing via disentanglement (LEED) framework that is capable of editing the expression of both frontal and profile facial images without requiring any expression label. The idea is to disentangle the identity and expression of a facial image in the expression manifold, where the neutral face captures the identity attribute and the displacement between the neutral image and the expressive image captures the expression attribute. Two novel losses are designed for optimal expression disentanglement and consistent synthesis, including a mutual expression information loss that aims to extract pure expression-related features and a siamese loss that aims to enhance the expression similarity between the synthesized image and the reference image. Extensive experiments over two public facial expression datasets show that LEED achieves superior facial expression editing qualitatively and quantitatively.



### AlignNet: Unsupervised Entity Alignment
- **Arxiv ID**: http://arxiv.org/abs/2007.08973v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.08973v2)
- **Published**: 2020-07-17 13:38:29+00:00
- **Updated**: 2020-07-21 13:12:07+00:00
- **Authors**: Antonia Creswell, Kyriacos Nikiforou, Oriol Vinyals, Andre Saraiva, Rishabh Kabra, Loic Matthey, Chris Burgess, Malcolm Reynolds, Richard Tanburn, Marta Garnelo, Murray Shanahan
- **Comment**: None
- **Journal**: None
- **Summary**: Recently developed deep learning models are able to learn to segment scenes into component objects without supervision. This opens many new and exciting avenues of research, allowing agents to take objects (or entities) as inputs, rather that pixels. Unfortunately, while these models provide excellent segmentation of a single frame, they do not keep track of how objects segmented at one time-step correspond (or align) to those at a later time-step. The alignment (or correspondence) problem has impeded progress towards using object representations in downstream tasks. In this paper we take steps towards solving the alignment problem, presenting the AlignNet, an unsupervised alignment module.



### URIE: Universal Image Enhancement for Visual Recognition in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2007.08979v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.08979v3)
- **Published**: 2020-07-17 13:45:56+00:00
- **Updated**: 2020-07-24 12:21:01+00:00
- **Authors**: Taeyoung Son, Juwon Kang, Namyup Kim, Sunghyun Cho, Suha Kwak
- **Comment**: Accepted as a conference paper at ECCV 2020
- **Journal**: None
- **Summary**: Despite the great advances in visual recognition, it has been witnessed that recognition models trained on clean images of common datasets are not robust against distorted images in the real world. To tackle this issue, we present a Universal and Recognition-friendly Image Enhancement network, dubbed URIE, which is attached in front of existing recognition models and enhances distorted input to improve their performance without retraining them. URIE is universal in that it aims to handle various factors of image degradation and to be incorporated with any arbitrary recognition models. Also, it is recognition-friendly since it is optimized to improve the robustness of following recognition models, instead of perceptual quality of output image. Our experiments demonstrate that URIE can handle various and latent image distortions and improve the performance of existing models for five diverse recognition tasks when input images are degraded.



### Online Invariance Selection for Local Feature Descriptors
- **Arxiv ID**: http://arxiv.org/abs/2007.08988v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.08988v3)
- **Published**: 2020-07-17 14:08:22+00:00
- **Updated**: 2020-07-23 15:16:23+00:00
- **Authors**: Rémi Pautrat, Viktor Larsson, Martin R. Oswald, Marc Pollefeys
- **Comment**: 27 pages, Accepted at ECCV 2020 (Oral)
- **Journal**: None
- **Summary**: To be invariant, or not to be invariant: that is the question formulated in this work about local descriptors. A limitation of current feature descriptors is the trade-off between generalization and discriminative power: more invariance means less informative descriptors. We propose to overcome this limitation with a disentanglement of invariance in local descriptors and with an online selection of the most appropriate invariance given the context. Our framework consists in a joint learning of multiple local descriptors with different levels of invariance and of meta descriptors encoding the regional variations of an image. The similarity of these meta descriptors across images is used to select the right invariance when matching the local descriptors. Our approach, named Local Invariance Selection at Runtime for Descriptors (LISRD), enables descriptors to adapt to adverse changes in images, while remaining discriminative when invariance is not required. We demonstrate that our method can boost the performance of current descriptors and outperforms state-of-the-art descriptors in several matching tasks, when evaluated on challenging datasets with day-night illumination as well as viewpoint changes.



### Region-based Non-local Operation for Video Classification
- **Arxiv ID**: http://arxiv.org/abs/2007.09033v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.09033v5)
- **Published**: 2020-07-17 14:57:05+00:00
- **Updated**: 2021-02-02 00:21:37+00:00
- **Authors**: Guoxi Huang, Adrian G. Bors
- **Comment**: None
- **Journal**: ICPR2020
- **Summary**: Convolutional Neural Networks (CNNs) model long-range dependencies by deeply stacking convolution operations with small window sizes, which makes the optimizations difficult. This paper presents region-based non-local (RNL) operations as a family of self-attention mechanisms, which can directly capture long-range dependencies without using a deep stack of local operations. Given an intermediate feature map, our method recalibrates the feature at a position by aggregating the information from the neighboring regions of all positions. By combining a channel attention module with the proposed RNL, we design an attention chain, which can be integrated into the off-the-shelf CNNs for end-to-end training. We evaluate our method on two video classification benchmarks. The experimental results of our method outperform other attention mechanisms, and we achieve state-of-the-art performance on the Something-Something V1 dataset.



### Learning to Discretely Compose Reasoning Module Networks for Video Captioning
- **Arxiv ID**: http://arxiv.org/abs/2007.09049v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.09049v1)
- **Published**: 2020-07-17 15:27:37+00:00
- **Updated**: 2020-07-17 15:27:37+00:00
- **Authors**: Ganchao Tan, Daqing Liu, Meng Wang, Zheng-Jun Zha
- **Comment**: Accepted at IJCAI 2020 Main Track. Sole copyright holder is IJCAI.
  Code is available at https://github.com/tgc1997/RMN
- **Journal**: IJCAI 2020, Pages 745-752
- **Summary**: Generating natural language descriptions for videos, i.e., video captioning, essentially requires step-by-step reasoning along the generation process. For example, to generate the sentence "a man is shooting a basketball", we need to first locate and describe the subject "man", next reason out the man is "shooting", then describe the object "basketball" of shooting. However, existing visual reasoning methods designed for visual question answering are not appropriate to video captioning, for it requires more complex visual reasoning on videos over both space and time, and dynamic module composition along the generation process. In this paper, we propose a novel visual reasoning approach for video captioning, named Reasoning Module Networks (RMN), to equip the existing encoder-decoder framework with the above reasoning capacity. Specifically, our RMN employs 1) three sophisticated spatio-temporal reasoning modules, and 2) a dynamic and discrete module selector trained by a linguistic loss with a Gumbel approximation. Extensive experiments on MSVD and MSR-VTT datasets demonstrate the proposed RMN outperforms the state-of-the-art methods while providing an explicit and explainable generation process. Our code is available at https://github.com/tgc1997/RMN.



### Self-Supervised Learning of Context-Aware Pitch Prosody Representations
- **Arxiv ID**: http://arxiv.org/abs/2007.09060v4
- **DOI**: None
- **Categories**: **cs.SD**, cs.CV, cs.IR, cs.LG, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2007.09060v4)
- **Published**: 2020-07-17 15:41:00+00:00
- **Updated**: 2021-08-01 05:16:48+00:00
- **Authors**: Camille Noufi, Prateek Verma
- **Comment**: None
- **Journal**: None
- **Summary**: In music and speech, meaning is derived at multiple levels of context. Affect, for example, can be inferred both by a short sound token and by sonic patterns over a longer temporal window such as an entire recording. In this letter, we focus on inferring meaning from this dichotomy of contexts. We show how contextual representations of short sung vocal lines can be implicitly learned from fundamental frequency ($F_0$) and thus be used as a meaningful feature space for downstream Music Information Retrieval (MIR) tasks. We propose three self-supervised deep learning paradigms which leverage pseudotask learning of these two levels of context to produce latent representation spaces. We evaluate the usefulness of these representations by embedding unseen pitch contours into each space and conducting downstream classification tasks. Our results show that contextual representation can enhance downstream classification by as much as 15\% as compared to using traditional statistical contour features.



### Multi-scale Interactive Network for Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2007.09062v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.09062v1)
- **Published**: 2020-07-17 15:41:37+00:00
- **Updated**: 2020-07-17 15:41:37+00:00
- **Authors**: Youwei Pang, Xiaoqi Zhao, Lihe Zhang, Huchuan Lu
- **Comment**: Accepted by CVPR 2020
- **Journal**: None
- **Summary**: Deep-learning based salient object detection methods achieve great progress. However, the variable scale and unknown category of salient objects are great challenges all the time. These are closely related to the utilization of multi-level and multi-scale features. In this paper, we propose the aggregate interaction modules to integrate the features from adjacent levels, in which less noise is introduced because of only using small up-/down-sampling rates. To obtain more efficient multi-scale features from the integrated features, the self-interaction modules are embedded in each decoder unit. Besides, the class imbalance issue caused by the scale variation weakens the effect of the binary cross entropy loss and results in the spatial inconsistency of the predictions. Therefore, we exploit the consistency-enhanced loss to highlight the fore-/back-ground difference and preserve the intra-class consistency. Experimental results on five benchmark datasets demonstrate that the proposed method without any post-processing performs favorably against 23 state-of-the-art approaches. The source code will be publicly available at https://github.com/lartpang/MINet.



### Hybrid Discriminative-Generative Training via Contrastive Learning
- **Arxiv ID**: http://arxiv.org/abs/2007.09070v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.09070v2)
- **Published**: 2020-07-17 15:50:34+00:00
- **Updated**: 2020-08-10 07:34:31+00:00
- **Authors**: Hao Liu, Pieter Abbeel
- **Comment**: Code: https://github.com/lhao499/HDGE
- **Journal**: None
- **Summary**: Contrastive learning and supervised learning have both seen significant progress and success. However, thus far they have largely been treated as two separate objectives, brought together only by having a shared neural network. In this paper we show that through the perspective of hybrid discriminative-generative training of energy-based models we can make a direct connection between contrastive learning and supervised learning. Beyond presenting this unified view, we show our specific choice of approximation of the energy-based loss outperforms the existing practice in terms of classification accuracy of WideResNet on CIFAR-10 and CIFAR-100. It also leads to improved performance on robustness, out-of-distribution detection, and calibration.



### GMNet: Graph Matching Network for Large Scale Part Semantic Segmentation in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2007.09073v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.09073v1)
- **Published**: 2020-07-17 15:53:40+00:00
- **Updated**: 2020-07-17 15:53:40+00:00
- **Authors**: Umberto Michieli, Edoardo Borsato, Luca Rossi, Pietro Zanuttigh
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: The semantic segmentation of parts of objects in the wild is a challenging task in which multiple instances of objects and multiple parts within those objects must be detected in the scene. This problem remains nowadays very marginally explored, despite its fundamental importance towards detailed object understanding. In this work, we propose a novel framework combining higher object-level context conditioning and part-level spatial relationships to address the task. To tackle object-level ambiguity, a class-conditioning module is introduced to retain class-level semantics when learning parts-level semantics. In this way, mid-level features carry also this information prior to the decoding stage. To tackle part-level ambiguity and localization we propose a novel adjacency graph-based module that aims at matching the relative spatial relationships between ground truth and predicted parts. The experimental evaluation on the Pascal-Part dataset shows that we achieve state-of-the-art results on this task.



### Generating Person Images with Appearance-aware Pose Stylizer
- **Arxiv ID**: http://arxiv.org/abs/2007.09077v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.09077v1)
- **Published**: 2020-07-17 15:58:05+00:00
- **Updated**: 2020-07-17 15:58:05+00:00
- **Authors**: Siyu Huang, Haoyi Xiong, Zhi-Qi Cheng, Qingzhong Wang, Xingran Zhou, Bihan Wen, Jun Huan, Dejing Dou
- **Comment**: Appearing at IJCAI 2020. The code is available at
  https://github.com/siyuhuang/PoseStylizer
- **Journal**: None
- **Summary**: Generation of high-quality person images is challenging, due to the sophisticated entanglements among image factors, e.g., appearance, pose, foreground, background, local details, global structures, etc. In this paper, we present a novel end-to-end framework to generate realistic person images based on given person poses and appearances. The core of our framework is a novel generator called Appearance-aware Pose Stylizer (APS) which generates human images by coupling the target pose with the conditioned person appearance progressively. The framework is highly flexible and controllable by effectively decoupling various complex person image factors in the encoding phase, followed by re-coupling them in the decoding phase. In addition, we present a new normalization method named adaptive patch normalization, which enables region-specific normalization and shows a good performance when adopted in person image generation model. Experiments on two benchmark datasets show that our method is capable of generating visually appealing and realistic-looking results using arbitrary image and pose inputs.



### Multi-Stage Influence Function
- **Arxiv ID**: http://arxiv.org/abs/2007.09081v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.09081v1)
- **Published**: 2020-07-17 16:03:11+00:00
- **Updated**: 2020-07-17 16:03:11+00:00
- **Authors**: Hongge Chen, Si Si, Yang Li, Ciprian Chelba, Sanjiv Kumar, Duane Boning, Cho-Jui Hsieh
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-stage training and knowledge transfer, from a large-scale pretraining task to various finetuning tasks, have revolutionized natural language processing and computer vision resulting in state-of-the-art performance improvements. In this paper, we develop a multi-stage influence function score to track predictions from a finetuned model all the way back to the pretraining data. With this score, we can identify the pretraining examples in the pretraining task that contribute most to a prediction in the finetuning task. The proposed multi-stage influence function generalizes the original influence function for a single model in (Koh & Liang, 2017), thereby enabling influence computation through both pretrained and finetuned models. We study two different scenarios with the pretrained embeddings fixed or updated in the finetuning tasks. We test our proposed method in various experiments to show its effectiveness and potential applications.



### TopoAL: An Adversarial Learning Approach for Topology-Aware Road Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2007.09084v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.09084v1)
- **Published**: 2020-07-17 16:06:45+00:00
- **Updated**: 2020-07-17 16:06:45+00:00
- **Authors**: Subeesh Vasu, Mateusz Kozinski, Leonardo Citraro, Pascal Fua
- **Comment**: None
- **Journal**: None
- **Summary**: Most state-of-the-art approaches to road extraction from aerial images rely on a CNN trained to label road pixels as foreground and remainder of the image as background. The CNN is usually trained by minimizing pixel-wise losses, which is less than ideal to produce binary masks that preserve the road network's global connectivity. To address this issue, we introduce an Adversarial Learning (AL) strategy tailored for our purposes. A naive one would treat the segmentation network as a generator and would feed its output along with ground-truth segmentations to a discriminator. It would then train the generator and discriminator jointly. We will show that this is not enough because it does not capture the fact that most errors are local and need to be treated as such. Instead, we use a more sophisticated discriminator that returns a label pyramid describing what portions of the road network are correct at several different scales. This discriminator and the structured labels it returns are what gives our approach its edge and we will show that it outperforms state-of-the-art ones on the challenging RoadTracer dataset.



### Tracking the Untrackable
- **Arxiv ID**: http://arxiv.org/abs/2007.10148v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.10148v1)
- **Published**: 2020-07-17 16:17:21+00:00
- **Updated**: 2020-07-17 16:17:21+00:00
- **Authors**: Fangyi Zhang
- **Comment**: 8 pages, 7 figures
- **Journal**: None
- **Summary**: Although short-term fully occlusion happens rare in visual object tracking, most trackers will fail under these circumstances. However, humans can still catch up the target by anticipating the trajectory of the target even the target is invisible. Recent psychology also has shown that humans build the mental image of the future. Inspired by that, we present a HAllucinating Features to Track (HAFT) model that enables to forecast the visual feature embedding of future frames. The anticipated future frames focus on the movement of the target while hallucinating the occluded part of the target. Jointly tracking on the hallucinated features and the real features improves the robustness of the tracker even when the target is highly occluded. Through extensive experimental evaluations, we achieve promising results on multiple datasets: OTB100, VOT2018, LaSOT, TrackingNet, and UAV123.



### Synthetic and Real Inputs for Tool Segmentation in Robotic Surgery
- **Arxiv ID**: http://arxiv.org/abs/2007.09107v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2007.09107v2)
- **Published**: 2020-07-17 16:33:33+00:00
- **Updated**: 2020-07-26 08:27:20+00:00
- **Authors**: Emanuele Colleoni, Philip Edwards, Danail Stoyanov
- **Comment**: None
- **Journal**: None
- **Summary**: Semantic tool segmentation in surgical videos is important for surgical scene understanding and computer-assisted interventions as well as for the development of robotic automation. The problem is challenging because different illumination conditions, bleeding, smoke and occlusions can reduce algorithm robustness. At present labelled data for training deep learning models is still lacking for semantic surgical instrument segmentation and in this paper we show that it may be possible to use robot kinematic data coupled with laparoscopic images to alleviate the labelling problem. We propose a new deep learning based model for parallel processing of both laparoscopic and simulation images for robust segmentation of surgical tools. Due to the lack of laparoscopic frames annotated with both segmentation ground truth and kinematic information a new custom dataset was generated using the da Vinci Research Kit (dVRK) and is made available.



### Scale Equivariance Improves Siamese Tracking
- **Arxiv ID**: http://arxiv.org/abs/2007.09115v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.09115v2)
- **Published**: 2020-07-17 16:55:51+00:00
- **Updated**: 2020-11-06 12:29:44+00:00
- **Authors**: Ivan Sosnovik, Artem Moskalev, Arnold Smeulders
- **Comment**: None
- **Journal**: None
- **Summary**: Siamese trackers turn tracking into similarity estimation between a template and the candidate regions in the frame. Mathematically, one of the key ingredients of success of the similarity function is translation equivariance. Non-translation-equivariant architectures induce a positional bias during training, so the location of the target will be hard to recover from the feature space. In real life scenarios, objects undergoe various transformations other than translation, such as rotation or scaling. Unless the model has an internal mechanism to handle them, the similarity may degrade. In this paper, we focus on scaling and we aim to equip the Siamese network with additional built-in scale equivariance to capture the natural variations of the target a priori. We develop the theory for scale-equivariant Siamese trackers, and provide a simple recipe for how to make a wide range of existing trackers scale-equivariant. We present SE-SiamFC, a scale-equivariant variant of SiamFC built according to the recipe. We conduct experiments on OTB and VOT benchmarks and on the synthetically generated T-MNIST and S-MNIST datasets. We demonstrate that a built-in additional scale equivariance is useful for visual object tracking.



### Improving Object Detection with Selective Self-supervised Self-training
- **Arxiv ID**: http://arxiv.org/abs/2007.09162v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.09162v2)
- **Published**: 2020-07-17 18:05:01+00:00
- **Updated**: 2020-07-24 19:34:54+00:00
- **Authors**: Yandong Li, Di Huang, Danfeng Qin, Liqiang Wang, Boqing Gong
- **Comment**: Accepted to ECCV 2020
- **Journal**: None
- **Summary**: We study how to leverage Web images to augment human-curated object detection datasets. Our approach is two-pronged. On the one hand, we retrieve Web images by image-to-image search, which incurs less domain shift from the curated data than other search methods. The Web images are diverse, supplying a wide variety of object poses, appearances, their interactions with the context, etc. On the other hand, we propose a novel learning method motivated by two parallel lines of work that explore unlabeled data for image classification: self-training and self-supervised learning. They fail to improve object detectors in their vanilla forms due to the domain gap between the Web images and curated datasets. To tackle this challenge, we propose a selective net to rectify the supervision signals in Web images. It not only identifies positive bounding boxes but also creates a safe zone for mining hard negative boxes. We report state-of-the-art results on detecting backpacks and chairs from everyday scenes, along with other challenging object classes.



### Wavelet Channel Attention Module with a Fusion Network for Single Image Deraining
- **Arxiv ID**: http://arxiv.org/abs/2007.09163v1
- **DOI**: 10.1109/ICIP40778.2020.9190720
- **Categories**: **cs.CV**, cs.LG, cs.NE, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.09163v1)
- **Published**: 2020-07-17 18:06:13+00:00
- **Updated**: 2020-07-17 18:06:13+00:00
- **Authors**: Hao-Hsiang Yang, Chao-Han Huck Yang, Yu-Chiang Frank Wang
- **Comment**: Accepted to IEEE ICIP 2020
- **Journal**: 2020 IEEE International Conference on Image Processing (ICIP)
- **Summary**: Single image deraining is a crucial problem because rain severely degenerates the visibility of images and affects the performance of computer vision tasks like outdoor surveillance systems and intelligent vehicles. In this paper, we propose the new convolutional neural network (CNN) called the wavelet channel attention module with a fusion network. Wavelet transform and the inverse wavelet transform are substituted for down-sampling and up-sampling so feature maps from the wavelet transform and convolutions contain different frequencies and scales. Furthermore, feature maps are integrated by channel attention. Our proposed network learns confidence maps of four sub-band images derived from the wavelet transform of the original images. Finally, the clear image can be well restored via the wavelet reconstruction and fusion of the low-frequency part and high-frequency parts. Several experimental results on synthetic and real images present that the proposed algorithm outperforms state-of-the-art methods.



### AutoCount: Unsupervised Segmentation and Counting of Organs in Field Images
- **Arxiv ID**: http://arxiv.org/abs/2007.09178v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.09178v1)
- **Published**: 2020-07-17 18:27:47+00:00
- **Updated**: 2020-07-17 18:27:47+00:00
- **Authors**: Jordan Ubbens, Tewodros Ayalew, Steve Shirtliffe, Anique Josuttes, Curtis Pozniak, Ian Stavness
- **Comment**: Computer Vision Problems in Plant Phenotyping (CVPPP) in conjunction
  with ECCV 2020
- **Journal**: None
- **Summary**: Counting plant organs such as heads or tassels from outdoor imagery is a popular benchmark computer vision task in plant phenotyping, which has been previously investigated in the literature using state-of-the-art supervised deep learning techniques. However, the annotation of organs in field images is time-consuming and prone to errors. In this paper, we propose a fully unsupervised technique for counting dense objects such as plant organs. We use a convolutional network-based unsupervised segmentation method followed by two post-hoc optimization steps. The proposed technique is shown to provide competitive counting performance on a range of organ counting tasks in sorghum (S. bicolor) and wheat (T. aestivum) with no dataset-dependent tuning or modifications.



### Off-Policy Reinforcement Learning for Efficient and Effective GAN Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/2007.09180v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.09180v1)
- **Published**: 2020-07-17 18:29:17+00:00
- **Updated**: 2020-07-17 18:29:17+00:00
- **Authors**: Yuan Tian, Qin Wang, Zhiwu Huang, Wen Li, Dengxin Dai, Minghao Yang, Jun Wang, Olga Fink
- **Comment**: To appear in ECCV 2020
- **Journal**: None
- **Summary**: In this paper, we introduce a new reinforcement learning (RL) based neural architecture search (NAS) methodology for effective and efficient generative adversarial network (GAN) architecture search. The key idea is to formulate the GAN architecture search problem as a Markov decision process (MDP) for smoother architecture sampling, which enables a more effective RL-based search algorithm by targeting the potential global optimal architecture. To improve efficiency, we exploit an off-policy GAN architecture search algorithm that makes efficient use of the samples generated by previous policies. Evaluation on two standard benchmark datasets (i.e., CIFAR-10 and STL-10) demonstrates that the proposed method is able to discover highly competitive architectures for generally better image generation results with a considerably reduced computational burden: 7 GPU hours. Our code is available at https://github.com/Yuantian013/E2GAN.



### Bi-directional Cross-Modality Feature Propagation with Separation-and-Aggregation Gate for RGB-D Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2007.09183v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.09183v1)
- **Published**: 2020-07-17 18:35:24+00:00
- **Updated**: 2020-07-17 18:35:24+00:00
- **Authors**: Xiaokang Chen, Kwan-Yee Lin, Jingbo Wang, Wayne Wu, Chen Qian, Hongsheng Li, Gang Zeng
- **Comment**: Accepted by ECCV 2020
- **Journal**: None
- **Summary**: Depth information has proven to be a useful cue in the semantic segmentation of RGB-D images for providing a geometric counterpart to the RGB representation. Most existing works simply assume that depth measurements are accurate and well-aligned with the RGB pixels and models the problem as a cross-modal feature fusion to obtain better feature representations to achieve more accurate segmentation. This, however, may not lead to satisfactory results as actual depth data are generally noisy, which might worsen the accuracy as the networks go deeper.   In this paper, we propose a unified and efficient Cross-modality Guided Encoder to not only effectively recalibrate RGB feature responses, but also to distill accurate depth information via multiple stages and aggregate the two recalibrated representations alternatively. The key of the proposed architecture is a novel Separation-and-Aggregation Gating operation that jointly filters and recalibrates both representations before cross-modality aggregation. Meanwhile, a Bi-direction Multi-step Propagation strategy is introduced, on the one hand, to help to propagate and fuse information between the two modalities, and on the other hand, to preserve their specificity along the long-term propagation process. Besides, our proposed encoder can be easily injected into the previous encoder-decoder structures to boost their performance on RGB-D semantic segmentation. Our model outperforms state-of-the-arts consistently on both in-door and out-door challenging datasets. Code of this work is available at https://charlescxk.github.io/



### Attention2AngioGAN: Synthesizing Fluorescein Angiography from Retinal Fundus Images using Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2007.09191v1
- **DOI**: 10.1109/ICPR48806.2021.9412428
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.09191v1)
- **Published**: 2020-07-17 18:58:44+00:00
- **Updated**: 2020-07-17 18:58:44+00:00
- **Authors**: Sharif Amit Kamran, Khondker Fariha Hossain, Alireza Tavakkoli, Stewart Lee Zuckerbrod
- **Comment**: 8 pages, 4 figures, 2 tables
- **Journal**: None
- **Summary**: Fluorescein Angiography (FA) is a technique that employs the designated camera for Fundus photography incorporating excitation and barrier filters. FA also requires fluorescein dye that is injected intravenously, which might cause adverse effects ranging from nausea, vomiting to even fatal anaphylaxis. Currently, no other fast and non-invasive technique exists that can generate FA without coupling with Fundus photography. To eradicate the need for an invasive FA extraction procedure, we introduce an Attention-based Generative network that can synthesize Fluorescein Angiography from Fundus images. The proposed gan incorporates multiple attention based skip connections in generators and comprises novel residual blocks for both generators and discriminators. It utilizes reconstruction, feature-matching, and perceptual loss along with adversarial training to produces realistic Angiograms that is hard for experts to distinguish from real ones. Our experiments confirm that the proposed architecture surpasses recent state-of-the-art generative networks for fundus-to-angio translation task.



### Speech2Video Synthesis with 3D Skeleton Regularization and Expressive Body Poses
- **Arxiv ID**: http://arxiv.org/abs/2007.09198v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2007.09198v5)
- **Published**: 2020-07-17 19:30:14+00:00
- **Updated**: 2020-10-08 23:19:03+00:00
- **Authors**: Miao Liao, Sibo Zhang, Peng Wang, Hao Zhu, Xinxin Zuo, Ruigang Yang
- **Comment**: Accepted by ACCV 2020
- **Journal**: None
- **Summary**: In this paper, we propose a novel approach to convert given speech audio to a photo-realistic speaking video of a specific person, where the output video has synchronized, realistic, and expressive rich body dynamics. We achieve this by first generating 3D skeleton movements from the audio sequence using a recurrent neural network (RNN), and then synthesizing the output video via a conditional generative adversarial network (GAN). To make the skeleton movement realistic and expressive, we embed the knowledge of an articulated 3D human skeleton and a learned dictionary of personal speech iconic gestures into the generation process in both learning and testing pipelines. The former prevents the generation of unreasonable body distortion, while the later helps our model quickly learn meaningful body movement through a few recorded videos. To produce photo-realistic and high-resolution video with motion details, we propose to insert part attention mechanisms in the conditional GAN, where each detailed part, e.g. head and hand, is automatically zoomed in to have their own discriminators. To validate our approach, we collect a dataset with 20 high-quality videos from 1 male and 1 female model reading various documents under different topics. Compared with previous SoTA pipelines handling similar tasks, our approach achieves better results by a user study.



### Neural Networks with Recurrent Generative Feedback
- **Arxiv ID**: http://arxiv.org/abs/2007.09200v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.09200v2)
- **Published**: 2020-07-17 19:32:48+00:00
- **Updated**: 2020-11-10 08:29:39+00:00
- **Authors**: Yujia Huang, James Gornet, Sihui Dai, Zhiding Yu, Tan Nguyen, Doris Y. Tsao, Anima Anandkumar
- **Comment**: NeurIPS 2020
- **Journal**: None
- **Summary**: Neural networks are vulnerable to input perturbations such as additive noise and adversarial attacks. In contrast, human perception is much more robust to such perturbations. The Bayesian brain hypothesis states that human brains use an internal generative model to update the posterior beliefs of the sensory input. This mechanism can be interpreted as a form of self-consistency between the maximum a posteriori (MAP) estimation of an internal generative model and the external environment. Inspired by such hypothesis, we enforce self-consistency in neural networks by incorporating generative recurrent feedback. We instantiate this design on convolutional neural networks (CNNs). The proposed framework, termed Convolutional Neural Networks with Feedback (CNN-F), introduces a generative feedback with latent variables to existing CNN architectures, where consistent predictions are made through alternating MAP inference under a Bayesian framework. In the experiments, CNN-F shows considerably improved adversarial robustness over conventional feedforward CNNs on standard benchmarks.



### People as Scene Probes
- **Arxiv ID**: http://arxiv.org/abs/2007.09209v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.09209v1)
- **Published**: 2020-07-17 19:50:42+00:00
- **Updated**: 2020-07-17 19:50:42+00:00
- **Authors**: Yifan Wang, Brian Curless, Steve Seitz
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: By analyzing the motion of people and other objects in a scene, we demonstrate how to infer depth, occlusion, lighting, and shadow information from video taken from a single camera viewpoint. This information is then used to composite new objects into the same scene with a high degree of automation and realism. In particular, when a user places a new object (2D cut-out) in the image, it is automatically rescaled, relit, occluded properly, and casts realistic shadows in the correct direction relative to the sun, and which conform properly to scene geometry. We demonstrate results (best viewed in supplementary video) on a range of scenes and compare to alternative methods for depth estimation and shadow compositing.



### Anomaly Detection in Unsupervised Surveillance Setting Using Ensemble of Multimodal Data with Adversarial Defense
- **Arxiv ID**: http://arxiv.org/abs/2007.10812v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.10812v1)
- **Published**: 2020-07-17 20:03:02+00:00
- **Updated**: 2020-07-17 20:03:02+00:00
- **Authors**: Sayeed Shafayet Chowdhury, Kaji Mejbaul Islam, Rouhan Noor
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2006.03733
- **Journal**: None
- **Summary**: Autonomous aerial surveillance using drone feed is an interesting and challenging research domain. To ensure safety from intruders and potential objects posing threats to the zone being protected, it is crucial to be able to distinguish between normal and abnormal states in real-time. Additionally, we also need to consider any device malfunction. However, the inherent uncertainty embedded within the type and level of abnormality makes supervised techniques less suitable since the adversary may present a unique anomaly for intrusion. As a result, an unsupervised method for anomaly detection is preferable taking the unpredictable nature of attacks into account. Again in our case, the autonomous drone provides heterogeneous data streams consisting of images and other analog or digital sensor data, all of which can play a role in anomaly detection if they are ensembled synergistically. To that end, an ensemble detection mechanism is proposed here which estimates the degree of abnormality of analyzing the real-time image and IMU (Inertial Measurement Unit) sensor data in an unsupervised manner. First, we have implemented a Convolutional Neural Network (CNN) regression block, named AngleNet to estimate the angle between a reference image and current test image, which provides us with a measure of the anomaly of the device. Moreover, the IMU data are used in autoencoders to predict abnormality. Finally, the results from these two pipelines are ensembled to estimate the final degree of abnormality. Furthermore, we have applied adversarial attack to test the robustness and security of the proposed approach and integrated defense mechanism. The proposed method performs satisfactorily on the IEEE SP Cup-2020 dataset with an accuracy of 97.8%. Additionally, we have also tested this approach on an in-house dataset to validate its robustness.



### DH3D: Deep Hierarchical 3D Descriptors for Robust Large-Scale 6DoF Relocalization
- **Arxiv ID**: http://arxiv.org/abs/2007.09217v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.09217v1)
- **Published**: 2020-07-17 20:21:22+00:00
- **Updated**: 2020-07-17 20:21:22+00:00
- **Authors**: Juan Du, Rui Wang, Daniel Cremers
- **Comment**: ECCV 2020, sportlight
- **Journal**: None
- **Summary**: For relocalization in large-scale point clouds, we propose the first approach that unifies global place recognition and local 6DoF pose refinement. To this end, we design a Siamese network that jointly learns 3D local feature detection and description directly from raw 3D points. It integrates FlexConv and Squeeze-and-Excitation (SE) to assure that the learned local descriptor captures multi-level geometric information and channel-wise relations. For detecting 3D keypoints we predict the discriminativeness of the local descriptors in an unsupervised manner. We generate the global descriptor by directly aggregating the learned local descriptors with an effective attention mechanism. In this way, local and global 3D descriptors are inferred in one single forward pass. Experiments on various benchmarks demonstrate that our method achieves competitive results for both global point cloud retrieval and local point cloud registration in comparison to state-of-the-art approaches. To validate the generalizability and robustness of our 3D keypoints, we demonstrate that our method also performs favorably without fine-tuning on the registration of point clouds that were generated by a visual SLAM system. Code and related materials are available at https://vision.in.tum.de/research/vslam/dh3d.



### Learn distributed GAN with Temporary Discriminators
- **Arxiv ID**: http://arxiv.org/abs/2007.09221v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.09221v1)
- **Published**: 2020-07-17 20:45:57+00:00
- **Updated**: 2020-07-17 20:45:57+00:00
- **Authors**: Hui Qu, Yikai Zhang, Qi Chang, Zhennan Yan, Chao Chen, Dimitris Metaxas
- **Comment**: Accepted by ECCV2020. Code: https://github.com/huiqu18/TDGAN-PyTorch
- **Journal**: None
- **Summary**: In this work, we propose a method for training distributed GAN with sequential temporary discriminators. Our proposed method tackles the challenge of training GAN in the federated learning manner: How to update the generator with a flow of temporary discriminators? We apply our proposed method to learn a self-adaptive generator with a series of local discriminators from multiple data centers. We show our design of loss function indeed learns the correct distribution with provable guarantees. The empirical experiments show that our approach is capable of generating synthetic data which is practical for real-world applications such as training a segmentation model.



### Classes Matter: A Fine-grained Adversarial Approach to Cross-domain Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2007.09222v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.09222v1)
- **Published**: 2020-07-17 20:50:59+00:00
- **Updated**: 2020-07-17 20:50:59+00:00
- **Authors**: Haoran Wang, Tong Shen, Wei Zhang, Lingyu Duan, Tao Mei
- **Comment**: Accepted to ECCV 2020
- **Journal**: None
- **Summary**: Despite great progress in supervised semantic segmentation,a large performance drop is usually observed when deploying the model in the wild. Domain adaptation methods tackle the issue by aligning the source domain and the target domain. However, most existing methods attempt to perform the alignment from a holistic view, ignoring the underlying class-level data structure in the target domain. To fully exploit the supervision in the source domain, we propose a fine-grained adversarial learning strategy for class-level feature alignment while preserving the internal structure of semantics across domains. We adopt a fine-grained domain discriminator that not only plays as a domain distinguisher, but also differentiates domains at class level. The traditional binary domain labels are also generalized to domain encodings as the supervision signal to guide the fine-grained feature alignment. An analysis with Class Center Distance (CCD) validates that our fine-grained adversarial strategy achieves better class-level alignment compared to other state-of-the-art methods. Our method is easy to implement and its effectiveness is evaluated on three classical domain adaptation tasks, i.e., GTA5 to Cityscapes, SYNTHIA to Cityscapes and Cityscapes to Cross-City. Large performance gains show that our method outperforms other global feature alignment based and class-wise alignment based counterparts. The code is publicly available at https://github.com/JDAI-CV/FADA.



### Adaptive Hierarchical Decomposition of Large Deep Networks
- **Arxiv ID**: http://arxiv.org/abs/2008.00809v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2008.00809v1)
- **Published**: 2020-07-17 21:04:50+00:00
- **Updated**: 2020-07-17 21:04:50+00:00
- **Authors**: Sumanth Chennupati, Sai Nooka, Shagan Sah, Raymond W Ptucha
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning has recently demonstrated its ability to rival the human brain for visual object recognition. As datasets get larger, a natural question to ask is if existing deep learning architectures can be extended to handle the 50+K classes thought to be perceptible by a typical human. Most deep learning architectures concentrate on splitting diverse categories, while ignoring the similarities amongst them. This paper introduces a framework that automatically analyzes and configures a family of smaller deep networks as a replacement to a singular, larger network. Class similarities guide the creation of a family from course to fine classifiers which solve categorical problems more effectively than a single large classifier. The resulting smaller networks are highly scalable, parallel and more practical to train, and achieve higher classification accuracy. This paper also proposes a method to adaptively select the configuration of the hierarchical family of classifiers using linkage statistics from overall and sub-classification confusion matrices. Depending on the number of classes and the complexity of the problem, a deep learning model is selected and the complexity is determined. Numerous experiments on network classes, layers, and architecture configurations validate our results.



### Unsupervised Controllable Generation with Self-Training
- **Arxiv ID**: http://arxiv.org/abs/2007.09250v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.09250v2)
- **Published**: 2020-07-17 21:50:35+00:00
- **Updated**: 2021-05-02 06:59:29+00:00
- **Authors**: Grigorios G Chrysos, Jean Kossaifi, Zhiding Yu, Anima Anandkumar
- **Comment**: Accepted in IJCNN 2021
- **Journal**: None
- **Summary**: Recent generative adversarial networks (GANs) are able to generate impressive photo-realistic images. However, controllable generation with GANs remains a challenging research problem. Achieving controllable generation requires semantically interpretable and disentangled factors of variation. It is challenging to achieve this goal using simple fixed distributions such as Gaussian distribution. Instead, we propose an unsupervised framework to learn a distribution of latent codes that control the generator through self-training. Self-training provides an iterative feedback in the GAN training, from the discriminator to the generator, and progressively improves the proposal of the latent codes as training proceeds. The latent codes are sampled from a latent variable model that is learned in the feature space of the discriminator. We consider a normalized independent component analysis model and learn its parameters through tensor factorization of the higher-order moments. Our framework exhibits better disentanglement compared to other variants such as the variational autoencoder, and is able to discover semantically meaningful latent codes without any supervision. We demonstrate empirically on both cars and faces datasets that each group of elements in the learned code controls a mode of variation with a semantic meaning, e.g. pose or background change. We also demonstrate with quantitative metrics that our method generates better results compared to other approaches.



### Domain2Vec: Domain Embedding for Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2007.09257v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.09257v1)
- **Published**: 2020-07-17 22:05:09+00:00
- **Updated**: 2020-07-17 22:05:09+00:00
- **Authors**: Xingchao Peng, Yichen Li, Kate Saenko
- **Comment**: ECCV 2020 paper
- **Journal**: None
- **Summary**: Conventional unsupervised domain adaptation (UDA) studies the knowledge transfer between a limited number of domains. This neglects the more practical scenario where data are distributed in numerous different domains in the real world. The domain similarity between those domains is critical for domain adaptation performance. To describe and learn relations between different domains, we propose a novel Domain2Vec model to provide vectorial representations of visual domains based on joint learning of feature disentanglement and Gram matrix. To evaluate the effectiveness of our Domain2Vec model, we create two large-scale cross-domain benchmarks. The first one is TinyDA, which contains 54 domains and about one million MNIST-style images. The second benchmark is DomainBank, which is collected from 56 existing vision datasets. We demonstrate that our embedding is capable of predicting domain similarities that match our intuition about visual relations between different domains. Extensive experiments are conducted to demonstrate the power of our new datasets in benchmarking state-of-the-art multi-source domain adaptation methods, as well as the advantage of our proposed model.



### Surface Normal Estimation of Tilted Images via Spatial Rectifier
- **Arxiv ID**: http://arxiv.org/abs/2007.09264v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.09264v2)
- **Published**: 2020-07-17 22:22:34+00:00
- **Updated**: 2022-07-14 17:43:20+00:00
- **Authors**: Tien Do, Khiem Vuong, Stergios I. Roumeliotis, Hyun Soo Park
- **Comment**: Appearing in the European Conference on Computer Vision 2020. This
  version fixes a typo on the L2 loss function
- **Journal**: None
- **Summary**: In this paper, we present a spatial rectifier to estimate surface normals of tilted images. Tilted images are of particular interest as more visual data are captured by arbitrarily oriented sensors such as body-/robot-mounted cameras. Existing approaches exhibit bounded performance on predicting surface normals because they were trained using gravity-aligned images. Our two main hypotheses are: (1) visual scene layout is indicative of the gravity direction; and (2) not all surfaces are equally represented by a learned estimator due to the structured distribution of the training data, thus, there exists a transformation for each tilted image that is more responsive to the learned estimator than others. We design a spatial rectifier that is learned to transform the surface normal distribution of a tilted image to the rectified one that matches the gravity-aligned training data distribution. Along with the spatial rectifier, we propose a novel truncated angular loss that offers a stronger gradient at smaller angular errors and robustness to outliers. The resulting estimator outperforms the state-of-the-art methods including data augmentation baselines not only on ScanNet and NYUv2 but also on a new dataset called Tilt-RGBD that includes considerable roll and pitch camera motion.



### Meshing Point Clouds with Predicted Intrinsic-Extrinsic Ratio Guidance
- **Arxiv ID**: http://arxiv.org/abs/2007.09267v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2007.09267v2)
- **Published**: 2020-07-17 22:36:00+00:00
- **Updated**: 2020-09-30 17:49:28+00:00
- **Authors**: Minghua Liu, Xiaoshuai Zhang, Hao Su
- **Comment**: ECCV 2020, code available
- **Journal**: None
- **Summary**: We are interested in reconstructing the mesh representation of object surfaces from point clouds. Surface reconstruction is a prerequisite for downstream applications such as rendering, collision avoidance for planning, animation, etc. However, the task is challenging if the input point cloud has a low resolution, which is common in real-world scenarios (e.g., from LiDAR or Kinect sensors). Existing learning-based mesh generative methods mostly predict the surface by first building a shape embedding that is at the whole object level, a design that causes issues in generating fine-grained details and generalizing to unseen categories. Instead, we propose to leverage the input point cloud as much as possible, by only adding connectivity information to existing points. Particularly, we predict which triplets of points should form faces. Our key innovation is a surrogate of local connectivity, calculated by comparing the intrinsic/extrinsic metrics. We learn to predict this surrogate using a deep point cloud network and then feed it to an efficient post-processing module for high-quality mesh generation. We demonstrate that our method can not only preserve details, handle ambiguous structures, but also possess strong generalizability to unseen categories by experiments on synthetic and real data. The code is available at https://github.com/Colin97/Point2Mesh.



### 3D Human Shape Reconstruction from a Polarization Image
- **Arxiv ID**: http://arxiv.org/abs/2007.09268v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.09268v1)
- **Published**: 2020-07-17 22:36:02+00:00
- **Updated**: 2020-07-17 22:36:02+00:00
- **Authors**: Shihao Zou, Xinxin Zuo, Yiming Qian, Sen Wang, Chi Xu, Minglun Gong, Li Cheng
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: This paper tackles the problem of estimating 3D body shape of clothed humans from single polarized 2D images, i.e. polarization images. Polarization images are known to be able to capture polarized reflected lights that preserve rich geometric cues of an object, which has motivated its recent applications in reconstructing surface normal of the objects of interest. Inspired by the recent advances in human shape estimation from single color images, in this paper, we attempt at estimating human body shapes by leveraging the geometric cues from single polarization images. A dedicated two-stage deep learning approach, SfP, is proposed: given a polarization image, stage one aims at inferring the fined-detailed body surface normal; stage two gears to reconstruct the 3D body shape of clothing details. Empirical evaluations on a synthetic dataset (SURREAL) as well as a real-world dataset (PHSPD) demonstrate the qualitative and quantitative performance of our approach in estimating human poses and shapes. This indicates polarization camera is a promising alternative to the more conventional color or depth imaging for human shape estimation. Further, normal maps inferred from polarization imaging play a significant role in accurately recovering the body shapes of clothed people.



### OnlineAugment: Online Data Augmentation with Less Domain Knowledge
- **Arxiv ID**: http://arxiv.org/abs/2007.09271v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.09271v2)
- **Published**: 2020-07-17 23:01:17+00:00
- **Updated**: 2020-08-22 19:54:23+00:00
- **Authors**: Zhiqiang Tang, Yunhe Gao, Leonid Karlinsky, Prasanna Sattigeri, Rogerio Feris, Dimitris Metaxas
- **Comment**: ECCV2020
- **Journal**: None
- **Summary**: Data augmentation is one of the most important tools in training modern deep neural networks. Recently, great advances have been made in searching for optimal augmentation policies in the image classification domain. However, two key points related to data augmentation remain uncovered by the current methods. First is that most if not all modern augmentation search methods are offline and learning policies are isolated from their usage. The learned policies are mostly constant throughout the training process and are not adapted to the current training model state. Second, the policies rely on class-preserving image processing functions. Hence applying current offline methods to new tasks may require domain knowledge to specify such kind of operations. In this work, we offer an orthogonal online data augmentation scheme together with three new augmentation networks, co-trained with the target learning task. It is both more efficient, in the sense that it does not require expensive offline training when entering a new domain, and more adaptive as it adapts to the learner state. Our augmentation networks require less domain knowledge and are easily applicable to new tasks. Extensive experiments demonstrate that the proposed scheme alone performs on par with the state-of-the-art offline data augmentation methods, as well as improving upon the state-of-the-art in combination with those methods. Code is available at https://github.com/zhiqiangdon/online-augment .



### On Disentangling Spoof Trace for Generic Face Anti-Spoofing
- **Arxiv ID**: http://arxiv.org/abs/2007.09273v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.09273v1)
- **Published**: 2020-07-17 23:14:16+00:00
- **Updated**: 2020-07-17 23:14:16+00:00
- **Authors**: Yaojie Liu, Joel Stehouwer, Xiaoming Liu
- **Comment**: To appear in ECCV 2020
- **Journal**: None
- **Summary**: Prior studies show that the key to face anti-spoofing lies in the subtle image pattern, termed "spoof trace", e.g., color distortion, 3D mask edge, Moire pattern, and many others. Designing a generic anti-spoofing model to estimate those spoof traces can improve not only the generalization of the spoof detection, but also the interpretability of the model's decision. Yet, this is a challenging task due to the diversity of spoof types and the lack of ground truth in spoof traces. This work designs a novel adversarial learning framework to disentangle the spoof traces from input faces as a hierarchical combination of patterns at multiple scales. With the disentangled spoof traces, we unveil the live counterpart of the original spoof face, and further synthesize realistic new spoof faces after a proper geometric correction. Our method demonstrates superior spoof detection performance on both seen and unseen spoof scenarios while providing visually convincing estimation of spoof traces. Code is available at https://github.com/yaojieliu/ECCV20-STDN.



### XingGAN for Person Image Generation
- **Arxiv ID**: http://arxiv.org/abs/2007.09278v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.09278v1)
- **Published**: 2020-07-17 23:40:22+00:00
- **Updated**: 2020-07-17 23:40:22+00:00
- **Authors**: Hao Tang, Song Bai, Li Zhang, Philip H. S. Torr, Nicu Sebe
- **Comment**: Accepted to ECCV 2020, camera ready (16 pages) + supplementary (6
  pages)
- **Journal**: None
- **Summary**: We propose a novel Generative Adversarial Network (XingGAN or CrossingGAN) for person image generation tasks, i.e., translating the pose of a given person to a desired one. The proposed Xing generator consists of two generation branches that model the person's appearance and shape information, respectively. Moreover, we propose two novel blocks to effectively transfer and update the person's shape and appearance embeddings in a crossing way to mutually improve each other, which has not been considered by any other existing GAN-based image generation work. Extensive experiments on two challenging datasets, i.e., Market-1501 and DeepFashion, demonstrate that the proposed XingGAN advances the state-of-the-art performance both in terms of objective quantitative scores and subjective visual realness. The source code and trained models are available at https://github.com/Ha0Tang/XingGAN.



