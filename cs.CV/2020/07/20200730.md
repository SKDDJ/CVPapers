# Arxiv Papers in cs.CV on 2020-07-30
### Benchmarking and Comparing Multi-exposure Image Fusion Algorithms
- **Arxiv ID**: http://arxiv.org/abs/2007.15156v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.15156v1)
- **Published**: 2020-07-30 00:19:37+00:00
- **Updated**: 2020-07-30 00:19:37+00:00
- **Authors**: Xingchen Zhang
- **Comment**: 24 pages, 5 figures, 4 tables
- **Journal**: None
- **Summary**: Multi-exposure image fusion (MEF) is an important area in computer vision and has attracted increasing interests in recent years. Apart from conventional algorithms, deep learning techniques have also been applied to multi-exposure image fusion. However, although much efforts have been made on developing MEF algorithms, the lack of benchmark makes it difficult to perform fair and comprehensive performance comparison among MEF algorithms, thus significantly hindering the development of this field. In this paper, we fill this gap by proposing a benchmark for multi-exposure image fusion (MEFB) which consists of a test set of 100 image pairs, a code library of 16 algorithms, 20 evaluation metrics, 1600 fused images and a software toolkit. To the best of our knowledge, this is the first benchmark in the field of multi-exposure image fusion. Extensive experiments have been conducted using MEFB for comprehensive performance evaluation and for identifying effective algorithms. We expect that MEFB will serve as an effective platform for researchers to compare performances and investigate MEF algorithms.



### Learning RGB-D Feature Embeddings for Unseen Object Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2007.15157v3
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.15157v3)
- **Published**: 2020-07-30 00:23:07+00:00
- **Updated**: 2021-03-03 10:45:42+00:00
- **Authors**: Yu Xiang, Christopher Xie, Arsalan Mousavian, Dieter Fox
- **Comment**: CoRL 2020
- **Journal**: None
- **Summary**: Segmenting unseen objects in cluttered scenes is an important skill that robots need to acquire in order to perform tasks in new environments. In this work, we propose a new method for unseen object instance segmentation by learning RGB-D feature embeddings from synthetic data. A metric learning loss function is utilized to learn to produce pixel-wise feature embeddings such that pixels from the same object are close to each other and pixels from different objects are separated in the embedding space. With the learned feature embeddings, a mean shift clustering algorithm can be applied to discover and segment unseen objects. We further improve the segmentation accuracy with a new two-stage clustering algorithm. Our method demonstrates that non-photorealistic synthetic RGB and depth images can be used to learn feature embeddings that transfer well to real-world images for unseen object instance segmentation.



### Rethinking Recurrent Neural Networks and Other Improvements for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2007.15161v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.15161v3)
- **Published**: 2020-07-30 00:40:50+00:00
- **Updated**: 2021-03-04 04:21:48+00:00
- **Authors**: Nguyen Huu Phong, Bernardete Ribeiro
- **Comment**: 16 pages
- **Journal**: None
- **Summary**: Over the long history of machine learning, which dates back several decades, recurrent neural networks (RNNs) have been used mainly for sequential data and time series and generally with 1D information. Even in some rare studies on 2D images, these networks are used merely to learn and generate data sequentially rather than for image recognition tasks. In this study, we propose integrating an RNN as an additional layer when designing image recognition models. We also develop end-to-end multimodel ensembles that produce expert predictions using several models. In addition, we extend the training strategy so that our model performs comparably to leading models and can even match the state-of-the-art models on several challenging datasets (e.g., SVHN (0.99), Cifar-100 (0.9027) and Cifar-10 (0.9852)). Moreover, our model sets a new record on the Surrey dataset (0.949). The source code of the methods provided in this article is available at https://github.com/leonlha/e2e-3m and http://nguyenhuuphong.me.



### An Improvement for Capsule Networks using Depthwise Separable Convolution
- **Arxiv ID**: http://arxiv.org/abs/2007.15167v1
- **DOI**: 10.1007/978-3-030-31332-6_45
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.15167v1)
- **Published**: 2020-07-30 00:58:34+00:00
- **Updated**: 2020-07-30 00:58:34+00:00
- **Authors**: Nguyen Huu Phong, Bernardete Ribeiro
- **Comment**: 6 pages
- **Journal**: IbPRIA 2019: Pattern Recognition and Image Analysis
- **Summary**: Capsule Networks face a critical problem in computer vision in the sense that the image background can challenge its performance, although they learn very well on training data. In this work, we propose to improve Capsule Networks' architecture by replacing the Standard Convolution with a Depthwise Separable Convolution. This new design significantly reduces the model's total parameters while increases stability and offers competitive accuracy. In addition, the proposed model on $64\times64$ pixel images outperforms standard models on $32\times32$ and $64\times64$ pixel images. Moreover, we empirically evaluate these models with Deep Learning architectures using state-of-the-art Transfer Learning networks such as Inception V3 and MobileNet V1. The results show that Capsule Networks perform equivalently against Deep Learning models. To the best of our knowledge, we believe that this is the first work on the integration of Depthwise Separable Convolution into Capsule Networks.



### Domain Adaptive Semantic Segmentation Using Weak Labels
- **Arxiv ID**: http://arxiv.org/abs/2007.15176v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.15176v2)
- **Published**: 2020-07-30 01:33:57+00:00
- **Updated**: 2020-08-12 10:05:48+00:00
- **Authors**: Sujoy Paul, Yi-Hsuan Tsai, Samuel Schulter, Amit K. Roy-Chowdhury, Manmohan Chandraker
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: Learning semantic segmentation models requires a huge amount of pixel-wise labeling. However, labeled data may only be available abundantly in a domain different from the desired target domain, which only has minimal or no annotations. In this work, we propose a novel framework for domain adaptation in semantic segmentation with image-level weak labels in the target domain. The weak labels may be obtained based on a model prediction for unsupervised domain adaptation (UDA), or from a human annotator in a new weakly-supervised domain adaptation (WDA) paradigm for semantic segmentation. Using weak labels is both practical and useful, since (i) collecting image-level target annotations is comparably cheap in WDA and incurs no cost in UDA, and (ii) it opens the opportunity for category-wise domain alignment. Our framework uses weak labels to enable the interplay between feature alignment and pseudo-labeling, improving both in the process of domain adaptation. Specifically, we develop a weak-label classification module to enforce the network to attend to certain categories, and then use such training signals to guide the proposed category-wise alignment method. In experiments, we show considerable improvements with respect to the existing state-of-the-arts in UDA and present a new benchmark in the WDA setting. Project page is at http://www.nec-labs.com/~mas/WeakSegDA.



### Pixel-wise Crowd Understanding via Synthetic Data
- **Arxiv ID**: http://arxiv.org/abs/2007.16032v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.16032v2)
- **Published**: 2020-07-30 02:37:51+00:00
- **Updated**: 2020-08-03 01:18:48+00:00
- **Authors**: Qi Wang, Junyu Gao, Wei Lin, Yuan Yuan
- **Comment**: Accepted by IJCV. arXiv admin note: text overlap with
  arXiv:1903.03303
- **Journal**: None
- **Summary**: Crowd analysis via computer vision techniques is an important topic in the field of video surveillance, which has wide-spread applications including crowd monitoring, public safety, space design and so on. Pixel-wise crowd understanding is the most fundamental task in crowd analysis because of its finer results for video sequences or still images than other analysis tasks. Unfortunately, pixel-level understanding needs a large amount of labeled training data. Annotating them is an expensive work, which causes that current crowd datasets are small. As a result, most algorithms suffer from over-fitting to varying degrees. In this paper, take crowd counting and segmentation as examples from the pixel-wise crowd understanding, we attempt to remedy these problems from two aspects, namely data and methodology. Firstly, we develop a free data collector and labeler to generate synthetic and labeled crowd scenes in a computer game, Grand Theft Auto V. Then we use it to construct a large-scale, diverse synthetic crowd dataset, which is named as "GCC Dataset". Secondly, we propose two simple methods to improve the performance of crowd understanding via exploiting the synthetic data. To be specific, 1) supervised crowd understanding: pre-train a crowd analysis model on the synthetic data, then fine-tune it using the real data and labels, which makes the model perform better on the real world; 2) crowd understanding via domain adaptation: translate the synthetic data to photo-realistic images, then train the model on translated data and labels. As a result, the trained model works well in real crowd scenes.



### Crowdsampling the Plenoptic Function
- **Arxiv ID**: http://arxiv.org/abs/2007.15194v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.15194v1)
- **Published**: 2020-07-30 02:52:10+00:00
- **Updated**: 2020-07-30 02:52:10+00:00
- **Authors**: Zhengqi Li, Wenqi Xian, Abe Davis, Noah Snavely
- **Comment**: ECCV, 2020 (Oral)
- **Journal**: None
- **Summary**: Many popular tourist landmarks are captured in a multitude of online, public photos. These photos represent a sparse and unstructured sampling of the plenoptic function for a particular scene. In this paper,we present a new approach to novel view synthesis under time-varying illumination from such data. Our approach builds on the recent multi-plane image (MPI) format for representing local light fields under fixed viewing conditions. We introduce a new DeepMPI representation, motivated by observations on the sparsity structure of the plenoptic function, that allows for real-time synthesis of photorealistic views that are continuous in both space and across changes in lighting. Our method can synthesize the same compelling parallax and view-dependent effects as previous MPI methods, while simultaneously interpolating along changes in reflectance and illumination with time. We show how to learn a model of these effects in an unsupervised way from an unstructured collection of photos without temporal registration, demonstrating significant improvements over recent work in neural rendering. More information can be found crowdsampling.io.



### Key Frame Proposal Network for Efficient Pose Estimation in Videos
- **Arxiv ID**: http://arxiv.org/abs/2007.15217v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.15217v1)
- **Published**: 2020-07-30 04:06:40+00:00
- **Updated**: 2020-07-30 04:06:40+00:00
- **Authors**: Yuexi Zhang, Yin Wang, Octavia Camps, Mario Sznaier
- **Comment**: Accepted by ECCV2020
- **Journal**: None
- **Summary**: Human pose estimation in video relies on local information by either estimating each frame independently or tracking poses across frames. In this paper, we propose a novel method combining local approaches with global context. We introduce a light weighted, unsupervised, key frame proposal network (K-FPN) to select informative frames and a learned dictionary to recover the entire pose sequence from these frames. The K-FPN speeds up the pose estimation and provides robustness to bad frames with occlusion, motion blur, and illumination changes, while the learned dictionary provides global dynamic context. Experiments on Penn Action and sub-JHMDB datasets show that the proposed method achieves state-of-the-art accuracy, with substantial speed-up.



### Detecting Suspicious Behavior: How to Deal with Visual Similarity through Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2007.15235v1
- **DOI**: 10.1109/ANDESCON50619.2020.9272175
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.15235v1)
- **Published**: 2020-07-30 05:13:52+00:00
- **Updated**: 2020-07-30 05:13:52+00:00
- **Authors**: Guillermo A. Martínez-Mascorro, José C. Ortiz-Bayliss, Hugo Terashima-Marín
- **Comment**: None
- **Journal**: 2020 IEEE ANDESCON
- **Summary**: Suspicious behavior is likely to threaten security, assets, life, or freedom. This behavior has no particular pattern, which complicates the tasks to detect it and define it. Even for human observers, it is complex to spot suspicious behavior in surveillance videos. Some proposals to tackle abnormal and suspicious behavior-related problems are available in the literature. However, they usually suffer from high false-positive rates due to different classes with high visual similarity. The Pre-Crime Behavior method removes information related to a crime commission to focus on suspicious behavior before the crime happens. The resulting samples from different types of crime have a high-visual similarity with normal-behavior samples. To address this problem, we implemented 3D Convolutional Neural Networks and trained them under different approaches. Also, we tested different values in the number-of-filter parameter to optimize computational resources. Finally, the comparison between the performance using different training approaches shows the best option to improve the suspicious behavior detection on surveillance videos.



### Unsupervised Event Detection, Clustering, and Use Case Exposition in Micro-PMU Measurements
- **Arxiv ID**: http://arxiv.org/abs/2007.15237v2
- **DOI**: 10.1109/TSG.2021.3063088
- **Categories**: **eess.SP**, cs.CV, cs.SY, eess.SY
- **Links**: [PDF](http://arxiv.org/pdf/2007.15237v2)
- **Published**: 2020-07-30 05:20:29+00:00
- **Updated**: 2021-01-30 21:23:52+00:00
- **Authors**: Armin Aligholian, Alireza Shahsavari, Emma Stewart, Ed Cortez, Hamed Mohsenian-Rad
- **Comment**: 8 pages, 12 figures, R1 IEEE Trans. on SmartGrid
- **Journal**: None
- **Summary**: Distribution-level phasor measurement units, a.k.a, micro-PMUs, report a large volume of high resolution phasor measurements which constitute a variety of event signatures of different phenomena that occur all across power distribution feeders. In order to implement an event-based analysis that has useful applications for the utility operator, one needs to extract these events from a large volume of micro-PMU data. However, due to the infrequent, unscheduled, and unknown nature of the events, it is often a challenge to even figure out what kind of events are out there to capture and scrutinize. In this paper, we seek to address this open problem by developing an unsupervised approach, which requires minimal prior human knowledge. First, we develop an unsupervised event detection method based on the concept of Generative Adversarial Networks (GAN). It works by training deep neural networks that learn the characteristics of the normal trends in micro-PMU measurements; and accordingly detect an event when there is any abnormality. We also propose a two-step unsupervised clustering method, based on a novel linear mixed integer programming formulation. It helps us categorize events based on their origin in the first step and their similarity in the second step. The active nature of the proposed clustering method makes it capable of identifying new clusters of events on an ongoing basis. The proposed unsupervised event detection and clustering methods are applied to real-world micro-PMU data. Results show that they can outperform the prevalent methods in the literature. These methods also facilitate our further analysis to identify important clusters of events that lead to unmasking several use cases that could be of value to the utility operator.



### Action2Motion: Conditioned Generation of 3D Human Motions
- **Arxiv ID**: http://arxiv.org/abs/2007.15240v1
- **DOI**: 10.1145/3394171.3413635
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.15240v1)
- **Published**: 2020-07-30 05:29:59+00:00
- **Updated**: 2020-07-30 05:29:59+00:00
- **Authors**: Chuan Guo, Xinxin Zuo, Sen Wang, Shihao Zou, Qingyao Sun, Annan Deng, Minglun Gong, Li Cheng
- **Comment**: 13 pages, ACM MultiMedia 2020
- **Journal**: None
- **Summary**: Action recognition is a relatively established task, where givenan input sequence of human motion, the goal is to predict its ac-tion category. This paper, on the other hand, considers a relativelynew problem, which could be thought of as an inverse of actionrecognition: given a prescribed action type, we aim to generateplausible human motion sequences in 3D. Importantly, the set ofgenerated motions are expected to maintain itsdiversityto be ableto explore the entire action-conditioned motion space; meanwhile,each sampled sequence faithfully resembles anaturalhuman bodyarticulation dynamics. Motivated by these objectives, we followthe physics law of human kinematics by adopting the Lie Algebratheory to represent thenaturalhuman motions; we also propose atemporal Variational Auto-Encoder (VAE) that encourages adiversesampling of the motion space. A new 3D human motion dataset, HumanAct12, is also constructed. Empirical experiments overthree distinct human motion datasets (including ours) demonstratethe effectiveness of our approach.



### Weakly Supervised Minirhizotron Image Segmentation with MIL-CAM
- **Arxiv ID**: http://arxiv.org/abs/2007.15243v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.15243v1)
- **Published**: 2020-07-30 05:49:30+00:00
- **Updated**: 2020-07-30 05:49:30+00:00
- **Authors**: Guohao Yu, Alina Zare, Weihuang Xu, Roser Matamala, Joel Reyes-Cabrera, Felix B. Fritschi, Thomas E. Juenger
- **Comment**: None
- **Journal**: None
- **Summary**: We present a multiple instance learning class activation map (MIL-CAM) approach for pixel-level minirhizotron image segmentation given weak image-level labels. Minirhizotrons are used to image plant roots in situ. Minirhizotron imagery is often composed of soil containing a few long and thin root objects of small diameter. The roots prove to be challenging for existing semantic image segmentation methods to discriminate. In addition to learning from weak labels, our proposed MIL-CAM approach re-weights the root versus soil pixels during analysis for improved performance due to the heavy imbalance between soil and root pixels. The proposed approach outperforms other attention map and multiple instance learning methods for localization of root objects in minirhizotron imagery.



### Hierarchical Action Classification with Network Pruning
- **Arxiv ID**: http://arxiv.org/abs/2007.15244v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.15244v1)
- **Published**: 2020-07-30 05:49:42+00:00
- **Updated**: 2020-07-30 05:49:42+00:00
- **Authors**: Mahdi Davoodikakhki, KangKang Yin
- **Comment**: None
- **Journal**: None
- **Summary**: Research on human action classification has made significant progresses in the past few years. Most deep learning methods focus on improving performance by adding more network components. We propose, however, to better utilize auxiliary mechanisms, including hierarchical classification, network pruning, and skeleton-based preprocessing, to boost the model robustness and performance. We test the effectiveness of our method on four commonly used testing datasets: NTU RGB+D 60, NTU RGB+D 120, Northwestern-UCLA Multiview Action 3D, and UTD Multimodal Human Action Dataset. Our experiments show that our method can achieve either comparable or better performance on all four datasets. In particular, our method sets up a new baseline for NTU 120, the largest dataset among the four. We also analyze our method with extensive comparisons and ablation studies.



### DeepPeep: Exploiting Design Ramifications to Decipher the Architecture of Compact DNNs
- **Arxiv ID**: http://arxiv.org/abs/2007.15248v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML, K.4.1; K.4.4
- **Links**: [PDF](http://arxiv.org/pdf/2007.15248v1)
- **Published**: 2020-07-30 06:01:41+00:00
- **Updated**: 2020-07-30 06:01:41+00:00
- **Authors**: Nandan Kumar Jha, Sparsh Mittal, Binod Kumar, Govardhan Mattela
- **Comment**: Accepted at The ACM Journal on Emerging Technologies in Computing
  Systems (JETC), 2020. 25 pages, 11 tables, and 11 figures
- **Journal**: None
- **Summary**: The remarkable predictive performance of deep neural networks (DNNs) has led to their adoption in service domains of unprecedented scale and scope. However, the widespread adoption and growing commercialization of DNNs have underscored the importance of intellectual property (IP) protection. Devising techniques to ensure IP protection has become necessary due to the increasing trend of outsourcing the DNN computations on the untrusted accelerators in cloud-based services. The design methodologies and hyper-parameters of DNNs are crucial information, and leaking them may cause massive economic loss to the organization. Furthermore, the knowledge of DNN's architecture can increase the success probability of an adversarial attack where an adversary perturbs the inputs and alter the prediction.   In this work, we devise a two-stage attack methodology "DeepPeep" which exploits the distinctive characteristics of design methodologies to reverse-engineer the architecture of building blocks in compact DNNs. We show the efficacy of "DeepPeep" on P100 and P4000 GPUs. Additionally, we propose intelligent design maneuvering strategies for thwarting IP theft through the DeepPeep attack and proposed "Secure MobileNet-V1". Interestingly, compared to vanilla MobileNet-V1, secure MobileNet-V1 provides a significant reduction in inference latency ($\approx$60%) and improvement in predictive performance ($\approx$2%) with very-low memory and computation overheads.



### Instance Selection for GANs
- **Arxiv ID**: http://arxiv.org/abs/2007.15255v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.15255v2)
- **Published**: 2020-07-30 06:33:51+00:00
- **Updated**: 2020-10-23 04:43:07+00:00
- **Authors**: Terrance DeVries, Michal Drozdzal, Graham W. Taylor
- **Comment**: Accepted to NeurIPS 2020
- **Journal**: None
- **Summary**: Recent advances in Generative Adversarial Networks (GANs) have led to their widespread adoption for the purposes of generating high quality synthetic imagery. While capable of generating photo-realistic images, these models often produce unrealistic samples which fall outside of the data manifold. Several recently proposed techniques attempt to avoid spurious samples, either by rejecting them after generation, or by truncating the model's latent space. While effective, these methods are inefficient, as a large fraction of training time and model capacity are dedicated towards samples that will ultimately go unused. In this work we propose a novel approach to improve sample quality: altering the training dataset via instance selection before model training has taken place. By refining the empirical data distribution before training, we redirect model capacity towards high-density regions, which ultimately improves sample fidelity, lowers model capacity requirements, and significantly reduces training time. Code is available at https://github.com/uoguelph-mlrg/instance_selection_for_gans.



### FaultFace: Deep Convolutional Generative Adversarial Network (DCGAN) based Ball-Bearing Failure Detection Method
- **Arxiv ID**: http://arxiv.org/abs/2008.00930v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2008.00930v1)
- **Published**: 2020-07-30 06:37:53+00:00
- **Updated**: 2020-07-30 06:37:53+00:00
- **Authors**: Jairo Viola, YangQuan Chen, Jing Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Failure detection is employed in the industry to improve system performance and reduce costs due to unexpected malfunction events. So, a good dataset of the system is desirable for designing an automated failure detection system. However, industrial process datasets are unbalanced and contain little information about failure behavior due to the uniqueness of these events and the high cost for running the system just to get information about the undesired behaviors. For this reason, performing correct training and validation of automated failure detection methods is challenging. This paper proposes a methodology called FaultFace for failure detection on Ball-Bearing joints for rotational shafts using deep learning techniques to create balanced datasets. The FaultFace methodology uses 2D representations of vibration signals denominated faceportraits obtained by time-frequency transformation techniques. From the obtained faceportraits, a Deep Convolutional Generative Adversarial Network is employed to produce new faceportraits of the nominal and failure behaviors to get a balanced dataset. A Convolutional Neural Network is trained for fault detection employing the balanced dataset. The FaultFace methodology is compared with other deep learning techniques to evaluate its performance in for fault detection with unbalanced datasets. Obtained results show that FaultFace methodology has a good performance for failure detection for unbalanced datasets.



### Weakly-Supervised Cell Tracking via Backward-and-Forward Propagation
- **Arxiv ID**: http://arxiv.org/abs/2007.15258v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.15258v1)
- **Published**: 2020-07-30 06:41:22+00:00
- **Updated**: 2020-07-30 06:41:22+00:00
- **Authors**: Kazuya Nishimura, Junya Hayashida, Chenyang Wang, Dai Fei Elmer Ker, Ryoma Bise
- **Comment**: 17 pages, figures, Accepted in ECCV 2020
- **Journal**: None
- **Summary**: We propose a weakly-supervised cell tracking method that can train a convolutional neural network (CNN) by using only the annotation of "cell detection" (i.e., the coordinates of cell positions) without association information, in which cell positions can be easily obtained by nuclear staining. First, we train co-detection CNN that detects cells in successive frames by using weak-labels. Our key assumption is that co-detection CNN implicitly learns association in addition to detection. To obtain the association, we propose a backward-and-forward propagation method that analyzes the correspondence of cell positions in the outputs of co-detection CNN. Experiments demonstrated that the proposed method can associate cells by analyzing co-detection CNN. Even though the method uses only weak supervision, the performance of our method was almost the same as the state-of-the-art supervised method. Code is publicly available in https://github.com/naivete5656/WSCTBFP



### The Blessing and the Curse of the Noise behind Facial Landmark Annotations
- **Arxiv ID**: http://arxiv.org/abs/2007.15269v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2007.15269v1)
- **Published**: 2020-07-30 07:13:45+00:00
- **Updated**: 2020-07-30 07:13:45+00:00
- **Authors**: Xiaoyu Xiang, Yang Cheng, Shaoyuan Xu, Qian Lin, Jan Allebach
- **Comment**: 10 pages, 10 figures, accepted to Electronic Imaging Symposium 2020
- **Journal**: None
- **Summary**: The evolving algorithms for 2D facial landmark detection empower people to recognize faces, analyze facial expressions, etc. However, existing methods still encounter problems of unstable facial landmarks when applied to videos. Because previous research shows that the instability of facial landmarks is caused by the inconsistency of labeling quality among the public datasets, we want to have a better understanding of the influence of annotation noise in them. In this paper, we make the following contributions: 1) we propose two metrics that quantitatively measure the stability of detected facial landmarks, 2) we model the annotation noise in an existing public dataset, 3) we investigate the influence of different types of noise in training face alignment neural networks, and propose corresponding solutions. Our results demonstrate improvements in both accuracy and stability of detected facial landmarks.



### Dynamic texture analysis for detecting fake faces in video sequences
- **Arxiv ID**: http://arxiv.org/abs/2007.15271v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2007.15271v1)
- **Published**: 2020-07-30 07:21:24+00:00
- **Updated**: 2020-07-30 07:21:24+00:00
- **Authors**: Mattia Bonomi, Cecilia Pasquini, Giulia Boato
- **Comment**: None
- **Journal**: None
- **Summary**: The creation of manipulated multimedia content involving human characters has reached in the last years unprecedented realism, calling for automated techniques to expose synthetically generated faces in images and videos. This work explores the analysis of spatio-temporal texture dynamics of the video signal, with the goal of characterizing and distinguishing real and fake sequences. We propose to build a binary decision on the joint analysis of multiple temporal segments and, in contrast to previous approaches, to exploit the textural dynamics of both the spatial and temporal dimensions. This is achieved through the use of Local Derivative Patterns on Three Orthogonal Planes (LDP-TOP), a compact feature representation known to be an important asset for the detection of face spoofing attacks. Experimental analyses on state-of-the-art datasets of manipulated videos show the discriminative power of such descriptors in separating real and fake sequences, and also identifying the creation method used. Linear Support Vector Machines (SVMs) are used which, despite the lower complexity, yield comparable performance to previously proposed deep models for fake content detection.



### Searching Collaborative Agents for Multi-plane Localization in 3D Ultrasound
- **Arxiv ID**: http://arxiv.org/abs/2007.15273v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2007.15273v1)
- **Published**: 2020-07-30 07:23:55+00:00
- **Updated**: 2020-07-30 07:23:55+00:00
- **Authors**: Yuhao Huang, Xin Yang, Rui Li, Jikuan Qian, Xiaoqiong Huang, Wenlong Shi, Haoran Dou, Chaoyu Chen, Yuanji Zhang, Huanjia Luo, Alejandro Frangi, Yi Xiong, Dong Ni
- **Comment**: Early accepted by MICCAI 2020
- **Journal**: None
- **Summary**: 3D ultrasound (US) is widely used due to its rich diagnostic information, portability and low cost. Automated standard plane (SP) localization in US volume not only improves efficiency and reduces user-dependence, but also boosts 3D US interpretation. In this study, we propose a novel Multi-Agent Reinforcement Learning (MARL) framework to localize multiple uterine SPs in 3D US simultaneously. Our contribution is two-fold. First, we equip the MARL with a one-shot neural architecture search (NAS) module to obtain the optimal agent for each plane. Specifically, Gradient-based search using Differentiable Architecture Sampler (GDAS) is employed to accelerate and stabilize the training process. Second, we propose a novel collaborative strategy to strengthen agents' communication. Our strategy uses recurrent neural network (RNN) to learn the spatial relationship among SPs effectively. Extensively validated on a large dataset, our approach achieves the accuracy of 7.05 degree/2.21mm, 8.62 degree/2.36mm and 5.93 degree/0.89mm for the mid-sagittal, transverse and coronal plane localization, respectively. The proposed MARL framework can significantly increase the plane localization accuracy and reduce the computational cost and model size.



### Black-box Adversarial Sample Generation Based on Differential Evolution
- **Arxiv ID**: http://arxiv.org/abs/2007.15310v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.15310v1)
- **Published**: 2020-07-30 08:43:45+00:00
- **Updated**: 2020-07-30 08:43:45+00:00
- **Authors**: Junyu Lin, Lei Xu, Yingqi Liu, Xiangyu Zhang
- **Comment**: 29 pages, 8 figures
- **Journal**: None
- **Summary**: Deep Neural Networks (DNNs) are being used in various daily tasks such as object detection, speech processing, and machine translation. However, it is known that DNNs suffer from robustness problems -- perturbed inputs called adversarial samples leading to misbehaviors of DNNs. In this paper, we propose a black-box technique called Black-box Momentum Iterative Fast Gradient Sign Method (BMI-FGSM) to test the robustness of DNN models. The technique does not require any knowledge of the structure or weights of the target DNN. Compared to existing white-box testing techniques that require accessing model internal information such as gradients, our technique approximates gradients through Differential Evolution and uses approximated gradients to construct adversarial samples. Experimental results show that our technique can achieve 100% success in generating adversarial samples to trigger misclassification, and over 95% success in generating samples to trigger misclassification to a specific target output label. It also demonstrates better perturbation distance and better transferability. Compared to the state-of-the-art black-box technique, our technique is more efficient. Furthermore, we conduct testing on the commercial Aliyun API and successfully trigger its misbehavior within a limited number of queries, demonstrating the feasibility of real-world black-box attack.



### Infrastructure-based Multi-Camera Calibration using Radial Projections
- **Arxiv ID**: http://arxiv.org/abs/2007.15330v2
- **DOI**: 10.1007/978-3-030-58517-4_20
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2007.15330v2)
- **Published**: 2020-07-30 09:21:04+00:00
- **Updated**: 2020-09-16 14:23:51+00:00
- **Authors**: Yukai Lin, Viktor Larsson, Marcel Geppert, Zuzana Kukelova, Marc Pollefeys, Torsten Sattler
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: Multi-camera systems are an important sensor platform for intelligent systems such as self-driving cars. Pattern-based calibration techniques can be used to calibrate the intrinsics of the cameras individually. However, extrinsic calibration of systems with little to no visual overlap between the cameras is a challenge. Given the camera intrinsics, infrastucture-based calibration techniques are able to estimate the extrinsics using 3D maps pre-built via SLAM or Structure-from-Motion. In this paper, we propose to fully calibrate a multi-camera system from scratch using an infrastructure-based approach. Assuming that the distortion is mainly radial, we introduce a two-stage approach. We first estimate the camera-rig extrinsics up to a single unknown translation component per camera. Next, we solve for both the intrinsic parameters and the missing translation components. Extensive experiments on multiple indoor and outdoor scenes with multiple multi-camera systems show that our calibration method achieves high accuracy and robustness. In particular, our approach is more robust than the naive approach of first estimating intrinsic parameters and pose per camera before refining the extrinsic parameters of the system. The implementation is available at https://github.com/youkely/InfrasCal.



### NormalGAN: Learning Detailed 3D Human from a Single RGB-D Image
- **Arxiv ID**: http://arxiv.org/abs/2007.15340v1
- **DOI**: None
- **Categories**: **cs.CV**, 68T07, I.4.5
- **Links**: [PDF](http://arxiv.org/pdf/2007.15340v1)
- **Published**: 2020-07-30 09:35:46+00:00
- **Updated**: 2020-07-30 09:35:46+00:00
- **Authors**: Lizhen Wang, Xiaochen Zhao, Tao Yu, Songtao Wang, Yebin Liu
- **Comment**: 10 pages, 11 figures, ECCV 2020
- **Journal**: None
- **Summary**: We propose NormalGAN, a fast adversarial learning-based method to reconstruct the complete and detailed 3D human from a single RGB-D image. Given a single front-view RGB-D image, NormalGAN performs two steps: front-view RGB-D rectification and back-view RGBD inference. The final model was then generated by simply combining the front-view and back-view RGB-D information. However, inferring backview RGB-D image with high-quality geometric details and plausible texture is not trivial. Our key observation is: Normal maps generally encode much more information of 3D surface details than RGB and depth images. Therefore, learning geometric details from normal maps is superior than other representations. In NormalGAN, an adversarial learning framework conditioned by normal maps is introduced, which is used to not only improve the front-view depth denoising performance, but also infer the back-view depth image with surprisingly geometric details. Moreover, for texture recovery, we remove shading information from the front-view RGB image based on the refined normal map, which further improves the quality of the back-view color inference. Results and experiments on both testing data set and real captured data demonstrate the superior performance of our approach. Given a consumer RGB-D sensor, NormalGAN can generate the complete and detailed 3D human reconstruction results in 20 fps, which further enables convenient interactive experiences in telepresence, AR/VR and gaming scenarios.



### Label or Message: A Large-Scale Experimental Survey of Texts and Objects Co-Occurrence
- **Arxiv ID**: http://arxiv.org/abs/2007.15381v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.15381v1)
- **Published**: 2020-07-30 11:18:10+00:00
- **Updated**: 2020-07-30 11:18:10+00:00
- **Authors**: Koki Takeshita, Juntaro Shioyama, Seiichi Uchida
- **Comment**: Accepted at ICPR2020
- **Journal**: None
- **Summary**: Our daily life is surrounded by textual information. Nowadays, the automatic collection of textual information becomes possible owing to the drastic improvement of scene text detectors and recognizer. The purpose of this paper is to conduct a large-scale survey of co-occurrence between visual objects (such as book and car) and scene texts with a large image dataset and a state-of-the-art scene text detector and recognizer. Especially, we focus on the function of "label" texts, which are attached to objects for detailing the objects. By analyzing co-occurrence between objects and scene texts, it is possible to observe the statistics about the label texts and understand how the scene texts will be useful for recognizing the objects and vice versa.



### Very Deep Super-Resolution of Remotely Sensed Images with Mean Square Error and Var-norm Estimators as Loss Functions
- **Arxiv ID**: http://arxiv.org/abs/2007.15417v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.15417v1)
- **Published**: 2020-07-30 12:26:38+00:00
- **Updated**: 2020-07-30 12:26:38+00:00
- **Authors**: Antigoni Panagiotopoulou, Lazaros Grammatikopoulos, Eleni Charou, Emmanuel Bratsolis, Nicholas Madamopoulos, John Petrogonas
- **Comment**: 19 pages, 8 figures
- **Journal**: None
- **Summary**: In this work, very deep super-resolution (VDSR) method is presented for improving the spatial resolution of remotely sensed (RS) images for scale factor 4. The VDSR net is re-trained with Sentinel-2 images and with drone aero orthophoto images, thus becomes RS-VDSR and Aero-VDSR, respectively. A novel loss function, the Var-norm estimator, is proposed in the regression layer of the convolutional neural network during re-training and prediction. According to numerical and optical comparisons, the proposed nets RS-VDSR and Aero-VDSR can outperform VDSR during prediction with RS images. RS-VDSR outperforms VDSR up to 3.16 dB in terms of PSNR in Sentinel-2 images.



### Few shot domain adaptation for in situ macromolecule structural classification in cryo-electron tomograms
- **Arxiv ID**: http://arxiv.org/abs/2007.15422v1
- **DOI**: 10.1093/bioinformatics/btaa671
- **Categories**: **q-bio.QM**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.15422v1)
- **Published**: 2020-07-30 12:39:21+00:00
- **Updated**: 2020-07-30 12:39:21+00:00
- **Authors**: Liangyong Yu, Ran Li, Xiangrui Zeng, Hongyi Wang, Jie Jin, Ge Yang, Rui Jiang, Min Xu
- **Comment**: This article has been accepted for publication in Bioinformatics
  Published by Oxford University Press
- **Journal**: Bioinformatics 2020
- **Summary**: Motivation: Cryo-Electron Tomography (cryo-ET) visualizes structure and spatial organization of macromolecules and their interactions with other subcellular components inside single cells in the close-to-native state at sub-molecular resolution. Such information is critical for the accurate understanding of cellular processes. However, subtomogram classification remains one of the major challenges for the systematic recognition and recovery of the macromolecule structures in cryo-ET because of imaging limits and data quantity. Recently, deep learning has significantly improved the throughput and accuracy of large-scale subtomogram classification. However often it is difficult to get enough high-quality annotated subtomogram data for supervised training due to the enormous expense of labeling. To tackle this problem, it is beneficial to utilize another already annotated dataset to assist the training process. However, due to the discrepancy of image intensity distribution between source domain and target domain, the model trained on subtomograms in source domainmay perform poorly in predicting subtomogram classes in the target domain.   Results: In this paper, we adapt a few shot domain adaptation method for deep learning based cross-domain subtomogram classification. The essential idea of our method consists of two parts: 1) take full advantage of the distribution of plentiful unlabeled target domain data, and 2) exploit the correlation between the whole source domain dataset and few labeled target domain data. Experiments conducted on simulated and real datasets show that our method achieves significant improvement on cross domain subtomogram classification compared with baseline methods.



### Searching for Pneumothorax in Half a Million Chest X-Ray Images
- **Arxiv ID**: http://arxiv.org/abs/2007.15429v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.15429v1)
- **Published**: 2020-07-30 13:03:52+00:00
- **Updated**: 2020-07-30 13:03:52+00:00
- **Authors**: Antonio Sze-To, Hamid Tizhoosh
- **Comment**: AIME 2020 International Conference on AI in Medicine, USA
- **Journal**: None
- **Summary**: Pneumothorax, a collapsed or dropped lung, is a fatal condition typically detected on a chest X-ray by an experienced radiologist. Due to shortage of such experts, automated detection systems based on deep neural networks have been developed. Nevertheless, applying such systems in practice remains a challenge. These systems, mostly compute a single probability as output, may not be enough for diagnosis. On the contrary, content-based medical image retrieval (CBIR) systems, such as image search, can assist clinicians for diagnostic purposes by enabling them to compare the case they are examining with previous (already diagnosed) cases. However, there is a lack of study on such attempt. In this study, we explored the use of image search to classify pneumothorax among chest X-ray images. All chest X-ray images were first tagged with deep pretrained features, which were obtained from existing deep learning models. Given a query chest X-ray image, the majority voting of the top K retrieved images was then used as a classifier, in which similar cases in the archive of past cases are provided besides the probability output. In our experiments, 551,383 chest X-ray images were obtained from three large recently released public datasets. Using 10-fold cross-validation, it is shown that image search on deep pretrained features achieved promising results compared to those obtained by traditional classifiers trained on the same features. To the best of knowledge, it is the first study to demonstrate that deep pretrained features can be used for CBIR of pneumothorax in half a million chest X-ray images.



### flexgrid2vec: Learning Efficient Visual Representations Vectors
- **Arxiv ID**: http://arxiv.org/abs/2007.15444v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.15444v6)
- **Published**: 2020-07-30 13:21:00+00:00
- **Updated**: 2021-09-29 09:34:42+00:00
- **Authors**: Ali Hamdi, Du Yong Kim, Flora D. Salim
- **Comment**: 13 pages
- **Journal**: None
- **Summary**: We propose flexgrid2vec, a novel approach for image representation learning. Existing visual representation methods suffer from several issues, including the need for highly intensive computation, the risk of losing in-depth structural information and the specificity of the method to certain shapes or objects. flexgrid2vec converts an image to a low-dimensional feature vector. We represent each image with a graph of flexible, unique node locations and edge distances. flexgrid2vec is a multi-channel GCN that learns features of the most representative image patches. We have investigated both spectral and non-spectral implementations of the GCN node-embedding. Specifically, we have implemented flexgrid2vec based on different node-aggregation methods, such as vector summation, concatenation and normalisation with eigenvector centrality. We compare the performance of flexgrid2vec with a set of state-of-the-art visual representation learning models on binary and multi-class image classification tasks. Although we utilise imbalanced, low-size and low-resolution datasets, flexgrid2vec shows stable and outstanding results against well-known base classifiers. flexgrid2vec achieves 96.23% on CIFAR-10, 83.05% on CIFAR-100, 94.50% on STL-10, 98.8% on ASIRRA and 89.69% on the COCO dataset.



### Learning from Few Samples: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2007.15484v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.15484v1)
- **Published**: 2020-07-30 14:28:57+00:00
- **Updated**: 2020-07-30 14:28:57+00:00
- **Authors**: Nihar Bendre, Hugo Terashima Marín, Peyman Najafirad
- **Comment**: 17 pages, 10 figures
- **Journal**: None
- **Summary**: Deep neural networks have been able to outperform humans in some cases like image recognition and image classification. However, with the emergence of various novel categories, the ability to continuously widen the learning capability of such networks from limited samples, still remains a challenge. Techniques like Meta-Learning and/or few-shot learning showed promising results, where they can learn or generalize to a novel category/task based on prior knowledge. In this paper, we perform a study of the existing few-shot meta-learning techniques in the computer vision domain based on their method and evaluation metrics. We provide a taxonomy for the techniques and categorize them as data-augmentation, embedding, optimization and semantics based learning for few-shot, one-shot and zero-shot settings. We then describe the seminal work done in each category and discuss their approach towards solving the predicament of learning from few samples. Lastly we provide a comparison of these techniques on the commonly used benchmark datasets: Omniglot, and MiniImagenet, along with a discussion towards the future direction of improving the performance of these techniques towards the final goal of outperforming humans.



### Revisiting the Modifiable Areal Unit Problem in Deep Traffic Prediction with Visual Analytics
- **Arxiv ID**: http://arxiv.org/abs/2007.15486v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2007.15486v3)
- **Published**: 2020-07-30 14:32:17+00:00
- **Updated**: 2020-09-07 14:20:33+00:00
- **Authors**: Wei Zeng, Chengqiao Lin, Juncong Lin, Jincheng Jiang, Jiazhi Xia, Cagatay Turkay, Wei Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning methods are being increasingly used for urban traffic prediction where spatiotemporal traffic data is aggregated into sequentially organized matrices that are then fed into convolution-based residual neural networks. However, the widely known modifiable areal unit problem within such aggregation processes can lead to perturbations in the network inputs. This issue can significantly destabilize the feature embeddings and the predictions, rendering deep networks much less useful for the experts. This paper approaches this challenge by leveraging unit visualization techniques that enable the investigation of many-to-many relationships between dynamically varied multi-scalar aggregations of urban traffic data and neural network predictions. Through regular exchanges with a domain expert, we design and develop a visual analytics solution that integrates 1) a Bivariate Map equipped with an advanced bivariate colormap to simultaneously depict input traffic and prediction errors across space, 2) a Morans I Scatterplot that provides local indicators of spatial association analysis, and 3) a Multi-scale Attribution View that arranges non-linear dot plots in a tree layout to promote model analysis and comparison across scales. We evaluate our approach through a series of case studies involving a real-world dataset of Shenzhen taxi trips, and through interviews with domain experts. We observe that geographical scale variations have important impact on prediction performances, and interactive visual exploration of dynamically varying inputs and outputs benefit experts in the development of deep traffic prediction models.



### Cascaded Non-local Neural Network for Point Cloud Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2007.15488v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.15488v1)
- **Published**: 2020-07-30 14:34:43+00:00
- **Updated**: 2020-07-30 14:34:43+00:00
- **Authors**: Mingmei Cheng, Le Hui, Jin Xie, Jian Yang, Hui Kong
- **Comment**: Accepted by IEEE/RSJ International Conference on Intelligent Robots
  and Systems 2020 (IROS)
- **Journal**: None
- **Summary**: In this paper, we propose a cascaded non-local neural network for point cloud segmentation. The proposed network aims to build the long-range dependencies of point clouds for the accurate segmentation. Specifically, we develop a novel cascaded non-local module, which consists of the neighborhood-level, superpoint-level and global-level non-local blocks. First, in the neighborhood-level block, we extract the local features of the centroid points of point clouds by assigning different weights to the neighboring points. The extracted local features of the centroid points are then used to encode the superpoint-level block with the non-local operation. Finally, the global-level block aggregates the non-local features of the superpoints for semantic segmentation in an encoder-decoder framework. Benefiting from the cascaded structure, geometric structure information of different neighborhoods with the same label can be propagated. In addition, the cascaded structure can largely reduce the computational cost of the original non-local operation on point clouds. Experiments on different indoor and outdoor datasets show that our method achieves state-of-the-art performance and effectively reduces the time consumption and memory occupation.



### SimPose: Effectively Learning DensePose and Surface Normals of People from Simulated Data
- **Arxiv ID**: http://arxiv.org/abs/2007.15506v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.15506v1)
- **Published**: 2020-07-30 14:59:38+00:00
- **Updated**: 2020-07-30 14:59:38+00:00
- **Authors**: Tyler Zhu, Per Karlsson, Christoph Bregler
- **Comment**: To appear in the Proceedings of ECCV 2020
- **Journal**: None
- **Summary**: With a proliferation of generic domain-adaptation approaches, we report a simple yet effective technique for learning difficult per-pixel 2.5D and 3D regression representations of articulated people. We obtained strong sim-to-real domain generalization for the 2.5D DensePose estimation task and the 3D human surface normal estimation task. On the multi-person DensePose MSCOCO benchmark, our approach outperforms the state-of-the-art methods which are trained on real images that are densely labelled. This is an important result since obtaining human manifold's intrinsic uv coordinates on real images is time consuming and prone to labeling noise. Additionally, we present our model's 3D surface normal predictions on the MSCOCO dataset that lacks any real 3D surface normal labels. The key to our approach is to mitigate the "Inter-domain Covariate Shift" with a carefully selected training batch from a mixture of domain samples, a deep batch-normalized residual network, and a modified multi-task learning objective. Our approach is complementary to existing domain-adaptation techniques and can be applied to other dense per-pixel pose estimation problems.



### A new Local Radon Descriptor for Content-Based Image Search
- **Arxiv ID**: http://arxiv.org/abs/2007.15523v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.15523v1)
- **Published**: 2020-07-30 15:22:57+00:00
- **Updated**: 2020-07-30 15:22:57+00:00
- **Authors**: Morteza Babaie, Hany Kashani, Meghana D. Kumar, Hamid. R. Tizhoosh
- **Comment**: {To appear in International Conference on AI in Medicine (AIME 2020),
  University of Minnesota, USA
- **Journal**: None
- **Summary**: Content-based image retrieval (CBIR) is an essential part of computer vision research, especially in medical expert systems. Having a discriminative image descriptor with the least number of parameters for tuning is desirable in CBIR systems. In this paper, we introduce a new simple descriptor based on the histogram of local Radon projections. We also propose a very fast convolution-based local Radon estimator to overcome the slow process of Radon projections. We performed our experiments using pathology images (KimiaPath24) and lung CT patches and test our proposed solution for medical image processing. We achieved superior results compared with other histogram-based descriptors such as LBP and HoG as well as some pre-trained CNNs.



### Epipolar-Guided Deep Object Matching for Scene Change Detection
- **Arxiv ID**: http://arxiv.org/abs/2007.15540v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.15540v1)
- **Published**: 2020-07-30 15:48:40+00:00
- **Updated**: 2020-07-30 15:48:40+00:00
- **Authors**: Kento Doi, Ryuhei Hamaguchi, Shun Iwase, Rio Yokota, Yutaka Matsuo, Ken Sakurada
- **Comment**: 8 pages, 4 figures
- **Journal**: None
- **Summary**: This paper describes a viewpoint-robust object-based change detection network (OBJ-CDNet). Mobile cameras such as drive recorders capture images from different viewpoints each time due to differences in camera trajectory and shutter timing. However, previous methods for pixel-wise change detection are vulnerable to the viewpoint differences because they assume aligned image pairs as inputs. To cope with the difficulty, we introduce a deep graph matching network that establishes object correspondence between an image pair. The introduction enables us to detect object-wise scene changes without precise image alignment. For more accurate object matching, we propose an epipolar-guided deep graph matching network (EGMNet), which incorporates the epipolar constraint into the deep graph matching layer used in OBJCDNet. To evaluate our network's robustness against viewpoint differences, we created synthetic and real datasets for scene change detection from an image pair. The experimental results verified the effectiveness of our network.



### Event-based Stereo Visual Odometry
- **Arxiv ID**: http://arxiv.org/abs/2007.15548v2
- **DOI**: 10.1109/TRO.2021.3062252
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2007.15548v2)
- **Published**: 2020-07-30 15:53:28+00:00
- **Updated**: 2021-02-22 14:52:21+00:00
- **Authors**: Yi Zhou, Guillermo Gallego, Shaojie Shen
- **Comment**: 18 pages, 18 figures, 7 tables
- **Journal**: IEEE Transactions on Robotics (T-RO), Vol. 37, No. 5, pp.
  1433-1450, Oct. 2021
- **Summary**: Event-based cameras are bio-inspired vision sensors whose pixels work independently from each other and respond asynchronously to brightness changes, with microsecond resolution. Their advantages make it possible to tackle challenging scenarios in robotics, such as high-speed and high dynamic range scenes. We present a solution to the problem of visual odometry from the data acquired by a stereo event-based camera rig. Our system follows a parallel tracking-and-mapping approach, where novel solutions to each subproblem (3D reconstruction and camera pose estimation) are developed with two objectives in mind: being principled and efficient, for real-time operation with commodity hardware. To this end, we seek to maximize the spatio-temporal consistency of stereo event-based data while using a simple and efficient representation. Specifically, the mapping module builds a semi-dense 3D map of the scene by fusing depth estimates from multiple local viewpoints (obtained by spatio-temporal consistency) in a probabilistic fashion. The tracking module recovers the pose of the stereo rig by solving a registration problem that naturally arises due to the chosen map and event data representation. Experiments on publicly available datasets and on our own recordings demonstrate the versatility of the proposed method in natural scenes with general 6-DoF motion. The system successfully leverages the advantages of event-based cameras to perform visual odometry in challenging illumination conditions, such as low-light and high dynamic range, while running in real-time on a standard CPU. We release the software and dataset under an open source licence to foster research in the emerging topic of event-based SLAM.



### Quantitative Distortion Analysis of Flattening Applied to the Scroll from En-Gedi
- **Arxiv ID**: http://arxiv.org/abs/2007.15551v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.15551v1)
- **Published**: 2020-07-30 15:55:50+00:00
- **Updated**: 2020-07-30 15:55:50+00:00
- **Authors**: Clifford Seth Parker, William Brent Seales, Pnina Shor
- **Comment**: 10 pages, 11 figures. In: Art & Archaeology, 2nd International
  Conference. 2016
- **Journal**: None
- **Summary**: Non-invasive volumetric imaging can now capture the internal structure and detailed evidence of ink-based writing from within the confines of damaged and deteriorated manuscripts that cannot be physically opened. As demonstrated recently on the En-Gedi scroll, our "virtual unwrapping" software pipeline enables the recovery of substantial ink-based text from damaged artifacts at a quality high enough for serious critical textual analysis. However, the quality of the resulting images is defined by the subjective evaluation of scholars, and a choice of specific algorithms and parameters must be available at each stage in the pipeline in order to maximize the output quality.



### Unsupervised Disentanglement GAN for Domain Adaptive Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2007.15560v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.15560v1)
- **Published**: 2020-07-30 16:07:05+00:00
- **Updated**: 2020-07-30 16:07:05+00:00
- **Authors**: Yacine Khraimeche, Guillaume-Alexandre Bilodeau, David Steele, Harshad Mahadik
- **Comment**: 8 pages, 5 figures, submitted to ICPR 2020
- **Journal**: None
- **Summary**: While recent person re-identification (ReID) methods achieve high accuracy in a supervised setting, their generalization to an unlabelled domain is still an open problem. In this paper, we introduce a novel unsupervised disentanglement generative adversarial network (UD-GAN) to address the domain adaptation issue of supervised person ReID. Our framework jointly trains a ReID network for discriminative features extraction in a source labelled domain using identity annotation, and adapts the ReID model to an unlabelled target domain by learning disentangled latent representations on the domain. Identity-unrelated features in the target domain are distilled from the latent features. As a result, the ReID features better encompass the identity of a person in the unsupervised domain. We conducted experiments on the Market1501, DukeMTMC and MSMT17 datasets. Results show that the unsupervised domain adaptation problem in ReID is very challenging. Nevertheless, our method shows improvement in half of the domain transfers and achieve state-of-the-art performance for one of them.



### Dense Scene Multiple Object Tracking with Box-Plane Matching
- **Arxiv ID**: http://arxiv.org/abs/2007.15576v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.15576v2)
- **Published**: 2020-07-30 16:39:22+00:00
- **Updated**: 2021-04-01 14:09:07+00:00
- **Authors**: Jinlong Peng, Yueyang Gu, Yabiao Wang, Chengjie Wang, Jilin Li, Feiyue Huang
- **Comment**: ACM Multimedia 2020 GC paper. ACM Multimedia Grand Challenge HiEve
  2020 Track-1 Winner
- **Journal**: None
- **Summary**: Multiple Object Tracking (MOT) is an important task in computer vision. MOT is still challenging due to the occlusion problem, especially in dense scenes. Following the tracking-by-detection framework, we propose the Box-Plane Matching (BPM) method to improve the MOT performacne in dense scenes. First, we design the Layer-wise Aggregation Discriminative Model (LADM) to filter the noisy detections. Then, to associate remaining detections correctly, we introduce the Global Attention Feature Model (GAFM) to extract appearance feature and use it to calculate the appearance similarity between history tracklets and current detections. Finally, we propose the Box-Plane Matching strategy to achieve data association according to the motion similarity and appearance similarity between tracklets and detections. With the effectiveness of the three modules, our team achieves the 1st place on the Track-1 leaderboard in the ACM MM Grand Challenge HiEve 2020.



### Heatmap-based Vanishing Point boosts Lane Detection
- **Arxiv ID**: http://arxiv.org/abs/2007.15602v1
- **DOI**: None
- **Categories**: **cs.CV**, 68T45 (Primary) 68T07 (Secondary), I.4.6
- **Links**: [PDF](http://arxiv.org/pdf/2007.15602v1)
- **Published**: 2020-07-30 17:17:00+00:00
- **Updated**: 2020-07-30 17:17:00+00:00
- **Authors**: Yin-Bo Liu, Ming Zeng, Qing-Hao Meng
- **Comment**: 5 pages, 3 figures, submitted to IEEE journal, under review
- **Journal**: None
- **Summary**: Vision-based lane detection (LD) is a key part of autonomous driving technology, and it is also a challenging problem. As one of the important constraints of scene composition, vanishing point (VP) may provide a useful clue for lane detection. In this paper, we proposed a new multi-task fusion network architecture for high-precision lane detection. Firstly, the ERFNet was used as the backbone to extract the hierarchical features of the road image. Then, the lanes were detected using image segmentation. Finally, combining the output of lane detection and the hierarchical features extracted by the backbone, the lane VP was predicted using heatmap regression. The proposed fusion strategy was tested using the public CULane dataset. The experimental results suggest that the lane detection accuracy of our method outperforms those of state-of-the-art (SOTA) methods.



### Multi-label Zero-shot Classification by Learning to Transfer from External Knowledge
- **Arxiv ID**: http://arxiv.org/abs/2007.15610v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.15610v2)
- **Published**: 2020-07-30 17:26:46+00:00
- **Updated**: 2020-07-31 01:29:56+00:00
- **Authors**: He Huang, Yuanwei Chen, Wei Tang, Wenhao Zheng, Qing-Guo Chen, Yao Hu, Philip Yu
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-label zero-shot classification aims to predict multiple unseen class labels for an input image. It is more challenging than its single-label counterpart. On one hand, the unconstrained number of labels assigned to each image makes the model more easily overfit to those seen classes. On the other hand, there is a large semantic gap between seen and unseen classes in the existing multi-label classification datasets. To address these difficult issues, this paper introduces a novel multi-label zero-shot classification framework by learning to transfer from external knowledge. We observe that ImageNet is commonly used to pretrain the feature extractor and has a large and fine-grained label space. This motivates us to exploit it as external knowledge to bridge the seen and unseen classes and promote generalization. Specifically, we construct a knowledge graph including not only classes from the target dataset but also those from ImageNet. Since ImageNet labels are not available in the target dataset, we propose a novel PosVAE module to infer their initial states in the extended knowledge graph. Then we design a relational graph convolutional network (RGCN) to propagate information among classes and achieve knowledge transfer. Experimental results on two benchmark datasets demonstrate the effectiveness of the proposed approach.



### Continuous Object Representation Networks: Novel View Synthesis without Target View Supervision
- **Arxiv ID**: http://arxiv.org/abs/2007.15627v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.15627v2)
- **Published**: 2020-07-30 17:49:44+00:00
- **Updated**: 2020-10-23 15:19:10+00:00
- **Authors**: Nicolai Häni, Selim Engin, Jun-Jee Chao, Volkan Isler
- **Comment**: To appear at Advances in Neural Information Processing Systems 33
  (NeurIPS 2020)
- **Journal**: None
- **Summary**: Novel View Synthesis (NVS) is concerned with synthesizing views under camera viewpoint transformations from one or multiple input images. NVS requires explicit reasoning about 3D object structure and unseen parts of the scene to synthesize convincing results. As a result, current approaches typically rely on supervised training with either ground truth 3D models or multiple target images. We propose Continuous Object Representation Networks (CORN), a conditional architecture that encodes an input image's geometry and appearance that map to a 3D consistent scene representation. We can train CORN with only two source images per object by combining our model with a neural renderer. A key feature of CORN is that it requires no ground truth 3D models or target view supervision. Regardless, CORN performs well on challenging tasks such as novel view synthesis and single-view 3D reconstruction and achieves performance comparable to state-of-the-art approaches that use direct supervision. For up-to-date information, data, and code, please see our project page: https://nicolaihaeni.github.io/corn/.



### LevelSet R-CNN: A Deep Variational Method for Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2007.15629v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.15629v1)
- **Published**: 2020-07-30 17:52:18+00:00
- **Updated**: 2020-07-30 17:52:18+00:00
- **Authors**: Namdar Homayounfar, Yuwen Xiong, Justin Liang, Wei-Chiu Ma, Raquel Urtasun
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: Obtaining precise instance segmentation masks is of high importance in many modern applications such as robotic manipulation and autonomous driving. Currently, many state of the art models are based on the Mask R-CNN framework which, while very powerful, outputs masks at low resolutions which could result in imprecise boundaries. On the other hand, classic variational methods for segmentation impose desirable global and local data and geometry constraints on the masks by optimizing an energy functional. While mathematically elegant, their direct dependence on good initialization, non-robust image cues and manual setting of hyperparameters renders them unsuitable for modern applications. We propose LevelSet R-CNN, which combines the best of both worlds by obtaining powerful feature representations that are combined in an end-to-end manner with a variational segmentation framework. We demonstrate the effectiveness of our approach on COCO and Cityscapes datasets.



### Rewriting a Deep Generative Model
- **Arxiv ID**: http://arxiv.org/abs/2007.15646v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG, I.2.6; I.2.10; I.3.3
- **Links**: [PDF](http://arxiv.org/pdf/2007.15646v1)
- **Published**: 2020-07-30 17:58:16+00:00
- **Updated**: 2020-07-30 17:58:16+00:00
- **Authors**: David Bau, Steven Liu, Tongzhou Wang, Jun-Yan Zhu, Antonio Torralba
- **Comment**: ECCV 2020 (oral). Code at https://github.com/davidbau/rewriting. For
  videos and demos see https://rewriting.csail.mit.edu/
- **Journal**: None
- **Summary**: A deep generative model such as a GAN learns to model a rich set of semantic and physical rules about the target distribution, but up to now, it has been obscure how such rules are encoded in the network, or how a rule could be changed. In this paper, we introduce a new problem setting: manipulation of specific rules encoded by a deep generative model. To address the problem, we propose a formulation in which the desired rule is changed by manipulating a layer of a deep network as a linear associative memory. We derive an algorithm for modifying one entry of the associative memory, and we demonstrate that several interesting structural rules can be located and modified within the layers of state-of-the-art generative models. We present a user interface to enable users to interactively change the rules of a generative model to achieve desired effects, and we show several proof-of-concept applications. Finally, results on multiple datasets demonstrate the advantage of our method against standard fine-tuning methods and edit transfer algorithms.



### Perceiving 3D Human-Object Spatial Arrangements from a Single Image in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2007.15649v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.15649v2)
- **Published**: 2020-07-30 17:59:50+00:00
- **Updated**: 2020-08-19 20:17:49+00:00
- **Authors**: Jason Y. Zhang, Sam Pepose, Hanbyul Joo, Deva Ramanan, Jitendra Malik, Angjoo Kanazawa
- **Comment**: In ECCV 2020. v2: Updated Related Work
- **Journal**: None
- **Summary**: We present a method that infers spatial arrangements and shapes of humans and objects in a globally consistent 3D scene, all from a single image in-the-wild captured in an uncontrolled environment. Notably, our method runs on datasets without any scene- or object-level 3D supervision. Our key insight is that considering humans and objects jointly gives rise to "3D common sense" constraints that can be used to resolve ambiguity. In particular, we introduce a scale loss that learns the distribution of object size from data; an occlusion-aware silhouette re-projection loss to optimize object pose; and a human-object interaction loss to capture the spatial layout of objects with which humans interact. We empirically validate that our constraints dramatically reduce the space of likely 3D spatial configurations. We demonstrate our approach on challenging, in-the-wild images of humans interacting with large objects (such as bicycles, motorcycles, and surfboards) and handheld objects (such as laptops, tennis rackets, and skateboards). We quantify the ability of our approach to recover human-object arrangements and outline remaining challenges in this relatively domain. The project webpage can be found at https://jasonyzhang.com/phosa.



### Contrastive Learning for Unpaired Image-to-Image Translation
- **Arxiv ID**: http://arxiv.org/abs/2007.15651v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.15651v3)
- **Published**: 2020-07-30 17:59:58+00:00
- **Updated**: 2020-08-20 17:33:08+00:00
- **Authors**: Taesung Park, Alexei A. Efros, Richard Zhang, Jun-Yan Zhu
- **Comment**: ECCV 2020. Please visit
  https://taesungp.github.io/ContrastiveUnpairedTranslation/ for introduction
  videos and more. v3 contains typo fixes and citation update
- **Journal**: None
- **Summary**: In image-to-image translation, each patch in the output should reflect the content of the corresponding patch in the input, independent of domain. We propose a straightforward method for doing so -- maximizing mutual information between the two, using a framework based on contrastive learning. The method encourages two elements (corresponding patches) to map to a similar point in a learned feature space, relative to other elements (other patches) in the dataset, referred to as negatives. We explore several critical design choices for making contrastive learning effective in the image synthesis setting. Notably, we use a multilayer, patch-based approach, rather than operate on entire images. Furthermore, we draw negatives from within the input image itself, rather than from the rest of the dataset. We demonstrate that our framework enables one-sided translation in the unpaired image-to-image translation setting, while improving quality and reducing training time. In addition, our method can even be extended to the training setting where each "domain" is only a single image.



### Mix Dimension in Poincaré Geometry for 3D Skeleton-based Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2007.15678v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.15678v2)
- **Published**: 2020-07-30 18:23:18+00:00
- **Updated**: 2020-08-03 14:19:47+00:00
- **Authors**: Wei Peng, Jingang Shi, Zhaoqiang Xia, Guoying Zhao
- **Comment**: Accepted by ACM MM2020
- **Journal**: None
- **Summary**: Graph Convolutional Networks (GCNs) have already demonstrated their powerful ability to model the irregular data, e.g., skeletal data in human action recognition, providing an exciting new way to fuse rich structural information for nodes residing in different parts of a graph. In human action recognition, current works introduce a dynamic graph generation mechanism to better capture the underlying semantic skeleton connections and thus improves the performance. In this paper, we provide an orthogonal way to explore the underlying connections. Instead of introducing an expensive dynamic graph generation paradigm, we build a more efficient GCN on a Riemann manifold, which we think is a more suitable space to model the graph data, to make the extracted representations fit the embedding matrix. Specifically, we present a novel spatial-temporal GCN (ST-GCN) architecture which is defined via the Poincar\'e geometry such that it is able to better model the latent anatomy of the structure data. To further explore the optimal projection dimension in the Riemann space, we mix different dimensions on the manifold and provide an efficient way to explore the dimension for each ST-GCN layer. With the final resulted architecture, we evaluate our method on two current largest scale 3D datasets, i.e., NTU RGB+D and NTU RGB+D 120. The comparison results show that the model could achieve a superior performance under any given evaluation metrics with only 40\% model size when compared with the previous best GCN method, which proves the effectiveness of our model.



### End-to-end Full Projector Compensation
- **Arxiv ID**: http://arxiv.org/abs/2008.00965v3
- **DOI**: 10.1109/TPAMI.2021.3050124
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.00965v3)
- **Published**: 2020-07-30 18:23:52+00:00
- **Updated**: 2021-01-07 18:49:49+00:00
- **Authors**: Bingyao Huang, Tao Sun, Haibin Ling
- **Comment**: Source code: https://github.com/BingyaoHuang/CompenNeSt-plusplus.
  arXiv admin note: text overlap with arXiv:1908.06246, arXiv:1904.04335
- **Journal**: None
- **Summary**: Full projector compensation aims to modify a projector input image to compensate for both geometric and photometric disturbance of the projection surface. Traditional methods usually solve the two parts separately and may suffer from suboptimal solutions. In this paper, we propose the first end-to-end differentiable solution, named CompenNeSt++, to solve the two problems jointly. First, we propose a novel geometric correction subnet, named WarpingNet, which is designed with a cascaded coarse-to-fine structure to learn the sampling grid directly from sampling images. Second, we propose a novel photometric compensation subnet, named CompenNeSt, which is designed with a siamese architecture to capture the photometric interactions between the projection surface and the projected images, and to use such information to compensate the geometrically corrected images. By concatenating WarpingNet with CompenNeSt, CompenNeSt++ accomplishes full projector compensation and is end-to-end trainable. Third, to improve practicability, we propose a novel synthetic data-based pre-training strategy to significantly reduce the number of training images and training time. Moreover, we construct the first setup-independent full compensation benchmark to facilitate future studies. In thorough experiments, our method shows clear advantages over prior art with promising compensation quality and meanwhile being practically convenient.



### From A Glance to "Gotcha": Interactive Facial Image Retrieval with Progressive Relevance Feedback
- **Arxiv ID**: http://arxiv.org/abs/2007.15683v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/2007.15683v1)
- **Published**: 2020-07-30 18:46:25+00:00
- **Updated**: 2020-07-30 18:46:25+00:00
- **Authors**: Xinru Yang, Haozhi Qi, Mingyang Li, Alexander Hauptmann
- **Comment**: None
- **Journal**: The SIGIR 2020 Workshop on Applied Interactive Information Systems
- **Summary**: Facial image retrieval plays a significant role in forensic investigations where an untrained witness tries to identify a suspect from a massive pool of images. However, due to the difficulties in describing human facial appearances verbally and directly, people naturally tend to depict by referring to well-known existing images and comparing specific areas of faces with them and it is also challenging to provide complete comparison at each time. Therefore, we propose an end-to-end framework to retrieve facial images with relevance feedback progressively provided by the witness, enabling an exploitation of history information during multiple rounds and an interactive and iterative approach to retrieving the mental image. With no need of any extra annotations, our model can be applied at the cost of a little response effort. We experiment on \texttt{CelebA} and evaluate the performance by ranking percentile and achieve 99\% under the best setting. Since this topic remains little explored to the best of our knowledge, we hope our work can serve as a stepping stone for further research.



### Deep learning for lithological classification of carbonate rock micro-CT images
- **Arxiv ID**: http://arxiv.org/abs/2007.15693v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.15693v1)
- **Published**: 2020-07-30 19:14:00+00:00
- **Updated**: 2020-07-30 19:14:00+00:00
- **Authors**: Carlos E. M. dos Anjos, Manuel R. V. Avila, Adna G. P. Vasconcelos, Aurea M. P. Neta, Lizianne C. Medeiros, Alexandre G. Evsukoff, Rodrigo Surmas
- **Comment**: 13 pages, 8 figures
- **Journal**: None
- **Summary**: In addition to the ongoing development, pre-salt carbonate reservoir characterization remains a challenge, primarily due to inherent geological particularities. These challenges stimulate the use of well-established technologies, such as artificial intelligence algorithms, for image classification tasks. Therefore, this work intends to present an application of deep learning techniques to identify patterns in Brazilian pre-salt carbonate rock microtomographic images, thus making possible lithological classification. Four convolutional neural network models were proposed. The first model includes three convolutional layers followed by fully connected layers and is used as a base model for the following proposals. In the next two models, we replace the max pooling layer with a spatial pyramid pooling and a global average pooling layer. The last model uses a combination of spatial pyramid pooling followed by global average pooling in place of the last pooling layer. All models are compared using original images, when possible, as well as resized images. The dataset consists of 6,000 images from three different classes. The model performances were evaluated by each image individually, as well as by the most frequently predicted class for each sample. According to accuracy, Model 2 trained on resized images achieved the best results, reaching an average of 75.54% for the first evaluation approach and an average of 81.33% for the second. We developed a workflow to automate and accelerate the lithology classification of Brazilian pre-salt carbonate samples by categorizing microtomographic images using deep learning algorithms in a non-destructive way.



### Deep Traffic Sign Detection and Recognition Without Target Domain Real Images
- **Arxiv ID**: http://arxiv.org/abs/2008.00962v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.00962v1)
- **Published**: 2020-07-30 21:06:47+00:00
- **Updated**: 2020-07-30 21:06:47+00:00
- **Authors**: Lucas Tabelini, Rodrigo Berriel, Thiago M. Paixão, Alberto F. De Souza, Claudine Badue, Nicu Sebe, Thiago Oliveira-Santos
- **Comment**: arXiv admin note: text overlap with arXiv:1907.09679
- **Journal**: None
- **Summary**: Deep learning has been successfully applied to several problems related to autonomous driving, often relying on large databases of real target-domain images for proper training. The acquisition of such real-world data is not always possible in the self-driving context, and sometimes their annotation is not feasible. Moreover, in many tasks, there is an intrinsic data imbalance that most learning-based methods struggle to cope with. Particularly, traffic sign detection is a challenging problem in which these three issues are seen altogether. To address these challenges, we propose a novel database generation method that requires only (i) arbitrary natural images, i.e., requires no real image from the target-domain, and (ii) templates of the traffic signs. The method does not aim at overcoming the training with real data, but to be a compatible alternative when the real data is not available. The effortlessly generated database is shown to be effective for the training of a deep detector on traffic signs from multiple countries. On large data sets, training with a fully synthetic data set almost matches the performance of training with a real one. When compared to training with a smaller data set of real images, training with synthetic images increased the accuracy by 12.25%. The proposed method also improves the performance of the detector when target-domain data are available.



### Unidentified Floating Object detection in maritime environment using dictionary learning
- **Arxiv ID**: http://arxiv.org/abs/2007.15757v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.15757v1)
- **Published**: 2020-07-30 21:59:09+00:00
- **Updated**: 2020-07-30 21:59:09+00:00
- **Authors**: Darshan Venkatrayappa, Agnès Desolneux, Jean-Michel Hubert, Josselin Manceau
- **Comment**: 9 pages, 6 figures
- **Journal**: None
- **Summary**: Maritime domain is one of the most challenging scenarios for object detection due to the complexity of the observed scene. In this article, we present a new approach to detect unidentified floating objects in the maritime environment. The proposed approach is capable of detecting floating objects without any prior knowledge of their visual appearance, shape or location. The input image from the video stream is denoised using a visual dictionary learned from a K-SVD algorithm. The denoised image is made of self-similar content. Later, we extract the residual image, which is the difference between the original image and the denoised (self-similar) image. Thus, the residual image contains noise and salient structures (objects). These salient structures can be extracted using an a contrario model. We demonstrate the capabilities of our algorithm by testing it on videos exhibiting varying maritime scenarios.



