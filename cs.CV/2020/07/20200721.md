# Arxiv Papers in cs.CV on 2020-07-21
### Regularizing Deep Networks with Semantic Data Augmentation
- **Arxiv ID**: http://arxiv.org/abs/2007.10538v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.10538v5)
- **Published**: 2020-07-21 00:32:44+00:00
- **Updated**: 2021-06-04 09:52:11+00:00
- **Authors**: Yulin Wang, Gao Huang, Shiji Song, Xuran Pan, Yitong Xia, Cheng Wu
- **Comment**: Accepted by IEEE Transactions on Pattern Analysis and Machine
  Intelligence (T-PAMI). Journal version of arXiv:1909.12220 (NeurIPS 2019).
  Code is available at
  https://github.com/blackfeather-wang/ISDA-for-Deep-Networks
- **Journal**: None
- **Summary**: Data augmentation is widely known as a simple yet surprisingly effective technique for regularizing deep networks. Conventional data augmentation schemes, e.g., flipping, translation or rotation, are low-level, data-independent and class-agnostic operations, leading to limited diversity for augmented samples. To this end, we propose a novel semantic data augmentation algorithm to complement traditional approaches. The proposed method is inspired by the intriguing property that deep networks are effective in learning linearized features, i.e., certain directions in the deep feature space correspond to meaningful semantic transformations, e.g., changing the background or view angle of an object. Based on this observation, translating training samples along many such directions in the feature space can effectively augment the dataset for more diversity. To implement this idea, we first introduce a sampling based method to obtain semantically meaningful directions efficiently. Then, an upper bound of the expected cross-entropy (CE) loss on the augmented training set is derived by assuming the number of augmented samples goes to infinity, yielding a highly efficient algorithm. In fact, we show that the proposed implicit semantic data augmentation (ISDA) algorithm amounts to minimizing a novel robust CE loss, which adds minimal extra computational cost to a normal training procedure. In addition to supervised learning, ISDA can be applied to semi-supervised learning tasks under the consistency regularization framework, where ISDA amounts to minimizing the upper bound of the expected KL-divergence between the augmented features and the original features. Although being simple, ISDA consistently improves the generalization performance of popular deep models (e.g., ResNets and DenseNets) on a variety of datasets, i.e., CIFAR-10, CIFAR-100, SVHN, ImageNet, and Cityscapes.



### Unified Multisensory Perception: Weakly-Supervised Audio-Visual Video Parsing
- **Arxiv ID**: http://arxiv.org/abs/2007.10558v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2007.10558v1)
- **Published**: 2020-07-21 01:53:31+00:00
- **Updated**: 2020-07-21 01:53:31+00:00
- **Authors**: Yapeng Tian, Dingzeyu Li, Chenliang Xu
- **Comment**: ECCV 2020 (Spotlight)
- **Journal**: None
- **Summary**: In this paper, we introduce a new problem, named audio-visual video parsing, which aims to parse a video into temporal event segments and label them as either audible, visible, or both. Such a problem is essential for a complete understanding of the scene depicted inside a video. To facilitate exploration, we collect a Look, Listen, and Parse (LLP) dataset to investigate audio-visual video parsing in a weakly-supervised manner. This task can be naturally formulated as a Multimodal Multiple Instance Learning (MMIL) problem. Concretely, we propose a novel hybrid attention network to explore unimodal and cross-modal temporal contexts simultaneously. We develop an attentive MMIL pooling method to adaptively explore useful audio and visual content from different temporal extent and modalities. Furthermore, we discover and mitigate modality bias and noisy label issues with an individual-guided learning mechanism and label smoothing technique, respectively. Experimental results show that the challenging audio-visual video parsing can be achieved even with only video-level weak labels. Our proposed framework can effectively leverage unimodal and cross-modal temporal contexts and alleviate modality bias and noisy labels problems.



### Learnable Cost Volume Using the Cayley Representation
- **Arxiv ID**: http://arxiv.org/abs/2007.11431v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.11431v1)
- **Published**: 2020-07-21 01:59:36+00:00
- **Updated**: 2020-07-21 01:59:36+00:00
- **Authors**: Taihong Xiao, Jinwei Yuan, Deqing Sun, Qifei Wang, Xin-Yu Zhang, Kehan Xu, Ming-Hsuan Yang
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: Cost volume is an essential component of recent deep models for optical flow estimation and is usually constructed by calculating the inner product between two feature vectors. However, the standard inner product in the commonly-used cost volume may limit the representation capacity of flow models because it neglects the correlation among different channel dimensions and weighs each dimension equally. To address this issue, we propose a learnable cost volume (LCV) using an elliptical inner product, which generalizes the standard inner product by a positive definite kernel matrix. To guarantee its positive definiteness, we perform spectral decomposition on the kernel matrix and re-parameterize it via the Cayley representation. The proposed LCV is a lightweight module and can be easily plugged into existing models to replace the vanilla cost volume. Experimental results show that the LCV module not only improves the accuracy of state-of-the-art models on standard benchmarks, but also promotes their robustness against illumination change, noises, and adversarial perturbations of the input signals.



### 3D Correspondence Grouping with Compatibility Features
- **Arxiv ID**: http://arxiv.org/abs/2007.10570v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.10570v1)
- **Published**: 2020-07-21 02:39:48+00:00
- **Updated**: 2020-07-21 02:39:48+00:00
- **Authors**: Jiaqi Yang, Jiahao Chen, Zhiqiang Huang, Siwen Quan, Yanning Zhang, Zhiguo Cao
- **Comment**: None
- **Journal**: None
- **Summary**: We present a simple yet effective method for 3D correspondence grouping. The objective is to accurately classify initial correspondences obtained by matching local geometric descriptors into inliers and outliers. Although the spatial distribution of correspondences is irregular, inliers are expected to be geometrically compatible with each other. Based on such observation, we propose a novel representation for 3D correspondences, dubbed compatibility feature (CF), to describe the consistencies within inliers and inconsistencies within outliers. CF consists of top-ranked compatibility scores of a candidate to other correspondences, which purely relies on robust and rotation-invariant geometric constraints. We then formulate the grouping problem as a classification problem for CF features, which is accomplished via a simple multilayer perceptron (MLP) network. Comparisons with nine state-of-the-art methods on four benchmarks demonstrate that: 1) CF is distinctive, robust, and rotation-invariant; 2) our CF-based method achieves the best overall performance and holds good generalization ability.



### Domain Generalization via Optimal Transport with Metric Similarity Learning
- **Arxiv ID**: http://arxiv.org/abs/2007.10573v2
- **DOI**: 10.1016/j.neucom.2020.09.091
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.10573v2)
- **Published**: 2020-07-21 02:56:05+00:00
- **Updated**: 2022-04-04 22:31:36+00:00
- **Authors**: Fan Zhou, Zhuqing Jiang, Changjian Shui, Boyu Wang, Brahim Chaib-draa
- **Comment**: None
- **Journal**: Neurocomputing 456 (2021) 469-480
- **Summary**: Generalizing knowledge to unseen domains, where data and labels are unavailable, is crucial for machine learning models. We tackle the domain generalization problem to learn from multiple source domains and generalize to a target domain with unknown statistics. The crucial idea is to extract the underlying invariant features across all the domains. Previous domain generalization approaches mainly focused on learning invariant features and stacking the learned features from each source domain to generalize to a new target domain while ignoring the label information, which will lead to indistinguishable features with an ambiguous classification boundary. For this, one possible solution is to constrain the label-similarity when extracting the invariant features and to take advantage of the label similarities for class-specific cohesion and separation of features across domains. Therefore we adopt optimal transport with Wasserstein distance, which could constrain the class label similarity, for adversarial training and also further deploy a metric learning objective to leverage the label information for achieving distinguishable classification boundary. Empirical results show that our proposed method could outperform most of the baselines. Furthermore, ablation studies also demonstrate the effectiveness of each component of our method.



### Learning to Compose Hypercolumns for Visual Correspondence
- **Arxiv ID**: http://arxiv.org/abs/2007.10587v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.10587v1)
- **Published**: 2020-07-21 04:03:22+00:00
- **Updated**: 2020-07-21 04:03:22+00:00
- **Authors**: Juhong Min, Jongmin Lee, Jean Ponce, Minsu Cho
- **Comment**: Accepted to ECCV 2020
- **Journal**: None
- **Summary**: Feature representation plays a crucial role in visual correspondence, and recent methods for image matching resort to deeply stacked convolutional layers. These models, however, are both monolithic and static in the sense that they typically use a specific level of features, e.g., the output of the last layer, and adhere to it regardless of the images to match. In this work, we introduce a novel approach to visual correspondence that dynamically composes effective features by leveraging relevant layers conditioned on the images to match. Inspired by both multi-layer feature composition in object detection and adaptive inference architectures in classification, the proposed method, dubbed Dynamic Hyperpixel Flow, learns to compose hypercolumn features on the fly by selecting a small number of relevant layers from a deep convolutional neural network. We demonstrate the effectiveness on the task of semantic correspondence, i.e., establishing correspondences between images depicting different instances of the same object or scene category. Experiments on standard benchmarks show that the proposed method greatly improves matching performance over the state of the art in an adaptive and efficient manner.



### CyCNN: A Rotation Invariant CNN using Polar Mapping and Cylindrical Convolution Layers
- **Arxiv ID**: http://arxiv.org/abs/2007.10588v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.10588v1)
- **Published**: 2020-07-21 04:05:35+00:00
- **Updated**: 2020-07-21 04:05:35+00:00
- **Authors**: Jinpyo Kim, Wooekun Jung, Hyungmo Kim, Jaejin Lee
- **Comment**: None
- **Journal**: None
- **Summary**: Deep Convolutional Neural Networks (CNNs) are empirically known to be invariant to moderate translation but not to rotation in image classification. This paper proposes a deep CNN model, called CyCNN, which exploits polar mapping of input images to convert rotation to translation. To deal with the cylindrical property of the polar coordinates, we replace convolution layers in conventional CNNs to cylindrical convolutional (CyConv) layers. A CyConv layer exploits the cylindrically sliding windows (CSW) mechanism that vertically extends the input-image receptive fields of boundary units in a convolutional layer. We evaluate CyCNN and conventional CNN models for classification tasks on rotated MNIST, CIFAR-10, and SVHN datasets. We show that if there is no data augmentation during training, CyCNN significantly improves classification accuracies when compared to conventional CNN models. Our implementation of CyCNN is publicly available on https://github.com/mcrl/CyCNN.



### AinnoSeg: Panoramic Segmentation with High Perfomance
- **Arxiv ID**: http://arxiv.org/abs/2007.10591v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.10591v1)
- **Published**: 2020-07-21 04:16:46+00:00
- **Updated**: 2020-07-21 04:16:46+00:00
- **Authors**: Jiahong Wu, Jianfei Lu, Xinxin Kang, Yiming Zhang, Yinhang Tang, Jianfei Song, Ze Huang, Shenglan Ben, Jiashui Huang, Faen Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Panoramic segmentation is a scene where image segmentation tasks is more difficult. With the development of CNN networks, panoramic segmentation tasks have been sufficiently developed.However, the current panoramic segmentation algorithms are more concerned with context semantics, but the details of image are not processed enough. Moreover, they cannot solve the problems which contains the accuracy of occluded object segmentation,little object segmentation,boundary pixel in object segmentation etc. Aiming to address these issues, this paper presents some useful tricks. (a) By changing the basic segmentation model, the model can take into account the large objects and the boundary pixel classification of image details. (b) Modify the loss function so that it can take into account the boundary pixels of multiple objects in the image. (c) Use a semi-supervised approach to regain control of the training process. (d) Using multi-scale training and reasoning. All these operations named AinnoSeg, AinnoSeg can achieve state-of-art performance on the well-known dataset ADE20K.



### Towards Visual Distortion in Black-Box Attacks
- **Arxiv ID**: http://arxiv.org/abs/2007.10593v2
- **DOI**: 10.1109/TIP.2021.3092822
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.10593v2)
- **Published**: 2020-07-21 04:42:43+00:00
- **Updated**: 2021-01-20 07:35:56+00:00
- **Authors**: Nannan Li, Zhenzhong Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Constructing adversarial examples in a black-box threat model injures the original images by introducing visual distortion. In this paper, we propose a novel black-box attack approach that can directly minimize the induced distortion by learning the noise distribution of the adversarial example, assuming only loss-oracle access to the black-box network. The quantified visual distortion, which measures the perceptual distance between the adversarial example and the original image, is introduced in our loss whilst the gradient of the corresponding non-differentiable loss function is approximated by sampling noise from the learned noise distribution. We validate the effectiveness of our attack on ImageNet. Our attack results in much lower distortion when compared to the state-of-the-art black-box attacks and achieves $100\%$ success rate on InceptionV3, ResNet50 and VGG16bn. The code is available at https://github.com/Alina-1997/visual-distortion-in-attack.



### Video Super-resolution with Temporal Group Attention
- **Arxiv ID**: http://arxiv.org/abs/2007.10595v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.10595v1)
- **Published**: 2020-07-21 04:54:30+00:00
- **Updated**: 2020-07-21 04:54:30+00:00
- **Authors**: Takashi Isobe, Songjiang Li, Xu Jia, Shanxin Yuan, Gregory Slabaugh, Chunjing Xu, Ya-Li Li, Shengjin Wang, Qi Tian
- **Comment**: CVPR 2020
- **Journal**: None
- **Summary**: Video super-resolution, which aims at producing a high-resolution video from its corresponding low-resolution version, has recently drawn increasing attention. In this work, we propose a novel method that can effectively incorporate temporal information in a hierarchical way. The input sequence is divided into several groups, with each one corresponding to a kind of frame rate. These groups provide complementary information to recover missing details in the reference frame, which is further integrated with an attention module and a deep intra-group fusion module. In addition, a fast spatial alignment is proposed to handle videos with large motion. Extensive results demonstrate the capability of the proposed model in handling videos with various motion. It achieves favorable performance against state-of-the-art methods on several benchmark datasets.



### Graph-PCNN: Two Stage Human Pose Estimation with Graph Pose Refinement
- **Arxiv ID**: http://arxiv.org/abs/2007.10599v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.10599v1)
- **Published**: 2020-07-21 04:59:15+00:00
- **Updated**: 2020-07-21 04:59:15+00:00
- **Authors**: Jian Wang, Xiang Long, Yuan Gao, Errui Ding, Shilei Wen
- **Comment**: Accepted to ECCV2020
- **Journal**: None
- **Summary**: Recently, most of the state-of-the-art human pose estimation methods are based on heatmap regression. The final coordinates of keypoints are obtained by decoding heatmap directly. In this paper, we aim to find a better approach to get more accurate localization results. We mainly put forward two suggestions for improvement: 1) different features and methods should be applied for rough and accurate localization, 2) relationship between keypoints should be considered. Specifically, we propose a two-stage graph-based and model-agnostic framework, called Graph-PCNN, with a localization subnet and a graph pose refinement module added onto the original heatmap regression network. In the first stage, heatmap regression network is applied to obtain a rough localization result, and a set of proposal keypoints, called guided points, are sampled. In the second stage, for each guided point, different visual feature is extracted by the localization subnet. The relationship between guided points is explored by the graph pose refinement module to get more accurate localization results. Experiments show that Graph-PCNN can be used in various backbones to boost the performance by a large margin. Without bells and whistles, our best model can achieve a new state-of-the-art 76.8% AP on COCO test-dev split.



### Feature-metric Loss for Self-supervised Learning of Depth and Egomotion
- **Arxiv ID**: http://arxiv.org/abs/2007.10603v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.10603v1)
- **Published**: 2020-07-21 05:19:07+00:00
- **Updated**: 2020-07-21 05:19:07+00:00
- **Authors**: Chang Shu, Kun Yu, Zhixiang Duan, Kuiyuan Yang
- **Comment**: Accepted by ECCV2020
- **Journal**: None
- **Summary**: Photometric loss is widely used for self-supervised depth and egomotion estimation. However, the loss landscapes induced by photometric differences are often problematic for optimization, caused by plateau landscapes for pixels in textureless regions or multiple local minima for less discriminative pixels. In this work, feature-metric loss is proposed and defined on feature representation, where the feature representation is also learned in a self-supervised manner and regularized by both first-order and second-order derivatives to constrain the loss landscapes to form proper convergence basins. Comprehensive experiments and detailed analysis via visualization demonstrate the effectiveness of the proposed feature-metric loss. In particular, our method improves state-of-the-art methods on KITTI from 0.885 to 0.925 measured by $\delta_1$ for depth estimation, and significantly outperforms previous method for visual odometry.



### Novel View Synthesis on Unpaired Data by Conditional Deformable Variational Auto-Encoder
- **Arxiv ID**: http://arxiv.org/abs/2007.10618v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.10618v1)
- **Published**: 2020-07-21 06:44:01+00:00
- **Updated**: 2020-07-21 06:44:01+00:00
- **Authors**: Mingyu Yin, Li Sun, Qingli Li
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: Novel view synthesis often needs the paired data from both the source and target views. This paper proposes a view translation model under cVAE-GAN framework without requiring the paired data. We design a conditional deformable module (CDM) which uses the view condition vectors as the filters to convolve the feature maps of the main branch in VAE. It generates several pairs of displacement maps to deform the features, like the 2D optical flows. The results are fed into the deformed feature based normalization module (DFNM), which scales and offsets the main branch feature, given its deformed one as the input from the side branch. Taking the advantage of the CDM and DFNM, the encoder outputs a view-irrelevant posterior, while the decoder takes the code drawn from it to synthesize the reconstructed and the viewtranslated images. To further ensure the disentanglement between the views and other factors, we add adversarial training on the code. The results and ablation studies on MultiPIE and 3D chair datasets validate the effectiveness of the framework in cVAE and the designed module.



### A Hybrid Neuromorphic Object Tracking and Classification Framework for Real-time Systems
- **Arxiv ID**: http://arxiv.org/abs/2007.11404v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.11404v1)
- **Published**: 2020-07-21 07:11:27+00:00
- **Updated**: 2020-07-21 07:11:27+00:00
- **Authors**: Andres Ussa, Chockalingam Senthil Rajen, Deepak Singla, Jyotibdha Acharya, Gideon Fu Chuanrong, Arindam Basu, Bharath Ramesh
- **Comment**: 11 pages, 8 figures. arXiv admin note: substantial text overlap with
  arXiv:1910.09806
- **Journal**: None
- **Summary**: Deep learning inference that needs to largely take place on the 'edge' is a highly computational and memory intensive workload, making it intractable for low-power, embedded platforms such as mobile nodes and remote security applications. To address this challenge, this paper proposes a real-time, hybrid neuromorphic framework for object tracking and classification using event-based cameras that possess properties such as low-power consumption (5-14 mW) and high dynamic range (120 dB). Nonetheless, unlike traditional approaches of using event-by-event processing, this work uses a mixed frame and event approach to get energy savings with high performance. Using a frame-based region proposal method based on the density of foreground events, a hardware-friendly object tracking scheme is implemented using the apparent object velocity while tackling occlusion scenarios. The object track input is converted back to spikes for TrueNorth classification via the energy-efficient deep network (EEDN) pipeline. Using originally collected datasets, we train the TrueNorth model on the hardware track outputs, instead of using ground truth object locations as commonly done, and demonstrate the ability of our system to handle practical surveillance scenarios. As an optional paradigm, to exploit the low latency and asynchronous nature of neuromorphic vision sensors (NVS), we also propose a continuous-time tracker with C++ implementation where each event is processed individually. Thereby, we extensively compare the proposed methodologies to state-of-the-art event-based and frame-based methods for object tracking and classification, and demonstrate the use case of our neuromorphic approach for real-time and embedded applications without sacrificing performance. Finally, we also showcase the efficacy of the proposed system to a standard RGB camera setup when evaluated over several hours of traffic recordings.



### Sparse Nonnegative Tensor Factorization and Completion with Noisy Observations
- **Arxiv ID**: http://arxiv.org/abs/2007.10626v3
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG, cs.NA, math.NA
- **Links**: [PDF](http://arxiv.org/pdf/2007.10626v3)
- **Published**: 2020-07-21 07:17:52+00:00
- **Updated**: 2021-10-20 12:47:16+00:00
- **Authors**: Xiongjun Zhang, Michael K. Ng
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we study the sparse nonnegative tensor factorization and completion problem from partial and noisy observations for third-order tensors. Because of sparsity and nonnegativity, the underlying tensor is decomposed into the tensor-tensor product of one sparse nonnegative tensor and one nonnegative tensor. We propose to minimize the sum of the maximum likelihood estimation for the observations with nonnegativity constraints and the tensor $\ell_0$ norm for the sparse factor. We show that the error bounds of the estimator of the proposed model can be established under general noise observations. The detailed error bounds under specific noise distributions including additive Gaussian noise, additive Laplace noise, and Poisson observations can be derived. Moreover, the minimax lower bounds are shown to be matched with the established upper bounds up to a logarithmic factor of the sizes of the underlying tensor. These theoretical results for tensors are better than those obtained for matrices, and this illustrates the advantage of the use of nonnegative sparse tensor models for completion and denoising. Numerical experiments are provided to validate the superiority of the proposed tensor-based method compared with the matrix-based approach.



### SLNSpeech: solving extended speech separation problem by the help of sign language
- **Arxiv ID**: http://arxiv.org/abs/2007.10629v1
- **DOI**: None
- **Categories**: **eess.AS**, cs.CV, cs.SD
- **Links**: [PDF](http://arxiv.org/pdf/2007.10629v1)
- **Published**: 2020-07-21 07:22:33+00:00
- **Updated**: 2020-07-21 07:22:33+00:00
- **Authors**: Jiasong Wu, Taotao Li, Youyong Kong, Guanyu Yang, Lotfi Senhadji, Huazhong Shu
- **Comment**: 33 pages, 8 figures, 5 tables
- **Journal**: None
- **Summary**: A speech separation task can be roughly divided into audio-only separation and audio-visual separation. In order to make speech separation technology applied in the real scenario of the disabled, this paper presents an extended speech separation problem which refers in particular to sign language assisted speech separation. However, most existing datasets for speech separation are audios and videos which contain audio and/or visual modalities. To address the extended speech separation problem, we introduce a large-scale dataset named Sign Language News Speech (SLNSpeech) dataset in which three modalities of audio, visual, and sign language are coexisted. Then, we design a general deep learning network for the self-supervised learning of three modalities, particularly, using sign language embeddings together with audio or audio-visual information for better solving the speech separation task. Specifically, we use 3D residual convolutional network to extract sign language features and use pretrained VGGNet model to exact visual features. After that, an improved U-Net with skip connections in feature extraction stage is applied for learning the embeddings among the mixed spectrogram transformed from source audios, the sign language features and visual features. Experiments results show that, besides visual modality, sign language modality can also be used alone to supervise speech separation task. Moreover, we also show the effectiveness of sign language assisted speech separation when the visual modality is disturbed. Source code will be released in http://cheertt.top/homepage/



### Learning Person Re-identification Models from Videos with Weak Supervision
- **Arxiv ID**: http://arxiv.org/abs/2007.10631v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.10631v1)
- **Published**: 2020-07-21 07:23:32+00:00
- **Updated**: 2020-07-21 07:23:32+00:00
- **Authors**: Xueping Wang, Sujoy Paul, Dripta S. Raychaudhuri, Min Liu, Yaonan Wang, Amit K. Roy-Chowdhury
- **Comment**: None
- **Journal**: None
- **Summary**: Most person re-identification methods, being supervised techniques, suffer from the burden of massive annotation requirement. Unsupervised methods overcome this need for labeled data, but perform poorly compared to the supervised alternatives. In order to cope with this issue, we introduce the problem of learning person re-identification models from videos with weak supervision. The weak nature of the supervision arises from the requirement of video-level labels, i.e. person identities who appear in the video, in contrast to the more precise framelevel annotations. Towards this goal, we propose a multiple instance attention learning framework for person re-identification using such video-level labels. Specifically, we first cast the video person re-identification task into a multiple instance learning setting, in which person images in a video are collected into a bag. The relations between videos with similar labels can be utilized to identify persons, on top of that, we introduce a co-person attention mechanism which mines the similarity correlations between videos with person identities in common. The attention weights are obtained based on all person images instead of person tracklets in a video, making our learned model less affected by noisy annotations. Extensive experiments demonstrate the superiority of the proposed method over the related methods on two weakly labeled person re-identification datasets.



### Multi-modal Transformer for Video Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2007.10639v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.10639v1)
- **Published**: 2020-07-21 07:38:46+00:00
- **Updated**: 2020-07-21 07:38:46+00:00
- **Authors**: Valentin Gabeur, Chen Sun, Karteek Alahari, Cordelia Schmid
- **Comment**: ECCV 2020 (spotlight paper)
- **Journal**: None
- **Summary**: The task of retrieving video content relevant to natural language queries plays a critical role in effectively handling internet-scale datasets. Most of the existing methods for this caption-to-video retrieval problem do not fully exploit cross-modal cues present in video. Furthermore, they aggregate per-frame visual features with limited or no temporal information. In this paper, we present a multi-modal transformer to jointly encode the different modalities in video, which allows each of them to attend to the others. The transformer architecture is also leveraged to encode and model the temporal information. On the natural language side, we investigate the best practices to jointly optimize the language embedding together with the multi-modal transformer. This novel framework allows us to establish state-of-the-art results for video retrieval on three datasets. More details are available at http://thoth.inrialpes.fr/research/MMT.



### Fine-Grained Image Captioning with Global-Local Discriminative Objective
- **Arxiv ID**: http://arxiv.org/abs/2007.10662v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2007.10662v1)
- **Published**: 2020-07-21 08:46:02+00:00
- **Updated**: 2020-07-21 08:46:02+00:00
- **Authors**: Jie Wu, Tianshui Chen, Hefeng Wu, Zhi Yang, Guangchun Luo, Liang Lin
- **Comment**: Accepted by TMM
- **Journal**: None
- **Summary**: Significant progress has been made in recent years in image captioning, an active topic in the fields of vision and language. However, existing methods tend to yield overly general captions and consist of some of the most frequent words/phrases, resulting in inaccurate and indistinguishable descriptions (see Figure 1). This is primarily due to (i) the conservative characteristic of traditional training objectives that drives the model to generate correct but hardly discriminative captions for similar images and (ii) the uneven word distribution of the ground-truth captions, which encourages generating highly frequent words/phrases while suppressing the less frequent but more concrete ones. In this work, we propose a novel global-local discriminative objective that is formulated on top of a reference model to facilitate generating fine-grained descriptive captions. Specifically, from a global perspective, we design a novel global discriminative constraint that pulls the generated sentence to better discern the corresponding image from all others in the entire dataset. From the local perspective, a local discriminative constraint is proposed to increase attention such that it emphasizes the less frequent but more concrete words/phrases, thus facilitating the generation of captions that better describe the visual details of the given images. We evaluate the proposed method on the widely used MS-COCO dataset, where it outperforms the baseline methods by a sizable margin and achieves competitive performance over existing leading approaches. We also conduct self-retrieval experiments to demonstrate the discriminability of the proposed method.



### Segmentation of the Left Ventricle by SDD double threshold selection and CHT
- **Arxiv ID**: http://arxiv.org/abs/2007.10665v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.10665v2)
- **Published**: 2020-07-21 08:50:21+00:00
- **Updated**: 2023-07-07 10:36:17+00:00
- **Authors**: ZiHao Wang, ZhenZhou Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic and robust segmentation of the left ventricle (LV) in magnetic resonance images (MRI) has remained challenging for many decades. With the great success of deep learning in object detection and classification, the research focus of LV segmentation has changed to convolutional neural network (CNN) in recent years. However, LV segmentation is a pixel-level classification problem and its categories are intractable compared to object detection and classification. In this paper, we proposed a robust LV segmentation method based on slope difference distribution (SDD) double threshold selection and circular Hough transform (CHT). The proposed method achieved 96.51% DICE score on the test set of automated cardiac diagnosis challenge (ACDC) which is higher than the best accuracy reported in recently published literatures.



### Deep Learning Techniques for Future Intelligent Cross-Media Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2008.01191v1
- **DOI**: None
- **Categories**: **cs.IR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.01191v1)
- **Published**: 2020-07-21 09:49:33+00:00
- **Updated**: 2020-07-21 09:49:33+00:00
- **Authors**: Sadaqat ur Rehman, Muhammad Waqas, Shanshan Tu, Anis Koubaa, Obaid ur Rehman, Jawad Ahmad, Muhammad Hanif, Zhu Han
- **Comment**: arXiv admin note: text overlap with arXiv:1804.09539 by other authors
- **Journal**: None
- **Summary**: With the advancement in technology and the expansion of broadcasting, cross-media retrieval has gained much attention. It plays a significant role in big data applications and consists in searching and finding data from different types of media. In this paper, we provide a novel taxonomy according to the challenges faced by multi-modal deep learning approaches in solving cross-media retrieval, namely: representation, alignment, and translation. These challenges are evaluated on deep learning (DL) based methods, which are categorized into four main groups: 1) unsupervised methods, 2) supervised methods, 3) pairwise based methods, and 4) rank based methods. Then, we present some well-known cross-media datasets used for retrieval, considering the importance of these datasets in the context in of deep learning based cross-media retrieval approaches. Moreover, we also present an extensive review of the state-of-the-art problems and its corresponding solutions for encouraging deep learning in cross-media retrieval. The fundamental objective of this work is to exploit Deep Neural Networks (DNNs) for bridging the "media gap", and provide researchers and developers with a better understanding of the underlying problems and the potential solutions of deep learning assisted cross-media retrieval. To the best of our knowledge, this is the first comprehensive survey to address cross-media retrieval under deep learning methods.



### A Deep Ordinal Distortion Estimation Approach for Distortion Rectification
- **Arxiv ID**: http://arxiv.org/abs/2007.10689v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.10689v1)
- **Published**: 2020-07-21 10:03:42+00:00
- **Updated**: 2020-07-21 10:03:42+00:00
- **Authors**: Kang Liao, Chunyu Lin, Yao Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: Distortion is widely existed in the images captured by popular wide-angle cameras and fisheye cameras. Despite the long history of distortion rectification, accurately estimating the distortion parameters from a single distorted image is still challenging. The main reason is these parameters are implicit to image features, influencing the networks to fully learn the distortion information. In this work, we propose a novel distortion rectification approach that can obtain more accurate parameters with higher efficiency. Our key insight is that distortion rectification can be cast as a problem of learning an ordinal distortion from a single distorted image. To solve this problem, we design a local-global associated estimation network that learns the ordinal distortion to approximate the realistic distortion distribution. In contrast to the implicit distortion parameters, the proposed ordinal distortion have more explicit relationship with image features, and thus significantly boosts the distortion perception of neural networks. Considering the redundancy of distortion information, our approach only uses a part of distorted image for the ordinal distortion estimation, showing promising applications in the efficient distortion rectification. To our knowledge, we first unify the heterogeneous distortion parameters into a learning-friendly intermediate representation through ordinal distortion, bridging the gap between image feature and distortion rectification. The experimental results demonstrate that our approach outperforms the state-of-the-art methods by a significant margin, with approximately 23% improvement on the quantitative evaluation while displaying the best performance on visual appearance.



### Minimal Cases for Computing the Generalized Relative Pose using Affine Correspondences
- **Arxiv ID**: http://arxiv.org/abs/2007.10700v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2007.10700v2)
- **Published**: 2020-07-21 10:34:45+00:00
- **Updated**: 2021-08-19 04:41:28+00:00
- **Authors**: Banglei Guan, Ji Zhao, Daniel Barath, Friedrich Fraundorfer
- **Comment**: ICCV 2021
- **Journal**: None
- **Summary**: We propose three novel solvers for estimating the relative pose of a multi-camera system from affine correspondences (ACs). A new constraint is derived interpreting the relationship of ACs and the generalized camera model. Using the constraint, we demonstrate efficient solvers for two types of motions assumed. Considering that the cameras undergo planar motion, we propose a minimal solution using a single AC and a solver with two ACs to overcome the degenerate case. Also, we propose a minimal solution using two ACs with known vertical direction, e.g., from an IMU. Since the proposed methods require significantly fewer correspondences than state-of-the-art algorithms, they can be efficiently used within RANSAC for outlier removal and initial motion estimation. The solvers are tested both on synthetic data and on real-world scenes from the KITTI odometry benchmark. It is shown that the accuracy of the estimated poses is superior to the state-of-the-art techniques.



### Deep Preset: Blending and Retouching Photos with Color Style Transfer
- **Arxiv ID**: http://arxiv.org/abs/2007.10701v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.10701v2)
- **Published**: 2020-07-21 10:41:03+00:00
- **Updated**: 2021-01-02 10:53:45+00:00
- **Authors**: Man M. Ho, Jinjia Zhou
- **Comment**: Revised and Accepted to WACV'2021. Our work is available at
  https://minhmanho.github.io/deep_preset
- **Journal**: None
- **Summary**: End-users, without knowledge in photography, desire to beautify their photos to have a similar color style as a well-retouched reference. However, the definition of style in recent image style transfer works is inappropriate. They usually synthesize undesirable results due to transferring exact colors to the wrong destination. It becomes even worse in sensitive cases such as portraits. In this work, we concentrate on learning low-level image transformation, especially color-shifting methods, rather than mixing contextual features, then present a novel scheme to train color style transfer with ground-truth. Furthermore, we propose a color style transfer named Deep Preset. It is designed to 1) generalize the features representing the color transformation from content with natural colors to retouched reference, then blend it into the contextual features of content, 2) predict hyper-parameters (settings or preset) of the applied low-level color transformation methods, 3) stylize content to have a similar color style as reference. We script Lightroom, a powerful tool in editing photos, to generate 600,000 training samples using 1,200 images from the Flick2K dataset and 500 user-generated presets with 69 settings. Experimental results show that our Deep Preset outperforms the previous works in color style transfer quantitatively and qualitatively.



### Uncertainty-Aware Weakly Supervised Action Detection from Untrimmed Videos
- **Arxiv ID**: http://arxiv.org/abs/2007.10703v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.10703v1)
- **Published**: 2020-07-21 10:45:05+00:00
- **Updated**: 2020-07-21 10:45:05+00:00
- **Authors**: Anurag Arnab, Chen Sun, Arsha Nagrani, Cordelia Schmid
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: Despite the recent advances in video classification, progress in spatio-temporal action recognition has lagged behind. A major contributing factor has been the prohibitive cost of annotating videos frame-by-frame. In this paper, we present a spatio-temporal action recognition model that is trained with only video-level labels, which are significantly easier to annotate. Our method leverages per-frame person detectors which have been trained on large image datasets within a Multiple Instance Learning framework. We show how we can apply our method in cases where the standard Multiple Instance Learning assumption, that each bag contains at least one instance with the specified label, is invalid using a novel probabilistic variant of MIL where we estimate the uncertainty of each prediction. Furthermore, we report the first weakly-supervised results on the AVA dataset and state-of-the-art results among weakly-supervised methods on UCF101-24.



### SparseTrain: Exploiting Dataflow Sparsity for Efficient Convolutional Neural Networks Training
- **Arxiv ID**: http://arxiv.org/abs/2007.13595v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AR
- **Links**: [PDF](http://arxiv.org/pdf/2007.13595v1)
- **Published**: 2020-07-21 11:01:36+00:00
- **Updated**: 2020-07-21 11:01:36+00:00
- **Authors**: Pengcheng Dai, Jianlei Yang, Xucheng Ye, Xingzhou Cheng, Junyu Luo, Linghao Song, Yiran Chen, Weisheng Zhao
- **Comment**: published on DAC 2020
- **Journal**: None
- **Summary**: Training Convolutional Neural Networks (CNNs) usually requires a large number of computational resources. In this paper, \textit{SparseTrain} is proposed to accelerate CNN training by fully exploiting the sparsity. It mainly involves three levels of innovations: activation gradients pruning algorithm, sparse training dataflow, and accelerator architecture. By applying a stochastic pruning algorithm on each layer, the sparsity of back-propagation gradients can be increased dramatically without degrading training accuracy and convergence rate. Moreover, to utilize both \textit{natural sparsity} (resulted from ReLU or Pooling layers) and \textit{artificial sparsity} (brought by pruning algorithm), a sparse-aware architecture is proposed for training acceleration. This architecture supports forward and back-propagation of CNN by adopting 1-Dimensional convolution dataflow. We have built %a simple compiler to map CNNs topology onto \textit{SparseTrain}, and a cycle-accurate architecture simulator to evaluate the performance and efficiency based on the synthesized design with $14nm$ FinFET technologies. Evaluation results on AlexNet/ResNet show that \textit{SparseTrain} could achieve about $2.7 \times$ speedup and $2.2 \times$ energy efficiency improvement on average compared with the original training process.



### Balance Scene Learning Mechanism for Offshore and Inshore Ship Detection in SAR Images
- **Arxiv ID**: http://arxiv.org/abs/2007.10714v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.10714v2)
- **Published**: 2020-07-21 11:12:05+00:00
- **Updated**: 2020-09-21 12:55:32+00:00
- **Authors**: Tianwen Zhang, Xiaoling Zhang, Jun Shi, Shunjun Wei, Jianguo Wang, Jianwei Li, Hao Su, Yue Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: Huge imbalance of different scenes' sample numbers seriously reduces Synthetic Aperture Radar (SAR) ship detection accuracy. Thus, to solve this problem, this letter proposes a Balance Scene Learning Mechanism (BSLM) for offshore and inshore ship detection in SAR images.



### Optimization of data-driven filterbank for automatic speaker verification
- **Arxiv ID**: http://arxiv.org/abs/2007.10729v1
- **DOI**: 10.1016/j.dsp.2020.102795
- **Categories**: **eess.AS**, cs.CV, cs.SD
- **Links**: [PDF](http://arxiv.org/pdf/2007.10729v1)
- **Published**: 2020-07-21 11:42:20+00:00
- **Updated**: 2020-07-21 11:42:20+00:00
- **Authors**: Susanta Sarangi, Md Sahidullah, Goutam Saha
- **Comment**: Published in Digital Signal Processing journal (Elsevier)
- **Journal**: None
- **Summary**: Most of the speech processing applications use triangular filters spaced in mel-scale for feature extraction. In this paper, we propose a new data-driven filter design method which optimizes filter parameters from a given speech data. First, we introduce a frame-selection based approach for developing speech-signal-based frequency warping scale. Then, we propose a new method for computing the filter frequency responses by using principal component analysis (PCA). The main advantage of the proposed method over the recently introduced deep learning based methods is that it requires very limited amount of unlabeled speech-data. We demonstrate that the proposed filterbank has more speaker discriminative power than commonly used mel filterbank as well as existing data-driven filterbank. We conduct automatic speaker verification (ASV) experiments with different corpora using various classifier back-ends. We show that the acoustic features created with proposed filterbank are better than existing mel-frequency cepstral coefficients (MFCCs) and speech-signal-based frequency cepstral coefficients (SFCCs) in most cases. In the experiments with VoxCeleb1 and popular i-vector back-end, we observe 9.75% relative improvement in equal error rate (EER) over MFCCs. Similarly, the relative improvement is 4.43% with recently introduced x-vector system. We obtain further improvement using fusion of the proposed method with standard MFCC-based approach.



### Video Representation Learning by Recognizing Temporal Transformations
- **Arxiv ID**: http://arxiv.org/abs/2007.10730v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.10730v1)
- **Published**: 2020-07-21 11:43:01+00:00
- **Updated**: 2020-07-21 11:43:01+00:00
- **Authors**: Simon Jenni, Givi Meishvili, Paolo Favaro
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: We introduce a novel self-supervised learning approach to learn representations of videos that are responsive to changes in the motion dynamics. Our representations can be learned from data without human annotation and provide a substantial boost to the training of neural networks on small labeled data sets for tasks such as action recognition, which require to accurately distinguish the motion of objects. We promote an accurate learning of motion without human annotation by training a neural network to discriminate a video sequence from its temporally transformed versions. To learn to distinguish non-trivial motions, the design of the transformations is based on two principles: 1) To define clusters of motions based on time warps of different magnitude; 2) To ensure that the discrimination is feasible only by observing and analyzing as many image frames as possible. Thus, we introduce the following transformations: forward-backward playback, random frame skipping, and uniform frame skipping. Our experiments show that networks trained with the proposed method yield representations with improved transfer performance for action recognition on UCF101 and HMDB51.



### Shape-aware Semi-supervised 3D Semantic Segmentation for Medical Images
- **Arxiv ID**: http://arxiv.org/abs/2007.10732v1
- **DOI**: 10.1007/978-3-030-59710-8_54
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.10732v1)
- **Published**: 2020-07-21 11:44:52+00:00
- **Updated**: 2020-07-21 11:44:52+00:00
- **Authors**: Shuailin Li, Chuyu Zhang, Xuming He
- **Comment**: Appear in MICCAI2020
- **Journal**: MICCAI 2020. Lecture Notes in Computer Science, vol 12261.
  Springer, Cham
- **Summary**: Semi-supervised learning has attracted much attention in medical image segmentation due to challenges in acquiring pixel-wise image annotations, which is a crucial step for building high-performance deep learning methods. Most existing semi-supervised segmentation approaches either tend to neglect geometric constraint in object segments, leading to incomplete object coverage, or impose strong shape prior that requires extra alignment. In this work, we propose a novel shapeaware semi-supervised segmentation strategy to leverage abundant unlabeled data and to enforce a geometric shape constraint on the segmentation output. To achieve this, we develop a multi-task deep network that jointly predicts semantic segmentation and signed distance map(SDM) of object surfaces. During training, we introduce an adversarial loss between the predicted SDMs of labeled and unlabeled data so that our network is able to capture shape-aware features more effectively. Experiments on the Atrial Segmentation Challenge dataset show that our method outperforms current state-of-the-art approaches with improved shape estimation, which validates its efficacy. Code is available at https://github.com/kleinzcy/SASSnet.



### A Comprehensive Review of Skeleton-based Movement Assessment Methods
- **Arxiv ID**: http://arxiv.org/abs/2007.10737v3
- **DOI**: 10.13140/RG.2.2.26189.46569/1
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.10737v3)
- **Published**: 2020-07-21 11:58:29+00:00
- **Updated**: 2020-07-29 23:29:30+00:00
- **Authors**: Tal Hakim
- **Comment**: None
- **Journal**: None
- **Summary**: The raising availability of 3D cameras and dramatic improvement of computer vision algorithms in the recent decade, accelerated the research of automatic movement assessment solutions. Such solutions can be implemented at home, using affordable equipment and dedicated software. In this paper, we divide the movement assessment task into secondary tasks and explain why they are needed and how they can be addressed. We review the recent solutions for automatic movement assessment from skeleton videos, comparing them by their objectives, features, movement domains and algorithmic approaches. In addition, we discuss the status of the research on this topic in a high level.



### Balanced Meta-Softmax for Long-Tailed Visual Recognition
- **Arxiv ID**: http://arxiv.org/abs/2007.10740v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.10740v3)
- **Published**: 2020-07-21 12:05:00+00:00
- **Updated**: 2020-11-22 05:27:41+00:00
- **Authors**: Jiawei Ren, Cunjun Yu, Shunan Sheng, Xiao Ma, Haiyu Zhao, Shuai Yi, Hongsheng Li
- **Comment**: NeurIPS 2020 camera-ready; Code available at
  https://github.com/jiawei-ren/BalancedMetaSoftmax
- **Journal**: None
- **Summary**: Deep classifiers have achieved great success in visual recognition. However, real-world data is long-tailed by nature, leading to the mismatch between training and testing distributions. In this paper, we show that the Softmax function, though used in most classification tasks, gives a biased gradient estimation under the long-tailed setup. This paper presents Balanced Softmax, an elegant unbiased extension of Softmax, to accommodate the label distribution shift between training and testing. Theoretically, we derive the generalization bound for multiclass Softmax regression and show our loss minimizes the bound. In addition, we introduce Balanced Meta-Softmax, applying a complementary Meta Sampler to estimate the optimal class sample rate and further improve long-tailed learning. In our experiments, we demonstrate that Balanced Meta-Softmax outperforms state-of-the-art long-tailed classification solutions on both visual recognition and instance segmentation tasks.



### Enhancement of damaged-image prediction through Cahn-Hilliard Image Inpainting
- **Arxiv ID**: http://arxiv.org/abs/2007.10753v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.NA, eess.IV, math.NA, 68U10, 94A08, 65M22, 76M25, 76M12
- **Links**: [PDF](http://arxiv.org/pdf/2007.10753v2)
- **Published**: 2020-07-21 12:29:43+00:00
- **Updated**: 2021-03-15 17:26:56+00:00
- **Authors**: José A. Carrillo, Serafim Kalliadasis, Fuyue Liang, Sergio P. Perez
- **Comment**: An interactive jupyter notebook with the code of this work is
  available at https://github.com/sergiopperez/Image_Inpainting. The MNIST
  dataset employed in this work can be downloaded from
  http://yann.lecun.com/exdb/mnist/
- **Journal**: None
- **Summary**: We assess the benefit of including an image inpainting filter before passing damaged images into a classification neural network. For this we employ a modified Cahn-Hilliard equation as an image inpainting filter, which is solved via a finite volume scheme with reduced computational cost and adequate properties for energy stability and boundedness. The benchmark dataset employed here is MNIST, which consists of binary images of handwritten digits and is a standard dataset to validate image-processing methodologies. We train a neural network based of dense layers with the training set of MNIST, and subsequently we contaminate the test set with damage of different types and intensities. We then compare the prediction accuracy of the neural network with and without applying the Cahn-Hilliard filter to the damaged images test. Our results quantify the significant improvement of damaged-image prediction due to applying the Cahn-Hilliard filter, which for specific damages can increase up to 50% and is in general advantageous for low to moderate damage.



### Backdoor Attacks and Countermeasures on Deep Learning: A Comprehensive Review
- **Arxiv ID**: http://arxiv.org/abs/2007.10760v3
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.10760v3)
- **Published**: 2020-07-21 12:49:12+00:00
- **Updated**: 2020-08-02 08:38:25+00:00
- **Authors**: Yansong Gao, Bao Gia Doan, Zhi Zhang, Siqi Ma, Jiliang Zhang, Anmin Fu, Surya Nepal, Hyoungshick Kim
- **Comment**: 29 pages, 9 figures, 2 tables
- **Journal**: None
- **Summary**: This work provides the community with a timely comprehensive review of backdoor attacks and countermeasures on deep learning. According to the attacker's capability and affected stage of the machine learning pipeline, the attack surfaces are recognized to be wide and then formalized into six categorizations: code poisoning, outsourcing, pretrained, data collection, collaborative learning and post-deployment. Accordingly, attacks under each categorization are combed. The countermeasures are categorized into four general classes: blind backdoor removal, offline backdoor inspection, online backdoor inspection, and post backdoor removal. Accordingly, we review countermeasures, and compare and analyze their advantages and disadvantages. We have also reviewed the flip side of backdoor attacks, which are explored for i) protecting intellectual property of deep learning models, ii) acting as a honeypot to catch adversarial example attacks, and iii) verifying data deletion requested by the data contributor.Overall, the research on defense is far behind the attack, and there is no single defense that can prevent all types of backdoor attacks. In some cases, an attacker can intelligently bypass existing defenses with an adaptive attack. Drawing the insights from the systematic review, we also present key areas for future research on the backdoor, such as empirical security evaluations from physical trigger attacks, and in particular, more efficient and practical countermeasures are solicited.



### Complementing Representation Deficiency in Few-shot Image Classification: A Meta-Learning Approach
- **Arxiv ID**: http://arxiv.org/abs/2007.10778v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.10778v1)
- **Published**: 2020-07-21 13:25:54+00:00
- **Updated**: 2020-07-21 13:25:54+00:00
- **Authors**: Xian Zhong, Cheng Gu, Wenxin Huang, Lin Li, Shuqin Chen, Chia-Wen Lin
- **Comment**: 25th International Conference on Pattern Recognition (ICPR2020)
- **Journal**: None
- **Summary**: Few-shot learning is a challenging problem that has attracted more and more attention recently since abundant training samples are difficult to obtain in practical applications. Meta-learning has been proposed to address this issue, which focuses on quickly adapting a predictor as a base-learner to new tasks, given limited labeled samples. However, a critical challenge for meta-learning is the representation deficiency since it is hard to discover common information from a small number of training samples or even one, as is the representation of key features from such little information. As a result, a meta-learner cannot be trained well in a high-dimensional parameter space to generalize to new tasks. Existing methods mostly resort to extracting less expressive features so as to avoid the representation deficiency. Aiming at learning better representations, we propose a meta-learning approach with complemented representations network (MCRNet) for few-shot image classification. In particular, we embed a latent space, where latent codes are reconstructed with extra representation information to complement the representation deficiency. Furthermore, the latent space is established with variational inference, collaborating well with different base-learners, and can be extended to other models. Finally, our end-to-end framework achieves the state-of-the-art performance in image classification on three standard few-shot learning datasets.



### Deep Semi-supervised Knowledge Distillation for Overlapping Cervical Cell Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2007.10787v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.10787v1)
- **Published**: 2020-07-21 13:27:09+00:00
- **Updated**: 2020-07-21 13:27:09+00:00
- **Authors**: Yanning Zhou, Hao Chen, Huangjing Lin, Pheng-Ann Heng
- **Comment**: to appear at MICCAI2020, supplementary material attached
- **Journal**: None
- **Summary**: Deep learning methods show promising results for overlapping cervical cell instance segmentation. However, in order to train a model with good generalization ability, voluminous pixel-level annotations are demanded which is quite expensive and time-consuming for acquisition. In this paper, we propose to leverage both labeled and unlabeled data for instance segmentation with improved accuracy by knowledge distillation. We propose a novel Mask-guided Mean Teacher framework with Perturbation-sensitive Sample Mining (MMT-PSM), which consists of a teacher and a student network during training. Two networks are encouraged to be consistent both in feature and semantic level under small perturbations. The teacher's self-ensemble predictions from $K$-time augmented samples are used to construct the reliable pseudo-labels for optimizing the student. We design a novel strategy to estimate the sensitivity to perturbations for each proposal and select informative samples from massive cases to facilitate fast and effective semantic distillation. In addition, to eliminate the unavoidable noise from the background region, we propose to use the predicted segmentation mask as guidance to enforce the feature distillation in the foreground region. Experiments show that the proposed method improves the performance significantly compared with the supervised method learned from labeled data only, and outperforms state-of-the-art semi-supervised methods.



### Split and Expand: An inference-time improvement for Weakly Supervised Cell Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2007.10817v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.10817v3)
- **Published**: 2020-07-21 14:05:09+00:00
- **Updated**: 2022-03-14 05:59:46+00:00
- **Authors**: Lin Geng Foo, Rui En Ho, Jiamei Sun, Alexander Binder
- **Comment**: None
- **Journal**: None
- **Summary**: We consider the problem of segmenting cell nuclei instances from Hematoxylin and Eosin (H&E) stains with weak supervision. While most recent works focus on improving the segmentation quality, this is usually insufficient for instance segmentation of cell instances clumped together or with a small size. In this work, we propose a two-step post-processing procedure, Split and Expand, that directly improves the conversion of segmentation maps to instances. In the Split step, we split clumps of cells from the segmentation map into individual cell instances with the guidance of cell-center predictions through Gaussian Mixture Model clustering. In the Expand step, we find missing small cells using the cell-center predictions (which tend to capture small cells more consistently as they are trained using reliable point annotations), and utilize Layer-wise Relevance Propagation (LRP) explanation results to expand those cell-center predictions into cell instances. Our Split and Expand post-processing procedure is training-free and is executed at inference-time only. To further improve the performance of our method, a feature re-weighting loss based on LRP is proposed. We test our procedure on the MoNuSeg and TNBC datasets and show that our proposed method provides statistically significant improvements on object-level metrics. Our code will be made available.



### IITK at SemEval-2020 Task 8: Unimodal and Bimodal Sentiment Analysis of Internet Memes
- **Arxiv ID**: http://arxiv.org/abs/2007.10822v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.10822v1)
- **Published**: 2020-07-21 14:06:26+00:00
- **Updated**: 2020-07-21 14:06:26+00:00
- **Authors**: Vishal Keswani, Sakshi Singh, Suryansh Agarwal, Ashutosh Modi
- **Comment**: 7 pages, 2 figures, 3 tables. Accepted at Proceedings of the 14th
  International Workshop on Semantic Evaluation (SemEval-2020)
- **Journal**: None
- **Summary**: Social media is abundant in visual and textual information presented together or in isolation. Memes are the most popular form, belonging to the former class. In this paper, we present our approaches for the Memotion Analysis problem as posed in SemEval-2020 Task 8. The goal of this task is to classify memes based on their emotional content and sentiment. We leverage techniques from Natural Language Processing (NLP) and Computer Vision (CV) towards the sentiment classification of internet memes (Subtask A). We consider Bimodal (text and image) as well as Unimodal (text-only) techniques in our study ranging from the Na\"ive Bayes classifier to Transformer-based approaches. Our results show that a text-only approach, a simple Feed Forward Neural Network (FFNN) with Word2vec embeddings as input, performs superior to all the others. We stand first in the Sentiment analysis task with a relative improvement of 63% over the baseline macro-F1 score. Our work is relevant to any task concerned with the combination of different modalities.



### Soft Expert Reward Learning for Vision-and-Language Navigation
- **Arxiv ID**: http://arxiv.org/abs/2007.10835v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.10835v1)
- **Published**: 2020-07-21 14:17:36+00:00
- **Updated**: 2020-07-21 14:17:36+00:00
- **Authors**: Hu Wang, Qi Wu, Chunhua Shen
- **Comment**: None
- **Journal**: None
- **Summary**: Vision-and-Language Navigation (VLN) requires an agent to find a specified spot in an unseen environment by following natural language instructions. Dominant methods based on supervised learning clone expert's behaviours and thus perform better on seen environments, while showing restricted performance on unseen ones. Reinforcement Learning (RL) based models show better generalisation ability but have issues as well, requiring large amount of manual reward engineering is one of which. In this paper, we introduce a Soft Expert Reward Learning (SERL) model to overcome the reward engineering designing and generalisation problems of the VLN task. Our proposed method consists of two complementary components: Soft Expert Distillation (SED) module encourages agents to behave like an expert as much as possible, but in a soft fashion; Self Perceiving (SP) module targets at pushing the agent towards the final destination as fast as possible. Empirically, we evaluate our model on the VLN seen, unseen and test splits and the model outperforms the state-of-the-art methods on most of the evaluation metrics.



### Joint Visual and Temporal Consistency for Unsupervised Domain Adaptive Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2007.10854v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.10854v1)
- **Published**: 2020-07-21 14:31:27+00:00
- **Updated**: 2020-07-21 14:31:27+00:00
- **Authors**: Jianing Li, Shiliang Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised domain adaptive person Re-IDentification (ReID) is challenging because of the large domain gap between source and target domains, as well as the lackage of labeled data on the target domain. This paper tackles this challenge through jointly enforcing visual and temporal consistency in the combination of a local one-hot classification and a global multi-class classification. The local one-hot classification assigns images in a training batch with different person IDs, then adopts a Self-Adaptive Classification (SAC) model to classify them. The global multi-class classification is achieved by predicting labels on the entire unlabeled training set with the Memory-based Temporal-guided Cluster (MTC). MTC predicts multi-class labels by considering both visual similarity and temporal consistency to ensure the quality of label prediction. The two classification models are combined in a unified framework, which effectively leverages the unlabeled data for discriminative feature learning. Experimental results on three large-scale ReID datasets demonstrate the superiority of proposed method in both unsupervised and unsupervised domain adaptive ReID tasks. For example, under unsupervised setting, our method outperforms recent unsupervised domain adaptive methods, which leverage more labels for training.



### Multi-label Thoracic Disease Image Classification with Cross-Attention Networks
- **Arxiv ID**: http://arxiv.org/abs/2007.10859v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.10859v1)
- **Published**: 2020-07-21 14:37:00+00:00
- **Updated**: 2020-07-21 14:37:00+00:00
- **Authors**: Congbo Ma, Hu Wang, Steven C. H. Hoi
- **Comment**: None
- **Journal**: None
- **Summary**: Automated disease classification of radiology images has been emerging as a promising technique to support clinical diagnosis and treatment planning. Unlike generic image classification tasks, a real-world radiology image classification task is significantly more challenging as it is far more expensive to collect the training data where the labeled data is in nature multi-label; and more seriously samples from easy classes often dominate; training data is highly class-imbalanced problem exists in practice as well. To overcome these challenges, in this paper, we propose a novel scheme of Cross-Attention Networks (CAN) for automated thoracic disease classification from chest x-ray images, which can effectively excavate more meaningful representation from data to boost the performance through cross-attention by only image-level annotations. We also design a new loss function that beyond cross-entropy loss to help cross-attention process and is able to overcome the imbalance between classes and easy-dominated samples within each class. The proposed method achieves state-of-the-art results.



### Dense Hybrid Recurrent Multi-view Stereo Net with Dynamic Consistency Checking
- **Arxiv ID**: http://arxiv.org/abs/2007.10872v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.10872v1)
- **Published**: 2020-07-21 14:59:59+00:00
- **Updated**: 2020-07-21 14:59:59+00:00
- **Authors**: Jianfeng Yan, Zizhuang Wei, Hongwei Yi, Mingyu Ding, Runze Zhang, Yisong Chen, Guoping Wang, Yu-Wing Tai
- **Comment**: Accepted by ECCV2020 as Spotlight
- **Journal**: ECCV2020
- **Summary**: In this paper, we propose an efficient and effective dense hybrid recurrent multi-view stereo net with dynamic consistency checking, namely $D^{2}$HC-RMVSNet, for accurate dense point cloud reconstruction. Our novel hybrid recurrent multi-view stereo net consists of two core modules: 1) a light DRENet (Dense Reception Expanded) module to extract dense feature maps of original size with multi-scale context information, 2) a HU-LSTM (Hybrid U-LSTM) to regularize 3D matching volume into predicted depth map, which efficiently aggregates different scale information by coupling LSTM and U-Net architecture. To further improve the accuracy and completeness of reconstructed point clouds, we leverage a dynamic consistency checking strategy instead of prefixed parameters and strategies widely adopted in existing methods for dense point cloud reconstruction. In doing so, we dynamically aggregate geometric consistency matching error among all the views. Our method ranks \textbf{$1^{st}$} on the complex outdoor \textsl{Tanks and Temples} benchmark over all the methods. Extensive experiments on the in-door DTU dataset show our method exhibits competitive performance to the state-of-the-art method while dramatically reduces memory consumption, which costs only $19.4\%$ of R-MVSNet memory consumption. The codebase is available at \hyperlink{https://github.com/yhw-yhw/D2HC-RMVSNet}{https://github.com/yhw-yhw/D2HC-RMVSNet}.



### Representative-Discriminative Learning for Open-set Land Cover Classification of Satellite Imagery
- **Arxiv ID**: http://arxiv.org/abs/2007.10891v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.10891v1)
- **Published**: 2020-07-21 15:28:56+00:00
- **Updated**: 2020-07-21 15:28:56+00:00
- **Authors**: Razieh Kaviani Baghbaderani, Ying Qu, Hairong Qi, Craig Stutts
- **Comment**: 20 pages, 10 figures, European Conference on Computer Vision (ECCV)
  2020
- **Journal**: None
- **Summary**: Land cover classification of satellite imagery is an important step toward analyzing the Earth's surface. Existing models assume a closed-set setting where both the training and testing classes belong to the same label set. However, due to the unique characteristics of satellite imagery with an extremely vast area of versatile cover materials, the training data are bound to be non-representative. In this paper, we study the problem of open-set land cover classification that identifies the samples belonging to unknown classes during testing, while maintaining performance on known classes. Although inherently a classification problem, both representative and discriminative aspects of data need to be exploited in order to better distinguish unknown classes from known. We propose a representative-discriminative open-set recognition (RDOSR) framework, which 1) projects data from the raw image space to the embedding feature space that facilitates differentiating similar classes, and further 2) enhances both the representative and discriminative capacity through transformation to a so-called abundance space. Experiments on multiple satellite benchmarks demonstrate the effectiveness of the proposed method. We also show the generality of the proposed approach by achieving promising results on open-set classification tasks using RGB images.



### Towards Nonlinear Disentanglement in Natural Data with Temporal Sparse Coding
- **Arxiv ID**: http://arxiv.org/abs/2007.10930v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.10930v2)
- **Published**: 2020-07-21 16:46:05+00:00
- **Updated**: 2021-03-17 14:20:05+00:00
- **Authors**: David Klindt, Lukas Schott, Yash Sharma, Ivan Ustyuzhaninov, Wieland Brendel, Matthias Bethge, Dylan Paiton
- **Comment**: ICLR 2021. Code is available at
  https://github.com/bethgelab/slow_disentanglement. The first three authors,
  as well as the last two authors, contributed equally
- **Journal**: None
- **Summary**: We construct an unsupervised learning model that achieves nonlinear disentanglement of underlying factors of variation in naturalistic videos. Previous work suggests that representations can be disentangled if all but a few factors in the environment stay constant at any point in time. As a result, algorithms proposed for this problem have only been tested on carefully constructed datasets with this exact property, leaving it unclear whether they will transfer to natural scenes. Here we provide evidence that objects in segmented natural movies undergo transitions that are typically small in magnitude with occasional large jumps, which is characteristic of a temporally sparse distribution. We leverage this finding and present SlowVAE, a model for unsupervised representation learning that uses a sparse prior on temporally adjacent observations to disentangle generative factors without any assumptions on the number of changing factors. We provide a proof of identifiability and show that the model reliably learns disentangled representations on several established benchmark datasets, often surpassing the current state-of-the-art. We additionally demonstrate transferability towards video datasets with natural dynamics, Natural Sprites and KITTI Masks, which we contribute as benchmarks for guiding disentanglement research towards more natural data domains.



### MovieNet: A Holistic Dataset for Movie Understanding
- **Arxiv ID**: http://arxiv.org/abs/2007.10937v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.10937v1)
- **Published**: 2020-07-21 16:54:33+00:00
- **Updated**: 2020-07-21 16:54:33+00:00
- **Authors**: Qingqiu Huang, Yu Xiong, Anyi Rao, Jiaze Wang, Dahua Lin
- **Comment**: Accepted by ECCV2020 as spotlight presentation. Project page:
  http://movienet.site
- **Journal**: None
- **Summary**: Recent years have seen remarkable advances in visual understanding. However, how to understand a story-based long video with artistic styles, e.g. movie, remains challenging. In this paper, we introduce MovieNet -- a holistic dataset for movie understanding. MovieNet contains 1,100 movies with a large amount of multi-modal data, e.g. trailers, photos, plot descriptions, etc. Besides, different aspects of manual annotations are provided in MovieNet, including 1.1M characters with bounding boxes and identities, 42K scene boundaries, 2.5K aligned description sentences, 65K tags of place and action, and 92K tags of cinematic style. To the best of our knowledge, MovieNet is the largest dataset with richest annotations for comprehensive movie understanding. Based on MovieNet, we set up several benchmarks for movie understanding from different angles. Extensive experiments are executed on these benchmarks to show the immeasurable value of MovieNet and the gap of current approaches towards comprehensive movie understanding. We believe that such a holistic dataset would promote the researches on story-based long video understanding and beyond. MovieNet will be published in compliance with regulations at https://movienet.github.io.



### Garment Design with Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2007.10947v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.10947v2)
- **Published**: 2020-07-21 17:03:33+00:00
- **Updated**: 2020-07-23 00:59:37+00:00
- **Authors**: Chenxi Yuan, Mohsen Moghaddam
- **Comment**: AdvML 2020, KDD workshop
- **Journal**: None
- **Summary**: The designers' tendency to adhere to a specific mental set and heavy emotional investment in their initial ideas often hinder their ability to innovate during the design thinking and ideation process. In the fashion industry, in particular, the growing diversity of customers' needs, the intense global competition, and the shrinking time-to-market (a.k.a., "fast fashion") further exacerbate this challenge for designers. Recent advances in deep generative models have created new possibilities to overcome the cognitive obstacles of designers through automated generation and/or editing of design concepts. This paper explores the capabilities of generative adversarial networks (GAN) for automated attribute-level editing of design concepts. Specifically, attribute GAN (AttGAN)---a generative model proven successful for attribute editing of human faces---is utilized for automated editing of the visual attributes of garments and tested on a large fashion dataset. The experiments support the hypothesized potentials of GAN for attribute-level editing of design concepts, and underscore several key limitations and research questions to be addressed in future work.



### Procrustean Regression Networks: Learning 3D Structure of Non-Rigid Objects from 2D Annotations
- **Arxiv ID**: http://arxiv.org/abs/2007.10961v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.10961v1)
- **Published**: 2020-07-21 17:29:20+00:00
- **Updated**: 2020-07-21 17:29:20+00:00
- **Authors**: Sungheon Park, Minsik Lee, Nojun Kwak
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: We propose a novel framework for training neural networks which is capable of learning 3D information of non-rigid objects when only 2D annotations are available as ground truths. Recently, there have been some approaches that incorporate the problem setting of non-rigid structure-from-motion (NRSfM) into deep learning to learn 3D structure reconstruction. The most important difficulty of NRSfM is to estimate both the rotation and deformation at the same time, and previous works handle this by regressing both of them. In this paper, we resolve this difficulty by proposing a loss function wherein the suitable rotation is automatically determined. Trained with the cost function consisting of the reprojection error and the low-rank term of aligned shapes, the network learns the 3D structures of such objects as human skeletons and faces during the training, whereas the testing is done in a single-frame basis. The proposed method can handle inputs with missing entries and experimental results validate that the proposed framework shows superior reconstruction performance to the state-of-the-art method on the Human 3.6M, 300-VW, and SURREAL datasets, even though the underlying network structure is very simple.



### Recurrent Exposure Generation for Low-Light Face Detection
- **Arxiv ID**: http://arxiv.org/abs/2007.10963v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.10963v1)
- **Published**: 2020-07-21 17:30:51+00:00
- **Updated**: 2020-07-21 17:30:51+00:00
- **Authors**: Jinxiu Liang, Jingwen Wang, Yuhui Quan, Tianyi Chen, Jiaying Liu, Haibin Ling, Yong Xu
- **Comment**: 11 pages
- **Journal**: None
- **Summary**: Face detection from low-light images is challenging due to limited photos and inevitable noise, which, to make the task even harder, are often spatially unevenly distributed. A natural solution is to borrow the idea from multi-exposure, which captures multiple shots to obtain well-exposed images under challenging conditions. High-quality implementation/approximation of multi-exposure from a single image is however nontrivial. Fortunately, as shown in this paper, neither is such high-quality necessary since our task is face detection rather than image enhancement. Specifically, we propose a novel Recurrent Exposure Generation (REG) module and couple it seamlessly with a Multi-Exposure Detection (MED) module, and thus significantly improve face detection performance by effectively inhibiting non-uniform illumination and noise issues. REG produces progressively and efficiently intermediate images corresponding to various exposure settings, and such pseudo-exposures are then fused by MED to detect faces across different lighting conditions. The proposed method, named REGDet, is the first `detection-with-enhancement' framework for low-light face detection. It not only encourages rich interaction and feature fusion across different illumination levels, but also enables effective end-to-end learning of the REG component to be better tailored for face detection. Moreover, as clearly shown in our experiments, REG can be flexibly coupled with different face detectors without extra low/normal-light image pairs for training. We tested REGDet on the DARK FACE low-light face benchmark with thorough ablation study, where REGDet outperforms previous state-of-the-arts by a significant margin, with only negligible extra parameters.



### Neural Mesh Flow: 3D Manifold Mesh Generation via Diffeomorphic Flows
- **Arxiv ID**: http://arxiv.org/abs/2007.10973v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.10973v2)
- **Published**: 2020-07-21 17:45:41+00:00
- **Updated**: 2020-12-02 17:00:19+00:00
- **Authors**: Kunal Gupta, Manmohan Chandraker
- **Comment**: Project Page:
  https://kunalmgupta.github.io/projects/NeuralMeshflow.html
- **Journal**: None
- **Summary**: Meshes are important representations of physical 3D entities in the virtual world. Applications like rendering, simulations and 3D printing require meshes to be manifold so that they can interact with the world like the real objects they represent. Prior methods generate meshes with great geometric accuracy but poor manifoldness. In this work, we propose Neural Mesh Flow (NMF) to generate two-manifold meshes for genus-0 shapes. Specifically, NMF is a shape auto-encoder consisting of several Neural Ordinary Differential Equation (NODE)[1] blocks that learn accurate mesh geometry by progressively deforming a spherical mesh. Training NMF is simpler compared to state-of-the-art methods since it does not require any explicit mesh-based regularization. Our experiments demonstrate that NMF facilitates several applications such as single-view mesh reconstruction, global shape parameterization, texture mapping, shape deformation and correspondence. Importantly, we demonstrate that manifold meshes generated using NMF are better-suited for physically-based rendering and simulation. Code and data are released.



### Shape and Viewpoint without Keypoints
- **Arxiv ID**: http://arxiv.org/abs/2007.10982v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.10982v1)
- **Published**: 2020-07-21 17:58:28+00:00
- **Updated**: 2020-07-21 17:58:28+00:00
- **Authors**: Shubham Goel, Angjoo Kanazawa, Jitendra Malik
- **Comment**: Accepted at ECCV 2020
- **Journal**: None
- **Summary**: We present a learning framework that learns to recover the 3D shape, pose and texture from a single image, trained on an image collection without any ground truth 3D shape, multi-view, camera viewpoints or keypoint supervision. We approach this highly under-constrained problem in a "analysis by synthesis" framework where the goal is to predict the likely shape, texture and camera viewpoint that could produce the image with various learned category-specific priors. Our particular contribution in this paper is a representation of the distribution over cameras, which we call "camera-multiplex". Instead of picking a point estimate, we maintain a set of camera hypotheses that are optimized during training to best explain the image given the current shape and texture. We call our approach Unsupervised Category-Specific Mesh Reconstruction (U-CMR), and present qualitative and quantitative results on CUB, Pascal 3D and new web-scraped datasets. We obtain state-of-the-art camera prediction results and show that we can learn to predict diverse shapes and textures across objects using an image collection without any keypoint annotations or 3D ground truth. Project page: https://shubham-goel.github.io/ucmr



### Learning Monocular Visual Odometry via Self-Supervised Long-Term Modeling
- **Arxiv ID**: http://arxiv.org/abs/2007.10983v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.10983v1)
- **Published**: 2020-07-21 17:59:01+00:00
- **Updated**: 2020-07-21 17:59:01+00:00
- **Authors**: Yuliang Zou, Pan Ji, Quoc-Huy Tran, Jia-Bin Huang, Manmohan Chandraker
- **Comment**: ECCV 2020. Project page: https://yuliang.vision/LTMVO
- **Journal**: None
- **Summary**: Monocular visual odometry (VO) suffers severely from error accumulation during frame-to-frame pose estimation. In this paper, we present a self-supervised learning method for VO with special consideration for consistency over longer sequences. To this end, we model the long-term dependency in pose prediction using a pose network that features a two-layer convolutional LSTM module. We train the networks with purely self-supervised losses, including a cycle consistency loss that mimics the loop closure module in geometric VO. Inspired by prior geometric systems, we allow the networks to see beyond a small temporal window during training, through a novel a loss that incorporates temporally distant (e.g., O(100)) frames. Given GPU memory constraints, we propose a stage-wise training mechanism, where the first stage operates in a local time window and the second stage refines the poses with a "global" loss given the first stage features. We demonstrate competitive results on several standard VO datasets, including KITTI and TUM RGB-D.



### Foley Music: Learning to Generate Music from Videos
- **Arxiv ID**: http://arxiv.org/abs/2007.10984v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2007.10984v1)
- **Published**: 2020-07-21 17:59:06+00:00
- **Updated**: 2020-07-21 17:59:06+00:00
- **Authors**: Chuang Gan, Deng Huang, Peihao Chen, Joshua B. Tenenbaum, Antonio Torralba
- **Comment**: ECCV 2020. Project page: http://foley-music.csail.mit.edu
- **Journal**: None
- **Summary**: In this paper, we introduce Foley Music, a system that can synthesize plausible music for a silent video clip about people playing musical instruments. We first identify two key intermediate representations for a successful video to music generator: body keypoints from videos and MIDI events from audio recordings. We then formulate music generation from videos as a motion-to-MIDI translation problem. We present a Graph$-$Transformer framework that can accurately predict MIDI event sequences in accordance with the body movements. The MIDI event can then be converted to realistic music using an off-the-shelf music synthesizer tool. We demonstrate the effectiveness of our models on videos containing a variety of music performances. Experimental results show that our model outperforms several existing systems in generating music that is pleasant to listen to. More importantly, the MIDI representations are fully interpretable and transparent, thus enabling us to perform music editing flexibly. We encourage the readers to watch the demo video with audio turned on to experience the results.



### PointContrast: Unsupervised Pre-training for 3D Point Cloud Understanding
- **Arxiv ID**: http://arxiv.org/abs/2007.10985v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.10985v3)
- **Published**: 2020-07-21 17:59:22+00:00
- **Updated**: 2020-11-21 00:42:46+00:00
- **Authors**: Saining Xie, Jiatao Gu, Demi Guo, Charles R. Qi, Leonidas J. Guibas, Or Litany
- **Comment**: ECCV 2020 (Spotlight); code available at
  https://github.com/facebookresearch/PointContrast
- **Journal**: None
- **Summary**: Arguably one of the top success stories of deep learning is transfer learning. The finding that pre-training a network on a rich source set (eg., ImageNet) can help boost performance once fine-tuned on a usually much smaller target set, has been instrumental to many applications in language and vision. Yet, very little is known about its usefulness in 3D point cloud understanding. We see this as an opportunity considering the effort required for annotating data in 3D. In this work, we aim at facilitating research on 3D representation learning. Different from previous works, we focus on high-level scene understanding tasks. To this end, we select a suite of diverse datasets and tasks to measure the effect of unsupervised pre-training on a large source set of 3D scenes. Our findings are extremely encouraging: using a unified triplet of architecture, source dataset, and contrastive loss for pre-training, we achieve improvement over recent best results in segmentation and detection across 6 different benchmarks for indoor and outdoor, real and synthetic datasets -- demonstrating that the learned representation can generalize across domains. Furthermore, the improvement was similar to supervised pre-training, suggesting that future efforts should favor scaling data collection over more detailed annotation. We hope these findings will encourage more research on unsupervised pretext task design for 3D deep learning.



### Multi-person 3D Pose Estimation in Crowded Scenes Based on Multi-View Geometry
- **Arxiv ID**: http://arxiv.org/abs/2007.10986v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.10986v1)
- **Published**: 2020-07-21 17:59:36+00:00
- **Updated**: 2020-07-21 17:59:36+00:00
- **Authors**: He Chen, Pengfei Guo, Pengfei Li, Gim Hee Lee, Gregory Chirikjian
- **Comment**: None
- **Journal**: None
- **Summary**: Epipolar constraints are at the core of feature matching and depth estimation in current multi-person multi-camera 3D human pose estimation methods. Despite the satisfactory performance of this formulation in sparser crowd scenes, its effectiveness is frequently challenged under denser crowd circumstances mainly due to two sources of ambiguity. The first is the mismatch of human joints resulting from the simple cues provided by the Euclidean distances between joints and epipolar lines. The second is the lack of robustness from the naive formulation of the problem as a least squares minimization. In this paper, we depart from the multi-person 3D pose estimation formulation, and instead reformulate it as crowd pose estimation. Our method consists of two key components: a graph model for fast cross-view matching, and a maximum a posteriori (MAP) estimator for the reconstruction of the 3D human poses. We demonstrate the effectiveness and superiority of our proposed method on four benchmark datasets.



### Learning Object Relation Graph and Tentative Policy for Visual Navigation
- **Arxiv ID**: http://arxiv.org/abs/2007.11018v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.11018v1)
- **Published**: 2020-07-21 18:03:05+00:00
- **Updated**: 2020-07-21 18:03:05+00:00
- **Authors**: Heming Du, Xin Yu, Liang Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: Target-driven visual navigation aims at navigating an agent towards a given target based on the observation of the agent. In this task, it is critical to learn informative visual representation and robust navigation policy. Aiming to improve these two components, this paper proposes three complementary techniques, object relation graph (ORG), trial-driven imitation learning (IL), and a memory-augmented tentative policy network (TPN). ORG improves visual representation learning by integrating object relationships, including category closeness and spatial correlations, e.g., a TV usually co-occurs with a remote spatially. Both Trial-driven IL and TPN underlie robust navigation policy, instructing the agent to escape from deadlock states, such as looping or being stuck. Specifically, trial-driven IL is a type of supervision used in policy network training, while TPN, mimicking the IL supervision in unseen environment, is applied in testing. Experiment in the artificial environment AI2-Thor validates that each of the techniques is effective. When combined, the techniques bring significantly improvement over baseline methods in navigation effectiveness and efficiency in unseen environments. We report 22.8% and 23.5% increase in success rate and Success weighted by Path Length (SPL), respectively. The code is available at https://github.com/xiaobaishu0097/ECCV-VN.git.



### CVR-Net: A deep convolutional neural network for coronavirus recognition from chest radiography images
- **Arxiv ID**: http://arxiv.org/abs/2007.11993v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.11993v1)
- **Published**: 2020-07-21 18:21:29+00:00
- **Updated**: 2020-07-21 18:21:29+00:00
- **Authors**: Md. Kamrul Hasan, Md. Ashraful Alam, Md. Toufick E Elahi, Shidhartho Roy, Sifat Redwan Wahid
- **Comment**: 31 Pages
- **Journal**: None
- **Summary**: The novel Coronavirus Disease 2019 (COVID-19) is a global pandemic disease spreading rapidly around the world. A robust and automatic early recognition of COVID-19, via auxiliary computer-aided diagnostic tools, is essential for disease cure and control. The chest radiography images, such as Computed Tomography (CT) and X-ray, and deep Convolutional Neural Networks (CNNs), can be a significant and useful material for designing such tools. However, designing such an automated tool is challenging as a massive number of manually annotated datasets are not publicly available yet, which is the core requirement of supervised learning systems. In this article, we propose a robust CNN-based network, called CVR-Net (Coronavirus Recognition Network), for the automatic recognition of the coronavirus from CT or X-ray images. The proposed end-to-end CVR-Net is a multi-scale-multi-encoder ensemble model, where we have aggregated the outputs from two different encoders and their different scales to obtain the final prediction probability. We train and test the proposed CVR-Net on three different datasets, where the images have collected from different open-source repositories. We compare our proposed CVR-Net with state-of-the-art methods, which are trained and tested on the same datasets. We split three datasets into five different tasks, where each task has a different number of classes, to evaluate the multi-tasking CVR-Net. Our model achieves an overall F1-score & accuracy of 0.997 & 0.998; 0.963 & 0.964; 0.816 & 0.820; 0.961 & 0.961; and 0.780 & 0.780, respectively, for task-1 to task-5. As the CVR-Net provides promising results on the small datasets, it can be an auspicious computer-aided diagnostic tool for the diagnosis of coronavirus to assist the clinical practitioners and radiologists. Our source codes and model are publicly available at https://github.com/kamruleee51/CVR-Net.



### Directional Temporal Modeling for Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2007.11040v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.11040v1)
- **Published**: 2020-07-21 18:49:57+00:00
- **Updated**: 2020-07-21 18:49:57+00:00
- **Authors**: Xinyu Li, Bing Shuai, Joseph Tighe
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: Many current activity recognition models use 3D convolutional neural networks (e.g. I3D, I3D-NL) to generate local spatial-temporal features. However, such features do not encode clip-level ordered temporal information. In this paper, we introduce a channel independent directional convolution (CIDC) operation, which learns to model the temporal evolution among local features. By applying multiple CIDC units we construct a light-weight network that models the clip-level temporal evolution across multiple spatial scales. Our CIDC network can be attached to any activity recognition backbone network. We evaluate our method on four popular activity recognition datasets and consistently improve upon state-of-the-art techniques. We further visualize the activation map of our CIDC network and show that it is able to focus on more meaningful, action related parts of the frame.



### An Image Analogies Approach for Multi-Scale Contour Detection
- **Arxiv ID**: http://arxiv.org/abs/2007.11047v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.11047v1)
- **Published**: 2020-07-21 19:14:18+00:00
- **Updated**: 2020-07-21 19:14:18+00:00
- **Authors**: Slimane Larabi, Neil M. Robertson
- **Comment**: Not published paper
- **Journal**: None
- **Summary**: In this paper we deal with contour detection based on the recent image analogy principle which has been successfully used for super-resolution, texture and curves synthesis and interactive editing. Hand-drawn outlines are initially as benchmarks. Given such a reference image, we present a new method based on this expertise to locate contours of a query image in the same way that it is done for the reference (i.e by analogy).   Applying a image analogies for contour detection using hand drawn images as leaning images cannot gives good result for any query image. The contour detection may be improved if we increase the number of learning images such that there will be exist similarity between query image and some reference images. In addition of the hardness of contours drawing task, this will increase considerably the time computation.   We investigated in this work, how can we avoid this constraint in order to guaranty that all contour pixels will be located for any query image. Fourteen derived stereo patches, derived from a mathematical study, are the knowledge used in order to locate contours at different scales independently of the light conditions.   Comprehensive experiments are conducted on different data sets (BSD 500, Horses of Weizmann). The obtained results show superior performance via precision and recall vs. hand-drawn contours at multiple resolutions to the reported state of the art.



### A Framework based on Deep Neural Networks to Extract Anatomy of Mosquitoes from Images
- **Arxiv ID**: http://arxiv.org/abs/2007.11052v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.11052v2)
- **Published**: 2020-07-21 19:27:46+00:00
- **Updated**: 2020-07-29 14:00:13+00:00
- **Authors**: Mona Minakshi, Pratool Bharti, Tanvir Bhuiyan, Sherzod Kariev, Sriram Chellappan
- **Comment**: None
- **Journal**: None
- **Summary**: We design a framework based on Mask Region-based Convolutional Neural Network (Mask R-CNN) to automatically detect and separately extract anatomical components of mosquitoes - thorax, wings, abdomen and legs from images. Our training dataset consisted of 1500 smartphone images of nine mosquito species trapped in Florida. In the proposed technique, the first step is to detect anatomical components within a mosquito image. Then, we localize and classify the extracted anatomical components, while simultaneously adding a branch in the neural network architecture to segment pixels containing only the anatomical components. Evaluation results are favorable. To evaluate generality, we test our architecture trained only with mosquito images on bumblebee images. We again reveal favorable results, particularly in extracting wings. Our techniques in this paper have practical applications in public health, taxonomy and citizen-science efforts.



### BorderDet: Border Feature for Dense Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2007.11056v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.11056v3)
- **Published**: 2020-07-21 19:30:36+00:00
- **Updated**: 2021-04-09 04:53:42+00:00
- **Authors**: Han Qiu, Yuchen Ma, Zeming Li, Songtao Liu, Jian Sun
- **Comment**: Accepted by ECCV 2020 as Oral. First two authors contributed equally
- **Journal**: None
- **Summary**: Dense object detectors rely on the sliding-window paradigm that predicts the object over a regular grid of image. Meanwhile, the feature maps on the point of the grid are adopted to generate the bounding box predictions. The point feature is convenient to use but may lack the explicit border information for accurate localization. In this paper, We propose a simple and efficient operator called Border-Align to extract "border features" from the extreme point of the border to enhance the point feature. Based on the BorderAlign, we design a novel detection architecture called BorderDet, which explicitly exploits the border information for stronger classification and more accurate localization. With ResNet-50 backbone, our method improves single-stage detector FCOS by 2.8 AP gains (38.6 v.s. 41.4). With the ResNeXt-101-DCN backbone, our BorderDet obtains 50.3 AP, outperforming the existing state-of-the-art approaches. The code is available at (https://github.com/Megvii-BaseDetection/BorderDet).



### Exploiting Temporal Coherence for Self-Supervised One-shot Video Re-identification
- **Arxiv ID**: http://arxiv.org/abs/2007.11064v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.11064v1)
- **Published**: 2020-07-21 19:49:06+00:00
- **Updated**: 2020-07-21 19:49:06+00:00
- **Authors**: Dripta S. Raychaudhuri, Amit K. Roy-Chowdhury
- **Comment**: Accepted at ECCV 2020
- **Journal**: None
- **Summary**: While supervised techniques in re-identification are extremely effective, the need for large amounts of annotations makes them impractical for large camera networks. One-shot re-identification, which uses a singular labeled tracklet for each identity along with a pool of unlabeled tracklets, is a potential candidate towards reducing this labeling effort. Current one-shot re-identification methods function by modeling the inter-relationships amongst the labeled and the unlabeled data, but fail to fully exploit such relationships that exist within the pool of unlabeled data itself. In this paper, we propose a new framework named Temporal Consistency Progressive Learning, which uses temporal coherence as a novel self-supervised auxiliary task in the one-shot learning paradigm to capture such relationships amongst the unlabeled tracklets. Optimizing two new losses, which enforce consistency on a local and global scale, our framework can learn learn richer and more discriminative representations. Extensive experiments on two challenging video re-identification datasets - MARS and DukeMTMC-VideoReID - demonstrate that our proposed method is able to estimate the true labels of the unlabeled data more accurately by up to $8\%$, and obtain significantly better re-identification performance compared to the existing state-of-the-art techniques.



### Self-supervised Feature Learning via Exploiting Multi-modal Data for Retinal Disease Diagnosis
- **Arxiv ID**: http://arxiv.org/abs/2007.11067v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.11067v1)
- **Published**: 2020-07-21 19:49:45+00:00
- **Updated**: 2020-07-21 19:49:45+00:00
- **Authors**: Xiaomeng Li, Mengyu Jia, Md Tauhidul Islam, Lequan Yu, Lei Xing
- **Comment**: IEEE Transactions on Medical Imaging, code is at
  https://github.com/xmengli999/self_supervised
- **Journal**: None
- **Summary**: The automatic diagnosis of various retinal diseases from fundus images is important to support clinical decision-making. However, developing such automatic solutions is challenging due to the requirement of a large amount of human-annotated data. Recently, unsupervised/self-supervised feature learning techniques receive a lot of attention, as they do not need massive annotations. Most of the current self-supervised methods are analyzed with single imaging modality and there is no method currently utilize multi-modal images for better results. Considering that the diagnostics of various vitreoretinal diseases can greatly benefit from another imaging modality, e.g., FFA, this paper presents a novel self-supervised feature learning method by effectively exploiting multi-modal data for retinal disease diagnosis. To achieve this, we first synthesize the corresponding FFA modality and then formulate a patient feature-based softmax embedding objective. Our objective learns both modality-invariant features and patient-similarity features. Through this mechanism, the neural network captures the semantically shared information across different modalities and the apparent visual similarity between patients. We evaluate our method on two public benchmark datasets for retinal disease diagnosis. The experimental results demonstrate that our method clearly outperforms other self-supervised feature learning methods and is comparable to the supervised baseline.



### One Click Lesion RECIST Measurement and Segmentation on CT Scans
- **Arxiv ID**: http://arxiv.org/abs/2007.11087v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.11087v1)
- **Published**: 2020-07-21 20:53:43+00:00
- **Updated**: 2020-07-21 20:53:43+00:00
- **Authors**: Youbao Tang, Ke Yan, Jing Xiao, Ranold M. Summers
- **Comment**: None
- **Journal**: None
- **Summary**: In clinical trials, one of the radiologists' routine work is to measure tumor sizes on medical images using the RECIST criteria (Response Evaluation Criteria In Solid Tumors). However, manual measurement is tedious and subject to inter-observer variability. We propose a unified framework named SEENet for semi-automatic lesion \textit{SE}gmentation and RECIST \textit{E}stimation on a variety of lesions over the entire human body. The user is only required to provide simple guidance by clicking once near the lesion. SEENet consists of two main parts. The first one extracts the lesion of interest with the one-click guidance, roughly segments the lesion, and estimates its RECIST measurement. Based on the results of the first network, the second one refines the lesion segmentation and RECIST estimation. SEENet achieves state-of-the-art performance in lesion segmentation and RECIST estimation on the large-scale public DeepLesion dataset. It offers a practical tool for radiologists to generate reliable lesion measurements (i.e. segmentation mask and RECIST) with minimal human effort and greatly reduced time.



### Accelerating Deep Learning Applications in Space
- **Arxiv ID**: http://arxiv.org/abs/2007.11089v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.DC, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.11089v1)
- **Published**: 2020-07-21 21:06:30+00:00
- **Updated**: 2020-07-21 21:06:30+00:00
- **Authors**: Martina Lofqvist, José Cano
- **Comment**: Published as a workshop paper at SmallSat 2020 - The 34th Annual
  Small Satellite Conference. 19 pages, 22 figures
- **Journal**: None
- **Summary**: Computing at the edge offers intriguing possibilities for the development of autonomy and artificial intelligence. The advancements in autonomous technologies and the resurgence of computer vision have led to a rise in demand for fast and reliable deep learning applications. In recent years, the industry has introduced devices with impressive processing power to perform various object detection tasks. However, with real-time detection, devices are constrained in memory, computational capacity, and power, which may compromise the overall performance. This could be solved either by optimizing the object detector or modifying the images. In this paper, we investigate the performance of CNN-based object detectors on constrained devices when applying different image compression techniques. We examine the capabilities of a NVIDIA Jetson Nano; a low-power, high-performance computer, with an integrated GPU, small enough to fit on-board a CubeSat. We take a closer look at the Single Shot MultiBox Detector (SSD) and Region-based Fully Convolutional Network (R-FCN) that are pre-trained on DOTA - a Large Scale Dataset for Object Detection in Aerial Images. The performance is measured in terms of inference time, memory consumption, and accuracy. By applying image compression techniques, we are able to optimize performance. The two techniques applied, lossless compression and image scaling, improves speed and memory consumption with no or little change in accuracy. The image scaling technique achieves a 100% runnable dataset and we suggest combining both techniques in order to optimize the speed/memory/accuracy trade-off.



### Who Left the Dogs Out? 3D Animal Reconstruction with Expectation Maximization in the Loop
- **Arxiv ID**: http://arxiv.org/abs/2007.11110v2
- **DOI**: 10.1007/978-3-030-58621-8
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.11110v2)
- **Published**: 2020-07-21 21:52:56+00:00
- **Updated**: 2021-02-11 13:47:24+00:00
- **Authors**: Benjamin Biggs, Oliver Boyne, James Charles, Andrew Fitzgibbon, Roberto Cipolla
- **Comment**: Accepted at ECCV 2020
- **Journal**: 16th European Conference Glasgow UK August 23 to 28 2020
  Proceedings Part XI
- **Summary**: We introduce an automatic, end-to-end method for recovering the 3D pose and shape of dogs from monocular internet images. The large variation in shape between dog breeds, significant occlusion and low quality of internet images makes this a challenging problem. We learn a richer prior over shapes than previous work, which helps regularize parameter estimation. We demonstrate results on the Stanford Dog dataset, an 'in the wild' dataset of 20,580 dog images for which we have collected 2D joint and silhouette annotations to split for training and evaluation. In order to capture the large shape variety of dogs, we show that the natural variation in the 2D dataset is enough to learn a detailed 3D prior through expectation maximization (EM). As a by-product of training, we generate a new parameterized model (including limb scaling) SMBLD which we release alongside our new annotation dataset StanfordExtra to the research community.



### Creating a Large-scale Synthetic Dataset for Human Activity Recognition
- **Arxiv ID**: http://arxiv.org/abs/2007.11118v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.11118v1)
- **Published**: 2020-07-21 22:20:21+00:00
- **Updated**: 2020-07-21 22:20:21+00:00
- **Authors**: Ollie Matthews, Koki Ryu, Tarun Srivastava
- **Comment**: None
- **Journal**: None
- **Summary**: Creating and labelling datasets of videos for use in training Human Activity Recognition models is an arduous task. In this paper, we approach this by using 3D rendering tools to generate a synthetic dataset of videos, and show that a classifier trained on these videos can generalise to real videos. We use five different augmentation techniques to generate the videos, leading to a wide variety of accurately labelled unique videos. We fine tune a pre-trained I3D model on our videos, and find that the model is able to achieve a high accuracy of 73% on the HMDB51 dataset over three classes. We also find that augmenting the HMDB training set with our dataset provides a 2% improvement in the performance of the classifier. Finally, we discuss possible extensions to the dataset, including virtual try on and modeling motion of the people.



