# Arxiv Papers in cs.CV on 2020-07-20
### A Gated and Bifurcated Stacked U-Net Module for Document Image Dewarping
- **Arxiv ID**: http://arxiv.org/abs/2007.09824v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.09824v1)
- **Published**: 2020-07-20 01:22:05+00:00
- **Updated**: 2020-07-20 01:22:05+00:00
- **Authors**: Hmrishav Bandyopadhyay, Tanmoy Dasgupta, Nibaran Das, Mita Nasipuri
- **Comment**: None
- **Journal**: None
- **Summary**: Capturing images of documents is one of the easiest and most used methods of recording them. These images however, being captured with the help of handheld devices, often lead to undesirable distortions that are hard to remove. We propose a supervised Gated and Bifurcated Stacked U-Net module to predict a dewarping grid and create a distortion free image from the input. While the network is trained on synthetically warped document images, results are calculated on the basis of real world images. The novelty in our methods exists not only in a bifurcation of the U-Net to help eliminate the intermingling of the grid coordinates, but also in the use of a gated network which adds boundary and other minute line level details to the model. The end-to-end pipeline proposed by us achieves state-of-the-art performance on the DocUNet dataset after being trained on just 8 percent of the data used in previous methods.



### MINI-Net: Multiple Instance Ranking Network for Video Highlight Detection
- **Arxiv ID**: http://arxiv.org/abs/2007.09833v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.09833v2)
- **Published**: 2020-07-20 01:56:32+00:00
- **Updated**: 2020-08-13 05:42:05+00:00
- **Authors**: Fa-Ting Hong, Xuanteng Huang, Wei-Hong Li, Wei-Shi Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: We address the weakly supervised video highlight detection problem for learning to detect segments that are more attractive in training videos given their video event label but without expensive supervision of manually annotating highlight segments. While manually averting localizing highlight segments, weakly supervised modeling is challenging, as a video in our daily life could contain highlight segments with multiple event types, e.g., skiing and surfing. In this work, we propose casting weakly supervised video highlight detection modeling for a given specific event as a multiple instance ranking network (MINI-Net) learning. We consider each video as a bag of segments, and therefore, the proposed MINI-Net learns to enforce a higher highlight score for a positive bag that contains highlight segments of a specific event than those for negative bags that are irrelevant. In particular, we form a max-max ranking loss to acquire a reliable relative comparison between the most likely positive segment instance and the hardest negative segment instance. With this max-max ranking loss, our MINI-Net effectively leverages all segment information to acquire a more distinct video feature representation for localizing the highlight segments of a specific event in a video. The extensive experimental results on three challenging public benchmarks clearly validate the efficacy of our multiple instance ranking approach for solving the problem.



### RT3D: Achieving Real-Time Execution of 3D Convolutional Neural Networks on Mobile Devices
- **Arxiv ID**: http://arxiv.org/abs/2007.09835v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.09835v2)
- **Published**: 2020-07-20 02:05:32+00:00
- **Updated**: 2021-01-03 18:03:16+00:00
- **Authors**: Wei Niu, Mengshu Sun, Zhengang Li, Jou-An Chen, Jiexiong Guan, Xipeng Shen, Yanzhi Wang, Sijia Liu, Xue Lin, Bin Ren
- **Comment**: To appear in Proceedings of the 35th AAAI Conference on Artificial
  Intelligence (AAAI-21)
- **Journal**: None
- **Summary**: Mobile devices are becoming an important carrier for deep learning tasks, as they are being equipped with powerful, high-end mobile CPUs and GPUs. However, it is still a challenging task to execute 3D Convolutional Neural Networks (CNNs) targeting for real-time performance, besides high inference accuracy. The reason is more complex model structure and higher model dimensionality overwhelm the available computation/storage resources on mobile devices. A natural way may be turning to deep learning weight pruning techniques. However, the direct generalization of existing 2D CNN weight pruning methods to 3D CNNs is not ideal for fully exploiting mobile parallelism while achieving high inference accuracy.   This paper proposes RT3D, a model compression and mobile acceleration framework for 3D CNNs, seamlessly integrating neural network weight pruning and compiler code generation techniques. We propose and investigate two structured sparsity schemes i.e., the vanilla structured sparsity and kernel group structured (KGS) sparsity that are mobile acceleration friendly. The vanilla sparsity removes whole kernel groups, while KGS sparsity is a more fine-grained structured sparsity that enjoys higher flexibility while exploiting full on-device parallelism. We propose a reweighted regularization pruning algorithm to achieve the proposed sparsity schemes. The inference time speedup due to sparsity is approaching the pruning rate of the whole model FLOPs (floating point operations). RT3D demonstrates up to 29.1$\times$ speedup in end-to-end inference time comparing with current mobile frameworks supporting 3D CNNs, with moderate 1%-1.5% accuracy loss. The end-to-end inference time for 16 video frames could be within 150 ms, when executing representative C3D and R(2+1)D models on a cellphone. For the first time, real-time execution of 3D CNNs is achieved on off-the-shelf mobiles.



### Object-Aware Centroid Voting for Monocular 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2007.09836v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.09836v1)
- **Published**: 2020-07-20 02:11:18+00:00
- **Updated**: 2020-07-20 02:11:18+00:00
- **Authors**: Wentao Bao, Qi Yu, Yu Kong
- **Comment**: IROS 2020 Accepted Paper
- **Journal**: None
- **Summary**: Monocular 3D object detection aims to detect objects in a 3D physical world from a single camera. However, recent approaches either rely on expensive LiDAR devices, or resort to dense pixel-wise depth estimation that causes prohibitive computational cost. In this paper, we propose an end-to-end trainable monocular 3D object detector without learning the dense depth. Specifically, the grid coordinates of a 2D box are first projected back to 3D space with the pinhole model as 3D centroids proposals. Then, a novel object-aware voting approach is introduced, which considers both the region-wise appearance attention and the geometric projection distribution, to vote the 3D centroid proposals for 3D object localization. With the late fusion and the predicted 3D orientation and dimension, the 3D bounding boxes of objects can be detected from a single RGB image. The method is straightforward yet significantly superior to other monocular-based methods. Extensive experimental results on the challenging KITTI benchmark validate the effectiveness of the proposed method.



### Seeing the Un-Scene: Learning Amodal Semantic Maps for Room Navigation
- **Arxiv ID**: http://arxiv.org/abs/2007.09841v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2007.09841v1)
- **Published**: 2020-07-20 02:19:26+00:00
- **Updated**: 2020-07-20 02:19:26+00:00
- **Authors**: Medhini Narasimhan, Erik Wijmans, Xinlei Chen, Trevor Darrell, Dhruv Batra, Devi Parikh, Amanpreet Singh
- **Comment**: Published at the European Conference on Computer Vision, 2020
- **Journal**: None
- **Summary**: We introduce a learning-based approach for room navigation using semantic maps. Our proposed architecture learns to predict top-down belief maps of regions that lie beyond the agent's field of view while modeling architectural and stylistic regularities in houses. First, we train a model to generate amodal semantic top-down maps indicating beliefs of location, size, and shape of rooms by learning the underlying architectural patterns in houses. Next, we use these maps to predict a point that lies in the target room and train a policy to navigate to the point. We empirically demonstrate that by predicting semantic maps, the model learns common correlations found in houses and generalizes to novel environments. We also demonstrate that reducing the task of room navigation to point navigation improves the performance further.



### Self-Loop Uncertainty: A Novel Pseudo-Label for Semi-Supervised Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2007.09854v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.09854v1)
- **Published**: 2020-07-20 02:52:07+00:00
- **Updated**: 2020-07-20 02:52:07+00:00
- **Authors**: Yuexiang Li, Jiawei Chen, Xinpeng Xie, Kai Ma, Yefeng Zheng
- **Comment**: Accepted by MICCAI 2020
- **Journal**: None
- **Summary**: Witnessing the success of deep learning neural networks in natural image processing, an increasing number of studies have been proposed to develop deep-learning-based frameworks for medical image segmentation. However, since the pixel-wise annotation of medical images is laborious and expensive, the amount of annotated data is usually deficient to well-train a neural network. In this paper, we propose a semi-supervised approach to train neural networks with limited labeled data and a large quantity of unlabeled images for medical image segmentation. A novel pseudo-label (namely self-loop uncertainty), generated by recurrently optimizing the neural network with a self-supervised task, is adopted as the ground-truth for the unlabeled images to augment the training set and boost the segmentation accuracy. The proposed self-loop uncertainty can be seen as an approximation of the uncertainty estimation yielded by ensembling multiple models with a significant reduction of inference time. Experimental results on two publicly available datasets demonstrate the effectiveness of our semi-supervied approach.



### Cross-View Image Synthesis with Deformable Convolution and Attention Mechanism
- **Arxiv ID**: http://arxiv.org/abs/2007.09858v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.09858v1)
- **Published**: 2020-07-20 03:08:36+00:00
- **Updated**: 2020-07-20 03:08:36+00:00
- **Authors**: Hao Ding, Songsong Wu, Hao Tang, Fei Wu, Guangwei Gao, Xiao-Yuan Jing
- **Comment**: None
- **Journal**: None
- **Summary**: Learning to generate natural scenes has always been a daunting task in computer vision. This is even more laborious when generating images with very different views. When the views are very different, the view fields have little overlap or objects are occluded, leading the task very challenging. In this paper, we propose to use Generative Adversarial Networks(GANs) based on a deformable convolution and attention mechanism to solve the problem of cross-view image synthesis (see Fig.1). It is difficult to understand and transform scenes appearance and semantic information from another view, thus we use deformed convolution in the U-net network to improve the network's ability to extract features of objects at different scales. Moreover, to better learn the correspondence between images from different views, we apply an attention mechanism to refine the intermediate feature map thus generating more realistic images. A large number of experiments on different size images on the Dayton dataset[1] show that our model can produce better results than state-of-the-art methods.



### Novel Approach to Use HU Moments with Image Processing Techniques for Real Time Sign Language Communication
- **Arxiv ID**: http://arxiv.org/abs/2007.09859v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.09859v1)
- **Published**: 2020-07-20 03:10:18+00:00
- **Updated**: 2020-07-20 03:10:18+00:00
- **Authors**: Matheesha Fernando, Janaka Wijayanayake
- **Comment**: None
- **Journal**: None
- **Summary**: Sign language is the fundamental communication method among people who suffer from speech and hearing defects. The rest of the world doesn't have a clear idea of sign language. "Sign Language Communicator" (SLC) is designed to solve the language barrier between the sign language users and the rest of the world. The main objective of this research is to provide a low cost affordable method of sign language interpretation. This system will also be very useful to the sign language learners as they can practice the sign language. During the research available human computer interaction techniques in posture recognition was tested and evaluated. A series of image processing techniques with Hu-moment classification was identified as the best approach. To improve the accuracy of the system, a new approach height to width ratio filtration was implemented along with Hu-moments. System is able to recognize selected Sign Language signs with the accuracy of 84% without a controlled background with small light adjustments



### Learning Gaussian Instance Segmentation in Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2007.09860v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.09860v1)
- **Published**: 2020-07-20 03:11:32+00:00
- **Updated**: 2020-07-20 03:11:32+00:00
- **Authors**: Shih-Hung Liu, Shang-Yi Yu, Shao-Chi Wu, Hwann-Tzong Chen, Tyng-Luh Liu
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a novel method for instance segmentation of 3D point clouds. The proposed method is called Gaussian Instance Center Network (GICN), which can approximate the distributions of instance centers scattered in the whole scene as Gaussian center heatmaps. Based on the predicted heatmaps, a small number of center candidates can be easily selected for the subsequent predictions with efficiency, including i) predicting the instance size of each center to decide a range for extracting features, ii) generating bounding boxes for centers, and iii) producing the final instance masks. GICN is a single-stage, anchor-free, and end-to-end architecture that is easy to train and efficient to perform inference. Benefited from the center-dictated mechanism with adaptive instance size selection, our method achieves state-of-the-art performance in the task of 3D instance segmentation on ScanNet and S3DIS datasets.



### Context-Aware RCNN: A Baseline for Action Detection in Videos
- **Arxiv ID**: http://arxiv.org/abs/2007.09861v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.09861v1)
- **Published**: 2020-07-20 03:11:48+00:00
- **Updated**: 2020-07-20 03:11:48+00:00
- **Authors**: Jianchao Wu, Zhanghui Kuang, Limin Wang, Wayne Zhang, Gangshan Wu
- **Comment**: ECCV 2020 Camera-ready version
- **Journal**: None
- **Summary**: Video action detection approaches usually conduct actor-centric action recognition over RoI-pooled features following the standard pipeline of Faster-RCNN. In this work, we first empirically find the recognition accuracy is highly correlated with the bounding box size of an actor, and thus higher resolution of actors contributes to better performance. However, video models require dense sampling in time to achieve accurate recognition. To fit in GPU memory, the frames to backbone network must be kept low-resolution, resulting in a coarse feature map in RoI-Pooling layer. Thus, we revisit RCNN for actor-centric action recognition via cropping and resizing image patches around actors before feature extraction with I3D deep network. Moreover, we found that expanding actor bounding boxes slightly and fusing the context features can further boost the performance. Consequently, we develop a surpringly effective baseline (Context-Aware RCNN) and it achieves new state-of-the-art results on two challenging action detection benchmarks of AVA and JHMDB. Our observations challenge the conventional wisdom of RoI-Pooling based pipeline and encourage researchers rethink the importance of resolution in actor-centric action recognition. Our approach can serve as a strong baseline for video action detection and is expected to inspire new ideas for this filed. The code is available at \url{https://github.com/MCG-NJU/CRCNN-Action}.



### Interpretable Foreground Object Search As Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/2007.09867v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.09867v2)
- **Published**: 2020-07-20 03:37:15+00:00
- **Updated**: 2020-07-22 03:33:16+00:00
- **Authors**: Boren Li, Po-Yu Zhuang, Jian Gu, Mingyang Li, Ping Tan
- **Comment**: This paper will appear at ECCV 2020
- **Journal**: None
- **Summary**: This paper proposes a knowledge distillation method for foreground object search (FoS). Given a background and a rectangle specifying the foreground location and scale, FoS retrieves compatible foregrounds in a certain category for later image composition. Foregrounds within the same category can be grouped into a small number of patterns. Instances within each pattern are compatible with any query input interchangeably. These instances are referred to as interchangeable foregrounds. We first present a pipeline to build pattern-level FoS dataset containing labels of interchangeable foregrounds. We then establish a benchmark dataset for further training and testing following the pipeline. As for the proposed method, we first train a foreground encoder to learn representations of interchangeable foregrounds. We then train a query encoder to learn query-foreground compatibility following a knowledge distillation framework. It aims to transfer knowledge from interchangeable foregrounds to supervise representation learning of compatibility. The query feature representation is projected to the same latent space as interchangeable foregrounds, enabling very efficient and interpretable instance-level search. Furthermore, pattern-level search is feasible to retrieve more controllable, reasonable and diverse foregrounds. The proposed method outperforms the previous state-of-the-art by 10.42% in absolute difference and 24.06% in relative improvement evaluated by mean average precision (mAP). Extensive experimental results also demonstrate its efficacy from various aspects. The benchmark dataset and code will be release shortly.



### Graph Neural Network for Video Relocalization
- **Arxiv ID**: http://arxiv.org/abs/2007.09877v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.09877v2)
- **Published**: 2020-07-20 04:01:40+00:00
- **Updated**: 2022-01-26 08:06:22+00:00
- **Authors**: Yuan Zhou, Mingfei Wang, Ruolin Wang, Shuwei Huo
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we focus on video relocalization task, which uses a query video clip as input to retrieve a semantic relative video clip in another untrimmed long video. we find that in video relocalization datasets, there exists a phenomenon showing that there does not exist consistent relationship between feature similarity by frame and feature similarity by video, which affects the feature fusion among frames. However, existing video relocalization methods do not fully consider it. Taking this phenomenon into account, in this article, we treat video features as a graph by concatenating the query video feature and proposal video feature along time dimension, where each timestep is treated as a node, each row of the feature matrix is treated as feature of each node. Then, with the power of graph neural networks, we propose a Multi-Graph Feature Fusion Module to fuse the relation feature of this graph. After evaluating our method on ActivityNet v1.2 dataset and Thumos14 dataset, we find that our proposed method outperforms the state of art methods.



### Complementary Boundary Generator with Scale-Invariant Relation Modeling for Temporal Action Localization: Submission to ActivityNet Challenge 2020
- **Arxiv ID**: http://arxiv.org/abs/2007.09883v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.09883v2)
- **Published**: 2020-07-20 04:35:40+00:00
- **Updated**: 2020-08-26 01:51:02+00:00
- **Authors**: Haisheng Su, Jinyuan Feng, Hao Shao, Zhenyu Jiang, Manyuan Zhang, Wei Wu, Yu Liu, Hongsheng Li, Junjie Yan
- **Comment**: Submitted to CVPR workshop of ActivityNet Challenge 2020
- **Journal**: None
- **Summary**: This technical report presents an overview of our solution used in the submission to ActivityNet Challenge 2020 Task 1 (\textbf{temporal action localization/detection}). Temporal action localization requires to not only precisely locate the temporal boundaries of action instances, but also accurately classify the untrimmed videos into specific categories. In this paper, we decouple the temporal action localization task into two stages (i.e. proposal generation and classification) and enrich the proposal diversity through exhaustively exploring the influences of multiple components from different but complementary perspectives. Specifically, in order to generate high-quality proposals, we consider several factors including the video feature encoder, the proposal generator, the proposal-proposal relations, the scale imbalance, and ensemble strategy. Finally, in order to obtain accurate detections, we need to further train an optimal video classifier to recognize the generated proposals. Our proposed scheme achieves the state-of-the-art performance on the temporal action localization task with \textbf{42.26} average mAP on the challenge testing set.



### Self-Supervision with Superpixels: Training Few-shot Medical Image Segmentation without Annotation
- **Arxiv ID**: http://arxiv.org/abs/2007.09886v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.09886v2)
- **Published**: 2020-07-20 04:46:33+00:00
- **Updated**: 2020-10-06 21:36:05+00:00
- **Authors**: Cheng Ouyang, Carlo Biffi, Chen Chen, Turkay Kart, Huaqi Qiu, Daniel Rueckert
- **Comment**: ECCV2020
- **Journal**: None
- **Summary**: Few-shot semantic segmentation (FSS) has great potential for medical imaging applications. Most of the existing FSS techniques require abundant annotated semantic classes for training. However, these methods may not be applicable for medical images due to the lack of annotations. To address this problem we make several contributions: (1) A novel self-supervised FSS framework for medical images in order to eliminate the requirement for annotations during training. Additionally, superpixel-based pseudo-labels are generated to provide supervision; (2) An adaptive local prototype pooling module plugged into prototypical networks, to solve the common challenging foreground-background imbalance problem in medical image segmentation; (3) We demonstrate the general applicability of the proposed approach for medical images using three different tasks: abdominal organ segmentation for CT and MRI, as well as cardiac segmentation for MRI. Our results show that, for medical image segmentation, the proposed method outperforms conventional FSS methods which require manual annotations for training.



### Deep Reflectance Volumes: Relightable Reconstructions from Multi-View Photometric Images
- **Arxiv ID**: http://arxiv.org/abs/2007.09892v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2007.09892v1)
- **Published**: 2020-07-20 05:38:11+00:00
- **Updated**: 2020-07-20 05:38:11+00:00
- **Authors**: Sai Bi, Zexiang Xu, Kalyan Sunkavalli, Miloš Hašan, Yannick Hold-Geoffroy, David Kriegman, Ravi Ramamoorthi
- **Comment**: Accepted to ECCV 2020
- **Journal**: None
- **Summary**: We present a deep learning approach to reconstruct scene appearance from unstructured images captured under collocated point lighting. At the heart of Deep Reflectance Volumes is a novel volumetric scene representation consisting of opacity, surface normal and reflectance voxel grids. We present a novel physically-based differentiable volume ray marching framework to render these scene volumes under arbitrary viewpoint and lighting. This allows us to optimize the scene volumes to minimize the error between their rendered images and the captured images. Our method is able to reconstruct real scenes with challenging non-Lambertian reflectance and complex geometry with occlusions and shadowing. Moreover, it accurately generalizes to novel viewpoints and lighting, including non-collocated lighting, rendering photorealistic images that are significantly better than state-of-the-art mesh-based methods. We also show that our learned reflectance volumes are editable, allowing for modifying the materials of the captured scenes.



### Solving Long-tailed Recognition with Deep Realistic Taxonomic Classifier
- **Arxiv ID**: http://arxiv.org/abs/2007.09898v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.09898v1)
- **Published**: 2020-07-20 05:57:42+00:00
- **Updated**: 2020-07-20 05:57:42+00:00
- **Authors**: Tz-Ying Wu, Pedro Morgado, Pei Wang, Chih-Hui Ho, Nuno Vasconcelos
- **Comment**: Accepted to ECCV 2020
- **Journal**: None
- **Summary**: Long-tail recognition tackles the natural non-uniformly distributed data in real-world scenarios. While modern classifiers perform well on populated classes, its performance degrades significantly on tail classes. Humans, however, are less affected by this since, when confronted with uncertain examples, they simply opt to provide coarser predictions. Motivated by this, a deep realistic taxonomic classifier (Deep-RTC) is proposed as a new solution to the long-tail problem, combining realism with hierarchical predictions. The model has the option to reject classifying samples at different levels of the taxonomy, once it cannot guarantee the desired performance. Deep-RTC is implemented with a stochastic tree sampling during training to simulate all possible classification conditions at finer or coarser levels and a rejection mechanism at inference time. Experiments on the long-tailed version of four datasets, CIFAR100, AWA2, Imagenet, and iNaturalist, demonstrate that the proposed approach preserves more information on all classes with different popularity levels. Deep-RTC also outperforms the state-of-the-art methods in longtailed recognition, hierarchical classification, and learning with rejection literature using the proposed correctly predicted bits (CPB) metric.



### Sep-Stereo: Visually Guided Stereophonic Audio Generation by Associating Source Separation
- **Arxiv ID**: http://arxiv.org/abs/2007.09902v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2007.09902v1)
- **Published**: 2020-07-20 06:20:26+00:00
- **Updated**: 2020-07-20 06:20:26+00:00
- **Authors**: Hang Zhou, Xudong Xu, Dahua Lin, Xiaogang Wang, Ziwei Liu
- **Comment**: To appear in Proceedings of the European Conference on Computer
  Vision (ECCV), 2020. Code, models, and video results are available on our
  webpage: https://hangz-nju-cuhk.github.io/projects/Sep-Stereo
- **Journal**: None
- **Summary**: Stereophonic audio is an indispensable ingredient to enhance human auditory experience. Recent research has explored the usage of visual information as guidance to generate binaural or ambisonic audio from mono ones with stereo supervision. However, this fully supervised paradigm suffers from an inherent drawback: the recording of stereophonic audio usually requires delicate devices that are expensive for wide accessibility. To overcome this challenge, we propose to leverage the vastly available mono data to facilitate the generation of stereophonic audio. Our key observation is that the task of visually indicated audio separation also maps independent audios to their corresponding visual positions, which shares a similar objective with stereophonic audio generation. We integrate both stereo generation and source separation into a unified framework, Sep-Stereo, by considering source separation as a particular type of audio spatialization. Specifically, a novel associative pyramid network architecture is carefully designed for audio-visual feature fusion. Extensive experiments demonstrate that our framework can improve the stereophonic audio generation results while performing accurate sound separation with a shared backbone.



### Statistical Downscaling of Temperature Distributions from the Synoptic Scale to the Mesoscale Using Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2007.10839v1
- **DOI**: None
- **Categories**: **physics.ao-ph**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.10839v1)
- **Published**: 2020-07-20 06:24:08+00:00
- **Updated**: 2020-07-20 06:24:08+00:00
- **Authors**: Tsuyoshi Thomas Sekiyama
- **Comment**: 15 pages, 4 figures, 4 tables.
  https://scholar.google.com/citations?user=K3vMmpMAAAAJ&hl=en
- **Journal**: None
- **Summary**: Deep learning, particularly convolutional neural networks for image recognition, has been recently used in meteorology. One of the promising applications is developing a statistical surrogate model that converts the output images of low-resolution dynamic models to high-resolution images. Our study exhibits a preliminary experiment that evaluates the performance of a model that downscales synoptic temperature fields to mesoscale temperature fields every 6 hours. The deep learning model was trained with operational 22-km gridded global analysis surface winds and temperatures as the input, operational 5-km gridded regional analysis surface temperatures as the desired output, and a target domain covering central Japan. The results confirm that our deep convolutional neural network (DCNN) is capable of estimating the locations of coastlines and mountain ridges in great detail, which are not retained in the inputs, and providing high-resolution surface temperature distributions. For instance, while the average root-mean-square error (RMSE) is 2.7 K between the global and regional analyses at altitudes greater than 1000 m, the RMSE is reduced to 1.0 K, and the correlation coefficient is improved from 0.6 to 0.9 by the surrogate model. Although this study evaluates a surrogate model only for surface temperature, it probably can be improved by augmenting the downscaling variables and vertical profiles. Surrogate models of DCNNs require only a small amount of computational power once their training is finished. Therefore, if the surrogate models are implemented at short time intervals, they will provide high-resolution weather forecast guidance or environment emergency alerts at low cost.



### Evaluating a Simple Retraining Strategy as a Defense Against Adversarial Attacks
- **Arxiv ID**: http://arxiv.org/abs/2007.09916v1
- **DOI**: None
- **Categories**: **cs.CV**, I.5.1; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/2007.09916v1)
- **Published**: 2020-07-20 07:49:33+00:00
- **Updated**: 2020-07-20 07:49:33+00:00
- **Authors**: Nupur Thakur, Yuzhen Ding, Baoxin Li
- **Comment**: 16 pages, 11 figures
- **Journal**: None
- **Summary**: Though deep neural networks (DNNs) have shown superiority over other techniques in major fields like computer vision, natural language processing, robotics, recently, it has been proven that they are vulnerable to adversarial attacks. The addition of a simple, small and almost invisible perturbation to the original input image can be used to fool DNNs into making wrong decisions. With more attack algorithms being designed, a need for defending the neural networks from such attacks arises. Retraining the network with adversarial images is one of the simplest techniques. In this paper, we evaluate the effectiveness of such a retraining strategy in defending against adversarial attacks. We also show how simple algorithms like KNN can be used to determine the labels of the adversarial images needed for retraining. We present the results on two standard datasets namely, CIFAR-10 and TinyImageNet.



### Robust Tracking against Adversarial Attacks
- **Arxiv ID**: http://arxiv.org/abs/2007.09919v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.09919v2)
- **Published**: 2020-07-20 08:05:55+00:00
- **Updated**: 2020-07-29 08:03:25+00:00
- **Authors**: Shuai Jia, Chao Ma, Yibing Song, Xiaokang Yang
- **Comment**: Accepted by ECCV2020
- **Journal**: None
- **Summary**: While deep convolutional neural networks (CNNs) are vulnerable to adversarial attacks, considerably few efforts have been paid to construct robust deep tracking algorithms against adversarial attacks. Current studies on adversarial attack and defense mainly reside in a single image. In this work, we first attempt to generate adversarial examples on top of video sequences to improve the tracking robustness against adversarial attacks. To this end, we take temporal motion into consideration when generating lightweight perturbations over the estimated tracking results frame-by-frame. On one hand, we add the temporal perturbations into the original video sequences as adversarial examples to greatly degrade the tracking performance. On the other hand, we sequentially estimate the perturbations from input sequences and learn to eliminate their effect for performance restoration. We apply the proposed adversarial attack and defense approaches to state-of-the-art deep tracking algorithms. Extensive evaluations on the benchmark datasets demonstrate that our defense method not only eliminates the large performance drops caused by adversarial attacks, but also achieves additional performance gains when deep trackers are not under adversarial attacks.



### Incorporating Reinforced Adversarial Learning in Autoregressive Image Generation
- **Arxiv ID**: http://arxiv.org/abs/2007.09923v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.09923v1)
- **Published**: 2020-07-20 08:10:07+00:00
- **Updated**: 2020-07-20 08:10:07+00:00
- **Authors**: Kenan E. Ak, Ning Xu, Zhe Lin, Yilin Wang
- **Comment**: Accepted to ECCV 2020
- **Journal**: None
- **Summary**: Autoregressive models recently achieved comparable results versus state-of-the-art Generative Adversarial Networks (GANs) with the help of Vector Quantized Variational AutoEncoders (VQ-VAE). However, autoregressive models have several limitations such as exposure bias and their training objective does not guarantee visual fidelity. To address these limitations, we propose to use Reinforced Adversarial Learning (RAL) based on policy gradient optimization for autoregressive models. By applying RAL, we enable a similar process for training and testing to address the exposure bias issue. In addition, visual fidelity has been further optimized with adversarial loss inspired by their strong counterparts: GANs. Due to the slow sampling speed of autoregressive models, we propose to use partial generation for faster training. RAL also empowers the collaboration between different modules of the VQ-VAE framework. To our best knowledge, the proposed method is first to enable adversarial learning in autoregressive models for image generation. Experiments on synthetic and real-world datasets show improvements over the MLE trained models. The proposed method improves both negative log-likelihood (NLL) and Fr\'echet Inception Distance (FID), which indicates improvements in terms of visual quality and diversity. The proposed method achieves state-of-the-art results on Celeba for 64 $\times$ 64 image resolution, showing promise for large scale image generation.



### MotionSqueeze: Neural Motion Feature Learning for Video Understanding
- **Arxiv ID**: http://arxiv.org/abs/2007.09933v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.09933v1)
- **Published**: 2020-07-20 08:30:14+00:00
- **Updated**: 2020-07-20 08:30:14+00:00
- **Authors**: Heeseung Kwon, Manjin Kim, Suha Kwak, Minsu Cho
- **Comment**: Accepted to ECCV 2020
- **Journal**: None
- **Summary**: Motion plays a crucial role in understanding videos and most state-of-the-art neural models for video classification incorporate motion information typically using optical flows extracted by a separate off-the-shelf method. As the frame-by-frame optical flows require heavy computation, incorporating motion information has remained a major computational bottleneck for video understanding. In this work, we replace external and heavy computation of optical flows with internal and light-weight learning of motion features. We propose a trainable neural module, dubbed MotionSqueeze, for effective motion feature extraction. Inserted in the middle of any neural network, it learns to establish correspondences across frames and convert them into motion features, which are readily fed to the next downstream layer for better prediction. We demonstrate that the proposed method provides a significant gain on four standard benchmarks for action recognition with only a small amount of additional cost, outperforming the state of the art on Something-Something-V1&V2 datasets.



### Including Images into Message Veracity Assessment in Social Media
- **Arxiv ID**: http://arxiv.org/abs/2008.01196v1
- **DOI**: None
- **Categories**: **cs.IR**, cs.CV, cs.LG, cs.SI
- **Links**: [PDF](http://arxiv.org/pdf/2008.01196v1)
- **Published**: 2020-07-20 08:42:17+00:00
- **Updated**: 2020-07-20 08:42:17+00:00
- **Authors**: Abderrazek Azri, Cécile Favre, Nouria Harbi, Jérôme Darmont
- **Comment**: None
- **Journal**: 8th International Conference on Innovation and New Trends in
  Information Technology (INTIS 2019), Dec 2019, Tangier, Morocco
- **Summary**: The extensive use of social media in the diffusion of information has also laid a fertile ground for the spread of rumors, which could significantly affect the credibility of social media. An ever-increasing number of users post news including, in addition to text, multimedia data such as images and videos. Yet, such multimedia content is easily editable due to the broad availability of simple and effective image and video processing tools. The problem of assessing the veracity of social network posts has attracted a lot of attention from researchers in recent years. However, almost all previous works have focused on analyzing textual contents to determine veracity, while visual contents, and more particularly images, remains ignored or little exploited in the literature. In this position paper, we propose a framework that explores two novel ways to assess the veracity of messages published on social networks by analyzing the credibility of both their textual and visual contents.



### TENet: Triple Excitation Network for Video Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2007.09943v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.09943v2)
- **Published**: 2020-07-20 08:45:41+00:00
- **Updated**: 2020-08-30 12:59:31+00:00
- **Authors**: Sucheng Ren, Chu Han, Xin Yang, Guoqiang Han, Shengfeng He
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a simple yet effective approach, named Triple Excitation Network, to reinforce the training of video salient object detection (VSOD) from three aspects, spatial, temporal, and online excitations. These excitation mechanisms are designed following the spirit of curriculum learning and aim to reduce learning ambiguities at the beginning of training by selectively exciting feature activations using ground truth. Then we gradually reduce the weight of ground truth excitations by a curriculum rate and replace it by a curriculum complementary map for better and faster convergence. In particular, the spatial excitation strengthens feature activations for clear object boundaries, while the temporal excitation imposes motions to emphasize spatio-temporal salient regions. Spatial and temporal excitations can combat the saliency shifting problem and conflict between spatial and temporal features of VSOD. Furthermore, our semi-curriculum learning design enables the first online refinement strategy for VSOD, which allows exciting and boosting saliency responses during testing without re-training. The proposed triple excitations can easily plug in different VSOD methods. Extensive experiments show the effectiveness of all three excitation methods and the proposed method outperforms state-of-the-art image and video salient object detection methods.



### Gesture Recognition for Initiating Human-to-Robot Handovers
- **Arxiv ID**: http://arxiv.org/abs/2007.09945v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2007.09945v2)
- **Published**: 2020-07-20 08:49:34+00:00
- **Updated**: 2020-12-30 07:51:16+00:00
- **Authors**: Jun Kwan, Chinkye Tan, Akansel Cosgun
- **Comment**: None
- **Journal**: None
- **Summary**: Human-to-Robot handovers are useful for many Human-Robot Interaction scenarios. It is important to recognize when a human intends to initiate handovers, so that the robot does not try to take objects from humans when a handover is not intended. We pose the handover gesture recognition as a binary classification problem in a single RGB image. Three separate neural network modules for detecting the object, human body key points and head orientation, are implemented to extract relevant features from the RGB images, and then the feature vectors are passed into a deep neural net to perform binary classification. Our results show that the handover gestures are correctly identified with an accuracy of over 90%. The abstraction of the features makes our approach modular and generalizable to different objects and human body types.



### HMQ: Hardware Friendly Mixed Precision Quantization Block for CNNs
- **Arxiv ID**: http://arxiv.org/abs/2007.09952v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.09952v1)
- **Published**: 2020-07-20 09:02:09+00:00
- **Updated**: 2020-07-20 09:02:09+00:00
- **Authors**: Hai Victor Habi, Roy H. Jennings, Arnon Netzer
- **Comment**: None
- **Journal**: None
- **Summary**: Recent work in network quantization produced state-of-the-art results using mixed precision quantization. An imperative requirement for many efficient edge device hardware implementations is that their quantizers are uniform and with power-of-two thresholds. In this work, we introduce the Hardware Friendly Mixed Precision Quantization Block (HMQ) in order to meet this requirement. The HMQ is a mixed precision quantization block that repurposes the Gumbel-Softmax estimator into a smooth estimator of a pair of quantization parameters, namely, bit-width and threshold. HMQs use this to search over a finite space of quantization schemes. Empirically, we apply HMQs to quantize classification models trained on CIFAR10 and ImageNet. For ImageNet, we quantize four different architectures and show that, in spite of the added restrictions to our quantization scheme, we achieve competitive and, in some cases, state-of-the-art results.



### Improving Memory Utilization in Convolutional Neural Network Accelerators
- **Arxiv ID**: http://arxiv.org/abs/2007.09963v2
- **DOI**: 10.1109/LES.2020.3009924
- **Categories**: **eess.IV**, cs.CV, cs.LG, cs.NE, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2007.09963v2)
- **Published**: 2020-07-20 09:34:36+00:00
- **Updated**: 2021-04-06 15:45:49+00:00
- **Authors**: Petar Jokic, Stephane Emery, Luca Benini
- **Comment**: None
- **Journal**: None
- **Summary**: While the accuracy of convolutional neural networks has achieved vast improvements by introducing larger and deeper network architectures, also the memory footprint for storing their parameters and activations has increased. This trend especially challenges power- and resource-limited accelerator designs, which are often restricted to store all network data in on-chip memory to avoid interfacing energy-hungry external memories. Maximizing the network size that fits on a given accelerator thus requires to maximize its memory utilization. While the traditionally used ping-pong buffering technique is mapping subsequent activation layers to disjunctive memory regions, we propose a mapping method that allows these regions to overlap and thus utilize the memory more efficiently. This work presents the mathematical model to compute the maximum activations memory overlap and thus the lower bound of on-chip memory needed to perform layer-by-layer processing of convolutional neural networks on memory-limited accelerators. Our experiments with various real-world object detector networks show that the proposed mapping technique can decrease the activations memory by up to 32.9%, reducing the overall memory for the entire network by up to 23.9% compared to traditional ping-pong buffering. For higher resolution de-noising networks, we achieve activation memory savings of 48.8%. Additionally, we implement a face detector network on an FPGA-based camera to validate these memory savings on a complete end-to-end system.



### GREEN: a Graph REsidual rE-ranking Network for Grading Diabetic Retinopathy
- **Arxiv ID**: http://arxiv.org/abs/2007.09968v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.09968v2)
- **Published**: 2020-07-20 09:41:18+00:00
- **Updated**: 2020-07-21 09:02:15+00:00
- **Authors**: Shaoteng Liu, Lijun Gong, Kai Ma, Yefeng Zheng
- **Comment**: MICCAI2020
- **Journal**: None
- **Summary**: The automatic grading of diabetic retinopathy (DR) facilitates medical diagnosis for both patients and physicians. Existing researches formulate DR grading as an image classification problem. As the stages/categories of DR correlate with each other, the relationship between different classes cannot be explicitly described via a one-hot label because it is empirically estimated by different physicians with different outcomes. This class correlation limits existing networks to achieve effective classification. In this paper, we propose a Graph REsidual rE-ranking Network (GREEN) to introduce a class dependency prior into the original image classification network. The class dependency prior is represented by a graph convolutional network with an adjacency matrix. This prior augments image classification pipeline by re-ranking classification results in a residual aggregation manner. Experiments on the standard benchmarks have shown that GREEN performs favorably against state-of-the-art approaches.



### Quantifying Model Uncertainty in Inverse Problems via Bayesian Deep Gradient Descent
- **Arxiv ID**: http://arxiv.org/abs/2007.09971v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.09971v2)
- **Published**: 2020-07-20 09:43:31+00:00
- **Updated**: 2020-10-19 10:58:39+00:00
- **Authors**: Riccardo Barbano, Chen Zhang, Simon Arridge, Bangti Jin
- **Comment**: 8 pages, 6 figures
- **Journal**: None
- **Summary**: Recent advances in reconstruction methods for inverse problems leverage powerful data-driven models, e.g., deep neural networks. These techniques have demonstrated state-of-the-art performances for several imaging tasks, but they often do not provide uncertainty on the obtained reconstruction. In this work, we develop a scalable, data-driven, knowledge-aided computational framework to quantify the model uncertainty via Bayesian neural networks. The approach builds on, and extends deep gradient descent, a recently developed greedy iterative training scheme, and recasts it within a probabilistic framework. Scalability is achieved by being hybrid in the architecture: only the last layer of each block is Bayesian, while the others remain deterministic, and by being greedy in training. The framework is showcased on one representative medical imaging modality, viz. computed tomography with either sparse view or limited view data, and exhibits competitive performance with respect to state-of-the-art benchmarks, e.g., total variation, deep gradient descent and learned primal-dual.



### Distractor-Aware Neuron Intrinsic Learning for Generic 2D Medical Image Classifications
- **Arxiv ID**: http://arxiv.org/abs/2007.09979v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.09979v2)
- **Published**: 2020-07-20 09:59:04+00:00
- **Updated**: 2020-07-21 08:58:04+00:00
- **Authors**: Lijun Gong, Kai Ma, Yefeng Zheng
- **Comment**: MICCAI2020
- **Journal**: None
- **Summary**: Medical image analysis benefits Computer Aided Diagnosis (CADx). A fundamental analyzing approach is the classification of medical images, which serves for skin lesion diagnosis, diabetic retinopathy grading, and cancer classification on histological images. When learning these discriminative classifiers, we observe that the convolutional neural networks (CNNs) are vulnerable to distractor interference. This is due to the similar sample appearances from different categories (i.e., small inter-class distance). Existing attempts select distractors from input images by empirically estimating their potential effects to the classifier. The essences of how these distractors affect CNN classification are not known. In this paper, we explore distractors from the CNN feature space via proposing a neuron intrinsic learning method. We formulate a novel distractor-aware loss that encourages large distance between the original image and its distractor in the feature space. The novel loss is combined with the original classification loss to update network parameters by back-propagation. Neuron intrinsic learning first explores distractors crucial to the deep classifier and then uses them to robustify CNN inherently. Extensive experiments on medical image benchmark datasets indicate that the proposed method performs favorably against the state-of-the-art approaches.



### Unsupervised Learning of Image Segmentation Based on Differentiable Feature Clustering
- **Arxiv ID**: http://arxiv.org/abs/2007.09990v1
- **DOI**: 10.1109/TIP.2020.3011269
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.09990v1)
- **Published**: 2020-07-20 10:28:36+00:00
- **Updated**: 2020-07-20 10:28:36+00:00
- **Authors**: Wonjik Kim, Asako Kanezaki, Masayuki Tanaka
- **Comment**: IEEE Transactions on Image Processing, Accepted in July, 2020
- **Journal**: None
- **Summary**: The usage of convolutional neural networks (CNNs) for unsupervised image segmentation was investigated in this study. In the proposed approach, label prediction and network parameter learning are alternately iterated to meet the following criteria: (a) pixels of similar features should be assigned the same label, (b) spatially continuous pixels should be assigned the same label, and (c) the number of unique labels should be large. Although these criteria are incompatible, the proposed approach minimizes the combination of similarity loss and spatial continuity loss to find a plausible solution of label assignment that balances the aforementioned criteria well. The contributions of this study are four-fold. First, we propose a novel end-to-end network of unsupervised image segmentation that consists of normalization and an argmax function for differentiable clustering. Second, we introduce a spatial continuity loss function that mitigates the limitations of fixed segment boundaries possessed by previous work. Third, we present an extension of the proposed method for segmentation with scribbles as user input, which showed better accuracy than existing methods while maintaining efficiency. Finally, we introduce another extension of the proposed method: unseen image segmentation by using networks pre-trained with a few reference images without re-training the networks. The effectiveness of the proposed approach was examined on several benchmark datasets of image segmentation.



### On the Comparison of Classic and Deep Keypoint Detector and Descriptor Methods
- **Arxiv ID**: http://arxiv.org/abs/2007.10000v2
- **DOI**: 10.1109/ISPA.2019.8868792
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.10000v2)
- **Published**: 2020-07-20 11:01:01+00:00
- **Updated**: 2020-07-29 08:26:33+00:00
- **Authors**: Kristijan Bartol, David Bojanić, Tomislav Pribanić, Tomislav Petković, Yago Diez Donoso, Joaquim Salvi Mas
- **Comment**: None
- **Journal**: Proceedings of the 2019 11th International Symposium on Image and
  Signal Processing and Analysis (ISPA), Page(s): 64-69
- **Summary**: The purpose of this study is to give a performance comparison between several classic hand-crafted and deep key-point detector and descriptor methods. In particular, we consider the following classical algorithms: SIFT, SURF, ORB, FAST, BRISK, MSER, HARRIS, KAZE, AKAZE, AGAST, GFTT, FREAK, BRIEF and RootSIFT, where a subset of all combinations is paired into detector-descriptor pipelines. Additionally, we analyze the performance of two recent and perspective deep detector-descriptor models, LF-Net and SuperPoint. Our benchmark relies on the HPSequences dataset that provides real and diverse images under various geometric and illumination changes. We analyze the performance on three evaluation tasks: keypoint verification, image matching and keypoint retrieval. The results show that certain classic and deep approaches are still comparable, with some classic detector-descriptor combinations overperforming pretrained deep models. In terms of the execution times of tested implementations, SuperPoint model is the fastest, followed by ORB. The source code is published on \url{https://github.com/kristijanbartol/keypoint-algorithms-benchmark}.



### Deep Image Clustering with Category-Style Representation
- **Arxiv ID**: http://arxiv.org/abs/2007.10004v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.10004v1)
- **Published**: 2020-07-20 11:20:35+00:00
- **Updated**: 2020-07-20 11:20:35+00:00
- **Authors**: Junjie Zhao, Donghuan Lu, Kai Ma, Yu Zhang, Yefeng Zheng
- **Comment**: Accepted at ECCV 2020. Project address:
  https://github.com/sKamiJ/DCCS
- **Journal**: None
- **Summary**: Deep clustering which adopts deep neural networks to obtain optimal representations for clustering has been widely studied recently. In this paper, we propose a novel deep image clustering framework to learn a category-style latent representation in which the category information is disentangled from image style and can be directly used as the cluster assignment. To achieve this goal, mutual information maximization is applied to embed relevant information in the latent representation. Moreover, augmentation-invariant loss is employed to disentangle the representation into category part and style part. Last but not least, a prior distribution is imposed on the latent representation to ensure the elements of the category vector can be used as the probabilities over clusters. Comprehensive experiments demonstrate that the proposed approach outperforms state-of-the-art methods significantly on five public datasets.



### A Macro-Micro Weakly-supervised Framework for AS-OCT Tissue Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2007.10007v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.10007v1)
- **Published**: 2020-07-20 11:26:32+00:00
- **Updated**: 2020-07-20 11:26:32+00:00
- **Authors**: Munan Ning, Cheng Bian, Donghuan Lu, Hong-Yu Zhou, Shuang Yu, Chenglang Yuan, Yang Guo, Yaohua Wang, Kai Ma, Yefeng Zheng
- **Comment**: MICCAI 2020
- **Journal**: None
- **Summary**: Primary angle closure glaucoma (PACG) is the leading cause of irreversible blindness among Asian people. Early detection of PACG is essential, so as to provide timely treatment and minimize the vision loss. In the clinical practice, PACG is diagnosed by analyzing the angle between the cornea and iris with anterior segment optical coherence tomography (AS-OCT). The rapid development of deep learning technologies provides the feasibility of building a computer-aided system for the fast and accurate segmentation of cornea and iris tissues. However, the application of deep learning methods in the medical imaging field is still restricted by the lack of enough fully-annotated samples. In this paper, we propose a novel framework to segment the target tissues accurately for the AS-OCT images, by using the combination of weakly-annotated images (majority) and fully-annotated images (minority). The proposed framework consists of two models which provide reliable guidance for each other. In addition, uncertainty guided strategies are adopted to increase the accuracy and stability of the guidance. Detailed experiments on the publicly available AGE dataset demonstrate that the proposed framework outperforms the state-of-the-art semi-/weakly-supervised methods and has a comparable performance as the fully-supervised method. Therefore, the proposed method is demonstrated to be effective in exploiting information contained in the weakly-annotated images and has the capability to substantively relieve the annotation workload.



### Search What You Want: Barrier Panelty NAS for Mixed Precision Quantization
- **Arxiv ID**: http://arxiv.org/abs/2007.10026v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.10026v1)
- **Published**: 2020-07-20 12:00:48+00:00
- **Updated**: 2020-07-20 12:00:48+00:00
- **Authors**: Haibao Yu, Qi Han, Jianbo Li, Jianping Shi, Guangliang Cheng, Bin Fan
- **Comment**: ECCV2020
- **Journal**: None
- **Summary**: Emergent hardwares can support mixed precision CNN models inference that assign different bitwidths for different layers. Learning to find an optimal mixed precision model that can preserve accuracy and satisfy the specific constraints on model size and computation is extremely challenge due to the difficult in training a mixed precision model and the huge space of all possible bit quantizations. In this paper, we propose a novel soft Barrier Penalty based NAS (BP-NAS) for mixed precision quantization, which ensures all the searched models are inside the valid domain defined by the complexity constraint, thus could return an optimal model under the given constraint by conducting search only one time. The proposed soft Barrier Penalty is differentiable and can impose very large losses to those models outside the valid domain while almost no punishment for models inside the valid domain, thus constraining the search only in the feasible domain. In addition, a differentiable Prob-1 regularizer is proposed to ensure learning with NAS is reasonable. A distribution reshaping training strategy is also used to make training more stable. BP-NAS sets new state of the arts on both classification (Cifar-10, ImageNet) and detection (COCO), surpassing all the efficient mixed precision methods designed manually and automatically. Particularly, BP-NAS achieves higher mAP (up to 2.7\% mAP improvement) together with lower bit computation cost compared with the existing best mixed precision model on COCO detection.



### Making Affine Correspondences Work in Camera Geometry Computation
- **Arxiv ID**: http://arxiv.org/abs/2007.10032v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.10032v1)
- **Published**: 2020-07-20 12:07:48+00:00
- **Updated**: 2020-07-20 12:07:48+00:00
- **Authors**: Daniel Barath, Michal Polic, Wolfgang Förstner, Torsten Sattler, Tomas Pajdla, Zuzana Kukelova
- **Comment**: None
- **Journal**: None
- **Summary**: Local features e.g. SIFT and its affine and learned variants provide region-to-region rather than point-to-point correspondences. This has recently been exploited to create new minimal solvers for classical problems such as homography, essential and fundamental matrix estimation. The main advantage of such solvers is that their sample size is smaller, e.g., only two instead of four matches are required to estimate a homography. Works proposing such solvers often claim a significant improvement in run-time thanks to fewer RANSAC iterations. We show that this argument is not valid in practice if the solvers are used naively. To overcome this, we propose guidelines for effective use of region-to-region matches in the course of a full model estimation pipeline. We propose a method for refining the local feature geometries by symmetric intensity-based matching, combine uncertainty propagation inside RANSAC with preemptive model verification, show a general scheme for computing uncertainty of minimal solvers results, and adapt the sample cheirality check for homography estimation. Our experiments show that affine solvers can achieve accuracy comparable to point-based solvers at faster run-times when following our guidelines. We make code available at https://github.com/danini/affine-correspondences-for-camera-geometry.



### Improving Semantic Segmentation via Decoupled Body and Edge Supervision
- **Arxiv ID**: http://arxiv.org/abs/2007.10035v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.10035v2)
- **Published**: 2020-07-20 12:11:22+00:00
- **Updated**: 2020-08-18 03:41:10+00:00
- **Authors**: Xiangtai Li, Xia Li, Li Zhang, Guangliang Cheng, Jianping Shi, Zhouchen Lin, Shaohua Tan, Yunhai Tong
- **Comment**: accepted by ECCV 2020
- **Journal**: None
- **Summary**: Existing semantic segmentation approaches either aim to improve the object's inner consistency by modeling the global context, or refine objects detail along their boundaries by multi-scale feature fusion. In this paper, a new paradigm for semantic segmentation is proposed. Our insight is that appealing performance of semantic segmentation requires \textit{explicitly} modeling the object \textit{body} and \textit{edge}, which correspond to the high and low frequency of the image. To do so, we first warp the image feature by learning a flow field to make the object part more consistent. The resulting body feature and the residual edge feature are further optimized under decoupled supervision by explicitly sampling different parts (body or edge) pixels. We show that the proposed framework with various baselines or backbone networks leads to better object inner consistency and object boundaries. Extensive experiments on four major road scene semantic segmentation benchmarks including \textit{Cityscapes}, \textit{CamVid}, \textit{KIITI} and \textit{BDD} show that our proposed approach establishes new state of the art while retaining high efficiency in inference. In particular, we achieve 83.7 mIoU \% on Cityscape with only fine-annotated data. Code and models are made available to foster any further research (\url{https://github.com/lxtGH/DecoupleSegNets}).



### Non-Local Spatial Propagation Network for Depth Completion
- **Arxiv ID**: http://arxiv.org/abs/2007.10042v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.10042v1)
- **Published**: 2020-07-20 12:26:51+00:00
- **Updated**: 2020-07-20 12:26:51+00:00
- **Authors**: Jinsun Park, Kyungdon Joo, Zhe Hu, Chi-Kuei Liu, In So Kweon
- **Comment**: To appear in ECCV 2020. Project page:
  https://github.com/zzangjinsun/NLSPN_ECCV20
- **Journal**: None
- **Summary**: In this paper, we propose a robust and efficient end-to-end non-local spatial propagation network for depth completion. The proposed network takes RGB and sparse depth images as inputs and estimates non-local neighbors and their affinities of each pixel, as well as an initial depth map with pixel-wise confidences. The initial depth prediction is then iteratively refined by its confidence and non-local spatial propagation procedure based on the predicted non-local neighbors and corresponding affinities. Unlike previous algorithms that utilize fixed-local neighbors, the proposed algorithm effectively avoids irrelevant local neighbors and concentrates on relevant non-local neighbors during propagation. In addition, we introduce a learnable affinity normalization to better learn the affinity combinations compared to conventional methods. The proposed algorithm is inherently robust to the mixed-depth problem on depth boundaries, which is one of the major issues for existing depth estimation/completion algorithms. Experimental results on indoor and outdoor datasets demonstrate that the proposed algorithm is superior to conventional algorithms in terms of depth completion accuracy and robustness to the mixed-depth problem. Our implementation is publicly available on the project page.



### Cephalometric Landmark Regression with Convolutional Neural Networks on 3D Computed Tomography Data
- **Arxiv ID**: http://arxiv.org/abs/2007.10052v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.10052v1)
- **Published**: 2020-07-20 12:45:38+00:00
- **Updated**: 2020-07-20 12:45:38+00:00
- **Authors**: Dmitry Lachinov, Alexandra Getmanskaya, Vadim Turlapov
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we address the problem of automatic three-dimensional cephalometric analysis. Cephalometric analysis performed on lateral radiographs doesn't fully exploit the structure of 3D objects due to projection onto the lateral plane. With the development of three-dimensional imaging techniques such as CT, several analysis methods have been proposed that extend to the 3D case. The analysis based on these methods is invariant to rotations and translations and can describe difficult skull deformation, where 2D cephalometry has no use. In this paper, we provide a wide overview of existing approaches for cephalometric landmark regression. Moreover, we perform a series of experiments with state of the art 3D convolutional neural network (CNN) based methods for keypoint regression: direct regression with CNN, heatmap regression and Softargmax regression. For the first time, we extensively evaluate the described methods and demonstrate their effectiveness in the estimation of Frankfort Horizontal and cephalometric points locations for patients with severe skull deformations. We demonstrate that Heatmap and Softargmax regression models provide sufficient regression error for medical applications (less than 4 mm). Moreover, the Softargmax model achieves 1.15o inclination error for the Frankfort horizontal. For the fair comparison with the prior art, we also report results projected on the lateral plane.



### Wearable camera-based human absolute localization in large warehouses
- **Arxiv ID**: http://arxiv.org/abs/2007.10066v1
- **DOI**: 10.1117/12.2559424
- **Categories**: **cs.CV**, I.1.4; I.1.5
- **Links**: [PDF](http://arxiv.org/pdf/2007.10066v1)
- **Published**: 2020-07-20 12:57:37+00:00
- **Updated**: 2020-07-20 12:57:37+00:00
- **Authors**: Gaël Écorchard, Karel Košnar, Libor Přeučil
- **Comment**: Conference paper presented at Twelfth International Conference on
  Machine Vision, 2019
- **Journal**: Twelfth International Conference on Machine Vision, 2019
- **Summary**: In a robotised warehouse, as in any place where robots move autonomously, a major issue is the localization or detection of human operators during their intervention in the work area of the robots. This paper introduces a wearable human localization system for large warehouses, which utilize preinstalled infrastructure used for localization of automated guided vehicles (AGVs). A monocular down-looking camera is detecting ground nodes, identifying them and computing the absolute position of the human to allow safe cooperation and coexistence of humans and AGVs in the same workspace. A virtual safety area around the human operator is set up and any AGV in this area is immediately stopped. In order to avoid triggering an emergency stop because of the short distance between robots and human operators, the trajectories of the robots have to be modified so that they do not interfere with the human. The purpose of this paper is to demonstrate an absolute visual localization method working in the challenging environment of an automated warehouse with low intensity of light, massively changing environment and using solely monocular camera placed on the human body.



### Investigating Bias and Fairness in Facial Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/2007.10075v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.10075v3)
- **Published**: 2020-07-20 13:12:53+00:00
- **Updated**: 2020-08-21 15:29:22+00:00
- **Authors**: Tian Xu, Jennifer White, Sinan Kalkan, Hatice Gunes
- **Comment**: None
- **Journal**: None
- **Summary**: Recognition of expressions of emotions and affect from facial images is a well-studied research problem in the fields of affective computing and computer vision with a large number of datasets available containing facial images and corresponding expression labels. However, virtually none of these datasets have been acquired with consideration of fair distribution across the human population. Therefore, in this work, we undertake a systematic investigation of bias and fairness in facial expression recognition by comparing three different approaches, namely a baseline, an attribute-aware and a disentangled approach, on two well-known datasets, RAF-DB and CelebA. Our results indicate that: (i) data augmentation improves the accuracy of the baseline model, but this alone is unable to mitigate the bias effect; (ii) both the attribute-aware and the disentangled approaches fortified with data augmentation perform better than the baseline approach in terms of accuracy and fairness; (iii) the disentangled approach is the best for mitigating demographic bias; and (iv) the bias mitigation strategies are more suitable in the existence of uneven attribute distribution or imbalanced number of subgroup data.



### Relative Pose from Deep Learned Depth and a Single Affine Correspondence
- **Arxiv ID**: http://arxiv.org/abs/2007.10082v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.10082v1)
- **Published**: 2020-07-20 13:24:28+00:00
- **Updated**: 2020-07-20 13:24:28+00:00
- **Authors**: Ivan Eichhardt, Daniel Barath
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a new approach for combining deep-learned non-metric monocular depth with affine correspondences (ACs) to estimate the relative pose of two calibrated cameras from a single correspondence. Considering the depth information and affine features, two new constraints on the camera pose are derived. The proposed solver is usable within 1-point RANSAC approaches. Thus, the processing time of the robust estimation is linear in the number of correspondences and, therefore, orders of magnitude faster than by using traditional approaches. The proposed 1AC+D solver is tested both on synthetic data and on 110395 publicly available real image pairs where we used an off-the-shelf monocular depth network to provide up-to-scale depth per pixel. The proposed 1AC+D leads to similar accuracy as traditional approaches while being significantly faster. When solving large-scale problems, e.g., pose-graph initialization for Structure-from-Motion (SfM) pipelines, the overhead of obtaining ACs and monocular depth is negligible compared to the speed-up gained in the pairwise geometric verification, i.e., relative pose estimation. This is demonstrated on scenes from the 1DSfM dataset using a state-of-the-art global SfM algorithm. Source code: https://github.com/eivan/one-ac-pose



### Improving Attention-Based Handwritten Mathematical Expression Recognition with Scale Augmentation and Drop Attention
- **Arxiv ID**: http://arxiv.org/abs/2007.10092v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.10092v1)
- **Published**: 2020-07-20 13:35:09+00:00
- **Updated**: 2020-07-20 13:35:09+00:00
- **Authors**: Zhe Li, Lianwen Jin, Songxuan Lai, Yecheng Zhu
- **Comment**: Accepted to appear in ICFHR 2020
- **Journal**: None
- **Summary**: Handwritten mathematical expression recognition (HMER) is an important research direction in handwriting recognition. The performance of HMER suffers from the two-dimensional structure of mathematical expressions (MEs). To address this issue, in this paper, we propose a high-performance HMER model with scale augmentation and drop attention. Specifically, tackling ME with unstable scale in both horizontal and vertical directions, scale augmentation improves the performance of the model on MEs of various scales. An attention-based encoder-decoder network is used for extracting features and generating predictions. In addition, drop attention is proposed to further improve performance when the attention distribution of the decoder is not precise. Compared with previous methods, our method achieves state-of-the-art performance on two public datasets of CROHME 2014 and CROHME 2016.



### GarNet++: Improving Fast and Accurate Static3D Cloth Draping by Curvature Loss
- **Arxiv ID**: http://arxiv.org/abs/2007.10867v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.10867v1)
- **Published**: 2020-07-20 13:40:15+00:00
- **Updated**: 2020-07-20 13:40:15+00:00
- **Authors**: Erhan Gundogdu, Victor Constantin, Shaifali Parashar, Amrollah Seifoddini, Minh Dang, Mathieu Salzmann, Pascal Fua
- **Comment**: Accepted to be published in IEEE Transactions on Pattern Analysis and
  Machine Intelligence (TPAMI), July 2020. arXiv admin note: text overlap with
  arXiv:1811.10983
- **Journal**: None
- **Summary**: In this paper, we tackle the problem of static 3D cloth draping on virtual human bodies. We introduce a two-stream deep network model that produces a visually plausible draping of a template cloth on virtual 3D bodies by extracting features from both the body and garment shapes. Our network learns to mimic a Physics-Based Simulation (PBS) method while requiring two orders of magnitude less computation time. To train the network, we introduce loss terms inspired by PBS to produce plausible results and make the model collision-aware. To increase the details of the draped garment, we introduce two loss functions that penalize the difference between the curvature of the predicted cloth and PBS. Particularly, we study the impact of mean curvature normal and a novel detail-preserving loss both qualitatively and quantitatively. Our new curvature loss computes the local covariance matrices of the 3D points, and compares the Rayleigh quotients of the prediction and PBS. This leads to more details while performing favorably or comparably against the loss that considers mean curvature normal vectors in the 3D triangulated meshes. We validate our framework on four garment types for various body shapes and poses. Finally, we achieve superior performance against a recently proposed data-driven method.



### ThriftyNets : Convolutional Neural Networks with Tiny Parameter Budget
- **Arxiv ID**: http://arxiv.org/abs/2007.10106v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.10106v1)
- **Published**: 2020-07-20 13:50:51+00:00
- **Updated**: 2020-07-20 13:50:51+00:00
- **Authors**: Guillaume Coiffier, Ghouthi Boukli Hacene, Vincent Gripon
- **Comment**: None
- **Journal**: None
- **Summary**: Typical deep convolutional architectures present an increasing number of feature maps as we go deeper in the network, whereas spatial resolution of inputs is decreased through downsampling operations. This means that most of the parameters lay in the final layers, while a large portion of the computations are performed by a small fraction of the total parameters in the first layers. In an effort to use every parameter of a network at its maximum, we propose a new convolutional neural network architecture, called ThriftyNet. In ThriftyNet, only one convolutional layer is defined and used recursively, leading to a maximal parameter factorization. In complement, normalization, non-linearities, downsamplings and shortcut ensure sufficient expressivity of the model. ThriftyNet achieves competitive performance on a tiny parameters budget, exceeding 91% accuracy on CIFAR-10 with less than 40K parameters in total, and 74.3% on CIFAR-100 with less than 600K parameters.



### Monte Carlo Dropout Ensembles for Robust Illumination Estimation
- **Arxiv ID**: http://arxiv.org/abs/2007.10114v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.10114v1)
- **Published**: 2020-07-20 13:56:14+00:00
- **Updated**: 2020-07-20 13:56:14+00:00
- **Authors**: Firas Laakom, Jenni Raitoharju, Alexandros Iosifidis, Jarno Nikkanen, Moncef Gabbouj
- **Comment**: 7 pages,6 figures
- **Journal**: None
- **Summary**: Computational color constancy is a preprocessing step used in many camera systems. The main aim is to discount the effect of the illumination on the colors in the scene and restore the original colors of the objects. Recently, several deep learning-based approaches have been proposed to solve this problem and they often led to state-of-the-art performance in terms of average errors. However, for extreme samples, these methods fail and lead to high errors. In this paper, we address this limitation by proposing to aggregate different deep learning methods according to their output uncertainty. We estimate the relative uncertainty of each approach using Monte Carlo dropout and the final illumination estimate is obtained as the sum of the different model estimates weighted by the log-inverse of their corresponding uncertainties. The proposed framework leads to state-of-the-art performance on INTEL-TAU dataset.



### Discrete Point Flow Networks for Efficient Point Cloud Generation
- **Arxiv ID**: http://arxiv.org/abs/2007.10170v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.10170v1)
- **Published**: 2020-07-20 14:48:00+00:00
- **Updated**: 2020-07-20 14:48:00+00:00
- **Authors**: Roman Klokov, Edmond Boyer, Jakob Verbeek
- **Comment**: In ECCV'20
- **Journal**: None
- **Summary**: Generative models have proven effective at modeling 3D shapes and their statistical variations. In this paper we investigate their application to point clouds, a 3D shape representation widely used in computer vision for which, however, only few generative models have yet been proposed. We introduce a latent variable model that builds on normalizing flows with affine coupling layers to generate 3D point clouds of an arbitrary size given a latent shape representation. To evaluate its benefits for shape modeling we apply this model for generation, autoencoding, and single-view shape reconstruction tasks. We improve over recent GAN-based models in terms of most metrics that assess generation and autoencoding. Compared to recent work based on continuous flows, our model offers a significant speedup in both training and inference times for similar or better performance. For single-view shape reconstruction we also obtain results on par with state-of-the-art voxel, point cloud, and mesh-based methods.



### NPCFace: Negative-Positive Collaborative Training for Large-scale Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2007.10172v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.10172v3)
- **Published**: 2020-07-20 14:52:29+00:00
- **Updated**: 2021-05-14 11:37:18+00:00
- **Authors**: Dan Zeng, Hailin Shi, Hang Du, Jun Wang, Zhen Lei, Tao Mei
- **Comment**: None
- **Journal**: None
- **Summary**: The training scheme of deep face recognition has greatly evolved in the past years, yet it encounters new challenges in the large-scale data situation where massive and diverse hard cases occur. Especially in the range of low false accept rate (FAR), there are various hard cases in both positives (intra-class) and negatives (inter-class). In this paper, we study how to make better use of these hard samples for improving the training. The literature approaches this by margin-based formulation in either positive logit or negative logits. However, the correlation between hard positive and hard negative is overlooked, and so is the relation between the margins in positive and negative logits. We find such correlation is significant, especially in the large-scale dataset, and one can take advantage from it to boost the training via relating the positive and negative margins for each training sample. To this end, we propose an explicit collaboration between positive and negative margins sample-wisely. Given a batch of hard samples, a novel Negative-Positive Collaboration loss, named NPCFace, is formulated, which emphasizes the training on both negative and positive hard cases via the collaborative-margin mechanism in the softmax logits, and also brings better interpretation of negative-positive hardness correlation. Besides, the emphasis is implemented with an improved formulation to achieve stable convergence and flexible parameter setting. We validate the effectiveness of our approach on various benchmarks of large-scale face recognition, and obtain advantageous results especially in the low FAR range.



### Can we cover navigational perception needs of the visually impaired by panoptic segmentation?
- **Arxiv ID**: http://arxiv.org/abs/2007.10202v1
- **DOI**: None
- **Categories**: **cs.CV**, 68T07, I.4.6; I.5.4; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2007.10202v1)
- **Published**: 2020-07-20 15:35:58+00:00
- **Updated**: 2020-07-20 15:35:58+00:00
- **Authors**: Wei Mao, Jiaming Zhang, Kailun Yang, Rainer Stiefelhagen
- **Comment**: 14 pages, 6 Figures, 2 tables
- **Journal**: None
- **Summary**: Navigational perception for visually impaired people has been substantially promoted by both classic and deep learning based segmentation methods. In classic visual recognition methods, the segmentation models are mostly object-dependent, which means a specific algorithm has to be devised for the object of interest. In contrast, deep learning based models such as instance segmentation and semantic segmentation allow to individually recognize part of the entire scene, namely things or stuff, for blind individuals. However, both of them can not provide a holistic understanding of the surroundings for the visually impaired. Panoptic segmentation is a newly proposed visual model with the aim of unifying semantic segmentation and instance segmentation. Motivated by that, we propose to utilize panoptic segmentation as an approach to navigating visually impaired people by offering both things and stuff awareness in the proximity of the visually impaired. We demonstrate that panoptic segmentation is able to equip the visually impaired with a holistic real-world scene perception through a wearable assistive system.



### Learning latent representations across multiple data domains using Lifelong VAEGAN
- **Arxiv ID**: http://arxiv.org/abs/2007.10221v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.10221v1)
- **Published**: 2020-07-20 16:08:36+00:00
- **Updated**: 2020-07-20 16:08:36+00:00
- **Authors**: Fei Ye, Adrian G. Bors
- **Comment**: Accepted as a conference paper at ECCV 2020
- **Journal**: None
- **Summary**: The problem of catastrophic forgetting occurs in deep learning models trained on multiple databases in a sequential manner. Recently, generative replay mechanisms (GRM), have been proposed to reproduce previously learned knowledge aiming to reduce the forgetting. However, such approaches lack an appropriate inference model and therefore can not provide latent representations of data. In this paper, we propose a novel lifelong learning approach, namely the Lifelong VAEGAN (L-VAEGAN), which not only induces a powerful generative replay network but also learns meaningful latent representations, benefiting representation learning. L-VAEGAN can allow to automatically embed the information associated with different domains into several clusters in the latent space, while also capturing semantically meaningful shared latent variables, across different data domains. The proposed model supports many downstream tasks that traditional generative replay methods can not, including interpolation and inference across different data domains.



### Unsupervised Domain Adaptation in the Absence of Source Data
- **Arxiv ID**: http://arxiv.org/abs/2007.10233v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.10233v1)
- **Published**: 2020-07-20 16:22:14+00:00
- **Updated**: 2020-07-20 16:22:14+00:00
- **Authors**: Roshni Sahoo, Divya Shanmugam, John Guttag
- **Comment**: None
- **Journal**: None
- **Summary**: Current unsupervised domain adaptation methods can address many types of distribution shift, but they assume data from the source domain is freely available. As the use of pre-trained models becomes more prevalent, it is reasonable to assume that source data is unavailable. We propose an unsupervised method for adapting a source classifier to a target domain that varies from the source domain along natural axes, such as brightness and contrast. Our method only requires access to unlabeled target instances and the source classifier. We validate our method in scenarios where the distribution shift involves brightness, contrast, and rotation and show that it outperforms fine-tuning baselines in scenarios with limited labeled data.



### Inter-Homines: Distance-Based Risk Estimation for Human Safety
- **Arxiv ID**: http://arxiv.org/abs/2007.10243v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.10243v1)
- **Published**: 2020-07-20 16:32:27+00:00
- **Updated**: 2020-07-20 16:32:27+00:00
- **Authors**: Matteo Fabbri, Fabio Lanzi, Riccardo Gasparini, Simone Calderara, Lorenzo Baraldi, Rita Cucchiara
- **Comment**: None
- **Journal**: None
- **Summary**: In this document, we report our proposal for modeling the risk of possible contagiousity in a given area monitored by RGB cameras where people freely move and interact. Our system, called Inter-Homines, evaluates in real-time the contagion risk in a monitored area by analyzing video streams: it is able to locate people in 3D space, calculate interpersonal distances and predict risk levels by building dynamic maps of the monitored area. Inter-Homines works both indoor and outdoor, in public and private crowded areas. The software is applicable to already installed cameras or low-cost cameras on industrial PCs, equipped with an additional embedded edge-AI system for temporary measurements. From the AI-side, we exploit a robust pipeline for real-time people detection and localization in the ground plane by homographic transformation based on state-of-the-art computer vision algorithms; it is a combination of a people detector and a pose estimator. From the risk modeling side, we propose a parametric model for a spatio-temporal dynamic risk estimation, that, validated by epidemiologists, could be useful for safety monitoring the acceptance of social distancing prevention measures by predicting the risk level of the scene.



### Learning Joint Spatial-Temporal Transformations for Video Inpainting
- **Arxiv ID**: http://arxiv.org/abs/2007.10247v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.10247v1)
- **Published**: 2020-07-20 16:35:48+00:00
- **Updated**: 2020-07-20 16:35:48+00:00
- **Authors**: Yanhong Zeng, Jianlong Fu, Hongyang Chao
- **Comment**: Accepted by ECCV2020
- **Journal**: None
- **Summary**: High-quality video inpainting that completes missing regions in video frames is a promising yet challenging task. State-of-the-art approaches adopt attention models to complete a frame by searching missing contents from reference frames, and further complete whole videos frame by frame. However, these approaches can suffer from inconsistent attention results along spatial and temporal dimensions, which often leads to blurriness and temporal artifacts in videos. In this paper, we propose to learn a joint Spatial-Temporal Transformer Network (STTN) for video inpainting. Specifically, we simultaneously fill missing regions in all input frames by self-attention, and propose to optimize STTN by a spatial-temporal adversarial loss. To show the superiority of the proposed model, we conduct both quantitative and qualitative evaluations by using standard stationary masks and more realistic moving object masks. Demo videos are available at https://github.com/researchmm/STTN.



### XMixup: Efficient Transfer Learning with Auxiliary Samples by Cross-domain Mixup
- **Arxiv ID**: http://arxiv.org/abs/2007.10252v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.10252v1)
- **Published**: 2020-07-20 16:42:29+00:00
- **Updated**: 2020-07-20 16:42:29+00:00
- **Authors**: Xingjian Li, Haoyi Xiong, Haozhe An, Chengzhong Xu, Dejing Dou
- **Comment**: None
- **Journal**: None
- **Summary**: Transferring knowledge from large source datasets is an effective way to fine-tune the deep neural networks of the target task with a small sample size. A great number of algorithms have been proposed to facilitate deep transfer learning, and these techniques could be generally categorized into two groups - Regularized Learning of the target task using models that have been pre-trained from source datasets, and Multitask Learning with both source and target datasets to train a shared backbone neural network. In this work, we aim to improve the multitask paradigm for deep transfer learning via Cross-domain Mixup (XMixup). While the existing multitask learning algorithms need to run backpropagation over both the source and target datasets and usually consume a higher gradient complexity, XMixup transfers the knowledge from source to target tasks more efficiently: for every class of the target task, XMixup selects the auxiliary samples from the source dataset and augments training samples via the simple mixup strategy. We evaluate XMixup over six real world transfer learning datasets. Experiment results show that XMixup improves the accuracy by 1.9% on average. Compared with other state-of-the-art transfer learning approaches, XMixup costs much less training time while still obtains higher accuracy.



### Relatable Clothing: Detecting Visual Relationships between People and Clothing
- **Arxiv ID**: http://arxiv.org/abs/2007.10283v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.10283v1)
- **Published**: 2020-07-20 17:10:32+00:00
- **Updated**: 2020-07-20 17:10:32+00:00
- **Authors**: Thomas Truong, Svetlana Yanushkevich
- **Comment**: 7 pages, 7 figures, accepted to ICPR 2020
- **Journal**: None
- **Summary**: Detecting visual relationships between people and clothing in an image has been a relatively unexplored problem in the field of computer vision and biometrics. The lack readily available public dataset for ``worn'' and ``unworn'' classification has slowed the development of solutions for this problem. We present the release of the Relatable Clothing Dataset which contains 35287 person-clothing pairs and segmentation masks for the development of ``worn'' and ``unworn'' classification models. Additionally, we propose a novel soft attention unit for performing ``worn'' and ``unworn'' classification using deep neural networks. The proposed soft attention models have an accuracy of upward $98.55\% \pm 0.35\%$ on the Relatable Clothing Dataset and demonstrate high generalizable, allowing us to classify unseen articles of clothing such as high visibility vests as ``worn'' or ``unworn''.



### Coupling Explicit and Implicit Surface Representations for Generative 3D Modeling
- **Arxiv ID**: http://arxiv.org/abs/2007.10294v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.10294v2)
- **Published**: 2020-07-20 17:24:51+00:00
- **Updated**: 2020-10-17 02:10:58+00:00
- **Authors**: Omid Poursaeed, Matthew Fisher, Noam Aigerman, Vladimir G. Kim
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: We propose a novel neural architecture for representing 3D surfaces, which harnesses two complementary shape representations: (i) an explicit representation via an atlas, i.e., embeddings of 2D domains into 3D; (ii) an implicit-function representation, i.e., a scalar function over the 3D volume, with its levels denoting surfaces. We make these two representations synergistic by introducing novel consistency losses that ensure that the surface created from the atlas aligns with the level-set of the implicit function. Our hybrid architecture outputs results which are superior to the output of the two equivalent single-representation networks, yielding smoother explicit surfaces with more accurate normals, and a more accurate implicit occupancy function. Additionally, our surface reconstruction step can directly leverage the explicit atlas-based representation. This process is computationally efficient, and can be directly used by differentiable rasterizers, enabling training our hybrid representation with image-based losses.



### Landmark Guidance Independent Spatio-channel Attention and Complementary Context Information based Facial Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/2007.10298v2
- **DOI**: 10.1016/j.patrec.2021.01.029
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.10298v2)
- **Published**: 2020-07-20 17:33:32+00:00
- **Updated**: 2020-07-25 14:50:25+00:00
- **Authors**: Darshan Gera, S Balasubramanian
- **Comment**: A couple of reference citations corrected, few details added and code
  link provided
- **Journal**: Pattern Recognition Letters 145 (2021)
- **Summary**: A recent trend to recognize facial expressions in the real-world scenario is to deploy attention based convolutional neural networks (CNNs) locally to signify the importance of facial regions and, combine it with global facial features and/or other complementary context information for performance gain. However, in the presence of occlusions and pose variations, different channels respond differently, and further that the response intensity of a channel differ across spatial locations. Also, modern facial expression recognition(FER) architectures rely on external sources like landmark detectors for defining attention. Failure of landmark detector will have a cascading effect on FER. Additionally, there is no emphasis laid on the relevance of features that are input to compute complementary context information. Leveraging on the aforementioned observations, an end-to-end architecture for FER is proposed in this work that obtains both local and global attention per channel per spatial location through a novel spatio-channel attention net (SCAN), without seeking any information from the landmark detectors. SCAN is complemented by a complementary context information (CCI) branch. Further, using efficient channel attention (ECA), the relevance of features input to CCI is also attended to. The representation learnt by the proposed architecture is robust to occlusions and pose variations. Robustness and superior performance of the proposed model is demonstrated on both in-lab and in-the-wild datasets (AffectNet, FERPlus, RAF-DB, FED-RO, SFEW, CK+, Oulu-CASIA and JAFFE) along with a couple of constructed face mask datasets resembling masked faces in COVID-19 scenario. Codes are publicly available at https://github.com/1980x/SCAN-CCI-FER



### Object-Centric Multi-View Aggregation
- **Arxiv ID**: http://arxiv.org/abs/2007.10300v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.10300v2)
- **Published**: 2020-07-20 17:38:31+00:00
- **Updated**: 2020-07-21 05:17:19+00:00
- **Authors**: Shubham Tulsiani, Or Litany, Charles R. Qi, He Wang, Leonidas J. Guibas
- **Comment**: None
- **Journal**: None
- **Summary**: We present an approach for aggregating a sparse set of views of an object in order to compute a semi-implicit 3D representation in the form of a volumetric feature grid. Key to our approach is an object-centric canonical 3D coordinate system into which views can be lifted, without explicit camera pose estimation, and then combined -- in a manner that can accommodate a variable number of views and is view order independent. We show that computing a symmetry-aware mapping from pixels to the canonical coordinate system allows us to better propagate information to unseen regions, as well as to robustly overcome pose ambiguities during inference. Our aggregate representation enables us to perform 3D inference tasks like volumetric reconstruction and novel view synthesis, and we use these tasks to demonstrate the benefits of our aggregation approach as compared to implicit or camera-centric alternatives.



### Adding Seemingly Uninformative Labels Helps in Low Data Regimes
- **Arxiv ID**: http://arxiv.org/abs/2008.00807v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2008.00807v2)
- **Published**: 2020-07-20 17:38:59+00:00
- **Updated**: 2020-08-11 10:52:43+00:00
- **Authors**: Christos Matsoukas, Albert Bou I Hernandez, Yue Liu, Karin Dembrower, Gisele Miranda, Emir Konuk, Johan Fredin Haslum, Athanasios Zouzos, Peter Lindholm, Fredrik Strand, Kevin Smith
- **Comment**: ICML 2020
- **Journal**: None
- **Summary**: Evidence suggests that networks trained on large datasets generalize well not solely because of the numerous training examples, but also class diversity which encourages learning of enriched features. This raises the question of whether this remains true when data is scarce - is there an advantage to learning with additional labels in low-data regimes? In this work, we consider a task that requires difficult-to-obtain expert annotations: tumor segmentation in mammography images. We show that, in low-data settings, performance can be improved by complementing the expert annotations with seemingly uninformative labels from non-expert annotators, turning the task into a multi-class problem. We reveal that these gains increase when less expert data is available, and uncover several interesting properties through further studies. We demonstrate our findings on CSAW-S, a new dataset that we introduce here, and confirm them on two public datasets.



### Joint Disentangling and Adaptation for Cross-Domain Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2007.10315v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.10315v1)
- **Published**: 2020-07-20 17:57:02+00:00
- **Updated**: 2020-07-20 17:57:02+00:00
- **Authors**: Yang Zou, Xiaodong Yang, Zhiding Yu, B. V. K. Vijaya Kumar, Jan Kautz
- **Comment**: ECCV 2020 (Oral)
- **Journal**: None
- **Summary**: Although a significant progress has been witnessed in supervised person re-identification (re-id), it remains challenging to generalize re-id models to new domains due to the huge domain gaps. Recently, there has been a growing interest in using unsupervised domain adaptation to address this scalability issue. Existing methods typically conduct adaptation on the representation space that contains both id-related and id-unrelated factors, thus inevitably undermining the adaptation efficacy of id-related features. In this paper, we seek to improve adaptation by purifying the representation space to be adapted. To this end, we propose a joint learning framework that disentangles id-related/unrelated features and enforces adaptation to work on the id-related feature space exclusively. Our model involves a disentangling module that encodes cross-domain images into a shared appearance space and two separate structure spaces, and an adaptation module that performs adversarial alignment and self-training on the shared appearance space. The two modules are co-designed to be mutually beneficial. Extensive experiments demonstrate that the proposed joint learning framework outperforms the state-of-the-art methods by clear margins.



### MCUNet: Tiny Deep Learning on IoT Devices
- **Arxiv ID**: http://arxiv.org/abs/2007.10319v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.10319v2)
- **Published**: 2020-07-20 17:59:01+00:00
- **Updated**: 2020-11-19 17:29:28+00:00
- **Authors**: Ji Lin, Wei-Ming Chen, Yujun Lin, John Cohn, Chuang Gan, Song Han
- **Comment**: NeurIPS 2020 (spotlight)
- **Journal**: None
- **Summary**: Machine learning on tiny IoT devices based on microcontroller units (MCU) is appealing but challenging: the memory of microcontrollers is 2-3 orders of magnitude smaller even than mobile phones. We propose MCUNet, a framework that jointly designs the efficient neural architecture (TinyNAS) and the lightweight inference engine (TinyEngine), enabling ImageNet-scale inference on microcontrollers. TinyNAS adopts a two-stage neural architecture search approach that first optimizes the search space to fit the resource constraints, then specializes the network architecture in the optimized search space. TinyNAS can automatically handle diverse constraints (i.e.device, latency, energy, memory) under low search costs.TinyNAS is co-designed with TinyEngine, a memory-efficient inference library to expand the search space and fit a larger model. TinyEngine adapts the memory scheduling according to the overall network topology rather than layer-wise optimization, reducing the memory usage by 4.8x, and accelerating the inference by 1.7-3.3x compared to TF-Lite Micro and CMSIS-NN. MCUNet is the first to achieves >70% ImageNet top1 accuracy on an off-the-shelf commercial microcontroller, using 3.5x less SRAM and 5.7x less Flash compared to quantized MobileNetV2 and ResNet-18. On visual&audio wake words tasks, MCUNet achieves state-of-the-art accuracy and runs 2.4-3.4x faster than MobileNetV2 and ProxylessNAS-based solutions with 3.7-4.1x smaller peak SRAM. Our study suggests that the era of always-on tiny machine learning on IoT devices has arrived. Code and models can be found here: https://tinyml.mit.edu.



### Hierarchical Contrastive Motion Learning for Video Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2007.10321v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.10321v3)
- **Published**: 2020-07-20 17:59:22+00:00
- **Updated**: 2022-01-17 09:30:18+00:00
- **Authors**: Xitong Yang, Xiaodong Yang, Sifei Liu, Deqing Sun, Larry Davis, Jan Kautz
- **Comment**: BMVC2021 camera ready (Oral)
- **Journal**: None
- **Summary**: One central question for video action recognition is how to model motion. In this paper, we present hierarchical contrastive motion learning, a new self-supervised learning framework to extract effective motion representations from raw video frames. Our approach progressively learns a hierarchy of motion features that correspond to different abstraction levels in a network. This hierarchical design bridges the semantic gap between low-level motion cues and high-level recognition tasks, and promotes the fusion of appearance and motion information at multiple levels. At each level, an explicit motion self-supervision is provided via contrastive learning to enforce the motion features at the current level to predict the future ones at the previous level. Thus, the motion features at higher levels are trained to gradually capture semantic dynamics and evolve more discriminative for action recognition. Our motion learning module is lightweight and flexible to be embedded into various backbone networks. Extensive experiments on four benchmarks show that the proposed approach consistently achieves superior results.



### Pillar-based Object Detection for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2007.10323v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2007.10323v2)
- **Published**: 2020-07-20 17:59:28+00:00
- **Updated**: 2020-07-26 21:13:04+00:00
- **Authors**: Yue Wang, Alireza Fathi, Abhijit Kundu, David Ross, Caroline Pantofaru, Thomas Funkhouser, Justin Solomon
- **Comment**: Accepted to ECCV2020
- **Journal**: None
- **Summary**: We present a simple and flexible object detection framework optimized for autonomous driving. Building on the observation that point clouds in this application are extremely sparse, we propose a practical pillar-based approach to fix the imbalance issue caused by anchors. In particular, our algorithm incorporates a cylindrical projection into multi-view feature learning, predicts bounding box parameters per pillar rather than per point or per anchor, and includes an aligned pillar-to-point projection module to improve the final prediction. Our anchor-free approach avoids hyperparameter search associated with past methods, simplifying 3D object detection while significantly improving upon state-of-the-art.



### Privacy Preserving Visual SLAM
- **Arxiv ID**: http://arxiv.org/abs/2007.10361v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.10361v2)
- **Published**: 2020-07-20 18:00:06+00:00
- **Updated**: 2020-07-27 07:34:46+00:00
- **Authors**: Mikiya Shibuya, Shinya Sumikura, Ken Sakurada
- **Comment**: ECCV2020, Project: https://xdspacelab.github.io/lcvslam/ , Video:
  https://youtu.be/gEtUqnHx83w
- **Journal**: None
- **Summary**: This study proposes a privacy-preserving Visual SLAM framework for estimating camera poses and performing bundle adjustment with mixed line and point clouds in real time. Previous studies have proposed localization methods to estimate a camera pose using a line-cloud map for a single image or a reconstructed point cloud. These methods offer a scene privacy protection against the inversion attacks by converting a point cloud to a line cloud, which reconstruct the scene images from the point cloud. However, they are not directly applicable to a video sequence because they do not address computational efficiency. This is a critical issue to solve for estimating camera poses and performing bundle adjustment with mixed line and point clouds in real time. Moreover, there has been no study on a method to optimize a line-cloud map of a server with a point cloud reconstructed from a client video because any observation points on the image coordinates are not available to prevent the inversion attacks, namely the reversibility of the 3D lines. The experimental results with synthetic and real data show that our Visual SLAM framework achieves the intended privacy-preserving formation and real-time performance using a line-cloud map.



### Generative Hierarchical Features from Synthesizing Images
- **Arxiv ID**: http://arxiv.org/abs/2007.10379v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.10379v2)
- **Published**: 2020-07-20 18:04:14+00:00
- **Updated**: 2021-04-03 13:21:08+00:00
- **Authors**: Yinghao Xu, Yujun Shen, Jiapeng Zhu, Ceyuan Yang, Bolei Zhou
- **Comment**: CVPR 2021 camera-ready
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) have recently advanced image synthesis by learning the underlying distribution of the observed data. However, how the features learned from solving the task of image generation are applicable to other vision tasks remains seldom explored. In this work, we show that learning to synthesize images can bring remarkable hierarchical visual features that are generalizable across a wide range of applications. Specifically, we consider the pre-trained StyleGAN generator as a learned loss function and utilize its layer-wise representation to train a novel hierarchical encoder. The visual feature produced by our encoder, termed as Generative Hierarchical Feature (GH-Feat), has strong transferability to both generative and discriminative tasks, including image editing, image harmonization, image classification, face verification, landmark detection, and layout prediction. Extensive qualitative and quantitative experimental results demonstrate the appealing performance of GH-Feat.



### NSGANetV2: Evolutionary Multi-Objective Surrogate-Assisted Neural Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/2007.10396v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2007.10396v1)
- **Published**: 2020-07-20 18:30:11+00:00
- **Updated**: 2020-07-20 18:30:11+00:00
- **Authors**: Zhichao Lu, Kalyanmoy Deb, Erik Goodman, Wolfgang Banzhaf, Vishnu Naresh Boddeti
- **Comment**: Accepted for oral presentation at ECCV 2020
- **Journal**: None
- **Summary**: In this paper, we propose an efficient NAS algorithm for generating task-specific models that are competitive under multiple competing objectives. It comprises of two surrogates, one at the architecture level to improve sample efficiency and one at the weights level, through a supernet, to improve gradient descent training efficiency. On standard benchmark datasets (C10, C100, ImageNet), the resulting models, dubbed NSGANetV2, either match or outperform models from existing approaches with the search being orders of magnitude more sample efficient. Furthermore, we demonstrate the effectiveness and versatility of the proposed method on six diverse non-standard datasets, e.g. STL-10, Flowers102, Oxford Pets, FGVC Aircrafts etc. In all cases, NSGANetV2s improve the state-of-the-art (under mobile setting), suggesting that NAS can be a viable alternative to conventional transfer learning approaches in handling diverse scenarios such as small-scale or fine-grained datasets. Code is available at https://github.com/mikelzc1990/nsganetv2



### PDO-eConvs: Partial Differential Operator Based Equivariant Convolutions
- **Arxiv ID**: http://arxiv.org/abs/2007.10408v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.10408v2)
- **Published**: 2020-07-20 18:57:26+00:00
- **Updated**: 2020-08-11 14:19:55+00:00
- **Authors**: Zhengyang Shen, Lingshen He, Zhouchen Lin, Jinwen Ma
- **Comment**: Accepted by ICML2020
- **Journal**: None
- **Summary**: Recent research has shown that incorporating equivariance into neural network architectures is very helpful, and there have been some works investigating the equivariance of networks under group actions. However, as digital images and feature maps are on the discrete meshgrid, corresponding equivariance-preserving transformation groups are very limited. In this work, we deal with this issue from the connection between convolutions and partial differential operators (PDOs). In theory, assuming inputs to be smooth, we transform PDOs and propose a system which is equivariant to a much more general continuous group, the $n$-dimension Euclidean group. In implementation, we discretize the system using the numerical schemes of PDOs, deriving approximately equivariant convolutions (PDO-eConvs). Theoretically, the approximation error of PDO-eConvs is of the quadratic order. It is the first time that the error analysis is provided when the equivariance is approximate. Extensive experiments on rotated MNIST and natural image classification show that PDO-eConvs perform competitively yet use parameters much more efficiently. Particularly, compared with Wide ResNets, our methods result in better results using only 12.6% parameters.



### Integrative Analysis for COVID-19 Patient Outcome Prediction
- **Arxiv ID**: http://arxiv.org/abs/2007.10416v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.10416v2)
- **Published**: 2020-07-20 19:08:50+00:00
- **Updated**: 2020-09-16 19:44:07+00:00
- **Authors**: Hanqing Chao, Xi Fang, Jiajin Zhang, Fatemeh Homayounieh, Chiara D. Arru, Subba R. Digumarthy, Rosa Babaei, Hadi K. Mobin, Iman Mohseni, Luca Saba, Alessandro Carriero, Zeno Falaschi, Alessio Pasche, Ge Wang, Mannudeep K. Kalra, Pingkun Yan
- **Comment**: This paper has been accepted by Medical Image Analysis. The source
  code of this work is available at
  https://github.com/DIAL-RPI/COVID19-ICUPrediction
- **Journal**: None
- **Summary**: While image analysis of chest computed tomography (CT) for COVID-19 diagnosis has been intensively studied, little work has been performed for image-based patient outcome prediction. Management of high-risk patients with early intervention is a key to lower the fatality rate of COVID-19 pneumonia, as a majority of patients recover naturally. Therefore, an accurate prediction of disease progression with baseline imaging at the time of the initial presentation can help in patient management. In lieu of only size and volume information of pulmonary abnormalities and features through deep learning based image segmentation, here we combine radiomics of lung opacities and non-imaging features from demographic data, vital signs, and laboratory findings to predict need for intensive care unit (ICU) admission. To our knowledge, this is the first study that uses holistic information of a patient including both imaging and non-imaging data for outcome prediction. The proposed methods were thoroughly evaluated on datasets separately collected from three hospitals, one in the United States, one in Iran, and another in Italy, with a total 295 patients with reverse transcription polymerase chain reaction (RT-PCR) assay positive COVID-19 pneumonia. Our experimental results demonstrate that adding non-imaging features can significantly improve the performance of prediction to achieve AUC up to 0.884 and sensitivity as high as 96.1%, which can be valuable to provide clinical decision support in managing COVID-19 patients. Our methods may also be applied to other lung diseases including but not limited to community acquired pneumonia. The source code of our work is available at https://github.com/DIAL-RPI/COVID19-ICUPrediction.



### Points2Surf: Learning Implicit Surfaces from Point Cloud Patches
- **Arxiv ID**: http://arxiv.org/abs/2007.10453v1
- **DOI**: 10.1007/978-3-030-58558-7_7
- **Categories**: **cs.CV**, I.4.5
- **Links**: [PDF](http://arxiv.org/pdf/2007.10453v1)
- **Published**: 2020-07-20 20:25:39+00:00
- **Updated**: 2020-07-20 20:25:39+00:00
- **Authors**: Philipp Erler, Paul Guerrero, Stefan Ohrhallinger, Michael Wimmer, Niloy J. Mitra
- **Comment**: To be published at ECCV 2020 Repository:
  https://github.com/ErlerPhilipp/points2surf
- **Journal**: Computer Vision -- ECCV 2020, 108--124
- **Summary**: A key step in any scanning-based asset creation workflow is to convert unordered point clouds to a surface. Classical methods (e.g., Poisson reconstruction) start to degrade in the presence of noisy and partial scans. Hence, deep learning based methods have recently been proposed to produce complete surfaces, even from partial scans. However, such data-driven methods struggle to generalize to new shapes with large geometric and topological variations. We present Points2Surf, a novel patch-based learning framework that produces accurate surfaces directly from raw scans without normals. Learning a prior over a combination of detailed local patches and coarse global information improves generalization performance and reconstruction accuracy. Our extensive comparison on both synthetic and real data demonstrates a clear advantage of our method over state-of-the-art alternatives on previously unseen classes (on average, Points2Surf brings down reconstruction error by 30\% over SPR and by 270\%+ over deep learning based SotA methods) at the cost of longer computation times and a slight increase in small-scale topological noise in some cases. Our source code, pre-trained model, and dataset are available on: https://github.com/ErlerPhilipp/points2surf



### Detection, Attribution and Localization of GAN Generated Images
- **Arxiv ID**: http://arxiv.org/abs/2007.10466v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.10466v1)
- **Published**: 2020-07-20 20:49:34+00:00
- **Updated**: 2020-07-20 20:49:34+00:00
- **Authors**: Michael Goebel, Lakshmanan Nataraj, Tejaswi Nanjundaswamy, Tajuddin Manhar Mohammed, Shivkumar Chandrasekaran, B. S. Manjunath
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advances in Generative Adversarial Networks (GANs) have led to the creation of realistic-looking digital images that pose a major challenge to their detection by humans or computers. GANs are used in a wide range of tasks, from modifying small attributes of an image (StarGAN [14]), transferring attributes between image pairs (CycleGAN [91]), as well as generating entirely new images (ProGAN [36], StyleGAN [37], SPADE/GauGAN [64]). In this paper, we propose a novel approach to detect, attribute and localize GAN generated images that combines image features with deep learning methods. For every image, co-occurrence matrices are computed on neighborhood pixels of RGB channels in different directions (horizontal, vertical and diagonal). A deep learning network is then trained on these features to detect, attribute and localize these GAN generated/manipulated images. A large scale evaluation of our approach on 5 GAN datasets comprising over 2.76 million images (ProGAN, StarGAN, CycleGAN, StyleGAN and SPADE/GauGAN) shows promising results in detecting GAN generated images.



### Second-Order Pooling for Graph Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2007.10467v1
- **DOI**: 10.1109/TPAMI.2020.2999032
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.10467v1)
- **Published**: 2020-07-20 20:52:36+00:00
- **Updated**: 2020-07-20 20:52:36+00:00
- **Authors**: Zhengyang Wang, Shuiwang Ji
- **Comment**: 12 pages, 2 figures,
  https://www.computer.org/csdl/journal/tp/5555/01/09104936/1kj0O2A1yBa
- **Journal**: IEEE Transactions on Pattern Analysis and Machine Intelligence,
  2020
- **Summary**: Graph neural networks have achieved great success in learning node representations for graph tasks such as node classification and link prediction. Graph representation learning requires graph pooling to obtain graph representations from node representations. It is challenging to develop graph pooling methods due to the variable sizes and isomorphic structures of graphs. In this work, we propose to use second-order pooling as graph pooling, which naturally solves the above challenges. In addition, compared to existing graph pooling methods, second-order pooling is able to use information from all nodes and collect second-order statistics, making it more powerful. We show that direct use of second-order pooling with graph neural networks leads to practical problems. To overcome these problems, we propose two novel global graph pooling methods based on second-order pooling; namely, bilinear mapping and attentional second-order pooling. In addition, we extend attentional second-order pooling to hierarchical graph pooling for more flexible use in GNNs. We perform thorough experiments on graph classification tasks to demonstrate the effectiveness and superiority of our proposed methods. Experimental results show that our methods improve the performance significantly and consistently.



### Active MR k-space Sampling with Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/2007.10469v2
- **DOI**: 10.1007/978-3-030-59713-9_3
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.10469v2)
- **Published**: 2020-07-20 20:54:57+00:00
- **Updated**: 2020-10-07 21:54:14+00:00
- **Authors**: Luis Pineda, Sumana Basu, Adriana Romero, Roberto Calandra, Michal Drozdzal
- **Comment**: Presented at the 23rd International Conference on Medical Image
  Computing and Computer Assisted Intervention, MICCAI 2020
- **Journal**: LNCS vol. 12262 (2020) 23-33
- **Summary**: Deep learning approaches have recently shown great promise in accelerating magnetic resonance image (MRI) acquisition. The majority of existing work have focused on designing better reconstruction models given a pre-determined acquisition trajectory, ignoring the question of trajectory optimization. In this paper, we focus on learning acquisition trajectories given a fixed image reconstruction model. We formulate the problem as a sequential decision process and propose the use of reinforcement learning to solve it. Experiments on a large scale public MRI dataset of knees show that our proposed models significantly outperform the state-of-the-art in active MRI acquisition, over a large range of acceleration factors.



### AdvFoolGen: Creating Persistent Troubles for Deep Classifiers
- **Arxiv ID**: http://arxiv.org/abs/2007.10485v1
- **DOI**: 10.1109/ICCVW54120.2021.00021
- **Categories**: **cs.CV**, I.4.9; I.5.1
- **Links**: [PDF](http://arxiv.org/pdf/2007.10485v1)
- **Published**: 2020-07-20 21:27:41+00:00
- **Updated**: 2020-07-20 21:27:41+00:00
- **Authors**: Yuzhen Ding, Nupur Thakur, Baoxin Li
- **Comment**: 11 pages, 5 figures
- **Journal**: None
- **Summary**: Researches have shown that deep neural networks are vulnerable to malicious attacks, where adversarial images are created to trick a network into misclassification even if the images may give rise to totally different labels by human eyes. To make deep networks more robust to such attacks, many defense mechanisms have been proposed in the literature, some of which are quite effective for guarding against typical attacks. In this paper, we present a new black-box attack termed AdvFoolGen, which can generate attacking images from the same feature space as that of the natural images, so as to keep baffling the network even though state-of-the-art defense mechanisms have been applied. We systematically evaluate our model by comparing with well-established attack algorithms. Through experiments, we demonstrate the effectiveness and robustness of our attack in the face of state-of-the-art defense techniques and unveil the potential reasons for its effectiveness through principled analysis. As such, AdvFoolGen contributes to understanding the vulnerability of deep networks from a new perspective and may, in turn, help in developing and evaluating new defense mechanisms.



### Sorted Pooling in Convolutional Networks for One-shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2007.10495v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.10495v1)
- **Published**: 2020-07-20 21:45:37+00:00
- **Updated**: 2020-07-20 21:45:37+00:00
- **Authors**: András Horváth
- **Comment**: Old paper submitted to ECCV 2018
- **Journal**: None
- **Summary**: We present generalized versions of the commonly used maximum pooling operation: $k$th maximum and sorted pooling operations which selects the $k$th largest response in each pooling region, selecting locally consistent features of the input images. This method is able to increase the generalization power of a network and can be used to decrease training time and error rate of networks and it can significantly improve accuracy in case of training scenarios where the amount of available data is limited, like one-shot learning scenarios



### The Effects of Approximate Multiplication on Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2007.10500v2
- **DOI**: 10.1109/TETC.2021.3050989
- **Categories**: **cs.LG**, cs.CV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2007.10500v2)
- **Published**: 2020-07-20 21:52:41+00:00
- **Updated**: 2021-01-09 17:06:41+00:00
- **Authors**: Min Soo Kim, Alberto A. Del Barrio, HyunJin Kim, Nader Bagherzadeh
- **Comment**: 12 pages, 11 figures, 4 tables, accepted for publication in the IEEE
  Transactions on Emerging Topics in Computing
- **Journal**: None
- **Summary**: This paper analyzes the effects of approximate multiplication when performing inferences on deep convolutional neural networks (CNNs). The approximate multiplication can reduce the cost of the underlying circuits so that CNN inferences can be performed more efficiently in hardware accelerators. The study identifies the critical factors in the convolution, fully-connected, and batch normalization layers that allow more accurate CNN predictions despite the errors from approximate multiplication. The same factors also provide an arithmetic explanation of why bfloat16 multiplication performs well on CNNs. The experiments are performed with recognized network architectures to show that the approximate multipliers can produce predictions that are nearly as accurate as the FP32 references, without additional training. For example, the ResNet and Inception-v4 models with Mitch-$w$6 multiplication produces Top-5 errors that are within 0.2% compared to the FP32 references. A brief cost comparison of Mitch-$w$6 against bfloat16 is presented, where a MAC operation saves up to 80% of energy compared to the bfloat16 arithmetic. The most far-reaching contribution of this paper is the analytical justification that multiplications can be approximated while additions need to be exact in CNN MAC operations.



### DeepCorn: A Semi-Supervised Deep Learning Method for High-Throughput Image-Based Corn Kernel Counting and Yield Estimation
- **Arxiv ID**: http://arxiv.org/abs/2007.10521v2
- **DOI**: 10.1016/j.knosys.2021.106874
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.10521v2)
- **Published**: 2020-07-20 23:00:39+00:00
- **Updated**: 2021-02-25 01:37:18+00:00
- **Authors**: Saeed Khaki, Hieu Pham, Ye Han, Andy Kuhl, Wade Kent, Lizhi Wang
- **Comment**: 27 pages, 7 figures
- **Journal**: Knowledge-Based Systems (2021): 106874
- **Summary**: The success of modern farming and plant breeding relies on accurate and efficient collection of data. For a commercial organization that manages large amounts of crops, collecting accurate and consistent data is a bottleneck. Due to limited time and labor, accurately phenotyping crops to record color, head count, height, weight, etc. is severely limited. However, this information, combined with other genetic and environmental factors, is vital for developing new superior crop species that help feed the world's growing population. Recent advances in machine learning, in particular deep learning, have shown promise in mitigating this bottleneck. In this paper, we propose a novel deep learning method for counting on-ear corn kernels in-field to aid in the gathering of real-time data and, ultimately, to improve decision making to maximize yield. We name this approach DeepCorn, and show that this framework is robust under various conditions. DeepCorn estimates the density of corn kernels in an image of corn ears and predicts the number of kernels based on the estimated density map. DeepCorn uses a truncated VGG-16 as a backbone for feature extraction and merges feature maps from multiple scales of the network to make it robust against image scale variations. We also adopt a semi-supervised learning approach to further improve the performance of our proposed method. Our proposed method achieves the MAE and RMSE of 41.36 and 60.27 in the corn kernel counting task, respectively. Our experimental results demonstrate the superiority and effectiveness of our proposed method compared to other state-of-the-art methods.



