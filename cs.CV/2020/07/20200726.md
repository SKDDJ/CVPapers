# Arxiv Papers in cs.CV on 2020-07-26
### Mask2CAD: 3D Shape Prediction by Learning to Segment and Retrieve
- **Arxiv ID**: http://arxiv.org/abs/2007.13034v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.13034v1)
- **Published**: 2020-07-26 00:08:37+00:00
- **Updated**: 2020-07-26 00:08:37+00:00
- **Authors**: Weicheng Kuo, Anelia Angelova, Tsung-Yi Lin, Angela Dai
- **Comment**: ECCV 2020 (Spotlight)
- **Journal**: None
- **Summary**: Object recognition has seen significant progress in the image domain, with focus primarily on 2D perception. We propose to leverage existing large-scale datasets of 3D models to understand the underlying 3D structure of objects seen in an image by constructing a CAD-based representation of the objects and their poses. We present Mask2CAD, which jointly detects objects in real-world images and for each detected object, optimizes for the most similar CAD model and its pose. We construct a joint embedding space between the detected regions of an image corresponding to an object and 3D CAD models, enabling retrieval of CAD models for an input RGB image. This produces a clean, lightweight representation of the objects in an image; this CAD-based representation ensures a valid, efficient shape representation for applications such as content creation or interactive scenarios, and makes a step towards understanding the transformation of real-world imagery to a synthetic domain. Experiments on real-world images from Pix3D demonstrate the advantage of our approach in comparison to state of the art. To facilitate future research, we additionally propose a new image-to-3D baseline on ScanNet which features larger shape diversity, real-world occlusions, and challenging image views.



### A Preliminary Exploration into an Alternative CellLineNet: An Evolutionary Approach
- **Arxiv ID**: http://arxiv.org/abs/2007.13044v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.13044v1)
- **Published**: 2020-07-26 02:36:56+00:00
- **Updated**: 2020-07-26 02:36:56+00:00
- **Authors**: Akwarandu Ugo Nwachuku, Xavier Lewis-Palmer, Darlington Ahiale Akogo
- **Comment**: None
- **Journal**: None
- **Summary**: Within this paper, the exploration of an evolutionary approach to an alternative CellLineNet: a convolutional neural network adept at the classification of epithelial breast cancer cell lines, is presented. This evolutionary algorithm introduces control variables that guide the search of architectures in the search space of inverted residual blocks, bottleneck blocks, residual blocks and a basic 2x2 convolutional block. The promise of EvoCELL is predicting what combination or arrangement of the feature extracting blocks that produce the best model architecture for a given task. Therein, the performance of how the fittest model evolved after each generation is shown. The final evolved model CellLineNet V2 classifies 5 types of epithelial breast cell lines consisting of two human cancer lines, 2 normal immortalized lines, and 1 immortalized mouse line (MDA-MB-468, MCF7, 10A, 12A and HC11). The Multiclass Cell Line Classification Convolutional Neural Network extends our earlier work on a Binary Breast Cancer Cell Line Classification model. This paper presents an on-going exploratory approach to neural network architecture design and is presented for further study.



### A Dual Iterative Refinement Method for Non-rigid Shape Matching
- **Arxiv ID**: http://arxiv.org/abs/2007.13049v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.13049v2)
- **Published**: 2020-07-26 03:35:37+00:00
- **Updated**: 2020-11-19 07:49:03+00:00
- **Authors**: Rui Xiang, Rongjie Lai, Hongkai Zhao
- **Comment**: 9 pages, 9 figures and 1 table
- **Journal**: None
- **Summary**: In this work, a simple and efficient dual iterative refinement (DIR) method is proposed for dense correspondence between two nearly isometric shapes. The key idea is to use dual information, such as spatial and spectral, or local and global features, in a complementary and effective way, and extract more accurate information from current iteration to use for the next iteration. In each DIR iteration, starting from current correspondence, a zoom-in process at each point is used to select well matched anchor pairs by a local mapping distortion criterion. These selected anchor pairs are then used to align spectral features (or other appropriate global features) whose dimension adaptively matches the capacity of the selected anchor pairs. Thanks to the effective combination of complementary information in a data-adaptive way, DIR is not only efficient but also robust to render accurate results within a few iterations. By choosing appropriate dual features, DIR has the flexibility to handle patch and partial matching as well. Extensive experiments on various data sets demonstrate the superiority of DIR over other state-of-the-art methods in terms of both accuracy and efficiency.



### Approaches of large-scale images recognition with more than 50,000 categoris
- **Arxiv ID**: http://arxiv.org/abs/2007.13072v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.13072v1)
- **Published**: 2020-07-26 07:33:22+00:00
- **Updated**: 2020-07-26 07:33:22+00:00
- **Authors**: Wanhong Huang, Rui Geng
- **Comment**: None
- **Journal**: None
- **Summary**: Though current CV models have been able to achieve high levels of accuracy on small-scale images classification dataset with hundreds or thousands of categories, many models become infeasible in computational or space consumption when it comes to large-scale dataset with more than 50,000 categories. In this paper, we provide a viable solution for classifying large-scale species datasets using traditional CV techniques such as.features extraction and processing, BOVW(Bag of Visual Words) and some statistical learning technics like Mini-Batch K-Means,SVM which are used in our works. And then mixed with a neural network model. When applying these techniques, we have done some optimization in time and memory consumption, so that it can be feasible for large-scale dataset. And we also use some technics to reduce the impact of mislabeling data. We use a dataset with more than 50, 000 categories, and all operations are done on common computer with l 6GB RAM and a CPU of 3. OGHz. Our contributions are: 1) analysis what problems may meet in the training processes, and presents several feasible ways to solve these problems. 2) Make traditional CV models combined with neural network models provide some feasible scenarios for training large-scale classified datasets within the constraints of time and spatial resources.



### SMART: Simultaneous Multi-Agent Recurrent Trajectory Prediction
- **Arxiv ID**: http://arxiv.org/abs/2007.13078v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.13078v1)
- **Published**: 2020-07-26 08:17:10+00:00
- **Updated**: 2020-07-26 08:17:10+00:00
- **Authors**: Sriram N N, Buyu Liu, Francesco Pittaluga, Manmohan Chandraker
- **Comment**: Accepted at ECCV 2020
- **Journal**: None
- **Summary**: We propose advances that address two key challenges in future trajectory prediction: (i) multimodality in both training data and predictions and (ii) constant time inference regardless of number of agents. Existing trajectory predictions are fundamentally limited by lack of diversity in training data, which is difficult to acquire with sufficient coverage of possible modes. Our first contribution is an automatic method to simulate diverse trajectories in the top-view. It uses pre-existing datasets and maps as initialization, mines existing trajectories to represent realistic driving behaviors and uses a multi-agent vehicle dynamics simulator to generate diverse new trajectories that cover various modes and are consistent with scene layout constraints. Our second contribution is a novel method that generates diverse predictions while accounting for scene semantics and multi-agent interactions, with constant-time inference independent of the number of agents. We propose a convLSTM with novel state pooling operations and losses to predict scene-consistent states of multiple agents in a single forward pass, along with a CVAE for diversity. We validate our proposed multi-agent trajectory prediction approach by training and testing on the proposed simulated dataset and existing real datasets of traffic scenes. In both cases, our approach outperforms SOTA methods by a large margin, highlighting the benefits of both our diverse dataset simulation and constant-time diverse trajectory prediction methods.



### MACU-Net for Semantic Segmentation of Fine-Resolution Remotely Sensed Images
- **Arxiv ID**: http://arxiv.org/abs/2007.13083v3
- **DOI**: 10.1109/LGRS.2021.3052886
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.13083v3)
- **Published**: 2020-07-26 08:56:47+00:00
- **Updated**: 2022-05-04 18:44:53+00:00
- **Authors**: Rui Li, Chenxi Duan, Shunyi Zheng, Ce Zhang, Peter M. Atkinson
- **Comment**: None
- **Journal**: None
- **Summary**: Semantic segmentation of remotely sensed images plays an important role in land resource management, yield estimation, and economic assessment. U-Net, a deep encoder-decoder architecture, has been used frequently for image segmentation with high accuracy. In this Letter, we incorporate multi-scale features generated by different layers of U-Net and design a multi-scale skip connected and asymmetric-convolution-based U-Net (MACU-Net), for segmentation using fine-resolution remotely sensed images. Our design has the following advantages: (1) The multi-scale skip connections combine and realign semantic features contained in both low-level and high-level feature maps; (2) the asymmetric convolution block strengthens the feature representation and feature extraction capability of a standard convolution layer. Experiments conducted on two remotely sensed datasets captured by different satellite sensors demonstrate that the proposed MACU-Net transcends the U-Net, U-NetPPL, U-Net 3+, amongst other benchmark approaches. Code is available at https://github.com/lironui/MACU-Net.



### U2-ONet: A Two-level Nested Octave U-structure with Multiscale Attention Mechanism for Moving Instances Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2007.13092v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.13092v1)
- **Published**: 2020-07-26 10:12:42+00:00
- **Updated**: 2020-07-26 10:12:42+00:00
- **Authors**: Chenjie Wang, Chengyuan Li, Bin Luo
- **Comment**: 10 pages, 7 figures,
- **Journal**: None
- **Summary**: Most scenes in practical applications are dynamic scenes containing moving objects, so segmenting accurately moving objects is crucial for many computer vision applications. In order to efficiently segment out all moving objects in the scene, regardless of whether the object has a predefined semantic label, we propose a two-level nested Octave U-structure network with a multiscale attention mechanism called U2-ONet. Each stage of U2-ONet is filled with our newly designed Octave ReSidual U-block (ORSU) to enhance the ability to obtain more context information at different scales while reducing spatial redundancy of feature maps. In order to efficiently train our multi-scale deep network, we introduce a hierarchical training supervision strategy that calculates the loss at each level while adding a knowledge matching loss to keep the optimization consistency. Experimental results show that our method achieves state-of-the-art performance in several general moving objects segmentation datasets.



### Towards Purely Unsupervised Disentanglement of Appearance and Shape for Person Images Generation
- **Arxiv ID**: http://arxiv.org/abs/2007.13098v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.13098v2)
- **Published**: 2020-07-26 10:56:37+00:00
- **Updated**: 2020-07-30 00:49:59+00:00
- **Authors**: Hongtao Yang, Tong Zhang, Wenbing Huang, Xuming He, Fatih Porikli
- **Comment**: None
- **Journal**: None
- **Summary**: There have been a fairly of research interests in exploring the disentanglement of appearance and shape from human images. Most existing endeavours pursuit this goal by either using training images with annotations or regulating the training process with external clues such as human skeleton, body segmentation or cloth patches etc. In this paper, we aim to address this challenge in a more unsupervised manner---we do not require any annotation nor any external task-specific clues. To this end, we formulate an encoder-decoder-like network to extract both the shape and appearance features from input images at the same time, and train the parameters by three losses: feature adversarial loss, color consistency loss and reconstruction loss. The feature adversarial loss mainly impose little to none mutual information between the extracted shape and appearance features, while the color consistency loss is to encourage the invariance of person appearance conditioned on different shapes. More importantly, our unsupervised (Unsupervised learning has many interpretations in different tasks. To be clear, in this paper, we refer unsupervised learning as learning without task-specific human annotations, pairs or any form of weak supervision.) framework utilizes learned shape features as masks which are applied to the input itself in order to obtain clean appearance features. Without using fixed input human skeleton, our network better preserves the conditional human posture while requiring less supervision. Experimental results on DeepFashion and Market1501 demonstrate that the proposed method achieves clean disentanglement and is able to synthesis novel images of comparable quality with state-of-the-art weakly-supervised or even supervised methods.



### Regularized Flexible Activation Function Combinations for Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2007.13101v2
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.13101v2)
- **Published**: 2020-07-26 11:32:52+00:00
- **Updated**: 2020-08-19 13:45:49+00:00
- **Authors**: Renlong Jie, Junbin Gao, Andrey Vasnev, Min-ngoc Tran
- **Comment**: None
- **Journal**: None
- **Summary**: Activation in deep neural networks is fundamental to achieving non-linear mappings. Traditional studies mainly focus on finding fixed activations for a particular set of learning tasks or model architectures. The research on flexible activation is quite limited in both designing philosophy and application scenarios. In this study, three principles of choosing flexible activation components are proposed and a general combined form of flexible activation functions is implemented. Based on this, a novel family of flexible activation functions that can replace sigmoid or tanh in LSTM cells are implemented, as well as a new family by combining ReLU and ELUs. Also, two new regularisation terms based on assumptions as prior knowledge are introduced. It has been shown that LSTM models with proposed flexible activations P-Sig-Ramp provide significant improvements in time series forecasting, while the proposed P-E2-ReLU achieves better and more stable performance on lossy image compression tasks with convolutional auto-encoders. In addition, the proposed regularization terms improve the convergence, performance and stability of the models with flexible activation functions.



### Detection and Annotation of Plant Organs from Digitized Herbarium Scans using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2007.13106v2
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.QM, I.2.10; J.3
- **Links**: [PDF](http://arxiv.org/pdf/2007.13106v2)
- **Published**: 2020-07-26 11:50:50+00:00
- **Updated**: 2020-07-31 14:02:44+00:00
- **Authors**: Sohaib Younis, Marco Schmidt, Claus Weiland, Stefan Dressler, Bernhard Seeger, Thomas Hickler
- **Comment**: Replaced version due to change in location of supplement dataset. For
  supplementary dataset go to: https://doi.pangaea.de/10.1594/PANGAEA.920895
  Article submitted to Biodiversity Data Journal
- **Journal**: None
- **Summary**: As herbarium specimens are increasingly becoming digitized and accessible in online repositories, advanced computer vision techniques are being used to extract information from them. The presence of certain plant organs on herbarium sheets is useful information in various scientific contexts and automatic recognition of these organs will help mobilize such information. In our study we use deep learning to detect plant organs on digitized herbarium specimens with Faster R-CNN. For our experiment we manually annotated hundreds of herbarium scans with thousands of bounding boxes for six types of plant organs and used them for training and evaluating the plant organ detection model. The model worked particularly well on leaves and stems, while flowers were also present in large numbers in the sheets, but not equally well recognized.



### UIAI System for Short-Duration Speaker Verification Challenge 2020
- **Arxiv ID**: http://arxiv.org/abs/2007.13118v1
- **DOI**: None
- **Categories**: **eess.AS**, cs.CV, cs.SD
- **Links**: [PDF](http://arxiv.org/pdf/2007.13118v1)
- **Published**: 2020-07-26 12:32:34+00:00
- **Updated**: 2020-07-26 12:32:34+00:00
- **Authors**: Md Sahidullah, Achintya Kumar Sarkar, Ville Vestman, Xuechen Liu, Romain Serizel, Tomi Kinnunen, Zheng-Hua Tan, Emmanuel Vincent
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we present the system description of the UIAI entry for the short-duration speaker verification (SdSV) challenge 2020. Our focus is on Task 1 dedicated to text-dependent speaker verification. We investigate different feature extraction and modeling approaches for automatic speaker verification (ASV) and utterance verification (UV). We have also studied different fusion strategies for combining UV and ASV modules. Our primary submission to the challenge is the fusion of seven subsystems which yields a normalized minimum detection cost function (minDCF) of 0.072 and an equal error rate (EER) of 2.14% on the evaluation set. The single system consisting of a pass-phrase identification based model with phone-discriminative bottleneck features gives a normalized minDCF of 0.118 and achieves 19% relative improvement over the state-of-the-art challenge baseline.



### SADet: Learning An Efficient and Accurate Pedestrian Detector
- **Arxiv ID**: http://arxiv.org/abs/2007.13119v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.13119v1)
- **Published**: 2020-07-26 12:32:38+00:00
- **Updated**: 2020-07-26 12:32:38+00:00
- **Authors**: Chubin Zhuang, Zhen Lei, Stan Z. Li
- **Comment**: None
- **Journal**: None
- **Summary**: Although the anchor-based detectors have taken a big step forward in pedestrian detection, the overall performance of algorithm still needs further improvement for practical applications, \emph{e.g.}, a good trade-off between the accuracy and efficiency. To this end, this paper proposes a series of systematic optimization strategies for the detection pipeline of one-stage detector, forming a single shot anchor-based detector (SADet) for efficient and accurate pedestrian detection, which includes three main improvements. Firstly, we optimize the sample generation process by assigning soft tags to the outlier samples to generate semi-positive samples with continuous tag value between $0$ and $1$, which not only produces more valid samples, but also strengthens the robustness of the model. Secondly, a novel Center-$IoU$ loss is applied as a new regression loss for bounding box regression, which not only retains the good characteristics of IoU loss, but also solves some defects of it. Thirdly, we also design Cosine-NMS for the postprocess of predicted bounding boxes, and further propose adaptive anchor matching to enable the model to adaptively match the anchor boxes to full or visible bounding boxes according to the degree of occlusion, making the NMS and anchor matching algorithms more suitable for occluded pedestrian detection. Though structurally simple, it presents state-of-the-art result and real-time speed of $20$ FPS for VGA-resolution images ($640 \times 480$) on challenging pedestrian detection benchmarks, i.e., CityPersons, Caltech, and human detection benchmark CrowdHuman, leading to a new attractive pedestrian detector.



### Towards End-to-end Video-based Eye-Tracking
- **Arxiv ID**: http://arxiv.org/abs/2007.13120v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.13120v1)
- **Published**: 2020-07-26 12:39:15+00:00
- **Updated**: 2020-07-26 12:39:15+00:00
- **Authors**: Seonwook Park, Emre Aksan, Xucong Zhang, Otmar Hilliges
- **Comment**: Accepted at ECCV 2020
- **Journal**: None
- **Summary**: Estimating eye-gaze from images alone is a challenging task, in large parts due to un-observable person-specific factors. Achieving high accuracy typically requires labeled data from test users which may not be attainable in real applications. We observe that there exists a strong relationship between what users are looking at and the appearance of the user's eyes. In response to this understanding, we propose a novel dataset and accompanying method which aims to explicitly learn these semantic and temporal relationships. Our video dataset consists of time-synchronized screen recordings, user-facing camera views, and eye gaze data, which allows for new benchmarks in temporal gaze tracking as well as label-free refinement of gaze. Importantly, we demonstrate that the fusion of information from visual stimuli as well as eye images can lead towards achieving performance similar to literature-reported figures acquired through supervised personalization. Our final method yields significant performance improvements on our proposed EVE dataset, with up to a 28 percent improvement in Point-of-Gaze estimates (resulting in 2.49 degrees in angular error), paving the path towards high-accuracy screen-based eye tracking purely from webcam sensors. The dataset and reference source code are available at https://ait.ethz.ch/projects/2020/EVE



### GSNet: Joint Vehicle Pose and Shape Reconstruction with Geometrical and Scene-aware Supervision
- **Arxiv ID**: http://arxiv.org/abs/2007.13124v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.13124v1)
- **Published**: 2020-07-26 13:05:55+00:00
- **Updated**: 2020-07-26 13:05:55+00:00
- **Authors**: Lei Ke, Shichao Li, Yanan Sun, Yu-Wing Tai, Chi-Keung Tang
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: We present a novel end-to-end framework named as GSNet (Geometric and Scene-aware Network), which jointly estimates 6DoF poses and reconstructs detailed 3D car shapes from single urban street view. GSNet utilizes a unique four-way feature extraction and fusion scheme and directly regresses 6DoF poses and shapes in a single forward pass. Extensive experiments show that our diverse feature extraction and fusion scheme can greatly improve model performance. Based on a divide-and-conquer 3D shape representation strategy, GSNet reconstructs 3D vehicle shape with great detail (1352 vertices and 2700 faces). This dense mesh representation further leads us to consider geometrical consistency and scene context, and inspires a new multi-objective loss function to regularize network training, which in turn improves the accuracy of 6D pose estimation and validates the merit of jointly performing both tasks. We evaluate GSNet on the largest multi-task ApolloCar3D benchmark and achieve state-of-the-art performance both quantitatively and qualitatively. Project page is available at https://lkeab.github.io/gsnet/.



### Contrastive Visual-Linguistic Pretraining
- **Arxiv ID**: http://arxiv.org/abs/2007.13135v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.13135v1)
- **Published**: 2020-07-26 14:26:18+00:00
- **Updated**: 2020-07-26 14:26:18+00:00
- **Authors**: Lei Shi, Kai Shuang, Shijie Geng, Peng Su, Zhengkai Jiang, Peng Gao, Zuohui Fu, Gerard de Melo, Sen Su
- **Comment**: None
- **Journal**: None
- **Summary**: Several multi-modality representation learning approaches such as LXMERT and ViLBERT have been proposed recently. Such approaches can achieve superior performance due to the high-level semantic information captured during large-scale multimodal pretraining. However, as ViLBERT and LXMERT adopt visual region regression and classification loss, they often suffer from domain gap and noisy label problems, based on the visual features having been pretrained on the Visual Genome dataset. To overcome these issues, we propose unbiased Contrastive Visual-Linguistic Pretraining (CVLP), which constructs a visual self-supervised loss built upon contrastive learning. We evaluate CVLP on several down-stream tasks, including VQA, GQA and NLVR2 to validate the superiority of contrastive learning on multi-modality representation learning. Our code is available at: https://github.com/ArcherYunDong/CVLP-.



### Virtual Multi-view Fusion for 3D Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2007.13138v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.13138v1)
- **Published**: 2020-07-26 14:46:55+00:00
- **Updated**: 2020-07-26 14:46:55+00:00
- **Authors**: Abhijit Kundu, Xiaoqi Yin, Alireza Fathi, David Ross, Brian Brewington, Thomas Funkhouser, Caroline Pantofaru
- **Comment**: To appear in ECCV 2020
- **Journal**: None
- **Summary**: Semantic segmentation of 3D meshes is an important problem for 3D scene understanding. In this paper we revisit the classic multiview representation of 3D meshes and study several techniques that make them effective for 3D semantic segmentation of meshes. Given a 3D mesh reconstructed from RGBD sensors, our method effectively chooses different virtual views of the 3D mesh and renders multiple 2D channels for training an effective 2D semantic segmentation model. Features from multiple per view predictions are finally fused on 3D mesh vertices to predict mesh semantic segmentation labels. Using the large scale indoor 3D semantic segmentation benchmark of ScanNet, we show that our virtual views enable more effective training of 2D semantic segmentation networks than previous multiview approaches. When the 2D per pixel predictions are aggregated on 3D surfaces, our virtual multiview fusion method is able to achieve significantly better 3D semantic segmentation results compared to all prior multiview approaches and competitive with recent 3D convolution approaches.



### Challenge-Aware RGBT Tracking
- **Arxiv ID**: http://arxiv.org/abs/2007.13143v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.13143v1)
- **Published**: 2020-07-26 15:11:44+00:00
- **Updated**: 2020-07-26 15:11:44+00:00
- **Authors**: Chenglong Li, Lei Liu, Andong Lu, Qing Ji, Jin Tang
- **Comment**: Accepted by ECCV 2020
- **Journal**: None
- **Summary**: RGB and thermal source data suffer from both shared and specific challenges, and how to explore and exploit them plays a critical role to represent the target appearance in RGBT tracking. In this paper, we propose a novel challenge-aware neural network to handle the modality-shared challenges (e.g., fast motion, scale variation and occlusion) and the modality-specific ones (e.g., illumination variation and thermal crossover) for RGBT tracking. In particular, we design several parameter-shared branches in each layer to model the target appearance under the modality-shared challenges, and several parameterindependent branches under the modality-specific ones. Based on the observation that the modality-specific cues of different modalities usually contains the complementary advantages, we propose a guidance module to transfer discriminative features from one modality to another one, which could enhance the discriminative ability of some weak modality. Moreover, all branches are aggregated together in an adaptive manner and parallel embedded in the backbone network to efficiently form more discriminative target representations. These challenge-aware branches are able to model the target appearance under certain challenges so that the target representations can be learnt by a few parameters even in the situation of insufficient training data. From the experimental results we will show that our method operates at a real-time speed while performing well against the state-of-the-art methods on three benchmark datasets.



### Deep Photometric Stereo for Non-Lambertian Surfaces
- **Arxiv ID**: http://arxiv.org/abs/2007.13145v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.13145v1)
- **Published**: 2020-07-26 15:20:53+00:00
- **Updated**: 2020-07-26 15:20:53+00:00
- **Authors**: Guanying Chen, Kai Han, Boxin Shi, Yasuyuki Matsushita, Kwan-Yee K. Wong
- **Comment**: None
- **Journal**: None
- **Summary**: This paper addresses the problem of photometric stereo, in both calibrated and uncalibrated scenarios, for non-Lambertian surfaces based on deep learning. We first introduce a fully convolutional deep network for calibrated photometric stereo, which we call PS-FCN. Unlike traditional approaches that adopt simplified reflectance models to make the problem tractable, our method directly learns the mapping from reflectance observations to surface normal, and is able to handle surfaces with general and unknown isotropic reflectance. At test time, PS-FCN takes an arbitrary number of images and their associated light directions as input and predicts a surface normal map of the scene in a fast feed-forward pass. To deal with the uncalibrated scenario where light directions are unknown, we introduce a new convolutional network, named LCNet, to estimate light directions from input images. The estimated light directions and the input images are then fed to PS-FCN to determine the surface normals. Our method does not require a pre-defined set of light directions and can handle multiple images in an order-agnostic manner. Thorough evaluation of our approach on both synthetic and real datasets shows that it outperforms state-of-the-art methods in both calibrated and uncalibrated scenarios.



### Learning and aggregating deep local descriptors for instance-level recognition
- **Arxiv ID**: http://arxiv.org/abs/2007.13172v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.13172v1)
- **Published**: 2020-07-26 16:30:56+00:00
- **Updated**: 2020-07-26 16:30:56+00:00
- **Authors**: Giorgos Tolias, Tomas Jenicek, Ondřej Chum
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: We propose an efficient method to learn deep local descriptors for instance-level recognition. The training only requires examples of positive and negative image pairs and is performed as metric learning of sum-pooled global image descriptors. At inference, the local descriptors are provided by the activations of internal components of the network. We demonstrate why such an approach learns local descriptors that work well for image similarity estimation with classical efficient match kernel methods. The experimental validation studies the trade-off between performance and memory requirements of the state-of-the-art image search approach based on match kernels. Compared to existing local descriptors, the proposed ones perform better in two instance-level recognition tasks and keep memory requirements lower. We experimentally show that global descriptors are not effective enough at large scale and that local descriptors are essential. We achieve state-of-the-art performance, in some cases even with a backbone network as small as ResNet18.



### An Uncertainty-aware Transfer Learning-based Framework for Covid-19 Diagnosis
- **Arxiv ID**: http://arxiv.org/abs/2007.14846v1
- **DOI**: 10.1109/TNNLS.2021.3054306
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.14846v1)
- **Published**: 2020-07-26 20:15:01+00:00
- **Updated**: 2020-07-26 20:15:01+00:00
- **Authors**: Afshar Shamsi Jokandan, Hamzeh Asgharnezhad, Shirin Shamsi Jokandan, Abbas Khosravi, Parham M. Kebria, Darius Nahavandi, Saeid Nahavandi, Dipti Srinivasan
- **Comment**: 9 pages, 9 figures, 3 tables
- **Journal**: None
- **Summary**: The early and reliable detection of COVID-19 infected patients is essential to prevent and limit its outbreak. The PCR tests for COVID-19 detection are not available in many countries and also there are genuine concerns about their reliability and performance. Motivated by these shortcomings, this paper proposes a deep uncertainty-aware transfer learning framework for COVID-19 detection using medical images. Four popular convolutional neural networks (CNNs) including VGG16, ResNet50, DenseNet121, and InceptionResNetV2 are first applied to extract deep features from chest X-ray and computed tomography (CT) images. Extracted features are then processed by different machine learning and statistical modelling techniques to identify COVID-19 cases. We also calculate and report the epistemic uncertainty of classification results to identify regions where the trained models are not confident about their decisions (out of distribution problem). Comprehensive simulation results for X-ray and CT image datasets indicate that linear support vector machine and neural network models achieve the best results as measured by accuracy, sensitivity, specificity, and AUC. Also it is found that predictive uncertainty estimates are much higher for CT images compared to X-ray images.



### OASIS: A Large-Scale Dataset for Single Image 3D in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2007.13215v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.13215v1)
- **Published**: 2020-07-26 20:46:41+00:00
- **Updated**: 2020-07-26 20:46:41+00:00
- **Authors**: Weifeng Chen, Shengyi Qian, David Fan, Noriyuki Kojima, Max Hamilton, Jia Deng
- **Comment**: Accepted to CVPR 2020
- **Journal**: None
- **Summary**: Single-view 3D is the task of recovering 3D properties such as depth and surface normals from a single image. We hypothesize that a major obstacle to single-image 3D is data. We address this issue by presenting Open Annotations of Single Image Surfaces (OASIS), a dataset for single-image 3D in the wild consisting of annotations of detailed 3D geometry for 140,000 images. We train and evaluate leading models on a variety of single-image 3D tasks. We expect OASIS to be a useful resource for 3D vision research. Project site: https://pvl.cs.princeton.edu/OASIS.



### Uniformizing Techniques to Process CT scans with 3D CNNs for Tuberculosis Prediction
- **Arxiv ID**: http://arxiv.org/abs/2007.13224v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.13224v1)
- **Published**: 2020-07-26 21:53:47+00:00
- **Updated**: 2020-07-26 21:53:47+00:00
- **Authors**: Hasib Zunair, Aimon Rahman, Nabeel Mohammed, Joseph Paul Cohen
- **Comment**: Accepted for publication at the MICCAI 2020 International Workshop on
  PRedictive Intelligence In MEdicine (PRIME)
- **Journal**: None
- **Summary**: A common approach to medical image analysis on volumetric data uses deep 2D convolutional neural networks (CNNs). This is largely attributed to the challenges imposed by the nature of the 3D data: variable volume size, GPU exhaustion during optimization. However, dealing with the individual slices independently in 2D CNNs deliberately discards the depth information which results in poor performance for the intended task. Therefore, it is important to develop methods that not only overcome the heavy memory and computation requirements but also leverage the 3D information. To this end, we evaluate a set of volume uniformizing methods to address the aforementioned issues. The first method involves sampling information evenly from a subset of the volume. Another method exploits the full geometry of the 3D volume by interpolating over the z-axis. We demonstrate performance improvements using controlled ablation studies as well as put this approach to the test on the ImageCLEF Tuberculosis Severity Assessment 2019 benchmark. We report 73% area under curve (AUC) and binary classification accuracy (ACC) of 67.5% on the test set beating all methods which leveraged only image information (without using clinical meta-data) achieving 5-th position overall. All codes and models are made available at https://github.com/hasibzunair/uniformizing-3D.



