# Arxiv Papers in cs.CV on 2020-07-18
### Slot Contrastive Networks: A Contrastive Approach for Representing Objects
- **Arxiv ID**: http://arxiv.org/abs/2007.09294v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.09294v1)
- **Published**: 2020-07-18 01:01:39+00:00
- **Updated**: 2020-07-18 01:01:39+00:00
- **Authors**: Evan Racah, Sarath Chandar
- **Comment**: Presented at ICML 2020 Workshop: Object-Oriented Learning (OOL):
  Perception, Representation, and Reasoning
- **Journal**: None
- **Summary**: Unsupervised extraction of objects from low-level visual data is an important goal for further progress in machine learning. Existing approaches for representing objects without labels use structured generative models with static images. These methods focus a large amount of their capacity on reconstructing unimportant background pixels, missing low contrast or small objects. Conversely, we present a new method that avoids losses in pixel space and over-reliance on the limited signal a static image provides. Our approach takes advantage of objects' motion by learning a discriminative, time-contrastive loss in the space of slot representations, attempting to force each slot to not only capture entities that move, but capture distinct objects from the other slots. Moreover, we introduce a new quantitative evaluation metric to measure how "diverse" a set of slot vectors are, and use it to evaluate our model on 20 Atari games.



### Unsupervised Shape Normality Metric for Severity Quantification
- **Arxiv ID**: http://arxiv.org/abs/2007.09307v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.09307v2)
- **Published**: 2020-07-18 01:53:45+00:00
- **Updated**: 2020-09-16 15:53:20+00:00
- **Authors**: Wenzheng Tao, Riddhish Bhalodia, Erin Anstadt, Ladislav Kavan, Ross T. Whitaker, Jesse A. Goldstein
- **Comment**: Add acknowledgements
- **Journal**: None
- **Summary**: This work describes an unsupervised method to objectively quantify the abnormality of general anatomical shapes. The severity of an anatomical deformity often serves as a determinant in the clinical management of patients. However, experiential bias and distinctive random residuals among specialist individuals bring variability in diagnosis and patient management decisions, irrespective of the objective deformity degree. Therefore, supervised methods are prone to be misled given insufficient labeling of pathological samples that inevitably preserve human bias and inconsistency. Furthermore, subjects demonstrating a specific pathology are naturally rare relative to the normal population. To avoid relying on sufficient pathological samples by fully utilizing the power of normal samples, we propose the shape normality metric (SNM), which requires learning only from normal samples and zero knowledge about the pathology. We represent shapes by landmarks automatically inferred from the data and model the normal group by a multivariate Gaussian distribution. Extensive experiments on different anatomical datasets, including skulls, femurs, scapulae, and humeri, demonstrate that SNM can provide an effective normality measurement, which can significantly detect and indicate pathology. Therefore, SNM offers promising value in a variety of clinical applications.



### Dynamic Dual-Attentive Aggregation Learning for Visible-Infrared Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2007.09314v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.09314v1)
- **Published**: 2020-07-18 03:08:13+00:00
- **Updated**: 2020-07-18 03:08:13+00:00
- **Authors**: Mang Ye, Jianbing Shen, David J. Crandall, Ling Shao, Jiebo Luo
- **Comment**: Accepted by ECCV20
- **Journal**: None
- **Summary**: Visible-infrared person re-identification (VI-ReID) is a challenging cross-modality pedestrian retrieval problem. Due to the large intra-class variations and cross-modality discrepancy with large amount of sample noise, it is difficult to learn discriminative part features. Existing VI-ReID methods instead tend to learn global representations, which have limited discriminability and weak robustness to noisy images. In this paper, we propose a novel dynamic dual-attentive aggregation (DDAG) learning method by mining both intra-modality part-level and cross-modality graph-level contextual cues for VI-ReID. We propose an intra-modality weighted-part attention module to extract discriminative part-aggregated features, by imposing the domain knowledge on the part relationship mining. To enhance robustness against noisy samples, we introduce cross-modality graph structured attention to reinforce the representation with the contextual relations across the two modalities. We also develop a parameter-free dynamic dual aggregation learning strategy to adaptively integrate the two components in a progressive joint training manner. Extensive experiments demonstrate that DDAG outperforms the state-of-the-art methods under various settings.



### Learning from Extrinsic and Intrinsic Supervisions for Domain Generalization
- **Arxiv ID**: http://arxiv.org/abs/2007.09316v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.09316v1)
- **Published**: 2020-07-18 03:12:24+00:00
- **Updated**: 2020-07-18 03:12:24+00:00
- **Authors**: Shujun Wang, Lequan Yu, Caizi Li, Chi-Wing Fu, Pheng-Ann Heng
- **Comment**: Accepted at ECCV 2020. Code is available at
  https://github.com/EmmaW8/EISNet
- **Journal**: None
- **Summary**: The generalization capability of neural networks across domains is crucial for real-world applications. We argue that a generalized object recognition system should well understand the relationships among different images and also the images themselves at the same time. To this end, we present a new domain generalization framework that learns how to generalize across domains simultaneously from extrinsic relationship supervision and intrinsic self-supervision for images from multi-source domains. To be specific, we formulate our framework with feature embedding using a multi-task learning paradigm. Besides conducting the common supervised recognition task, we seamlessly integrate a momentum metric learning task and a self-supervised auxiliary task to collectively utilize the extrinsic supervision and intrinsic supervision. Also, we develop an effective momentum metric learning scheme with K-hard negative mining to boost the network to capture image relationship for domain generalization. We demonstrate the effectiveness of our approach on two standard object recognition benchmarks VLCS and PACS, and show that our methods achieve state-of-the-art performance.



### LiteFlowNet3: Resolving Correspondence Ambiguity for More Accurate Optical Flow Estimation
- **Arxiv ID**: http://arxiv.org/abs/2007.09319v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.09319v1)
- **Published**: 2020-07-18 03:30:39+00:00
- **Updated**: 2020-07-18 03:30:39+00:00
- **Authors**: Tak-Wai Hui, Chen Change Loy
- **Comment**: Accepted to ECCV 2020. Trained models and code package are available
  at https://github.com/twhui/LiteFlowNet3
- **Journal**: None
- **Summary**: Deep learning approaches have achieved great success in addressing the problem of optical flow estimation. The keys to success lie in the use of cost volume and coarse-to-fine flow inference. However, the matching problem becomes ill-posed when partially occluded or homogeneous regions exist in images. This causes a cost volume to contain outliers and affects the flow decoding from it. Besides, the coarse-to-fine flow inference demands an accurate flow initialization. Ambiguous correspondence yields erroneous flow fields and affects the flow inferences in subsequent levels. In this paper, we introduce LiteFlowNet3, a deep network consisting of two specialized modules, to address the above challenges. (1) We ameliorate the issue of outliers in the cost volume by amending each cost vector through an adaptive modulation prior to the flow decoding. (2) We further improve the flow accuracy by exploring local flow consistency. To this end, each inaccurate optical flow is replaced with an accurate one from a nearby position through a novel warping of the flow field. LiteFlowNet3 not only achieves promising results on public benchmarks but also has a small model size and a fast runtime.



### AABO: Adaptive Anchor Box Optimization for Object Detection via Bayesian Sub-sampling
- **Arxiv ID**: http://arxiv.org/abs/2007.09336v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.09336v1)
- **Published**: 2020-07-18 05:44:26+00:00
- **Updated**: 2020-07-18 05:44:26+00:00
- **Authors**: Wenshuo Ma, Tingzhong Tian, Hang Xu, Yimin Huang, Zhenguo Li
- **Comment**: Accepted by ECCV 2020 (spotlight)
- **Journal**: None
- **Summary**: Most state-of-the-art object detection systems follow an anchor-based diagram. Anchor boxes are densely proposed over the images and the network is trained to predict the boxes position offset as well as the classification confidence. Existing systems pre-define anchor box shapes and sizes and ad-hoc heuristic adjustments are used to define the anchor configurations. However, this might be sub-optimal or even wrong when a new dataset or a new model is adopted. In this paper, we study the problem of automatically optimizing anchor boxes for object detection. We first demonstrate that the number of anchors, anchor scales and ratios are crucial factors for a reliable object detection system. By carefully analyzing the existing bounding box patterns on the feature hierarchy, we design a flexible and tight hyper-parameter space for anchor configurations. Then we propose a novel hyper-parameter optimization method named AABO to determine more appropriate anchor boxes for a certain dataset, in which Bayesian Optimization and subsampling method are combined to achieve precise and efficient anchor configuration optimization. Experiments demonstrate the effectiveness of our proposed method on different detectors and datasets, e.g. achieving around 2.4% mAP improvement on COCO, 1.6% on ADE and 1.5% on VG, and the optimal anchors can bring 1.4% to 2.4% mAP improvement on SOTA detectors by only optimizing anchor configurations, e.g. boosting Mask RCNN from 40.3% to 42.3%, and HTC detector from 46.8% to 48.2%.



### Multi-Task Neural Networks with Spatial Activation for Retinal Vessel Segmentation and Artery/Vein Classification
- **Arxiv ID**: http://arxiv.org/abs/2007.09337v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.09337v1)
- **Published**: 2020-07-18 05:46:47+00:00
- **Updated**: 2020-07-18 05:46:47+00:00
- **Authors**: Wenao Ma, Shuang Yu, Kai Ma, Jiexiang Wang, Xinghao Ding, Yefeng Zheng
- **Comment**: None
- **Journal**: MICCAI 2019
- **Summary**: Retinal artery/vein (A/V) classification plays a critical role in the clinical biomarker study of how various systemic and cardiovascular diseases affect the retinal vessels. Conventional methods of automated A/V classification are generally complicated and heavily depend on the accurate vessel segmentation. In this paper, we propose a multi-task deep neural network with spatial activation mechanism that is able to segment full retinal vessel, artery and vein simultaneously, without the pre-requirement of vessel segmentation. The input module of the network integrates the domain knowledge of widely used retinal preprocessing and vessel enhancement techniques. We specially customize the output block of the network with a spatial activation mechanism, which takes advantage of a relatively easier task of vessel segmentation and exploits it to boost the performance of A/V classification. In addition, deep supervision is introduced to the network to assist the low level layers to extract more semantic information. The proposed network achieves pixel-wise accuracy of 95.70% for vessel segmentation, and A/V classification accuracy of 94.50%, which is the state-of-the-art performance for both tasks on the AV-DRIVE dataset. Furthermore, we have also tested the model performance on INSPIRE-AVR dataset, which achieves a skeletal A/V classification accuracy of 91.6%.



### Unsupervised Domain Attention Adaptation Network for Caricature Attribute Recognition
- **Arxiv ID**: http://arxiv.org/abs/2007.09344v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.09344v1)
- **Published**: 2020-07-18 06:38:45+00:00
- **Updated**: 2020-07-18 06:38:45+00:00
- **Authors**: Wen Ji, Kelei He, Jing Huo, Zheng Gu, Yang Gao
- **Comment**: This paper has been accepted by ECCV 2020
- **Journal**: None
- **Summary**: Caricature attributes provide distinctive facial features to help research in Psychology and Neuroscience. However, unlike the facial photo attribute datasets that have a quantity of annotated images, the annotations of caricature attributes are rare. To facility the research in attribute learning of caricatures, we propose a caricature attribute dataset, namely WebCariA. Moreover, to utilize models that trained by face attributes, we propose a novel unsupervised domain adaptation framework for cross-modality (i.e., photos to caricatures) attribute recognition, with an integrated inter- and intra-domain consistency learning scheme. Specifically, the inter-domain consistency learning scheme consisting an image-to-image translator to first fill the domain gap between photos and caricatures by generating intermediate image samples, and a label consistency learning module to align their semantic information. The intra-domain consistency learning scheme integrates the common feature consistency learning module with a novel attribute-aware attention-consistency learning module for a more efficient alignment. We did an extensive ablation study to show the effectiveness of the proposed method. And the proposed method also outperforms the state-of-the-art methods by a margin. The implementation of the proposed method is available at https://github.com/KeleiHe/DAAN.



### Thinking in Frequency: Face Forgery Detection by Mining Frequency-aware Clues
- **Arxiv ID**: http://arxiv.org/abs/2007.09355v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.09355v2)
- **Published**: 2020-07-18 07:39:08+00:00
- **Updated**: 2020-10-27 04:04:29+00:00
- **Authors**: Yuyang Qian, Guojun Yin, Lu Sheng, Zixuan Chen, Jing Shao
- **Comment**: 21 pages, 9 figures, accepted as a POSTER at ECCV2020, UPDATE the
  appendix of the paper
- **Journal**: None
- **Summary**: As realistic facial manipulation technologies have achieved remarkable progress, social concerns about potential malicious abuse of these technologies bring out an emerging research topic of face forgery detection. However, it is extremely challenging since recent advances are able to forge faces beyond the perception ability of human eyes, especially in compressed images and videos. We find that mining forgery patterns with the awareness of frequency could be a cure, as frequency provides a complementary viewpoint where either subtle forgery artifacts or compression errors could be well described. To introduce frequency into the face forgery detection, we propose a novel Frequency in Face Forgery Network (F3-Net), taking advantages of two different but complementary frequency-aware clues, 1) frequency-aware decomposed image components, and 2) local frequency statistics, to deeply mine the forgery patterns via our two-stream collaborative learning framework. We apply DCT as the applied frequency-domain transformation. Through comprehensive studies, we show that the proposed F3-Net significantly outperforms competing state-of-the-art methods on all compression qualities in the challenging FaceForensics++ dataset, especially wins a big lead upon low-quality media.



### Temporal Complementary Learning for Video Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2007.09357v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.09357v1)
- **Published**: 2020-07-18 07:59:01+00:00
- **Updated**: 2020-07-18 07:59:01+00:00
- **Authors**: Ruibing Hou, Hong Chang, Bingpeng Ma, Shiguang Shan, Xilin Chen
- **Comment**: 17 pages, 6 figures, accepted by ECCV2020
- **Journal**: None
- **Summary**: This paper proposes a Temporal Complementary Learning Network that extracts complementary features of consecutive video frames for video person re-identification. Firstly, we introduce a Temporal Saliency Erasing (TSE) module including a saliency erasing operation and a series of ordered learners. Specifically, for a specific frame of a video, the saliency erasing operation drives the specific learner to mine new and complementary parts by erasing the parts activated by previous frames. Such that the diverse visual features can be discovered for consecutive frames and finally form an integral characteristic of the target identity. Furthermore, a Temporal Saliency Boosting (TSB) module is designed to propagate the salient information among video frames to enhance the salient feature. It is complementary to TSE by effectively alleviating the information loss caused by the erasing operation of TSE. Extensive experiments show our method performs favorably against state-of-the-arts. The source code is available at https://github.com/blue-blue272/VideoReID-TCLNet.



### Malleable 2.5D Convolution: Learning Receptive Fields along the Depth-axis for RGB-D Scene Parsing
- **Arxiv ID**: http://arxiv.org/abs/2007.09365v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.09365v1)
- **Published**: 2020-07-18 08:26:11+00:00
- **Updated**: 2020-07-18 08:26:11+00:00
- **Authors**: Yajie Xing, Jingbo Wang, Gang Zeng
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: Depth data provide geometric information that can bring progress in RGB-D scene parsing tasks. Several recent works propose RGB-D convolution operators that construct receptive fields along the depth-axis to handle 3D neighborhood relations between pixels. However, these methods pre-define depth receptive fields by hyperparameters, making them rely on parameter selection. In this paper, we propose a novel operator called malleable 2.5D convolution to learn the receptive field along the depth-axis. A malleable 2.5D convolution has one or more 2D convolution kernels. Our method assigns each pixel to one of the kernels or none of them according to their relative depth differences, and the assigning process is formulated as a differentiable form so that it can be learnt by gradient descent. The proposed operator runs on standard 2D feature maps and can be seamlessly incorporated into pre-trained CNNs. We conduct extensive experiments on two challenging RGB-D semantic segmentation dataset NYUDv2 and Cityscapes to validate the effectiveness and the generalization ability of our method.



### Attract, Perturb, and Explore: Learning a Feature Alignment Network for Semi-supervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2007.09375v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.09375v1)
- **Published**: 2020-07-18 09:26:25+00:00
- **Updated**: 2020-07-18 09:26:25+00:00
- **Authors**: Taekyung Kim, Changick Kim
- **Comment**: Accepted at ECCV 2020
- **Journal**: None
- **Summary**: Although unsupervised domain adaptation methods have been widely adopted across several computer vision tasks, it is more desirable if we can exploit a few labeled data from new domains encountered in a real application. The novel setting of the semi-supervised domain adaptation (SSDA) problem shares the challenges with the domain adaptation problem and the semi-supervised learning problem. However, a recent study shows that conventional domain adaptation and semi-supervised learning methods often result in less effective or negative transfer in the SSDA problem. In order to interpret the observation and address the SSDA problem, in this paper, we raise the intra-domain discrepancy issue within the target domain, which has never been discussed so far. Then, we demonstrate that addressing the intra-domain discrepancy leads to the ultimate goal of the SSDA problem. We propose an SSDA framework that aims to align features via alleviation of the intra-domain discrepancy. Our framework mainly consists of three schemes, i.e., attraction, perturbation, and exploration. First, the attraction scheme globally minimizes the intra-domain discrepancy within the target domain. Second, we demonstrate the incompatibility of the conventional adversarial perturbation methods with SSDA. Then, we present a domain adaptive adversarial perturbation scheme, which perturbs the given target samples in a way that reduces the intra-domain discrepancy. Finally, the exploration scheme locally aligns features in a class-wise manner complementary to the attraction scheme by selectively aligning unlabeled target features complementary to the perturbation scheme. We conduct extensive experiments on domain adaptation benchmark datasets such as DomainNet, Office-Home, and Office. Our method achieves state-of-the-art performances on all datasets.



### Bounding Maps for Universal Lesion Detection
- **Arxiv ID**: http://arxiv.org/abs/2007.09383v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.09383v1)
- **Published**: 2020-07-18 09:47:09+00:00
- **Updated**: 2020-07-18 09:47:09+00:00
- **Authors**: Han Li, Hu Han, S. Kevin Zhou
- **Comment**: 11 pages, 4 figures
- **Journal**: None
- **Summary**: Universal Lesion Detection (ULD) in computed tomography plays an essential role in computer-aided diagnosis systems. Many detection approaches achieve excellent results for ULD using possible bounding boxes (or anchors) as proposals. However, empirical evidence shows that using anchor-based proposals leads to a high false-positive (FP) rate. In this paper, we propose a box-to-map method to represent a bounding box with three soft continuous maps with bounds in x-, y- and xy- directions. The bounding maps (BMs) are used in two-stage anchor-based ULD frameworks to reduce the FP rate. In the 1 st stage of the region proposal network, we replace the sharp binary ground-truth label of anchors with the corresponding xy-direction BM hence the positive anchors are now graded. In the 2 nd stage, we add a branch that takes our continuous BMs in x- and y- directions for extra supervision of detailed locations. Our method, when embedded into three state-of-the-art two-stage anchor-based detection methods, brings a free detection accuracy improvement (e.g., a 1.68% to 3.85% boost of sensitivity at 4 FPs) without extra inference time.



### Multi-Scale Positive Sample Refinement for Few-Shot Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2007.09384v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.09384v1)
- **Published**: 2020-07-18 09:48:29+00:00
- **Updated**: 2020-07-18 09:48:29+00:00
- **Authors**: Jiaxi Wu, Songtao Liu, Di Huang, Yunhong Wang
- **Comment**: Accepted at ECCV 2020
- **Journal**: None
- **Summary**: Few-shot object detection (FSOD) helps detectors adapt to unseen classes with few training instances, and is useful when manual annotation is time-consuming or data acquisition is limited. Unlike previous attempts that exploit few-shot classification techniques to facilitate FSOD, this work highlights the necessity of handling the problem of scale variations, which is challenging due to the unique sample distribution. To this end, we propose a Multi-scale Positive Sample Refinement (MPSR) approach to enrich object scales in FSOD. It generates multi-scale positive samples as object pyramids and refines the prediction at various scales. We demonstrate its advantage by integrating it as an auxiliary branch to the popular architecture of Faster R-CNN with FPN, delivering a strong FSOD solution. Several experiments are conducted on PASCAL VOC and MS COCO, and the proposed approach achieves state of the art results and significantly outperforms other counterparts, which shows its effectiveness. Code is available at https://github.com/jiaxi-wu/MPSR.



### SRNet: Improving Generalization in 3D Human Pose Estimation with a Split-and-Recombine Approach
- **Arxiv ID**: http://arxiv.org/abs/2007.09389v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.09389v1)
- **Published**: 2020-07-18 10:02:36+00:00
- **Updated**: 2020-07-18 10:02:36+00:00
- **Authors**: Ailing Zeng, Xiao Sun, Fuyang Huang, Minhao Liu, Qiang Xu, Stephen Lin
- **Comment**: European Conference on Computer Vision (ECCV), 2020
- **Journal**: None
- **Summary**: Human poses that are rare or unseen in a training set are challenging for a network to predict. Similar to the long-tailed distribution problem in visual recognition, the small number of examples for such poses limits the ability of networks to model them. Interestingly, local pose distributions suffer less from the long-tail problem, i.e., local joint configurations within a rare pose may appear within other poses in the training set, making them less rare. We propose to take advantage of this fact for better generalization to rare and unseen poses. To be specific, our method splits the body into local regions and processes them in separate network branches, utilizing the property that a joint position depends mainly on the joints within its local body region. Global coherence is maintained by recombining the global context from the rest of the body into each branch as a low-dimensional vector. With the reduced dimensionality of less relevant body areas, the training set distribution within network branches more closely reflects the statistics of local poses instead of global body poses, without sacrificing information important for joint inference. The proposed split-and-recombine approach, called SRNet, can be easily adapted to both single-image and temporal models, and it leads to appreciable improvements in the prediction of rare and unseen poses.



### Weakly Supervised Instance Segmentation by Learning Annotation Consistent Instances
- **Arxiv ID**: http://arxiv.org/abs/2007.09397v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.09397v1)
- **Published**: 2020-07-18 10:32:11+00:00
- **Updated**: 2020-07-18 10:32:11+00:00
- **Authors**: Aditya Arun, C. V. Jawahar, M. Pawan Kumar
- **Comment**: To appear at ECCV 2020
- **Journal**: None
- **Summary**: Recent approaches for weakly supervised instance segmentations depend on two components: (i) a pseudo label generation model that provides instances which are consistent with a given annotation; and (ii) an instance segmentation model, which is trained in a supervised manner using the pseudo labels as ground-truth. Unlike previous approaches, we explicitly model the uncertainty in the pseudo label generation process using a conditional distribution. The samples drawn from our conditional distribution provide accurate pseudo labels due to the use of semantic class aware unary terms, boundary aware pairwise smoothness terms, and annotation aware higher order terms. Furthermore, we represent the instance segmentation model as an annotation agnostic prediction distribution. In contrast to previous methods, our representation allows us to define a joint probabilistic learning objective that minimizes the dissimilarity between the two distributions. Our approach achieves state of the art results on the PASCAL VOC 2012 data set, outperforming the best baseline by 4.2% mAP@0.5 and 4.8% mAP@0.75.



### DDR-ID: Dual Deep Reconstruction Networks Based Image Decomposition for Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/2007.09431v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.09431v1)
- **Published**: 2020-07-18 13:54:59+00:00
- **Updated**: 2020-07-18 13:54:59+00:00
- **Authors**: Dongyun Lin, Yiqun Li, Shudong Xie, Tin Lay Nwe, Sheng Dong
- **Comment**: None
- **Journal**: None
- **Summary**: One pivot challenge for image anomaly (AD) detection is to learn discriminative information only from normal class training images. Most image reconstruction based AD methods rely on the discriminative capability of reconstruction error. This is heuristic as image reconstruction is unsupervised without incorporating normal-class-specific information. In this paper, we propose an AD method called dual deep reconstruction networks based image decomposition (DDR-ID). The networks are trained by jointly optimizing for three losses: the one-class loss, the latent space constrain loss and the reconstruction loss. After training, DDR-ID can decompose an unseen image into its normal class and the residual components, respectively. Two anomaly scores are calculated to quantify the anomalous degree of the image in either normal class latent space or reconstruction image space. Thereby, anomaly detection can be performed via thresholding the anomaly score. The experiments demonstrate that DDR-ID outperforms multiple related benchmarking methods in image anomaly detection using MNIST, CIFAR-10 and Endosome datasets and adversarial attack detection using GTSRB dataset.



### Volumetric Transformer Networks
- **Arxiv ID**: http://arxiv.org/abs/2007.09433v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.09433v1)
- **Published**: 2020-07-18 14:00:12+00:00
- **Updated**: 2020-07-18 14:00:12+00:00
- **Authors**: Seungryong Kim, Sabine Süsstrunk, Mathieu Salzmann
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: Existing techniques to encode spatial invariance within deep convolutional neural networks (CNNs) apply the same warping field to all the feature channels. This does not account for the fact that the individual feature channels can represent different semantic parts, which can undergo different spatial transformations w.r.t. a canonical configuration. To overcome this limitation, we introduce a learnable module, the volumetric transformer network (VTN), that predicts channel-wise warping fields so as to reconfigure intermediate CNN features spatially and channel-wisely. We design our VTN as an encoder-decoder network, with modules dedicated to letting the information flow across the feature channels, to account for the dependencies between the semantic parts. We further propose a loss function defined between the warped features of pairs of instances, which improves the localization ability of VTN. Our experiments show that VTN consistently boosts the features' representation power and consequently the networks' accuracy on fine-grained image recognition and instance-level image retrieval.



### Few-Shot Defect Segmentation Leveraging Abundant Normal Training Samples Through Normal Background Regularization and Crop-and-Paste Operation
- **Arxiv ID**: http://arxiv.org/abs/2007.09438v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.09438v2)
- **Published**: 2020-07-18 14:15:42+00:00
- **Updated**: 2021-04-06 07:14:39+00:00
- **Authors**: Dongyun Lin, Yanpeng Cao, Wenbing Zhu, Yiqun Li
- **Comment**: Will be appeared in ICME2021 Oral Presentation
- **Journal**: None
- **Summary**: In industrial product quality assessment, it is essential to determine whether a product is defect-free and further analyze the severity of anomality. To this end, accurate defect segmentation on images of products provides an important functionality. In industrial inspection tasks, it is common to capture abundant defect-free image samples but very limited anomalous ones. Therefore, it is critical to develop automatic and accurate defect segmentation systems using only a small number of annotated anomalous training images. This paper tackles the challenging few-shot defect segmentation task with sufficient normal (defect-free) training images but very few anomalous ones. We present two effective regularization techniques via incorporating abundant defect-free images into the training of a UNet-like encoder-decoder defect segmentation network. We first propose a Normal Background Regularization (NBR) loss which is jointly minimized with the segmentation loss, enhancing the encoder network to produce distinctive representations for normal regions. Secondly, we crop/paste defective regions to the randomly selected normal images for data augmentation and propose a weighted binary cross-entropy loss to enhance the training by emphasizing more realistic crop-and-pasted augmented images based on feature-level similarity comparison. Both techniques are implemented on an encoder-decoder segmentation network backboned by ResNet-34 for few-shot defect segmentation. Extensive experiments are conducted on the recently released MVTec Anomaly Detection dataset with high-resolution industrial images. Under both 1-shot and 5-shot defect segmentation settings, the proposed method significantly outperforms several benchmarking methods.



### Feature Pyramid Transformer
- **Arxiv ID**: http://arxiv.org/abs/2007.09451v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.09451v1)
- **Published**: 2020-07-18 15:16:32+00:00
- **Updated**: 2020-07-18 15:16:32+00:00
- **Authors**: Dong Zhang, Hanwang Zhang, Jinhui Tang, Meng Wang, Xiansheng Hua, Qianru Sun
- **Comment**: Published at the European Conference on Computer Vision, 2020
- **Journal**: None
- **Summary**: Feature interactions across space and scales underpin modern visual recognition systems because they introduce beneficial visual contexts. Conventionally, spatial contexts are passively hidden in the CNN's increasing receptive fields or actively encoded by non-local convolution. Yet, the non-local spatial interactions are not across scales, and thus they fail to capture the non-local contexts of objects (or parts) residing in different scales. To this end, we propose a fully active feature interaction across both space and scales, called Feature Pyramid Transformer (FPT). It transforms any feature pyramid into another feature pyramid of the same size but with richer contexts, by using three specially designed transformers in self-level, top-down, and bottom-up interaction fashion. FPT serves as a generic visual backbone with fair computational overhead. We conduct extensive experiments in both instance-level (i.e., object detection and instance segmentation) and pixel-level segmentation tasks, using various backbones and head networks, and observe consistent improvement over all the baselines and the state-of-the-art methods.



### Robust Image Classification Using A Low-Pass Activation Function and DCT Augmentation
- **Arxiv ID**: http://arxiv.org/abs/2007.09453v2
- **DOI**: 10.1109/ACCESS.2021.3089598
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.09453v2)
- **Published**: 2020-07-18 15:24:13+00:00
- **Updated**: 2021-06-13 03:01:34+00:00
- **Authors**: Md Tahmid Hossain, Shyh Wei Teng, Ferdous Sohel, Guojun Lu
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional Neural Network's (CNN's) performance disparity on clean and corrupted datasets has recently come under scrutiny. In this work, we analyse common corruptions in the frequency domain, i.e., High Frequency corruptions (HFc, e.g., noise) and Low Frequency corruptions (LFc, e.g., blur). Although a simple solution to HFc is low-pass filtering, ReLU -- a widely used Activation Function (AF), does not have any filtering mechanism. In this work, we instill low-pass filtering into the AF (LP-ReLU) to improve robustness against HFc. To deal with LFc, we complement LP-ReLU with Discrete Cosine Transform based augmentation. LP-ReLU, coupled with DCT augmentation, enables a deep network to tackle the entire spectrum of corruption. We use CIFAR-10-C and Tiny ImageNet-C for evaluation and demonstrate improvements of 5% and 7.3% in accuracy respectively, compared to the State-Of-The-Art (SOTA). We further evaluate our method's stability on a variety of perturbations in CIFAR-10-P and Tiny ImageNet-P, achieving new SOTA in these experiments as well. To further strengthen our understanding regarding CNN's lack of robustness, a decision space visualisation process is proposed and presented in this work.



### Face Super-Resolution Guided by 3D Facial Priors
- **Arxiv ID**: http://arxiv.org/abs/2007.09454v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.09454v1)
- **Published**: 2020-07-18 15:26:07+00:00
- **Updated**: 2020-07-18 15:26:07+00:00
- **Authors**: Xiaobin Hu, Wenqi Ren, John LaMaster, Xiaochun Cao, Xiaoming Li, Zechao Li, Bjoern Menze, Wei Liu
- **Comment**: Accepted as a spotlight paper, European Conference on Computer Vision
  2020 (ECCV)
- **Journal**: None
- **Summary**: State-of-the-art face super-resolution methods employ deep convolutional neural networks to learn a mapping between low- and high- resolution facial patterns by exploring local appearance knowledge. However, most of these methods do not well exploit facial structures and identity information, and struggle to deal with facial images that exhibit large pose variations. In this paper, we propose a novel face super-resolution method that explicitly incorporates 3D facial priors which grasp the sharp facial structures. Our work is the first to explore 3D morphable knowledge based on the fusion of parametric descriptions of face attributes (e.g., identity, facial expression, texture, illumination, and face pose). Furthermore, the priors can easily be incorporated into any network and are extremely efficient in improving the performance and accelerating the convergence speed. Firstly, a 3D face rendering branch is set up to obtain 3D priors of salient facial structures and identity knowledge. Secondly, the Spatial Attention Module is used to better exploit this hierarchical information (i.e., intensity similarity, 3D facial structure, and identity content) for the super-resolution problem. Extensive experiments demonstrate that the proposed 3D priors achieve superior face super-resolution results over the state-of-the-arts.



### ICA-UNet: ICA Inspired Statistical UNet for Real-time 3D Cardiac Cine MRI Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2007.09455v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.09455v1)
- **Published**: 2020-07-18 15:29:01+00:00
- **Updated**: 2020-07-18 15:29:01+00:00
- **Authors**: Tianchen Wang, Xiaowei Xu, Jinjun Xiong, Qianjun Jia, Haiyun Yuan, Meiping Huang, Jian Zhuang, Yiyu Shi
- **Comment**: MICCAI2020, 12 pages, 3 figures
- **Journal**: None
- **Summary**: Real-time cine magnetic resonance imaging (MRI) plays an increasingly important role in various cardiac interventions. In order to enable fast and accurate visual assistance, the temporal frames need to be segmented on-the-fly. However, state-of-the-art MRI segmentation methods are used either offline because of their high computation complexity, or in real-time but with significant accuracy loss and latency increase (causing visually noticeable lag). As such, they can hardly be adopted to assist visual guidance. In this work, inspired by a new interpretation of Independent Component Analysis (ICA) for learning, we propose a novel ICA-UNet for real-time 3D cardiac cine MRI segmentation. Experiments using the MICCAI ACDC 2017 dataset show that, compared with the state-of-the-arts, ICA-UNet not only achieves higher Dice scores, but also meets the real-time requirements for both throughput and latency (up to 12.6X reduction), enabling real-time guidance for cardiac interventions without visual lag.



### A Bag of Visual Words Model for Medical Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2007.09464v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/2007.09464v1)
- **Published**: 2020-07-18 16:21:30+00:00
- **Updated**: 2020-07-18 16:21:30+00:00
- **Authors**: Sowmya Kamath S, Karthik K
- **Comment**: In the proceedings of the 7th International Engineering Symposium
  (IES 2018), Kumamoto University, Kumamoto, Japan, Mar 7-9, 2018
- **Journal**: None
- **Summary**: Medical Image Retrieval is a challenging field in Visual information retrieval, due to the multi-dimensional and multi-modal context of the underlying content. Traditional models often fail to take the intrinsic characteristics of data into consideration, and have thus achieved limited accuracy when applied to medical images. The Bag of Visual Words (BoVW) is a technique that can be used to effectively represent intrinsic image features in vector space, so that applications like image classification and similar-image search can be optimized. In this paper, we present a MedIR approach based on the BoVW model for content-based medical image retrieval. As medical images as multi-dimensional, they exhibit underlying cluster and manifold information which enhances semantic relevance and allows for label uniformity. Hence, the BoVW features extracted for each image are used to train a supervised machine learning classifier based on positive and negative training images, for extending content based image retrieval. During experimental validation, the proposed model performed very well, achieving a Mean Average Precision of 88.89% during top-3 image retrieval experiments.



### PSIGAN: Joint probabilistic segmentation and image distribution matching for unpaired cross-modality adaptation based MRI segmentation
- **Arxiv ID**: http://arxiv.org/abs/2007.09465v2
- **DOI**: 10.1109/TMI.2020.3011626
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.09465v2)
- **Published**: 2020-07-18 16:23:02+00:00
- **Updated**: 2021-07-18 16:01:15+00:00
- **Authors**: Jue Jiang, Yu Chi Hu, Neelam Tyagi, Andreas Rimner, Nancy Lee, Joseph O. Deasy, Sean Berry, Harini Veeraraghavan
- **Comment**: This paper has been accepted by IEEE Transactions on Medical Imaging
- **Journal**: IEEE Transactions on Medical Imaging, 2020
- **Summary**: We developed a new joint probabilistic segmentation and image distribution matching generative adversarial network (PSIGAN) for unsupervised domain adaptation (UDA) and multi-organ segmentation from magnetic resonance (MRI) images. Our UDA approach models the co-dependency between images and their segmentation as a joint probability distribution using a new structure discriminator. The structure discriminator computes structure of interest focused adversarial loss by combining the generated pseudo MRI with probabilistic segmentations produced by a simultaneously trained segmentation sub-network. The segmentation sub-network is trained using the pseudo MRI produced by the generator sub-network. This leads to a cyclical optimization of both the generator and segmentation sub-networks that are jointly trained as part of an end-to-end network. Extensive experiments and comparisons against multiple state-of-the-art methods were done on four different MRI sequences totalling 257 scans for generating multi-organ and tumor segmentation. The experiments included, (a) 20 T1-weighted (T1w) in-phase mdixon and (b) 20 T2-weighted (T2w) abdominal MRI for segmenting liver, spleen, left and right kidneys, (c) 162 T2-weighted fat suppressed head and neck MRI (T2wFS) for parotid gland segmentation, and (d) 75 T2w MRI for lung tumor segmentation. Our method achieved an overall average DSC of 0.87 on T1w and 0.90 on T2w for the abdominal organs, 0.82 on T2wFS for the parotid glands, and 0.77 on T2w MRI for lung tumors.



### ESCELL: Emergent Symbolic Cellular Language
- **Arxiv ID**: http://arxiv.org/abs/2007.09469v1
- **DOI**: 10.1109/ISBI45749.2020.9098343
- **Categories**: **cs.AI**, cs.CV, cs.LG, q-bio.CB
- **Links**: [PDF](http://arxiv.org/pdf/2007.09469v1)
- **Published**: 2020-07-18 16:34:36+00:00
- **Updated**: 2020-07-18 16:34:36+00:00
- **Authors**: Aritra Chowdhury, James R. Kubricht, Anup Sood, Peter Tu, Alberto Santamaria-Pang
- **Comment**: IEEE International Symposium on Biomedical Imaging (IEEE ISBI 2020)
- **Journal**: 2020 IEEE 17th International Symposium on Biomedical Imaging
  (ISBI), Iowa City, IA, USA, 2020, pp. 1604-1607
- **Summary**: We present ESCELL, a method for developing an emergent symbolic language of communication between multiple agents reasoning about cells. We show how agents are able to cooperate and communicate successfully in the form of symbols similar to human language to accomplish a task in the form of a referential game (Lewis' signaling game). In one form of the game, a sender and a receiver observe a set of cells from 5 different cell phenotypes. The sender is told one cell is a target and is allowed to send one symbol to the receiver from a fixed arbitrary vocabulary size. The receiver relies on the information in the symbol to identify the target cell. We train the sender and receiver networks to develop an innate emergent language between themselves to accomplish this task. We observe that the networks are able to successfully identify cells from 5 different phenotypes with an accuracy of 93.2%. We also introduce a new form of the signaling game where the sender is shown one image instead of all the images that the receiver sees. The networks successfully develop an emergent language to get an identification accuracy of 77.8%.



### Social Adaptive Module for Weakly-supervised Group Activity Recognition
- **Arxiv ID**: http://arxiv.org/abs/2007.09470v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.09470v1)
- **Published**: 2020-07-18 16:40:55+00:00
- **Updated**: 2020-07-18 16:40:55+00:00
- **Authors**: Rui Yan, Lingxi Xie, Jinhui Tang, Xiangbo Shu, Qi Tian
- **Comment**: Accepted by ECCV2020
- **Journal**: None
- **Summary**: This paper presents a new task named weakly-supervised group activity recognition (GAR) which differs from conventional GAR tasks in that only video-level labels are available, yet the important persons within each frame are not provided even in the training data. This eases us to collect and annotate a large-scale NBA dataset and thus raise new challenges to GAR. To mine useful information from weak supervision, we present a key insight that key instances are likely to be related to each other, and thus design a social adaptive module (SAM) to reason about key persons and frames from noisy data. Experiments show significant improvement on the NBA dataset as well as the popular volleyball dataset. In particular, our model trained on video-level annotation achieves comparable accuracy to prior algorithms which required strong labels.



### Automated Phenotyping via Cell Auto Training (CAT) on the Cell DIVE Platform
- **Arxiv ID**: http://arxiv.org/abs/2007.09471v1
- **DOI**: 10.1109/BIBM47256.2019.8983271
- **Categories**: **eess.IV**, cs.CV, q-bio.CB, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2007.09471v1)
- **Published**: 2020-07-18 16:45:32+00:00
- **Updated**: 2020-07-18 16:45:32+00:00
- **Authors**: Alberto Santamaria-Pang, Anup Sood, Dan Meyer, Aritra Chowdhury, Fiona Ginty
- **Comment**: 2019 IEEE International Conference on Bioinformatics and Biomedicine
  (BIBM)
- **Journal**: 2019 IEEE International Conference on Bioinformatics and
  Biomedicine (BIBM), San Diego, CA, USA, 2019, pp. 2750-2756
- **Summary**: We present a method for automatic cell classification in tissue samples using an automated training set from multiplexed immunofluorescence images. The method utilizes multiple markers stained in situ on a single tissue section on a robust hyperplex immunofluorescence platform (Cell DIVE, GE Healthcare) that provides multi-channel images allowing analysis at single cell/sub-cellular levels. The cell classification method consists of two steps: first, an automated training set from every image is generated using marker-to-cell staining information. This mimics how a pathologist would select samples from a very large cohort at the image level. In the second step, a probability model is inferred from the automated training set. The probabilistic model captures staining patterns in mutually exclusive cell types and builds a single probability model for the data cohort. We have evaluated the proposed approach to classify: i) immune cells in cancer and ii) brain cells in neurological degenerative diseased tissue with average accuracies above 95%.



### Classification of Diabetic Retinopathy via Fundus Photography: Utilization of Deep Learning Approaches to Speed up Disease Detection
- **Arxiv ID**: http://arxiv.org/abs/2007.09478v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, I.4.6; I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/2007.09478v1)
- **Published**: 2020-07-18 17:11:20+00:00
- **Updated**: 2020-07-18 17:11:20+00:00
- **Authors**: Hangwei Zhuang, Nabil Ettehadi
- **Comment**: 6 pages, 9 figures
- **Journal**: None
- **Summary**: In this paper, we propose two distinct solutions to the problem of Diabetic Retinopathy (DR) classification. In the first approach, we introduce a shallow neural network architecture. This model performs well on classification of the most frequent classes while fails at classifying the less frequent ones. In the second approach, we use transfer learning to re-train the last modified layer of a very deep neural network to improve the generalization ability of the model to the less frequent classes. Our results demonstrate superior abilities of transfer learning in DR classification of less frequent classes compared to the shallow neural network.



### Deep Learning Based Brain Tumor Segmentation: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2007.09479v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.09479v3)
- **Published**: 2020-07-18 17:14:50+00:00
- **Updated**: 2021-11-17 11:21:55+00:00
- **Authors**: Zhihua Liu, Lei Tong, Zheheng Jiang, Long Chen, Feixiang Zhou, Qianni Zhang, Xiangrong Zhang, Yaochu Jin, Huiyu Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: Brain tumor segmentation is one of the most challenging problems in medical image analysis. The goal of brain tumor segmentation is to generate accurate delineation of brain tumor regions. In recent years, deep learning methods have shown promising performance in solving various computer vision problems, such as image classification, object detection and semantic segmentation. A number of deep learning based methods have been applied to brain tumor segmentation and achieved promising results. Considering the remarkable breakthroughs made by state-of-the-art technologies, we use this survey to provide a comprehensive study of recently developed deep learning based brain tumor segmentation techniques. More than 100 scientific papers are selected and discussed in this survey, extensively covering technical aspects such as network architecture design, segmentation under imbalanced conditions, and multi-modality processes. We also provide insightful discussions for future development directions.



### Mask TextSpotter v3: Segmentation Proposal Network for Robust Scene Text Spotting
- **Arxiv ID**: http://arxiv.org/abs/2007.09482v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.09482v1)
- **Published**: 2020-07-18 17:25:50+00:00
- **Updated**: 2020-07-18 17:25:50+00:00
- **Authors**: Minghui Liao, Guan Pang, Jing Huang, Tal Hassner, Xiang Bai
- **Comment**: Accepted by ECCV 2020
- **Journal**: None
- **Summary**: Recent end-to-end trainable methods for scene text spotting, integrating detection and recognition, showed much progress. However, most of the current arbitrary-shape scene text spotters use region proposal networks (RPN) to produce proposals. RPN relies heavily on manually designed anchors and its proposals are represented with axis-aligned rectangles. The former presents difficulties in handling text instances of extreme aspect ratios or irregular shapes, and the latter often includes multiple neighboring instances into a single proposal, in cases of densely oriented text. To tackle these problems, we propose Mask TextSpotter v3, an end-to-end trainable scene text spotter that adopts a Segmentation Proposal Network (SPN) instead of an RPN. Our SPN is anchor-free and gives accurate representations of arbitrary-shape proposals. It is therefore superior to RPN in detecting text instances of extreme aspect ratios or irregular shapes. Furthermore, the accurate proposals produced by SPN allow masked RoI features to be used for decoupling neighboring text instances. As a result, our Mask TextSpotter v3 can handle text instances of extreme aspect ratios or irregular shapes, and its recognition accuracy won't be affected by nearby text or background noise. Specifically, we outperform state-of-the-art methods by 21.9 percent on the Rotated ICDAR 2013 dataset (rotation robustness), 5.9 percent on the Total-Text dataset (shape robustness), and achieve state-of-the-art performance on the MSRA-TD500 dataset (aspect ratio robustness). Code is available at: https://github.com/MhLiao/MaskTextSpotterV3



### Deep Hough-Transform Line Priors
- **Arxiv ID**: http://arxiv.org/abs/2007.09493v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.09493v1)
- **Published**: 2020-07-18 18:12:42+00:00
- **Updated**: 2020-07-18 18:12:42+00:00
- **Authors**: Yancong Lin, Silvia L. Pintea, Jan C. van Gemert
- **Comment**: ECCV 2020, code online:
  https://github.com/yanconglin/Deep-Hough-Transform-Line-Priors
- **Journal**: None
- **Summary**: Classical work on line segment detection is knowledge-based; it uses carefully designed geometric priors using either image gradients, pixel groupings, or Hough transform variants. Instead, current deep learning methods do away with all prior knowledge and replace priors by training deep networks on large manually annotated datasets. Here, we reduce the dependency on labeled data by building on the classic knowledge-based priors while using deep networks to learn features. We add line priors through a trainable Hough transform block into a deep network. Hough transform provides the prior knowledge about global line parameterizations, while the convolutional layers can learn the local gradient-like line features. On the Wireframe (ShanghaiTech) and York Urban datasets we show that adding prior knowledge improves data efficiency as line priors no longer need to be learned from data. Keywords: Hough transform; global line prior, line segment detection.



### MIX'EM: Unsupervised Image Classification using a Mixture of Embeddings
- **Arxiv ID**: http://arxiv.org/abs/2007.09502v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.09502v2)
- **Published**: 2020-07-18 19:24:22+00:00
- **Updated**: 2020-10-02 23:01:29+00:00
- **Authors**: Ali Varamesh, Tinne Tuytelaars
- **Comment**: None
- **Journal**: None
- **Summary**: We present MIX'EM, a novel solution for unsupervised image classification. MIX'EM generates representations that by themselves are sufficient to drive a general-purpose clustering algorithm to deliver high-quality classification. This is accomplished by building a mixture of embeddings module into a contrastive visual representation learning framework in order to disentangle representations at the category level. It first generates a set of embedding and mixing coefficients from a given visual representation, and then combines them into a single embedding. We introduce three techniques to successfully train MIX'EM and avoid degenerate solutions; (i) diversify mixture components by maximizing entropy, (ii) minimize instance conditioned component entropy to enforce a clustered embedding space, and (iii) use an associative embedding loss to enforce semantic separability. By applying (i) and (ii), semantic categories emerge through the mixture coefficients, making it possible to apply (iii). Subsequently, we run K-means on the representations to acquire semantic classification. We conduct extensive experiments and analyses on STL10, CIFAR10, and CIFAR100-20 datasets, achieving state-of-the-art classification accuracy of 78\%, 82\%, and 44\%, respectively. To achieve robust and high accuracy, it is essential to use the mixture components to initialize K-means. Finally, we report competitive baselines (70\% on STL10) obtained by applying K-means to the "normalized" representations learned using the contrastive loss.



### Backpropagated Gradient Representations for Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/2007.09507v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.09507v1)
- **Published**: 2020-07-18 19:39:42+00:00
- **Updated**: 2020-07-18 19:39:42+00:00
- **Authors**: Gukyeong Kwon, Mohit Prabhushankar, Dogancan Temel, Ghassan AlRegib
- **Comment**: European Conference on Computer Vision (ECCV) 2020
- **Journal**: None
- **Summary**: Learning representations that clearly distinguish between normal and abnormal data is key to the success of anomaly detection. Most of existing anomaly detection algorithms use activation representations from forward propagation while not exploiting gradients from backpropagation to characterize data. Gradients capture model updates required to represent data. Anomalies require more drastic model updates to fully represent them compared to normal data. Hence, we propose the utilization of backpropagated gradients as representations to characterize model behavior on anomalies and, consequently, detect such anomalies. We show that the proposed method using gradient-based representations achieves state-of-the-art anomaly detection performance in benchmark image recognition datasets. Also, we highlight the computational efficiency and the simplicity of the proposed method in comparison with other state-of-the-art methods relying on adversarial networks or autoregressive models, which require at least 27 times more model parameters than the proposed method.



### Tracking-by-Counting: Using Network Flows on Crowd Density Maps for Tracking Multiple Targets
- **Arxiv ID**: http://arxiv.org/abs/2007.09509v1
- **DOI**: 10.1109/TIP.2020.3044219
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.09509v1)
- **Published**: 2020-07-18 19:51:53+00:00
- **Updated**: 2020-07-18 19:51:53+00:00
- **Authors**: Weihong Ren, Xinchao Wang, Jiandong Tian, Yandong Tang, Antoni B. Chan
- **Comment**: 14 pages
- **Journal**: None
- **Summary**: State-of-the-art multi-object tracking~(MOT) methods follow the tracking-by-detection paradigm, where object trajectories are obtained by associating per-frame outputs of object detectors. In crowded scenes, however, detectors often fail to obtain accurate detections due to heavy occlusions and high crowd density. In this paper, we propose a new MOT paradigm, tracking-by-counting, tailored for crowded scenes. Using crowd density maps, we jointly model detection, counting, and tracking of multiple targets as a network flow program, which simultaneously finds the global optimal detections and trajectories of multiple targets over the whole video. This is in contrast to prior MOT methods that either ignore the crowd density and thus are prone to errors in crowded scenes, or rely on a suboptimal two-step process using heuristic density-aware point-tracks for matching targets.Our approach yields promising results on public benchmarks of various domains including people tracking, cell tracking, and fish tracking.



### FaceHop: A Light-Weight Low-Resolution Face Gender Classification Method
- **Arxiv ID**: http://arxiv.org/abs/2007.09510v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.09510v3)
- **Published**: 2020-07-18 19:59:31+00:00
- **Updated**: 2020-11-13 02:16:59+00:00
- **Authors**: Mozhdeh Rouhsedaghat, Yifan Wang, Xiou Ge, Shuowen Hu, Suya You, C. -C. Jay Kuo
- **Comment**: None
- **Journal**: None
- **Summary**: A light-weight low-resolution face gender classification method, called FaceHop, is proposed in this research. We have witnessed rapid progress in face gender classification accuracy due to the adoption of deep learning (DL) technology. Yet, DL-based systems are not suitable for resource-constrained environments with limited networking and computing. FaceHop offers an interpretable non-parametric machine learning solution. It has desired characteristics such as a small model size, a small training data amount, low training complexity, and low-resolution input images. FaceHop is developed with the successive subspace learning (SSL) principle and built upon the foundation of PixelHop++. The effectiveness of the FaceHop method is demonstrated by experiments. For gray-scale face images of resolution $32 \times 32$ in the LFW and the CMU Multi-PIE datasets, FaceHop achieves correct gender classification rates of 94.63% and 95.12% with model sizes of 16.9K and 17.6K parameters, respectively. It outperforms LeNet-5 in classification accuracy while LeNet-5 has a model size of 75.8K parameters.



### Probabilistic Neighbourhood Component Analysis: Sample Efficient Uncertainty Estimation in Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2007.10800v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.10800v1)
- **Published**: 2020-07-18 21:36:31+00:00
- **Updated**: 2020-07-18 21:36:31+00:00
- **Authors**: Ankur Mallick, Chaitanya Dwivedi, Bhavya Kailkhura, Gauri Joshi, T. Yong-Jin Han
- **Comment**: None
- **Journal**: None
- **Summary**: While Deep Neural Networks (DNNs) achieve state-of-the-art accuracy in various applications, they often fall short in accurately estimating their predictive uncertainty and, in turn, fail to recognize when these predictions may be wrong. Several uncertainty-aware models, such as Bayesian Neural Network (BNNs) and Deep Ensembles have been proposed in the literature for quantifying predictive uncertainty. However, research in this area has been largely confined to the big data regime. In this work, we show that the uncertainty estimation capability of state-of-the-art BNNs and Deep Ensemble models degrades significantly when the amount of training data is small. To address the issue of accurate uncertainty estimation in the small-data regime, we propose a probabilistic generalization of the popular sample-efficient non-parametric kNN approach. Our approach enables deep kNN classifier to accurately quantify underlying uncertainties in its prediction. We demonstrate the usefulness of the proposed approach by achieving superior uncertainty quantification as compared to state-of-the-art on a real-world application of COVID-19 diagnosis from chest X-Rays. Our code is available at https://github.com/ankurmallick/sample-efficient-uq



### Single View Metrology in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2007.09529v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.09529v3)
- **Published**: 2020-07-18 22:31:33+00:00
- **Updated**: 2021-02-23 05:40:10+00:00
- **Authors**: Rui Zhu, Xingyi Yang, Yannick Hold-Geoffroy, Federico Perazzi, Jonathan Eisenmann, Kalyan Sunkavalli, Manmohan Chandraker
- **Comment**: ECCV 2020, camera-ready version
- **Journal**: None
- **Summary**: Most 3D reconstruction methods may only recover scene properties up to a global scale ambiguity. We present a novel approach to single view metrology that can recover the absolute scale of a scene represented by 3D heights of objects or camera height above the ground as well as camera parameters of orientation and field of view, using just a monocular image acquired in unconstrained condition. Our method relies on data-driven priors learned by a deep network specifically designed to imbibe weakly supervised constraints from the interplay of the unknown camera with 3D entities such as object heights, through estimation of bounding box projections. We leverage categorical priors for objects such as humans or cars that commonly occur in natural images, as references for scale estimation. We demonstrate state-of-the-art qualitative and quantitative results on several datasets as well as applications including virtual object insertion. Furthermore, the perceptual quality of our outputs is validated by a user study.



