# Arxiv Papers in cs.CV on 2020-07-27
### Dual Distribution Alignment Network for Generalizable Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2007.13249v1
- **DOI**: None
- **Categories**: **cs.IR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.13249v1)
- **Published**: 2020-07-27 00:08:07+00:00
- **Updated**: 2020-07-27 00:08:07+00:00
- **Authors**: Peixian Chen, Pingyang Dai, Jianzhuang Liu, Feng Zheng, Qi Tian, Rongrong Ji
- **Comment**: 8 pages, 3 figures
- **Journal**: None
- **Summary**: Domain generalization (DG) serves as a promising solution to handle person Re-Identification (Re-ID), which trains the model using labels from the source domain alone, and then directly adopts the trained model to the target domain without model updating. However, existing DG approaches are usually disturbed by serious domain variations due to significant dataset variations. Subsequently, DG highly relies on designing domain-invariant features, which is however not well exploited, since most existing approaches directly mix multiple datasets to train DG based models without considering the local dataset similarities, i.e., examples that are very similar but from different domains. In this paper, we present a Dual Distribution Alignment Network (DDAN), which handles this challenge by mapping images into a domain-invariant feature space by selectively aligning distributions of multiple source domains. Such an alignment is conducted by dual-level constraints, i.e., the domain-wise adversarial feature learning and the identity-wise similarity enhancement. We evaluate our DDAN on a large-scale Domain Generalization Re-ID (DG Re-ID) benchmark. Quantitative results demonstrate that the proposed DDAN can well align the distributions of various source domains, and significantly outperforms all existing domain generalization approaches.



### RANDOM MASK: Towards Robust Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2007.14249v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.14249v1)
- **Published**: 2020-07-27 00:10:28+00:00
- **Updated**: 2020-07-27 00:10:28+00:00
- **Authors**: Tiange Luo, Tianle Cai, Mengxiao Zhang, Siyu Chen, Liwei Wang
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1911.08432
- **Journal**: None
- **Summary**: Robustness of neural networks has recently been highlighted by the adversarial examples, i.e., inputs added with well-designed perturbations which are imperceptible to humans but can cause the network to give incorrect outputs. In this paper, we design a new CNN architecture that by itself has good robustness. We introduce a simple but powerful technique, Random Mask, to modify existing CNN structures. We show that CNN with Random Mask achieves state-of-the-art performance against black-box adversarial attacks without applying any adversarial training. We next investigate the adversarial examples which 'fool' a CNN with Random Mask. Surprisingly, we find that these adversarial examples often 'fool' humans as well. This raises fundamental questions on how to define adversarial examples and robustness properly.



### Point-to-set distance functions for weakly supervised segmentation
- **Arxiv ID**: http://arxiv.org/abs/2007.13251v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, 68T45
- **Links**: [PDF](http://arxiv.org/pdf/2007.13251v1)
- **Published**: 2020-07-27 00:15:13+00:00
- **Updated**: 2020-07-27 00:15:13+00:00
- **Authors**: Bas Peters
- **Comment**: None
- **Journal**: None
- **Summary**: When pixel-level masks or partial annotations are not available for training neural networks for semantic segmentation, it is possible to use higher-level information in the form of bounding boxes, or image tags. In the imaging sciences, many applications do not have an object-background structure and bounding boxes are not available. Any available annotation typically comes from ground truth or domain experts. A direct way to train without masks is using prior knowledge on the size of objects/classes in the segmentation. We present a new algorithm to include such information via constraints on the network output, implemented via projection-based point-to-set distance functions. This type of distance functions always has the same functional form of the derivative, and avoids the need to adapt penalty functions to different constraints, as well as issues related to constraining properties typically associated with non-differentiable functions. Whereas object size information is known to enable object segmentation from bounding boxes from datasets with many general and medical images, we show that the applications extend to the imaging sciences where data represents indirect measurements, even in the case of single examples. We illustrate the capabilities in case of a) one or more classes do not have any annotation; b) there is no annotation at all; c) there are bounding boxes. We use data for hyperspectral time-lapse imaging, object segmentation in corrupted images, and sub-surface aquifer mapping from airborne-geophysical remote-sensing data. The examples verify that the developed methodology alleviates difficulties with annotating non-visual imagery for a range of experimental settings.



### REXUP: I REason, I EXtract, I UPdate with Structured Compositional Reasoning for Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/2007.13262v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2007.13262v2)
- **Published**: 2020-07-27 00:54:50+00:00
- **Updated**: 2020-09-14 09:18:20+00:00
- **Authors**: Siwen Luo, Soyeon Caren Han, Kaiyuan Sun, Josiah Poon
- **Comment**: Accepted by ICONIP 2020
- **Journal**: None
- **Summary**: Visual question answering (VQA) is a challenging multi-modal task that requires not only the semantic understanding of both images and questions, but also the sound perception of a step-by-step reasoning process that would lead to the correct answer. So far, most successful attempts in VQA have been focused on only one aspect, either the interaction of visual pixel features of images and word features of questions, or the reasoning process of answering the question in an image with simple objects. In this paper, we propose a deep reasoning VQA model with explicit visual structure-aware textual information, and it works well in capturing step-by-step reasoning process and detecting a complex object-relationship in photo-realistic images. REXUP network consists of two branches, image object-oriented and scene graph oriented, which jointly works with super-diagonal fusion compositional attention network. We quantitatively and qualitatively evaluate REXUP on the GQA dataset and conduct extensive ablation studies to explore the reasons behind REXUP's effectiveness. Our best model significantly outperforms the precious state-of-the-art, which delivers 92.7% on the validation set and 73.1% on the test-dev set.



### Learning Task-oriented Disentangled Representations for Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2007.13264v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.13264v1)
- **Published**: 2020-07-27 01:21:18+00:00
- **Updated**: 2020-07-27 01:21:18+00:00
- **Authors**: Pingyang Dai, Peixian Chen, Qiong Wu, Xiaopeng Hong, Qixiang Ye, Qi Tian, Rongrong Ji
- **Comment**: 9 pages, 6 figures
- **Journal**: None
- **Summary**: Unsupervised domain adaptation (UDA) aims to address the domain-shift problem between a labeled source domain and an unlabeled target domain. Many efforts have been made to address the mismatch between the distributions of training and testing data, but unfortunately, they ignore the task-oriented information across domains and are inflexible to perform well in complicated open-set scenarios. Many efforts have been made to eliminate the mismatch between the distributions of training and testing data by learning domain-invariant representations. However, the learned representations are usually not task-oriented, i.e., being class-discriminative and domain-transferable simultaneously. This drawback limits the flexibility of UDA in complicated open-set tasks where no labels are shared between domains. In this paper, we break the concept of task-orientation into task-relevance and task-irrelevance, and propose a dynamic task-oriented disentangling network (DTDN) to learn disentangled representations in an end-to-end fashion for UDA. The dynamic disentangling network effectively disentangles data representations into two components: the task-relevant ones embedding critical information associated with the task across domains, and the task-irrelevant ones with the remaining non-transferable or disturbing information. These two components are regularized by a group of task-specific objective functions across domains. Such regularization explicitly encourages disentangling and avoids the use of generative models or decoders. Experiments in complicated, open-set scenarios (retrieval tasks) and empirical benchmarks (classification tasks) demonstrate that the proposed method captures rich disentangled information and achieves superior performance.



### Representation Learning with Video Deep InfoMax
- **Arxiv ID**: http://arxiv.org/abs/2007.13278v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.13278v2)
- **Published**: 2020-07-27 02:28:47+00:00
- **Updated**: 2020-07-28 01:27:14+00:00
- **Authors**: R Devon Hjelm, Philip Bachman
- **Comment**: None
- **Journal**: None
- **Summary**: Self-supervised learning has made unsupervised pretraining relevant again for difficult computer vision tasks. The most effective self-supervised methods involve prediction tasks based on features extracted from diverse views of the data. DeepInfoMax (DIM) is a self-supervised method which leverages the internal structure of deep networks to construct such views, forming prediction tasks between local features which depend on small patches in an image and global features which depend on the whole image. In this paper, we extend DIM to the video domain by leveraging similar structure in spatio-temporal networks, producing a method we call Video Deep InfoMax(VDIM). We find that drawing views from both natural-rate sequences and temporally-downsampled sequences yields results on Kinetics-pretrained action recognition tasks which match or outperform prior state-of-the-art methods that use more costly large-time-scale transformer models. We also examine the effects of data augmentation and fine-tuning methods, accomplishingSoTA by a large margin when training only on the UCF-101 dataset.



### Research Progress of Convolutional Neural Network and its Application in Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2007.13284v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, I.2
- **Links**: [PDF](http://arxiv.org/pdf/2007.13284v1)
- **Published**: 2020-07-27 02:45:59+00:00
- **Updated**: 2020-07-27 02:45:59+00:00
- **Authors**: Wei Zhang, Zuoxiang Zeng
- **Comment**: 11 pages, journal paper
- **Journal**: None
- **Summary**: With the improvement of computer performance and the increase of data volume, the object detection based on convolutional neural network (CNN) has become the main algorithm for object detection. This paper summarizes the research progress of convolutional neural networks and their applications in object detection, and focuses on analyzing and discussing a specific idea and method of applying convolutional neural networks for object detection, pointing out the current deficiencies and future development direction.



### Reconstructing NBA Players
- **Arxiv ID**: http://arxiv.org/abs/2007.13303v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.13303v1)
- **Published**: 2020-07-27 04:09:53+00:00
- **Updated**: 2020-07-27 04:09:53+00:00
- **Authors**: Luyang Zhu, Konstantinos Rematas, Brian Curless, Steve Seitz, Ira Kemelmacher-Shlizerman
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: Great progress has been made in 3D body pose and shape estimation from a single photo. Yet, state-of-the-art results still suffer from errors due to challenging body poses, modeling clothing, and self occlusions. The domain of basketball games is particularly challenging, as it exhibits all of these challenges. In this paper, we introduce a new approach for reconstruction of basketball players that outperforms the state-of-the-art. Key to our approach is a new method for creating poseable, skinned models of NBA players, and a large database of meshes (derived from the NBA2K19 video game), that we are releasing to the research community. Based on these models, we introduce a new method that takes as input a single photo of a clothed player in any basketball pose and outputs a high resolution mesh and 3D pose for that player. We demonstrate substantial improvement over state-of-the-art, single-image methods for body shape reconstruction.



### K-Shot Contrastive Learning of Visual Features with Multiple Instance Augmentations
- **Arxiv ID**: http://arxiv.org/abs/2007.13310v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.13310v2)
- **Published**: 2020-07-27 04:56:41+00:00
- **Updated**: 2021-02-01 08:00:58+00:00
- **Authors**: Haohang Xu, Hongkai Xiong, Guo-Jun Qi
- **Comment**: None
- **Journal**: IEEE Transactions on Pattern Analysis and Machine Intelligence,
  2021
- **Summary**: In this paper, we propose the $K$-Shot Contrastive Learning (KSCL) of visual features by applying multiple augmentations to investigate the sample variations within individual instances. It aims to combine the advantages of inter-instance discrimination by learning discriminative features to distinguish between different instances, as well as intra-instance variations by matching queries against the variants of augmented samples over instances. Particularly, for each instance, it constructs an instance subspace to model the configuration of how the significant factors of variations in $K$-shot augmentations can be combined to form the variants of augmentations. Given a query, the most relevant variant of instances is then retrieved by projecting the query onto their subspaces to predict the positive instance class. This generalizes the existing contrastive learning that can be viewed as a special one-shot case. An eigenvalue decomposition is performed to configure instance subspaces, and the embedding network can be trained end-to-end through the differentiable subspace configuration. Experiment results demonstrate the proposed $K$-shot contrastive learning achieves superior performances to the state-of-the-art unsupervised methods.



### Split Computing for Complex Object Detectors: Challenges and Preliminary Results
- **Arxiv ID**: http://arxiv.org/abs/2007.13312v2
- **DOI**: 10.1145/3410338.3412338
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.13312v2)
- **Published**: 2020-07-27 05:03:37+00:00
- **Updated**: 2020-07-30 05:14:59+00:00
- **Authors**: Yoshitomo Matsubara, Marco Levorato
- **Comment**: Accepted to EMDL '20 (4th International Workshop on Embedded and
  Mobile Deep Learning) co-located with ACM MobiCom 2020
- **Journal**: Proceedings of the 4th International Workshop on Embedded and
  Mobile Deep Learning (EMDL'20) (2020). Association for Computing Machinery,
  New York, NY, USA, 7-12
- **Summary**: Following the trends of mobile and edge computing for DNN models, an intermediate option, split computing, has been attracting attentions from the research community. Previous studies empirically showed that while mobile and edge computing often would be the best options in terms of total inference time, there are some scenarios where split computing methods can achieve shorter inference time. All the proposed split computing approaches, however, focus on image classification tasks, and most are assessed with small datasets that are far from the practical scenarios. In this paper, we discuss the challenges in developing split computing methods for powerful R-CNN object detectors trained on a large dataset, COCO 2017. We extensively analyze the object detectors in terms of layer-wise tensor size and model size, and show that naive split computing methods would not reduce inference time. To the best of our knowledge, this is the first study to inject small bottlenecks to such object detectors and unveil the potential of a split computing approach. The source code and trained models' weights used in this study are available at https://github.com/yoshitomo-matsubara/hnd-ghnd-object-detectors .



### Rethinking Generative Zero-Shot Learning: An Ensemble Learning Perspective for Recognising Visual Patches
- **Arxiv ID**: http://arxiv.org/abs/2007.13314v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2007.13314v3)
- **Published**: 2020-07-27 05:49:44+00:00
- **Updated**: 2020-08-07 01:14:32+00:00
- **Authors**: Zhi Chen, Sen Wang, Jingjing Li, Zi Huang
- **Comment**: ACM MM 2020
- **Journal**: None
- **Summary**: Zero-shot learning (ZSL) is commonly used to address the very pervasive problem of predicting unseen classes in fine-grained image classification and other tasks. One family of solutions is to learn synthesised unseen visual samples produced by generative models from auxiliary semantic information, such as natural language descriptions. However, for most of these models, performance suffers from noise in the form of irrelevant image backgrounds. Further, most methods do not allocate a calculated weight to each semantic patch. Yet, in the real world, the discriminative power of features can be quantified and directly leveraged to improve accuracy and reduce computational complexity. To address these issues, we propose a novel framework called multi-patch generative adversarial nets (MPGAN) that synthesises local patch features and labels unseen classes with a novel weighted voting strategy. The process begins by generating discriminative visual features from noisy text descriptions for a set of predefined local patches using multiple specialist generative models. The features synthesised from each patch for unseen classes are then used to construct an ensemble of diverse supervised classifiers, each corresponding to one local patch. A voting strategy averages the probability distributions output from the classifiers and, given that some patches are more discriminative than others, a discrimination-based attention mechanism helps to weight each patch accordingly. Extensive experiments show that MPGAN has significantly greater accuracy than state-of-the-art methods.



### Few-shot Knowledge Transfer for Fine-grained Cartoon Face Generation
- **Arxiv ID**: http://arxiv.org/abs/2007.13332v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.13332v1)
- **Published**: 2020-07-27 07:13:10+00:00
- **Updated**: 2020-07-27 07:13:10+00:00
- **Authors**: Nan Zhuang, Cheng Yang
- **Comment**: Technical Report
- **Journal**: None
- **Summary**: In this paper, we are interested in generating fine-grained cartoon faces for various groups. We assume that one of these groups consists of sufficient training data while the others only contain few samples. Although the cartoon faces of these groups share similar style, the appearances in various groups could still have some specific characteristics, which makes them differ from each other. A major challenge of this task is how to transfer knowledge among groups and learn group-specific characteristics with only few samples. In order to solve this problem, we propose a two-stage training process. First, a basic translation model for the basic group (which consists of sufficient data) is trained. Then, given new samples of other groups, we extend the basic model by creating group-specific branches for each new group. Group-specific branches are updated directly to capture specific appearances for each group while the remaining group-shared parameters are updated indirectly to maintain the distribution of intermediate feature space. In this manner, our approach is capable to generate high-quality cartoon faces for various groups.



### Self-Prediction for Joint Instance and Semantic Segmentation of Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2007.13344v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2007.13344v1)
- **Published**: 2020-07-27 07:58:00+00:00
- **Updated**: 2020-07-27 07:58:00+00:00
- **Authors**: Jinxian Liu, Minghui Yu, Bingbing Ni, Ye Chen
- **Comment**: Accepted to ECCV 2020
- **Journal**: None
- **Summary**: We develop a novel learning scheme named Self-Prediction for 3D instance and semantic segmentation of point clouds. Distinct from most existing methods that focus on designing convolutional operators, our method designs a new learning scheme to enhance point relation exploring for better segmentation. More specifically, we divide a point cloud sample into two subsets and construct a complete graph based on their representations. Then we use label propagation algorithm to predict labels of one subset when given labels of the other subset. By training with this Self-Prediction task, the backbone network is constrained to fully explore relational context/geometric/shape information and learn more discriminative features for segmentation. Moreover, a general associated framework equipped with our Self-Prediction scheme is designed for enhancing instance and semantic segmentation simultaneously, where instance and semantic representations are combined to perform Self-Prediction. Through this way, instance and semantic segmentation are collaborated and mutually reinforced. Significant performance improvements on instance and semantic segmentation compared with baseline are achieved on S3DIS and ShapeNet. Our method achieves state-of-the-art instance segmentation results on S3DIS and comparable semantic segmentation results compared with state-of-the-arts on S3DIS and ShapeNet when we only take PointNet++ as the backbone network.



### Feature visualization of Raman spectrum analysis with deep convolutional neural network
- **Arxiv ID**: http://arxiv.org/abs/2007.13354v1
- **DOI**: 10.1016/j.aca.2019.08.064
- **Categories**: **cs.CV**, eess.IV, physics.chem-ph
- **Links**: [PDF](http://arxiv.org/pdf/2007.13354v1)
- **Published**: 2020-07-27 08:15:38+00:00
- **Updated**: 2020-07-27 08:15:38+00:00
- **Authors**: Masashi Fukuhara, Kazuhiko Fujiwara, Yoshihiro Maruyama, Hiroyasu Itoh
- **Comment**: None
- **Journal**: Analytica Chimica Acta, Volume 1087, 9 December 2019, Pages 11-19
- **Summary**: We demonstrate a recognition and feature visualization method that uses a deep convolutional neural network for Raman spectrum analysis. The visualization is achieved by calculating important regions in the spectra from weights in pooling and fully-connected layers. The method is first examined for simple Lorentzian spectra, then applied to the spectra of pharmaceutical compounds and numerically mixed amino acids. We investigate the effects of the size and number of convolution filters on the extracted regions for Raman-peak signals using the Lorentzian spectra. It is confirmed that the Raman peak contributes to the recognition by visualizing the extracted features. A near-zero weight value is obtained at the background level region, which appears to be used for baseline correction. Common component extraction is confirmed by an evaluation of numerically mixed amino acid spectra. High weight values at the common peaks and negative values at the distinctive peaks appear, even though the model is given one-hot vectors as the training labels (without a mix ratio). This proposed method is potentially suitable for applications such as the validation of trained models, ensuring the reliability of common component extraction from compound samples for spectral analysis.



### Part-Aware Data Augmentation for 3D Object Detection in Point Cloud
- **Arxiv ID**: http://arxiv.org/abs/2007.13373v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.13373v2)
- **Published**: 2020-07-27 08:47:19+00:00
- **Updated**: 2021-07-11 05:35:31+00:00
- **Authors**: Jaeseok Choi, Yeji Song, Nojun Kwak
- **Comment**: This paper has been accepted by IROS 2021
- **Journal**: None
- **Summary**: Data augmentation has greatly contributed to improving the performance in image recognition tasks, and a lot of related studies have been conducted. However, data augmentation on 3D point cloud data has not been much explored. 3D label has more sophisticated and rich structural information than the 2D label, so it enables more diverse and effective data augmentation. In this paper, we propose part-aware data augmentation (PA-AUG) that can better utilize rich information of 3D label to enhance the performance of 3D object detectors. PA-AUG divides objects into partitions and stochastically applies five augmentation methods to each local region. It is compatible with existing point cloud data augmentation methods and can be used universally regardless of the detector's architecture. PA-AUG has improved the performance of state-of-the-art 3D object detector for all classes of the KITTI dataset and has the equivalent effect of increasing the train data by about 2.5$\times$. We also show that PA-AUG not only increases performance for a given dataset but also is robust to corrupted data. The code is available at https://github.com/sky77764/pa-aug.pytorch



### Decomposing Generation Networks with Structure Prediction for Recipe Generation
- **Arxiv ID**: http://arxiv.org/abs/2007.13374v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.13374v2)
- **Published**: 2020-07-27 08:47:50+00:00
- **Updated**: 2022-02-16 07:03:38+00:00
- **Authors**: Hao Wang, Guosheng Lin, Steven C. H. Hoi, Chunyan Miao
- **Comment**: Accepted at Pattern Recognition
- **Journal**: None
- **Summary**: Recipe generation from food images and ingredients is a challenging task, which requires the interpretation of the information from another modality. Different from the image captioning task, where the captions usually have one sentence, cooking instructions contain multiple sentences and have obvious structures. To help the model capture the recipe structure and avoid missing some cooking details, we propose a novel framework: Decomposing Generation Networks (DGN) with structure prediction, to get more structured and complete recipe generation outputs. Specifically, we split each cooking instruction into several phases, and assign different sub-generators to each phase. Our approach includes two novel ideas: (i) learning the recipe structures with the global structure prediction component and (ii) producing recipe phases in the sub-generator output component based on the predicted structure. Extensive experiments on the challenging large-scale Recipe1M dataset validate the effectiveness of our proposed model, which improves the performance over the state-of-the-art results.



### NOH-NMS: Improving Pedestrian Detection by Nearby Objects Hallucination
- **Arxiv ID**: http://arxiv.org/abs/2007.13376v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.13376v1)
- **Published**: 2020-07-27 08:51:55+00:00
- **Updated**: 2020-07-27 08:51:55+00:00
- **Authors**: Penghao Zhou, Chong Zhou, Pai Peng, Junlong Du, Xing Sun, Xiaowei Guo, Feiyue Huang
- **Comment**: Accepted at the ACM International Conference on Multimedia (ACM MM)
  2020
- **Journal**: None
- **Summary**: Greedy-NMS inherently raises a dilemma, where a lower NMS threshold will potentially lead to a lower recall rate and a higher threshold introduces more false positives. This problem is more severe in pedestrian detection because the instance density varies more intensively. However, previous works on NMS don't consider or vaguely consider the factor of the existent of nearby pedestrians. Thus, we propose Nearby Objects Hallucinator (NOH), which pinpoints the objects nearby each proposal with a Gaussian distribution, together with NOH-NMS, which dynamically eases the suppression for the space that might contain other objects with a high likelihood. Compared to Greedy-NMS, our method, as the state-of-the-art, improves by $3.9\%$ AP, $5.1\%$ Recall, and $0.8\%$ $\text{MR}^{-2}$ on CrowdHuman to $89.0\%$ AP and $92.9\%$ Recall, and $43.9\%$ $\text{MR}^{-2}$ respectively.



### ALF: Autoencoder-based Low-rank Filter-sharing for Efficient Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2007.13384v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.13384v1)
- **Published**: 2020-07-27 09:01:22+00:00
- **Updated**: 2020-07-27 09:01:22+00:00
- **Authors**: Alexander Frickenstein, Manoj-Rohit Vemparala, Nael Fasfous, Laura Hauenschild, Naveen-Shankar Nagaraja, Christian Unger, Walter Stechele
- **Comment**: Accepted by DAC'20
- **Journal**: None
- **Summary**: Closing the gap between the hardware requirements of state-of-the-art convolutional neural networks and the limited resources constraining embedded applications is the next big challenge in deep learning research. The computational complexity and memory footprint of such neural networks are typically daunting for deployment in resource constrained environments. Model compression techniques, such as pruning, are emphasized among other optimization methods for solving this problem. Most existing techniques require domain expertise or result in irregular sparse representations, which increase the burden of deploying deep learning applications on embedded hardware accelerators. In this paper, we propose the autoencoder-based low-rank filter-sharing technique technique (ALF). When applied to various networks, ALF is compared to state-of-the-art pruning methods, demonstrating its efficient compression capabilities on theoretical metrics as well as on an accurate, deterministic hardware-model. In our experiments, ALF showed a reduction of 70\% in network parameters, 61\% in operations and 41\% in execution time, with minimal loss in accuracy.



### Ladybird: Quasi-Monte Carlo Sampling for Deep Implicit Field Based 3D Reconstruction with Symmetry
- **Arxiv ID**: http://arxiv.org/abs/2007.13393v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.13393v1)
- **Published**: 2020-07-27 09:17:00+00:00
- **Updated**: 2020-07-27 09:17:00+00:00
- **Authors**: Yifan Xu, Tianqi Fan, Yi Yuan, Gurprit Singh
- **Comment**: European Conference on Computer Vision 2020 (ECCV 2020)
- **Journal**: None
- **Summary**: Deep implicit field regression methods are effective for 3D reconstruction from single-view images. However, the impact of different sampling patterns on the reconstruction quality is not well-understood. In this work, we first study the effect of point set discrepancy on the network training. Based on Farthest Point Sampling algorithm, we propose a sampling scheme that theoretically encourages better generalization performance, and results in fast convergence for SGD-based optimization algorithms. Secondly, based on the reflective symmetry of an object, we propose a feature fusion method that alleviates issues due to self-occlusions which makes it difficult to utilize local image features. Our proposed system Ladybird is able to create high quality 3D object reconstructions from a single input image. We evaluate Ladybird on a large scale 3D dataset (ShapeNet) demonstrating highly competitive results in terms of Chamfer distance, Earth Mover's distance and Intersection Over Union (IoU).



### YOLOpeds: Efficient Real-Time Single-Shot Pedestrian Detection for Smart Camera Applications
- **Arxiv ID**: http://arxiv.org/abs/2007.13404v2
- **DOI**: 10.1049/iet-cvi.2019.0897
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.13404v2)
- **Published**: 2020-07-27 09:50:11+00:00
- **Updated**: 2020-10-29 16:23:12+00:00
- **Authors**: Christos Kyrkou
- **Comment**: None
- **Journal**: None
- **Summary**: Deep Learning-based object detectors can enhance the capabilities of smart camera systems in a wide spectrum of machine vision applications including video surveillance, autonomous driving, robots and drones, smart factory, and health monitoring. Pedestrian detection plays a key role in all these applications and deep learning can be used to construct accurate state-of-the-art detectors. However, such complex paradigms do not scale easily and are not traditionally implemented in resource-constrained smart cameras for on-device processing which offers significant advantages in situations when real-time monitoring and robustness are vital. Efficient neural networks can not only enable mobile applications and on-device experiences but can also be a key enabler of privacy and security allowing a user to gain the benefits of neural networks without needing to send their data to the server to be evaluated. This work addresses the challenge of achieving a good trade-off between accuracy and speed for efficient deployment of deep-learning-based pedestrian detection in smart camera applications. A computationally efficient architecture is introduced based on separable convolutions and proposes integrating dense connections across layers and multi-scale feature fusion to improve representational capacity while decreasing the number of parameters and operations. In particular, the contributions of this work are the following: 1) An efficient backbone combining multi-scale feature operations, 2) a more elaborate loss function for improved localization, 3) an anchor-less approach for detection, The proposed approach called YOLOpeds is evaluated using the PETS2009 surveillance dataset on 320x320 images. Overall, YOLOpeds provides real-time sustained operation of over 30 frames per second with detection rates in the range of 86% outperforming existing deep learning models.



### Contraction Mapping of Feature Norms for Classifier Learning on the Data with Different Quality
- **Arxiv ID**: http://arxiv.org/abs/2007.13406v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.13406v2)
- **Published**: 2020-07-27 09:53:55+00:00
- **Updated**: 2020-07-28 01:07:53+00:00
- **Authors**: Weihua Liu, Xiabi Liu, Murong Wang, Ling Ma
- **Comment**: None
- **Journal**: None
- **Summary**: The popular softmax loss and its recent extensions have achieved great success in the deep learning-based image classification. However, the data for training image classifiers usually has different quality. Ignoring such problem, the correct classification of low quality data is hard to be solved. In this paper, we discover the positive correlation between the feature norm of an image and its quality through careful experiments on various applications and various deep neural networks. Based on this finding, we propose a contraction mapping function to compress the range of feature norms of training images according to their quality and embed this contraction mapping function into softmax loss or its extensions to produce novel learning objectives. The experiments on various classification applications, including handwritten digit recognition, lung nodule classification, face verification and face recognition, demonstrate that the proposed approach is promising to effectively deal with the problem of learning on the data with different quality and leads to the significant and stable improvements in the classification accuracy.



### XCAT-GAN for Synthesizing 3D Consistent Labeled Cardiac MR Images on Anatomically Variable XCAT Phantoms
- **Arxiv ID**: http://arxiv.org/abs/2007.13408v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.13408v2)
- **Published**: 2020-07-27 10:05:04+00:00
- **Updated**: 2020-07-31 14:27:59+00:00
- **Authors**: Sina Amirrajab, Samaneh Abbasi-Sureshjani, Yasmina Al Khalil, Cristian Lorenz, Juergen Weese, Josien Pluim, Marcel Breeuwer
- **Comment**: Accepted for MICCAI 2020
- **Journal**: None
- **Summary**: Generative adversarial networks (GANs) have provided promising data enrichment solutions by synthesizing high-fidelity images. However, generating large sets of labeled images with new anatomical variations remains unexplored. We propose a novel method for synthesizing cardiac magnetic resonance (CMR) images on a population of virtual subjects with a large anatomical variation, introduced using the 4D eXtended Cardiac and Torso (XCAT) computerized human phantom. We investigate two conditional image synthesis approaches grounded on a semantically-consistent mask-guided image generation technique: 4-class and 8-class XCAT-GANs. The 4-class technique relies on only the annotations of the heart; while the 8-class technique employs a predicted multi-tissue label map of the heart-surrounding organs and provides better guidance for our conditional image synthesis. For both techniques, we train our conditional XCAT-GAN with real images paired with corresponding labels and subsequently at the inference time, we substitute the labels with the XCAT derived ones. Therefore, the trained network accurately transfers the tissue-specific textures to the new label maps. By creating 33 virtual subjects of synthetic CMR images at the end-diastolic and end-systolic phases, we evaluate the usefulness of such data in the downstream cardiac cavity segmentation task under different augmentation strategies. Results demonstrate that even with only 20% of real images (40 volumes) seen during training, segmentation performance is retained with the addition of synthetic CMR images. Moreover, the improvement in utilizing synthetic images for augmenting the real data is evident through the reduction of Hausdorff distance up to 28% and an increase in the Dice score up to 5%, indicating a higher similarity to the ground truth in all dimensions.



### Image-driven discriminative and generative machine learning algorithms for establishing microstructure-processing relationships
- **Arxiv ID**: http://arxiv.org/abs/2007.13417v1
- **DOI**: 10.1063/5.0013720
- **Categories**: **physics.app-ph**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.13417v1)
- **Published**: 2020-07-27 10:36:18+00:00
- **Updated**: 2020-07-27 10:36:18+00:00
- **Authors**: Wufei Ma, Elizabeth Kautz, Arun Baskaran, Aritra Chowdhury, Vineet Joshi, Bülent Yener, Daniel Lewis
- **Comment**: 14 pages, 15 figures
- **Journal**: None
- **Summary**: We investigate methods of microstructure representation for the purpose of predicting processing condition from microstructure image data. A binary alloy (uranium-molybdenum) that is currently under development as a nuclear fuel was studied for the purpose of developing an improved machine learning approach to image recognition, characterization, and building predictive capabilities linking microstructure to processing conditions. Here, we test different microstructure representations and evaluate model performance based on the F1 score. A F1 score of 95.1% was achieved for distinguishing between micrographs corresponding to ten different thermo-mechanical material processing conditions. We find that our newly developed microstructure representation describes image data well, and the traditional approach of utilizing area fractions of different phases is insufficient for distinguishing between multiple classes using a relatively small, imbalanced original data set of 272 images. To explore the applicability of generative methods for supplementing such limited data sets, generative adversarial networks were trained to generate artificial microstructure images. Two different generative networks were trained and tested to assess performance. Challenges and best practices associated with applying machine learning to limited microstructure image data sets is also discussed. Our work has implications for quantitative microstructure analysis, and development of microstructure-processing relationships in limited data sets typical of metallurgical process design studies.



### Two-Level Residual Distillation based Triple Network for Incremental Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2007.13428v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.13428v1)
- **Published**: 2020-07-27 11:04:57+00:00
- **Updated**: 2020-07-27 11:04:57+00:00
- **Authors**: Dongbao Yang, Yu Zhou, Dayan Wu, Can Ma, Fei Yang, Weiping Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Modern object detection methods based on convolutional neural network suffer from severe catastrophic forgetting in learning new classes without original data. Due to time consumption, storage burden and privacy of old data, it is inadvisable to train the model from scratch with both old and new data when new object classes emerge after the model trained. In this paper, we propose a novel incremental object detector based on Faster R-CNN to continuously learn from new object classes without using old data. It is a triple network where an old model and a residual model as assistants for helping the incremental model learning on new classes without forgetting the previous learned knowledge. To better maintain the discrimination of features between old and new classes, the residual model is jointly trained on new classes in the incremental learning procedure. In addition, a corresponding distillation scheme is designed to guide the training process, which consists of a two-level residual distillation loss and a joint classification distillation loss. Extensive experiments on VOC2007 and COCO are conducted, and the results demonstrate that the proposed method can effectively learn to incrementally detect objects of new classes, and the problem of catastrophic forgetting is mitigated in this context.



### Identity-Guided Human Semantic Parsing for Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2007.13467v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.13467v1)
- **Published**: 2020-07-27 12:12:27+00:00
- **Updated**: 2020-07-27 12:12:27+00:00
- **Authors**: Kuan Zhu, Haiyun Guo, Zhiwei Liu, Ming Tang, Jinqiao Wang
- **Comment**: Accepted by ECCV 2020 spotlight
- **Journal**: None
- **Summary**: Existing alignment-based methods have to employ the pretrained human parsing models to achieve the pixel-level alignment, and cannot identify the personal belongings (e.g., backpacks and reticule) which are crucial to person re-ID. In this paper, we propose the identity-guided human semantic parsing approach (ISP) to locate both the human body parts and personal belongings at pixel-level for aligned person re-ID only with person identity labels. We design the cascaded clustering on feature maps to generate the pseudo-labels of human parts. Specifically, for the pixels of all images of a person, we first group them to foreground or background and then group the foreground pixels to human parts. The cluster assignments are subsequently used as pseudo-labels of human parts to supervise the part estimation and ISP iteratively learns the feature maps and groups them. Finally, local features of both human body parts and personal belongings are obtained according to the selflearned part estimation, and only features of visible parts are utilized for the retrieval. Extensive experiments on three widely used datasets validate the superiority of ISP over lots of state-of-the-art methods. Our code is available at https://github.com/CASIA-IVA-Lab/ISP-reID.



### The Effect of Wearing a Mask on Face Recognition Performance: an Exploratory Study
- **Arxiv ID**: http://arxiv.org/abs/2007.13521v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.13521v2)
- **Published**: 2020-07-27 13:03:32+00:00
- **Updated**: 2020-08-20 18:57:25+00:00
- **Authors**: Naser Damer, Jonas Henry Grebe, Cong Chen, Fadi Boutros, Florian Kirchbuchner, Arjan Kuijper
- **Comment**: Accepted at BIOSIG2020
- **Journal**: None
- **Summary**: Face recognition has become essential in our daily lives as a convenient and contactless method of accurate identity verification. Process such as identity verification at automatic border control gates or the secure login to electronic devices are increasingly dependant on such technologies. The recent COVID-19 pandemic have increased the value of hygienic and contactless identity verification. However, the pandemic led to the wide use of face masks, essential to keep the pandemic under control. The effect of wearing a mask on face recognition in a collaborative environment is currently sensitive yet understudied issue. We address that by presenting a specifically collected database containing three session, each with three different capture instructions, to simulate realistic use cases. We further study the effect of masked face probes on the behaviour of three top-performing face recognition systems, two academic solutions and one commercial off-the-shelf (COTS) system.



### Reconstruction Regularized Deep Metric Learning for Multi-label Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2007.13547v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.13547v1)
- **Published**: 2020-07-27 13:28:50+00:00
- **Updated**: 2020-07-27 13:28:50+00:00
- **Authors**: Changsheng Li, Chong Liu, Lixin Duan, Peng Gao, Kai Zheng
- **Comment**: Accepted by IEEE TNNLS
- **Journal**: None
- **Summary**: In this paper, we present a novel deep metric learning method to tackle the multi-label image classification problem. In order to better learn the correlations among images features, as well as labels, we attempt to explore a latent space, where images and labels are embedded via two unique deep neural networks, respectively. To capture the relationships between image features and labels, we aim to learn a \emph{two-way} deep distance metric over the embedding space from two different views, i.e., the distance between one image and its labels is not only smaller than those distances between the image and its labels' nearest neighbors, but also smaller than the distances between the labels and other images corresponding to the labels' nearest neighbors. Moreover, a reconstruction module for recovering correct labels is incorporated into the whole framework as a regularization term, such that the label embedding space is more representative. Our model can be trained in an end-to-end manner. Experimental results on publicly available image datasets corroborate the efficacy of our method compared with the state-of-the-arts.



### Differentiable Manifold Reconstruction for Point Cloud Denoising
- **Arxiv ID**: http://arxiv.org/abs/2007.13551v2
- **DOI**: 10.1145/3394171.3413727
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.13551v2)
- **Published**: 2020-07-27 13:31:41+00:00
- **Updated**: 2020-08-09 09:23:44+00:00
- **Authors**: Shitong Luo, Wei Hu
- **Comment**: This work has been accepted to ACM MM 2020
- **Journal**: None
- **Summary**: 3D point clouds are often perturbed by noise due to the inherent limitation of acquisition equipments, which obstructs downstream tasks such as surface reconstruction, rendering and so on. Previous works mostly infer the displacement of noisy points from the underlying surface, which however are not designated to recover the surface explicitly and may lead to sub-optimal denoising results. To this end, we propose to learn the underlying manifold of a noisy point cloud from differentiably subsampled points with trivial noise perturbation and their embedded neighborhood feature, aiming to capture intrinsic structures in point clouds. Specifically, we present an autoencoder-like neural network. The encoder learns both local and non-local feature representations of each point, and then samples points with low noise via an adaptive differentiable pooling operation. Afterwards, the decoder infers the underlying manifold by transforming each sampled point along with the embedded feature of its neighborhood to a local surface centered around the point. By resampling on the reconstructed manifold, we obtain a denoised point cloud. Further, we design an unsupervised training loss, so that our network can be trained in either an unsupervised or supervised fashion. Experiments show that our method significantly outperforms state-of-the-art denoising methods under both synthetic noise and real world noise. The code and data are available at https://github.com/luost26/DMRDenoise



### Towards Accuracy-Fairness Paradox: Adversarial Example-based Data Augmentation for Visual Debiasing
- **Arxiv ID**: http://arxiv.org/abs/2007.13632v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.13632v2)
- **Published**: 2020-07-27 15:17:52+00:00
- **Updated**: 2020-08-13 08:29:49+00:00
- **Authors**: Yi Zhang, Jitao Sang
- **Comment**: None
- **Journal**: None
- **Summary**: Machine learning fairness concerns about the biases towards certain protected or sensitive group of people when addressing the target tasks. This paper studies the debiasing problem in the context of image classification tasks. Our data analysis on facial attribute recognition demonstrates (1) the attribution of model bias from imbalanced training data distribution and (2) the potential of adversarial examples in balancing data distribution. We are thus motivated to employ adversarial example to augment the training data for visual debiasing. Specifically, to ensure the adversarial generalization as well as cross-task transferability, we propose to couple the operations of target task classifier training, bias task classifier training, and adversarial example generation. The generated adversarial examples supplement the target task training dataset via balancing the distribution over bias variables in an online fashion. Results on simulated and real-world debiasing experiments demonstrate the effectiveness of the proposed solution in simultaneously improving model accuracy and fairness. Preliminary experiment on few-shot learning further shows the potential of adversarial attack-based pseudo sample generation as alternative solution to make up for the training data lackage.



### Black-Box Face Recovery from Identity Features
- **Arxiv ID**: http://arxiv.org/abs/2007.13635v3
- **DOI**: 10.1007/978-3-030-68238-5_34
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.13635v3)
- **Published**: 2020-07-27 15:25:38+00:00
- **Updated**: 2020-07-30 13:24:39+00:00
- **Authors**: Anton Razzhigaev, Klim Kireev, Edgar Kaziakhmedov, Nurislam Tursynbek, Aleksandr Petiushko
- **Comment**: None
- **Journal**: ECCV Workshops (5) 2020: 462-475
- **Summary**: In this work, we present a novel algorithm based on an it-erative sampling of random Gaussian blobs for black-box face recovery, given only an output feature vector of deep face recognition systems. We attack the state-of-the-art face recognition system (ArcFace) to test our algorithm. Another network with different architecture (FaceNet) is used as an independent critic showing that the target person can be identified with the reconstructed image even with no access to the attacked model. Furthermore, our algorithm requires a significantly less number of queries compared to the state-of-the-art solution.



### Pre-training for Video Captioning Challenge 2020 Summary
- **Arxiv ID**: http://arxiv.org/abs/2008.00947v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2008.00947v1)
- **Published**: 2020-07-27 15:31:27+00:00
- **Updated**: 2020-07-27 15:31:27+00:00
- **Authors**: Yingwei Pan, Jun Xu, Yehao Li, Ting Yao, Tao Mei
- **Comment**: None
- **Journal**: None
- **Summary**: The Pre-training for Video Captioning Challenge 2020 Summary: results and challenge participants' technical reports.



### Message Passing Least Squares Framework and its Application to Rotation Synchronization
- **Arxiv ID**: http://arxiv.org/abs/2007.13638v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.IT, math.IT, stat.ML, 90C26, 90C17, 68Q87, 65C20, 90-08, 60-08, G.1.6; I.4.0
- **Links**: [PDF](http://arxiv.org/pdf/2007.13638v3)
- **Published**: 2020-07-27 15:39:19+00:00
- **Updated**: 2020-08-15 02:00:34+00:00
- **Authors**: Yunpeng Shi, Gilad Lerman
- **Comment**: To Appear in ICML 2020 Proceedings
- **Journal**: International Conference on Machine Learning, 8796-8806 (2020)
- **Summary**: We propose an efficient algorithm for solving group synchronization under high levels of corruption and noise, while we focus on rotation synchronization. We first describe our recent theoretically guaranteed message passing algorithm that estimates the corruption levels of the measured group ratios. We then propose a novel reweighted least squares method to estimate the group elements, where the weights are initialized and iteratively updated using the estimated corruption levels. We demonstrate the superior performance of our algorithm over state-of-the-art methods for rotation synchronization using both synthetic and real data.



### Solving Linear Inverse Problems Using the Prior Implicit in a Denoiser
- **Arxiv ID**: http://arxiv.org/abs/2007.13640v3
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.13640v3)
- **Published**: 2020-07-27 15:40:46+00:00
- **Updated**: 2021-05-07 02:34:03+00:00
- **Authors**: Zahra Kadkhodaie, Eero P. Simoncelli
- **Comment**: 19 pages, 12 figures. Changes: more detailed description of
  relationships to previous literature, including empirical comparisons for
  super-resolution, debarring, and compressive sensing
- **Journal**: None
- **Summary**: Prior probability models are a fundamental component of many image processing problems, but density estimation is notoriously difficult for high-dimensional signals such as photographic images. Deep neural networks have provided state-of-the-art solutions for problems such as denoising, which implicitly rely on a prior probability model of natural images. Here, we develop a robust and general methodology for making use of this implicit prior. We rely on a statistical result due to Miyasawa (1961), who showed that the least-squares solution for removing additive Gaussian noise can be written directly in terms of the gradient of the log of the noisy signal density. We use this fact to develop a stochastic coarse-to-fine gradient ascent procedure for drawing high-probability samples from the implicit prior embedded within a CNN trained to perform blind (i.e., with unknown noise level) least-squares denoising. A generalization of this algorithm to constrained sampling provides a method for using the implicit prior to solve any linear inverse problem, with no additional training. We demonstrate this general form of transfer learning in multiple applications, using the same algorithm to produce state-of-the-art levels of unsupervised performance for deblurring, super-resolution, inpainting, and compressive sensing.



### Towards Learning Convolutions from Scratch
- **Arxiv ID**: http://arxiv.org/abs/2007.13657v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.13657v1)
- **Published**: 2020-07-27 16:13:13+00:00
- **Updated**: 2020-07-27 16:13:13+00:00
- **Authors**: Behnam Neyshabur
- **Comment**: 18 pages, 9 figures, 4 tables
- **Journal**: None
- **Summary**: Convolution is one of the most essential components of architectures used in computer vision. As machine learning moves towards reducing the expert bias and learning it from data, a natural next step seems to be learning convolution-like structures from scratch. This, however, has proven elusive. For example, current state-of-the-art architecture search algorithms use convolution as one of the existing modules rather than learning it from data. In an attempt to understand the inductive bias that gives rise to convolutions, we investigate minimum description length as a guiding principle and show that in some settings, it can indeed be indicative of the performance of architectures. To find architectures with small description length, we propose $\beta$-LASSO, a simple variant of LASSO algorithm that, when applied on fully-connected networks for image classification tasks, learns architectures with local connections and achieves state-of-the-art accuracies for training fully-connected nets on CIFAR-10 (85.19%), CIFAR-100 (59.56%) and SVHN (94.07%) bridging the gap between fully-connected and convolutional nets.



### 3D Human Shape and Pose from a Single Low-Resolution Image with Self-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2007.13666v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.13666v2)
- **Published**: 2020-07-27 16:19:52+00:00
- **Updated**: 2020-08-09 17:22:43+00:00
- **Authors**: Xiangyu Xu, Hao Chen, Francesc Moreno-Noguer, Laszlo A. Jeni, Fernando De la Torre
- **Comment**: ECCV 2020, project page:
  https://sites.google.com/view/xiangyuxu/3d_eccv20
- **Journal**: None
- **Summary**: 3D human shape and pose estimation from monocular images has been an active area of research in computer vision, having a substantial impact on the development of new applications, from activity recognition to creating virtual avatars. Existing deep learning methods for 3D human shape and pose estimation rely on relatively high-resolution input images; however, high-resolution visual content is not always available in several practical scenarios such as video surveillance and sports broadcasting. Low-resolution images in real scenarios can vary in a wide range of sizes, and a model trained in one resolution does not typically degrade gracefully across resolutions. Two common approaches to solve the problem of low-resolution input are applying super-resolution techniques to the input images which may result in visual artifacts, or simply training one model for each resolution, which is impractical in many realistic applications. To address the above issues, this paper proposes a novel algorithm called RSC-Net, which consists of a Resolution-aware network, a Self-supervision loss, and a Contrastive learning scheme. The proposed network is able to learn the 3D body shape and pose across different resolutions with a single model. The self-supervision loss encourages scale-consistency of the output, and the contrastive learning scheme enforces scale-consistency of the deep features. We show that both these new training losses provide robustness when learning 3D shape and pose in a weakly-supervised manner. Extensive experiments demonstrate that the RSC-Net can achieve consistently better results than the state-of-the-art methods for challenging low-resolution images.



### Ordinary Differential Equation and Complex Matrix Exponential for Multi-resolution Image Registration
- **Arxiv ID**: http://arxiv.org/abs/2007.13683v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.13683v1)
- **Published**: 2020-07-27 16:51:25+00:00
- **Updated**: 2020-07-27 16:51:25+00:00
- **Authors**: Abhishek Nan, Matthew Tennant, Uriel Rubin, Nilanjan Ray
- **Comment**: Software: https://github.com/abnan/ODECME
- **Journal**: None
- **Summary**: Autograd-based software packages have recently renewed interest in image registration using homography and other geometric models by gradient descent and optimization, e.g., AirLab and DRMIME. In this work, we emphasize on using complex matrix exponential (CME) over real matrix exponential to compute transformation matrices. CME is theoretically more suitable and practically provides faster convergence as our experiments show. Further, we demonstrate that the use of an ordinary differential equation (ODE) as an optimizable dynamical system can adapt the transformation matrix more accurately to the multi-resolution Gaussian pyramid for image registration. Our experiments include four publicly available benchmark datasets, two of them 2D and the other two being 3D. Experiments demonstrate that our proposed method yields significantly better registration compared to a number of off-the-shelf, popular, state-of-the-art image registration toolboxes.



### The MAMe Dataset: On the relevance of High Resolution and Variable Shape image properties
- **Arxiv ID**: http://arxiv.org/abs/2007.13693v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.13693v3)
- **Published**: 2020-07-27 17:13:14+00:00
- **Updated**: 2021-05-20 10:57:06+00:00
- **Authors**: Ferran Parés, Anna Arias-Duart, Dario Garcia-Gasulla, Gema Campo-Francés, Nina Viladrich, Eduard Ayguadé, Jesús Labarta
- **Comment**: None
- **Journal**: None
- **Summary**: In the image classification task, the most common approach is to resize all images in a dataset to a unique shape, while reducing their precision to a size which facilitates experimentation at scale. This practice has benefits from a computational perspective, but it entails negative side-effects on performance due to loss of information and image deformation. In this work we introduce the MAMe dataset, an image classification dataset with remarkable high resolution and variable shape properties. The goal of MAMe is to provide a tool for studying the impact of such properties in image classification, while motivating research in the field. The MAMe dataset contains thousands of artworks from three different museums, and proposes a classification task consisting on differentiating between 29 mediums (i.e. materials and techniques) supervised by art experts. After reviewing the singularity of MAMe in the context of current image classification tasks, a thorough description of the task is provided, together with dataset statistics. Experiments are conducted to evaluate the impact of using high resolution images, variable shape inputs and both properties at the same time. Results illustrate the positive impact in performance when using high resolution images, while highlighting the lack of solutions to exploit variable shapes. An additional experiment exposes the distinctiveness between the MAMe dataset and the prototypical ImageNet dataset. Finally, the baselines are inspected using explainability methods and expert knowledge, to gain insights on the challenges that remain ahead.



### Defining Traffic States using Spatio-temporal Traffic Graphs
- **Arxiv ID**: http://arxiv.org/abs/2008.00827v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2008.00827v1)
- **Published**: 2020-07-27 17:27:52+00:00
- **Updated**: 2020-07-27 17:27:52+00:00
- **Authors**: Debaditya Roy, K. Naveen Kumar, C. Krishna Mohan
- **Comment**: Accepted in 23rd IEEE International Conference on Intelligent
  Transportation Systems September 20 to 23, 2020. 6 pages, 6 figures
- **Journal**: None
- **Summary**: Intersections are one of the main sources of congestion and hence, it is important to understand traffic behavior at intersections. Particularly, in developing countries with high vehicle density, mixed traffic type, and lane-less driving behavior, it is difficult to distinguish between congested and normal traffic behavior. In this work, we propose a way to understand the traffic state of smaller spatial regions at intersections using traffic graphs. The way these traffic graphs evolve over time reveals different traffic states - a) a congestion is forming (clumping), the congestion is dispersing (unclumping), or c) the traffic is flowing normally (neutral). We train a spatio-temporal deep network to identify these changes. Also, we introduce a large dataset called EyeonTraffic (EoT) containing 3 hours of aerial videos collected at 3 busy intersections in Ahmedabad, India. Our experiments on the EoT dataset show that the traffic graphs can help in correctly identifying congestion-prone behavior in different spatial regions of an intersection.



### Deep learning Framework for Mobile Microscopy
- **Arxiv ID**: http://arxiv.org/abs/2007.13701v3
- **DOI**: 10.1109/ISBI48211.2021.9434133
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.13701v3)
- **Published**: 2020-07-27 17:27:59+00:00
- **Updated**: 2021-02-18 14:51:48+00:00
- **Authors**: Anatasiia Kornilova, Mikhail Salnikov, Olga Novitskaya, Maria Begicheva, Egor Sevriugov, Kirill Shcherbakov, Valeriya Pronina, Dmitry V. Dylov
- **Comment**: None
- **Journal**: None
- **Summary**: Mobile microscopy is a promising technology to assist and to accelerate disease diagnostics, with its widespread adoption being hindered by the mediocre quality of acquired images. Although some paired image translation and super-resolution approaches for mobile microscopy have emerged, a set of essential challenges, necessary for automating it in a high-throughput setting, still await to be addressed. The issues like in-focus/out-of-focus classification, fast scanning deblurring, focus-stacking, etc. -- all have specific peculiarities when the data are recorded using a mobile device. In this work, we aspire to create a comprehensive pipeline by connecting a set of methods purposely tuned to mobile microscopy: (1) a CNN model for stable in-focus / out-of-focus classification, (2) modified DeblurGAN architecture for image deblurring, (3) FuseGAN model for combining in-focus parts from multiple images to boost the detail. We discuss the limitations of the existing solutions developed for professional clinical microscopes, propose corresponding improvements, and compare to the other state-of-the-art mobile analytics solutions.



### WGANVO: Monocular Visual Odometry based on Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2007.13704v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2007.13704v1)
- **Published**: 2020-07-27 17:31:24+00:00
- **Updated**: 2020-07-27 17:31:24+00:00
- **Authors**: Javier Cremona, Lucas Uzal, Taihú Pire
- **Comment**: None
- **Journal**: None
- **Summary**: In this work we present WGANVO, a Deep Learning based monocular Visual Odometry method. In particular, a neural network is trained to regress a pose estimate from an image pair. The training is performed using a semi-supervised approach. Unlike geometry based monocular methods, the proposed method can recover the absolute scale of the scene without neither prior knowledge nor extra information. The evaluation of the system is carried out on the well-known KITTI dataset where it is shown to work in real time and the accuracy obtained is encouraging to continue the development of Deep Learning based methods.



### The Unsupervised Method of Vessel Movement Trajectory Prediction
- **Arxiv ID**: http://arxiv.org/abs/2007.13712v3
- **DOI**: None
- **Categories**: **cs.CV**, stat.AP
- **Links**: [PDF](http://arxiv.org/pdf/2007.13712v3)
- **Published**: 2020-07-27 17:45:21+00:00
- **Updated**: 2020-07-29 15:42:43+00:00
- **Authors**: Chih-Wei Chen, Charles Harrison, Hsin-Hsiung Huang
- **Comment**: None
- **Journal**: None
- **Summary**: In real-world application scenarios, it is crucial for marine navigators and security analysts to predict vessel movement trajectories at sea based on the Automated Identification System (AIS) data in a given time span. This article presents an unsupervised method of ship movement trajectory prediction which represents the data in a three-dimensional space which consists of time difference between points, the scaled error distance between the tested and its predicted forward and backward locations, and the space-time angle. The representation feature space reduces the search scope for the next point to a collection of candidates which fit the local path prediction well, and therefore improve the accuracy. Unlike most statistical learning or deep learning methods, the proposed clustering-based trajectory reconstruction method does not require computationally expensive model training. This makes real-time reliable and accurate prediction feasible without using a training set. Our results show that the most prediction trajectories accurately consist of the true vessel paths.



### Point Cloud Based Reinforcement Learning for Sim-to-Real and Partial Observability in Visual Navigation
- **Arxiv ID**: http://arxiv.org/abs/2007.13715v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.13715v1)
- **Published**: 2020-07-27 17:46:59+00:00
- **Updated**: 2020-07-27 17:46:59+00:00
- **Authors**: Kenzo Lobos-Tsunekawa, Tatsuya Harada
- **Comment**: Accepted to IROS'2020
- **Journal**: None
- **Summary**: Reinforcement Learning (RL), among other learning-based methods, represents powerful tools to solve complex robotic tasks (e.g., actuation, manipulation, navigation, etc.), with the need for real-world data to train these systems as one of its most important limitations. The use of simulators is one way to address this issue, yet knowledge acquired in simulations does not work directly in the real-world, which is known as the sim-to-real transfer problem. While previous works focus on the nature of the images used as observations (e.g., textures and lighting), which has proven useful for a sim-to-sim transfer, they neglect other concerns regarding said observations, such as precise geometrical meanings, failing at robot-to-robot, and thus in sim-to-real transfers. We propose a method that learns on an observation space constructed by point clouds and environment randomization, generalizing among robots and simulators to achieve sim-to-real, while also addressing partial observability. We demonstrate the benefits of our methodology on the point goal navigation task, in which our method proves to be highly unaffected to unseen scenarios produced by robot-to-robot transfer, outperforms image-based baselines in robot-randomized experiments, and presents high performances in sim-to-sim conditions. Finally, we perform several experiments to validate the sim-to-real transfer to a physical domestic robot platform, confirming the out-of-the-box performance of our system.



### Associative3D: Volumetric Reconstruction from Sparse Views
- **Arxiv ID**: http://arxiv.org/abs/2007.13727v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.13727v1)
- **Published**: 2020-07-27 17:58:53+00:00
- **Updated**: 2020-07-27 17:58:53+00:00
- **Authors**: Shengyi Qian, Linyi Jin, David F. Fouhey
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: This paper studies the problem of 3D volumetric reconstruction from two views of a scene with an unknown camera. While seemingly easy for humans, this problem poses many challenges for computers since it requires simultaneously reconstructing objects in the two views while also figuring out their relationship. We propose a new approach that estimates reconstructions, distributions over the camera/object and camera/camera transformations, as well as an inter-view object affinity matrix. This information is then jointly reasoned over to produce the most likely explanation of the scene. We train and test our approach on a dataset of indoor scenes, and rigorously evaluate the merits of our joint reasoning approach. Our experiments show that it is able to recover reasonable scenes from sparse views, while the problem is still challenging. Project site: https://jasonqsy.github.io/Associative3D



### Noisy Agents: Self-supervised Exploration by Predicting Auditory Events
- **Arxiv ID**: http://arxiv.org/abs/2007.13729v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2007.13729v1)
- **Published**: 2020-07-27 17:59:08+00:00
- **Updated**: 2020-07-27 17:59:08+00:00
- **Authors**: Chuang Gan, Xiaoyu Chen, Phillip Isola, Antonio Torralba, Joshua B. Tenenbaum
- **Comment**: Project page: http://noisy-agent.csail.mit.edu
- **Journal**: None
- **Summary**: Humans integrate multiple sensory modalities (e.g. visual and audio) to build a causal understanding of the physical world. In this work, we propose a novel type of intrinsic motivation for Reinforcement Learning (RL) that encourages the agent to understand the causal effect of its actions through auditory event prediction. First, we allow the agent to collect a small amount of acoustic data and use K-means to discover underlying auditory event clusters. We then train a neural network to predict the auditory events and use the prediction errors as intrinsic rewards to guide RL exploration. Experimental results on Atari games show that our new intrinsic motivation significantly outperforms several state-of-the-art baselines. We further visualize our noisy agents' behavior in a physics environment and demonstrate that our newly designed intrinsic reward leads to the emergence of physical interaction behaviors (e.g. contact with objects).



### Learning Lane Graph Representations for Motion Forecasting
- **Arxiv ID**: http://arxiv.org/abs/2007.13732v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.13732v1)
- **Published**: 2020-07-27 17:59:49+00:00
- **Updated**: 2020-07-27 17:59:49+00:00
- **Authors**: Ming Liang, Bin Yang, Rui Hu, Yun Chen, Renjie Liao, Song Feng, Raquel Urtasun
- **Comment**: ECCV 2020 Oral
- **Journal**: None
- **Summary**: We propose a motion forecasting model that exploits a novel structured map representation as well as actor-map interactions. Instead of encoding vectorized maps as raster images, we construct a lane graph from raw map data to explicitly preserve the map structure. To capture the complex topology and long range dependencies of the lane graph, we propose LaneGCN which extends graph convolutions with multiple adjacency matrices and along-lane dilation. To capture the complex interactions between actors and maps, we exploit a fusion network consisting of four types of interactions, actor-to-lane, lane-to-lane, lane-to-actor and actor-to-actor. Powered by LaneGCN and actor-map interactions, our model is able to predict accurate and realistic multi-modal trajectories. Our approach significantly outperforms the state-of-the-art on the large scale Argoverse motion forecasting benchmark.



### Corner Proposal Network for Anchor-free, Two-stage Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2007.13816v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.13816v1)
- **Published**: 2020-07-27 19:04:57+00:00
- **Updated**: 2020-07-27 19:04:57+00:00
- **Authors**: Kaiwen Duan, Lingxi Xie, Honggang Qi, Song Bai, Qingming Huang, Qi Tian
- **Comment**: 18 pages (including 3 pages of References), 3 figures, 7 tables,
  accepted by ECCV 2020
- **Journal**: None
- **Summary**: The goal of object detection is to determine the class and location of objects in an image. This paper proposes a novel anchor-free, two-stage framework which first extracts a number of object proposals by finding potential corner keypoint combinations and then assigns a class label to each proposal by a standalone classification stage. We demonstrate that these two stages are effective solutions for improving recall and precision, respectively, and they can be integrated into an end-to-end network. Our approach, dubbed Corner Proposal Network (CPN), enjoys the ability to detect objects of various scales and also avoids being confused by a large number of false-positive proposals. On the MS-COCO dataset, CPN achieves an AP of 49.2% which is competitive among state-of-the-art object detection methods. CPN also fits the scenario of computational efficiency, which achieves an AP of 41.6%/39.7% at 26.2/43.3 FPS, surpassing most competitors with the same inference speed. Code is available at https://github.com/Duankaiwen/CPNDet



### Chest X-ray Report Generation through Fine-Grained Label Learning
- **Arxiv ID**: http://arxiv.org/abs/2007.13831v1
- **DOI**: None
- **Categories**: **cs.CV**, I.2.1; I.4.9; J.3
- **Links**: [PDF](http://arxiv.org/pdf/2007.13831v1)
- **Published**: 2020-07-27 19:50:56+00:00
- **Updated**: 2020-07-27 19:50:56+00:00
- **Authors**: Tanveer Syeda-Mahmood, Ken C. L. Wong, Yaniv Gur, Joy T. Wu, Ashutosh Jadhav, Satyananda Kashyap, Alexandros Karargyris, Anup Pillai, Arjun Sharma, Ali Bin Syed, Orest Boyko, Mehdi Moradi
- **Comment**: 11 pages, 5 figures, to appear in MICCAI 2020 Conference
- **Journal**: None
- **Summary**: Obtaining automated preliminary read reports for common exams such as chest X-rays will expedite clinical workflows and improve operational efficiencies in hospitals. However, the quality of reports generated by current automated approaches is not yet clinically acceptable as they cannot ensure the correct detection of a broad spectrum of radiographic findings nor describe them accurately in terms of laterality, anatomical location, severity, etc. In this work, we present a domain-aware automatic chest X-ray radiology report generation algorithm that learns fine-grained description of findings from images and uses their pattern of occurrences to retrieve and customize similar reports from a large report database. We also develop an automatic labeling algorithm for assigning such descriptors to images and build a novel deep learning network that recognizes both coarse and fine-grained descriptions of findings. The resulting report generation algorithm significantly outperforms the state of the art using established score metrics.



### Adaptive LiDAR Sampling and Depth Completion using Ensemble Variance
- **Arxiv ID**: http://arxiv.org/abs/2007.13834v2
- **DOI**: 10.1109/TIP.2021.3120042
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.13834v2)
- **Published**: 2020-07-27 19:54:42+00:00
- **Updated**: 2021-10-05 14:37:19+00:00
- **Authors**: Eyal Gofer, Shachar Praisler, Guy Gilboa
- **Comment**: Accepted for publication in IEEE Transactions on Image Processing.
  For associated examples, see
  https://www.vision-and-sensing.com/post/adaptive-lidar-sampling-and-depth-completion-using-ensemble-variance-new-publication
- **Journal**: None
- **Summary**: This work considers the problem of depth completion, with or without image data, where an algorithm may measure the depth of a prescribed limited number of pixels. The algorithmic challenge is to choose pixel positions strategically and dynamically to maximally reduce overall depth estimation error. This setting is realized in daytime or nighttime depth completion for autonomous vehicles with a programmable LiDAR. Our method uses an ensemble of predictors to define a sampling probability over pixels. This probability is proportional to the variance of the predictions of ensemble members, thus highlighting pixels that are difficult to predict. By additionally proceeding in several prediction phases, we effectively reduce redundant sampling of similar pixels. Our ensemble-based method may be implemented using any depth-completion learning algorithm, such as a state-of-the-art neural network, treated as a black box. In particular, we also present a simple and effective Random Forest-based algorithm, and similarly use its internal ensemble in our design. We conduct experiments on the KITTI dataset, using the neural network algorithm of Ma et al. and our Random Forest based learner for implementing our method. The accuracy of both implementations exceeds the state of the art. Compared with a random or grid sampling pattern, our method allows a reduction by a factor of 4-10 in the number of measurements required to attain the same accuracy.



### Learned Pre-Processing for Automatic Diabetic Retinopathy Detection on Eye Fundus Images
- **Arxiv ID**: http://arxiv.org/abs/2007.13838v1
- **DOI**: 10.1007/978-3-030-27272-2_32
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.13838v1)
- **Published**: 2020-07-27 20:06:13+00:00
- **Updated**: 2020-07-27 20:06:13+00:00
- **Authors**: Asim Smailagic, Anupma Sharan, Pedro Costa, Adrian Galdran, Alex Gaudio, Aurélio Campilho
- **Comment**: Accepted to International Conference on Image Analysis and
  Recognition ICIAR 2019 Published at
  https://doi.org/10.1007/978-3-030-27272-2_32
- **Journal**: None
- **Summary**: Diabetic Retinopathy is the leading cause of blindness in the working-age population of the world. The main aim of this paper is to improve the accuracy of Diabetic Retinopathy detection by implementing a shadow removal and color correction step as a preprocessing stage from eye fundus images. For this, we rely on recent findings indicating that application of image dehazing on the inverted intensity domain amounts to illumination compensation. Inspired by this work, we propose a Shadow Removal Layer that allows us to learn the pre-processing function for a particular task. We show that learning the pre-processing function improves the performance of the network on the Diabetic Retinopathy detection task.



### Saliency Prediction with External Knowledge
- **Arxiv ID**: http://arxiv.org/abs/2007.13839v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.13839v1)
- **Published**: 2020-07-27 20:12:28+00:00
- **Updated**: 2020-07-27 20:12:28+00:00
- **Authors**: Yifeng Zhang, Ming Jiang, Qi Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: The last decades have seen great progress in saliency prediction, with the success of deep neural networks that are able to encode high-level semantics. Yet, while humans have the innate capability in leveraging their knowledge to decide where to look (e.g. people pay more attention to familiar faces such as celebrities), saliency prediction models have only been trained with large eye-tracking datasets. This work proposes to bridge this gap by explicitly incorporating external knowledge for saliency models as humans do. We develop networks that learn to highlight regions by incorporating prior knowledge of semantic relationships, be it general or domain-specific, depending on the task of interest. At the core of the method is a new Graph Semantic Saliency Network (GraSSNet) that constructs a graph that encodes semantic relationships learned from external knowledge. A Spatial Graph Attention Network is then developed to update saliency features based on the learned graph. Experiments show that the proposed model learns to predict saliency from the external knowledge and outperforms the state-of-the-art on four saliency benchmarks.



### Improving Lesion Segmentation for Diabetic Retinopathy using Adversarial Learning
- **Arxiv ID**: http://arxiv.org/abs/2007.13854v1
- **DOI**: 10.1007/978-3-030-27272-2_29
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.13854v1)
- **Published**: 2020-07-27 20:43:36+00:00
- **Updated**: 2020-07-27 20:43:36+00:00
- **Authors**: Qiqi Xiao, Jiaxu Zou, Muqiao Yang, Alex Gaudio, Kris Kitani, Asim Smailagic, Pedro Costa, Min Xu
- **Comment**: Accepted to International Conference on Image Analysis and
  Recognition, ICIAR 2019. Published at
  https://doi.org/10.1007/978-3-030-27272-2_29 Code:
  https://github.com/zoujx96/DR-segmentation
- **Journal**: None
- **Summary**: Diabetic Retinopathy (DR) is a leading cause of blindness in working age adults. DR lesions can be challenging to identify in fundus images, and automatic DR detection systems can offer strong clinical value. Of the publicly available labeled datasets for DR, the Indian Diabetic Retinopathy Image Dataset (IDRiD) presents retinal fundus images with pixel-level annotations of four distinct lesions: microaneurysms, hemorrhages, soft exudates and hard exudates. We utilize the HEDNet edge detector to solve a semantic segmentation task on this dataset, and then propose an end-to-end system for pixel-level segmentation of DR lesions by incorporating HEDNet into a Conditional Generative Adversarial Network (cGAN). We design a loss function that adds adversarial loss to segmentation loss. Our experiments show that the addition of the adversarial loss improves the lesion segmentation performance over the baseline.



### se(3)-TrackNet: Data-driven 6D Pose Tracking by Calibrating Image Residuals in Synthetic Domains
- **Arxiv ID**: http://arxiv.org/abs/2007.13866v1
- **DOI**: 10.1109/IROS45743.2020.9341314
- **Categories**: **cs.CV**, cs.GR, cs.LG, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.13866v1)
- **Published**: 2020-07-27 21:09:36+00:00
- **Updated**: 2020-07-27 21:09:36+00:00
- **Authors**: Bowen Wen, Chaitanya Mitash, Baozhang Ren, Kostas E. Bekris
- **Comment**: None
- **Journal**: International Conference on Intelligent Robots and Systems (IROS)
  2020
- **Summary**: Tracking the 6D pose of objects in video sequences is important for robot manipulation. This task, however, introduces multiple challenges: (i) robot manipulation involves significant occlusions; (ii) data and annotations are troublesome and difficult to collect for 6D poses, which complicates machine learning solutions, and (iii) incremental error drift often accumulates in long term tracking to necessitate re-initialization of the object's pose. This work proposes a data-driven optimization approach for long-term, 6D pose tracking. It aims to identify the optimal relative pose given the current RGB-D observation and a synthetic image conditioned on the previous best estimate and the object's model. The key contribution in this context is a novel neural network architecture, which appropriately disentangles the feature encoding to help reduce domain shift, and an effective 3D orientation representation via Lie Algebra. Consequently, even when the network is trained only with synthetic data can work effectively over real images. Comprehensive experiments over benchmarks - existing ones as well as a new dataset with significant occlusions related to object manipulation - show that the proposed approach achieves consistently robust estimates and outperforms alternatives, even though they have been trained with real images. The approach is also the most computationally efficient among the alternatives and achieves a tracking frequency of 90.9Hz.



### Robust Image Retrieval-based Visual Localization using Kapture
- **Arxiv ID**: http://arxiv.org/abs/2007.13867v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.13867v3)
- **Published**: 2020-07-27 21:10:35+00:00
- **Updated**: 2022-01-07 10:05:49+00:00
- **Authors**: Martin Humenberger, Yohann Cabon, Nicolas Guerin, Julien Morat, Vincent Leroy, Jérôme Revaud, Philippe Rerole, Noé Pion, Cesar de Souza, Gabriela Csurka
- **Comment**: None
- **Journal**: None
- **Summary**: Visual localization tackles the challenge of estimating the camera pose from images by using correspondence analysis between query images and a map. This task is computation and data intensive which poses challenges on thorough evaluation of methods on various datasets. However, in order to further advance in the field, we claim that robust visual localization algorithms should be evaluated on multiple datasets covering a broad domain variety. To facilitate this, we introduce kapture, a new, flexible, unified data format and toolbox for visual localization and structure-from-motion (SFM). It enables easy usage of different datasets as well as efficient and reusable data processing. To demonstrate this, we present a versatile pipeline for visual localization that facilitates the use of different local and global features, 3D data (e.g. depth maps), non-vision sensor data (e.g. IMU, GPS, WiFi), and various processing algorithms. Using multiple configurations of the pipeline, we show the great versatility of kapture in our experiments. Furthermore, we evaluate our methods on eight public datasets where they rank top on all and first on many of them. To foster future research, we release code, models, and all datasets used in this paper in the kapture format open source under a permissive BSD license. github.com/naver/kapture, github.com/naver/kapture-localization



### A Unified Framework of Surrogate Loss by Refactoring and Interpolation
- **Arxiv ID**: http://arxiv.org/abs/2007.13870v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.13870v1)
- **Published**: 2020-07-27 21:16:51+00:00
- **Updated**: 2020-07-27 21:16:51+00:00
- **Authors**: Lanlan Liu, Mingzhe Wang, Jia Deng
- **Comment**: Accepted to ECCV 2020
- **Journal**: None
- **Summary**: We introduce UniLoss, a unified framework to generate surrogate losses for training deep networks with gradient descent, reducing the amount of manual design of task-specific surrogate losses. Our key observation is that in many cases, evaluating a model with a performance metric on a batch of examples can be refactored into four steps: from input to real-valued scores, from scores to comparisons of pairs of scores, from comparisons to binary variables, and from binary variables to the final performance metric. Using this refactoring we generate differentiable approximations for each non-differentiable step through interpolation. Using UniLoss, we can optimize for different tasks and metrics using one unified framework, achieving comparable performance compared with task-specific losses. We validate the effectiveness of UniLoss on three tasks and four datasets. Code is available at https://github.com/princeton-vl/uniloss.



### Perpetual Motion: Generating Unbounded Human Motion
- **Arxiv ID**: http://arxiv.org/abs/2007.13886v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.13886v1)
- **Published**: 2020-07-27 21:50:36+00:00
- **Updated**: 2020-07-27 21:50:36+00:00
- **Authors**: Yan Zhang, Michael J. Black, Siyu Tang
- **Comment**: 15 pages with appendix
- **Journal**: None
- **Summary**: The modeling of human motion using machine learning methods has been widely studied. In essence it is a time-series modeling problem involving predicting how a person will move in the future given how they moved in the past. Existing methods, however, typically have a short time horizon, predicting a only few frames to a few seconds of human motion. Here we focus on long-term prediction; that is, generating long sequences (potentially infinite) of human motion that is plausible. Furthermore, we do not rely on a long sequence of input motion for conditioning, but rather, can predict how someone will move from as little as a single pose. Such a model has many uses in graphics (video games and crowd animation) and vision (as a prior for human motion estimation or for dataset creation). To address this problem, we propose a model to generate non-deterministic, \textit{ever-changing}, perpetual human motion, in which the global trajectory and the body pose are cross-conditioned. We introduce a novel KL-divergence term with an implicit, unknown, prior. We train this using a heavy-tailed function of the KL divergence of a white-noise Gaussian process, allowing latent sequence temporal dependency. We perform systematic experiments to verify its effectiveness and find that it is superior to baseline methods.



### 3DMaterialGAN: Learning 3D Shape Representation from Latent Space for Materials Science Applications
- **Arxiv ID**: http://arxiv.org/abs/2007.13887v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.13887v1)
- **Published**: 2020-07-27 21:55:16+00:00
- **Updated**: 2020-07-27 21:55:16+00:00
- **Authors**: Devendra K. Jangid, Neal R. Brodnik, Amil Khan, McLean P. Echlin, Tresa M. Pollock, Sam Daly, B. S. Manjunath
- **Comment**: None
- **Journal**: None
- **Summary**: In the field of computer vision, unsupervised learning for 2D object generation has advanced rapidly in the past few years. However, 3D object generation has not garnered the same attention or success as its predecessor. To facilitate novel progress at the intersection of computer vision and materials science, we propose a 3DMaterialGAN network that is capable of recognizing and synthesizing individual grains whose morphology conforms to a given 3D polycrystalline material microstructure. This Generative Adversarial Network (GAN) architecture yields complex 3D objects from probabilistic latent space vectors with no additional information from 2D rendered images. We show that this method performs comparably or better than state-of-the-art on benchmark annotated 3D datasets, while also being able to distinguish and generate objects that are not easily annotated, such as grain morphologies. The value of our algorithm is demonstrated with analysis on experimental real-world data, namely generating 3D grain structures found in a commercially relevant wrought titanium alloy, which were validated through statistical shape comparison. This framework lays the foundation for the recognition and synthesis of polycrystalline material microstructures, which are used in additive manufacturing, aerospace, and structural design applications.



### Unsupervised Domain Adaptation in the Dissimilarity Space for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/2007.13890v1
- **DOI**: 10.1007/978-3-030-58583-9_10
- **Categories**: **cs.CV**, I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/2007.13890v1)
- **Published**: 2020-07-27 22:10:46+00:00
- **Updated**: 2020-07-27 22:10:46+00:00
- **Authors**: Djebril Mekhazni, Amran Bhuiyan, George Ekladious, Eric Granger
- **Comment**: 14 pages (16 pages with references), 7 figures, conference ECCV
- **Journal**: None
- **Summary**: Person re-identification (ReID) remains a challenging task in many real-word video analytics and surveillance applications, even though state-of-the-art accuracy has improved considerably with the advent of deep learning (DL) models trained on large image datasets. Given the shift in distributions that typically occurs between video data captured from the source and target domains, and absence of labeled data from the target domain, it is difficult to adapt a DL model for accurate recognition of target data. We argue that for pair-wise matchers that rely on metric learning, e.g., Siamese networks for person ReID, the unsupervised domain adaptation (UDA) objective should consist in aligning pair-wise dissimilarity between domains, rather than aligning feature representations. Moreover, dissimilarity representations are more suitable for designing open-set ReID systems, where identities differ in the source and target domains. In this paper, we propose a novel Dissimilarity-based Maximum Mean Discrepancy (D-MMD) loss for aligning pair-wise distances that can be optimized via gradient descent. From a person ReID perspective, the evaluation of D-MMD loss is straightforward since the tracklet information allows to label a distance vector as being either within-class or between-class. This allows approximating the underlying distribution of target pair-wise distances for D-MMD loss optimization, and accordingly align source and target distance distributions. Empirical results with three challenging benchmark datasets show that the proposed D-MMD loss decreases as source and domain distributions become more similar. Extensive experimental evaluation also indicates that UDA methods that rely on the D-MMD loss can significantly outperform baseline and state-of-the-art UDA methods for person ReID without the common requirement for data augmentation and/or complex networks.



### Automatic Detection and Classification of Waste Consumer Medications for Proper Management and Disposal
- **Arxiv ID**: http://arxiv.org/abs/2007.13903v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.13903v1)
- **Published**: 2020-07-27 23:03:14+00:00
- **Updated**: 2020-07-27 23:03:14+00:00
- **Authors**: Bahram Marami, Atabak Reza Royaee
- **Comment**: None
- **Journal**: None
- **Summary**: Every year, millions of pounds of medicines remain unused in the U.S. and are subject to an in-home disposal, i.e., kept in medicine cabinets, flushed in toilet or thrown in regular trash. In-home disposal, however, can negatively impact the environment and public health. The drug take-back programs (drug take-backs) sponsored by the Drug Enforcement Administration (DEA) and its state and industry partners collect unused consumer medications and provide the best alternative to in-home disposal of medicines. However, the drug take-backs are expensive to operate and not widely available. In this paper, we show that artificial intelligence (AI) can be applied to drug take-backs to render them operationally more efficient. Since identification of any waste is crucial to a proper disposal, we showed that it is possible to accurately identify loose consumer medications solely based on the physical features and visual appearance. We have developed an automatic technique that uses deep neural networks and computer vision to identify and segregate solid medicines. We applied the technique to images of about one thousand loose pills and succeeded in correctly identifying the pills with an accuracy of 0.912 and top-5 accuracy of 0.984. We also showed that hazardous pills could be distinguished from non-hazardous pills within the dataset with an accuracy of 0.984. We believe that the power of artificial intelligence could be harnessed in products that would facilitate the operation of the drug take-backs more efficiently and help them become widely available throughout the country.



### Deep Hashing with Hash-Consistent Large Margin Proxy Embeddings
- **Arxiv ID**: http://arxiv.org/abs/2007.13912v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.13912v1)
- **Published**: 2020-07-27 23:47:43+00:00
- **Updated**: 2020-07-27 23:47:43+00:00
- **Authors**: Pedro Morgado, Yunsheng Li, Jose Costa Pereira, Mohammad Saberian, Nuno Vasconcelos
- **Comment**: Accepted at International Journal of Computer Vision
- **Journal**: None
- **Summary**: Image hash codes are produced by binarizing the embeddings of convolutional neural networks (CNN) trained for either classification or retrieval. While proxy embeddings achieve good performance on both tasks, they are non-trivial to binarize, due to a rotational ambiguity that encourages non-binary embeddings. The use of a fixed set of proxies (weights of the CNN classification layer) is proposed to eliminate this ambiguity, and a procedure to design proxy sets that are nearly optimal for both classification and hashing is introduced. The resulting hash-consistent large margin (HCLM) proxies are shown to encourage saturation of hashing units, thus guaranteeing a small binarization error, while producing highly discriminative hash-codes. A semantic extension (sHCLM), aimed to improve hashing performance in a transfer scenario, is also proposed. Extensive experiments show that sHCLM embeddings achieve significant improvements over state-of-the-art hashing procedures on several small and large datasets, both within and beyond the set of training classes.



### Active Learning for Video Description With Cluster-Regularized Ensemble Ranking
- **Arxiv ID**: http://arxiv.org/abs/2007.13913v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.13913v3)
- **Published**: 2020-07-27 23:52:41+00:00
- **Updated**: 2020-12-02 23:38:20+00:00
- **Authors**: David M. Chan, Sudheendra Vijayanarasimhan, David A. Ross, John Canny
- **Comment**: Published at the 15th Asian Conference on Computer Vision (ACCV 2020)
- **Journal**: None
- **Summary**: Automatic video captioning aims to train models to generate text descriptions for all segments in a video, however, the most effective approaches require large amounts of manual annotation which is slow and expensive. Active learning is a promising way to efficiently build a training set for video captioning tasks while reducing the need to manually label uninformative examples. In this work we both explore various active learning approaches for automatic video captioning and show that a cluster-regularized ensemble strategy provides the best active learning approach to efficiently gather training sets for video captioning. We evaluate our approaches on the MSR-VTT and LSMDC datasets using both transformer and LSTM based captioning models and show that our novel strategy can achieve high performance while using up to 60% fewer training data than the strong state of the art baselines.



