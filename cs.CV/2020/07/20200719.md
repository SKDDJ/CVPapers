# Arxiv Papers in cs.CV on 2020-07-19
### Gaussian kernel smoothing
- **Arxiv ID**: http://arxiv.org/abs/2007.09539v4
- **DOI**: None
- **Categories**: **stat.ME**, cs.CV, stat.CO
- **Links**: [PDF](http://arxiv.org/pdf/2007.09539v4)
- **Published**: 2020-07-19 00:19:07+00:00
- **Updated**: 2021-11-29 18:34:12+00:00
- **Authors**: Moo K. Chung
- **Comment**: None
- **Journal**: None
- **Summary**: Image acquisition and segmentation are likely to introduce noise. Further image processing such as image registration and parameterization can introduce additional noise. It is thus imperative to reduce noise measurements and boost signal. In order to increase the signal-to-noise ratio (SNR) and smoothness of data required for the subsequent random field theory based statistical inference, some type of smoothing is necessary. Among many image smoothing methods, Gaussian kernel smoothing has emerged as a de facto smoothing technique among brain imaging researchers due to its simplicity in numerical implementation. Gaussian kernel smoothing also increases statistical sensitivity and statistical power as well as Gausianness. Gaussian kernel smoothing can be viewed as weighted averaging of voxel values. Then from the central limit theorem, the weighted average should be more Gaussian.



### ContactPose: A Dataset of Grasps with Object Contact and Hand Pose
- **Arxiv ID**: http://arxiv.org/abs/2007.09545v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.09545v1)
- **Published**: 2020-07-19 01:01:14+00:00
- **Updated**: 2020-07-19 01:01:14+00:00
- **Authors**: Samarth Brahmbhatt, Chengcheng Tang, Christopher D. Twigg, Charles C. Kemp, James Hays
- **Comment**: The European Conference on Computer Vision (ECCV) 2020
- **Journal**: None
- **Summary**: Grasping is natural for humans. However, it involves complex hand configurations and soft tissue deformation that can result in complicated regions of contact between the hand and the object. Understanding and modeling this contact can potentially improve hand models, AR/VR experiences, and robotic grasping. Yet, we currently lack datasets of hand-object contact paired with other data modalities, which is crucial for developing and evaluating contact modeling techniques. We introduce ContactPose, the first dataset of hand-object contact paired with hand pose, object pose, and RGB-D images. ContactPose has 2306 unique grasps of 25 household objects grasped with 2 functional intents by 50 participants, and more than 2.9 M RGB-D grasp images. Analysis of ContactPose data reveals interesting relationships between hand pose and contact. We use this data to rigorously evaluate various data representations, heuristics from the literature, and learning methods for contact modeling. Data, code, and trained models are available at https://contactpose.cc.gatech.edu.



### Sat2Graph: Road Graph Extraction through Graph-Tensor Encoding
- **Arxiv ID**: http://arxiv.org/abs/2007.09547v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.09547v1)
- **Published**: 2020-07-19 01:04:19+00:00
- **Updated**: 2020-07-19 01:04:19+00:00
- **Authors**: Songtao He, Favyen Bastani, Satvat Jagwani, Mohammad Alizadeh, Hari Balakrishnan, Sanjay Chawla, Mohamed M. Elshrif, Samuel Madden, Amin Sadeghi
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: Inferring road graphs from satellite imagery is a challenging computer vision task. Prior solutions fall into two categories: (1) pixel-wise segmentation-based approaches, which predict whether each pixel is on a road, and (2) graph-based approaches, which predict the road graph iteratively. We find that these two approaches have complementary strengths while suffering from their own inherent limitations.   In this paper, we propose a new method, Sat2Graph, which combines the advantages of the two prior categories into a unified framework. The key idea in Sat2Graph is a novel encoding scheme, graph-tensor encoding (GTE), which encodes the road graph into a tensor representation. GTE makes it possible to train a simple, non-recurrent, supervised model to predict a rich set of features that capture the graph structure directly from an image. We evaluate Sat2Graph using two large datasets. We find that Sat2Graph surpasses prior methods on two widely used metrics, TOPO and APLS. Furthermore, whereas prior work only infers planar road graphs, our approach is capable of inferring stacked roads (e.g., overpasses), and does so robustly.



### Kinematic 3D Object Detection in Monocular Video
- **Arxiv ID**: http://arxiv.org/abs/2007.09548v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.09548v1)
- **Published**: 2020-07-19 01:15:12+00:00
- **Updated**: 2020-07-19 01:15:12+00:00
- **Authors**: Garrick Brazil, Gerard Pons-Moll, Xiaoming Liu, Bernt Schiele
- **Comment**: To appear in ECCV 2020
- **Journal**: None
- **Summary**: Perceiving the physical world in 3D is fundamental for self-driving applications. Although temporal motion is an invaluable resource to human vision for detection, tracking, and depth perception, such features have not been thoroughly utilized in modern 3D object detectors. In this work, we propose a novel method for monocular video-based 3D object detection which carefully leverages kinematic motion to improve precision of 3D localization. Specifically, we first propose a novel decomposition of object orientation as well as a self-balancing 3D confidence. We show that both components are critical to enable our kinematic model to work effectively. Collectively, using only a single model, we efficiently leverage 3D kinematics from monocular videos to improve the overall localization precision in 3D object detection while also producing useful by-products of scene dynamics (ego-motion and per-object velocity). We achieve state-of-the-art performance on monocular 3D object detection and the Bird's Eye View tasks within the KITTI self-driving dataset.



### Leveraging Seen and Unseen Semantic Relationships for Generative Zero-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2007.09549v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.09549v1)
- **Published**: 2020-07-19 01:25:53+00:00
- **Updated**: 2020-07-19 01:25:53+00:00
- **Authors**: Maunil R Vyas, Hemanth Venkateswara, Sethuraman Panchanathan
- **Comment**: 19 Pages, To be appear in ECCV 2020
- **Journal**: None
- **Summary**: Zero-shot learning (ZSL) addresses the unseen class recognition problem by leveraging semantic information to transfer knowledge from seen classes to unseen classes. Generative models synthesize the unseen visual features and convert ZSL into a classical supervised learning problem. These generative models are trained using the seen classes and are expected to implicitly transfer the knowledge from seen to unseen classes. However, their performance is stymied by overfitting, which leads to substandard performance on Generalized Zero-Shot learning (GZSL). To address this concern, we propose the novel LsrGAN, a generative model that Leverages the Semantic Relationship between seen and unseen categories and explicitly performs knowledge transfer by incorporating a novel Semantic Regularized Loss (SR-Loss). The SR-loss guides the LsrGAN to generate visual features that mirror the semantic relationships between seen and unseen classes. Experiments on seven benchmark datasets, including the challenging Wikipedia text-based CUB and NABirds splits, and Attribute-based AWA, CUB, and SUN, demonstrates the superiority of the LsrGAN compared to previous state-of-the-art approaches under both ZSL and GZSL. Code is available at https: // github. com/ Maunil/ LsrGAN



### Predicting risk of late age-related macular degeneration using deep learning
- **Arxiv ID**: http://arxiv.org/abs/2007.09550v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.09550v1)
- **Published**: 2020-07-19 01:32:09+00:00
- **Updated**: 2020-07-19 01:32:09+00:00
- **Authors**: Yifan Peng, Tiarnan D. Keenan, Qingyu Chen, Elvira Agr√≥n, Alexis Allot, Wai T. Wong, Emily Y. Chew, Zhiyong Lu
- **Comment**: Accepted by npj Digital Medicine
- **Journal**: None
- **Summary**: By 2040, age-related macular degeneration (AMD) will affect approximately 288 million people worldwide. Identifying individuals at high risk of progression to late AMD, the sight-threatening stage, is critical for clinical actions, including medical interventions and timely monitoring. Although deep learning has shown promise in diagnosing/screening AMD using color fundus photographs, it remains difficult to predict individuals' risks of late AMD accurately. For both tasks, these initial deep learning attempts have remained largely unvalidated in independent cohorts. Here, we demonstrate how deep learning and survival analysis can predict the probability of progression to late AMD using 3,298 participants (over 80,000 images) from the Age-Related Eye Disease Studies AREDS and AREDS2, the largest longitudinal clinical trials in AMD. When validated against an independent test dataset of 601 participants, our model achieved high prognostic accuracy (five-year C-statistic 86.4 (95% confidence interval 86.2-86.6)) that substantially exceeded that of retinal specialists using two existing clinical standards (81.3 (81.1-81.5) and 82.0 (81.8-82.3), respectively). Interestingly, our approach offers additional strengths over the existing clinical standards in AMD prognosis (e.g., risk ascertainment above 50%) and is likely to be highly generalizable, given the breadth of training data from 82 US retinal specialty clinics. Indeed, during external validation through training on AREDS and testing on AREDS2 as an independent cohort, our model retained substantially higher prognostic accuracy than existing clinical standards. These results highlight the potential of deep learning systems to enhance clinical decision-making in AMD patients.



### Understanding Spatial Relations through Multiple Modalities
- **Arxiv ID**: http://arxiv.org/abs/2007.09551v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.09551v1)
- **Published**: 2020-07-19 01:35:08+00:00
- **Updated**: 2020-07-19 01:35:08+00:00
- **Authors**: Soham Dan, Hangfeng He, Dan Roth
- **Comment**: None
- **Journal**: LREC 2020
- **Summary**: Recognizing spatial relations and reasoning about them is essential in multiple applications including navigation, direction giving and human-computer interaction in general. Spatial relations between objects can either be explicit -- expressed as spatial prepositions, or implicit -- expressed by spatial verbs such as moving, walking, shifting, etc. Both these, but implicit relations in particular, require significant common sense understanding. In this paper, we introduce the task of inferring implicit and explicit spatial relations between two entities in an image. We design a model that uses both textual and visual information to predict the spatial relations, making use of both positional and size information of objects and image embeddings. We contrast our spatial model with powerful language models and show how our modeling complements the power of these, improving prediction accuracy and coverage and facilitates dealing with unseen subjects, objects and relations.



### Sequential Hierarchical Learning with Distribution Transformation for Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2007.09552v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.09552v4)
- **Published**: 2020-07-19 01:35:53+00:00
- **Updated**: 2023-05-03 11:35:41+00:00
- **Authors**: Yuqing Liu, Xinfeng Zhang, Shanshe Wang, Siwei Ma, Wen Gao
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-scale design has been considered in recent image super-resolution (SR) works to explore the hierarchical feature information. Existing multi-scale networks aim to build elaborate blocks or progressive architecture for restoration. In general, larger scale features concentrate more on structural and high-level information, while smaller scale features contain plentiful details and textured information. In this point of view, information from larger scale features can be derived from smaller ones. Based on the observation, in this paper, we build a sequential hierarchical learning super-resolution network (SHSR) for effective image SR. Specially, we consider the inter-scale correlations of features, and devise a sequential multi-scale block (SMB) to progressively explore the hierarchical information. SMB is designed in a recursive way based on the linearity of convolution with restricted parameters. Besides the sequential hierarchical learning, we also investigate the correlations among the feature maps and devise a distribution transformation block (DTB). Different from attention-based methods, DTB regards the transformation in a normalization manner, and jointly considers the spatial and channel-wise correlations with scaling and bias factors. Experiment results show SHSR achieves superior quantitative performance and visual quality to state-of-the-art methods with near 34\% parameters and 50\% MACs off when scaling factor is $\times4$. To boost the performance without further training, the extension model SHSR$^+$ with self-ensemble achieves competitive performance than larger networks with near 92\% parameters and 42\% MACs off with scaling factor $\times4$.



### Referring Expression Comprehension: A Survey of Methods and Datasets
- **Arxiv ID**: http://arxiv.org/abs/2007.09554v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2007.09554v2)
- **Published**: 2020-07-19 01:45:02+00:00
- **Updated**: 2020-12-07 04:56:24+00:00
- **Authors**: Yanyuan Qiao, Chaorui Deng, Qi Wu
- **Comment**: Accepted to IEEE TMM
- **Journal**: None
- **Summary**: Referring expression comprehension (REC) aims to localize a target object in an image described by a referring expression phrased in natural language. Different from the object detection task that queried object labels have been pre-defined, the REC problem only can observe the queries during the test. It thus more challenging than a conventional computer vision problem. This task has attracted a lot of attention from both computer vision and natural language processing community, and several lines of work have been proposed, from CNN-RNN model, modular network to complex graph-based model. In this survey, we first examine the state of the art by comparing modern approaches to the problem. We classify methods by their mechanism to encode the visual and textual modalities. In particular, we examine the common approach of joint embedding images and expressions to a common feature space. We also discuss modular architectures and graph-based models that interface with structured graph representation. In the second part of this survey, we review the datasets available for training and evaluating REC systems. We then group results according to the datasets, backbone models, settings so that they can be fairly compared. Finally, we discuss promising future directions for the field, in particular the compositional referring expression comprehension that requires longer reasoning chain to address.



### Resolution Switchable Networks for Runtime Efficient Image Recognition
- **Arxiv ID**: http://arxiv.org/abs/2007.09558v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.09558v3)
- **Published**: 2020-07-19 02:12:59+00:00
- **Updated**: 2020-11-09 07:18:01+00:00
- **Authors**: Yikai Wang, Fuchun Sun, Duo Li, Anbang Yao
- **Comment**: ECCV 2020. Code and models: https://github.com/yikaiw/RS-Nets
- **Journal**: None
- **Summary**: We propose a general method to train a single convolutional neural network which is capable of switching image resolutions at inference. Thus the running speed can be selected to meet various computational resource limits. Networks trained with the proposed method are named Resolution Switchable Networks (RS-Nets). The basic training framework shares network parameters for handling images which differ in resolution, yet keeps separate batch normalization layers. Though it is parameter-efficient in design, it leads to inconsistent accuracy variations at different resolutions, for which we provide a detailed analysis from the aspect of the train-test recognition discrepancy. A multi-resolution ensemble distillation is further designed, where a teacher is learnt on the fly as a weighted ensemble over resolutions. Thanks to the ensemble and knowledge distillation, RS-Nets enjoy accuracy improvements at a wide range of resolutions compared with individually trained models. Extensive experiments on the ImageNet dataset are provided, and we additionally consider quantization problems. Code and models are available at https://github.com/yikaiw/RS-Nets.



### Length-Controllable Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/2007.09580v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.09580v1)
- **Published**: 2020-07-19 03:40:51+00:00
- **Updated**: 2020-07-19 03:40:51+00:00
- **Authors**: Chaorui Deng, Ning Ding, Mingkui Tan, Qi Wu
- **Comment**: To be appeared in ECCV 2020
- **Journal**: None
- **Summary**: The last decade has witnessed remarkable progress in the image captioning task; however, most existing methods cannot control their captions, \emph{e.g.}, choosing to describe the image either roughly or in detail. In this paper, we propose to use a simple length level embedding to endow them with this ability. Moreover, due to their autoregressive nature, the computational complexity of existing models increases linearly as the length of the generated captions grows. Thus, we further devise a non-autoregressive image captioning approach that can generate captions in a length-irrelevant complexity. We verify the merit of the proposed length level embedding on three models: two state-of-the-art (SOTA) autoregressive models with different types of decoder, as well as our proposed non-autoregressive model, to show its generalization ability. In the experiments, our length-controllable image captioning models not only achieve SOTA performance on the challenging MS COCO dataset but also generate length-controllable and diverse image captions. Specifically, our non-autoregressive model outperforms the autoregressive baselines in terms of controllability and diversity, and also significantly improves the decoding efficiency for long captions. Our code and models are released at \textcolor{magenta}{\texttt{https://github.com/bearcatt/LaBERT}}.



### PIoU Loss: Towards Accurate Oriented Object Detection in Complex Environments
- **Arxiv ID**: http://arxiv.org/abs/2007.09584v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.09584v1)
- **Published**: 2020-07-19 03:51:59+00:00
- **Updated**: 2020-07-19 03:51:59+00:00
- **Authors**: Zhiming Chen, Kean Chen, Weiyao Lin, John See, Hui Yu, Yan Ke, Cong Yang
- **Comment**: None
- **Journal**: European Conference on Computer Vision, 2020
- **Summary**: Object detection using an oriented bounding box (OBB) can better target rotated objects by reducing the overlap with background areas. Existing OBB approaches are mostly built on horizontal bounding box detectors by introducing an additional angle dimension optimized by a distance loss. However, as the distance loss only minimizes the angle error of the OBB and that it loosely correlates to the IoU, it is insensitive to objects with high aspect ratios. Therefore, a novel loss, Pixels-IoU (PIoU) Loss, is formulated to exploit both the angle and IoU for accurate OBB regression. The PIoU loss is derived from IoU metric with a pixel-wise form, which is simple and suitable for both horizontal and oriented bounding box. To demonstrate its effectiveness, we evaluate the PIoU loss on both anchor-based and anchor-free frameworks. The experimental results show that PIoU loss can dramatically improve the performance of OBB detectors, particularly on objects with high aspect ratios and complex backgrounds. Besides, previous evaluation datasets did not include scenarios where the objects have high aspect ratios, hence a new dataset, Retail50K, is introduced to encourage the community to adapt OBB detectors for more complex environments.



### AWR: Adaptive Weighting Regression for 3D Hand Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2007.09590v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.09590v1)
- **Published**: 2020-07-19 04:57:13+00:00
- **Updated**: 2020-07-19 04:57:13+00:00
- **Authors**: Weiting Huang, Pengfei Ren, Jingyu Wang, Qi Qi, Haifeng Sun
- **Comment**: Accepted by AAAI-2020
- **Journal**: published 2020
- **Summary**: In this paper, we propose an adaptive weighting regression (AWR) method to leverage the advantages of both detection-based and regression-based methods. Hand joint coordinates are estimated as discrete integration of all pixels in dense representation, guided by adaptive weight maps. This learnable aggregation process introduces both dense and joint supervision that allows end-to-end training and brings adaptability to weight maps, making the network more accurate and robust. Comprehensive exploration experiments are conducted to validate the effectiveness and generality of AWR under various experimental settings, especially its usefulness for different types of dense representation and input modality. Our method outperforms other state-of-the-art methods on four publicly available datasets, including NYU, ICVL, MSRA and HANDS 2017 dataset.



### Semantic Equivalent Adversarial Data Augmentation for Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/2007.09592v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.09592v1)
- **Published**: 2020-07-19 05:01:01+00:00
- **Updated**: 2020-07-19 05:01:01+00:00
- **Authors**: Ruixue Tang, Chao Ma, Wei Emma Zhang, Qi Wu, Xiaokang Yang
- **Comment**: To appear in ECCV 2020
- **Journal**: None
- **Summary**: Visual Question Answering (VQA) has achieved great success thanks to the fast development of deep neural networks (DNN). On the other hand, the data augmentation, as one of the major tricks for DNN, has been widely used in many computer vision tasks. However, there are few works studying the data augmentation problem for VQA and none of the existing image based augmentation schemes (such as rotation and flipping) can be directly applied to VQA due to its semantic structure -- an $\langle image, question, answer\rangle$ triplet needs to be maintained correctly. For example, a direction related Question-Answer (QA) pair may not be true if the associated image is rotated or flipped. In this paper, instead of directly manipulating images and questions, we use generated adversarial examples for both images and questions as the augmented data. The augmented examples do not change the visual properties presented in the image as well as the \textbf{semantic} meaning of the question, the correctness of the $\langle image, question, answer\rangle$ is thus still maintained. We then use adversarial learning to train a classic VQA model (BUTD) with our augmented data. We find that we not only improve the overall performance on VQAv2, but also can withstand adversarial attack effectively, compared to the baseline model. The source code is available at https://github.com/zaynmi/seada-vqa.



### Mapping in a cycle: Sinkhorn regularized unsupervised learning for point cloud shapes
- **Arxiv ID**: http://arxiv.org/abs/2007.09594v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.09594v1)
- **Published**: 2020-07-19 05:21:33+00:00
- **Updated**: 2020-07-19 05:21:33+00:00
- **Authors**: Lei Yang, Wenxi Liu, Zhiming Cui, Nenglun Chen, Wenping Wang
- **Comment**: Accepted to ECCV2020
- **Journal**: None
- **Summary**: We propose an unsupervised learning framework with the pretext task of finding dense correspondences between point cloud shapes from the same category based on the cycle-consistency formulation. In order to learn discriminative pointwise features from point cloud data, we incorporate in the formulation a regularization term based on Sinkhorn normalization to enhance the learned pointwise mappings to be as bijective as possible. Besides, a random rigid transform of the source shape is introduced to form a triplet cycle to improve the model's robustness against perturbations. Comprehensive experiments demonstrate that the learned pointwise features through our framework benefits various point cloud analysis tasks, e.g. partial shape registration and keypoint transfer. We also show that the learned pointwise features can be leveraged by supervised methods to improve the part segmentation performance with either the full training dataset or just a small portion of it.



### Adaptive Video Highlight Detection by Learning from User History
- **Arxiv ID**: http://arxiv.org/abs/2007.09598v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.09598v1)
- **Published**: 2020-07-19 05:52:20+00:00
- **Updated**: 2020-07-19 05:52:20+00:00
- **Authors**: Mrigank Rochan, Mahesh Kumar Krishna Reddy, Linwei Ye, Yang Wang
- **Comment**: Accepted to ECCV 2020
- **Journal**: None
- **Summary**: Recently, there is an increasing interest in highlight detection research where the goal is to create a short duration video from a longer video by extracting its interesting moments. However, most existing methods ignore the fact that the definition of video highlight is highly subjective. Different users may have different preferences of highlight for the same input video. In this paper, we propose a simple yet effective framework that learns to adapt highlight detection to a user by exploiting the user's history in the form of highlights that the user has previously created. Our framework consists of two sub-networks: a fully temporal convolutional highlight detection network $H$ that predicts highlight for an input video and a history encoder network $M$ for user history. We introduce a newly designed temporal-adaptive instance normalization (T-AIN) layer to $H$ where the two sub-networks interact with each other. T-AIN has affine parameters that are predicted from $M$ based on the user history and is responsible for the user-adaptive signal to $H$. Extensive experiments on a large-scale dataset show that our framework can make more accurate and user-specific highlight predictions.



### EllSeg: An Ellipse Segmentation Framework for Robust Gaze Tracking
- **Arxiv ID**: http://arxiv.org/abs/2007.09600v2
- **DOI**: 10.1109/TVCG.2021.3067765
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.09600v2)
- **Published**: 2020-07-19 06:13:01+00:00
- **Updated**: 2022-05-04 09:14:20+00:00
- **Authors**: Rakshit S. Kothari, Aayush K. Chaudhary, Reynold J. Bailey, Jeff B. Pelz, Gabriel J. Diaz
- **Comment**: Code available at https://bitbucket.org/RSKothari/ellseg/src/master/
- **Journal**: None
- **Summary**: Ellipse fitting, an essential component in pupil or iris tracking based video oculography, is performed on previously segmented eye parts generated using various computer vision techniques. Several factors, such as occlusions due to eyelid shape, camera position or eyelashes, frequently break ellipse fitting algorithms that rely on well-defined pupil or iris edge segments. In this work, we propose training a convolutional neural network to directly segment entire elliptical structures and demonstrate that such a framework is robust to occlusions and offers superior pupil and iris tracking performance (at least 10$\%$ and 24$\%$ increase in pupil and iris center detection rate respectively within a two-pixel error margin) compared to using standard eye parts segmentation for multiple publicly available synthetic segmentation datasets.



### Symbiotic Adversarial Learning for Attribute-based Person Search
- **Arxiv ID**: http://arxiv.org/abs/2007.09609v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.09609v2)
- **Published**: 2020-07-19 07:24:45+00:00
- **Updated**: 2020-08-24 12:24:34+00:00
- **Authors**: Yu-Tong Cao, Jingya Wang, Dacheng Tao
- **Comment**: 17 pages, 5 figures. Accepted to ECCV2020
- **Journal**: None
- **Summary**: Attribute-based person search is in significant demand for applications where no detected query images are available, such as identifying a criminal from witness. However, the task itself is quite challenging because there is a huge modality gap between images and physical descriptions of attributes. Often, there may also be a large number of unseen categories (attribute combinations). The current state-of-the-art methods either focus on learning better cross-modal embeddings by mining only seen data, or they explicitly use generative adversarial networks (GANs) to synthesize unseen features. The former tends to produce poor embeddings due to insufficient data, while the latter does not preserve intra-class compactness during generation. In this paper, we present a symbiotic adversarial learning framework, called SAL.Two GANs sit at the base of the framework in a symbiotic learning scheme: one synthesizes features of unseen classes/categories, while the other optimizes the embedding and performs the cross-modal alignment on the common embedding space .Specifically, two different types of generative adversarial networks learn collaboratively throughout the training process and the interactions between the two mutually benefit each other. Extensive evaluations show SAL's superiority over nine state-of-the-art methods with two challenging pedestrian benchmarks, PETA and Market-1501. The code is publicly available at: https://github.com/ycao5602/SAL .



### Self-similarity Student for Partial Label Histopathology Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2007.09610v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.09610v1)
- **Published**: 2020-07-19 07:34:18+00:00
- **Updated**: 2020-07-19 07:34:18+00:00
- **Authors**: Hsien-Tzu Cheng, Chun-Fu Yeh, Po-Chen Kuo, Andy Wei, Keng-Chi Liu, Mong-Chi Ko, Kuan-Hua Chao, Yu-Ching Peng, Tyng-Luh Liu
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: Delineation of cancerous regions in gigapixel whole slide images (WSIs) is a crucial diagnostic procedure in digital pathology. This process is time-consuming because of the large search space in the gigapixel WSIs, causing chances of omission and misinterpretation at indistinct tumor lesions. To tackle this, the development of an automated cancerous region segmentation method is imperative. We frame this issue as a modeling problem with partial label WSIs, where some cancerous regions may be misclassified as benign and vice versa, producing patches with noisy labels. To learn from these patches, we propose Self-similarity Student, combining teacher-student model paradigm with similarity learning. Specifically, for each patch, we first sample its similar and dissimilar patches according to spatial distance. A teacher-student model is then introduced, featuring the exponential moving average on both student model weights and teacher predictions ensemble. While our student model takes patches, teacher model takes all their corresponding similar and dissimilar patches for learning robust representation against noisy label patches. Following this similarity learning, our similarity ensemble merges similar patches' ensembled predictions as the pseudo-label of a given patch to counteract its noisy label. On the CAMELYON16 dataset, our method substantially outperforms state-of-the-art noise-aware learning methods by 5$\%$ and the supervised-trained baseline by 10$\%$ in various degrees of noise. Moreover, our method is superior to the baseline on our TVGH TURP dataset with 2$\%$ improvement, demonstrating the generalizability to more clinical histopathology segmentation tasks.



### Character Region Attention For Text Spotting
- **Arxiv ID**: http://arxiv.org/abs/2007.09629v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.09629v1)
- **Published**: 2020-07-19 09:12:23+00:00
- **Updated**: 2020-07-19 09:12:23+00:00
- **Authors**: Youngmin Baek, Seung Shin, Jeonghun Baek, Sungrae Park, Junyeop Lee, Daehyun Nam, Hwalsuk Lee
- **Comment**: 17 pages, 9 figures, Accepted by ECCV 2020
- **Journal**: None
- **Summary**: A scene text spotter is composed of text detection and recognition modules. Many studies have been conducted to unify these modules into an end-to-end trainable model to achieve better performance. A typical architecture places detection and recognition modules into separate branches, and a RoI pooling is commonly used to let the branches share a visual feature. However, there still exists a chance of establishing a more complimentary connection between the modules when adopting recognizer that uses attention-based decoder and detector that represents spatial information of the character regions. This is possible since the two modules share a common sub-task which is to find the location of the character regions. Based on the insight, we construct a tightly coupled single pipeline model. This architecture is formed by utilizing detection outputs in the recognizer and propagating the recognition loss through the detection stage. The use of character score map helps the recognizer attend better to the character center points, and the recognition loss propagation to the detector module enhances the localization of the character regions. Also, a strengthened sharing stage allows feature rectification and boundary localization of arbitrary-shaped text regions. Extensive experiments demonstrate state-of-the-art performance in publicly available straight and curved benchmark dataset.



### Survey on Deep Learning-based Kuzushiji Recognition
- **Arxiv ID**: http://arxiv.org/abs/2007.09637v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.09637v1)
- **Published**: 2020-07-19 09:46:46+00:00
- **Updated**: 2020-07-19 09:46:46+00:00
- **Authors**: Kazuya Ueki, Tomoka Kojima
- **Comment**: None
- **Journal**: None
- **Summary**: Owing to the overwhelming accuracy of the deep learning method demonstrated at the 2012 image classification competition, deep learning has been successfully applied to a variety of other tasks. The high-precision detection and recognition of Kuzushiji, a Japanese cursive script used for transcribing historical documents, has been made possible through the use of deep learning. In recent years, competitions on Kuzushiji recognition have been held, and many researchers have proposed various recognition methods. This study examines recent research trends, current problems, and future prospects in Kuzushiji recognition using deep learning.



### Distribution-Balanced Loss for Multi-Label Classification in Long-Tailed Datasets
- **Arxiv ID**: http://arxiv.org/abs/2007.09654v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.09654v4)
- **Published**: 2020-07-19 11:50:10+00:00
- **Updated**: 2021-12-04 02:47:46+00:00
- **Authors**: Tong Wu, Qingqiu Huang, Ziwei Liu, Yu Wang, Dahua Lin
- **Comment**: To appear in ECCV 2020 as a spotlight presentation. Code and models
  are available at: https://github.com/wutong16/DistributionBalancedLoss
- **Journal**: Proceedings Of The European Conference On Computer Vision (ECCV),
  2020
- **Summary**: We present a new loss function called Distribution-Balanced Loss for the multi-label recognition problems that exhibit long-tailed class distributions. Compared to conventional single-label classification problem, multi-label recognition problems are often more challenging due to two significant issues, namely the co-occurrence of labels and the dominance of negative labels (when treated as multiple binary classification problems). The Distribution-Balanced Loss tackles these issues through two key modifications to the standard binary cross-entropy loss: 1) a new way to re-balance the weights that takes into account the impact caused by label co-occurrence, and 2) a negative tolerant regularization to mitigate the over-suppression of negative labels. Experiments on both Pascal VOC and COCO show that the models trained with this new loss function achieve significant performance gains over existing methods. Code and models are available at: https://github.com/wutong16/DistributionBalancedLoss .



### Unified cross-modality feature disentangler for unsupervised multi-domain MRI abdomen organs segmentation
- **Arxiv ID**: http://arxiv.org/abs/2007.09669v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.09669v1)
- **Published**: 2020-07-19 13:33:41+00:00
- **Updated**: 2020-07-19 13:33:41+00:00
- **Authors**: Jue Jiang, Harini Veeraraghavan
- **Comment**: This paper has been accepted by MICCAI2020
- **Journal**: MICCAI 2020
- **Summary**: Our contribution is a unified cross-modality feature disentagling approach for multi-domain image translation and multiple organ segmentation. Using CT as the labeled source domain, our approach learns to segment multi-modal (T1-weighted and T2-weighted) MRI having no labeled data. Our approach uses a variational auto-encoder (VAE) to disentangle the image content from style. The VAE constrains the style feature encoding to match a universal prior (Gaussian) that is assumed to span the styles of all the source and target modalities. The extracted image style is converted into a latent style scaling code, which modulates the generator to produce multi-modality images according to the target domain code from the image content features. Finally, we introduce a joint distribution matching discriminator that combines the translated images with task-relevant segmentation probability maps to further constrain and regularize image-to-image (I2I) translations. We performed extensive comparisons to multiple state-of-the-art I2I translation and segmentation methods. Our approach resulted in the lowest average multi-domain image reconstruction error of 1.34$\pm$0.04. Our approach produced an average Dice similarity coefficient (DSC) of 0.85 for T1w and 0.90 for T2w MRI for multi-organ segmentation, which was highly comparable to a fully supervised MRI multi-organ segmentation network (DSC of 0.86 for T1w and 0.90 for T2w MRI).



### Learning Error-Driven Curriculum for Crowd Counting
- **Arxiv ID**: http://arxiv.org/abs/2007.09676v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.09676v1)
- **Published**: 2020-07-19 14:18:55+00:00
- **Updated**: 2020-07-19 14:18:55+00:00
- **Authors**: Wenxi Li, Zhuoqun Cao, Qian Wang, Songjian Chen, Rui Feng
- **Comment**: This paper is accepted by ICPR2020
- **Journal**: None
- **Summary**: Density regression has been widely employed in crowd counting. However, the frequency imbalance of pixel values in the density map is still an obstacle to improve the performance. In this paper, we propose a novel learning strategy for learning error-driven curriculum, which uses an additional network to supervise the training of the main network. A tutoring network called TutorNet is proposed to repetitively indicate the critical errors of the main network. TutorNet generates pixel-level weights to formulate the curriculum for the main network during training, so that the main network will assign a higher weight to those hard examples than easy examples. Furthermore, we scale the density map by a factor to enlarge the distance among inter-examples, which is well known to improve the performance. Extensive experiments on two challenging benchmark datasets show that our method has achieved state-of-the-art performance.



### Class-wise Dynamic Graph Convolution for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2007.09690v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.09690v1)
- **Published**: 2020-07-19 15:26:50+00:00
- **Updated**: 2020-07-19 15:26:50+00:00
- **Authors**: Hanzhe Hu, Deyi Ji, Weihao Gan, Shuai Bai, Wei Wu, Junjie Yan
- **Comment**: Accepted by ECCV2020
- **Journal**: None
- **Summary**: Recent works have made great progress in semantic segmentation by exploiting contextual information in a local or global manner with dilated convolutions, pyramid pooling or self-attention mechanism. In order to avoid potential misleading contextual information aggregation in previous works, we propose a class-wise dynamic graph convolution (CDGC) module to adaptively propagate information. The graph reasoning is performed among pixels in the same class. Based on the proposed CDGC module, we further introduce the Class-wise Dynamic Graph Convolution Network(CDGCNet), which consists of two main parts including the CDGC module and a basic segmentation network, forming a coarse-to-fine paradigm. Specifically, the CDGC module takes the coarse segmentation result as class mask to extract node features for graph construction and performs dynamic graph convolutions on the constructed graph to learn the feature aggregation and weight allocation. Then the refined feature and the original feature are fused to get the final prediction. We conduct extensive experiments on three popular semantic segmentation benchmarks including Cityscapes, PASCAL VOC 2012 and COCO Stuff, and achieve state-of-the-art performance on all three benchmarks.



### Using Deep Convolutional Neural Networks to Diagnose COVID-19 From Chest X-Ray Images
- **Arxiv ID**: http://arxiv.org/abs/2007.09695v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.09695v1)
- **Published**: 2020-07-19 15:47:37+00:00
- **Updated**: 2020-07-19 15:47:37+00:00
- **Authors**: Yi Zhong
- **Comment**: None
- **Journal**: None
- **Summary**: The COVID-19 epidemic has become a major safety and health threat worldwide. Imaging diagnosis is one of the most effective ways to screen COVID-19. This project utilizes several open-source or public datasets to present an open-source dataset of COVID-19 CXRs, named COVID-19-CXR-Dataset, and introduces a deep convolutional neural network model. The model validates on 740 test images and achieves 87.3% accuracy, 89.67 % precision, and 84.46% recall, and correctly classifies 98 out of 100 COVID-19 x-ray images in test set with more than 81% prediction probability under the condition of 95% confidence interval. This project may serve as a reference for other researchers aiming to advance the development of deep learning applications in medical imaging.



### Improving the HardNet Descriptor
- **Arxiv ID**: http://arxiv.org/abs/2007.09699v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.09699v2)
- **Published**: 2020-07-19 16:05:09+00:00
- **Updated**: 2020-11-23 23:02:04+00:00
- **Authors**: Milan Pultar
- **Comment**: The thesis was supervised by Dmytro Mishkin. Many pieces of advice
  came from Ji\v{r}\'i Matas
- **Journal**: None
- **Summary**: In the thesis we consider the problem of local feature descriptor learning for wide baseline stereo focusing on the HardNet descriptor, which is close to state-of-the-art. AMOS Patches dataset is introduced, which improves robustness to illumination and appearance changes. It is based on registered images from selected cameras from the AMOS dataset. We provide recommendations on the patch dataset creation process and evaluate HardNet trained on data of different modalities. We also introduce a dataset combination and reduction methods, that allow comparable performance on a significantly smaller dataset.   HardNet8, consistently outperforming the original HardNet, benefits from the architectural choices made: connectivity pattern, final pooling, receptive field, CNN building blocks found by manual or automatic search algorithms -- DARTS. We show impact of overlooked hyperparameters such as batch size and length of training on the descriptor quality. PCA dimensionality reduction further boosts performance and also reduces memory footprint.   Finally, the insights gained lead to two HardNet8 descriptors: one performing well on a variety of benchmarks -- HPatches, AMOS Patches and IMW Phototourism, the other is optimized for IMW Phototourism.



### Geometry Constrained Weakly Supervised Object Localization
- **Arxiv ID**: http://arxiv.org/abs/2007.09727v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.09727v1)
- **Published**: 2020-07-19 17:33:42+00:00
- **Updated**: 2020-07-19 17:33:42+00:00
- **Authors**: Weizeng Lu, Xi Jia, Weicheng Xie, Linlin Shen, Yicong Zhou, Jinming Duan
- **Comment**: This paper (ID 5424) is accepted to ECCV 2020
- **Journal**: None
- **Summary**: We propose a geometry constrained network, termed GC-Net, for weakly supervised object localization (WSOL). GC-Net consists of three modules: a detector, a generator and a classifier. The detector predicts the object location defined by a set of coefficients describing a geometric shape (i.e. ellipse or rectangle), which is geometrically constrained by the mask produced by the generator. The classifier takes the resulting masked images as input and performs two complementary classification tasks for the object and background. To make the mask more compact and more complete, we propose a novel multi-task loss function that takes into account area of the geometric shape, the categorical cross-entropy and the negative entropy. In contrast to previous approaches, GC-Net is trained end-to-end and predict object location without any post-processing (e.g. thresholding) that may require additional tuning. Extensive experiments on the CUB-200-2011 and ILSVRC2012 datasets show that GC-Net outperforms state-of-the-art methods by a large margin. Our source code is available at https://github.com/lwzeng/GC-Net.



### Kinematics of motion tracking using computer vision
- **Arxiv ID**: http://arxiv.org/abs/2008.00813v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.00813v1)
- **Published**: 2020-07-19 18:02:13+00:00
- **Updated**: 2020-07-19 18:02:13+00:00
- **Authors**: Jos√© L. Escalona
- **Comment**: None
- **Journal**: None
- **Summary**: This paper describes the kinematics of the motion tracking of a rigid body using video recording. The novelty of the paper is on the adaptation of the methods and nomenclature used in Computer Vision to those used in Multibody System Dynamics. That way, the equations presented here can be used, for example, for inverse-dynamics multibody simulations driven by the motion tracking of selected bodies. This paper also adapts the well-known Zhang calibration method to the presented nomenclature.



### Beyond Single Stage Encoder-Decoder Networks: Deep Decoders for Semantic Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2007.09746v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2007.09746v1)
- **Published**: 2020-07-19 18:44:34+00:00
- **Updated**: 2020-07-19 18:44:34+00:00
- **Authors**: Gabriel L. Oliveira, Senthil Yogamani, Wolfram Burgard, Thomas Brox
- **Comment**: None
- **Journal**: None
- **Summary**: Single encoder-decoder methodologies for semantic segmentation are reaching their peak in terms of segmentation quality and efficiency per number of layers. To address these limitations, we propose a new architecture based on a decoder which uses a set of shallow networks for capturing more information content. The new decoder has a new topology of skip connections, namely backward and stacked residual connections. In order to further improve the architecture we introduce a weight function which aims to re-balance classes to increase the attention of the networks to under-represented objects. We carried out an extensive set of experiments that yielded state-of-the-art results for the CamVid, Gatech and Freiburg Forest datasets. Moreover, to further prove the effectiveness of our decoder, we conducted a set of experiments studying the impact of our decoder to state-of-the-art segmentation techniques. Additionally, we present a set of experiments augmenting semantic segmentation with optical flow information, showing that motion clues can boost pure image based semantic segmentation approaches.



### A Generic Visualization Approach for Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2007.09748v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.09748v1)
- **Published**: 2020-07-19 18:46:56+00:00
- **Updated**: 2020-07-19 18:46:56+00:00
- **Authors**: Ahmed Taha, Xitong Yang, Abhinav Shrivastava, Larry Davis
- **Comment**: ECCV'2020
- **Journal**: None
- **Summary**: Retrieval networks are essential for searching and indexing. Compared to classification networks, attention visualization for retrieval networks is hardly studied. We formulate attention visualization as a constrained optimization problem. We leverage the unit L2-Norm constraint as an attention filter (L2-CAF) to localize attention in both classification and retrieval networks. Unlike recent literature, our approach requires neither architectural changes nor fine-tuning. Thus, a pre-trained network's performance is never undermined   L2-CAF is quantitatively evaluated using weakly supervised object localization. State-of-the-art results are achieved on classification networks. For retrieval networks, significant improvement margins are achieved over a Grad-CAM baseline. Qualitative evaluation demonstrates how the L2-CAF visualizes attention per frame for a recurrent retrieval network. Further ablation studies highlight the computational cost of our approach and compare L2-CAF with other feasible alternatives. Code available at https://bit.ly/3iDBLFv



### Full Quaternion Representation of Color images: A Case Study on QSVD-based Color Image Compression
- **Arxiv ID**: http://arxiv.org/abs/2007.09758v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2007.09758v1)
- **Published**: 2020-07-19 19:13:21+00:00
- **Updated**: 2020-07-19 19:13:21+00:00
- **Authors**: Alireza Parchami, Mojtaba Mahdavi
- **Comment**: 15 pages, 16 figures, 1 table, submitted to Signal Processing journal
- **Journal**: None
- **Summary**: For many years, channels of a color image have been processed individually, or the image has been converted to grayscale one with respect to color image processing. Pure quaternion representation of color images solves this issue as it allows images to be processed in a holistic space. Nevertheless, it brings additional costs due to the extra fourth dimension. In this paper, we propose an approach for representing color images with full quaternion numbers that enables us to process color images holistically without additional cost in time, space and computation. With taking auto- and cross-correlation of color channels into account, an autoencoder neural network is used to generate a global model for transforming a color image into a full quaternion matrix. To evaluate the model, we use UCID dataset, and the results indicate that the model has an acceptable performance on color images. Moreover, we propose a compression method based on the generated model and QSVD as a case study. The method is compared with the same compression method using pure quaternion representation and is assessed with UCID dataset. The results demonstrate that the compression method using the proposed full quaternion representation fares better than the other in terms of time, quality, and size of compressed files.



### Connecting the Dots: Detecting Adversarial Perturbations Using Context Inconsistency
- **Arxiv ID**: http://arxiv.org/abs/2007.09763v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.09763v2)
- **Published**: 2020-07-19 19:46:45+00:00
- **Updated**: 2020-07-24 17:02:41+00:00
- **Authors**: Shasha Li, Shitong Zhu, Sudipta Paul, Amit Roy-Chowdhury, Chengyu Song, Srikanth Krishnamurthy, Ananthram Swami, Kevin S Chan
- **Comment**: The paper is accepted by ECCV 2020
- **Journal**: None
- **Summary**: There has been a recent surge in research on adversarial perturbations that defeat Deep Neural Networks (DNNs) in machine vision; most of these perturbation-based attacks target object classifiers. Inspired by the observation that humans are able to recognize objects that appear out of place in a scene or along with other unlikely objects, we augment the DNN with a system that learns context consistency rules during training and checks for the violations of the same during testing. Our approach builds a set of auto-encoders, one for each object class, appropriately trained so as to output a discrepancy between the input and output if an added adversarial perturbation violates context consistency rules. Experiments on PASCAL VOC and MS COCO show that our method effectively detects various adversarial attacks and achieves high ROC-AUC (over 0.95 in most cases); this corresponds to over 20% improvement over a state-of-the-art context-agnostic method.



### Exploiting vulnerabilities of deep neural networks for privacy protection
- **Arxiv ID**: http://arxiv.org/abs/2007.09766v1
- **DOI**: 10.1109/TMM.2020.2987694
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.09766v1)
- **Published**: 2020-07-19 20:03:42+00:00
- **Updated**: 2020-07-19 20:03:42+00:00
- **Authors**: Ricardo Sanchez-Matilla, Chau Yi Li, Ali Shahin Shamsabadi, Riccardo Mazzon, Andrea Cavallaro
- **Comment**: None
- **Journal**: IEEE Transactions on Multimedia 2020
- **Summary**: Adversarial perturbations can be added to images to protect their content from unwanted inferences. These perturbations may, however, be ineffective against classifiers that were not {seen} during the generation of the perturbation, or against defenses {based on re-quantization, median filtering or JPEG compression. To address these limitations, we present an adversarial attack {that is} specifically designed to protect visual content against { unseen} classifiers and known defenses. We craft perturbations using an iterative process that is based on the Fast Gradient Signed Method and {that} randomly selects a classifier and a defense, at each iteration}. This randomization prevents an undesirable overfitting to a specific classifier or defense. We validate the proposed attack in both targeted and untargeted settings on the private classes of the Places365-Standard dataset. Using ResNet18, ResNet50, AlexNet and DenseNet161 {as classifiers}, the performance of the proposed attack exceeds that of eleven state-of-the-art attacks. The implementation is available at https://github.com/smartcameras/RP-FGSM/.



### Deep Representation Learning For Multimodal Brain Networks
- **Arxiv ID**: http://arxiv.org/abs/2007.09777v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.09777v1)
- **Published**: 2020-07-19 20:32:05+00:00
- **Updated**: 2020-07-19 20:32:05+00:00
- **Authors**: Wen Zhang, Liang Zhan, Paul Thompson, Yalin Wang
- **Comment**: 11 pages, 3 figures, MICCAI 2020
- **Journal**: None
- **Summary**: Applying network science approaches to investigate the functions and anatomy of the human brain is prevalent in modern medical imaging analysis. Due to the complex network topology, for an individual brain, mining a discriminative network representation from the multimodal brain networks is non-trivial. The recent success of deep learning techniques on graph-structured data suggests a new way to model the non-linear cross-modality relationship. However, current deep brain network methods either ignore the intrinsic graph topology or require a network basis shared within a group. To address these challenges, we propose a novel end-to-end deep graph representation learning (Deep Multimodal Brain Networks - DMBN) to fuse multimodal brain networks. Specifically, we decipher the cross-modality relationship through a graph encoding and decoding process. The higher-order network mappings from brain structural networks to functional networks are learned in the node domain. The learned network representation is a set of node features that are informative to induce brain saliency maps in a supervised manner. We test our framework in both synthetic and real image data. The experimental results show the superiority of the proposed method over some other state-of-the-art deep brain network models.



### ASAP-NMS: Accelerating Non-Maximum Suppression Using Spatially Aware Priors
- **Arxiv ID**: http://arxiv.org/abs/2007.09785v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.09785v2)
- **Published**: 2020-07-19 21:15:48+00:00
- **Updated**: 2020-08-21 15:18:57+00:00
- **Authors**: Rohun Tripathi, Vasu Singla, Mahyar Najibi, Bharat Singh, Abhishek Sharma, Larry Davis
- **Comment**: Under Review at CVIU
- **Journal**: None
- **Summary**: The widely adopted sequential variant of Non Maximum Suppression (or Greedy-NMS) is a crucial module for object-detection pipelines. Unfortunately, for the region proposal stage of two/multi-stage detectors, NMS is turning out to be a latency bottleneck due to its sequential nature. In this article, we carefully profile Greedy-NMS iterations to find that a major chunk of computation is wasted in comparing proposals that are already far-away and have a small chance of suppressing each other. We address this issue by comparing only those proposals that are generated from nearby anchors. The translation-invariant property of the anchor lattice affords generation of a lookup table, which provides an efficient access to nearby proposals, during NMS. This leads to an Accelerated NMS algorithm which leverages Spatially Aware Priors, or ASAP-NMS, and improves the latency of the NMS step from 13.6ms to 1.2 ms on a CPU without sacrificing the accuracy of a state-of-the-art two-stage detector on COCO and VOC datasets. Importantly, ASAP-NMS is agnostic to image resolution and can be used as a simple drop-in module during inference. Using ASAP-NMS at run-time only, we obtain an mAP of 44.2\%@25Hz on the COCO dataset with a V100 GPU.



### Generative Adversarial Stacked Autoencoders for Facial Pose Normalization and Emotion Recognition
- **Arxiv ID**: http://arxiv.org/abs/2007.09790v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.09790v1)
- **Published**: 2020-07-19 21:47:16+00:00
- **Updated**: 2020-07-19 21:47:16+00:00
- **Authors**: Ariel Ruiz-Garcia, Vasile Palade, Mark Elshaw, Mariette Awad
- **Comment**: Accepted at IJCNN 2020
- **Journal**: None
- **Summary**: In this work, we propose a novel Generative Adversarial Stacked Autoencoder that learns to map facial expressions, with up to plus or minus 60 degrees, to an illumination invariant facial representation of 0 degrees. We accomplish this by using a novel convolutional layer that exploits both local and global spatial information, and a convolutional layer with a reduced number of parameters that exploits facial symmetry. Furthermore, we introduce a generative adversarial gradual greedy layer-wise learning algorithm designed to train Adversarial Autoencoders in an efficient and incremental manner. We demonstrate the efficiency of our method and report state-of-the-art performance on several facial emotion recognition corpora, including one collected in the wild.



### E$^2$Net: An Edge Enhanced Network for Accurate Liver and Tumor Segmentation on CT Scans
- **Arxiv ID**: http://arxiv.org/abs/2007.09791v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.09791v1)
- **Published**: 2020-07-19 21:50:22+00:00
- **Updated**: 2020-07-19 21:50:22+00:00
- **Authors**: Youbao Tang, Yuxing Tang, Yingying Zhu, Jing Xiao, Ronald M. Summers
- **Comment**: None
- **Journal**: None
- **Summary**: Developing an effective liver and liver tumor segmentation model from CT scans is very important for the success of liver cancer diagnosis, surgical planning and cancer treatment. In this work, we propose a two-stage framework for 2D liver and tumor segmentation. The first stage is a coarse liver segmentation network, while the second stage is an edge enhanced network (E$^2$Net) for more accurate liver and tumor segmentation. E$^2$Net explicitly models complementary objects (liver and tumor) and their edge information within the network to preserve the organ and lesion boundaries. We introduce an edge prediction module in E$^2$Net and design an edge distance map between liver and tumor boundaries, which is used as an extra supervision signal to train the edge enhanced network. We also propose a deep cross feature fusion module to refine multi-scale features from both objects and their edges. E$^2$Net is more easily and efficiently trained with a small labeled dataset, and it can be trained/tested on the original 2D CT slices (resolve resampling error issue in 3D models). The proposed framework has shown superior performance on both liver and liver tumor segmentation compared to several state-of-the-art 2D, 3D and 2D/3D hybrid frameworks.



### Learning to Generate Customized Dynamic 3D Facial Expressions
- **Arxiv ID**: http://arxiv.org/abs/2007.09805v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.09805v2)
- **Published**: 2020-07-19 22:38:43+00:00
- **Updated**: 2020-07-21 16:18:15+00:00
- **Authors**: Rolandos Alexandros Potamias, Jiali Zheng, Stylianos Ploumpis, Giorgos Bouritsas, Evangelos Ververas, Stefanos Zafeiriou
- **Comment**: accepted at European Conference on Computer Vision 2020 (ECCV)
- **Journal**: None
- **Summary**: Recent advances in deep learning have significantly pushed the state-of-the-art in photorealistic video animation given a single image. In this paper, we extrapolate those advances to the 3D domain, by studying 3D image-to-video translation with a particular focus on 4D facial expressions. Although 3D facial generative models have been widely explored during the past years, 4D animation remains relatively unexplored. To this end, in this study we employ a deep mesh encoder-decoder like architecture to synthesize realistic high resolution facial expressions by using a single neutral frame along with an expression identification. In addition, processing 3D meshes remains a non-trivial task compared to data that live on grid-like structures, such as images. Given the recent progress in mesh processing with graph convolutions, we make use of a recently introduced learnable operator which acts directly on the mesh structure by taking advantage of local vertex orderings. In order to generalize to 4D facial expressions across subjects, we trained our model using a high resolution dataset with 4D scans of six facial expressions from 180 subjects. Experimental results demonstrate that our approach preserves the subject's identity information even for unseen subjects and generates high quality expressions. To the best of our knowledge, this is the first study tackling the problem of 4D facial expression synthesis.



### DBQ: A Differentiable Branch Quantizer for Lightweight Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2007.09818v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.09818v1)
- **Published**: 2020-07-19 23:50:09+00:00
- **Updated**: 2020-07-19 23:50:09+00:00
- **Authors**: Hassan Dbouk, Hetul Sanghvi, Mahesh Mehendale, Naresh Shanbhag
- **Comment**: Published as a conference paper in ECCV 2020
- **Journal**: None
- **Summary**: Deep neural networks have achieved state-of-the art performance on various computer vision tasks. However, their deployment on resource-constrained devices has been hindered due to their high computational and storage complexity. While various complexity reduction techniques, such as lightweight network architecture design and parameter quantization, have been successful in reducing the cost of implementing these networks, these methods have often been considered orthogonal. In reality, existing quantization techniques fail to replicate their success on lightweight architectures such as MobileNet. To this end, we present a novel fully differentiable non-uniform quantizer that can be seamlessly mapped onto efficient ternary-based dot product engines. We conduct comprehensive experiments on CIFAR-10, ImageNet, and Visual Wake Words datasets. The proposed quantizer (DBQ) successfully tackles the daunting task of aggressively quantizing lightweight networks such as MobileNetV1, MobileNetV2, and ShuffleNetV2. DBQ achieves state-of-the art results with minimal training overhead and provides the best (pareto-optimal) accuracy-complexity trade-off.



