# Arxiv Papers in cs.CV on 2020-07-04
### Structure-Aware Human-Action Generation
- **Arxiv ID**: http://arxiv.org/abs/2007.01971v3
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.01971v3)
- **Published**: 2020-07-04 00:18:27+00:00
- **Updated**: 2020-08-16 20:05:43+00:00
- **Authors**: Ping Yu, Yang Zhao, Chunyuan Li, Junsong Yuan, Changyou Chen
- **Comment**: accepted by ECCV 2020
- **Journal**: None
- **Summary**: Generating long-range skeleton-based human actions has been a challenging problem since small deviations of one frame can cause a malformed action sequence. Most existing methods borrow ideas from video generation, which naively treat skeleton nodes/joints as pixels of images without considering the rich inter-frame and intra-frame structure information, leading to potential distorted actions. Graph convolutional networks (GCNs) is a promising way to leverage structure information to learn structure representations. However, directly adopting GCNs to tackle such continuous action sequences both in spatial and temporal spaces is challenging as the action graph could be huge. To overcome this issue, we propose a variant of GCNs to leverage the powerful self-attention mechanism to adaptively sparsify a complete action graph in the temporal space. Our method could dynamically attend to important past frames and construct a sparse graph to apply in the GCN framework, well-capturing the structure information in action sequences. Extensive experimental results demonstrate the superiority of our method on two standard human action datasets compared with existing methods.



### Interpretation of Disease Evidence for Medical Images Using Adversarial Deformation Fields
- **Arxiv ID**: http://arxiv.org/abs/2007.01975v2
- **DOI**: 10.1007/978-3-030-59713-9_71
- **Categories**: **eess.IV**, cs.CV, cs.GR, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2007.01975v2)
- **Published**: 2020-07-04 00:51:54+00:00
- **Updated**: 2023-04-20 02:05:48+00:00
- **Authors**: Ricardo Bigolin Lanfredi, Joyce D. Schroeder, Clement Vachet, Tolga Tasdizen
- **Comment**: Presented at MICCAI 2020
- **Journal**: None
- **Summary**: The high complexity of deep learning models is associated with the difficulty of explaining what evidence they recognize as correlating with specific disease labels. This information is critical for building trust in models and finding their biases. Until now, automated deep learning visualization solutions have identified regions of images used by classifiers, but these solutions are too coarse, too noisy, or have a limited representation of the way images can change. We propose a novel method for formulating and presenting spatial explanations of disease evidence, called deformation field interpretation with generative adversarial networks (DeFI-GAN). An adversarially trained generator produces deformation fields that modify images of diseased patients to resemble images of healthy patients. We validate the method studying chronic obstructive pulmonary disease (COPD) evidence in chest x-rays (CXRs) and Alzheimer's disease (AD) evidence in brain MRIs. When extracting disease evidence in longitudinal data, we show compelling results against a baseline producing difference maps. DeFI-GAN also highlights disease biomarkers not found by previous methods and potential biases that may help in investigations of the dataset and of the adopted learning methods.



### A Survey on Sensor Technologies for Unmanned Ground Vehicles
- **Arxiv ID**: http://arxiv.org/abs/2007.01992v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2007.01992v2)
- **Published**: 2020-07-04 03:15:13+00:00
- **Updated**: 2020-07-12 04:51:07+00:00
- **Authors**: Qi Liu, Shihua Yuan, Zirui Li
- **Comment**: 9 pages,6 tables
- **Journal**: None
- **Summary**: Unmanned ground vehicles have a huge development potential in both civilian and military fields, and have become the focus of research in various countries. In addition, high-precision, high-reliability sensors are significant for UGVs' efficient operation. This paper proposes a brief review on sensor technologies for UGVs. Firstly, characteristics of various sensors are introduced. Then the strengths and weaknesses of different sensors as well as their application scenarios are compared. Furthermore, sensor applications in some existing UGVs are summarized. Finally, the hotspots of sensor technologies are forecasted to point the development direction.



### DessiLBI: Exploring Structural Sparsity of Deep Networks via Differential Inclusion Paths
- **Arxiv ID**: http://arxiv.org/abs/2007.02010v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, math.DS, stat.AP, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.02010v1)
- **Published**: 2020-07-04 04:40:16+00:00
- **Updated**: 2020-07-04 04:40:16+00:00
- **Authors**: Yanwei Fu, Chen Liu, Donghao Li, Xinwei Sun, Jinshan Zeng, Yuan Yao
- **Comment**: conference , 23 pages https://github.com/corwinliu9669/dS2LBI. arXiv
  admin note: text overlap with arXiv:1905.09449
- **Journal**: ICML 2020
- **Summary**: Over-parameterization is ubiquitous nowadays in training neural networks to benefit both optimization in seeking global optima and generalization in reducing prediction error. However, compressive networks are desired in many real world applications and direct training of small networks may be trapped in local optima. In this paper, instead of pruning or distilling over-parameterized models to compressive ones, we propose a new approach based on differential inclusions of inverse scale spaces. Specifically, it generates a family of models from simple to complex ones that couples a pair of parameters to simultaneously train over-parameterized deep models and structural sparsity on weights of fully connected and convolutional layers. Such a differential inclusion scheme has a simple discretization, proposed as Deep structurally splitting Linearized Bregman Iteration (DessiLBI), whose global convergence analysis in deep learning is established that from any initializations, algorithmic iterations converge to a critical point of empirical risks. Experimental evidence shows that DessiLBI achieve comparable and even better performance than the competitive optimizers in exploring the structural sparsity of several widely used backbones on the benchmark datasets. Remarkably, with early stopping, DessiLBI unveils "winning tickets" in early epochs: the effective sparse structure with comparable test accuracy to fully trained over-parameterized models.



### FracBits: Mixed Precision Quantization via Fractional Bit-Widths
- **Arxiv ID**: http://arxiv.org/abs/2007.02017v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.02017v2)
- **Published**: 2020-07-04 06:09:09+00:00
- **Updated**: 2020-12-03 03:22:55+00:00
- **Authors**: Linjie Yang, Qing Jin
- **Comment**: Accepted by AAAI 2021
- **Journal**: None
- **Summary**: Model quantization helps to reduce model size and latency of deep neural networks. Mixed precision quantization is favorable with customized hardwares supporting arithmetic operations at multiple bit-widths to achieve maximum efficiency. We propose a novel learning-based algorithm to derive mixed precision models end-to-end under target computation constraints and model sizes. During the optimization, the bit-width of each layer / kernel in the model is at a fractional status of two consecutive bit-widths which can be adjusted gradually. With a differentiable regularization term, the resource constraints can be met during the quantization-aware training which results in an optimized mixed precision model. Further, our method can be naturally combined with channel pruning for better computation cost allocation. Our final models achieve comparable or better performance than previous quantization methods with mixed precision on MobilenetV1/V2, ResNet18 under different resource constraints on ImageNet dataset.



### Deep Bilateral Retinex for Low-Light Image Enhancement
- **Arxiv ID**: http://arxiv.org/abs/2007.02018v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.02018v1)
- **Published**: 2020-07-04 06:26:44+00:00
- **Updated**: 2020-07-04 06:26:44+00:00
- **Authors**: Jinxiu Liang, Yong Xu, Yuhui Quan, Jingwen Wang, Haibin Ling, Hui Ji
- **Comment**: 15 pages
- **Journal**: None
- **Summary**: Low-light images, i.e. the images captured in low-light conditions, suffer from very poor visibility caused by low contrast, color distortion and significant measurement noise. Low-light image enhancement is about improving the visibility of low-light images. As the measurement noise in low-light images is usually significant yet complex with spatially-varying characteristic, how to handle the noise effectively is an important yet challenging problem in low-light image enhancement. Based on the Retinex decomposition of natural images, this paper proposes a deep learning method for low-light image enhancement with a particular focus on handling the measurement noise. The basic idea is to train a neural network to generate a set of pixel-wise operators for simultaneously predicting the noise and the illumination layer, where the operators are defined in the bilateral space. Such an integrated approach allows us to have an accurate prediction of the reflectance layer in the presence of significant spatially-varying measurement noise. Extensive experiments on several benchmark datasets have shown that the proposed method is very competitive to the state-of-the-art methods, and has significant advantage over others when processing images captured in extremely low lighting conditions.



### Alpha-Refine: Boosting Tracking Performance by Precise Bounding Box Estimation
- **Arxiv ID**: http://arxiv.org/abs/2007.02024v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.02024v2)
- **Published**: 2020-07-04 07:02:25+00:00
- **Updated**: 2021-04-05 10:50:13+00:00
- **Authors**: Bin Yan, Dong Wang, Huchuan Lu, Xiaoyun Yang
- **Comment**: This version is out-of-update. Please refer to the latest one
  arXiv:2012.06815
- **Journal**: None
- **Summary**: In recent years, the multiple-stage strategy has become a popular trend for visual tracking. This strategy first utilizes a base tracker to coarsely locate the target and then exploits a refinement module to obtain more accurate results. However, existing refinement modules suffer from the limited transferability and precision. In this work, we propose a novel, flexible and accurate refinement module called Alpha-Refine, which exploits a precise pixel-wise correlation layer together with a spatial-aware non-local layer to fuse features and can predict three complementary outputs: bounding box, corners and mask. To wisely choose the most adequate output, we also design a light-weight branch selector module. We apply the proposed Alpha-Refine module to five famous and state-of-the-art base trackers: DiMP, ATOM, SiamRPN++, RTMDNet and ECO. The comprehensive experiments on TrackingNet, LaSOT and VOT2018 benchmarks demonstrate that our approach significantly improves the tracking performance in comparison with other existing refinement methods. The source codes will be available at https://github.com/MasterBin-IIAU/AlphaRefine.



### DRDr: Automatic Masking of Exudates and Microaneurysms Caused By Diabetic Retinopathy Using Mask R-CNN and Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/2007.02026v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.02026v1)
- **Published**: 2020-07-04 07:20:03+00:00
- **Updated**: 2020-07-04 07:20:03+00:00
- **Authors**: Farzan Shenavarmasouleh, Hamid R. Arabnia
- **Comment**: The 6th International Conference on Health Informatics & Medical
  Systems (HIMS'20: July 27-30, 2020, USA)
- **Journal**: None
- **Summary**: This paper addresses the problem of identifying two main types of lesions - Exudates and Microaneurysms - caused by Diabetic Retinopathy (DR) in the eyes of diabetic patients. We make use of Convolutional Neural Networks (CNNs) and Transfer Learning to locate and generate high-quality segmentation mask for each instance of the lesion that can be found in the patients' fundus images. We create our normalized database out of e-ophtha EX and e-ophtha MA and tweak Mask R-CNN to detect small lesions. Moreover, we employ data augmentation and the pre-trained weights of ResNet101 to compensate for our small dataset. Our model achieves promising test mAP of 0.45, altogether showing that it can aid clinicians and ophthalmologist in the process of detecting and treating the infamous DR.



### Shape-aware Meta-learning for Generalizing Prostate MRI Segmentation to Unseen Domains
- **Arxiv ID**: http://arxiv.org/abs/2007.02035v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.02035v1)
- **Published**: 2020-07-04 07:56:02+00:00
- **Updated**: 2020-07-04 07:56:02+00:00
- **Authors**: Quande Liu, Qi Dou, Pheng-Ann Heng
- **Comment**: Early accepted by MICCAI 2020; code and dataset are available at
  https://github.com/liuquande/SAML
- **Journal**: None
- **Summary**: Model generalization capacity at domain shift (e.g., various imaging protocols and scanners) is crucial for deep learning methods in real-world clinical deployment. This paper tackles the challenging problem of domain generalization, i.e., learning a model from multi-domain source data such that it can directly generalize to an unseen target domain. We present a novel shape-aware meta-learning scheme to improve the model generalization in prostate MRI segmentation. Our learning scheme roots in the gradient-based meta-learning, by explicitly simulating domain shift with virtual meta-train and meta-test during training. Importantly, considering the deficiencies encountered when applying a segmentation model to unseen domains (i.e., incomplete shape and ambiguous boundary of the prediction masks), we further introduce two complementary loss objectives to enhance the meta-optimization, by particularly encouraging the shape compactness and shape smoothness of the segmentations under simulated domain shift. We evaluate our method on prostate MRI data from six different institutions with distribution shifts acquired from public datasets. Experimental results show that our approach outperforms many state-of-the-art generalization methods consistently across all six settings of unseen domains.



### Modality Shifting Attention Network for Multi-modal Video Question Answering
- **Arxiv ID**: http://arxiv.org/abs/2007.02036v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.02036v1)
- **Published**: 2020-07-04 08:01:10+00:00
- **Updated**: 2020-07-04 08:01:10+00:00
- **Authors**: Junyeong Kim, Minuk Ma, Trung Pham, Kyungsu Kim, Chang D. Yoo
- **Comment**: CVPR2020 accepted; poster
- **Journal**: None
- **Summary**: This paper considers a network referred to as Modality Shifting Attention Network (MSAN) for Multimodal Video Question Answering (MVQA) task. MSAN decomposes the task into two sub-tasks: (1) localization of temporal moment relevant to the question, and (2) accurate prediction of the answer based on the localized moment. The modality required for temporal localization may be different from that for answer prediction, and this ability to shift modality is essential for performing the task. To this end, MSAN is based on (1) the moment proposal network (MPN) that attempts to locate the most appropriate temporal moment from each of the modalities, and also on (2) the heterogeneous reasoning network (HRN) that predicts the answer using an attention mechanism on both modalities. MSAN is able to place importance weight on the two modalities for each sub-task using a component referred to as Modality Importance Modulation (MIM). Experimental results show that MSAN outperforms previous state-of-the-art by achieving 71.13\% test accuracy on TVQA benchmark dataset. Extensive ablation studies and qualitative analysis are conducted to validate various components of the network.



### Jointly Modeling Motion and Appearance Cues for Robust RGB-T Tracking
- **Arxiv ID**: http://arxiv.org/abs/2007.02041v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.02041v1)
- **Published**: 2020-07-04 08:11:33+00:00
- **Updated**: 2020-07-04 08:11:33+00:00
- **Authors**: Pengyu Zhang, Jie Zhao, Dong Wang, Huchuan Lu, Xiaoyun Yang
- **Comment**: 12 pages, 11 figures
- **Journal**: None
- **Summary**: In this study, we propose a novel RGB-T tracking framework by jointly modeling both appearance and motion cues. First, to obtain a robust appearance model, we develop a novel late fusion method to infer the fusion weight maps of both RGB and thermal (T) modalities. The fusion weights are determined by using offline-trained global and local multimodal fusion networks, and then adopted to linearly combine the response maps of RGB and T modalities. Second, when the appearance cue is unreliable, we comprehensively take motion cues, i.e., target and camera motions, into account to make the tracker robust. We further propose a tracker switcher to switch the appearance and motion trackers flexibly. Numerous results on three recent RGB-T tracking datasets show that the proposed tracker performs significantly better than other state-of-the-art algorithms.



### Single Image Brightening via Multi-Scale Exposure Fusion with Hybrid Learning
- **Arxiv ID**: http://arxiv.org/abs/2007.02042v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.02042v2)
- **Published**: 2020-07-04 08:23:07+00:00
- **Updated**: 2020-07-12 09:36:36+00:00
- **Authors**: Chaobing Zheng, Zhengguo Li, Yi Yang, Shiqian Wu
- **Comment**: 11 pages
- **Journal**: None
- **Summary**: A small ISO and a small exposure time are usually used to capture an image in the back or low light conditions which results in an image with negligible motion blur and small noise but look dark. In this paper, a single image brightening algorithm is introduced to brighten such an image. The proposed algorithm includes a unique hybrid learning framework to generate two virtual images with large exposure times. The virtual images are first generated via intensity mapping functions (IMFs) which are computed using camera response functions (CRFs) and this is a model-driven approach. Both the virtual images are then enhanced by using a data-driven approach, i.e. a residual convolutional neural network to approach the ground truth images. The model-driven approach and the data-driven one compensate each other in the proposed hybrid learning framework. The final brightened image is obtained by fusing the original image and two virtual images via a multi-scale exposure fusion algorithm with properly defined weights. Experimental results show that the proposed brightening algorithm outperforms existing algorithms in terms of the MEF-SSIM metric.



### Self-Calibration Supported Robust Projective Structure-from-Motion
- **Arxiv ID**: http://arxiv.org/abs/2007.02045v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.02045v1)
- **Published**: 2020-07-04 08:47:10+00:00
- **Updated**: 2020-07-04 08:47:10+00:00
- **Authors**: Rui Gong, Danda Pani Paudel, Ajad Chhatkuli, Luc Van Gool
- **Comment**: 21 pages, 5 figures, 2 tables
- **Journal**: None
- **Summary**: Typical Structure-from-Motion (SfM) pipelines rely on finding correspondences across images, recovering the projective structure of the observed scene and upgrading it to a metric frame using camera self-calibration constraints. Solving each problem is mainly carried out independently from the others. For instance, camera self-calibration generally assumes correct matches and a good projective reconstruction have been obtained. In this paper, we propose a unified SfM method, in which the matching process is supported by self-calibration constraints. We use the idea that good matches should yield a valid calibration. In this process, we make use of the Dual Image of Absolute Quadric projection equations within a multiview correspondence framework, in order to obtain robust matching from a set of putative correspondences. The matching process classifies points as inliers or outliers, which is learned in an unsupervised manner using a deep neural network. Together with theoretical reasoning why the self-calibration constraints are necessary, we show experimental results demonstrating robust multiview matching and accurate camera calibration by exploiting these constraints.



### Inference Stage Optimization for Cross-scenario 3D Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2007.02054v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.02054v1)
- **Published**: 2020-07-04 09:45:18+00:00
- **Updated**: 2020-07-04 09:45:18+00:00
- **Authors**: Jianfeng Zhang, Xuecheng Nie, Jiashi Feng
- **Comment**: None
- **Journal**: None
- **Summary**: Existing 3D human pose estimation models suffer performance drop when applying to new scenarios with unseen poses due to their limited generalizability. In this work, we propose a novel framework, Inference Stage Optimization (ISO), for improving the generalizability of 3D pose models when source and target data come from different pose distributions. Our main insight is that the target data, even though not labeled, carry valuable priors about their underlying distribution. To exploit such information, the proposed ISO performs geometry-aware self-supervised learning (SSL) on each single target instance and updates the 3D pose model before making prediction. In this way, the model can mine distributional knowledge about the target scenario and quickly adapt to it with enhanced generalization performance. In addition, to handle sequential target data, we propose an online mode for implementing our ISO framework via streaming the SSL, which substantially enhances its effectiveness. We systematically analyze why and how our ISO framework works on diverse benchmarks under cross-scenario setup. Remarkably, it yields new state-of-the-art of 83.6% 3D PCK on MPI-INF-3DHP, improving upon the previous best result by 9.7%. Code will be released.



### Efficient and accurate object detection with simultaneous classification and tracking
- **Arxiv ID**: http://arxiv.org/abs/2007.02065v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2007.02065v1)
- **Published**: 2020-07-04 10:22:33+00:00
- **Updated**: 2020-07-04 10:22:33+00:00
- **Authors**: Xuesong Li, Jose Guivant
- **Comment**: None
- **Journal**: None
- **Summary**: Interacting with the environment, such as object detection and tracking, is a crucial ability of mobile robots. Besides high accuracy, efficiency in terms of processing effort and energy consumption are also desirable. To satisfy both requirements, we propose a detection framework based on simultaneous classification and tracking in the point stream. In this framework, a tracker performs data association in sequences of the point cloud, guiding the detector to avoid redundant processing (i.e. classifying already-known objects). For objects whose classification is not sufficiently certain, a fusion model is designed to fuse selected key observations that provide different perspectives across the tracking span. Therefore, performance (accuracy and efficiency of detection) can be enhanced. This method is particularly suitable for detecting and tracking moving objects, a process that would require expensive computations if solved using conventional procedures. Experiments were conducted on the benchmark dataset, and the results showed that the proposed method outperforms original tracking-by-detection approaches in both efficiency and accuracy.



### Weight-dependent Gates for Network Pruning
- **Arxiv ID**: http://arxiv.org/abs/2007.02066v4
- **DOI**: 10.1109/TCSVT.2022.3175762
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.02066v4)
- **Published**: 2020-07-04 10:29:07+00:00
- **Updated**: 2022-05-14 11:40:51+00:00
- **Authors**: Yun Li, Zechun Liu, Weiqun Wu, Haotian Yao, Xiangyu Zhang, Chi Zhang, Baoqun Yin
- **Comment**: 13 pages, 7 figures, to appear at IEEE TCSVT
- **Journal**: None
- **Summary**: In this paper, a simple yet effective network pruning framework is proposed to simultaneously address the problems of pruning indicator, pruning ratio, and efficiency constraint. This paper argues that the pruning decision should depend on the convolutional weights, and thus proposes novel weight-dependent gates (W-Gates) to learn the information from filter weights and obtain binary gates to prune or keep the filters automatically. To prune the network under efficiency constraints, a switchable Efficiency Module is constructed to predict the hardware latency or FLOPs of candidate pruned networks. Combined with the proposed Efficiency Module, W-Gates can perform filter pruning in an efficiency-aware manner and achieve a compact network with a better accuracy-efficiency trade-off. We have demonstrated the effectiveness of the proposed method on ResNet34, ResNet50, and MobileNet V2, respectively achieving up to 1.33/1.28/1.1 higher Top-1 accuracy with lower hardware latency on ImageNet. Compared with state-of-the-art methods, W-Gates also achieves superior performance.



### Quo Vadis, Skeleton Action Recognition ?
- **Arxiv ID**: http://arxiv.org/abs/2007.02072v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2007.02072v2)
- **Published**: 2020-07-04 11:02:21+00:00
- **Updated**: 2021-04-07 16:30:54+00:00
- **Authors**: Pranay Gupta, Anirudh Thatipelli, Aditya Aggarwal, Shubh Maheshwari, Neel Trivedi, Sourav Das, Ravi Kiran Sarvadevabhatla
- **Comment**: To appear in International Journal of Computer Vision (IJCV). Project
  page: https://skeleton.iiit.ac.in/
- **Journal**: None
- **Summary**: In this paper, we study current and upcoming frontiers across the landscape of skeleton-based human action recognition. To study skeleton-action recognition in the wild, we introduce Skeletics-152, a curated and 3-D pose-annotated subset of RGB videos sourced from Kinetics-700, a large-scale action dataset. We extend our study to include out-of-context actions by introducing Skeleton-Mimetics, a dataset derived from the recently introduced Mimetics dataset. We also introduce Metaphorics, a dataset with caption-style annotated YouTube videos of the popular social game Dumb Charades and interpretative dance performances. We benchmark state-of-the-art models on the NTU-120 dataset and provide multi-layered assessment of the results. The results from benchmarking the top performers of NTU-120 on the newly introduced datasets reveal the challenges and domain gap induced by actions in the wild. Overall, our work characterizes the strengths and limitations of existing approaches and datasets. Via the introduced datasets, our work enables new frontiers for human action recognition.



### Speckle2Void: Deep Self-Supervised SAR Despeckling with Blind-Spot Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2007.02075v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.02075v1)
- **Published**: 2020-07-04 11:38:48+00:00
- **Updated**: 2020-07-04 11:38:48+00:00
- **Authors**: Andrea Bordone Molini, Diego Valsesia, Giulia Fracastoro, Enrico Magli
- **Comment**: None
- **Journal**: None
- **Summary**: Information extraction from synthetic aperture radar (SAR) images is heavily impaired by speckle noise, hence despeckling is a crucial preliminary step in scene analysis algorithms. The recent success of deep learning envisions a new generation of despeckling techniques that could outperform classical model-based methods. However, current deep learning approaches to despeckling require supervision for training, whereas clean SAR images are impossible to obtain. In the literature, this issue is tackled by resorting to either synthetically speckled optical images, which exhibit different properties with respect to true SAR images, or multi-temporal SAR images, which are difficult to acquire or fuse accurately. In this paper, inspired by recent works on blind-spot denoising networks, we propose a self-supervised Bayesian despeckling method. The proposed method is trained employing only noisy SAR images and can therefore learn features of real SAR images rather than synthetic data. Experiments show that the performance of the proposed approach is very close to the supervised training approach on synthetic data and superior on real data in both quantitative and visual assessments.



### Registration of Histopathogy Images Using Structural Information From Fine Grained Feature Maps
- **Arxiv ID**: http://arxiv.org/abs/2007.02078v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.02078v1)
- **Published**: 2020-07-04 12:05:03+00:00
- **Updated**: 2020-07-04 12:05:03+00:00
- **Authors**: Dwarikanath Mahapatra
- **Comment**: None
- **Journal**: None
- **Summary**: Registration is an important part of many clinical workflows and factually, including information of structures of interest improves registration performance. We propose a novel approach of combining segmentation information in a registration framework using self supervised segmentation feature maps extracted using a pre-trained segmentation network followed by clustering. Using self supervised feature maps enables us to use segmentation information despite the unavailability of manual segmentations. Experimental results show our approach effectively replaces manual segmentation maps and demonstrate the possibility of obtaining state of the art registration performance in real world cases where manual segmentation maps are unavailable.



### End-to-end Learning of a Fisher Vector Encoding for Part Features in Fine-grained Recognition
- **Arxiv ID**: http://arxiv.org/abs/2007.02080v2
- **DOI**: 10.1007/978-3-030-92659-5_9
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.02080v2)
- **Published**: 2020-07-04 12:17:25+00:00
- **Updated**: 2023-07-28 07:17:05+00:00
- **Authors**: Dimitri Korsch, Paul Bodesheim, Joachim Denzler
- **Comment**: Published in the proceedings of the German Conference on Pattern
  Recognition 2021 (GCPR21)
- **Journal**: None
- **Summary**: Part-based approaches for fine-grained recognition do not show the expected performance gain over global methods, although explicitly focusing on small details that are relevant for distinguishing highly similar classes. We assume that part-based methods suffer from a missing representation of local features, which is invariant to the order of parts and can handle a varying number of visible parts appropriately. The order of parts is artificial and often only given by ground-truth annotations, whereas viewpoint variations and occlusions result in not observable parts. Therefore, we propose integrating a Fisher vector encoding of part features into convolutional neural networks. The parameters for this encoding are estimated by an online EM algorithm jointly with those of the neural network and are more precise than the estimates of previous works. Our approach improves state-of-the-art accuracies for three bird species classification datasets.



### Multi-Sensor Next-Best-View Planning as Matroid-Constrained Submodular Maximization
- **Arxiv ID**: http://arxiv.org/abs/2007.02084v1
- **DOI**: 10.1109/LRA.2020.3007445
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.02084v1)
- **Published**: 2020-07-04 12:28:18+00:00
- **Updated**: 2020-07-04 12:28:18+00:00
- **Authors**: Mikko Lauri, Joni Pajarinen, Jan Peters, Simone Frintrop
- **Comment**: 8 pages, 7 figures. Accepted for publication in IEEE Robotics and
  Automation Letters
- **Journal**: None
- **Summary**: 3D scene models are useful in robotics for tasks such as path planning, object manipulation, and structural inspection. We consider the problem of creating a 3D model using depth images captured by a team of multiple robots. Each robot selects a viewpoint and captures a depth image from it, and the images are fused to update the scene model. The process is repeated until a scene model of desired quality is obtained. Next-best-view planning uses the current scene model to select the next viewpoints. The objective is to select viewpoints so that the images captured using them improve the quality of the scene model the most. In this paper, we address next-best-view planning for multiple depth cameras. We propose a utility function that scores sets of viewpoints and avoids overlap between multiple sensors. We show that multi-sensor next-best-view planning with this utility function is an instance of submodular maximization under a matroid constraint. This allows the planning problem to be solved by a polynomial-time greedy algorithm that yields a solution within a constant factor from the optimal. We evaluate the performance of our planning algorithm in simulated experiments with up to 8 sensors, and in real-world experiments using two robot arms equipped with depth cameras.



### Multi-Site Infant Brain Segmentation Algorithms: The iSeg-2019 Challenge
- **Arxiv ID**: http://arxiv.org/abs/2007.02096v2
- **DOI**: 10.1109/TMI.2021.3055428
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.02096v2)
- **Published**: 2020-07-04 13:39:48+00:00
- **Updated**: 2020-07-11 13:24:15+00:00
- **Authors**: Yue Sun, Kun Gao, Zhengwang Wu, Zhihao Lei, Ying Wei, Jun Ma, Xiaoping Yang, Xue Feng, Li Zhao, Trung Le Phan, Jitae Shin, Tao Zhong, Yu Zhang, Lequan Yu, Caizi Li, Ramesh Basnet, M. Omair Ahmad, M. N. S. Swamy, Wenao Ma, Qi Dou, Toan Duc Bui, Camilo Bermudez Noguera, Bennett Landman, Ian H. Gotlib, Kathryn L. Humphreys, Sarah Shultz, Longchuan Li, Sijie Niu, Weili Lin, Valerie Jewells, Gang Li, Dinggang Shen, Li Wang
- **Comment**: None
- **Journal**: IEEE Transactions on Medical Imaging, 40(5), 1363-1376, 2021
- **Summary**: To better understand early brain growth patterns in health and disorder, it is critical to accurately segment infant brain magnetic resonance (MR) images into white matter (WM), gray matter (GM), and cerebrospinal fluid (CSF). Deep learning-based methods have achieved state-of-the-art performance; however, one of major limitations is that the learning-based methods may suffer from the multi-site issue, that is, the models trained on a dataset from one site may not be applicable to the datasets acquired from other sites with different imaging protocols/scanners. To promote methodological development in the community, iSeg-2019 challenge (http://iseg2019.web.unc.edu) provides a set of 6-month infant subjects from multiple sites with different protocols/scanners for the participating methods. Training/validation subjects are from UNC (MAP) and testing subjects are from UNC/UMN (BCP), Stanford University, and Emory University. By the time of writing, there are 30 automatic segmentation methods participating in iSeg-2019. We review the 8 top-ranked teams by detailing their pipelines/implementations, presenting experimental results and evaluating performance in terms of the whole brain, regions of interest, and gyral landmark curves. We also discuss their limitations and possible future directions for the multi-site issue. We hope that the multi-site dataset in iSeg-2019 and this review article will attract more researchers on the multi-site issue.



### Local Grid Rendering Networks for 3D Object Detection in Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2007.02099v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.02099v1)
- **Published**: 2020-07-04 13:57:43+00:00
- **Updated**: 2020-07-04 13:57:43+00:00
- **Authors**: Jianan Li, Jiashi Feng
- **Comment**: None
- **Journal**: None
- **Summary**: The performance of 3D object detection models over point clouds highly depends on their capability of modeling local geometric patterns. Conventional point-based models exploit local patterns through a symmetric function (e.g. max pooling) or based on graphs, which easily leads to loss of fine-grained geometric structures. Regarding capturing spatial patterns, CNNs are powerful but it would be computationally costly to directly apply convolutions on point data after voxelizing the entire point clouds to a dense regular 3D grid. In this work, we aim to improve performance of point-based models by enhancing their pattern learning ability through leveraging CNNs while preserving computational efficiency. We propose a novel and principled Local Grid Rendering (LGR) operation to render the small neighborhood of a subset of input points into a low-resolution 3D grid independently, which allows small-size CNNs to accurately model local patterns and avoids convolutions over a dense grid to save computation cost. With the LGR operation, we introduce a new generic backbone called LGR-Net for point cloud feature extraction with simple design and high efficiency. We validate LGR-Net for 3D object detection on the challenging ScanNet and SUN RGB-D datasets. It advances state-of-the-art results significantly by 5.5 and 4.5 mAP, respectively, with only slight increased computation overhead.



### Automatic Target Recognition on Synthetic Aperture Radar Imagery: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2007.02106v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2007.02106v2)
- **Published**: 2020-07-04 14:22:30+00:00
- **Updated**: 2020-12-12 21:11:18+00:00
- **Authors**: O. Kechagias-Stamatis, N. Aouf
- **Comment**: 21 pages, 14 figures
- **Journal**: None
- **Summary**: Automatic Target Recognition (ATR) for military applications is one of the core processes towards enhancing intelligencer and autonomously operating military platforms. Spurred by this and given that Synthetic Aperture Radar (SAR) presents several advantages over its counterpart data domains, this paper surveys and assesses current SAR ATR architectures that employ the most popular dataset for the SAR domain, namely the Moving and Stationary Target Acquisition and Recognition (MSTAR) dataset. Based on the current methodology trends, we propose a taxonomy for the SAR ATR architectures, along with a direct comparison of the strengths and weaknesses of each method under both standard and extended operational conditions. Additionally, despite MSTAR being the standard SAR ATR benchmarking dataset we also highlight its weaknesses and suggest future research directions.



### SplitFusion: Simultaneous Tracking and Mapping for Non-Rigid Scenes
- **Arxiv ID**: http://arxiv.org/abs/2007.02108v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.02108v1)
- **Published**: 2020-07-04 14:27:16+00:00
- **Updated**: 2020-07-04 14:27:16+00:00
- **Authors**: Yang Li, Tianwei Zhang, Yoshihiko Nakamura, Tatsuya Harada
- **Comment**: Accepted to IROS'2020
- **Journal**: None
- **Summary**: We present SplitFusion, a novel dense RGB-D SLAM framework that simultaneously performs tracking and dense reconstruction for both rigid and non-rigid components of the scene. SplitFusion first adopts deep learning based semantic instant segmentation technique to split the scene into rigid or non-rigid surfaces. The split surfaces are independently tracked via rigid or non-rigid ICP and reconstructed through incremental depth map fusion. Experimental results show that the proposed approach can provide not only accurate environment maps but also well-reconstructed non-rigid targets, e.g. the moving humans.



### On Class Orderings for Incremental Learning
- **Arxiv ID**: http://arxiv.org/abs/2007.02145v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.02145v2)
- **Published**: 2020-07-04 17:07:08+00:00
- **Updated**: 2020-07-07 06:46:16+00:00
- **Authors**: Marc Masana, Bartłomiej Twardowski, Joost van de Weijer
- **Comment**: Accepted at CL-ICML 2020. First two authors contributed equally
- **Journal**: None
- **Summary**: The influence of class orderings in the evaluation of incremental learning has received very little attention. In this paper, we investigate the impact of class orderings for incrementally learned classifiers. We propose a method to compute various orderings for a dataset. The orderings are derived by simulated annealing optimization from the confusion matrix and reflect different incremental learning scenarios, including maximally and minimally confusing tasks. We evaluate a wide range of state-of-the-art incremental learning methods on the proposed orderings. Results show that orderings can have a significant impact on performance and the ranking of the methods.



### Human Assisted Artificial Intelligence Based Technique to Create Natural Features for OpenStreetMap
- **Arxiv ID**: http://arxiv.org/abs/2007.02149v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/2007.02149v2)
- **Published**: 2020-07-04 17:26:46+00:00
- **Updated**: 2020-07-08 18:33:35+00:00
- **Authors**: Piyush Yadav, Dipto Sarkar, Shailesh Deshpande, Edward Curry
- **Comment**: 3 pages, 2 Figures, Submitted to FOSS4G Europe 2020 Academic Track
  (Postponed to 2021)
- **Journal**: None
- **Summary**: In this work, we propose an AI-based technique using freely available satellite images like Landsat and Sentinel to create natural features over OSM in congruence with human editors acting as initiators and validators. The method is based on Interactive Machine Learning technique where human inputs are coupled with the machine to solve complex problems efficiently as compare to pure autonomous process. We use a bottom-up approach where a machine learning (ML) pipeline in loop with editors is used to extract classes using spectral signatures of images and later convert them to editable features to create natural features.



### Face Anti-Spoofing with Human Material Perception
- **Arxiv ID**: http://arxiv.org/abs/2007.02157v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.02157v1)
- **Published**: 2020-07-04 18:25:53+00:00
- **Updated**: 2020-07-04 18:25:53+00:00
- **Authors**: Zitong Yu, Xiaobai Li, Xuesong Niu, Jingang Shi, Guoying Zhao
- **Comment**: Accepted by ECCV2020
- **Journal**: None
- **Summary**: Face anti-spoofing (FAS) plays a vital role in securing the face recognition systems from presentation attacks. Most existing FAS methods capture various cues (e.g., texture, depth and reflection) to distinguish the live faces from the spoofing faces. All these cues are based on the discrepancy among physical materials (e.g., skin, glass, paper and silicone). In this paper we rephrase face anti-spoofing as a material recognition problem and combine it with classical human material perception [1], intending to extract discriminative and robust features for FAS. To this end, we propose the Bilateral Convolutional Networks (BCN), which is able to capture intrinsic material-based patterns via aggregating multi-level bilateral macro- and micro- information. Furthermore, Multi-level Feature Refinement Module (MFRM) and multi-head supervision are utilized to learn more robust features. Comprehensive experiments are performed on six benchmark datasets, and the proposed method achieves superior performance on both intra- and cross-dataset testings. One highlight is that we achieve overall 11.3$\pm$9.5\% EER for cross-type testing in SiW-M dataset, which significantly outperforms previous results. We hope this work will facilitate future cooperation between FAS and material communities.



### Neuro-Symbolic Generative Art: A Preliminary Study
- **Arxiv ID**: http://arxiv.org/abs/2007.02171v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.02171v1)
- **Published**: 2020-07-04 19:40:00+00:00
- **Updated**: 2020-07-04 19:40:00+00:00
- **Authors**: Gunjan Aggarwal, Devi Parikh
- **Comment**: Accepted as a short paper at ICCC 2020
- **Journal**: None
- **Summary**: There are two classes of generative art approaches: neural, where a deep model is trained to generate samples from a data distribution, and symbolic or algorithmic, where an artist designs the primary parameters and an autonomous system generates samples within these constraints. In this work, we propose a new hybrid genre: neuro-symbolic generative art. As a preliminary study, we train a generative deep neural network on samples from the symbolic approach. We demonstrate through human studies that subjects find the final artifacts and the creation process using our neuro-symbolic approach to be more creative than the symbolic approach 61% and 82% of the time respectively.



### A Weakly Supervised Consistency-based Learning Method for COVID-19 Segmentation in CT Images
- **Arxiv ID**: http://arxiv.org/abs/2007.02180v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.02180v2)
- **Published**: 2020-07-04 20:41:17+00:00
- **Updated**: 2020-07-07 11:56:15+00:00
- **Authors**: Issam Laradji, Pau Rodriguez, Oscar Mañas, Keegan Lensink, Marco Law, Lironne Kurzman, William Parker, David Vazquez, Derek Nowrouzezahrai
- **Comment**: None
- **Journal**: None
- **Summary**: Coronavirus Disease 2019 (COVID-19) has spread aggressively across the world causing an existential health crisis. Thus, having a system that automatically detects COVID-19 in tomography (CT) images can assist in quantifying the severity of the illness. Unfortunately, labelling chest CT scans requires significant domain expertise, time, and effort. We address these labelling challenges by only requiring point annotations, a single pixel for each infected region on a CT image. This labeling scheme allows annotators to label a pixel in a likely infected region, only taking 1-3 seconds, as opposed to 10-15 seconds to segment a region. Conventionally, segmentation models train on point-level annotations using the cross-entropy loss function on these labels. However, these models often suffer from low precision. Thus, we propose a consistency-based (CB) loss function that encourages the output predictions to be consistent with spatial transformations of the input images. The experiments on 3 open-source COVID-19 datasets show that this loss function yields significant improvement over conventional point-level loss functions and almost matches the performance of models trained with full supervision with much less human effort. Code is available at: \url{https://github.com/IssamLaradji/covid19_weak_supervision}.



### BézierSketch: A generative model for scalable vector sketches
- **Arxiv ID**: http://arxiv.org/abs/2007.02190v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.02190v2)
- **Published**: 2020-07-04 21:30:52+00:00
- **Updated**: 2020-07-14 15:13:44+00:00
- **Authors**: Ayan Das, Yongxin Yang, Timothy Hospedales, Tao Xiang, Yi-Zhe Song
- **Comment**: Accepted as poster at ECCV 2020
- **Journal**: None
- **Summary**: The study of neural generative models of human sketches is a fascinating contemporary modeling problem due to the links between sketch image generation and the human drawing process. The landmark SketchRNN provided breakthrough by sequentially generating sketches as a sequence of waypoints. However this leads to low-resolution image generation, and failure to model long sketches. In this paper we present B\'ezierSketch, a novel generative model for fully vector sketches that are automatically scalable and high-resolution. To this end, we first introduce a novel inverse graphics approach to stroke embedding that trains an encoder to embed each stroke to its best fit B\'ezier curve. This enables us to treat sketches as short sequences of paramaterized strokes and thus train a recurrent sketch generator with greater capacity for longer sketches, while producing scalable high-resolution results. We report qualitative and quantitative results on the Quick, Draw! benchmark.



### Deep Active Learning via Open Set Recognition
- **Arxiv ID**: http://arxiv.org/abs/2007.02196v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.02196v4)
- **Published**: 2020-07-04 22:09:17+00:00
- **Updated**: 2021-04-05 18:47:17+00:00
- **Authors**: Jaya Krishna Mandivarapu, Blake Camp, Rolando Estrada
- **Comment**: Withdrawn to address fundamental concerns with the text
- **Journal**: None
- **Summary**: In many applications, data is easy to acquire but expensive and time-consuming to label prominent examples include medical imaging and NLP. This disparity has only grown in recent years as our ability to collect data improves. Under these constraints, it makes sense to select only the most informative instances from the unlabeled pool and request an oracle (e.g., a human expert) to provide labels for those samples. The goal of active learning is to infer the informativeness of unlabeled samples so as to minimize the number of requests to the oracle. Here, we formulate active learning as an open-set recognition problem. In this paradigm, only some of the inputs belong to known classes; the classifier must identify the rest as unknown. More specifically, we leverage variational neural networks (VNNs), which produce high-confidence (i.e., low-entropy) predictions only for inputs that closely resemble the training data. We use the inverse of this confidence measure to select the samples that the oracle should label. Intuitively, unlabeled samples that the VNN is uncertain about are more informative for future training. We carried out an extensive evaluation of our novel, probabilistic formulation of active learning, achieving state-of-the-art results on MNIST, CIFAR-10, and CIFAR-100. Additionally, unlike current active learning methods, our algorithm can learn tasks without the need for task labels. As our experiments show, when the unlabeled pool consists of a mixture of samples from multiple datasets, our approach can automatically distinguish between samples from seen vs. unseen tasks.



### Offline versus Online Triplet Mining based on Extreme Distances of Histopathology Patches
- **Arxiv ID**: http://arxiv.org/abs/2007.02200v3
- **DOI**: 10.1007/978-3-030-64556-4_26
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.02200v3)
- **Published**: 2020-07-04 22:33:08+00:00
- **Updated**: 2022-08-10 05:39:56+00:00
- **Authors**: Milad Sikaroudi, Benyamin Ghojogh, Amir Safarpoor, Fakhri Karray, Mark Crowley, H. R. Tizhoosh
- **Comment**: Accepted for presentation at the 15th International Symposium on
  Visual Computing (ISVC) 2020, Springer. v2: corrected a small mathematical
  typo
- **Journal**: 15th International Symposium on Visual Computing (ISVC), pp.
  333-345, Springer, 2020
- **Summary**: We analyze the effect of offline and online triplet mining for colorectal cancer (CRC) histopathology dataset containing 100,000 patches. We consider the extreme, i.e., farthest and nearest patches to a given anchor, both in online and offline mining. While many works focus solely on selecting the triplets online (batch-wise), we also study the effect of extreme distances and neighbor patches before training in an offline fashion. We analyze extreme cases' impacts in terms of embedding distance for offline versus online mining, including easy positive, batch semi-hard, batch hard triplet mining, neighborhood component analysis loss, its proxy version, and distance weighted sampling. We also investigate online approaches based on extreme distance and comprehensively compare offline, and online mining performance based on the data patterns and explain offline mining as a tractable generalization of the online mining with large mini-batch size. As well, we discuss the relations of different colorectal tissue types in terms of extreme distances. We found that offline and online mining approaches have comparable performances for a specific architecture, such as ResNet-18 in this study. Moreover, we found the assorted case, including different extreme distances, is promising, especially in the online approach.



### On Connections between Regularizations for Improving DNN Robustness
- **Arxiv ID**: http://arxiv.org/abs/2007.02209v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2007.02209v1)
- **Published**: 2020-07-04 23:43:32+00:00
- **Updated**: 2020-07-04 23:43:32+00:00
- **Authors**: Yiwen Guo, Long Chen, Yurong Chen, Changshui Zhang
- **Comment**: Accepted by TPAMI
- **Journal**: None
- **Summary**: This paper analyzes regularization terms proposed recently for improving the adversarial robustness of deep neural networks (DNNs), from a theoretical point of view. Specifically, we study possible connections between several effective methods, including input-gradient regularization, Jacobian regularization, curvature regularization, and a cross-Lipschitz functional. We investigate them on DNNs with general rectified linear activations, which constitute one of the most prevalent families of models for image classification and a host of other machine learning applications. We shed light on essential ingredients of these regularizations and re-interpret their functionality. Through the lens of our study, more principled and efficient regularizations can possibly be invented in the near future.



