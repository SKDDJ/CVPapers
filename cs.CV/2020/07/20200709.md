# Arxiv Papers in cs.CV on 2020-07-09
### Towards Unsupervised Learning for Instrument Segmentation in Robotic Surgery with Cycle-Consistent Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2007.04505v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.04505v1)
- **Published**: 2020-07-09 01:39:39+00:00
- **Updated**: 2020-07-09 01:39:39+00:00
- **Authors**: Daniil Pakhomov, Wei Shen, Nassir Navab
- **Comment**: IROS 2020
- **Journal**: None
- **Summary**: Surgical tool segmentation in endoscopic images is an important problem: it is a crucial step towards full instrument pose estimation and it is used for integration of pre- and intra-operative images into the endoscopic view. While many recent approaches based on convolutional neural networks have shown great results, a key barrier to progress lies in the acquisition of a large number of manually-annotated images which is necessary for an algorithm to generalize and work well in diverse surgical scenarios. Unlike the surgical image data itself, annotations are difficult to acquire and may be of variable quality. On the other hand, synthetic annotations can be automatically generated by using forward kinematic model of the robot and CAD models of tools by projecting them onto an image plane. Unfortunately, this model is very inaccurate and cannot be used for supervised learning of image segmentation models. Since generated annotations will not directly correspond to endoscopic images due to errors, we formulate the problem as an unpaired image-to-image translation where the goal is to learn the mapping between an input endoscopic image and a corresponding annotation using an adversarial model. Our approach allows to train image segmentation models without the need to acquire expensive annotations and can potentially exploit large unlabeled endoscopic image collection outside the annotated distributions of image/annotation data. We test our proposed method on Endovis 2017 challenge dataset and show that it is competitive with supervised segmentation methods.



### DECAPS: Detail-Oriented Capsule Networks
- **Arxiv ID**: http://arxiv.org/abs/2007.05343v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.05343v1)
- **Published**: 2020-07-09 01:48:22+00:00
- **Updated**: 2020-07-09 01:48:22+00:00
- **Authors**: Aryan Mobiny, Pengyu Yuan, Pietro Antonio Cicalese, Hien Van Nguyen
- **Comment**: arXiv admin note: text overlap with arXiv:2004.07407
- **Journal**: None
- **Summary**: Capsule Networks (CapsNets) have demonstrated to be a promising alternative to Convolutional Neural Networks (CNNs). However, they often fall short of state-of-the-art accuracies on large-scale high-dimensional datasets. We propose a Detail-Oriented Capsule Network (DECAPS) that combines the strength of CapsNets with several novel techniques to boost its classification accuracies. First, DECAPS uses an Inverted Dynamic Routing (IDR) mechanism to group lower-level capsules into heads before sending them to higher-level capsules. This strategy enables capsules to selectively attend to small but informative details within the data which may be lost during pooling operations in CNNs. Second, DECAPS employs a Peekaboo training procedure, which encourages the network to focus on fine-grained information through a second-level attention scheme. Finally, the distillation process improves the robustness of DECAPS by averaging over the original and attended image region predictions. We provide extensive experiments on the CheXpert and RSNA Pneumonia datasets to validate the effectiveness of DECAPS. Our networks achieve state-of-the-art accuracies not only in classification (increasing the average area under ROC curves from 87.24% to 92.82% on the CheXpert dataset) but also in the weakly-supervised localization of diseased areas (increasing average precision from 41.7% to 80% for the RSNA Pneumonia detection dataset).



### Deep Multi-task Learning for Facial Expression Recognition and Synthesis Based on Selective Feature Sharing
- **Arxiv ID**: http://arxiv.org/abs/2007.04514v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.04514v2)
- **Published**: 2020-07-09 02:29:34+00:00
- **Updated**: 2021-11-28 06:48:22+00:00
- **Authors**: Rui Zhao, Tianshan Liu, Jun Xiao, Daniel P. K. Lun, Kin-Man Lam
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-task learning is an effective learning strategy for deep-learning-based facial expression recognition tasks. However, most existing methods take into limited consideration the feature selection, when transferring information between different tasks, which may lead to task interference when training the multi-task networks. To address this problem, we propose a novel selective feature-sharing method, and establish a multi-task network for facial expression recognition and facial expression synthesis. The proposed method can effectively transfer beneficial features between different tasks, while filtering out useless and harmful information. Moreover, we employ the facial expression synthesis task to enlarge and balance the training dataset to further enhance the generalization ability of the proposed method. Experimental results show that the proposed method achieves state-of-the-art performance on those commonly used facial expression recognition benchmarks, which makes it a potential solution to real-world facial expression recognition problems.



### Aligning Videos in Space and Time
- **Arxiv ID**: http://arxiv.org/abs/2007.04515v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.04515v1)
- **Published**: 2020-07-09 02:30:48+00:00
- **Updated**: 2020-07-09 02:30:48+00:00
- **Authors**: Senthil Purushwalkam, Tian Ye, Saurabh Gupta, Abhinav Gupta
- **Comment**: To appear at the European Conference on Computer Vision (ECCV) 2020
- **Journal**: None
- **Summary**: In this paper, we focus on the task of extracting visual correspondences across videos. Given a query video clip from an action class, we aim to align it with training videos in space and time. Obtaining training data for such a fine-grained alignment task is challenging and often ambiguous. Hence, we propose a novel alignment procedure that learns such correspondence in space and time via cross video cycle-consistency. During training, given a pair of videos, we compute cycles that connect patches in a given frame in the first video by matching through frames in the second video. Cycles that connect overlapping patches together are encouraged to score higher than cycles that connect non-overlapping patches. Our experiments on the Penn Action and Pouring datasets demonstrate that the proposed method can successfully learn to correspond semantically similar patches across videos, and learns representations that are sensitive to object and action states.



### PointMask: Towards Interpretable and Bias-Resilient Point Cloud Processing
- **Arxiv ID**: http://arxiv.org/abs/2007.04525v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.04525v1)
- **Published**: 2020-07-09 03:06:06+00:00
- **Updated**: 2020-07-09 03:06:06+00:00
- **Authors**: Saeid Asgari Taghanaki, Kaveh Hassani, Pradeep Kumar Jayaraman, Amir Hosein Khasahmadi, Tonya Custis
- **Comment**: Accepted to ICML 2020 WHI
- **Journal**: None
- **Summary**: Deep classifiers tend to associate a few discriminative input variables with their objective function, which in turn, may hurt their generalization capabilities. To address this, one can design systematic experiments and/or inspect the models via interpretability methods. In this paper, we investigate both of these strategies on deep models operating on point clouds. We propose PointMask, a model-agnostic interpretable information-bottleneck approach for attribution in point cloud models. PointMask encourages exploring the majority of variation factors in the input space while gradually converging to a general solution. More specifically, PointMask introduces a regularization term that minimizes the mutual information between the input and the latent features used to masks out irrelevant variables. We show that coupling a PointMask layer with an arbitrary model can discern the points in the input space which contribute the most to the prediction score, thereby leading to interpretability. Through designed bias experiments, we also show that thanks to its gradual masking feature, our proposed method is effective in handling data bias.



### Attention-based Residual Speech Portrait Model for Speech to Face Generation
- **Arxiv ID**: http://arxiv.org/abs/2007.04536v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.04536v1)
- **Published**: 2020-07-09 03:31:33+00:00
- **Updated**: 2020-07-09 03:31:33+00:00
- **Authors**: Jianrong Wang, Xiaosheng Hu, Li Liu, Wei Liu, Mei Yu, Tianyi Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Given a speaker's speech, it is interesting to see if it is possible to generate this speaker's face. One main challenge in this task is to alleviate the natural mismatch between face and speech. To this end, in this paper, we propose a novel Attention-based Residual Speech Portrait Model (AR-SPM) by introducing the ideal of the residual into a hybrid encoder-decoder architecture, where face prior features are merged with the output of speech encoder to form the final face feature. In particular, we innovatively establish a tri-item loss function, which is a weighted linear combination of the L2-norm, L1-norm and negative cosine loss, to train our model by comparing the final face feature and true face feature. Evaluation on AVSpeech dataset shows that our proposed model accelerates the convergence of training, outperforms the state-of-the-art in terms of quality of the generated face, and achieves superior recognition accuracy of gender and age compared with the ground truth.



### Point Set Voting for Partial Point Cloud Analysis
- **Arxiv ID**: http://arxiv.org/abs/2007.04537v2
- **DOI**: 10.1109/LRA.2020.3048658
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2007.04537v2)
- **Published**: 2020-07-09 03:37:31+00:00
- **Updated**: 2021-01-02 17:37:19+00:00
- **Authors**: Junming Zhang, Weijia Chen, Yuping Wang, Ram Vasudevan, Matthew Johnson-Roberson
- **Comment**: IEEE Robotics and Automation Letters (RA-L)
- **Journal**: None
- **Summary**: The continual improvement of 3D sensors has driven the development of algorithms to perform point cloud analysis. In fact, techniques for point cloud classification and segmentation have in recent years achieved incredible performance driven in part by leveraging large synthetic datasets. Unfortunately these same state-of-the-art approaches perform poorly when applied to incomplete point clouds. This limitation of existing algorithms is particularly concerning since point clouds generated by 3D sensors in the real world are usually incomplete due to perspective view or occlusion by other objects. This paper proposes a general model for partial point clouds analysis wherein the latent feature encoding a complete point clouds is inferred by applying a local point set voting strategy. In particular, each local point set constructs a vote that corresponds to a distribution in the latent space, and the optimal latent feature is the one with the highest probability. This approach ensures that any subsequent point cloud analysis is robust to partial observation while simultaneously guaranteeing that the proposed model is able to output multiple possible results. This paper illustrates that this proposed method achieves state-of-the-art performance on shape classification, part segmentation and point cloud completion.



### EPI-based Oriented Relation Networks for Light Field Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/2007.04538v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.04538v2)
- **Published**: 2020-07-09 03:39:09+00:00
- **Updated**: 2020-08-26 07:10:04+00:00
- **Authors**: Kunyuan Li, Jun Zhang, Rui Sun, Xudong Zhang, Jun Gao
- **Comment**: None
- **Journal**: British Machine Vision Conference, 2020
- **Summary**: Light field cameras record not only the spatial information of observed scenes but also the directions of all incoming light rays. The spatial and angular information implicitly contain geometrical characteristics such as multi-view or epipolar geometry, which can be exploited to improve the performance of depth estimation. An Epipolar Plane Image (EPI), the unique 2D spatial-angular slice of the light field, contains patterns of oriented lines. The slope of these lines is associated with the disparity. Benefiting from this property of EPIs, some representative methods estimate depth maps by analyzing the disparity of each line in EPIs. However, these methods often extract the optimal slope of the lines from EPIs while ignoring the relationship between neighboring pixels, which leads to inaccurate depth map predictions. Based on the observation that an oriented line and its neighboring pixels in an EPI share a similar linear structure, we propose an end-to-end fully convolutional network (FCN) to estimate the depth value of the intersection point on the horizontal and vertical EPIs. Specifically, we present a new feature-extraction module, called Oriented Relation Module (ORM), that constructs the relationship between the line orientations. To facilitate training, we also propose a refocusing-based data augmentation method to obtain different slopes from EPIs of the same scene point. Extensive experiments verify the efficacy of learning relations and show that our approach is competitive to other state-of-the-art methods. The code and the trained models are available at https://github.com/lkyahpu/EPI_ORM.git.



### Blur Invariant Kernel-Adaptive Network for Single Image Blind deblurring
- **Arxiv ID**: http://arxiv.org/abs/2007.04543v3
- **DOI**: None
- **Categories**: **cs.CV**, I.4.3
- **Links**: [PDF](http://arxiv.org/pdf/2007.04543v3)
- **Published**: 2020-07-09 03:53:33+00:00
- **Updated**: 2020-12-15 07:28:07+00:00
- **Authors**: Sungkwon An, Hyungmin Roh, Myungjoo Kang
- **Comment**: 9 pages, 7 figures
- **Journal**: None
- **Summary**: We present a novel, blind, single image deblurring method that utilizes information regarding blur kernels. Our model solves the deblurring problem by dividing it into two successive tasks: (1) blur kernel estimation and (2) sharp image restoration. We first introduce a kernel estimation network that produces adaptive blur kernels based on the analysis of the blurred image. The network learns the blur pattern of the input image and trains to generate the estimation of image-specific blur kernels. Subsequently, we propose a deblurring network that restores sharp images using the estimated blur kernel. To use the kernel efficiently, we propose a kernel-adaptive AE block that encodes features from both blurred images and blur kernels into a low dimensional space and then decodes them simultaneously to obtain an appropriately synthesized feature representation. We evaluate our model on REDS, GOPRO and Flickr2K datasets using various Gaussian blur kernels. Experiments show that our model can achieve state-of-the-art results on each dataset.



### Wandering Within a World: Online Contextualized Few-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2007.04546v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.04546v3)
- **Published**: 2020-07-09 04:05:04+00:00
- **Updated**: 2021-04-22 20:15:19+00:00
- **Authors**: Mengye Ren, Michael L. Iuzzolino, Michael C. Mozer, Richard S. Zemel
- **Comment**: ICLR 2021
- **Journal**: None
- **Summary**: We aim to bridge the gap between typical human and machine-learning environments by extending the standard framework of few-shot learning to an online, continual setting. In this setting, episodes do not have separate training and testing phases, and instead models are evaluated online while learning novel classes. As in the real world, where the presence of spatiotemporal context helps us retrieve learned skills in the past, our online few-shot learning setting also features an underlying context that changes throughout time. Object classes are correlated within a context and inferring the correct context can lead to better performance. Building upon this setting, we propose a new few-shot learning dataset based on large scale indoor imagery that mimics the visual experience of an agent wandering within a world. Furthermore, we convert popular few-shot learning approaches into online versions and we also propose a new contextual prototypical memory model that can make use of spatiotemporal contextual information from the recent past.



### Alleviating the Burden of Labeling: Sentence Generation by Attention Branch Encoder-Decoder Network
- **Arxiv ID**: http://arxiv.org/abs/2007.04557v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2007.04557v1)
- **Published**: 2020-07-09 05:02:23+00:00
- **Updated**: 2020-07-09 05:02:23+00:00
- **Authors**: Tadashi Ogura, Aly Magassouba, Komei Sugiura, Tsubasa Hirakawa, Takayoshi Yamashita, Hironobu Fujiyoshi, Hisashi Kawai
- **Comment**: 9 pages, 8 figures. accepted for IEEE Robotics and Automation Letters
  (RA-L) with presentation at IROS 2020
- **Journal**: None
- **Summary**: Domestic service robots (DSRs) are a promising solution to the shortage of home care workers. However, one of the main limitations of DSRs is their inability to interact naturally through language. Recently, data-driven approaches have been shown to be effective for tackling this limitation; however, they often require large-scale datasets, which is costly. Based on this background, we aim to perform automatic sentence generation of fetching instructions: for example, "Bring me a green tea bottle on the table." This is particularly challenging because appropriate expressions depend on the target object, as well as its surroundings. In this paper, we propose the attention branch encoder--decoder network (ABEN), to generate sentences from visual inputs. Unlike other approaches, the ABEN has multimodal attention branches that use subword-level attention and generate sentences based on subword embeddings. In experiments, we compared the ABEN with a baseline method using four standard metrics in image captioning. Results show that the ABEN outperformed the baseline in terms of these metrics.



### Auxiliary Tasks Speed Up Learning PointGoal Navigation
- **Arxiv ID**: http://arxiv.org/abs/2007.04561v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2007.04561v2)
- **Published**: 2020-07-09 05:22:40+00:00
- **Updated**: 2020-11-04 20:29:06+00:00
- **Authors**: Joel Ye, Dhruv Batra, Erik Wijmans, Abhishek Das
- **Comment**: 8 pages. Accepted to CoRL 2020
- **Journal**: None
- **Summary**: PointGoal Navigation is an embodied task that requires agents to navigate to a specified point in an unseen environment. Wijmans et al. showed that this task is solvable but their method is computationally prohibitive, requiring 2.5 billion frames and 180 GPU-days. In this work, we develop a method to significantly increase sample and time efficiency in learning PointNav using self-supervised auxiliary tasks (e.g. predicting the action taken between two egocentric observations, predicting the distance between two observations from a trajectory,etc.).We find that naively combining multiple auxiliary tasks improves sample efficiency,but only provides marginal gains beyond a point. To overcome this, we use attention to combine representations learnt from individual auxiliary tasks. Our best agent is 5.5x faster to reach the performance of the previous state-of-the-art, DD-PPO, at 40M frames, and improves on DD-PPO's performance at 40M frames by 0.16 SPL. Our code is publicly available at https://github.com/joel99/habitat-pointnav-aux.



### Efficient detection of adversarial images
- **Arxiv ID**: http://arxiv.org/abs/2007.04564v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.04564v1)
- **Published**: 2020-07-09 05:35:49+00:00
- **Updated**: 2020-07-09 05:35:49+00:00
- **Authors**: Darpan Kumar Yadav, Kartik Mundra, Rahul Modpur, Arpan Chattopadhyay, Indra Narayan Kar
- **Comment**: 10 pages, 3 figures, 3 algorithms, 8 tables. Extension of the
  Conference paper:- Kartik Mundra, Rahul Modpur, Arpan Chattopadhyay, and
  Indra Narayan Kar. Adversarial image detection in cyber-physical systems. In
  Proceedings of the 1st ACM Workshop on Autonomous and Intelligent Mobile
  Systems, pages 1-5, 2020. Can be found at
  https://dl.acm.org/doi/abs/10.1145/3377283.3377285
- **Journal**: None
- **Summary**: In this paper, detection of deception attack on deep neural network (DNN) based image classification in autonomous and cyber-physical systems is considered. Several studies have shown the vulnerability of DNN to malicious deception attacks. In such attacks, some or all pixel values of an image are modified by an external attacker, so that the change is almost invisible to the human eye but significant enough for a DNN-based classifier to misclassify it. This paper first proposes a novel pre-processing technique that facilitates the detection of such modified images under any DNN-based image classifier as well as the attacker model. The proposed pre-processing algorithm involves a certain combination of principal component analysis (PCA)-based decomposition of the image, and random perturbation based detection to reduce computational complexity. Next, an adaptive version of this algorithm is proposed where a random number of perturbations are chosen adaptively using a doubly-threshold policy, and the threshold values are learnt via stochastic approximation in order to minimize the expected number of perturbations subject to constraints on the false alarm and missed detection probabilities. Numerical experiments show that the proposed detection scheme outperforms a competing algorithm while achieving reasonably low computational complexity.



### Neural Video Coding using Multiscale Motion Compensation and Spatiotemporal Context Model
- **Arxiv ID**: http://arxiv.org/abs/2007.04574v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.04574v1)
- **Published**: 2020-07-09 06:15:17+00:00
- **Updated**: 2020-07-09 06:15:17+00:00
- **Authors**: Haojie Liu, Ming Lu, Zhan Ma, Fan Wang, Zhihuang Xie, Xun Cao, Yao Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Over the past two decades, traditional block-based video coding has made remarkable progress and spawned a series of well-known standards such as MPEG-4, H.264/AVC and H.265/HEVC. On the other hand, deep neural networks (DNNs) have shown their powerful capacity for visual content understanding, feature extraction and compact representation. Some previous works have explored the learnt video coding algorithms in an end-to-end manner, which show the great potential compared with traditional methods. In this paper, we propose an end-to-end deep neural video coding framework (NVC), which uses variational autoencoders (VAEs) with joint spatial and temporal prior aggregation (PA) to exploit the correlations in intra-frame pixels, inter-frame motions and inter-frame compensation residuals, respectively. Novel features of NVC include: 1) To estimate and compensate motion over a large range of magnitudes, we propose an unsupervised multiscale motion compensation network (MS-MCN) together with a pyramid decoder in the VAE for coding motion features that generates multiscale flow fields, 2) we design a novel adaptive spatiotemporal context model for efficient entropy coding for motion information, 3) we adopt nonlocal attention modules (NLAM) at the bottlenecks of the VAEs for implicit adaptive feature extraction and activation, leveraging its high transformation capacity and unequal weighting with joint global and local information, and 4) we introduce multi-module optimization and a multi-frame training strategy to minimize the temporal error propagation among P-frames. NVC is evaluated for the low-delay causal settings and compared with H.265/HEVC, H.264/AVC and the other learnt video compression methods following the common test conditions, demonstrating consistent gains across all popular test sequences for both PSNR and MS-SSIM distortion metrics.



### VisImages: A Fine-Grained Expert-Annotated Visualization Dataset
- **Arxiv ID**: http://arxiv.org/abs/2007.04584v5
- **DOI**: 10.1109/TVCG.2022.3155440
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.04584v5)
- **Published**: 2020-07-09 06:47:49+00:00
- **Updated**: 2022-03-06 13:02:40+00:00
- **Authors**: Dazhen Deng, Yihong Wu, Xinhuan Shu, Jiang Wu, Siwei Fu, Weiwei Cui, Yingcai Wu
- **Comment**: None
- **Journal**: None
- **Summary**: Images in visualization publications contain rich information, e.g., novel visualization designs and implicit design patterns of visualizations. A systematic collection of these images can contribute to the community in many aspects, such as literature analysis and automated tasks for visualization. In this paper, we build and make public a dataset, VisImages, which collects 12,267 images with captions from 1,397 papers in IEEE InfoVis and VAST. Built upon a comprehensive visualization taxonomy, the dataset includes 35,096 visualizations and their bounding boxes in the images.We demonstrate the usefulness of VisImages through three use cases: 1) investigating the use of visualizations in the publications with VisImages Explorer, 2) training and benchmarking models for visualization classification, and 3) localizing visualizations in the visual analytics systems automatically.



### InfoMax-GAN: Improved Adversarial Image Generation via Information Maximization and Contrastive Learning
- **Arxiv ID**: http://arxiv.org/abs/2007.04589v6
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.04589v6)
- **Published**: 2020-07-09 06:56:11+00:00
- **Updated**: 2020-11-22 18:40:18+00:00
- **Authors**: Kwot Sin Lee, Ngoc-Trung Tran, Ngai-Man Cheung
- **Comment**: Accepted to WACV 2021. An initial version was accepted to NeurIPS
  2019 Workshop on Information Theory and Machine Learning
- **Journal**: None
- **Summary**: While Generative Adversarial Networks (GANs) are fundamental to many generative modelling applications, they suffer from numerous issues. In this work, we propose a principled framework to simultaneously mitigate two fundamental issues in GANs: catastrophic forgetting of the discriminator and mode collapse of the generator. We achieve this by employing for GANs a contrastive learning and mutual information maximization approach, and perform extensive analyses to understand sources of improvements. Our approach significantly stabilizes GAN training and improves GAN performance for image synthesis across five datasets under the same training and evaluation conditions against state-of-the-art works. In particular, compared to the state-of-the-art SSGAN, our approach does not suffer from poorer performance on image domains such as faces, and instead improves performance significantly. Our approach is simple to implement and practical: it involves only one auxiliary objective, has a low computational cost, and performs robustly across a wide range of training settings and datasets without any hyperparameter tuning. For reproducibility, our code is available in Mimicry: https://github.com/kwotsin/mimicry.



### Monocular Vision based Crowdsourced 3D Traffic Sign Positioning with Unknown Camera Intrinsics and Distortion Coefficients
- **Arxiv ID**: http://arxiv.org/abs/2007.04592v1
- **DOI**: 10.1109/ITSC45102.2020.9294445
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2007.04592v1)
- **Published**: 2020-07-09 07:03:17+00:00
- **Updated**: 2020-07-09 07:03:17+00:00
- **Authors**: Hemang Chawla, Matti Jukola, Elahe Arani, Bahram Zonooz
- **Comment**: Accepted at 2020 IEEE 23rd International Conference on Intelligent
  Transportation Systems (ITSC)
- **Journal**: None
- **Summary**: Autonomous vehicles and driver assistance systems utilize maps of 3D semantic landmarks for improved decision making. However, scaling the mapping process as well as regularly updating such maps come with a huge cost. Crowdsourced mapping of these landmarks such as traffic sign positions provides an appealing alternative. The state-of-the-art approaches to crowdsourced mapping use ground truth camera parameters, which may not always be known or may change over time. In this work, we demonstrate an approach to computing 3D traffic sign positions without knowing the camera focal lengths, principal point, and distortion coefficients a priori. We validate our proposed approach on a public dataset of traffic signs in KITTI. Using only a monocular color camera and GPS, we achieve an average single journey relative and absolute positioning accuracy of 0.26 m and 1.38 m, respectively.



### Camera-Lidar Integration: Probabilistic sensor fusion for semantic mapping
- **Arxiv ID**: http://arxiv.org/abs/2007.05490v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2007.05490v1)
- **Published**: 2020-07-09 07:59:39+00:00
- **Updated**: 2020-07-09 07:59:39+00:00
- **Authors**: Julie Stephany Berrio, Mao Shan, Stewart Worrall, Eduardo Nebot
- **Comment**: 15 pages. arXiv admin note: text overlap with arXiv:2003.01871
- **Journal**: None
- **Summary**: An automated vehicle operating in an urban environment must be able to perceive and recognise object/obstacles in a three-dimensional world while navigating in a constantly changing environment. In order to plan and execute accurate sophisticated driving maneuvers, a high-level contextual understanding of the surroundings is essential. Due to the recent progress in image processing, it is now possible to obtain high definition semantic information in 2D from monocular cameras, though cameras cannot reliably provide the highly accurate 3D information provided by lasers. The fusion of these two sensor modalities can overcome the shortcomings of each individual sensor, though there are a number of important challenges that need to be addressed in a probabilistic manner. In this paper, we address the common, yet challenging, lidar/camera/semantic fusion problems which are seldom approached in a wholly probabilistic manner. Our approach is capable of using a multi-sensor platform to build a three-dimensional semantic voxelized map that considers the uncertainty of all of the processes involved. We present a probabilistic pipeline that incorporates uncertainties from the sensor readings (cameras, lidar, IMU and wheel encoders), compensation for the motion of the vehicle, and heuristic label probabilities for the semantic images. We also present a novel and efficient viewpoint validation algorithm to check for occlusions from the camera frames. A probabilistic projection is performed from the camera images to the lidar point cloud. Each labelled lidar scan then feeds into an octree map building algorithm that updates the class probabilities of the map voxels every time a new observation is available. We validate our approach using a set of qualitative and quantitative experimental tests on the USyd Dataset.



### Attention Neural Network for Trash Detection on Water Channels
- **Arxiv ID**: http://arxiv.org/abs/2007.04639v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.04639v1)
- **Published**: 2020-07-09 08:41:30+00:00
- **Updated**: 2020-07-09 08:41:30+00:00
- **Authors**: Mohbat Tharani, Abdul Wahab Amin, Mohammad Maaz, Murtaza Taj
- **Comment**: Object Detection, Trash Detection, Water Quality
- **Journal**: None
- **Summary**: Rivers and canals flowing through cities are often used illegally for dumping the trash. This contaminates freshwater channels as well as causes blockage in sewerage resulting in urban flooding. When this contaminated water reaches agricultural fields, it results in degradation of soil and poses critical environmental as well as economic threats. The dumped trash is often found floating on the water surface. The trash could be disfigured, partially submerged, decomposed into smaller pieces, clumped together with other objects which obscure its shape and creates a challenging detection problem. This paper proposes a method for the detection of visible trash floating on the water surface of the canals in urban areas. We also provide a large dataset, first of its kind, trash in water channels that contains object-level annotations. A novel attention layer is proposed that improves the detection of smaller objects. Towards the end of this paper, we provide a detailed comparison of our method with state-of-the-art object detectors and show that our method significantly improves the detection of smaller objects. The dataset will be made publicly available.



### ESA-ReID: Entropy-Based Semantic Feature Alignment for Person re-ID
- **Arxiv ID**: http://arxiv.org/abs/2007.04644v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.04644v1)
- **Published**: 2020-07-09 08:56:28+00:00
- **Updated**: 2020-07-09 08:56:28+00:00
- **Authors**: Chaoping Tu, Yin Zhao, Longjun Cai
- **Comment**: None
- **Journal**: None
- **Summary**: Person re-identification (re-ID) is a challenging task in real-world. Besides the typical application in surveillance system, re-ID also has significant values to improve the recall rate of people identification in content video (TV or Movies). However, the occlusion, shot angle variations and complicated background make it far away from application, especially in content video. In this paper we propose an entropy based semantic feature alignment model, which takes advantages of the detailed information of the human semantic feature. Considering the uncertainty of semantic segmentation, we introduce a semantic alignment with an entropy-based mask which can reduce the negative effects of mask segmentation errors. We construct a new re-ID dataset based on content videos with many cases of occlusion and body part missing, which will be released in future. Extensive studies on both existing datasets and the new dataset demonstrate the superior performance of the proposed model.



### Learning to Switch CNNs with Model Agnostic Meta Learning for Fine Precision Visual Servoing
- **Arxiv ID**: http://arxiv.org/abs/2007.04645v1
- **DOI**: 10.1109/IROS45743.2020.9341756
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.04645v1)
- **Published**: 2020-07-09 08:56:53+00:00
- **Updated**: 2020-07-09 08:56:53+00:00
- **Authors**: Prem Raj, Vinay P. Namboodiri, L. Behera
- **Comment**: Accepted in IEEE/RSJ International Conference on Intelligent Robots
  and Systems (IROS-2020). For video visit - https://youtu.be/GSG20lmWDUo
- **Journal**: 2020 IEEE/RSJ International Conference on Intelligent Robots and
  Systems (IROS), 10210-10217
- **Summary**: Convolutional Neural Networks (CNNs) have been successfully applied for relative camera pose estimation from labeled image-pair data, without requiring any hand-engineered features, camera intrinsic parameters or depth information. The trained CNN can be utilized for performing pose based visual servo control (PBVS). One of the ways to improve the quality of visual servo output is to improve the accuracy of the CNN for estimating the relative pose estimation. With a given state-of-the-art CNN for relative pose regression, how can we achieve an improved performance for visual servo control? In this paper, we explore switching of CNNs to improve the precision of visual servo control. The idea of switching a CNN is due to the fact that the dataset for training a relative camera pose regressor for visual servo control must contain variations in relative pose ranging from a very small scale to eventually a larger scale. We found that, training two different instances of the CNN, one for large-scale-displacements (LSD) and another for small-scale-displacements (SSD) and switching them during the visual servo execution yields better results than training a single CNN with the combined LSD+SSD data. However, it causes extra storage overhead and switching decision is taken by a manually set threshold which may not be optimal for all the scenes. To eliminate these drawbacks, we propose an efficient switching strategy based on model agnostic meta learning (MAML) algorithm. In this, a single model is trained to learn parameters which are simultaneously good for multiple tasks, namely a binary classification for switching decision, a 6DOF pose regression for LSD data and also a 6DOF pose regression for SSD data. The proposed approach performs far better than the naive approach, while storage and run-time overheads are almost negligible.



### JGR-P2O: Joint Graph Reasoning based Pixel-to-Offset Prediction Network for 3D Hand Pose Estimation from a Single Depth Image
- **Arxiv ID**: http://arxiv.org/abs/2007.04646v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.04646v2)
- **Published**: 2020-07-09 08:57:19+00:00
- **Updated**: 2020-07-10 03:49:36+00:00
- **Authors**: Linpu Fang, Xingyan Liu, Li Liu, Hang Xu, Wenxiong Kang
- **Comment**: Accepted by ECCV2020 as a Spotlight paper
- **Journal**: None
- **Summary**: State-of-the-art single depth image-based 3D hand pose estimation methods are based on dense predictions, including voxel-to-voxel predictions, point-to-point regression, and pixel-wise estimations. Despite the good performance, those methods have a few issues in nature, such as the poor trade-off between accuracy and efficiency, and plain feature representation learning with local convolutions. In this paper, a novel pixel-wise prediction-based method is proposed to address the above issues. The key ideas are two-fold: a) explicitly modeling the dependencies among joints and the relations between the pixels and the joints for better local feature representation learning; b) unifying the dense pixel-wise offset predictions and direct joint regression for end-to-end training. Specifically, we first propose a graph convolutional network (GCN) based joint graph reasoning module to model the complex dependencies among joints and augment the representation capability of each pixel. Then we densely estimate all pixels' offsets to joints in both image plane and depth space and calculate the joints' positions by a weighted average over all pixels' predictions, totally discarding the complex postprocessing operations. The proposed model is implemented with an efficient 2D fully convolutional network (FCN) backbone and has only about 1.4M parameters. Extensive experiments on multiple 3D hand pose estimation benchmarks demonstrate that the proposed method achieves new state-of-the-art accuracy while running very efficiently with around a speed of 110fps on a single NVIDIA 1080Ti GPU.



### Maximum Entropy Regularization and Chinese Text Recognition
- **Arxiv ID**: http://arxiv.org/abs/2007.04651v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.04651v1)
- **Published**: 2020-07-09 09:19:56+00:00
- **Updated**: 2020-07-09 09:19:56+00:00
- **Authors**: Changxu Cheng, Wuheng Xu, Xiang Bai, Bin Feng, Wenyu Liu
- **Comment**: 15 pages
- **Journal**: None
- **Summary**: Chinese text recognition is more challenging than Latin text due to the large amount of fine-grained Chinese characters and the great imbalance over classes, which causes a serious overfitting problem. We propose to apply Maximum Entropy Regularization to regularize the training process, which is to simply add a negative entropy term to the canonical cross-entropy loss without any additional parameters and modification of a model. We theoretically give the convergence probability distribution and analyze how the regularization influence the learning process. Experiments on Chinese character recognition, Chinese text line recognition and fine-grained image classification achieve consistent improvement, proving that the regularization is beneficial to generalization and robustness of a recognition model.



### Inertial Measurements for Motion Compensation in Weight-bearing Cone-beam CT of the Knee
- **Arxiv ID**: http://arxiv.org/abs/2007.04655v1
- **DOI**: 10.1007/978-3-030-59716-0_2
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.04655v1)
- **Published**: 2020-07-09 09:26:27+00:00
- **Updated**: 2020-07-09 09:26:27+00:00
- **Authors**: Jennifer Maier, Marlies Nitschke, Jang-Hwan Choi, Garry Gold, Rebecca Fahrig, Bjoern M. Eskofier, Andreas Maier
- **Comment**: 10 pages, 2 figures, 2 tables, accepted at MICCAI 2020
- **Journal**: None
- **Summary**: Involuntary motion during weight-bearing cone-beam computed tomography (CT) scans of the knee causes artifacts in the reconstructed volumes making them unusable for clinical diagnosis. Currently, image-based or marker-based methods are applied to correct for this motion, but often require long execution or preparation times. We propose to attach an inertial measurement unit (IMU) containing an accelerometer and a gyroscope to the leg of the subject in order to measure the motion during the scan and correct for it. To validate this approach, we present a simulation study using real motion measured with an optical 3D tracking system. With this motion, an XCAT numerical knee phantom is non-rigidly deformed during a simulated CT scan creating motion corrupted projections. A biomechanical model is animated with the same tracked motion in order to generate measurements of an IMU placed below the knee. In our proposed multi-stage algorithm, these signals are transformed to the global coordinate system of the CT scan and applied for motion compensation during reconstruction. Our proposed approach can effectively reduce motion artifacts in the reconstructed volumes. Compared to the motion corrupted case, the average structural similarity index and root mean squared error with respect to the no-motion case improved by 13-21% and 68-70%, respectively. These results are qualitatively and quantitatively on par with a state-of-the-art marker-based method we compared our approach to. The presented study shows the feasibility of this novel approach, and yields promising results towards a purely IMU-based motion compensation in C-arm CT.



### The autonomous hidden camera crew
- **Arxiv ID**: http://arxiv.org/abs/2007.04657v1
- **DOI**: 10.23919/MVA.2017.7986769
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.04657v1)
- **Published**: 2020-07-09 09:27:34+00:00
- **Updated**: 2020-07-09 09:27:34+00:00
- **Authors**: Timothy Callemein, Wiebe Van Ranst, Toon Goedemé
- **Comment**: 4 pages, 6 figures
- **Journal**: None
- **Summary**: Reality TV shows that follow people in their day-to-day lives are not a new concept. However, the traditional methods used in the industry require a lot of manual labour and need the presence of at least one physical camera man. Because of this, the subjects tend to behave differently when they are aware of being recorded. This paper will present an approach to follow people in their day-to-day lives, for long periods of time (months to years), while being as unobtrusive as possible. To do this, we use unmanned cinematographically-aware cameras hidden in people's houses. Our contribution in this paper is twofold: First, we create a system to limit the amount of recorded data by intelligently controlling a video switch matrix, in combination with a multi-channel recorder. Second, we create a virtual camera man by controlling a PTZ camera to automatically make cinematographically pleasing shots. Throughout this paper, we worked closely with a real camera crew. This enabled us to compare the results of our system to the work of trained professionals.



### Building Robust Industrial Applicable Object Detection Models Using Transfer Learning and Single Pass Deep Learning Architectures
- **Arxiv ID**: http://arxiv.org/abs/2007.04666v1
- **DOI**: 10.5220/0006562002090217
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.04666v1)
- **Published**: 2020-07-09 09:50:45+00:00
- **Updated**: 2020-07-09 09:50:45+00:00
- **Authors**: Steven Puttemans, Timothy Callemein, Toon Goedemé
- **Comment**: None
- **Journal**: None
- **Summary**: The uprising trend of deep learning in computer vision and artificial intelligence can simply not be ignored. On the most diverse tasks, from recognition and detection to segmentation, deep learning is able to obtain state-of-the-art results, reaching top notch performance. In this paper we explore how deep convolutional neural networks dedicated to the task of object detection can improve our industrial-oriented object detection pipelines, using state-of-the-art open source deep learning frameworks, like Darknet. By using a deep learning architecture that integrates region proposals, classification and probability estimation in a single run, we aim at obtaining real-time performance. We focus on reducing the needed amount of training data drastically by exploring transfer learning, while still maintaining a high average precision. Furthermore we apply these algorithms to two industrially relevant applications, one being the detection of promotion boards in eye tracking data and the other detecting and recognizing packages of warehouse products for augmented advertisements.



### Multi-Granularity Modularized Network for Abstract Visual Reasoning
- **Arxiv ID**: http://arxiv.org/abs/2007.04670v2
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.04670v2)
- **Published**: 2020-07-09 09:54:05+00:00
- **Updated**: 2020-07-10 02:32:25+00:00
- **Authors**: Xiangru Tang, Haoyuan Wang, Xiang Pan, Jiyang Qi
- **Comment**: None
- **Journal**: None
- **Summary**: Abstract visual reasoning connects mental abilities to the physical world, which is a crucial factor in cognitive development. Most toddlers display sensitivity to this skill, but it is not easy for machines. Aimed at it, we focus on the Raven Progressive Matrices Test, designed to measure cognitive reasoning. Recent work designed some black-boxes to solve it in an end-to-end fashion, but they are incredibly complicated and difficult to explain. Inspired by cognitive studies, we propose a Multi-Granularity Modularized Network (MMoN) to bridge the gap between the processing of raw sensory information and symbolic reasoning. Specifically, it learns modularized reasoning functions to model the semantic rule from the visual grounding in a neuro-symbolic and semi-supervision way. To comprehensively evaluate MMoN, our experiments are conducted on the dataset of both seen and unseen reasoning rules. The result shows that MMoN is well suited for abstract visual reasoning and also explainable on the generalization test.



### Automated analysis of eye-tracker-based human-human interaction studies
- **Arxiv ID**: http://arxiv.org/abs/2007.04671v1
- **DOI**: 10.1007/978-981-13-1056-0_50
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.04671v1)
- **Published**: 2020-07-09 10:00:03+00:00
- **Updated**: 2020-07-09 10:00:03+00:00
- **Authors**: Timothy Callemein, Kristof Van Beeck, Geert Brône, Toon Goedemé
- **Comment**: None
- **Journal**: None
- **Summary**: Mobile eye-tracking systems have been available for about a decade now and are becoming increasingly popular in different fields of application, including marketing, sociology, usability studies and linguistics. While the user-friendliness and ergonomics of the hardware are developing at a rapid pace, the software for the analysis of mobile eye-tracking data in some points still lacks robustness and functionality. With this paper, we investigate which state-of-the-art computer vision algorithms may be used to automate the post-analysis of mobile eye-tracking data. For the case study in this paper, we focus on mobile eye-tracker recordings made during human-human face-to-face interactions. We compared two recent publicly available frameworks (YOLOv2 and OpenPose) to relate the gaze location generated by the eye-tracker to the head and hands visible in the scene camera data. In this paper we will show that the use of this single-pipeline framework provides robust results, which are both more accurate and faster than previous work in the field. Moreover, our approach does not rely on manual interventions during this process.



### How low can you go? Privacy-preserving people detection with an omni-directional camera
- **Arxiv ID**: http://arxiv.org/abs/2007.04678v1
- **DOI**: 10.5220/0007573206300637
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.04678v1)
- **Published**: 2020-07-09 10:10:23+00:00
- **Updated**: 2020-07-09 10:10:23+00:00
- **Authors**: Timothy Callemein, Kristof Van Beeck, Toon Goedemé
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we use a ceiling-mounted omni-directional camera to detect people in a room. This can be used as a sensor to measure the occupancy of meeting rooms and count the amount of flex-desk working spaces available. If these devices can be integrated in an embedded low-power sensor, it would form an ideal extension of automated room reservation systems in office environments. The main challenge we target here is ensuring the privacy of the people filmed. The approach we propose is going to extremely low image resolutions, such that it is impossible to recognise people or read potentially confidential documents. Therefore, we retrained a single-shot low-resolution person detection network with automatically generated ground truth. In this paper, we prove the functionality of this approach and explore how low we can go in resolution, to determine the optimal trade-off between recognition accuracy and privacy preservation. Because of the low resolution, the result is a lightweight network that can potentially be deployed on embedded hardware. Such embedded implementation enables the development of a decentralised smart camera which only outputs the required meta-data (i.e. the number of persons in the meeting room).



### Not only Look, but also Listen: Learning Multimodal Violence Detection under Weak Supervision
- **Arxiv ID**: http://arxiv.org/abs/2007.04687v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.04687v2)
- **Published**: 2020-07-09 10:29:31+00:00
- **Updated**: 2020-07-13 04:16:22+00:00
- **Authors**: Peng Wu, Jing Liu, Yujia Shi, Yujia Sun, Fangtao Shao, Zhaoyang Wu, Zhiwei Yang
- **Comment**: To appear in ECCV 2020
- **Journal**: None
- **Summary**: Violence detection has been studied in computer vision for years. However, previous work are either superficial, e.g., classification of short-clips, and the single scenario, or undersupplied, e.g., the single modality, and hand-crafted features based multimodality. To address this problem, in this work we first release a large-scale and multi-scene dataset named XD-Violence with a total duration of 217 hours, containing 4754 untrimmed videos with audio signals and weak labels. Then we propose a neural network containing three parallel branches to capture different relations among video snippets and integrate features, where holistic branch captures long-range dependencies using similarity prior, localized branch captures local positional relation using proximity prior, and score branch dynamically captures the closeness of predicted score. Besides, our method also includes an approximator to meet the needs of online detection. Our method outperforms other state-of-the-art methods on our released dataset and other existing benchmark. Moreover, extensive experimental results also show the positive effect of multimodal (audio-visual) input and modeling relationships. The code and dataset will be released in https://roc-ng.github.io/XD-Violence/.



### Pollen13K: A Large Scale Microscope Pollen Grain Image Dataset
- **Arxiv ID**: http://arxiv.org/abs/2007.04690v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.04690v1)
- **Published**: 2020-07-09 10:33:31+00:00
- **Updated**: 2020-07-09 10:33:31+00:00
- **Authors**: Sebastiano Battiato, Alessandro Ortis, Francesca Trenta, Lorenzo Ascari, Mara Politi, Consolata Siniscalco
- **Comment**: This paper is a preprint of a paper accepted at the IEEE
  International Conference on Image Processing 2020
- **Journal**: None
- **Summary**: Pollen grain classification has a remarkable role in many fields from medicine to biology and agronomy. Indeed, automatic pollen grain classification is an important task for all related applications and areas. This work presents the first large-scale pollen grain image dataset, including more than 13 thousands objects. After an introduction to the problem of pollen grain classification and its motivations, the paper focuses on the employed data acquisition steps, which include aerobiological sampling, microscope image acquisition, object detection, segmentation and labelling. Furthermore, a baseline experimental assessment for the task of pollen classification on the built dataset, together with discussion on the achieved results, is presented.



### Animated GIF optimization by adaptive color local table management
- **Arxiv ID**: http://arxiv.org/abs/2007.04717v1
- **DOI**: 10.1109/ICIP40778.2020.9190967
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.04717v1)
- **Published**: 2020-07-09 11:36:48+00:00
- **Updated**: 2020-07-09 11:36:48+00:00
- **Authors**: Oliver Giudice, Dario Allegra, Francesco Guarnera, Filippo Stanco, Sebastiano Battiato
- **Comment**: None
- **Journal**: 2020 IEEE International Conference on Image Processing (ICIP)
- **Summary**: After thirty years of the GIF file format, today is becoming more popular than ever: being a great way of communication for friends and communities on Instant Messengers and Social Networks. While being so popular, the original compression method to encode GIF images have not changed a bit. On the other hand popularity means that storage saving becomes an issue for hosting platforms. In this paper a parametric optimization technique for animated GIFs will be presented. The proposed technique is based on Local Color Table selection and color remapping in order to create optimized animated GIFs while preserving the original format. The technique achieves good results in terms of byte reduction with limited or no loss of perceived color quality. Tests carried out on 1000 GIF files demonstrate the effectiveness of the proposed optimization strategy.



### Brain Tumor Anomaly Detection via Latent Regularized Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/2007.04734v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, 68T07, I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/2007.04734v1)
- **Published**: 2020-07-09 12:12:16+00:00
- **Updated**: 2020-07-09 12:12:16+00:00
- **Authors**: Nan Wang, Chengwei Chen, Yuan Xie, Lizhuang Ma
- **Comment**: 9 pages, 7 figures
- **Journal**: None
- **Summary**: With the development of medical imaging technology, medical images have become an important basis for doctors to diagnose patients. The brain structure in the collected data is complicated, thence, doctors are required to spend plentiful energy when diagnosing brain abnormalities. Aiming at the imbalance of brain tumor data and the rare amount of labeled data, we propose an innovative brain tumor abnormality detection algorithm. The semi-supervised anomaly detection model is proposed in which only healthy (normal) brain images are trained. Model capture the common pattern of the normal images in the training process and detect anomalies based on the reconstruction error of latent space. Furthermore, the method first uses singular value to constrain the latent space and jointly optimizes the image space through multiple loss functions, which make normal samples and abnormal samples more separable in the feature-level. This paper utilizes BraTS, HCP, MNIST, and CIFAR-10 datasets to comprehensively evaluate the effectiveness and practicability. Extensive experiments on intra- and cross-dataset tests prove that our semi-supervised method achieves outperforms or comparable results to state-of-the-art supervised techniques.



### JBFnet -- Low Dose CT Denoising by Trainable Joint Bilateral Filtering
- **Arxiv ID**: http://arxiv.org/abs/2007.04754v1
- **DOI**: 10.1007/978-3-030-59713-9_49
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.04754v1)
- **Published**: 2020-07-09 12:59:28+00:00
- **Updated**: 2020-07-09 12:59:28+00:00
- **Authors**: Mayank Patwari, Ralf Gutjahr, Rainer Raupach, Andreas Maier
- **Comment**: 10 pages, 4 figures, 1 table. Accepted at MICCAI2020
- **Journal**: None
- **Summary**: Deep neural networks have shown great success in low dose CT denoising. However, most of these deep neural networks have several hundred thousand trainable parameters. This, combined with the inherent non-linearity of the neural network, makes the deep neural network diffcult to understand with low accountability. In this study we introduce JBFnet, a neural network for low dose CT denoising. The architecture of JBFnet implements iterative bilateral filtering. The filter functions of the Joint Bilateral Filter (JBF) are learned via shallow convolutional networks. The guidance image is estimated by a deep neural network. JBFnet is split into four filtering blocks, each of which performs Joint Bilateral Filtering. Each JBF block consists of 112 trainable parameters, making the noise removal process comprehendable. The Noise Map (NM) is added after filtering to preserve high level features. We train JBFnet with the data from the body scans of 10 patients, and test it on the AAPM low dose CT Grand Challenge dataset. We compare JBFnet with state-of-the-art deep learning networks. JBFnet outperforms CPCE3D, GAN and deep GFnet on the test dataset in terms of noise removal while preserving structures. We conduct several ablation studies to test the performance of our network architecture and training method. Our current setup achieves the best performance, while still maintaining behavioural accountability.



### Generalized Few-Shot Video Classification with Video Retrieval and Feature Generation
- **Arxiv ID**: http://arxiv.org/abs/2007.04755v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.04755v2)
- **Published**: 2020-07-09 13:05:32+00:00
- **Updated**: 2021-10-13 13:31:06+00:00
- **Authors**: Yongqin Xian, Bruno Korbar, Matthijs Douze, Lorenzo Torresani, Bernt Schiele, Zeynep Akata
- **Comment**: Accepted by TPAMI in October, 2021
- **Journal**: None
- **Summary**: Few-shot learning aims to recognize novel classes from a few examples. Although significant progress has been made in the image domain, few-shot video classification is relatively unexplored. We argue that previous methods underestimate the importance of video feature learning and propose to learn spatiotemporal features using a 3D CNN. Proposing a two-stage approach that learns video features on base classes followed by fine-tuning the classifiers on novel classes, we show that this simple baseline approach outperforms prior few-shot video classification methods by over 20 points on existing benchmarks. To circumvent the need of labeled examples, we present two novel approaches that yield further improvement. First, we leverage tag-labeled videos from a large dataset using tag retrieval followed by selecting the best clips with visual similarities. Second, we learn generative adversarial networks that generate video features of novel classes from their semantic embeddings. Moreover, we find existing benchmarks are limited because they only focus on 5 novel classes in each testing episode and introduce more realistic benchmarks by involving more novel classes, i.e. few-shot learning, as well as a mixture of novel and base classes, i.e. generalized few-shot learning. The experimental results show that our retrieval and feature generation approach significantly outperform the baseline approach on the new benchmarks.



### Learning to Prune Deep Neural Networks via Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/2007.04756v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2007.04756v1)
- **Published**: 2020-07-09 13:06:07+00:00
- **Updated**: 2020-07-09 13:06:07+00:00
- **Authors**: Manas Gupta, Siddharth Aravindan, Aleksandra Kalisz, Vijay Chandrasekhar, Lin Jie
- **Comment**: Accepted at the ICML 2020 Workshop on Automated Machine Learning
  (AutoML 2020)
- **Journal**: None
- **Summary**: This paper proposes PuRL - a deep reinforcement learning (RL) based algorithm for pruning neural networks. Unlike current RL based model compression approaches where feedback is given only at the end of each episode to the agent, PuRL provides rewards at every pruning step. This enables PuRL to achieve sparsity and accuracy comparable to current state-of-the-art methods, while having a much shorter training cycle. PuRL achieves more than 80% sparsity on the ResNet-50 model while retaining a Top-1 accuracy of 75.37% on the ImageNet dataset. Through our experiments we show that PuRL is also able to sparsify already efficient architectures like MobileNet-V2. In addition to performance characterisation experiments, we also provide a discussion and analysis of the various RL design choices that went into the tuning of the Markov Decision Process underlying PuRL. Lastly, we point out that PuRL is simple to use and can be easily adapted for various architectures.



### Low Dose CT Denoising via Joint Bilateral Filtering and Intelligent Parameter Optimization
- **Arxiv ID**: http://arxiv.org/abs/2007.04768v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.04768v1)
- **Published**: 2020-07-09 13:17:36+00:00
- **Updated**: 2020-07-09 13:17:36+00:00
- **Authors**: Mayank Patwari, Ralf Gutjahr, Rainer Raupach, Andreas Maier
- **Comment**: 4 pages, 5 figures, 1 table. Accepted at CT Meeting 2020
- **Journal**: None
- **Summary**: Denoising of clinical CT images is an active area for deep learning research. Current clinically approved methods use iterative reconstruction methods to reduce the noise in CT images. Iterative reconstruction techniques require multiple forward and backward projections, which are time-consuming and computationally expensive. Recently, deep learning methods have been successfully used to denoise CT images. However, conventional deep learning methods suffer from the 'black box' problem. They have low accountability, which is necessary for use in clinical imaging situations. In this paper, we use a Joint Bilateral Filter (JBF) to denoise our CT images. The guidance image of the JBF is estimated using a deep residual convolutional neural network (CNN). The range smoothing and spatial smoothing parameters of the JBF are tuned by a deep reinforcement learning task. Our actor first chooses a parameter, and subsequently chooses an action to tune the value of the parameter. A reward network is designed to direct the reinforcement learning task. Our denoising method demonstrates good denoising performance, while retaining structural information. Our method significantly outperforms state of the art deep neural networks. Moreover, our method has only two parameters, which makes it significantly more interpretable and reduces the 'black box' problem. We experimentally measure the impact of our intelligent parameter optimization and our reward network. Our studies show that our current setup yields the best results in terms of structural preservation.



### Modelling the Distribution of 3D Brain MRI using a 2D Slice VAE
- **Arxiv ID**: http://arxiv.org/abs/2007.04780v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.04780v1)
- **Published**: 2020-07-09 13:23:15+00:00
- **Updated**: 2020-07-09 13:23:15+00:00
- **Authors**: Anna Volokitin, Ertunc Erdil, Neerav Karani, Kerem Can Tezcan, Xiaoran Chen, Luc Van Gool, Ender Konukoglu
- **Comment**: accepted for publication at MICCAI 2020. Code available
  https://github.com/voanna/slices-to-3d-brain-vae/
- **Journal**: None
- **Summary**: Probabilistic modelling has been an essential tool in medical image analysis, especially for analyzing brain Magnetic Resonance Images (MRI). Recent deep learning techniques for estimating high-dimensional distributions, in particular Variational Autoencoders (VAEs), opened up new avenues for probabilistic modeling. Modelling of volumetric data has remained a challenge, however, because constraints on available computation and training data make it difficult effectively leverage VAEs, which are well-developed for 2D images. We propose a method to model 3D MR brain volumes distribution by combining a 2D slice VAE with a Gaussian model that captures the relationships between slices. We do so by estimating the sample mean and covariance in the latent space of the 2D model over the slice direction. This combined model lets us sample new coherent stacks of latent variables to decode into slices of a volume. We also introduce a novel evaluation method for generated volumes that quantifies how well their segmentations match those of true brain anatomy. We demonstrate that our proposed model is competitive in generating high quality volumes at high resolutions according to both traditional metrics and our proposed evaluation.



### A Systematic Review on Context-Aware Recommender Systems using Deep Learning and Embeddings
- **Arxiv ID**: http://arxiv.org/abs/2007.04782v1
- **DOI**: None
- **Categories**: **cs.IR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.04782v1)
- **Published**: 2020-07-09 13:23:40+00:00
- **Updated**: 2020-07-09 13:23:40+00:00
- **Authors**: Igor André Pegoraro Santana, Marcos Aurelio Domingues
- **Comment**: 15 pages
- **Journal**: None
- **Summary**: Recommender Systems are tools that improve how users find relevant information in web systems, so they do not face too much information. In order to generate better recommendations, the context of information should be used in the recommendation process. Context-Aware Recommender Systems were created, accomplishing state-of-the-art results and improving traditional recommender systems. There are many approaches to build recommender systems, and two of the most prominent advances in area have been the use of Embeddings to represent the data in the recommender system, and the use of Deep Learning architectures to generate the recommendations to the user. A systematic review adopts a formal and systematic method to perform a bibliographic review, and it is used to identify and evaluate all the research in certain area of study, by analyzing the relevant research published. A systematic review was conducted to understand how the Deep Learning and Embeddings techniques are being applied to improve Context-Aware Recommender Systems. We summarized the architectures that are used to create those and the domains that they are used.



### Accuracy Prediction with Non-neural Model for Neural Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/2007.04785v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.04785v3)
- **Published**: 2020-07-09 13:28:49+00:00
- **Updated**: 2021-07-19 07:31:57+00:00
- **Authors**: Renqian Luo, Xu Tan, Rui Wang, Tao Qin, Enhong Chen, Tie-Yan Liu
- **Comment**: Code is available at https://github.com/renqianluo/GBDT-NAS
- **Journal**: None
- **Summary**: Neural architecture search (NAS) with an accuracy predictor that predicts the accuracy of candidate architectures has drawn increasing attention due to its simplicity and effectiveness. Previous works usually employ neural network-based predictors which require more delicate design and are easy to overfit. Considering that most architectures are represented as sequences of discrete symbols which are more like tabular data and preferred by non-neural predictors, in this paper, we study an alternative approach which uses non-neural model for accuracy prediction. Specifically, as decision tree based models can better handle tabular data, we leverage gradient boosting decision tree (GBDT) as the predictor for NAS. We demonstrate that the GBDT predictor can achieve comparable (if not better) prediction accuracy than neural network based predictors. Moreover, considering that a compact search space can ease the search process, we propose to prune the search space gradually according to important features derived from GBDT. In this way, NAS can be performed by first pruning the search space and then searching a neural architecture, which is more efficient and effective. Experiments on NASBench-101 and ImageNet demonstrate the effectiveness of using GBDT as predictor for NAS: (1) On NASBench-101, it is 22x, 8x, and 6x more sample efficient than random search, regularized evolution, and Monte Carlo Tree Search (MCTS) in finding the global optimum; (2) It achieves 24.2% top-1 error rate on ImageNet, and further achieves 23.4% top-1 error rate on ImageNet when enhanced with search space pruning. Code is provided at https://github.com/renqianluo/GBDT-NAS.



### Client Adaptation improves Federated Learning with Simulated Non-IID Clients
- **Arxiv ID**: http://arxiv.org/abs/2007.04806v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.04806v1)
- **Published**: 2020-07-09 13:48:39+00:00
- **Updated**: 2020-07-09 13:48:39+00:00
- **Authors**: Laura Rieger, Rasmus M. Th. Høegh, Lars K. Hansen
- **Comment**: 11 pages, 11 figures. To appear at International Workshop on
  Federated Learning for User Privacy and Data Confidentiality in Conjunction
  with ICML 2020
- **Journal**: None
- **Summary**: We present a federated learning approach for learning a client adaptable, robust model when data is non-identically and non-independently distributed (non-IID) across clients. By simulating heterogeneous clients, we show that adding learned client-specific conditioning improves model performance, and the approach is shown to work on balanced and imbalanced data set from both audio and image domains. The client adaptation is implemented by a conditional gated activation unit and is particularly beneficial when there are large differences between the data distribution for each client, a common scenario in federated learning.



### Medical Instrument Detection in Ultrasound-Guided Interventions: A Review
- **Arxiv ID**: http://arxiv.org/abs/2007.04807v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2007.04807v2)
- **Published**: 2020-07-09 13:50:18+00:00
- **Updated**: 2021-02-01 15:32:12+00:00
- **Authors**: Hongxu Yang, Caifeng Shan, Alexander F. Kolen, Peter H. N. de With
- **Comment**: Draft paper
- **Journal**: None
- **Summary**: Medical instrument detection is essential for computer-assisted interventions since it would facilitate the surgeons to find the instrument efficiently with a better interpretation, which leads to a better outcome. This article reviews medical instrument detection methods in the ultrasound-guided intervention. First, we present a comprehensive review of instrument detection methodologies, which include traditional non-data-driven methods and data-driven methods. The non-data-driven methods were extensively studied prior to the era of machine learning, i.e. data-driven approaches. We discuss the main clinical applications of medical instrument detection in ultrasound, including anesthesia, biopsy, prostate brachytherapy, and cardiac catheterization, which were validated on clinical datasets. Finally, we selected several principal publications to summarize the key issues and potential research directions for the computer-assisted intervention community.



### A Deep Joint Sparse Non-negative Matrix Factorization Framework for Identifying the Common and Subject-specific Functional Units of Tongue Motion During Speech
- **Arxiv ID**: http://arxiv.org/abs/2007.04865v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.04865v2)
- **Published**: 2020-07-09 15:05:44+00:00
- **Updated**: 2021-06-06 23:10:25+00:00
- **Authors**: Jonghye Woo, Fangxu Xing, Jerry L. Prince, Maureen Stone, Arnold Gomez, Timothy G. Reese, Van J. Wedeen, Georges El Fakhri
- **Comment**: Accepted by Medical Image Analysis
- **Journal**: None
- **Summary**: Intelligible speech is produced by creating varying internal local muscle groupings -- i.e., functional units -- that are generated in a systematic and coordinated manner. There are two major challenges in characterizing and analyzing functional units.~First, due to the complex and convoluted nature of tongue structure and function, it is of great importance to develop a method that can accurately decode complex muscle coordination patterns during speech. Second, it is challenging to keep identified functional units across subjects comparable due to their substantial variability. In this work, to address these challenges, we develop a new deep learning framework to identify common and subject-specific functional units of tongue motion during speech.~Our framework hinges on joint deep graph-regularized sparse non-negative matrix factorization (NMF) using motion quantities derived from displacements by tagged Magnetic Resonance Imaging. More specifically, we transform NMF with sparse and graph regularizations into modular architectures akin to deep neural networks by means of unfolding the Iterative Shrinkage-Thresholding Algorithm to learn interpretable building blocks and associated weighting map. We then apply spectral clustering to common and subject-specific weighting maps from which we jointly determine the common and subject-specific functional units. Experiments carried out with simulated datasets show that the proposed method achieved on par or better clustering performance over the comparison methods. Experiments carried out with in vivo tongue motion data show that the proposed method can determine the common and subject-specific functional units with increased interpretability and decreased size variability.



### Invertible Zero-Shot Recognition Flows
- **Arxiv ID**: http://arxiv.org/abs/2007.04873v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.04873v1)
- **Published**: 2020-07-09 15:21:28+00:00
- **Updated**: 2020-07-09 15:21:28+00:00
- **Authors**: Yuming Shen, Jie Qin, Lei Huang
- **Comment**: ECCV2020
- **Journal**: None
- **Summary**: Deep generative models have been successfully applied to Zero-Shot Learning (ZSL) recently. However, the underlying drawbacks of GANs and VAEs (e.g., the hardness of training with ZSL-oriented regularizers and the limited generation quality) hinder the existing generative ZSL models from fully bypassing the seen-unseen bias. To tackle the above limitations, for the first time, this work incorporates a new family of generative models (i.e., flow-based models) into ZSL. The proposed Invertible Zero-shot Flow (IZF) learns factorized data embeddings (i.e., the semantic factors and the non-semantic ones) with the forward pass of an invertible flow network, while the reverse pass generates data samples. This procedure theoretically extends conventional generative flows to a factorized conditional scheme. To explicitly solve the bias problem, our model enlarges the seen-unseen distributional discrepancy based on negative sample-based distance measurement. Notably, IZF works flexibly with either a naive Bayesian classifier or a held-out trainable one for zero-shot recognition. Experiments on widely-adopted ZSL benchmarks demonstrate the significant performance gain of IZF over existing methods, in both classic and generalized settings.



### PIE-NET: Parametric Inference of Point Cloud Edges
- **Arxiv ID**: http://arxiv.org/abs/2007.04883v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.04883v2)
- **Published**: 2020-07-09 15:35:10+00:00
- **Updated**: 2020-10-25 15:25:35+00:00
- **Authors**: Xiaogang Wang, Yuelang Xu, Kai Xu, Andrea Tagliasacchi, Bin Zhou, Ali Mahdavi-Amiri, Hao Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce an end-to-end learnable technique to robustly identify feature edges in 3D point cloud data. We represent these edges as a collection of parametric curves (i.e.,lines, circles, and B-splines). Accordingly, our deep neural network, coined PIE-NET, is trained for parametric inference of edges. The network relies on a "region proposal" architecture, where a first module proposes an over-complete collection of edge and corner points, and a second module ranks each proposal to decide whether it should be considered. We train and evaluate our method on the ABC dataset, a large dataset of CAD models, and compare our results to those produced by traditional (non-learning) processing pipelines, as well as a recent deep learning based edge detector (EC-NET). Our results significantly improve over the state-of-the-art from both a quantitative and qualitative standpoint.



### Cross-Modal Weighting Network for RGB-D Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2007.04901v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.04901v1)
- **Published**: 2020-07-09 16:01:44+00:00
- **Updated**: 2020-07-09 16:01:44+00:00
- **Authors**: Gongyang Li, Zhi Liu, Linwei Ye, Yang Wang, Haibin Ling
- **Comment**: Accepted in ECCV2020. Code: https://github.com/MathLee/CMWNet
- **Journal**: None
- **Summary**: Depth maps contain geometric clues for assisting Salient Object Detection (SOD). In this paper, we propose a novel Cross-Modal Weighting (CMW) strategy to encourage comprehensive interactions between RGB and depth channels for RGB-D SOD. Specifically, three RGB-depth interaction modules, named CMW-L, CMW-M and CMW-H, are developed to deal with respectively low-, middle- and high-level cross-modal information fusion. These modules use Depth-to-RGB Weighing (DW) and RGB-to-RGB Weighting (RW) to allow rich cross-modal and cross-scale interactions among feature layers generated by different network blocks. To effectively train the proposed Cross-Modal Weighting Network (CMWNet), we design a composite loss function that summarizes the errors between intermediate predictions and ground truth over different scales. With all these novel components working together, CMWNet effectively fuses information from RGB and depth channels, and meanwhile explores object localization and details across scales. Thorough evaluations demonstrate CMWNet consistently outperforms 15 state-of-the-art RGB-D SOD methods on seven popular benchmarks.



### Uncertainty Quantification in Deep Residual Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2007.04905v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.04905v1)
- **Published**: 2020-07-09 16:05:37+00:00
- **Updated**: 2020-07-09 16:05:37+00:00
- **Authors**: Lukasz Wandzik, Raul Vicente Garcia, Jörg Krüger
- **Comment**: None
- **Journal**: None
- **Summary**: Uncertainty quantification is an important and challenging problem in deep learning. Previous methods rely on dropout layers which are not present in modern deep architectures or batch normalization which is sensitive to batch sizes. In this work, we address the problem of uncertainty quantification in deep residual networks by using a regularization technique called stochastic depth. We show that training residual networks using stochastic depth can be interpreted as a variational approximation to the intractable posterior over the weights in Bayesian neural networks. We demonstrate that by sampling from a distribution of residual networks with varying depth and shared weights, meaningful uncertainty estimates can be obtained. Moreover, compared to the original formulation of residual networks, our method produces well-calibrated softmax probabilities with only minor changes to the network's structure. We evaluate our approach on popular computer vision datasets and measure the quality of uncertainty estimates. We also test the robustness to domain shift and show that our method is able to express higher predictive uncertainty on out-of-distribution samples. Finally, we demonstrate how the proposed approach could be used to obtain uncertainty estimates in facial verification applications.



### Patient-Specific Domain Adaptation for Fast Optical Flow Based on Teacher-Student Knowledge Transfer
- **Arxiv ID**: http://arxiv.org/abs/2007.04928v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.04928v1)
- **Published**: 2020-07-09 17:01:08+00:00
- **Updated**: 2020-07-09 17:01:08+00:00
- **Authors**: Sontje Ihler, Max-Heinrich Laves, Tobias Ortmaier
- **Comment**: None
- **Journal**: None
- **Summary**: Fast motion feedback is crucial in computer-aided surgery (CAS) on moving tissue. Image-assistance in safety-critical vision applications requires a dense tracking of tissue motion. This can be done using optical flow (OF). Accurate motion predictions at high processing rates lead to higher patient safety. Current deep learning OF models show the common speed vs. accuracy trade-off. To achieve high accuracy at high processing rates, we propose patient-specific fine-tuning of a fast model. This minimizes the domain gap between training and application data, while reducing the target domain to the capability of the lower complex, fast model. We propose to obtain training sequences pre-operatively in the operation room. We handle missing ground truth, by employing teacher-student learning. Using flow estimations from teacher model FlowNet2 we specialize a fast student model FlowNet2S on the patient-specific domain. Evaluation is performed on sequences from the Hamlyn dataset. Our student model shows very good performance after fine-tuning. Tracking accuracy is comparable to the teacher model at a speed up of factor six. Fine-tuning can be performed within minutes, making it feasible for the operation room. Our method allows to use a real-time capable model that was previously not suited for this task. This method is laying the path for improved patient-specific motion estimation in CAS.



### Single architecture and multiple task deep neural network for altered fingerprint analysis
- **Arxiv ID**: http://arxiv.org/abs/2007.04931v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.04931v1)
- **Published**: 2020-07-09 17:02:09+00:00
- **Updated**: 2020-07-09 17:02:09+00:00
- **Authors**: Oliver Giudice, Mattia Litrico, Sebastiano Battiato
- **Comment**: None
- **Journal**: None
- **Summary**: Fingerprints are one of the most copious evidence in a crime scene and, for this reason, they are frequently used by law enforcement for identification of individuals. But fingerprints can be altered. "Altered fingerprints", refers to intentionally damage of the friction ridge pattern and they are often used by smart criminals in hope to evade law enforcement. We use a deep neural network approach training an Inception-v3 architecture. This paper proposes a method for detection of altered fingerprints, identification of types of alterations and recognition of gender, hand and fingers. We also produce activation maps that show which part of a fingerprint the neural network has focused on, in order to detect where alterations are positioned. The proposed approach achieves an accuracy of 98.21%, 98.46%, 92.52%, 97.53% and 92,18% for the classification of fakeness, alterations, gender, hand and fingers, respectively on the SO.CO.FING. dataset.



### Anyone here? Smart embedded low-resolution omnidirectional video sensor to measure room occupancy
- **Arxiv ID**: http://arxiv.org/abs/2007.04934v1
- **DOI**: 10.1109/ICMLA.2019.00319
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.04934v1)
- **Published**: 2020-07-09 17:05:32+00:00
- **Updated**: 2020-07-09 17:05:32+00:00
- **Authors**: Timothy Callemein, Kristof Van Beeck, Toon Goedemé
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present a room occupancy sensing solution with unique properties: (i) It is based on an omnidirectional vision camera, capturing rich scene info over a wide angle, enabling to count the number of people in a room and even their position. (ii) Although it uses a camera-input, no privacy issues arise because its extremely low image resolution, rendering people unrecognisable. (iii) The neural network inference is running entirely on a low-cost processing platform embedded in the sensor, reducing the privacy risk even further. (iv) Limited manual data annotation is needed, because of the self-training scheme we propose. Such a smart room occupancy rate sensor can be used in e.g. meeting rooms and flex-desks. Indeed, by encouraging flex-desking, the required office space can be reduced significantly. In some cases, however, a flex-desk that has been reserved remains unoccupied without an update in the reservation system. A similar problem occurs with meeting rooms, which are often under-occupied. By optimising the occupancy rate a huge reduction in costs can be achieved. Therefore, in this paper, we develop such system which determines the number of people present in office flex-desks and meeting rooms. Using an omnidirectional camera mounted in the ceiling, combined with a person detector, the company can intelligently update the reservation system based on the measured occupancy. Next to the optimisation and embedded implementation of such a self-training omnidirectional people detection algorithm, in this work we propose a novel approach that combines spatial and temporal image data, improving performance of our system on extreme low-resolution images.



### Semi-supervised Task-driven Data Augmentation for Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2007.05363v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.05363v2)
- **Published**: 2020-07-09 17:05:42+00:00
- **Updated**: 2020-11-19 17:34:51+00:00
- **Authors**: Krishna Chaitanya, Neerav Karani, Christian F. Baumgartner, Ertunc Erdil, Anton Becker, Olivio Donati, Ender Konukoglu
- **Comment**: 15 pages, 11 Figures, 3 tables. Accepted at Medical Image Analysis,
  2020
- **Journal**: None
- **Summary**: Supervised learning-based segmentation methods typically require a large number of annotated training data to generalize well at test time. In medical applications, curating such datasets is not a favourable option because acquiring a large number of annotated samples from experts is time-consuming and expensive. Consequently, numerous methods have been proposed in the literature for learning with limited annotated examples. Unfortunately, the proposed approaches in the literature have not yet yielded significant gains over random data augmentation for image segmentation, where random augmentations themselves do not yield high accuracy. In this work, we propose a novel task-driven data augmentation method for learning with limited labeled data where the synthetic data generator, is optimized for the segmentation task. The generator of the proposed method models intensity and shape variations using two sets of transformations, as additive intensity transformations and deformation fields. Both transformations are optimized using labeled as well as unlabeled examples in a semi-supervised framework. Our experiments on three medical datasets, namely cardic, prostate and pancreas, show that the proposed approach significantly outperforms standard augmentation and semi-supervised approaches for image segmentation in the limited annotation setting. The code is made publicly available at https://github.com/krishnabits001/task$\_$driven$\_$data$\_$augmentation.



### The Phong Surface: Efficient 3D Model Fitting using Lifted Optimization
- **Arxiv ID**: http://arxiv.org/abs/2007.04940v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.04940v1)
- **Published**: 2020-07-09 17:10:11+00:00
- **Updated**: 2020-07-09 17:10:11+00:00
- **Authors**: Jingjing Shen, Thomas J. Cashman, Qi Ye, Tim Hutton, Toby Sharp, Federica Bogo, Andrew William Fitzgibbon, Jamie Shotton
- **Comment**: None
- **Journal**: ECCV2020
- **Summary**: Realtime perceptual and interaction capabilities in mixed reality require a range of 3D tracking problems to be solved at low latency on resource-constrained hardware such as head-mounted devices. Indeed, for devices such as HoloLens 2 where the CPU and GPU are left available for applications, multiple tracking subsystems are required to run on a continuous, real-time basis while sharing a single Digital Signal Processor. To solve model-fitting problems for HoloLens 2 hand tracking, where the computational budget is approximately 100 times smaller than an iPhone 7, we introduce a new surface model: the `Phong surface'. Using ideas from computer graphics, the Phong surface describes the same 3D shape as a triangulated mesh model, but with continuous surface normals which enable the use of lifting-based optimization, providing significant efficiency gains over ICP-based methods. We show that Phong surfaces retain the convergence benefits of smoother surface models, while triangle meshes do not.



### Real-time Embedded Person Detection and Tracking for Shopping Behaviour Analysis
- **Arxiv ID**: http://arxiv.org/abs/2007.04942v1
- **DOI**: 10.1007/978-3-030-40605-9_46
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.04942v1)
- **Published**: 2020-07-09 17:14:15+00:00
- **Updated**: 2020-07-09 17:14:15+00:00
- **Authors**: Robin Schrijvers, Steven Puttemans, Timothy Callemein, Toon Goedemé
- **Comment**: None
- **Journal**: None
- **Summary**: Shopping behaviour analysis through counting and tracking of people in shop-like environments offers valuable information for store operators and provides key insights in the stores layout (e.g. frequently visited spots). Instead of using extra staff for this, automated on-premise solutions are preferred. These automated systems should be cost-effective, preferably on lightweight embedded hardware, work in very challenging situations (e.g. handling occlusions) and preferably work real-time. We solve this challenge by implementing a real-time TensorRT optimized YOLOv3-based pedestrian detector, on a Jetson TX2 hardware platform. By combining the detector with a sparse optical flow tracker we assign a unique ID to each customer and tackle the problem of loosing partially occluded customers. Our detector-tracker based solution achieves an average precision of 81.59% at a processing speed of 10 FPS. Besides valuable statistics, heat maps of frequently visited spots are extracted and used as an overlay on the video stream.



### AI Assisted Apparel Design
- **Arxiv ID**: http://arxiv.org/abs/2007.04950v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2007.04950v2)
- **Published**: 2020-07-09 17:24:40+00:00
- **Updated**: 2020-07-10 17:14:17+00:00
- **Authors**: Alpana Dubey, Nitish Bhardwaj, Kumar Abhinav, Suma Mani Kuriakose, Sakshi Jain, Veenu Arora
- **Comment**: None
- **Journal**: None
- **Summary**: Fashion is a fast-changing industry where designs are refreshed at large scale every season. Moreover, it faces huge challenge of unsold inventory as not all designs appeal to customers. This puts designers under significant pressure. Firstly, they need to create innumerous fresh designs. Secondly, they need to create designs that appeal to customers. Although we see advancements in approaches to help designers analyzing consumers, often such insights are too many. Creating all possible designs with those insights is time consuming. In this paper, we propose a system of AI assistants that assists designers in their design journey. The proposed system assists designers in analyzing different selling/trending attributes of apparels. We propose two design generation assistants namely Apparel-Style-Merge and Apparel-Style-Transfer. Apparel-Style-Merge generates new designs by combining high level components of apparels whereas Apparel-Style-Transfer generates multiple customization of apparels by applying different styles, colors and patterns. We compose a new dataset, named DeepAttributeStyle, with fine-grained annotation of landmarks of different apparel components such as neck, sleeve etc. The proposed system is evaluated on a user group consisting of people with and without design background. Our evaluation result demonstrates that our approach generates high quality designs that can be easily used in fabrication. Moreover, the suggested designs aid to the designers creativity.



### ThreeDWorld: A Platform for Interactive Multi-Modal Physical Simulation
- **Arxiv ID**: http://arxiv.org/abs/2007.04954v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2007.04954v2)
- **Published**: 2020-07-09 17:33:27+00:00
- **Updated**: 2021-12-28 17:03:21+00:00
- **Authors**: Chuang Gan, Jeremy Schwartz, Seth Alter, Damian Mrowca, Martin Schrimpf, James Traer, Julian De Freitas, Jonas Kubilius, Abhishek Bhandwaldar, Nick Haber, Megumi Sano, Kuno Kim, Elias Wang, Michael Lingelbach, Aidan Curtis, Kevin Feigelis, Daniel M. Bear, Dan Gutfreund, David Cox, Antonio Torralba, James J. DiCarlo, Joshua B. Tenenbaum, Josh H. McDermott, Daniel L. K. Yamins
- **Comment**: Oral Presentation at NeurIPS 21 Datasets and Benchmarks Track.
  Project page: http://www.threedworld.org
- **Journal**: None
- **Summary**: We introduce ThreeDWorld (TDW), a platform for interactive multi-modal physical simulation. TDW enables simulation of high-fidelity sensory data and physical interactions between mobile agents and objects in rich 3D environments. Unique properties include: real-time near-photo-realistic image rendering; a library of objects and environments, and routines for their customization; generative procedures for efficiently building classes of new environments; high-fidelity audio rendering; realistic physical interactions for a variety of material types, including cloths, liquid, and deformable objects; customizable agents that embody AI agents; and support for human interactions with VR devices. TDW's API enables multiple agents to interact within a simulation and returns a range of sensor and physics data representing the state of the world. We present initial experiments enabled by TDW in emerging research directions in computer vision, machine learning, and cognitive science, including multi-modal physical scene understanding, physical dynamics predictions, multi-agent interactions, models that learn like a child, and attention studies in humans and neural networks.



### Improving Style-Content Disentanglement in Image-to-Image Translation
- **Arxiv ID**: http://arxiv.org/abs/2007.04964v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.04964v1)
- **Published**: 2020-07-09 17:51:56+00:00
- **Updated**: 2020-07-09 17:51:56+00:00
- **Authors**: Aviv Gabbay, Yedid Hoshen
- **Comment**: Project page:
  http://www.vision.huji.ac.il/style-content-disentanglement
- **Journal**: None
- **Summary**: Unsupervised image-to-image translation methods have achieved tremendous success in recent years. However, it can be easily observed that their models contain significant entanglement which often hurts the translation performance. In this work, we propose a principled approach for improving style-content disentanglement in image-to-image translation. By considering the information flow into each of the representations, we introduce an additional loss term which serves as a content-bottleneck. We show that the results of our method are significantly more disentangled than those produced by current methods, while further improving the visual quality and translation diversity.



### One Policy to Control Them All: Shared Modular Policies for Agent-Agnostic Control
- **Arxiv ID**: http://arxiv.org/abs/2007.04976v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.04976v1)
- **Published**: 2020-07-09 17:59:35+00:00
- **Updated**: 2020-07-09 17:59:35+00:00
- **Authors**: Wenlong Huang, Igor Mordatch, Deepak Pathak
- **Comment**: Accepted at ICML 2020. Videos and code at
  https://huangwl18.github.io/modular-rl/
- **Journal**: None
- **Summary**: Reinforcement learning is typically concerned with learning control policies tailored to a particular agent. We investigate whether there exists a single global policy that can generalize to control a wide variety of agent morphologies -- ones in which even dimensionality of state and action spaces changes. We propose to express this global policy as a collection of identical modular neural networks, dubbed as Shared Modular Policies (SMP), that correspond to each of the agent's actuators. Every module is only responsible for controlling its corresponding actuator and receives information from only its local sensors. In addition, messages are passed between modules, propagating information between distant modules. We show that a single modular policy can successfully generate locomotion behaviors for several planar agents with different skeletal structures such as monopod hoppers, quadrupeds, bipeds, and generalize to variants not seen during training -- a process that would normally require training and manual hyperparameter tuning for each morphology. We observe that a wide variety of drastically diverse locomotion styles across morphologies as well as centralized coordination emerges via message passing between decentralized modules purely from the reinforcement learning objective. Videos and code at https://huangwl18.github.io/modular-rl/



### Novel Subtypes of Pulmonary Emphysema Based on Spatially-Informed Lung Texture Learning
- **Arxiv ID**: http://arxiv.org/abs/2007.04978v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.04978v1)
- **Published**: 2020-07-09 17:59:54+00:00
- **Updated**: 2020-07-09 17:59:54+00:00
- **Authors**: Jie Yang, Elsa D. Angelini, Pallavi P. Balte, Eric A. Hoffman, John H. M. Austin, Benjamin M. Smith, R. Graham Barr, Andrew F. Laine
- **Comment**: None
- **Journal**: None
- **Summary**: Pulmonary emphysema overlaps considerably with chronic obstructive pulmonary disease (COPD), and is traditionally subcategorized into three subtypes previously identified on autopsy. Unsupervised learning of emphysema subtypes on computed tomography (CT) opens the way to new definitions of emphysema subtypes and eliminates the need of thorough manual labeling. However, CT-based emphysema subtypes have been limited to texture-based patterns without considering spatial location. In this work, we introduce a standardized spatial mapping of the lung for quantitative study of lung texture location, and propose a novel framework for combining spatial and texture information to discover spatially-informed lung texture patterns (sLTPs) that represent novel emphysema subtypes. Exploiting two cohorts of full-lung CT scans from the MESA COPD and EMCAP studies, we first show that our spatial mapping enables population-wide study of emphysema spatial location. We then evaluate the characteristics of the sLTPs discovered on MESA COPD, and show that they are reproducible, able to encode standard emphysema subtypes, and associated with physiological symptoms.



### A Cordial Sync: Going Beyond Marginal Policies for Multi-Agent Embodied Tasks
- **Arxiv ID**: http://arxiv.org/abs/2007.04979v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.MA
- **Links**: [PDF](http://arxiv.org/pdf/2007.04979v1)
- **Published**: 2020-07-09 17:59:57+00:00
- **Updated**: 2020-07-09 17:59:57+00:00
- **Authors**: Unnat Jain, Luca Weihs, Eric Kolve, Ali Farhadi, Svetlana Lazebnik, Aniruddha Kembhavi, Alexander Schwing
- **Comment**: Accepted to ECCV 2020 (spotlight); Project page:
  https://unnat.github.io/cordial-sync
- **Journal**: None
- **Summary**: Autonomous agents must learn to collaborate. It is not scalable to develop a new centralized agent every time a task's difficulty outpaces a single agent's abilities. While multi-agent collaboration research has flourished in gridworld-like environments, relatively little work has considered visually rich domains. Addressing this, we introduce the novel task FurnMove in which agents work together to move a piece of furniture through a living room to a goal. Unlike existing tasks, FurnMove requires agents to coordinate at every timestep. We identify two challenges when training agents to complete FurnMove: existing decentralized action sampling procedures do not permit expressive joint action policies and, in tasks requiring close coordination, the number of failed actions dominates successful actions. To confront these challenges we introduce SYNC-policies (synchronize your actions coherently) and CORDIAL (coordination loss). Using SYNC-policies and CORDIAL, our agents achieve a 58% completion rate on FurnMove, an impressive absolute gain of 25 percentage points over competitive decentralized baselines. Our dataset, code, and pretrained models are available at https://unnat.github.io/cordial-sync .



### StyPath: Style-Transfer Data Augmentation For Robust Histology Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2007.05008v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.05008v1)
- **Published**: 2020-07-09 18:02:49+00:00
- **Updated**: 2020-07-09 18:02:49+00:00
- **Authors**: Pietro Antonio Cicalese, Aryan Mobiny, Pengyu Yuan, Jan Becker, Chandra Mohan, Hien Van Nguyen
- **Comment**: None
- **Journal**: None
- **Summary**: The classification of Antibody Mediated Rejection (AMR) in kidney transplant remains challenging even for experienced nephropathologists; this is partly because histological tissue stain analysis is often characterized by low inter-observer agreement and poor reproducibility. One of the implicated causes for inter-observer disagreement is the variability of tissue stain quality between (and within) pathology labs, coupled with the gradual fading of archival sections. Variations in stain colors and intensities can make tissue evaluation difficult for pathologists, ultimately affecting their ability to describe relevant morphological features. Being able to accurately predict the AMR status based on kidney histology images is crucial for improving patient treatment and care. We propose a novel pipeline to build robust deep neural networks for AMR classification based on StyPath, a histological data augmentation technique that leverages a light weight style-transfer algorithm as a means to reduce sample-specific bias. Each image was generated in 1.84 +- 0.03 seconds using a single GTX TITAN V gpu and pytorch, making it faster than other popular histological data augmentation techniques. We evaluated our model using a Monte Carlo (MC) estimate of Bayesian performance and generate an epistemic measure of uncertainty to compare both the baseline and StyPath augmented models. We also generated Grad-CAM representations of the results which were assessed by an experienced nephropathologist; we used this qualitative analysis to elucidate on the assumptions being made by each model. Our results imply that our style-transfer augmentation technique improves histological classification performance (reducing error from 14.8% to 11.5%) and generalization ability.



### Few Is Enough: Task-Augmented Active Meta-Learning for Brain Cell Classification
- **Arxiv ID**: http://arxiv.org/abs/2007.05009v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.05009v1)
- **Published**: 2020-07-09 18:03:12+00:00
- **Updated**: 2020-07-09 18:03:12+00:00
- **Authors**: Pengyu Yuan, Aryan Mobiny, Jahandar Jahanipour, Xiaoyang Li, Pietro Antonio Cicalese, Badrinath Roysam, Vishal Patel, Maric Dragan, Hien Van Nguyen
- **Comment**: None
- **Journal**: None
- **Summary**: Deep Neural Networks (or DNNs) must constantly cope with distribution changes in the input data when the task of interest or the data collection protocol changes. Retraining a network from scratch to combat this issue poses a significant cost. Meta-learning aims to deliver an adaptive model that is sensitive to these underlying distribution changes, but requires many tasks during the meta-training process. In this paper, we propose a tAsk-auGmented actIve meta-LEarning (AGILE) method to efficiently adapt DNNs to new tasks by using a small number of training examples. AGILE combines a meta-learning algorithm with a novel task augmentation technique which we use to generate an initial adaptive model. It then uses Bayesian dropout uncertainty estimates to actively select the most difficult samples when updating the model to a new task. This allows AGILE to learn with fewer tasks and a few informative samples, achieving high performance with a limited dataset. We perform our experiments using the brain cell classification task and compare the results to a plain meta-learning model trained from scratch. We show that the proposed task-augmented meta-learning framework can learn to classify new cell types after a single gradient step with a limited number of training samples. We show that active learning with Bayesian uncertainty can further improve the performance when the number of training samples is extremely small. Using only 1% of the training data and a single update step, we achieved 90% accuracy on the new cell type classification task, a 50% points improvement over a state-of-the-art meta-learning algorithm.



### Multi-view Orthonormalized Partial Least Squares: Regularizations and Deep Extensions
- **Arxiv ID**: http://arxiv.org/abs/2007.05028v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.05028v1)
- **Published**: 2020-07-09 19:00:39+00:00
- **Updated**: 2020-07-09 19:00:39+00:00
- **Authors**: Li Wang, Ren-Cang Li, Wen-Wei
- **Comment**: None
- **Journal**: None
- **Summary**: We establish a family of subspace-based learning method for multi-view learning using the least squares as the fundamental basis. Specifically, we investigate orthonormalized partial least squares (OPLS) and study its important properties for both multivariate regression and classification. Building on the least squares reformulation of OPLS, we propose a unified multi-view learning framework to learn a classifier over a common latent space shared by all views. The regularization technique is further leveraged to unleash the power of the proposed framework by providing three generic types of regularizers on its inherent ingredients including model parameters, decision values and latent projected points. We instantiate a set of regularizers in terms of various priors. The proposed framework with proper choices of regularizers not only can recast existing methods, but also inspire new models. To further improve the performance of the proposed framework on complex real problems, we propose to learn nonlinear transformations parameterized by deep networks. Extensive experiments are conducted to compare various methods on nine data sets with different numbers of views in terms of both feature extraction and cross-modal retrieval.



### Adversarially-learned Inference via an Ensemble of Discrete Undirected Graphical Models
- **Arxiv ID**: http://arxiv.org/abs/2007.05033v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.05033v3)
- **Published**: 2020-07-09 19:13:36+00:00
- **Updated**: 2020-10-22 05:05:01+00:00
- **Authors**: Adarsh K. Jeewajee, Leslie P. Kaelbling
- **Comment**: 17 pages, 5 figures, 5 tables. NeurIPS 2020
- **Journal**: None
- **Summary**: Undirected graphical models are compact representations of joint probability distributions over random variables. To solve inference tasks of interest, graphical models of arbitrary topology can be trained using empirical risk minimization. However, to solve inference tasks that were not seen during training, these models (EGMs) often need to be re-trained. Instead, we propose an inference-agnostic adversarial training framework which produces an infinitely-large ensemble of graphical models (AGMs). The ensemble is optimized to generate data within the GAN framework, and inference is performed using a finite subset of these models. AGMs perform comparably with EGMs on inference tasks that the latter were specifically optimized for. Most importantly, AGMs show significantly better generalization to unseen inference tasks compared to EGMs, as well as deep neural architectures like GibbsNet and VAEAC which allow arbitrary conditioning. Finally, AGMs allow fast data sampling, competitive with Gibbs sampling from EGMs.



### Multimodal price prediction
- **Arxiv ID**: http://arxiv.org/abs/2007.05056v4
- **DOI**: 10.1007/s40745-021-00326-z
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.05056v4)
- **Published**: 2020-07-09 20:46:13+00:00
- **Updated**: 2021-04-02 05:19:39+00:00
- **Authors**: Aidin Zehtab-Salmasi, Ali-Reza Feizi-Derakhshi, Narjes Nikzad-Khasmakhi, Meysam Asgari-Chenaghlu, Saeideh Nabipour
- **Comment**: This is a preprint of an article published in "Annals of Data
  Science". The final authenticated version is available online at:
  https://link.springer.com/article/10.1007/s40745-021-00326-z
- **Journal**: None
- **Summary**: Price prediction is one of the examples related to forecasting tasks and is a project based on data science. Price prediction analyzes data and predicts the cost of new products. The goal of this research is to achieve an arrangement to predict the price of a cellphone based on its specifications. So, five deep learning models are proposed to predict the price range of a cellphone, one unimodal and four multimodal approaches. The multimodal methods predict the prices based on the graphical and non-graphical features of cellphones that have an important effect on their valorizations. Also, to evaluate the efficiency of the proposed methods, a cellphone dataset has been gathered from GSMArena. The experimental results show 88.3% F1-score, which confirms that multimodal learning leads to more accurate predictions than state-of-the-art techniques.



### Learning Representations that Support Extrapolation
- **Arxiv ID**: http://arxiv.org/abs/2007.05059v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.05059v2)
- **Published**: 2020-07-09 20:53:45+00:00
- **Updated**: 2020-08-08 22:36:46+00:00
- **Authors**: Taylor W. Webb, Zachary Dulberg, Steven M. Frankland, Alexander A. Petrov, Randall C. O'Reilly, Jonathan D. Cohen
- **Comment**: ICML 2020
- **Journal**: None
- **Summary**: Extrapolation -- the ability to make inferences that go beyond the scope of one's experiences -- is a hallmark of human intelligence. By contrast, the generalization exhibited by contemporary neural network algorithms is largely limited to interpolation between data points in their training corpora. In this paper, we consider the challenge of learning representations that support extrapolation. We introduce a novel visual analogy benchmark that allows the graded evaluation of extrapolation as a function of distance from the convex domain defined by the training data. We also introduce a simple technique, temporal context normalization, that encourages representations that emphasize the relations between objects. We find that this technique enables a significant improvement in the ability to extrapolate, considerably outperforming a number of competitive techniques.



### Automatic Detection of Major Freeway Congestion Events Using Wireless Traffic Sensor Data: A Machine Learning Approach
- **Arxiv ID**: http://arxiv.org/abs/2007.05079v1
- **DOI**: 10.1177/0361198119843859
- **Categories**: **cs.CV**, cs.LG, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2007.05079v1)
- **Published**: 2020-07-09 21:38:45+00:00
- **Updated**: 2020-07-09 21:38:45+00:00
- **Authors**: Sanaz Aliari, Kaveh F. Sadabadi
- **Comment**: 12 pages, 3 figures
- **Journal**: Transportation Research Record 2673.7 (2019): 436-442
- **Summary**: Monitoring the dynamics of traffic in major corridors can provide invaluable insight for traffic planning purposes. An important requirement for this monitoring is the availability of methods to automatically detect major traffic events and to annotate the abundance of travel data. This paper introduces a machine learning based approach for reliable detection and characterization of highway traffic congestion events from hundreds of hours of traffic speed data. Indeed, the proposed approach is a generic approach for detection of changes in any given time series, which is the wireless traffic sensor data in the present study. The speed data is initially time-windowed by a ten-hour long sliding window and fed into three Neural Networks that are used to detect the existence and duration of congestion events (slowdowns) in each window. The sliding window captures each slowdown event multiple times and results in increased confidence in congestion detection. The training and parameter tuning are performed on 17,483 hours of data that includes 168 slowdown events. This data is collected and labeled as part of the ongoing probe data validation studies at the Center for Advanced Transportation Technologies (CATT) at the University of Maryland. The Neural networks are carefully trained to reduce the chances of over-fitting to the training data. The experimental results show that this approach is able to successfully detect most of the congestion events, while significantly outperforming a heuristic rule-based approach. Moreover, the proposed approach is shown to be more accurate in estimation of the start-time and end-time of the congestion events.



### A Benchmark for Inpainting of Clothing Images with Irregular Holes
- **Arxiv ID**: http://arxiv.org/abs/2007.05080v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.05080v3)
- **Published**: 2020-07-09 21:44:08+00:00
- **Updated**: 2020-08-27 17:43:38+00:00
- **Authors**: Furkan Kınlı, Barış Özcan, Furkan Kıraç
- **Comment**: Accepted to AIM2020: Advanced Image Manipulation workshop and
  challenges at ECCV2020
- **Journal**: None
- **Summary**: Fashion image understanding is an active research field with a large number of practical applications for the industry. Despite its practical impacts on intelligent fashion analysis systems, clothing image inpainting has not been extensively examined yet. For that matter, we present an extensive benchmark of clothing image inpainting on well-known fashion datasets. Furthermore, we introduce the use of a dilated version of partial convolutions, which efficiently derive the mask update step, and empirically show that the proposed method reduces the required number of layers to form fully-transparent masks. Experiments show that dilated partial convolutions (DPConv) improve the quantitative inpainting performance when compared to the other inpainting strategies, especially it performs better when the mask size is 20% or more of the image. \keywords{image inpainting, fashion image understanding, dilated convolutions, partial convolutions



### DCANet: Learning Connected Attentions for Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2007.05099v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.05099v1)
- **Published**: 2020-07-09 22:37:25+00:00
- **Updated**: 2020-07-09 22:37:25+00:00
- **Authors**: Xu Ma, Jingda Guo, Sihai Tang, Zhinan Qiao, Qi Chen, Qing Yang, Song Fu
- **Comment**: None
- **Journal**: None
- **Summary**: While self-attention mechanism has shown promising results for many vision tasks, it only considers the current features at a time. We show that such a manner cannot take full advantage of the attention mechanism. In this paper, we present Deep Connected Attention Network (DCANet), a novel design that boosts attention modules in a CNN model without any modification of the internal structure. To achieve this, we interconnect adjacent attention blocks, making information flow among attention blocks possible. With DCANet, all attention blocks in a CNN model are trained jointly, which improves the ability of attention learning. Our DCANet is generic. It is not limited to a specific attention module or base network architecture. Experimental results on ImageNet and MS COCO benchmarks show that DCANet consistently outperforms the state-of-the-art attention modules with a minimal additional computational overhead in all test cases. All code and models are made publicly available.



### LORCK: Learnable Object-Resembling Convolution Kernels
- **Arxiv ID**: http://arxiv.org/abs/2007.05103v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.05103v2)
- **Published**: 2020-07-09 23:17:40+00:00
- **Updated**: 2020-12-07 12:51:31+00:00
- **Authors**: Elizaveta Lazareva, Oleg Rogov, Olga Shegai, Denis Larionov, Dmitry V. Dylov
- **Comment**: 18 pages total. Main: 12 figures and 3 tables (main and
  supplemental). D.V.D is corresponding author
- **Journal**: None
- **Summary**: Segmentation of certain hollow organs, such as the bladder, is especially hard to automate due to their complex geometry, vague intensity gradients in the soft tissues, and a tedious manual process of the data annotation routine. Yet, accurate localization of the walls and the cancer regions in the radiologic images of such organs is an essential step in oncology. To address this issue, we propose a new class of hollow kernels that learn to 'mimic' the contours of the segmented organ, effectively replicating its shape and structural complexity. We train a series of the U-Net-like neural networks using the proposed kernels and demonstrate the superiority of the idea in various spatio-temporal convolution scenarios. Specifically, the dilated hollow-kernel architecture outperforms state-of-the-art spatial segmentation models, whereas the addition of temporal blocks with, e.g., Bi-LSTM, establishes a new multi-class baseline for the bladder segmentation challenge. Our spatio-temporal model based on the hollow kernels reaches the mean dice scores of 0.936, 0.736, and 0.712 for the bladder's inner wall, the outer wall, and the tumor regions, respectively. The results pave the way towards other domain-specific deep learning applications where the shape of the segmented object could be used to form a proper convolution kernel for boosting the segmentation outcome.



### $n$-Reference Transfer Learning for Saliency Prediction
- **Arxiv ID**: http://arxiv.org/abs/2007.05104v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.05104v1)
- **Published**: 2020-07-09 23:20:44+00:00
- **Updated**: 2020-07-09 23:20:44+00:00
- **Authors**: Yan Luo, Yongkang Wong, Mohan S. Kankanhalli, Qi Zhao
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: Benefiting from deep learning research and large-scale datasets, saliency prediction has achieved significant success in the past decade. However, it still remains challenging to predict saliency maps on images in new domains that lack sufficient data for data-hungry models. To solve this problem, we propose a few-shot transfer learning paradigm for saliency prediction, which enables efficient transfer of knowledge learned from the existing large-scale saliency datasets to a target domain with limited labeled examples. Specifically, very few target domain examples are used as the reference to train a model with a source domain dataset such that the training process can converge to a local minimum in favor of the target domain. Then, the learned model is further fine-tuned with the reference. The proposed framework is gradient-based and model-agnostic. We conduct comprehensive experiments and ablation study on various source domain and target domain pairs. The results show that the proposed framework achieves a significant performance improvement. The code is publicly available at \url{https://github.com/luoyan407/n-reference}.



