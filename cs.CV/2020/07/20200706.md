# Arxiv Papers in cs.CV on 2020-07-06
### Accelerated MRI with Un-trained Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2007.02471v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.02471v3)
- **Published**: 2020-07-06 00:01:25+00:00
- **Updated**: 2021-04-27 18:23:17+00:00
- **Authors**: Mohammad Zalbagi Darestani, Reinhard Heckel
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) are highly effective for image reconstruction problems. Typically, CNNs are trained on large amounts of training images. Recently, however, un-trained CNNs such as the Deep Image Prior and Deep Decoder have achieved excellent performance for image reconstruction problems such as denoising and inpainting, \emph{without using any training data}. Motivated by this development, we address the reconstruction problem arising in accelerated MRI with un-trained neural networks. We propose a highly optimized un-trained recovery approach based on a variation of the Deep Decoder and show that it significantly outperforms other un-trained methods, in particular sparsity-based classical compressed sensing methods and naive applications of un-trained neural networks. We also compare performance (both in terms of reconstruction accuracy and computational cost) in an ideal setup for trained methods, specifically on the fastMRI dataset, where the training and test data come from the same distribution. We find that our un-trained algorithm achieves similar performance to a baseline trained neural network, but a state-of-the-art trained network outperforms the un-trained one. Finally, we perform a comparison on a non-ideal setup where the train and test distributions are slightly different, and find that our un-trained method achieves similar performance to a state-of-the-art accelerated MRI reconstruction method.



### Automatic semantic segmentation for prediction of tuberculosis using lens-free microscopy images
- **Arxiv ID**: http://arxiv.org/abs/2007.02482v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.02482v1)
- **Published**: 2020-07-06 00:36:44+00:00
- **Updated**: 2020-07-06 00:36:44+00:00
- **Authors**: Dennis Núñez-Fernández, Lamberto Ballan, Gabriel Jiménez-Avalos, Jorge Coronel, Mirko Zimic
- **Comment**: ML for Global Health Workshop at ICML 2020
- **Journal**: None
- **Summary**: Tuberculosis (TB), caused by a germ called Mycobacterium tuberculosis, is one of the most serious public health problems in Peru and the world. The development of this project seeks to facilitate and automate the diagnosis of tuberculosis by the MODS method and using lens-free microscopy, due they are easier to calibrate and easier to use (by untrained personnel) in comparison with lens microscopy. Thus, we employ a U-Net network in our collected dataset to perform the automatic segmentation of the TB cords in order to predict tuberculosis. Our initial results show promising evidence for automatic segmentation of TB cords.



### EagleEye: Fast Sub-net Evaluation for Efficient Neural Network Pruning
- **Arxiv ID**: http://arxiv.org/abs/2007.02491v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.02491v2)
- **Published**: 2020-07-06 01:32:31+00:00
- **Updated**: 2020-08-05 09:32:58+00:00
- **Authors**: Bailin Li, Bowen Wu, Jiang Su, Guangrun Wang, Liang Lin
- **Comment**: Accepted in ECCV 2020(Oral). Codes are available on
  https://github.com/anonymous47823493/EagleEye
- **Journal**: None
- **Summary**: Finding out the computational redundant part of a trained Deep Neural Network (DNN) is the key question that pruning algorithms target on. Many algorithms try to predict model performance of the pruned sub-nets by introducing various evaluation methods. But they are either inaccurate or very complicated for general application. In this work, we present a pruning method called EagleEye, in which a simple yet efficient evaluation component based on adaptive batch normalization is applied to unveil a strong correlation between different pruned DNN structures and their final settled accuracy. This strong correlation allows us to fast spot the pruned candidates with highest potential accuracy without actually fine-tuning them. This module is also general to plug-in and improve some existing pruning algorithms. EagleEye achieves better pruning performance than all of the studied pruning algorithms in our experiments. Concretely, to prune MobileNet V1 and ResNet-50, EagleEye outperforms all compared methods by up to 3.8%. Even in the more challenging experiments of pruning the compact model of MobileNet V1, EagleEye achieves the highest accuracy of 70.9% with an overall 50% operations (FLOPs) pruned. All accuracy results are Top-1 ImageNet classification accuracy. Source code and models are accessible to open-source community https://github.com/anonymous47823493/EagleEye .



### Deep Learning for Anomaly Detection: A Review
- **Arxiv ID**: http://arxiv.org/abs/2007.02500v3
- **DOI**: 10.1145/3439950
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.02500v3)
- **Published**: 2020-07-06 02:21:16+00:00
- **Updated**: 2020-12-05 04:53:35+00:00
- **Authors**: Guansong Pang, Chunhua Shen, Longbing Cao, Anton van den Hengel
- **Comment**: Survey paper, 36 pages, 180 references, 2 figures, 4 tables
- **Journal**: ACM Computing Surveys, 2020
- **Summary**: Anomaly detection, a.k.a. outlier detection or novelty detection, has been a lasting yet active research area in various research communities for several decades. There are still some unique problem complexities and challenges that require advanced approaches. In recent years, deep learning enabled anomaly detection, i.e., deep anomaly detection, has emerged as a critical direction. This paper surveys the research of deep anomaly detection with a comprehensive taxonomy, covering advancements in three high-level categories and 11 fine-grained categories of the methods. We review their key intuitions, objective functions, underlying assumptions, advantages and disadvantages, and discuss how they address the aforementioned challenges. We further discuss a set of possible future opportunities and new perspectives on addressing the challenges.



### Learning Motion Flows for Semi-supervised Instrument Segmentation from Robotic Surgical Video
- **Arxiv ID**: http://arxiv.org/abs/2007.02501v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.02501v1)
- **Published**: 2020-07-06 02:39:32+00:00
- **Updated**: 2020-07-06 02:39:32+00:00
- **Authors**: Zixu Zhao, Yueming Jin, Xiaojie Gao, Qi Dou, Pheng-Ann Heng
- **Comment**: Accepted for MICCAI 2020
- **Journal**: None
- **Summary**: Performing low hertz labeling for surgical videos at intervals can greatly releases the burden of surgeons. In this paper, we study the semi-supervised instrument segmentation from robotic surgical videos with sparse annotations. Unlike most previous methods using unlabeled frames individually, we propose a dual motion based method to wisely learn motion flows for segmentation enhancement by leveraging temporal dynamics. We firstly design a flow predictor to derive the motion for jointly propagating the frame-label pairs given the current labeled frame. Considering the fast instrument motion, we further introduce a flow compensator to estimate intermediate motion within continuous frames, with a novel cycle learning strategy. By exploiting generated data pairs, our framework can recover and even enhance temporal consistency of training sequences to benefit segmentation. We validate our framework with binary, part, and type tasks on 2017 MICCAI EndoVis Robotic Instrument Segmentation Challenge dataset. Results show that our method outperforms the state-of-the-art semi-supervised methods by a large margin, and even exceeds fully supervised training on two tasks.



### Tree-Augmented Cross-Modal Encoding for Complex-Query Video Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2007.02503v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.02503v1)
- **Published**: 2020-07-06 02:50:27+00:00
- **Updated**: 2020-07-06 02:50:27+00:00
- **Authors**: Xun Yang, Jianfeng Dong, Yixin Cao, Xun Wang, Meng Wang, Tat-Seng Chua
- **Comment**: Accepted For 43rd International ACM SIGIR Conference on Research and
  Development in Information Retrieval (SIGIR 2020)
- **Journal**: None
- **Summary**: The rapid growth of user-generated videos on the Internet has intensified the need for text-based video retrieval systems. Traditional methods mainly favor the concept-based paradigm on retrieval with simple queries, which are usually ineffective for complex queries that carry far more complex semantics. Recently, embedding-based paradigm has emerged as a popular approach. It aims to map the queries and videos into a shared embedding space where semantically-similar texts and videos are much closer to each other. Despite its simplicity, it forgoes the exploitation of the syntactic structure of text queries, making it suboptimal to model the complex queries.   To facilitate video retrieval with complex queries, we propose a Tree-augmented Cross-modal Encoding method by jointly learning the linguistic structure of queries and the temporal representation of videos. Specifically, given a complex user query, we first recursively compose a latent semantic tree to structurally describe the text query. We then design a tree-augmented query encoder to derive structure-aware query representation and a temporal attentive video encoder to model the temporal characteristics of videos. Finally, both the query and videos are mapped into a joint embedding space for matching and ranking. In this approach, we have a better understanding and modeling of the complex queries, thereby achieving a better video retrieval performance. Extensive experiments on large scale video retrieval benchmark datasets demonstrate the effectiveness of our approach.



### TLIO: Tight Learned Inertial Odometry
- **Arxiv ID**: http://arxiv.org/abs/2007.01867v3
- **DOI**: 10.1109/LRA.2020.3007421
- **Categories**: **cs.RO**, cs.CV, cs.LG, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2007.01867v3)
- **Published**: 2020-07-06 03:13:34+00:00
- **Updated**: 2020-07-10 23:15:52+00:00
- **Authors**: Wenxin Liu, David Caruso, Eddy Ilg, Jing Dong, Anastasios I. Mourikis, Kostas Daniilidis, Vijay Kumar, Jakob Engel
- **Comment**: Correcting graph and bibliography. Adding journal reference
  information and DOI, in IEEE Robotics and Automation Letters
- **Journal**: None
- **Summary**: In this work we propose a tightly-coupled Extended Kalman Filter framework for IMU-only state estimation. Strap-down IMU measurements provide relative state estimates based on IMU kinematic motion model. However the integration of measurements is sensitive to sensor bias and noise, causing significant drift within seconds. Recent research by Yan et al. (RoNIN) and Chen et al. (IONet) showed the capability of using trained neural networks to obtain accurate 2D displacement estimates from segments of IMU data and obtained good position estimates from concatenating them. This paper demonstrates a network that regresses 3D displacement estimates and its uncertainty, giving us the ability to tightly fuse the relative state measurement into a stochastic cloning EKF to solve for pose, velocity and sensor biases. We show that our network, trained with pedestrian data from a headset, can produce statistically consistent measurement and uncertainty to be used as the update step in the filter, and the tightly-coupled system outperforms velocity integration approaches in position estimates, and AHRS attitude filter in orientation estimates.



### Traffic Agent Trajectory Prediction Using Social Convolution and Attention Mechanism
- **Arxiv ID**: http://arxiv.org/abs/2007.02515v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.02515v1)
- **Published**: 2020-07-06 03:48:08+00:00
- **Updated**: 2020-07-06 03:48:08+00:00
- **Authors**: Tao Yang, Zhixiong Nan, He Zhang, Shitao Chen, Nanning Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: The trajectory prediction is significant for the decision-making of autonomous driving vehicles. In this paper, we propose a model to predict the trajectories of target agents around an autonomous vehicle. The main idea of our method is considering the history trajectories of the target agent and the influence of surrounding agents on the target agent. To this end, we encode the target agent history trajectories as an attention mask and construct a social map to encode the interactive relationship between the target agent and its surrounding agents. Given a trajectory sequence, the LSTM networks are firstly utilized to extract the features for all agents, based on which the attention mask and social map are formed. Then, the attention mask and social map are fused to get the fusion feature map, which is processed by the social convolution to obtain a fusion feature representation. Finally, this fusion feature is taken as the input of a variable-length LSTM to predict the trajectory of the target agent. We note that the variable-length LSTM enables our model to handle the case that the number of agents in the sensing scope is highly dynamic in traffic scenes. To verify the effectiveness of our method, we widely compare with several methods on a public dataset, achieving a 20% error decrease. In addition, the model satisfies the real-time requirement with the 32 fps.



### EDSL: An Encoder-Decoder Architecture with Symbol-Level Features for Printed Mathematical Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/2007.02517v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.02517v1)
- **Published**: 2020-07-06 03:53:52+00:00
- **Updated**: 2020-07-06 03:53:52+00:00
- **Authors**: Yingnan Fu, Tingting Liu, Ming Gao, Aoying Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: Printed Mathematical expression recognition (PMER) aims to transcribe a printed mathematical expression image into a structural expression, such as LaTeX expression. It is a crucial task for many applications, including automatic question recommendation, automatic problem solving and analysis of the students, etc. Currently, the mainstream solutions rely on solving image captioning tasks, all addressing image summarization. As such, these methods can be suboptimal for solving MER problem.   In this paper, we propose a new method named EDSL, shorted for encoder-decoder with symbol-level features, to identify the printed mathematical expressions from images. The symbol-level image encoder of EDSL consists of segmentation module and reconstruction module. By performing segmentation module, we identify all the symbols and their spatial information from images in an unsupervised manner. We then design a novel reconstruction module to recover the symbol dependencies after symbol segmentation. Especially, we employ a position correction attention mechanism to capture the spatial relationships between symbols. To alleviate the negative impact from long output, we apply the transformer model for transcribing the encoded image into the sequential and structural output. We conduct extensive experiments on two real datasets to verify the effectiveness and rationality of our proposed EDSL method. The experimental results have illustrated that EDSL has achieved 92.7\% and 89.0\% in evaluation metric Match, which are 3.47\% and 4.04\% higher than the state-of-the-art method. Our code and datasets are available at https://github.com/abcAnonymous/EDSL .



### FLUID: A Unified Evaluation Framework for Flexible Sequential Data
- **Arxiv ID**: http://arxiv.org/abs/2007.02519v6
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.02519v6)
- **Published**: 2020-07-06 04:31:54+00:00
- **Updated**: 2023-04-10 23:13:56+00:00
- **Authors**: Matthew Wallingford, Aditya Kusupati, Keivan Alizadeh-Vahid, Aaron Walsman, Aniruddha Kembhavi, Ali Farhadi
- **Comment**: 27 pages, 6 figures. Project page:
  https://raivn.cs.washington.edu/projects/FLUID/
- **Journal**: Transactions on Machine Learning Research 2023
- **Summary**: Modern ML methods excel when training data is IID, large-scale, and well labeled. Learning in less ideal conditions remains an open challenge. The sub-fields of few-shot, continual, transfer, and representation learning have made substantial strides in learning under adverse conditions; each affording distinct advantages through methods and insights. These methods address different challenges such as data arriving sequentially or scarce training examples, however often the difficult conditions an ML system will face over its lifetime cannot be anticipated prior to deployment. Therefore, general ML systems which can handle the many challenges of learning in practical settings are needed. To foster research towards the goal of general ML methods, we introduce a new unified evaluation framework - FLUID (Flexible Sequential Data). FLUID integrates the objectives of few-shot, continual, transfer, and representation learning while enabling comparison and integration of techniques across these subfields. In FLUID, a learner faces a stream of data and must make sequential predictions while choosing how to update itself, adapt quickly to novel classes, and deal with changing data distributions; while accounting for the total amount of compute. We conduct experiments on a broad set of methods which shed new insight on the advantages and limitations of current solutions and indicate new research problems to solve. As a starting point towards more general methods, we present two new baselines which outperform other evaluated methods on FLUID. Project page: https://raivn.cs.washington.edu/projects/FLUID/.



### Geometric Attention for Prediction of Differential Properties in 3D Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2007.02571v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.02571v3)
- **Published**: 2020-07-06 07:40:26+00:00
- **Updated**: 2020-08-06 09:10:21+00:00
- **Authors**: Albert Matveev, Alexey Artemov, Denis Zorin, Evgeny Burnaev
- **Comment**: None
- **Journal**: None
- **Summary**: Estimation of differential geometric quantities in discrete 3D data representations is one of the crucial steps in the geometry processing pipeline. Specifically, estimating normals and sharp feature lines from raw point cloud helps improve meshing quality and allows us to use more precise surface reconstruction techniques. When designing a learnable approach to such problems, the main difficulty is selecting neighborhoods in a point cloud and incorporating geometric relations between the points. In this study, we present a geometric attention mechanism that can provide such properties in a learnable fashion. We establish the usefulness of the proposed technique with several experiments on the prediction of normal vectors and the extraction of feature lines.



### Probabilistic Multi-modal Trajectory Prediction with Lane Attention for Autonomous Vehicles
- **Arxiv ID**: http://arxiv.org/abs/2007.02574v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.02574v1)
- **Published**: 2020-07-06 07:57:23+00:00
- **Updated**: 2020-07-06 07:57:23+00:00
- **Authors**: Chenxu Luo, Lin Sun, Dariush Dabiri, Alan Yuille
- **Comment**: IROS 2020. 3rd place solution for the Argoverse motion forecasting
  challenge at NeurIPS 2019. A variation of the method also won the 1st place
  in the Nuscenes prediction challenge 2020
- **Journal**: None
- **Summary**: Trajectory prediction is crucial for autonomous vehicles. The planning system not only needs to know the current state of the surrounding objects but also their possible states in the future. As for vehicles, their trajectories are significantly influenced by the lane geometry and how to effectively use the lane information is of active interest. Most of the existing works use rasterized maps to explore road information, which does not distinguish different lanes. In this paper, we propose a novel instance-aware representation for lane representation. By integrating the lane features and trajectory features, a goal-oriented lane attention module is proposed to predict the future locations of the vehicle. We show that the proposed lane representation together with the lane attention module can be integrated into the widely used encoder-decoder framework to generate diverse predictions. Most importantly, each generated trajectory is associated with a probability to handle the uncertainty. Our method does not suffer from collapsing to one behavior modal and can cover diverse possibilities. Extensive experiments and ablation studies on the benchmark datasets corroborate the effectiveness of our proposed method. Notably, our proposed method ranks third place in the Argoverse motion forecasting competition at NeurIPS 2019.



### Progressive Cluster Purification for Unsupervised Feature Learning
- **Arxiv ID**: http://arxiv.org/abs/2007.02577v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.02577v2)
- **Published**: 2020-07-06 08:11:03+00:00
- **Updated**: 2020-07-15 17:11:45+00:00
- **Authors**: Yifei Zhang, Chang Liu, Yu Zhou, Wei Wang, Weiping Wang, Qixiang Ye
- **Comment**: 8 pages, 5 figures
- **Journal**: None
- **Summary**: In unsupervised feature learning, sample specificity based methods ignore the inter-class information, which deteriorates the discriminative capability of representation models. Clustering based methods are error-prone to explore the complete class boundary information due to the inevitable class inconsistent samples in each cluster. In this work, we propose a novel clustering based method, which, by iteratively excluding class inconsistent samples during progressive cluster formation, alleviates the impact of noise samples in a simple-yet-effective manner. Our approach, referred to as Progressive Cluster Purification (PCP), implements progressive clustering by gradually reducing the number of clusters during training, while the sizes of clusters continuously expand consistently with the growth of model representation capability. With a well-designed cluster purification mechanism, it further purifies clusters by filtering noise samples which facilitate the subsequent feature learning by utilizing the refined clusters as pseudo-labels. Experiments on commonly used benchmarks demonstrate that the proposed PCP improves baseline method with significant margins. Our code will be available at https://github.com/zhangyifei0115/PCP.



### Learning Graph-Convolutional Representations for Point Cloud Denoising
- **Arxiv ID**: http://arxiv.org/abs/2007.02578v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.02578v1)
- **Published**: 2020-07-06 08:11:28+00:00
- **Updated**: 2020-07-06 08:11:28+00:00
- **Authors**: Francesca Pistilli, Giulia Fracastoro, Diego Valsesia, Enrico Magli
- **Comment**: European Conference on Computer Vision (ECCV) 2020
- **Journal**: None
- **Summary**: Point clouds are an increasingly relevant data type but they are often corrupted by noise. We propose a deep neural network based on graph-convolutional layers that can elegantly deal with the permutation-invariance problem encountered by learning-based point cloud processing methods. The network is fully-convolutional and can build complex hierarchies of features by dynamically constructing neighborhood graphs from similarity among the high-dimensional feature representations of the points. When coupled with a loss promoting proximity to the ideal surface, the proposed approach significantly outperforms state-of-the-art methods on a variety of metrics. In particular, it is able to improve in terms of Chamfer measure and of quality of the surface normals that can be estimated from the denoised data. We also show that it is especially robust both at high noise levels and in presence of structured noise such as the one encountered in real LiDAR scans.



### On the Connection between Dynamical Optimal Transport and Functional Lifting
- **Arxiv ID**: http://arxiv.org/abs/2007.02587v1
- **DOI**: None
- **Categories**: **math.OC**, cs.CV, math.FA
- **Links**: [PDF](http://arxiv.org/pdf/2007.02587v1)
- **Published**: 2020-07-06 08:53:35+00:00
- **Updated**: 2020-07-06 08:53:35+00:00
- **Authors**: Thomas Vogt, Roland Haase, Danielle Bednarski, Jan Lellmann
- **Comment**: None
- **Journal**: None
- **Summary**: Functional lifting methods provide a tool for approximating solutions of difficult non-convex problems by embedding them into a larger space. In this work, we investigate a mathematically rigorous formulation based on embedding into the space of pointwise probability measures over a fixed range $\Gamma$. Interestingly, this approach can be derived as a generalization of the theory of dynamical optimal transport. Imposing the established continuity equation as a constraint corresponds to variational models with first-order regularization. By modifying the continuity equation, the approach can also be extended to models with higher-order regularization.



### Learning a Domain Classifier Bank for Unsupervised Adaptive Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2007.02595v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.02595v1)
- **Published**: 2020-07-06 09:12:46+00:00
- **Updated**: 2020-07-06 09:12:46+00:00
- **Authors**: Sanli Tang, Zhanzhan Cheng, Shiliang Pu, Dashan Guo, Yi Niu, Fei Wu
- **Comment**: None
- **Journal**: None
- **Summary**: In real applications, object detectors based on deep networks still face challenges of the large domain gap between the labeled training data and unlabeled testing data. To reduce the gap, recent techniques are proposed by aligning the image/instance-level features between source and unlabeled target domains. However, these methods suffer from the suboptimal problem mainly because of ignoring the category information of object instances. To tackle this issue, we develop a fine-grained domain alignment approach with a well-designed domain classifier bank that achieves the instance-level alignment respecting to their categories. Specifically, we first employ the mean teacher paradigm to generate pseudo labels for unlabeled samples. Then we implement the class-level domain classifiers and group them together, called domain classifier bank, in which each domain classifier is responsible for aligning features of a specific class. We assemble the bare object detector with the proposed fine-grained domain alignment mechanism as the adaptive detector, and optimize it with a developed crossed adaptive weighting mechanism. Extensive experiments on three popular transferring benchmarks demonstrate the effectiveness of our method and achieve the new remarkable state-of-the-arts.



### A Convolutional Approach to Vertebrae Detection and Labelling in Whole Spine MRI
- **Arxiv ID**: http://arxiv.org/abs/2007.02606v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.02606v3)
- **Published**: 2020-07-06 09:37:12+00:00
- **Updated**: 2020-07-13 13:10:27+00:00
- **Authors**: Rhydian Windsor, Amir Jamaludin, Timor Kadir, Andrew Zisserman
- **Comment**: Accepted full paper to Medical Image Computing and Computer Assisted
  Intervention 2020. 11 pages plus appendix
- **Journal**: None
- **Summary**: We propose a novel convolutional method for the detection and identification of vertebrae in whole spine MRIs. This involves using a learnt vector field to group detected vertebrae corners together into individual vertebral bodies and convolutional image-to-image translation followed by beam search to label vertebral levels in a self-consistent manner. The method can be applied without modification to lumbar, cervical and thoracic-only scans across a range of different MR sequences. The resulting system achieves 98.1% detection rate and 96.5% identification rate on a challenging clinical dataset of whole spine scans and matches or exceeds the performance of previous systems on lumbar-only scans. Finally, we demonstrate the clinical applicability of this method, using it for automated scoliosis detection in both lumbar and whole spine MR scans.



### Understanding and Improving Fast Adversarial Training
- **Arxiv ID**: http://arxiv.org/abs/2007.02617v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.02617v2)
- **Published**: 2020-07-06 10:16:43+00:00
- **Updated**: 2020-10-24 11:20:16+00:00
- **Authors**: Maksym Andriushchenko, Nicolas Flammarion
- **Comment**: The camera-ready version (accepted at NeurIPS 2020)
- **Journal**: None
- **Summary**: A recent line of work focused on making adversarial training computationally efficient for deep learning models. In particular, Wong et al. (2020) showed that $\ell_\infty$-adversarial training with fast gradient sign method (FGSM) can fail due to a phenomenon called "catastrophic overfitting", when the model quickly loses its robustness over a single epoch of training. We show that adding a random step to FGSM, as proposed in Wong et al. (2020), does not prevent catastrophic overfitting, and that randomness is not important per se -- its main role being simply to reduce the magnitude of the perturbation. Moreover, we show that catastrophic overfitting is not inherent to deep and overparametrized networks, but can occur in a single-layer convolutional network with a few filters. In an extreme case, even a single filter can make the network highly non-linear locally, which is the main reason why FGSM training fails. Based on this observation, we propose a new regularization method, GradAlign, that prevents catastrophic overfitting by explicitly maximizing the gradient alignment inside the perturbation set and improves the quality of the FGSM solution. As a result, GradAlign allows to successfully apply FGSM training also for larger $\ell_\infty$-perturbations and reduce the gap to multi-step adversarial training. The code of our experiments is available at https://github.com/tml-epfl/understanding-fast-adv-training.



### Integrating Distributed Architectures in Highly Modular RL Libraries
- **Arxiv ID**: http://arxiv.org/abs/2007.02622v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.02622v3)
- **Published**: 2020-07-06 10:22:07+00:00
- **Updated**: 2023-06-12 08:40:02+00:00
- **Authors**: Albert Bou, Sebastian Dittert, Gianni De Fabritiis
- **Comment**: 9 pages, 9 figures
- **Journal**: None
- **Summary**: Advancing reinforcement learning (RL) requires tools that are flexible enough to easily prototype new methods while avoiding impractically slow experimental turnaround times. To match the first requirement, the most popular RL libraries advocate for highly modular agent composability, which facilitates experimentation and development. To solve challenging environments within reasonable time frames, scaling RL to large sampling and computing resources has proved a successful strategy. However, this capability has been so far difficult to combine with modularity. In this work, we explore design choices to allow agent composability both at a local and distributed level of execution. We propose a versatile approach that allows the definition of RL agents at different scales through independent reusable components. We demonstrate experimentally that our design choices allow us to reproduce classical benchmarks, explore multiple distributed architectures, and solve novel and complex environments while giving full control to the user in the agent definition and training scheme definition. We believe this work can provide useful insights to the next generation of RL libraries.



### Joint Learning of Social Groups, Individuals Action and Sub-group Activities in Videos
- **Arxiv ID**: http://arxiv.org/abs/2007.02632v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.02632v2)
- **Published**: 2020-07-06 10:42:11+00:00
- **Updated**: 2020-07-28 00:57:21+00:00
- **Authors**: Mahsa Ehsanpour, Alireza Abedin, Fatemeh Saleh, Javen Shi, Ian Reid, Hamid Rezatofighi
- **Comment**: Accepted in the European Conference On Computer Vision (ECCV) 2020
- **Journal**: None
- **Summary**: The state-of-the art solutions for human activity understanding from a video stream formulate the task as a spatio-temporal problem which requires joint localization of all individuals in the scene and classification of their actions or group activity over time. Who is interacting with whom, e.g. not everyone in a queue is interacting with each other, is often not predicted. There are scenarios where people are best to be split into sub-groups, which we call social groups, and each social group may be engaged in a different social activity. In this paper, we solve the problem of simultaneously grouping people by their social interactions, predicting their individual actions and the social activity of each social group, which we call the social task. Our main contributions are: i) we propose an end-to-end trainable framework for the social task; ii) our proposed method also sets the state-of-the-art results on two widely adopted benchmarks for the traditional group activity recognition task (assuming individuals of the scene form a single group and predicting a single group activity label for the scene); iii) we introduce new annotations on an existing group activity dataset, re-purposing it for the social task.



### Dynamic memory to alleviate catastrophic forgetting in continuous learning settings
- **Arxiv ID**: http://arxiv.org/abs/2007.02639v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.02639v2)
- **Published**: 2020-07-06 11:02:38+00:00
- **Updated**: 2020-07-07 09:05:02+00:00
- **Authors**: Johannes Hofmanninger, Matthias Perkonigg, James A. Brink, Oleg Pianykh, Christian Herold, Georg Langs
- **Comment**: The first two authors contributed equally. Accepted at MICCAI 2020
- **Journal**: None
- **Summary**: In medical imaging, technical progress or changes in diagnostic procedures lead to a continuous change in image appearance. Scanner manufacturer, reconstruction kernel, dose, other protocol specific settings or administering of contrast agents are examples that influence image content independent of the scanned biology. Such domain and task shifts limit the applicability of machine learning algorithms in the clinical routine by rendering models obsolete over time. Here, we address the problem of data shifts in a continuous learning scenario by adapting a model to unseen variations in the source domain while counteracting catastrophic forgetting effects. Our method uses a dynamic memory to facilitate rehearsal of a diverse training data subset to mitigate forgetting. We evaluated our approach on routine clinical CT data obtained with two different scanner protocols and synthetic classification tasks. Experiments show that dynamic memory counters catastrophic forgetting in a setting with multiple data shifts without the necessity for explicit knowledge about when these shifts occur.



### Toward unsupervised, multi-object discovery in large-scale image collections
- **Arxiv ID**: http://arxiv.org/abs/2007.02662v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.02662v2)
- **Published**: 2020-07-06 11:43:47+00:00
- **Updated**: 2020-08-25 11:11:31+00:00
- **Authors**: Huy V. Vo, Patrick Pérez, Jean Ponce
- **Comment**: Accepted for publication in European Conference on Computer Vision
  (ECCV) 2020
- **Journal**: None
- **Summary**: This paper addresses the problem of discovering the objects present in a collection of images without any supervision. We build on the optimization approach of Vo et al. (CVPR'19) with several key novelties: (1) We propose a novel saliency-based region proposal algorithm that achieves significantly higher overlap with ground-truth objects than other competitive methods. This procedure leverages off-the-shelf CNN features trained on classification tasks without any bounding box information, but is otherwise unsupervised. (2) We exploit the inherent hierarchical structure of proposals as an effective regularizer for the approach to object discovery of Vo et al., boosting its performance to significantly improve over the state of the art on several standard benchmarks. (3) We adopt a two-stage strategy to select promising proposals using small random sets of images before using the whole image collection to discover the objects it depicts, allowing us to tackle, for the first time (to the best of our knowledge), the discovery of multiple objects in each one of the pictures making up datasets with up to 20,000 images, an over five-fold increase compared to existing methods, and a first step toward true large-scale unsupervised image interpretation.



### An Elastic Interaction-Based Loss Function for Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2007.02663v2
- **DOI**: 10.1007/978-3-030-59722-1_73
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.02663v2)
- **Published**: 2020-07-06 11:49:14+00:00
- **Updated**: 2020-10-11 13:09:02+00:00
- **Authors**: Yuan Lan, Yang Xiang, Luchan Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning techniques have shown their success in medical image segmentation since they are easy to manipulate and robust to various types of datasets. The commonly used loss functions in the deep segmentation task are pixel-wise loss functions. This results in a bottleneck for these models to achieve high precision for complicated structures in biomedical images. For example, the predicted small blood vessels in retinal images are often disconnected or even missed under the supervision of the pixel-wise losses. This paper addresses this problem by introducing a long-range elastic interaction-based training strategy. In this strategy, convolutional neural network (CNN) learns the target region under the guidance of the elastic interaction energy between the boundary of the predicted region and that of the actual object. Under the supervision of the proposed loss, the boundary of the predicted region is attracted strongly by the object boundary and tends to stay connected. Experimental results show that our method is able to achieve considerable improvements compared to commonly used pixel-wise loss functions (cross entropy and dice Loss) and other recent loss functions on three retinal vessel segmentation datasets, DRIVE, STARE and CHASEDB1.



### On the Influence of Ageing on Face Morph Attacks: Vulnerability and Detection
- **Arxiv ID**: http://arxiv.org/abs/2007.02684v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.02684v2)
- **Published**: 2020-07-06 12:32:41+00:00
- **Updated**: 2020-09-19 16:56:29+00:00
- **Authors**: Sushma Venkatesh, Kiran Raja, Raghavendra Ramachandra, Christoph Busch
- **Comment**: Accepted in IJCB 2020
- **Journal**: None
- **Summary**: Face morphing attacks have raised critical concerns as they demonstrate a new vulnerability of Face Recognition Systems (FRS), which are widely deployed in border control applications. The face morphing process uses the images from multiple data subjects and performs an image blending operation to generate a morphed image of high quality. The generated morphed image exhibits similar visual characteristics corresponding to the biometric characteristics of the data subjects that contributed to the composite image and thus making it difficult for both humans and FRS, to detect such attacks. In this paper, we report a systematic investigation on the vulnerability of the Commercial-Off-The-Shelf (COTS) FRS when morphed images under the influence of ageing are presented. To this extent, we have introduced a new morphed face dataset with ageing derived from the publicly available MORPH II face dataset, which we refer to as MorphAge dataset. The dataset has two bins based on age intervals, the first bin - MorphAge-I dataset has 1002 unique data subjects with the age variation of 1 year to 2 years while the MorphAge-II dataset consists of 516 data subjects whose age intervals are from 2 years to 5 years. To effectively evaluate the vulnerability for morphing attacks, we also introduce a new evaluation metric, namely the Fully Mated Morphed Presentation Match Rate (FMMPMR), to quantify the vulnerability effectively in a realistic scenario. Extensive experiments are carried out by using two different COTS FRS (COTS I - Cognitec and COTS II - Neurotechnology) to quantify the vulnerability with ageing. Further, we also evaluate five different Morph Attack Detection (MAD) techniques to benchmark their detection performance with ageing.



### Bifurcated backbone strategy for RGB-D salient object detection
- **Arxiv ID**: http://arxiv.org/abs/2007.02713v3
- **DOI**: 10.1109/TIP.2021.3116793
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.02713v3)
- **Published**: 2020-07-06 13:01:30+00:00
- **Updated**: 2021-08-18 01:13:38+00:00
- **Authors**: Yingjie Zhai, Deng-Ping Fan, Jufeng Yang, Ali Borji, Ling Shao, Junwei Han, Liang Wang
- **Comment**: A preliminary version of this work has been accepted in ECCV 2020
- **Journal**: None
- **Summary**: Multi-level feature fusion is a fundamental topic in computer vision. It has been exploited to detect, segment and classify objects at various scales. When multi-level features meet multi-modal cues, the optimal feature aggregation and multi-modal learning strategy become a hot potato. In this paper, we leverage the inherent multi-modal and multi-level nature of RGB-D salient object detection to devise a novel cascaded refinement network. In particular, first, we propose to regroup the multi-level features into teacher and student features using a bifurcated backbone strategy (BBS). Second, we introduce a depth-enhanced module (DEM) to excavate informative depth cues from the channel and spatial views. Then, RGB and depth modalities are fused in a complementary way. Our architecture, named Bifurcated Backbone Strategy Network (BBS-Net), is simple, efficient, and backbone-independent. Extensive experiments show that BBS-Net significantly outperforms eighteen SOTA models on eight challenging datasets under five evaluation measures, demonstrating the superiority of our approach ($\sim 4 \%$ improvement in S-measure $vs.$ the top-ranked model: DMRA-iccv2019). In addition, we provide a comprehensive analysis on the generalization ability of different RGB-D datasets and provide a powerful training set for future research.



### Image Stitching Based on Planar Region Consensus
- **Arxiv ID**: http://arxiv.org/abs/2007.02722v1
- **DOI**: 10.1109/TIP.2021.3086079
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.02722v1)
- **Published**: 2020-07-06 13:07:20+00:00
- **Updated**: 2020-07-06 13:07:20+00:00
- **Authors**: Aocheng Li, Jie Guo, Yanwen Guo
- **Comment**: 15 pages, 9 figures
- **Journal**: None
- **Summary**: Image stitching for two images without a global transformation between them is notoriously difficult. In this paper, noticing the importance of planar structure under perspective geometry, we propose a new image stitching method which stitches images by allowing for the alignment of a set of matched dominant planar regions. Clearly different from previous methods resorting to plane segmentation, the key to our approach is to utilize rich semantic information directly from RGB images to extract planar image regions with a deep Convolutional Neural Network (CNN). We specifically design a new module to make fully use of existing semantic segmentation networks to accommodate planar segmentation. To train the network, a dataset for planar region segmentation is contributed. With the planar region knowledge, a set of local transformations can be obtained by constraining matched regions, enabling more precise alignment in the overlapping area. We also use planar knowledge to estimate a transformation field over the whole image. The final mosaic is obtained by a mesh-based optimization framework which maintains high alignment accuracy and relaxes similarity transformation at the same time. Extensive experiments with quantitative comparisons show that our method can deal with different situations and outperforms the state-of-the-arts on challenging scenes.



### Black-box Adversarial Example Generation with Normalizing Flows
- **Arxiv ID**: http://arxiv.org/abs/2007.02734v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.02734v1)
- **Published**: 2020-07-06 13:14:21+00:00
- **Updated**: 2020-07-06 13:14:21+00:00
- **Authors**: Hadi M. Dolatabadi, Sarah Erfani, Christopher Leckie
- **Comment**: Accepted to the 2nd workshop on Invertible Neural Networks,
  Normalizing Flows, and Explicit Likelihood Models (ICML 2020), Virtual
  Conference
- **Journal**: None
- **Summary**: Deep neural network classifiers suffer from adversarial vulnerability: well-crafted, unnoticeable changes to the input data can affect the classifier decision. In this regard, the study of powerful adversarial attacks can help shed light on sources of this malicious behavior. In this paper, we propose a novel black-box adversarial attack using normalizing flows. We show how an adversary can be found by searching over a pre-trained flow-based model base distribution. This way, we can generate adversaries that resemble the original data closely as the perturbations are in the shape of the data. We then demonstrate the competitive performance of the proposed approach against well-known black-box adversarial attack methods.



### Multi-Objective Neural Architecture Search Based on Diverse Structures and Adaptive Recommendation
- **Arxiv ID**: http://arxiv.org/abs/2007.02749v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.02749v2)
- **Published**: 2020-07-06 13:42:33+00:00
- **Updated**: 2020-08-13 15:14:07+00:00
- **Authors**: Chunnan Wang, Hongzhi Wang, Guosheng Feng, Fei Geng
- **Comment**: 11pages
- **Journal**: None
- **Summary**: The search space of neural architecture search (NAS) for convolutional neural network (CNN) is huge. To reduce searching cost, most NAS algorithms use fixed outer network level structure, and search the repeatable cell structure only. Such kind of fixed architecture performs well when enough cells and channels are used. However, when the architecture becomes more lightweight, the performance decreases significantly. To obtain better lightweight architectures, more flexible and diversified neural architectures are in demand, and more efficient methods should be designed for larger search space. Motivated by this, we propose MoARR algorithm, which utilizes the existing research results and historical information to quickly find architectures that are both lightweight and accurate. We use the discovered high-performance cells to construct network architectures. This method increases the network architecture diversity while also reduces the search space of cell structure design. In addition, we designs a novel multi-objective method to effectively analyze the historical evaluation information, so as to efficiently search for the Pareto optimal architectures with high accuracy and small parameter number. Experimental results show that our MoARR can achieve a powerful and lightweight model (with 1.9% error rate and 2.3M parameters) on CIFAR-10 in 6 GPU hours, which is better than the state-of-the-arts. The explored architecture is transferable to ImageNet and achieves 76.0% top-1 accuracy with 4.9M parameters.



### Adversarial Uni- and Multi-modal Stream Networks for Multimodal Image Registration
- **Arxiv ID**: http://arxiv.org/abs/2007.02790v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.02790v2)
- **Published**: 2020-07-06 14:44:06+00:00
- **Updated**: 2020-09-21 06:12:10+00:00
- **Authors**: Zhe Xu, Jie Luo, Jiangpeng Yan, Ritvik Pulya, Xiu Li, William Wells III, Jayender Jagadeesan
- **Comment**: accepted by MICCAI 2020
- **Journal**: None
- **Summary**: Deformable image registration between Computed Tomography (CT) images and Magnetic Resonance (MR) imaging is essential for many image-guided therapies. In this paper, we propose a novel translation-based unsupervised deformable image registration method. Distinct from other translation-based methods that attempt to convert the multimodal problem (e.g., CT-to-MR) into a unimodal problem (e.g., MR-to-MR) via image-to-image translation, our method leverages the deformation fields estimated from both: (i) the translated MR image and (ii) the original CT image in a dual-stream fashion, and automatically learns how to fuse them to achieve better registration performance. The multimodal registration network can be effectively trained by computationally efficient similarity metrics without any ground-truth deformation. Our method has been evaluated on two clinical datasets and demonstrates promising results compared to state-of-the-art traditional and learning-based methods.



### Gradient Origin Networks
- **Arxiv ID**: http://arxiv.org/abs/2007.02798v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, 68T01 (Primary), 68T07 (Secondary), I.5.0; I.4.0; G.3
- **Links**: [PDF](http://arxiv.org/pdf/2007.02798v5)
- **Published**: 2020-07-06 15:00:11+00:00
- **Updated**: 2021-03-24 14:15:29+00:00
- **Authors**: Sam Bond-Taylor, Chris G. Willcocks
- **Comment**: 16 pages, 17 figures, accepted at ICLR 2021, camera-ready version
- **Journal**: None
- **Summary**: This paper proposes a new type of generative model that is able to quickly learn a latent representation without an encoder. This is achieved using empirical Bayes to calculate the expectation of the posterior, which is implemented by initialising a latent vector with zeros, then using the gradient of the log-likelihood of the data with respect to this zero vector as new latent points. The approach has similar characteristics to autoencoders, but with a simpler architecture, and is demonstrated in a variational autoencoder equivalent that permits sampling. This also allows implicit representation networks to learn a space of implicit functions without requiring a hypernetwork, retaining their representation advantages across datasets. The experiments show that the proposed method converges faster, with significantly lower reconstruction error than autoencoders, while requiring half the parameters.



### Novel-View Human Action Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2007.02808v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.02808v3)
- **Published**: 2020-07-06 15:11:51+00:00
- **Updated**: 2020-10-08 10:02:36+00:00
- **Authors**: Mohamed Ilyes Lakhal, Davide Boscaini, Fabio Poiesi, Oswald Lanz, Andrea Cavallaro
- **Comment**: Asian Conference on Computer Vision (ACCV) 2020
- **Journal**: None
- **Summary**: Novel-View Human Action Synthesis aims to synthesize the movement of a body from a virtual viewpoint, given a video from a real viewpoint. We present a novel 3D reasoning to synthesize the target viewpoint. We first estimate the 3D mesh of the target body and transfer the rough textures from the 2D images to the mesh. As this transfer may generate sparse textures on the mesh due to frame resolution or occlusions. We produce a semi-dense textured mesh by propagating the transferred textures both locally, within local geodesic neighborhoods, and globally, across symmetric semantic parts. Next, we introduce a context-based generator to learn how to correct and complete the residual appearance information. This allows the network to independently focus on learning the foreground and background synthesis tasks. We validate the proposed solution on the public NTU RGB+D dataset. The code and resources are available at https://bit.ly/36u3h4K.



### Complex Human Action Recognition in Live Videos Using Hybrid FR-DL Method
- **Arxiv ID**: http://arxiv.org/abs/2007.02811v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.02811v1)
- **Published**: 2020-07-06 15:12:50+00:00
- **Updated**: 2020-07-06 15:12:50+00:00
- **Authors**: Fatemeh Serpush, Mahdi Rezaei
- **Comment**: None
- **Journal**: None
- **Summary**: Automated human action recognition is one of the most attractive and practical research fields in computer vision, in spite of its high computational costs. In such systems, the human action labelling is based on the appearance and patterns of the motions in the video sequences; however, the conventional methodologies and classic neural networks cannot use temporal information for action recognition prediction in the upcoming frames in a video sequence. On the other hand, the computational cost of the preprocessing stage is high. In this paper, we address challenges of the preprocessing phase, by an automated selection of representative frames among the input sequences. Furthermore, we extract the key features of the representative frame rather than the entire features. We propose a hybrid technique using background subtraction and HOG, followed by application of a deep neural network and skeletal modelling method. The combination of a CNN and the LSTM recursive network is considered for feature selection and maintaining the previous information, and finally, a Softmax-KNN classifier is used for labelling human activities. We name our model as Feature Reduction & Deep Learning based action recognition method, or FR-DL in short. To evaluate the proposed method, we use the UCF dataset for the benchmarking which is widely-used among researchers in action recognition research. The dataset includes 101 complicated activities in the wild. Experimental results show a significant improvement in terms of accuracy and speed in comparison with six state-of-the-art articles.



### Distance-Geometric Graph Convolutional Network (DG-GCN) for Three-Dimensional (3D) Graphs
- **Arxiv ID**: http://arxiv.org/abs/2007.03513v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.03513v4)
- **Published**: 2020-07-06 15:20:52+00:00
- **Updated**: 2021-03-22 17:53:03+00:00
- **Authors**: Daniel T. Chang
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2006.01785
- **Journal**: None
- **Summary**: The distance-geometric graph representation adopts a unified scheme (distance) for representing the geometry of three-dimensional(3D) graphs. It is invariant to rotation and translation of the graph and it reflects pair-wise node interactions and their generally local nature. To facilitate the incorporation of geometry in deep learning on 3D graphs, we propose a message-passing graph convolutional network based on the distance-geometric graph representation: DG-GCN (distance-geometric graph convolution network). It utilizes continuous-filter convolutional layers, with filter-generating networks, that enable learning of filter weights from distances, thereby incorporating the geometry of 3D graphs in graph convolutions. Our results for the ESOL and FreeSolv datasets show major improvement over those of standard graph convolutions. They also show significant improvement over those of geometric graph convolutions employing edge weight / edge distance power laws. Our work demonstrates the utility and value of DG-GCN for end-to-end deep learning on 3D graphs, particularly molecular graphs.



### Exploration of Optimized Semantic Segmentation Architectures for edge-Deployment on Drones
- **Arxiv ID**: http://arxiv.org/abs/2007.02839v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.02839v1)
- **Published**: 2020-07-06 15:49:18+00:00
- **Updated**: 2020-07-06 15:49:18+00:00
- **Authors**: Vivek Parmar, Narayani Bhatia, Shubham Negi, Manan Suri
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present an analysis on the impact of network parameters for semantic segmentation architectures in context of UAV data processing. We present the analysis on the DroneDeploy Segmentation benchmark. Based on the comparative analysis we identify the optimal network architecture to be FPN-EfficientNetB3 with pretrained encoder backbones based on Imagenet Dataset. The network achieves IoU score of 0.65 and F1-score of 0.71 over the validation dataset. We also compare the various architectures in terms of their memory footprint and inference latency with further exploration of the impact of TensorRT based optimizations. We achieve memory savings of ~4.1x and latency improvement of 10% compared to Model: FPN and Backbone: InceptionResnetV2.



### Point-Set Anchors for Object Detection, Instance Segmentation and Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2007.02846v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.02846v4)
- **Published**: 2020-07-06 15:59:56+00:00
- **Updated**: 2020-08-03 06:14:19+00:00
- **Authors**: Fangyun Wei, Xiao Sun, Hongyang Li, Jingdong Wang, Stephen Lin
- **Comment**: To appear in ECCV 2020
- **Journal**: None
- **Summary**: A recent approach for object detection and human pose estimation is to regress bounding boxes or human keypoints from a central point on the object or person. While this center-point regression is simple and efficient, we argue that the image features extracted at a central point contain limited information for predicting distant keypoints or bounding box boundaries, due to object deformation and scale/orientation variation. To facilitate inference, we propose to instead perform regression from a set of points placed at more advantageous positions. This point set is arranged to reflect a good initialization for the given task, such as modes in the training data for pose estimation, which lie closer to the ground truth than the central point and provide more informative features for regression. As the utility of a point set depends on how well its scale, aspect ratio and rotation matches the target, we adopt the anchor box technique of sampling these transformations to generate additional point-set candidates. We apply this proposed framework, called Point-Set Anchors, to object detection, instance segmentation, and human pose estimation. Our results show that this general-purpose approach can achieve performance competitive with state-of-the-art methods for each of these tasks. Code is available at \url{https://github.com/FangyunWei/PointSetAnchor}



### Augment Yourself: Mixed Reality Self-Augmentation Using Optical See-through Head-mounted Displays and Physical Mirrors
- **Arxiv ID**: http://arxiv.org/abs/2007.02884v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.02884v1)
- **Published**: 2020-07-06 16:53:47+00:00
- **Updated**: 2020-07-06 16:53:47+00:00
- **Authors**: Mathias Unberath, Kevin Yu, Roghayeh Barmaki, Alex Johnson, Nassir Navab
- **Comment**: This manuscript was initially submitted to IEEE VR TVCG 2018 on
  November 22, 2017
- **Journal**: None
- **Summary**: Optical see-though head-mounted displays (OST HMDs) are one of the key technologies for merging virtual objects and physical scenes to provide an immersive mixed reality (MR) environment to its user. A fundamental limitation of HMDs is, that the user itself cannot be augmented conveniently as, in casual posture, only the distal upper extremities are within the field of view of the HMD. Consequently, most MR applications that are centered around the user, such as virtual dressing rooms or learning of body movements, cannot be realized with HMDs. In this paper, we propose a novel concept and prototype system that combines OST HMDs and physical mirrors to enable self-augmentation and provide an immersive MR environment centered around the user. Our system, to the best of our knowledge the first of its kind, estimates the user's pose in the virtual image generated by the mirror using an RGBD camera attached to the HMD and anchors virtual objects to the reflection rather than the user directly. We evaluate our system quantitatively with respect to calibration accuracy and infrared signal degradation effects due to the mirror, and show its potential in applications where large mirrors are already an integral part of the facility. Particularly, we demonstrate its use for virtual fitting rooms, gaming applications, anatomy learning, and personal fitness. In contrast to competing devices such as LCD-equipped smart mirrors, the proposed system consists of only an HMD with RGBD camera and, thus, does not require a prepared environment making it very flexible and generic. In future work, we will aim to investigate how the system can be optimally used for physical rehabilitation and personal training as a promising application.



### Coronary Heart Disease Diagnosis Based on Improved Ensemble Learning
- **Arxiv ID**: http://arxiv.org/abs/2007.02895v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.02895v1)
- **Published**: 2020-07-06 17:14:30+00:00
- **Updated**: 2020-07-06 17:14:30+00:00
- **Authors**: Kuntoro Adi Nugroho, Noor Akhmad Setiawan, Teguh Bharata Adji
- **Comment**: None
- **Journal**: J. Converg. Inf. Technol, 8(13), p.13. 2013
- **Summary**: Accurate diagnosis is required before performing proper treatments for coronary heart disease. Machine learning based approaches have been proposed by many researchers to improve the accuracy of coronary heart disease diagnosis. Ensemble learning and cascade generalization are among the methods which can be used to improve the generalization ability of learning algorithm. The objective of this study is to develop heart disease diagnosis method based on ensemble learning and cascade generalization. Cascade generalization method with loose coupling strategy is proposed in this study. C4. 5 and RIPPER algorithm were used as meta-level algorithm and Naive Bayes was used as baselevel algorithm. Bagging and Random Subspace were evaluated for constructing the ensemble. The hybrid cascade ensemble methods are compared with the learning algorithms in non-ensemble mode and non-cascade mode. The methods are also compared with Rotation Forest. Based on the evaluation result, the hybrid cascade ensemble method demonstrated the best result for the given heart disease diagnosis case. Accuracy and diversity evaluation was performed to analyze the impact of the cascade strategy. Based on the result, the accuracy of the classifiers in the ensemble is increased but the diversity is decreased.



### Are Labels Always Necessary for Classifier Accuracy Evaluation?
- **Arxiv ID**: http://arxiv.org/abs/2007.02915v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.02915v3)
- **Published**: 2020-07-06 17:45:39+00:00
- **Updated**: 2021-05-25 06:32:00+00:00
- **Authors**: Weijian Deng, Liang Zheng
- **Comment**: CVPR 2021 camera-ready
- **Journal**: None
- **Summary**: To calculate the model accuracy on a computer vision task, e.g., object recognition, we usually require a test set composing of test samples and their ground truth labels. Whilst standard usage cases satisfy this requirement, many real-world scenarios involve unlabeled test data, rendering common model evaluation methods infeasible. We investigate this important and under-explored problem, Automatic model Evaluation (AutoEval). Specifically, given a labeled training set and a classifier, we aim to estimate the classification accuracy on unlabeled test datasets. We construct a meta-dataset: a dataset comprised of datasets generated from the original images via various transformations such as rotation, background substitution, foreground scaling, etc. As the classification accuracy of the model on each sample (dataset) is known from the original dataset labels, our task can be solved via regression. Using the feature statistics to represent the distribution of a sample dataset, we can train regression models (e.g., a regression neural network) to predict model performance. Using synthetic meta-dataset and real-world datasets in training and testing, respectively, we report a reasonable and promising prediction of the model accuracy. We also provide insights into the application scope, limitation, and potential future direction of AutoEval.



### MCMI: Multi-Cycle Image Translation with Mutual Information Constraints
- **Arxiv ID**: http://arxiv.org/abs/2007.02919v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.02919v1)
- **Published**: 2020-07-06 17:50:43+00:00
- **Updated**: 2020-07-06 17:50:43+00:00
- **Authors**: Xiang Xu, Megha Nawhal, Greg Mori, Manolis Savva
- **Comment**: None
- **Journal**: None
- **Summary**: We present a mutual information-based framework for unsupervised image-to-image translation. Our MCMI approach treats single-cycle image translation models as modules that can be used recurrently in a multi-cycle translation setting where the translation process is bounded by mutual information constraints between the input and output images. The proposed mutual information constraints can improve cross-domain mappings by optimizing out translation functions that fail to satisfy the Markov property during image translations. We show that models trained with MCMI produce higher quality images and learn more semantically-relevant mappings compared to state-of-the-art image translation methods. The MCMI framework can be applied to existing unpaired image-to-image translation models with minimum modifications. Qualitative experiments and a perceptual study demonstrate the image quality improvements and generality of our approach using several backbone models and a variety of image datasets.



### Deep Learning for Apple Diseases: Classification and Identification
- **Arxiv ID**: http://arxiv.org/abs/2007.02980v1
- **DOI**: 10.1504/IJCISTUDIES.2021.10033513
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.02980v1)
- **Published**: 2020-07-06 18:08:58+00:00
- **Updated**: 2020-07-06 18:08:58+00:00
- **Authors**: Asif Iqbal Khan, SMK Quadri, Saba Banday
- **Comment**: None
- **Journal**: None
- **Summary**: Diseases and pests cause huge economic loss to the apple industry every year. The identification of various apple diseases is challenging for the farmers as the symptoms produced by different diseases may be very similar, and may be present simultaneously. This paper is an attempt to provide the timely and accurate detection and identification of apple diseases. In this study, we propose a deep learning based approach for identification and classification of apple diseases. The first part of the study is dataset creation which includes data collection and data labelling. Next, we train a Convolutional Neural Network (CNN) model on the prepared dataset for automatic classification of apple diseases. CNNs are end-to-end learning algorithms which perform automatic feature extraction and learn complex features directly from raw images, making them suitable for wide variety of tasks like image classification, object detection, segmentation etc. We applied transfer learning to initialize the parameters of the proposed deep model. Data augmentation techniques like rotation, translation, reflection and scaling were also applied to prevent overfitting. The proposed CNN model obtained encouraging results, reaching around 97.18% of accuracy on our prepared dataset. The results validate that the proposed method is effective in classifying various types of apple diseases and can be used as a practical tool by farmers.



### Determination of the most representative descriptor among a set of feature vectors for the same object
- **Arxiv ID**: http://arxiv.org/abs/2007.03021v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.03021v1)
- **Published**: 2020-07-06 19:09:06+00:00
- **Updated**: 2020-07-06 19:09:06+00:00
- **Authors**: Dmitry Pozdnyakov
- **Comment**: 8 pages, 1 figure
- **Journal**: None
- **Summary**: On an example of solution of the face recognition problem the approach for estimation of the most representative descriptor among a set of feature vectors for the same face is considered in present study. The estimation is based on robust calculation of the mode-median mixture vector for the set as the descriptor by means of Welsch/Leclerc loss function application in case of very sparse filling of the feature space with feature vectors



### Labeling of Multilingual Breast MRI Reports
- **Arxiv ID**: http://arxiv.org/abs/2007.03028v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2007.03028v3)
- **Published**: 2020-07-06 19:22:44+00:00
- **Updated**: 2020-11-11 13:07:54+00:00
- **Authors**: Chen-Han Tsai, Nahum Kiryati, Eli Konen, Miri Sklair-Levy, Arnaldo Mayer
- **Comment**: 10 pages, 5 figures, MICCAI LABELS Workshop 2020
- **Journal**: Lecture Notes in Computer Science 12446 (2020) 233-241
- **Summary**: Medical reports are an essential medium in recording a patient's condition throughout a clinical trial. They contain valuable information that can be extracted to generate a large labeled dataset needed for the development of clinical tools. However, the majority of medical reports are stored in an unregularized format, and a trained human annotator (typically a doctor) must manually assess and label each case, resulting in an expensive and time consuming procedure. In this work, we present a framework for developing a multilingual breast MRI report classifier using a custom-built language representation called LAMBR. Our proposed method overcomes practical challenges faced in clinical settings, and we demonstrate improved performance in extracting labels from medical reports when compared with conventional approaches.



### Continual Learning in Human Activity Recognition: an Empirical Analysis of Regularization
- **Arxiv ID**: http://arxiv.org/abs/2007.03032v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.03032v1)
- **Published**: 2020-07-06 19:30:37+00:00
- **Updated**: 2020-07-06 19:30:37+00:00
- **Authors**: Saurav Jha, Martin Schiemer, Juan Ye
- **Comment**: 7 pages, 5 figures, 3 tables (Appendix included)
- **Journal**: None
- **Summary**: Given the growing trend of continual learning techniques for deep neural networks focusing on the domain of computer vision, there is a need to identify which of these generalizes well to other tasks such as human activity recognition (HAR). As recent methods have mostly been composed of loss regularization terms and memory replay, we provide a constituent-wise analysis of some prominent task-incremental learning techniques employing these on HAR datasets. We find that most regularization approaches lack substantial effect and provide an intuition of when they fail. Thus, we make the case that the development of continual learning algorithms should be motivated by rather diverse task domains.



### Robust Technique for Representative Volume Element Identification in Noisy Microtomography Images of Porous Materials Based on Pores Morphology and Their Spatial Distribution
- **Arxiv ID**: http://arxiv.org/abs/2007.03035v1
- **DOI**: None
- **Categories**: **physics.comp-ph**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.03035v1)
- **Published**: 2020-07-06 19:34:09+00:00
- **Updated**: 2020-07-06 19:34:09+00:00
- **Authors**: Maxim Grigoriev, Anvar Khafizov, Vladislav Kokhan, Viktor Asadchikov
- **Comment**: None
- **Journal**: None
- **Summary**: Microtomography is a powerful method of materials investigation. It enables to obtain physical properties of porous media non-destructively that is useful in studies. One of the application ways is a calculation of porosity, pore sizes, surface area, and other parameters of metal-ceramic (cermet) membranes which are widely spread in the filtration industry. The microtomography approach is efficient because all of those parameters are calculated simultaneously in contrast to the conventional techniques. Nevertheless, the calculations on Micro-CT reconstructed images appear to be time-consuming, consequently representative volume element should be chosen to speed them up. This research sheds light on representative elementary volume identification without consideration of any physical parameters such as porosity, etc. Thus, the volume element could be found even in noised and grayscale images. The proposed method is flexible and does not overestimate the volume size in the case of anisotropic samples. The obtained volume element could be used for computations of the domain's physical characteristics if the image is filtered and binarized, or for selections of optimal filtering parameters for denoising procedure.



### Leveraging Class Hierarchies with Metric-Guided Prototype Learning
- **Arxiv ID**: http://arxiv.org/abs/2007.03047v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.03047v3)
- **Published**: 2020-07-06 20:22:08+00:00
- **Updated**: 2021-11-29 14:07:06+00:00
- **Authors**: Vivien Sainte Fare Garnot, Loic Landrieu
- **Comment**: Published at BMVC2021
- **Journal**: None
- **Summary**: In many classification tasks, the set of target classes can be organized into a hierarchy. This structure induces a semantic distance between classes, and can be summarised under the form of a cost matrix, which defines a finite metric on the class set. In this paper, we propose to model the hierarchical class structure by integrating this metric in the supervision of a prototypical network. Our method relies on jointly learning a feature-extracting network and a set of class prototypes whose relative arrangement in the embedding space follows an hierarchical metric. We show that this approach allows for a consistent improvement of the error rate weighted by the cost matrix when compared to traditional methods and other prototype-based strategies. Furthermore, when the induced metric contains insight on the data structure, our method improves the overall precision as well. Experiments on four different public datasets - from agricultural time series classification to depth image semantic segmentation - validate our approach.



### Learning to Segment Anatomical Structures Accurately from One Exemplar
- **Arxiv ID**: http://arxiv.org/abs/2007.03052v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.03052v2)
- **Published**: 2020-07-06 20:27:38+00:00
- **Updated**: 2020-07-08 01:00:27+00:00
- **Authors**: Yuhang Lu, Weijian Li, Kang Zheng, Yirui Wang, Adam P. Harrison, Chihung Lin, Song Wang, Jing Xiao, Le Lu, Chang-Fu Kuo, Shun Miao
- **Comment**: MICCAI2020 (Early accept)
- **Journal**: None
- **Summary**: Accurate segmentation of critical anatomical structures is at the core of medical image analysis. The main bottleneck lies in gathering the requisite expert-labeled image annotations in a scalable manner. Methods that permit to produce accurate anatomical structure segmentation without using a large amount of fully annotated training images are highly desirable. In this work, we propose a novel contribution of Contour Transformer Network (CTN), a one-shot anatomy segmentor including a naturally built-in human-in-the-loop mechanism. Segmentation is formulated by learning a contour evolution behavior process based on graph convolutional networks (GCNs). Training of our CTN model requires only one labeled image exemplar and leverages additional unlabeled data through newly introduced loss functions that measure the global shape and appearance consistency of contours. We demonstrate that our one-shot learning method significantly outperforms non-learning-based methods and performs competitively to the state-of-the-art fully supervised deep learning approaches. With minimal human-in-the-loop editing feedback, the segmentation performance can be further improved and tailored towards the observer desired outcomes. This can facilitate the clinician designed imaging-based biomarker assessments (to support personalized quantitative clinical diagnosis) and outperforms fully supervised baselines.



### Benefiting from Bicubically Down-Sampled Images for Learning Real-World Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2007.03053v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.03053v2)
- **Published**: 2020-07-06 20:27:58+00:00
- **Updated**: 2020-11-05 18:25:16+00:00
- **Authors**: Mohammad Saeed Rad, Thomas Yu, Claudiu Musat, Hazim Kemal Ekenel, Behzad Bozorgtabar, Jean-Philippe Thiran
- **Comment**: WACV 2021
- **Journal**: None
- **Summary**: Super-resolution (SR) has traditionally been based on pairs of high-resolution images (HR) and their low-resolution (LR) counterparts obtained artificially with bicubic downsampling. However, in real-world SR, there is a large variety of realistic image degradations and analytically modeling these realistic degradations can prove quite difficult. In this work, we propose to handle real-world SR by splitting this ill-posed problem into two comparatively more well-posed steps. First, we train a network to transform real LR images to the space of bicubically downsampled images in a supervised manner, by using both real LR/HR pairs and synthetic pairs. Second, we take a generic SR network trained on bicubically downsampled images to super-resolve the transformed LR image. The first step of the pipeline addresses the problem by registering the large variety of degraded images to a common, well understood space of images. The second step then leverages the already impressive performance of SR on bicubically downsampled images, sidestepping the issues of end-to-end training on datasets with many different image degradations. We demonstrate the effectiveness of our proposed method by comparing it to recent methods in real-world SR and show that our proposed approach outperforms the state-of-the-art works in terms of both qualitative and quantitative results, as well as results of an extensive user study conducted on several real image datasets.



### VPN: Learning Video-Pose Embedding for Activities of Daily Living
- **Arxiv ID**: http://arxiv.org/abs/2007.03056v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.03056v1)
- **Published**: 2020-07-06 20:39:08+00:00
- **Updated**: 2020-07-06 20:39:08+00:00
- **Authors**: Srijan Das, Saurav Sharma, Rui Dai, Francois Bremond, Monique Thonnat
- **Comment**: Accepted in ECCV 2020
- **Journal**: None
- **Summary**: In this paper, we focus on the spatio-temporal aspect of recognizing Activities of Daily Living (ADL). ADL have two specific properties (i) subtle spatio-temporal patterns and (ii) similar visual patterns varying with time. Therefore, ADL may look very similar and often necessitate to look at their fine-grained details to distinguish them. Because the recent spatio-temporal 3D ConvNets are too rigid to capture the subtle visual patterns across an action, we propose a novel Video-Pose Network: VPN. The 2 key components of this VPN are a spatial embedding and an attention network. The spatial embedding projects the 3D poses and RGB cues in a common semantic space. This enables the action recognition framework to learn better spatio-temporal features exploiting both modalities. In order to discriminate similar actions, the attention network provides two functionalities - (i) an end-to-end learnable pose backbone exploiting the topology of human body, and (ii) a coupler to provide joint spatio-temporal attention weights across a video. Experiments show that VPN outperforms the state-of-the-art results for action classification on a large scale human activity dataset: NTU-RGB+D 120, its subset NTU-RGB+D 60, a real-world challenging human activity dataset: Toyota Smarthome and a small scale human-object interaction dataset Northwestern UCLA.



### Guided Fine-Tuning for Large-Scale Material Transfer
- **Arxiv ID**: http://arxiv.org/abs/2007.03059v2
- **DOI**: 10.1111/cgf.14056
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.03059v2)
- **Published**: 2020-07-06 20:55:37+00:00
- **Updated**: 2020-07-08 17:12:21+00:00
- **Authors**: Valentin Deschaintre, George Drettakis, Adrien Bousseau
- **Comment**: Published in Computer Graphics Forum, 39(4); Proceedings of the
  Eurographics Symposium on Rendering 2020
- **Journal**: None
- **Summary**: We present a method to transfer the appearance of one or a few exemplar SVBRDFs to a target image representing similar materials. Our solution is extremely simple: we fine-tune a deep appearance-capture network on the provided exemplars, such that it learns to extract similar SVBRDF values from the target image. We introduce two novel material capture and design workflows that demonstrate the strength of this simple approach. Our first workflow allows to produce plausible SVBRDFs of large-scale objects from only a few pictures. Specifically, users only need take a single picture of a large surface and a few close-up flash pictures of some of its details. We use existing methods to extract SVBRDF parameters from the close-ups, and our method to transfer these parameters to the entire surface, enabling the lightweight capture of surfaces several meters wide such as murals, floors and furniture. In our second workflow, we provide a powerful way for users to create large SVBRDFs from internet pictures by transferring the appearance of existing, pre-designed SVBRDFs. By selecting different exemplars, users can control the materials assigned to the target image, greatly enhancing the creative possibilities offered by deep appearance capture.



### Generative Model-Based Loss to the Rescue: A Method to Overcome Annotation Errors for Depth-Based Hand Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2007.03073v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.03073v2)
- **Published**: 2020-07-06 21:24:25+00:00
- **Updated**: 2021-05-30 11:36:43+00:00
- **Authors**: Jiayi Wang, Franziska Mueller, Florian Bernard, Christian Theobalt
- **Comment**: None
- **Journal**: None
- **Summary**: We propose to use a model-based generative loss for training hand pose estimators on depth images based on a volumetric hand model. This additional loss allows training of a hand pose estimator that accurately infers the entire set of 21 hand keypoints while only using supervision for 6 easy-to-annotate keypoints (fingertips and wrist). We show that our partially-supervised method achieves results that are comparable to those of fully-supervised methods which enforce articulation consistency. Moreover, for the first time we demonstrate that such an approach can be used to train on datasets that have erroneous annotations, i.e. "ground truth" with notable measurement errors, while obtaining predictions that explain the depth images better than the given "ground truth".



### Kernel Stein Generative Modeling
- **Arxiv ID**: http://arxiv.org/abs/2007.03074v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.03074v1)
- **Published**: 2020-07-06 21:26:04+00:00
- **Updated**: 2020-07-06 21:26:04+00:00
- **Authors**: Wei-Cheng Chang, Chun-Liang Li, Youssef Mroueh, Yiming Yang
- **Comment**: None
- **Journal**: None
- **Summary**: We are interested in gradient-based Explicit Generative Modeling where samples can be derived from iterative gradient updates based on an estimate of the score function of the data distribution. Recent advances in Stochastic Gradient Langevin Dynamics (SGLD) demonstrates impressive results with energy-based models on high-dimensional and complex data distributions. Stein Variational Gradient Descent (SVGD) is a deterministic sampling algorithm that iteratively transports a set of particles to approximate a given distribution, based on functional gradient descent that decreases the KL divergence. SVGD has promising results on several Bayesian inference applications. However, applying SVGD on high dimensional problems is still under-explored. The goal of this work is to study high dimensional inference with SVGD. We first identify key challenges in practical kernel SVGD inference in high-dimension. We propose noise conditional kernel SVGD (NCK-SVGD), that works in tandem with the recently introduced Noise Conditional Score Network estimator. NCK is crucial for successful inference with SVGD in high dimension, as it adapts the kernel to the noise level of the score estimate. As we anneal the noise, NCK-SVGD targets the real data distribution. We then extend the annealed SVGD with an entropic regularization. We show that this offers a flexible control between sample quality and diversity, and verify it empirically by precision and recall evaluations. The NCK-SVGD produces samples comparable to GANs and annealed SGLD on computer vision benchmarks, including MNIST and CIFAR-10.



### Scalable, Proposal-free Instance Segmentation Network for 3D Pixel Clustering and Particle Trajectory Reconstruction in Liquid Argon Time Projection Chambers
- **Arxiv ID**: http://arxiv.org/abs/2007.03083v1
- **DOI**: None
- **Categories**: **physics.ins-det**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.03083v1)
- **Published**: 2020-07-06 21:37:28+00:00
- **Updated**: 2020-07-06 21:37:28+00:00
- **Authors**: Dae Heun Koh, Pierre Côte de Soux, Laura Dominé, François Drielsma, Ran Itay, Qing Lin, Kazuhiro Terao, Ka Vang Tsang, Tracy Usher
- **Comment**: None
- **Journal**: None
- **Summary**: Liquid Argon Time Projection Chambers (LArTPCs) are high resolution particle imaging detectors, employed by accelerator-based neutrino oscillation experiments for high precision physics measurements. While images of particle trajectories are intuitive to analyze for physicists, the development of a high quality, automated data reconstruction chain remains challenging. One of the most critical reconstruction steps is particle clustering: the task of grouping 3D image pixels into different particle instances that share the same particle type. In this paper, we propose the first scalable deep learning algorithm for particle clustering in LArTPC data using sparse convolutional neural networks (SCNN). Building on previous works on SCNNs and proposal free instance segmentation, we build an end-to-end trainable instance segmentation network that learns an embedding of the image pixels to perform point cloud clustering in a transformed space. We benchmark the performance of our algorithm on PILArNet, a public 3D particle imaging dataset, with respect to common clustering evaluation metrics. 3D pixels were successfully clustered into individual particle trajectories with 90% of them having an adjusted Rand index score greater than 92% with a mean pixel clustering efficiency and purity above 96%. This work contributes to the development of an end-to-end optimizable full data reconstruction chain for LArTPCs, in particular pixel-based 3D imaging detectors including the near detector of the Deep Underground Neutrino Experiment. Our algorithm is made available in the open access repository, and we share our Singularity software container, which can be used to reproduce our work on the dataset.



### Wasserstein Distances for Stereo Disparity Estimation
- **Arxiv ID**: http://arxiv.org/abs/2007.03085v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.03085v2)
- **Published**: 2020-07-06 21:37:50+00:00
- **Updated**: 2021-03-29 09:42:37+00:00
- **Authors**: Divyansh Garg, Yan Wang, Bharath Hariharan, Mark Campbell, Kilian Q. Weinberger, Wei-Lun Chao
- **Comment**: Accepted to NeurIPS 2020 (spotlight)
- **Journal**: None
- **Summary**: Existing approaches to depth or disparity estimation output a distribution over a set of pre-defined discrete values. This leads to inaccurate results when the true depth or disparity does not match any of these values. The fact that this distribution is usually learned indirectly through a regression loss causes further problems in ambiguous regions around object boundaries. We address these issues using a new neural network architecture that is capable of outputting arbitrary depth values, and a new loss function that is derived from the Wasserstein distance between the true and the predicted distributions. We validate our approach on a variety of tasks, including stereo disparity and depth estimation, and the downstream 3D object detection. Our approach drastically reduces the error in ambiguous regions, especially around object boundaries that greatly affect the localization of objects in 3D, achieving the state-of-the-art in 3D object detection for autonomous driving. Our code will be available at https://github.com/Div99/W-Stereo-Disp.



### Text Recognition -- Real World Data and Where to Find Them
- **Arxiv ID**: http://arxiv.org/abs/2007.03098v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.03098v2)
- **Published**: 2020-07-06 22:23:27+00:00
- **Updated**: 2020-07-17 15:07:40+00:00
- **Authors**: Klára Janoušková, Jiri Matas, Lluis Gomez, Dimosthenis Karatzas
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: We present a method for exploiting weakly annotated images to improve text extraction pipelines. The approach uses an arbitrary end-to-end text recognition system to obtain text region proposals and their, possibly erroneous, transcriptions. The proposed method includes matching of imprecise transcription to weak annotations and edit distance guided neighbourhood search. It produces nearly error-free, localised instances of scene text, which we treat as "pseudo ground truth" (PGT).   We apply the method to two weakly-annotated datasets. Training with the extracted PGT consistently improves the accuracy of a state of the art recognition model, by 3.7~\% on average, across different benchmark datasets (image domains) and 24.5~\% on one of the weakly annotated datasets.



### Multi-image Super Resolution of Remotely Sensed Images using Residual Feature Attention Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2007.03107v2
- **DOI**: 10.3390/rs12142207
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.03107v2)
- **Published**: 2020-07-06 22:54:02+00:00
- **Updated**: 2020-07-08 10:02:13+00:00
- **Authors**: Francesco Salvetti, Vittorio Mazzia, Aleem Khaliq, Marcello Chiaberge
- **Comment**: None
- **Journal**: Remote Sens. 2020, 12(14), 2207
- **Summary**: Convolutional Neural Networks (CNNs) have been consistently proved state-of-the-art results in image Super-Resolution (SR), representing an exceptional opportunity for the remote sensing field to extract further information and knowledge from captured data. However, most of the works published in the literature have been focusing on the Single-Image Super-Resolution problem so far. At present, satellite based remote sensing platforms offer huge data availability with high temporal resolution and low spatial resolution. In this context, the presented research proposes a novel residual attention model (RAMS) that efficiently tackles the multi-image super-resolution task, simultaneously exploiting spatial and temporal correlations to combine multiple images. We introduce the mechanism of visual feature attention with 3D convolutions in order to obtain an aware data fusion and information extraction of the multiple low-resolution images, transcending limitations of the local region of convolutional operations. Moreover, having multiple inputs with the same scene, our representation learning network makes extensive use of nestled residual connections to let flow redundant low-frequency signals and focus the computation on more important high-frequency components. Extensive experimentation and evaluations against other available solutions, either for single or multi-image super-resolution, have demonstrated that the proposed deep learning-based solution can be considered state-of-the-art for Multi-Image Super-Resolution for remote sensing applications.



### Learning Embeddings for Image Clustering: An Empirical Study of Triplet Loss Approaches
- **Arxiv ID**: http://arxiv.org/abs/2007.03123v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.03123v1)
- **Published**: 2020-07-06 23:38:14+00:00
- **Updated**: 2020-07-06 23:38:14+00:00
- **Authors**: Kalun Ho, Janis Keuper, Franz-Josef Pfreundt, Margret Keuper
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we evaluate two different image clustering objectives, k-means clustering and correlation clustering, in the context of Triplet Loss induced feature space embeddings. Specifically, we train a convolutional neural network to learn discriminative features by optimizing two popular versions of the Triplet Loss in order to study their clustering properties under the assumption of noisy labels. Additionally, we propose a new, simple Triplet Loss formulation, which shows desirable properties with respect to formal clustering objectives and outperforms the existing methods. We evaluate all three Triplet loss formulations for K-means and correlation clustering on the CIFAR-10 image classification dataset.



