# Arxiv Papers in cs.CV on 2020-07-13
### Gender Classification and Bias Mitigation in Facial Images
- **Arxiv ID**: http://arxiv.org/abs/2007.06141v1
- **DOI**: 10.1145/3394231
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.06141v1)
- **Published**: 2020-07-13 01:09:06+00:00
- **Updated**: 2020-07-13 01:09:06+00:00
- **Authors**: Wenying Wu, Pavlos Protopapas, Zheng Yang, Panagiotis Michalatos
- **Comment**: 9 pages
- **Journal**: WebSci (2020) 106-114
- **Summary**: Gender classification algorithms have important applications in many domains today such as demographic research, law enforcement, as well as human-computer interaction. Recent research showed that algorithms trained on biased benchmark databases could result in algorithmic bias. However, to date, little research has been carried out on gender classification algorithms' bias towards gender minorities subgroups, such as the LGBTQ and the non-binary population, who have distinct characteristics in gender expression. In this paper, we began by conducting surveys on existing benchmark databases for facial recognition and gender classification tasks. We discovered that the current benchmark databases lack representation of gender minority subgroups. We worked on extending the current binary gender classifier to include a non-binary gender class. We did that by assembling two new facial image databases: 1) a racially balanced inclusive database with a subset of LGBTQ population 2) an inclusive-gender database that consists of people with non-binary gender. We worked to increase classification accuracy and mitigate algorithmic biases on our baseline model trained on the augmented benchmark database. Our ensemble model has achieved an overall accuracy score of 90.39%, which is a 38.72% increase from the baseline binary gender classifier trained on Adience. While this is an initial attempt towards mitigating bias in gender classification, more work is needed in modeling gender as a continuum by assembling more inclusive databases.



### Embedded Deep Bilinear Interactive Information and Selective Fusion for Multi-view Learning
- **Arxiv ID**: http://arxiv.org/abs/2007.06143v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.06143v1)
- **Published**: 2020-07-13 01:13:23+00:00
- **Updated**: 2020-07-13 01:13:23+00:00
- **Authors**: Jinglin Xu, Wenbin Li, Jiantao Shen, Xinwang Liu, Peicheng Zhou, Xiangsen Zhang, Xiwen Yao, Junwei Han
- **Comment**: None
- **Journal**: None
- **Summary**: As a concrete application of multi-view learning, multi-view classification improves the traditional classification methods significantly by integrating various views optimally. Although most of the previous efforts have been demonstrated the superiority of multi-view learning, it can be further improved by comprehensively embedding more powerful cross-view interactive information and a more reliable multi-view fusion strategy in intensive studies. To fulfill this goal, we propose a novel multi-view learning framework to make the multi-view classification better aimed at the above-mentioned two aspects. That is, we seamlessly embed various intra-view information, cross-view multi-dimension bilinear interactive information, and a new view ensemble mechanism into a unified framework to make a decision via the optimization. In particular, we train different deep neural networks to learn various intra-view representations, and then dynamically learn multi-dimension bilinear interactive information from different bilinear similarities via the bilinear function between views. After that, we adaptively fuse the representations of multiple views by flexibly tuning the parameters of the view-weight, which not only avoids the trivial solution of weight but also provides a new way to select a few discriminative views that are beneficial to make a decision for the multi-view classification. Extensive experiments on six publicly available datasets demonstrate the effectiveness of the proposed method.



### Temporal Self-Ensembling Teacher for Semi-Supervised Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2007.06144v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.06144v3)
- **Published**: 2020-07-13 01:17:25+00:00
- **Updated**: 2020-09-02 09:26:25+00:00
- **Authors**: Cong Chen, Shouyang Dong, Ye Tian, Kunlin Cao, Li Liu, Yuanhao Guo
- **Comment**: 13 papges, 4 figures, preprint for submission
- **Journal**: None
- **Summary**: This paper focuses on Semi-Supervised Object Detection (SSOD). Knowledge Distillation (KD) has been widely used for semi-supervised image classification. However, adapting these methods for SSOD has the following obstacles. (1) The teacher model serves a dual role as a teacher and a student, such that the teacher predictions on unlabeled images may be very close to those of student, which limits the upper-bound of the student. (2) The class imbalance issue in SSOD hinders an efficient knowledge transfer from teacher to student. To address these problems, we propose a novel method Temporal Self-Ensembling Teacher (TSE-T) for SSOD. Differently from previous KD based methods, we devise a temporally evolved teacher model. First, our teacher model ensembles its temporal predictions for unlabeled images under stochastic perturbations. Second, our teacher model ensembles its temporal model weights with the student model weights by an exponential moving average (EMA) which allows the teacher gradually learn from the student. These self-ensembling strategies increase data and model diversity, thus improving teacher predictions on unlabeled images. Finally, we use focal loss to formulate consistency regularization term to handle the data imbalance problem, which is a more efficient manner to utilize the useful information from unlabeled images than a simple hard-thresholding method which solely preserves confident predictions. Evaluated on the widely used VOC and COCO benchmarks, the mAP of our method has achieved 80.73% and 40.52% on the VOC2007 test set and the COCO2014 minval5k set respectively, which outperforms a strong fully-supervised detector by 2.37% and 1.49%. Furthermore, our method sets the new state-of-the-art in SSOD on VOC2007 test set which outperforms the baseline SSOD method by 1.44%. The source code of this work is publicly available at http://github.com/syangdong/tse-t.



### Fine-Grained Crowd Counting
- **Arxiv ID**: http://arxiv.org/abs/2007.06146v1
- **DOI**: 10.1109/TIP.2021.3049938
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.06146v1)
- **Published**: 2020-07-13 01:31:12+00:00
- **Updated**: 2020-07-13 01:31:12+00:00
- **Authors**: Jia Wan, Nikil Senthil Kumar, Antoni B. Chan
- **Comment**: None
- **Journal**: None
- **Summary**: Current crowd counting algorithms are only concerned about the number of people in an image, which lacks low-level fine-grained information of the crowd. For many practical applications, the total number of people in an image is not as useful as the number of people in each sub-category. E.g., knowing the number of people waiting inline or browsing can help retail stores; knowing the number of people standing/sitting can help restaurants/cafeterias; knowing the number of violent/non-violent people can help police in crowd management. In this paper, we propose fine-grained crowd counting, which differentiates a crowd into categories based on the low-level behavior attributes of the individuals (e.g. standing/sitting or violent behavior) and then counts the number of people in each category. To enable research in this area, we construct a new dataset of four real-world fine-grained counting tasks: traveling direction on a sidewalk, standing or sitting, waiting in line or not, and exhibiting violent behavior or not. Since the appearance features of different crowd categories are similar, the challenge of fine-grained crowd counting is to effectively utilize contextual information to distinguish between categories. We propose a two branch architecture, consisting of a density map estimation branch and a semantic segmentation branch. We propose two refinement strategies for improving the predictions of the two branches. First, to encode contextual information, we propose feature propagation guided by the density map prediction, which eliminates the effect of background features during propagation. Second, we propose a complementary attention model to share information between the two branches. Experiment results confirm the effectiveness of our method.



### Universal-to-Specific Framework for Complex Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2007.06149v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.06149v1)
- **Published**: 2020-07-13 01:49:07+00:00
- **Updated**: 2020-07-13 01:49:07+00:00
- **Authors**: Peisen Zhao, Lingxi Xie, Ya Zhang, Qi Tian
- **Comment**: 13 pages, 8 figures
- **Journal**: None
- **Summary**: Video-based action recognition has recently attracted much attention in the field of computer vision. To solve more complex recognition tasks, it has become necessary to distinguish different levels of interclass variations. Inspired by a common flowchart based on the human decision-making process that first narrows down the probable classes and then applies a "rethinking" process for finer-level recognition, we propose an effective universal-to-specific (U2S) framework for complex action recognition. The U2S framework is composed of three subnetworks: a universal network, a category-specific network, and a mask network. The universal network first learns universal feature representations. The mask network then generates attention masks for confusing classes through category regularization based on the output of the universal network. The mask is further used to guide the category-specific network for class-specific feature representations. The entire framework is optimized in an end-to-end manner. Experiments on a variety of benchmark datasets, e.g., the Something-Something, UCF101, and HMDB51 datasets, demonstrate the effectiveness of the U2S framework; i.e., U2S can focus on discriminative spatiotemporal regions for confusing categories. We further visualize the relationship between different classes, showing that U2S indeed improves the discriminability of learned features. Moreover, the proposed U2S model is a general framework and may adopt any base recognition network.



### MS-NAS: Multi-Scale Neural Architecture Search for Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2007.06151v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.06151v1)
- **Published**: 2020-07-13 02:02:00+00:00
- **Updated**: 2020-07-13 02:02:00+00:00
- **Authors**: Xingang Yan, Weiwen Jiang, Yiyu Shi, Cheng Zhuo
- **Comment**: None
- **Journal**: None
- **Summary**: The recent breakthroughs of Neural Architecture Search (NAS) have motivated various applications in medical image segmentation. However, most existing work either simply rely on hyper-parameter tuning or stick to a fixed network backbone, thereby limiting the underlying search space to identify more efficient architecture. This paper presents a Multi-Scale NAS (MS-NAS) framework that is featured with multi-scale search space from network backbone to cell operation, and multi-scale fusion capability to fuse features with different sizes. To mitigate the computational overhead due to the larger search space, a partial channel connection scheme and a two-step decoding method are utilized to reduce computational overhead while maintaining optimization quality. Experimental results show that on various datasets for segmentation, MS-NAS outperforms the state-of-the-art methods and achieves 0.6-5.4% mIOU and 0.4-3.5% DSC improvements, while the computational resource consumption is reduced by 18.0-24.9%.



### AI Playground: Unreal Engine-based Data Ablation Tool for Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2007.06153v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.06153v1)
- **Published**: 2020-07-13 02:04:39+00:00
- **Updated**: 2020-07-13 02:04:39+00:00
- **Authors**: Mehdi Mousavi, Aashis Khanal, Rolando Estrada
- **Comment**: 14 pages, 7 figures
- **Journal**: None
- **Summary**: Machine learning requires data, but acquiring and labeling real-world data is challenging, expensive, and time-consuming. More importantly, it is nearly impossible to alter real data post-acquisition (e.g., change the illumination of a room), making it very difficult to measure how specific properties of the data affect performance. In this paper, we present AI Playground (AIP), an open-source, Unreal Engine-based tool for generating and labeling virtual image data. With AIP, it is trivial to capture the same image under different conditions (e.g., fidelity, lighting, etc.) and with different ground truths (e.g., depth or surface normal values). AIP is easily extendable and can be used with or without code. To validate our proposed tool, we generated eight datasets of otherwise identical but varying lighting and fidelity conditions. We then trained deep neural networks to predict (1) depth values, (2) surface normals, or (3) object labels and assessed each network's intra- and cross-dataset performance. Among other insights, we verified that sensitivity to different settings is problem-dependent. We confirmed the findings of other studies that segmentation models are very sensitive to fidelity, but we also found that they are just as sensitive to lighting. In contrast, depth and normal estimation models seem to be less sensitive to fidelity or lighting and more sensitive to the structure of the image. Finally, we tested our trained depth-estimation networks on two real-world datasets and obtained results comparable to training on real data alone, confirming that our virtual environments are realistic enough for real-world tasks.



### Deep Reinforced Attention Learning for Quality-Aware Visual Recognition
- **Arxiv ID**: http://arxiv.org/abs/2007.06156v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.06156v2)
- **Published**: 2020-07-13 02:44:38+00:00
- **Updated**: 2022-08-30 17:45:52+00:00
- **Authors**: Duo Li, Qifeng Chen
- **Comment**: A related work (DIANet in AAAI-20) should be discussed and cited
- **Journal**: None
- **Summary**: In this paper, we build upon the weakly-supervised generation mechanism of intermediate attention maps in any convolutional neural networks and disclose the effectiveness of attention modules more straightforwardly to fully exploit their potential. Given an existing neural network equipped with arbitrary attention modules, we introduce a meta critic network to evaluate the quality of attention maps in the main network. Due to the discreteness of our designed reward, the proposed learning method is arranged in a reinforcement learning setting, where the attention actors and recurrent critics are alternately optimized to provide instant critique and revision for the temporary attention representation, hence coined as Deep REinforced Attention Learning (DREAL). It could be applied universally to network architectures with different types of attention modules and promotes their expressive ability by maximizing the relative gain of the final recognition performance arising from each individual attention module, as demonstrated by extensive experiments on both category and instance recognition benchmarks.



### Low to High Dimensional Modality Hallucination using Aggregated Fields of View
- **Arxiv ID**: http://arxiv.org/abs/2007.06166v1
- **DOI**: 10.1109/LRA.2020.2970679
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.06166v1)
- **Published**: 2020-07-13 03:13:48+00:00
- **Updated**: 2020-07-13 03:13:48+00:00
- **Authors**: Kausic Gunasekar, Qiang Qiu, Yezhou Yang
- **Comment**: None
- **Journal**: IEEE Robotics and Automation Letters, vol. 5, no. 2, pp.
  1983-1990, April 2020
- **Summary**: Real-world robotics systems deal with data from a multitude of modalities, especially for tasks such as navigation and recognition. The performance of those systems can drastically degrade when one or more modalities become inaccessible, due to factors such as sensors' malfunctions or adverse environments. Here, we argue modality hallucination as one effective way to ensure consistent modality availability and thereby reduce unfavorable consequences. While hallucinating data from a modality with richer information, e.g., RGB to depth, has been researched extensively, we investigate the more challenging low-to-high modality hallucination with interesting use cases in robotics and autonomous systems. We present a novel hallucination architecture that aggregates information from multiple fields of view of the local neighborhood to recover the lost information from the extant modality. The process is implemented by capturing a non-linear mapping between the data modalities and the learned mapping is used to aid the extant modality to mitigate the risk posed to the system in the adverse scenarios which involve modality loss. We also conduct extensive classification and segmentation experiments on UWRGBD and NYUD datasets and demonstrate that hallucination allays the negative effects of the modality loss. Implementation and models: https://github.com/kausic94/Hallucination



### Bridging Maximum Likelihood and Adversarial Learning via $α$-Divergence
- **Arxiv ID**: http://arxiv.org/abs/2007.06178v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.06178v1)
- **Published**: 2020-07-13 04:06:43+00:00
- **Updated**: 2020-07-13 04:06:43+00:00
- **Authors**: Miaoyun Zhao, Yulai Cong, Shuyang Dai, Lawrence Carin
- **Comment**: AAAI 2020
- **Journal**: None
- **Summary**: Maximum likelihood (ML) and adversarial learning are two popular approaches for training generative models, and from many perspectives these techniques are complementary. ML learning encourages the capture of all data modes, and it is typically characterized by stable training. However, ML learning tends to distribute probability mass diffusely over the data space, $e.g.$, yielding blurry synthetic images. Adversarial learning is well known to synthesize highly realistic natural images, despite practical challenges like mode dropping and delicate training. We propose an $\alpha$-Bridge to unify the advantages of ML and adversarial learning, enabling the smooth transfer from one to the other via the $\alpha$-divergence. We reveal that generalizations of the $\alpha$-Bridge are closely related to approaches developed recently to regularize adversarial learning, providing insights into that prior work, and further understanding of why the $\alpha$-Bridge performs well in practice.



### Learning to Learn Parameterized Classification Networks for Scalable Input Images
- **Arxiv ID**: http://arxiv.org/abs/2007.06181v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.06181v1)
- **Published**: 2020-07-13 04:27:25+00:00
- **Updated**: 2020-07-13 04:27:25+00:00
- **Authors**: Duo Li, Anbang Yao, Qifeng Chen
- **Comment**: Accepted by ECCV 2020. Code and models are available at
  https://github.com/d-li14/SAN
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) do not have a predictable recognition behavior with respect to the input resolution change. This prevents the feasibility of deployment on different input image resolutions for a specific model. To achieve efficient and flexible image classification at runtime, we employ meta learners to generate convolutional weights of main networks for various input scales and maintain privatized Batch Normalization layers per scale. For improved training performance, we further utilize knowledge distillation on the fly over model predictions based on different input resolutions. The learned meta network could dynamically parameterize main networks to act on input images of arbitrary size with consistently better accuracy compared to individually trained models. Extensive experiments on the ImageNet demonstrate that our method achieves an improved accuracy-efficiency trade-off during the adaptive inference process. By switching executable input resolutions, our method could satisfy the requirement of fast adaption in different resource-constrained environments. Code and models are available at https://github.com/d-li14/SAN.



### Understanding Adversarial Examples from the Mutual Influence of Images and Perturbations
- **Arxiv ID**: http://arxiv.org/abs/2007.06189v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.06189v1)
- **Published**: 2020-07-13 05:00:09+00:00
- **Updated**: 2020-07-13 05:00:09+00:00
- **Authors**: Chaoning Zhang, Philipp Benz, Tooba Imtiaz, In-So Kweon
- **Comment**: Accepted at CVPR 2020
- **Journal**: None
- **Summary**: A wide variety of works have explored the reason for the existence of adversarial examples, but there is no consensus on the explanation. We propose to treat the DNN logits as a vector for feature representation, and exploit them to analyze the mutual influence of two independent inputs based on the Pearson correlation coefficient (PCC). We utilize this vector representation to understand adversarial examples by disentangling the clean images and adversarial perturbations, and analyze their influence on each other. Our results suggest a new perspective towards the relationship between images and universal perturbations: Universal perturbations contain dominant features, and images behave like noise to them. This feature perspective leads to a new method for generating targeted universal adversarial perturbations using random source images. We are the first to achieve the challenging task of a targeted universal attack without utilizing original training data. Our approach using a proxy dataset achieves comparable performance to the state-of-the-art baselines which utilize the original training dataset.



### PSConv: Squeezing Feature Pyramid into One Compact Poly-Scale Convolutional Layer
- **Arxiv ID**: http://arxiv.org/abs/2007.06191v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.06191v1)
- **Published**: 2020-07-13 05:14:11+00:00
- **Updated**: 2020-07-13 05:14:11+00:00
- **Authors**: Duo Li, Anbang Yao, Qifeng Chen
- **Comment**: Accepted by ECCV 2020. Code and models are available at
  https://github.com/d-li14/PSConv
- **Journal**: None
- **Summary**: Despite their strong modeling capacities, Convolutional Neural Networks (CNNs) are often scale-sensitive. For enhancing the robustness of CNNs to scale variance, multi-scale feature fusion from different layers or filters attracts great attention among existing solutions, while the more granular kernel space is overlooked. We bridge this regret by exploiting multi-scale features in a finer granularity. The proposed convolution operation, named Poly-Scale Convolution (PSConv), mixes up a spectrum of dilation rates and tactfully allocate them in the individual convolutional kernels of each filter regarding a single convolutional layer. Specifically, dilation rates vary cyclically along the axes of input and output channels of the filters, aggregating features over a wide range of scales in a neat style. PSConv could be a drop-in replacement of the vanilla convolution in many prevailing CNN backbones, allowing better representation learning without introducing additional parameters and computational complexities. Comprehensive experiments on the ImageNet and MS COCO benchmarks validate the superior performance of PSConv. Code and models are available at https://github.com/d-li14/PSConv.



### Data from Model: Extracting Data from Non-robust and Robust Models
- **Arxiv ID**: http://arxiv.org/abs/2007.06196v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.06196v1)
- **Published**: 2020-07-13 05:27:48+00:00
- **Updated**: 2020-07-13 05:27:48+00:00
- **Authors**: Philipp Benz, Chaoning Zhang, Tooba Imtiaz, In-So Kweon
- **Comment**: Accepted at the CVPR 2020 Workshop on Adversarial Machine Learning in
  Computer Vision
- **Journal**: None
- **Summary**: The essence of deep learning is to exploit data to train a deep neural network (DNN) model. This work explores the reverse process of generating data from a model, attempting to reveal the relationship between the data and the model. We repeat the process of Data to Model (DtM) and Data from Model (DfM) in sequence and explore the loss of feature mapping information by measuring the accuracy drop on the original validation dataset. We perform this experiment for both a non-robust and robust origin model. Our results show that the accuracy drop is limited even after multiple sequences of DtM and DfM, especially for robust models. The success of this cycling transformation can be attributed to the shared feature mapping existing in data and model. Using the same data, we observe that different DtM processes result in models having different features, especially for different network architecture families, even though they achieve comparable performance.



### Reducing Language Biases in Visual Question Answering with Visually-Grounded Question Encoder
- **Arxiv ID**: http://arxiv.org/abs/2007.06198v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2007.06198v2)
- **Published**: 2020-07-13 05:36:36+00:00
- **Updated**: 2020-07-18 13:09:29+00:00
- **Authors**: Gouthaman KV, Anurag Mittal
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: Recent studies have shown that current VQA models are heavily biased on the language priors in the train set to answer the question, irrespective of the image. E.g., overwhelmingly answer "what sport is" as "tennis" or "what color banana" as "yellow." This behavior restricts them from real-world application scenarios. In this work, we propose a novel model-agnostic question encoder, Visually-Grounded Question Encoder (VGQE), for VQA that reduces this effect. VGQE utilizes both visual and language modalities equally while encoding the question. Hence the question representation itself gets sufficient visual-grounding, and thus reduces the dependency of the model on the language priors. We demonstrate the effect of VGQE on three recent VQA models and achieve state-of-the-art results on the bias-sensitive split of the VQAv2 dataset; VQA-CPv2. Further, unlike the existing bias-reduction techniques, on the standard VQAv2 benchmark, our approach does not drop the accuracy; instead, it improves the performance.



### CheXphoto: 10,000+ Photos and Transformations of Chest X-rays for Benchmarking Deep Learning Robustness
- **Arxiv ID**: http://arxiv.org/abs/2007.06199v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.06199v2)
- **Published**: 2020-07-13 05:37:00+00:00
- **Updated**: 2020-12-11 10:11:00+00:00
- **Authors**: Nick A. Phillips, Pranav Rajpurkar, Mark Sabini, Rayan Krishnan, Sharon Zhou, Anuj Pareek, Nguyet Minh Phu, Chris Wang, Mudit Jain, Nguyen Duong Du, Steven QH Truong, Andrew Y. Ng, Matthew P. Lungren
- **Comment**: None
- **Journal**: None
- **Summary**: Clinical deployment of deep learning algorithms for chest x-ray interpretation requires a solution that can integrate into the vast spectrum of clinical workflows across the world. An appealing approach to scaled deployment is to leverage the ubiquity of smartphones by capturing photos of x-rays to share with clinicians using messaging services like WhatsApp. However, the application of chest x-ray algorithms to photos of chest x-rays requires reliable classification in the presence of artifacts not typically encountered in digital x-rays used to train machine learning models. We introduce CheXphoto, a dataset of smartphone photos and synthetic photographic transformations of chest x-rays sampled from the CheXpert dataset. To generate CheXphoto we (1) automatically and manually captured photos of digital x-rays under different settings, and (2) generated synthetic transformations of digital x-rays targeted to make them look like photos of digital x-rays and x-ray films. We release this dataset as a resource for testing and improving the robustness of deep learning algorithms for automated chest x-ray interpretation on smartphone photos of chest x-rays.



### Hardware Implementation of Hyperbolic Tangent Function using Catmull-Rom Spline Interpolation
- **Arxiv ID**: http://arxiv.org/abs/2007.13516v1
- **DOI**: None
- **Categories**: **cs.AR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.13516v1)
- **Published**: 2020-07-13 07:11:59+00:00
- **Updated**: 2020-07-13 07:11:59+00:00
- **Authors**: Mahesh Chandra
- **Comment**: 4 pages, 3 figures. arXiv admin note: substantial text overlap with
  arXiv:2007.11976
- **Journal**: None
- **Summary**: Deep neural networks yield the state of the art results in many computer vision and human machine interface tasks such as object recognition, speech recognition etc. Since, these networks are computationally expensive, customized accelerators are designed for achieving the required performance at lower cost and power. One of the key building blocks of these neural networks is non-linear activation function such as sigmoid, hyperbolic tangent (tanh), and ReLU. A low complexity accurate hardware implementation of the activation function is required to meet the performance and area targets of the neural network accelerators. This paper presents an implementation of tanh function using the Catmull-Rom spline interpolation. State of the art results are achieved using this method with comparatively smaller logic area.



### Comparative Analysis of Polynomial and Rational Approximations of Hyperbolic Tangent Function for VLSI Implementation
- **Arxiv ID**: http://arxiv.org/abs/2007.11976v2
- **DOI**: None
- **Categories**: **cs.AR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.11976v2)
- **Published**: 2020-07-13 07:31:02+00:00
- **Updated**: 2020-09-24 13:41:18+00:00
- **Authors**: Mahesh Chandra
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks yield the state-of-the-art results in many computer vision and human machine interface applications such as object detection, speech recognition etc. Since, these networks are computationally expensive, customized accelerators are designed for achieving the required performance at lower cost and power. One of the key building blocks of these neural networks is non-linear activation function such as sigmoid, hyperbolic tangent (tanh), and ReLU. A low complexity accurate hardware implementation of the activation function is required to meet the performance and area targets of the neural network accelerators. Even though, various methods and implementations of tanh activation function have been published, a comparative study is missing. This paper presents comparative analysis of polynomial and rational methods and their hardware implementation.



### Hierarchical Dynamic Filtering Network for RGB-D Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2007.06227v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.06227v3)
- **Published**: 2020-07-13 07:59:55+00:00
- **Updated**: 2020-07-16 09:15:49+00:00
- **Authors**: Youwei Pang, Lihe Zhang, Xiaoqi Zhao, Huchuan Lu
- **Comment**: Accepted by ECCV 2020
- **Journal**: None
- **Summary**: The main purpose of RGB-D salient object detection (SOD) is how to better integrate and utilize cross-modal fusion information. In this paper, we explore these issues from a new perspective. We integrate the features of different modalities through densely connected structures and use their mixed features to generate dynamic filters with receptive fields of different sizes. In the end, we implement a kind of more flexible and efficient multi-scale cross-modal feature processing, i.e. dynamic dilated pyramid module. In order to make the predictions have sharper edges and consistent saliency regions, we design a hybrid enhanced loss function to further optimize the results. This loss function is also validated to be effective in the single-modal RGB SOD task. In terms of six metrics, the proposed method outperforms the existing twelve methods on eight challenging benchmark datasets. A large number of experiments verify the effectiveness of the proposed module and loss function. Our code, model and results are available at \url{https://github.com/lartpang/HDFNet}.



### Location-Aware Box Reasoning for Anchor-Based Single-Shot Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2007.06233v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.06233v1)
- **Published**: 2020-07-13 08:24:41+00:00
- **Updated**: 2020-07-13 08:24:41+00:00
- **Authors**: Wenchi Ma, Kaidong Li, Guanghui Wang
- **Comment**: None
- **Journal**: None
- **Summary**: In the majority of object detection frameworks, the confidence of instance classification is used as the quality criterion of predicted bounding boxes, like the confidence-based ranking in non-maximum suppression (NMS). However, the quality of bounding boxes, indicating the spatial relations, is not only correlated with the classification scores. Compared with the region proposal network (RPN) based detectors, single-shot object detectors suffer the box quality as there is a lack of pre-selection of box proposals. In this paper, we aim at single-shot object detectors and propose a location-aware anchor-based reasoning (LAAR) for the bounding boxes. LAAR takes both the location and classification confidences into consideration for the quality evaluation of bounding boxes. We introduce a novel network block to learn the relative location between the anchors and the ground truths, denoted as a localization score, which acts as a location reference during the inference stage. The proposed localization score leads to an independent regression branch and calibrates the bounding box quality by scoring the predicted localization score so that the best-qualified bounding boxes can be picked up in NMS. Experiments on MS COCO and PASCAL VOC benchmarks demonstrate that the proposed location-aware framework enhances the performances of current anchor-based single-shot object detection frameworks and yields consistent and robust detection results.



### FADACS: A Few-shot Adversarial Domain Adaptation Architecture for Context-Aware Parking Availability Sensing
- **Arxiv ID**: http://arxiv.org/abs/2007.08551v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.SY, eess.SP, eess.SY
- **Links**: [PDF](http://arxiv.org/pdf/2007.08551v2)
- **Published**: 2020-07-13 08:25:26+00:00
- **Updated**: 2021-01-28 01:05:04+00:00
- **Authors**: Wei Shao, Sichen Zhao, Zhen Zhang, Shiyu Wang, Mohammad Saiedur Rahaman, Andy Song, Flora Dilys Salim
- **Comment**: 9 pages, 3 gifures, 7 tables, Revised version
- **Journal**: None
- **Summary**: Existing research on parking availability sensing mainly relies on extensive contextual and historical information. In practice, the availability of such information is a challenge as it requires continuous collection of sensory signals. In this study, we design an end-to-end transfer learning framework for parking availability sensing to predict parking occupancy in areas in which the parking data is insufficient to feed into data-hungry models. This framework overcomes two main challenges: 1) many real-world cases cannot provide enough data for most existing data-driven models, and 2) it is difficult to merge sensor data and heterogeneous contextual information due to the differing urban fabric and spatial characteristics. Our work adopts a widely-used concept, adversarial domain adaptation, to predict the parking occupancy in an area without abundant sensor data by leveraging data from other areas with similar features. In this paper, we utilise more than 35 million parking data records from sensors placed in two different cities, one a city centre and the other a coastal tourist town. We also utilise heterogeneous spatio-temporal contextual information from external resources, including weather and points of interest. We quantify the strength of our proposed framework in different cases and compare it to the existing data-driven approaches. The results show that the proposed framework is comparable to existing state-of-the-art methods and also provide some valuable insights on parking availability prediction.



### Expert Training: Task Hardness Aware Meta-Learning for Few-Shot Classification
- **Arxiv ID**: http://arxiv.org/abs/2007.06240v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.06240v1)
- **Published**: 2020-07-13 08:49:00+00:00
- **Updated**: 2020-07-13 08:49:00+00:00
- **Authors**: Yucan Zhou, Yu Wang, Jianfei Cai, Yu Zhou, Qinghua Hu, Weiping Wang
- **Comment**: 9 pages, 6 figures
- **Journal**: None
- **Summary**: Deep neural networks are highly effective when a large number of labeled samples are available but fail with few-shot classification tasks. Recently, meta-learning methods have received much attention, which train a meta-learner on massive additional tasks to gain the knowledge to instruct the few-shot classification. Usually, the training tasks are randomly sampled and performed indiscriminately, often making the meta-learner stuck into a bad local optimum. Some works in the optimization of deep neural networks have shown that a better arrangement of training data can make the classifier converge faster and perform better. Inspired by this idea, we propose an easy-to-hard expert meta-training strategy to arrange the training tasks properly, where easy tasks are preferred in the first phase, then, hard tasks are emphasized in the second phase. A task hardness aware module is designed and integrated into the training procedure to estimate the hardness of a task based on the distinguishability of its categories. In addition, we explore multiple hardness measurements including the semantic relation, the pairwise Euclidean distance, the Hausdorff distance, and the Hilbert-Schmidt independence criterion. Experimental results on the miniImageNet and tieredImageNetSketch datasets show that the meta-learners can obtain better results with our expert training strategy.



### RATT: Recurrent Attention to Transient Tasks for Continual Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/2007.06271v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.06271v2)
- **Published**: 2020-07-13 09:52:37+00:00
- **Updated**: 2020-10-29 11:20:16+00:00
- **Authors**: Riccardo Del Chiaro, Bartłomiej Twardowski, Andrew D. Bagdanov, Joost van de Weijer
- **Comment**: 9 pages, 4 figures, 8 supplementary pages, 12 supplementary images,
  to be published in NeurIPS 2020
- **Journal**: None
- **Summary**: Research on continual learning has led to a variety of approaches to mitigating catastrophic forgetting in feed-forward classification networks. Until now surprisingly little attention has been focused on continual learning of recurrent models applied to problems like image captioning. In this paper we take a systematic look at continual learning of LSTM-based models for image captioning. We propose an attention-based approach that explicitly accommodates the transient nature of vocabularies in continual image captioning tasks -- i.e. that task vocabularies are not disjoint. We call our method Recurrent Attention to Transient Tasks (RATT), and also show how to adapt continual learning approaches based on weight egularization and knowledge distillation to recurrent continual learning problems. We apply our approaches to incremental image captioning problem on two new continual learning benchmarks we define using the MS-COCO and Flickr30 datasets. Our results demonstrate that RATT is able to sequentially learn five captioning tasks while incurring no forgetting of previously learned ones.



### Screen Tracking for Clinical Translation of Live Ultrasound Image Analysis Methods
- **Arxiv ID**: http://arxiv.org/abs/2007.06272v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, 68T45
- **Links**: [PDF](http://arxiv.org/pdf/2007.06272v1)
- **Published**: 2020-07-13 09:53:20+00:00
- **Updated**: 2020-07-13 09:53:20+00:00
- **Authors**: Simona Treivase, Alberto Gomez, Jacqueline Matthew, Emily Skelton, Julia A. Schnabel, Nicolas Toussaint
- **Comment**: None
- **Journal**: None
- **Summary**: Ultrasound (US) imaging is one of the most commonly used non-invasive imaging techniques. However, US image acquisition requires simultaneous guidance of the transducer and interpretation of images, which is a highly challenging task that requires years of training. Despite many recent developments in intra-examination US image analysis, the results are not easy to translate to a clinical setting. We propose a generic framework to extract the US images and superimpose the results of an analysis task, without any need for physical connection or alteration to the US system. The proposed method captures the US image by tracking the screen with a camera fixed at the sonographer's view point and reformats the captured image to the right aspect ratio, in 87.66 +- 3.73ms on average.   It is hypothesized that this would enable to input such retrieved image into an image processing pipeline to extract information that can help improve the examination. This information could eventually be projected back to the sonographer's field of view in real time using, for example, an augmented reality (AR) headset.



### OpenStreetMap: Challenges and Opportunities in Machine Learning and Remote Sensing
- **Arxiv ID**: http://arxiv.org/abs/2007.06277v1
- **DOI**: 10.1109/MGRS.2020.2994107
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.06277v1)
- **Published**: 2020-07-13 09:58:14+00:00
- **Updated**: 2020-07-13 09:58:14+00:00
- **Authors**: John Vargas, Shivangi Srivastava, Devis Tuia, Alexandre Falcao
- **Comment**: None
- **Journal**: None
- **Summary**: OpenStreetMap (OSM) is a community-based, freely available, editable map service that was created as an alternative to authoritative ones. Given that it is edited mainly by volunteers with different mapping skills, the completeness and quality of its annotations are heterogeneous across different geographical locations. Despite that, OSM has been widely used in several applications in {Geosciences}, Earth Observation and environmental sciences. In this work, we present a review of recent methods based on machine learning to improve and use OSM data. Such methods aim either 1) at improving the coverage and quality of OSM layers, typically using GIS and remote sensing technologies, or 2) at using the existing OSM layers to train models based on image data to serve applications like navigation or {land use} classification. We believe that OSM (as well as other sources of open land maps) can change the way we interpret remote sensing data and that the synergy with machine learning can scale participatory map making and its quality to the level needed to serve global and up-to-date land mapping.



### Dual-Teacher: Integrating Intra-domain and Inter-domain Teachers for Annotation-efficient Cardiac Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2007.06279v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.06279v1)
- **Published**: 2020-07-13 10:00:44+00:00
- **Updated**: 2020-07-13 10:00:44+00:00
- **Authors**: Kang Li, Shujun Wang, Lequan Yu, Pheng-Ann Heng
- **Comment**: Accepted at MICCAI 2020
- **Journal**: None
- **Summary**: Medical image annotations are prohibitively time-consuming and expensive to obtain. To alleviate annotation scarcity, many approaches have been developed to efficiently utilize extra information, e.g.,semi-supervised learning further exploring plentiful unlabeled data, domain adaptation including multi-modality learning and unsupervised domain adaptation resorting to the prior knowledge from additional modality. In this paper, we aim to investigate the feasibility of simultaneously leveraging abundant unlabeled data and well-established cross-modality data for annotation-efficient medical image segmentation. To this end, we propose a novel semi-supervised domain adaptation approach, namely Dual-Teacher, where the student model not only learns from labeled target data (e.g., CT), but also explores unlabeled target data and labeled source data (e.g., MR) by two teacher models. Specifically, the student model learns the knowledge of unlabeled target data from intra-domain teacher by encouraging prediction consistency, as well as the shape priors embedded in labeled source data from inter-domain teacher via knowledge distillation. Consequently, the student model can effectively exploit the information from all three data resources and comprehensively integrate them to achieve improved performance. We conduct extensive experiments on MM-WHS 2017 dataset and demonstrate that our approach is able to concurrently utilize unlabeled data and cross-modality data with superior performance, outperforming semi-supervised learning and domain adaptation methods with a large margin.



### Fusing Motion Patterns and Key Visual Information for Semantic Event Recognition in Basketball Videos
- **Arxiv ID**: http://arxiv.org/abs/2007.06288v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.06288v1)
- **Published**: 2020-07-13 10:15:44+00:00
- **Updated**: 2020-07-13 10:15:44+00:00
- **Authors**: Lifang Wu, Zhou Yang, Qi Wang, Meng Jian, Boxuan Zhao, Junchi Yan, Chang Wen Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Many semantic events in team sport activities e.g. basketball often involve both group activities and the outcome (score or not). Motion patterns can be an effective means to identify different activities. Global and local motions have their respective emphasis on different activities, which are difficult to capture from the optical flow due to the mixture of global and local motions. Hence it calls for a more effective way to separate the global and local motions. When it comes to the specific case for basketball game analysis, the successful score for each round can be reliably detected by the appearance variation around the basket. Based on the observations, we propose a scheme to fuse global and local motion patterns (MPs) and key visual information (KVI) for semantic event recognition in basketball videos. Firstly, an algorithm is proposed to estimate the global motions from the mixed motions based on the intrinsic property of camera adjustments. And the local motions could be obtained from the mixed and global motions. Secondly, a two-stream 3D CNN framework is utilized for group activity recognition over the separated global and local motion patterns. Thirdly, the basket is detected and its appearance features are extracted through a CNN structure. The features are utilized to predict the success or failure. Finally, the group activity recognition and success/failure prediction results are integrated using the kronecker product for event recognition. Experiments on NCAA dataset demonstrate that the proposed method obtains state-of-the-art performance.



### Accelerated FBP for computed tomography image reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2007.06289v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.06289v1)
- **Published**: 2020-07-13 10:16:54+00:00
- **Updated**: 2020-07-13 10:16:54+00:00
- **Authors**: Anastasiya Dolmatova, Marina Chukalina, Dmitry Nikolaev
- **Comment**: None
- **Journal**: None
- **Summary**: Filtered back projection (FBP) is a commonly used technique in tomographic image reconstruction demonstrating acceptable quality. The classical direct implementations of this algorithm require the execution of $\Theta(N^3)$ operations, where $N$ is the linear size of the 2D slice. Recent approaches including reconstruction via the Fourier slice theorem require $\Theta(N^2\log N)$ multiplication operations. In this paper, we propose a novel approach that reduces the computational complexity of the algorithm to $\Theta(N^2\log N)$ addition operations avoiding Fourier space. For speeding up the convolution, ramp filter is approximated by a pair of causal and anticausal recursive filters, also known as Infinite Impulse Response filters. The back projection is performed with the fast discrete Hough transform. Experimental results on simulated data demonstrate the efficiency of the proposed approach.



### Knowledge Graph Driven Approach to Represent Video Streams for Spatiotemporal Event Pattern Matching in Complex Event Processing
- **Arxiv ID**: http://arxiv.org/abs/2007.06292v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.DB, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2007.06292v1)
- **Published**: 2020-07-13 10:20:58+00:00
- **Updated**: 2020-07-13 10:20:58+00:00
- **Authors**: Piyush Yadav, Dhaval Salwala, Edward Curry
- **Comment**: 31 pages, 14 Figures, Publication accepted in International Journal
  of Graph Computing
- **Journal**: None
- **Summary**: Complex Event Processing (CEP) is an event processing paradigm to perform real-time analytics over streaming data and match high-level event patterns. Presently, CEP is limited to process structured data stream. Video streams are complicated due to their unstructured data model and limit CEP systems to perform matching over them. This work introduces a graph-based structure for continuous evolving video streams, which enables the CEP system to query complex video event patterns. We propose the Video Event Knowledge Graph (VEKG), a graph driven representation of video data. VEKG models video objects as nodes and their relationship interaction as edges over time and space. It creates a semantic knowledge representation of video data derived from the detection of high-level semantic concepts from the video using an ensemble of deep learning models. A CEP-based state optimization - VEKG-Time Aggregated Graph (VEKG-TAG) is proposed over VEKG representation for faster event detection. VEKG-TAG is a spatiotemporal graph aggregation method that provides a summarized view of the VEKG graph over a given time length. We defined a set of nine event pattern rules for two domains (Activity Recognition and Traffic Management), which act as a query and applied over VEKG graphs to discover complex event patterns. To show the efficacy of our approach, we performed extensive experiments over 801 video clips across 10 datasets. The proposed VEKG approach was compared with other state-of-the-art methods and was able to detect complex event patterns over videos with F-Score ranging from 0.44 to 0.90. In the given experiments, the optimized VEKG-TAG was able to reduce 99% and 93% of VEKG nodes and edges, respectively, with 5.19X faster search time, achieving sub-second median latency of 4-20 milliseconds.



### Stutter Diagnosis and Therapy System Based on Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2007.08003v1
- **DOI**: None
- **Categories**: **cs.CY**, cs.CV, cs.LG, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2007.08003v1)
- **Published**: 2020-07-13 10:24:02+00:00
- **Updated**: 2020-07-13 10:24:02+00:00
- **Authors**: Gresha Bhatia, Binoy Saha, Mansi Khamkar, Ashish Chandwani, Reshma Khot
- **Comment**: About stutter classification, severity diagnosis and therapy
  recommendation
- **Journal**: None
- **Summary**: Stuttering, also called stammering, is a communication disorder that breaks the continuity of the speech. This program of work is an attempt to develop automatic recognition procedures to assess stuttered dysfluencies and use these assessments to filter out speech therapies for an individual. Stuttering may be in the form of repetitions, prolongations or abnormal stoppages of sounds and syllables. Our system aims to help stutterers by diagnosing the severity and type of stutter and also by suggesting appropriate therapies for practice by learning the correlation between stutter descriptors and the effectiveness of speech therapies on them. This paper focuses on the implementation of a stutter diagnosis agent using Gated Recurrent CNN on MFCC audio features and therapy recommendation agent using SVM. It also presents the results obtained and various key findings of the system developed.



### Seeing eye-to-eye? A comparison of object recognition performance in humans and deep convolutional neural networks under image manipulation
- **Arxiv ID**: http://arxiv.org/abs/2007.06294v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/2007.06294v2)
- **Published**: 2020-07-13 10:26:30+00:00
- **Updated**: 2020-12-13 11:08:45+00:00
- **Authors**: Leonard E. van Dyck, Walter R. Gruber
- **Comment**: 19 pages, 7 figures, 3 tables
- **Journal**: None
- **Summary**: For a considerable time, deep convolutional neural networks (DCNNs) have reached human benchmark performance in object recognition. On that account, computational neuroscience and the field of machine learning have started to attribute numerous similarities and differences to artificial and biological vision. This study aims towards a behavioral comparison of visual core object recognition performance between humans and feedforward neural networks in a classification learning paradigm on an ImageNet data set. For this purpose, human participants (n = 65) competed in an online experiment against different feedforward DCNNs. The designed approach based on a typical learning process of seven different monkey categories included a training and validation phase with natural examples, as well as a testing phase with novel, unexperienced shape and color manipulations. Analyses of accuracy revealed that humans not only outperform DCNNs on all conditions, but also display significantly greater robustness towards shape and most notably color alterations. Furthermore, a precise examination of behavioral patterns highlights these findings by revealing independent classification errors between the groups. The obtained results show that humans contrast strongly with artificial feedforward architectures when it comes to visual core object recognition of manipulated images. In general, these findings are in line with a growing body of literature, that hints towards recurrence as a crucial factor for adequate generalization abilities.



### Part-aware Prototype Network for Few-shot Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2007.06309v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.06309v3)
- **Published**: 2020-07-13 11:03:09+00:00
- **Updated**: 2022-12-02 07:14:06+00:00
- **Authors**: Yongfei Liu, Xiangyi Zhang, Songyang Zhang, Xuming He
- **Comment**: ECCV-2020
- **Journal**: None
- **Summary**: Few-shot semantic segmentation aims to learn to segment new object classes with only a few annotated examples, which has a wide range of real-world applications. Most existing methods either focus on the restrictive setting of one-way few-shot segmentation or suffer from incomplete coverage of object regions. In this paper, we propose a novel few-shot semantic segmentation framework based on the prototype representation. Our key idea is to decompose the holistic class representation into a set of part-aware prototypes, capable of capturing diverse and fine-grained object features. In addition, we propose to leverage unlabeled data to enrich our part-aware prototypes, resulting in better modeling of intra-class variations of semantic objects. We develop a novel graph neural network model to generate and enhance the proposed part-aware prototypes based on labeled and unlabeled images. Extensive experimental evaluations on two benchmarks show that our method outperforms the prior art with a sizable margin.



### Domain aware medical image classifier interpretation by counterfactual impact analysis
- **Arxiv ID**: http://arxiv.org/abs/2007.06312v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, I.2.6
- **Links**: [PDF](http://arxiv.org/pdf/2007.06312v2)
- **Published**: 2020-07-13 11:11:17+00:00
- **Updated**: 2020-10-01 16:55:12+00:00
- **Authors**: Dimitrios Lenis, David Major, Maria Wimmer, Astrid Berg, Gert Sluiter, Katja Bühler
- **Comment**: Accepted for publication at International Conference on Medical Image
  Computing and Computer Assisted Intervention (MICCAI) 2020.This version
  differs from the published conference version only in a funding agencies
  name, and additional clarifying changes and references in figure 3
- **Journal**: None
- **Summary**: The success of machine learning methods for computer vision tasks has driven a surge in computer assisted prediction for medicine and biology. Based on a data-driven relationship between input image and pathological classification, these predictors deliver unprecedented accuracy. Yet, the numerous approaches trying to explain the causality of this learned relationship have fallen short: time constraints, coarse, diffuse and at times misleading results, caused by the employment of heuristic techniques like Gaussian noise and blurring, have hindered their clinical adoption.   In this work, we discuss and overcome these obstacles by introducing a neural-network based attribution method, applicable to any trained predictor. Our solution identifies salient regions of an input image in a single forward-pass by measuring the effect of local image-perturbations on a predictor's score. We replace heuristic techniques with a strong neighborhood conditioned inpainting approach, avoiding anatomically implausible, hence adversarial artifacts. We evaluate on public mammography data and compare against existing state-of-the-art methods. Furthermore, we exemplify the approach's generalizability by demonstrating results on chest X-rays. Our solution shows, both quantitatively and qualitatively, a significant reduction of localization ambiguity and clearer conveying results, without sacrificing time efficiency.



### IntegralAction: Pose-driven Feature Integration for Robust Human Action Recognition in Videos
- **Arxiv ID**: http://arxiv.org/abs/2007.06317v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.06317v2)
- **Published**: 2020-07-13 11:24:48+00:00
- **Updated**: 2021-04-15 07:29:32+00:00
- **Authors**: Gyeongsik Moon, Heeseung Kwon, Kyoung Mu Lee, Minsu Cho
- **Comment**: Published at CVPRW 2021
- **Journal**: None
- **Summary**: Most current action recognition methods heavily rely on appearance information by taking an RGB sequence of entire image regions as input. While being effective in exploiting contextual information around humans, e.g., human appearance and scene category, they are easily fooled by out-of-context action videos where the contexts do not exactly match with target actions. In contrast, pose-based methods, which take a sequence of human skeletons only as input, suffer from inaccurate pose estimation or ambiguity of human pose per se. Integrating these two approaches has turned out to be non-trivial; training a model with both appearance and pose ends up with a strong bias towards appearance and does not generalize well to unseen videos. To address this problem, we propose to learn pose-driven feature integration that dynamically combines appearance and pose streams by observing pose features on the fly. The main idea is to let the pose stream decide how much and which appearance information is used in integration based on whether the given pose information is reliable or not. We show that the proposed IntegralAction achieves highly robust performance across in-context and out-of-context action video datasets. The codes are available in https://github.com/mks0601/IntegralAction_RELEASE.



### Experimental results on palmvein-based personal recognition by multi-snapshot fusion of textural features
- **Arxiv ID**: http://arxiv.org/abs/2008.00821v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2008.00821v2)
- **Published**: 2020-07-13 11:34:46+00:00
- **Updated**: 2020-10-17 12:00:52+00:00
- **Authors**: Mohanad Abukmeil, Gian Luca Marcialis
- **Comment**: 22 pages, 5 figures
- **Journal**: None
- **Summary**: In this paper, we investigate multiple snapshot fusion of textural features for palmvein recognition including identification and verification. Although the literature proposed several approaches for palmvein recognition, the palmvein performance is still affected by identification and verification errors. As well-known, palmveins are usually described by line-based methods which enhance the vein flow. This is claimed to be unique from person to person. However, palmvein images are also characterized by texture that can be pointed out by textural features, which relies on recent and efficient hand-crafted algorithms such as Local Binary Patterns, Local Phase Quantization, Local Tera Pattern, Local directional Pattern, and Binarized Statistical Image Features (LBP, LPQ, LTP, LDP and BSIF, respectively), among others. Finally, they can be easily managed at feature-level fusion, when more than one sample can be acquired for recognition. Therefore, multi-snapshot fusion can be adopted for exploiting these features complementarity. Our goal in this paper is to show that this is confirmed for palmvein recognition, thus allowing to achieve very high recognition rates on a well-known benchmark data set.



### Active Crowd Counting with Limited Supervision
- **Arxiv ID**: http://arxiv.org/abs/2007.06334v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.06334v2)
- **Published**: 2020-07-13 12:07:25+00:00
- **Updated**: 2020-07-14 21:28:20+00:00
- **Authors**: Zhen Zhao, Miaojing Shi, Xiaoxiao Zhao, Li Li
- **Comment**: ECCV2020 camera ready
- **Journal**: None
- **Summary**: To learn a reliable people counter from crowd images, head center annotations are normally required. Annotating head centers is however a laborious and tedious process in dense crowds. In this paper, we present an active learning framework which enables accurate crowd counting with limited supervision: given a small labeling budget, instead of randomly selecting images to annotate, we first introduce an active labeling strategy to annotate the most informative images in the dataset and learn the counting model upon them. The process is repeated such that in every cycle we select the samples that are diverse in crowd density and dissimilar to previous selections. In the last cycle when the labeling budget is met, the large amount of unlabeled data are also utilized: a distribution classifier is introduced to align the labeled data with unlabeled data; furthermore, we propose to mix up the distribution labels and latent representations of data in the network to particularly improve the distribution alignment in-between training samples. We follow the popular density estimation pipeline for crowd counting. Extensive experiments are conducted on standard benchmarks i.e. ShanghaiTech, UCF CC 50, MAll, TRANCOS, and DCC. By annotating limited number of images (e.g. 10% of the dataset), our method reaches levels of performance not far from the state of the art which utilize full annotations of the dataset.



### DeU-Net: Deformable U-Net for 3D Cardiac MRI Video Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2007.06341v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.06341v1)
- **Published**: 2020-07-13 12:19:03+00:00
- **Updated**: 2020-07-13 12:19:03+00:00
- **Authors**: Shunjie Dong, Jinlong Zhao, Maojun Zhang, Zhengxue Shi, Jianing Deng, Yiyu Shi, Mei Tian, Cheng Zhuo
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic segmentation of cardiac magnetic resonance imaging (MRI) facilitates efficient and accurate volume measurement in clinical applications. However, due to anisotropic resolution and ambiguous border (e.g., right ventricular endocardium), existing methods suffer from the degradation of accuracy and robustness in 3D cardiac MRI video segmentation. In this paper, we propose a novel Deformable U-Net (DeU-Net) to fully exploit spatio-temporal information from 3D cardiac MRI video, including a Temporal Deformable Aggregation Module (TDAM) and a Deformable Global Position Attention (DGPA) network. First, the TDAM takes a cardiac MRI video clip as input with temporal information extracted by an offset prediction network. Then we fuse extracted temporal information via a temporal aggregation deformable convolution to produce fused feature maps. Furthermore, to aggregate meaningful features, we devise the DGPA network by employing deformable attention U-Net, which can encode a wider range of multi-dimensional contextual information into global and local features. Experimental results show that our DeU-Net achieves the state-of-the-art performance on commonly used evaluation metrics, especially for cardiac marginal information (ASSD and HD).



### End-to-End Multi-Object Tracking with Global Response Map
- **Arxiv ID**: http://arxiv.org/abs/2007.06344v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.06344v1)
- **Published**: 2020-07-13 12:30:49+00:00
- **Updated**: 2020-07-13 12:30:49+00:00
- **Authors**: Xingyu Wan, Jiakai Cao, Sanping Zhou, Jinjun Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Most existing Multi-Object Tracking (MOT) approaches follow the Tracking-by-Detection paradigm and the data association framework where objects are firstly detected and then associated. Although deep-learning based method can noticeably improve the object detection performance and also provide good appearance features for cross-frame association, the framework is not completely end-to-end, and therefore the computation is huge while the performance is limited. To address the problem, we present a completely end-to-end approach that takes image-sequence/video as input and outputs directly the located and tracked objects of learned types. Specifically, with our introduced multi-object representation strategy, a global response map can be accurately generated over frames, from which the trajectory of each tracked object can be easily picked up, just like how a detector inputs an image and outputs the bounding boxes of each detected object. The proposed model is fast and accurate. Experimental results based on the MOT16 and MOT17 benchmarks show that our proposed on-line tracker achieved state-of-the-art performance on several tracking metrics.



### Whitening for Self-Supervised Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2007.06346v5
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.06346v5)
- **Published**: 2020-07-13 12:33:25+00:00
- **Updated**: 2021-05-14 15:10:06+00:00
- **Authors**: Aleksandr Ermolov, Aliaksandr Siarohin, Enver Sangineto, Nicu Sebe
- **Comment**: ICML 2021
- **Journal**: None
- **Summary**: Most of the current self-supervised representation learning (SSL) methods are based on the contrastive loss and the instance-discrimination task, where augmented versions of the same image instance ("positives") are contrasted with instances extracted from other images ("negatives"). For the learning to be effective, many negatives should be compared with a positive pair, which is computationally demanding. In this paper, we propose a different direction and a new loss function for SSL, which is based on the whitening of the latent-space features. The whitening operation has a "scattering" effect on the batch samples, avoiding degenerate solutions where all the sample representations collapse to a single point. Our solution does not require asymmetric networks and it is conceptually simple. Moreover, since negatives are not needed, we can extract multiple positive pairs from the same image instance. The source code of the method and of all the experiments is available at: https://github.com/htdt/self-supervised.



### Multiple Sound Sources Localization from Coarse to Fine
- **Arxiv ID**: http://arxiv.org/abs/2007.06355v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.06355v2)
- **Published**: 2020-07-13 12:59:40+00:00
- **Updated**: 2020-07-14 13:38:52+00:00
- **Authors**: Rui Qian, Di Hu, Heinrich Dinkel, Mengyue Wu, Ning Xu, Weiyao Lin
- **Comment**: to appear in ECCV 2020
- **Journal**: None
- **Summary**: How to visually localize multiple sound sources in unconstrained videos is a formidable problem, especially when lack of the pairwise sound-object annotations. To solve this problem, we develop a two-stage audiovisual learning framework that disentangles audio and visual representations of different categories from complex scenes, then performs cross-modal feature alignment in a coarse-to-fine manner. Our model achieves state-of-the-art results on public dataset of localization, as well as considerable performance on multi-source sound localization in complex scenes. We then employ the localization results for sound separation and obtain comparable performance to existing methods. These outcomes demonstrate our model's ability in effectively aligning sounds with specific visual sources. Code is available at https://github.com/shvdiwnkozbw/Multi-Source-Sound-Localization



### Disentanglement of Color and Shape Representations for Continual Learning
- **Arxiv ID**: http://arxiv.org/abs/2007.06356v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.06356v1)
- **Published**: 2020-07-13 13:05:45+00:00
- **Updated**: 2020-07-13 13:05:45+00:00
- **Authors**: David Berga, Marc Masana, Joost Van de Weijer
- **Comment**: Accepted at CL-ICML 2020
- **Journal**: None
- **Summary**: We hypothesize that disentangled feature representations suffer less from catastrophic forgetting. As a case study we perform explicit disentanglement of color and shape, by adjusting the network architecture. We tested classification accuracy and forgetting in a task-incremental setting with Oxford-102 Flowers dataset. We combine our method with Elastic Weight Consolidation, Learning without Forgetting, Synaptic Intelligence and Memory Aware Synapses, and show that feature disentanglement positively impacts continual learning performance.



### On uncertainty estimation in active learning for image segmentation
- **Arxiv ID**: http://arxiv.org/abs/2007.06364v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.06364v1)
- **Published**: 2020-07-13 13:20:32+00:00
- **Updated**: 2020-07-13 13:20:32+00:00
- **Authors**: Bo Li, Tommy Sonne Alstrøm
- **Comment**: Presented at ICML 2020 Workshop on Uncertainty & Robustness in Deep
  Learning
- **Journal**: None
- **Summary**: Uncertainty estimation is important for interpreting the trustworthiness of machine learning models in many applications. This is especially critical in the data-driven active learning setting where the goal is to achieve a certain accuracy with minimum labeling effort. In such settings, the model learns to select the most informative unlabeled samples for annotation based on its estimated uncertainty. The highly uncertain predictions are assumed to be more informative for improving model performance. In this paper, we explore uncertainty calibration within an active learning framework for medical image segmentation, an area where labels often are scarce. Various uncertainty estimation methods and acquisition strategies (regions and full images) are investigated. We observe that selecting regions to annotate instead of full images leads to more well-calibrated models. Additionally, we experimentally show that annotating regions can cut 50% of pixels that need to be labeled by humans compared to annotating full images.



### Learning and Exploiting Interclass Visual Correlations for Medical Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2007.06371v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.06371v1)
- **Published**: 2020-07-13 13:31:38+00:00
- **Updated**: 2020-07-13 13:31:38+00:00
- **Authors**: Dong Wei, Shilei Cao, Kai Ma, Yefeng Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural network-based medical image classifications often use "hard" labels for training, where the probability of the correct category is 1 and those of others are 0. However, these hard targets can drive the networks over-confident about their predictions and prone to overfit the training data, affecting model generalization and adaption. Studies have shown that label smoothing and softening can improve classification performance. Nevertheless, existing approaches are either non-data-driven or limited in applicability. In this paper, we present the Class-Correlation Learning Network (CCL-Net) to learn interclass visual correlations from given training data, and produce soft labels to help with classification tasks. Instead of letting the network directly learn the desired correlations, we propose to learn them implicitly via distance metric learning of class-specific embeddings with a lightweight plugin CCL block. An intuitive loss based on a geometrical explanation of correlation is designed for bolstering learning of the interclass correlations. We further present end-to-end training of the proposed CCL block as a plugin head together with the classification backbone while generating soft labels on the fly. Our experimental results on the International Skin Imaging Collaboration 2018 dataset demonstrate effective learning of the interclass correlations from training data, as well as consistent improvements in performance upon several widely used modern network structures with the CCL block.



### Symmetric Dilated Convolution for Surgical Gesture Recognition
- **Arxiv ID**: http://arxiv.org/abs/2007.06373v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.06373v2)
- **Published**: 2020-07-13 13:34:48+00:00
- **Updated**: 2020-07-14 15:19:55+00:00
- **Authors**: Jinglu Zhang, Yinyu Nie, Yao Lyu, Hailin Li, Jian Chang, Xiaosong Yang, Jian Jun Zhang
- **Comment**: Accepted to MICCAI 2020
- **Journal**: None
- **Summary**: Automatic surgical gesture recognition is a prerequisite of intra-operative computer assistance and objective surgical skill assessment. Prior works either require additional sensors to collect kinematics data or have limitations on capturing temporal information from long and untrimmed surgical videos. To tackle these challenges, we propose a novel temporal convolutional architecture to automatically detect and segment surgical gestures with corresponding boundaries only using RGB videos. We devise our method with a symmetric dilation structure bridged by a self-attention module to encode and decode the long-term temporal patterns and establish the frame-to-frame relationship accordingly. We validate the effectiveness of our approach on a fundamental robotic suturing task from the JIGSAWS dataset. The experiment results demonstrate the ability of our method on capturing long-term frame dependencies, which largely outperform the state-of-the-art methods on the frame-wise accuracy up to ~6 points and the F1@50 score ~6 points.



### CenterNet3D: An Anchor Free Object Detector for Point Cloud
- **Arxiv ID**: http://arxiv.org/abs/2007.07214v4
- **DOI**: 10.1109/TITS.2021.3118698
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.07214v4)
- **Published**: 2020-07-13 13:53:56+00:00
- **Updated**: 2021-10-25 14:49:24+00:00
- **Authors**: Guojun Wang, Jian Wu, Bin Tian, Siyu Teng, Long Chen, Dongpu Cao
- **Comment**: 13 pages, 5 figures
- **Journal**: None
- **Summary**: Accurate and fast 3D object detection from point clouds is a key task in autonomous driving. Existing one-stage 3D object detection methods can achieve real-time performance, however, they are dominated by anchor-based detectors which are inefficient and require additional post-processing. In this paper, we eliminate anchors and model an object as a single point--the center point of its bounding box. Based on the center point, we propose an anchor-free CenterNet3D network that performs 3D object detection without anchors. Our CenterNet3D uses keypoint estimation to find center points and directly regresses 3D bounding boxes. However, because inherent sparsity of point clouds, 3D object center points are likely to be in empty space which makes it difficult to estimate accurate boundaries. To solve this issue, we propose an extra corner attention module to enforce the CNN backbone to pay more attention to object boundaries. Besides, considering that one-stage detectors suffer from the discordance between the predicted bounding boxes and corresponding classification confidences, we develop an efficient keypoint-sensitive warping operation to align the confidences to the predicted bounding boxes. Our proposed CenterNet3D is non-maximum suppression free which makes it more efficient and simpler. We evaluate CenterNet3D on the widely used KITTI dataset and more challenging nuScenes dataset. Our method outperforms all state-of-the-art anchor-based one-stage methods and has comparable performance to two-stage methods as well. It has an inference speed of 20 FPS and achieves the best speed and accuracy trade-off. Our source code will be released at https://github.com/wangguojun2018/CenterNet3d.



### Term Revealing: Furthering Quantization at Run Time on Quantized DNNs
- **Arxiv ID**: http://arxiv.org/abs/2007.06389v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.06389v2)
- **Published**: 2020-07-13 14:03:10+00:00
- **Updated**: 2020-07-26 19:24:51+00:00
- **Authors**: H. T. Kung, Bradley McDanel, Sai Qian Zhang
- **Comment**: 13 pages, 19 figures, 4 tables, To appear in Proceedings of the
  International Conference for High Performance Computing, Networking, Storage
  and Analysis (SC), 2020 Update: Revised writing/figures and added more
  references for Section IV Update: Revised Section IV writing/figures and
  added additional references on signed digit representations
- **Journal**: None
- **Summary**: We present a novel technique, called Term Revealing (TR), for furthering quantization at run time for improved performance of Deep Neural Networks (DNNs) already quantized with conventional quantization methods. TR operates on power-of-two terms in binary expressions of values. In computing a dot-product computation, TR dynamically selects a fixed number of largest terms to use from the values of the two vectors in the dot product. By exploiting normal-like weight and data distributions typically present in DNNs, TR has a minimal impact on DNN model performance (i.e., accuracy or perplexity). We use TR to facilitate tightly synchronized processor arrays, such as systolic arrays, for efficient parallel processing. We show an FPGA implementation that can use a small number of control bits to switch between conventional quantization and TR-enabled quantization with a negligible delay. To enhance TR efficiency further, we use a signed digit representation (SDR), as opposed to classic binary encoding with only nonnegative power-of-two terms. To perform conversion from binary to SDR, we develop an efficient encoding method called HESE (Hybrid Encoding for Signed Expressions) that can be performed in one pass looking at only two bits at a time. We evaluate TR with HESE encoded values on an MLP for MNIST, multiple CNNs for ImageNet, and an LSTM for Wikitext-2, and show significant reductions in inference computations (between 3-10x) compared to conventional quantization for the same level of model performance.



### DeepHAZMAT: Hazardous Materials Sign Detection and Segmentation with Restricted Computational Resources
- **Arxiv ID**: http://arxiv.org/abs/2007.06392v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO, cs.SY, eess.IV, eess.SY
- **Links**: [PDF](http://arxiv.org/pdf/2007.06392v2)
- **Published**: 2020-07-13 14:15:37+00:00
- **Updated**: 2020-07-18 12:12:55+00:00
- **Authors**: Amir Sharifi, Ahmadreza Zibaei, Mahdi Rezaei
- **Comment**: None
- **Journal**: None
- **Summary**: One of the most challenging and non-trivial tasks in robot-based rescue operations is the Hazardous Materials or HAZMATs sign detection in the operation field, to prevent further unexpected disasters. Each Hazmat sign has a specific meaning that the rescue robot should detect and interpret it to take a safe action, accordingly. Accurate Hazmat detection and real-time processing are the two most important factors in such robotics applications. Furthermore, we also have to cope with some secondary challenges such as image distortion and restricted CPU and computational resources which are embedded in a rescue robot. In this paper, we propose a CNN-Based pipeline called DeepHAZMAT for detecting and segmenting Hazmats in four steps; 1) optimising the number of input images that are fed into the CNN network, 2) using the YOLOv3-tiny structure to collect the required visual information from the hazardous areas, 3) Hazmat sign segmentation and separation from the background using GrabCut technique, and 4) post-processing the result with morphological operators and convex hull algorithm. In spite of the utilisation of a very limited memory and CPU resources, the experimental results show the proposed method has successfully maintained a better performance in terms of detection-speed and detection-accuracy, compared with the state-of-the-art methods.



### Nested Learning For Multi-Granular Tasks
- **Arxiv ID**: http://arxiv.org/abs/2007.06402v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.06402v1)
- **Published**: 2020-07-13 14:27:14+00:00
- **Updated**: 2020-07-13 14:27:14+00:00
- **Authors**: Raphaël Achddou, J. Matias di Martino, Guillermo Sapiro
- **Comment**: None
- **Journal**: None
- **Summary**: Standard deep neural networks (DNNs) are commonly trained in an end-to-end fashion for specific tasks such as object recognition, face identification, or character recognition, among many examples. This specificity often leads to overconfident models that generalize poorly to samples that are not from the original training distribution. Moreover, such standard DNNs do not allow to leverage information from heterogeneously annotated training data, where for example, labels may be provided with different levels of granularity. Furthermore, DNNs do not produce results with simultaneous different levels of confidence for different levels of detail, they are most commonly an all or nothing approach. To address these challenges, we introduce the concept of nested learning: how to obtain a hierarchical representation of the input such that a coarse label can be extracted first, and sequentially refine this representation, if the sample permits, to obtain successively refined predictions, all of them with the corresponding confidence. We explicitly enforce this behavior by creating a sequence of nested information bottlenecks. Looking at the problem of nested learning from an information theory perspective, we design a network topology with two important properties. First, a sequence of low dimensional (nested) feature embeddings are enforced. Then we show how the explicit combination of nested outputs can improve both the robustness and the accuracy of finer predictions. Experimental results on Cifar-10, Cifar-100, MNIST, Fashion-MNIST, Dbpedia, and Plantvillage demonstrate that nested learning outperforms the same network trained in the standard end-to-end fashion.



### Fashion-IQ 2020 Challenge 2nd Place Team's Solution
- **Arxiv ID**: http://arxiv.org/abs/2007.06404v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.06404v1)
- **Published**: 2020-07-13 14:28:37+00:00
- **Updated**: 2020-07-13 14:28:37+00:00
- **Authors**: Minchul Shin, Yoonjae Cho, Seongwuk Hong
- **Comment**: 4 pages, CVPR 2020 Workshop, Fashion IQ Challenge
- **Journal**: None
- **Summary**: This paper is dedicated to team VAA's approach submitted to the Fashion-IQ challenge in CVPR 2020. Given a pair of the image and the text, we present a novel multimodal composition method, RTIC, that can effectively combine the text and the image modalities into a semantic space. We extract the image and the text features that are encoded by the CNNs and the sequential models (e.g., LSTM or GRU), respectively. To emphasize the meaning of the residual of the feature between the target and candidate, the RTIC is composed of N-blocks with channel-wise attention modules. Then, we add the encoded residual to the feature of the candidate image to obtain a synthesized feature. We also explored an ensemble strategy with variants of models and achieved a significant boost in performance comparing to the best single model. Finally, our approach achieved 2nd place in the Fashion-IQ 2020 Challenge with a test score of 48.02 on the leaderboard.



### Multitask Non-Autoregressive Model for Human Motion Prediction
- **Arxiv ID**: http://arxiv.org/abs/2007.06426v1
- **DOI**: 10.1109/TIP.2020.3038362
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.06426v1)
- **Published**: 2020-07-13 15:00:19+00:00
- **Updated**: 2020-07-13 15:00:19+00:00
- **Authors**: Bin Li, Jian Tian, Zhongfei Zhang, Hailin Feng, Xi Li
- **Comment**: None
- **Journal**: None
- **Summary**: Human motion prediction, which aims at predicting future human skeletons given the past ones, is a typical sequence-to-sequence problem. Therefore, extensive efforts have been continued on exploring different RNN-based encoder-decoder architectures. However, by generating target poses conditioned on the previously generated ones, these models are prone to bringing issues such as error accumulation problem. In this paper, we argue that such issue is mainly caused by adopting autoregressive manner. Hence, a novel Non-auToregressive Model (NAT) is proposed with a complete non-autoregressive decoding scheme, as well as a context encoder and a positional encoding module. More specifically, the context encoder embeds the given poses from temporal and spatial perspectives. The frame decoder is responsible for predicting each future pose independently. The positional encoding module injects positional signal into the model to indicate temporal order. Moreover, a multitask training paradigm is presented for both low-level human skeleton prediction and high-level human action recognition, resulting in the convincing improvement for the prediction task. Our approach is evaluated on Human3.6M and CMU-Mocap benchmarks and outperforms state-of-the-art autoregressive methods.



### Implicit Euler ODE Networks for Single-Image Dehazing
- **Arxiv ID**: http://arxiv.org/abs/2007.06443v1
- **DOI**: None
- **Categories**: **cs.CV**, F.2.2; I.4.3
- **Links**: [PDF](http://arxiv.org/pdf/2007.06443v1)
- **Published**: 2020-07-13 15:27:33+00:00
- **Updated**: 2020-07-13 15:27:33+00:00
- **Authors**: Jiawei Shen, Zhuoyan Li, Lei Yu, Gui-Song Xia, Wen Yang
- **Comment**: 10pages, 10 figures, "for the associate project, see
  https://github.com/Jiawei-Shen?tab=repositories", submitted to CVPR workshop
  "vision for four seasons",
- **Journal**: None
- **Summary**: Deep convolutional neural networks (CNN) have been applied for image dehazing tasks, where the residual network (ResNet) is often adopted as the basic component to avoid the vanishing gradient problem. Recently, many works indicate that the ResNet can be considered as the explicit Euler forward approximation of an ordinary differential equation (ODE). In this paper, we extend the explicit forward approximation to the implicit backward counterpart, which can be realized via a recursive neural network, named IM-block. Given that, we propose an efficient end-to-end multi-level implicit network (MI-Net) for the single image dehazing problem. Moreover, multi-level fusing (MLF) mechanism and residual channel attention block (RCA-block) are adopted to boost performance of our network. Experiments on several dehazing benchmark datasets demonstrate that our method outperforms existing methods and achieves the state-of-the-art performance.



### Automatic Pass Annotation from Soccer VideoStreams Based on Object Detection and LSTM
- **Arxiv ID**: http://arxiv.org/abs/2007.06475v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.06475v1)
- **Published**: 2020-07-13 16:14:41+00:00
- **Updated**: 2020-07-13 16:14:41+00:00
- **Authors**: Danilo Sorano, Fabio Carrara, Paolo Cintia, Fabrizio Falchi, Luca Pappalardo
- **Comment**: None
- **Journal**: None
- **Summary**: Soccer analytics is attracting increasing interest in academia and industry, thanks to the availability of data that describe all the spatio-temporal events that occur in each match. These events (e.g., passes, shots, fouls) are collected by human operators manually, constituting a considerable cost for data providers in terms of time and economic resources. In this paper, we describe PassNet, a method to recognize the most frequent events in soccer, i.e., passes, from video streams. Our model combines a set of artificial neural networks that perform feature extraction from video streams, object detection to identify the positions of the ball and the players, and classification of frame sequences as passes or not passes. We test PassNet on different scenarios, depending on the similarity of conditions to the match used for training. Our results show good classification results and significant improvement in the accuracy of pass detection with respect to baseline classifiers, even when the match's video conditions of the test and training sets are considerably different. PassNet is the first step towards an automated event annotation system that may break the time and the costs for event annotation, enabling data collections for minor and non-professional divisions, youth leagues and, in general, competitions whose matches are not currently annotated by data providers.



### Accelerating Translational Image Registration for HDR Images on GPU
- **Arxiv ID**: http://arxiv.org/abs/2007.06483v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.DC
- **Links**: [PDF](http://arxiv.org/pdf/2007.06483v1)
- **Published**: 2020-07-13 16:34:05+00:00
- **Updated**: 2020-07-13 16:34:05+00:00
- **Authors**: Kadir Cenk Alpay, Kadir Berkay Aydemir, Alptekin Temizel
- **Comment**: Submitted for Consideration for Publication in High Performance
  Computing Conference 2020
- **Journal**: None
- **Summary**: High Dynamic Range (HDR) images are generated using multiple exposures of a scene. When a hand-held camera is used to capture a static scene, these images need to be aligned by globally shifting each image in both dimensions. For a fast and robust alignment, the shift amount is commonly calculated using Median Threshold Bitmaps (MTB) and creating an image pyramid. In this study, we optimize these computations using a parallel processing approach utilizing GPU. Experimental evaluation shows that the proposed implementation achieves a speed-up of up to 6.24 times over the baseline multi-threaded CPU implementation on the alignment of one image pair. The source code is available at https://github.com/kadircenk/WardMTBCuda



### Towards Practical Lipreading with Distilled and Efficient Models
- **Arxiv ID**: http://arxiv.org/abs/2007.06504v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.06504v3)
- **Published**: 2020-07-13 16:56:27+00:00
- **Updated**: 2021-06-02 09:02:09+00:00
- **Authors**: Pingchuan Ma, Brais Martinez, Stavros Petridis, Maja Pantic
- **Comment**: Accepted to ICASSP 2021
- **Journal**: None
- **Summary**: Lipreading has witnessed a lot of progress due to the resurgence of neural networks. Recent works have placed emphasis on aspects such as improving performance by finding the optimal architecture or improving generalization. However, there is still a significant gap between the current methodologies and the requirements for an effective deployment of lipreading in practical scenarios. In this work, we propose a series of innovations that significantly bridge that gap: first, we raise the state-of-the-art performance by a wide margin on LRW and LRW-1000 to 88.5% and 46.6%, respectively using self-distillation. Secondly, we propose a series of architectural changes, including a novel Depthwise Separable Temporal Convolutional Network (DS-TCN) head, that slashes the computational cost to a fraction of the (already quite efficient) original model. Thirdly, we show that knowledge distillation is a very effective tool for recovering performance of the lightweight models. This results in a range of models with different accuracy-efficiency trade-offs. However, our most promising lightweight models are on par with the current state-of-the-art while showing a reduction of 8.2x and 3.9x in terms of computational cost and number of parameters, respectively, which we hope will enable the deployment of lipreading models in practical applications.



### Towards causal benchmarking of bias in face analysis algorithms
- **Arxiv ID**: http://arxiv.org/abs/2007.06570v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.06570v1)
- **Published**: 2020-07-13 17:10:34+00:00
- **Updated**: 2020-07-13 17:10:34+00:00
- **Authors**: Guha Balakrishnan, Yuanjun Xiong, Wei Xia, Pietro Perona
- **Comment**: Long-form version of ECCV 2020 paper
- **Journal**: None
- **Summary**: Measuring algorithmic bias is crucial both to assess algorithmic fairness, and to guide the improvement of algorithms. Current methods to measure algorithmic bias in computer vision, which are based on observational datasets, are inadequate for this task because they conflate algorithmic bias with dataset bias.   To address this problem we develop an experimental method for measuring algorithmic bias of face analysis algorithms, which manipulates directly the attributes of interest, e.g., gender and skin tone, in order to reveal causal links between attribute variation and performance change. Our proposed method is based on generating synthetic ``transects'' of matched sample images that are designed to differ along specific attributes while leaving other attributes constant. A crucial aspect of our approach is relying on the perception of human observers, both to guide manipulations, and to measure algorithmic bias.   Besides allowing the measurement of algorithmic bias, synthetic transects have other advantages with respect to observational datasets: they sample attributes more evenly allowing for more straightforward bias analysis on minority and intersectional groups, they enable prediction of bias in new scenarios, they greatly reduce ethical and legal challenges, and they are economical and fast to obtain, helping make bias testing affordable and widely available.   We validate our method by comparing it to a study that employs the traditional observational method for analyzing bias in gender classification algorithms. The two methods reach different conclusions. While the observational method reports gender and skin color biases, the experimental method reveals biases due to gender, hair length, age, and facial hair.



### Uncertain-DeepSSM: From Images to Probabilistic Shape Models
- **Arxiv ID**: http://arxiv.org/abs/2007.06516v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.06516v1)
- **Published**: 2020-07-13 17:18:21+00:00
- **Updated**: 2020-07-13 17:18:21+00:00
- **Authors**: Jadie Adams, Riddhish Bhalodia, Shireen Elhabian
- **Comment**: 16 pages, 7 figures
- **Journal**: None
- **Summary**: Statistical shape modeling (SSM) has recently taken advantage of advances in deep learning to alleviate the need for a time-consuming and expert-driven workflow of anatomy segmentation, shape registration, and the optimization of population-level shape representations. DeepSSM is an end-to-end deep learning approach that extracts statistical shape representation directly from unsegmented images with little manual overhead. It performs comparably with state-of-the-art shape modeling methods for estimating morphologies that are viable for subsequent downstream tasks. Nonetheless, DeepSSM produces an overconfident estimate of shape that cannot be blindly assumed to be accurate. Hence, conveying what DeepSSM does not know, via quantifying granular estimates of uncertainty, is critical for its direct clinical application as an on-demand diagnostic tool to determine how trustworthy the model output is. Here, we propose Uncertain-DeepSSM as a unified model that quantifies both, data-dependent aleatoric uncertainty by adapting the network to predict intrinsic input variance, and model-dependent epistemic uncertainty via a Monte Carlo dropout sampling to approximate a variational distribution over the network parameters. Experiments show an accuracy improvement over DeepSSM while maintaining the same benefits of being end-to-end with little pre-processing.



### Free-running SIMilarity-Based Angiography (SIMBA) for simplified anatomical MR imaging of the heart
- **Arxiv ID**: http://arxiv.org/abs/2007.06544v1
- **DOI**: 10.1002/mrm.28713
- **Categories**: **eess.IV**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2007.06544v1)
- **Published**: 2020-07-13 17:49:30+00:00
- **Updated**: 2020-07-13 17:49:30+00:00
- **Authors**: John Heerfordt, Kevin K. Whitehead, Jessica A. M. Bastiaansen, Lorenzo Di Sopra, Christopher W. Roy, Jérôme Yerly, Bastien Milani, Mark A. Fogel, Matthias Stuber, Davide Piccini
- **Comment**: 8 figures, 2 tables
- **Journal**: Magnetic Resonance in Medicine, 24 February 2021
- **Summary**: Purpose: Whole-heart MRA techniques typically target pre-determined motion states and address cardiac and respiratory dynamics independently. We propose a novel fast reconstruction algorithm, applicable to ungated free-running sequences, that leverages inherent similarities in the acquired data to avoid such physiological constraints.   Theory and Methods: The proposed SIMilarity-Based Angiography (SIMBA) method clusters the continuously acquired k-space data in order to find a motion-consistent subset that can be reconstructed into a motion-suppressed whole-heart MRA. Free-running 3D radial datasets from six ferumoxytol-enhanced scans of pediatric cardiac patients and twelve non-contrast scans of healthy volunteers were reconstructed with a non-motion-suppressed regridding of all the acquired data (All Data), our proposed SIMBA method, and a previously published free-running framework (FRF) that uses cardiac and respiratory self-gating and compressed sensing. Images were compared for blood-myocardium interface sharpness, contrast ratio, and visibility of coronary artery ostia.   Results: Both the fast SIMBA reconstruction (~20s) and the FRF provided significantly higher blood-myocardium sharpness than All Data (P<0.001). No significant difference was observed among the former two. Significantly higher blood-myocardium contrast ratio was obtained with SIMBA compared to All Data and FRF (P<0.01). More coronary ostia could be visualized with both SIMBA and FRF than with All Data (All Data: 4/36, SIMBA: 30/36, FRF: 33/36, both P<0.001) but no significant difference was found between the first two.   Conclusion: The combination of free-running sequences and the fast SIMBA reconstruction, which operates without a priori assumptions related to physiological motion, forms a simple workflow for obtaining whole-heart MRA with sharp anatomical structures.



### Graph Structure of Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2007.06559v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.SI, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.06559v2)
- **Published**: 2020-07-13 17:59:31+00:00
- **Updated**: 2020-08-27 17:58:07+00:00
- **Authors**: Jiaxuan You, Jure Leskovec, Kaiming He, Saining Xie
- **Comment**: ICML 2020, with open-source code
- **Journal**: None
- **Summary**: Neural networks are often represented as graphs of connections between neurons. However, despite their wide use, there is currently little understanding of the relationship between the graph structure of the neural network and its predictive performance. Here we systematically investigate how does the graph structure of neural networks affect their predictive performance. To this end, we develop a novel graph-based representation of neural networks called relational graph, where layers of neural network computation correspond to rounds of message exchange along the graph structure. Using this representation we show that: (1) a "sweet spot" of relational graphs leads to neural networks with significantly improved predictive performance; (2) neural network's performance is approximately a smooth function of the clustering coefficient and average path length of its relational graph; (3) our findings are consistent across many different tasks and datasets; (4) the sweet spot can be identified efficiently; (5) top-performing neural networks have graph structure surprisingly similar to those of real biological neural networks. Our work opens new directions for the design of neural architectures and the understanding on neural networks in general.



### Closed-Form Factorization of Latent Semantics in GANs
- **Arxiv ID**: http://arxiv.org/abs/2007.06600v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.06600v4)
- **Published**: 2020-07-13 18:05:36+00:00
- **Updated**: 2021-04-03 13:30:22+00:00
- **Authors**: Yujun Shen, Bolei Zhou
- **Comment**: CVPR 2021 camera-ready
- **Journal**: None
- **Summary**: A rich set of interpretable dimensions has been shown to emerge in the latent space of the Generative Adversarial Networks (GANs) trained for synthesizing images. In order to identify such latent dimensions for image editing, previous methods typically annotate a collection of synthesized samples and train linear classifiers in the latent space. However, they require a clear definition of the target attribute as well as the corresponding manual annotations, limiting their applications in practice. In this work, we examine the internal representation learned by GANs to reveal the underlying variation factors in an unsupervised manner. In particular, we take a closer look into the generation mechanism of GANs and further propose a closed-form factorization algorithm for latent semantic discovery by directly decomposing the pre-trained weights. With a lightning-fast implementation, our approach is capable of not only finding semantically meaningful dimensions comparably to the state-of-the-art supervised methods, but also resulting in far more versatile concepts across multiple GAN models trained on a wide range of datasets.



### Inferring the 3D Standing Spine Posture from 2D Radiographs
- **Arxiv ID**: http://arxiv.org/abs/2007.06612v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.06612v2)
- **Published**: 2020-07-13 18:37:00+00:00
- **Updated**: 2021-01-13 15:15:38+00:00
- **Authors**: Amirhossein Bayat, Anjany Sekuboyina, Johannes C. Paetzold, Christian Payer, Darko Stern, Martin Urschler, Jan S. Kirschke, Bjoern H. Menze
- **Comment**: None
- **Journal**: None
- **Summary**: The treatment of degenerative spinal disorders requires an understanding of the individual spinal anatomy and curvature in 3D. An upright spinal pose (i.e. standing) under natural weight bearing is crucial for such bio-mechanical analysis. 3D volumetric imaging modalities (e.g. CT and MRI) are performed in patients lying down. On the other hand, radiographs are captured in an upright pose, but result in 2D projections. This work aims to integrate the two realms, i.e. it combines the upright spinal curvature from radiographs with the 3D vertebral shape from CT imaging for synthesizing an upright 3D model of spine, loaded naturally. Specifically, we propose a novel neural network architecture working vertebra-wise, termed \emph{TransVert}, which takes orthogonal 2D radiographs and infers the spine's 3D posture. We validate our architecture on digitally reconstructed radiographs, achieving a 3D reconstruction Dice of $95.52\%$, indicating an almost perfect 2D-to-3D domain translation. Deploying our model on clinical radiographs, we successfully synthesise full-3D, upright, patient-specific spine models for the first time.



### AUTO3D: Novel view synthesis through unsupervisely learned variational viewpoint and global 3D representation
- **Arxiv ID**: http://arxiv.org/abs/2007.06620v2
- **DOI**: 10.1007/978-3-030-58545-7_4
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.06620v2)
- **Published**: 2020-07-13 18:51:27+00:00
- **Updated**: 2020-08-27 22:18:24+00:00
- **Authors**: Xiaofeng Liu, Tong Che, Yiqun Lu, Chao Yang, Site Li, Jane You
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: This paper targets on learning-based novel view synthesis from a single or limited 2D images without the pose supervision. In the viewer-centered coordinates, we construct an end-to-end trainable conditional variational framework to disentangle the unsupervisely learned relative-pose/rotation and implicit global 3D representation (shape, texture and the origin of viewer-centered coordinates, etc.). The global appearance of the 3D object is given by several appearance-describing images taken from any number of viewpoints. Our spatial correlation module extracts a global 3D representation from the appearance-describing images in a permutation invariant manner. Our system can achieve implicitly 3D understanding without explicitly 3D reconstruction. With an unsupervisely learned viewer-centered relative-pose/rotation code, the decoder can hallucinate the novel view continuously by sampling the relative-pose in a prior distribution. In various applications, we demonstrate that our model can achieve comparable or even better results than pose/3D model-supervised learning-based novel view synthesis (NVS) methods with any number of input views.



### A new approach to descriptors generation for image retrieval by analyzing activations of deep neural network layers
- **Arxiv ID**: http://arxiv.org/abs/2007.06624v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.06624v1)
- **Published**: 2020-07-13 18:53:10+00:00
- **Updated**: 2020-07-13 18:53:10+00:00
- **Authors**: Paweł Staszewski, Maciej Jaworski, Jinde Cao, Leszek Rutkowski
- **Comment**: 8
- **Journal**: None
- **Summary**: In this paper, we consider the problem of descriptors construction for the task of content-based image retrieval using deep neural networks. The idea of neural codes, based on fully connected layers activations, is extended by incorporating the information contained in convolutional layers. It is known that the total number of neurons in the convolutional part of the network is large and the majority of them have little influence on the final classification decision. Therefore, in the paper we propose a novel algorithm that allows us to extract the most significant neuron activations and utilize this information to construct effective descriptors. The descriptors consisting of values taken from both the fully connected and convolutional layers perfectly represent the whole image content. The images retrieved using these descriptors match semantically very well to the query image, and also they are similar in other secondary image characteristics, like background, textures or color distribution. These features of the proposed descriptors are verified experimentally based on the IMAGENET1M dataset using the VGG16 neural network.



### Dense Crowds Detection and Counting with a Lightweight Architecture
- **Arxiv ID**: http://arxiv.org/abs/2007.06630v1
- **DOI**: 10.1007/s10846-021-01380-8
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2007.06630v1)
- **Published**: 2020-07-13 19:02:25+00:00
- **Updated**: 2020-07-13 19:02:25+00:00
- **Authors**: Javier Antonio Gonzalez-Trejo, Diego Alberto Mercado-Ravell
- **Comment**: None
- **Journal**: J Intell Robot Syst 102, 7 (2021)
- **Summary**: In the context of crowd counting, most of the works have focused on improving the accuracy without regard to the performance leading to algorithms that are not suitable for embedded applications. In this paper, we propose a lightweight convolutional neural network architecture to perform crowd detection and counting using fewer computer resources without a significant loss on count accuracy. The architecture was trained using the Bayes loss function to further improve its accuracy and then pruned to further reduce the computational resources used. The proposed architecture was tested over the USF-QNRF achieving a competitive Mean Average Error of 154.07 and a superior Mean Square Error of 241.77 while maintaining a competitive number of parameters of 0.067 Million. The obtained results suggest that the Bayes loss can be used with other architectures to further improve them and also the last convolutional layer provides no significant information and even encourage over-fitting at training.



### T-Basis: a Compact Representation for Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2007.06631v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.06631v2)
- **Published**: 2020-07-13 19:03:22+00:00
- **Updated**: 2021-07-13 17:34:09+00:00
- **Authors**: Anton Obukhov, Maxim Rakhuba, Stamatios Georgoulis, Menelaos Kanakis, Dengxin Dai, Luc Van Gool
- **Comment**: Accepted at ICML 2020
- **Journal**: None
- **Summary**: We introduce T-Basis, a novel concept for a compact representation of a set of tensors, each of an arbitrary shape, which is often seen in Neural Networks. Each of the tensors in the set is modeled using Tensor Rings, though the concept applies to other Tensor Networks. Owing its name to the T-shape of nodes in diagram notation of Tensor Rings, T-Basis is simply a list of equally shaped three-dimensional tensors, used to represent Tensor Ring nodes. Such representation allows us to parameterize the tensor set with a small number of parameters (coefficients of the T-Basis tensors), scaling logarithmically with each tensor's size in the set and linearly with the dimensionality of T-Basis. We evaluate the proposed approach on the task of neural network compression and demonstrate that it reaches high compression rates at acceptable performance drops. Finally, we analyze memory and operation requirements of the compressed networks and conclude that T-Basis networks are equally well suited for training and inference in resource-constrained environments and usage on the edge devices.



### Adversarial Background-Aware Loss for Weakly-supervised Temporal Activity Localization
- **Arxiv ID**: http://arxiv.org/abs/2007.06643v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.06643v1)
- **Published**: 2020-07-13 19:33:24+00:00
- **Updated**: 2020-07-13 19:33:24+00:00
- **Authors**: Kyle Min, Jason J. Corso
- **Comment**: ECCV 2020 camera ready (Supplementary material: on ECVA soon)
- **Journal**: None
- **Summary**: Temporally localizing activities within untrimmed videos has been extensively studied in recent years. Despite recent advances, existing methods for weakly-supervised temporal activity localization struggle to recognize when an activity is not occurring. To address this issue, we propose a novel method named A2CL-PT. Two triplets of the feature space are considered in our approach: one triplet is used to learn discriminative features for each activity class, and the other one is used to distinguish the features where no activity occurs (i.e. background features) from activity-related features for each video. To further improve the performance, we build our network using two parallel branches which operate in an adversarial way: the first branch localizes the most salient activities of a video and the second one finds other supplementary activities from non-localized parts of the video. Extensive experiments performed on THUMOS14 and ActivityNet datasets demonstrate that our proposed method is effective. Specifically, the average mAP of IoU thresholds from 0.1 to 0.9 on the THUMOS14 dataset is significantly improved from 27.9% to 30.0%.



### Improving Pixel Embedding Learning through Intermediate Distance Regression Supervision for Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2007.06660v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.06660v1)
- **Published**: 2020-07-13 20:03:30+00:00
- **Updated**: 2020-07-13 20:03:30+00:00
- **Authors**: Yuli Wu, Long Chen, Dorit Merhof
- **Comment**: ECCV 2020 Workshop: Computer Vision Problems in Plant Phenotyping
  (CVPPP 2020)
- **Journal**: None
- **Summary**: As a proposal-free approach, instance segmentation through pixel embedding learning and clustering is gaining more emphasis. Compared with bounding box refinement approaches, such as Mask R-CNN, it has potential advantages in handling complex shapes and dense objects. In this work, we propose a simple, yet highly effective, architecture for object-aware embedding learning. A distance regression module is incorporated into our architecture to generate seeds for fast clustering. At the same time, we show that the features learned by the distance regression module are able to promote the accuracy of learned object-aware embeddings significantly. By simply concatenating features of the distance regression module to the images as inputs of the embedding module, the mSBD scores on the CVPPP Leaf Segmentation Challenge can be further improved by more than 8% compared to the identical set-up without concatenation, yielding the best overall result amongst the leaderboard at CodaLab.



### Learning Differential Diagnosis of Skin Conditions with Co-occurrence Supervision using Graph Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/2007.06666v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.06666v1)
- **Published**: 2020-07-13 20:13:25+00:00
- **Updated**: 2020-07-13 20:13:25+00:00
- **Authors**: Junyan Wu, Hao Jiang, Xiaowei Ding, Anudeep Konda, Jin Han, Yang Zhang, Qian Li
- **Comment**: None
- **Journal**: None
- **Summary**: Skin conditions are reported the 4th leading cause of nonfatal disease burden worldwide. However, given the colossal spectrum of skin disorders defined clinically and shortage in dermatology expertise, diagnosing skin conditions in a timely and accurate manner remains a challenging task. Using computer vision technologies, a deep learning system has proven effective assisting clinicians in image diagnostics of radiology, ophthalmology and more. In this paper, we propose a deep learning system (DLS) that may predict differential diagnosis of skin conditions using clinical images. Our DLS formulates the differential diagnostics as a multi-label classification task over 80 conditions when only incomplete image labels are available. We tackle the label incompleteness problem by combining a classification network with a Graph Convolutional Network (GCN) that characterizes label co-occurrence and effectively regularizes it towards a sparse representation. Our approach is demonstrated on 136,462 clinical images and concludes that the classification accuracy greatly benefit from the Co-occurrence supervision. Our DLS achieves 93.6% top-5 accuracy on 12,378 test images and consistently outperform the baseline classification network.



### Landslide Segmentation with U-Net: Evaluating Different Sampling Methods and Patch Sizes
- **Arxiv ID**: http://arxiv.org/abs/2007.06672v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.06672v1)
- **Published**: 2020-07-13 20:28:46+00:00
- **Updated**: 2020-07-13 20:28:46+00:00
- **Authors**: Lucas P. Soares, Helen C. Dias, Carlos H. Grohmann
- **Comment**: 13 pages, 7 figures, 3 tables
- **Journal**: None
- **Summary**: Landslide inventory maps are crucial to validate predictive landslide models; however, since most mapping methods rely on visual interpretation or expert knowledge, detailed inventory maps are still lacking. This study used a fully convolutional deep learning model named U-net to automatically segment landslides in the city of Nova Friburgo, located in the mountainous range of Rio de Janeiro, southeastern Brazil. The objective was to evaluate the impact of patch sizes, sampling methods, and datasets on the overall accuracy of the models. The training data used the optical information from RapidEye satellite, and a digital elevation model (DEM) derived from the L-band sensor of the ALOS satellite. The data was sampled using random and regular grid methods and patched in three sizes (32x32, 64x64, and 128x128 pixels). The models were evaluated on two areas with precision, recall, f1-score, and mean intersect over union (mIoU) metrics. The results show that the models trained with 32x32 tiles tend to have higher recall values due to higher true positive rates; however, they misclassify more background areas as landslides (false positives). Models trained with 128x128 tiles usually achieve higher precision values because they make less false positive errors. In both test areas, DEM and augmentation increased the accuracy of the models. Random sampling helped in model generalization. Models trained with 128x128 random tiles from the data that used the RapidEye image, DEM information, and augmentation achieved the highest f1-score, 0.55 in test area one, and 0.58 in test area two. The results achieved in this study are comparable to other fully convolutional models found in the literature, increasing the knowledge in the area.



### UnRectDepthNet: Self-Supervised Monocular Depth Estimation using a Generic Framework for Handling Common Camera Distortion Models
- **Arxiv ID**: http://arxiv.org/abs/2007.06676v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2007.06676v4)
- **Published**: 2020-07-13 20:35:05+00:00
- **Updated**: 2023-06-06 14:26:28+00:00
- **Authors**: Varun Ravi Kumar, Senthil Yogamani, Markus Bach, Christian Witt, Stefan Milz, Patrick Mader
- **Comment**: Minor fixes added after IROS 2020 Camera ready submission. IROS 2020
  presentation video - https://www.youtube.com/watch?v=3Br2KSWZRrY
- **Journal**: None
- **Summary**: In classical computer vision, rectification is an integral part of multi-view depth estimation. It typically includes epipolar rectification and lens distortion correction. This process simplifies the depth estimation significantly, and thus it has been adopted in CNN approaches. However, rectification has several side effects, including a reduced field of view (FOV), resampling distortion, and sensitivity to calibration errors. The effects are particularly pronounced in case of significant distortion (e.g., wide-angle fisheye cameras). In this paper, we propose a generic scale-aware self-supervised pipeline for estimating depth, euclidean distance, and visual odometry from unrectified monocular videos. We demonstrate a similar level of precision on the unrectified KITTI dataset with barrel distortion comparable to the rectified KITTI dataset. The intuition being that the rectification step can be implicitly absorbed within the CNN model, which learns the distortion model without increasing complexity. Our approach does not suffer from a reduced field of view and avoids computational costs for rectification at inference time. To further illustrate the general applicability of the proposed framework, we apply it to wide-angle fisheye cameras with 190$^\circ$ horizontal field of view. The training framework UnRectDepthNet takes in the camera distortion model as an argument and adapts projection and unprojection functions accordingly. The proposed algorithm is evaluated further on the KITTI rectified dataset, and we achieve state-of-the-art results that improve upon our previous work FisheyeDistanceNet. Qualitative results on a distorted test scene video sequence indicate excellent performance https://youtu.be/K6pbx3bU4Ss.



### GeoStat Representations of Time Series for Fast Classification
- **Arxiv ID**: http://arxiv.org/abs/2007.06682v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.06682v3)
- **Published**: 2020-07-13 20:48:03+00:00
- **Updated**: 2021-01-11 22:03:10+00:00
- **Authors**: Robert J. Ravier, Mohammadreza Soltani, Miguel Simões, Denis Garagic, Vahid Tarokh
- **Comment**: 28 pages, 8 tables, 5 figures
- **Journal**: None
- **Summary**: Recent advances in time series classification have largely focused on methods that either employ deep learning or utilize other machine learning models for feature extraction. Though successful, their power often comes at the requirement of computational complexity. In this paper, we introduce GeoStat representations for time series. GeoStat representations are based off of a generalization of recent methods for trajectory classification, and summarize the information of a time series in terms of comprehensive statistics of (possibly windowed) distributions of easy to compute differential geometric quantities, requiring no dynamic time warping. The features used are intuitive and require minimal parameter tuning. We perform an exhaustive evaluation of GeoStat on a number of real datasets, showing that simple KNN and SVM classifiers trained on these representations exhibit surprising performance relative to modern single model methods requiring significant computational power, achieving state of the art results in many cases. In particular, we show that this methodology achieves good performance on a challenging dataset involving the classification of fishing vessels, where our methods achieve good performance relative to the state of the art despite only having access to approximately two percent of the dataset used in training and evaluating this state of the art.



### DETCID: Detection of Elongated Touching Cells with Inhomogeneous Illumination using a Deep Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/2007.06716v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.06716v1)
- **Published**: 2020-07-13 21:43:27+00:00
- **Updated**: 2020-07-13 21:43:27+00:00
- **Authors**: Ali Memariani, Ioannis A. Kakadiaris
- **Comment**: None
- **Journal**: None
- **Summary**: Clostridioides difficile infection (C. diff) is the most common cause of death due to secondary infection in hospital patients in the United States. Detection of C. diff cells in scanning electron microscopy (SEM) images is an important task to quantify the efficacy of the under-development treatments. However, detecting C. diff cells in SEM images is a challenging problem due to the presence of inhomogeneous illumination and occlusion. An Illumination normalization pre-processing step destroys the texture and adds noise to the image. Furthermore, cells are often clustered together resulting in touching cells and occlusion. In this paper, DETCID, a deep cell detection method using adversarial training, specifically robust to inhomogeneous illumination and occlusion, is proposed. An adversarial network is developed to provide region proposals and pass the proposals to a feature extraction network. Furthermore, a modified IoU metric is developed to allow the detection of touching cells in various orientations. The results indicate that DETCID outperforms the state-of-the-art in detection of touching cells in SEM images by at least 20 percent improvement of mean average precision.



