# Arxiv Papers in cs.CV on 2020-07-11
### Evolving Graphical Planner: Contextual Global Planning for Vision-and-Language Navigation
- **Arxiv ID**: http://arxiv.org/abs/2007.05655v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2007.05655v1)
- **Published**: 2020-07-11 00:21:05+00:00
- **Updated**: 2020-07-11 00:21:05+00:00
- **Authors**: Zhiwei Deng, Karthik Narasimhan, Olga Russakovsky
- **Comment**: None
- **Journal**: None
- **Summary**: The ability to perform effective planning is crucial for building an instruction-following agent. When navigating through a new environment, an agent is challenged with (1) connecting the natural language instructions with its progressively growing knowledge of the world; and (2) performing long-range planning and decision making in the form of effective exploration and error correction. Current methods are still limited on both fronts despite extensive efforts. In this paper, we introduce the Evolving Graphical Planner (EGP), a model that performs global planning for navigation based on raw sensory input. The model dynamically constructs a graphical representation, generalizes the action space to allow for more flexible decision making, and performs efficient planning on a proxy graph representation. We evaluate our model on a challenging Vision-and-Language Navigation (VLN) task with photorealistic images and achieve superior performance compared to previous navigation architectures. For instance, we achieve a 53% success rate on the test split of the Room-to-Room navigation task through pure imitation learning, outperforming previous navigation architectures by up to 5%.



### Deep Patch-based Human Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2007.05661v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2007.05661v1)
- **Published**: 2020-07-11 01:51:23+00:00
- **Updated**: 2020-07-11 01:51:23+00:00
- **Authors**: Dongbo Zhang, Zheng Fang, Xuequan Lu, Hong Qin, Antonio Robles-Kelly, Chao Zhang, Ying He
- **Comment**: submitted for review
- **Journal**: None
- **Summary**: 3D human segmentation has seen noticeable progress in re-cent years. It, however, still remains a challenge to date. In this paper, weintroduce a deep patch-based method for 3D human segmentation. Wefirst extract a local surface patch for each vertex and then parameterizeit into a 2D grid (or image). We then embed identified shape descriptorsinto the 2D grids which are further fed into the powerful 2D Convolu-tional Neural Network for regressing corresponding semantic labels (e.g.,head, torso). Experiments demonstrate that our method is effective inhuman segmentation, and achieves state-of-the-art accuracy.



### To Filter Prune, or to Layer Prune, That Is The Question
- **Arxiv ID**: http://arxiv.org/abs/2007.05667v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.05667v3)
- **Published**: 2020-07-11 02:51:40+00:00
- **Updated**: 2020-11-08 17:48:23+00:00
- **Authors**: Sara Elkerdawy, Mostafa Elhoushi, Abhineet Singh, Hong Zhang, Nilanjan Ray
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advances in pruning of neural networks have made it possible to remove a large number of filters or weights without any perceptible drop in accuracy. The number of parameters and that of FLOPs are usually the reported metrics to measure the quality of the pruned models. However, the gain in speed for these pruned models is often overlooked in the literature due to the complex nature of latency measurements. In this paper, we show the limitation of filter pruning methods in terms of latency reduction and propose LayerPrune framework. LayerPrune presents a set of layer pruning methods based on different criteria that achieve higher latency reduction than filter pruning methods on similar accuracy. The advantage of layer pruning over filter pruning in terms of latency reduction is a result of the fact that the former is not constrained by the original model's depth and thus allows for a larger range of latency reduction. For each filter pruning method we examined, we use the same filter importance criterion to calculate a per-layer importance score in one-shot. We then prune the least important layers and fine-tune the shallower model which obtains comparable or better accuracy than its filter-based pruning counterpart. This one-shot process allows to remove layers from single path networks like VGG before fine-tuning, unlike in iterative filter pruning, a minimum number of filters per layer is required to allow for data flow which constraint the search space. To the best of our knowledge, we are the first to examine the effect of pruning methods on latency metric instead of FLOPs for multiple networks, datasets and hardware targets. LayerPrune also outperforms handcrafted architectures such as Shufflenet, MobileNet, MNASNet and ResNet18 by 7.3%, 4.6%, 2.8% and 0.5% respectively on similar latency budget on ImageNet dataset.



### Towards Cross-Granularity Few-Shot Learning: Coarse-to-Fine Pseudo-Labeling with Visual-Semantic Meta-Embedding
- **Arxiv ID**: http://arxiv.org/abs/2007.05675v3
- **DOI**: 10.1145/3474085.3475200
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.05675v3)
- **Published**: 2020-07-11 03:44:21+00:00
- **Updated**: 2021-07-20 12:39:41+00:00
- **Authors**: Jinhai Yang, Hua Yang, Lin Chen
- **Comment**: Accepted by ACM MM 2021
- **Journal**: None
- **Summary**: Few-shot learning aims at rapidly adapting to novel categories with only a handful of samples at test time, which has been predominantly tackled with the idea of meta-learning. However, meta-learning approaches essentially learn across a variety of few-shot tasks and thus still require large-scale training data with fine-grained supervision to derive a generalized model, thereby involving prohibitive annotation cost. In this paper, we advance the few-shot classification paradigm towards a more challenging scenario, i.e., cross-granularity few-shot classification, where the model observes only coarse labels during training while is expected to perform fine-grained classification during testing. This task largely relieves the annotation cost since fine-grained labeling usually requires strong domain-specific expertise. To bridge the cross-granularity gap, we approximate the fine-grained data distribution by greedy clustering of each coarse-class into pseudo-fine-classes according to the similarity of image embeddings. We then propose a meta-embedder that jointly optimizes the visual- and semantic-discrimination, in both instance-wise and coarse class-wise, to obtain a good feature space for this coarse-to-fine pseudo-labeling process. Extensive experiments and ablation studies are conducted to demonstrate the effectiveness and robustness of our approach on three representative datasets.



### Learning Object Depth from Camera Motion and Video Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2007.05676v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.05676v3)
- **Published**: 2020-07-11 03:50:57+00:00
- **Updated**: 2020-12-18 17:43:07+00:00
- **Authors**: Brent A. Griffin, Jason J. Corso
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: Video object segmentation, i.e., the separation of a target object from background in video, has made significant progress on real and challenging videos in recent years. To leverage this progress in 3D applications, this paper addresses the problem of learning to estimate the depth of segmented objects given some measurement of camera motion (e.g., from robot kinematics or vehicle odometry). We achieve this by, first, introducing a diverse, extensible dataset and, second, designing a novel deep network that estimates the depth of objects using only segmentation masks and uncalibrated camera movement. Our data-generation framework creates artificial object segmentations that are scaled for changes in distance between the camera and object, and our network learns to estimate object depth even with segmentation errors. We demonstrate our approach across domains using a robot camera to locate objects from the YCB dataset and a vehicle camera to locate obstacles while driving.



### Batch-level Experience Replay with Review for Continual Learning
- **Arxiv ID**: http://arxiv.org/abs/2007.05683v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.05683v1)
- **Published**: 2020-07-11 05:20:09+00:00
- **Updated**: 2020-07-11 05:20:09+00:00
- **Authors**: Zheda Mai, Hyunwoo Kim, Jihwan Jeong, Scott Sanner
- **Comment**: None
- **Journal**: None
- **Summary**: Continual learning is a branch of deep learning that seeks to strike a balance between learning stability and plasticity. The CVPR 2020 CLVision Continual Learning for Computer Vision challenge is dedicated to evaluating and advancing the current state-of-the-art continual learning methods using the CORe50 dataset with three different continual learning scenarios. This paper presents our approach, called Batch-level Experience Replay with Review, to this challenge. Our team achieved the 1'st place in all three scenarios out of 79 participated teams. The codebase of our implementation is publicly available at https://github.com/RaptorMai/CVPR20_CLVision_challenge



### Fast Real-time Counterfactual Explanations
- **Arxiv ID**: http://arxiv.org/abs/2007.05684v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.05684v2)
- **Published**: 2020-07-11 05:29:28+00:00
- **Updated**: 2020-08-29 06:05:21+00:00
- **Authors**: Yunxia Zhao
- **Comment**: This paper has been accepted by ICML workshop 2020
- **Journal**: None
- **Summary**: Counterfactual explanations are considered, which is to answer {\it why the prediction is class A but not B.} Different from previous optimization based methods, an optimization-free Fast ReAl-time Counterfactual Explanation (FRACE) algorithm is proposed benefiting from the development of multi-domain image to image translation algorithms. Built from starGAN, a transformer is trained as a residual generator conditional on a classifier constrained under a proposal perturbation loss which maintains the content information of the query image, but just the class-specific semantic information is changed. The transformer can transfer the query image to any counterfactual class, and during inference, our explanation can be generated by it only within a forward time. It is fast and can satisfy the real-time practical application. Because of the adversarial training of GAN, our explanation is also more realistic compared to other counterparts. The experimental results demonstrate that our proposal is better than the existing state of the art in terms of quality and speed.



### Fast Video Object Segmentation With Temporal Aggregation Network and Dynamic Template Matching
- **Arxiv ID**: http://arxiv.org/abs/2007.05687v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.05687v1)
- **Published**: 2020-07-11 05:44:16+00:00
- **Updated**: 2020-07-11 05:44:16+00:00
- **Authors**: Xuhua Huang, Jiarui Xu, Yu-Wing Tai, Chi-Keung Tang
- **Comment**: CVPR2020
- **Journal**: None
- **Summary**: Significant progress has been made in Video Object Segmentation (VOS), the video object tracking task in its finest level. While the VOS task can be naturally decoupled into image semantic segmentation and video object tracking, significantly much more research effort has been made in segmentation than tracking. In this paper, we introduce "tracking-by-detection" into VOS which can coherently integrate segmentation into tracking, by proposing a new temporal aggregation network and a novel dynamic time-evolving template matching mechanism to achieve significantly improved performance. Notably, our method is entirely online and thus suitable for one-shot learning, and our end-to-end trainable model allows multiple object segmentation in one forward pass. We achieve new state-of-the-art performance on the DAVIS benchmark without complicated bells and whistles in both speed and accuracy, with a speed of 0.14 second per frame and J&F measure of 75.9% respectively.



### Automated Intracranial Artery Labeling using a Graph Neural Network and Hierarchical Refinement
- **Arxiv ID**: http://arxiv.org/abs/2007.14472v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.14472v1)
- **Published**: 2020-07-11 06:22:35+00:00
- **Updated**: 2020-07-11 06:22:35+00:00
- **Authors**: Li Chen, Thomas Hatsukami, Jenq-Neng Hwang, Chun Yuan
- **Comment**: MICCAI 2020
- **Journal**: None
- **Summary**: Automatically labeling intracranial arteries (ICA) with their anatomical names is beneficial for feature extraction and detailed analysis of intracranial vascular structures. There are significant variations in the ICA due to natural and pathological causes, making it challenging for automated labeling. However, the existing public dataset for evaluation of anatomical labeling is limited. We construct a comprehensive dataset with 729 Magnetic Resonance Angiography scans and propose a Graph Neural Network (GNN) method to label arteries by classifying types of nodes and edges in an attributed relational graph. In addition, a hierarchical refinement framework is developed for further improving the GNN outputs to incorporate structural and relational knowledge about the ICA. Our method achieved a node labeling accuracy of 97.5%, and 63.8% of scans were correctly labeled for all Circle of Willis nodes, on a testing set of 105 scans with both healthy and diseased subjects. This is a significant improvement over available state-of-the-art methods. Automatic artery labeling is promising to minimize manual effort in characterizing the complicated ICA networks and provides valuable information for the identification of geometric risk factors of vascular disease. Our code and dataset are available at https://github.com/clatfd/GNN-ARTLABEL.



### Cascade Network with Guided Loss and Hybrid Attention for Two-view Geometry
- **Arxiv ID**: http://arxiv.org/abs/2007.05706v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.05706v2)
- **Published**: 2020-07-11 07:44:04+00:00
- **Updated**: 2020-07-16 03:03:22+00:00
- **Authors**: Zhi Chen, Fan Yang, Wenbing Tao
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we are committed to designing a high-performance network for two-view geometry. We first propose a Guided Loss and theoretically establish the direct negative correlation between the loss and Fn-measure by dynamically adjusting the weights of positive and negative classes during training, so that the network is always trained towards the direction of increasing Fn-measure. By this way, the network can maintain the advantage of the cross-entropy loss while maximizing the Fn-measure. We then propose a hybrid attention block to extract feature, which integrates the bayesian attentive context normalization (BACN) and channel-wise attention (CA). BACN can mine the prior information to better exploit global context and CA can capture complex channel context to enhance the channel awareness of the network. Finally, based on our Guided Loss and hybrid attention block, a cascade network is designed to gradually optimize the result for more superior performance. Experiments have shown that our network achieves the state-of-the-art performance on benchmark datasets.



### AutoTrajectory: Label-free Trajectory Extraction and Prediction from Videos using Dynamic Points
- **Arxiv ID**: http://arxiv.org/abs/2007.05719v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.05719v1)
- **Published**: 2020-07-11 08:43:34+00:00
- **Updated**: 2020-07-11 08:43:34+00:00
- **Authors**: Yuexin Ma, Xinge ZHU, Xinjing Cheng, Ruigang Yang, Jiming Liu, Dinesh Manocha
- **Comment**: None
- **Journal**: ECCV 2020
- **Summary**: Current methods for trajectory prediction operate in supervised manners, and therefore require vast quantities of corresponding ground truth data for training. In this paper, we present a novel, label-free algorithm, AutoTrajectory, for trajectory extraction and prediction to use raw videos directly. To better capture the moving objects in videos, we introduce dynamic points. We use them to model dynamic motions by using a forward-backward extractor to keep temporal consistency and using image reconstruction to keep spatial consistency in an unsupervised manner. Then we aggregate dynamic points to instance points, which stand for moving objects such as pedestrians in videos. Finally, we extract trajectories by matching instance points for prediction training. To the best of our knowledge, our method is the first to achieve unsupervised learning of trajectory extraction and prediction. We evaluate the performance on well-known trajectory datasets and show that our method is effective for real-world videos and can use raw videos to further improve the performance of existing models.



### ECML: An Ensemble Cascade Metric Learning Mechanism towards Face Verification
- **Arxiv ID**: http://arxiv.org/abs/2007.05720v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.05720v1)
- **Published**: 2020-07-11 08:47:07+00:00
- **Updated**: 2020-07-11 08:47:07+00:00
- **Authors**: Fu Xiong, Yang Xiao, Zhiguo Cao, Yancheng Wang, Joey Tianyi Zhou, Jianxi Wu
- **Comment**: Accepted to IEEE Transaction on Cybernetics
- **Journal**: None
- **Summary**: Face verification can be regarded as a 2-class fine-grained visual recognition problem. Enhancing the feature's discriminative power is one of the key problems to improve its performance. Metric learning technology is often applied to address this need, while achieving a good tradeoff between underfitting and overfitting plays the vital role in metric learning. Hence, we propose a novel ensemble cascade metric learning (ECML) mechanism. In particular, hierarchical metric learning is executed in the cascade way to alleviate underfitting. Meanwhile, at each learning level, the features are split into non-overlapping groups. Then, metric learning is executed among the feature groups in the ensemble manner to resist overfitting. Considering the feature distribution characteristics of faces, a robust Mahalanobis metric learning method (RMML) with closed-form solution is additionally proposed. It can avoid the computation failure issue on inverse matrix faced by some well-known metric learning approaches (e.g., KISSME). Embedding RMML into the proposed ECML mechanism, our metric learning paradigm (EC-RMML) can run in the one-pass learning manner. Experimental results demonstrate that EC-RMML is superior to state-of-the-art metric learning methods for face verification. And, the proposed ensemble cascade metric learning mechanism is also applicable to other metric learning approaches.



### Do We Need Sound for Sound Source Localization?
- **Arxiv ID**: http://arxiv.org/abs/2007.05722v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2007.05722v1)
- **Published**: 2020-07-11 08:57:58+00:00
- **Updated**: 2020-07-11 08:57:58+00:00
- **Authors**: Takashi Oya, Shohei Iwase, Ryota Natsume, Takahiro Itazuri, Shugo Yamaguchi, Shigeo Morishima
- **Comment**: Paper: 14 pages, 6 figures. Supplementary Material: 6 pages, 3
  figures. Videos and Codes will be released later
- **Journal**: None
- **Summary**: During the performance of sound source localization which uses both visual and aural information, it presently remains unclear how much either image or sound modalities contribute to the result, i.e. do we need both image and sound for sound source localization? To address this question, we develop an unsupervised learning system that solves sound source localization by decomposing this task into two steps: (i) "potential sound source localization", a step that localizes possible sound sources using only visual information (ii) "object selection", a step that identifies which objects are actually sounding using aural information. Our overall system achieves state-of-the-art performance in sound source localization, and more importantly, we find that despite the constraint on available information, the results of (i) achieve similar performance. From this observation and further experiments, we show that visual information is dominant in "sound" source localization when evaluated with the currently adopted benchmark dataset. Moreover, we show that the majority of sound-producing objects within the samples in this dataset can be inherently identified using only visual information, and thus that the dataset is inadequate to evaluate a system's capability to leverage aural information. As an alternative, we present an evaluation protocol that enforces both visual and aural information to be leveraged, and verify this property through several experiments.



### Usefulness of interpretability methods to explain deep learning based plant stress phenotyping
- **Arxiv ID**: http://arxiv.org/abs/2007.05729v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.05729v1)
- **Published**: 2020-07-11 09:28:50+00:00
- **Updated**: 2020-07-11 09:28:50+00:00
- **Authors**: Koushik Nagasubramanian, Asheesh K. Singh, Arti Singh, Soumik Sarkar, Baskar Ganapathysubramanian
- **Comment**: 15 pages, 6 figures
- **Journal**: None
- **Summary**: Deep learning techniques have been successfully deployed for automating plant stress identification and quantification. In recent years, there is a growing push towards training models that are interpretable -i.e. that justify their classification decisions by visually highlighting image features that were crucial for classification decisions. The expectation is that trained network models utilize image features that mimic visual cues used by plant pathologists. In this work, we compare some of the most popular interpretability methods: Saliency Maps, SmoothGrad, Guided Backpropogation, Deep Taylor Decomposition, Integrated Gradients, Layer-wise Relevance Propagation and Gradient times Input, for interpreting the deep learning model. We train a DenseNet-121 network for the classification of eight different soybean stresses (biotic and abiotic). Using a dataset consisting of 16,573 RGB images of healthy and stressed soybean leaflets captured under controlled conditions, we obtained an overall classification accuracy of 95.05 \%. For a diverse subset of the test data, we compared the important features with those identified by a human expert. We observed that most interpretability methods identify the infected regions of the leaf as important features for some -- but not all -- of the correctly classified images. For some images, the output of the interpretability methods indicated that spurious feature correlations may have been used to correctly classify them. Although the output explanation maps of these interpretability methods may be different from each other for a given image, we advocate the use of these interpretability methods as `hypothesis generation' mechanisms that can drive scientific insight.



### A Competitive Deep Neural Network Approach for the ImageCLEFmed Caption 2020 Task
- **Arxiv ID**: http://arxiv.org/abs/2007.14226v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.14226v3)
- **Published**: 2020-07-11 09:30:13+00:00
- **Updated**: 2020-09-22 19:34:50+00:00
- **Authors**: Marimuthu Kalimuthu, Fabrizio Nunnari, Daniel Sonntag
- **Comment**: Camera-ready version for ImageCLEF-2020.
  http://ceur-ws.org/Vol-2696/paper_93.pdf
- **Journal**: CEUR-WS, Volume 2696, 2020
- **Summary**: The aim of ImageCLEFmed Caption task is to develop a system that automatically labels radiology images with relevant medical concepts. We describe our Deep Neural Network (DNN) based approach for tackling this problem. On the challenge test set of 3,534 radiology images, our system achieves an F1 score of 0.375 and ranks high, 12th among all systems that were successfully submitted to the challenge, whereby we only rely on the provided data sources and do not use any external medical knowledge or ontologies, or pretrained models from other medical image repositories or application domains.



### Enhanced Behavioral Cloning Based self-driving Car Using Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/2007.05740v1
- **DOI**: 10.1007/978-981-16-2937-2_14
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.05740v1)
- **Published**: 2020-07-11 10:44:41+00:00
- **Updated**: 2020-07-11 10:44:41+00:00
- **Authors**: Uppala Sumanth, Narinder Singh Punn, Sanjay Kumar Sonbhadra, Sonali Agarwal
- **Comment**: None
- **Journal**: None
- **Summary**: With the growing phase of artificial intelligence and autonomous learning, the self-driving car is one of the promising area of research and emerging as a center of focus for automobile industries. Behavioral cloning is the process of replicating human behavior via visuomotor policies by means of machine learning algorithms. In recent years, several deep learning-based behavioral cloning approaches have been developed in the context of self-driving cars specifically based on the concept of transfer learning. Concerning the same, the present paper proposes a transfer learning approach using VGG16 architecture, which is fine tuned by retraining the last block while keeping other blocks as non-trainable. The performance of proposed architecture is further compared with existing NVIDIA architecture and its pruned variants (pruned by 22.2% and 33.85% using 1x1 filter to decrease the total number of parameters). Experimental results show that the VGG16 with transfer learning architecture has outperformed other discussed approaches with faster convergence.



### Relation-Guided Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2007.05742v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.05742v1)
- **Published**: 2020-07-11 10:57:45+00:00
- **Updated**: 2020-07-11 10:57:45+00:00
- **Authors**: Zhao Kang, Xiao Lu, Jian Liang, Kun Bai, Zenglin Xu
- **Comment**: Appear in Neural Networks
- **Journal**: None
- **Summary**: Deep auto-encoders (DAEs) have achieved great success in learning data representations via the powerful representability of neural networks. But most DAEs only focus on the most dominant structures which are able to reconstruct the data from a latent space and neglect rich latent structural information. In this work, we propose a new representation learning method that explicitly models and leverages sample relations, which in turn is used as supervision to guide the representation learning. Different from previous work, our framework well preserves the relations between samples. Since the prediction of pairwise relations themselves is a fundamental problem, our model adaptively learns them from data. This provides much flexibility to encode real data manifold. The important role of relation and representation learning is evaluated on the clustering task. Extensive experiments on benchmark data sets demonstrate the superiority of our approach. By seeking to embed samples into subspace, we further show that our method can address the large-scale and out-of-sample problem.



### Distangling Biological Noise in Cellular Images with a focus on Explainability
- **Arxiv ID**: http://arxiv.org/abs/2007.05743v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.05743v1)
- **Published**: 2020-07-11 11:04:16+00:00
- **Updated**: 2020-07-11 11:04:16+00:00
- **Authors**: Manik Sharma, Ganapathy Krishnamurthi
- **Comment**: 13 Pages, 12 figures
- **Journal**: None
- **Summary**: The cost of some drugs and medical treatments has risen in recent years that many patients are having to go without. A classification project could make researchers more efficient.   One of the more surprising reasons behind the cost is how long it takes to bring new treatments to market. Despite improvements in technology and science, research and development continues to lag. In fact, finding new treatment takes, on average, more than 10 years and costs hundreds of millions of dollars. In turn, greatly decreasing the cost of treatments can make ensure these treatments get to patients faster. This work aims at solving a part of this problem by creating a cellular image classification model which can decipher the genetic perturbations in cell (occurring naturally or artificially). Another interesting question addressed is what makes the deep-learning model decide in a particular fashion, which can further help in demystifying the mechanism of action of certain perturbations and paves a way towards the explainability of the deep-learning model.   We show the results of Grad-CAM visualizations and make a case for the significance of certain features over others. Further we discuss how these significant features are pivotal in extracting useful diagnostic information from the deep-learning model.



### Driver Behavior Modelling at the Urban Intersection via Canonical Correlation Analysis
- **Arxiv ID**: http://arxiv.org/abs/2007.05751v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2007.05751v1)
- **Published**: 2020-07-11 11:34:22+00:00
- **Updated**: 2020-07-11 11:34:22+00:00
- **Authors**: Zirui Li, Chao Lu, Cheng Gong, Cheng Gong, Jinghang Li, Lianzhen Wei
- **Comment**: 2020 3rd IEEE International Conference on Unmanned Systems (ICUS)
- **Journal**: None
- **Summary**: The urban intersection is a typically dynamic and complex scenario for intelligent vehicles, which exists a variety of driving behaviors and traffic participants. Accurately modelling the driver behavior at the intersection is essential for intelligent transportation systems (ITS). Previous researches mainly focus on using attention mechanism to model the degree of correlation. In this research, a canonical correlation analysis (CCA)-based framework is proposed. The value of canonical correlation is used for feature selection. Gaussian mixture model and Gaussian process regression are applied for driver behavior modelling. Two experiments using simulated and naturalistic driving data are designed for verification. Experimental results are consistent with the driver's judgment. Comparative studies show that the proposed framework can obtain a better performance.



### Generative Compositional Augmentations for Scene Graph Prediction
- **Arxiv ID**: http://arxiv.org/abs/2007.05756v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.05756v3)
- **Published**: 2020-07-11 12:11:53+00:00
- **Updated**: 2021-10-01 15:33:30+00:00
- **Authors**: Boris Knyazev, Harm de Vries, Cătălina Cangea, Graham W. Taylor, Aaron Courville, Eugene Belilovsky
- **Comment**: ICCV 2021 camera ready. Added more baselines, combining GANs with
  Neural Motifs and t-sne visualizations. Code is available at
  https://github.com/bknyaz/sgg
- **Journal**: None
- **Summary**: Inferring objects and their relationships from an image in the form of a scene graph is useful in many applications at the intersection of vision and language. We consider a challenging problem of compositional generalization that emerges in this task due to a long tail data distribution. Current scene graph generation models are trained on a tiny fraction of the distribution corresponding to the most frequent compositions, e.g. <cup, on, table>. However, test images might contain zero- and few-shot compositions of objects and relationships, e.g. <cup, on, surfboard>. Despite each of the object categories and the predicate (e.g. 'on') being frequent in the training data, the models often fail to properly understand such unseen or rare compositions. To improve generalization, it is natural to attempt increasing the diversity of the training distribution. However, in the graph domain this is non-trivial. To that end, we propose a method to synthesize rare yet plausible scene graphs by perturbing real ones. We then propose and empirically study a model based on conditional generative adversarial networks (GANs) that allows us to generate visual features of perturbed scene graphs and learn from them in a joint fashion. When evaluated on the Visual Genome dataset, our approach yields marginal, but consistent improvements in zero- and few-shot metrics. We analyze the limitations of our approach indicating promising directions for future research.



### Exploit the potential of Multi-column architecture for Crowd Counting
- **Arxiv ID**: http://arxiv.org/abs/2007.05779v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.05779v2)
- **Published**: 2020-07-11 14:08:25+00:00
- **Updated**: 2020-07-28 09:52:38+00:00
- **Authors**: Junhao Cheng, Zhuojun Chen, XinYu Zhang, Yizhou Li, Xiaoyuan Jing
- **Comment**: None
- **Journal**: None
- **Summary**: Crowd counting is an important yet challenging task in computer vision due to serious occlusions, complex background and large scale variations, etc. Multi-column architecture is widely adopted to overcome these challenges, yielding state-of-the-art performance in many public benchmarks. However, there still are two issues in such design: scale limitation and feature similarity. Further performance improvements are thus restricted. In this paper, we propose a novel crowd counting framework called Pyramid Scale Network (PSNet) to explicitly address these issues. Specifically, for scale limitation, we adopt three Pyramid Scale Modules (PSM) to efficiently capture multi-scale features, which integrate a message passing mechanism and an attention mechanism into multi-column architecture. Moreover, for feature similarity, a novel loss function named Multi-column variance loss is introduced to make the features learned by each column in PSM appropriately different from each other. To the best of our knowledge, PSNet is the first work to explicitly address scale limitation and feature similarity in multi-column design. Extensive experiments on five benchmark datasets demonstrate the effectiveness of the proposed innovations as well as the superior performance over the state-of-the-art. Our code is publicly available at: https://github.com/oahunc/Pyramid_Scale_Network



### Incorporating Learnable Membrane Time Constant to Enhance Learning of Spiking Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2007.05785v5
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.05785v5)
- **Published**: 2020-07-11 14:35:42+00:00
- **Updated**: 2021-08-17 03:04:45+00:00
- **Authors**: Wei Fang, Zhaofei Yu, Yanqi Chen, Timothee Masquelier, Tiejun Huang, Yonghong Tian
- **Comment**: Accepted by International Conference on Computer Vision (ICCV) 2021
- **Journal**: None
- **Summary**: Spiking Neural Networks (SNNs) have attracted enormous research interest due to temporal information processing capability, low power consumption, and high biological plausibility. However, the formulation of efficient and high-performance learning algorithms for SNNs is still challenging. Most existing learning methods learn weights only, and require manual tuning of the membrane-related parameters that determine the dynamics of a single spiking neuron. These parameters are typically chosen to be the same for all neurons, which limits the diversity of neurons and thus the expressiveness of the resulting SNNs. In this paper, we take inspiration from the observation that membrane-related parameters are different across brain regions, and propose a training algorithm that is capable of learning not only the synaptic weights but also the membrane time constants of SNNs. We show that incorporating learnable membrane time constants can make the network less sensitive to initial values and can speed up learning. In addition, we reevaluate the pooling methods in SNNs and find that max-pooling will not lead to significant information loss and have the advantage of low computation cost and binary compatibility. We evaluate the proposed method for image classification tasks on both traditional static MNIST, Fashion-MNIST, CIFAR-10 datasets, and neuromorphic N-MNIST, CIFAR10-DVS, DVS128 Gesture datasets. The experiment results show that the proposed method outperforms the state-of-the-art accuracy on nearly all datasets, using fewer time-steps. Our codes are available at https://github.com/fangwei123456/Parametric-Leaky-Integrate-and-Fire-Spiking-Neuron.



### Generalization of Deep Convolutional Neural Networks -- A Case-study on Open-source Chest Radiographs
- **Arxiv ID**: http://arxiv.org/abs/2007.05786v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.05786v1)
- **Published**: 2020-07-11 14:37:28+00:00
- **Updated**: 2020-07-11 14:37:28+00:00
- **Authors**: Nazanin Mashhaditafreshi, Amara Tariq, Judy Wawira Gichoya, Imon Banerjee
- **Comment**: None
- **Journal**: None
- **Summary**: Deep Convolutional Neural Networks (DCNNs) have attracted extensive attention and been applied in many areas, including medical image analysis and clinical diagnosis. One major challenge is to conceive a DCNN model with remarkable performance on both internal and external data. We demonstrate that DCNNs may not generalize to new data, but increasing the quality and heterogeneity of the training data helps to improve the generalizibility factor. We use InceptionResNetV2 and DenseNet121 architectures to predict the risk of 5 common chest pathologies. The experiments were conducted on three publicly available databases: CheXpert, ChestX-ray14, and MIMIC Chest Xray JPG. The results show the internal performance of each of the 5 pathologies outperformed external performance on both of the models. Moreover, our strategy of exposing the models to a mix of different datasets during the training phase helps to improve model performance on the external dataset.



### Decoupling Inherent Risk and Early Cancer Signs in Image-based Breast Cancer Risk Models
- **Arxiv ID**: http://arxiv.org/abs/2007.05791v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.05791v4)
- **Published**: 2020-07-11 15:04:28+00:00
- **Updated**: 2020-09-16 12:33:52+00:00
- **Authors**: Yue Liu, Hossein Azizpour, Fredrik Strand, Kevin Smith
- **Comment**: Accepted by MICCAI 2020 (Medical Image Computing and Computer
  Assisted Intervention)
- **Journal**: None
- **Summary**: The ability to accurately estimate risk of developing breast cancer would be invaluable for clinical decision-making. One promising new approach is to integrate image-based risk models based on deep neural networks. However, one must take care when using such models, as selection of training data influences the patterns the network will learn to identify. With this in mind, we trained networks using three different criteria to select the positive training data (i.e. images from patients that will develop cancer): an inherent risk model trained on images with no visible signs of cancer, a cancer signs model trained on images containing cancer or early signs of cancer, and a conflated model trained on all images from patients with a cancer diagnosis. We find that these three models learn distinctive features that focus on different patterns, which translates to contrasts in performance. Short-term risk is best estimated by the cancer signs model, whilst long-term risk is best estimated by the inherent risk model. Carelessly training with all images conflates inherent risk with early cancer signs, and yields sub-optimal estimates in both regimes. As a consequence, conflated models may lead physicians to recommend preventative action when early cancer signs are already visible.



### Nodule2vec: a 3D Deep Learning System for Pulmonary Nodule Retrieval Using Semantic Representation
- **Arxiv ID**: http://arxiv.org/abs/2007.07081v1
- **DOI**: None
- **Categories**: **cs.IR**, cs.CV, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.07081v1)
- **Published**: 2020-07-11 16:26:16+00:00
- **Updated**: 2020-07-11 16:26:16+00:00
- **Authors**: Ilia Kravets, Tal Heletz, Hayit Greenspan
- **Comment**: to appear at MICCAI 2020
- **Journal**: None
- **Summary**: Content-based retrieval supports a radiologist decision making process by presenting the doctor the most similar cases from the database containing both historical diagnosis and further disease development history. We present a deep learning system that transforms a 3D image of a pulmonary nodule from a CT scan into a low-dimensional embedding vector. We demonstrate that such a vector representation preserves semantic information about the nodule and offers a viable approach for content-based image retrieval (CBIR). We discuss the theoretical limitations of the available datasets and overcome them by applying transfer learning of the state-of-the-art lung nodule detection model. We evaluate the system using the LIDC-IDRI dataset of thoracic CT scans. We devise a similarity score and show that it can be utilized to measure similarity 1) between annotations of the same nodule by different radiologists and 2) between the query nodule and the top four CBIR results. A comparison between doctors and algorithm scores suggests that the benefit provided by the system to the radiologist end-user is comparable to obtaining a second radiologist's opinion.



### Look and Listen: A Multi-modality Late Fusion Approach to Scene Classification for Autonomous Machines
- **Arxiv ID**: http://arxiv.org/abs/2007.10175v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2007.10175v1)
- **Published**: 2020-07-11 16:47:05+00:00
- **Updated**: 2020-07-11 16:47:05+00:00
- **Authors**: Jordan J. Bird, Diego R. Faria, Cristiano Premebida, Anikó Ekárt, George Vogiatzis
- **Comment**: 6 pages, 10 figures, 3 tables
- **Journal**: None
- **Summary**: The novelty of this study consists in a multi-modality approach to scene classification, where image and audio complement each other in a process of deep late fusion. The approach is demonstrated on a difficult classification problem, consisting of two synchronised and balanced datasets of 16,000 data objects, encompassing 4.4 hours of video of 8 environments with varying degrees of similarity. We first extract video frames and accompanying audio at one second intervals. The image and the audio datasets are first classified independently, using a fine-tuned VGG16 and an evolutionary optimised deep neural network, with accuracies of 89.27% and 93.72%, respectively. This is followed by late fusion of the two neural networks to enable a higher order function, leading to accuracy of 96.81% in this multi-modality classifier with synchronised video frames and audio clips. The tertiary neural network implemented for late fusion outperforms classical state-of-the-art classifiers by around 3% when the two primary networks are considered as feature generators. We show that situations where a single-modality may be confused by anomalous data points are now corrected through an emerging higher order integration. Prominent examples include a water feature in a city misclassified as a river by the audio classifier alone and a densely crowded street misclassified as a forest by the image classifier alone. Both are examples which are correctly classified by our multi-modality approach.



### Understanding Object Detection Through An Adversarial Lens
- **Arxiv ID**: http://arxiv.org/abs/2007.05828v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.05828v1)
- **Published**: 2020-07-11 18:41:47+00:00
- **Updated**: 2020-07-11 18:41:47+00:00
- **Authors**: Ka-Ho Chow, Ling Liu, Mehmet Emre Gursoy, Stacey Truex, Wenqi Wei, Yanzhao Wu
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks based object detection models have revolutionized computer vision and fueled the development of a wide range of visual recognition applications. However, recent studies have revealed that deep object detectors can be compromised under adversarial attacks, causing a victim detector to detect no object, fake objects, or mislabeled objects. With object detection being used pervasively in many security-critical applications, such as autonomous vehicles and smart cities, we argue that a holistic approach for an in-depth understanding of adversarial attacks and vulnerabilities of deep object detection systems is of utmost importance for the research community to develop robust defense mechanisms. This paper presents a framework for analyzing and evaluating vulnerabilities of the state-of-the-art object detectors under an adversarial lens, aiming to analyze and demystify the attack strategies, adverse effects, and costs, as well as the cross-model and cross-resolution transferability of attacks. Using a set of quantitative metrics, extensive experiments are performed on six representative deep object detectors from three popular families (YOLOv3, SSD, and Faster R-CNN) with two benchmark datasets (PASCAL VOC and MS COCO). We demonstrate that the proposed framework can serve as a methodical benchmark for analyzing adversarial behaviors and risks in real-time object detection systems. We conjecture that this framework can also serve as a tool to assess the security risks and the adversarial robustness of deep object detectors to be deployed in real-world applications.



### AutoEmbedder: A semi-supervised DNN embedding system for clustering
- **Arxiv ID**: http://arxiv.org/abs/2007.05830v1
- **DOI**: 10.1016/j.knosys.2020.106190
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.05830v1)
- **Published**: 2020-07-11 19:00:45+00:00
- **Updated**: 2020-07-11 19:00:45+00:00
- **Authors**: Abu Quwsar Ohi, M. F. Mridha, Farisa Benta Safir, Md. Abdul Hamid, Muhammad Mostafa Monowar
- **Comment**: The manuscript is accepted and published in Knowledge-Based System
- **Journal**: Knowledge-Based Systems, p.106190 (2020)
- **Summary**: Clustering is widely used in unsupervised learning method that deals with unlabeled data. Deep clustering has become a popular study area that relates clustering with Deep Neural Network (DNN) architecture. Deep clustering method downsamples high dimensional data, which may also relate clustering loss. Deep clustering is also introduced in semi-supervised learning (SSL). Most SSL methods depend on pairwise constraint information, which is a matrix containing knowledge if data pairs can be in the same cluster or not. This paper introduces a novel embedding system named AutoEmbedder, that downsamples higher dimensional data to clusterable embedding points. To the best of our knowledge, this is the first research endeavor that relates to traditional classifier DNN architecture with a pairwise loss reduction technique. The training process is semi-supervised and uses Siamese network architecture to compute pairwise constraint loss in the feature learning phase. The AutoEmbedder outperforms most of the existing DNN based semi-supervised methods tested on famous datasets.



### Distributed optimization for nonrigid nano-tomography
- **Arxiv ID**: http://arxiv.org/abs/2008.03375v2
- **DOI**: 10.1109/TCI.2021.3060915
- **Categories**: **cs.CE**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2008.03375v2)
- **Published**: 2020-07-11 19:22:43+00:00
- **Updated**: 2021-02-28 17:14:31+00:00
- **Authors**: Viktor Nikitin, Vincent De Andrade, Azat Slyamov, Benjamin J. Gould, Yuepeng Zhang, Vandana Sampathkumar, Narayanan Kasthuri, Doga Gursoy, Francesco De Carlo
- **Comment**: None
- **Journal**: None
- **Summary**: Resolution level and reconstruction quality in nano-computed tomography (nano-CT) are in part limited by the stability of microscopes, because the magnitude of mechanical vibrations during scanning becomes comparable to the imaging resolution, and the ability of the samples to resist beam damage during data acquisition. In such cases, there is no incentive in recovering the sample state at different time steps like in time-resolved reconstruction methods, but instead the goal is to retrieve a single reconstruction at the highest possible spatial resolution and without any imaging artifacts. Here we propose a joint solver for imaging samples at the nanoscale with projection alignment, unwarping and regularization. Projection data consistency is regulated by dense optical flow estimated by Farneback's algorithm, leading to sharp sample reconstructions with less artifacts. Synthetic data tests show robustness of the method to Poisson and low-frequency background noise. Applicability of the method is demonstrated on two large-scale nano-imaging experimental data sets.



### Lightweight Modules for Efficient Deep Learning based Image Restoration
- **Arxiv ID**: http://arxiv.org/abs/2007.05835v1
- **DOI**: 10.1109/TCSVT.2020.3007723
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.05835v1)
- **Published**: 2020-07-11 19:35:00+00:00
- **Updated**: 2020-07-11 19:35:00+00:00
- **Authors**: Avisek Lahiri, Sourav Bairagya, Sutanu Bera, Siddhant Haldar, Prabir Kumar Biswas
- **Comment**: Accepted at: IEEE Transactions on Circuits and Systems for Video
  Technology (Early Access Print) | |Codes Available at:
  https://github.com/avisekiit/TCSVT-LightWeight-CNNs | Supplementary Document
  at:
  https://drive.google.com/file/d/1BQhkh33Sen-d0qOrjq5h8ahw2VCUIVLg/view?usp=sharing
- **Journal**: None
- **Summary**: Low level image restoration is an integral component of modern artificial intelligence (AI) driven camera pipelines. Most of these frameworks are based on deep neural networks which present a massive computational overhead on resource constrained platform like a mobile phone. In this paper, we propose several lightweight low-level modules which can be used to create a computationally low cost variant of a given baseline model. Recent works for efficient neural networks design have mainly focused on classification. However, low-level image processing falls under the image-to-image' translation genre which requires some additional computational modules not present in classification. This paper seeks to bridge this gap by designing generic efficient modules which can replace essential components used in contemporary deep learning based image restoration networks. We also present and analyse our results highlighting the drawbacks of applying depthwise separable convolutional kernel (a popular method for efficient classification network) for sub-pixel convolution based upsampling (a popular upsampling strategy for low-level vision applications). This shows that concepts from domain of classification cannot always be seamlessly integrated into image-to-image translation tasks. We extensively validate our findings on three popular tasks of image inpainting, denoising and super-resolution. Our results show that proposed networks consistently output visually similar reconstructions compared to full capacity baselines with significant reduction of parameters, memory footprint and execution speeds on contemporary mobile devices.



### Meta Soft Label Generation for Noisy Labels
- **Arxiv ID**: http://arxiv.org/abs/2007.05836v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.05836v2)
- **Published**: 2020-07-11 19:37:44+00:00
- **Updated**: 2021-01-19 09:59:38+00:00
- **Authors**: Görkem Algan, Ilkay Ulusoy
- **Comment**: Accepted by ICPR 2020
- **Journal**: None
- **Summary**: The existence of noisy labels in the dataset causes significant performance degradation for deep neural networks (DNNs). To address this problem, we propose a Meta Soft Label Generation algorithm called MSLG, which can jointly generate soft labels using meta-learning techniques and learn DNN parameters in an end-to-end fashion. Our approach adapts the meta-learning paradigm to estimate optimal label distribution by checking gradient directions on both noisy training data and noise-free meta-data. In order to iteratively update soft labels, meta-gradient descent step is performed on estimated labels, which would minimize the loss of noise-free meta samples. In each iteration, the base classifier is trained on estimated meta labels. MSLG is model-agnostic and can be added on top of any existing model at hand with ease. We performed extensive experiments on CIFAR10, Clothing1M and Food101N datasets. Results show that our approach outperforms other state-of-the-art methods by a large margin.



### Representation Learning via Adversarially-Contrastive Optimal Transport
- **Arxiv ID**: http://arxiv.org/abs/2007.05840v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.05840v1)
- **Published**: 2020-07-11 19:46:18+00:00
- **Updated**: 2020-07-11 19:46:18+00:00
- **Authors**: Anoop Cherian, Shuchin Aeron
- **Comment**: Accepted at ICML 2020
- **Journal**: None
- **Summary**: In this paper, we study the problem of learning compact (low-dimensional) representations for sequential data that captures its implicit spatio-temporal cues. To maximize extraction of such informative cues from the data, we set the problem within the context of contrastive representation learning and to that end propose a novel objective via optimal transport. Specifically, our formulation seeks a low-dimensional subspace representation of the data that jointly (i) maximizes the distance of the data (embedded in this subspace) from an adversarial data distribution under the optimal transport, a.k.a. the Wasserstein distance, (ii) captures the temporal order, and (iii) minimizes the data distortion. To generate the adversarial distribution, we propose a novel framework connecting Wasserstein GANs with a classifier, allowing a principled mechanism for producing good negative distributions for contrastive learning, which is currently a challenging problem. Our full objective is cast as a subspace learning problem on the Grassmann manifold and solved via Riemannian optimization. To empirically study our formulation, we provide experiments on the task of human action recognition in video sequences. Our results demonstrate competitive performance against challenging baselines.



### FocusLiteNN: High Efficiency Focus Quality Assessment for Digital Pathology
- **Arxiv ID**: http://arxiv.org/abs/2007.06565v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.06565v2)
- **Published**: 2020-07-11 20:52:01+00:00
- **Updated**: 2020-10-01 17:21:55+00:00
- **Authors**: Zhongling Wang, Mahdi S. Hosseini, Adyn Miles, Konstantinos N. Plataniotis, Zhou Wang
- **Comment**: To be published in the 23rd International Conference on Medical Image
  Computing and Computer Assisted Intervention (MICCAI) 2020
- **Journal**: None
- **Summary**: Out-of-focus microscopy lens in digital pathology is a critical bottleneck in high-throughput Whole Slide Image (WSI) scanning platforms, for which pixel-level automated Focus Quality Assessment (FQA) methods are highly desirable to help significantly accelerate the clinical workflows. Existing FQA methods include both knowledge-driven and data-driven approaches. While data-driven approaches such as Convolutional Neural Network (CNN) based methods have shown great promises, they are difficult to use in practice due to their high computational complexity and lack of transferability. Here, we propose a highly efficient CNN-based model that maintains fast computations similar to the knowledge-driven methods without excessive hardware requirements such as GPUs. We create a training dataset using FocusPath which encompasses diverse tissue slides across nine different stain colors, where the stain diversity greatly helps the model to learn diverse color spectrum and tissue structures. In our attempt to reduce the CNN complexity, we find with surprise that even trimming down the CNN to the minimal level, it still achieves a highly competitive performance. We introduce a novel comprehensive evaluation dataset, the largest of its kind, annotated and compiled from TCGA repository for model assessment and comparison, for which the proposed method exhibits superior precision-speed trade-off when compared with existing knowledge-driven and data-driven FQA approaches.



### Complex Wavelet SSIM based Image Data Augmentation
- **Arxiv ID**: http://arxiv.org/abs/2007.05853v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.05853v1)
- **Published**: 2020-07-11 21:11:46+00:00
- **Updated**: 2020-07-11 21:11:46+00:00
- **Authors**: Ritin Raveendran, Aviral Singh, Rajesh Kumar M
- **Comment**: None
- **Journal**: None
- **Summary**: One of the biggest problems in neural learning networks is the lack of training data available to train the network. Data augmentation techniques over the past few years, have therefore been developed, aiming to increase the amount of artificial training data with the limited number of real world samples. In this paper, we look particularly at the MNIST handwritten dataset an image dataset used for digit recognition, and the methods of data augmentation done on this data set. We then take a detailed look into one of the most popular augmentation techniques used for this data set elastic deformation; and highlight its demerit of degradation in the quality of data, which introduces irrelevant data to the training set. To decrease this irrelevancy, we propose to use a similarity measure called Complex Wavelet Structural Similarity Index Measure (CWSSIM) to selectively filter out the irrelevant data before we augment the data set. We compare our observations with the existing augmentation technique and find our proposed method works yields better results than the existing technique.



### Efficient resource management in UAVs for Visual Assistance
- **Arxiv ID**: http://arxiv.org/abs/2007.05854v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.05854v3)
- **Published**: 2020-07-11 21:12:24+00:00
- **Updated**: 2020-08-04 17:12:26+00:00
- **Authors**: Bapireddy Karri
- **Comment**: 7 pages
- **Journal**: None
- **Summary**: There is an increased interest in the use of Unmanned Aerial Vehicles (UAVs) for agriculture, military, disaster management and aerial photography around the world. UAVs are scalable, flexible and are useful in various environments where direct human intervention is difficult. In general, the use of UAVs with cameras mounted to them has increased in number due to their wide range of applications in real life scenarios. With the advent of deep learning models in computer vision many models have shown great success in visual tasks. But most of evaluation models are done on high end CPUs and GPUs. One of major challenges in using UAVs for Visual Assistance tasks in real time is managing the memory usage and power consumption of the these tasks which are computationally intensive and are difficult to be performed on low end processor board of the UAV. This projects describes a novel method to optimize the general image processing tasks like object tracking and object detection for UAV hardware in real time scenarios without affecting the flight time and not tampering the latency and accuracy of these models.



### Anomaly Detection-Based Unknown Face Presentation Attack Detection
- **Arxiv ID**: http://arxiv.org/abs/2007.05856v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.05856v1)
- **Published**: 2020-07-11 21:20:55+00:00
- **Updated**: 2020-07-11 21:20:55+00:00
- **Authors**: Yashasvi Baweja, Poojan Oza, Pramuditha Perera, Vishal M. Patel
- **Comment**: None
- **Journal**: None
- **Summary**: Anomaly detection-based spoof attack detection is a recent development in face Presentation Attack Detection (fPAD), where a spoof detector is learned using only non-attacked images of users. These detectors are of practical importance as they are shown to generalize well to new attack types. In this paper, we present a deep-learning solution for anomaly detection-based spoof attack detection where both classifier and feature representations are learned together end-to-end. First, we introduce a pseudo-negative class during training in the absence of attacked images. The pseudo-negative class is modeled using a Gaussian distribution whose mean is calculated by a weighted running mean. Secondly, we use pairwise confusion loss to further regularize the training process. The proposed approach benefits from the representation learning power of the CNNs and learns better features for fPAD task as shown in our ablation study. We perform extensive experiments on four publicly available datasets: Replay-Attack, Rose-Youtu, OULU-NPU and Spoof in Wild to show the effectiveness of the proposed approach over the previous methods. Code is available at: \url{https://github.com/yashasvi97/IJCB2020_anomaly}



### Bayesian Multi Scale Neural Network for Crowd Counting
- **Arxiv ID**: http://arxiv.org/abs/2007.14245v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.14245v3)
- **Published**: 2020-07-11 21:43:20+00:00
- **Updated**: 2022-05-21 14:54:29+00:00
- **Authors**: Abhinav Sagar
- **Comment**: This work makes assumptions which were found wrong later by the
  author
- **Journal**: None
- **Summary**: Crowd Counting is a difficult but important problem in computer vision. Convolutional Neural Networks based on estimating the density map over the image has been highly successful in this domain. However dense crowd counting remains an open problem because of severe occlusion and perspective view in which people can be present at various sizes. In this work, we propose a new network which uses a ResNet based feature extractor, downsampling block which uses dilated convolutions and upsampling block using transposed convolutions. We present a novel aggregation module which makes our network robust to the perspective view problem. We present the optimization details, loss functions and the algorithm used in our work. On evaluating on ShanghaiTech, UCF-CC-50 and UCF-QNRF datasets using MSE and MAE as evaluation metrics, our network outperforms previous state of the art approaches while giving uncertainty estimates in a principled bayesian manner.



