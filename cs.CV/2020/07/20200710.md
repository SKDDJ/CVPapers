# Arxiv Papers in cs.CV on 2020-07-10
### FC2RN: A Fully Convolutional Corner Refinement Network for Accurate Multi-Oriented Scene Text Detection
- **Arxiv ID**: http://arxiv.org/abs/2007.05113v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.05113v1)
- **Published**: 2020-07-10 00:04:24+00:00
- **Updated**: 2020-07-10 00:04:24+00:00
- **Authors**: Xugong Qin, Yu Zhou, Dayan Wu, Yinliang Yue, Weiping Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Recent scene text detection works mainly focus on curve text detection. However, in real applications, the curve texts are more scarce than the multi-oriented ones. Accurate detection of multi-oriented text with large variations of scales, orientations, and aspect ratios is of great significance. Among the multi-oriented detection methods, direct regression for the geometry of scene text shares a simple yet powerful pipeline and gets popular in academic and industrial communities, but it may produce imperfect detections, especially for long texts due to the limitation of the receptive field. In this work, we aim to improve this while keeping the pipeline simple. A fully convolutional corner refinement network (FC2RN) is proposed for accurate multi-oriented text detection, in which an initial corner prediction and a refined corner prediction are obtained at one pass. With a novel quadrilateral RoI convolution operation tailed for multi-oriented scene text, the initial quadrilateral prediction is encoded into the feature maps which can be further used to predict offset between the initial prediction and the ground-truth as well as output a refined confidence score. Experimental results on four public datasets including MSRA-TD500, ICDAR2017-RCTW, ICDAR2015, and COCO-Text demonstrate that FC2RN can outperform the state-of-the-art methods. The ablation study shows the effectiveness of corner refinement and scoring for accurate text localization.



### Improving Adversarial Robustness by Enforcing Local and Global Compactness
- **Arxiv ID**: http://arxiv.org/abs/2007.05123v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.05123v1)
- **Published**: 2020-07-10 00:43:06+00:00
- **Updated**: 2020-07-10 00:43:06+00:00
- **Authors**: Anh Bui, Trung Le, He Zhao, Paul Montague, Olivier deVel, Tamas Abraham, Dinh Phung
- **Comment**: Proceeding of the European Conference on Computer Vision (ECCV) 2020
- **Journal**: None
- **Summary**: The fact that deep neural networks are susceptible to crafted perturbations severely impacts the use of deep learning in certain domains of application. Among many developed defense models against such attacks, adversarial training emerges as the most successful method that consistently resists a wide range of attacks. In this work, based on an observation from a previous study that the representations of a clean data example and its adversarial examples become more divergent in higher layers of a deep neural net, we propose the Adversary Divergence Reduction Network which enforces local/global compactness and the clustering assumption over an intermediate layer of a deep neural network. We conduct comprehensive experiments to understand the isolating behavior of each component (i.e., local/global compactness and the clustering assumption) and compare our proposed model with state-of-the-art adversarial training methods. The experimental results demonstrate that augmenting adversarial training with our proposed components can further improve the robustness of the network, leading to higher unperturbed and adversarial predictive performances.



### Multi-level Cross-modal Interaction Network for RGB-D Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2007.14352v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.14352v2)
- **Published**: 2020-07-10 02:21:02+00:00
- **Updated**: 2021-03-08 08:38:34+00:00
- **Authors**: Zhou Huang, Huai-Xin Chen, Tao Zhou, Yun-Zhi Yang, Bi-Yuan Liu
- **Comment**: None
- **Journal**: Neurocomputing 2021
- **Summary**: Depth cues with affluent spatial information have been proven beneficial in boosting salient object detection (SOD), while the depth quality directly affects the subsequent SOD performance. However, it is inevitable to obtain some low-quality depth cues due to limitations of its acquisition devices, which can inhibit the SOD performance. Besides, existing methods tend to combine RGB images and depth cues in a direct fusion or a simple fusion module, which makes they can not effectively exploit the complex correlations between the two sources. Moreover, few methods design an appropriate module to fully fuse multi-level features, resulting in cross-level feature interaction insufficient. To address these issues, we propose a novel Multi-level Cross-modal Interaction Network (MCINet) for RGB-D based SOD. Our MCI-Net includes two key components: 1) a cross-modal feature learning network, which is used to learn the high-level features for the RGB images and depth cues, effectively enabling the correlations between the two sources to be exploited; and 2) a multi-level interactive integration network, which integrates multi-level cross-modal features to boost the SOD performance. Extensive experiments on six benchmark datasets demonstrate the superiority of our MCI-Net over 14 state-of-the-art methods, and validate the effectiveness of different components in our MCI-Net. More important, our MCI-Net significantly improves the SOD performance as well as has a higher FPS.



### Optical Flow Distillation: Towards Efficient and Stable Video Style Transfer
- **Arxiv ID**: http://arxiv.org/abs/2007.05146v2
- **DOI**: 10.1007/978-3-030-58539-6_37
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.05146v2)
- **Published**: 2020-07-10 03:00:33+00:00
- **Updated**: 2020-07-22 07:24:29+00:00
- **Authors**: Xinghao Chen, Yiman Zhang, Yunhe Wang, Han Shu, Chunjing Xu, Chang Xu
- **Comment**: None
- **Journal**: ECCV 2020
- **Summary**: Video style transfer techniques inspire many exciting applications on mobile devices. However, their efficiency and stability are still far from satisfactory. To boost the transfer stability across frames, optical flow is widely adopted, despite its high computational complexity, e.g. occupying over 97% inference time. This paper proposes to learn a lightweight video style transfer network via knowledge distillation paradigm. We adopt two teacher networks, one of which takes optical flow during inference while the other does not. The output difference between these two teacher networks highlights the improvements made by optical flow, which is then adopted to distill the target student network. Furthermore, a low-rank distillation loss is employed to stabilize the output of student network by mimicking the rank of input videos. Extensive experiments demonstrate that our student network without an optical flow module is still able to generate stable video and runs much faster than the teacher network.



### Localized Motion Artifact Reduction on Brain MRI Using Deep Learning with Effective Data Augmentation Techniques
- **Arxiv ID**: http://arxiv.org/abs/2007.05149v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.05149v2)
- **Published**: 2020-07-10 03:30:27+00:00
- **Updated**: 2020-10-30 18:28:49+00:00
- **Authors**: Yijun Zhao, Jacek Ossowski, Xuming Wang, Shangjin Li, Orrin Devinsky, Samantha P. Martin, Heath R. Pardoe
- **Comment**: 11 pages, 8 figures
- **Journal**: None
- **Summary**: In-scanner motion degrades the quality of magnetic resonance imaging (MRI) thereby reducing its utility in the detection of clinically relevant abnormalities. We introduce a deep learning-based MRI artifact reduction model (DMAR) to localize and correct head motion artifacts in brain MRI scans. Our approach integrates the latest advances in object detection and noise reduction in Computer Vision. Specifically, DMAR employs a two-stage approach: in the first, degraded regions are detected using the Single Shot Multibox Detector (SSD), and in the second, the artifacts within the found regions are reduced using a convolutional autoencoder (CAE). We further introduce a set of novel data augmentation techniques to address the high dimensionality of MRI images and the scarcity of available data. As a result, our model was trained on a large synthetic dataset of 225,000 images generated from 375 whole brain T1-weighted MRI scans. DMAR visibly reduces image artifacts when applied to both synthetic test images and 55 real-world motion-affected slices from 18 subjects from the multi-center Autism Brain Imaging Data Exchange (ABIDE) study. Quantitatively, depending on the level of degradation, our model achieves a 27.8%-48.1% reduction in RMSE and a 2.88--5.79 dB gain in PSNR on a 5000-sample set of synthetic images. For real-world artifact-affected scans from ABIDE, our model reduced the variance of image voxel intensity within artifact-affected brain regions (p = 0.014).



### Loss Function Search for Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2007.06542v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.06542v1)
- **Published**: 2020-07-10 03:40:10+00:00
- **Updated**: 2020-07-10 03:40:10+00:00
- **Authors**: Xiaobo Wang, Shuo Wang, Cheng Chi, Shifeng Zhang, Tao Mei
- **Comment**: Accepted by ICML2020. arXiv admin note: substantial text overlap with
  arXiv:1912.00833; text overlap with arXiv:1905.07375 by other authors
- **Journal**: None
- **Summary**: In face recognition, designing margin-based (e.g., angular, additive, additive angular margins) softmax loss functions plays an important role in learning discriminative features. However, these hand-crafted heuristic methods are sub-optimal because they require much effort to explore the large design space. Recently, an AutoML for loss function search method AM-LFS has been derived, which leverages reinforcement learning to search loss functions during the training process. But its search space is complex and unstable that hindering its superiority. In this paper, we first analyze that the key to enhance the feature discrimination is actually \textbf{how to reduce the softmax probability}. We then design a unified formulation for the current margin-based softmax losses. Accordingly, we define a novel search space and develop a reward-guided search method to automatically obtain the best candidate. Experimental results on a variety of face recognition benchmarks have demonstrated the effectiveness of our method over the state-of-the-art alternatives.



### Self-Reflective Variational Autoencoder
- **Arxiv ID**: http://arxiv.org/abs/2007.05166v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.05166v1)
- **Published**: 2020-07-10 05:05:26+00:00
- **Updated**: 2020-07-10 05:05:26+00:00
- **Authors**: Ifigeneia Apostolopoulou, Elan Rosenfeld, Artur Dubrawski
- **Comment**: None
- **Journal**: None
- **Summary**: The Variational Autoencoder (VAE) is a powerful framework for learning probabilistic latent variable generative models. However, typical assumptions on the approximate posterior distribution of the encoder and/or the prior, seriously restrict its capacity for inference and generative modeling. Variational inference based on neural autoregressive models respects the conditional dependencies of the exact posterior, but this flexibility comes at a cost: such models are expensive to train in high-dimensional regimes and can be slow to produce samples. In this work, we introduce an orthogonal solution, which we call self-reflective inference. By redesigning the hierarchical structure of existing VAE architectures, self-reflection ensures that the stochastic flow preserves the factorization of the exact posterior, sequentially updating the latent codes in a recurrent manner consistent with the generative model. We empirically demonstrate the clear advantages of matching the variational posterior to the exact posterior - on binarized MNIST, self-reflective inference achieves state-of-the art performance without resorting to complex, computationally expensive components such as autoregressive layers. Moreover, we design a variational normalizing flow that employs the proposed architecture, yielding predictive benefits compared to its purely generative counterpart. Our proposed modification is quite general and complements the existing literature; self-reflective inference can naturally leverage advances in distribution estimation and generative modeling to improve the capacity of each layer in the hierarchy.



### Rain Streak Removal in a Video to Improve Visibility by TAWL Algorithm
- **Arxiv ID**: http://arxiv.org/abs/2007.05167v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.05167v1)
- **Published**: 2020-07-10 05:07:59+00:00
- **Updated**: 2020-07-10 05:07:59+00:00
- **Authors**: Muhammad Rafiqul Islam, Manoranjan Paul
- **Comment**: None
- **Journal**: None
- **Summary**: In computer vision applications, the visibility of the video content is crucial to perform analysis for better accuracy. The visibility can be affected by several atmospheric interferences in challenging weather-one of them is the appearance of rain streak. In recent time, rain streak removal achieves lots of interest to the researchers as it has some exciting applications such as autonomous car, intelligent traffic monitoring system, multimedia, etc. In this paper, we propose a novel and simple method by combining three novel extracted features focusing on temporal appearance, wide shape and relative location of the rain streak and we called it TAWL (Temporal Appearance, Width, and Location) method. The proposed TAWL method adaptively uses features from different resolutions and frame rates. Moreover, it progressively processes features from the up-coming frames so that it can remove rain in the real-time. The experiments have been conducted using video sequences with both real rains and synthetic rains to compare the performance of the proposed method against the relevant state-of-the-art methods. The experimental results demonstrate that the proposed method outperforms the state-of-the-art methods by removing more rain streaks while keeping other moving regions.



### SeqHAND:RGB-Sequence-Based 3D Hand Pose and Shape Estimation
- **Arxiv ID**: http://arxiv.org/abs/2007.05168v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.05168v1)
- **Published**: 2020-07-10 05:11:14+00:00
- **Updated**: 2020-07-10 05:11:14+00:00
- **Authors**: John Yang, Hyung Jin Chang, Seungeui Lee, Nojun Kwak
- **Comment**: None
- **Journal**: None
- **Summary**: 3D hand pose estimation based on RGB images has been studied for a long time. Most of the studies, however, have performed frame-by-frame estimation based on independent static images. In this paper, we attempt to not only consider the appearance of a hand but incorporate the temporal movement information of a hand in motion into the learning framework for better 3D hand pose estimation performance, which leads to the necessity of a large scale dataset with sequential RGB hand images. We propose a novel method that generates a synthetic dataset that mimics natural human hand movements by re-engineering annotations of an extant static hand pose dataset into pose-flows. With the generated dataset, we train a newly proposed recurrent framework, exploiting visuo-temporal features from sequential images of synthetic hands in motion and emphasizing temporal smoothness of estimations with a temporal consistency constraint. Our novel training strategy of detaching the recurrent layer of the framework during domain finetuning from synthetic to real allows preservation of the visuo-temporal features learned from sequential synthetic hand images. Hand poses that are sequentially estimated consequently produce natural and smooth hand movements which lead to more robust estimations. We show that utilizing temporal information for 3D hand pose estimation significantly enhances general pose estimations by outperforming state-of-the-art methods in experiments on hand pose estimation benchmarks.



### Affine Non-negative Collaborative Representation Based Pattern Classification
- **Arxiv ID**: http://arxiv.org/abs/2007.05175v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.05175v1)
- **Published**: 2020-07-10 05:48:54+00:00
- **Updated**: 2020-07-10 05:48:54+00:00
- **Authors**: He-Feng Yin, Xiao-Jun Wu, Zhen-Hua Feng, Josef Kittler
- **Comment**: submitted to the 25th International Conference on Pattern Recognition
  (ICPR2020)
- **Journal**: None
- **Summary**: During the past decade, representation-based classification methods have received considerable attention in pattern recognition. In particular, the recently proposed non-negative representation based classification (NRC) method has been reported to achieve promising results in a wide range of classification tasks. However, NRC has two major drawbacks. First, there is no regularization term in the formulation of NRC, which may result in unstable solution and misclassification. Second, NRC ignores the fact that data usually lies in a union of multiple affine subspaces, rather than linear subspaces in practical applications. To address the above issues, this paper presents an affine non-negative collaborative representation (ANCR) model for pattern classification. To be more specific, ANCR imposes a regularization term on the coding vector. Moreover, ANCR introduces an affine constraint to better represent the data from affine subspaces. The experimental results on several benchmarking datasets demonstrate the merits of the proposed ANCR method. The source code of our ANCR is publicly available at https://github.com/yinhefeng/ANCR.



### Single Image Dehazing Algorithm Based on Sky Region Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2007.06492v1
- **DOI**: 10.1007/978-3-030-35231-8_35
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.06492v1)
- **Published**: 2020-07-10 06:03:55+00:00
- **Updated**: 2020-07-10 06:03:55+00:00
- **Authors**: Weixiang Li, Wei Jie, Somaiyeh MahmoudZadeh
- **Comment**: None
- **Journal**: International Conference on Advanced Data Mining and Applications,
  2019
- **Summary**: In this paper a hybrid image defogging approach based on region segmentation is proposed to address the dark channel priori algorithm's shortcomings in de-fogging the sky regions. The preliminary stage of the proposed approach focuses on the segmentation of sky and non-sky regions in a foggy image taking the advantageous of Meanshift and edge detection with embedded confidence. In the second stage, an improved dark channel priori algorithm is employed to defog the non-sky region. Ultimately, the sky area is processed by DehazeNet algorithm, which relies on deep learning Convolutional Neural Networks. The simulation results show that the proposed hybrid approach in this research addresses the problem of color distortion associated with sky regions in foggy images. The approach greatly improves the image quality indices including entropy information, visibility ratio of the edges, average gradient, and the saturation percentage with a very fast computation time, which is a good indication of the excellent performance of this model.



### Hyperspectral Imaging to detect Age, Defects and Individual Nutrient Deficiency in Grapevine Leaves
- **Arxiv ID**: http://arxiv.org/abs/2007.05197v1
- **DOI**: None
- **Categories**: **q-bio.TO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.05197v1)
- **Published**: 2020-07-10 06:42:24+00:00
- **Updated**: 2020-07-10 06:42:24+00:00
- **Authors**: Manoranjan Paul, Sourabhi Debnath, Tanmoy Debnath, Suzy Rogiers, Tintu Baby, DM Motiur Rahaman, Lihong Zheng, Leigh Schmidtke
- **Comment**: 24 pages, 23 figures
- **Journal**: None
- **Summary**: Hyperspectral (HS) imaging was successfully employed in the 380 nm to 1000 nm wavelength range to investigate the efficacy of detecting age, healthiness and individual nutrient deficiency of grapevine leaves collected from vineyards located in central west NSW, Australia. For age detection, the appearance of many healthy grapevine leaves has been examined. Then visually defective leaves were compared with healthy leaves. Control leaves and individual nutrient-deficient leaves (e.g. N, K and Mg) were also analysed. Several features were employed at various stages in the Ultraviolet (UV), Visible (VIS) and Near Infrared (NIR) regions to evaluate the experimental data: mean brightness, mean 1st derivative brightness, variation index, mean spectral ratio, normalised difference vegetation index (NDVI) and standard deviation (SD). Experiment results demonstrate that these features could be utilised with a high degree of effectiveness to compare age, identify unhealthy samples and not only to distinguish from control and nutrient deficiency but also to identify individual nutrient defects. Therefore, our work corroborated that HS imaging has excellent potential as a non-destructive as well as a non-contact method to detect age, healthiness and individual nutrient deficiencies of grapevine leaves



### ROSE: A Retinal OCT-Angiography Vessel Segmentation Dataset and New Model
- **Arxiv ID**: http://arxiv.org/abs/2007.05201v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.05201v2)
- **Published**: 2020-07-10 06:54:19+00:00
- **Updated**: 2020-12-09 07:45:51+00:00
- **Authors**: Yuhui Ma, Huaying Hao, Huazhu Fu, Jiong Zhang, Jianlong Yang, Jiang Liu, Yalin Zheng, Yitian Zhao
- **Comment**: 10 pages, 9 figures
- **Journal**: None
- **Summary**: Optical Coherence Tomography Angiography (OCT-A) is a non-invasive imaging technique, and has been increasingly used to image the retinal vasculature at capillary level resolution. However, automated segmentation of retinal vessels in OCT-A has been under-studied due to various challenges such as low capillary visibility and high vessel complexity, despite its significance in understanding many eye-related diseases. In addition, there is no publicly available OCT-A dataset with manually graded vessels for training and validation. To address these issues, for the first time in the field of retinal image analysis we construct a dedicated Retinal OCT-A SEgmentation dataset (ROSE), which consists of 229 OCT-A images with vessel annotations at either centerline-level or pixel level. This dataset has been released for public access to assist researchers in the community in undertaking research in related topics. Secondly, we propose a novel Split-based Coarse-to-Fine vessel segmentation network (SCF-Net), with the ability to detect thick and thin vessels separately. In the SCF-Net, a split-based coarse segmentation (SCS) module is first introduced to produce a preliminary confidence map of vessels, and a split-based refinement (SRN) module is then used to optimize the shape/contour of the retinal microvasculature. Thirdly, we perform a thorough evaluation of the state-of-the-art vessel segmentation models and our SCF-Net on the proposed ROSE dataset. The experimental results demonstrate that our SCF-Net yields better vessel segmentation performance in OCT-A than both traditional methods and other deep learning methods.



### OT-driven Multi-Domain Unsupervised Ultrasound Image Artifact Removal using a Single CNN
- **Arxiv ID**: http://arxiv.org/abs/2007.05205v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.05205v1)
- **Published**: 2020-07-10 07:11:04+00:00
- **Updated**: 2020-07-10 07:11:04+00:00
- **Authors**: Jaeyoung Huh, Shujaat Khan, Jong Chul Ye
- **Comment**: None
- **Journal**: None
- **Summary**: Ultrasound imaging (US) often suffers from distinct image artifacts from various sources. Classic approaches for solving these problems are usually model-based iterative approaches that have been developed specifically for each type of artifact, which are often computationally intensive. Recently, deep learning approaches have been proposed as computationally efficient and high performance alternatives. Unfortunately, in the current deep learning approaches, a dedicated neural network should be trained with matched training data for each specific artifact type. This poses a fundamental limitation in the practical use of deep learning for US, since large number of models should be stored to deal with various US image artifacts. Inspired by the recent success of multi-domain image transfer, here we propose a novel, unsupervised, deep learning approach in which a single neural network can be used to deal with different types of US artifacts simply by changing a mask vector that switches between different target domains. Our algorithm is rigorously derived using an optimal transport (OT) theory for cascaded probability measures. Experimental results using phantom and in vivo data demonstrate that the proposed method can generate high quality image by removing distinct artifacts, which are comparable to those obtained by separately trained multiple neural networks.



### Efficient Unpaired Image Dehazing with Cyclic Perceptual-Depth Supervision
- **Arxiv ID**: http://arxiv.org/abs/2007.05220v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.05220v1)
- **Published**: 2020-07-10 07:50:04+00:00
- **Updated**: 2020-07-10 07:50:04+00:00
- **Authors**: Chen Liu, Jiaqi Fan, Guosheng Yin
- **Comment**: None
- **Journal**: None
- **Summary**: Image dehazing without paired haze-free images is of immense importance, as acquiring paired images often entails significant cost. However, we observe that previous unpaired image dehazing approaches tend to suffer from performance degradation near depth borders, where depth tends to vary abruptly. Hence, we propose to anneal the depth border degradation in unpaired image dehazing with cyclic perceptual-depth supervision. Coupled with the dual-path feature re-using backbones of the generators and discriminators, our model achieves $\mathbf{20.36}$ Peak Signal-to-Noise Ratio (PSNR) on NYU Depth V2 dataset, significantly outperforming its predecessors with reduced Floating Point Operations (FLOPs).



### Distillation Guided Residual Learning for Binary Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2007.05223v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.05223v2)
- **Published**: 2020-07-10 07:55:39+00:00
- **Updated**: 2020-07-27 01:21:29+00:00
- **Authors**: Jianming Ye, Shiliang Zhang, Jingdong Wang
- **Comment**: None
- **Journal**: None
- **Summary**: It is challenging to bridge the performance gap between Binary CNN (BCNN) and Floating point CNN (FCNN). We observe that, this performance gap leads to substantial residuals between intermediate feature maps of BCNN and FCNN. To minimize the performance gap, we enforce BCNN to produce similar intermediate feature maps with the ones of FCNN. This training strategy, i.e., optimizing each binary convolutional block with block-wise distillation loss derived from FCNN, leads to a more effective optimization to BCNN. It also motivates us to update the binary convolutional block architecture to facilitate the optimization of block-wise distillation loss. Specifically, a lightweight shortcut branch is inserted into each binary convolutional block to complement residuals at each block. Benefited from its Squeeze-and-Interaction (SI) structure, this shortcut branch introduces a fraction of parameters, e.g., 10\% overheads, but effectively complements the residuals. Extensive experiments on ImageNet demonstrate the superior performance of our method in both classification efficiency and accuracy, e.g., BCNN trained with our methods achieves the accuracy of 60.45\% on ImageNet.



### Automatic Segmentation of Non-Tumor Tissues in Glioma MR Brain Images Using Deformable Registration with Partial Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/2007.05224v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.05224v1)
- **Published**: 2020-07-10 07:58:23+00:00
- **Updated**: 2020-07-10 07:58:23+00:00
- **Authors**: Zhongqiang Liu
- **Comment**: None
- **Journal**: None
- **Summary**: In brain tumor diagnosis and surgical planning, segmentation of tumor regions and accurate analysis of surrounding normal tissues are necessary for physicians. Pathological variability often renders difficulty to register a well-labeled normal atlas to such images and to automatic segment/label surrounding normal brain tissues. In this paper, we propose a new registration approach that first segments brain tumor using a U-Net and then simulates missed normal tissues within the tumor region using a partial convolutional network. Then, a standard normal brain atlas image is registered onto such tumor-removed images in order to segment/label the normal brain tissues. In this way, our new approach greatly reduces the effects of pathological variability in deformable registration and segments the normal tissues surrounding brain tumor well. In experiments, we used MICCAI BraTS2018 T1 tumor images to evaluate the proposed algorithm. By comparing direct registration with the proposed algorithm, the results showed that the Dice coefficient for gray matters was significantly improved for surrounding normal brain tissues.



### Miss the Point: Targeted Adversarial Attack on Multiple Landmark Detection
- **Arxiv ID**: http://arxiv.org/abs/2007.05225v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.05225v1)
- **Published**: 2020-07-10 07:58:35+00:00
- **Updated**: 2020-07-10 07:58:35+00:00
- **Authors**: Qingsong Yao, Zecheng He, Hu Han, S. Kevin Zhou
- **Comment**: accepted by MICCAI2020
- **Journal**: None
- **Summary**: Recent methods in multiple landmark detection based on deep convolutional neural networks (CNNs) reach high accuracy and improve traditional clinical workflow. However, the vulnerability of CNNs to adversarial-example attacks can be easily exploited to break classification and segmentation tasks. This paper is the first to study how fragile a CNN-based model on multiple landmark detection to adversarial perturbations. Specifically, we propose a novel Adaptive Targeted Iterative FGSM (ATI-FGSM) attack against the state-of-the-art models in multiple landmark detection. The attacker can use ATI-FGSM to precisely control the model predictions of arbitrarily selected landmarks, while keeping other stationary landmarks still, by adding imperceptible perturbations to the original image. A comprehensive evaluation on a public dataset for cephalometric landmark detection demonstrates that the adversarial examples generated by ATI-FGSM break the CNN-based network more effectively and efficiently, compared with the original Iterative FGSM attack. Our work reveals serious threats to patients' health. Furthermore, we discuss the limitations of our method and provide potential defense directions, by investigating the coupling effect of nearby landmarks, i.e., a major source of divergence in our experiments. Our source code is available at https://github.com/qsyao/attack_landmark_detection.



### Cross-Attention in Coupled Unmixing Nets for Unsupervised Hyperspectral Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2007.05230v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.05230v3)
- **Published**: 2020-07-10 08:08:20+00:00
- **Updated**: 2020-08-01 12:48:53+00:00
- **Authors**: Jing Yao, Danfeng Hong, Jocelyn Chanussot, Deyu Meng, Xiaoxiang Zhu, Zongben Xu
- **Comment**: None
- **Journal**: None
- **Summary**: The recent advancement of deep learning techniques has made great progress on hyperspectral image super-resolution (HSI-SR). Yet the development of unsupervised deep networks remains challenging for this task. To this end, we propose a novel coupled unmixing network with a cross-attention mechanism, CUCaNet for short, to enhance the spatial resolution of HSI by means of higher-spatial-resolution multispectral image (MSI). Inspired by coupled spectral unmixing, a two-stream convolutional autoencoder framework is taken as backbone to jointly decompose MS and HS data into a spectrally meaningful basis and corresponding coefficients. CUCaNet is capable of adaptively learning spectral and spatial response functions from HS-MS correspondences by enforcing reasonable consistency assumptions on the networks. Moreover, a cross-attention module is devised to yield more effective spatial-spectral information transfer in networks. Extensive experiments are conducted on three widely-used HS-MS datasets in comparison with state-of-the-art HSI-SR models, demonstrating the superiority of the CUCaNet in the HSI-SR application. Furthermore, the codes and datasets will be available at: https://github.com/danfenghong/ECCV2020_CUCaNet.



### Continual Adaptation for Deep Stereo
- **Arxiv ID**: http://arxiv.org/abs/2007.05233v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.05233v3)
- **Published**: 2020-07-10 08:15:58+00:00
- **Updated**: 2021-05-03 07:53:22+00:00
- **Authors**: Matteo Poggi, Alessio Tonioni, Fabio Tosi, Stefano Mattoccia, Luigi Di Stefano
- **Comment**: Extended version of CVPR 2019 paper "Real-time self-adaptive deep
  stereo" - Accepted to TPAMI
- **Journal**: None
- **Summary**: Depth estimation from stereo images is carried out with unmatched results by convolutional neural networks trained end-to-end to regress dense disparities. Like for most tasks, this is possible if large amounts of labelled samples are available for training, possibly covering the whole data distribution encountered at deployment time. Being such an assumption systematically unmet in real applications, the capacity of adapting to any unseen setting becomes of paramount importance. Purposely, we propose a continual adaptation paradigm for deep stereo networks designed to deal with challenging and ever-changing environments. We design a lightweight and modular architecture, Modularly ADaptive Network (MADNet), and formulate Modular ADaptation algorithms (MAD, MAD++) which permit efficient optimization of independent sub-portions of the entire network. In our paradigm, the learning signals needed to continuously adapt models online can be sourced from self-supervision via right-to-left image warping or from traditional stereo algorithms. With both sources, no other data than the input images being gathered at deployment time are needed. Thus, our network architecture and adaptation algorithms realize the first real-time self-adaptive deep stereo system and pave the way for a new paradigm that can facilitate practical deployment of end-to-end architectures for dense disparity regression.



### Using Machine Learning to Detect Ghost Images in Automotive Radar
- **Arxiv ID**: http://arxiv.org/abs/2007.05280v1
- **DOI**: 10.1109/ITSC45102.2020.9294631
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.05280v1)
- **Published**: 2020-07-10 09:51:43+00:00
- **Updated**: 2020-07-10 09:51:43+00:00
- **Authors**: Florian Kraus, Nicolas Scheiner, Werner Ritter, Klaus Dietmayer
- **Comment**: None
- **Journal**: IEEE 23rd International Conference on Intelligent Transportation
  Systems (ITSC), Rhodes, Greece, 2020
- **Summary**: Radar sensors are an important part of driver assistance systems and intelligent vehicles due to their robustness against all kinds of adverse conditions, e.g., fog, snow, rain, or even direct sunlight. This robustness is achieved by a substantially larger wavelength compared to light-based sensors such as cameras or lidars. As a side effect, many surfaces act like mirrors at this wavelength, resulting in unwanted ghost detections. In this article, we present a novel approach to detect these ghost objects by applying data-driven machine learning algorithms. For this purpose, we use a large-scale automotive data set with annotated ghost objects. We show that we can use a state-of-the-art automotive radar classifier in order to detect ghost objects alongside real objects. Furthermore, we are able to reduce the amount of false positive detections caused by ghost images in some settings.



### Deep Learning-Based Regression and Classification for Automatic Landmark Localization in Medical Images
- **Arxiv ID**: http://arxiv.org/abs/2007.05295v1
- **DOI**: 10.1109/TMI.2020.3009002
- **Categories**: **eess.IV**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2007.05295v1)
- **Published**: 2020-07-10 10:46:18+00:00
- **Updated**: 2020-07-10 10:46:18+00:00
- **Authors**: Julia M. H. Noothout, Bob D. de Vos, Jelmer M. Wolterink, Elbrich M. Postma, Paul A. M. Smeets, Richard A. P. Takx, Tim Leiner, Max A. Viergever, Ivana Išgum
- **Comment**: 12 pages, accepted at IEEE transactions in Medical Imaging
- **Journal**: None
- **Summary**: In this study, we propose a fast and accurate method to automatically localize anatomical landmarks in medical images. We employ a global-to-local localization approach using fully convolutional neural networks (FCNNs). First, a global FCNN localizes multiple landmarks through the analysis of image patches, performing regression and classification simultaneously. In regression, displacement vectors pointing from the center of image patches towards landmark locations are determined. In classification, presence of landmarks of interest in the patch is established. Global landmark locations are obtained by averaging the predicted displacement vectors, where the contribution of each displacement vector is weighted by the posterior classification probability of the patch that it is pointing from. Subsequently, for each landmark localized with global localization, local analysis is performed. Specialized FCNNs refine the global landmark locations by analyzing local sub-images in a similar manner, i.e. by performing regression and classification simultaneously and combining the results. Evaluation was performed through localization of 8 anatomical landmarks in CCTA scans, 2 landmarks in olfactory MR scans, and 19 landmarks in cephalometric X-rays. We demonstrate that the method performs similarly to a second observer and is able to localize landmarks in a diverse set of medical images, differing in image modality, image dimensionality, and anatomical coverage.



### Data-Efficient Ranking Distillation for Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2007.05299v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.05299v2)
- **Published**: 2020-07-10 10:59:16+00:00
- **Updated**: 2020-07-13 10:51:04+00:00
- **Authors**: Zakaria Laskar, Juho Kannala
- **Comment**: 10 pages, 2 figures. Edited figure 7
- **Journal**: None
- **Summary**: Recent advances in deep learning has lead to rapid developments in the field of image retrieval. However, the best performing architectures incur significant computational cost. Recent approaches tackle this issue using knowledge distillation to transfer knowledge from a deeper and heavier architecture to a much smaller network. In this paper we address knowledge distillation for metric learning problems. Unlike previous approaches, our proposed method jointly addresses the following constraints i) limited queries to teacher model, ii) black box teacher model with access to the final output representation, and iii) small fraction of original training data without any ground-truth labels. In addition, the distillation method does not require the student and teacher to have same dimensionality. Addressing these constraints reduces computation requirements, dependency on large-scale training datasets and addresses practical scenarios of limited or partial access to private data such as teacher models or the corresponding training data/labels. The key idea is to augment the original training set with additional samples by performing linear interpolation in the final output representation space. Distillation is then performed in the joint space of original and augmented teacher-student sample representations. Results demonstrate that our approach can match baseline models trained with full supervision. In low training sample settings, our approach outperforms the fully supervised approach on two challenging image retrieval datasets, ROxford5k and RParis6k \cite{Roxf} with the least possible teacher supervision.



### TIMELY: Improving Labeling Consistency in Medical Imaging for Cell Type Classification
- **Arxiv ID**: http://arxiv.org/abs/2007.05307v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2007.05307v1)
- **Published**: 2020-07-10 11:13:13+00:00
- **Updated**: 2020-07-10 11:13:13+00:00
- **Authors**: Yushan Liu, Markus M. Geipel, Christoph Tietz, Florian Buettner
- **Comment**: Accepted at ECAI 2020 (24th European Conference on Artificial
  Intelligence)
- **Journal**: None
- **Summary**: Diagnosing diseases such as leukemia or anemia requires reliable counts of blood cells. Hematologists usually label and count microscopy images of blood cells manually. In many cases, however, cells in different maturity states are difficult to distinguish, and in combination with image noise and subjectivity, humans are prone to make labeling mistakes. This results in labels that are often not reproducible, which can directly affect the diagnoses. We introduce TIMELY, a probabilistic model that combines pseudotime inference methods with inhomogeneous hidden Markov trees, which addresses this challenge of label inconsistency. We show first on simulation data that TIMELY is able to identify and correct wrong labels with higher precision and recall than baseline methods for labeling correction. We then apply our method to two real-world datasets of blood cell data and show that TIMELY successfully finds inconsistent labels, thereby improving the quality of human-generated labels.



### Blockchain-Federated-Learning and Deep Learning Models for COVID-19 detection using CT Imaging
- **Arxiv ID**: http://arxiv.org/abs/2007.06537v2
- **DOI**: 10.1109/JSEN.2021.3076767
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.06537v2)
- **Published**: 2020-07-10 11:23:14+00:00
- **Updated**: 2020-12-08 07:52:02+00:00
- **Authors**: Rajesh Kumar, Abdullah Aman Khan, Sinmin Zhang, Jay Kumar, Ting Yang, Noorbakhash Amiri Golalirz, Zakria, Ikram Ali, Sidra Shafiq, WenYong Wang
- **Comment**: arXiv admin note: text overlap with arXiv:2003.10849 by other authors
- **Journal**: None
- **Summary**: With the increase of COVID-19 cases worldwide, an effective way is required to diagnose COVID-19 patients. The primary problem in diagnosing COVID-19 patients is the shortage and reliability of testing kits, due to the quick spread of the virus, medical practitioners are facing difficulty identifying the positive cases. The second real-world problem is to share the data among the hospitals globally while keeping in view the privacy concerns of the organizations. Building a collaborative model and preserving privacy are major concerns for training a global deep learning model. This paper proposes a framework that collects a small amount of data from different sources (various hospitals) and trains a global deep learning model using blockchain based federated learning. Blockchain technology authenticates the data and federated learning trains the model globally while preserving the privacy of the organization. First, we propose a data normalization technique that deals with the heterogeneity of data as the data is gathered from different hospitals having different kinds of CT scanners. Secondly, we use Capsule Network-based segmentation and classification to detect COVID-19 patients. Thirdly, we design a method that can collaboratively train a global model using blockchain technology with federated learning while preserving privacy. Additionally, we collected real-life COVID-19 patients data, which is, open to the research community. The proposed framework can utilize up-to-date data which improves the recognition of computed tomography (CT) images. Finally, our results demonstrate a better performance to detect COVID-19 patients.



### A distance-based loss for smooth and continuous skin layer segmentation in optoacoustic images
- **Arxiv ID**: http://arxiv.org/abs/2007.05324v1
- **DOI**: 10.1007/978-3-030-59725-2_30
- **Categories**: **eess.IV**, cs.CV, I.2.1; J.3
- **Links**: [PDF](http://arxiv.org/pdf/2007.05324v1)
- **Published**: 2020-07-10 12:02:57+00:00
- **Updated**: 2020-07-10 12:02:57+00:00
- **Authors**: Stefan Gerl, Johannes C. Paetzold, Hailong He, Ivan Ezhov, Suprosanna Shit, Florian Kofler, Amirhossein Bayat, Giles Tetteh, Vasilis Ntziachristos, Bjoern Menze
- **Comment**: Accepted at International Conference on Medical Image Computing and
  Computer-Assisted Intervention (MICCAI) 2020
- **Journal**: Medical Image Computing and Computer Assisted Intervention MICCAI
  2020. MICCAI 2020. Lecture Notes in Computer Science, vol 12266. Springer
- **Summary**: Raster-scan optoacoustic mesoscopy (RSOM) is a powerful, non-invasive optical imaging technique for functional, anatomical, and molecular skin and tissue analysis. However, both the manual and the automated analysis of such images are challenging, because the RSOM images have very low contrast, poor signal to noise ratio, and systematic overlaps between the absorption spectra of melanin and hemoglobin. Nonetheless, the segmentation of the epidermis layer is a crucial step for many downstream medical and diagnostic tasks, such as vessel segmentation or monitoring of cancer progression. We propose a novel, shape-specific loss function that overcomes discontinuous segmentations and achieves smooth segmentation surfaces while preserving the same volumetric Dice and IoU. Further, we validate our epidermis segmentation through the sensitivity of vessel segmentation. We found a 20 $\%$ improvement in Dice for vessel segmentation tasks when the epidermis mask is provided as additional information to the vessel segmentation network.



### Are pathologist-defined labels reproducible? Comparison of the TUPAC16 mitotic figure dataset with an alternative set of labels
- **Arxiv ID**: http://arxiv.org/abs/2007.05351v1
- **DOI**: 10.1007/978-3-030-61166-8_22
- **Categories**: **cs.CV**, cs.LG, eess.IV, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2007.05351v1)
- **Published**: 2020-07-10 12:44:54+00:00
- **Updated**: 2020-07-10 12:44:54+00:00
- **Authors**: Christof A. Bertram, Mitko Veta, Christian Marzahl, Nikolas Stathonikos, Andreas Maier, Robert Klopfleisch, Marc Aubreville
- **Comment**: 10 pages, submitted to LABELS@MICCAI 2020
- **Journal**: In: Cardoso J. et al. (eds) Interpretable and Annotation-Efficient
  Learning for Medical Image Computing. IMIMIC 2020, MIL3ID 2020, LABELS 2020.
  Lecture Notes in Computer Science, vol 12446. Springer, Cham
- **Summary**: Pathologist-defined labels are the gold standard for histopathological data sets, regardless of well-known limitations in consistency for some tasks. To date, some datasets on mitotic figures are available and were used for development of promising deep learning-based algorithms. In order to assess robustness of those algorithms and reproducibility of their methods it is necessary to test on several independent datasets. The influence of different labeling methods of these available datasets is currently unknown. To tackle this, we present an alternative set of labels for the images of the auxiliary mitosis dataset of the TUPAC16 challenge. Additional to manual mitotic figure screening, we used a novel, algorithm-aided labeling process, that allowed to minimize the risk of missing rare mitotic figures in the images. All potential mitotic figures were independently assessed by two pathologists. The novel, publicly available set of labels contains 1,999 mitotic figures (+28.80%) and additionally includes 10,483 labels of cells with high similarities to mitotic figures (hard examples). We found significant difference comparing F_1 scores between the original label set (0.549) and the new alternative label set (0.735) using a standard deep learning object detection architecture. The models trained on the alternative set showed higher overall confidence values, suggesting a higher overall label consistency. Findings of the present study show that pathologists-defined labels may vary significantly resulting in notable difference in the model performance. Comparison of deep learning-based algorithms between independent datasets with different labeling methods should be done with caution.



### Spine Landmark Localization with combining of Heatmap Regression and Direct Coordinate Regression
- **Arxiv ID**: http://arxiv.org/abs/2007.05355v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.05355v1)
- **Published**: 2020-07-10 12:47:00+00:00
- **Updated**: 2020-07-10 12:47:00+00:00
- **Authors**: Wanhong Huang, Chunxi Yang, TianHong Hou
- **Comment**: None
- **Journal**: None
- **Summary**: Landmark Localization plays a very important role in processing medical images as well as in disease identification. However, In medical field, it's a challenging task because of the complexity of medical images and the high requirement of accuracy for disease identification and treatment.There are two dominant ways to regress landmark coordination, one using the full convolutional network to regress the heatmaps of landmarks , which is a complex way and heatmap post-process strategies are needed, and the other way is to regress the coordination using CNN + Full Connective Network directly, which is very simple and faster training , but larger dataset and deeper model are needed to achieve higher accuracy. Though with data augmentation and deeper network it can reach a reasonable accuracy, but the accuracy still not reach the requirement of medical field. In addition, a deeper networks also means larger space consumption. To achieve a higher accuracy, we contrived a new landmark regression method which combing heatmap regression and direct coordinate regression base on probability methods and system control theory.



### Progressive Point Cloud Deconvolution Generation Network
- **Arxiv ID**: http://arxiv.org/abs/2007.05361v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.05361v1)
- **Published**: 2020-07-10 13:07:00+00:00
- **Updated**: 2020-07-10 13:07:00+00:00
- **Authors**: Le Hui, Rui Xu, Jin Xie, Jianjun Qian, Jian Yang
- **Comment**: Accepted to ECCV 2020; Project page: https://github.com/fpthink/PDGN
- **Journal**: None
- **Summary**: In this paper, we propose an effective point cloud generation method, which can generate multi-resolution point clouds of the same shape from a latent vector. Specifically, we develop a novel progressive deconvolution network with the learning-based bilateral interpolation. The learning-based bilateral interpolation is performed in the spatial and feature spaces of point clouds so that local geometric structure information of point clouds can be exploited. Starting from the low-resolution point clouds, with the bilateral interpolation and max-pooling operations, the deconvolution network can progressively output high-resolution local and global feature maps. By concatenating different resolutions of local and global feature maps, we employ the multi-layer perceptron as the generation network to generate multi-resolution point clouds. In order to keep the shapes of different resolutions of point clouds consistent, we propose a shape-preserving adversarial loss to train the point cloud deconvolution generation network. Experimental results demonstrate the effectiveness of our proposed method.



### Context-Aware Refinement Network Incorporating Structural Connectivity Prior for Brain Midline Delineation
- **Arxiv ID**: http://arxiv.org/abs/2007.05393v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.05393v1)
- **Published**: 2020-07-10 14:01:20+00:00
- **Updated**: 2020-07-10 14:01:20+00:00
- **Authors**: Shen Wang, Kongming Liang, Yiming Li, Yizhou Yu, Yizhou Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Brain midline delineation can facilitate the clinical evaluation of brain midline shift, which plays an important role in the diagnosis and prognosis of various brain pathology. Nevertheless, there are still great challenges with brain midline delineation, such as the largely deformed midline caused by the mass effect and the possible morphological failure that the predicted midline is not a connected curve. To address these challenges, we propose a context-aware refinement network (CAR-Net) to refine and integrate the feature pyramid representation generated by the UNet. Consequently, the proposed CAR-Net explores more discriminative contextual features and a larger receptive field, which is of great importance to predict largely deformed midline. For keeping the structural connectivity of the brain midline, we introduce a novel connectivity regular loss (CRL) to punish the disconnectivity between adjacent coordinates. Moreover, we address the ignored prerequisite of previous regression-based methods that the brain CT image must be in the standard pose. A simple pose rectification network is presented to align the source input image to the standard pose image. Extensive experimental results on the CQ dataset and one inhouse dataset show that the proposed method requires fewer parameters and outperforms three state-of-the-art methods in terms of four evaluation metrics. Code is available at https://github.com/ShawnBIT/Brain-Midline-Detection.



### VRUNet: Multi-Task Learning Model for Intent Prediction of Vulnerable Road Users
- **Arxiv ID**: http://arxiv.org/abs/2007.05397v1
- **DOI**: 10.2352/ISSN.2470-1173.2020.16.AVM-109
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2007.05397v1)
- **Published**: 2020-07-10 14:02:25+00:00
- **Updated**: 2020-07-10 14:02:25+00:00
- **Authors**: Adithya Ranga, Filippo Giruzzi, Jagdish Bhanushali, Emilie Wirbel, Patrick Pérez, Tuan-Hung Vu, Xavier Perrotton
- **Comment**: This paper is reprinted from, "VRUNet: Multi-Task Learning Model for
  Intent Prediction of Vulnerable Road Users, IS&T Electronic Imaging:
  Autonomous Vehicles and Machines 2020 Proceedings, (IS&T, Springfield, VA,
  2020) page 109-1-10. DOI: 10.2352/ISSN.2470-1173.2020.16.AVM-109." Reprinted
  with permission of The Society for Imaging Science and Technology, holders of
  the 2020 copyright
- **Journal**: None
- **Summary**: Advanced perception and path planning are at the core for any self-driving vehicle. Autonomous vehicles need to understand the scene and intentions of other road users for safe motion planning. For urban use cases it is very important to perceive and predict the intentions of pedestrians, cyclists, scooters, etc., classified as vulnerable road users (VRU). Intent is a combination of pedestrian activities and long term trajectories defining their future motion. In this paper we propose a multi-task learning model to predict pedestrian actions, crossing intent and forecast their future path from video sequences. We have trained the model on naturalistic driving open-source JAAD dataset, which is rich in behavioral annotations and real world scenarios. Experimental results show state-of-the-art performance on JAAD dataset and how we can benefit from jointly learning and predicting actions and trajectories using 2D human pose features and scene context.



### Recognition of Instrument-Tissue Interactions in Endoscopic Videos via Action Triplets
- **Arxiv ID**: http://arxiv.org/abs/2007.05405v1
- **DOI**: 10.1007/978-3-030-59716-0_35
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.05405v1)
- **Published**: 2020-07-10 14:17:10+00:00
- **Updated**: 2020-07-10 14:17:10+00:00
- **Authors**: Chinedu Innocent Nwoye, Cristians Gonzalez, Tong Yu, Pietro Mascagni, Didier Mutter, Jacques Marescaux, Nicolas Padoy
- **Comment**: 13 pages, 4 figures, 6 tables. Accepted and to be published in MICCAI
  2020
- **Journal**: Medical Image Computing and Computer Assisted Intervention MICCAI
  12263 (2020) 364-374
- **Summary**: Recognition of surgical activity is an essential component to develop context-aware decision support for the operating room. In this work, we tackle the recognition of fine-grained activities, modeled as action triplets <instrument, verb, target> representing the tool activity. To this end, we introduce a new laparoscopic dataset, CholecT40, consisting of 40 videos from the public dataset Cholec80 in which all frames have been annotated using 128 triplet classes. Furthermore, we present an approach to recognize these triplets directly from the video data. It relies on a module called Class Activation Guide (CAG), which uses the instrument activation maps to guide the verb and target recognition. To model the recognition of multiple triplets in the same frame, we also propose a trainable 3D Interaction Space, which captures the associations between the triplet components. Finally, we demonstrate the significance of these contributions via several ablation studies and comparisons to baselines on CholecT40.



### Joint Blind Deconvolution and Robust Principal Component Analysis for Blood Flow Estimation in Medical Ultrasound Imaging
- **Arxiv ID**: http://arxiv.org/abs/2007.05428v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.05428v1)
- **Published**: 2020-07-10 15:03:33+00:00
- **Updated**: 2020-07-10 15:03:33+00:00
- **Authors**: Duong-Hung Pham, Adrian Basarab, Ilyess Zemmoura, Jean-Pierre Remenieras, Denis Kouame
- **Comment**: 9 pages, 7 figures
- **Journal**: None
- **Summary**: This paper addresses the problem of high-resolution Doppler blood flow estimation from an ultrafast sequence of ultrasound images. Formulating the separation of clutter and blood components as an inverse problem has been shown in the literature to be a good alternative to spatio-temporal singular value decomposition (SVD)-based clutter filtering. In particular, a deconvolution step has recently been embedded in such a problem to mitigate the influence of the experimentally measured point spread function (PSF) of the imaging system. Deconvolution was shown in this context to improve the accuracy of the blood flow reconstruction. However, measuring the PSF requires non-trivial experimental setups. To overcome this limitation, we propose herein a blind deconvolution method able to estimate both the blood component and the PSF from Doppler data. Numerical experiments conducted on simulated and in vivo data demonstrate qualitatively and quantitatively the effectiveness of the proposed approach in comparison with the previous method based on experimentally measured PSF and two other state-of-the-art approaches.



### Impression Space from Deep Template Network
- **Arxiv ID**: http://arxiv.org/abs/2007.05441v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.05441v1)
- **Published**: 2020-07-10 15:29:33+00:00
- **Updated**: 2020-07-10 15:29:33+00:00
- **Authors**: Gongfan Fang, Xinchao Wang, Haofei Zhang, Jie Song, Mingli Song
- **Comment**: None
- **Journal**: None
- **Summary**: It is an innate ability for humans to imagine something only according to their impression, without having to memorize all the details of what they have seen. In this work, we would like to demonstrate that a trained convolutional neural network also has the capability to "remember" its input images. To achieve this, we propose a simple but powerful framework to establish an {\emph{Impression Space}} upon an off-the-shelf pretrained network. This network is referred to as the {\emph{Template Network}} because its filters will be used as templates to reconstruct images from the impression. In our framework, the impression space and image space are bridged by a layer-wise encoding and iterative decoding process. It turns out that the impression space indeed captures the salient features from images, and it can be directly applied to tasks such as unpaired image translation and image synthesis through impression matching without further network training. Furthermore, the impression naturally constructs a high-level common space for different data. Based on this, we propose a mechanism to model the data relations inside the impression space, which is able to reveal the feature similarity between images. Our code will be released.



### Evaluation of Big Data based CNN Models in Classification of Skin Lesions with Melanoma
- **Arxiv ID**: http://arxiv.org/abs/2007.05446v1
- **DOI**: 10.1007/978-981-15-6321-8
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.05446v1)
- **Published**: 2020-07-10 15:39:32+00:00
- **Updated**: 2020-07-10 15:39:32+00:00
- **Authors**: Prasitthichai Naronglerdrit, Iosif Mporas
- **Comment**: Series Title: Studies in Computational Intelligence, Book Title: Deep
  Learning for Cancer Diagnosis, Series Volume: 908, DOI:
  10.1007/978-981-15-6321-8, eBook ISBN: 978-981-15-6321-8
- **Journal**: None
- **Summary**: This chapter presents a methodology for diagnosis of pigmented skin lesions using convolutional neural networks. The architecture is based on convolu-tional neural networks and it is evaluated using new CNN models as well as re-trained modification of pre-existing CNN models were used. The experi-mental results showed that CNN models pre-trained on big datasets for gen-eral purpose image classification when re-trained in order to identify skin le-sion types offer more accurate results when compared to convolutional neural network models trained explicitly from the dermatoscopic images. The best performance was achieved by re-training a modified version of ResNet-50 convolutional neural network with accuracy equal to 93.89%. Analysis on skin lesion pathology type was also performed with classification accuracy for melanoma and basal cell carcinoma being equal to 79.13% and 82.88%, respectively.



### Weakly Supervised Deep Nuclei Segmentation Using Partial Points Annotation in Histopathology Images
- **Arxiv ID**: http://arxiv.org/abs/2007.05448v1
- **DOI**: 10.1109/TMI.2020.3002244
- **Categories**: **cs.CV**, 68-06
- **Links**: [PDF](http://arxiv.org/pdf/2007.05448v1)
- **Published**: 2020-07-10 15:41:29+00:00
- **Updated**: 2020-07-10 15:41:29+00:00
- **Authors**: Hui Qu, Pengxiang Wu, Qiaoying Huang, Jingru Yi, Zhennan Yan, Kang Li, Gregory M. Riedlinger, Subhajyoti De, Shaoting Zhang, Dimitris N. Metaxas
- **Comment**: 12 pages
- **Journal**: None
- **Summary**: Nuclei segmentation is a fundamental task in histopathology image analysis. Typically, such segmentation tasks require significant effort to manually generate accurate pixel-wise annotations for fully supervised training. To alleviate such tedious and manual effort, in this paper we propose a novel weakly supervised segmentation framework based on partial points annotation, i.e., only a small portion of nuclei locations in each image are labeled. The framework consists of two learning stages. In the first stage, we design a semi-supervised strategy to learn a detection model from partially labeled nuclei locations. Specifically, an extended Gaussian mask is designed to train an initial model with partially labeled data. Then, selftraining with background propagation is proposed to make use of the unlabeled regions to boost nuclei detection and suppress false positives. In the second stage, a segmentation model is trained from the detected nuclei locations in a weakly-supervised fashion. Two types of coarse labels with complementary information are derived from the detected points and are then utilized to train a deep neural network. The fully-connected conditional random field loss is utilized in training to further refine the model without introducing extra computational complexity during inference. The proposed method is extensively evaluated on two nuclei segmentation datasets. The experimental results demonstrate that our method can achieve competitive performance compared to the fully supervised counterpart and the state-of-the-art methods while requiring significantly less annotation effort.



### SIMBA: Specific Identity Markers for Bone Age Assessment
- **Arxiv ID**: http://arxiv.org/abs/2007.05454v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2007.05454v2)
- **Published**: 2020-07-10 15:48:48+00:00
- **Updated**: 2020-07-13 16:30:44+00:00
- **Authors**: Cristina González, María Escobar, Laura Daza, Felipe Torres, Gustavo Triana, Pablo Arbeláez
- **Comment**: Accepted at MICCAI 2020
- **Journal**: None
- **Summary**: Bone Age Assessment (BAA) is a task performed by radiologists to diagnose abnormal growth in a child. In manual approaches, radiologists take into account different identity markers when calculating bone age, i.e., chronological age and gender. However, the current automated Bone Age Assessment methods do not completely exploit the information present in the patient's metadata. With this lack of available methods as motivation, we present SIMBA: Specific Identity Markers for Bone Age Assessment. SIMBA is a novel approach for the task of BAA based on the use of identity markers. For this purpose, we build upon the state-of-the-art model, fusing the information present in the identity markers with the visual features created from the original hand radiograph. We then use this robust representation to estimate the patient's relative bone age: the difference between chronological age and bone age. We validate SIMBA on the Radiological Hand Pose Estimation dataset and find that it outperforms previous state-of-the-art methods. SIMBA sets a trend of a new wave of Computer-aided Diagnosis methods that incorporate all of the data that is available regarding a patient. To promote further research in this area and ensure reproducibility we will provide the source code as well as the pre-trained models of SIMBA.



### ISINet: An Instance-Based Approach for Surgical Instrument Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2007.05533v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.05533v1)
- **Published**: 2020-07-10 16:20:56+00:00
- **Updated**: 2020-07-10 16:20:56+00:00
- **Authors**: Cristina González, Laura Bravo-Sánchez, Pablo Arbelaez
- **Comment**: Accepted at MICCAI2020
- **Journal**: None
- **Summary**: We study the task of semantic segmentation of surgical instruments in robotic-assisted surgery scenes. We propose the Instance-based Surgical Instrument Segmentation Network (ISINet), a method that addresses this task from an instance-based segmentation perspective. Our method includes a temporal consistency module that takes into account the previously overlooked and inherent temporal information of the problem. We validate our approach on the existing benchmark for the task, the Endoscopic Vision 2017 Robotic Instrument Segmentation Dataset, and on the 2018 version of the dataset, whose annotations we extended for the fine-grained version of instrument segmentation. Our results show that ISINet significantly outperforms state-of-the-art methods, with our baseline version duplicating the Intersection over Union (IoU) of previous methods and our complete model triplicating the IoU.



### Geometric Style Transfer
- **Arxiv ID**: http://arxiv.org/abs/2007.05471v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.05471v1)
- **Published**: 2020-07-10 16:33:23+00:00
- **Updated**: 2020-07-10 16:33:23+00:00
- **Authors**: Xiao-Chang Liu, Xuan-Yi Li, Ming-Ming Cheng, Peter Hall
- **Comment**: 10 pages, 12 figures
- **Journal**: None
- **Summary**: Neural style transfer (NST), where an input image is rendered in the style of another image, has been a topic of considerable progress in recent years. Research over that time has been dominated by transferring aspects of color and texture, yet these factors are only one component of style. Other factors of style include composition, the projection system used, and the way in which artists warp and bend objects. Our contribution is to introduce a neural architecture that supports transfer of geometric style. Unlike recent work in this area, we are unique in being general in that we are not restricted by semantic content. This new architecture runs prior to a network that transfers texture style, enabling us to transfer texture to a warped image. This form of network supports a second novelty: we extend the NST input paradigm. Users can input content/style pair as is common, or they can chose to input a content/texture-style/geometry-style triple. This three image input paradigm divides style into two parts and so provides significantly greater versatility to the output we can produce. We provide user studies that show the quality of our output, and quantify the importance of geometric style transfer to style recognition by humans.



### Multi-Domain Image Completion for Random Missing Input Data
- **Arxiv ID**: http://arxiv.org/abs/2007.05534v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.05534v1)
- **Published**: 2020-07-10 16:38:48+00:00
- **Updated**: 2020-07-10 16:38:48+00:00
- **Authors**: Liyue Shen, Wentao Zhu, Xiaosong Wang, Lei Xing, John M. Pauly, Baris Turkbey, Stephanie Anne Harmon, Thomas Hogue Sanford, Sherif Mehralivand, Peter Choyke, Bradford Wood, Daguang Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-domain data are widely leveraged in vision applications taking advantage of complementary information from different modalities, e.g., brain tumor segmentation from multi-parametric magnetic resonance imaging (MRI). However, due to possible data corruption and different imaging protocols, the availability of images for each domain could vary amongst multiple data sources in practice, which makes it challenging to build a universal model with a varied set of input data. To tackle this problem, we propose a general approach to complete the random missing domain(s) data in real applications. Specifically, we develop a novel multi-domain image completion method that utilizes a generative adversarial network (GAN) with a representational disentanglement scheme to extract shared skeleton encoding and separate flesh encoding across multiple domains. We further illustrate that the learned representation in multi-domain image completion could be leveraged for high-level tasks, e.g., segmentation, by introducing a unified framework consisting of image completion and segmentation with a shared content encoder. The experiments demonstrate consistent performance improvement on three datasets for brain tumor segmentation, prostate segmentation, and facial expression image completion respectively.



### STaRFlow: A SpatioTemporal Recurrent Cell for Lightweight Multi-Frame Optical Flow Estimation
- **Arxiv ID**: http://arxiv.org/abs/2007.05481v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.05481v1)
- **Published**: 2020-07-10 17:01:34+00:00
- **Updated**: 2020-07-10 17:01:34+00:00
- **Authors**: Pierre Godet, Alexandre Boulch, Aurélien Plyer, Guy Le Besnerais
- **Comment**: 9 pages, 7 figures, 4 tables
- **Journal**: None
- **Summary**: We present a new lightweight CNN-based algorithm for multi-frame optical flow estimation. Our solution introduces a double recurrence over spatial scale and time through repeated use of a generic "STaR" (SpatioTemporal Recurrent) cell. It includes (i) a temporal recurrence based on conveying learned features rather than optical flow estimates; (ii) an occlusion detection process which is coupled with optical flow estimation and therefore uses a very limited number of extra parameters. The resulting STaRFlow algorithm gives state-of-the-art performances on MPI Sintel and Kitti2015 and involves significantly less parameters than all other methods with comparable results.



### Scientific Discovery by Generating Counterfactuals using Image Translation
- **Arxiv ID**: http://arxiv.org/abs/2007.05500v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.05500v2)
- **Published**: 2020-07-10 17:25:52+00:00
- **Updated**: 2020-07-19 23:38:22+00:00
- **Authors**: Arunachalam Narayanaswamy, Subhashini Venugopalan, Dale R. Webster, Lily Peng, Greg Corrado, Paisan Ruamviboonsuk, Pinal Bavishi, Rory Sayres, Abigail Huang, Siva Balasubramanian, Michael Brenner, Philip Nelson, Avinash V. Varadarajan
- **Comment**: Accepted at MICCAI 2020. This version combines camera-ready and
  supplement
- **Journal**: MICCAI 2020
- **Summary**: Model explanation techniques play a critical role in understanding the source of a model's performance and making its decisions transparent. Here we investigate if explanation techniques can also be used as a mechanism for scientific discovery. We make three contributions: first, we propose a framework to convert predictions from explanation techniques to a mechanism of discovery. Second, we show how generative models in combination with black-box predictors can be used to generate hypotheses (without human priors) that can be critically examined. Third, with these techniques we study classification models for retinal images predicting Diabetic Macular Edema (DME), where recent work showed that a CNN trained on these images is likely learning novel features in the image. We demonstrate that the proposed framework is able to explain the underlying scientific mechanism, thus bridging the gap between the model's performance and human understanding.



### AViD Dataset: Anonymized Videos from Diverse Countries
- **Arxiv ID**: http://arxiv.org/abs/2007.05515v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.05515v3)
- **Published**: 2020-07-10 17:50:38+00:00
- **Updated**: 2020-11-03 15:10:44+00:00
- **Authors**: AJ Piergiovanni, Michael S. Ryoo
- **Comment**: https://github.com/piergiaj/AViD
- **Journal**: NeurIPS 2020
- **Summary**: We introduce a new public video dataset for action recognition: Anonymized Videos from Diverse countries (AViD). Unlike existing public video datasets, AViD is a collection of action videos from many different countries. The motivation is to create a public dataset that would benefit training and pretraining of action recognition models for everybody, rather than making it useful for limited countries. Further, all the face identities in the AViD videos are properly anonymized to protect their privacy. It also is a static dataset where each video is licensed with the creative commons license. We confirm that most of the existing video datasets are statistically biased to only capture action videos from a limited number of countries. We experimentally illustrate that models trained with such biased datasets do not transfer perfectly to action videos from the other countries, and show that AViD addresses such problem. We also confirm that the new AViD dataset could serve as a good dataset for pretraining the models, performing comparably or better than prior datasets.



### Improved Detection of Adversarial Images Using Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2007.05573v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/2007.05573v1)
- **Published**: 2020-07-10 19:02:24+00:00
- **Updated**: 2020-07-10 19:02:24+00:00
- **Authors**: Yutong Gao, Yi Pan
- **Comment**: None
- **Journal**: None
- **Summary**: Machine learning techniques are immensely deployed in both industry and academy. Recent studies indicate that machine learning models used for classification tasks are vulnerable to adversarial examples, which limits the usage of applications in the fields with high precision requirements. We propose a new approach called Feature Map Denoising to detect the adversarial inputs and show the performance of detection on the mixed dataset consisting of adversarial examples generated by different attack algorithms, which can be used to associate with any pre-trained DNNs at a low cost. Wiener filter is also introduced as the denoise algorithm to the defense model, which can further improve performance. Experimental results indicate that good accuracy of detecting the adversarial examples can be achieved through our Feature Map Denoising algorithm.



### Attention-guided Quality Assessment for Automated Cryo-EM Grid Screening
- **Arxiv ID**: http://arxiv.org/abs/2007.05593v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.05593v2)
- **Published**: 2020-07-10 20:11:43+00:00
- **Updated**: 2020-07-21 21:55:17+00:00
- **Authors**: Hong Xu, David E. Timm, Shireen Y. Elhabian
- **Comment**: Accepted for publication in MICCAI 2020, the 23rd International
  Conference on Medical Image Computing and Computer Assisted Intervention
- **Journal**: None
- **Summary**: Cryogenic electron microscopy (cryo-EM) has become an enabling technology in drug discovery and in understanding molecular bases of disease by producing near-atomic resolution (less than 0.4 nm) 3D reconstructions of biological macromolecules. The imaging process required for 3D reconstructions involves a highly iterative and empirical screening process, starting with the acquisition of low magnification images of the cryo-EM grids. These images are inspected for squares that are likely to contain useful molecular signals. Potentially useful squares within the grid are then imaged at progressively higher magnifications, with the goal of identifying sub-micron areas within circular holes (bounded by the squares) for imaging at high magnification. This arduous, multi-step data acquisition process represents a bottleneck for obtaining a high throughput data collection. Here, we focus on automating the early decision making for the microscope operator, scoring low magnification images of squares, and proposing the first deep learning framework, XCryoNet, for automated cryo-EM grid screening. XCryoNet is a semi-supervised, attention-guided deep learning approach that provides explainable scoring of automatically extracted square images using limited amounts of labeled data. Results show up to 8% and 37% improvements over a fully supervised and a no-attention solution, respectively, when labeled data is scarce.



### EMIXER: End-to-end Multimodal X-ray Generation via Self-supervision
- **Arxiv ID**: http://arxiv.org/abs/2007.05597v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.05597v2)
- **Published**: 2020-07-10 20:19:01+00:00
- **Updated**: 2021-01-15 19:07:26+00:00
- **Authors**: Siddharth Biswal, Peiye Zhuang, Ayis Pyrros, Nasir Siddiqui, Sanmi Koyejo, Jimeng Sun
- **Comment**: None
- **Journal**: None
- **Summary**: Deep generative models have enabled the automated synthesis of high-quality data for diverse applications. However, the most effective generative models are specialized to data from a single domain (e.g., images or text). Real-world applications such as healthcare require multi-modal data from multiple domains (e.g., both images and corresponding text), which are difficult to acquire due to limited availability and privacy concerns and are much harder to synthesize. To tackle this joint synthesis challenge, we propose an End-to-end MultImodal X-ray genERative model (EMIXER) for jointly synthesizing x-ray images and corresponding free-text reports, all conditional on diagnosis labels. EMIXER is an conditional generative adversarial model by 1) generating an image based on a label, 2) encoding the image to a hidden embedding, 3) producing the corresponding text via a hierarchical decoder from the image embedding, and 4) a joint discriminator for assessing both the image and the corresponding text. EMIXER also enables self-supervision to leverage vast amount of unlabeled data. Extensive experiments with real X-ray reports data illustrate how data augmentation using synthesized multimodal samples can improve the performance of a variety of supervised tasks including COVID-19 X-ray classification with very limited samples. The quality of generated images and reports are also confirmed by radiologists. We quantitatively show that EMIXER generated synthetic datasets can augment X-ray image classification, report generation models to achieve 5.94% and 6.9% improvement on models trained only on real data samples. Taken together, our results highlight the promise of state of generative models to advance clinical machine learning.



### Neuromorphic Processing and Sensing: Evolutionary Progression of AI to Spiking
- **Arxiv ID**: http://arxiv.org/abs/2007.05606v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.05606v1)
- **Published**: 2020-07-10 20:54:42+00:00
- **Updated**: 2020-07-10 20:54:42+00:00
- **Authors**: Philippe Reiter, Geet Rose Jose, Spyridon Bizmpikis, Ionela-Ancuţa Cîrjilă
- **Comment**: 15 pages, 13 figures
- **Journal**: None
- **Summary**: The increasing rise in machine learning and deep learning applications is requiring ever more computational resources to successfully meet the growing demands of an always-connected, automated world. Neuromorphic technologies based on Spiking Neural Network algorithms hold the promise to implement advanced artificial intelligence using a fraction of the computations and power requirements by modeling the functioning, and spiking, of the human brain. With the proliferation of tools and platforms aiding data scientists and machine learning engineers to develop the latest innovations in artificial and deep neural networks, a transition to a new paradigm will require building from the current well-established foundations. This paper explains the theoretical workings of neuromorphic technologies based on spikes, and overviews the state-of-art in hardware processors, software platforms and neuromorphic sensing devices. A progression path is paved for current machine learning specialists to update their skillset, as well as classification or predictive models from the current generation of deep neural networks to SNNs. This can be achieved by leveraging existing, specialized hardware in the form of SpiNNaker and the Nengo migration toolkit. First-hand, experimental results of converting a VGG-16 neural network to an SNN are shared. A forward gaze into industrial, medical and commercial applications that can readily benefit from SNNs wraps up this investigation into the neuromorphic computing future.



### Cloud Detection through Wavelet Transforms in Machine Learning and Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2007.13678v1
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.13678v1)
- **Published**: 2020-07-10 20:55:11+00:00
- **Updated**: 2020-07-10 20:55:11+00:00
- **Authors**: Philippe Reiter
- **Comment**: 4 pages, 2 figures
- **Journal**: None
- **Summary**: Cloud detection is a specialized application of image recognition and object detection using remotely sensed data. The task presents a number of challenges, including analyzing images obtained in visible, infrared and multi-spectral frequencies, usually without ground truth data for comparison. Moreover, machine learning and deep learning (MLDL) algorithms applied to this task are required to be computationally efficient, as they are typically deployed in low-power devices and called to operate in real-time.   This paper explains Wavelet Transform (WT) theory, comparing it to more widely used image and signal processing transforms, and explores the use of WT as a powerful signal compressor and feature extractor for MLDL classifiers.



### Image Captioning with Compositional Neural Module Networks
- **Arxiv ID**: http://arxiv.org/abs/2007.05608v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2007.05608v1)
- **Published**: 2020-07-10 20:58:04+00:00
- **Updated**: 2020-07-10 20:58:04+00:00
- **Authors**: Junjiao Tian, Jean Oh
- **Comment**: International Joint Conference on Artificial Intelligence (IJCAI-19)
- **Journal**: None
- **Summary**: In image captioning where fluency is an important factor in evaluation, e.g., $n$-gram metrics, sequential models are commonly used; however, sequential models generally result in overgeneralized expressions that lack the details that may be present in an input image. Inspired by the idea of the compositional neural module networks in the visual question answering task, we introduce a hierarchical framework for image captioning that explores both compositionality and sequentiality of natural language. Our algorithm learns to compose a detail-rich sentence by selectively attending to different modules corresponding to unique aspects of each object detected in an input image to include specific descriptions such as counts and color. In a set of experiments on the MSCOCO dataset, the proposed model outperforms a state-of-the art model across multiple evaluation metrics, more importantly, presenting visually interpretable results. Furthermore, the breakdown of subcategories $f$-scores of the SPICE metric and human evaluation on Amazon Mechanical Turk show that our compositional module networks effectively generate accurate and detailed captions.



### Batch-Incremental Triplet Sampling for Training Triplet Networks Using Bayesian Updating Theorem
- **Arxiv ID**: http://arxiv.org/abs/2007.05610v2
- **DOI**: 10.1109/ICPR48806.2021.9412478
- **Categories**: **stat.ML**, cs.CV, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2007.05610v2)
- **Published**: 2020-07-10 21:07:51+00:00
- **Updated**: 2020-10-13 14:13:35+00:00
- **Authors**: Milad Sikaroudi, Benyamin Ghojogh, Fakhri Karray, Mark Crowley, H. R. Tizhoosh
- **Comment**: Accepted for presentation at the 25th International Conference on
  Pattern Recognition (ICPR), IEEE, 2020. The first two authors contributed
  equally to this work
- **Journal**: 25th IEEE International Conference on Pattern Recognition (ICPR),
  pp. 7080-7086, 2020
- **Summary**: Variants of Triplet networks are robust entities for learning a discriminative embedding subspace. There exist different triplet mining approaches for selecting the most suitable training triplets. Some of these mining methods rely on the extreme distances between instances, and some others make use of sampling. However, sampling from stochastic distributions of data rather than sampling merely from the existing embedding instances can provide more discriminative information. In this work, we sample triplets from distributions of data rather than from existing instances. We consider a multivariate normal distribution for the embedding of each class. Using Bayesian updating and conjugate priors, we update the distributions of classes dynamically by receiving the new mini-batches of training data. The proposed triplet mining with Bayesian updating can be used with any triplet-based loss function, e.g., triplet-loss or Neighborhood Component Analysis (NCA) loss. Accordingly, Our triplet mining approaches are called Bayesian Updating Triplet (BUT) and Bayesian Updating NCA (BUNCA), depending on which loss function is being used. Experimental results on two public datasets, namely MNIST and histopathology colorectal cancer (CRC), substantiate the effectiveness of the proposed triplet mining method.



### PCAMs: Weakly Supervised Semantic Segmentation Using Point Supervision
- **Arxiv ID**: http://arxiv.org/abs/2007.05615v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.05615v1)
- **Published**: 2020-07-10 21:25:27+00:00
- **Updated**: 2020-07-10 21:25:27+00:00
- **Authors**: R. Austin McEver, B. S. Manjunath
- **Comment**: None
- **Journal**: None
- **Summary**: Current state of the art methods for generating semantic segmentation rely heavily on a large set of images that have each pixel labeled with a class of interest label or background. Coming up with such labels, especially in domains that require an expert to do annotations, comes at a heavy cost in time and money. Several methods have shown that we can learn semantic segmentation from less expensive image-level labels, but the effectiveness of point level labels, a healthy compromise between all pixels labelled and none, still remains largely unexplored. This paper presents a novel procedure for producing semantic segmentation from images given some point level annotations. This method includes point annotations in the training of a convolutional neural network (CNN) for producing improved localization and class activation maps. Then, we use another CNN for predicting semantic affinities in order to propagate rough class labels and create pseudo semantic segmentation labels. Finally, we propose training a CNN that is normally fully supervised using our pseudo labels in place of ground truth labels, which further improves performance and simplifies the inference process by requiring just one CNN during inference rather than two. Our method achieves state of the art results for point supervised semantic segmentation on the PASCAL VOC 2012 dataset \cite{everingham2010pascal}, even outperforming state of the art methods for stronger bounding box and squiggle supervision.



### Quantization in Relative Gradient Angle Domain For Building Polygon Estimation
- **Arxiv ID**: http://arxiv.org/abs/2007.05617v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.05617v1)
- **Published**: 2020-07-10 21:33:06+00:00
- **Updated**: 2020-07-10 21:33:06+00:00
- **Authors**: Yuhao Chen, Yifan Wu, Linlin Xu, Alexander Wong
- **Comment**: None
- **Journal**: None
- **Summary**: Building footprint extraction in remote sensing data benefits many important applications, such as urban planning and population estimation. Recently, rapid development of Convolutional Neural Networks (CNNs) and open-sourced high resolution satellite building image datasets have pushed the performance boundary further for automated building extractions. However, CNN approaches often generate imprecise building morphologies including noisy edges and round corners. In this paper, we leverage the performance of CNNs, and propose a module that uses prior knowledge of building corners to create angular and concise building polygons from CNN segmentation outputs. We describe a new transform, Relative Gradient Angle Transform (RGA Transform) that converts object contours from time vs. space to time vs. angle. We propose a new shape descriptor, Boundary Orientation Relation Set (BORS), to describe angle relationship between edges in RGA domain, such as orthogonality and parallelism. Finally, we develop an energy minimization framework that makes use of the angle relationship in BORS to straighten edges and reconstruct sharp corners, and the resulting corners create a polygon. Experimental results demonstrate that our method refines CNN output from a rounded approximation to a more clear-cut angular shape of the building footprint.



### Learning Accurate and Human-Like Driving using Semantic Maps and Attention
- **Arxiv ID**: http://arxiv.org/abs/2007.07218v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2007.07218v1)
- **Published**: 2020-07-10 22:25:27+00:00
- **Updated**: 2020-07-10 22:25:27+00:00
- **Authors**: Simon Hecker, Dengxin Dai, Alexander Liniger, Luc Van Gool
- **Comment**: IROS 2020 final version. arXiv admin note: text overlap with
  arXiv:1903.10995
- **Journal**: None
- **Summary**: This paper investigates how end-to-end driving models can be improved to drive more accurately and human-like. To tackle the first issue we exploit semantic and visual maps from HERE Technologies and augment the existing Drive360 dataset with such. The maps are used in an attention mechanism that promotes segmentation confidence masks, thus focusing the network on semantic classes in the image that are important for the current driving situation. Human-like driving is achieved using adversarial learning, by not only minimizing the imitation loss with respect to the human driver but by further defining a discriminator, that forces the driving model to produce action sequences that are human-like. Our models are trained and evaluated on the Drive360 + HERE dataset, which features 60 hours and 3000 km of real-world driving data. Extensive experiments show that our driving models are more accurate and behave more human-like than previous methods.



### Learning Local Complex Features using Randomized Neural Networks for Texture Analysis
- **Arxiv ID**: http://arxiv.org/abs/2007.05643v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2007.05643v2)
- **Published**: 2020-07-10 23:18:01+00:00
- **Updated**: 2020-08-17 18:51:30+00:00
- **Authors**: Lucas C. Ribas, Leonardo F. S. Scabini, Jarbas Joaci de Mesquita Sá Junior, Odemir M. Bruno
- **Comment**: None
- **Journal**: None
- **Summary**: Texture is a visual attribute largely used in many problems of image analysis. Currently, many methods that use learning techniques have been proposed for texture discrimination, achieving improved performance over previous handcrafted methods. In this paper, we present a new approach that combines a learning technique and the Complex Network (CN) theory for texture analysis. This method takes advantage of the representation capacity of CN to model a texture image as a directed network and uses the topological information of vertices to train a randomized neural network. This neural network has a single hidden layer and uses a fast learning algorithm, which is able to learn local CN patterns for texture characterization. Thus, we use the weighs of the trained neural network to compose a feature vector. These feature vectors are evaluated in a classification experiment in four widely used image databases. Experimental results show a high classification performance of the proposed method when compared to other methods, indicating that our approach can be used in many image analysis problems.



