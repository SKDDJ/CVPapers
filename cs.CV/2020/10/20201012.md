# Arxiv Papers in cs.CV on 2020-10-12
### MAF: Multimodal Alignment Framework for Weakly-Supervised Phrase Grounding
- **Arxiv ID**: http://arxiv.org/abs/2010.05379v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.05379v1)
- **Published**: 2020-10-12 00:43:52+00:00
- **Updated**: 2020-10-12 00:43:52+00:00
- **Authors**: Qinxin Wang, Hao Tan, Sheng Shen, Michael W. Mahoney, Zhewei Yao
- **Comment**: None
- **Journal**: None
- **Summary**: Phrase localization is a task that studies the mapping from textual phrases to regions of an image. Given difficulties in annotating phrase-to-object datasets at scale, we develop a Multimodal Alignment Framework (MAF) to leverage more widely-available caption-image datasets, which can then be used as a form of weak supervision. We first present algorithms to model phrase-object relevance by leveraging fine-grained visual representations and visually-aware language representations. By adopting a contrastive objective, our method uses information in caption-image pairs to boost the performance in weakly-supervised scenarios. Experiments conducted on the widely-adopted Flickr30k dataset show a significant improvement over existing weakly-supervised methods. With the help of the visually-aware language representations, we can also improve the previous best unsupervised result by 5.56%. We conduct ablation studies to show that both our novel model and our weakly-supervised strategies significantly contribute to our strong results.



### Miniscope3D: optimized single-shot miniature 3D fluorescence microscopy
- **Arxiv ID**: http://arxiv.org/abs/2010.05382v1
- **DOI**: 10.1038/s41377-020-00403-7
- **Categories**: **eess.IV**, cs.CV, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/2010.05382v1)
- **Published**: 2020-10-12 01:19:31+00:00
- **Updated**: 2020-10-12 01:19:31+00:00
- **Authors**: Kyrollos Yanny, Nick Antipa, William Liberti, Sam Dehaeck, Kristina Monakhova, Fanglin Linda Liu, Konlin Shen, Ren Ng, Laura Waller
- **Comment**: Published with Nature Springer in Light: Science and Applications
- **Journal**: Light: Science & Applications 9.1 (2020): 1-13
- **Summary**: Miniature fluorescence microscopes are a standard tool in systems biology. However, widefield miniature microscopes capture only 2D information, and modifications that enable 3D capabilities increase the size and weight and have poor resolution outside a narrow depth range. Here, we achieve the 3D capability by replacing the tube lens of a conventional 2D Miniscope with an optimized multifocal phase mask at the objective's aperture stop. Placing the phase mask at the aperture stop significantly reduces the size of the device, and varying the focal lengths enables a uniform resolution across a wide depth range. The phase mask encodes the 3D fluorescence intensity into a single 2D measurement, and the 3D volume is recovered by solving a sparsity-constrained inverse problem. We provide methods for designing and fabricating the phase mask and an efficient forward model that accounts for the field-varying aberrations in miniature objectives. We demonstrate a prototype that is 17 mm tall and weighs 2.5 grams, achieving 2.76 $\mu$m lateral, and 15 $\mu$m axial resolution across most of the 900x700x390 $\mu m^3$ volume at 40 volumes per second. The performance is validated experimentally on resolution targets, dynamic biological samples, and mouse brain tissue. Compared with existing miniature single-shot volume-capture implementations, our system is smaller and lighter and achieves a more than 2x better lateral and axial resolution throughout a 10x larger usable depth range. Our microscope design provides single-shot 3D imaging for applications where a compact platform matters, such as volumetric neural imaging in freely moving animals and 3D motion studies of dynamic samples in incubators and lab-on-a-chip devices.



### A Progressive Conditional Generative Adversarial Network for Generating Dense and Colored 3D Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2010.05391v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.05391v1)
- **Published**: 2020-10-12 01:32:13+00:00
- **Updated**: 2020-10-12 01:32:13+00:00
- **Authors**: Mohammad Samiul Arshad, William J. Beksi
- **Comment**: To be published in the 2020 International Conference on 3D Vision
  (3DV)
- **Journal**: None
- **Summary**: In this paper, we introduce a novel conditional generative adversarial network that creates dense 3D point clouds, with color, for assorted classes of objects in an unsupervised manner. To overcome the difficulty of capturing intricate details at high resolutions, we propose a point transformer that progressively grows the network through the use of graph convolutions. The network is composed of a leaf output layer and an initial set of branches. Every training iteration evolves a point vector into a point cloud of increasing resolution. After a fixed number of iterations, the number of branches is increased by replicating the last branch. Experimental results show that our network is capable of learning and mimicking a 3D data distribution, and produces colored point clouds with fine details at multiple resolutions.



### Reconstruction of Quantitative Susceptibility Maps from Phase of Susceptibility Weighted Imaging with Cross-Connected $Î¨$-Net
- **Arxiv ID**: http://arxiv.org/abs/2010.05395v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2010.05395v3)
- **Published**: 2020-10-12 01:53:21+00:00
- **Updated**: 2021-01-12 13:37:59+00:00
- **Authors**: Zhiyang Lu, Jun Li, Zheng Li, Hongjian He, Jun Shi
- **Comment**: None
- **Journal**: None
- **Summary**: Quantitative Susceptibility Mapping (QSM) is a new phase-based technique for quantifying magnetic susceptibility. The existing QSM reconstruction methods generally require complicated pre-processing on high-quality phase data. In this work, we propose to explore a new value of the high-pass filtered phase data generated in susceptibility weighted imaging (SWI), and develop an end-to-end Cross-connected $\Psi$-Net (C$\Psi$-Net) to reconstruct QSM directly from these phase data in SWI without additional pre-processing. C$\Psi$-Net adds an intermediate branch in the classical U-Net to form a $\Psi$-like structure. The specially designed dilated interaction block is embedded in each level of this branch to enlarge the receptive fields for capturing more susceptibility information from a wider spatial range of phase images. Moreover, the crossed connections are utilized between branches to implement a multi-resolution feature fusion scheme, which helps C$\Psi$-Net capture rich contextual information for accurate reconstruction. The experimental results on a human dataset show that C$\Psi$-Net achieves superior performance in our task over other QSM reconstruction algorithms.



### RNN Training along Locally Optimal Trajectories via Frank-Wolfe Algorithm
- **Arxiv ID**: http://arxiv.org/abs/2010.05397v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.05397v3)
- **Published**: 2020-10-12 01:59:18+00:00
- **Updated**: 2020-10-15 16:02:28+00:00
- **Authors**: Yun Yue, Ming Li, Venkatesh Saligrama, Ziming Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel and efficient training method for RNNs by iteratively seeking a local minima on the loss surface within a small region, and leverage this directional vector for the update, in an outer-loop. We propose to utilize the Frank-Wolfe (FW) algorithm in this context. Although, FW implicitly involves normalized gradients, which can lead to a slow convergence rate, we develop a novel RNN training method that, surprisingly, even with the additional cost, the overall training cost is empirically observed to be lower than back-propagation. Our method leads to a new Frank-Wolfe method, that is in essence an SGD algorithm with a restart scheme. We prove that under certain conditions our algorithm has a sublinear convergence rate of $O(1/\epsilon)$ for $\epsilon$ error. We then conduct empirical experiments on several benchmark datasets including those that exhibit long-term dependencies, and show significant performance improvement. We also experiment with deep RNN architectures and show efficient training performance. Finally, we demonstrate that our training method is robust to noisy data.



### VMSMO: Learning to Generate Multimodal Summary for Video-based News Articles
- **Arxiv ID**: http://arxiv.org/abs/2010.05406v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.05406v1)
- **Published**: 2020-10-12 02:19:16+00:00
- **Updated**: 2020-10-12 02:19:16+00:00
- **Authors**: Mingzhe Li, Xiuying Chen, Shen Gao, Zhangming Chan, Dongyan Zhao, Rui Yan
- **Comment**: Accepted by The 2020 Conference on Empirical Methods in Natural
  Language Processing (EMNLP 2020)
- **Journal**: None
- **Summary**: A popular multimedia news format nowadays is providing users with a lively video and a corresponding news article, which is employed by influential news media including CNN, BBC, and social media including Twitter and Weibo. In such a case, automatically choosing a proper cover frame of the video and generating an appropriate textual summary of the article can help editors save time, and readers make the decision more effectively. Hence, in this paper, we propose the task of Video-based Multimodal Summarization with Multimodal Output (VMSMO) to tackle such a problem. The main challenge in this task is to jointly model the temporal dependency of video with semantic meaning of article. To this end, we propose a Dual-Interaction-based Multimodal Summarizer (DIMS), consisting of a dual interaction module and multimodal generator. In the dual interaction module, we propose a conditional self-attention mechanism that captures local semantic information within video and a global-attention mechanism that handles the semantic relationship between news text and video from a high level. Extensive experiments conducted on a large-scale real-world VMSMO dataset show that DIMS achieves the state-of-the-art performance in terms of both automatic metrics and human evaluations.



### LASSR: Effective Super-Resolution Method for Plant Disease Diagnosis
- **Arxiv ID**: http://arxiv.org/abs/2010.06499v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2010.06499v1)
- **Published**: 2020-10-12 02:33:49+00:00
- **Updated**: 2020-10-12 02:33:49+00:00
- **Authors**: Quan Huu Cap, Hiroki Tani, Hiroyuki Uga, Satoshi Kagiwada, Hitoshi Iyatomi
- **Comment**: None
- **Journal**: None
- **Summary**: The collection of high-resolution training data is crucial in building robust plant disease diagnosis systems, since such data have a significant impact on diagnostic performance. However, they are very difficult to obtain and are not always available in practice. Deep learning-based techniques, and particularly generative adversarial networks (GANs), can be applied to generate high-quality super-resolution images, but these methods often produce unexpected artifacts that can lower the diagnostic performance. In this paper, we propose a novel artifact-suppression super-resolution method that is specifically designed for diagnosing leaf disease, called Leaf Artifact-Suppression Super Resolution (LASSR). Thanks to its own artifact removal module that detects and suppresses artifacts to a considerable extent, LASSR can generate much more pleasing, high-quality images compared to the state-of-the-art ESRGAN model. Experiments based on a five-class cucumber disease (including healthy) discrimination model show that training with data generated by LASSR significantly boosts the performance on an unseen test dataset by nearly 22% compared with the baseline, and that our approach is more than 2% better than a model trained with images generated by ESRGAN.



### Top-DB-Net: Top DropBlock for Activation Enhancement in Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2010.05435v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.05435v1)
- **Published**: 2020-10-12 03:49:58+00:00
- **Updated**: 2020-10-12 03:49:58+00:00
- **Authors**: Rodolfo Quispe, Helio Pedrini
- **Comment**: Accepted on 25th International Conference on Pattern Recognition
  (ICPR2020)
- **Journal**: ICPR 2020
- **Summary**: Person Re-Identification is a challenging task that aims to retrieve all instances of a query image across a system of non-overlapping cameras. Due to the various extreme changes of view, it is common that local regions that could be used to match people are suppressed, which leads to a scenario where approaches have to evaluate the similarity of images based on less informative regions. In this work, we introduce the Top-DB-Net, a method based on Top DropBlock that pushes the network to learn to focus on the scene foreground, with special emphasis on the most task-relevant regions and, at the same time, encodes low informative regions to provide high discriminability. The Top-DB-Net is composed of three streams: (i) a global stream encodes rich image information from a backbone, (ii) the Top DropBlock stream encourages the backbone to encode low informative regions with high discriminative features, and (iii) a regularization stream helps to deal with the noise created by the dropping process of the second stream, when testing the first two streams are used. Vast experiments on three challenging datasets show the capabilities of our approach against state-of-the-art methods. Qualitative results demonstrate that our method exhibits better activation maps focusing on reliable parts of the input images.



### Discriminative Sounding Objects Localization via Self-supervised Audiovisual Matching
- **Arxiv ID**: http://arxiv.org/abs/2010.05466v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2010.05466v1)
- **Published**: 2020-10-12 05:51:55+00:00
- **Updated**: 2020-10-12 05:51:55+00:00
- **Authors**: Di Hu, Rui Qian, Minyue Jiang, Xiao Tan, Shilei Wen, Errui Ding, Weiyao Lin, Dejing Dou
- **Comment**: To appear in NeurIPS 2020. Previous Title: Learning to
  Discriminatively Localize Sounding Objects in a Cocktail-party Scenario
- **Journal**: None
- **Summary**: Discriminatively localizing sounding objects in cocktail-party, i.e., mixed sound scenes, is commonplace for humans, but still challenging for machines. In this paper, we propose a two-stage learning framework to perform self-supervised class-aware sounding object localization. First, we propose to learn robust object representations by aggregating the candidate sound localization results in the single source scenes. Then, class-aware object localization maps are generated in the cocktail-party scenarios by referring the pre-learned object knowledge, and the sounding objects are accordingly selected by matching audio and visual object category distributions, where the audiovisual consistency is viewed as the self-supervised signal. Experimental results in both realistic and synthesized cocktail-party videos demonstrate that our model is superior in filtering out silent objects and pointing out the location of sounding objects of different classes. Code is available at https://github.com/DTaoo/Discriminative-Sounding-Objects-Localization.



### TSPNet: Hierarchical Feature Learning via Temporal Semantic Pyramid for Sign Language Translation
- **Arxiv ID**: http://arxiv.org/abs/2010.05468v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.HC, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2010.05468v1)
- **Published**: 2020-10-12 05:58:09+00:00
- **Updated**: 2020-10-12 05:58:09+00:00
- **Authors**: Dongxu Li, Chenchen Xu, Xin Yu, Kaihao Zhang, Ben Swift, Hanna Suominen, Hongdong Li
- **Comment**: NeurIPS 2020 preprint
- **Journal**: None
- **Summary**: Sign language translation (SLT) aims to interpret sign video sequences into text-based natural language sentences. Sign videos consist of continuous sequences of sign gestures with no clear boundaries in between. Existing SLT models usually represent sign visual features in a frame-wise manner so as to avoid needing to explicitly segmenting the videos into isolated signs. However, these methods neglect the temporal information of signs and lead to substantial ambiguity in translation. In this paper, we explore the temporal semantic structures of signvideos to learn more discriminative features. To this end, we first present a novel sign video segment representation which takes into account multiple temporal granularities, thus alleviating the need for accurate video segmentation. Taking advantage of the proposed segment representation, we develop a novel hierarchical sign video feature learning method via a temporal semantic pyramid network, called TSPNet. Specifically, TSPNet introduces an inter-scale attention to evaluate and enhance local semantic consistency of sign segments and an intra-scale attention to resolve semantic ambiguity by using non-local video context. Experiments show that our TSPNet outperforms the state-of-the-art with significant improvements on the BLEU score (from 9.58 to 13.41) and ROUGE score (from 31.80 to 34.96)on the largest commonly-used SLT dataset. Our implementation is available at https://github.com/verashira/TSPNet.



### CC-Loss: Channel Correlation Loss For Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2010.05469v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.05469v1)
- **Published**: 2020-10-12 05:59:06+00:00
- **Updated**: 2020-10-12 05:59:06+00:00
- **Authors**: Zeyu Song, Dongliang Chang, Zhanyu Ma, Xiaoxu Li, Zheng-Hua Tan
- **Comment**: accepted by ICPR2020
- **Journal**: None
- **Summary**: The loss function is a key component in deep learning models. A commonly used loss function for classification is the cross entropy loss, which is a simple yet effective application of information theory for classification problems. Based on this loss, many other loss functions have been proposed,~\emph{e.g.}, by adding intra-class and inter-class constraints to enhance the discriminative ability of the learned features. However, these loss functions fail to consider the connections between the feature distribution and the model structure. Aiming at addressing this problem, we propose a channel correlation loss (CC-Loss) that is able to constrain the specific relations between classes and channels as well as maintain the intra-class and the inter-class separability. CC-Loss uses a channel attention module to generate channel attention of features for each sample in the training stage. Next, an Euclidean distance matrix is calculated to make the channel attention vectors associated with the same class become identical and to increase the difference between different classes. Finally, we obtain a feature embedding with good intra-class compactness and inter-class separability.Experimental results show that two different backbone models trained with the proposed CC-Loss outperform the state-of-the-art loss functions on three image classification datasets.



### Increasing the Robustness of Semantic Segmentation Models with Painting-by-Numbers
- **Arxiv ID**: http://arxiv.org/abs/2010.05495v1
- **DOI**: 10.1007/978-3-030-58607-2_22
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.05495v1)
- **Published**: 2020-10-12 07:42:39+00:00
- **Updated**: 2020-10-12 07:42:39+00:00
- **Authors**: Christoph Kamann, Burkhard GÃ¼ssefeld, Robin Hutmacher, Jan Hendrik Metzen, Carsten Rother
- **Comment**: None
- **Journal**: None
- **Summary**: For safety-critical applications such as autonomous driving, CNNs have to be robust with respect to unavoidable image corruptions, such as image noise. While previous works addressed the task of robust prediction in the context of full-image classification, we consider it for dense semantic segmentation. We build upon an insight from image classification that output robustness can be improved by increasing the network-bias towards object shapes. We present a new training schema that increases this shape bias. Our basic idea is to alpha-blend a portion of the RGB training images with faked images, where each class-label is given a fixed, randomly chosen color that is not likely to appear in real imagery. This forces the network to rely more strongly on shape cues. We call this data augmentation technique ``Painting-by-Numbers''. We demonstrate the effectiveness of our training schema for DeepLabv3+ with various network backbones, MobileNet-V2, ResNets, and Xception, and evaluate it on the Cityscapes dataset. With respect to our 16 different types of image corruptions and 5 different network backbones, we are in 74% better than training with clean data. For cases where we are worse than a model trained without our training schema, it is mostly only marginally worse. However, for some image corruptions such as images with noise, we see a considerable performance gain of up to 25%.



### BiPointNet: Binary Neural Network for Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2010.05501v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.05501v4)
- **Published**: 2020-10-12 07:54:51+00:00
- **Updated**: 2021-06-11 15:03:56+00:00
- **Authors**: Haotong Qin, Zhongang Cai, Mingyuan Zhang, Yifu Ding, Haiyu Zhao, Shuai Yi, Xianglong Liu, Hao Su
- **Comment**: None
- **Journal**: None
- **Summary**: To alleviate the resource constraint for real-time point cloud applications that run on edge devices, in this paper we present BiPointNet, the first model binarization approach for efficient deep learning on point clouds. We discover that the immense performance drop of binarized models for point clouds mainly stems from two challenges: aggregation-induced feature homogenization that leads to a degradation of information entropy, and scale distortion that hinders optimization and invalidates scale-sensitive structures. With theoretical justifications and in-depth analysis, our BiPointNet introduces Entropy-Maximizing Aggregation (EMA) to modulate the distribution before aggregation for the maximum information entropy, and Layer-wise Scale Recovery (LSR) to efficiently restore feature representation capacity. Extensive experiments show that BiPointNet outperforms existing binarization methods by convincing margins, at the level even comparable with the full precision counterpart. We highlight that our techniques are generic, guaranteeing significant improvements on various fundamental tasks and mainstream backbones. Moreover, BiPointNet gives an impressive 14.7x speedup and 18.9x storage saving on real-world resource-constrained devices.



### Scene Gated Social Graph: Pedestrian Trajectory Prediction Based on Dynamic Social Graphs and Scene Constraints
- **Arxiv ID**: http://arxiv.org/abs/2010.05507v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.05507v1)
- **Published**: 2020-10-12 08:04:05+00:00
- **Updated**: 2020-10-12 08:04:05+00:00
- **Authors**: Hao Xue, Du Q. Huynh, Mark Reynolds
- **Comment**: None
- **Journal**: None
- **Summary**: Pedestrian trajectory prediction is valuable for understanding human motion behaviors and it is challenging because of the social influence from other pedestrians, the scene constraints and the multimodal possibilities of predicted trajectories. Most existing methods only focus on two of the above three key elements. In order to jointly consider all these elements, we propose a novel trajectory prediction method named Scene Gated Social Graph (SGSG). In the proposed SGSG, dynamic graphs are used to describe the social relationship among pedestrians. The social and scene influences are taken into account through the scene gated social graph features which combine the encoded social graph features and semantic scene features. In addition, a VAE module is incorporated to learn the scene gated social feature and sample latent variables for generating multiple trajectories that are socially and environmentally acceptable. We compare our SGSG against twenty state-of-the-art pedestrian trajectory prediction methods and the results show that the proposed method achieves superior performance on two widely used trajectory prediction benchmarks.



### Implicit Subspace Prior Learning for Dual-Blind Face Restoration
- **Arxiv ID**: http://arxiv.org/abs/2010.05508v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4.4; I.3.3
- **Links**: [PDF](http://arxiv.org/pdf/2010.05508v1)
- **Published**: 2020-10-12 08:04:24+00:00
- **Updated**: 2020-10-12 08:04:24+00:00
- **Authors**: Lingbo Yang, Pan Wang, Zhanning Gao, Shanshe Wang, Peiran Ren, Siwei Ma, Wen Gao
- **Comment**: TPAMI submission
- **Journal**: None
- **Summary**: Face restoration is an inherently ill-posed problem, where additional prior constraints are typically considered crucial for mitigating such pathology. However, real-world image prior are often hard to simulate with precise mathematical models, which inevitably limits the performance and generalization ability of existing prior-regularized restoration methods. In this paper, we study the problem of face restoration under a more practical ``dual blind'' setting, i.e., without prior assumptions or hand-crafted regularization terms on the degradation profile or image contents.   To this end, a novel implicit subspace prior learning (ISPL) framework is proposed as a generic solution to dual-blind face restoration, with two key elements: 1) an implicit formulation to circumvent the ill-defined restoration mapping and 2) a subspace prior decomposition and fusion mechanism to dynamically handle inputs at varying degradation levels with consistent high-quality restoration results.   Experimental results demonstrate significant perception-distortion improvement of ISPL against existing state-of-the-art methods for a variety of restoration subtasks, including a 3.69db PSNR and 45.8% FID gain against ESRGAN, the 2018 NTIRE SR challenge winner. Overall, we prove that it is possible to capture and utilize prior knowledge without explicitly formulating it, which will help inspire new research paradigms towards low-level vision tasks.



### Automatic Quantification of Settlement Damage using Deep Learning of Satellite Images
- **Arxiv ID**: http://arxiv.org/abs/2010.05512v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.05512v1)
- **Published**: 2020-10-12 08:06:33+00:00
- **Updated**: 2020-10-12 08:06:33+00:00
- **Authors**: Lili Lu, Weisi Guo
- **Comment**: None
- **Journal**: None
- **Summary**: Humanitarian disasters and political violence cause significant damage to our living space. The reparation cost to homes, infrastructure, and the ecosystem is often difficult to quantify in real-time. Real-time quantification is critical to both informing relief operations, but also planning ahead for rebuilding. Here, we use satellite images before and after major crisis around the world to train a robust baseline Residual Network (ResNet) and a disaster quantification Pyramid Scene Parsing Network (PSPNet). ResNet offers robustness to poor image quality and can identify areas of destruction with high accuracy (92\%), whereas PSPNet offers contextualised quantification of built environment damage with good accuracy (84\%). As there are multiple damage dimensions to consider (e.g. economic loss and fatalities), we fit a multi-linear regression model to quantify the overall damage. To validate our combined system of deep learning and regression modeling, we successfully match our prediction to the ongoing recovery in the 2020 Beirut port explosion. These innovations provide a better quantification of overall disaster magnitude and inform intelligent humanitarian systems of unfolding disasters.



### Unsupervised Semantic Aggregation and Deformable Template Matching for Semi-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2010.05517v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.05517v1)
- **Published**: 2020-10-12 08:17:56+00:00
- **Updated**: 2020-10-12 08:17:56+00:00
- **Authors**: Tao Han, Junyu Gao, Yuan Yuan, Qi Wang
- **Comment**: accepted by NeurIPS 2020
- **Journal**: None
- **Summary**: Unlabeled data learning has attracted considerable attention recently. However, it is still elusive to extract the expected high-level semantic feature with mere unsupervised learning. In the meantime, semi-supervised learning (SSL) demonstrates a promising future in leveraging few samples. In this paper, we combine both to propose an Unsupervised Semantic Aggregation and Deformable Template Matching (USADTM) framework for SSL, which strives to improve the classification performance with few labeled data and then reduce the cost in data annotating. Specifically, unsupervised semantic aggregation based on Triplet Mutual Information (T-MI) loss is explored to generate semantic labels for unlabeled data. Then the semantic labels are aligned to the actual class by the supervision of labeled data. Furthermore, a feature pool that stores the labeled samples is dynamically updated to assign proxy labels for unlabeled data, which are used as targets for cross-entropy minimization. Extensive experiments and analysis across four standard semi-supervised learning benchmarks validate that USADTM achieves top performance (e.g., 90.46$\%$ accuracy on CIFAR-10 with 40 labels and 95.20$\%$ accuracy with 250 labels). The code is released at https://github.com/taohan10200/USADTM.



### Learning Selective Mutual Attention and Contrast for RGB-D Saliency Detection
- **Arxiv ID**: http://arxiv.org/abs/2010.05537v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.05537v1)
- **Published**: 2020-10-12 08:50:10+00:00
- **Updated**: 2020-10-12 08:50:10+00:00
- **Authors**: Nian Liu, Ni Zhang, Ling Shao, Junwei Han
- **Comment**: Journal extension of our CVPR paper entitled "Learning Selective
  Self-Mutual Attention for RGB-D Saliency Detection" appeared in CVPR 2020
- **Journal**: None
- **Summary**: How to effectively fuse cross-modal information is the key problem for RGB-D salient object detection. Early fusion and the result fusion schemes fuse RGB and depth information at the input and output stages, respectively, hence incur the problem of distribution gap or information loss. Many models use the feature fusion strategy but are limited by the low-order point-to-point fusion methods. In this paper, we propose a novel mutual attention model by fusing attention and contexts from different modalities. We use the non-local attention of one modality to propagate long-range contextual dependencies for the other modality, thus leveraging complementary attention cues to perform high-order and trilinear cross-modal interaction. We also propose to induce contrast inference from the mutual attention and obtain a unified model. Considering low-quality depth data may detriment the model performance, we further propose selective attention to reweight the added depth cues. We embed the proposed modules in a two-stream CNN for RGB-D SOD. Experimental results have demonstrated the effectiveness of our proposed model. Moreover, we also construct a new challenging large-scale RGB-D SOD dataset with high-quality, thus can both promote the training and evaluation of deep models.



### High-Fidelity 3D Digital Human Head Creation from RGB-D Selfies
- **Arxiv ID**: http://arxiv.org/abs/2010.05562v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2010.05562v2)
- **Published**: 2020-10-12 09:31:53+00:00
- **Updated**: 2021-06-29 09:51:51+00:00
- **Authors**: Linchao Bao, Xiangkai Lin, Yajing Chen, Haoxian Zhang, Sheng Wang, Xuefei Zhe, Di Kang, Haozhi Huang, Xinwei Jiang, Jue Wang, Dong Yu, Zhengyou Zhang
- **Comment**: Code: https://github.com/tencent-ailab/hifi3dface
- **Journal**: None
- **Summary**: We present a fully automatic system that can produce high-fidelity, photo-realistic 3D digital human heads with a consumer RGB-D selfie camera. The system only needs the user to take a short selfie RGB-D video while rotating his/her head, and can produce a high quality head reconstruction in less than 30 seconds. Our main contribution is a new facial geometry modeling and reflectance synthesis procedure that significantly improves the state-of-the-art. Specifically, given the input video a two-stage frame selection procedure is first employed to select a few high-quality frames for reconstruction. Then a differentiable renderer based 3D Morphable Model (3DMM) fitting algorithm is applied to recover facial geometries from multiview RGB-D data, which takes advantages of a powerful 3DMM basis constructed with extensive data generation and perturbation. Our 3DMM has much larger expressive capacities than conventional 3DMM, allowing us to recover more accurate facial geometry using merely linear basis. For reflectance synthesis, we present a hybrid approach that combines parametric fitting and CNNs to synthesize high-resolution albedo/normal maps with realistic hair/pore/wrinkle details. Results show that our system can produce faithful 3D digital human faces with extremely realistic details. The main code and the newly constructed 3DMM basis is publicly available.



### Diptychs of human and machine perceptions
- **Arxiv ID**: http://arxiv.org/abs/2010.13864v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.13864v1)
- **Published**: 2020-10-12 10:22:28+00:00
- **Updated**: 2020-10-12 10:22:28+00:00
- **Authors**: Vivien Cabannes, Thomas Kerdreux, Louis Thiry
- **Comment**: 7 pages, 36 images
- **Journal**: creativity workshop NeurIPS 2020
- **Summary**: We propose visual creations that put differences in algorithms and humans \emph{perceptions} into perspective. We exploit saliency maps of neural networks and visual focus of humans to create diptychs that are reinterpretations of an original image according to both machine and human attentions. Using those diptychs as a qualitative evaluation of perception, we discuss some crucial issues of current \textit{task-oriented} artificial intelligence.



### MS$^2$L: Multi-Task Self-Supervised Learning for Skeleton Based Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2010.05599v2
- **DOI**: 10.1145/3394171.3413548
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2010.05599v2)
- **Published**: 2020-10-12 11:09:44+00:00
- **Updated**: 2020-10-14 07:07:05+00:00
- **Authors**: Lilang Lin, Sijie Song, Wenhan Yan, Jiaying Liu
- **Comment**: Accepted by ACMMM 2020
- **Journal**: None
- **Summary**: In this paper, we address self-supervised representation learning from human skeletons for action recognition. Previous methods, which usually learn feature presentations from a single reconstruction task, may come across the overfitting problem, and the features are not generalizable for action recognition. Instead, we propose to integrate multiple tasks to learn more general representations in a self-supervised manner. To realize this goal, we integrate motion prediction, jigsaw puzzle recognition, and contrastive learning to learn skeleton features from different aspects. Skeleton dynamics can be modeled through motion prediction by predicting the future sequence. And temporal patterns, which are critical for action recognition, are learned through solving jigsaw puzzles. We further regularize the feature space by contrastive learning. Besides, we explore different training strategies to utilize the knowledge from self-supervised tasks for action recognition. We evaluate our multi-task self-supervised learning approach with action classifiers trained under different configurations, including unsupervised, semi-supervised and fully-supervised settings. Our experiments on the NW-UCLA, NTU RGB+D, and PKUMMD datasets show remarkable performance for action recognition, demonstrating the superiority of our method in learning more discriminative and general features. Our project website is available at https://langlandslin.github.io/projects/MSL/.



### Omni-Directional Image Generation from Single Snapshot Image
- **Arxiv ID**: http://arxiv.org/abs/2010.05600v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.05600v1)
- **Published**: 2020-10-12 11:12:04+00:00
- **Updated**: 2020-10-12 11:12:04+00:00
- **Authors**: Keisuke Okubo, Takao Yamanaka
- **Comment**: SMC2020
- **Journal**: None
- **Summary**: An omni-directional image (ODI) is the image that has a field of view covering the entire sphere around the camera. The ODIs have begun to be used in a wide range of fields such as virtual reality (VR), robotics, and social network services. Although the contents using ODI have increased, the available images and videos are still limited, compared with widespread snapshot images. A large number of ODIs are desired not only for the VR contents, but also for training deep learning models for ODI. For these purposes, a novel computer vision task to generate ODI from a single snapshot image is proposed in this paper. To tackle this problem, the conditional generative adversarial network was applied in combination with class-conditioned convolution layers. With this novel task, VR images and videos will be easily created even with a smartphone camera.



### Convolutional Neural Network optimization via Channel Reassessment Attention module
- **Arxiv ID**: http://arxiv.org/abs/2010.05605v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.05605v1)
- **Published**: 2020-10-12 11:27:17+00:00
- **Updated**: 2020-10-12 11:27:17+00:00
- **Authors**: YuTao Shen, Ying Wen
- **Comment**: None
- **Journal**: None
- **Summary**: The performance of convolutional neural networks (CNNs) can be improved by adjusting the interrelationship between channels with attention mechanism. However, attention mechanism in recent advance has not fully utilized spatial information of feature maps, which makes a great difference to the results of generated channel attentions. In this paper, we propose a novel network optimization module called Channel Reassessment Attention (CRA) module which uses channel attentions with spatial information of feature maps to enhance representational power of networks. We employ CRA module to assess channel attentions based on feature maps in different channels, then the final features are refined adaptively by product between channel attentions and feature maps.CRA module is a computational lightweight module and it can be embedded into any architectures of CNNs. The experiments on ImageNet, CIFAR and MS COCO datasets demonstrate that the embedding of CRA module on various networks effectively improves the performance under different evaluation standards.



### A Unified Framework for Generic, Query-Focused, Privacy Preserving and Update Summarization using Submodular Information Measures
- **Arxiv ID**: http://arxiv.org/abs/2010.05631v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.05631v1)
- **Published**: 2020-10-12 12:03:03+00:00
- **Updated**: 2020-10-12 12:03:03+00:00
- **Authors**: Vishal Kaushal, Suraj Kothawade, Ganesh Ramakrishnan, Jeff Bilmes, Himanshu Asnani, Rishabh Iyer
- **Comment**: 35 pages, 14 figures, 5 tables
- **Journal**: None
- **Summary**: We study submodular information measures as a rich framework for generic, query-focused, privacy sensitive, and update summarization tasks. While past work generally treats these problems differently ({\em e.g.}, different models are often used for generic and query-focused summarization), the submodular information measures allow us to study each of these problems via a unified approach. We first show that several previous query-focused and update summarization techniques have, unknowingly, used various instantiations of the aforesaid submodular information measures, providing evidence for the benefit and naturalness of these models. We then carefully study and demonstrate the modelling capabilities of the proposed functions in different settings and empirically verify our findings on both a synthetic dataset and an existing real-world image collection dataset (that has been extended by adding concept annotations to each image making it suitable for this task) and will be publicly released. We employ a max-margin framework to learn a mixture model built using the proposed instantiations of submodular information measures and demonstrate the effectiveness of our approach. While our experiments are in the context of image summarization, our framework is generic and can be easily extended to other summarization settings (e.g., videos or documents).



### The MECCANO Dataset: Understanding Human-Object Interactions from Egocentric Videos in an Industrial-like Domain
- **Arxiv ID**: http://arxiv.org/abs/2010.05654v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.05654v1)
- **Published**: 2020-10-12 12:50:30+00:00
- **Updated**: 2020-10-12 12:50:30+00:00
- **Authors**: Francesco Ragusa, Antonino Furnari, Salvatore Livatino, Giovanni Maria Farinella
- **Comment**: None
- **Journal**: None
- **Summary**: Wearable cameras allow to collect images and videos of humans interacting with the world. While human-object interactions have been thoroughly investigated in third person vision, the problem has been understudied in egocentric settings and in industrial scenarios. To fill this gap, we introduce MECCANO, the first dataset of egocentric videos to study human-object interactions in industrial-like settings. MECCANO has been acquired by 20 participants who were asked to build a motorbike model, for which they had to interact with tiny objects and tools. The dataset has been explicitly labeled for the task of recognizing human-object interactions from an egocentric perspective. Specifically, each interaction has been labeled both temporally (with action segments) and spatially (with active object bounding boxes). With the proposed dataset, we investigate four different tasks including 1) action recognition, 2) active object detection, 3) active object recognition and 4) egocentric human-object interaction detection, which is a revisited version of the standard human-object interaction detection task. Baseline results show that the MECCANO dataset is a challenging benchmark to study egocentric human-object interactions in industrial-like scenarios. We publicy release the dataset at https://iplab.dmi.unict.it/MECCANO.



### Graph Regularized Nonnegative Tensor Ring Decomposition for Multiway Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2010.05657v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.05657v1)
- **Published**: 2020-10-12 12:54:20+00:00
- **Updated**: 2020-10-12 12:54:20+00:00
- **Authors**: Yuyuan Yu, Guoxu Zhou, Ning Zheng, Shengli Xie, Qibin Zhao
- **Comment**: 13 pages, 7 figures
- **Journal**: None
- **Summary**: Tensor ring (TR) decomposition is a powerful tool for exploiting the low-rank nature of multiway data and has demonstrated great potential in a variety of important applications. In this paper, nonnegative tensor ring (NTR) decomposition and graph regularized NTR (GNTR) decomposition are proposed, where the former equips TR decomposition with local feature extraction by imposing nonnegativity on the core tensors and the latter is additionally able to capture manifold geometry information of tensor data, both significantly extend the applications of TR decomposition for nonnegative multiway representation learning. Accelerated proximal gradient based methods are derived for NTR and GNTR. The experimental result demonstrate that the proposed algorithms can extract parts-based basis with rich colors and rich lines from tensor objects that provide more interpretable and meaningful representation, and hence yield better performance than the state-of-the-art tensor based methods in clustering and classification tasks.



### Semantic Change Detection with Asymmetric Siamese Networks
- **Arxiv ID**: http://arxiv.org/abs/2010.05687v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.05687v2)
- **Published**: 2020-10-12 13:26:30+00:00
- **Updated**: 2021-05-08 16:23:46+00:00
- **Authors**: Kunping Yang, Gui-Song Xia, Zicheng Liu, Bo Du, Wen Yang, Marcello Pelillo, Liangpei Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Given two multi-temporal aerial images, semantic change detection aims to locate the land-cover variations and identify their change types with pixel-wise boundaries. This problem is vital in many earth vision related tasks, such as precise urban planning and natural resource management. Existing state-of-the-art algorithms mainly identify the changed pixels by applying homogeneous operations on each input image and comparing the extracted features. However, in changed regions, totally different land-cover distributions often require heterogeneous features extraction procedures w.r.t each input. In this paper, we present an asymmetric siamese network (ASN) to locate and identify semantic changes through feature pairs obtained from modules of widely different structures, which involve areas of various sizes and apply different quantities of parameters to factor in the discrepancy across different land-cover distributions. To better train and evaluate our model, we create a large-scale well-annotated SEmantic Change detectiON Dataset (SECOND), while an Adaptive Threshold Learning (ATL) module and a Separated Kappa (SeK) coefficient are proposed to alleviate the influences of label imbalance in model training and evaluation. The experimental results demonstrate that the proposed model can stably outperform the state-of-the-art algorithms with different encoder backbones.



### Unsupervised Image-to-Image Translation via Pre-trained StyleGAN2 Network
- **Arxiv ID**: http://arxiv.org/abs/2010.05713v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.05713v2)
- **Published**: 2020-10-12 13:51:40+00:00
- **Updated**: 2020-10-27 01:18:01+00:00
- **Authors**: Jialu Huang, Jing Liao, Sam Kwong
- **Comment**: 2020 IEEE. Personal use of this material is permitted. Permission
  from IEEE must be obtained for all other uses, in any current or future
  media, including reprinting/republishing this material for advertising or
  promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works
- **Journal**: None
- **Summary**: Image-to-Image (I2I) translation is a heated topic in academia, and it also has been applied in real-world industry for tasks like image synthesis, super-resolution, and colorization. However, traditional I2I translation methods train data in two or more domains together. This requires lots of computation resources. Moreover, the results are of lower quality, and they contain many more artifacts. The training process could be unstable when the data in different domains are not balanced, and modal collapse is more likely to happen. We proposed a new I2I translation method that generates a new model in the target domain via a series of model transformations on a pre-trained StyleGAN2 model in the source domain. After that, we proposed an inversion method to achieve the conversion between an image and its latent vector. By feeding the latent vector into the generated model, we can perform I2I translation between the source domain and target domain. Both qualitative and quantitative evaluations were conducted to prove that the proposed method can achieve outstanding performance in terms of image quality, diversity and semantic similarity to the input and reference images compared to state-of-the-art works.



### Hierarchical Attention Learning of Scene Flow in 3D Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2010.05762v1
- **DOI**: 10.1109/TIP.2021.3079796
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.05762v1)
- **Published**: 2020-10-12 14:56:08+00:00
- **Updated**: 2020-10-12 14:56:08+00:00
- **Authors**: Guangming Wang, Xinrui Wu, Zhe Liu, Hesheng Wang
- **Comment**: 13 pages, 7 figures, under review
- **Journal**: None
- **Summary**: Scene flow represents the 3D motion of every point in the dynamic environments. Like the optical flow that represents the motion of pixels in 2D images, 3D motion representation of scene flow benefits many applications, such as autonomous driving and service robot. This paper studies the problem of scene flow estimation from two consecutive 3D point clouds. In this paper, a novel hierarchical neural network with double attention is proposed for learning the correlation of point features in adjacent frames and refining scene flow from coarse to fine layer by layer. The proposed network has a new more-for-less hierarchical architecture. The more-for-less means that the number of input points is greater than the number of output points for scene flow estimation, which brings more input information and balances the precision and resource consumption. In this hierarchical architecture, scene flow of different levels is generated and supervised respectively. A novel attentive embedding module is introduced to aggregate the features of adjacent points using a double attention method in a patch-to-patch manner. The proper layers for flow embedding and flow supervision are carefully considered in our network designment. Experiments show that the proposed network outperforms the state-of-the-art performance of 3D scene flow estimation on the FlyingThings3D and KITTI Scene Flow 2015 datasets. We also apply the proposed network to realistic LiDAR odometry task, which is an key problem in autonomous driving. The experiment results demonstrate that our proposed network can outperform the ICP-based method and shows the good practical application ability.



### Pedestrian Trajectory Prediction with Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2010.05796v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, I.2.6; I.5.1
- **Links**: [PDF](http://arxiv.org/pdf/2010.05796v2)
- **Published**: 2020-10-12 15:51:01+00:00
- **Updated**: 2021-09-16 13:19:14+00:00
- **Authors**: Simone Zamboni, Zekarias Tilahun Kefato, Sarunas Girdzijauskas, Noren Christoffer, Laura Dal Col
- **Comment**: 35 pages, 8 figures. This article is a the accepted manuscript
  submitted to Pattern Recognition Journal. Article DOI:
  https://doi.org/10.1016/j.patcog.2021.108252. This manuscript version is made
  available under the CC-BY-NC-ND 4.0 license
  (https://creativecommons.org/licenses/by-nc-nd/4.0/)
- **Journal**: Pattern Recognition, Volume 121, 2022, 108252, ISSN 0031-3203
- **Summary**: Predicting the future trajectories of pedestrians is a challenging problem that has a range of application, from crowd surveillance to autonomous driving. In literature, methods to approach pedestrian trajectory prediction have evolved, transitioning from physics-based models to data-driven models based on recurrent neural networks. In this work, we propose a new approach to pedestrian trajectory prediction, with the introduction of a novel 2D convolutional model. This new model outperforms recurrent models, and it achieves state-of-the-art results on the ETH and TrajNet datasets. We also present an effective system to represent pedestrian positions and powerful data augmentation techniques, such as the addition of Gaussian noise and the use of random rotations, which can be applied to any model. As an additional exploratory analysis, we present experimental results on the inclusion of occupancy methods to model social information, which empirically show that these methods are ineffective in capturing social interaction.



### Viewpoint-Aware Channel-Wise Attentive Network for Vehicle Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2010.05810v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.05810v1)
- **Published**: 2020-10-12 16:05:41+00:00
- **Updated**: 2020-10-12 16:05:41+00:00
- **Authors**: Tsai-Shien Chen, Man-Yu Lee, Chih-Ting Liu, Shao-Yi Chien
- **Comment**: CVPR Workshop 2020
- **Journal**: None
- **Summary**: Vehicle re-identification (re-ID) matches images of the same vehicle across different cameras. It is fundamentally challenging because the dramatically different appearance caused by different viewpoints would make the framework fail to match two vehicles of the same identity. Most existing works solved the problem by extracting viewpoint-aware feature via spatial attention mechanism, which, yet, usually suffers from noisy generated attention map or otherwise requires expensive keypoint labels to improve the quality. In this work, we propose Viewpoint-aware Channel-wise Attention Mechanism (VCAM) by observing the attention mechanism from a different aspect. Our VCAM enables the feature learning framework channel-wisely reweighing the importance of each feature maps according to the "viewpoint" of input vehicle. Extensive experiments validate the effectiveness of the proposed method and show that we perform favorably against state-of-the-arts methods on the public VeRi-776 dataset and obtain promising results on the 2020 AI City Challenge. We also conduct other experiments to demonstrate the interpretability of how our VCAM practically assists the learning framework.



### Open-sourced Dataset Protection via Backdoor Watermarking
- **Arxiv ID**: http://arxiv.org/abs/2010.05821v3
- **DOI**: None
- **Categories**: **cs.CR**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.05821v3)
- **Published**: 2020-10-12 16:16:27+00:00
- **Updated**: 2020-11-19 04:51:13+00:00
- **Authors**: Yiming Li, Ziqi Zhang, Jiawang Bai, Baoyuan Wu, Yong Jiang, Shu-Tao Xia
- **Comment**: Accepted by the NeurIPS Workshop on Dataset Curation and Security,
  2020. 6 pages
- **Journal**: None
- **Summary**: The rapid development of deep learning has benefited from the release of some high-quality open-sourced datasets ($e.g.$, ImageNet), which allows researchers to easily verify the effectiveness of their algorithms. Almost all existing open-sourced datasets require that they can only be adopted for academic or educational purposes rather than commercial purposes, whereas there is still no good way to protect them. In this paper, we propose a \emph{backdoor embedding based dataset watermarking} method to protect an open-sourced image-classification dataset by verifying whether it is used for training a third-party model. Specifically, the proposed method contains two main processes, including \emph{dataset watermarking} and \emph{dataset verification}. We adopt classical poisoning-based backdoor attacks ($e.g.$, BadNets) for dataset watermarking, ie, generating some poisoned samples by adding a certain trigger ($e.g.$, a local patch) onto some benign samples, labeled with a pre-defined target class. Based on the proposed backdoor-based watermarking, we use a hypothesis test guided method for dataset verification based on the posterior probability generated by the suspicious third-party model of the benign samples and their correspondingly watermarked samples ($i.e.$, images with trigger) on the target class. Experiments on some benchmark datasets are conducted, which verify the effectiveness of the proposed method.



### Neural Enhancement in Content Delivery Systems: The State-of-the-Art and Future Directions
- **Arxiv ID**: http://arxiv.org/abs/2010.05838v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.DC, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2010.05838v2)
- **Published**: 2020-10-12 16:41:29+00:00
- **Updated**: 2020-10-22 12:42:00+00:00
- **Authors**: Royson Lee, Stylianos I. Venieris, Nicholas D. Lane
- **Comment**: Accepted at the 1st Workshop on Distributed Machine Learning at
  CoNEXT 2020 (DistributedML 2020)
- **Journal**: None
- **Summary**: Internet-enabled smartphones and ultra-wide displays are transforming a variety of visual apps spanning from on-demand movies and 360-degree videos to video-conferencing and live streaming. However, robustly delivering visual content under fluctuating networking conditions on devices of diverse capabilities remains an open problem. In recent years, advances in the field of deep learning on tasks such as super-resolution and image enhancement have led to unprecedented performance in generating high-quality images from low-quality ones, a process we refer to as neural enhancement. In this paper, we survey state-of-the-art content delivery systems that employ neural enhancement as a key component in achieving both fast response time and high visual quality. We first present the deployment challenges of neural enhancement models. We then cover systems targeting diverse use-cases and analyze their design decisions in overcoming technical challenges. Moreover, we present promising directions based on the latest insights from deep learning research to further boost the quality of experience of these systems.



### Fully Automatic Wound Segmentation with Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2010.05855v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.05855v1)
- **Published**: 2020-10-12 17:02:48+00:00
- **Updated**: 2020-10-12 17:02:48+00:00
- **Authors**: Chuanbo Wang, DM Anisuzzaman, Victor Williamson, Mrinal Kanti Dhar, Behrouz Rostami, Jeffrey Niezgoda, Sandeep Gopalakrishnan, Zeyun Yu
- **Comment**: None
- **Journal**: None
- **Summary**: Acute and chronic wounds have varying etiologies and are an economic burden to healthcare systems around the world. The advanced wound care market is expected to exceed $22 billion by 2024. Wound care professionals rely heavily on images and image documentation for proper diagnosis and treatment. Unfortunately lack of expertise can lead to improper diagnosis of wound etiology and inaccurate wound management and documentation. Fully automatic segmentation of wound areas in natural images is an important part of the diagnosis and care protocol since it is crucial to measure the area of the wound and provide quantitative parameters in the treatment. Various deep learning models have gained success in image analysis including semantic segmentation. Particularly, MobileNetV2 stands out among others due to its lightweight architecture and uncompromised performance. This manuscript proposes a novel convolutional framework based on MobileNetV2 and connected component labelling to segment wound regions from natural images. We build an annotated wound image dataset consisting of 1,109 foot ulcer images from 889 patients to train and test the deep learning models. We demonstrate the effectiveness and mobility of our method by conducting comprehensive experiments and analyses on various segmentation neural networks.



### On the Minimal Recognizable Image Patch
- **Arxiv ID**: http://arxiv.org/abs/2010.05858v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.05858v1)
- **Published**: 2020-10-12 17:06:16+00:00
- **Updated**: 2020-10-12 17:06:16+00:00
- **Authors**: Mark Fonaryov, Michael Lindenbaum
- **Comment**: Submitted to ICPR2020
- **Journal**: None
- **Summary**: In contrast to human vision, common recognition algorithms often fail on partially occluded images. We propose characterizing, empirically, the algorithmic limits by finding a minimal recognizable patch (MRP) that is by itself sufficient to recognize the image. A specialized deep network allows us to find the most informative patches of a given size, and serves as an experimental tool. A human vision study recently characterized related (but different) minimally recognizable configurations (MIRCs) [1], for which we specify computational analogues (denoted cMIRCs). The drop in human decision accuracy associated with size reduction of these MIRCs is substantial and sharp. Interestingly, such sharp reductions were also found for the computational versions we specified.



### Robust Optimal Transport with Applications in Generative Modeling and Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2010.05862v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.05862v1)
- **Published**: 2020-10-12 17:13:40+00:00
- **Updated**: 2020-10-12 17:13:40+00:00
- **Authors**: Yogesh Balaji, Rama Chellappa, Soheil Feizi
- **Comment**: Accepted in NeurIPS 2020. Code available at
  https://github.com/yogeshbalaji/robustOT
- **Journal**: None
- **Summary**: Optimal Transport (OT) distances such as Wasserstein have been used in several areas such as GANs and domain adaptation. OT, however, is very sensitive to outliers (samples with large noise) in the data since in its objective function, every sample, including outliers, is weighed similarly due to the marginal constraints. To remedy this issue, robust formulations of OT with unbalanced marginal constraints have previously been proposed. However, employing these methods in deep learning problems such as GANs and domain adaptation is challenging due to the instability of their dual optimization solvers. In this paper, we resolve these issues by deriving a computationally-efficient dual form of the robust OT optimization that is amenable to modern deep learning applications. We demonstrate the effectiveness of our formulation in two applications of GANs and domain adaptation. Our approach can train state-of-the-art GAN models on noisy datasets corrupted with outlier distributions. In particular, our optimization computes weights for training samples reflecting how difficult it is for those samples to be generated in the model. In domain adaptation, our robust OT formulation leads to improved accuracy compared to the standard adversarial adaptation methods. Our code is available at https://github.com/yogeshbalaji/robustOT.



### Webly Supervised Image Classification with Metadata: Automatic Noisy Label Correction via Visual-Semantic Graph
- **Arxiv ID**: http://arxiv.org/abs/2010.05864v1
- **DOI**: 10.1145/3394171.3413952
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.05864v1)
- **Published**: 2020-10-12 17:15:51+00:00
- **Updated**: 2020-10-12 17:15:51+00:00
- **Authors**: Jingkang Yang, Weirong Chen, Litong Feng, Xiaopeng Yan, Huabin Zheng, Wayne Zhang
- **Comment**: Accepted to ACM Multimedia 2020 (Oral)
- **Journal**: None
- **Summary**: Webly supervised learning becomes attractive recently for its efficiency in data expansion without expensive human labeling. However, adopting search queries or hashtags as web labels of images for training brings massive noise that degrades the performance of DNNs. Especially, due to the semantic confusion of query words, the images retrieved by one query may contain tremendous images belonging to other concepts. For example, searching `tiger cat' on Flickr will return a dominating number of tiger images rather than the cat images. These realistic noisy samples usually have clear visual semantic clusters in the visual space that mislead DNNs from learning accurate semantic labels. To correct real-world noisy labels, expensive human annotations seem indispensable. Fortunately, we find that metadata can provide extra knowledge to discover clean web labels in a labor-free fashion, making it feasible to automatically provide correct semantic guidance among the massive label-noisy web data. In this paper, we propose an automatic label corrector VSGraph-LC based on the visual-semantic graph. VSGraph-LC starts from anchor selection referring to the semantic similarity between metadata and correct label concepts, and then propagates correct labels from anchors on a visual graph using graph neural network (GNN). Experiments on realistic webly supervised learning datasets Webvision-1000 and NUS-81-Web show the effectiveness and robustness of VSGraph-LC. Moreover, VSGraph-LC reveals its advantage on the open-set validation set.



### PANDA: Adapting Pretrained Features for Anomaly Detection and Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2010.05903v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.05903v3)
- **Published**: 2020-10-12 17:52:50+00:00
- **Updated**: 2021-08-12 16:53:29+00:00
- **Authors**: Tal Reiss, Niv Cohen, Liron Bergman, Yedid Hoshen
- **Comment**: CVPR 2021
- **Journal**: None
- **Summary**: Anomaly detection methods require high-quality features. In recent years, the anomaly detection community has attempted to obtain better features using advances in deep self-supervised feature learning. Surprisingly, a very promising direction, using pretrained deep features, has been mostly overlooked. In this paper, we first empirically establish the perhaps expected, but unreported result, that combining pretrained features with simple anomaly detection and segmentation methods convincingly outperforms, much more complex, state-of-the-art methods.   In order to obtain further performance gains in anomaly detection, we adapt pretrained features to the target distribution. Although transfer learning methods are well established in multi-class classification problems, the one-class classification (OCC) setting is not as well explored. It turns out that naive adaptation methods, which typically work well in supervised learning, often result in catastrophic collapse (feature deterioration) and reduce performance in OCC settings. A popular OCC method, DeepSVDD, advocates using specialized architectures, but this limits the adaptation performance gain. We propose two methods for combating collapse: i) a variant of early stopping that dynamically learns the stopping iteration ii) elastic regularization inspired by continual learning. Our method, PANDA, outperforms the state-of-the-art in the OCC, outlier exposure and anomaly segmentation settings by large margins.



### Cut-and-Paste Object Insertion by Enabling Deep Image Prior for Reshading
- **Arxiv ID**: http://arxiv.org/abs/2010.05907v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.05907v2)
- **Published**: 2020-10-12 17:59:55+00:00
- **Updated**: 2022-09-13 17:58:43+00:00
- **Authors**: Anand Bhattad, David A. Forsyth
- **Comment**: 3DV 2022
- **Journal**: None
- **Summary**: We show how to insert an object from one image to another and get realistic results in the hard case, where the shading of the inserted object clashes with the shading of the scene. Rendering objects using an illumination model of the scene doesn't work, because doing so requires a geometric and material model of the object, which is hard to recover from a single image. In this paper, we introduce a method that corrects shading inconsistencies of the inserted object without requiring a geometric and physical model or an environment map. Our method uses a deep image prior (DIP), trained to produce reshaded renderings of inserted objects via consistent image decomposition inferential losses. The resulting image from DIP aims to have (a) an albedo similar to the cut-and-paste albedo, (b) a similar shading field to that of the target scene, and (c) a shading that is consistent with the cut-and-paste surface normals. The result is a simple procedure that produces convincing shading of the inserted object. We show the efficacy of our method both qualitatively and quantitatively for several objects with complex surface properties and also on a dataset of spherical lampshades for quantitative evaluation. Our method significantly outperforms an Image Harmonization (IH) baseline for all these objects. They also outperform the cut-and-paste and IH baselines in a user study with over 100 users.



### Towards human-level performance on automatic pose estimation of infant spontaneous movements
- **Arxiv ID**: http://arxiv.org/abs/2010.05949v5
- **DOI**: 10.1016/j.compmedimag.2021.102012
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.05949v5)
- **Published**: 2020-10-12 18:17:47+00:00
- **Updated**: 2021-12-16 16:09:10+00:00
- **Authors**: Daniel Groos, Lars Adde, Ragnhild StÃ¸en, Heri Ramampiaro, Espen A. F. Ihlen
- **Comment**: Published in Computerized Medical Imaging and Graphics (CMIG)
- **Journal**: Computerized Medical Imaging and Graphics 95 (2022) 102012
- **Summary**: Assessment of spontaneous movements can predict the long-term developmental disorders in high-risk infants. In order to develop algorithms for automated prediction of later disorders, highly precise localization of segments and joints by infant pose estimation is required. Four types of convolutional neural networks were trained and evaluated on a novel infant pose dataset, covering the large variation in 1 424 videos from a clinical international community. The localization performance of the networks was evaluated as the deviation between the estimated keypoint positions and human expert annotations. The computational efficiency was also assessed to determine the feasibility of the neural networks in clinical practice. The best performing neural network had a similar localization error to the inter-rater spread of human expert annotations, while still operating efficiently. Overall, the results of our study show that pose estimation of infant spontaneous movements has a great potential to support research initiatives on early detection of developmental disorders in children with perinatal brain injuries by quantifying infant movements from video recordings with human-level performance.



### Robots State Estimation and Observability Analysis Based on Statistical Motion Models
- **Arxiv ID**: http://arxiv.org/abs/2010.05957v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.05957v1)
- **Published**: 2020-10-12 18:35:33+00:00
- **Updated**: 2020-10-12 18:35:33+00:00
- **Authors**: Wei Xu, Dongjiao He, Yixi Cai, Fu Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a generic motion model to capture mobile robots' dynamic behaviors (translation and rotation). The model is based on statistical models driven by white random processes and is formulated into a full state estimation algorithm based on the error-state extended Kalman filtering framework (ESEKF). Major benefits of this method are its versatility, being applicable to different robotic systems without accurately modeling the robots' specific dynamics, and ability to estimate the robot's (angular) acceleration, jerk, or higher-order dynamic states with low delay. Mathematical analysis with numerical simulations are presented to show the properties of the statistical model-based estimation framework and to reveal its connection to existing low-pass filters. Furthermore, a new paradigm is developed for robots observability analysis by developing Lie derivatives and associated partial differentiation directly on manifolds. It is shown that this new paradigm is much simpler and more natural than existing methods based on quaternion parameterizations. It is also scalable to high dimensional systems. A novel \textbf{\textit{thin}} set concept is introduced to characterize the unobservable subset of the system states, providing the theoretical foundation to observability analysis of robotic systems operating on manifolds and in high dimension. Finally, extensive experiments including full state estimation and extrinsic calibration (both POS-IMU and IMU-IMU) on a quadrotor UAV, a handheld platform and a ground vehicle are conducted. Comparisons with existing methods show that the proposed method can effectively estimate all extrinsic parameters, the robot's translation/angular acceleration and other state variables (e.g., position, velocity, attitude) of high accuracy and low delay.



### Monitoring War Destruction from Space: A Machine Learning Approach
- **Arxiv ID**: http://arxiv.org/abs/2010.05970v2
- **DOI**: 10.1073/pnas.2025400118
- **Categories**: **econ.GN**, cs.AI, cs.CV, q-fin.EC
- **Links**: [PDF](http://arxiv.org/pdf/2010.05970v2)
- **Published**: 2020-10-12 19:01:20+00:00
- **Updated**: 2020-10-14 03:47:28+00:00
- **Authors**: Hannes Mueller, Andre Groger, Jonathan Hersh, Andrea Matranga, Joan Serrat
- **Comment**: None
- **Journal**: None
- **Summary**: Existing data on building destruction in conflict zones rely on eyewitness reports or manual detection, which makes it generally scarce, incomplete and potentially biased. This lack of reliable data imposes severe limitations for media reporting, humanitarian relief efforts, human rights monitoring, reconstruction initiatives, and academic studies of violent conflict. This article introduces an automated method of measuring destruction in high-resolution satellite images using deep learning techniques combined with data augmentation to expand training samples. We apply this method to the Syrian civil war and reconstruct the evolution of damage in major cities across the country. The approach allows generating destruction data with unprecedented scope, resolution, and frequency - only limited by the available satellite imagery - which can alleviate data limitations decisively.



### Shape-Texture Debiased Neural Network Training
- **Arxiv ID**: http://arxiv.org/abs/2010.05981v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.05981v2)
- **Published**: 2020-10-12 19:16:12+00:00
- **Updated**: 2021-03-30 19:16:30+00:00
- **Authors**: Yingwei Li, Qihang Yu, Mingxing Tan, Jieru Mei, Peng Tang, Wei Shen, Alan Yuille, Cihang Xie
- **Comment**: ICLR 2021. The code is available here:
  https://github.com/LiYingwei/ShapeTextureDebiasedTraining
- **Journal**: None
- **Summary**: Shape and texture are two prominent and complementary cues for recognizing objects. Nonetheless, Convolutional Neural Networks are often biased towards either texture or shape, depending on the training dataset. Our ablation shows that such bias degenerates model performance. Motivated by this observation, we develop a simple algorithm for shape-texture debiased learning. To prevent models from exclusively attending on a single cue in representation learning, we augment training data with images with conflicting shape and texture information (eg, an image of chimpanzee shape but with lemon texture) and, most importantly, provide the corresponding supervisions from shape and texture simultaneously.   Experiments show that our method successfully improves model performance on several image recognition benchmarks and adversarial robustness. For example, by training on ImageNet, it helps ResNet-152 achieve substantial improvements on ImageNet (+1.2%), ImageNet-A (+5.2%), ImageNet-C (+8.3%) and Stylized-ImageNet (+11.1%), and on defending against FGSM adversarial attacker on ImageNet (+14.4%). Our method also claims to be compatible with other advanced data augmentation strategies, eg, Mixup, and CutMix. The code is available here: https://github.com/LiYingwei/ShapeTextureDebiasedTraining.



### MedICaT: A Dataset of Medical Images, Captions, and Textual References
- **Arxiv ID**: http://arxiv.org/abs/2010.06000v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2010.06000v1)
- **Published**: 2020-10-12 19:56:08+00:00
- **Updated**: 2020-10-12 19:56:08+00:00
- **Authors**: Sanjay Subramanian, Lucy Lu Wang, Sachin Mehta, Ben Bogin, Madeleine van Zuylen, Sravanthi Parasa, Sameer Singh, Matt Gardner, Hannaneh Hajishirzi
- **Comment**: EMNLP-Findings 2020
- **Journal**: None
- **Summary**: Understanding the relationship between figures and text is key to scientific document understanding. Medical figures in particular are quite complex, often consisting of several subfigures (75% of figures in our dataset), with detailed text describing their content. Previous work studying figures in scientific papers focused on classifying figure content rather than understanding how images relate to the text. To address challenges in figure retrieval and figure-to-text alignment, we introduce MedICaT, a dataset of medical images in context. MedICaT consists of 217K images from 131K open access biomedical papers, and includes captions, inline references for 74% of figures, and manually annotated subfigures and subcaptions for a subset of figures. Using MedICaT, we introduce the task of subfigure to subcaption alignment in compound figures and demonstrate the utility of inline references in image-text matching. Our data and code can be accessed at https://github.com/allenai/medicat.



### Assessing Lesion Segmentation Bias of Neural Networks on Motion Corrupted Brain MRI
- **Arxiv ID**: http://arxiv.org/abs/2010.06027v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2010.06027v1)
- **Published**: 2020-10-12 21:06:40+00:00
- **Updated**: 2020-10-12 21:06:40+00:00
- **Authors**: Tejas Sudharshan Mathai, Yi Wang, Nathan Cross
- **Comment**: First two authors contributed equally. Accepted at MICCAI BrainLes
  2020
- **Journal**: None
- **Summary**: Patient motion during the magnetic resonance imaging (MRI) acquisition process results in motion artifacts, which limits the ability of radiologists to provide a quantitative assessment of a condition visualized. Often times, radiologists either "see through" the artifacts with reduced diagnostic confidence, or the MR scans are rejected and patients are asked to be recalled and re-scanned. Presently, there are many published approaches that focus on MRI artifact detection and correction. However, the key question of the bias exhibited by these algorithms on motion corrupted MRI images is still unanswered. In this paper, we seek to quantify the bias in terms of the impact that different levels of motion artifacts have on the performance of neural networks engaged in a lesion segmentation task. Additionally, we explore the effect of a different learning strategy, curriculum learning, on the segmentation performance. Our results suggest that a network trained using curriculum learning is effective at compensating for different levels of motion artifacts, and improved the segmentation performance by ~9%-15% (p < 0.05) when compared against a conventional shuffled learning strategy on the same motion data. Within each motion category, it either improved or maintained the dice score. To the best of our knowledge, we are the first to quantitatively assess the segmentation bias on various levels of motion artifacts present in a brain MRI image.



### Deep learning for detection and segmentation of artefact and disease instances in gastrointestinal endoscopy
- **Arxiv ID**: http://arxiv.org/abs/2010.06034v2
- **DOI**: 10.1016/j.media.2021.102002
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.06034v2)
- **Published**: 2020-10-12 21:22:37+00:00
- **Updated**: 2021-02-17 17:49:32+00:00
- **Authors**: Sharib Ali, Mariia Dmitrieva, Noha Ghatwary, Sophia Bano, Gorkem Polat, Alptekin Temizel, Adrian Krenzer, Amar Hekalo, Yun Bo Guo, Bogdan Matuszewski, Mourad Gridach, Irina Voiculescu, Vishnusai Yoganand, Arnav Chavan, Aryan Raj, Nhan T. Nguyen, Dat Q. Tran, Le Duy Huynh, Nicolas Boutry, Shahadate Rezvy, Haijian Chen, Yoon Ho Choi, Anand Subramanian, Velmurugan Balasubramanian, Xiaohong W. Gao, Hongyu Hu, Yusheng Liao, Danail Stoyanov, Christian Daul, Stefano Realdon, Renato Cannizzaro, Dominique Lamarque, Terry Tran-Nguyen, Adam Bailey, Barbara Braden, James East, Jens Rittscher
- **Comment**: 32 pages
- **Journal**: None
- **Summary**: The Endoscopy Computer Vision Challenge (EndoCV) is a crowd-sourcing initiative to address eminent problems in developing reliable computer aided detection and diagnosis endoscopy systems and suggest a pathway for clinical translation of technologies. Whilst endoscopy is a widely used diagnostic and treatment tool for hollow-organs, there are several core challenges often faced by endoscopists, mainly: 1) presence of multi-class artefacts that hinder their visual interpretation, and 2) difficulty in identifying subtle precancerous precursors and cancer abnormalities. Artefacts often affect the robustness of deep learning methods applied to the gastrointestinal tract organs as they can be confused with tissue of interest. EndoCV2020 challenges are designed to address research questions in these remits. In this paper, we present a summary of methods developed by the top 17 teams and provide an objective comparison of state-of-the-art methods and methods designed by the participants for two sub-challenges: i) artefact detection and segmentation (EAD2020), and ii) disease detection and segmentation (EDD2020). Multi-center, multi-organ, multi-class, and multi-modal clinical endoscopy datasets were compiled for both EAD2020 and EDD2020 sub-challenges. The out-of-sample generalization ability of detection algorithms was also evaluated. Whilst most teams focused on accuracy improvements, only a few methods hold credibility for clinical usability. The best performing teams provided solutions to tackle class imbalance, and variabilities in size, origin, modality and occurrences by exploring data augmentation, data fusion, and optimal class thresholding techniques.



### Spectral Synthesis for Satellite-to-Satellite Translation
- **Arxiv ID**: http://arxiv.org/abs/2010.06045v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2010.06045v1)
- **Published**: 2020-10-12 21:36:39+00:00
- **Updated**: 2020-10-12 21:36:39+00:00
- **Authors**: Thomas Vandal, Daniel McDuff, Weile Wang, Andrew Michaelis, Ramakrishna Nemani
- **Comment**: None
- **Journal**: None
- **Summary**: Earth observing satellites carrying multi-spectral sensors are widely used to monitor the physical and biological states of the atmosphere, land, and oceans. These satellites have different vantage points above the earth and different spectral imaging bands resulting in inconsistent imagery from one to another. This presents challenges in building downstream applications. What if we could generate synthetic bands for existing satellites from the union of all domains? We tackle the problem of generating synthetic spectral imagery for multispectral sensors as an unsupervised image-to-image translation problem with partial labels and introduce a novel shared spectral reconstruction loss. Simulated experiments performed by dropping one or more spectral bands show that cross-domain reconstruction outperforms measurements obtained from a second vantage point. On a downstream cloud detection task, we show that generating synthetic bands with our model improves segmentation performance beyond our baseline. Our proposed approach enables synchronization of multispectral data and provides a basis for more homogeneous remote sensing datasets.



### A catalog of broad morphology of Pan-STARRS galaxies based on deep learning
- **Arxiv ID**: http://arxiv.org/abs/2010.06073v1
- **DOI**: 10.3847/1538-4365/abc0ed
- **Categories**: **astro-ph.GA**, astro-ph.IM, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.06073v1)
- **Published**: 2020-10-12 23:20:35+00:00
- **Updated**: 2020-10-12 23:20:35+00:00
- **Authors**: Hunter Goddard, Lior Shamir
- **Comment**: ApJS, accepted
- **Journal**: None
- **Summary**: Autonomous digital sky surveys such as Pan-STARRS have the ability to image a very large number of galactic and extra-galactic objects, and the large and complex nature of the image data reinforces the use of automation. Here we describe the design and implementation of a data analysis process for automatic broad morphology annotation of galaxies, and applied it to the data of Pan-STARRS DR1. The process is based on filters followed by a two-step convolutional neural network (CNN) classification. Training samples are generated by using an augmented and balanced set of manually classified galaxies. Results are evaluated for accuracy by comparison to the annotation of Pan-STARRS included in a previous broad morphology catalog of SDSS galaxies. Our analysis shows that a CNN combined with several filters is an effective approach for annotating the galaxies and removing unclean images. The catalog contains morphology labels for 1,662,190 galaxies with ~95% accuracy. The accuracy can be further improved by selecting labels above certain confidence thresholds. The catalog is publicly available.



