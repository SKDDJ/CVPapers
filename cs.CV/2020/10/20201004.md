# Arxiv Papers in cs.CV on 2020-10-04
### 3D Orientation Field Transform
- **Arxiv ID**: http://arxiv.org/abs/2010.01453v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2010.01453v1)
- **Published**: 2020-10-04 00:29:46+00:00
- **Updated**: 2020-10-04 00:29:46+00:00
- **Authors**: Wai-Tsun Yeung, Xiaohao Cai, Zizhen Liang, Byung-Ho Kang
- **Comment**: None
- **Journal**: None
- **Summary**: The two-dimensional (2D) orientation field transform has been proved to be effective at enhancing 2D contours and curves in images by means of top-down processing. It, however, has no counterpart in three-dimensional (3D) images due to the extremely complicated orientation in 3D compared to 2D. Practically and theoretically, the demand and interest in 3D can only be increasing. In this work, we modularise the concept and generalise it to 3D curves. Different modular combinations are found to enhance curves to different extents and with different sensitivity to the packing of the 3D curves. In principle, the proposed 3D orientation field transform can naturally tackle any dimensions. As a special case, it is also ideal for 2D images, owning simpler methodology compared to the previous 2D orientation field transform. The proposed method is demonstrated with several transmission electron microscopy tomograms ranging from 2D curve enhancement to, the more important and interesting, 3D ones.



### LEGAN: Disentangled Manipulation of Directional Lighting and Facial Expressions by Leveraging Human Perceptual Judgements
- **Arxiv ID**: http://arxiv.org/abs/2010.01464v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.01464v3)
- **Published**: 2020-10-04 01:56:54+00:00
- **Updated**: 2021-06-18 22:00:59+00:00
- **Authors**: Sandipan Banerjee, Ajjen Joshi, Prashant Mahajan, Sneha Bhattacharya, Survi Kyal, Taniya Mishra
- **Comment**: AMFG Workshop at CVPR 2021 (CVPRW 2021). Get our synthetic face
  images & their perceptual labels here:
  https://github.com/Affectiva/LEGAN_Perceptual_Dataset
- **Journal**: None
- **Summary**: Building facial analysis systems that generalize to extreme variations in lighting and facial expressions is a challenging problem that can potentially be alleviated using natural-looking synthetic data. Towards that, we propose LEGAN, a novel synthesis framework that leverages perceptual quality judgments for jointly manipulating lighting and expressions in face images, without requiring paired training data. LEGAN disentangles the lighting and expression subspaces and performs transformations in the feature space before upscaling to the desired output image. The fidelity of the synthetic image is further refined by integrating a perceptual quality estimation model, trained with face images rendered using multiple synthesis methods and their crowd-sourced naturalness ratings, into the LEGAN framework as an auxiliary discriminator. Using objective metrics like FID and LPIPS, LEGAN is shown to generate higher quality face images when compared with popular GAN models like StarGAN and StarGAN-v2 for lighting and expression synthesis. We also conduct a perceptual study using images synthesized by LEGAN and other GAN models and show the correlation between our quality estimation and visual fidelity. Finally, we demonstrate the effectiveness of LEGAN as training data augmenter for expression recognition and face verification tasks.



### MDReg-Net: Multi-resolution diffeomorphic image registration using fully convolutional networks with deep self-supervision
- **Arxiv ID**: http://arxiv.org/abs/2010.01465v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.01465v1)
- **Published**: 2020-10-04 02:00:37+00:00
- **Updated**: 2020-10-04 02:00:37+00:00
- **Authors**: Hongming Li, Yong Fan
- **Comment**: None
- **Journal**: None
- **Summary**: We present a diffeomorphic image registration algorithm to learn spatial transformations between pairs of images to be registered using fully convolutional networks (FCNs) under a self-supervised learning setting. The network is trained to estimate diffeomorphic spatial transformations between pairs of images by maximizing an image-wise similarity metric between fixed and warped moving images, similar to conventional image registration algorithms. It is implemented in a multi-resolution image registration framework to optimize and learn spatial transformations at different image resolutions jointly and incrementally with deep self-supervision in order to better handle large deformation between images. A spatial Gaussian smoothing kernel is integrated with the FCNs to yield sufficiently smooth deformation fields to achieve diffeomorphic image registration. Particularly, spatial transformations learned at coarser resolutions are utilized to warp the moving image, which is subsequently used for learning incremental transformations at finer resolutions. This procedure proceeds recursively to the full image resolution and the accumulated transformations serve as the final transformation to warp the moving image at the finest resolution. Experimental results for registering high resolution 3D structural brain magnetic resonance (MR) images have demonstrated that image registration networks trained by our method obtain robust, diffeomorphic image registration results within seconds with improved accuracy compared with state-of-the-art image registration algorithms.



### Generalized Two-Dimensional Quaternion Principal Component Analysis with Weighting for Color Image Recognition
- **Arxiv ID**: http://arxiv.org/abs/2010.01477v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.01477v2)
- **Published**: 2020-10-04 03:37:23+00:00
- **Updated**: 2023-06-12 03:28:15+00:00
- **Authors**: Zhi-Gang Jia, Zi-Jin Qiu, Qian-Yu Wang, Mei-Xiang Zhao, Dan-Dan Zhu
- **Comment**: 17 pages, 13 figures
- **Journal**: None
- **Summary**: One of the most powerful methods of color image recognition is the two-dimensional principle component analysis (2DQPCA) approach, which is based on quaternion representation and preserves color information very well. However, the current versions of 2DQPCA are still not feasible to extract different geometric properties of color images according to practical data analysis requirements and they are vulnerable to strong noise. In this paper, a generalized 2DQPCA approach with weighting is presented with imposing $L_{p}$ norms on both constraint and objective functions. As a unit 2DQPCA framework, this new version makes it possible to choose adaptive regularizations and constraints according to actual applications and can extract both geometric properties and color information of color images. The projection vectors generated by the deflating scheme are required to be orthogonal to each other. A weighting matrix is defined to magnify the effect of main features. This overcomes the shortcomings of traditional 2DQPCA that the recognition rate decreases as the number of principal components increases. The numerical results based on the real face databases validate that the newly proposed method is robust to noise and performs better than the state-of-the-art 2DQPCA-based algorithms and four prominent deep learning methods.



### Improving Lesion Detection by exploring bias on Skin Lesion dataset
- **Arxiv ID**: http://arxiv.org/abs/2010.01485v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.01485v1)
- **Published**: 2020-10-04 05:04:58+00:00
- **Updated**: 2020-10-04 05:04:58+00:00
- **Authors**: Anusua Trivedi, Sreya Muppalla, Shreyaan Pathak, Azadeh Mobasher, Pawel Janowski, Rahul Dodhia, Juan M. Lavista Ferres
- **Comment**: None
- **Journal**: None
- **Summary**: All datasets contain some biases, often unintentional, due to how they were acquired and annotated. These biases distort machine-learning models' performance, creating spurious correlations that the models can unfairly exploit, or, contrarily destroying clear correlations that the models could learn. With the popularity of deep learning models, automated skin lesion analysis is starting to play an essential role in the early detection of Melanoma. The ISIC Archive is one of the most used skin lesion sources to benchmark deep learning-based tools. Bissoto et al. experimented with different bounding-box based masks and showed that deep learning models could classify skin lesion images without clinically meaningful information in the input data. Their findings seem confounding since the ablated regions (random rectangular boxes) are not significant. The shape of the lesion is a crucial factor in the clinical characterization of a skin lesion. In that context, we performed a set of experiments that generate shape-preserving masks instead of rectangular bounding-box based masks. A deep learning model trained on these shape-preserving masked images does not outperform models trained on images without clinically meaningful information. That strongly suggests spurious correlations guiding the models. We propose use of general adversarial network (GAN) to mitigate the underlying bias.



### A New Mask R-CNN Based Method for Improved Landslide Detection
- **Arxiv ID**: http://arxiv.org/abs/2010.01499v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.01499v1)
- **Published**: 2020-10-04 07:46:37+00:00
- **Updated**: 2020-10-04 07:46:37+00:00
- **Authors**: Silvia Liberata Ullo, Amrita Mohan, Alessandro Sebastianelli, Shaik Ejaz Ahamed, Basant Kumar, Ramji Dwivedi, G. R. Sinha
- **Comment**: 9 pages, 8 figures, 6 tables, submitted to JSTARS special issue on
  Cultural Heritage
- **Journal**: None
- **Summary**: This paper presents a novel method of landslide detection by exploiting the Mask R-CNN capability of identifying an object layout by using a pixel-based segmentation, along with transfer learning used to train the proposed model. A data set of 160 elements is created containing landslide and non-landslide images. The proposed method consists of three steps: (i) augmenting training image samples to increase the volume of the training data, (ii) fine tuning with limited image samples, and (iii) performance evaluation of the algorithm in terms of precision, recall and F1 measure, on the considered landslide images, by adopting ResNet-50 and 101 as backbone models. The experimental results are quite encouraging as the proposed method achieves Precision equals to 1.00, Recall 0.93 and F1 measure 0.97, when ResNet-101 is used as backbone model, and with a low number of landslide photographs used as training samples. The proposed algorithm can be potentially useful for land use planners and policy makers of hilly areas where intermittent slope deformations necessitate landslide detection as prerequisite before planning.



### A Study for Universal Adversarial Attacks on Texture Recognition
- **Arxiv ID**: http://arxiv.org/abs/2010.01506v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2010.01506v1)
- **Published**: 2020-10-04 08:11:11+00:00
- **Updated**: 2020-10-04 08:11:11+00:00
- **Authors**: Yingpeng Deng, Lina J. Karam
- **Comment**: None
- **Journal**: None
- **Summary**: Given the outstanding progress that convolutional neural networks (CNNs) have made on natural image classification and object recognition problems, it is shown that deep learning methods can achieve very good recognition performance on many texture datasets. However, while CNNs for natural image classification/object recognition tasks have been revealed to be highly vulnerable to various types of adversarial attack methods, the robustness of deep learning methods for texture recognition is yet to be examined. In our paper, we show that there exist small image-agnostic/univesal perturbations that can fool the deep learning models with more than 80\% of testing fooling rates on all tested texture datasets. The computed perturbations using various attack methods on the tested datasets are generally quasi-imperceptible, containing structured patterns with low, middle and high frequency components.



### Remembering for the Right Reasons: Explanations Reduce Catastrophic Forgetting
- **Arxiv ID**: http://arxiv.org/abs/2010.01528v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.01528v2)
- **Published**: 2020-10-04 10:05:27+00:00
- **Updated**: 2021-05-03 03:26:30+00:00
- **Authors**: Sayna Ebrahimi, Suzanne Petryk, Akash Gokul, William Gan, Joseph E. Gonzalez, Marcus Rohrbach, Trevor Darrell
- **Comment**: Accepted at ICLR 2021
- **Journal**: None
- **Summary**: The goal of continual learning (CL) is to learn a sequence of tasks without suffering from the phenomenon of catastrophic forgetting. Previous work has shown that leveraging memory in the form of a replay buffer can reduce performance degradation on prior tasks. We hypothesize that forgetting can be further reduced when the model is encouraged to remember the \textit{evidence} for previously made decisions. As a first step towards exploring this hypothesis, we propose a simple novel training paradigm, called Remembering for the Right Reasons (RRR), that additionally stores visual model explanations for each example in the buffer and ensures the model has "the right reasons" for its predictions by encouraging its explanations to remain consistent with those used to make decisions at training time. Without this constraint, there is a drift in explanations and increase in forgetting as conventional continual learning algorithms learn new tasks. We demonstrate how RRR can be easily added to any memory or regularization-based approach and results in reduced forgetting, and more importantly, improved model explanations. We have evaluated our approach in the standard and few-shot settings and observed a consistent improvement across various CL approaches using different architectures and techniques to generate model explanations and demonstrated our approach showing a promising connection between explainability and continual learning. Our code is available at \url{https://github.com/SaynaEbrahimi/Remembering-for-the-Right-Reasons}.



### Towards Cross-modality Medical Image Segmentation with Online Mutual Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/2010.01532v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.01532v1)
- **Published**: 2020-10-04 10:25:13+00:00
- **Updated**: 2020-10-04 10:25:13+00:00
- **Authors**: Kang Li, Lequan Yu, Shujun Wang, Pheng-Ann Heng
- **Comment**: Accepted by AAAI 2020
- **Journal**: AAAI 2020
- **Summary**: The success of deep convolutional neural networks is partially attributed to the massive amount of annotated training data. However, in practice, medical data annotations are usually expensive and time-consuming to be obtained. Considering multi-modality data with the same anatomic structures are widely available in clinic routine, in this paper, we aim to exploit the prior knowledge (e.g., shape priors) learned from one modality (aka., assistant modality) to improve the segmentation performance on another modality (aka., target modality) to make up annotation scarcity. To alleviate the learning difficulties caused by modality-specific appearance discrepancy, we first present an Image Alignment Module (IAM) to narrow the appearance gap between assistant and target modality data.We then propose a novel Mutual Knowledge Distillation (MKD) scheme to thoroughly exploit the modality-shared knowledge to facilitate the target-modality segmentation. To be specific, we formulate our framework as an integration of two individual segmentors. Each segmentor not only explicitly extracts one modality knowledge from corresponding annotations, but also implicitly explores another modality knowledge from its counterpart in mutual-guided manner. The ensemble of two segmentors would further integrate the knowledge from both modalities and generate reliable segmentation results on target modality. Experimental results on the public multi-class cardiac segmentation data, i.e., MMWHS 2017, show that our method achieves large improvements on CT segmentation by utilizing additional MRI data and outperforms other state-of-the-art multi-modality learning methods.



### Static and Animated 3D Scene Generation from Free-form Text Descriptions
- **Arxiv ID**: http://arxiv.org/abs/2010.01549v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2010.01549v2)
- **Published**: 2020-10-04 11:31:21+00:00
- **Updated**: 2020-11-28 19:28:30+00:00
- **Authors**: Faria Huq, Nafees Ahmed, Anindya Iqbal
- **Comment**: None
- **Journal**: None
- **Summary**: Generating coherent and useful image/video scenes from a free-form textual description is technically a very difficult problem to handle. Textual description of the same scene can vary greatly from person to person, or sometimes even for the same person from time to time. As the choice of words and syntax vary while preparing a textual description, it is challenging for the system to reliably produce a consistently desirable output from different forms of language input. The prior works of scene generation have been mostly confined to rigorous sentence structures of text input which restrict the freedom of users to write description. In our work, we study a new pipeline that aims to generate static as well as animated 3D scenes from different types of free-form textual scene description without any major restriction. In particular, to keep our study practical and tractable, we focus on a small subspace of all possible 3D scenes, containing various combinations of cube, cylinder and sphere. We design a two-stage pipeline. In the first stage, we encode the free-form text using an encoder-decoder neural architecture. In the second stage, we generate a 3D scene based on the generated encoding. Our neural architecture exploits state-of-the-art language model as encoder to leverage rich contextual encoding and a new multi-head decoder to predict multiple features of an object in the scene simultaneously. For our experiments, we generate a large synthetic data-set which contains 13,00,000 and 14,00,000 samples of unique static and animated scene descriptions, respectively. We achieve 98.427% accuracy on test data set in detecting the 3D objects features successfully. Our work shows a proof of concept of one approach towards solving the problem, and we believe with enough training data, the same pipeline can be expanded to handle even broader set of 3D scene generation problems.



### The FaceChannelS: Strike of the Sequences for the AffWild 2 Challenge
- **Arxiv ID**: http://arxiv.org/abs/2010.01557v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2010.01557v1)
- **Published**: 2020-10-04 12:00:48+00:00
- **Updated**: 2020-10-04 12:00:48+00:00
- **Authors**: Pablo Barros, Alessandra Sciutti
- **Comment**: Submission that describes the models submitted to the AffWild2
  challenge
- **Journal**: None
- **Summary**: Predicting affective information from human faces became a popular task for most of the machine learning community in the past years. The development of immense and dense deep neural networks was backed by the availability of numerous labeled datasets. These models, most of the time, present state-of-the-art results in such benchmarks, but are very difficult to adapt to other scenarios. In this paper, we present one more chapter of benchmarking different versions of the FaceChannel neural network: we demonstrate how our little model can predict affective information from the facial expression on the novel AffWild2 dataset.



### Facial gesture interfaces for expression and communication
- **Arxiv ID**: http://arxiv.org/abs/2010.01567v1
- **DOI**: 10.1109/ICSMC.2004.1398365
- **Categories**: **cs.HC**, cs.CV, H.5.2
- **Links**: [PDF](http://arxiv.org/pdf/2010.01567v1)
- **Published**: 2020-10-04 12:51:48+00:00
- **Updated**: 2020-10-04 12:51:48+00:00
- **Authors**: Michael J. Lyons
- **Comment**: 6 pages, 8 figures
- **Journal**: 2004 IEEE International Conference on Systems, Man and Cybernetics
- **Summary**: Considerable effort has been devoted to the automatic extraction of information about action of the face from image sequences. Within the context of human-computer interaction (HCI) we may distinguish systems that allow expression from those which aim at recognition. Most of the work in facial action processing has been directed at automatically recognizing affect from facial actions. By contrast, facial gesture interfaces, which respond to deliberate facial actions, have received comparatively little attention. This paper reviews several projects on vision-based interfaces that rely on facial action for intentional HCI. Applications to several domains are introduced, including text entry, artistic and musical expression and assistive technology for motor-impaired users.



### Collaborative Tracking and Capture of Aerial Object using UAVs
- **Arxiv ID**: http://arxiv.org/abs/2010.01588v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.SY, eess.SY
- **Links**: [PDF](http://arxiv.org/pdf/2010.01588v1)
- **Published**: 2020-10-04 14:23:03+00:00
- **Updated**: 2020-10-04 14:23:03+00:00
- **Authors**: Lima Agnel Tony, Shuvrangshu Jana, Varun V P, Vidyadhara B V, Mohitvishnu S Gadde, Abhishek Kashyap, Rahul Ravichandran, Debasish Ghose
- **Comment**: None
- **Journal**: MBZIRC Symposium 2020, ADNEC, Abu Dhabi
- **Summary**: This work details the problem of aerial target capture using multiple UAVs. This problem is motivated from the challenge 1 of Mohammed Bin Zayed International Robotic Challenge 2020. The UAVs utilise visual feedback to autonomously detect target, approach it and capture without disturbing the vehicle which carries the target. Multi-UAV collaboration improves the efficiency of the system and increases the chance of capturing the ball robustly in short span of time. In this paper, the proposed architecture is validated through simulation in ROS-Gazebo environment and is further implemented on hardware.



### Unknown Presentation Attack Detection against Rational Attackers
- **Arxiv ID**: http://arxiv.org/abs/2010.01592v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.GT, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.01592v2)
- **Published**: 2020-10-04 14:37:10+00:00
- **Updated**: 2021-07-02 22:37:17+00:00
- **Authors**: Ali Khodabakhsh, Zahid Akhtar
- **Comment**: None
- **Journal**: None
- **Summary**: Despite the impressive progress in the field of presentation attack detection and multimedia forensics over the last decade, these systems are still vulnerable to attacks in real-life settings. Some of the challenges for existing solutions are the detection of unknown attacks, the ability to perform in adversarial settings, few-shot learning, and explainability. In this study, these limitations are approached by reliance on a game-theoretic view for modeling the interactions between the attacker and the detector. Consequently, a new optimization criterion is proposed and a set of requirements are defined for improving the performance of these systems in real-life settings. Furthermore, a novel detection technique is proposed using generator-based feature sets that are not biased towards any specific attack species. To further optimize the performance on known attacks, a new loss function coined categorical margin maximization loss (C-marmax) is proposed which gradually improves the performance against the most powerful attack. The proposed approach provides a more balanced performance across known and unknown attacks and achieves state-of-the-art performance in known and unknown attack detection cases against rational attackers. Lastly, the few-shot learning potential of the proposed approach is studied as well as its ability to provide pixel-level explainability.



### AIFNet: Automatic Vascular Function Estimation for Perfusion Analysis Using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2010.01617v1
- **DOI**: 10.1016/j.media.2021.102211
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.01617v1)
- **Published**: 2020-10-04 16:14:45+00:00
- **Updated**: 2020-10-04 16:14:45+00:00
- **Authors**: Ezequiel de la Rosa, Diana M. Sima, Bjoern Menze, Jan S. Kirschke, David Robben
- **Comment**: Preprint submitted to Elsevier
- **Journal**: Medical Image Analysis 74 (2021): 102211
- **Summary**: Perfusion imaging is crucial in acute ischemic stroke for quantifying the salvageable penumbra and irreversibly damaged core lesions. As such, it helps clinicians to decide on the optimal reperfusion treatment. In perfusion CT imaging, deconvolution methods are used to obtain clinically interpretable perfusion parameters that allow identifying brain tissue abnormalities. Deconvolution methods require the selection of two reference vascular functions as inputs to the model: the arterial input function (AIF) and the venous output function, with the AIF as the most critical model input. When manually performed, the vascular function selection is time demanding, suffers from poor reproducibility and is subject to the professionals' experience. This leads to potentially unreliable quantification of the penumbra and core lesions and, hence, might harm the treatment decision process. In this work we automatize the perfusion analysis with AIFNet, a fully automatic and end-to-end trainable deep learning approach for estimating the vascular functions. Unlike previous methods using clustering or segmentation techniques to select vascular voxels, AIFNet is directly optimized at the vascular function estimation, which allows to better recognise the time-curve profiles. Validation on the public ISLES18 stroke database shows that AIFNet reaches inter-rater performance for the vascular function estimation and, subsequently, for the parameter maps and core lesion quantification obtained through deconvolution. We conclude that AIFNet has potential for clinical transfer and could be incorporated in perfusion deconvolution software.



### AFN: Attentional Feedback Network based 3D Terrain Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2010.01626v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.01626v1)
- **Published**: 2020-10-04 16:51:39+00:00
- **Updated**: 2020-10-04 16:51:39+00:00
- **Authors**: Ashish Kubade, Diptiben Patel, Avinash Sharma, K. S. Rajan
- **Comment**: Accepted as oral at ACCV 2020
- **Journal**: None
- **Summary**: Terrain, representing features of an earth surface, plays a crucial role in many applications such as simulations, route planning, analysis of surface dynamics, computer graphics-based games, entertainment, films, to name a few. With recent advancements in digital technology, these applications demand the presence of high-resolution details in the terrain. In this paper, we propose a novel fully convolutional neural network-based super-resolution architecture to increase the resolution of low-resolution Digital Elevation Model (LRDEM) with the help of information extracted from the corresponding aerial image as a complementary modality. We perform the super-resolution of LRDEM using an attention-based feedback mechanism named 'Attentional Feedback Network' (AFN), which selectively fuses the information from LRDEM and aerial image to enhance and infuse the high-frequency features and to produce the terrain realistically. We compare the proposed architecture with existing state-of-the-art DEM super-resolution methods and show that the proposed architecture outperforms enhancing the resolution of input LRDEM accurately and in a realistic manner.



### Mapping of Sparse 3D Data using Alternating Projection
- **Arxiv ID**: http://arxiv.org/abs/2010.02516v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2010.02516v2)
- **Published**: 2020-10-04 17:40:30+00:00
- **Updated**: 2020-10-09 18:22:45+00:00
- **Authors**: Siddhant Ranade, Xin Yu, Shantnu Kakkar, Pedro Miraldo, Srikumar Ramalingam
- **Comment**: ACCV2020 oral. This article supersedes arXiv:1906.05888
- **Journal**: None
- **Summary**: We propose a novel technique to register sparse 3D scans in the absence of texture. While existing methods such as KinectFusion or Iterative Closest Points (ICP) heavily rely on dense point clouds, this task is particularly challenging under sparse conditions without RGB data. Sparse texture-less data does not come with high-quality boundary signal, and this prohibits the use of correspondences from corners, junctions, or boundary lines. Moreover, in the case of sparse data, it is incorrect to assume that the same point will be captured in two consecutive scans. We take a different approach and first re-parameterize the point-cloud using a large number of line segments. In this re-parameterized data, there exists a large number of line intersection (and not correspondence) constraints that allow us to solve the registration task. We propose the use of a two-step alternating projection algorithm by formulating the registration as the simultaneous satisfaction of intersection and rigidity constraints. The proposed approach outperforms other top-scoring algorithms on both Kinect and LiDAR datasets. In Kinect, we can use 100X downsampled sparse data and still outperform competing methods operating on full-resolution data.



### Supporting large-scale image recognition with out-of-domain samples
- **Arxiv ID**: http://arxiv.org/abs/2010.01650v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.01650v1)
- **Published**: 2020-10-04 18:44:01+00:00
- **Updated**: 2020-10-04 18:44:01+00:00
- **Authors**: Christof Henkel, Philipp Singer
- **Comment**: None
- **Journal**: None
- **Summary**: This article presents an efficient end-to-end method to perform instance-level recognition employed to the task of labeling and ranking landmark images. In a first step, we embed images in a high dimensional feature space using convolutional neural networks trained with an additive angular margin loss and classify images using visual similarity. We then efficiently re-rank predictions and filter noise utilizing similarity to out-of-domain images. Using this approach we achieved the 1st place in the 2020 edition of the Google Landmark Recognition challenge.



### KiU-Net: Overcomplete Convolutional Architectures for Biomedical Image and Volumetric Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2010.01663v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2010.01663v2)
- **Published**: 2020-10-04 19:23:33+00:00
- **Updated**: 2021-10-14 20:27:36+00:00
- **Authors**: Jeya Maria Jose Valanarasu, Vishwanath A. Sindagi, Ilker Hacihaliloglu, Vishal M. Patel
- **Comment**: Journal Extension of KiU-Net (MICCAI-2020)
- **Journal**: None
- **Summary**: Most methods for medical image segmentation use U-Net or its variants as they have been successful in most of the applications. After a detailed analysis of these "traditional" encoder-decoder based approaches, we observed that they perform poorly in detecting smaller structures and are unable to segment boundary regions precisely. This issue can be attributed to the increase in receptive field size as we go deeper into the encoder. The extra focus on learning high level features causes the U-Net based approaches to learn less information about low-level features which are crucial for detecting small structures. To overcome this issue, we propose using an overcomplete convolutional architecture where we project our input image into a higher dimension such that we constrain the receptive field from increasing in the deep layers of the network. We design a new architecture for image segmentation- KiU-Net which has two branches: (1) an overcomplete convolutional network Kite-Net which learns to capture fine details and accurate edges of the input, and (2) U-Net which learns high level features. Furthermore, we also propose KiU-Net 3D which is a 3D convolutional architecture for volumetric segmentation. We perform a detailed study of KiU-Net by performing experiments on five different datasets covering various image modalities like ultrasound (US), magnetic resonance imaging (MRI), computed tomography (CT), microscopic and fundus images. The proposed method achieves a better performance as compared to all the recent methods with an additional benefit of fewer parameters and faster convergence. Additionally, we also demonstrate that the extensions of KiU-Net based on residual blocks and dense blocks result in further performance improvements. The implementation of KiU-Net can be found here: https://github.com/jeya-maria-jose/KiU-Net-pytorch



### Multi-Resolution Fusion and Multi-scale Input Priors Based Crowd Counting
- **Arxiv ID**: http://arxiv.org/abs/2010.01664v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.01664v1)
- **Published**: 2020-10-04 19:30:13+00:00
- **Updated**: 2020-10-04 19:30:13+00:00
- **Authors**: Usman Sajid, Wenchi Ma, Guanghui Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Crowd counting in still images is a challenging problem in practice due to huge crowd-density variations, large perspective changes, severe occlusion, and variable lighting conditions. The state-of-the-art patch rescaling module (PRM) based approaches prove to be very effective in improving the crowd counting performance. However, the PRM module requires an additional and compromising crowd-density classification process. To address these issues and challenges, the paper proposes a new multi-resolution fusion based end-to-end crowd counting network. It employs three deep-layers based columns/branches, each catering the respective crowd-density scale. These columns regularly fuse (share) the information with each other. The network is divided into three phases with each phase containing one or more columns. Three input priors are introduced to serve as an efficient and effective alternative to the PRM module, without requiring any additional classification operations. Along with the final crowd count regression head, the network also contains three auxiliary crowd estimation regression heads, which are strategically placed at each phase end to boost the overall performance. Comprehensive experiments on three benchmark datasets demonstrate that the proposed approach outperforms all the state-of-the-art models under the RMSE evaluation metric. The proposed approach also has better generalization capability with the best results during the cross-dataset experiments.



### Surface Agnostic Metrics for Cortical Volume Segmentation and Regression
- **Arxiv ID**: http://arxiv.org/abs/2010.01669v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.01669v1)
- **Published**: 2020-10-04 19:46:04+00:00
- **Updated**: 2020-10-04 19:46:04+00:00
- **Authors**: Samuel Budd, Prachi Patkee, Ana Baburamani, Mary Rutherford, Emma C. Robinson, Bernhard Kainz
- **Comment**: Best Paper Honourable Mentions @ MLCN 2020 (MICCAI 2020)
- **Journal**: None
- **Summary**: The cerebral cortex performs higher-order brain functions and is thus implicated in a range of cognitive disorders. Current analysis of cortical variation is typically performed by fitting surface mesh models to inner and outer cortical boundaries and investigating metrics such as surface area and cortical curvature or thickness. These, however, take a long time to run, and are sensitive to motion and image and surface resolution, which can prohibit their use in clinical settings. In this paper, we instead propose a machine learning solution, training a novel architecture to predict cortical thickness and curvature metrics from T2 MRI images, while additionally returning metrics of prediction uncertainty. Our proposed model is tested on a clinical cohort (Down Syndrome) for which surface-based modelling often fails. Results suggest that deep convolutional neural networks are a viable option to predict cortical metrics across a range of brain development stages and pathologies.



### Learning Complete 3D Morphable Face Models from Images and Videos
- **Arxiv ID**: http://arxiv.org/abs/2010.01679v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2010.01679v1)
- **Published**: 2020-10-04 20:51:23+00:00
- **Updated**: 2020-10-04 20:51:23+00:00
- **Authors**: Mallikarjun B R, Ayush Tewari, Hans-Peter Seidel, Mohamed Elgharib, Christian Theobalt
- **Comment**: Project Page - https://gvv.mpi-inf.mpg.de/projects/LeMoMo
- **Journal**: None
- **Summary**: Most 3D face reconstruction methods rely on 3D morphable models, which disentangle the space of facial deformations into identity geometry, expressions and skin reflectance. These models are typically learned from a limited number of 3D scans and thus do not generalize well across different identities and expressions. We present the first approach to learn complete 3D models of face identity geometry, albedo and expression just from images and videos. The virtually endless collection of such data, in combination with our self-supervised learning-based approach allows for learning face models that generalize beyond the span of existing approaches. Our network design and loss functions ensure a disentangled parameterization of not only identity and albedo, but also, for the first time, an expression basis. Our method also allows for in-the-wild monocular reconstruction at test time. We show that our learned models better generalize and lead to higher quality image-based reconstructions than existing approaches.



### Generating Gameplay-Relevant Art Assets with Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/2010.01681v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2010.01681v1)
- **Published**: 2020-10-04 20:58:40+00:00
- **Updated**: 2020-10-04 20:58:40+00:00
- **Authors**: Adrian Gonzalez, Matthew Guzdial, Felix Ramos
- **Comment**: 7 pages, 8 figures
- **Journal**: Proceedings of the 2020 Experimental AI in Games Workshop
- **Summary**: In game development, designing compelling visual assets that convey gameplay-relevant features requires time and experience. Recent image generation methods that create high-quality content could reduce development costs, but these approaches do not consider game mechanics. We propose a Convolutional Variational Autoencoder (CVAE) system to modify and generate new game visuals based on their gameplay relevance. We test this approach with Pok\'emon sprites and Pok\'emon type information, since types are one of the game's core mechanics and they directly impact the game's visuals. Our experimental results indicate that adopting a transfer learning approach can help to improve visual quality and stability over unseen data.



### MetaDetect: Uncertainty Quantification and Prediction Quality Estimates for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2010.01695v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2010.01695v2)
- **Published**: 2020-10-04 21:49:23+00:00
- **Updated**: 2020-10-06 15:38:53+00:00
- **Authors**: Marius Schubert, Karsten Kahl, Matthias Rottmann
- **Comment**: 11 pages, 5 figures, 5 tables
- **Journal**: None
- **Summary**: In object detection with deep neural networks, the box-wise objectness score tends to be overconfident, sometimes even indicating high confidence in presence of inaccurate predictions. Hence, the reliability of the prediction and therefore reliable uncertainties are of highest interest. In this work, we present a post processing method that for any given neural network provides predictive uncertainty estimates and quality estimates. These estimates are learned by a post processing model that receives as input a hand-crafted set of transparent metrics in form of a structured dataset. Therefrom, we learn two tasks for predicted bounding boxes. We discriminate between true positives ($\mathit{IoU}\geq0.5$) and false positives ($\mathit{IoU} < 0.5$) which we term meta classification, and we predict $\mathit{IoU}$ values directly which we term meta regression. The probabilities of the meta classification model aim at learning the probabilities of success and failure and therefore provide a modelled predictive uncertainty estimate. On the other hand, meta regression gives rise to a quality estimate. In numerical experiments, we use the publicly available YOLOv3 network and the Faster-RCNN network and evaluate meta classification and regression performance on the Kitti, Pascal VOC and COCO datasets. We demonstrate that our metrics are indeed well correlated with the $\mathit{IoU}$. For meta classification we obtain classification accuracies of up to 98.92% and AUROCs of up to 99.93%. For meta regression we obtain an $R^2$ value of up to 91.78%. These results yield significant improvements compared to other network's objectness score and other baseline approaches. Therefore, we obtain more reliable uncertainty and quality estimates which is particularly interesting in the absence of ground truth.



